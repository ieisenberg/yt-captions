[
  {
    "text": "hello everyone how are we doing thank",
    "start": "760",
    "end": "2919"
  },
  {
    "text": "you for coming to in Friday afternoon",
    "start": "2919",
    "end": "5160"
  },
  {
    "text": "session um today we'll be talking about",
    "start": "5160",
    "end": "8280"
  },
  {
    "text": "Bloomberg's journey to improve resour",
    "start": "8280",
    "end": "10160"
  },
  {
    "text": "utilization in multicluster platform my",
    "start": "10160",
    "end": "13080"
  },
  {
    "text": "name is Leon and I I am a software",
    "start": "13080",
    "end": "15519"
  },
  {
    "text": "engineer on the data science platform",
    "start": "15519",
    "end": "16960"
  },
  {
    "text": "team at",
    "start": "16960",
    "end": "18600"
  },
  {
    "text": "Bloomberg uh hi uh I am Yao I'm a senior",
    "start": "18600",
    "end": "21800"
  },
  {
    "text": "software de uh developer at Bloomberg",
    "start": "21800",
    "end": "24560"
  },
  {
    "text": "data science platform team",
    "start": "24560",
    "end": "27840"
  },
  {
    "text": "yeah cool so for today agenda we'll",
    "start": "27840",
    "end": "30960"
  },
  {
    "text": "first be talking about some common",
    "start": "30960",
    "end": "32800"
  },
  {
    "text": "challenges with managing large GPU",
    "start": "32800",
    "end": "35120"
  },
  {
    "text": "clusters and then we'll talk about how",
    "start": "35120",
    "end": "37399"
  },
  {
    "text": "we can solve these challenges by bending",
    "start": "37399",
    "end": "40079"
  },
  {
    "text": "the GPU growth curve with a",
    "start": "40079",
    "end": "42360"
  },
  {
    "text": "scheduler and then we'll talk about how",
    "start": "42360",
    "end": "44360"
  },
  {
    "text": "we can use kamada scheduler for the data",
    "start": "44360",
    "end": "46680"
  },
  {
    "text": "science platform team uh data science",
    "start": "46680",
    "end": "48760"
  },
  {
    "text": "platform at Bloomberg and lastly we'll",
    "start": "48760",
    "end": "51000"
  },
  {
    "text": "wrap everything up with the future",
    "start": "51000",
    "end": "53320"
  },
  {
    "text": "project roadmap for",
    "start": "53320",
    "end": "56680"
  },
  {
    "text": "kamada our data science platform is an",
    "start": "57719",
    "end": "60680"
  },
  {
    "text": "onp parameter beer metal kubernetes",
    "start": "60680",
    "end": "62680"
  },
  {
    "text": "cluster and a One-Stop shop uh",
    "start": "62680",
    "end": "65680"
  },
  {
    "text": "kubernetes infrastructure and a One-Stop",
    "start": "65680",
    "end": "67960"
  },
  {
    "text": "shop for every stage of the machine",
    "start": "67960",
    "end": "69759"
  },
  {
    "text": "learning life cycle from running",
    "start": "69759",
    "end": "71880"
  },
  {
    "text": "experiments to batch training workloads",
    "start": "71880",
    "end": "74720"
  },
  {
    "text": "to deploying inference applications in",
    "start": "74720",
    "end": "77479"
  },
  {
    "text": "production and when it comes to",
    "start": "77479",
    "end": "79680"
  },
  {
    "text": "challenges in managing a large GPU",
    "start": "79680",
    "end": "81759"
  },
  {
    "text": "cluster you will hear all about them",
    "start": "81759",
    "end": "83960"
  },
  {
    "text": "today and how we overcome these",
    "start": "83960",
    "end": "86240"
  },
  {
    "text": "challenges",
    "start": "86240",
    "end": "89240"
  },
  {
    "text": "with each new generation of large models",
    "start": "90439",
    "end": "93000"
  },
  {
    "text": "the resource needed for training",
    "start": "93000",
    "end": "94759"
  },
  {
    "text": "increases significantly along with the",
    "start": "94759",
    "end": "96799"
  },
  {
    "text": "need to serve more inference",
    "start": "96799",
    "end": "99759"
  },
  {
    "text": "applications and both of these",
    "start": "99759",
    "end": "102439"
  },
  {
    "text": "requirements are calling for new",
    "start": "102439",
    "end": "104360"
  },
  {
    "text": "generation of gpus to handle more data",
    "start": "104360",
    "end": "106799"
  },
  {
    "text": "in larger models further raising the bar",
    "start": "106799",
    "end": "109799"
  },
  {
    "text": "for machine learning infrastructure",
    "start": "109799",
    "end": "113159"
  },
  {
    "text": "requirements additionally today's GPU",
    "start": "113159",
    "end": "115560"
  },
  {
    "text": "nodes consumes a lot more power than the",
    "start": "115560",
    "end": "117759"
  },
  {
    "text": "average CPU node or previous gener ation",
    "start": "117759",
    "end": "120200"
  },
  {
    "text": "of the GPU nodes meaning more power",
    "start": "120200",
    "end": "122880"
  },
  {
    "text": "capacity is needed in order to grow the",
    "start": "122880",
    "end": "125280"
  },
  {
    "text": "GPU cluster capacity as",
    "start": "125280",
    "end": "127920"
  },
  {
    "text": "well as the GPU demand curve is",
    "start": "127920",
    "end": "130800"
  },
  {
    "text": "accelerating we need to be Innovative",
    "start": "130800",
    "end": "133200"
  },
  {
    "text": "and bend the demand curve by improving",
    "start": "133200",
    "end": "135640"
  },
  {
    "text": "the resource utilization rate so that",
    "start": "135640",
    "end": "138280"
  },
  {
    "text": "our GPU cluster growth can keep up with",
    "start": "138280",
    "end": "140720"
  },
  {
    "text": "the GPU",
    "start": "140720",
    "end": "143200"
  },
  {
    "text": "demand so top of our list of challenges",
    "start": "143239",
    "end": "146800"
  },
  {
    "text": "we found that our resour realization",
    "start": "146800",
    "end": "148959"
  },
  {
    "text": "rate is held down by over",
    "start": "148959",
    "end": "151599"
  },
  {
    "text": "budgeting so in this diagram it",
    "start": "151599",
    "end": "154120"
  },
  {
    "text": "illustrates the resource utilization of",
    "start": "154120",
    "end": "156400"
  },
  {
    "text": "batch workload Illustrated in green and",
    "start": "156400",
    "end": "159000"
  },
  {
    "text": "long running Services utilization in",
    "start": "159000",
    "end": "161440"
  },
  {
    "text": "Orange with respect to the static quota",
    "start": "161440",
    "end": "164280"
  },
  {
    "text": "drawn in the blue",
    "start": "164280",
    "end": "165720"
  },
  {
    "text": "line so our for long running Services",
    "start": "165720",
    "end": "169239"
  },
  {
    "text": "the resour utilization is relatively",
    "start": "169239",
    "end": "172080"
  },
  {
    "text": "stable in static res in a static kuber",
    "start": "172080",
    "end": "175040"
  },
  {
    "text": "netics resource Coda however does not",
    "start": "175040",
    "end": "177599"
  },
  {
    "text": "quite work well with the machine",
    "start": "177599",
    "end": "179360"
  },
  {
    "text": "learning training batch",
    "start": "179360",
    "end": "181959"
  },
  {
    "text": "workload this is because the resour",
    "start": "181959",
    "end": "184400"
  },
  {
    "text": "utilization is low most of the time but",
    "start": "184400",
    "end": "187799"
  },
  {
    "text": "with sporadic spikes when a large ml job",
    "start": "187799",
    "end": "191400"
  },
  {
    "text": "is submitted into the",
    "start": "191400",
    "end": "192879"
  },
  {
    "text": "cluster so as a result the users must",
    "start": "192879",
    "end": "196280"
  },
  {
    "text": "budget for their name space based on the",
    "start": "196280",
    "end": "198920"
  },
  {
    "text": "maximum resource",
    "start": "198920",
    "end": "200680"
  },
  {
    "text": "usage which is fine for long running",
    "start": "200680",
    "end": "202920"
  },
  {
    "text": "services but not ideal for our ml batch",
    "start": "202920",
    "end": "206720"
  },
  {
    "text": "workload so this leads to over budgeting",
    "start": "206720",
    "end": "210120"
  },
  {
    "text": "and waste of Hardware resources so over",
    "start": "210120",
    "end": "213159"
  },
  {
    "text": "so to overcome this we would like a",
    "start": "213159",
    "end": "216000"
  },
  {
    "text": "solution to manage Cota based on the uh",
    "start": "216000",
    "end": "219840"
  },
  {
    "text": "to manage our Koda based on the actual",
    "start": "219840",
    "end": "222599"
  },
  {
    "text": "usage in time of the jobs running",
    "start": "222599",
    "end": "225280"
  },
  {
    "text": "instead of a instead of a static",
    "start": "225280",
    "end": "229599"
  },
  {
    "text": "limit at the same time we want to be",
    "start": "229599",
    "end": "231560"
  },
  {
    "text": "able to enforce some sort of fairness in",
    "start": "231560",
    "end": "233760"
  },
  {
    "text": "priority between the jobs in",
    "start": "233760",
    "end": "235560"
  },
  {
    "text": "multi-tenancy environment ideally",
    "start": "235560",
    "end": "238000"
  },
  {
    "text": "through the priority and preemption",
    "start": "238000",
    "end": "241720"
  },
  {
    "text": "mechanism additionally we also observed",
    "start": "242280",
    "end": "245760"
  },
  {
    "text": "an unbalanced usage is holding down our",
    "start": "245760",
    "end": "248480"
  },
  {
    "text": "resource utilization rate even though",
    "start": "248480",
    "end": "251280"
  },
  {
    "text": "the GPU clusters are identical many",
    "start": "251280",
    "end": "254239"
  },
  {
    "text": "users feel more and more comfortable",
    "start": "254239",
    "end": "256639"
  },
  {
    "text": "keep submitting to the same",
    "start": "256639",
    "end": "258320"
  },
  {
    "text": "clusters most of the time leading to",
    "start": "258320",
    "end": "261400"
  },
  {
    "text": "imbalanced usage between all the",
    "start": "261400",
    "end": "263919"
  },
  {
    "text": "Clusters and distributed machine",
    "start": "263919",
    "end": "265919"
  },
  {
    "text": "learning jobs requires G scheduling",
    "start": "265919",
    "end": "268080"
  },
  {
    "text": "already and this nature nature of",
    "start": "268080",
    "end": "270240"
  },
  {
    "text": "scheduling is already causing small",
    "start": "270240",
    "end": "272960"
  },
  {
    "text": "pockets of unused resources on nodes",
    "start": "272960",
    "end": "276400"
  },
  {
    "text": "essentially wasting available capacity",
    "start": "276400",
    "end": "278919"
  },
  {
    "text": "because the G Size Doesn't perfectly",
    "start": "278919",
    "end": "280880"
  },
  {
    "text": "match the available resources on each",
    "start": "280880",
    "end": "283120"
  },
  {
    "text": "node causing resource fragmentation",
    "start": "283120",
    "end": "286000"
  },
  {
    "text": "where not all the gpus are fully",
    "start": "286000",
    "end": "289000"
  },
  {
    "text": "utilized and this problem is only made",
    "start": "289000",
    "end": "291199"
  },
  {
    "text": "worse by this unbalanced load so this",
    "start": "291199",
    "end": "294680"
  },
  {
    "text": "time we wish to have a solution that",
    "start": "294680",
    "end": "296960"
  },
  {
    "text": "will consider resource utilization",
    "start": "296960",
    "end": "298880"
  },
  {
    "text": "holistically across all the cluster and",
    "start": "298880",
    "end": "301520"
  },
  {
    "text": "submit machine learning training jobs",
    "start": "301520",
    "end": "303520"
  },
  {
    "text": "evenly evenly to each member",
    "start": "303520",
    "end": "307800"
  },
  {
    "text": "cluster and another reason causing",
    "start": "308400",
    "end": "310600"
  },
  {
    "text": "imbalanced load across the cluster is a",
    "start": "310600",
    "end": "313280"
  },
  {
    "text": "difficulty with multicluster",
    "start": "313280",
    "end": "315240"
  },
  {
    "text": "Federation of configuration and",
    "start": "315240",
    "end": "318720"
  },
  {
    "text": "credential for example a developer wants",
    "start": "318720",
    "end": "321120"
  },
  {
    "text": "to launch a machine learning training",
    "start": "321120",
    "end": "322560"
  },
  {
    "text": "job that needs to get an S3 credential",
    "start": "322560",
    "end": "325280"
  },
  {
    "text": "stored in a kuet secret but the secret",
    "start": "325280",
    "end": "328440"
  },
  {
    "text": "is cluster scopes",
    "start": "328440",
    "end": "330800"
  },
  {
    "text": "and it and it and it all exists on one",
    "start": "330800",
    "end": "333560"
  },
  {
    "text": "of the cluster so the machine learning",
    "start": "333560",
    "end": "335520"
  },
  {
    "text": "job can only be successfully submitted",
    "start": "335520",
    "end": "337600"
  },
  {
    "text": "to that same cluster and developers will",
    "start": "337600",
    "end": "340600"
  },
  {
    "text": "need to manually copy the configuration",
    "start": "340600",
    "end": "342800"
  },
  {
    "text": "and credentials to make their jobs work",
    "start": "342800",
    "end": "345240"
  },
  {
    "text": "across all the other",
    "start": "345240",
    "end": "347080"
  },
  {
    "text": "clusters so third on our wish list is a",
    "start": "347080",
    "end": "350360"
  },
  {
    "text": "solution to automatically copy the",
    "start": "350360",
    "end": "353000"
  },
  {
    "text": "configurations and credentials to all",
    "start": "353000",
    "end": "355199"
  },
  {
    "text": "the",
    "start": "355199",
    "end": "356479"
  },
  {
    "text": "Clusters in summary here are three",
    "start": "356479",
    "end": "359800"
  },
  {
    "text": "things we wish to have for our GPU",
    "start": "359800",
    "end": "361800"
  },
  {
    "text": "cluster management to achieve higher",
    "start": "361800",
    "end": "363759"
  },
  {
    "text": "resource utilization rate namely a Time",
    "start": "363759",
    "end": "367280"
  },
  {
    "text": "based quota management through priority",
    "start": "367280",
    "end": "369639"
  },
  {
    "text": "and preemption being able to submit",
    "start": "369639",
    "end": "372400"
  },
  {
    "text": "submit machine learning training jobs",
    "start": "372400",
    "end": "374440"
  },
  {
    "text": "evenly to all the cluster and the same",
    "start": "374440",
    "end": "376560"
  },
  {
    "text": "tier and automatically copy",
    "start": "376560",
    "end": "378759"
  },
  {
    "text": "configurations and credentials to all",
    "start": "378759",
    "end": "380880"
  },
  {
    "text": "the",
    "start": "380880",
    "end": "382039"
  },
  {
    "text": "Clusters so given these three",
    "start": "382039",
    "end": "384000"
  },
  {
    "text": "requirements we naturally start looking",
    "start": "384000",
    "end": "386560"
  },
  {
    "text": "for a better scheduler to help us bend",
    "start": "386560",
    "end": "388919"
  },
  {
    "text": "the GP growth",
    "start": "388919",
    "end": "391440"
  },
  {
    "text": "curve and it comes to selecting an ideal",
    "start": "391440",
    "end": "394400"
  },
  {
    "text": "scheduler for our GPU cluster here are",
    "start": "394400",
    "end": "397400"
  },
  {
    "text": "five key factors that we really",
    "start": "397400",
    "end": "401479"
  },
  {
    "text": "value first we want a scheduler that is",
    "start": "402240",
    "end": "405280"
  },
  {
    "text": "designed for multi cluster and multi-",
    "start": "405280",
    "end": "407560"
  },
  {
    "text": "tendency such as features that will",
    "start": "407560",
    "end": "409960"
  },
  {
    "text": "increase the cluster resource",
    "start": "409960",
    "end": "412160"
  },
  {
    "text": "utility and support multicluster",
    "start": "412160",
    "end": "414520"
  },
  {
    "text": "Federation as well as workload and job",
    "start": "414520",
    "end": "417160"
  },
  {
    "text": "level",
    "start": "417160",
    "end": "418520"
  },
  {
    "text": "queuing that will enable time-based",
    "start": "418520",
    "end": "423000"
  },
  {
    "text": "budgeting so for a quick cons comparison",
    "start": "423400",
    "end": "426080"
  },
  {
    "text": "between in cluster and cross cluster",
    "start": "426080",
    "end": "428680"
  },
  {
    "text": "scheduler for an in cluster uh",
    "start": "428680",
    "end": "431120"
  },
  {
    "text": "in-cluster scheduler user have to",
    "start": "431120",
    "end": "433639"
  },
  {
    "text": "manually pick a cluster um that will be",
    "start": "433639",
    "end": "437639"
  },
  {
    "text": "optimal for their machine learning",
    "start": "437639",
    "end": "440639"
  },
  {
    "text": "workload whereas the Federate scheduler",
    "start": "440639",
    "end": "443639"
  },
  {
    "text": "user only have to submit to a global",
    "start": "443639",
    "end": "445759"
  },
  {
    "text": "queue and the multicluster scheduler",
    "start": "445759",
    "end": "448199"
  },
  {
    "text": "such as kamada",
    "start": "448199",
    "end": "451479"
  },
  {
    "text": "oops will a automatically choose the",
    "start": "451879",
    "end": "454960"
  },
  {
    "text": "optimum cluster that this machine",
    "start": "454960",
    "end": "456919"
  },
  {
    "text": "learning workflow should be running",
    "start": "456919",
    "end": "459599"
  },
  {
    "text": "on and by choosing a Federate",
    "start": "459599",
    "end": "461840"
  },
  {
    "text": "multicluster scheduler we'll be able to",
    "start": "461840",
    "end": "464879"
  },
  {
    "text": "perform Federated identity management",
    "start": "464879",
    "end": "467840"
  },
  {
    "text": "job level queuing job PRI uh job",
    "start": "467840",
    "end": "471080"
  },
  {
    "text": "prioritization and job",
    "start": "471080",
    "end": "474520"
  },
  {
    "text": "preemption the second key factor for the",
    "start": "475720",
    "end": "478199"
  },
  {
    "text": "scheduler",
    "start": "478199",
    "end": "479840"
  },
  {
    "text": "is to support machine learning batch",
    "start": "479840",
    "end": "482759"
  },
  {
    "text": "workload we want the schedule to support",
    "start": "482759",
    "end": "485199"
  },
  {
    "text": "G scheduling for distributed machine",
    "start": "485199",
    "end": "487120"
  },
  {
    "text": "learning workload as well as priority in",
    "start": "487120",
    "end": "489759"
  },
  {
    "text": "preemption for G workloads as well also",
    "start": "489759",
    "end": "493879"
  },
  {
    "text": "the ability to schedule and cancel these",
    "start": "493879",
    "end": "496000"
  },
  {
    "text": "batch",
    "start": "496000",
    "end": "498240"
  },
  {
    "text": "workloads built-in observability is also",
    "start": "499039",
    "end": "501800"
  },
  {
    "text": "very important to us for example we want",
    "start": "501800",
    "end": "504440"
  },
  {
    "text": "to be able to monitor workflow location",
    "start": "504440",
    "end": "506960"
  },
  {
    "text": "in a queue monitor size of the queue",
    "start": "506960",
    "end": "509879"
  },
  {
    "text": "integrate easily with",
    "start": "509879",
    "end": "511959"
  },
  {
    "text": "Prometheus and receive preemption",
    "start": "511959",
    "end": "513959"
  },
  {
    "text": "notifications whenever high priority",
    "start": "513959",
    "end": "515919"
  },
  {
    "text": "jobs pre evict a lower priority",
    "start": "515919",
    "end": "520320"
  },
  {
    "text": "job we also want to find a scheduler",
    "start": "520760",
    "end": "524080"
  },
  {
    "text": "that is supported by collaborative and",
    "start": "524080",
    "end": "526160"
  },
  {
    "text": "active open source",
    "start": "526160",
    "end": "527760"
  },
  {
    "text": "Community for example it's owned by a",
    "start": "527760",
    "end": "529839"
  },
  {
    "text": "foundation the prodject is neutral",
    "start": "529839",
    "end": "532320"
  },
  {
    "text": "supported by strong Community have a",
    "start": "532320",
    "end": "534440"
  },
  {
    "text": "good documentation and is a mature",
    "start": "534440",
    "end": "536760"
  },
  {
    "text": "project with plenty of internal",
    "start": "536760",
    "end": "538839"
  },
  {
    "text": "adoptions",
    "start": "538839",
    "end": "541440"
  },
  {
    "text": "last but not least we want the scheduler",
    "start": "542440",
    "end": "544480"
  },
  {
    "text": "to be easy to deploy and",
    "start": "544480",
    "end": "546920"
  },
  {
    "text": "maintain so it can integrate easily with",
    "start": "546920",
    "end": "549920"
  },
  {
    "text": "existing data science platform",
    "start": "549920",
    "end": "551680"
  },
  {
    "text": "infrastructure and potentially extend to",
    "start": "551680",
    "end": "554040"
  },
  {
    "text": "Long running Services run times such as",
    "start": "554040",
    "end": "556240"
  },
  {
    "text": "streaming and inference so that other",
    "start": "556240",
    "end": "558839"
  },
  {
    "text": "platform teams within Bloomberg don't",
    "start": "558839",
    "end": "560839"
  },
  {
    "text": "have to reinvent the",
    "start": "560839",
    "end": "562640"
  },
  {
    "text": "wheel and also we want the scheduler to",
    "start": "562640",
    "end": "565040"
  },
  {
    "text": "be highly available",
    "start": "565040",
    "end": "568480"
  },
  {
    "text": "and with all these criterias in",
    "start": "569720",
    "end": "571959"
  },
  {
    "text": "mind we found",
    "start": "571959",
    "end": "574440"
  },
  {
    "text": "kamada it's a cumes management system",
    "start": "574440",
    "end": "577360"
  },
  {
    "text": "that enables you to run your Cloud",
    "start": "577360",
    "end": "578920"
  },
  {
    "text": "native applications across multiple",
    "start": "578920",
    "end": "581079"
  },
  {
    "text": "kubernetes clusters in",
    "start": "581079",
    "end": "584519"
  },
  {
    "text": "clouds so to list a few reasons why we",
    "start": "584839",
    "end": "588360"
  },
  {
    "text": "choose",
    "start": "588360",
    "end": "589920"
  },
  {
    "text": "kamada first of all it's compatible with",
    "start": "589920",
    "end": "592519"
  },
  {
    "text": "ku's Native Native apis so it's not a",
    "start": "592519",
    "end": "595399"
  },
  {
    "text": "lot of hassle to upgrade from a single",
    "start": "595399",
    "end": "597360"
  },
  {
    "text": "cluster scheduling to multicluster",
    "start": "597360",
    "end": "599000"
  },
  {
    "text": "schedu",
    "start": "599000",
    "end": "600680"
  },
  {
    "text": "and all of the two chains that is",
    "start": "600680",
    "end": "602200"
  },
  {
    "text": "kubernetes native you can keep using",
    "start": "602200",
    "end": "604720"
  },
  {
    "text": "them it also supports multiple multi",
    "start": "604720",
    "end": "607399"
  },
  {
    "text": "cluster scenarios with its built-in",
    "start": "607399",
    "end": "609839"
  },
  {
    "text": "policy sets such as Geographic",
    "start": "609839",
    "end": "613000"
  },
  {
    "text": "redundancy active active and remote",
    "start": "613000",
    "end": "616120"
  },
  {
    "text": "Disaster",
    "start": "616120",
    "end": "618279"
  },
  {
    "text": "Recovery kada also guarantees no vendor",
    "start": "618279",
    "end": "621279"
  },
  {
    "text": "lock in and provides a centralized",
    "start": "621279",
    "end": "623640"
  },
  {
    "text": "management",
    "start": "623640",
    "end": "624920"
  },
  {
    "text": "solution it it provides support for",
    "start": "624920",
    "end": "627079"
  },
  {
    "text": "multicloud platforms auto resource ource",
    "start": "627079",
    "end": "629760"
  },
  {
    "text": "allocation as well as free",
    "start": "629760",
    "end": "633399"
  },
  {
    "text": "migration it also provides various",
    "start": "634279",
    "end": "636560"
  },
  {
    "text": "multicluster scheduling policies whether",
    "start": "636560",
    "end": "639200"
  },
  {
    "text": "you want to schedule based off of",
    "start": "639200",
    "end": "640920"
  },
  {
    "text": "affinity or multicluster or based off of",
    "start": "640920",
    "end": "644120"
  },
  {
    "text": "regions clusters and",
    "start": "644120",
    "end": "647680"
  },
  {
    "text": "vendors kamado will handle them for",
    "start": "647680",
    "end": "651680"
  },
  {
    "text": "you now for a quick uh look at the",
    "start": "651680",
    "end": "655399"
  },
  {
    "text": "commod arch kamada architecture",
    "start": "655399",
    "end": "659839"
  },
  {
    "text": "um so in the first box here the commod",
    "start": "659839",
    "end": "664160"
  },
  {
    "text": "API server uh is the front end of the",
    "start": "664160",
    "end": "666600"
  },
  {
    "text": "commod control plane and it exposes both",
    "start": "666600",
    "end": "669600"
  },
  {
    "text": "the kada API as well as the kubernetes",
    "start": "669600",
    "end": "671839"
  },
  {
    "text": "API to the",
    "start": "671839",
    "end": "673360"
  },
  {
    "text": "user and on the right we have the kamada",
    "start": "673360",
    "end": "675800"
  },
  {
    "text": "schedule component which is responsible",
    "start": "675800",
    "end": "678560"
  },
  {
    "text": "for scheduling kuet kuet native API",
    "start": "678560",
    "end": "681320"
  },
  {
    "text": "resource objects as well as your",
    "start": "681320",
    "end": "683440"
  },
  {
    "text": "business logic specific crds to the m",
    "start": "683440",
    "end": "686760"
  },
  {
    "text": "clusters and on the left we have",
    "start": "686760",
    "end": "689440"
  },
  {
    "text": "workload",
    "start": "689440",
    "end": "690839"
  },
  {
    "text": "controller which is an abstraction for a",
    "start": "690839",
    "end": "693519"
  },
  {
    "text": "set of several controllers which will",
    "start": "693519",
    "end": "695760"
  },
  {
    "text": "watch the commod objects defined on the",
    "start": "695760",
    "end": "698920"
  },
  {
    "text": "multicluster layer and then talk to the",
    "start": "698920",
    "end": "701320"
  },
  {
    "text": "underlying clusters API servers to",
    "start": "701320",
    "end": "703920"
  },
  {
    "text": "create both the regular kuber resources",
    "start": "703920",
    "end": "706920"
  },
  {
    "text": "as well as your",
    "start": "706920",
    "end": "709160"
  },
  {
    "text": "crds and in each in each of the member",
    "start": "709160",
    "end": "712000"
  },
  {
    "text": "cluster we have kamada agent because",
    "start": "712000",
    "end": "714959"
  },
  {
    "text": "kamada has two cluster registration mode",
    "start": "714959",
    "end": "717600"
  },
  {
    "text": "such as push and pull",
    "start": "717600",
    "end": "719800"
  },
  {
    "text": "Comm agents is deployed on all of the PO",
    "start": "719800",
    "end": "722680"
  },
  {
    "text": "mode enabled member",
    "start": "722680",
    "end": "725440"
  },
  {
    "text": "clusters next I'll hand out uh hand to",
    "start": "725440",
    "end": "728279"
  },
  {
    "text": "ya to talk about how we use kamada in",
    "start": "728279",
    "end": "731720"
  },
  {
    "text": "Bloomberg yeah uh thank you Leon for uh",
    "start": "731720",
    "end": "735480"
  },
  {
    "text": "explaining our challenges and",
    "start": "735480",
    "end": "737160"
  },
  {
    "text": "introducing kamada uh now let's take a",
    "start": "737160",
    "end": "740680"
  },
  {
    "text": "closer look at how we use this",
    "start": "740680",
    "end": "742720"
  },
  {
    "text": "capabilities at life of",
    "start": "742720",
    "end": "745199"
  },
  {
    "text": "blomberg um sorry so one of the big",
    "start": "745199",
    "end": "750440"
  },
  {
    "text": "challenge is about resilience to ensure",
    "start": "750440",
    "end": "753160"
  },
  {
    "text": "that we deploy the commod control plane",
    "start": "753160",
    "end": "756639"
  },
  {
    "text": "on across two data centers and in this",
    "start": "756639",
    "end": "759800"
  },
  {
    "text": "architecture if you see that if you look",
    "start": "759800",
    "end": "762320"
  },
  {
    "text": "at this diagram um the the command API",
    "start": "762320",
    "end": "765839"
  },
  {
    "text": "server on each side connect to the K uh",
    "start": "765839",
    "end": "769560"
  },
  {
    "text": "K provide a etcd like interface and is",
    "start": "769560",
    "end": "772760"
  },
  {
    "text": "backhanded by a stretched postgress",
    "start": "772760",
    "end": "775480"
  },
  {
    "text": "database uh postgress database across",
    "start": "775480",
    "end": "778320"
  },
  {
    "text": "both data set cers so it means that um",
    "start": "778320",
    "end": "781839"
  },
  {
    "text": "when when a data when when one data",
    "start": "781839",
    "end": "785000"
  },
  {
    "text": "center experiences an aage the control",
    "start": "785000",
    "end": "788399"
  },
  {
    "text": "plane on the other will immediately take",
    "start": "788399",
    "end": "791440"
  },
  {
    "text": "over as a primary so in this in this fa",
    "start": "791440",
    "end": "795240"
  },
  {
    "text": "over mechanism it means that our user",
    "start": "795240",
    "end": "798279"
  },
  {
    "text": "can continue submitting their training",
    "start": "798279",
    "end": "800880"
  },
  {
    "text": "job through kada uh even during a data",
    "start": "800880",
    "end": "803880"
  },
  {
    "text": "center",
    "start": "803880",
    "end": "806560"
  },
  {
    "text": "outage um beside the resilience",
    "start": "806560",
    "end": "809639"
  },
  {
    "text": "um besides resilience the the",
    "start": "809639",
    "end": "813160"
  },
  {
    "text": "availability of the workload across the",
    "start": "813160",
    "end": "816000"
  },
  {
    "text": "member clusters is also critical kada",
    "start": "816000",
    "end": "819440"
  },
  {
    "text": "API server keeps sending houseal check",
    "start": "819440",
    "end": "822199"
  },
  {
    "text": "to the member clusters so once uh a",
    "start": "822199",
    "end": "825720"
  },
  {
    "text": "cluster is identified as unhealthy the",
    "start": "825720",
    "end": "829199"
  },
  {
    "text": "commander API server will mark it with a",
    "start": "829199",
    "end": "831800"
  },
  {
    "text": "no schedu Tain and preventing the new",
    "start": "831800",
    "end": "834920"
  },
  {
    "text": "workloads from being uh scheduled there",
    "start": "834920",
    "end": "840040"
  },
  {
    "text": "and uh most training workflows require",
    "start": "840040",
    "end": "843320"
  },
  {
    "text": "storage access to download models and",
    "start": "843320",
    "end": "846560"
  },
  {
    "text": "checkpoints so as as Leon mentioned",
    "start": "846560",
    "end": "849680"
  },
  {
    "text": "before um before we're adopting kada our",
    "start": "849680",
    "end": "853519"
  },
  {
    "text": "user have to create credentials on each",
    "start": "853519",
    "end": "857199"
  },
  {
    "text": "member clusters and we find that a lot",
    "start": "857199",
    "end": "860160"
  },
  {
    "text": "of uh job failure due to uh Missing",
    "start": "860160",
    "end": "863440"
  },
  {
    "text": "credentials and then we Leverage The",
    "start": "863440",
    "end": "866480"
  },
  {
    "text": "commod propagation policy and it",
    "start": "866480",
    "end": "869639"
  },
  {
    "text": "guaranteed that all the credentials are",
    "start": "869639",
    "end": "872600"
  },
  {
    "text": "duplicated to the member clusters and",
    "start": "872600",
    "end": "875680"
  },
  {
    "text": "even uh if we add a new cluster the",
    "start": "875680",
    "end": "878800"
  },
  {
    "text": "commod also guaranteeing these",
    "start": "878800",
    "end": "880959"
  },
  {
    "text": "credentials are populated there so which",
    "start": "880959",
    "end": "884240"
  },
  {
    "text": "means like our user just need to create",
    "start": "884240",
    "end": "887160"
  },
  {
    "text": "their credential ones in the tier level",
    "start": "887160",
    "end": "889519"
  },
  {
    "text": "and help like simplify the user uh",
    "start": "889519",
    "end": "894279"
  },
  {
    "text": "process and then",
    "start": "894279",
    "end": "897079"
  },
  {
    "text": "the yeah and then the training workload",
    "start": "897079",
    "end": "900519"
  },
  {
    "text": "uh differs from the credentials because",
    "start": "900519",
    "end": "903000"
  },
  {
    "text": "our users typically want to run their",
    "start": "903000",
    "end": "905959"
  },
  {
    "text": "job once and",
    "start": "905959",
    "end": "907920"
  },
  {
    "text": "then before in the past our user have to",
    "start": "907920",
    "end": "911959"
  },
  {
    "text": "check the resource gr uh resource Matrix",
    "start": "911959",
    "end": "916199"
  },
  {
    "text": "and then find which cluster have enough",
    "start": "916199",
    "end": "918680"
  },
  {
    "text": "resources to run their job and then",
    "start": "918680",
    "end": "920720"
  },
  {
    "text": "submit job there with command the",
    "start": "920720",
    "end": "923720"
  },
  {
    "text": "scheduling uh is streamlined the commod",
    "start": "923720",
    "end": "926720"
  },
  {
    "text": "scheduler will evaluate the Capac the",
    "start": "926720",
    "end": "929800"
  },
  {
    "text": "resource capacity across all the member",
    "start": "929800",
    "end": "932880"
  },
  {
    "text": "clusters and then find the available",
    "start": "932880",
    "end": "935120"
  },
  {
    "text": "cluster and start the workload there so",
    "start": "935120",
    "end": "938759"
  },
  {
    "text": "this will help uh simplify the user uh",
    "start": "938759",
    "end": "942800"
  },
  {
    "text": "ex uh the you the process for the user",
    "start": "942800",
    "end": "945720"
  },
  {
    "text": "and also help us to maximize uh the",
    "start": "945720",
    "end": "948240"
  },
  {
    "text": "resource",
    "start": "948240",
    "end": "950680"
  },
  {
    "text": "utilization the Comm command also",
    "start": "951199",
    "end": "953600"
  },
  {
    "text": "provide a global view uh it means that",
    "start": "953600",
    "end": "956519"
  },
  {
    "text": "the users they can view or say the the",
    "start": "956519",
    "end": "960360"
  },
  {
    "text": "the users they can perform operations",
    "start": "960360",
    "end": "962880"
  },
  {
    "text": "against the resources on the member",
    "start": "962880",
    "end": "964680"
  },
  {
    "text": "cluster so for ex without the need to",
    "start": "964680",
    "end": "967480"
  },
  {
    "text": "access the member cluster so for example",
    "start": "967480",
    "end": "970319"
  },
  {
    "text": "uh our user can access or view the um",
    "start": "970319",
    "end": "973959"
  },
  {
    "text": "workload uh logs through the commod",
    "start": "973959",
    "end": "976480"
  },
  {
    "text": "control plan directly uh without need to",
    "start": "976480",
    "end": "979600"
  },
  {
    "text": "log into the member",
    "start": "979600",
    "end": "982759"
  },
  {
    "text": "clusters uh in Bloomberg uh we develop a",
    "start": "982759",
    "end": "986360"
  },
  {
    "text": "customer resource to manage users uh",
    "start": "986360",
    "end": "989399"
  },
  {
    "text": "training workloads and kada understand",
    "start": "989399",
    "end": "993040"
  },
  {
    "text": "those workloads by uh by a mechanism",
    "start": "993040",
    "end": "996480"
  },
  {
    "text": "called interpreter and we call it uh we",
    "start": "996480",
    "end": "1000759"
  },
  {
    "text": "call it uh interpreter web hook and this",
    "start": "1000759",
    "end": "1004199"
  },
  {
    "text": "web hook just translate the customer uh",
    "start": "1004199",
    "end": "1007519"
  },
  {
    "text": "resource definition into a kubernetes um",
    "start": "1007519",
    "end": "1011440"
  },
  {
    "text": "resource requirement that the scheduler",
    "start": "1011440",
    "end": "1013839"
  },
  {
    "text": "can comprehend so here uh you can see an",
    "start": "1013839",
    "end": "1017240"
  },
  {
    "text": "example uh the gpus small is translated",
    "start": "1017240",
    "end": "1021040"
  },
  {
    "text": "to uh one CPU core8 GPU uh 8 gigb of",
    "start": "1021040",
    "end": "1025839"
  },
  {
    "text": "memory and um 1",
    "start": "1025839",
    "end": "1029959"
  },
  {
    "text": "GPU now uh let's take a closer look at",
    "start": "1030280",
    "end": "1033678"
  },
  {
    "text": "the scheduler and how Comm scheduler um",
    "start": "1033679",
    "end": "1037760"
  },
  {
    "text": "find available uh cluster and the start",
    "start": "1037760",
    "end": "1040880"
  },
  {
    "text": "job there so by default the command",
    "start": "1040880",
    "end": "1044319"
  },
  {
    "text": "scheduler um make the decision based on",
    "start": "1044319",
    "end": "1047720"
  },
  {
    "text": "the total based the free resource within",
    "start": "1047720",
    "end": "1050720"
  },
  {
    "text": "the entire cluster so here is an example",
    "start": "1050720",
    "end": "1054679"
  },
  {
    "text": "uh in this St cluster uh I have three",
    "start": "1054679",
    "end": "1057160"
  },
  {
    "text": "node and then uh the resource summary",
    "start": "1057160",
    "end": "1060320"
  },
  {
    "text": "just adds up uh just adds up the",
    "start": "1060320",
    "end": "1063080"
  },
  {
    "text": "allocatable and allocated resource of",
    "start": "1063080",
    "end": "1066240"
  },
  {
    "text": "all the node and then we can do some",
    "start": "1066240",
    "end": "1068360"
  },
  {
    "text": "calculation so which means like in this",
    "start": "1068360",
    "end": "1070840"
  },
  {
    "text": "cluster the free resources is about 200",
    "start": "1070840",
    "end": "1074240"
  },
  {
    "text": "and uh let me do some calculation it's",
    "start": "1074240",
    "end": "1077760"
  },
  {
    "text": "250 5 uh course and also around 6 59 GB",
    "start": "1077760",
    "end": "1083679"
  },
  {
    "text": "of memory and then yeah this approach uh",
    "start": "1083679",
    "end": "1087880"
  },
  {
    "text": "is very efficient however uh it can lead",
    "start": "1087880",
    "end": "1091440"
  },
  {
    "text": "uh resource fragmentation let me give",
    "start": "1091440",
    "end": "1093679"
  },
  {
    "text": "you another example so I want so for",
    "start": "1093679",
    "end": "1097799"
  },
  {
    "text": "example if I want to uh schedule a job",
    "start": "1097799",
    "end": "1100520"
  },
  {
    "text": "with one replica and require uh 30 CPU",
    "start": "1100520",
    "end": "1104799"
  },
  {
    "text": "cores and 30 gyes of memory and then the",
    "start": "1104799",
    "end": "1107960"
  },
  {
    "text": "resource from the resource summary it",
    "start": "1107960",
    "end": "1110480"
  },
  {
    "text": "indicates that this cluster have enough",
    "start": "1110480",
    "end": "1113159"
  },
  {
    "text": "resource to run my job however no none",
    "start": "1113159",
    "end": "1116640"
  },
  {
    "text": "of the node on this cluster have enough",
    "start": "1116640",
    "end": "1119600"
  },
  {
    "text": "uh memory to fit my",
    "start": "1119600",
    "end": "1123280"
  },
  {
    "text": "workload uh to address this problem uh",
    "start": "1123480",
    "end": "1127840"
  },
  {
    "text": "we enable commod estimator at Bloomberg",
    "start": "1127840",
    "end": "1131880"
  },
  {
    "text": "the um command estimator is just a grpc",
    "start": "1131880",
    "end": "1135880"
  },
  {
    "text": "server so it connects to the single",
    "start": "1135880",
    "end": "1139080"
  },
  {
    "text": "cluster and list and watches uh it list",
    "start": "1139080",
    "end": "1142559"
  },
  {
    "text": "and watches the node and pod information",
    "start": "1142559",
    "end": "1145720"
  },
  {
    "text": "the scheduler will ask the estimator",
    "start": "1145720",
    "end": "1148159"
  },
  {
    "text": "like how many replica I can run on this",
    "start": "1148159",
    "end": "1151400"
  },
  {
    "text": "cluster by giving each replica's",
    "start": "1151400",
    "end": "1154159"
  },
  {
    "text": "resource requirement and then the",
    "start": "1154159",
    "end": "1156960"
  },
  {
    "text": "estimat the estimator will calculate the",
    "start": "1156960",
    "end": "1160559"
  },
  {
    "text": "maximum available replica based on the",
    "start": "1160559",
    "end": "1163520"
  },
  {
    "text": "Node and pod information and thus the",
    "start": "1163520",
    "end": "1166679"
  },
  {
    "text": "estimator can provide the",
    "start": "1166679",
    "end": "1169320"
  },
  {
    "text": "uh scheduler a more accurate resource",
    "start": "1169320",
    "end": "1172200"
  },
  {
    "text": "information to make the final",
    "start": "1172200",
    "end": "1175960"
  },
  {
    "text": "decision yeah and uh so as at and at",
    "start": "1176919",
    "end": "1182559"
  },
  {
    "text": "Bloomberg uh our platform uh is support",
    "start": "1182559",
    "end": "1187280"
  },
  {
    "text": "multitenant and we assign a resource",
    "start": "1187280",
    "end": "1190760"
  },
  {
    "text": "Coda uh in each tenants name space to",
    "start": "1190760",
    "end": "1193960"
  },
  {
    "text": "control like the resource usage so uh in",
    "start": "1193960",
    "end": "1197799"
  },
  {
    "text": "this example",
    "start": "1197799",
    "end": "1199280"
  },
  {
    "text": "uh my name space say only have eight",
    "start": "1199280",
    "end": "1202720"
  },
  {
    "text": "gigabytes uh sorry 8 GPU resource coder",
    "start": "1202720",
    "end": "1206360"
  },
  {
    "text": "and then I want to schedule a job with",
    "start": "1206360",
    "end": "1208520"
  },
  {
    "text": "four replica and each of them needs one",
    "start": "1208520",
    "end": "1211720"
  },
  {
    "text": "GPU and the command scheduler will pick",
    "start": "1211720",
    "end": "1215520"
  },
  {
    "text": "uh Dev 01 cluster from me because uh it",
    "start": "1215520",
    "end": "1218760"
  },
  {
    "text": "has more available resources however",
    "start": "1218760",
    "end": "1222320"
  },
  {
    "text": "when this job is launched to de01",
    "start": "1222320",
    "end": "1225120"
  },
  {
    "text": "Cluster it would be rejected because I",
    "start": "1225120",
    "end": "1228360"
  },
  {
    "text": "don't have enough um resource Coda to",
    "start": "1228360",
    "end": "1231400"
  },
  {
    "text": "run it and to address this problem uh we",
    "start": "1231400",
    "end": "1234960"
  },
  {
    "text": "enable the resource coda pluging in the",
    "start": "1234960",
    "end": "1238240"
  },
  {
    "text": "estimator and to force um the resource",
    "start": "1238240",
    "end": "1242480"
  },
  {
    "text": "coder check before it do any calculation",
    "start": "1242480",
    "end": "1246120"
  },
  {
    "text": "so uh that's all about how we use uh",
    "start": "1246120",
    "end": "1249679"
  },
  {
    "text": "kada at Bloomberg and for more details",
    "start": "1249679",
    "end": "1252640"
  },
  {
    "text": "uh you can check the main our maintainer",
    "start": "1252640",
    "end": "1254799"
  },
  {
    "text": "track on YouTube later and I will pass",
    "start": "1254799",
    "end": "1257480"
  },
  {
    "text": "the talk back to Leah to talk about our",
    "start": "1257480",
    "end": "1260000"
  },
  {
    "text": "future",
    "start": "1260000",
    "end": "1261600"
  },
  {
    "text": "plan thanks",
    "start": "1261600",
    "end": "1264880"
  },
  {
    "text": "y for our future project road map of",
    "start": "1265760",
    "end": "1270080"
  },
  {
    "text": "kamada uh we're mainly focused on",
    "start": "1270080",
    "end": "1272320"
  },
  {
    "text": "priority in preemption for the next one",
    "start": "1272320",
    "end": "1274720"
  },
  {
    "text": "or two month and what that means is",
    "start": "1274720",
    "end": "1277600"
  },
  {
    "text": "ensure that critical workloads can have",
    "start": "1277600",
    "end": "1280120"
  },
  {
    "text": "preferential access to the resources in",
    "start": "1280120",
    "end": "1282279"
  },
  {
    "text": "gpus through a priority uh",
    "start": "1282279",
    "end": "1285600"
  },
  {
    "text": "queue and we also want to ensure optimal",
    "start": "1285600",
    "end": "1288279"
  },
  {
    "text": "re resource utilization and availability",
    "start": "1288279",
    "end": "1290640"
  },
  {
    "text": "for essential workloads through the",
    "start": "1290640",
    "end": "1292720"
  },
  {
    "text": "preemption",
    "start": "1292720",
    "end": "1294799"
  },
  {
    "text": "mechanism um and if you have a similar",
    "start": "1294799",
    "end": "1297400"
  },
  {
    "text": "use case please talk to us and let us",
    "start": "1297400",
    "end": "1300640"
  },
  {
    "text": "know um your special use cases so we",
    "start": "1300640",
    "end": "1303760"
  },
  {
    "text": "have we can have a better um design for",
    "start": "1303760",
    "end": "1305960"
  },
  {
    "text": "the parity and preemption",
    "start": "1305960",
    "end": "1309158"
  },
  {
    "text": "mechanism thank you so much for tuning",
    "start": "1309720",
    "end": "1312000"
  },
  {
    "text": "into our talk uh we're ready to take any",
    "start": "1312000",
    "end": "1313919"
  },
  {
    "text": "questions you have",
    "start": "1313919",
    "end": "1316110"
  },
  {
    "text": "[Applause]",
    "start": "1316110",
    "end": "1322869"
  },
  {
    "text": "uh I have one question in regards to",
    "start": "1323320",
    "end": "1325080"
  },
  {
    "text": "your kada scheduler where uh you",
    "start": "1325080",
    "end": "1328240"
  },
  {
    "text": "mentioned that you are using the",
    "start": "1328240",
    "end": "1330440"
  },
  {
    "text": "resource Kota F feature from the",
    "start": "1330440",
    "end": "1332320"
  },
  {
    "text": "kubernetes which helps you to calculate",
    "start": "1332320",
    "end": "1335000"
  },
  {
    "text": "the resource Cota between two different",
    "start": "1335000",
    "end": "1336360"
  },
  {
    "text": "clusters and then you can decide whether",
    "start": "1336360",
    "end": "1338000"
  },
  {
    "text": "you can schedule the given workload on",
    "start": "1338000",
    "end": "1340400"
  },
  {
    "text": "targeted cluster or not however when in",
    "start": "1340400",
    "end": "1342400"
  },
  {
    "text": "the end you mentioned that you're going",
    "start": "1342400",
    "end": "1344520"
  },
  {
    "text": "to provide the support for the higher",
    "start": "1344520",
    "end": "1346520"
  },
  {
    "text": "priority versus lower priority workloads",
    "start": "1346520",
    "end": "1348720"
  },
  {
    "text": "so what happens if there are two or",
    "start": "1348720",
    "end": "1351919"
  },
  {
    "text": "three higher priority workload comes at",
    "start": "1351919",
    "end": "1354000"
  },
  {
    "text": "the same time and then who wins the race",
    "start": "1354000",
    "end": "1356600"
  },
  {
    "text": "or you just pick randomly or there is",
    "start": "1356600",
    "end": "1358200"
  },
  {
    "text": "like a Q based approach you're going to",
    "start": "1358200",
    "end": "1359679"
  },
  {
    "text": "use",
    "start": "1359679",
    "end": "1360760"
  },
  {
    "text": "that yeah um that's a really good",
    "start": "1360760",
    "end": "1363400"
  },
  {
    "text": "question so the question is more about",
    "start": "1363400",
    "end": "1365679"
  },
  {
    "text": "like uh if we introduce the priority uh",
    "start": "1365679",
    "end": "1369400"
  },
  {
    "text": "in the control plane and they compete",
    "start": "1369400",
    "end": "1371600"
  },
  {
    "text": "for the resource but uh they are limited",
    "start": "1371600",
    "end": "1374520"
  },
  {
    "text": "by the uh resource Cota so the so we are",
    "start": "1374520",
    "end": "1378120"
  },
  {
    "text": "still in the transition period be uh so",
    "start": "1378120",
    "end": "1381480"
  },
  {
    "text": "before uh we move to the priority uh",
    "start": "1381480",
    "end": "1384960"
  },
  {
    "text": "base scheduling and we still apply the",
    "start": "1384960",
    "end": "1387720"
  },
  {
    "text": "resource Coda on that and after like we",
    "start": "1387720",
    "end": "1390720"
  },
  {
    "text": "we after uh we have the priority",
    "start": "1390720",
    "end": "1394640"
  },
  {
    "text": "mechanism and uh we will release the",
    "start": "1394640",
    "end": "1397840"
  },
  {
    "text": "resource Coda like all it seem it means",
    "start": "1397840",
    "end": "1401039"
  },
  {
    "text": "like all the users or or all the tenants",
    "start": "1401039",
    "end": "1403960"
  },
  {
    "text": "can access the cluster's resource and",
    "start": "1403960",
    "end": "1406640"
  },
  {
    "text": "that can enhance the uh the uh resource",
    "start": "1406640",
    "end": "1411200"
  },
  {
    "text": "utilization so but but it's still a good",
    "start": "1411200",
    "end": "1414120"
  },
  {
    "text": "feature for long running uh Services",
    "start": "1414120",
    "end": "1417200"
  },
  {
    "text": "right to apply the uh resource Coda yeah",
    "start": "1417200",
    "end": "1421559"
  },
  {
    "text": "thank you",
    "start": "1421559",
    "end": "1423400"
  },
  {
    "text": "yeah hi thank you for your talk uh I had",
    "start": "1423400",
    "end": "1426320"
  },
  {
    "text": "a question is it possible to split up",
    "start": "1426320",
    "end": "1428520"
  },
  {
    "text": "workloads based on let's say a lot of um",
    "start": "1428520",
    "end": "1433760"
  },
  {
    "text": "ETL uh a lot of ETL tasks have multiple",
    "start": "1433760",
    "end": "1438000"
  },
  {
    "text": "batches that depend on one another is it",
    "start": "1438000",
    "end": "1440679"
  },
  {
    "text": "possible to split those out across",
    "start": "1440679",
    "end": "1444600"
  },
  {
    "text": "clusters uh I think the simple answer is",
    "start": "1444600",
    "end": "1448760"
  },
  {
    "text": "yes but you still Define how those bat",
    "start": "1448760",
    "end": "1452279"
  },
  {
    "text": "shop are um how you define the best jop",
    "start": "1452279",
    "end": "1456360"
  },
  {
    "text": "so uh I can explain our use case um our",
    "start": "1456360",
    "end": "1460159"
  },
  {
    "text": "use case is very simple because we only",
    "start": "1460159",
    "end": "1462440"
  },
  {
    "text": "support Pythor shop which means like for",
    "start": "1462440",
    "end": "1465240"
  },
  {
    "text": "each replica the resource requirement is",
    "start": "1465240",
    "end": "1467840"
  },
  {
    "text": "the same",
    "start": "1467840",
    "end": "1469399"
  },
  {
    "text": "uh Comm still has a limitation like to",
    "start": "1469399",
    "end": "1472279"
  },
  {
    "text": "interpret uh replica with different uh",
    "start": "1472279",
    "end": "1475520"
  },
  {
    "text": "resource requirement thank you",
    "start": "1475520",
    "end": "1479640"
  },
  {
    "text": "yeah thank you good talk um I have a",
    "start": "1479640",
    "end": "1482880"
  },
  {
    "text": "question two questions one is about uh",
    "start": "1482880",
    "end": "1486480"
  },
  {
    "text": "resource uh management cuz I think you",
    "start": "1486480",
    "end": "1489240"
  },
  {
    "text": "said you are using the resource",
    "start": "1489240",
    "end": "1490760"
  },
  {
    "text": "estimator right so have two side one",
    "start": "1490760",
    "end": "1493039"
  },
  {
    "text": "side on workload you have estimator to",
    "start": "1493039",
    "end": "1495080"
  },
  {
    "text": "to translate the CDs into the",
    "start": "1495080",
    "end": "1497399"
  },
  {
    "text": "requirement and and on the on the",
    "start": "1497399",
    "end": "1499320"
  },
  {
    "text": "cluster side you have a mechanism to",
    "start": "1499320",
    "end": "1501520"
  },
  {
    "text": "kind of track how many resources it has",
    "start": "1501520",
    "end": "1503919"
  },
  {
    "text": "right then you can do the match uh then",
    "start": "1503919",
    "end": "1506000"
  },
  {
    "text": "the question is after the scheduler um",
    "start": "1506000",
    "end": "1508840"
  },
  {
    "text": "say I have I need a five gpus and send",
    "start": "1508840",
    "end": "1511120"
  },
  {
    "text": "it to the this cluster um when does the",
    "start": "1511120",
    "end": "1514480"
  },
  {
    "text": "schedule actually know this resources is",
    "start": "1514480",
    "end": "1516880"
  },
  {
    "text": "not available so that my next scheduling",
    "start": "1516880",
    "end": "1520000"
  },
  {
    "text": "will not reuse I still think that",
    "start": "1520000",
    "end": "1522640"
  },
  {
    "text": "cluster still have that five gpus does",
    "start": "1522640",
    "end": "1525240"
  },
  {
    "text": "kamada handle",
    "start": "1525240",
    "end": "1527080"
  },
  {
    "text": "that uh so the classic scheduling",
    "start": "1527080",
    "end": "1529799"
  },
  {
    "text": "problem right you have a schedule and",
    "start": "1529799",
    "end": "1531399"
  },
  {
    "text": "it's distributed so you have a scheduler",
    "start": "1531399",
    "end": "1533039"
  },
  {
    "text": "think that cluster has 10 GS I'm going",
    "start": "1533039",
    "end": "1535240"
  },
  {
    "text": "to take five out of it oh so uh Comm",
    "start": "1535240",
    "end": "1540360"
  },
  {
    "text": "scheduler you can think it's like uh",
    "start": "1540360",
    "end": "1543120"
  },
  {
    "text": "it's one it's one thread right so when",
    "start": "1543120",
    "end": "1545720"
  },
  {
    "text": "it make the uh this like when when it",
    "start": "1545720",
    "end": "1549720"
  },
  {
    "text": "try to schedule an object it will uh",
    "start": "1549720",
    "end": "1552559"
  },
  {
    "text": "send the request to all the estimator",
    "start": "1552559",
    "end": "1555799"
  },
  {
    "text": "because each estimator just like manage",
    "start": "1555799",
    "end": "1558720"
  },
  {
    "text": "one cluster so it will collect re will",
    "start": "1558720",
    "end": "1561799"
  },
  {
    "text": "collect the response from all the",
    "start": "1561799",
    "end": "1563760"
  },
  {
    "text": "estimator and then um and then make the",
    "start": "1563760",
    "end": "1566919"
  },
  {
    "text": "final decision and and after that it",
    "start": "1566919",
    "end": "1570279"
  },
  {
    "text": "will um in the kada uh we have something",
    "start": "1570279",
    "end": "1574159"
  },
  {
    "text": "uh similar like the worker uh we have a",
    "start": "1574159",
    "end": "1577919"
  },
  {
    "text": "a resource called worker which similar",
    "start": "1577919",
    "end": "1580640"
  },
  {
    "text": "to kubernetes CET this worker will",
    "start": "1580640",
    "end": "1582960"
  },
  {
    "text": "launch the job on the member cluster so",
    "start": "1582960",
    "end": "1585679"
  },
  {
    "text": "it's like the uh one single proc",
    "start": "1585679",
    "end": "1588799"
  },
  {
    "text": "so I don't think uh the situation you",
    "start": "1588799",
    "end": "1591960"
  },
  {
    "text": "mention will happens the the whole",
    "start": "1591960",
    "end": "1594360"
  },
  {
    "text": "schedule part is very similar like",
    "start": "1594360",
    "end": "1596279"
  },
  {
    "text": "kubernetes native schuer that's that's",
    "start": "1596279",
    "end": "1598720"
  },
  {
    "text": "where the difference is so the",
    "start": "1598720",
    "end": "1600039"
  },
  {
    "text": "difference on the kubernetes one is uh",
    "start": "1600039",
    "end": "1602320"
  },
  {
    "text": "on a note you cannot add any um new pod",
    "start": "1602320",
    "end": "1606000"
  },
  {
    "text": "down to that note right but on a cluster",
    "start": "1606000",
    "end": "1608720"
  },
  {
    "text": "you have a you have this um on a cluster",
    "start": "1608720",
    "end": "1611679"
  },
  {
    "text": "you could have two things one is U um",
    "start": "1611679",
    "end": "1614080"
  },
  {
    "text": "maybe in your case do you not allow any",
    "start": "1614080",
    "end": "1616640"
  },
  {
    "text": "user to touch that oh M CL at all we",
    "start": "1616640",
    "end": "1619559"
  },
  {
    "text": "don't allow wait for yeah this no no no",
    "start": "1619559",
    "end": "1622760"
  },
  {
    "text": "uh like Risk condition happens in this",
    "start": "1622760",
    "end": "1625360"
  },
  {
    "text": "case yeah and the second case is for the",
    "start": "1625360",
    "end": "1627960"
  },
  {
    "text": "objects uh resources applied to that",
    "start": "1627960",
    "end": "1630120"
  },
  {
    "text": "cluster they will not have any automatic",
    "start": "1630120",
    "end": "1631960"
  },
  {
    "text": "scaling yeah we don't enable HPA yeah",
    "start": "1631960",
    "end": "1634919"
  },
  {
    "text": "okay I see and and even in that case",
    "start": "1634919",
    "end": "1637799"
  },
  {
    "text": "what happens when there there's a",
    "start": "1637799",
    "end": "1639279"
  },
  {
    "text": "network partition so you think that this",
    "start": "1639279",
    "end": "1641679"
  },
  {
    "text": "this cluster has five gpus you you you",
    "start": "1641679",
    "end": "1644120"
  },
  {
    "text": "reserve deducted that from your in",
    "start": "1644120",
    "end": "1645880"
  },
  {
    "text": "memory right but that cluster somehow is",
    "start": "1645880",
    "end": "1648279"
  },
  {
    "text": "not right or whatever happens you know",
    "start": "1648279",
    "end": "1650640"
  },
  {
    "text": "things would happen yeah we lose the",
    "start": "1650640",
    "end": "1652080"
  },
  {
    "text": "connection like that lose the connection",
    "start": "1652080",
    "end": "1653399"
  },
  {
    "text": "or the some Noe is gone so now it cannot",
    "start": "1653399",
    "end": "1655919"
  },
  {
    "text": "have that 5 gpus uh what happens there",
    "start": "1655919",
    "end": "1659200"
  },
  {
    "text": "uh okay that's a really good question so",
    "start": "1659200",
    "end": "1661799"
  },
  {
    "text": "uh one thing is a is about like the",
    "start": "1661799",
    "end": "1663760"
  },
  {
    "text": "house check right so which So currently",
    "start": "1663760",
    "end": "1666600"
  },
  {
    "text": "the house check only check the API",
    "start": "1666600",
    "end": "1668440"
  },
  {
    "text": "server but but it doesn't means like the",
    "start": "1668440",
    "end": "1671399"
  },
  {
    "text": "the cluster is really helps say if",
    "start": "1671399",
    "end": "1674360"
  },
  {
    "text": "something happens like P DNS is down so",
    "start": "1674360",
    "end": "1676640"
  },
  {
    "text": "command Community has a uh proposal to",
    "start": "1676640",
    "end": "1680120"
  },
  {
    "text": "improve that health check and then like",
    "start": "1680120",
    "end": "1682600"
  },
  {
    "text": "to give a more accurate information back",
    "start": "1682600",
    "end": "1685200"
  },
  {
    "text": "to the scheduler to see this cluster is",
    "start": "1685200",
    "end": "1687480"
  },
  {
    "text": "really has really healthy and give the",
    "start": "1687480",
    "end": "1690519"
  },
  {
    "text": "what happened to the previous 5gp I",
    "start": "1690519",
    "end": "1692320"
  },
  {
    "text": "already deducted from that",
    "start": "1692320",
    "end": "1694360"
  },
  {
    "text": "uh yeah this",
    "start": "1694360",
    "end": "1696640"
  },
  {
    "text": "is we yeah I I don't think we problem",
    "start": "1696640",
    "end": "1700120"
  },
  {
    "text": "yeah I just want to curious whether it's",
    "start": "1700120",
    "end": "1701840"
  },
  {
    "text": "solved or not thank",
    "start": "1701840",
    "end": "1704720"
  },
  {
    "text": "you hi great talk thank you have you",
    "start": "1704720",
    "end": "1707679"
  },
  {
    "text": "considered using something like Q",
    "start": "1707679",
    "end": "1709399"
  },
  {
    "text": "instead as as the scheduler and not rely",
    "start": "1709399",
    "end": "1712360"
  },
  {
    "text": "on kada scheduler yeah this is a really",
    "start": "1712360",
    "end": "1714880"
  },
  {
    "text": "good question uh we heard this question",
    "start": "1714880",
    "end": "1717039"
  },
  {
    "text": "before um so we we treat kamada as a",
    "start": "1717039",
    "end": "1720600"
  },
  {
    "text": "multicluster management other than only",
    "start": "1720600",
    "end": "1724320"
  },
  {
    "text": "uh bat job scheduler but we talk with",
    "start": "1724320",
    "end": "1726840"
  },
  {
    "text": "the uh Q community and we are also going",
    "start": "1726840",
    "end": "1729640"
  },
  {
    "text": "to talk with volcano and kamada U",
    "start": "1729640",
    "end": "1732159"
  },
  {
    "text": "Community to try to see if we can have a",
    "start": "1732159",
    "end": "1735360"
  },
  {
    "text": "better multiple que to replace the",
    "start": "1735360",
    "end": "1737480"
  },
  {
    "text": "current commod schedule like we can",
    "start": "1737480",
    "end": "1739320"
  },
  {
    "text": "converge finally at the same the same",
    "start": "1739320",
    "end": "1741880"
  },
  {
    "text": "point yeah thank you that's our goal",
    "start": "1741880",
    "end": "1743840"
  },
  {
    "text": "yeah",
    "start": "1743840",
    "end": "1745559"
  },
  {
    "text": "okay hi thank you for your talk I've got",
    "start": "1745559",
    "end": "1748600"
  },
  {
    "text": "a quick question how is the Federation",
    "start": "1748600",
    "end": "1750960"
  },
  {
    "text": "on this with Cara function differently",
    "start": "1750960",
    "end": "1754200"
  },
  {
    "text": "than the Federation in the more",
    "start": "1754200",
    "end": "1755840"
  },
  {
    "text": "traditional HPC uh schedule or slurm so",
    "start": "1755840",
    "end": "1759600"
  },
  {
    "text": "if you can tell me the difference how",
    "start": "1759600",
    "end": "1760799"
  },
  {
    "text": "they do stuff it would be appreciate it",
    "start": "1760799",
    "end": "1763200"
  },
  {
    "text": "I I'm I'm not aser for slurm I canot",
    "start": "1763200",
    "end": "1767559"
  },
  {
    "text": "talk to because it does the same thing",
    "start": "1767559",
    "end": "1770120"
  },
  {
    "text": "without using kubernetes and I wanted to",
    "start": "1770120",
    "end": "1771799"
  },
  {
    "text": "know if there was a further difference",
    "start": "1771799",
    "end": "1773519"
  },
  {
    "text": "than simply um what do you call it it's",
    "start": "1773519",
    "end": "1776679"
  },
  {
    "text": "just kubernetes native what not yeah",
    "start": "1776679",
    "end": "1778679"
  },
  {
    "text": "it's just kubernetes native yeah thank",
    "start": "1778679",
    "end": "1783039"
  },
  {
    "text": "you uh okay",
    "start": "1787559",
    "end": "1790399"
  },
  {
    "text": "cool U thank you everyone for coming",
    "start": "1790399",
    "end": "1796600"
  }
]