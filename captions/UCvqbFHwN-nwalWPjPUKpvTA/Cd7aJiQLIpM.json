[
  {
    "text": "[Applause] right good afternoon everyone hope",
    "start": "0",
    "end": "7230"
  },
  {
    "text": "you're having a great conference so today I'm going to be talking about some",
    "start": "7230",
    "end": "12450"
  },
  {
    "text": "design and architectural considerations for running highly available stateful",
    "start": "12450",
    "end": "17850"
  },
  {
    "text": "applications in kubernetes over the last few years we've made tremendous strides",
    "start": "17850",
    "end": "26060"
  },
  {
    "text": "supporting stateful workloads and adoption has been increasing every year",
    "start": "26060",
    "end": "32180"
  },
  {
    "text": "Portability and self-healing are one of the most compelling aspects of",
    "start": "32180",
    "end": "37230"
  },
  {
    "text": "kubernetes and as you learned from the keynote earlier this morning and the",
    "start": "37230",
    "end": "42270"
  },
  {
    "text": "storage one-on-one session earlier today in storage persistent volume claims",
    "start": "42270",
    "end": "47370"
  },
  {
    "text": "provide that portability layer for kubernetes storage however that doesn't",
    "start": "47370",
    "end": "54899"
  },
  {
    "text": "mean you can completely forget that house storage you still need to understand what storage options are",
    "start": "54899",
    "end": "61949"
  },
  {
    "text": "available to you what are their various trade-offs and what is best for your",
    "start": "61949",
    "end": "67500"
  },
  {
    "text": "application so today I'm going to try to answer some of those questions I'll",
    "start": "67500",
    "end": "74250"
  },
  {
    "text": "first give an overview of the various options that we support today in kubernetes and then I'm going to deep",
    "start": "74250",
    "end": "80220"
  },
  {
    "text": "dive into building highly available stateful applications specifically",
    "start": "80220",
    "end": "85409"
  },
  {
    "text": "focusing on how to achieve failure domain spreading with storage I'm going",
    "start": "85409",
    "end": "91619"
  },
  {
    "text": "to give a demo of various real storage systems that are in kubernetes today and",
    "start": "91619",
    "end": "98270"
  },
  {
    "text": "demo how they behave in different failure scenarios and then I'm going to",
    "start": "98270",
    "end": "103680"
  },
  {
    "text": "end with a discussion on pod downtime and how to build recovery into your",
    "start": "103680",
    "end": "110549"
  },
  {
    "text": "application so let's dive right into",
    "start": "110549",
    "end": "115890"
  },
  {
    "text": "what we support in kubernetes today there are a lot of options available to",
    "start": "115890",
    "end": "121710"
  },
  {
    "text": "you we have over 15 in tree storage drivers and over 35 CSI drivers and the",
    "start": "121710",
    "end": "129149"
  },
  {
    "text": "count keeps on increasing these drivers interface with a wide",
    "start": "129149",
    "end": "134849"
  },
  {
    "text": "architecture of different storage systems ranging from local to remote",
    "start": "134849",
    "end": "140069"
  },
  {
    "text": "storage cloud services to network appliances software-defined storage",
    "start": "140069",
    "end": "145440"
  },
  {
    "text": "systems distributed systems hyper-converged systems the list goes on so with all of these various options",
    "start": "145440",
    "end": "154080"
  },
  {
    "text": "available to you how do you go about picking what is the best for your",
    "start": "154080",
    "end": "159470"
  },
  {
    "text": "applications in your environment let's break down these systems into various",
    "start": "159470",
    "end": "167069"
  },
  {
    "text": "characteristics that can help narrow down your choices so here are some",
    "start": "167069",
    "end": "174450"
  },
  {
    "text": "characteristics that I think are especially important for building a highly available application first is",
    "start": "174450",
    "end": "182750"
  },
  {
    "text": "accessibility this is about how close your application needs to be to the",
    "start": "182750",
    "end": "189870"
  },
  {
    "text": "underlying storage back-end does they need to be in the same node it doesn't need to be in the same rack or zone or",
    "start": "189870",
    "end": "196680"
  },
  {
    "text": "can it be anywhere in the cluster this is very important because this will",
    "start": "196680",
    "end": "202320"
  },
  {
    "text": "determine how flexible your application pods can be placed in the cluster",
    "start": "202320",
    "end": "208100"
  },
  {
    "text": "especially when there's a lot of moving parts and you might have different node",
    "start": "208100",
    "end": "213120"
  },
  {
    "text": "failures going on next is availability this is about how resilient your storage",
    "start": "213120",
    "end": "221639"
  },
  {
    "text": "system is to different cluster failures can your storage system survive a node",
    "start": "221639",
    "end": "228870"
  },
  {
    "text": "failure or a rack failure or a zone failure often times availability is very",
    "start": "228870",
    "end": "236010"
  },
  {
    "text": "close or very similar to accessibility but that's not always the case and I",
    "start": "236010",
    "end": "241829"
  },
  {
    "text": "will provide an example of this later then we have durability this is about",
    "start": "241829",
    "end": "248519"
  },
  {
    "text": "the data replication factor underneath and how many disks can fail before",
    "start": "248519",
    "end": "255660"
  },
  {
    "text": "you've completely lost your data and then we have access mode",
    "start": "255660",
    "end": "261620"
  },
  {
    "text": "accessmode determines how many nodes can simultaneously mount the volume at the",
    "start": "261620",
    "end": "268350"
  },
  {
    "text": "same time and this is important because it's going to restrict how many replicas",
    "start": "268350",
    "end": "275100"
  },
  {
    "text": "of your application you can have running concurrently all right and some other",
    "start": "275100",
    "end": "282930"
  },
  {
    "text": "important characteristics that you should always be considering also with any system is performance and cost so",
    "start": "282930",
    "end": "290970"
  },
  {
    "text": "now I'm going to go into some examples of specific storage systems and where",
    "start": "290970",
    "end": "297150"
  },
  {
    "text": "they fall into these various characteristics let's start with local disks a local disk is directly attached",
    "start": "297150",
    "end": "304830"
  },
  {
    "text": "to a node and it generally offers the best performance and the best cost",
    "start": "304830",
    "end": "312200"
  },
  {
    "text": "however it has the worst availability and durability if that disk or that node",
    "start": "312200",
    "end": "318870"
  },
  {
    "text": "fails then any pod using that disk is also unavailable it's important to also",
    "start": "318870",
    "end": "326940"
  },
  {
    "text": "point out that in many cloud environments the local disk offerings are not durable beyond the life of the",
    "start": "326940",
    "end": "333450"
  },
  {
    "text": "VM which means that if that VM gets terminated or the physical host has an",
    "start": "333450",
    "end": "339420"
  },
  {
    "text": "error then all the data on that local disk is completely lost so this is",
    "start": "339420",
    "end": "345270"
  },
  {
    "text": "something you really need to carefully consider if you want to use local disks in your application next we have cloud",
    "start": "345270",
    "end": "354480"
  },
  {
    "text": "disks these are generally block devices that show up as block devices in your VM",
    "start": "354480",
    "end": "361650"
  },
  {
    "text": "but underneath are actually remotely accessed over the network typically they",
    "start": "361650",
    "end": "367680"
  },
  {
    "text": "are restricted to a single zone but they",
    "start": "367680",
    "end": "372960"
  },
  {
    "text": "are replicated within that zone so compared to local disks cloud disks have",
    "start": "372960",
    "end": "380200"
  },
  {
    "text": "proved durability and accessibility but the performance may not be as good and",
    "start": "380200",
    "end": "386650"
  },
  {
    "text": "the cost is a little more some examples of cloud disks include GCE PD AWS EBS",
    "start": "386650",
    "end": "393760"
  },
  {
    "text": "volumes and Azure disks some cloud providers also have a replicated cloud",
    "start": "393760",
    "end": "400240"
  },
  {
    "text": "disk offering such as GCE regional PDS these types of disks actually",
    "start": "400240",
    "end": "407260"
  },
  {
    "text": "synchronously replicate the data across multiple zones so you have improved accessibility and availability but the",
    "start": "407260",
    "end": "415590"
  },
  {
    "text": "performance may not be as good because now you have to have synchronous replication whenever you write and the",
    "start": "415590",
    "end": "421060"
  },
  {
    "text": "cost is also increased because now you're taking up the disk is taking up twice as many next we have a single NFS",
    "start": "421060",
    "end": "431830"
  },
  {
    "text": "server access is greatly improved because you can mount that NFS share",
    "start": "431830",
    "end": "437410"
  },
  {
    "text": "from anywhere in the cluster you can also have multiple amounts of the same",
    "start": "437410",
    "end": "442960"
  },
  {
    "text": "share across multiple nodes at the same time however this single server remains",
    "start": "442960",
    "end": "448750"
  },
  {
    "text": "a single point of failure in your system so this is one example where availability and accessibility are not",
    "start": "448750",
    "end": "455020"
  },
  {
    "text": "the same cost compared to a cloud disk or let's say durability also depends on",
    "start": "455020",
    "end": "462510"
  },
  {
    "text": "what the underlying block device is that's backing your NFS server if say we",
    "start": "462510",
    "end": "468370"
  },
  {
    "text": "use a cloud disk to back it then we'll have that 3x durability within that zone",
    "start": "468370",
    "end": "474010"
  },
  {
    "text": "and cost is a little more expensive because now we have to pay for compute",
    "start": "474010",
    "end": "480820"
  },
  {
    "text": "and network resources and in terms of performance now you have an extra overhead of some Network protocol on top",
    "start": "480820",
    "end": "487120"
  },
  {
    "text": "and you might also have traffic coming across various zones lastly we have",
    "start": "487120",
    "end": "494200"
  },
  {
    "text": "scale out or H a file or systems when you configure it as such you can have",
    "start": "494200",
    "end": "501100"
  },
  {
    "text": "globe you can have global accessibility global availability and durability the performance varies a",
    "start": "501100",
    "end": "509330"
  },
  {
    "text": "lot depending on what the actual underlying system is but there are a lot",
    "start": "509330",
    "end": "514370"
  },
  {
    "text": "of options available in this space both commercial options and open source options but generally the cost of these",
    "start": "514370",
    "end": "522979"
  },
  {
    "text": "systems are the most expensive because they're all they're generally the most complex software buys so now that you",
    "start": "522979",
    "end": "535250"
  },
  {
    "text": "have an understanding of some of the storage options that are available in kubernetes now you can use that",
    "start": "535250",
    "end": "542180"
  },
  {
    "text": "information to decide what is best for your application but before we get into",
    "start": "542180",
    "end": "549860"
  },
  {
    "text": "that first when you're building a highly available application it's very",
    "start": "549860",
    "end": "556370"
  },
  {
    "text": "important to consider to build redundancy into the system and to avoid",
    "start": "556370",
    "end": "562790"
  },
  {
    "text": "single points of failure so in kubernetes one way that helps achieve",
    "start": "562790",
    "end": "570980"
  },
  {
    "text": "that goal is pod and high-affinity this is this gives you the ability to spread",
    "start": "570980",
    "end": "577940"
  },
  {
    "text": "your applications pods across different failure domains so that failure in one",
    "start": "577940",
    "end": "583400"
  },
  {
    "text": "area does not take down your entire application in this particular example",
    "start": "583400",
    "end": "589700"
  },
  {
    "text": "the failure domain is zone as specified by the topology key but it can also be",
    "start": "589700",
    "end": "597020"
  },
  {
    "text": "node it can also be rack it can be any topology you define in your environment",
    "start": "597020",
    "end": "604150"
  },
  {
    "text": "this feature just leverages the same kubernetes labels that you use for",
    "start": "604150",
    "end": "609500"
  },
  {
    "text": "anything else so let's go into some",
    "start": "609500",
    "end": "615070"
  },
  {
    "text": "common common stateful application architectures see how they can use pod",
    "start": "615070",
    "end": "623140"
  },
  {
    "text": "affinity and see how that works with some of the various storage options that we discussed so the first type of",
    "start": "623140",
    "end": "632680"
  },
  {
    "text": "application to consider is one that follows the 12 factor model this is",
    "start": "632680",
    "end": "638890"
  },
  {
    "text": "where the application does not store any unique state between all its replicas so",
    "start": "638890",
    "end": "647589"
  },
  {
    "text": "all of the replicas can share the same storage back-end and some examples of",
    "start": "647589",
    "end": "654279"
  },
  {
    "text": "these are content management systems these applications do not provide data",
    "start": "654279",
    "end": "661390"
  },
  {
    "text": "redundancy at the application level so they depend on the storage layer to",
    "start": "661390",
    "end": "667570"
  },
  {
    "text": "provide that high availability and data redundancy features so storage systems",
    "start": "667570",
    "end": "674020"
  },
  {
    "text": "that are really good for this type of application are multi writer systems that are globally accessible and",
    "start": "674020",
    "end": "680320"
  },
  {
    "text": "available so the scale out or H a filer systems will fit well into this model",
    "start": "680320",
    "end": "687930"
  },
  {
    "text": "and to deploy this model in kubernetes",
    "start": "687930",
    "end": "693510"
  },
  {
    "text": "the best way to do that is to use a kubernetes deployment in a kubernetes",
    "start": "693510",
    "end": "700990"
  },
  {
    "text": "deployment all of the pod replicas are the same including what volumes they use",
    "start": "700990",
    "end": "708190"
  },
  {
    "text": "so you can see here in this example all the pods are spread across different",
    "start": "708190",
    "end": "713680"
  },
  {
    "text": "zones but they're all sharing the same persistent volume claim and what that",
    "start": "713680",
    "end": "719770"
  },
  {
    "text": "means is that this persistent volume claim also has to be accessible across all of those zones and when a one one of",
    "start": "719770",
    "end": "729190"
  },
  {
    "text": "the zones becomes unavailable then this application is still able to continue",
    "start": "729190",
    "end": "734950"
  },
  {
    "text": "serving its data because the storage layer is providing that",
    "start": "734950",
    "end": "740060"
  },
  {
    "text": "high availability and these other Papa replica pods in these other zones are",
    "start": "740060",
    "end": "746660"
  },
  {
    "text": "still able to access their storage so",
    "start": "746660",
    "end": "752630"
  },
  {
    "text": "it's important to point out here that deployments are good with multi writer",
    "start": "752630",
    "end": "760850"
  },
  {
    "text": "volume types single writer volume types do not work well with dis model because",
    "start": "760850",
    "end": "766940"
  },
  {
    "text": "they can only be accessed from one node at a time which means that you would be",
    "start": "766940",
    "end": "772010"
  },
  {
    "text": "restricted to only having one replica in your deployment which will not give you",
    "start": "772010",
    "end": "777020"
  },
  {
    "text": "high availability instead single writer",
    "start": "777020",
    "end": "782630"
  },
  {
    "text": "volumes are better suited for distributed applications these are",
    "start": "782630",
    "end": "787820"
  },
  {
    "text": "applications that shard and replicate their data between the application pots",
    "start": "787820",
    "end": "793510"
  },
  {
    "text": "some examples of these include distributed databases like Cassandra and",
    "start": "793510",
    "end": "798590"
  },
  {
    "text": " just to name a few and these applications do not need high",
    "start": "798590",
    "end": "804710"
  },
  {
    "text": "availability at the storage layer because they've already built it in to the application layer so these single",
    "start": "804710",
    "end": "812180"
  },
  {
    "text": "writer volumes are great for these they don't have to be globally accessible or",
    "start": "812180",
    "end": "817610"
  },
  {
    "text": "available so you can choose potentially",
    "start": "817610",
    "end": "822620"
  },
  {
    "text": "cheaper and more performant options such as local disks or cloud disks for these types of applications and in kubernetes",
    "start": "822620",
    "end": "832430"
  },
  {
    "text": "the way that the stateful sets are the way to that best support distributed",
    "start": "832430",
    "end": "838670"
  },
  {
    "text": "applications the main difference between stateful sets and deployments are that",
    "start": "838670",
    "end": "846010"
  },
  {
    "text": "two main things one the every pod in a stateful set has a unique and stable",
    "start": "846010",
    "end": "852920"
  },
  {
    "text": "identity and then second every replicas in a stateful set has its own unique",
    "start": "852920",
    "end": "859430"
  },
  {
    "text": "persistent volume claim so you can see in this example that we have pot zero",
    "start": "859430",
    "end": "865340"
  },
  {
    "text": "using PVC zero pod one using PVC one and pod using PVC - if any of those replicas",
    "start": "865340",
    "end": "872230"
  },
  {
    "text": "dies it's going to come back with the same name and it's going to come back with the same storage and in this case",
    "start": "872230",
    "end": "879700"
  },
  {
    "text": "if one of the zones goes out the application is still able to serve the",
    "start": "879700",
    "end": "885430"
  },
  {
    "text": "data from its remaining shards and you can bring up new pods to replace the",
    "start": "885430",
    "end": "891970"
  },
  {
    "text": "failed one and potentially new disks to replace to failed one and those those",
    "start": "891970",
    "end": "897580"
  },
  {
    "text": "new replicas will end up reconstructing the data and rejoining the application",
    "start": "897580",
    "end": "903970"
  },
  {
    "text": "cluster all right so this wasn't always",
    "start": "903970",
    "end": "912280"
  },
  {
    "text": "easy to do in trooper Nettie's especially when using volume types that",
    "start": "912280",
    "end": "918660"
  },
  {
    "text": "were not globally available or accessible in the cluster kubernetes originally assumed that you can access",
    "start": "918660",
    "end": "925420"
  },
  {
    "text": "volumes from any node but now with the volume topology feature we've added",
    "start": "925420",
    "end": "931570"
  },
  {
    "text": "intelligence into the scheduler so that it can better understand these more",
    "start": "931570",
    "end": "937360"
  },
  {
    "text": "constrained volume types there are there's no user configuration that you",
    "start": "937360",
    "end": "942520"
  },
  {
    "text": "need to use this feature it's the underlying storage provider is giving",
    "start": "942520",
    "end": "948580"
  },
  {
    "text": "this information to kubernetes but now you will be able to auto scale your",
    "start": "948580",
    "end": "954730"
  },
  {
    "text": "replicas and dynamic dynamically provisioned those volumes across zones",
    "start": "954730",
    "end": "961560"
  },
  {
    "text": "alright so now i'm going to go into a demo i'm going to demo a app i'm going",
    "start": "962970",
    "end": "973300"
  },
  {
    "text": "to demo applications that are not highly available and the reason for that is i",
    "start": "973300",
    "end": "979300"
  },
  {
    "text": "want to be able to show you how some real storage systems behave in certain",
    "start": "979300",
    "end": "986050"
  },
  {
    "text": "scenarios so that you can see that behavior you know without the",
    "start": "986050",
    "end": "992260"
  },
  {
    "text": "application layer doing redundancy and you can decide if that's",
    "start": "992260",
    "end": "997530"
  },
  {
    "text": "a model that is best suited for your application so in my initial setup in my",
    "start": "997530",
    "end": "1003860"
  },
  {
    "text": "cluster I have six nodes spread across three zones and I have four stateful",
    "start": "1003860",
    "end": "1011030"
  },
  {
    "text": "sets each with one replica and each of",
    "start": "1011030",
    "end": "1016370"
  },
  {
    "text": "the four stateful sets are using a different type of storage class I have",
    "start": "1016370",
    "end": "1022160"
  },
  {
    "text": "one storage class that uses local disks one that uses a single zone volume",
    "start": "1022160",
    "end": "1028910"
  },
  {
    "text": "another one that uses a multi zone volume and the last one which is using",
    "start": "1028910",
    "end": "1034430"
  },
  {
    "text": "an NFS volume that's globally available so let's go into the cluster alright",
    "start": "1034430",
    "end": "1046600"
  },
  {
    "text": "okay so let's take a look at my notes again so you can see I've got six notes",
    "start": "1046600",
    "end": "1054970"
  },
  {
    "text": "spread across three zones two nodes each let's take a look",
    "start": "1054970",
    "end": "1061180"
  },
  {
    "text": "mypods so I've got four pots and you can",
    "start": "1061180",
    "end": "1068810"
  },
  {
    "text": "see I've labeled them I've named the pods with the the name of the type of",
    "start": "1068810",
    "end": "1073850"
  },
  {
    "text": "storage that it's using and if we take a look at the actual disks themselves I've",
    "start": "1073850",
    "end": "1086030"
  },
  {
    "text": "also got four PPC's let's take a look at the TVs for fun so let's look at the",
    "start": "1086030",
    "end": "1095390"
  },
  {
    "text": "local disk first you'll see here that this volume is has",
    "start": "1095390",
    "end": "1104560"
  },
  {
    "text": "a type of a local volume it has a path to that disc on that node and it has",
    "start": "1104560",
    "end": "1111190"
  },
  {
    "text": "this node affinity field filled in to it and you can see this node affinity is",
    "start": "1111190",
    "end": "1117520"
  },
  {
    "text": "basically telling kubernetes that any pod using this volume can only be",
    "start": "1117520",
    "end": "1123730"
  },
  {
    "text": "scheduled two nodes that have the label hostname with this value so this is",
    "start": "1123730",
    "end": "1131440"
  },
  {
    "text": "information that the storage provider inserted itself this is not something",
    "start": "1131440",
    "end": "1137020"
  },
  {
    "text": "that you as users have to specify in your PVCs but basically you can see here",
    "start": "1137020",
    "end": "1145120"
  },
  {
    "text": "this local volume is restricted to this single zone or certainly single node and if we take a look at the next volume",
    "start": "1145120",
    "end": "1157169"
  },
  {
    "text": "let's look at our single zone volume you",
    "start": "1158310",
    "end": "1163540"
  },
  {
    "text": "can see here that this type is a cloud disk it's a GCE persistent disk and it also",
    "start": "1163540",
    "end": "1169480"
  },
  {
    "text": "has no Definity but instead of having no affinity on a single node it has no",
    "start": "1169480",
    "end": "1175540"
  },
  {
    "text": "affinity in a single zone and here it's saying the zone is Europe West 1c so",
    "start": "1175540",
    "end": "1182500"
  },
  {
    "text": "this tells the scheduler that pods using this type of all or this volume can only",
    "start": "1182500",
    "end": "1187750"
  },
  {
    "text": "be scheduled within that one zone and if we look at our next one which is our",
    "start": "1187750",
    "end": "1195400"
  },
  {
    "text": "multi zone disk you will see that this",
    "start": "1195400",
    "end": "1200830"
  },
  {
    "text": "is also a GCE persistent disk it's a special flavor of it called a regional PD and the special thing about it is",
    "start": "1200830",
    "end": "1209140"
  },
  {
    "text": "that it can be accessed from two zones so if you have a node if you have a",
    "start": "1209140",
    "end": "1215230"
  },
  {
    "text": "complete failure in one of the zones you can have replacement pods come up in a",
    "start": "1215230",
    "end": "1220600"
  },
  {
    "text": "second zone and lastly we will look at",
    "start": "1220600",
    "end": "1226860"
  },
  {
    "text": "the",
    "start": "1226860",
    "end": "1229860"
  },
  {
    "text": "here we can see it's an NFS volume it's here's a type II and there is no node",
    "start": "1235790",
    "end": "1241860"
  },
  {
    "text": "affinity on it this is telling the scheduler you can schedule pods using this volume anywhere alright so that is",
    "start": "1241860",
    "end": "1251880"
  },
  {
    "text": "all the volumes on my system now I'm going to have some fun and fail some",
    "start": "1251880",
    "end": "1259230"
  },
  {
    "text": "notes so let's",
    "start": "1259230",
    "end": "1265549"
  },
  {
    "text": "alright ok I got my four pods you will notice that I've initially scheduled all",
    "start": "1271990",
    "end": "1279730"
  },
  {
    "text": "four pods on the same node you probably don't want to do this in production but",
    "start": "1279730",
    "end": "1285340"
  },
  {
    "text": "I'm just doing it to demonstrate so I'm gonna go ahead and fill this note",
    "start": "1285340",
    "end": "1292780"
  },
  {
    "text": "does anyone want to take a guess what's going to happen anyone anyone yeah yes",
    "start": "1292780",
    "end": "1309550"
  },
  {
    "text": "that's right the local the pod using the local volume is not going to be able to",
    "start": "1309550",
    "end": "1314650"
  },
  {
    "text": "come up after I've failed this node because the disk that it needs is on that node which is now unavailable but",
    "start": "1314650",
    "end": "1321790"
  },
  {
    "text": "the other parts since they are accessible to more than one node aren't",
    "start": "1321790",
    "end": "1326800"
  },
  {
    "text": "going to be able to recover so let's go ahead and fail that node all right so",
    "start": "1326800",
    "end": "1338400"
  },
  {
    "text": "you can see that all the pods have immediately are terminating and restarting so while we're waiting for",
    "start": "1338400",
    "end": "1345100"
  },
  {
    "text": "that I just wanted to point out the way I failed this node is by using a node",
    "start": "1345100",
    "end": "1351280"
  },
  {
    "text": "tinct this is a mechanism in kubernetes to apply special conditions to notes so",
    "start": "1351280",
    "end": "1358960"
  },
  {
    "text": "you can see here we have this cue cuddle taint command and you specify what node",
    "start": "1358960",
    "end": "1364240"
  },
  {
    "text": "you want to taint and you give it a condition and I I just made up some condition and the other thing you can",
    "start": "1364240",
    "end": "1374430"
  },
  {
    "text": "the other thing that's associated with a taint is an effect and the main I think",
    "start": "1374430",
    "end": "1380590"
  },
  {
    "text": "the two main effects that are supported today are no schedule which means it tells the scheduler if a node is tainted",
    "start": "1380590",
    "end": "1387070"
  },
  {
    "text": "don't schedule any more pods to this node and what I've used here is the no",
    "start": "1387070",
    "end": "1392830"
  },
  {
    "text": "execute effect which means not only don't schedule any notes to this or",
    "start": "1392830",
    "end": "1397990"
  },
  {
    "text": "don't schedule any pods to this node anymore but also evict all of the pods",
    "start": "1397990",
    "end": "1403450"
  },
  {
    "text": "that are on it because this is unhealthy and I don't want anything running there anymore um let's see",
    "start": "1403450",
    "end": "1415730"
  },
  {
    "text": "what's going on okay my pots are still",
    "start": "1415730",
    "end": "1422390"
  },
  {
    "text": "pending let's take a look at one of the",
    "start": "1422390",
    "end": "1428420"
  },
  {
    "text": "pods and see what's going on",
    "start": "1428420",
    "end": "1432040"
  },
  {
    "text": "oh that's fun okay so I accidentally",
    "start": "1445270",
    "end": "1451590"
  },
  {
    "text": "tainted all of the nodes in my cluster let me go reset this all right let's try",
    "start": "1451590",
    "end": "1463870"
  },
  {
    "text": "this again I'm going to go into it or fail one node okay",
    "start": "1463870",
    "end": "1473100"
  },
  {
    "text": "all right here we go so we see this NFS pod has so far been able to recover",
    "start": "1488860",
    "end": "1496559"
  },
  {
    "text": "we'll wait for the others well I guess while we're waiting for the others let's look at the so we said that we expected",
    "start": "1496679",
    "end": "1504880"
  },
  {
    "text": "the local pod to not be able to come up let's take a look at that spell so we'll",
    "start": "1504880",
    "end": "1520210"
  },
  {
    "text": "see that the local pod has failed scheduling because one of the nodes has",
    "start": "1520210",
    "end": "1526690"
  },
  {
    "text": "a taint on it and the other five of the other remaining five nodes in my cluster",
    "start": "1526690",
    "end": "1532139"
  },
  {
    "text": "cannot access that volume alright so I",
    "start": "1532139",
    "end": "1538419"
  },
  {
    "text": "think the three the remaining three pods were able to recover now let's clean",
    "start": "1538419",
    "end": "1547149"
  },
  {
    "text": "this up a little bit okay so we have three running pods now the local pod is",
    "start": "1547149",
    "end": "1553240"
  },
  {
    "text": "gone now I'm going to go and taint the next node this node that I'm going to",
    "start": "1553240",
    "end": "1559929"
  },
  {
    "text": "take taint is going to be the second node in that zone which is going to take",
    "start": "1559929",
    "end": "1565149"
  },
  {
    "text": "out all of the notes for that zone so",
    "start": "1565149",
    "end": "1570480"
  },
  {
    "text": "let's fill another node okay now all the pods are we starting again and what",
    "start": "1570600",
    "end": "1578649"
  },
  {
    "text": "should happen this time now is that the single zone pod is going to fail to come",
    "start": "1578649",
    "end": "1584440"
  },
  {
    "text": "up because now the whole zone is gone but the other two the my multi zone pod",
    "start": "1584440",
    "end": "1590049"
  },
  {
    "text": "and my NFS pod will should still be able to recover",
    "start": "1590049",
    "end": "1595620"
  },
  {
    "text": "let's take a look",
    "start": "1600259",
    "end": "1603649"
  },
  {
    "text": "so what happened there was that when the first time I felt the note it got",
    "start": "1615240",
    "end": "1620669"
  },
  {
    "text": "rescheduled to a completely other zone so when I filled the whole whole zone these two pods already they did not have",
    "start": "1620669",
    "end": "1629010"
  },
  {
    "text": "to restart because they were not in the zone that I failed so yeah now we can see let's take a look at my",
    "start": "1629010",
    "end": "1637919"
  },
  {
    "text": "single zone pod and we can see it failed",
    "start": "1637919",
    "end": "1643289"
  },
  {
    "text": "scheduling because two of the notes that it could run on have tanks on them and the remainder of the remaining four",
    "start": "1643289",
    "end": "1650970"
  },
  {
    "text": "nodes cannot access that single zone pod alright so as you can see from that demo",
    "start": "1650970",
    "end": "1659970"
  },
  {
    "text": "it's really important to consider when you're choosing a storage system to consider their availability and",
    "start": "1659970",
    "end": "1666960"
  },
  {
    "text": "accessibility characteristics and see if your application can handle that so",
    "start": "1666960",
    "end": "1675659"
  },
  {
    "text": "let's go back to the presentation that's not ok alright so while we're on the",
    "start": "1675659",
    "end": "1687120"
  },
  {
    "text": "topic of pod failures now I'm going to talk about pod downtime and why it's",
    "start": "1687120",
    "end": "1695190"
  },
  {
    "text": "important to factor this into any applications architecture the total time",
    "start": "1695190",
    "end": "1703169"
  },
  {
    "text": "that a pod is down is consists of the time to detect failure and the time it",
    "start": "1703169",
    "end": "1710610"
  },
  {
    "text": "takes to replace that pod and from the demo the time to detect failure was",
    "start": "1710610",
    "end": "1718110"
  },
  {
    "text": "essentially zero because I just I manually tainted the note when I wanted",
    "start": "1718110",
    "end": "1723900"
  },
  {
    "text": "to fail it but in a real production environment there has to be something",
    "start": "1723900",
    "end": "1729990"
  },
  {
    "text": "that is monitoring those nodes and that something also has to after some",
    "start": "1729990",
    "end": "1735570"
  },
  {
    "text": "threshold period go ahead and taint that node as healthy in order to trigger all the",
    "start": "1735570",
    "end": "1742620"
  },
  {
    "text": "affections of those pods so that's going to add some time to your downtime and",
    "start": "1742620",
    "end": "1749000"
  },
  {
    "text": "then to bring up a new pod you kind of saw a little bit of this in the demo",
    "start": "1749000",
    "end": "1755809"
  },
  {
    "text": "some pods were faster to come up than others the single rider volume pods have",
    "start": "1755809",
    "end": "1764460"
  },
  {
    "text": "to first because they're only accessible on a single node in order to bring it up",
    "start": "1764460",
    "end": "1770640"
  },
  {
    "text": "on a new node you need to detach it from the first node and then go and we attach",
    "start": "1770640",
    "end": "1775800"
  },
  {
    "text": "it to that second node so you might have noticed that the NFS pod was able to",
    "start": "1775800",
    "end": "1781320"
  },
  {
    "text": "come up a lot faster because it doesn't have to go it can it can mount multiple",
    "start": "1781320",
    "end": "1786929"
  },
  {
    "text": "on multiple nodes at the same time and it doesn't have to wait for this detach and reattach so all of this factors in",
    "start": "1786929",
    "end": "1796260"
  },
  {
    "text": "to the total downtime of your pod so it's very important to build this in",
    "start": "1796260",
    "end": "1801750"
  },
  {
    "text": "when you are figuring out what your what your maximum degraded",
    "start": "1801750",
    "end": "1807360"
  },
  {
    "text": "downtime State can be and there's one",
    "start": "1807360",
    "end": "1812700"
  },
  {
    "text": "more caveat that I want to point out especially with stateful sets so many",
    "start": "1812700",
    "end": "1819150"
  },
  {
    "text": "stateful applications require exactly once semantics what this means is that",
    "start": "1819150",
    "end": "1824280"
  },
  {
    "text": "it cannot have two containers write to the same volume otherwise it can cause",
    "start": "1824280",
    "end": "1830580"
  },
  {
    "text": "data corruption to that disk so the way that this is supported in kubernetes is",
    "start": "1830580",
    "end": "1838130"
  },
  {
    "text": "that when you delete your pod that pod",
    "start": "1838130",
    "end": "1843420"
  },
  {
    "text": "object doesn't actually get deleted immediately what happens is that it gets",
    "start": "1843420",
    "end": "1849270"
  },
  {
    "text": "into a termination phase or a terminating phase and then cubelet on",
    "start": "1849270",
    "end": "1854429"
  },
  {
    "text": "that node will start tearing down the containers and then once those containers are torn",
    "start": "1854429",
    "end": "1860380"
  },
  {
    "text": "down it's going to tear down the volumes and cubelet is going to be the one that",
    "start": "1860380",
    "end": "1866230"
  },
  {
    "text": "finally deletes that pod object from the API server once only after it has",
    "start": "1866230",
    "end": "1872230"
  },
  {
    "text": "confirmed all the containers are not running anymore and all the volumes are unmelted however if cubelet itself is",
    "start": "1872230",
    "end": "1880540"
  },
  {
    "text": "unresponsive then you have this split-brain scenario where you don't",
    "start": "1880540",
    "end": "1886690"
  },
  {
    "text": "know if you don't know if the node is still healthy but it's just partitioned and the applications are still running",
    "start": "1886690",
    "end": "1893530"
  },
  {
    "text": "fine or the node is actually down and you want to recover your application so",
    "start": "1893530",
    "end": "1901470"
  },
  {
    "text": "when cubelet is not responsive it's not able to delete that pot object and then",
    "start": "1901470",
    "end": "1907150"
  },
  {
    "text": "the stateful set controller is not able to create a replacement pod for that until cubelet is able to acknowledge",
    "start": "1907150",
    "end": "1914350"
  },
  {
    "text": "that the pod is actually not running anymore so a common way to solve this is",
    "start": "1914350",
    "end": "1920590"
  },
  {
    "text": "a technique called node fencing this gives the this technique ensures that",
    "start": "1920590",
    "end": "1928049"
  },
  {
    "text": "the workload containers are not running on that node anymore in bare metal",
    "start": "1928049",
    "end": "1935110"
  },
  {
    "text": "environments this is often achieved by using IPMI power we set to the server",
    "start": "1935110",
    "end": "1942640"
  },
  {
    "text": "and in cloud environments something similar can be done by deleting the VM",
    "start": "1942640",
    "end": "1949080"
  },
  {
    "text": "but because this is environment specific there is no common solution in",
    "start": "1949080",
    "end": "1954130"
  },
  {
    "text": "kubernetes many kubernetes providers have built functionality on top to give",
    "start": "1954130",
    "end": "1961360"
  },
  {
    "text": "this capability as an example gke has a node auto repair feature that will go",
    "start": "1961360",
    "end": "1967450"
  },
  {
    "text": "and kill your VM after it has detected the node to be unhealthy for 10 minutes",
    "start": "1967450",
    "end": "1973110"
  },
  {
    "text": "so you know adding all of these factors together recovering a single state full",
    "start": "1973110",
    "end": "1981370"
  },
  {
    "text": "set pod can take on the order of minutes or even hours if you don't have such an",
    "start": "1981370",
    "end": "1987880"
  },
  {
    "text": "automated solution and you need to go in and manually covered them so for that reason it's",
    "start": "1987880",
    "end": "1993960"
  },
  {
    "text": "very important that when you're designing your highly available application you build pot you don't",
    "start": "1993960",
    "end": "1999630"
  },
  {
    "text": "depend on the availability of a single pot because that single pod will become",
    "start": "1999630",
    "end": "2006470"
  },
  {
    "text": "your single point of failure right so in",
    "start": "2006470",
    "end": "2012950"
  },
  {
    "text": "summary while the stateful workload experience is not completely seamless",
    "start": "2012950",
    "end": "2019180"
  },
  {
    "text": "we've made many improvements in kubernetes over the last few releases to",
    "start": "2019180",
    "end": "2024770"
  },
  {
    "text": "make this better we have features like volume topology pod anti affinity and no",
    "start": "2024770",
    "end": "2031010"
  },
  {
    "text": "taints that will help with in making it easier to deploy and manage your highly",
    "start": "2031010",
    "end": "2040070"
  },
  {
    "text": "available applications we also covered some high-level architectural models of",
    "start": "2040070",
    "end": "2048440"
  },
  {
    "text": "deploying stateful applications using pod anti affinity and we discussed when",
    "start": "2048440",
    "end": "2055129"
  },
  {
    "text": "it's appropriate to use a deployment versus a stateful set and also consider",
    "start": "2055130",
    "end": "2061540"
  },
  {
    "text": "either hat when you're designing applications to have the data redundancy",
    "start": "2061540",
    "end": "2067100"
  },
  {
    "text": "either at the storage layer or at the application layer and perhaps most",
    "start": "2067100",
    "end": "2073129"
  },
  {
    "text": "importantly when you are designing an application a highly available application be sure to build redundancy",
    "start": "2073130",
    "end": "2080929"
  },
  {
    "text": "into your system and account for downtime of your individual replicas so",
    "start": "2080930",
    "end": "2087379"
  },
  {
    "text": "doing so will make your applications more resilient in the long run so that's",
    "start": "2087380",
    "end": "2096500"
  },
  {
    "text": "it for more details on the various",
    "start": "2096500",
    "end": "2101750"
  },
  {
    "text": "specific topics that I covered today feel free to these slides are available",
    "start": "2101750",
    "end": "2106880"
  },
  {
    "text": "on the schedule so you can go ahead and click on any of these links for more",
    "start": "2106880",
    "end": "2112040"
  },
  {
    "text": "information I covered a lot of topics",
    "start": "2112040",
    "end": "2117380"
  },
  {
    "text": "across a lot of SIG's so I think everything I covered here is generally falls under the realm of six",
    "start": "2117380",
    "end": "2124560"
  },
  {
    "text": "storage SiC apps cig note and cig scheduling if you're interested in any",
    "start": "2124560",
    "end": "2130230"
  },
  {
    "text": "of these specific topics you can join the community meetings and slack",
    "start": "2130230",
    "end": "2136080"
  },
  {
    "text": "channels and to ask to ask for anything and if you want to get involved in the",
    "start": "2136080",
    "end": "2143310"
  },
  {
    "text": "project you can also feel free to reach out to me and I can help point you in the right direction right so I'm going",
    "start": "2143310",
    "end": "2150810"
  },
  {
    "text": "to leave the remainder of time for questions",
    "start": "2150810",
    "end": "2155119"
  },
  {
    "text": "[Applause] [Music] [Applause]",
    "start": "2157070",
    "end": "2168850"
  },
  {
    "text": "hi thanks for the talk there are some cases like in when running in AWS where",
    "start": "2175570",
    "end": "2183460"
  },
  {
    "text": "some of these strivers like the elastic file system for leverages a sin alpha",
    "start": "2183460",
    "end": "2189340"
  },
  {
    "text": "since a month so this is track class is not available so you have to be playing",
    "start": "2189340",
    "end": "2195220"
  },
  {
    "text": "with selectors and a particular pool for for this this some other way to play one",
    "start": "2195220",
    "end": "2203260"
  },
  {
    "text": "with MPs in this case when this storage tracks are not available for your",
    "start": "2203260",
    "end": "2209200"
  },
  {
    "text": "provider oh sorry I'm not I'm not able to like catch I think it's there are for",
    "start": "2209200",
    "end": "2215350"
  },
  {
    "text": "example if we have the classes in AWS and some of these drivers like the last",
    "start": "2215350",
    "end": "2221470"
  },
  {
    "text": "e-file system the MPs are not available or in alpha version which really earlier",
    "start": "2221470",
    "end": "2227020"
  },
  {
    "text": "states so we ended having separated pool with an MVS mounted and the pot",
    "start": "2227020",
    "end": "2233230"
  },
  {
    "text": "basically moons a host path and is the way you can use Network file system with",
    "start": "2233230",
    "end": "2240690"
  },
  {
    "text": "oh I see so you're asking specifically because the EF SCSI driver is still",
    "start": "2240690",
    "end": "2247030"
  },
  {
    "text": "alpha yeah then what is the best way to use that exam I am NOT very familiar",
    "start": "2247030",
    "end": "2253150"
  },
  {
    "text": "with AWS I would say like so my",
    "start": "2253150",
    "end": "2261040"
  },
  {
    "text": "understanding is that EFS is like an NFS type of thing yes it is so I think that you can there's an entry",
    "start": "2261040",
    "end": "2268390"
  },
  {
    "text": "NFS driver that you can use to just generally mount NFS shares it works",
    "start": "2268390",
    "end": "2274420"
  },
  {
    "text": "across to any it doesn't actually care what is what that backing NFS server is it just needs an IP and a share I'm not",
    "start": "2274420",
    "end": "2282820"
  },
  {
    "text": "sure if EFS has like any special protocols or permissions that you need the special driver for I wouldn't",
    "start": "2282820",
    "end": "2289990"
  },
  {
    "text": "thinking it okay anyway thank you but I can if you want to sync up I know the",
    "start": "2289990",
    "end": "2296050"
  },
  {
    "text": "person who's looking on the EFS driver I can like yeah yeah and related to that question",
    "start": "2296050",
    "end": "2306140"
  },
  {
    "text": "if you need to to expose some storage read/write many to many parts for",
    "start": "2306140",
    "end": "2314010"
  },
  {
    "text": "example in in the case where you were talking about the SMEs would it be",
    "start": "2314010",
    "end": "2320270"
  },
  {
    "text": "logical to have some multi zone disk",
    "start": "2320270",
    "end": "2325609"
  },
  {
    "text": "have a pod running an FS server and maybe the cluster making a persistent",
    "start": "2325609",
    "end": "2334230"
  },
  {
    "text": "volume claim for it so this is you're",
    "start": "2334230",
    "end": "2340290"
  },
  {
    "text": "asking about how to make an NFS server more available across multiple zones",
    "start": "2340290",
    "end": "2348440"
  },
  {
    "text": "yeah and inside the cluster I mean running the NFS server not outside the",
    "start": "2348440",
    "end": "2353760"
  },
  {
    "text": "cluster uh-huh but inside the draft area for yeah so there are various open",
    "start": "2353760",
    "end": "2360480"
  },
  {
    "text": "source scale-out NFS solutions I think like Stefan cluster are some popular",
    "start": "2360480",
    "end": "2367200"
  },
  {
    "text": "ones where they basically take inventory of all the local disks and then present",
    "start": "2367200",
    "end": "2374640"
  },
  {
    "text": "this you know like scalable NFS layer on top of it so that's one way to do it the",
    "start": "2374640",
    "end": "2384359"
  },
  {
    "text": "other common way I see people do do this in kubernetes is to deploy like a single",
    "start": "2384359",
    "end": "2390420"
  },
  {
    "text": "NFS server using some cloud disk that's backing it so I like I mentioned like",
    "start": "2390420",
    "end": "2400230"
  },
  {
    "text": "that single NFS server is a single point of failure if you use something that is",
    "start": "2400230",
    "end": "2405990"
  },
  {
    "text": "a single zone cloud disk then if that zone goes down that your NFS is down if",
    "start": "2405990",
    "end": "2411839"
  },
  {
    "text": "you use like a replicated cloud disk solution like regional PD then you are",
    "start": "2411839",
    "end": "2418230"
  },
  {
    "text": "more resilient to that single zone failures so now you can if that whole zone fails you can do your NFS server",
    "start": "2418230",
    "end": "2424349"
  },
  {
    "text": "pod can scheduled to a different zone and and",
    "start": "2424349",
    "end": "2429850"
  },
  {
    "text": "that Regional PD can access that there's still downtime though there's still some",
    "start": "2429850",
    "end": "2436810"
  },
  {
    "text": "downtime because the regional PD is still a single rider volume so you have",
    "start": "2436810",
    "end": "2442150"
  },
  {
    "text": "to detach it and reattach it so it's not going to be like completely seamless failover this one last question",
    "start": "2442150",
    "end": "2451300"
  },
  {
    "text": "here right far far away so I'm wondering",
    "start": "2451300",
    "end": "2460420"
  },
  {
    "text": "about this potential split brain situation then if you're not worrying",
    "start": "2460420",
    "end": "2466390"
  },
  {
    "text": "about someone had something writing to volume but you are worried about something updating a database through",
    "start": "2466390",
    "end": "2475290"
  },
  {
    "text": "yeah over TCP connection to some other or that still alive",
    "start": "2475290",
    "end": "2480340"
  },
  {
    "text": "what does Courtin help can you use that",
    "start": "2480340",
    "end": "2485070"
  },
  {
    "text": "so I think if you don't actually care about the volume layer but you care",
    "start": "2485910",
    "end": "2491380"
  },
  {
    "text": "about the date the application layer writing to a database then that's something where you might still need to",
    "start": "2491380",
    "end": "2497620"
  },
  {
    "text": "build in some concurrency in your application so like having some sort of leader election or distributed locking",
    "start": "2497620",
    "end": "2504490"
  },
  {
    "text": "system all right thank you very much if",
    "start": "2504490",
    "end": "2511360"
  },
  {
    "text": "you have any further questions you can catch her outside",
    "start": "2511360",
    "end": "2516270"
  }
]