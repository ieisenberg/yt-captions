[
  {
    "start": "0",
    "end": "74000"
  },
  {
    "text": "so welcome to San Diego Cuba knots I've not been back to San Diego since I was",
    "start": "30",
    "end": "5879"
  },
  {
    "text": "actually born was born in the area but I've never been back so this talk is",
    "start": "5879",
    "end": "11400"
  },
  {
    "text": "living with the pathology of the cloud and how AWS ruins lots and lots of clusters I couldn't fit in lots and lots",
    "start": "11400",
    "end": "17880"
  },
  {
    "text": "I think there's like a 75 character limit so I had to abbreviate it we'll",
    "start": "17880",
    "end": "25380"
  },
  {
    "text": "jump right in so first I kind of wanna give a little bit of background on Amazon eks it's our elastic kubernetes",
    "start": "25380",
    "end": "31980"
  },
  {
    "text": "service you may have heard of it this talk before I jump into even eks",
    "start": "31980",
    "end": "39420"
  },
  {
    "text": "and sort of what what we do and what it is is not really intended to give you",
    "start": "39420",
    "end": "45710"
  },
  {
    "text": "you don't have to follow all of what we do to build a planet-sized line at scale application but I kind of want to pull",
    "start": "45710",
    "end": "53370"
  },
  {
    "text": "back the curtain a little bit and so you can kind of see how does AWS run a service and or build a service and kind",
    "start": "53370",
    "end": "60539"
  },
  {
    "text": "of talk about some design decisions trade-offs nuances so there's not always",
    "start": "60539",
    "end": "65640"
  },
  {
    "text": "a best answer to everything so what are some of the decisions we made for",
    "start": "65640",
    "end": "71100"
  },
  {
    "text": "operating our service so before I even jump into the service itself the first",
    "start": "71100",
    "end": "79170"
  },
  {
    "start": "74000",
    "end": "204000"
  },
  {
    "text": "thing I wanted set first just some level setting is what are the eks service",
    "start": "79170",
    "end": "84630"
  },
  {
    "text": "priorities so at Amazon you may have heard this if you've heard other Amazon talks talking about services the first",
    "start": "84630",
    "end": "89880"
  },
  {
    "text": "really two things that we really value in operating our services our security",
    "start": "89880",
    "end": "95220"
  },
  {
    "text": "and operational reliability these aren't just things we say we actually like say these to each other a lot day to day",
    "start": "95220",
    "end": "102360"
  },
  {
    "text": "when we get like a security issue whether it's a CBE or just something",
    "start": "102360",
    "end": "107490"
  },
  {
    "text": "that we noticed or something that's even a concern of a customer we take that like very seriously that good like gets",
    "start": "107490",
    "end": "114360"
  },
  {
    "text": "escalated very very quickly and we drop everything to make that right and that",
    "start": "114360",
    "end": "119640"
  },
  {
    "text": "goes the same for our operational reliability as well like our customers",
    "start": "119640",
    "end": "125340"
  },
  {
    "text": "trust us to run you I'm assuming you here are some of our customers to run",
    "start": "125340",
    "end": "131069"
  },
  {
    "text": "your critical applications on our service and so we take that really seriously and we want to offer a really",
    "start": "131069",
    "end": "136959"
  },
  {
    "text": "highly secure and stable environment for you to do that so today like we've eks",
    "start": "136959",
    "end": "143560"
  },
  {
    "text": "has created in literally millions of kubernetes clusters and run hundreds of",
    "start": "143560",
    "end": "150849"
  },
  {
    "text": "thousands of cluster upgrades so this is something that like we do a lot so we have a lot of we've developed a lot of",
    "start": "150849",
    "end": "157629"
  },
  {
    "text": "experience doing but we also like highly automated as well so we want you to be",
    "start": "157629",
    "end": "165040"
  },
  {
    "text": "able to benefit from our operational stability in so in the last year and a",
    "start": "165040",
    "end": "171699"
  },
  {
    "text": "half since we launched we've seen like you know a number of kubernetes cds come out and so over the over that period for",
    "start": "171699",
    "end": "180400"
  },
  {
    "text": "even some of the more recent ones you might not have even known that we necessarily upgraded your cluster before",
    "start": "180400",
    "end": "187120"
  },
  {
    "text": "the CV was hit announced that's that's kind of the level that we operate and we take again we take like security and",
    "start": "187120",
    "end": "192639"
  },
  {
    "text": "operational stability very very seriously we want you to basically rely on us to make sure that the cluster work",
    "start": "192639",
    "end": "199419"
  },
  {
    "text": "the control plane works and your applications run so backing up there's",
    "start": "199419",
    "end": "207159"
  },
  {
    "text": "kind of a high-level overview of what does the sort of service map look like",
    "start": "207159",
    "end": "212409"
  },
  {
    "text": "for when you have an e KS cluster so what is it what is AWS provide for you today it's really well up until",
    "start": "212409",
    "end": "220329"
  },
  {
    "text": "yesterday it really was just the managed kubernetes control plane so what does",
    "start": "220329",
    "end": "225340"
  },
  {
    "text": "that mean well if you're running kubernetes there's the sort of three primary components the cube root cube",
    "start": "225340",
    "end": "230349"
  },
  {
    "text": "api server the controller manager is scheduler and then there's the sed database behind it it CDs the key value",
    "start": "230349",
    "end": "238239"
  },
  {
    "text": "store database that's open source we'll get into the a little bit more of a TD",
    "start": "238239",
    "end": "243579"
  },
  {
    "text": "later but we run that across multiple availability zones for you for high availability we automatically fail away",
    "start": "243579",
    "end": "250209"
  },
  {
    "text": "from any you know AZ in a specific zone issue if there's an issue aspect",
    "start": "250209",
    "end": "256000"
  },
  {
    "text": "affecting a specific zone we maintain that these actually implement this with",
    "start": "256000",
    "end": "261760"
  },
  {
    "text": "Auto scale groups just like you would as a customer for your own application",
    "start": "261760",
    "end": "267430"
  },
  {
    "text": "using a lot of the same a Tobias primitives we'll get into a little bit we have a network load balancer again",
    "start": "267430",
    "end": "273250"
  },
  {
    "text": "another AWS product surprise in front of the API servers and then you the",
    "start": "273250",
    "end": "278979"
  },
  {
    "text": "customer have the data playing what we're calling in your account so that's where you see your ec2 instances and",
    "start": "278979",
    "end": "284830"
  },
  {
    "text": "those can be you see join Tunes instances you launch in the console could be it could be more likely",
    "start": "284830",
    "end": "291460"
  },
  {
    "text": "probably auto-scaling groups that you manage with either cloud formation or terraform or something like that or cops",
    "start": "291460",
    "end": "299050"
  },
  {
    "text": "or any or not guess not cops in this case but cops is this sort of similar thing for it's it's a data plane so a",
    "start": "299050",
    "end": "306250"
  },
  {
    "text": "feature we just launched yesterday has managed node groups and you see that here that feature I won't go into a huge",
    "start": "306250",
    "end": "314169"
  },
  {
    "text": "feature pitch but what the basic premise is that you can click a button and get nodes and get minute upgrades so you can",
    "start": "314169",
    "end": "321780"
  },
  {
    "text": "look for more information there but back to kind of our service well how do we",
    "start": "321780",
    "end": "328060"
  },
  {
    "start": "325000",
    "end": "365000"
  },
  {
    "text": "build this well as I've been saying it's all based on all these AWS primitives",
    "start": "328060",
    "end": "333190"
  },
  {
    "text": "we're not using any super-secret",
    "start": "333190",
    "end": "338099"
  },
  {
    "text": "virtualization technology that is not available to our customers eks is actually implemented with almost",
    "start": "338340",
    "end": "344289"
  },
  {
    "text": "entirely actually I could say entirely aw public AWS services that you can you",
    "start": "344289",
    "end": "349570"
  },
  {
    "text": "can use there's a few api's pop you know like things like billing that I would we expose that as an API you don't need",
    "start": "349570",
    "end": "355300"
  },
  {
    "text": "that but for the for the rest of the service it's all public API s one of the",
    "start": "355300",
    "end": "362889"
  },
  {
    "text": "other things that we do is as we've built the services we've adopted what's",
    "start": "362889",
    "end": "368259"
  },
  {
    "start": "365000",
    "end": "504000"
  },
  {
    "text": "called a cellular base architecture this is becoming more and more popular inside AWS and we're starting to talk about this more what this means is we have",
    "start": "368259",
    "end": "375699"
  },
  {
    "text": "sort of independent silos of operation and I'll just kind of show you what that looks like in a second these can be",
    "start": "375699",
    "end": "382020"
  },
  {
    "text": "depending on how you implement it logical or physical the silos can buy",
    "start": "382020",
    "end": "388120"
  },
  {
    "text": "physical I mean could be say as own or it could be a logical across zones what",
    "start": "388120",
    "end": "393729"
  },
  {
    "text": "this really helps with is horizontal charting and why do you want when do you choose charting you choose",
    "start": "393729",
    "end": "399729"
  },
  {
    "text": "when you have a large scale and that's what AWS is sort of known for right like we have we operated a really large scale",
    "start": "399729",
    "end": "406479"
  },
  {
    "text": "we have small high-functioning service teams and they can't do one-off things",
    "start": "406479",
    "end": "412689"
  },
  {
    "text": "for every customer that we we operate so we we really need high skyper formance",
    "start": "412689",
    "end": "418990"
  },
  {
    "text": "high scale operation here so this this horizontal starting sort of helps us scale out not up especially when you do",
    "start": "418990",
    "end": "427479"
  },
  {
    "text": "operate at scale you you typically do hit lines not and I don't just mean AWS limits these can be physical limits all",
    "start": "427479",
    "end": "433539"
  },
  {
    "text": "right like how many how many packets can you get in a in a network connection or",
    "start": "433539",
    "end": "439020"
  },
  {
    "text": "you know how much bandwidth or how much CPU you can get on a box like yeah there",
    "start": "439020",
    "end": "445899"
  },
  {
    "text": "are a number of limits that you start to hit so that's why we favor more the the scaling out model other benefits to this",
    "start": "445899",
    "end": "452229"
  },
  {
    "text": "the cellular architecture are you get reduced blast radius if if one cell is",
    "start": "452229",
    "end": "458889"
  },
  {
    "text": "affected by say a downstream service or configure 8 miss configuration or",
    "start": "458889",
    "end": "464169"
  },
  {
    "text": "something like that you don't end up affecting everything in that will call it partition or that group of cells we",
    "start": "464169",
    "end": "472599"
  },
  {
    "text": "actually so we we char de cells by region so ETS is a regional service so you see kis in u.s. East One or us os/2",
    "start": "472599",
    "end": "480639"
  },
  {
    "text": "or or Dublin or some other AWS region",
    "start": "480639",
    "end": "485879"
  },
  {
    "text": "but we have in some of our bigger regions up to 20 cells or more than 20",
    "start": "485879",
    "end": "491229"
  },
  {
    "text": "cells so the idea being when we start a roll out there if we start noticing issues in one cell we block the roll out",
    "start": "491229",
    "end": "497259"
  },
  {
    "text": "roll back and then only customers in that one cell or affected and not not the rest and then some of the trade-offs",
    "start": "497259",
    "end": "505990"
  },
  {
    "start": "504000",
    "end": "602000"
  },
  {
    "text": "of this are again not everything is PG right there's there's work for this",
    "start": "505990",
    "end": "511449"
  },
  {
    "text": "there's a increased complexity if I have a say a customer cluster we'll call it a",
    "start": "511449",
    "end": "518018"
  },
  {
    "text": "customer right cluster ID a unique identifier for a cluster and I'm an operator and I need to debug an issue",
    "start": "518019",
    "end": "523870"
  },
  {
    "text": "I can't just look up from a single database Oh which which cluster is this I have to do a little bit more",
    "start": "523870",
    "end": "529860"
  },
  {
    "text": "indirection because we're split in two separate shards this also necessitates",
    "start": "529860",
    "end": "536700"
  },
  {
    "text": "then the upfront investment in operational tooling and automation",
    "start": "536700",
    "end": "542380"
  },
  {
    "text": "because when you have n copies of something instead of just one",
    "start": "542380",
    "end": "548110"
  },
  {
    "text": "if you've ever op like for those of you who are AWS customers which I imagine a large portion of you are here if you",
    "start": "548110",
    "end": "554830"
  },
  {
    "text": "have just one Amazon account like no no shade that's fine but what I what we",
    "start": "554830",
    "end": "560530"
  },
  {
    "text": "highly recommend is is splitting up your if you have a pre-production environment put that in a different account because",
    "start": "560530",
    "end": "567310"
  },
  {
    "text": "that helps for some of these same sorts of reasons insulate you so this if",
    "start": "567310",
    "end": "573070"
  },
  {
    "text": "you've worked across multiple Amazon account so you sort of have started if you see the need for okay I need tooling because I can't just query it in in one",
    "start": "573070",
    "end": "580600"
  },
  {
    "text": "account imagine that times 20 per region so you have whatever how many public",
    "start": "580600",
    "end": "587170"
  },
  {
    "text": "regions are there 18 now I think and you have numerous cells we're talking about hundreds of cells so investigating a",
    "start": "587170",
    "end": "594790"
  },
  {
    "text": "specific issue requires an investment upfront investment in in this automation and tooling so how is the cluster",
    "start": "594790",
    "end": "603850"
  },
  {
    "start": "602000",
    "end": "828000"
  },
  {
    "text": "created you as a customer say okay of your command line the AWS CLI you say aw",
    "start": "603850",
    "end": "609880"
  },
  {
    "text": "F CKS create cluster or maybe use ETS Kittel or you use the console or whatever is well you in the end or the",
    "start": "609880",
    "end": "619330"
  },
  {
    "text": "customer making an API request against an API and in our case that's it was on",
    "start": "619330",
    "end": "625000"
  },
  {
    "text": "API gateway we actually implement our service with lambda so we have a lambda",
    "start": "625000",
    "end": "631240"
  },
  {
    "text": "backed API gateway back by a lambda function which will take your request do",
    "start": "631240",
    "end": "637450"
  },
  {
    "text": "some look at lookups about your account about your you know authorization",
    "start": "637450",
    "end": "642970"
  },
  {
    "text": "authentication all that and then can we create this cluster okay let's create this cluster and this gets handed off to",
    "start": "642970",
    "end": "650520"
  },
  {
    "text": "through a number of number of steps but it goes to a step function so we're using again same sort of a public AWS",
    "start": "650520",
    "end": "657280"
  },
  {
    "text": "tooling that you're and products that you're familiar with and in this whole",
    "start": "657280",
    "end": "663550"
  },
  {
    "text": "cellular based architecture [Music] even if you have all these cells you still have to have a singular front end",
    "start": "663550",
    "end": "669460"
  },
  {
    "text": "right like there's a single ingress point for all this so we try to keep that front end layer as thin as possible",
    "start": "669460",
    "end": "675850"
  },
  {
    "text": "and sort of start out to these various horizontal cells so when this step",
    "start": "675850",
    "end": "683110"
  },
  {
    "text": "function gets your cluster require create request the next thing that it",
    "start": "683110",
    "end": "688630"
  },
  {
    "text": "does is profit obviously process this request and creates a cloud formation stack for your cluster so in this setup",
    "start": "688630",
    "end": "695560"
  },
  {
    "text": "we create a unique stack for your cluster which contains all the other AWS",
    "start": "695560",
    "end": "703510"
  },
  {
    "text": "components required for that specific cluster and we sort of think about and",
    "start": "703510",
    "end": "712360"
  },
  {
    "text": "on just on our team when we talk about our control plane team in our data plane team our control plane is all the lambda",
    "start": "712360",
    "end": "719980"
  },
  {
    "text": "API gateway Dunwoody be all that fun stuff and our data plane is the actual",
    "start": "719980",
    "end": "725500"
  },
  {
    "text": "sort of ec2 instances and components that you the customer interact with when you interact with kubernetes so when you",
    "start": "725500",
    "end": "732070"
  },
  {
    "text": "interact with when you create the AWS eks cluster we're using the AWS CLI when",
    "start": "732070",
    "end": "737470"
  },
  {
    "text": "you you start interacting with the cluster using the kubernetes cube control and that is talking to a",
    "start": "737470",
    "end": "744940"
  },
  {
    "text": "different end point right it's not the AWS you kiss service it's a different it's a different endpoint back by again",
    "start": "744940",
    "end": "751330"
  },
  {
    "text": "the API servers so this is this cluster",
    "start": "751330",
    "end": "758740"
  },
  {
    "text": "how is it created so like we said there's a step function creates cloud formation but what else all in that",
    "start": "758740",
    "end": "765070"
  },
  {
    "text": "cloud formation well there's a number of things so it's it's a progression and",
    "start": "765070",
    "end": "770440"
  },
  {
    "text": "this is sort of why you see a lot of question you get is like why does the",
    "start": "770440",
    "end": "775690"
  },
  {
    "text": "eks cluster take so long to create so today we've just it's something that we",
    "start": "775690",
    "end": "781780"
  },
  {
    "text": "are continually optimizing and we'll be optimizing more but we create you a specific VPC for your cluster so each",
    "start": "781780",
    "end": "788470"
  },
  {
    "text": "cluster gets its own AWS PPC we create two Auto scale groups one for at cd14",
    "start": "788470",
    "end": "794760"
  },
  {
    "text": "API servers then I am roles for those and elastic load balancers in front as well",
    "start": "794760",
    "end": "802269"
  },
  {
    "text": "as a number of other resources it came as keys all those kinds of things so that that's sort of the like the high",
    "start": "802269",
    "end": "809829"
  },
  {
    "text": "level stack of when when your clusters created these these are all the things",
    "start": "809829",
    "end": "814870"
  },
  {
    "text": "that are going on in the background so yeah again this is just kind of a",
    "start": "814870",
    "end": "820779"
  },
  {
    "text": "high-level view so now now that the cluster is created let's talk about some of the like more fun stuff the failure",
    "start": "820779",
    "end": "827260"
  },
  {
    "text": "stories so in creating these millions of clusters there's a lot of things that",
    "start": "827260",
    "end": "834010"
  },
  {
    "start": "828000",
    "end": "1218000"
  },
  {
    "text": "can can happen right most of the time the fast fast fast majority of timing",
    "start": "834010",
    "end": "840850"
  },
  {
    "text": "creating millions of Luster's nothing bad happens which is really great but even when creating or when it's up",
    "start": "840850",
    "end": "848800"
  },
  {
    "text": "occasionally things don't go as expected and sometimes this is a downstream dependency sometimes this is could be an",
    "start": "848800",
    "end": "854889"
  },
  {
    "text": "AWS you know you guess code bug it could but also it could be specific to kubernetes so I kind of want to dive",
    "start": "854889",
    "end": "860740"
  },
  {
    "text": "into some of the interest in kubernetes learnings because I think those are",
    "start": "860740",
    "end": "865839"
  },
  {
    "text": "really like really helpful just in general so one of one of the more interesting ones to me is this this semi",
    "start": "865839",
    "end": "874480"
  },
  {
    "text": "recent bug that we got a bug request from a customer it's basically queue baby GI server",
    "start": "874480",
    "end": "881100"
  },
  {
    "text": "fails to connect to a new web app on so the the group quests basically stated",
    "start": "881100",
    "end": "887440"
  },
  {
    "text": "customers running a mutating admission web book which I'll show kind of what",
    "start": "887440",
    "end": "893019"
  },
  {
    "text": "that means in a minute if you're not familiar with that it was happened to be in this case sto the ec2 node that the",
    "start": "893019",
    "end": "901149"
  },
  {
    "text": "pod for this web hook was running on gets terminated there's no fin in the TCP stream a new",
    "start": "901149",
    "end": "910000"
  },
  {
    "text": "web hook pod comes up on a new instance to answer requests but the API server",
    "start": "910000",
    "end": "916720"
  },
  {
    "text": "the eks managed API server doesn't connect to this new web hook for almost",
    "start": "916720",
    "end": "924370"
  },
  {
    "text": "15 minutes so if you're familiar with this to you at all if this customer is running when you don't mutate pods they",
    "start": "924370",
    "end": "930490"
  },
  {
    "text": "don't get the Envoy sidecar which they can't connect to your services which is production outage so that's a",
    "start": "930490",
    "end": "936690"
  },
  {
    "text": "bad thing right so as we as we kind of unpacking what does this mean on sort of",
    "start": "936690",
    "end": "943709"
  },
  {
    "text": "a network level so when we create these cube API servers again with like",
    "start": "943709",
    "end": "949200"
  },
  {
    "text": "controller manager scheduler a couple of other components that we put on there like or we have a we run CloudWatch log",
    "start": "949200",
    "end": "954540"
  },
  {
    "text": "agent so when you actually enable cluster logging on your account that's CloudWatch log agent putting your",
    "start": "954540",
    "end": "961200"
  },
  {
    "text": "your logs into carwash logs one of the other things that we do when we create these instances is we have a secondary",
    "start": "961200",
    "end": "968450"
  },
  {
    "text": "elastic network interface or a and I attached to it normally when you have any c2 instance you see there's a",
    "start": "968450",
    "end": "974850"
  },
  {
    "text": "network interface attached you can attach secondary interfaces well we have attached one but we attach it in your",
    "start": "974850",
    "end": "979890"
  },
  {
    "text": "account it's across the county and I this is what enables cube cuddle exec",
    "start": "979890",
    "end": "985050"
  },
  {
    "text": "and Long's so when I want to do a port forward or a logs or exact to get down into my",
    "start": "985050",
    "end": "990870"
  },
  {
    "text": "cluster don't with kubernetes that's one of the beautiful things for deep free debugging you don't have to use SSH",
    "start": "990870",
    "end": "996540"
  },
  {
    "text": "everywhere to get into your pods how does that work well you the client talk",
    "start": "996540",
    "end": "1001820"
  },
  {
    "text": "to the API server the API server reaches out over this network interface into you the customers",
    "start": "1001820",
    "end": "1006980"
  },
  {
    "text": "norther customers CPC to talk to cubelet to exact logs do whatever it needs to so",
    "start": "1006980",
    "end": "1013420"
  },
  {
    "text": "this sort of same communication path is what api server uses to talk to web",
    "start": "1013420",
    "end": "1020420"
  },
  {
    "text": "hooks mutating Webfoot mutating admission controller web hooks in your cluster so the API server was reaching",
    "start": "1020420",
    "end": "1026600"
  },
  {
    "text": "out into the customers account like normal to this web hook pod as it was getting pot of great requests would talk",
    "start": "1026600",
    "end": "1033558"
  },
  {
    "text": "to this web hook say okay is everything good okay pass it on mutate it the pod gets created except in this case the",
    "start": "1033559",
    "end": "1043688"
  },
  {
    "text": "node like we said went away for whatever reason and the problem was that this TCP",
    "start": "1043689",
    "end": "1053360"
  },
  {
    "text": "connection TV TCP stream or or this connection was never remade to the new",
    "start": "1053360",
    "end": "1058490"
  },
  {
    "text": "web app odd because the kubernetes scheduler placed it on the write that a new one on the rectangular on",
    "start": "1058490",
    "end": "1063730"
  },
  {
    "text": "different node in the correct place but again 15 minutes after 15 minutes finally the API server started connected",
    "start": "1063730",
    "end": "1070660"
  },
  {
    "text": "to the new one so what do we do we one of the first things we looked at was okay go into the go to the API server run netstat what",
    "start": "1070660",
    "end": "1078940"
  },
  {
    "text": "what's actually where's this connection going and we'd see this foreign address",
    "start": "1078940",
    "end": "1084010"
  },
  {
    "text": "it so the the local address cube API server the foreign address would be the",
    "start": "1084010",
    "end": "1090190"
  },
  {
    "text": "dead nodes IP but it still shows has established that's weird right like what",
    "start": "1090190",
    "end": "1097270"
  },
  {
    "text": "why is that still there after we did a bit of digging around looking to sort of",
    "start": "1097270",
    "end": "1102610"
  },
  {
    "text": "the the Linux kernel system tuning parameters so there's this TCP tries to I won't read all this text but the long",
    "start": "1102610",
    "end": "1111429"
  },
  {
    "text": "and short of this means based on an unacknowledged TCP connection because",
    "start": "1111429",
    "end": "1117520"
  },
  {
    "text": "well I'll back up a little bit go in the cube API server is using obviously TCP",
    "start": "1117520",
    "end": "1123549"
  },
  {
    "text": "here but it's also using HTTP too so the go net HTTP library is using an HTTP to",
    "start": "1123549",
    "end": "1131590"
  },
  {
    "text": "stack to talk to the mutating web book which is the server and when that fin is",
    "start": "1131590",
    "end": "1140440"
  },
  {
    "text": "never sent the TCP connection is held open up to 15 retries here which comes",
    "start": "1140440",
    "end": "1149650"
  },
  {
    "text": "to that 15-minute mark so it was in effect there was no fin being sent but",
    "start": "1149650",
    "end": "1156549"
  },
  {
    "text": "goes HTTP to stack has a bug that forced that connection to not be open basically",
    "start": "1156549",
    "end": "1162520"
  },
  {
    "text": "the client was not sending a ping to say server are you still around and so it just kept the connection open so there's",
    "start": "1162520",
    "end": "1168910"
  },
  {
    "text": "actually been upstream go a bug report and and patch made for this that's not",
    "start": "1168910",
    "end": "1174700"
  },
  {
    "text": "quite yet merged and so one of the",
    "start": "1174700",
    "end": "1180010"
  },
  {
    "text": "short-term fix that we actually recommended this customer in this case is on your Envoy web hook proxy disable",
    "start": "1180010",
    "end": "1188260"
  },
  {
    "text": "ation to be to so have it just an HTTP 1 connection ok yes you might have a few",
    "start": "1188260",
    "end": "1193570"
  },
  {
    "text": "more TCP connections but you won't have this 15 knowledge and then also the upstream",
    "start": "1193570",
    "end": "1199370"
  },
  {
    "text": "communities there's a upstream change until the NGO patch merges and then the",
    "start": "1199370",
    "end": "1205250"
  },
  {
    "text": "kubernetes upgrades to next go version to force the API server to reach out",
    "start": "1205250",
    "end": "1210620"
  },
  {
    "text": "over HTTP one even if the the web book server offers HTTP to one of the other",
    "start": "1210620",
    "end": "1218920"
  },
  {
    "start": "1218000",
    "end": "1611000"
  },
  {
    "text": "interesting edge cases I think is at CD so we did this earlier about how STDs a",
    "start": "1218920",
    "end": "1226190"
  },
  {
    "text": "managed service or part of the managed service for you you don't actually you're not actually directly exposed to",
    "start": "1226190",
    "end": "1231650"
  },
  {
    "text": "it but it's the open-source key value data store it's based on the raft consensus algorithm and on a high level",
    "start": "1231650",
    "end": "1240500"
  },
  {
    "text": "but I won't get all into what raft is but basically wrapped relies on what we call consensus so more than or some",
    "start": "1240500",
    "end": "1248030"
  },
  {
    "text": "number of the instances in the the cluster the sed cluster agreeing on",
    "start": "1248030",
    "end": "1253130"
  },
  {
    "text": "something so that's how state is maintained when you make a change in kubernetes it gets propagated down into its V and the the cluster agrees yes",
    "start": "1253130",
    "end": "1261920"
  },
  {
    "text": "this isn't the the state of this of this object so in it CD quorum is a means",
    "start": "1261920",
    "end": "1268970"
  },
  {
    "text": "that over 50 percent of the instances in a cluster agree to something you want",
    "start": "1268970",
    "end": "1275360"
  },
  {
    "text": "odd numbers for this corn not even numbers because if you have a network partition you want your cluster to",
    "start": "1275360",
    "end": "1284510"
  },
  {
    "text": "continue to function or at least part of the cluster to continue to function and to work right so in this case or we'll",
    "start": "1284510",
    "end": "1292730"
  },
  {
    "text": "use the example of five if there was a direct network partition between the",
    "start": "1292730",
    "end": "1297770"
  },
  {
    "text": "three two of the two and three of these five instances you might see this these",
    "start": "1297770",
    "end": "1303170"
  },
  {
    "text": "sort of access patterns disappear the smaller left side of the cluster would not have porn because quorum a net CD is",
    "start": "1303170",
    "end": "1310910"
  },
  {
    "text": "more than half n minus two plus one we don't meet that here on the left side so that if you try to do reads or writes",
    "start": "1310910",
    "end": "1317060"
  },
  {
    "text": "for those nodes from kubernetes api server it's gonna fail but on the right side three out of the five nodes are",
    "start": "1317060",
    "end": "1323690"
  },
  {
    "text": "still talking so they can still continue operation so this is actually really awesome for high availability",
    "start": "1323690",
    "end": "1331150"
  },
  {
    "text": "it helps you avoid split-brain scenario that's kind of the other reason for this",
    "start": "1331820",
    "end": "1336820"
  },
  {
    "text": "hardly ever though interestingly I mean that this is this is very helpful and good or is there a direct split down the",
    "start": "1337450",
    "end": "1345620"
  },
  {
    "text": "middle like sometimes it's one direction can talk about the other can't or three out of the five can talk to each other but one can only talk to one of the",
    "start": "1345620",
    "end": "1351800"
  },
  {
    "text": "others and it gets really interesting but for the simple case we'll just talk",
    "start": "1351800",
    "end": "1357410"
  },
  {
    "text": "about this so kind of back to the sort of a 3-node case because that's that's what he case operates today but for",
    "start": "1357410",
    "end": "1363260"
  },
  {
    "text": "every eks cluster that you create or",
    "start": "1363260",
    "end": "1368270"
  },
  {
    "text": "that we that we create for you there's three nodes so what happens if one of",
    "start": "1368270",
    "end": "1374000"
  },
  {
    "text": "them goes away well we're still good at quorum right like we're over half but what if that one that you thought was",
    "start": "1374000",
    "end": "1383870"
  },
  {
    "text": "gone is now actually gone so if one is",
    "start": "1383870",
    "end": "1390200"
  },
  {
    "text": "still around and then it comes back but you created a new one and now you have",
    "start": "1390200",
    "end": "1396320"
  },
  {
    "text": "for your quorum just changed so in this case what's quorum it's half plus one so",
    "start": "1396320",
    "end": "1403820"
  },
  {
    "text": "that's three so now that we're at at four nodes and if we get a network",
    "start": "1403820",
    "end": "1411080"
  },
  {
    "text": "partition we're not in a good space right like we're gonna we don't want to",
    "start": "1411080",
    "end": "1416360"
  },
  {
    "text": "we want to avoid slug rain so none of the nodes can answer requests reads or",
    "start": "1416360",
    "end": "1421430"
  },
  {
    "text": "writes and that's obviously the least ideal scenario so one of the the",
    "start": "1421430",
    "end": "1429110"
  },
  {
    "text": "interesting cases that we've seen with sed that we experience sort of early on was around this sort of this idea of",
    "start": "1429110",
    "end": "1437420"
  },
  {
    "text": "nodes that you think are gone but aren't normally when you terminate an instance",
    "start": "1437420",
    "end": "1443090"
  },
  {
    "text": "it happens almost immediately but on a very like single digit number of times it can take like upwards of four us we",
    "start": "1443090",
    "end": "1449810"
  },
  {
    "text": "found we saw a weird event where it took upwards of twenty minutes and we just so",
    "start": "1449810",
    "end": "1455510"
  },
  {
    "text": "happened to have the same habit to have a 20 minute time out in our sed who have a sed",
    "start": "1455510",
    "end": "1461370"
  },
  {
    "text": "manager code so we terminate this instance thinking it's gone",
    "start": "1461370",
    "end": "1468030"
  },
  {
    "text": "our nanny code what it does is watches for that member to delete and then once",
    "start": "1468030",
    "end": "1473460"
  },
  {
    "text": "it's actually deleted removes it from the membership of the cluster so if you",
    "start": "1473460",
    "end": "1478760"
  },
  {
    "text": "delete an instance but don't go to the sed API and say hey that guy's gone now",
    "start": "1478760",
    "end": "1483770"
  },
  {
    "text": "the cluster still thinks he's around somewhere because it doesn't know any better and so in this case this is sort of what",
    "start": "1483770",
    "end": "1491730"
  },
  {
    "text": "this is what happened is we said okay we have now a new instance that came online because that one got terminated but",
    "start": "1491730",
    "end": "1498809"
  },
  {
    "text": "because that member and eventually that member did actually terminate but because of our timeouts and we never",
    "start": "1498809",
    "end": "1504900"
  },
  {
    "text": "actually just cleaned him up what happens next well we do our normal say",
    "start": "1504900",
    "end": "1510990"
  },
  {
    "text": "you a cluster upgrading so we also upgrade your @cd for you it happens behind the covers but we are",
    "start": "1510990",
    "end": "1517200"
  },
  {
    "text": "automation thinking okay we have three nodes we're good if we terminate one to do our normal cluster upgrade",
    "start": "1517200",
    "end": "1522960"
  },
  {
    "text": "everything's going to be fine right we're at quorum we think but that node was actually never terminated so this",
    "start": "1522960",
    "end": "1529860"
  },
  {
    "text": "case looks almost the exact same to at CD as two and two because now two",
    "start": "1529860",
    "end": "1538530"
  },
  {
    "text": "instances don't have a quorum out of four so that that was about to me that",
    "start": "1538530",
    "end": "1544650"
  },
  {
    "text": "was like one of those cases where it was a really interesting learning at CD and kind of how it operates so some of the",
    "start": "1544650",
    "end": "1552059"
  },
  {
    "text": "some for us some of the lessons that are learned from this are like keep it keep backups at that CD for this we already",
    "start": "1552059",
    "end": "1557550"
  },
  {
    "text": "actually had this so we when you create an EPS cluster we manage the sed backups",
    "start": "1557550",
    "end": "1563490"
  },
  {
    "text": "for you like it's it's transparent you don't even know what's happening so if there were something sort of a code bug",
    "start": "1563490",
    "end": "1570090"
  },
  {
    "text": "or data corruption or whatever to happen that we couldn't recover from we have like very regular backups for your sed",
    "start": "1570090",
    "end": "1576960"
  },
  {
    "text": "cluster also monitor your quorum size and membership so when you when you",
    "start": "1576960",
    "end": "1583470"
  },
  {
    "text": "think that you have remembers makes like if you did if you're doing if you're managing it see to yourself if you look",
    "start": "1583470",
    "end": "1589440"
  },
  {
    "text": "at how many instances you have and you think oh I only have three instances and you see two check out CD make sure they actually",
    "start": "1589440",
    "end": "1595140"
  },
  {
    "text": "have three instances in your cluster and then check your check your depends these are telling the truth and if they're not",
    "start": "1595140",
    "end": "1601620"
  },
  {
    "text": "don't assume that they'll get get ready in time like actually and do the manual step of",
    "start": "1601620",
    "end": "1607410"
  },
  {
    "text": "cleaning up one of the other learnings",
    "start": "1607410",
    "end": "1612750"
  },
  {
    "start": "1611000",
    "end": "1667000"
  },
  {
    "text": "this was a fun one too that we had and I want to share this one because I think",
    "start": "1612750",
    "end": "1619620"
  },
  {
    "text": "this one is interesting for anyone who's sort of developing on Koreans you don't have to be operating kubernetes for for",
    "start": "1619620",
    "end": "1625560"
  },
  {
    "text": "this case so I am for service accounts it's a feature we recently launched its using kubernetes projected protected",
    "start": "1625560",
    "end": "1632760"
  },
  {
    "text": "service account volumes and projected projected token volumes it's a way that",
    "start": "1632760",
    "end": "1638790"
  },
  {
    "text": "you can it's a feature we recently launched that lets pods get AWS",
    "start": "1638790",
    "end": "1644160"
  },
  {
    "text": "identities so if you want a pod to assume an AWS role to say to access DynamoDB or s3 or something else like",
    "start": "1644160",
    "end": "1651060"
  },
  {
    "text": "that this is the feature for that and you can have different roles for different pods even on the same node you",
    "start": "1651060",
    "end": "1657600"
  },
  {
    "text": "don't have to have all of your pods you use the same AWS role or using actual",
    "start": "1657600",
    "end": "1665930"
  },
  {
    "text": "key pairs so the way this I won't get into the the entire feature but the way",
    "start": "1665930",
    "end": "1672480"
  },
  {
    "text": "that you as a user of kubernetes consume this there's a little bit of setup involved but you annotate your service",
    "start": "1672480",
    "end": "1679320"
  },
  {
    "text": "account you put an annotation that's got this eks Amazon AWS ro Laren and you put your your role aren't as a",
    "start": "1679320",
    "end": "1686010"
  },
  {
    "text": "eval you that you want pods that are using this service account to assume as",
    "start": "1686010",
    "end": "1692580"
  },
  {
    "text": "we this is sort of a convenience for you so when you do this we have a little bit",
    "start": "1692580",
    "end": "1698610"
  },
  {
    "text": "of magic in the backend the magic is again getting in which admission webhook no surprise right so when you submit a",
    "start": "1698610",
    "end": "1705630"
  },
  {
    "text": "pod that uses that service account that you've annotated we have a web hook that",
    "start": "1705630",
    "end": "1711600"
  },
  {
    "text": "ingests your pod and then based on that annotation will update the pods back so",
    "start": "1711600",
    "end": "1718080"
  },
  {
    "text": "will do will add sort of all the kubernetes config that that you could add yourself and wouldn't",
    "start": "1718080",
    "end": "1724260"
  },
  {
    "text": "the annotation and the this web hook for but again it's a convenience thing so all you can see the sort of the line",
    "start": "1724260",
    "end": "1730230"
  },
  {
    "text": "there and everything below the line gets added by the web hook everything above the line is what you define and if again",
    "start": "1730230",
    "end": "1736320"
  },
  {
    "text": "if you defined existing volumes and environment variables that would all work this gets appended at the end so",
    "start": "1736320",
    "end": "1742710"
  },
  {
    "start": "1741000",
    "end": "1946000"
  },
  {
    "text": "what sort of what's the data path here well it looks very similar to that that first bug that we talked about when I as",
    "start": "1742710",
    "end": "1749970"
  },
  {
    "text": "a user go to create a pod in the base case I expect it to just go and create",
    "start": "1749970",
    "end": "1756179"
  },
  {
    "text": "right like happens normally pretty fast time if you have an ec2 instance ready and it's got available CPU and memory or",
    "start": "1756179",
    "end": "1762690"
  },
  {
    "text": "other resources that are on your your pops back like that that happens pretty fast so just for for measurements that",
    "start": "1762690",
    "end": "1769169"
  },
  {
    "text": "we took on this the base case of creating say five thousand pods across",
    "start": "1769169",
    "end": "1774330"
  },
  {
    "text": "the five hundred nodes p100 so timed till every one of those 500 pods was",
    "start": "1774330",
    "end": "1781080"
  },
  {
    "text": "actually created around ballpark five five minutes 14 seconds so that's",
    "start": "1781080",
    "end": "1786480"
  },
  {
    "text": "actually that yeah that's pretty good like if you're actually doing a like zero to two that roll out that that's pretty fast as we added this webhook",
    "start": "1786480",
    "end": "1794280"
  },
  {
    "text": "before we launched again this wasn't a fortunately not a customer impacting event but before we launch this as we",
    "start": "1794280",
    "end": "1801840"
  },
  {
    "text": "tried this out those same numbers five hundred five hundred nodes five thousand",
    "start": "1801840",
    "end": "1807090"
  },
  {
    "text": "pods as you the user create that in the API server what was happening is the API",
    "start": "1807090",
    "end": "1814380"
  },
  {
    "text": "server reaches out to this web hook hosted on the control plane the web hook was actually calling back to describe to",
    "start": "1814380",
    "end": "1822390"
  },
  {
    "text": "say okay hey I see the service account on the pod does that service account have the annotation or not and then",
    "start": "1822390",
    "end": "1827850"
  },
  {
    "text": "getting that back response back from the API server and then responding to the API servers request and saying hey here's your mutated pod or you know it",
    "start": "1827850",
    "end": "1834600"
  },
  {
    "text": "doesn't happen there's no annotation here's just the pod keep it going because of all this back-and-forth it",
    "start": "1834600",
    "end": "1842250"
  },
  {
    "text": "seems like it's just a serve account right like you're looking up just a string on a service account that shouldn't be that that long",
    "start": "1842250",
    "end": "1847980"
  },
  {
    "text": "the to create all those what 5,000 pods the the p99 was actually 46 minutes or",
    "start": "1847980",
    "end": "1856980"
  },
  {
    "text": "PP 100 so that's like what not nine hundred and",
    "start": "1856980",
    "end": "1862249"
  },
  {
    "text": "almost a thousand percent worse right like that that's unacceptable you wouldn't like all your even if your",
    "start": "1862249",
    "end": "1868619"
  },
  {
    "text": "pods weren't weren't using this Web book at all like that was still the case it was that slow and also the other like",
    "start": "1868619",
    "end": "1877169"
  },
  {
    "text": "worst part of this was only just over half of the pods were actually getting correctly mutated that needed to be so",
    "start": "1877169",
    "end": "1884359"
  },
  {
    "text": "when you're writing a controller you want it to not be slow right so what was",
    "start": "1884359",
    "end": "1892049"
  },
  {
    "text": "our fix for this well we actually ended up adding a cache so an our web hook as",
    "start": "1892049",
    "end": "1897210"
  },
  {
    "text": "it starts up it had this it basically requests it what are all the surface accounts in the cluster and then keeping",
    "start": "1897210",
    "end": "1904019"
  },
  {
    "text": "a map just in memory because it's just strings of surface accounts to annotations and then keeping an open",
    "start": "1904019",
    "end": "1910499"
  },
  {
    "text": "watch for updates deletions all that kinds of stuff so we could just update our local cache whenever that happens so",
    "start": "1910499",
    "end": "1917519"
  },
  {
    "text": "now when whenever you as a user go to create a pod or the the replica set or",
    "start": "1917519",
    "end": "1924299"
  },
  {
    "text": "Dana site controller whatever creates your pod it's just a single back and forth and it creates your pod and so",
    "start": "1924299",
    "end": "1931139"
  },
  {
    "text": "after adding this cache back there to the web hook RP 90 or P 100 was back",
    "start": "1931139",
    "end": "1936749"
  },
  {
    "text": "down to slightly maybe one second more it's almost indistinguishable for that",
    "start": "1936749",
    "end": "1943109"
  },
  {
    "text": "same same same set of number of nodes and pods so kind of the lessons learned here when you're writing like",
    "start": "1943109",
    "end": "1949679"
  },
  {
    "start": "1946000",
    "end": "2012000"
  },
  {
    "text": "interacting with kubernetes especially when you're writing like a some sort of mutating or validating admission web",
    "start": "1949679",
    "end": "1955289"
  },
  {
    "text": "hook like deep web hooks a stateless as possible and when you when you can't do that like you need some other data to",
    "start": "1955289",
    "end": "1961950"
  },
  {
    "text": "reason about do I make changes to this web hook or not like it's typically a good idea to add a cache if you use open",
    "start": "1961950",
    "end": "1969029"
  },
  {
    "text": "OPA open policy agent they actually it does this it has it doesn't actually request from the API server for every",
    "start": "1969029",
    "end": "1976019"
  },
  {
    "text": "resource that you're putting policy on it actually has its own sort of sidecar cache to check hey is this what's the",
    "start": "1976019",
    "end": "1983399"
  },
  {
    "text": "state of this object or this other object that I need a reference and then also one of the things is that",
    "start": "1983399",
    "end": "1989530"
  },
  {
    "text": "was another lesson learned was always be measuring your changes these these these",
    "start": "1989530",
    "end": "1995950"
  },
  {
    "text": "things are sort of subtle and like if you're not not paying attention like especially if you're not operating on large scales and you're like oh it just",
    "start": "1995950",
    "end": "2002310"
  },
  {
    "text": "works on this small you know three nodes three pods or whatever like you might not even notice the difference but but",
    "start": "2002310",
    "end": "2007800"
  },
  {
    "text": "keep measurements of what you're doing mm I actually think we're out of time",
    "start": "2007800",
    "end": "2013980"
  },
  {
    "start": "2012000",
    "end": "2111000"
  },
  {
    "text": "for questions but I want to say thank you all for coming I work with a lot of awesome people who",
    "start": "2013980",
    "end": "2020730"
  },
  {
    "text": "make eks awesome and make it happen and I'll be available all around at the",
    "start": "2020730",
    "end": "2025800"
  },
  {
    "text": "end for questions but I think we could take like two or three questions Jeffrey yeah all right what contain your",
    "start": "2025800",
    "end": "2038730"
  },
  {
    "text": "time a runtime platform you use to run ich es is it cryo is it Container G is a",
    "start": "2038730",
    "end": "2043770"
  },
  {
    "text": "doctor for the customer data plane yeah so in our Amazon managed eks ami ami",
    "start": "2043770",
    "end": "2051330"
  },
  {
    "text": "however you like to say it we use presently docker yep one more",
    "start": "2051330",
    "end": "2061970"
  },
  {
    "text": "I is there something you can say about",
    "start": "2066720",
    "end": "2072060"
  },
  {
    "text": "how you managed PKI for the master in the control plane yeah so we actually do",
    "start": "2072060",
    "end": "2077520"
  },
  {
    "text": "a separate we use cube atom keep it I'm sure about like if you have a separate",
    "start": "2077520",
    "end": "2083280"
  },
  {
    "text": "system that manages like the ki distribute certificates or no I mean yeah we we use in case atom for",
    "start": "2083280",
    "end": "2089819"
  },
  {
    "text": "generating in maintaining the certs so you your cluster has an individual like certificate authority that's unique to",
    "start": "2089819",
    "end": "2095280"
  },
  {
    "text": "it that's self signed and everything it's a lot easier to rotate things around that way yep great q alright",
    "start": "2095280",
    "end": "2106230"
  },
  {
    "text": "thanks everybody [Applause]",
    "start": "2106230",
    "end": "2113359"
  }
]