[
  {
    "text": "today we start our misadventure off unfortunately with a",
    "start": "440",
    "end": "6480"
  },
  {
    "text": "betrayal I trusted you kubernetes when you told me that this was the kubernetes",
    "start": "6480",
    "end": "13320"
  },
  {
    "text": "scalability numbers I took that on Blind Faith so much so that when I started",
    "start": "13320",
    "end": "19119"
  },
  {
    "text": "with the cloud provider that I work for and they were like who are we going to find dumb enough to take scalability",
    "start": "19119",
    "end": "24800"
  },
  {
    "text": "issues all day I looked that man in the eye and I told him I'm that dumb he",
    "start": "24800",
    "end": "30240"
  },
  {
    "text": "looked at me quickly agreed and that became my full-time job but quickly there was a problem I",
    "start": "30240",
    "end": "38399"
  },
  {
    "text": "would go out to customers as small as 5,000 nodes and I was experiencing the slowness sometimes even crashes",
    "start": "38399",
    "end": "45520"
  },
  {
    "text": "meanwhile on the other end of the spectrum I go out to customers had 6,000 nodes running just fine something was",
    "start": "45520",
    "end": "53120"
  },
  {
    "text": "wrong now I knew nodes wouldn't be the me best measure of scale cuz they come in all different sizes and usually it's",
    "start": "53120",
    "end": "60000"
  },
  {
    "text": "a mix but that was okay because pods pods will tell me what's the best but I",
    "start": "60000",
    "end": "66000"
  },
  {
    "text": "would have 10,000 pods that wouldn't talk to the control plane and be longlived and not to have 10,000 pods",
    "start": "66000",
    "end": "71799"
  },
  {
    "text": "that might only live for 15 minutes on a spark job and then another in another hour another 10,000 and that's where I",
    "start": "71799",
    "end": "77759"
  },
  {
    "text": "would get the crashes okay so wait a minute so nodes and pods don't correlate in any way to",
    "start": "77759",
    "end": "85560"
  },
  {
    "text": "if something's going to scale successfully this is a dis",
    "start": "85560",
    "end": "90720"
  },
  {
    "text": "disaster gang ever since I was a little Shane I had one",
    "start": "90720",
    "end": "96640"
  },
  {
    "text": "dream that was going to grow up one day and I was going to get paid money to nerd Flex on people but now that dream",
    "start": "96640",
    "end": "104560"
  },
  {
    "text": "is lying in tatters we've got to figure out how are we going to get to the",
    "start": "104560",
    "end": "109840"
  },
  {
    "text": "bottom of kubernetes scalability and with the stage set in the most ridiculous way possible we are",
    "start": "109840",
    "end": "117399"
  },
  {
    "text": "going to go all the way down the rabbit hole to understand the true nature of kubernetes scalability my late night",
    "start": "117399",
    "end": "123680"
  },
  {
    "text": "crowd are you ready to hit this really hard let me hear it all right it's the late night crowd",
    "start": "123680",
    "end": "131039"
  },
  {
    "text": "right there I love it all right gang I had let we start our journey off with the clue it's that spark job I have",
    "start": "131039",
    "end": "137680"
  },
  {
    "text": "10,000 pods I dumped them all on the scheduler that's when I was getting the slowness same number of PODS same number",
    "start": "137680",
    "end": "145879"
  },
  {
    "text": "same exact cluster I batch it out just a little bit over time runs is fine it's",
    "start": "145879",
    "end": "151640"
  },
  {
    "text": "almost like the rate of change that I'm pushing to the control plane at any",
    "start": "151640",
    "end": "156800"
  },
  {
    "text": "point in time seems to be somehow related to kubernetes scalability and that makes a lot of",
    "start": "156800",
    "end": "162840"
  },
  {
    "text": "sense if you look at the discrete components in kubernetes each one has its own unique mechanism that controls",
    "start": "162840",
    "end": "171159"
  },
  {
    "text": "that rate of change set that value too high we're going to crash the coupe controller manager we're going to lock",
    "start": "171159",
    "end": "177360"
  },
  {
    "text": "the edcd leave it at its default value however ever and we underutilize the hardware and cause the scale problems",
    "start": "177360",
    "end": "184040"
  },
  {
    "text": "that we're trying to prevent I don't know what you're thinking you're thinking Shane I'm going to go fix that on the API server and look I'm more",
    "start": "184040",
    "end": "190799"
  },
  {
    "text": "guilty of that than anyone I've written a ton about troubleshooting the API priority and fairness system if you're",
    "start": "190799",
    "end": "196599"
  },
  {
    "text": "interested in that you can go check that out here but for today's adventure if it's truly the rate of",
    "start": "196599",
    "end": "202640"
  },
  {
    "text": "change that we're pushing through the control plane at any given point in time would it make sense to take a look at",
    "start": "202640",
    "end": "210120"
  },
  {
    "text": "the thing that's making those changes the controllers the thing that's taking that declarative yaml and changing the",
    "start": "210120",
    "end": "216319"
  },
  {
    "text": "state of the world to match our declared intent the minute that we do that we",
    "start": "216319",
    "end": "222239"
  },
  {
    "text": "would probably go to the cube controller manager which all these controllers live in the control plane and we immediately see the problem that spark job is",
    "start": "222239",
    "end": "229720"
  },
  {
    "text": "completely saturated that job's controller but every other controller on the system is running just fine would",
    "start": "229720",
    "end": "237159"
  },
  {
    "text": "you say that kubernetes is slow no not really right but now we have to",
    "start": "237159",
    "end": "243360"
  },
  {
    "text": "shift our thinking we'd have to think in terms of how much volume were we sending to Any",
    "start": "243360",
    "end": "250439"
  },
  {
    "text": "Given controller in that fixed amount of time to understand this better we're",
    "start": "250439",
    "end": "257320"
  },
  {
    "text": "going to have to take a big step back and ask a basic question what does a",
    "start": "257320",
    "end": "264000"
  },
  {
    "text": "controller look like does it look like a a q you knew",
    "start": "264000",
    "end": "271919"
  },
  {
    "text": "exactly where I was going with that that's right it does look like a q gang we're looking at the work Q package",
    "start": "271919",
    "end": "278120"
  },
  {
    "text": "inside of the job controller but this could be inside of the coup uh controller manager or any controller",
    "start": "278120",
    "end": "284320"
  },
  {
    "text": "load balancer controller ET and what we're looking at here is in this work CU",
    "start": "284320",
    "end": "289560"
  },
  {
    "text": "we have these objects that the job controller is in charge of",
    "start": "289560",
    "end": "295639"
  },
  {
    "text": "reconciling so let's take a big step back how did we get here there is a pod",
    "start": "295639",
    "end": "300960"
  },
  {
    "text": "that is associated with this job it's created it's deleted it's going to",
    "start": "300960",
    "end": "307280"
  },
  {
    "text": "update that pod Informer or that pod cash that job controller is watching",
    "start": "307280",
    "end": "312800"
  },
  {
    "text": "that pod cash it knows it needs to reconcile something so it puts a new object in that",
    "start": "312800",
    "end": "318560"
  },
  {
    "text": "queue this reconciliation that that job controller is directly related to scale",
    "start": "318560",
    "end": "326199"
  },
  {
    "text": "if that's getting saturated we need to change the speed of that reconciliation and I'm going to teach you how to do",
    "start": "326199",
    "end": "331919"
  },
  {
    "text": "that right now gang we're back at that work Q package you know what that looks",
    "start": "331919",
    "end": "337199"
  },
  {
    "text": "like but we have two new Fields concurrency and QPS what this concurrency field is all",
    "start": "337199",
    "end": "343720"
  },
  {
    "text": "about is the goth threads that are doing the actual reconciliation so",
    "start": "343720",
    "end": "349600"
  },
  {
    "text": "basically by default the jobs controller only has five of these threads in a",
    "start": "349600",
    "end": "356000"
  },
  {
    "text": "larger cluster it wouldn't be unusual to have this as 100 threads in a really big cluster it wouldn't be crazy to put it",
    "start": "356000",
    "end": "362440"
  },
  {
    "text": "at 500 and because of this each of these controllers has a flow control mechanism",
    "start": "362440",
    "end": "368280"
  },
  {
    "text": "that I know you're familiar with from kuet and scheduler and everywhere else in the control plane this query per second and burst rate because we need to",
    "start": "368280",
    "end": "375360"
  },
  {
    "text": "make sure that we don't overflow the API and etcd server but what do you set",
    "start": "375360",
    "end": "381319"
  },
  {
    "text": "these values at let's take a look at that if I increase this queries per",
    "start": "381319",
    "end": "387199"
  },
  {
    "text": "second I have to be mindful that I do not exhaust this API priority and",
    "start": "387199",
    "end": "392319"
  },
  {
    "text": "fairness que or the flow and hand sizes that are in that API server but more",
    "start": "392319",
    "end": "398560"
  },
  {
    "text": "interestingly this concurrency rate if I was doing five rights to the ETD server",
    "start": "398560",
    "end": "405960"
  },
  {
    "text": "and I switch that concurrency value to 100 I now would need to make sure that I",
    "start": "405960",
    "end": "411960"
  },
  {
    "text": "have enough Headroom in that etcd or I will cause a disaster but how do we do that Shane I",
    "start": "411960",
    "end": "419120"
  },
  {
    "text": "don't even have access to the ET don't worry I got you so what we're going to look at is this client agent inside of",
    "start": "419120",
    "end": "425720"
  },
  {
    "text": "the API server that you do have access to and what we're going to look for is we're going to make sure that our crud",
    "start": "425720",
    "end": "431520"
  },
  {
    "text": "verbs are under 1 second and our cluster wide list are under 30",
    "start": "431520",
    "end": "437039"
  },
  {
    "text": "seconds okay we're going to hit our first chart and what we're looking for this is ETD request duration seconds and",
    "start": "437039",
    "end": "444960"
  },
  {
    "text": "notice that it's a P99 when we use a P99 we're going to use bucketed metrics that's going to be important later why",
    "start": "444960",
    "end": "451560"
  },
  {
    "text": "we use a P99 for all control plane stuff you don't want to say three API servers and average them together you don't want",
    "start": "451560",
    "end": "457599"
  },
  {
    "text": "to take three ETD servers and average them together now let's go look at these fields we are under 1 second for all the",
    "start": "457599",
    "end": "465240"
  },
  {
    "text": "crud verbs we are under well under 1 second for the cluster wide list do I have enough Headroom to start playing",
    "start": "465240",
    "end": "472080"
  },
  {
    "text": "with these tunables in the kubernetes control plane yes we do in fact would I",
    "start": "472080",
    "end": "478120"
  },
  {
    "text": "check these values if if I was tuning anything in the control plane yes I would ensuring that you have this",
    "start": "478120",
    "end": "484120"
  },
  {
    "text": "Headroom is critically important but do you have this problem",
    "start": "484120",
    "end": "489280"
  },
  {
    "text": "give me five minutes and I'll teach you how to detect this problem in the control plane but for some Theory okay I",
    "start": "489280",
    "end": "496960"
  },
  {
    "text": "have a a kubernetes client that decides I'm going to do a thousand jobs all at the same time but that QPS bir rate is",
    "start": "496960",
    "end": "504520"
  },
  {
    "text": "causing a bottleneck those objects are going to stack in the queue luckily we have a metrics to see if this condition",
    "start": "504520",
    "end": "510599"
  },
  {
    "text": "is happening and this is called work depth do you see that Spike with the",
    "start": "510599",
    "end": "515800"
  },
  {
    "text": "10,000 right there you putting up those kind of numbers you got my attention but with uh kubernetes control plane numbers",
    "start": "515800",
    "end": "523120"
  },
  {
    "text": "are bursty by Nature if in one metric period it's busy and the next minute it's not I'm not so worried about that",
    "start": "523120",
    "end": "529839"
  },
  {
    "text": "the etcd might defrag or something like that however if I see these spikes",
    "start": "529839",
    "end": "535680"
  },
  {
    "text": "happening a bunch of times in just a few hours I do have a problem and let's go into investigate that together what am I",
    "start": "535680",
    "end": "542120"
  },
  {
    "text": "looking for ladies and gentlemen if you remember nothing else latency is the enemy of scale so we're going to measure",
    "start": "542120",
    "end": "549279"
  },
  {
    "text": "the latency from the time that that object goes in the KCM to the time that it's processed low SC low latency equals",
    "start": "549279",
    "end": "558040"
  },
  {
    "text": "scale we're going to check this out with this new metric work Q or sorry Q",
    "start": "558040",
    "end": "563480"
  },
  {
    "text": "duration how long did it sit in that queue and what do we see at that top bucket multiple periods right at timed",
    "start": "563480",
    "end": "569800"
  },
  {
    "text": "out we need to go take a look at that let's do that now now this is can to happen for another reason a number of",
    "start": "569800",
    "end": "575880"
  },
  {
    "text": "reasons but let me talk about one of the more complex ones there might just be",
    "start": "575880",
    "end": "581079"
  },
  {
    "text": "some slowness in getting that one reconcile done maybe it's got a lot of stuff to reconcile maybe it's got errors",
    "start": "581079",
    "end": "587279"
  },
  {
    "text": "we'll talk about some of the reasons why this happens but what I want you to focus on with me for a second is the",
    "start": "587279",
    "end": "592680"
  },
  {
    "text": "impact that this is going to have that go thread gets locked up for a",
    "start": "592680",
    "end": "598680"
  },
  {
    "text": "period of time I only have four left I've just reduced the scale and performance of that",
    "start": "598680",
    "end": "605399"
  },
  {
    "text": "controller by what like 20% that's a big deal plus I'm holding open the seats on",
    "start": "605399",
    "end": "611279"
  },
  {
    "text": "the API server and this is also having an impact on the etcd how am I going to see if this is",
    "start": "611279",
    "end": "618360"
  },
  {
    "text": "happening this is the last metric on KCM I have for you work duration seconds how",
    "start": "618360",
    "end": "624760"
  },
  {
    "text": "long did that unit of work take to reconcile when I see this",
    "start": "624760",
    "end": "629920"
  },
  {
    "text": "at the top for hours on in do I have a problem you bet I do does this mean that",
    "start": "629920",
    "end": "637120"
  },
  {
    "text": "these requests are timing out no because we've been betrayed yet again this time",
    "start": "637120",
    "end": "644920"
  },
  {
    "text": "bu a metrics bucket how on Earth do you even get betrayed by a bucket let me",
    "start": "644920",
    "end": "650120"
  },
  {
    "text": "tell you all right so bucketed metrics are really expensive so typically we",
    "start": "650120",
    "end": "655440"
  },
  {
    "text": "only do 10 of those at a given time for whatever reason though they started",
    "start": "655440",
    "end": "660519"
  },
  {
    "text": "those buckets at 10 seconds and exponentially went down to the Nano second level which is great if you're",
    "start": "660519",
    "end": "667440"
  },
  {
    "text": "troubleshooting Quantum uncertainty but for what we need it's practically useless because we need to go into the",
    "start": "667440",
    "end": "673320"
  },
  {
    "text": "other direction okay how are we going to get around this we're going to use an average right but wait a minute Shane",
    "start": "673320",
    "end": "680079"
  },
  {
    "text": "didn't you just tell me not to use an average yeah but I told you this was a story of betrayal right so we get rid we",
    "start": "680079",
    "end": "686079"
  },
  {
    "text": "get rid of those buckets and what do we find that wasn't 10 seconds that was 4",
    "start": "686079",
    "end": "692320"
  },
  {
    "text": "minutes was this wrong in kubernetes the whole time yes me and my buddy yens are",
    "start": "692320",
    "end": "697480"
  },
  {
    "text": "trying to get this fixed right now but the point I want to make is this we use metrics to find what we want to look at",
    "start": "697480",
    "end": "703920"
  },
  {
    "text": "and the time frame and then we move to the logs and I'll cover this in a little bit more detail in a",
    "start": "703920",
    "end": "709800"
  },
  {
    "text": "second okay gang I what I'm going to show you is the output from a bunch of queries don't worry I'm going to provide",
    "start": "709800",
    "end": "716720"
  },
  {
    "text": "you with 25 different queries all with explanations that we use globally around the world every day to identify these uh",
    "start": "716720",
    "end": "724079"
  },
  {
    "text": "you know drunk dialing style uh workloads I'm also going to provide you with uh um a guide that basically goes",
    "start": "724079",
    "end": "732000"
  },
  {
    "text": "through everything that I wrote uh that covers a little bit more than I have time for today but back to our story",
    "start": "732000",
    "end": "737720"
  },
  {
    "text": "what I'm going to do is that duration in seconds on the end that thing that's taking the longest I'm going to sort by",
    "start": "737720",
    "end": "743120"
  },
  {
    "text": "that duration of second I'm going to look for the agent that villain in my story that's causing that slowness on",
    "start": "743120",
    "end": "748680"
  },
  {
    "text": "that control and once I find that I'll have the URI to see what it's doing what's it doing it's requesting all",
    "start": "748680",
    "end": "754959"
  },
  {
    "text": "50,000 Pods unpaginated at the same time that's my villain but we're not going to",
    "start": "754959",
    "end": "760000"
  },
  {
    "text": "stop there because it once we find the agent we're going to go into the individual request and let me tell you",
    "start": "760000",
    "end": "766720"
  },
  {
    "text": "why there are some really awesome annotations that will tell you where the latency is at if we look up here at the",
    "start": "766720",
    "end": "773560"
  },
  {
    "text": "top and it's been expanded in 131 but let me give you a brief tour it will give you the ETD if ETD slow don't",
    "start": "773560",
    "end": "780120"
  },
  {
    "text": "troubleshoot anything else that's the villain this slow response right what",
    "start": "780120",
    "end": "785279"
  },
  {
    "text": "that is is if the client is slow to respond back to the API server what's some common reasons for this um really",
    "start": "785279",
    "end": "792680"
  },
  {
    "text": "aggressive CPU limits that workload uh node is out of bandwidth things of that",
    "start": "792680",
    "end": "797959"
  },
  {
    "text": "nature but the villain in our story today is the serialization time what",
    "start": "797959",
    "end": "803440"
  },
  {
    "text": "even is that that Json of 50,000 pods serializing to go over the network",
    "start": "803440",
    "end": "809920"
  },
  {
    "text": "34 seconds that's over half of the API server would I have found that in a",
    "start": "809920",
    "end": "815680"
  },
  {
    "text": "million years had I not went to this level in the logs I don't know about a million years but I spent three weeks my",
    "start": "815680",
    "end": "822079"
  },
  {
    "text": "mentors came in and they spent 20 minutes triggering out if you want to be on the pro level gang these guys they",
    "start": "822079",
    "end": "828680"
  },
  {
    "text": "spend 80 to 90% of their time in logs for everything they troubleshoot and I recommend you do the same now to give",
    "start": "828680",
    "end": "836800"
  },
  {
    "text": "credit where credits du um the most of the research that I'm going to share",
    "start": "836800",
    "end": "841880"
  },
  {
    "text": "with you today probably comes from one of these four gentlemen I believe one or two of them are here today and so they",
    "start": "841880",
    "end": "848120"
  },
  {
    "text": "don't beat me up in the parking lot for stealing the research I want to give credit where credits do but now that",
    "start": "848120",
    "end": "853680"
  },
  {
    "text": "we've got my safety out of the way I want to take everything and put this together for you gang if I am under 1",
    "start": "853680",
    "end": "862040"
  },
  {
    "text": "second of latency to the API and etcd server the number of objects that those",
    "start": "862040",
    "end": "868040"
  },
  {
    "text": "controllers can reconcile will be a big number let's call that a thousand",
    "start": "868040",
    "end": "873160"
  },
  {
    "text": "however if that number if that latency is elevated the number of objects that",
    "start": "873160",
    "end": "879199"
  },
  {
    "text": "that controller can reconcile will go down and go down dramatically so for every rung I go up on this latency",
    "start": "879199",
    "end": "886759"
  },
  {
    "text": "ladder that number of objects that I can reconcile drops dramatically what does this mean in practice right when you",
    "start": "886759",
    "end": "894720"
  },
  {
    "text": "need that performance the most that latency spikes and the performance and",
    "start": "894720",
    "end": "900480"
  },
  {
    "text": "scalability that you had just moment be moments before now just slip through our",
    "start": "900480",
    "end": "906959"
  },
  {
    "text": "fingers gang if you've ever had where you put some kind of workload on and the thing just struggles to reconcile this",
    "start": "906959",
    "end": "914639"
  },
  {
    "text": "could be the villain in your story but what makes this latency go so high a few",
    "start": "914639",
    "end": "921560"
  },
  {
    "text": "different reasons but one of the biggest is these clients that are talking to the control plane Sometimes some things are",
    "start": "921560",
    "end": "928639"
  },
  {
    "text": "just going going crazy on the control plane other times it's something legitimate a getop system it needs to",
    "start": "928639",
    "end": "934600"
  },
  {
    "text": "know the status of all 50,000 PS your metric system it can't cash that information the metrics have to be new",
    "start": "934600",
    "end": "940880"
  },
  {
    "text": "each time if that stuff starts coming in too fast here's what happens bam I make",
    "start": "940880",
    "end": "946839"
  },
  {
    "text": "a request I've got to send all 50,000 pods back it takes a long time to serialize I have to hold that response",
    "start": "946839",
    "end": "952720"
  },
  {
    "text": "in memory the next thing comes in all of a sudden you see where this is going my memory starts to exhaust my networking",
    "start": "952720",
    "end": "959639"
  },
  {
    "text": "starts to exhaust my etcd latency time due to some complicated B tree read",
    "start": "959639",
    "end": "965160"
  },
  {
    "text": "stuff also elevates and this is the villains in the story gang if you look",
    "start": "965160",
    "end": "971480"
  },
  {
    "text": "at this list of villains you'll notice much of the things that we talked about happen client side and we talked about",
    "start": "971480",
    "end": "979160"
  },
  {
    "text": "these it if it's serialization delay maybe it's a bad web hook just something drunk dialing the API server if you have",
    "start": "979160",
    "end": "986880"
  },
  {
    "text": "these going on in your network does it make sense to touch any of these tunables in the control plane most",
    "start": "986880",
    "end": "994120"
  },
  {
    "text": "adamantly I would tell you no gang but I'm only telling you half",
    "start": "994120",
    "end": "1002839"
  },
  {
    "text": "the story for the other half of the story we'll have to find someone that",
    "start": "1002839",
    "end": "1008560"
  },
  {
    "text": "pushes their workloads and clusters in that same loving and gentle Marine Corps",
    "start": "1008560",
    "end": "1014120"
  },
  {
    "text": "fashion that I do and if we want to find out what",
    "start": "1014120",
    "end": "1019480"
  },
  {
    "text": "kubernetes major malfunction is we're going to have to talk to my buddy",
    "start": "1019480",
    "end": "1025959"
  },
  {
    "text": "tea complicated boring stuff right like listen I got a simple fix for all your",
    "start": "1035439",
    "end": "1040558"
  },
  {
    "text": "cluster scalability problems you run a custom coup scheduler you set the strategy to most delicated and your",
    "start": "1040559",
    "end": "1046280"
  },
  {
    "text": "cluster is going to be reduced in half that's it we're done okay Jokes Aside we",
    "start": "1046280",
    "end": "1051720"
  },
  {
    "text": "had a plan to optimize the workload that involved custom Coupe scheduler but first let me introduce you to the",
    "start": "1051720",
    "end": "1057520"
  },
  {
    "text": "workload itself that spark chains interest the workload consists of spark jobs that run in the cluster mode which",
    "start": "1057520",
    "end": "1064880"
  },
  {
    "text": "is a bunch of driver pods faning out the work to executors and the driver pods are created by kubernetes jobs we were",
    "start": "1064880",
    "end": "1071559"
  },
  {
    "text": "creating about 2 thousands of them a second in the largest cluster and as you can imagine the",
    "start": "1071559",
    "end": "1078200"
  },
  {
    "text": "control plane was not really happy about it uh we were setting new records for the Qs but at first glance you would",
    "start": "1078200",
    "end": "1086280"
  },
  {
    "text": "think that like we were doing this to ourselves right creating so many jobs so quickly like what did we",
    "start": "1086280",
    "end": "1092720"
  },
  {
    "text": "expect but it turns out that it's totally possible to scale kubernetes for",
    "start": "1092720",
    "end": "1097760"
  },
  {
    "text": "workloads like ours we just had to really look at how workload uses kubernetes what's going on",
    "start": "1097760",
    "end": "1104600"
  },
  {
    "text": "in the data plane and along the way resolve a number of scalability bottlenecks",
    "start": "1104600",
    "end": "1110039"
  },
  {
    "text": "the first one of which was seemingly related to The Container storage interface in the early iteration of this",
    "start": "1110039",
    "end": "1115480"
  },
  {
    "text": "workload we had a persistent volume mounted to some of the executive pods which was used by spark for Scratch",
    "start": "1115480",
    "end": "1122120"
  },
  {
    "text": "space and over time we noticed that a number of pending pods were building up in the cluster and they were waiting for",
    "start": "1122120",
    "end": "1128520"
  },
  {
    "text": "these volumes so we were really curious why and the answer is always in the logs so",
    "start": "1128520",
    "end": "1135440"
  },
  {
    "text": "the container storage interface driver was timing out waiting for some external attacher what is that it turns out",
    "start": "1135440",
    "end": "1142880"
  },
  {
    "text": "that's a storage system that is external to the cluster that was used for these persistent volumes and it simply",
    "start": "1142880",
    "end": "1148320"
  },
  {
    "text": "couldn't keep up with the rate of change how many volumes we tried to create and delete but how this impacted a workload",
    "start": "1148320",
    "end": "1155080"
  },
  {
    "text": "in order to estimate that we had to get a bit creative we wanted to measure a",
    "start": "1155080",
    "end": "1161480"
  },
  {
    "text": "delay of the persistent volumes provisioning and we did that by subtracting the volume creation time",
    "start": "1161480",
    "end": "1168320"
  },
  {
    "text": "stamp from from the volume attachment time stamp and what we saw is that both the delay and the number of the delayed",
    "start": "1168320",
    "end": "1175240"
  },
  {
    "text": "volumes were trending up and to the right was the pods that were pending following the same Trend and our",
    "start": "1175240",
    "end": "1180760"
  },
  {
    "text": "workload was approaching breakage of some of the slos but at the same time the control",
    "start": "1180760",
    "end": "1186799"
  },
  {
    "text": "plane was also impacted negatively kuet was trying to reconcile all these spending objects and some of",
    "start": "1186799",
    "end": "1193120"
  },
  {
    "text": "them it couldn't creating a steady stream of API retries and filling up the controller",
    "start": "1193120",
    "end": "1198280"
  },
  {
    "text": "cues but kuet and the control plane are not the villains here like if if you really",
    "start": "1198280",
    "end": "1203720"
  },
  {
    "text": "think about this at this time our scalability is limited by that external storage system that's not even part of the",
    "start": "1203720",
    "end": "1209960"
  },
  {
    "text": "cluster something had to be done about this obviously so as you might know the best solution to a problem is not to",
    "start": "1209960",
    "end": "1215840"
  },
  {
    "text": "have the problem in the first place and that's what we did we got rid of the container storage interface and the",
    "start": "1215840",
    "end": "1221320"
  },
  {
    "text": "external storage and all that stuff made it all simple right keep it simple as principles and the persistent volumes",
    "start": "1221320",
    "end": "1227360"
  },
  {
    "text": "replace them with empty door volume on the Node file system and it worked",
    "start": "1227360",
    "end": "1233039"
  },
  {
    "text": "for a while we on boarded some additional jobs and on one the time but",
    "start": "1233039",
    "end": "1238440"
  },
  {
    "text": "then we found a new problem suddenly we were seeing that in the cluster there was a number of terminating pods that",
    "start": "1238440",
    "end": "1245120"
  },
  {
    "text": "were just hanging around like for days for no good reason it seems like and inherently or intuitively you would",
    "start": "1245120",
    "end": "1252039"
  },
  {
    "text": "think that when something like that happens it's your control plane state that is not being updated by kuid which",
    "start": "1252039",
    "end": "1258240"
  },
  {
    "text": "seemed to be true like the duration of the the work you duration for the job controller was at its peak but keep a",
    "start": "1258240",
    "end": "1265559"
  },
  {
    "text": "mental note of this graph because that 10c number at the top or 9.84 is pretty",
    "start": "1265559",
    "end": "1271159"
  },
  {
    "text": "important we'll get back to this at the but for the moment uh we didn't really know if this",
    "start": "1271159",
    "end": "1276919"
  },
  {
    "text": "was the source or the consequence because the other side of",
    "start": "1276919",
    "end": "1283000"
  },
  {
    "text": "the equation was kuet itself in kuet logs we found this error that we doed error 32",
    "start": "1283000",
    "end": "1289120"
  },
  {
    "text": "which basically told us that kuet was failing to unmount some simple projected volume for kubernetes API access token",
    "start": "1289120",
    "end": "1296799"
  },
  {
    "text": "but what's worse is that this error repeated itself for hours sometimes for days which seemed to match what we were",
    "start": "1296799",
    "end": "1303120"
  },
  {
    "text": "seeing with terminating pots so could this be the problem but it turns out those terminating pods are actually just a",
    "start": "1303120",
    "end": "1309279"
  },
  {
    "text": "canary in the coal mine the real problem was that we were",
    "start": "1309279",
    "end": "1314640"
  },
  {
    "text": "back to square one in terms of the effect on the control plane once again kuet was trying to reconcile",
    "start": "1314640",
    "end": "1321400"
  },
  {
    "text": "the objects that it couldn't reconcile because of that error 32 and given how frequently the error happened across the",
    "start": "1321400",
    "end": "1327120"
  },
  {
    "text": "fleet which was in the thousands per hour arguably the effect in the control plane was even worse than",
    "start": "1327120",
    "end": "1333600"
  },
  {
    "text": "before and in times like this you might be tempted to just blame kuet right it's at the centerpiece and that seems to be",
    "start": "1333600",
    "end": "1340679"
  },
  {
    "text": "the the villain and we did kind of cuz it started to look like kuet bug like",
    "start": "1340679",
    "end": "1346720"
  },
  {
    "text": "what else could it be either that or the storage is so overloaded or some some issue exists there that the simple",
    "start": "1346720",
    "end": "1353120"
  },
  {
    "text": "projected volume on Mount operation for some reason would fail between hunting for kuet bugs and really not the storage",
    "start": "1353120",
    "end": "1359760"
  },
  {
    "text": "we chose the simple pass and we decided to investigate the storage first and we were glad that we did because in the",
    "start": "1359760",
    "end": "1366480"
  },
  {
    "text": "cluster a lot of nodes were actually spiking at 100% on node time spent on",
    "start": "1366480",
    "end": "1371679"
  },
  {
    "text": "time spent do in IO which matched the behavior of era 32",
    "start": "1371679",
    "end": "1376720"
  },
  {
    "text": "which also happened only on some of the so we thought to ourselves that maybe resolving the source of this excessive",
    "start": "1376720",
    "end": "1382799"
  },
  {
    "text": "IO would lead us to the resolution of error 32 itself but that proved to be a little",
    "start": "1382799",
    "end": "1388799"
  },
  {
    "text": "trickier than we anticipated when we looked at the workload from the Pod pods perspective and we summarized everything",
    "start": "1388799",
    "end": "1395559"
  },
  {
    "text": "that was reported by the pods in a given node in terms of IO that didn't match the node what was reported by the node",
    "start": "1395559",
    "end": "1402000"
  },
  {
    "text": "itself we were like couple thousand iops short that were just somewhere hiding it",
    "start": "1402000",
    "end": "1407080"
  },
  {
    "text": "turns out that the Colonel right back process was responsible for those missing IO",
    "start": "1407080",
    "end": "1413559"
  },
  {
    "text": "operations in turn the fluent bit pod the demon set that was running on all these nods was aggressively buffering",
    "start": "1413559",
    "end": "1420039"
  },
  {
    "text": "logs in the memory mapped files but the iio that was generated from those memory",
    "start": "1420039",
    "end": "1425080"
  },
  {
    "text": "from the ride back from those memory map files wasn't attributed to the part itself it was attributed to the root C group of the node which is why we",
    "start": "1425080",
    "end": "1431520"
  },
  {
    "text": "couldn't see it but then changing the single setting in fluent bit reduced the",
    "start": "1431520",
    "end": "1436760"
  },
  {
    "text": "time spent doing IO about in half half across the fleet being aware now that the local",
    "start": "1436760",
    "end": "1442320"
  },
  {
    "text": "storage could impact our scalability we decided to implement some additional fixes and improvements and we knew that",
    "start": "1442320",
    "end": "1447600"
  },
  {
    "text": "storage wasn't the problem but the error persisted in fact the situation was",
    "start": "1447600",
    "end": "1454279"
  },
  {
    "text": "looking even worse in the control plane we knew that 84,000 items in the worku",
    "start": "1454279",
    "end": "1460240"
  },
  {
    "text": "deps for the job is bad but how bad is it like could we quantify the impact on",
    "start": "1460240",
    "end": "1467360"
  },
  {
    "text": "actual workload when stuff like this when you see in the q's this large what it typically means",
    "start": "1467360",
    "end": "1474840"
  },
  {
    "text": "is that the whatever objects that controller is reconciling that reconciliation is delayed in this case",
    "start": "1474840",
    "end": "1481039"
  },
  {
    "text": "it's a job controller so we decided to measure the delay and we subtracted the job creation time stamp from the",
    "start": "1481039",
    "end": "1487279"
  },
  {
    "text": "creation time stamp of the driver pod that was spawned by that job and this is the call back to that",
    "start": "1487279",
    "end": "1494480"
  },
  {
    "text": "graph remember there was saying telling us that hey your reconcil takes 10 seconds only well it turns out that",
    "start": "1494480",
    "end": "1501120"
  },
  {
    "text": "across all those like multiple thousands of jobs that we created in the cluster every second the delay could be as high",
    "start": "1501120",
    "end": "1507440"
  },
  {
    "text": "as 2 minutes and for us this was a big deal because 10 seconds we didn't care",
    "start": "1507440",
    "end": "1513159"
  },
  {
    "text": "but two minutes could really affect our slos which were which were tied to data",
    "start": "1513159",
    "end": "1519159"
  },
  {
    "text": "processing latency and so we had to abandon everything else and really Zone into finding really finding the source",
    "start": "1519159",
    "end": "1525480"
  },
  {
    "text": "of error 32 but we were also coming short on ideas like it was happening only in some nodes",
    "start": "1525480",
    "end": "1532799"
  },
  {
    "text": "we didn't find any bugs any reports about a kuet issue that was widespread so we thought to ourselves well it must",
    "start": "1532799",
    "end": "1540679"
  },
  {
    "text": "be happening under some conditions right and so if we find what those conditions are we could potentially find the source",
    "start": "1540679",
    "end": "1547600"
  },
  {
    "text": "of error 32 itself but then we got a bit lucky and it turns out that just like in the good",
    "start": "1547600",
    "end": "1553960"
  },
  {
    "text": "old days of Web 2.0 when everything was a DNS problem in kubernetes everything the demon set",
    "start": "1553960",
    "end": "1559799"
  },
  {
    "text": "problem so the left side of the left part of the slide looks familiar to you it's our good old friend kuet flooding",
    "start": "1559799",
    "end": "1566039"
  },
  {
    "text": "the control plane with error 32 and reconciliation but the right side is",
    "start": "1566039",
    "end": "1571120"
  },
  {
    "text": "what actually caused all this it turns out the eror 32 was caused by a combination of the demon set agent and a",
    "start": "1571120",
    "end": "1577200"
  },
  {
    "text": "kernel bug the demon set agent used fa notify kernel API to monitor file system",
    "start": "1577200",
    "end": "1583200"
  },
  {
    "text": "event changes the fa notify itself in the kernel had a L that sometimes held",
    "start": "1583200",
    "end": "1589799"
  },
  {
    "text": "the monitored files open and together those two things prevented kuet from deleting those projected volume",
    "start": "1589799",
    "end": "1596640"
  },
  {
    "text": "directories and thus resulted in error 32 as it's frequently the case in this",
    "start": "1596640",
    "end": "1601760"
  },
  {
    "text": "situation the fix is super simple it's on line config change that resolved all these errors the error 32 went away and",
    "start": "1601760",
    "end": "1608840"
  },
  {
    "text": "the delay dropped like 10 seconds at Peak from 2 minutes which was great but",
    "start": "1608840",
    "end": "1614279"
  },
  {
    "text": "just to reiterate all this time our skill a ability was not limited by the",
    "start": "1614279",
    "end": "1619960"
  },
  {
    "text": "control plane itself right it was limited by the data plane components",
    "start": "1619960",
    "end": "1625640"
  },
  {
    "text": "that are not even part of our workload strictly speaking right that actually how the way they used the control",
    "start": "1625640",
    "end": "1633240"
  },
  {
    "text": "plane it took us six months to find all this stuff to track down all these errors to resolve the bottlenecks and to",
    "start": "1633480",
    "end": "1640279"
  },
  {
    "text": "finally start onboarding jobs again it took a ton of uh gentle Marine Corps style encouragement from my friend Shane",
    "start": "1640279",
    "end": "1647720"
  },
  {
    "text": "and a team of smart Engineers particularly these fellas who uh entertained my crazy ideas and uh",
    "start": "1647720",
    "end": "1654440"
  },
  {
    "text": "supported roll out of sketch and production changes we're not going to talk about that with kubernetes skillability out of",
    "start": "1654440",
    "end": "1661200"
  },
  {
    "text": "the way it was finally time for our 50% cluster reduction trick which involved",
    "start": "1661200",
    "end": "1666360"
  },
  {
    "text": "the coupe scheduler but why how well the default scheduler uses the least",
    "start": "1666360",
    "end": "1671919"
  },
  {
    "text": "allocated strategy which requires bin packing at runtime in order to maximize the compute",
    "start": "1671919",
    "end": "1679039"
  },
  {
    "text": "utilization which leads to the interruptions to the workload for the spark jobs we wanted to minimize the",
    "start": "1679039",
    "end": "1685080"
  },
  {
    "text": "interruptions and yet still maximize the computer utilization which is exactly what the Max uh the custom CP scheduler",
    "start": "1685080",
    "end": "1692159"
  },
  {
    "text": "allowed us to do as it's performed the bin pack in its schedule time and the results were honestly even",
    "start": "1692159",
    "end": "1698600"
  },
  {
    "text": "better than we expected just from this change alone we were able to regular scale down the compute and yielded us",
    "start": "1698600",
    "end": "1704080"
  },
  {
    "text": "about 20% of savings but there was one flaw in this near",
    "start": "1704080",
    "end": "1710000"
  },
  {
    "text": "Perfection the roll out of custom Coupe schuer had a negative side effect and our work our workload was",
    "start": "1710000",
    "end": "1718640"
  },
  {
    "text": "suddenly running in a single zone single zone not being designed to",
    "start": "1718640",
    "end": "1724120"
  },
  {
    "text": "host the entire workload obviously we ran out on a bunch of scalability restrictions once again but it turns out kubernetes was actually doing exactly",
    "start": "1724120",
    "end": "1730200"
  },
  {
    "text": "what we told it to do we set up the pot Affinity rules between the drivers and the executiv hosting them in the same",
    "start": "1730200",
    "end": "1736919"
  },
  {
    "text": "Zone but then after scheduling those all those pods there was inevitably some capacity left and the most alligated",
    "start": "1736919",
    "end": "1743720"
  },
  {
    "text": "scheduler fit the next driver into that capacity in the same Zone the executors for that driver followed and after a few",
    "start": "1743720",
    "end": "1749840"
  },
  {
    "text": "Cycles the entire workload was just suddenly running in the same Zone this was perhaps surprising because",
    "start": "1749840",
    "end": "1755880"
  },
  {
    "text": "inherently we expected kubernetes or CP schedular to have some sort of mechanism",
    "start": "1755880",
    "end": "1761080"
  },
  {
    "text": "to prevent these events from happening right but it turns out the defaults as far as the topology spread constraints",
    "start": "1761080",
    "end": "1766200"
  },
  {
    "text": "are merely suggestions which works at large it's just not a good idea for our",
    "start": "1766200",
    "end": "1771240"
  },
  {
    "text": "workload and so to restore order we had to deploy a more strict pot apology",
    "start": "1771240",
    "end": "1776399"
  },
  {
    "text": "spread constraints which uh prevented the scheduling of the pods unless they could be satisfied and minimize the skew",
    "start": "1776399",
    "end": "1784760"
  },
  {
    "text": "of the drivers between the zones and thus the entire workload at this point in time you might",
    "start": "1784760",
    "end": "1790600"
  },
  {
    "text": "be wondering like dude what are you talking about like all this stuff where's my magic trick where's the 50% cluster reduction you're a charlatan",
    "start": "1790600",
    "end": "1796600"
  },
  {
    "text": "right I couldn't possibly lie to you but at the same time I'm not telling you the",
    "start": "1796600",
    "end": "1802679"
  },
  {
    "text": "whole truth either you see scaling kubernetes is not about some magic",
    "start": "1802679",
    "end": "1807760"
  },
  {
    "text": "tricks it's true that over this period of time we reduced the compute by about",
    "start": "1807760",
    "end": "1812919"
  },
  {
    "text": "three times while the number of jobs increased by about five times but we didn't have to do anything to the",
    "start": "1812919",
    "end": "1818840"
  },
  {
    "text": "control plane at all no tunables no concurrency increas is nothing all this",
    "start": "1818840",
    "end": "1824600"
  },
  {
    "text": "time our scalability was limited by the way the data plane components use the control plane or even like external",
    "start": "1824600",
    "end": "1830799"
  },
  {
    "text": "systems to the cluster the storage system right with their own scalability limits that only once we resolved all",
    "start": "1830799",
    "end": "1837559"
  },
  {
    "text": "those bottlenecks we were able to deploy the optimizations to the workload itself",
    "start": "1837559",
    "end": "1843080"
  },
  {
    "text": "which finally yielded the dramatic reduction in size in the Clusters and that is the other part of",
    "start": "1843080",
    "end": "1849559"
  },
  {
    "text": "the kubernetes scalability story your workload might have such a dramatic effect on your cluster scalability that",
    "start": "1849559",
    "end": "1856480"
  },
  {
    "text": "by the time you're done fixing it you might have already reached your goals without even touching the control",
    "start": "1856480",
    "end": "1864480"
  },
  {
    "text": "plane gang were we betrayed by kubernetes not really but that would",
    "start": "1865320",
    "end": "1871720"
  },
  {
    "text": "have made for a terrible opening gang those numbers that I shared with you in the beginning are the kubernetes",
    "start": "1871720",
    "end": "1878279"
  },
  {
    "text": "scalability regression testing numbers that they use between versions they're not real workloads they were never",
    "start": "1878279",
    "end": "1884480"
  },
  {
    "text": "intended as any type of scale guide but that's okay you don't need any of that",
    "start": "1884480",
    "end": "1889919"
  },
  {
    "text": "anymore because we provided you with the same queries that we use every day around the world to find these",
    "start": "1889919",
    "end": "1896679"
  },
  {
    "text": "villainous workloads that are robbing you of your control plane scalability and once you have that under control we",
    "start": "1896679",
    "end": "1903679"
  },
  {
    "text": "provided you these guide books to help you understand the metrics and know how",
    "start": "1903679",
    "end": "1908960"
  },
  {
    "text": "to set these control parameters inside of the control planes without guessing and",
    "start": "1908960",
    "end": "1914320"
  },
  {
    "text": "safely gang but if we want to get on the Dema level not to let us get too big but if we want to get on the Dema level of",
    "start": "1914320",
    "end": "1920639"
  },
  {
    "text": "this optimization stuff we need to focus on the workloads and I know that sounds",
    "start": "1920639",
    "end": "1926799"
  },
  {
    "text": "like a just fix the workloads draw the rest of the owl problem but don't worry I provided you two guides that we use",
    "start": "1926799",
    "end": "1933159"
  },
  {
    "text": "every single day right that I wrote for you that normally we'll do like maybe a third of the size of the Clusters with",
    "start": "1933159",
    "end": "1939799"
  },
  {
    "text": "these engagements that's yours now and don't worry they're in the slides and then if you're interested in going all the way down the rabbit hole on the",
    "start": "1939799",
    "end": "1945720"
  },
  {
    "text": "optimization side on the workload don't forget our misadventure Series in Detroit if you've we hope that we",
    "start": "1945720",
    "end": "1952440"
  },
  {
    "text": "provided you with all the information that you need to find the scalability",
    "start": "1952440",
    "end": "1957840"
  },
  {
    "text": "villain in your story gang if you find this stuff helpful do me a quick favor before you go these uh survey numbers",
    "start": "1957840",
    "end": "1965360"
  },
  {
    "text": "are the difference if I'm allowed to take two months out of my year to put all this stuff together for you or if",
    "start": "1965360",
    "end": "1971519"
  },
  {
    "text": "I'm helping you with your next laptop purchase at Best Buy they're pretty important and a little bit of love goes",
    "start": "1971519",
    "end": "1977159"
  },
  {
    "text": "a long way gang it's our it's dimma and I's",
    "start": "1977159",
    "end": "1982720"
  },
  {
    "text": "greatest hope that when you walk into work on Monday you're going to look",
    "start": "1982720",
    "end": "1988159"
  },
  {
    "text": "those people in the eye and you're going to tell them that all of this research is really your research and you're going",
    "start": "1988159",
    "end": "1994279"
  },
  {
    "text": "to nerd Flex on anyone that talks to you about kubernetes scalability gang thanks",
    "start": "1994279",
    "end": "2000240"
  },
  {
    "text": "for making my dream come true have a wonderful rest of your conference",
    "start": "2000240",
    "end": "2006559"
  }
]