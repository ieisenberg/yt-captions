[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "laughter noon everyone thank you for coming to the last session of the conference making it thank you so much",
    "start": "390",
    "end": "6509"
  },
  {
    "text": "my name is Vinod Shekar Sheikh and I would like to introduce the co-presenter",
    "start": "6509",
    "end": "11730"
  },
  {
    "text": "in elementary we both work at Cisco we are part of the Cisco container platform",
    "start": "11730",
    "end": "17910"
  },
  {
    "text": "team and we also enable to flow on our platform and many of our customers our",
    "start": "17910",
    "end": "23880"
  },
  {
    "text": "machine learning customers and they have started looking at distributed training so what we are going to do is share our",
    "start": "23880",
    "end": "31470"
  },
  {
    "text": "learnings as we are helping customers through their journey so the agenda for",
    "start": "31470",
    "end": "37710"
  },
  {
    "text": "today's session is as follows I'm going to start by describing the use case which is to compress end-to-end machine",
    "start": "37710",
    "end": "45960"
  },
  {
    "start": "38000",
    "end": "38000"
  },
  {
    "text": "running time and this is all the way starting from the exploration phase right into the production phase and then",
    "start": "45960",
    "end": "54000"
  },
  {
    "text": "we talked about how queue flow makes that very simple and then finally we",
    "start": "54000",
    "end": "61140"
  },
  {
    "text": "will walk through an example of increasing the performance use of your training using distributed distributed",
    "start": "61140",
    "end": "68130"
  },
  {
    "text": "training and hyper parameter tuning and then look at the infrastructure and the",
    "start": "68130",
    "end": "74909"
  },
  {
    "text": "OS optimization to make your training faster and finally provide a summary so",
    "start": "74909",
    "end": "82710"
  },
  {
    "start": "82000",
    "end": "82000"
  },
  {
    "text": "to start by looking at what is the opportunity so as I mentioned the opportunity is to compress end-to-end",
    "start": "82710",
    "end": "88140"
  },
  {
    "text": "machine learning so make the pre-processing training and serving",
    "start": "88140",
    "end": "93720"
  },
  {
    "text": "faster and there are many ways to achieve that and I'm going to give an",
    "start": "93720",
    "end": "99150"
  },
  {
    "text": "example of three ways so for example you can reduce the input/output stall so",
    "start": "99150",
    "end": "104490"
  },
  {
    "text": "that your data is always available when you're training loop leads it the second is you reduce all your",
    "start": "104490",
    "end": "110970"
  },
  {
    "text": "overhead so that you are utilizing your memory efficiently and then the third is",
    "start": "110970",
    "end": "116070"
  },
  {
    "text": "that you distribute your training by scaling up within the node through the GPUs and then scaling out across the",
    "start": "116070",
    "end": "123149"
  },
  {
    "text": "nodes and make your machine a machine running faster so what we are going to look at is just the last piece of how",
    "start": "123149",
    "end": "130500"
  },
  {
    "text": "you can do that scaling using GPUs and we're going to look at in the context of cue flow so before we look at",
    "start": "130500",
    "end": "138819"
  },
  {
    "text": "what is a cue flow wanted to show a result so this is the result which we",
    "start": "138819",
    "end": "144459"
  },
  {
    "start": "141000",
    "end": "141000"
  },
  {
    "text": "did in our lab this is on UCS 480 ml which is a Cisco UCS box which has eight",
    "start": "144459",
    "end": "150430"
  },
  {
    "text": "GPUs and as you can see that we are doing this tensor flow benchmarking using the resonate model and as we are",
    "start": "150430",
    "end": "157840"
  },
  {
    "text": "scaling up the number of GPUs you can see the number of images per second that we can use in the training has improved",
    "start": "157840",
    "end": "164049"
  },
  {
    "text": "and this we are doing using env link let's say we were not using the NB link a faster link if you would see this same",
    "start": "164049",
    "end": "170410"
  },
  {
    "text": "scale-up except maybe the reduction of like 40 to 50 percent so this is the",
    "start": "170410",
    "end": "175750"
  },
  {
    "text": "opportunity which you have with distributive training so what is Q flow",
    "start": "175750",
    "end": "181870"
  },
  {
    "text": "I know by now you have might have listened to it so many times that I'm going to say this once more it is a",
    "start": "181870",
    "end": "187569"
  },
  {
    "text": "collection of tool which makes the end-to-end machine learning lifecycle easier and the thing is that it is built",
    "start": "187569",
    "end": "195730"
  },
  {
    "text": "on top of kubernetes so it is as portable as kubernetes is which is super portable so you can actually run it in",
    "start": "195730",
    "end": "202930"
  },
  {
    "text": "public cloud you can run it in a in a private cloud it's it's justa kubernetes",
    "start": "202930",
    "end": "208150"
  },
  {
    "text": "application and the other thing about the queue flow is that the job of queue",
    "start": "208150",
    "end": "213459"
  },
  {
    "text": "flow is it gives you a very easy interface and it hides the complexity of orchestrating whatever the tooling which",
    "start": "213459",
    "end": "220510"
  },
  {
    "text": "the queue flow provides and it does the underlying orchestration of kubernetes pod the secrets and everything so it",
    "start": "220510",
    "end": "226720"
  },
  {
    "text": "manages for you and you get this CRD interface so let's look into the queue",
    "start": "226720",
    "end": "233769"
  },
  {
    "text": "flow and the tools that the queue flow provides so the first is and this is where many of our customers are you know",
    "start": "233769",
    "end": "239680"
  },
  {
    "text": "you they have the data and they are trying to do is just exploration and many of them are maintaining their own",
    "start": "239680",
    "end": "245590"
  },
  {
    "text": "notebooks and so the first thing they like about queue flow is it standardizes and you have a single place where you",
    "start": "245590",
    "end": "252639"
  },
  {
    "text": "know you are not security and compliance across each of these individual magic machine learning teams but you are in",
    "start": "252639",
    "end": "259030"
  },
  {
    "text": "the central theme and you are managing the notebook and you're using the notebook server so when you're using the",
    "start": "259030",
    "end": "264700"
  },
  {
    "text": "notebook server you can start with the singles CPU image but then within the notebook server itself you have an opportunity to",
    "start": "264700",
    "end": "271250"
  },
  {
    "text": "scale so like let me just give a quick example of that so if you look look here",
    "start": "271250",
    "end": "277430"
  },
  {
    "text": "and I go to a notebook server so I can start for example with a if I want to",
    "start": "277430",
    "end": "283760"
  },
  {
    "text": "scale from single GPU to a single CPU to a GPU and want to tell scale-up",
    "start": "283760",
    "end": "288860"
  },
  {
    "text": "I would use I choose a GPO image which has all the CUDA driver libraries all",
    "start": "288860",
    "end": "295010"
  },
  {
    "text": "installed into that image and then there is just a simple place where you can add the number of GPUs you can use so there",
    "start": "295010",
    "end": "303020"
  },
  {
    "text": "is a simple command for that so this is the command so let me just copy this and show and then if I just launched this",
    "start": "303020",
    "end": "310970"
  },
  {
    "text": "then now I I have one CPU and you can scale it so within even an exploration phase you can go from CPUs to GPUs and",
    "start": "310970",
    "end": "318050"
  },
  {
    "text": "start scaling up but what happens underneath is that this notebook",
    "start": "318050",
    "end": "323350"
  },
  {
    "text": "eventually brings up just a single pod so you're scaling up that for number of",
    "start": "323350",
    "end": "328490"
  },
  {
    "text": "GPUs and that pod but you're not you are not able to scale out so if you have to scale out then you can use then then you",
    "start": "328490",
    "end": "336680"
  },
  {
    "text": "go ahead and use the model training model training operators which are",
    "start": "336680",
    "end": "342200"
  },
  {
    "text": "available within the queue flow so tensorflow supports variety of model training framework sorry the two floor",
    "start": "342200",
    "end": "348980"
  },
  {
    "text": "supports variety of model training frameworks such as tensorflow PI Taj XE boost MPI char and it also",
    "start": "348980",
    "end": "355430"
  },
  {
    "text": "supports this pre-processing library framework which is SPARC so many of our",
    "start": "355430",
    "end": "361490"
  },
  {
    "text": "customers for example initially have been just doing data analysis and on using spark and there's just started",
    "start": "361490",
    "end": "368810"
  },
  {
    "text": "looking at doing some kind of a deep learning so this they are at this phase and the tensorflow provides these",
    "start": "368810",
    "end": "375620"
  },
  {
    "text": "operators to do do just that and then once you have trained the model then you want to optimize the model to be as",
    "start": "375620",
    "end": "382160"
  },
  {
    "text": "performant as possible and you can do that using hyper parameter training and",
    "start": "382160",
    "end": "387280"
  },
  {
    "text": "in queue flow there is a and of course if you wanted to even figure out what",
    "start": "387280",
    "end": "392570"
  },
  {
    "text": "model it is you could use the neural architecture search and so both of these are part of this tool which is",
    "start": "392570",
    "end": "400610"
  },
  {
    "text": "or the library which is called catty and then when you're ready to serve it you could you do the serving using care",
    "start": "400610",
    "end": "407180"
  },
  {
    "text": "survey so I won't want to thank the cave serving team for getting my care serving set up up in the hallway yesterday they",
    "start": "407180",
    "end": "414650"
  },
  {
    "text": "have a very cool talk where you can see that KF surveying itself is built on a server s framework which is K native so",
    "start": "414650",
    "end": "421969"
  },
  {
    "text": "basically the surveillance framework when you are asked sending request so that it can serve you you can scale from",
    "start": "421969",
    "end": "429590"
  },
  {
    "text": "zero to as much as you want because you're behind your server less framework like a native and while it is getting",
    "start": "429590",
    "end": "435919"
  },
  {
    "text": "served you can use a single GPU a single CPU or you can use a GPU that",
    "start": "435919",
    "end": "441650"
  },
  {
    "text": "specification you can do as well it's called as accelerators in Kiev serving I",
    "start": "441650",
    "end": "447500"
  },
  {
    "text": "encourage you to check out their talk where they are just they described the exact syntax of doing that so and so the",
    "start": "447500",
    "end": "457340"
  },
  {
    "start": "456000",
    "end": "456000"
  },
  {
    "text": "way the cue flow implements all the tools in the cue flow implement their logic is through extending kubernetes",
    "start": "457340",
    "end": "463969"
  },
  {
    "text": "using cue flow operators so basically have it has custom resource definition",
    "start": "463969",
    "end": "469520"
  },
  {
    "text": "and as I mentioned that it has these different layers the pre-processing the",
    "start": "469520",
    "end": "474680"
  },
  {
    "text": "training hyper parameter tuning and serving and talked about spark in the",
    "start": "474680",
    "end": "480319"
  },
  {
    "text": "training so for example what happens with these operators is let's take an example of a denser fluid thr operator",
    "start": "480319",
    "end": "486710"
  },
  {
    "text": "what it does is that when you when you are looking at distributed training in your training code what you would",
    "start": "486710",
    "end": "493219"
  },
  {
    "text": "specify is a cluster spec and clusters spec is how you want to distribute your training workload and the tensorflow job",
    "start": "493219",
    "end": "501199"
  },
  {
    "text": "makes it very easy to describe that as you will see in the CML file it makes it very easy to describe your cluster spec",
    "start": "501199",
    "end": "508129"
  },
  {
    "text": "and then underneath it spins all the pods necessary to maintain that cluster definition and because the part of the",
    "start": "508129",
    "end": "516050"
  },
  {
    "text": "tensorflow job specification you specify the container image that you want to run or your training code image that you",
    "start": "516050",
    "end": "522078"
  },
  {
    "text": "want to run it injects the environment variable TF config inside that pod so",
    "start": "522079",
    "end": "527660"
  },
  {
    "text": "that the cluster spec is now available to your training code so it reduces for example when you're doing",
    "start": "527660",
    "end": "533840"
  },
  {
    "text": "distributed training it is helping you the distributed training orchestration so and similarly the other models such",
    "start": "533840",
    "end": "541400"
  },
  {
    "text": "as hyper parameter tuning which Neela is going to talk about when you want to",
    "start": "541400",
    "end": "546460"
  },
  {
    "text": "find the best model using a neural architecture search you are able to do",
    "start": "546460",
    "end": "551570"
  },
  {
    "text": "that because it makes it very simple to specify what you want and underlying it orchestrates all the different parts and",
    "start": "551570",
    "end": "558920"
  },
  {
    "text": "other resources for you so how many of you are familiar with deep neural",
    "start": "558920",
    "end": "564470"
  },
  {
    "start": "562000",
    "end": "562000"
  },
  {
    "text": "network because I wanted to walk through the layers and wanted to talk about what communication exists between the why we",
    "start": "564470",
    "end": "572990"
  },
  {
    "text": "need that distributed training communication so maybe like 50 percent",
    "start": "572990",
    "end": "578930"
  },
  {
    "text": "so I'll just quickly walk through the deep neural network and just give a",
    "start": "578930",
    "end": "585170"
  },
  {
    "text": "high-level view of what communication is needed and how it can be optimized across different layers so this is this",
    "start": "585170",
    "end": "592880"
  },
  {
    "text": "is a view of a model and the blue dots represents the different features of the",
    "start": "592880",
    "end": "600080"
  },
  {
    "text": "input and the green dot represents the output so for example the blue dots",
    "start": "600080",
    "end": "605570"
  },
  {
    "text": "could be the characteristics of a flower it could be the flower color the types",
    "start": "605570",
    "end": "611330"
  },
  {
    "text": "of the petals of the flower and the stem of the flower and the green dot would be",
    "start": "611330",
    "end": "617360"
  },
  {
    "text": "what you would see is your output which describes the species of the flower so for example it's hibiscus and the the",
    "start": "617360",
    "end": "625850"
  },
  {
    "text": "whole idea of finding the model is to find these layers these hidden layers which are these orange layers these",
    "start": "625850",
    "end": "632110"
  },
  {
    "text": "layers which with weights and activation functions which when multiplied with the",
    "start": "632110",
    "end": "638540"
  },
  {
    "text": "input can produce the right output that it can say that oh this is hibiscus so in this example the weight is supposed",
    "start": "638540",
    "end": "647480"
  },
  {
    "text": "to is called the model parameters and the activation function which is user supplied is called as a hyper parameter",
    "start": "647480",
    "end": "653090"
  },
  {
    "text": "and what happens is that the input is multiplied by this weight and activation",
    "start": "653090",
    "end": "658280"
  },
  {
    "text": "function depending on the way the arrows are attached to these neurons and finally you evaluate the weight so",
    "start": "658280",
    "end": "664250"
  },
  {
    "text": "that's the deep so in distributed training there are two ways to distribute the training one is",
    "start": "664250",
    "end": "671170"
  },
  {
    "text": "called data parallelism and the other is called as the model parallelism so in",
    "start": "671170",
    "end": "677110"
  },
  {
    "start": "677000",
    "end": "677000"
  },
  {
    "text": "data parallelism what happens is that in each of the worker in your distributed training you add the same model to each",
    "start": "677110",
    "end": "685180"
  },
  {
    "text": "of the workers and you split the data so your overall data is the sum of the three data but you have split the data",
    "start": "685180",
    "end": "691470"
  },
  {
    "text": "the and then each worker is independently working on its set of data",
    "start": "691470",
    "end": "697389"
  },
  {
    "text": "and then there has to be some communication and which I'm going to talk about to make this data parallelism",
    "start": "697389",
    "end": "702699"
  },
  {
    "text": "work so what we have seen so far in tensor flow is that all the model frameworks support data parallelism so",
    "start": "702699",
    "end": "709480"
  },
  {
    "text": "with the and the model parallelism is where you split the model and have a",
    "start": "709480",
    "end": "717430"
  },
  {
    "text": "portion of the model run in one worker and a second portion of the model run in",
    "start": "717430",
    "end": "722529"
  },
  {
    "text": "the second worker while you are training or the entire set of the data on that worker so as you can see here so",
    "start": "722529",
    "end": "728709"
  },
  {
    "text": "although this is not supported in queue flow in what you can do this manually by",
    "start": "728709",
    "end": "735310"
  },
  {
    "text": "pinning the each of the sub sections of the model to a different device and",
    "start": "735310",
    "end": "740680"
  },
  {
    "text": "simulate that model framer but it's not of the box currently supported so within",
    "start": "740680",
    "end": "745720"
  },
  {
    "text": "the data parallelism to again dig a little bit deeper as to figure out what communication happens in context of a",
    "start": "745720",
    "end": "752140"
  },
  {
    "text": "data parallelism so let's look at one worker view and it has this snapshot of",
    "start": "752140",
    "end": "757300"
  },
  {
    "text": "data and what what happens is that the way it works is it's a series of forward",
    "start": "757300",
    "end": "762519"
  },
  {
    "text": "and backward propagations so in the forward propagation phase what happens is that the inputs get multiplied by as",
    "start": "762519",
    "end": "769720"
  },
  {
    "text": "I set up with the weights and the activation function to produce the output so there is no the new",
    "start": "769720",
    "end": "775360"
  },
  {
    "text": "communication needed across the workers in the forward pass in the reverse pass or in the backward backward propagation",
    "start": "775360",
    "end": "782139"
  },
  {
    "text": "what happens is that the output is then compared with the actual output and the",
    "start": "782139",
    "end": "787870"
  },
  {
    "text": "last function you want to minimize so you can calculate the gradient and with that gradient you try to find the next",
    "start": "787870",
    "end": "794800"
  },
  {
    "text": "weight you find a better way than the old order way to minimize so as you can see when we're finding the",
    "start": "794800",
    "end": "801290"
  },
  {
    "text": "weight we are only finding the weight for this section of the data in this worker but then the other other workers",
    "start": "801290",
    "end": "807500"
  },
  {
    "text": "are finding on a different section so you have to combine or you have to agree gate the weight so this aggregation of",
    "start": "807500",
    "end": "813290"
  },
  {
    "text": "the weight is the communication that happens in during the distributed training so now that we've looked at",
    "start": "813290",
    "end": "819320"
  },
  {
    "text": "that now let's look at how the different layers can impact this so so to look at",
    "start": "819320",
    "end": "825230"
  },
  {
    "start": "824000",
    "end": "824000"
  },
  {
    "text": "the different layers we want to start with the first layer which is the infrastructure layer so in the",
    "start": "825230",
    "end": "832040"
  },
  {
    "text": "infrastructure layer let's start by assuming that the model is not distributed so it is running on one GPU",
    "start": "832040",
    "end": "838640"
  },
  {
    "text": "in one node if we want to distribute to a second GPU on the same node then the",
    "start": "838640",
    "end": "845470"
  },
  {
    "text": "the bandwidth between the GPUs becomes more important and so the way the GPUs",
    "start": "845470",
    "end": "852260"
  },
  {
    "text": "are connected that link impacts the performance and so it can be a PCIe link",
    "start": "852260",
    "end": "857600"
  },
  {
    "text": "or it could be a faster env link so that that would impact as you're scaling up",
    "start": "857600",
    "end": "863210"
  },
  {
    "text": "within the node and then if you are scaling from one node scaling out from",
    "start": "863210",
    "end": "869000"
  },
  {
    "text": "one node to the second node then it's the communication between the notes or the links there's this link speed",
    "start": "869000",
    "end": "875450"
  },
  {
    "text": "between the node which becomes important and then it could be anything like a high-speed Ethernet playing or take for",
    "start": "875450",
    "end": "881930"
  },
  {
    "text": "example InfiniBand or any of the other links and that would impact the performance so in the next layer which",
    "start": "881930",
    "end": "889340"
  },
  {
    "text": "is the OS layer this is the layer which exposes it D thinks a functionality to",
    "start": "889340",
    "end": "895340"
  },
  {
    "text": "into the kubernetes so in the OS layer you would want to expose your the GPUs",
    "start": "895340",
    "end": "900860"
  },
  {
    "text": "the drivers eye drivers to expose to kubernetes and the second thing is it should be also be",
    "start": "900860",
    "end": "906470"
  },
  {
    "text": "able to expose the env link for example if you had a faster link to the upper",
    "start": "906470",
    "end": "911840"
  },
  {
    "text": "layers and when the selection is made so for example if your workload wants three",
    "start": "911840",
    "end": "917240"
  },
  {
    "text": "GPUs it should be able to select the three GPUs which are connected with the fastest link rather than with some slow",
    "start": "917240",
    "end": "924350"
  },
  {
    "text": "links because that would impact the performance and then as we talked like as I mentioned about to flow it",
    "start": "924350",
    "end": "931300"
  },
  {
    "text": "it is it makes the orchestration of the distributed machine learning framework",
    "start": "931300",
    "end": "936459"
  },
  {
    "text": "easier and so it provides so so for example I talked about tensorflow job",
    "start": "936459",
    "end": "942370"
  },
  {
    "text": "and the hyper parameter tuning with khatib it it provides a simple interface",
    "start": "942370",
    "end": "949120"
  },
  {
    "text": "and then underlying orchestrate spot and other resources for you and finally you",
    "start": "949120",
    "end": "955360"
  },
  {
    "text": "run the as part of that tensorflow job or MPI job you would run your training",
    "start": "955360",
    "end": "961540"
  },
  {
    "text": "container and within the training container that you have a training code in addition to that you would have a",
    "start": "961540",
    "end": "967029"
  },
  {
    "text": "collective communication layer which is basically used to communicate and this",
    "start": "967029",
    "end": "972310"
  },
  {
    "text": "is what is communicating your weights so these weights could consist these",
    "start": "972310",
    "end": "978940"
  },
  {
    "text": "weights which are getting communicated you can communicate through any number of libraries for example there is nikkor",
    "start": "978940",
    "end": "983950"
  },
  {
    "text": "there is Qi RPC and each of the model frameworks support many other utilities",
    "start": "983950",
    "end": "989260"
  },
  {
    "text": "so with that dilemma will continue and walk through the distributed training",
    "start": "989260",
    "end": "994930"
  },
  {
    "start": "991000",
    "end": "991000"
  },
  {
    "text": "Thank You me next year so you actually walked us through the different types of",
    "start": "994930",
    "end": "1000959"
  },
  {
    "text": "parallelism so how do the different training pods or jobs communicate with",
    "start": "1000959",
    "end": "1007740"
  },
  {
    "text": "each other one of the common patterns that we see is using a parameter server this can be a part that is separate from",
    "start": "1007740",
    "end": "1014640"
  },
  {
    "text": "the training pods or it can be a part of the training part itself so we will see",
    "start": "1014640",
    "end": "1020399"
  },
  {
    "start": "1020000",
    "end": "1020000"
  },
  {
    "text": "an example of both using chip flow so this is an example of specifying a tensorflow job using as an explicit",
    "start": "1020399",
    "end": "1028319"
  },
  {
    "text": "parameter server so you have here I have two replicas so there are two parameter servers and there four replicas for the",
    "start": "1028319",
    "end": "1036750"
  },
  {
    "text": "workers so there for training parts that are running in parallel the pods can",
    "start": "1036750",
    "end": "1042120"
  },
  {
    "text": "talk to the parameters of the server that is most closest to them to be optimal in terms of the bandwidth and",
    "start": "1042120",
    "end": "1048540"
  },
  {
    "text": "the communication links the parameter servers then aggregate the weights within each other and pass the updated",
    "start": "1048540",
    "end": "1054870"
  },
  {
    "text": "weights back to the training pods if you want to change from this model of a synchronous parameter service to a",
    "start": "1054870",
    "end": "1061260"
  },
  {
    "text": "synchronous parameter server it's as simple as removing the the service pack in your TF job",
    "start": "1061260",
    "end": "1066960"
  },
  {
    "text": "specification so it's almost identical to the previous specification that we saw you just remove the peer the PS",
    "start": "1066960",
    "end": "1073800"
  },
  {
    "text": "section and I have increased the number of replicas here just to make the picture look nice so we have four",
    "start": "1073800",
    "end": "1079440"
  },
  {
    "text": "replicas there is no explicit parameter server here but all the pods are communicating with each other all the",
    "start": "1079440",
    "end": "1084900"
  },
  {
    "text": "time when would you use one versus the other so if you have fast communication",
    "start": "1084900",
    "end": "1091500"
  },
  {
    "text": "links between the pods this model is preferred more and more this model is preferred because you have all the pods",
    "start": "1091500",
    "end": "1098790"
  },
  {
    "text": "always in sync they all see the same weights and there's no drift between them but if you don't have a fast",
    "start": "1098790",
    "end": "1104700"
  },
  {
    "text": "communication link between the pods it gets very slow to do this so you want to use an asynchronous parameter server we",
    "start": "1104700",
    "end": "1111510"
  },
  {
    "text": "will look at distributed training using an MPI job spec in cube flow the reason",
    "start": "1111510",
    "end": "1118050"
  },
  {
    "text": "being MPI allows you to use multiple frameworks so this is a TF job specification tensorflow with 2.0 allows",
    "start": "1118050",
    "end": "1125790"
  },
  {
    "text": "you to do fast communication across nodes and within a node but that's only available with intensive flow so if you",
    "start": "1125790",
    "end": "1132060"
  },
  {
    "text": "want to use the same functionality with PI torch or other frameworks it's easier to use the MPI model so how does cube",
    "start": "1132060",
    "end": "1140370"
  },
  {
    "start": "1140000",
    "end": "1140000"
  },
  {
    "text": "flow implement the MPI job this is a very high level architecture diagram so",
    "start": "1140370",
    "end": "1146430"
  },
  {
    "text": "you have a custom resource width which is the MPI job it it composes of two primary components one is the launcher",
    "start": "1146430",
    "end": "1152610"
  },
  {
    "text": "and the second one is the worker the launcher acts as the orchestrator of the workers so it creates multiple",
    "start": "1152610",
    "end": "1158430"
  },
  {
    "text": "worker pods and handles all the coordination between them how does this",
    "start": "1158430",
    "end": "1163740"
  },
  {
    "text": "specification look so here we have the launcher I'm saying there's a replica of one and I have slots per worker is one",
    "start": "1163740",
    "end": "1171570"
  },
  {
    "text": "so there's a little bit of tweaking required here so if you have the slots",
    "start": "1171570",
    "end": "1176580"
  },
  {
    "text": "per worker specifies how many tasks you want to run in one pod so I have one GPU",
    "start": "1176580",
    "end": "1182460"
  },
  {
    "text": "so I'm running one slot in one worker and the MPI run command here says in",
    "start": "1182460",
    "end": "1188130"
  },
  {
    "text": "total I have to process only two roses because I'm running only two workers and each of them has a single slot the",
    "start": "1188130",
    "end": "1195060"
  },
  {
    "text": "worker definition here is very similar have two replicas and each of them has",
    "start": "1195060",
    "end": "1200290"
  },
  {
    "start": "1196000",
    "end": "1196000"
  },
  {
    "text": "one GPU so they have to request one GPU from kubernetes this is similar to any kubernetes resource request definition",
    "start": "1200290",
    "end": "1207820"
  },
  {
    "text": "the only difference being with GPUs you can't really have a high and a low limit",
    "start": "1207820",
    "end": "1213400"
  },
  {
    "text": "you just specify what you want and that's all you get if you want to scale",
    "start": "1213400",
    "end": "1218950"
  },
  {
    "start": "1217000",
    "end": "1217000"
  },
  {
    "text": "the MPI jobs it's just as simple as changing the number of workers that you have and the number of GPUs that you",
    "start": "1218950",
    "end": "1226510"
  },
  {
    "text": "have so the same specification that we saw before I have updated it to have three workers and each of them have",
    "start": "1226510",
    "end": "1234310"
  },
  {
    "start": "1228000",
    "end": "1228000"
  },
  {
    "text": "three replicas right so there's nine processes that are running in parallel and all of them are running the",
    "start": "1234310",
    "end": "1241570"
  },
  {
    "text": "tensorflow benchmark ResNet model and they are communicating to each other using MPI so that is why the variable",
    "start": "1241570",
    "end": "1249010"
  },
  {
    "text": "update of the parameter server update mechanism is specified as however if you",
    "start": "1249010",
    "end": "1254230"
  },
  {
    "start": "1254000",
    "end": "1254000"
  },
  {
    "text": "want to scale up that has increased the number of slots in your worker and increase the number of GPUs that are",
    "start": "1254230",
    "end": "1259690"
  },
  {
    "text": "available you basically update the resources that is specified here and again this is the scaling out part where",
    "start": "1259690",
    "end": "1265780"
  },
  {
    "text": "I've increased the number of replicas so this is an example of an MPI specification let us do a quick demo and",
    "start": "1265780",
    "end": "1272800"
  },
  {
    "text": "have this run and hope it works",
    "start": "1272800",
    "end": "1276930"
  },
  {
    "text": "we just ran it and deleted it so so this is a tensorflow benchmark that I'm",
    "start": "1278310",
    "end": "1283720"
  },
  {
    "text": "running it's an MPI job we look at the specification while the pods are",
    "start": "1283720",
    "end": "1289240"
  },
  {
    "text": "starting very similar to what we talked about we have one pod one function that",
    "start": "1289240",
    "end": "1295660"
  },
  {
    "text": "is running in one worker you have one one launcher two workers and the workers",
    "start": "1295660",
    "end": "1301090"
  },
  {
    "text": "have one GPU that is available we can look at the log launcher pod logs and it",
    "start": "1301090",
    "end": "1308530"
  },
  {
    "text": "tells you that it is if you go to the very beginning you can see that it is starting to workers so here I have",
    "start": "1308530",
    "end": "1313690"
  },
  {
    "text": "worker one is the font visible to everyone it's just small",
    "start": "1313690",
    "end": "1320399"
  },
  {
    "text": "this is better okay so yeah so we have",
    "start": "1322170",
    "end": "1328440"
  },
  {
    "text": "yeah the first part that is starting here the second part here you will see",
    "start": "1328440",
    "end": "1333580"
  },
  {
    "text": "each of the devices being identified so we will have a Tesla v 100 that comes up",
    "start": "1333580",
    "end": "1340300"
  },
  {
    "text": "here so and then you will see the actual job running and the training happening",
    "start": "1340300",
    "end": "1346000"
  },
  {
    "text": "takes about a minute because I'm just running a sample job at the end of which it will get it accumulate the",
    "start": "1346000",
    "end": "1352540"
  },
  {
    "text": "communication between the worker zero worker one and give you the end result of your model so going back to the",
    "start": "1352540",
    "end": "1360700"
  },
  {
    "text": "presentation we'll skip the slides",
    "start": "1360700",
    "end": "1367330"
  },
  {
    "start": "1365000",
    "end": "1365000"
  },
  {
    "text": "because we already saw that so that's how you implement a distributed training using cube flow we just saw one example",
    "start": "1367330",
    "end": "1374200"
  },
  {
    "text": "of MPI job but it's very similar irrespective of what operator you choose",
    "start": "1374200",
    "end": "1379450"
  },
  {
    "text": "TF job is very similar any model is very similar right so we trained a model but",
    "start": "1379450",
    "end": "1385990"
  },
  {
    "text": "how do you start how where did that model come from right so you have a user who's looking at that data and saying",
    "start": "1385990",
    "end": "1393730"
  },
  {
    "text": "okay this is the sort of architecture that will fit my data right so they take",
    "start": "1393730",
    "end": "1398950"
  },
  {
    "text": "an architecture see run train it and see how well it is performing it doesn't",
    "start": "1398950",
    "end": "1404710"
  },
  {
    "text": "perform well they change it tweak it a little bit get a next model and then keep repeating the process until they",
    "start": "1404710",
    "end": "1410530"
  },
  {
    "text": "finally find a model that works for them and you this process is very laborious",
    "start": "1410530",
    "end": "1415870"
  },
  {
    "text": "and manuals so you want to reduce the amount of human effort involved there and the way cube flow allows you to do",
    "start": "1415870",
    "end": "1423550"
  },
  {
    "text": "that is using something called Kathy Kathy is a hyper parameter tuner and a",
    "start": "1423550",
    "end": "1428770"
  },
  {
    "text": "neural architecture such this we can argue about whether neural architecture search is also high Povera major tuning",
    "start": "1428770",
    "end": "1434410"
  },
  {
    "text": "but it basically does both it lets you identify the best hyper parameters and",
    "start": "1434410",
    "end": "1439480"
  },
  {
    "text": "the best models for your data it does that but it does that in a way using",
    "start": "1439480",
    "end": "1447310"
  },
  {
    "start": "1444000",
    "end": "1444000"
  },
  {
    "text": "multimodal parallelism so your you have multiple models or a same model with",
    "start": "1447310",
    "end": "1452740"
  },
  {
    "text": "different hyper parameters and you training them on multiple devices multiple nodes using multiple GPUs so",
    "start": "1452740",
    "end": "1459230"
  },
  {
    "text": "you're distributing each of your model training but you're also running it in parallel with multiple models again",
    "start": "1459230",
    "end": "1465559"
  },
  {
    "text": "doing this in cube flow is very very simple so we look at how it act how",
    "start": "1465559",
    "end": "1470600"
  },
  {
    "start": "1470000",
    "end": "1470000"
  },
  {
    "text": "Cathy actually achieves this there is something called a model manager which handles this job of creating the models",
    "start": "1470600",
    "end": "1477259"
  },
  {
    "text": "and sending it down to a trial controller the triangle controller takes each of the models takes your data and",
    "start": "1477259",
    "end": "1484249"
  },
  {
    "text": "trains them it then evaluates how well they're performing and creates this the",
    "start": "1484249",
    "end": "1489559"
  },
  {
    "text": "feedback to the suggestion controller which creates which tells the model manager what type of model or what type",
    "start": "1489559",
    "end": "1495799"
  },
  {
    "text": "of modification it needs to make or it gives new hyper parameters that you want to try out this process continues until",
    "start": "1495799",
    "end": "1502489"
  },
  {
    "text": "either khatib finds the model that fits your requirements or it exhausts the",
    "start": "1502489",
    "end": "1508429"
  },
  {
    "text": "budget that you have specified for it so this is an example of how you specify a",
    "start": "1508429",
    "end": "1514190"
  },
  {
    "start": "1512000",
    "end": "1512000"
  },
  {
    "text": "job in Khateeb so we here we have you're creating an experiment so you're saying",
    "start": "1514190",
    "end": "1519980"
  },
  {
    "text": "here that create three create maximum twelve trials run at most three trials",
    "start": "1519980",
    "end": "1526700"
  },
  {
    "text": "in parallel so I'm running this on my laptop and I don't really want to run thousands of trials in parallel that have a small spec and then you say what",
    "start": "1526700",
    "end": "1534769"
  },
  {
    "text": "is your objective so you want to maximize the validation accuracy here you can set any objective that you want",
    "start": "1534769",
    "end": "1540919"
  },
  {
    "text": "any data that's output by your model and here I'm asking it to identify the best",
    "start": "1540919",
    "end": "1546529"
  },
  {
    "text": "neural architecture so that layer them name is neural architecture search khateeb supports a lot of hype a",
    "start": "1546529",
    "end": "1552470"
  },
  {
    "text": "parameter tuning model so you can do random search you can do grid search you",
    "start": "1552470",
    "end": "1557509"
  },
  {
    "text": "can do Bayesian optimization or if you wanted identify the architecture itself you can use the neural architecture",
    "start": "1557509",
    "end": "1563239"
  },
  {
    "text": "search the job specification for khateeb",
    "start": "1563239",
    "end": "1568519"
  },
  {
    "text": "is very similar to the job specification that we saw for the MPI job you just say",
    "start": "1568519",
    "end": "1573529"
  },
  {
    "text": "what type of a job it is either a built in kubernetes resource like a pod or a",
    "start": "1573529",
    "end": "1578929"
  },
  {
    "text": "job or it can be a cube flow custom resource like a TF job and here again",
    "start": "1578929",
    "end": "1585679"
  },
  {
    "text": "I'm saying this is how you specify a GP you to be used this is an example of a",
    "start": "1585679",
    "end": "1592640"
  },
  {
    "start": "1591000",
    "end": "1591000"
  },
  {
    "text": "cue flow neural architecture search so not very clear but you can see that",
    "start": "1592640",
    "end": "1597950"
  },
  {
    "text": "every suggestion generates the number of layers the type of activation functions",
    "start": "1597950",
    "end": "1604430"
  },
  {
    "text": "that you want to use how many neurons are activated in each layer and runs them too good through the trials we",
    "start": "1604430",
    "end": "1610250"
  },
  {
    "text": "didn't want to do that life because it takes quite some time this is an example of that running and with the view of the",
    "start": "1610250",
    "end": "1617300"
  },
  {
    "text": "different nodes on which it is distributed so you can very clearly see that it's run across multiple nodes so",
    "start": "1617300",
    "end": "1623840"
  },
  {
    "text": "very easy to do distributed hyper parameter tuning using Kathy alright so",
    "start": "1623840",
    "end": "1631070"
  },
  {
    "start": "1630000",
    "end": "1630000"
  },
  {
    "text": "we have looked at model training how do we distribute it we've looked at hyper",
    "start": "1631070",
    "end": "1636950"
  },
  {
    "text": "parameter tuning now if Kathy makes it all so super easy do we need to do",
    "start": "1636950",
    "end": "1642170"
  },
  {
    "text": "anything else the answer is yes right often our customers come to us asking us",
    "start": "1642170",
    "end": "1649900"
  },
  {
    "start": "1649000",
    "end": "1649000"
  },
  {
    "text": "how do I set up my cluster do I go all bare-metal do I use VMs do I use V GPUs",
    "start": "1649900",
    "end": "1657230"
  },
  {
    "text": "they sound cool what do I use right and the answer is it depends it depends on what you are trying to do what kind of",
    "start": "1657230",
    "end": "1664070"
  },
  {
    "text": "resources you have and how much you want to share if you have a single dedicated",
    "start": "1664070",
    "end": "1669860"
  },
  {
    "text": "user or team to whom you want to give all your GPU resources then where metal",
    "start": "1669860",
    "end": "1675170"
  },
  {
    "text": "may be the best option to go because you can get the last bit of performance out of your GPUs there is no interference",
    "start": "1675170",
    "end": "1680930"
  },
  {
    "text": "there's no sharing right but then you're effectively locking down GPUs which are",
    "start": "1680930",
    "end": "1686780"
  },
  {
    "text": "potentially a very costly resource right so if you want to avoid that and if you",
    "start": "1686780",
    "end": "1692180"
  },
  {
    "text": "have multiple users with whom you want to share using virtualization is a very easy mechanism if you combine",
    "start": "1692180",
    "end": "1699530"
  },
  {
    "text": "virtualization with PCI pass-through you get near native bare-metal performance so there's absolutely no difference in",
    "start": "1699530",
    "end": "1706130"
  },
  {
    "text": "the performance that you see with bare metal or PCI pass through using GPUs if",
    "start": "1706130",
    "end": "1711320"
  },
  {
    "text": "you combine that with auto scaling now you are able to share your GPUs across",
    "start": "1711320",
    "end": "1716540"
  },
  {
    "text": "different clusters somebody creates a pod or a workload they ask for two GPUs",
    "start": "1716540",
    "end": "1721940"
  },
  {
    "text": "the entire workload runs and the workload is done so your autoscaler does it says okay I don't need this GPO Nord",
    "start": "1721940",
    "end": "1727850"
  },
  {
    "text": "anymore it removes it and that's now available for another part to use in another cluster potentially what about V",
    "start": "1727850",
    "end": "1735980"
  },
  {
    "text": "GP is when de you use V GPUs if you have diverse types of workloads that you are",
    "start": "1735980",
    "end": "1741890"
  },
  {
    "text": "trying to run so you're not doing all model training so it may be some people in your team are doing training some",
    "start": "1741890",
    "end": "1747380"
  },
  {
    "text": "people want to do inference some people want to just run a Jupiter notebook and do data exploration all of these have",
    "start": "1747380",
    "end": "1754010"
  },
  {
    "text": "different compute and GPU resource requirements so in such cases it's",
    "start": "1754010",
    "end": "1759920"
  },
  {
    "text": "easier to divide and partition the GPU into pieces by using V GPUs or virtual",
    "start": "1759920",
    "end": "1765950"
  },
  {
    "text": "GPUs so there's a component that goes into your hypervisor layer which takes a",
    "start": "1765950",
    "end": "1771530"
  },
  {
    "text": "physical GPU and splits it up into fractional DPS so you can say I have an inference serving part it needs one",
    "start": "1771530",
    "end": "1777830"
  },
  {
    "text": "tenth of a GPA and when that part is done that one tenth is freed up you could again combine the entire GPU give",
    "start": "1777830",
    "end": "1784460"
  },
  {
    "text": "it to someone else so it's easy to share your resources across a diverse set of",
    "start": "1784460",
    "end": "1791560"
  },
  {
    "text": "TAS if you have different kinds of workloads that are running in your clusters and let's go a little bit",
    "start": "1791560",
    "end": "1798050"
  },
  {
    "text": "deeper then how you set up your service how does the communication actually happen within the server so this is a",
    "start": "1798050",
    "end": "1806270"
  },
  {
    "start": "1803000",
    "end": "1803000"
  },
  {
    "text": "topology of the GPUs on one of our servers see 480 ml from Cisco",
    "start": "1806270",
    "end": "1812750"
  },
  {
    "text": "it has an 8 GPUs there are many such servers that are available with a GPUs",
    "start": "1812750",
    "end": "1818600"
  },
  {
    "text": "which you can see which have similar topology restrictions so what this gives you is a 6 env link lanes connectivity",
    "start": "1818600",
    "end": "1825770"
  },
  {
    "text": "between multiple GPUs it also has multiple pcs which is to connect the GPUs to the CPUs so you can have fast",
    "start": "1825770",
    "end": "1833180"
  },
  {
    "text": "communication paths why does it matter right so each of the env link lanes",
    "start": "1833180",
    "end": "1838790"
  },
  {
    "text": "gives you 25 gigabits of bandwidth together all of them give you the six of them give you 300 gigabytes of bandwidth",
    "start": "1838790",
    "end": "1845480"
  },
  {
    "text": "if you were to take compare it to just a PCIe link that just gives you 30 GB it's",
    "start": "1845480",
    "end": "1852440"
  },
  {
    "text": "like a 10x improvement in performance if you have time we will do a demo that will actually show how much of a difference",
    "start": "1852440",
    "end": "1859310"
  },
  {
    "text": "it makes so if you have multiple GPUs that are really connected using fast",
    "start": "1859310",
    "end": "1864500"
  },
  {
    "text": "interconnects and you're doing things like distributed training which have a lot of which are using the synchronous",
    "start": "1864500",
    "end": "1871370"
  },
  {
    "text": "parameter server model where all the quads are talking to each other all the time it makes a huge difference what GPUs you",
    "start": "1871370",
    "end": "1878570"
  },
  {
    "text": "pick right if you pick the GPUs that are really tightly connected it you can be",
    "start": "1878570",
    "end": "1883910"
  },
  {
    "text": "really fast if you pick GPU 0 here and GPU 8 here as the two GPUs you give to your pod then it's basically not it's",
    "start": "1883910",
    "end": "1891920"
  },
  {
    "text": "just the same as running them on two different nodes right so you're missing out on the fast connect so it helps to",
    "start": "1891920",
    "end": "1898520"
  },
  {
    "text": "be cognizant of the resources underneath as much as cube flow tries to abstract",
    "start": "1898520",
    "end": "1903710"
  },
  {
    "text": "it away from us we also need to have little bit of awareness there are improvements going on in the community",
    "start": "1903710",
    "end": "1910610"
  },
  {
    "text": "in the form of the changes to the topology manager which allows you to do placement which automatically do",
    "start": "1910610",
    "end": "1916040"
  },
  {
    "text": "placement within a cluster by picking the optimal GPUs but again if you're",
    "start": "1916040",
    "end": "1921670"
  },
  {
    "text": "dividing it out across different clusters then you still need to",
    "start": "1921670",
    "end": "1927260"
  },
  {
    "text": "understand this I don't think we have time for a demo so let me conclude quickly so we basically we saw how cube",
    "start": "1927260",
    "end": "1936290"
  },
  {
    "start": "1931000",
    "end": "1931000"
  },
  {
    "text": "flow lets you take data be processing training hyper-v remaining model serving",
    "start": "1936290",
    "end": "1942110"
  },
  {
    "text": "any of these tasks in your machine learning workflow and easily distributed",
    "start": "1942110",
    "end": "1948410"
  },
  {
    "text": "across multiple nodes across multiple GPUs right so one how many of you have",
    "start": "1948410",
    "end": "1959450"
  },
  {
    "text": "used something like this right so I was thinking of what how do we what do I",
    "start": "1959450",
    "end": "1965510"
  },
  {
    "text": "save cube flow is right I've heard of it referred to as a buffet of different things right because when you come to",
    "start": "1965510",
    "end": "1972530"
  },
  {
    "text": "cube flow it looks like you have everything thrown in there's just so many operators there they're doing so many different things but it's it's",
    "start": "1972530",
    "end": "1979520"
  },
  {
    "text": "exactly like this it's like a drill electron electric drill so you have lots of bits there what it does is gives you",
    "start": "1979520",
    "end": "1986420"
  },
  {
    "text": "a standardized interface right so you have cube flow and you take any of",
    "start": "1986420",
    "end": "1991429"
  },
  {
    "text": "these different frameworks you have the same knobs you increase the speed on your drill it doesn't matter what bit",
    "start": "1991429",
    "end": "1997279"
  },
  {
    "text": "you're plugged in it's the same way to do it it makes it easy for you to switch from one to another to give a",
    "start": "1997279",
    "end": "2003100"
  },
  {
    "text": "standardized environment to all your consumers of your cluster and to take",
    "start": "2003100",
    "end": "2008529"
  },
  {
    "text": "advantage of new things that are coming in just as easily with that I would like",
    "start": "2008529",
    "end": "2013600"
  },
  {
    "start": "2013000",
    "end": "2013000"
  },
  {
    "text": "to thank everyone who was contributed to cube flow because without you we wouldn't be here huge thanks to the",
    "start": "2013600",
    "end": "2022899"
  },
  {
    "text": "Cisco cube flow team that helps us with our customer engagements answers",
    "start": "2022899",
    "end": "2027970"
  },
  {
    "text": "questions and helps us anytime we need and also to the Cisco UCS team who gives us hardware and again they're available",
    "start": "2027970",
    "end": "2034659"
  },
  {
    "text": "anytime they're awesome last but not least thank you so much all",
    "start": "2034659",
    "end": "2040480"
  },
  {
    "text": "of you this is last day last session I can't imagine how much patience you have to be here but we really really",
    "start": "2040480",
    "end": "2047590"
  },
  {
    "text": "appreciate it thank you [Applause]",
    "start": "2047590",
    "end": "2054249"
  }
]