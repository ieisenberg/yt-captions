[
  {
    "text": "hey uh Welcome to our talk today uh we'll be talking about decentralized routing for a sharded application on",
    "start": "659",
    "end": "7740"
  },
  {
    "text": "service mesh I'm vinay gonogundlaf and this is pankajika",
    "start": "7740",
    "end": "13139"
  },
  {
    "text": "so we are software engineers at Intuit and we work on the service mesh platform at into it",
    "start": "13139",
    "end": "19020"
  },
  {
    "text": "so let me go over uh the agenda for today um we'll be talking about a little bit",
    "start": "19020",
    "end": "24779"
  },
  {
    "text": "about what service mesh is and how it is set up at Intuit a little bit about the starter",
    "start": "24779",
    "end": "30779"
  },
  {
    "text": "application and how routing is done on a starter application our design and some challenges we faced we'll go over the",
    "start": "30779",
    "end": "37440"
  },
  {
    "text": "demo as well as uh some future investment we have on the solution",
    "start": "37440",
    "end": "43800"
  },
  {
    "text": "so a little bit about Intuit uh into it is a leading fintech company",
    "start": "43800",
    "end": "49140"
  },
  {
    "text": "um I would consider ourselves more of a bleeding edge a platform company uh we are really",
    "start": "49140",
    "end": "54800"
  },
  {
    "text": "honored to have received the end user award for 2019 as well as 2022. if you",
    "start": "54800",
    "end": "61260"
  },
  {
    "text": "look at the cncf portfolio into pretty much uses most of the products",
    "start": "61260",
    "end": "66479"
  },
  {
    "text": "there we also are a big open source contributor uh we contribute to more",
    "start": "66479",
    "end": "71939"
  },
  {
    "text": "than 75 plus projects and some of the projects we maintain and our open source",
    "start": "71939",
    "end": "78840"
  },
  {
    "text": "are Argo Kiko Admiral and new approach",
    "start": "78840",
    "end": "84600"
  },
  {
    "text": "so uh let me go into a little bit of the scale at which Intuit operates",
    "start": "84600",
    "end": "90060"
  },
  {
    "text": "um Intuit has more than 245 plus kubernetes clusters running in production",
    "start": "90060",
    "end": "95900"
  },
  {
    "text": "we spend about 16 000 name spaces in those clusters at Peak we recorded more",
    "start": "95900",
    "end": "102299"
  },
  {
    "text": "than 77 000 nodes are running in production uh with more than 7 million pods and we",
    "start": "102299",
    "end": "108840"
  },
  {
    "text": "have 2000 plus unique Services which are running among these clusters and these",
    "start": "108840",
    "end": "114060"
  },
  {
    "text": "services are a mix of both um end user Services as well as internal services",
    "start": "114060",
    "end": "119100"
  },
  {
    "text": "so next I want to just walk through service mesh how many of you here know what service mesh is",
    "start": "119100",
    "end": "125880"
  },
  {
    "text": "I think most of you do so I can go a little fast on this section is an infrastructure layer to facilitate",
    "start": "125880",
    "end": "133319"
  },
  {
    "text": "service to service communication I think the most popular approach right now is to use the sidecar",
    "start": "133319",
    "end": "138959"
  },
  {
    "text": "to intercept traffic for Ingress and egress communication through the application this allows us to do few",
    "start": "138959",
    "end": "146040"
  },
  {
    "text": "cool things with uh by adding a sidecars or using service mesh some of them are",
    "start": "146040",
    "end": "152760"
  },
  {
    "text": "security by providing Mutual TLS observability by taking a snapshot of",
    "start": "152760",
    "end": "158819"
  },
  {
    "text": "the entire system and finding the bottlenecks routing by providing things",
    "start": "158819",
    "end": "165239"
  },
  {
    "text": "like Canary or traffic splitting and also reliability by automating retries",
    "start": "165239",
    "end": "172080"
  },
  {
    "text": "some of the most popular service mesh offerings out there are istio linkery",
    "start": "172080",
    "end": "177780"
  },
  {
    "text": "and console we had to use istio as our service mesh",
    "start": "177780",
    "end": "182879"
  },
  {
    "text": "platform so there's a little bit about istio since most of you are familiar I'll just",
    "start": "182879",
    "end": "188819"
  },
  {
    "text": "I'll go through this pretty quick istio has or many services are mainly",
    "start": "188819",
    "end": "194640"
  },
  {
    "text": "divided logically into two parts control plane and data plane the data plane part is where the service or the application",
    "start": "194640",
    "end": "201840"
  },
  {
    "text": "lies and this is where the communication happens and to basically um",
    "start": "201840",
    "end": "208400"
  },
  {
    "text": "maintain or configure these proxies we have a control plane section and uh here",
    "start": "208400",
    "end": "213840"
  },
  {
    "text": "I'm just mentioning two important components in istio one is pilot and",
    "start": "213840",
    "end": "219360"
  },
  {
    "text": "others Galley pilot is responsible for configuring the proxy and Galley is mainly for uh",
    "start": "219360",
    "end": "226099"
  },
  {
    "text": "user configuration management and it listens to your requests and sends the instructions to Pilot so at Intuit we",
    "start": "226099",
    "end": "234780"
  },
  {
    "text": "have a multi-cluster istio setup um so here I'm just representing like three clusters with istio installed",
    "start": "234780",
    "end": "241440"
  },
  {
    "text": "individually uh is your control plane installed individually in each of these clusters",
    "start": "241440",
    "end": "246959"
  },
  {
    "text": "now think of a communication say from service C to service B which is within the cluster and istio is responsible for",
    "start": "246959",
    "end": "254159"
  },
  {
    "text": "that now for example if you think of communication say from service a to service C or service C to service d uh",
    "start": "254159",
    "end": "261120"
  },
  {
    "text": "this would require some sort of a multi-cluster setup we use Admiral as",
    "start": "261120",
    "end": "266940"
  },
  {
    "text": "our open source control plane component which manages uh",
    "start": "266940",
    "end": "272580"
  },
  {
    "text": "different istio resources among these clusters and allows communication between different Services here",
    "start": "272580",
    "end": "281759"
  },
  {
    "text": "so uh now that we have an overview of solar smash head into it I'll hand it off to pankaj who will talk a little bit",
    "start": "281759",
    "end": "288900"
  },
  {
    "text": "about our use case thanks vinay",
    "start": "288900",
    "end": "294120"
  },
  {
    "text": "thanks for the introduction um so uh before I dive into the details",
    "start": "294120",
    "end": "301199"
  },
  {
    "text": "a little bit about the traffic the way it's handled out into it right so all the North South traffic at Intuit is",
    "start": "301199",
    "end": "308520"
  },
  {
    "text": "handled through into it managed API Gateway and internally we are going through this journey of moving all these",
    "start": "308520",
    "end": "315900"
  },
  {
    "text": "internal application traffic to service mesh while we were going through this journey we came across this use case",
    "start": "315900",
    "end": "322740"
  },
  {
    "text": "where an internal service wanted to talk to a sharded application um which was you know distributed across",
    "start": "322740",
    "end": "328740"
  },
  {
    "text": "multiple charts before we you know get into the details about so so basically what we had to do",
    "start": "328740",
    "end": "335400"
  },
  {
    "text": "was solve for which particular Shard request needs to go based on the request",
    "start": "335400",
    "end": "340740"
  },
  {
    "text": "context before we dive into the details I think it's I'll touch upon why a",
    "start": "340740",
    "end": "346199"
  },
  {
    "text": "sharded application is required and what charted application is so think of a normal web application",
    "start": "346199",
    "end": "352380"
  },
  {
    "text": "which is fronted by API Gateway and has a bunch of users that are making requests to it",
    "start": "352380",
    "end": "358139"
  },
  {
    "text": "as the number of users grow there's some kind of some need of some scaling on the",
    "start": "358139",
    "end": "364860"
  },
  {
    "text": "web application one approach to do so is sharding where the web application splits its data into multiple shards and",
    "start": "364860",
    "end": "373440"
  },
  {
    "text": "API Gateway the way it makes the routing decision to these shards it could do so",
    "start": "373440",
    "end": "378479"
  },
  {
    "text": "by maintaining a static list but as the number of users grow even further static",
    "start": "378479",
    "end": "384360"
  },
  {
    "text": "list is no longer a viable approach and there's a need for a better solution at Intuit what we do is we have a external",
    "start": "384360",
    "end": "391860"
  },
  {
    "text": "lookup service that has the logic and it can you know it's using the basically",
    "start": "391860",
    "end": "398639"
  },
  {
    "text": "the same back end as these web applications so it knows where the data is charted and it also has an algorithm",
    "start": "398639",
    "end": "406259"
  },
  {
    "text": "to determine The Shard where it needs to go to so Gateway makes a call to this",
    "start": "406259",
    "end": "412380"
  },
  {
    "text": "lookup service and then it knows what chart it needs to Route it to to make",
    "start": "412380",
    "end": "417479"
  },
  {
    "text": "this lookup small more efficient we are also using a distributed cache so one example of an application that",
    "start": "417479",
    "end": "424500"
  },
  {
    "text": "does that at Intuit is QuickBooks Online which is an accounting software more",
    "start": "424500",
    "end": "429840"
  },
  {
    "text": "than 150 000 companies use it and there are more than 5 million customers across the",
    "start": "429840",
    "end": "436139"
  },
  {
    "text": "globe so let's let's look at how this routing",
    "start": "436139",
    "end": "442080"
  },
  {
    "text": "looks like when we have to do so on mesh in this diagram what you see on the left",
    "start": "442080",
    "end": "447720"
  },
  {
    "text": "is service a which has a sidecar proxy it's trying to communicate with a sharded application which is distributed",
    "start": "447720",
    "end": "454800"
  },
  {
    "text": "among three different charts so here the routing decision can be made on the",
    "start": "454800",
    "end": "459900"
  },
  {
    "text": "online proxy or one approach is to let this request go through API Gateway as",
    "start": "459900",
    "end": "465300"
  },
  {
    "text": "well um but then that introduces an additional hop between the service to service communication and um usually",
    "start": "465300",
    "end": "472919"
  },
  {
    "text": "what the services that talk to each other a lot are co-located within the same cluster probably also within the",
    "start": "472919",
    "end": "480360"
  },
  {
    "text": "same VPC and then this call adds Network latency by going out of the VPC and",
    "start": "480360",
    "end": "485759"
  },
  {
    "text": "back-end and so on right so we can avoid that by using service mesh so the approach we took was to implement the",
    "start": "485759",
    "end": "493620"
  },
  {
    "text": "routing logic within the proxy itself but before we go there I want to you",
    "start": "493620",
    "end": "500039"
  },
  {
    "text": "know go through the goals we had for this approach so as the shards are maintained uh by by",
    "start": "500039",
    "end": "508800"
  },
  {
    "text": "the services themselves we did not want the client to be aware of them these decisions need to be transparent",
    "start": "508800",
    "end": "516000"
  },
  {
    "text": "to the client services as well so we did not want to have any changes or if any changes we wanted them to be very",
    "start": "516000",
    "end": "522539"
  },
  {
    "text": "minimal then ah also as I spoke about the Qbo use case which shards its uh data based on",
    "start": "522539",
    "end": "530399"
  },
  {
    "text": "the company IDs we have to handle that and also other applications identity",
    "start": "530399",
    "end": "536220"
  },
  {
    "text": "that have millions of users they may chart their data based upon a user location multiple criteria so we our",
    "start": "536220",
    "end": "542940"
  },
  {
    "text": "solution needed to handle all those use cases as well so when imagine I'll go back to the Qbo",
    "start": "542940",
    "end": "550380"
  },
  {
    "text": "example when the data in a particular Shard you know grows beyond a certain",
    "start": "550380",
    "end": "555779"
  },
  {
    "text": "threshold there could be a requirement of dividing that data into multiple for the shards and this needs to be",
    "start": "555779",
    "end": "564240"
  },
  {
    "text": "supported in near real time as well so we don't want to cause any disruption to the ongoing traffic and the routing",
    "start": "564240",
    "end": "570360"
  },
  {
    "text": "decisions still need to be made correctly right as I mentioned earlier service owners",
    "start": "570360",
    "end": "576540"
  },
  {
    "text": "control the routing configuration we wanted this to be transparent to the client side so no changes on the client",
    "start": "576540",
    "end": "582600"
  },
  {
    "text": "side we wanted to give this control to the service owners",
    "start": "582600",
    "end": "587660"
  },
  {
    "text": "so we went out and looked at the existing solutions that the service mesh",
    "start": "588140",
    "end": "593580"
  },
  {
    "text": "had to offer now since we were using istio service mesh I'll look at istio examples here it still provides a custom",
    "start": "593580",
    "end": "601320"
  },
  {
    "text": "resource called virtual service which defines a set of traffic rules that",
    "start": "601320",
    "end": "606540"
  },
  {
    "text": "apply when a given host is at rest in this in this example right whenever a",
    "start": "606540",
    "end": "612060"
  },
  {
    "text": "request is made to demo.greeting dot mesh these traffic rules will apply",
    "start": "612060",
    "end": "617519"
  },
  {
    "text": "and each traffic rule defines a protocol and the matching criteria in this example Company ID 1 and 2 those",
    "start": "617519",
    "end": "625620"
  },
  {
    "text": "requests are routed to short one whereas the request for company ID3 are routed to chart 2. this is very",
    "start": "625620",
    "end": "632640"
  },
  {
    "text": "a basic example of a virtual service and but then the virtual service did not",
    "start": "632640",
    "end": "638580"
  },
  {
    "text": "work for us due to some limitations uh due to our use cases uh so there were",
    "start": "638580",
    "end": "645060"
  },
  {
    "text": "maintainability problems what I mean by that is uh if we have to uh incorporate",
    "start": "645060",
    "end": "650399"
  },
  {
    "text": "a routing logic in the virtual service there has to be some kind of coupling between the sharding logic and it has to",
    "start": "650399",
    "end": "657120"
  },
  {
    "text": "go to the virtual service which needs to be created into multiple clusters where the client services exist so this is a",
    "start": "657120",
    "end": "663540"
  },
  {
    "text": "lot of overhead and then also the size of the virtual service object itself can grow a lot",
    "start": "663540",
    "end": "669740"
  },
  {
    "text": "which is again a management problem uh as I mentioned earlier uh we need",
    "start": "669740",
    "end": "676680"
  },
  {
    "text": "near real-time updates imagine a new company getting added and we need that charting info reflected in the virtual",
    "start": "676680",
    "end": "682620"
  },
  {
    "text": "Service uh this is uh this is not possible with the virtual service approach",
    "start": "682620",
    "end": "688440"
  },
  {
    "text": "we have to update the virtual service every time a new company gets added across all the shards again right across",
    "start": "688440",
    "end": "694200"
  },
  {
    "text": "all the client clusters and then if the data can move among the",
    "start": "694200",
    "end": "700860"
  },
  {
    "text": "shards as well virtual Service as you are aware it's a static mapping so we cannot make that decision in the virtual",
    "start": "700860",
    "end": "708480"
  },
  {
    "text": "service so due to these limitations we started designing our solution and I will walk",
    "start": "708480",
    "end": "714420"
  },
  {
    "text": "through that approach what we took in this diagram we'll see a request that is originating on the this is this block is",
    "start": "714420",
    "end": "722640"
  },
  {
    "text": "the client that is making a request to the to a sharded application divided into three different shards",
    "start": "722640",
    "end": "729300"
  },
  {
    "text": "so when an HTTP client makes a request to the sharded application the on-way",
    "start": "729300",
    "end": "736200"
  },
  {
    "text": "proxy intercepts this request as always so onward proxy has a filter chain and",
    "start": "736200",
    "end": "743279"
  },
  {
    "text": "istio provides a custom resource called on-way filter that gives the capability to extend the on web proxy",
    "start": "743279",
    "end": "750360"
  },
  {
    "text": "and we have built the logic in an HTTP filter or HTTP Envy filter that applies",
    "start": "750360",
    "end": "756839"
  },
  {
    "text": "to all the sidecar outbound requests so what this on my filter does is as if you",
    "start": "756839",
    "end": "763440"
  },
  {
    "text": "remember the API Gateway use case the same lookup service is called",
    "start": "763440",
    "end": "769680"
  },
  {
    "text": "to ask where a particular request needs to be routed to based on the request context",
    "start": "769680",
    "end": "775320"
  },
  {
    "text": "and the lookup service responds with the DNS of the chart that it needs to go to then the within the onboard filter we",
    "start": "775320",
    "end": "782519"
  },
  {
    "text": "are able to Route the request to that chart now this creation of the on-way filter we have automated this through",
    "start": "782519",
    "end": "789240"
  },
  {
    "text": "Admiral which is an open source tool under the HTO ecosystem an airspace that",
    "start": "789240",
    "end": "794639"
  },
  {
    "text": "Intuit manages uh so what Admiral does is it you know looks for defines custom",
    "start": "794639",
    "end": "800519"
  },
  {
    "text": "resources that allow it to create on-way filters and other SEO specific resources",
    "start": "800519",
    "end": "805860"
  },
  {
    "text": "in the client clusters so this is how with this solution this",
    "start": "805860",
    "end": "812160"
  },
  {
    "text": "is how the dynamic routing identity looks like this typically Source service and the destination",
    "start": "812160",
    "end": "818639"
  },
  {
    "text": "service live in different clusters and for an admiral lies on top of all the",
    "start": "818639",
    "end": "825480"
  },
  {
    "text": "Clusters it watches all the clusters for the custom resources it defines and it creates",
    "start": "825480",
    "end": "831000"
  },
  {
    "text": "um istio resources in the in the in all the Clusters where it needs to uh to",
    "start": "831000",
    "end": "836579"
  },
  {
    "text": "make the mesh like the life of the mesh operators and developers easier so for",
    "start": "836579",
    "end": "843300"
  },
  {
    "text": "this Dynamic routing use case we introduced a new custom resource called routing policy routing policy uh defines",
    "start": "843300",
    "end": "850800"
  },
  {
    "text": "just the config that needs to go to the onward filter so the onward filter can",
    "start": "850800",
    "end": "856500"
  },
  {
    "text": "be created with that config now Admiral watches for it and it has a mapping of all the Clusters where a",
    "start": "856500",
    "end": "864180"
  },
  {
    "text": "particular Services dependencies exist so it's able to create uh the on my filter and the source source cluster as",
    "start": "864180",
    "end": "870300"
  },
  {
    "text": "well so this is how the routing policy gets",
    "start": "870300",
    "end": "875519"
  },
  {
    "text": "translated to the on by filter now we understand so this this is basically the",
    "start": "875519",
    "end": "881940"
  },
  {
    "text": "Manifest of a routing policy on the right and how Admiral translates it to a",
    "start": "881940",
    "end": "887040"
  },
  {
    "text": "onward filter on the left If You observe the config section is pretty much the same it's just that you know it's",
    "start": "887040",
    "end": "894120"
  },
  {
    "text": "rearranged to fit the on-way filter spec and the onward filter that we create is",
    "start": "894120",
    "end": "899339"
  },
  {
    "text": "also a HTTP filter that matches all the requests that are for the sidecar",
    "start": "899339",
    "end": "904620"
  },
  {
    "text": "outbound context uh now uh now we understand the solution",
    "start": "904620",
    "end": "910079"
  },
  {
    "text": "there were some challenges associated with it that I'll walk through so we use workload selectors which are",
    "start": "910079",
    "end": "918600"
  },
  {
    "text": "provided by on by filter custom resource to match the client workloads where a",
    "start": "918600",
    "end": "923699"
  },
  {
    "text": "particular filter needs to apply to so because the workloads filter can only match one workload as of now so what it",
    "start": "923699",
    "end": "931620"
  },
  {
    "text": "meant for us is that if a given service has 10 different clients in a cluster we",
    "start": "931620",
    "end": "937260"
  },
  {
    "text": "have to create 10 different on-way filters because the or operation was not supported uh we were not able to use",
    "start": "937260",
    "end": "943860"
  },
  {
    "text": "that we also use a tiny go and wasm for",
    "start": "943860",
    "end": "949740"
  },
  {
    "text": "creating a Runway filters due to the limitations that tinygo has it does not",
    "start": "949740",
    "end": "954839"
  },
  {
    "text": "expose full language feature set we could not use an internal cache within the onward filter so we had to create an",
    "start": "954839",
    "end": "961740"
  },
  {
    "text": "external caching service to make the calls to the lookup service more",
    "start": "961740",
    "end": "967199"
  },
  {
    "text": "efficient also the the logic where the envoy",
    "start": "967199",
    "end": "974459"
  },
  {
    "text": "filter makes a call to the lookup service and and then it also retries based on uh you know if the data is",
    "start": "974459",
    "end": "981060"
  },
  {
    "text": "present in a chart or not that logic had to be pre-built in the onward proxy uh we uh we did that because uh so so",
    "start": "981060",
    "end": "990240"
  },
  {
    "text": "anytime we need to update uh update that logic right we have to rebuild the onward proxy and then make sure the",
    "start": "990240",
    "end": "996540"
  },
  {
    "text": "client workloads are rotated um we had to do that because the version of istio we were using earlier did not",
    "start": "996540",
    "end": "1002720"
  },
  {
    "text": "support wasn't plugin but now with the newer versions We can use version plugin which can load all this business logic",
    "start": "1002720",
    "end": "1009380"
  },
  {
    "text": "dynamically so uh that's it for the challenges uh I'll hand it over to uh vinay now who'll",
    "start": "1009380",
    "end": "1016279"
  },
  {
    "text": "show us a live demo thanks pankaj so before I go through the",
    "start": "1016279",
    "end": "1023060"
  },
  {
    "text": "demo I just want to explain a little bit about the demo setup so this is how our demo is set up we have a started",
    "start": "1023060",
    "end": "1030020"
  },
  {
    "text": "application with three shards and we have a company data in the 9000 range in",
    "start": "1030020",
    "end": "1035298"
  },
  {
    "text": "short one 8000 range in shot two and seven thousand range in shot three and we have a call client which is basically",
    "start": "1035299",
    "end": "1042140"
  },
  {
    "text": "requesting data from this chartered application and as you saw before we have a lookup service and a cache or",
    "start": "1042140",
    "end": "1049700"
  },
  {
    "text": "lying in between which gives us the information about the shot so uh let me switch the screen here so",
    "start": "1049700",
    "end": "1057740"
  },
  {
    "text": "this is a kiali dashboard Cali is an observability dashboard which shows uh",
    "start": "1057740",
    "end": "1063679"
  },
  {
    "text": "exactly what a traffic is running within a service mesh and within our cluster so",
    "start": "1063679",
    "end": "1068960"
  },
  {
    "text": "here you don't see any connections because we don't have traffic live traffic yet but you can see all the components that I described we have all",
    "start": "1068960",
    "end": "1075320"
  },
  {
    "text": "the the three shards the client the router and cache and the lookup service",
    "start": "1075320",
    "end": "1080419"
  },
  {
    "text": "so let's go to the configuration first I hope it's visible to everyone so this is",
    "start": "1080419",
    "end": "1087559"
  },
  {
    "text": "um basically the routing policy we are using um the same spec what pankaj explained",
    "start": "1087559",
    "end": "1093320"
  },
  {
    "text": "about so we have a cache section here and we have a host section here so host",
    "start": "1093320",
    "end": "1098539"
  },
  {
    "text": "section is basically the URL we are going to use from the client which gets overwritten to one of The Shard urls",
    "start": "1098539",
    "end": "1105200"
  },
  {
    "text": "and on the left I'm doing a watch so let me rerun The Watch Once More",
    "start": "1105200",
    "end": "1112039"
  },
  {
    "text": "oops so I'm doing a watch on Automotive filters on the client cluster here so",
    "start": "1112039",
    "end": "1117860"
  },
  {
    "text": "this is gonna update automatically because once I create the routing policy",
    "start": "1117860",
    "end": "1123320"
  },
  {
    "text": "Admiral adds a on my filter onto the client cluster so let me create that",
    "start": "1123320",
    "end": "1129980"
  },
  {
    "text": "and here you see a new Envoy filter that got created we can",
    "start": "1129980",
    "end": "1135559"
  },
  {
    "text": "take a look at that on my filter to see how it looks",
    "start": "1135559",
    "end": "1140720"
  },
  {
    "text": "so if you see here I mean pretty much the same configuration it's copied over from the routing policy to the online",
    "start": "1140720",
    "end": "1148220"
  },
  {
    "text": "filter one thing you can uh notice the workload selector workload selector",
    "start": "1148220",
    "end": "1154160"
  },
  {
    "text": "applies only to the client here and not to any other workloads so this is",
    "start": "1154160",
    "end": "1160700"
  },
  {
    "text": "determined by Admiral by maintaining a map next I I have uh I'm telling logs from",
    "start": "1160700",
    "end": "1170000"
  },
  {
    "text": "the different shards so this is Shard one chart two and short three and also",
    "start": "1170000",
    "end": "1176179"
  },
  {
    "text": "I'm telling logs from a lookup service so now we know that Envoy filter got applied to the client so I can run some",
    "start": "1176179",
    "end": "1182840"
  },
  {
    "text": "requests from the client from the client section",
    "start": "1182840",
    "end": "1188480"
  },
  {
    "text": "so let me copy over a request",
    "start": "1188480",
    "end": "1192700"
  },
  {
    "text": "so I'm just doing a slash company and using the same URL as as one we",
    "start": "1193640",
    "end": "1198980"
  },
  {
    "text": "configured before so now first I will search for something in the 9000 range",
    "start": "1198980",
    "end": "1204380"
  },
  {
    "text": "and we expect the request to go to Shard one and the lookup service",
    "start": "1204380",
    "end": "1209720"
  },
  {
    "text": "so here you see a log showing up in lookup Service First it looks up the where the data for 9000 is and then",
    "start": "1209720",
    "end": "1217520"
  },
  {
    "text": "routes it to the short one so rerunning the request multiple times you see it just goes to Shard one because we cache",
    "start": "1217520",
    "end": "1224360"
  },
  {
    "text": "the data inside our inside our service similarly if I try a request say in the",
    "start": "1224360",
    "end": "1232700"
  },
  {
    "text": "8000 range we expect it to go to Shard 2 and you see a a login lookup Service as",
    "start": "1232700",
    "end": "1239059"
  },
  {
    "text": "well as Chart 2 and repeating the request use oops I just did it for it yeah repeating the request you just see",
    "start": "1239059",
    "end": "1245900"
  },
  {
    "text": "it going to chart too similarly I can also do a request on chart 3 which is a",
    "start": "1245900",
    "end": "1251960"
  },
  {
    "text": "7000 range company and yeah you can see that showing up there so now we can see",
    "start": "1251960",
    "end": "1258020"
  },
  {
    "text": "this a little bit better in the kiali dashboard so here you can see client",
    "start": "1258020",
    "end": "1264200"
  },
  {
    "text": "talking to Shard One Shot Two and short three and you see some unknown connection happening to router and",
    "start": "1264200",
    "end": "1271520"
  },
  {
    "text": "lookup right now kiali does not have a ability to show um onoi filter calls so it shows it up",
    "start": "1271520",
    "end": "1279620"
  },
  {
    "text": "as unknown but basically the filter is making a call to Route router and lookup getting the sharp DNS sending it to the",
    "start": "1279620",
    "end": "1287780"
  },
  {
    "text": "or rewriting it for the client and the client communicates to the multiple shards located here",
    "start": "1287780",
    "end": "1295340"
  },
  {
    "text": "so um now let's go back to the slides I just",
    "start": "1295340",
    "end": "1301760"
  },
  {
    "text": "want to talk a little bit about the future work we planned for this we want to also add client-side rate",
    "start": "1301760",
    "end": "1309500"
  },
  {
    "text": "limiting uh using a similar approach of creating on my filter and you know at",
    "start": "1309500",
    "end": "1315260"
  },
  {
    "text": "the client side and rate limiting it there and the services can control that also we wanted to work with the istio",
    "start": "1315260",
    "end": "1322820"
  },
  {
    "text": "community and see if we can enhance the workload selector I just saw before we",
    "start": "1322820",
    "end": "1328400"
  },
  {
    "text": "have to create one workload selector per client if you could find a way to add one workload selector per cluster and",
    "start": "1328400",
    "end": "1335299"
  },
  {
    "text": "apply it to all the clients we would want to explore that approach ah also we use tiny go as our wasm",
    "start": "1335299",
    "end": "1345020"
  },
  {
    "text": "language and we did that um because of more familiarity with golang but tinygo has a limited feature",
    "start": "1345020",
    "end": "1352340"
  },
  {
    "text": "set and we wanted to look at maybe C plus plus and rust to you know enhance the entire",
    "start": "1352340",
    "end": "1359919"
  },
  {
    "text": "solution to better cater for uh for our use case",
    "start": "1359919",
    "end": "1366279"
  },
  {
    "text": "um that's it for the presentation guys uh if you have any feedback please uh take a snapshot of this and let us know",
    "start": "1366620",
    "end": "1374380"
  },
  {
    "text": "also you can talk to us on the Admiral channel in the istio",
    "start": "1374380",
    "end": "1380260"
  },
  {
    "text": "istio slack and also we are open for any questions you have on this",
    "start": "1380260",
    "end": "1386559"
  },
  {
    "text": "thank you we have one here",
    "start": "1387820",
    "end": "1393519"
  },
  {
    "text": "yeah just one give me one second",
    "start": "1394700",
    "end": "1398320"
  },
  {
    "text": "so in the introduction you you mentioned about The Shard application for magnetic applications can you not be more how you",
    "start": "1403760",
    "end": "1411200"
  },
  {
    "text": "Shard your Mana net application to the sharp the the pods",
    "start": "1411200",
    "end": "1416659"
  },
  {
    "text": "uh can you repeat that once more so you mentioned this is a sharded application",
    "start": "1416659",
    "end": "1421760"
  },
  {
    "text": "right for a man and ethnic application right can you explain how you shared",
    "start": "1421760",
    "end": "1427159"
  },
  {
    "text": "that so we charted based on company IDs right now for me company IDs uh which I mean the",
    "start": "1427159",
    "end": "1434120"
  },
  {
    "text": "the sharding logic is a little more complex it's based on business logic but we sharded based on the company IDs",
    "start": "1434120",
    "end": "1439900"
  },
  {
    "text": "so for us Qbo is the application uh which uh is a basically a started",
    "start": "1439900",
    "end": "1446179"
  },
  {
    "text": "application and we started based on company IDs and that could be based on region or based on locality and things",
    "start": "1446179",
    "end": "1453500"
  },
  {
    "text": "like that so it's a complex logic it's not uh just based on IDs of this range",
    "start": "1453500",
    "end": "1458539"
  },
  {
    "text": "so the sharding logic we don't need to you know for for our solution we don't really need to think about what the",
    "start": "1458539",
    "end": "1464840"
  },
  {
    "text": "logic is for sharding that the applications are using we as uh you know mesh operators mesh platform owners what",
    "start": "1464840",
    "end": "1473179"
  },
  {
    "text": "we do is we provide them the capability to have that uh to have to have their",
    "start": "1473179",
    "end": "1478520"
  },
  {
    "text": "charging logic reflect in the lookup service and from mesh standpoint we just make a call to the lookup service and it",
    "start": "1478520",
    "end": "1484880"
  },
  {
    "text": "Returns the chart that is there and it could be the sharding logic itself could be depending upon use case right for",
    "start": "1484880",
    "end": "1491419"
  },
  {
    "text": "example Qbo which charge its data based on the company IDs can do so using the",
    "start": "1491419",
    "end": "1497659"
  },
  {
    "text": "company information and the request and it can identify where a request goes to depending upon the request context right",
    "start": "1497659",
    "end": "1504620"
  },
  {
    "text": "and then another like for example other teams are generated that chart their",
    "start": "1504620",
    "end": "1510980"
  },
  {
    "text": "data based on customer location for instance they can just return you know",
    "start": "1510980",
    "end": "1516140"
  },
  {
    "text": "get the request context based on the origin and stuff like that they can return The Shard information and all",
    "start": "1516140",
    "end": "1522080"
  },
  {
    "text": "this is managed within the lookup service but do you do anything to re-arctic your applications to make them",
    "start": "1522080",
    "end": "1528500"
  },
  {
    "text": "sharded so we didn't uh we I mean if you're",
    "start": "1528500",
    "end": "1535520"
  },
  {
    "text": "asking uh are we moving to regular application to sharded uh that's that",
    "start": "1535520",
    "end": "1541159"
  },
  {
    "text": "wasn't the intention so they were they are built that way as a started application for scaling",
    "start": "1541159",
    "end": "1547159"
  },
  {
    "text": "yeah thank you",
    "start": "1547159",
    "end": "1550000"
  },
  {
    "text": "um I think you mentioned about uh drawbacks of virtual service and limitations for those",
    "start": "1553880",
    "end": "1560720"
  },
  {
    "text": "um uh specifically talking about uh being able to build routes dynamically and",
    "start": "1560720",
    "end": "1568400"
  },
  {
    "text": "possibly growing virtual service um you know did you consider creating",
    "start": "1568400",
    "end": "1576200"
  },
  {
    "text": "or adding these through controllers",
    "start": "1576200",
    "end": "1581600"
  },
  {
    "text": "yeah so I just want to clarify it's not like a drawback of virtual service itself but you know the kind of use",
    "start": "1581600",
    "end": "1588380"
  },
  {
    "text": "cases that we were trying to solve for with virtual service we could not use it for so uh to your question do we use",
    "start": "1588380",
    "end": "1594740"
  },
  {
    "text": "controllers to update the virtual Services as I mentioned we already have Admiral uh that that is like a",
    "start": "1594740",
    "end": "1602419"
  },
  {
    "text": "management plane for the sdo control plane we already had that in place and that was the easiest route to take and",
    "start": "1602419",
    "end": "1609200"
  },
  {
    "text": "it could potentially update the virtual services but then we would have to try the sharding logic",
    "start": "1609200",
    "end": "1615200"
  },
  {
    "text": "admiral has to know the sharding logic and then update the virtual Services accordingly so we we wanted to avoid",
    "start": "1615200",
    "end": "1621140"
  },
  {
    "text": "that complexity thank you did you notice any latency issue or",
    "start": "1621140",
    "end": "1626480"
  },
  {
    "text": "memory a change in memory or CPU usage in the onboard sidecast good question so",
    "start": "1626480",
    "end": "1632659"
  },
  {
    "text": "uh first thing on latency so initially we were doing a lookup for every request",
    "start": "1632659",
    "end": "1638179"
  },
  {
    "text": "that definitely added couple of milliseconds of latency so that's when we introduced our cache we still have",
    "start": "1638179",
    "end": "1644480"
  },
  {
    "text": "subsequent uh latency but it was acceptable for our",
    "start": "1644480",
    "end": "1649580"
  },
  {
    "text": "solution and for ah CPU usage yes we do encode a",
    "start": "1649580",
    "end": "1656120"
  },
  {
    "text": "little bit more whenever you add a new Envoy filter but it was negligible to",
    "start": "1656120",
    "end": "1661520"
  },
  {
    "text": "any other solution we were using there okay first of all great presentation",
    "start": "1661520",
    "end": "1668179"
  },
  {
    "text": "wonderful engineering my question is so on which cluster did you apply the",
    "start": "1668179",
    "end": "1673940"
  },
  {
    "text": "routing policy so uh the routing policy as I said the service owner controls how",
    "start": "1673940",
    "end": "1680659"
  },
  {
    "text": "the sharding happens and then the routing policy happens on the destination cluster where the service is",
    "start": "1680659",
    "end": "1687380"
  },
  {
    "text": "running so the service owner defines the routing policy as they also dictate the",
    "start": "1687380",
    "end": "1692840"
  },
  {
    "text": "sharding logic so that's the config that gets translated to the onward filter and",
    "start": "1692840",
    "end": "1698360"
  },
  {
    "text": "so does so do they apply it on one sharded cluster or an admiral takes care",
    "start": "1698360",
    "end": "1704299"
  },
  {
    "text": "of the rest or do they apply on every cluster yeah Admiral knows where a",
    "start": "1704299",
    "end": "1709460"
  },
  {
    "text": "particular services dependencies are there's another custom resource that Admiral defines it's called dependency",
    "start": "1709460",
    "end": "1715400"
  },
  {
    "text": "that is how you know when a service gets created there's also a dependency custom",
    "start": "1715400",
    "end": "1721039"
  },
  {
    "text": "resource they Define which tells them what all clients it needs to apply to",
    "start": "1721039",
    "end": "1726200"
  },
  {
    "text": "and each workload that is running in our multi-cluster setup has an identity associated with it and it's Unique per",
    "start": "1726200",
    "end": "1734840"
  },
  {
    "text": "service so Admiral knows what all clusters uh the clients of that application are running in and that's",
    "start": "1734840",
    "end": "1740960"
  },
  {
    "text": "how it's able to you know create the envoy filter in those clusters right it can do so in all of them to your answer",
    "start": "1740960",
    "end": "1746480"
  },
  {
    "text": "thank you",
    "start": "1746480",
    "end": "1749200"
  },
  {
    "text": "um so in the client in the filter that you built you need to extract the the ID",
    "start": "1756260",
    "end": "1762500"
  },
  {
    "text": "to Route the chart or throughout the request to how are you doing that extraction and doesn't that tightly",
    "start": "1762500",
    "end": "1769220"
  },
  {
    "text": "couple your client filter we don't we don't yeah so so the entire logic of",
    "start": "1769220",
    "end": "1775340"
  },
  {
    "text": "that was in the lookup service we just forward the path of the entire request path to the lookup service yeah python",
    "start": "1775340",
    "end": "1782720"
  },
  {
    "text": "headers and lookup service gives it back that's our implementation but I mean as you said we could you could uh you know",
    "start": "1782720",
    "end": "1789679"
  },
  {
    "text": "regex it and break it up and send it so yeah so our solution just involves lookup service to take care of that",
    "start": "1789679",
    "end": "1795740"
  },
  {
    "text": "information yeah so our solution provides you a capability to define a routing policy which creates an HTTP",
    "start": "1795740",
    "end": "1801919"
  },
  {
    "text": "sidecar outbound filter on the clients that that service has right what you do",
    "start": "1801919",
    "end": "1807500"
  },
  {
    "text": "with that on my filter is you know depending dependent upon your use case okay",
    "start": "1807500",
    "end": "1814240"
  },
  {
    "text": "uh you mentioned why um the virtual routes would not work for",
    "start": "1817580",
    "end": "1824179"
  },
  {
    "text": "your use case are there any other methods that you tried or explored that didn't work for any particular reason",
    "start": "1824179",
    "end": "1832399"
  },
  {
    "text": "I mean have you tried one more but it wasn't really for this use case it was the authent based onward filter but it",
    "start": "1832399",
    "end": "1840980"
  },
  {
    "text": "wasn't really for this use case but it you can do an external call to an authentication Service and get and get",
    "start": "1840980",
    "end": "1847640"
  },
  {
    "text": "it back but that didn't give us that doesn't give you a metadata back it just gives you a 200 okay and forwards the",
    "start": "1847640",
    "end": "1853399"
  },
  {
    "text": "request so that kind of didn't work for our favor so yeah that's right also we",
    "start": "1853399",
    "end": "1858980"
  },
  {
    "text": "we wanted to uh you know if any other Solutions did not give us the",
    "start": "1858980",
    "end": "1864440"
  },
  {
    "text": "flexibility to adapt our solution for multiple use cases as I mentioned right",
    "start": "1864440",
    "end": "1870500"
  },
  {
    "text": "the Qbo recharges data based on a company and then other multiple other",
    "start": "1870500",
    "end": "1876260"
  },
  {
    "text": "applications that attend to it with millions of users they chart their data based on user data so we wanted to",
    "start": "1876260",
    "end": "1882919"
  },
  {
    "text": "support those and routing policy allows us to give this flexibility to the service owners to define the config they",
    "start": "1882919",
    "end": "1889279"
  },
  {
    "text": "want to use and also you know Define the lookup service that that needs to be used for that",
    "start": "1889279",
    "end": "1895580"
  },
  {
    "text": "yeah if you guys want to try this out uh uh we have we will push this up onto the Admiral repo which part of the",
    "start": "1895580",
    "end": "1903020"
  },
  {
    "text": "um ecosystem so uh yeah this feature is live on Admiral so you guys can play",
    "start": "1903020",
    "end": "1908779"
  },
  {
    "text": "with it as well all right there's a question yeah so I have one question regarding so when the source cluster you",
    "start": "1908779",
    "end": "1914899"
  },
  {
    "text": "have created an online filter right there is a VM underscore code section and you have additive binary called",
    "start": "1914899",
    "end": "1920899"
  },
  {
    "text": "vasum Vasan binary right how do you how do you guys are storing that binary in the uh yes we had some issue with binary",
    "start": "1920899",
    "end": "1928580"
  },
  {
    "text": "storing the container because we build the binary and we created some config map and the config Mark supports are",
    "start": "1928580",
    "end": "1935179"
  },
  {
    "text": "like 100 MB of some Maximum if it is more than that it cannot even store",
    "start": "1935179",
    "end": "1940520"
  },
  {
    "text": "right yeah what we did it basically we use the nginx nginx proxy and we put all the binaries in there and then we are",
    "start": "1940520",
    "end": "1947360"
  },
  {
    "text": "calling that nginx ysm binary from the container so how you guys like if the binary is like more than 100 MBR like",
    "start": "1947360",
    "end": "1954679"
  },
  {
    "text": "how your guys are like yeah using I mean a very good question um so for this solution right uh I'm not",
    "start": "1954679",
    "end": "1962059"
  },
  {
    "text": "sure if this is what you want to hear but for this solution we pre-built the envoy proxy",
    "start": "1962059",
    "end": "1967940"
  },
  {
    "text": "um we we have a we just do a copy of the awesome binary into the envoy proxy",
    "start": "1967940",
    "end": "1973700"
  },
  {
    "text": "image and we just reference it in our Envoy filter yeah so this is the the",
    "start": "1973700",
    "end": "1980779"
  },
  {
    "text": "reason for this is we used 1.10 and Below istio when we designed the solution and that was the approach we",
    "start": "1980779",
    "end": "1987320"
  },
  {
    "text": "took but wasn't plugins would give you that opportunity right we tried it but at that time dust was like",
    "start": "1987320",
    "end": "1993260"
  },
  {
    "text": "authentication problem was issues so I yeah version plug-in we did some pocs",
    "start": "1993260",
    "end": "2000820"
  },
  {
    "text": "with it it worked for us we pull it from something like artifactory or a Docker Hub and it works just fine yeah and and",
    "start": "2000820",
    "end": "2008140"
  },
  {
    "text": "did you mention that there's a limit of 100 MB for that plug-in uh to be loaded no I'm saying kubernetes we've tried",
    "start": "2008140",
    "end": "2015159"
  },
  {
    "text": "with like uh config Maps right oh the config map is exceeding the limit that is CD allows yeah our advising binary",
    "start": "2015159",
    "end": "2022419"
  },
  {
    "text": "was more than 100 mbu and it was not fine got it okay and if you're writing a awesome plug-in more than 100 MB that's",
    "start": "2022419",
    "end": "2028899"
  },
  {
    "text": "that's pretty much I mean I don't know I'm just saying okay pretty big",
    "start": "2028899",
    "end": "2035140"
  },
  {
    "text": "I do believe that's all the time we have for questions today thank you thank you thank you thank you",
    "start": "2035140",
    "end": "2041398"
  }
]