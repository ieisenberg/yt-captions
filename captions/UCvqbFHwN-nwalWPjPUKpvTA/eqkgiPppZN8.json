[
  {
    "start": "0",
    "end": "32000"
  },
  {
    "text": "all right everybody we're gonna go ahead and get started today we're gonna talk about kubernetes local persistent",
    "start": "0",
    "end": "7259"
  },
  {
    "text": "volumes in production and if you sat through Sods talk it's a great introduction to kubernetes storage and",
    "start": "7259",
    "end": "13710"
  },
  {
    "text": "we're really going to kind of build on that and talk a lot about local persistent volumes Michelle is the",
    "start": "13710",
    "end": "20039"
  },
  {
    "text": "primary developer and really the lead when it comes to local persistent volumes and kubernetes she's been",
    "start": "20039",
    "end": "25410"
  },
  {
    "text": "working on it for more than two years and I've spent more than a year working on it as well so today we're gonna",
    "start": "25410",
    "end": "30689"
  },
  {
    "text": "actually tell you some of the things about how you will leverage or how you can leverage and some of the tricks for",
    "start": "30689",
    "end": "36660"
  },
  {
    "start": "32000",
    "end": "32000"
  },
  {
    "text": "leveraging local persistent volumes in kubernetes the agenda is I'm gonna start",
    "start": "36660",
    "end": "41730"
  },
  {
    "text": "out by talking about why kubernetes and also why local storage and some of the work that we've done at Salesforce over",
    "start": "41730",
    "end": "48390"
  },
  {
    "text": "the last year then we're gonna dive into local persistent volumes and give you more",
    "start": "48390",
    "end": "53789"
  },
  {
    "text": "information about when you should leverage them when you should not what is good about them what some of the",
    "start": "53789",
    "end": "59370"
  },
  {
    "text": "caveats are and then the bulk of the talk is going to be about the local volume life cycle so part of the life",
    "start": "59370",
    "end": "67200"
  },
  {
    "text": "cycle saag talked about when it comes to persistent volumes and persistent volume claims but with local volumes there's",
    "start": "67200",
    "end": "73680"
  },
  {
    "text": "some additional work that administrators need to do to set up and handle the life",
    "start": "73680",
    "end": "79140"
  },
  {
    "text": "cycle of these local volumes and we're going to talk a lot about that and something about node preparation",
    "start": "79140",
    "end": "84720"
  },
  {
    "text": "essentially preparing these local volumes for use in kubernetes then i'm",
    "start": "84720",
    "end": "90570"
  },
  {
    "text": "going to give a brief demo about some of our node preparation at Salesforce and then we're gonna close out by talking",
    "start": "90570",
    "end": "97200"
  },
  {
    "text": "about the future roadmap as well as summarizing the talk so why are we using",
    "start": "97200",
    "end": "106380"
  },
  {
    "start": "103000",
    "end": "103000"
  },
  {
    "text": "kubernetes and local volumes of salesforce at salesforce we are seeing an extreme growth in the demand for",
    "start": "106380",
    "end": "114590"
  },
  {
    "text": "storage and we ourselves at salesforce are consuming that storage as well as",
    "start": "114590",
    "end": "120570"
  },
  {
    "text": "we're storing things on behalf of all of our customers and really this isn't looking like a linear Kirk graph you",
    "start": "120570",
    "end": "127079"
  },
  {
    "text": "know this is going or going growing like a hockey stick and it's the responsibility of the",
    "start": "127079",
    "end": "133890"
  },
  {
    "text": "infrastructure teams and one of the teams that I was on to kind of keep ahead of this growth curve and just to",
    "start": "133890",
    "end": "139710"
  },
  {
    "text": "give you an example of the type of growth that we're seeing we're seeing a growth rate that's higher than our",
    "start": "139710",
    "end": "145530"
  },
  {
    "text": "stateless workloads especially for resources for storage and so we have",
    "start": "145530",
    "end": "151500"
  },
  {
    "text": "more than doubled our kubernetes production fleet over the last year and we've actually really energized more",
    "start": "151500",
    "end": "158340"
  },
  {
    "text": "than 10 petabytes of storage capacity that is made available through kubernetes and the storage services",
    "start": "158340",
    "end": "164700"
  },
  {
    "text": "themselves are running on kubernetes so why did we choose kubernetes and why are",
    "start": "164700",
    "end": "172800"
  },
  {
    "start": "168000",
    "end": "168000"
  },
  {
    "text": "we leveraging it for running storage services so last year Steve Sankey from",
    "start": "172800",
    "end": "178920"
  },
  {
    "text": "Salesforce came and talked about our work with stateless workloads and so we",
    "start": "178920",
    "end": "184530"
  },
  {
    "text": "have had experience running stateless workloads and the developers love it we love the whole workflow for kubernetes",
    "start": "184530",
    "end": "191820"
  },
  {
    "text": "and its deployment of services the way that you develop the way that you manage",
    "start": "191820",
    "end": "196890"
  },
  {
    "text": "them as you go forward and the scalability associated with them and so things like a manifest check in to",
    "start": "196890",
    "end": "204630"
  },
  {
    "text": "production in tens of minutes are really powerful when you compare them with",
    "start": "204630",
    "end": "209700"
  },
  {
    "text": "legacy imperative mechanisms related to deployment and management of these services including the build phases as",
    "start": "209700",
    "end": "216420"
  },
  {
    "text": "well and so we wanted to provide that same type of experience to our storage",
    "start": "216420",
    "end": "222030"
  },
  {
    "text": "services as I mentioned we have a lot of different storage services that store the data that arrives into Salesforce",
    "start": "222030",
    "end": "229530"
  },
  {
    "text": "and so these storage service owners want that same type of experience and some of",
    "start": "229530",
    "end": "235290"
  },
  {
    "text": "the key things that we needed to do there were to leverage our local storage we are all in the cloud much in our",
    "start": "235290",
    "end": "242340"
  },
  {
    "text": "first party data centers as well as across many of the different public clouds and so leveraging that local",
    "start": "242340",
    "end": "249239"
  },
  {
    "text": "storage was really key for providing these storage services and some of the",
    "start": "249239",
    "end": "254280"
  },
  {
    "text": "other things we loved about kubernetes were things like immutable containers declarative manifests and the active",
    "start": "254280",
    "end": "260549"
  },
  {
    "text": "reconciliation that's happening and these are really key for that scale aspect so when we want to increase the",
    "start": "260549",
    "end": "268210"
  },
  {
    "text": "cluster size it's a manifest change when we want when a note goes down active",
    "start": "268210",
    "end": "273699"
  },
  {
    "text": "reconciliation takes care of it so kubernetes has been a really an enabler",
    "start": "273699",
    "end": "279639"
  },
  {
    "text": "for our storage services all right so if",
    "start": "279639",
    "end": "284889"
  },
  {
    "start": "282000",
    "end": "282000"
  },
  {
    "text": "you are if you attended a sods talked just before this you can see that it's",
    "start": "284889",
    "end": "293289"
  },
  {
    "text": "really easy in kubernetes today to access remote storage by using persistent volume claims in storage",
    "start": "293289",
    "end": "300070"
  },
  {
    "text": "classes and dynamic provisioning but there are some reasons why you might",
    "start": "300070",
    "end": "305680"
  },
  {
    "text": "still want to use local storage in your environment for one reason if you have",
    "start": "305680",
    "end": "311050"
  },
  {
    "text": "high performance workloads that need direct access to SSDs to bypass the",
    "start": "311050",
    "end": "316479"
  },
  {
    "text": "network another reason is that you might want to save some costs because local disks are generally cheaper than their",
    "start": "316479",
    "end": "323259"
  },
  {
    "text": "remote counterparts and especially in bare metal environments you might have a lot of spare disks lying around that you",
    "start": "323259",
    "end": "330370"
  },
  {
    "text": "want to use for more efficient and better utilization but there are",
    "start": "330370",
    "end": "335590"
  },
  {
    "start": "335000",
    "end": "335000"
  },
  {
    "text": "definitely major trade-offs you need to consider if you're going to use local storage in your pods first is now your",
    "start": "335590",
    "end": "344199"
  },
  {
    "text": "pods will become less flexible in terms of placement because now your pod you're",
    "start": "344199",
    "end": "349750"
  },
  {
    "text": "tying your pod to the availability of your node and your disk so if your node",
    "start": "349750",
    "end": "355270"
  },
  {
    "text": "or disc becomes unavailable or fails your pod is also going to become unavailable until you can resolve that",
    "start": "355270",
    "end": "361539"
  },
  {
    "text": "issue so that potentially results in lower availability for your pod and in",
    "start": "361539",
    "end": "367990"
  },
  {
    "text": "addition local disks are generally not replicated so you might encounter lower",
    "start": "367990",
    "end": "374259"
  },
  {
    "text": "data durability as well in many cloud environments the local disk offerings",
    "start": "374259",
    "end": "381310"
  },
  {
    "text": "are basically offer very low data durability guarantees if the VM dies",
    "start": "381310",
    "end": "389320"
  },
  {
    "text": "then all the data on the local SSD is wiped out so you need to be really",
    "start": "389320",
    "end": "395080"
  },
  {
    "text": "careful if you want to consider using local persistent storage and you",
    "start": "395080",
    "end": "401080"
  },
  {
    "text": "have to really understand the trade-offs and make sure that your workloads can",
    "start": "401080",
    "end": "406900"
  },
  {
    "text": "handle all of these trade-offs so for that reason local storage is definitely",
    "start": "406900",
    "end": "413290"
  },
  {
    "text": "not a general purpose solution and it is really only suitable for very specific use cases one such use case is those for",
    "start": "413290",
    "end": "422380"
  },
  {
    "start": "420000",
    "end": "420000"
  },
  {
    "text": "distributed data stores these workloads typically shard and replicate their data",
    "start": "422380",
    "end": "428290"
  },
  {
    "text": "across multiple notes so that they can handle potential node failure or data loss on any of those notes there are",
    "start": "428290",
    "end": "435480"
  },
  {
    "text": "quite a few applications that fall into this category including SEF Cassandra",
    "start": "435480",
    "end": "442180"
  },
  {
    "text": "cluster HDFS etc another suitable class",
    "start": "442180",
    "end": "447400"
  },
  {
    "text": "of applications are those that require high performance on disk caches",
    "start": "447400",
    "end": "453630"
  },
  {
    "text": "typically for those because it's because it's caching it it can survive handling",
    "start": "453630",
    "end": "461560"
  },
  {
    "text": "losing that data or having the node fail because you can always restore that data from its source but they still want some",
    "start": "461560",
    "end": "469780"
  },
  {
    "text": "semi persistence of the data so that they can avoid the cold we start issue",
    "start": "469780",
    "end": "475270"
  },
  {
    "text": "and avoid having to reload its cache from the source every time a POC restarts some examples of these include",
    "start": "475270",
    "end": "483419"
  },
  {
    "text": "data analytics so those are some",
    "start": "483419",
    "end": "489310"
  },
  {
    "text": "examples of applications that are suitable for using local storage it's",
    "start": "489310",
    "end": "494590"
  },
  {
    "text": "not a comprehensive list but those are the major ones that we have been targeting as part of this feature so the",
    "start": "494590",
    "end": "504010"
  },
  {
    "text": "main problem is is that before we develop the local persistent volumes feature it was very difficult to",
    "start": "504010",
    "end": "510850"
  },
  {
    "text": "actually run these types of workloads on top of local storage in kubernetes to",
    "start": "510850",
    "end": "516760"
  },
  {
    "start": "516000",
    "end": "516000"
  },
  {
    "text": "give some background on that previously the only way to access local volumes in",
    "start": "516760",
    "end": "522849"
  },
  {
    "text": "kubernetes was to use something called a host path volume here you are specifying",
    "start": "522849",
    "end": "528640"
  },
  {
    "text": "the path to your filesystem correctly in your pod spec and this has",
    "start": "528640",
    "end": "533650"
  },
  {
    "text": "a lot of problems first this is not secure because you are allowing your pod",
    "start": "533650",
    "end": "539770"
  },
  {
    "text": "aka your user to specify any path in your cluster so you need to really trust",
    "start": "539770",
    "end": "546910"
  },
  {
    "text": "that your user doesn't make a mistake and also trust that your user you know",
    "start": "546910",
    "end": "551980"
  },
  {
    "text": "you have to you have to trust your user to choose the right path this is also",
    "start": "551980",
    "end": "558490"
  },
  {
    "text": "not portable because you've encoded your node specific paths into your pods back",
    "start": "558490",
    "end": "564790"
  },
  {
    "text": "so if you wanted to take your pod and run it on a different cloud or you know a different cluster that has its node",
    "start": "564790",
    "end": "571810"
  },
  {
    "text": "setup node and disk setup all completely different you would have to go and manually change all the pod specs to",
    "start": "571810",
    "end": "579670"
  },
  {
    "text": "conform to the disk layout on your new environment so if you imagine having to",
    "start": "579670",
    "end": "585900"
  },
  {
    "text": "manage like thousands of these pod specs and you want it like and then you want",
    "start": "585900",
    "end": "591100"
  },
  {
    "text": "to port your workload over to a new cluster you have to go in and manually",
    "start": "591100",
    "end": "596410"
  },
  {
    "text": "change all those thousand specs to conform to your new environment",
    "start": "596410",
    "end": "602010"
  },
  {
    "text": "similarly post path volumes doesn't it doesn't really provide any way to do",
    "start": "602010",
    "end": "608580"
  },
  {
    "text": "scalable disk accounting from here you can't really tell what disks are",
    "start": "608580",
    "end": "614740"
  },
  {
    "text": "available in your cluster and it's really hard to tell what disks are being used by other pods in your system so all",
    "start": "614740",
    "end": "623980"
  },
  {
    "text": "of these problems put together make needed really hard to use local storage",
    "start": "623980",
    "end": "630270"
  },
  {
    "text": "in production at any sort of scale what many workloads have had to do is to",
    "start": "630270",
    "end": "638020"
  },
  {
    "text": "write very complex operators that basically implement a disk reservation",
    "start": "638020",
    "end": "643420"
  },
  {
    "text": "and management system and also have to implement manual scheduling of pods to",
    "start": "643420",
    "end": "649270"
  },
  {
    "text": "those individual nodes and when you start having to manage individual pods the major downside there is you can't",
    "start": "649270",
    "end": "657160"
  },
  {
    "text": "take advantage of existing high level controllers and kubernetes such as",
    "start": "657160",
    "end": "662320"
  },
  {
    "text": "stateful sets so if you wanted to if you wanted to do",
    "start": "662320",
    "end": "667330"
  },
  {
    "text": "if you wanted to handle scaling or upgrades or health checks or any of the",
    "start": "667330",
    "end": "672760"
  },
  {
    "text": "roll backs and any of those other stateful set features you would have to end up we implementing those in your in",
    "start": "672760",
    "end": "679540"
  },
  {
    "text": "your custom operator so the local persistent volumes feature aims to fix",
    "start": "679540",
    "end": "686980"
  },
  {
    "text": "all of those issues by leveraging the existing persistent volumes interface",
    "start": "686980",
    "end": "692950"
  },
  {
    "text": "that we have in kubernetes today by doing this you now get security",
    "start": "692950",
    "end": "698890"
  },
  {
    "text": "because only administrators can create and define the content of the persistent",
    "start": "698890",
    "end": "705430"
  },
  {
    "text": "volumes you get portability because the persistent volume claims will add a",
    "start": "705430",
    "end": "711550"
  },
  {
    "text": "level of indirection in between the pod and the underlying storage implementation so if you look at this",
    "start": "711550",
    "end": "718780"
  },
  {
    "text": "speck here you've noticed now so if I go back to the hosts path spec I was",
    "start": "718780",
    "end": "724630"
  },
  {
    "text": "encoding specific paths in my cluster in my pod but now if I use a persistent",
    "start": "724630",
    "end": "731140"
  },
  {
    "start": "730000",
    "end": "730000"
  },
  {
    "text": "volume claim it's completely portable there's nothing in here that gives any",
    "start": "731140",
    "end": "736630"
  },
  {
    "text": "hints of any particular you know environment or node setup in addition",
    "start": "736630",
    "end": "743370"
  },
  {
    "text": "persistent volumes and persistent volume claims gives a better way to do disk accounting in kubernetes because every",
    "start": "743370",
    "end": "750850"
  },
  {
    "text": "single disk is explicitly exposed as a persistent volume and those persistent",
    "start": "750850",
    "end": "757000"
  },
  {
    "text": "volumes have statuses of available or bound so you can easily tell which disks",
    "start": "757000",
    "end": "763300"
  },
  {
    "text": "are available in your system and which disks are being used by exactly which claims and which pots so by and by using",
    "start": "763300",
    "end": "775150"
  },
  {
    "text": "the persistent volume claims objects now you can really take advantage of the",
    "start": "775150",
    "end": "781089"
  },
  {
    "text": "powerful features built into stateful sets the stateful sets include volume",
    "start": "781089",
    "end": "788290"
  },
  {
    "text": "claim templates built-in which gives you one volume per stateful set instance and",
    "start": "788290",
    "end": "795100"
  },
  {
    "text": "it automatically handles things like scale up and scale down so if there are existing operators or helm",
    "start": "795100",
    "end": "803000"
  },
  {
    "text": "charts using stateful sets that were previously just tested against remote",
    "start": "803000",
    "end": "808399"
  },
  {
    "text": "storage now you can easily leverage those same stateful sets and just switch",
    "start": "808399",
    "end": "813620"
  },
  {
    "text": "to local storage just by changing the storage class name so this feature this",
    "start": "813620",
    "end": "822680"
  },
  {
    "start": "821000",
    "end": "821000"
  },
  {
    "text": "local persistent volumes feature is now available in beta as of 1:10 there are a",
    "start": "822680",
    "end": "829250"
  },
  {
    "text": "few limitations for this initial beta phase first you must format and pre mount the",
    "start": "829250",
    "end": "836449"
  },
  {
    "text": "disks first on the notes and we don't support dynamic provisioning yet but those those two items are under active",
    "start": "836449",
    "end": "843920"
  },
  {
    "text": "development currently and also as part of this feature we made significant",
    "start": "843920",
    "end": "849589"
  },
  {
    "text": "enhancements to the scheduler first to provide data gravity so what this means",
    "start": "849589",
    "end": "854959"
  },
  {
    "text": "is that if you have a pod that's using a persistent volume claim that points to a",
    "start": "854959",
    "end": "861579"
  },
  {
    "text": "local volume the scheduler is smart enough to understand that this local",
    "start": "861579",
    "end": "867709"
  },
  {
    "text": "volume is tied to a specific node and will always schedule your pod to that",
    "start": "867709",
    "end": "872839"
  },
  {
    "text": "correct node and on the other side we've also made the pod scheduling and initial",
    "start": "872839",
    "end": "881750"
  },
  {
    "text": "volume binding operations more tightly coupled together so that the scheduler",
    "start": "881750",
    "end": "888079"
  },
  {
    "text": "and the volume controller can coordinate better when scheduling pods so now",
    "start": "888079",
    "end": "894290"
  },
  {
    "text": "whenever we we are whenever the volume controller is trying to find a volume to",
    "start": "894290",
    "end": "901130"
  },
  {
    "text": "bind the claim to it's not only going to look at all the volumes available on which nodes it's also going to look at",
    "start": "901130",
    "end": "907910"
  },
  {
    "text": "your pod requirements such as CPU memory affinity and high affinity nodes tints",
    "start": "907910",
    "end": "914690"
  },
  {
    "text": "and Toleration it's going to look at everything along with all the local volumes that are available and be able",
    "start": "914690",
    "end": "921949"
  },
  {
    "text": "to choose an appropriate node that can access the storage you need and also run your pod that same logic also",
    "start": "921949",
    "end": "930430"
  },
  {
    "text": "handles a case where if you specify multiple claims in a pod we make sure that we will bind to local volumes that",
    "start": "930430",
    "end": "938440"
  },
  {
    "text": "are all on the same note so that's some general back background on the future",
    "start": "938440",
    "end": "945210"
  },
  {
    "text": "now for the rest of the talk we're going to focus on the local volume life cycle",
    "start": "945210",
    "end": "950490"
  },
  {
    "text": "from when you initially set up your cluster and your disks to win your pods",
    "start": "950490",
    "end": "955750"
  },
  {
    "text": "end up consuming and releasing the persistent volumes we've split up the",
    "start": "955750",
    "end": "962410"
  },
  {
    "start": "961000",
    "end": "961000"
  },
  {
    "text": "phase into the life cycle into two main phases the first phase is the node and",
    "start": "962410",
    "end": "968950"
  },
  {
    "text": "disk preparation parts this is specific this is specific to your environment and",
    "start": "968950",
    "end": "974470"
  },
  {
    "text": "it involves things like partitioning or rating the disks formatting and mounting",
    "start": "974470",
    "end": "980920"
  },
  {
    "text": "them once the environment has been prepared now it's ready for use in",
    "start": "980920",
    "end": "987339"
  },
  {
    "text": "kubernetes and it's going to enter the second phase which is the persistent volume life cycle we've written a open source",
    "start": "987339",
    "end": "996640"
  },
  {
    "text": "controller that can automate this part of the life cycle for you and it's going",
    "start": "996640",
    "end": "1001709"
  },
  {
    "text": "to handle things like creating the persistent volumes and then when the",
    "start": "1001709",
    "end": "1007320"
  },
  {
    "text": "pods end up consuming and releasing these volumes then it's going to clean",
    "start": "1007320",
    "end": "1013050"
  },
  {
    "text": "them up delete them and then put them back into the kubernetes pool for use so",
    "start": "1013050",
    "end": "1020520"
  },
  {
    "text": "we're going to deep dive now into each of these phases so going back to the first phase which is the node and disk",
    "start": "1020520",
    "end": "1026550"
  },
  {
    "text": "preparation this is so you might be asking why is this part environment",
    "start": "1026550",
    "end": "1031980"
  },
  {
    "text": "specific and why can't kubernetes do this part for me the main challenge here",
    "start": "1031980",
    "end": "1038308"
  },
  {
    "text": "is that there are so many different choices in methods that you could use to",
    "start": "1038309",
    "end": "1043558"
  },
  {
    "text": "configure your local disks you can partition them you can you know do any of these raid configurations you can use",
    "start": "1043559",
    "end": "1050250"
  },
  {
    "text": "LEM there might be proprietary methods and setup that you need to do and it",
    "start": "1050250",
    "end": "1055800"
  },
  {
    "text": "would be very difficult to encode all of this into the API kubernetes api so we",
    "start": "1055800",
    "end": "1063030"
  },
  {
    "text": "chose to the API very simple and just have a path",
    "start": "1063030",
    "end": "1069059"
  },
  {
    "text": "to the mounted volume passed in and then leave all the steps before that on how",
    "start": "1069059",
    "end": "1075929"
  },
  {
    "text": "you actually get to that path up to the environment so going back to all of",
    "start": "1075929",
    "end": "1082470"
  },
  {
    "text": "these choices you might have what what which choice should you choose or which",
    "start": "1082470",
    "end": "1088320"
  },
  {
    "text": "you know which one or number of combination of these should you choose the answer really depends on what",
    "start": "1088320",
    "end": "1097710"
  },
  {
    "text": "workloads you intend to run and support using local storage for example if your",
    "start": "1097710",
    "end": "1104309"
  },
  {
    "text": "workload requires higher data durability you might consider rating the disks",
    "start": "1104309",
    "end": "1109620"
  },
  {
    "text": "together but if you are just if the workload is just using just once the",
    "start": "1109620",
    "end": "1115919"
  },
  {
    "text": "fastest disk available then you'll give it the whole disk a lot of it also",
    "start": "1115919",
    "end": "1121320"
  },
  {
    "text": "depends on the platform and ops requirements in your environment so if",
    "start": "1121320",
    "end": "1127409"
  },
  {
    "text": "say your workload requires more capacity that's bigger than any disk capacity you",
    "start": "1127409",
    "end": "1134970"
  },
  {
    "text": "have then you may also want to consider aggregating the disks together into a larger logical volume to use you might",
    "start": "1134970",
    "end": "1143100"
  },
  {
    "text": "be limited by the number of disks available on your nodes the capacity of",
    "start": "1143100",
    "end": "1148890"
  },
  {
    "text": "those disks and also the speeds of those disks there might be there might be",
    "start": "1148890",
    "end": "1155159"
  },
  {
    "text": "special reserved disks on your hosts that you don't want kubernetes to touch and you might be bound by existing",
    "start": "1155159",
    "end": "1162929"
  },
  {
    "text": "processes and tools in your company for the node setup such as ansible",
    "start": "1162929",
    "end": "1168059"
  },
  {
    "text": "or puppet you might need to integrate consider integrating all this all these",
    "start": "1168059",
    "end": "1173669"
  },
  {
    "text": "steps into that or potentially not so putting all of these factors together is",
    "start": "1173669",
    "end": "1180539"
  },
  {
    "text": "really what's going to determine what is the best way to set up the local disks",
    "start": "1180539",
    "end": "1185700"
  },
  {
    "text": "in your environment so now we're going to go we're going to give some specific",
    "start": "1185700",
    "end": "1192120"
  },
  {
    "text": "examples of how this node preparation step is done in some real environments here so I'm",
    "start": "1192120",
    "end": "1199830"
  },
  {
    "start": "1199000",
    "end": "1199000"
  },
  {
    "text": "gonna start by talking about how note preparation is done in gke I would",
    "start": "1199830",
    "end": "1205980"
  },
  {
    "text": "consider this to be a very simple use case because gke is a cloud provider",
    "start": "1205980",
    "end": "1213360"
  },
  {
    "text": "platform the disk naming of the local SSDs are very well defined across all",
    "start": "1213360",
    "end": "1220770"
  },
  {
    "text": "machine types so what we've done here is we've written a very simple shell script",
    "start": "1220770",
    "end": "1227880"
  },
  {
    "text": "that runs whenever that runs whenever the VM boots up and before kubernetes",
    "start": "1227880",
    "end": "1233760"
  },
  {
    "text": "actually starts and it just looks for disks at predefined locations and then",
    "start": "1233760",
    "end": "1241380"
  },
  {
    "text": "it will go ahead format and mount them into directories that kubernetes can understand later and because also this",
    "start": "1241380",
    "end": "1250440"
  },
  {
    "text": "is a cloud provider service we've actually included this node preparation",
    "start": "1250440",
    "end": "1257640"
  },
  {
    "text": "step into the whole cluster bring up so as a gke end user you don't actually",
    "start": "1257640",
    "end": "1263550"
  },
  {
    "text": "have to do the node prep you just have to create a cluster with some local",
    "start": "1263550",
    "end": "1269070"
  },
  {
    "text": "disks and by the time the clusters up and you can log into it all the disks",
    "start": "1269070",
    "end": "1274440"
  },
  {
    "text": "have been prepared and formatted into the correct locations so again I would",
    "start": "1274440",
    "end": "1281280"
  },
  {
    "text": "consider this a pretty simple example on the spectrum now I'm gonna hand it over",
    "start": "1281280",
    "end": "1287309"
  },
  {
    "text": "to Ian who's going to give an example of a more complicated environment alright",
    "start": "1287309",
    "end": "1294600"
  },
  {
    "text": "so I'm gonna talk about a more complicated environment I'm gonna talk about first party cloud data centers for",
    "start": "1294600",
    "end": "1303120"
  },
  {
    "start": "1301000",
    "end": "1301000"
  },
  {
    "text": "Salesforce and some of the things we encountered and also some of the design decisions that we made I'd mentioned",
    "start": "1303120",
    "end": "1309540"
  },
  {
    "text": "earlier about the scale at which were operating and that's very important but also we have a very heterogeneous set of",
    "start": "1309540",
    "end": "1316200"
  },
  {
    "text": "environments so one size does not fit all as well as a whole suite of processes and tools around provisioning",
    "start": "1316200",
    "end": "1323820"
  },
  {
    "text": "around accounting around financial items the different disk layouts and configurations",
    "start": "1323820",
    "end": "1329850"
  },
  {
    "text": "so we have a lot of different things to consider and it's much more complex than a simple shell script one of the initial",
    "start": "1329850",
    "end": "1336179"
  },
  {
    "text": "decisions that we made which is kind of interesting is we chose to perform the node preparation after kubernetes is",
    "start": "1336179",
    "end": "1342929"
  },
  {
    "text": "already running one of the reasons for that was we didn't want to become embedded with other processes tools and",
    "start": "1342929",
    "end": "1349650"
  },
  {
    "text": "configuration and our teams already working very actively on kubernetes and we loved some of the features I'll talk",
    "start": "1349650",
    "end": "1355890"
  },
  {
    "text": "about some of the features that we really leverage specifically daemon sets in this slide and in the demo so as an",
    "start": "1355890",
    "end": "1363840"
  },
  {
    "text": "example of some of the challenges and some of the things we need to do each of the environments may be different each",
    "start": "1363840",
    "end": "1370470"
  },
  {
    "text": "of the different server technologies may be different and so we actually describe in a claret of manner both the",
    "start": "1370470",
    "end": "1377789"
  },
  {
    "text": "configuration that we expect as well as the configuration or what we want this node preparation phase to do and so we",
    "start": "1377789",
    "end": "1385470"
  },
  {
    "text": "have a daemon set that consumes that particular configuration and information",
    "start": "1385470",
    "end": "1390630"
  },
  {
    "text": "and that node prep daemon set is watching all the time for whenever new servers roll in and our provisioned and",
    "start": "1390630",
    "end": "1397679"
  },
  {
    "text": "our setup by kubernetes so that it can be provisioned and part of this node preparation so that those local disks",
    "start": "1397679",
    "end": "1405659"
  },
  {
    "text": "and local storage resources that we want to be exposed to kubernetes for storage services can be used in later parts of",
    "start": "1405659",
    "end": "1413159"
  },
  {
    "text": "the lifecycle so that node prep daemon set is doing that operation and it's doing things like partitioning the disks",
    "start": "1413159",
    "end": "1420390"
  },
  {
    "text": "cleaning them up or mounting them can deal with accounting as well and inventory and really at the end of it",
    "start": "1420390",
    "end": "1427470"
  },
  {
    "text": "it's preparing things for that local volume provisioner that Michelle mentioned and so what the preparation",
    "start": "1427470",
    "end": "1434159"
  },
  {
    "text": "there is to place these resources in a directory that can be discovered by the",
    "start": "1434159",
    "end": "1439710"
  },
  {
    "text": "local volume provision and using some of the daemon set magic which I'll",
    "start": "1439710",
    "end": "1446370"
  },
  {
    "text": "demonstrate we actually kind of do a very clean handoff between the node preparation phase and the later phase of",
    "start": "1446370",
    "end": "1453210"
  },
  {
    "text": "the lifecycle which includes and is managed by the local volume provision",
    "start": "1453210",
    "end": "1460039"
  },
  {
    "start": "1460000",
    "end": "1460000"
  },
  {
    "text": "so here's some of the manifests from our daemon sets so I mentioned first we have",
    "start": "1460680",
    "end": "1467370"
  },
  {
    "text": "the node prep daemon set on the left hand side and then we have the local volume provision or daemon set on the right hand side and on the left hand",
    "start": "1467370",
    "end": "1474420"
  },
  {
    "text": "side we say hey if there's no label for node preparation no preparation has not been completed on this node then run",
    "start": "1474420",
    "end": "1481530"
  },
  {
    "text": "this daemon set and so what will happen is if a new rack or a new servers enter",
    "start": "1481530",
    "end": "1486870"
  },
  {
    "text": "this cluster then the node prep daemon set will run automatically on those nodes and on the right hand side we say",
    "start": "1486870",
    "end": "1493770"
  },
  {
    "text": "wait until this label exists and the value of that label is mounted to",
    "start": "1493770",
    "end": "1498990"
  },
  {
    "text": "indicate that node preparation has already completed and what you'll see is actually the node preparation labels the",
    "start": "1498990",
    "end": "1506100"
  },
  {
    "text": "node at the end of completing its job and when that label appears kubernetes",
    "start": "1506100",
    "end": "1512160"
  },
  {
    "text": "will actually remove the instance of the node preparation and it will start up an",
    "start": "1512160",
    "end": "1517380"
  },
  {
    "text": "instance of the local volume provisioner so we talked a lot about phase one and",
    "start": "1517380",
    "end": "1525450"
  },
  {
    "start": "1522000",
    "end": "1522000"
  },
  {
    "text": "that node preparation which is really environment specific and now I'm going to cover the second part the second",
    "start": "1525450",
    "end": "1531750"
  },
  {
    "text": "phase here which is not environment specific we are leveraging the open source local volume provisioner the same",
    "start": "1531750",
    "end": "1538590"
  },
  {
    "text": "one that Michelle mentioned that gke uses and other people can use for the second phase of this lifecycle that is",
    "start": "1538590",
    "end": "1545010"
  },
  {
    "text": "all occurring within kubernetes and what the local volume provisioner does is it's looking for certain it's looking",
    "start": "1545010",
    "end": "1553980"
  },
  {
    "text": "within certain discovery directories and any of the directories that it finds the",
    "start": "1553980",
    "end": "1559050"
  },
  {
    "text": "local discs that it finds in those directories it will create persistent",
    "start": "1559050",
    "end": "1564450"
  },
  {
    "text": "volumes for those once those persistent volumes are created in kubernetes they",
    "start": "1564450",
    "end": "1569670"
  },
  {
    "text": "become consumable so you can write a persistent volume claim or a volume claim template in your stateful set and",
    "start": "1569670",
    "end": "1576260"
  },
  {
    "text": "when that pod and that workload is scheduled it'll actually be allocated",
    "start": "1576260",
    "end": "1582030"
  },
  {
    "text": "and bound with persistent volumes and so this is kind of the same life cycle as",
    "start": "1582030",
    "end": "1588540"
  },
  {
    "text": "normal persistent volumes even if they were non-local the difference being that these are going to actually leverage the local discs",
    "start": "1588540",
    "end": "1595120"
  },
  {
    "text": "instead of a network-attached disc and so it follows that same life cycle where",
    "start": "1595120",
    "end": "1600730"
  },
  {
    "text": "you can claim them they become bound eventually at the end of their life they can be released and when they're",
    "start": "1600730",
    "end": "1606730"
  },
  {
    "text": "released the local volume provisioner takes care of cleaning up the kubernetes resources as well as the local discs and",
    "start": "1606730",
    "end": "1613720"
  },
  {
    "text": "then this life cycle can repeat in terms of those persistent volumes of any discs",
    "start": "1613720",
    "end": "1619840"
  },
  {
    "text": "that are not represented in kubernetes will be reintroduced as persistent volumes that can be claimed so now I'm",
    "start": "1619840",
    "end": "1629110"
  },
  {
    "start": "1628000",
    "end": "1628000"
  },
  {
    "text": "going to give a demo of some of that and daemon SAP magic for the node preparation as well as for the local",
    "start": "1629110",
    "end": "1636759"
  },
  {
    "text": "volume provision now in this demo we have nineteen notes we do not have the",
    "start": "1636759",
    "end": "1643840"
  },
  {
    "text": "node preparation running because as soon as the node preparation day misete runs it's going to prepare everything we do",
    "start": "1643840",
    "end": "1650259"
  },
  {
    "text": "have the local volume provisioner running though and so what you'll see",
    "start": "1650259",
    "end": "1655779"
  },
  {
    "text": "here is that we we have the local volume provision or running but there are no",
    "start": "1655779",
    "end": "1662919"
  },
  {
    "text": "instances of it running because none of the nodes have the node prep label so",
    "start": "1662919",
    "end": "1668559"
  },
  {
    "text": "what we would expect is it would be nineteen if it all the nodes were prepared and the other thing you can see",
    "start": "1668559",
    "end": "1674440"
  },
  {
    "text": "here there are no persistent volumes configured in this cluster so here I'm showing the manifest for the local",
    "start": "1674440",
    "end": "1682029"
  },
  {
    "text": "volume provisioner and showing you that yes it says right here if you see the node prep label with mounted then I want",
    "start": "1682029",
    "end": "1689289"
  },
  {
    "text": "you to start running the daemon set an instance of that daemon set on this node and it's not running yet what we're",
    "start": "1689289",
    "end": "1697000"
  },
  {
    "text": "gonna do we're gonna start up the node prep soon so here's the node prep configuration and as Michelle mentioned",
    "start": "1697000",
    "end": "1703539"
  },
  {
    "text": "this is specific to our environment to a set of machines a specific data center or a specific cluster and what we're",
    "start": "1703539",
    "end": "1710200"
  },
  {
    "text": "showing here is we want to expose several different discs at these well-known locations and we want to",
    "start": "1710200",
    "end": "1717100"
  },
  {
    "text": "expose those for discovery in a different location that's going to be configured and managed by the local",
    "start": "1717100",
    "end": "1722830"
  },
  {
    "text": "volume provision and what you see on the bottom there is the SSDs actually there is no data zero so just keep that in mind as",
    "start": "1722830",
    "end": "1730450"
  },
  {
    "text": "we go forward because maybe this disk is being leveraged by another system process and we don't want that exposed",
    "start": "1730450",
    "end": "1736419"
  },
  {
    "text": "through kubernetes now what I'm showing here is the FS tab or what is mounted on",
    "start": "1736419",
    "end": "1742240"
  },
  {
    "text": "this machine because this is important these are the disks and the drives that are on this machine before node",
    "start": "1742240",
    "end": "1748899"
  },
  {
    "text": "preparation and what you see is there are three hdds and two SSDs so the data",
    "start": "1748899",
    "end": "1754899"
  },
  {
    "text": "and the date fast data there and what we're gonna do now is we're gonna",
    "start": "1754899",
    "end": "1760090"
  },
  {
    "text": "actually start that node prep so when the node prep starts what we expect to see is instances of the node prep daemon",
    "start": "1760090",
    "end": "1767169"
  },
  {
    "text": "set to run on every single one of the notes and then when it completes its job",
    "start": "1767169",
    "end": "1772230"
  },
  {
    "text": "the local volume provisioner will start running on each and what you see is very quickly the node prep goes to nineteen",
    "start": "1772230",
    "end": "1779320"
  },
  {
    "text": "instances one for each node and then it actually completes its job and goes to",
    "start": "1779320",
    "end": "1784870"
  },
  {
    "text": "zero and what you see is the local volume provisioner it started at zero and then went to 19 so now the local",
    "start": "1784870",
    "end": "1792340"
  },
  {
    "text": "volume provisioner continues to run the node prep is all done so all that work",
    "start": "1792340",
    "end": "1798399"
  },
  {
    "text": "has done the local volume provisioner discovers all those resources based upon them being placed in the discovery",
    "start": "1798399",
    "end": "1804909"
  },
  {
    "text": "directories and you'll see here we have now 95 different local persistent",
    "start": "1804909",
    "end": "1810399"
  },
  {
    "text": "volumes several of them are HD DS and some are SSDs now the final I'm going to",
    "start": "1810399",
    "end": "1816460"
  },
  {
    "text": "show here is what happened to our FS tab right our mount points because we needed",
    "start": "1816460",
    "end": "1821980"
  },
  {
    "text": "those to be remapped and mounted so that the local volume provisioner could take over and also we want this to be",
    "start": "1821980",
    "end": "1828070"
  },
  {
    "text": "persistent right every time this node boots up or every time this machine is",
    "start": "1828070",
    "end": "1834010"
  },
  {
    "text": "being used we want those mount points to persist and what you see here is that the HDD s and SSDs have been vine",
    "start": "1834010",
    "end": "1842409"
  },
  {
    "text": "mounted into the location expected by the local volume provisioner and you'll",
    "start": "1842409",
    "end": "1848289"
  },
  {
    "text": "also notice that fast data 0 isn't there so this is kind of that example of you may need to do things for your",
    "start": "1848289",
    "end": "1854830"
  },
  {
    "text": "environment and you may not want to expose all the local resources that are available",
    "start": "1854830",
    "end": "1861870"
  },
  {
    "text": "alright thanks Ian so yeah does a great example of a very",
    "start": "1865320",
    "end": "1870850"
  },
  {
    "text": "complex environment where you have different machine types with different number of discs on each node and it was",
    "start": "1870850",
    "end": "1879460"
  },
  {
    "text": "good that you guys are able to leverage able to automate all of that",
    "start": "1879460",
    "end": "1884789"
  },
  {
    "start": "1880000",
    "end": "1880000"
  },
  {
    "text": "so to summarize hopefully you guys now",
    "start": "1884789",
    "end": "1890440"
  },
  {
    "text": "have some good ideas about how you can leverage or how can you can do local",
    "start": "1890440",
    "end": "1896889"
  },
  {
    "text": "disk administration in your own clusters there is some work that you need to do",
    "start": "1896889",
    "end": "1902739"
  },
  {
    "text": "it's not as magical as cloud providers storage where you can just click a",
    "start": "1902739",
    "end": "1908049"
  },
  {
    "text": "button and it disk shows up but hopefully we showed you today that the",
    "start": "1908049",
    "end": "1913210"
  },
  {
    "text": "process can be automated and made made manageable and then if you step back a",
    "start": "1913210",
    "end": "1922690"
  },
  {
    "text": "little bit you can see that this node preparation step is just a relatively short part of the entire local volume",
    "start": "1922690",
    "end": "1930700"
  },
  {
    "text": "life cycle once those disks are prepared now you can hand it over to the local",
    "start": "1930700",
    "end": "1937619"
  },
  {
    "text": "volume static provisioner to manage the remaining part of the kubernetes life",
    "start": "1937619",
    "end": "1943090"
  },
  {
    "text": "cycle as as pods can now consume them and they end up eventually getting",
    "start": "1943090",
    "end": "1949179"
  },
  {
    "text": "released and then this provisioner can now manage the cleanup and reuse of",
    "start": "1949179",
    "end": "1955359"
  },
  {
    "text": "those volumes and perhaps the the most powerful part is that from the users",
    "start": "1955359",
    "end": "1963970"
  },
  {
    "text": "from the users perspective their workflow has not changed at all most of",
    "start": "1963970",
    "end": "1971559"
  },
  {
    "text": "the work is hidden underneath in the admin flow but to the user they can still use reuse the existing persistent",
    "start": "1971559",
    "end": "1979269"
  },
  {
    "text": "volume claims that they have been already using in their saiful sets and",
    "start": "1979269",
    "end": "1984580"
  },
  {
    "text": "to just use local storage all you have to do is change the storage class name",
    "start": "1984580",
    "end": "1990820"
  },
  {
    "text": "in your persistent volume claim you don't have to actually you know include all the complex logic in your",
    "start": "1990820",
    "end": "1998210"
  },
  {
    "text": "operators that you used to have to do if you wanted to use local storage so",
    "start": "1998210",
    "end": "2004840"
  },
  {
    "text": "looking forward there are still plenty of enhancements planned in this area one",
    "start": "2004840",
    "end": "2013000"
  },
  {
    "start": "2005000",
    "end": "2005000"
  },
  {
    "text": "enhancement is to enable raw block raw block volumes this is now available in",
    "start": "2013000",
    "end": "2020500"
  },
  {
    "text": "alpha in kubernetes 110 and it's very useful for those use cases that can get",
    "start": "2020500",
    "end": "2027790"
  },
  {
    "text": "higher performance by bypassing the file system layer so for example at Salesforce they intend to use this",
    "start": "2027790",
    "end": "2034870"
  },
  {
    "text": "feature to store these self metadata on another very popular ask has been to",
    "start": "2034870",
    "end": "2042310"
  },
  {
    "text": "support dynamic provisioning with LVM this is very useful for those",
    "start": "2042310",
    "end": "2047380"
  },
  {
    "text": "environments where you want to have simplified disk management and better",
    "start": "2047380",
    "end": "2053888"
  },
  {
    "text": "disk utilization as long as your workloads can handle the performance",
    "start": "2053889",
    "end": "2059919"
  },
  {
    "text": "penalty of potentially sharing discs with other workloads and finally the",
    "start": "2059919",
    "end": "2065530"
  },
  {
    "text": "last major improvement we're looking at is to do the filesystem formatting and",
    "start": "2065530",
    "end": "2071169"
  },
  {
    "text": "mounting steps in the kubernetes plugin itself right now you have to do this in",
    "start": "2071169",
    "end": "2076570"
  },
  {
    "text": "the node prep step but once we can do this in kubernetes now you can remove",
    "start": "2076570",
    "end": "2081760"
  },
  {
    "text": "that from the node preparation and so that will really simplify those simple",
    "start": "2081760",
    "end": "2087638"
  },
  {
    "text": "environments where you just want to expose the whole disk to two kubernetes",
    "start": "2087639",
    "end": "2094720"
  },
  {
    "text": "so then you can just instead of having to partition and format and mount them",
    "start": "2094720",
    "end": "2100090"
  },
  {
    "text": "all you have to do is just tell kubernetes the paths directly so if",
    "start": "2100090",
    "end": "2106420"
  },
  {
    "start": "2106000",
    "end": "2106000"
  },
  {
    "text": "you're interested in using this feature we have a lot of documentation available both the official kubernetes",
    "start": "2106420",
    "end": "2112990"
  },
  {
    "text": "documentation and ian and i have also done a few blog posts on all of the stuff we've discussed here today you can",
    "start": "2112990",
    "end": "2121240"
  },
  {
    "text": "find these slides on my speaker deck file speaker comm / NSA you 42 so you'll",
    "start": "2121240",
    "end": "2129220"
  },
  {
    "text": "be able to download the PDF to these slides and get these links can give you",
    "start": "2129220",
    "end": "2134440"
  },
  {
    "text": "time yeah and so if you have any further feedback",
    "start": "2134440",
    "end": "2142270"
  },
  {
    "start": "2141000",
    "end": "2141000"
  },
  {
    "text": "or questions or you want to get involved in helping to develop and test these features please join us in the storage",
    "start": "2142270",
    "end": "2149349"
  },
  {
    "text": "cig we hold meetings every two weeks on Thursdays and we are also active on the six storage slack Channel as well and",
    "start": "2149349",
    "end": "2156700"
  },
  {
    "text": "you can also contact Ian or me directly our handles github handles are here so",
    "start": "2156700",
    "end": "2163630"
  },
  {
    "text": "yeah we can open it up for questions thank you [Applause]",
    "start": "2163630",
    "end": "2175140"
  },
  {
    "text": "all right thanks great talk any hooks to extend the anti affinity",
    "start": "2175140",
    "end": "2181870"
  },
  {
    "text": "policies for for scheduling my stateful sets to kind of take into account for",
    "start": "2181870",
    "end": "2188290"
  },
  {
    "text": "example I could I could read from my matrix collection how are the disks",
    "start": "2188290",
    "end": "2193420"
  },
  {
    "text": "being utilized and encounter basically the performance penalties with the dynamic provisioning so the question was",
    "start": "2193420",
    "end": "2201940"
  },
  {
    "text": "you want to dynamically create anti",
    "start": "2201940",
    "end": "2207280"
  },
  {
    "text": "affinity policies based off of metrics yes so basically to avoid having the",
    "start": "2207280",
    "end": "2212830"
  },
  {
    "text": "performance pay no penalties then haha with multiple disk intensive applications okay you can potentially",
    "start": "2212830",
    "end": "2219730"
  },
  {
    "text": "let's see so if you want to dynamically generate anti affinity policies you",
    "start": "2219730",
    "end": "2226210"
  },
  {
    "text": "probably need to have like an operator an operator that will read the custom",
    "start": "2226210",
    "end": "2232240"
  },
  {
    "text": "metrics and then generate the anti affinity I mean you can always try to you can put anti affinity on pods or",
    "start": "2232240",
    "end": "2242080"
  },
  {
    "text": "sorry on nodes into your stateful set and that will spread your pods across",
    "start": "2242080",
    "end": "2248260"
  },
  {
    "text": "nodes to spread the IO that way but you're asking about spreading IC o----",
    "start": "2248260",
    "end": "2269080"
  },
  {
    "text": "so you can actually do pod anti affinity in that case right you can say if you",
    "start": "2269080",
    "end": "2274330"
  },
  {
    "text": "have two different io intensive workloads oh if they're your users pods",
    "start": "2274330",
    "end": "2284650"
  },
  {
    "text": "oh that's harder but yeah that could be",
    "start": "2284650",
    "end": "2290140"
  },
  {
    "text": "something yeah the suggestion was to write an admission a dynamic admission webhook to",
    "start": "2290140",
    "end": "2296890"
  },
  {
    "text": "be able to dynamically generate those pod palettes and high affinity pod policies but I think I think pod anti",
    "start": "2296890",
    "end": "2304000"
  },
  {
    "text": "affinity is what you or look for to be able to say two different workloads that are both i/o intensive",
    "start": "2304000",
    "end": "2310839"
  },
  {
    "text": "you want them to go on different notes yeah hey thanks really enjoyed the talk",
    "start": "2310839",
    "end": "2318460"
  },
  {
    "text": "for the local provision or can it say detect disk failure and then move a pod",
    "start": "2318460",
    "end": "2323890"
  },
  {
    "text": "like reschedule pod off of a node if say off like an operator that handles like changing replication and stuff great",
    "start": "2323890",
    "end": "2330910"
  },
  {
    "text": "question so right now we don't have health metrics built in right now but it's",
    "start": "2330910",
    "end": "2337359"
  },
  {
    "text": "currently I think some people are experimenting with that to add be able to add health checks and to be able to",
    "start": "2337359",
    "end": "2345359"
  },
  {
    "text": "put some sort of metadata in the persistent volume object to denote the",
    "start": "2345359",
    "end": "2350770"
  },
  {
    "text": "health of the volume and then from there people can build higher level",
    "start": "2350770",
    "end": "2356200"
  },
  {
    "text": "controllers and operators to be able to react to those conditions so you can say like for your application you can say if",
    "start": "2356200",
    "end": "2362950"
  },
  {
    "text": "this volume becomes unhealthy then and my application can handle the failover",
    "start": "2362950",
    "end": "2370000"
  },
  {
    "text": "of that data then you can you know start the failover process to release to",
    "start": "2370000",
    "end": "2378400"
  },
  {
    "text": "delete your persistent volume claim which will delete the disk and delete the data and then that will free up your",
    "start": "2378400",
    "end": "2385690"
  },
  {
    "text": "pod from that and from that node binding and then allow your pod to reschedule",
    "start": "2385690",
    "end": "2391599"
  },
  {
    "text": "and get a blank new disk and you know do the reconstruction or whatever it needs to yes so I have a question for you so",
    "start": "2391599",
    "end": "2402640"
  },
  {
    "text": "you mentioned salesforce is managing like morning PBS of data and then even",
    "start": "2402640",
    "end": "2408880"
  },
  {
    "text": "the lists are local storage is the operator feature so I wonder how much of",
    "start": "2408880",
    "end": "2414369"
  },
  {
    "text": "data are actually using this feature and how long have you guys been using so I",
    "start": "2414369",
    "end": "2423040"
  },
  {
    "text": "can't tell you about the data side in terms of how exactly we're using it but",
    "start": "2423040",
    "end": "2428140"
  },
  {
    "text": "I can say we've spent multiple years using kubernetes in production at",
    "start": "2428140",
    "end": "2434170"
  },
  {
    "text": "Salesforce now and that has given us the confidence just running our storage services there as",
    "start": "2434170",
    "end": "2439460"
  },
  {
    "text": "well as I would say our storage services or replicated data stores so we do",
    "start": "2439460",
    "end": "2445220"
  },
  {
    "text": "retain multiple copies as well as application owners that we talk to for",
    "start": "2445220",
    "end": "2450619"
  },
  {
    "text": "leveraging those storage services often leverage another storage service for really high durability situations blob",
    "start": "2450619",
    "end": "2459529"
  },
  {
    "text": "and object stores just like you would in any other cloud environment again great",
    "start": "2459529",
    "end": "2469009"
  },
  {
    "text": "talk one of the most useful and applicable things to my personal issues I've seen all week so thank you two",
    "start": "2469009",
    "end": "2475609"
  },
  {
    "text": "questions first of all I should imagine this room is gonna go and start like some open-source pre provisioned their",
    "start": "2475609",
    "end": "2480859"
  },
  {
    "text": "daemon sets now probably like 10 or 12 of them is there any chance you can just open source the one you have is like a",
    "start": "2480859",
    "end": "2486559"
  },
  {
    "text": "base for the community and the second question is you said that when a pod",
    "start": "2486559",
    "end": "2493880"
  },
  {
    "text": "reschedules it will always get the same volume it had in that stateful set what underlying kind of kubernetes primitives",
    "start": "2493880",
    "end": "2500690"
  },
  {
    "text": "is it using like what is that in excellent question so as part of making local storage work",
    "start": "2500690",
    "end": "2509000"
  },
  {
    "text": "in kubernetes we completely revamped how the scheduler works in its understanding",
    "start": "2509000",
    "end": "2515450"
  },
  {
    "text": "of volumes so we've added a node affinity field to the persistent volume",
    "start": "2515450",
    "end": "2521660"
  },
  {
    "text": "so this is just like how in the pod you can specify node affinity to force your",
    "start": "2521660",
    "end": "2528230"
  },
  {
    "text": "pod to go to specific nodes now you can put it on a volume instead so you no",
    "start": "2528230",
    "end": "2534109"
  },
  {
    "text": "longer your pod no longer has to be aware of the node constraints of your",
    "start": "2534109",
    "end": "2540170"
  },
  {
    "text": "volumes your pod doesn't need node affinity now if it's using a volume that has no affinity on it the scheduler will",
    "start": "2540170",
    "end": "2546890"
  },
  {
    "text": "still be able to schedule it correctly based on that so this actually works for more than just local storage it works",
    "start": "2546890",
    "end": "2554210"
  },
  {
    "text": "for zonal storage and it also works for really crazy distributed systems that",
    "start": "2554210",
    "end": "2559339"
  },
  {
    "text": "might only have disks available in like three out of ten nodes so and it's based",
    "start": "2559339",
    "end": "2565039"
  },
  {
    "text": "all off of labels so you can define your topology for your volume exactly",
    "start": "2565039",
    "end": "2570210"
  },
  {
    "text": "you want and I'll follow up with open sourcing so Salesforce is pretty big in",
    "start": "2570210",
    "end": "2575970"
  },
  {
    "text": "open sourcing things and sharing information I don't have a particular timeline but I will say in addition to",
    "start": "2575970",
    "end": "2581940"
  },
  {
    "text": "things like node preparation we have higher level abstractions that help with distributed data stores and we do intend",
    "start": "2581940",
    "end": "2588030"
  },
  {
    "text": "to open-source those so keep an eye out I think it's Salesforce github dot",
    "start": "2588030",
    "end": "2593810"
  },
  {
    "text": "and that's where it would end up great",
    "start": "2593810",
    "end": "2602550"
  },
  {
    "text": "talk just a follow up question um so you mentioned you made some changes to",
    "start": "2602550",
    "end": "2607710"
  },
  {
    "text": "scheduler is aware of local storage and also you had a slide about local storage",
    "start": "2607710",
    "end": "2614280"
  },
  {
    "text": "is not prime for general purpose storage instead use distributed file systems like safe HDFS so in scenarios like that",
    "start": "2614280",
    "end": "2621900"
  },
  {
    "text": "where you have HDFS let's say doing 3-way replication should there be a mechanism for this type of the sugar for",
    "start": "2621900",
    "end": "2629400"
  },
  {
    "text": "systems to communicate with kubernetes to say where the replicas are you know which knows there are yeah you can do",
    "start": "2629400",
    "end": "2636089"
  },
  {
    "text": "that exactly using the persistent volume node affinity so say if HDFS you have",
    "start": "2636089",
    "end": "2642839"
  },
  {
    "text": "replicas on node 1 5 and 10 in the volume for your HDFS volume you can say",
    "start": "2642839",
    "end": "2650070"
  },
  {
    "text": "you could put a node affinity to it on nodes 1 5 and 10 so we will always",
    "start": "2650070",
    "end": "2656160"
  },
  {
    "text": "schedule your pod to one of those three nodes and it you need to you need to",
    "start": "2656160",
    "end": "2664589"
  },
  {
    "text": "represent it as a volume an HDFS volume I'll add to that a little bit kubernetes",
    "start": "2664589",
    "end": "2671790"
  },
  {
    "text": "one of the really powerful features is all its labeling capabilities and so as an example if you care about racks right",
    "start": "2671790",
    "end": "2680640"
  },
  {
    "text": "you can label your objects and make your system knowledgeable about that type of",
    "start": "2680640",
    "end": "2687450"
  },
  {
    "text": "topology information and leverage it when doing things like scheduling and in the reverse let's say you're a storage",
    "start": "2687450",
    "end": "2693930"
  },
  {
    "text": "service you can add labels and things to pods and nodes and racks and persistent volumes that allow other controllers to",
    "start": "2693930",
    "end": "2701849"
  },
  {
    "text": "watch and observe and reacts to those labels so kind of labels can provide a communication",
    "start": "2701849",
    "end": "2708690"
  },
  {
    "text": "mechanism that is asynchronous between different processes yeah so the whatever",
    "start": "2708690",
    "end": "2724109"
  },
  {
    "text": "the SEF provisioner or cluster provisioner is in you need to somehow encode the topology information into the",
    "start": "2724109",
    "end": "2731039"
  },
  {
    "text": "provisioner so when the provisioner creates those volumes it can fill in the node affinity the other cool thing about",
    "start": "2731039",
    "end": "2741630"
  },
  {
    "text": "node affinity is it can support requirements and preferences so you",
    "start": "2741630",
    "end": "2747299"
  },
  {
    "text": "could do things like my volume is globally accessible on all nodes but I",
    "start": "2747299",
    "end": "2753420"
  },
  {
    "text": "prefer your schedule on these nodes when my replicas are so I can get better performance alright thank you everyone",
    "start": "2753420",
    "end": "2763640"
  },
  {
    "text": "[Applause]",
    "start": "2763640",
    "end": "2767589"
  }
]