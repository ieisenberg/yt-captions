[
  {
    "text": "my clicker working there we go so we're off start I'm going to ask a question what is this anyone and shout out we got",
    "start": "60",
    "end": "9540"
  },
  {
    "text": "fire yeah we got fire and it's not system outage it's fire warmth light it",
    "start": "9540",
    "end": "17699"
  },
  {
    "text": "could even be prometheus however we're going to refer to it as a signal here today and we can put it on top of a",
    "start": "17699",
    "end": "23880"
  },
  {
    "text": "watch our to signal something's happen maybe an oncoming attack we can then use these watch towers to help defend our",
    "start": "23880",
    "end": "32008"
  },
  {
    "text": "town I work for a games company so I like to use pixel art and games when we're describing staff as an attacker",
    "start": "32009",
    "end": "40350"
  },
  {
    "text": "approaches we can light the signal fire to tell our town that an oncoming attack is coming",
    "start": "40350",
    "end": "46280"
  },
  {
    "text": "however as we grow and our city spreads out we need more towers to signal and an",
    "start": "46280",
    "end": "51960"
  },
  {
    "text": "attacks coming so we build out more towers and now when an attack comes the first light goes up to signal that",
    "start": "51960",
    "end": "58559"
  },
  {
    "text": "attacks coming it then it propagates back to tell the town that the attack is coming",
    "start": "58559",
    "end": "63780"
  },
  {
    "text": "however in this point we tend to lose some resolution and the three signals at the bottom have now just become one here",
    "start": "63780",
    "end": "71549"
  },
  {
    "text": "we see a goblin that man's the middle tower and this is David however he's quite forgetful he only ever remembers",
    "start": "71549",
    "end": "77130"
  },
  {
    "text": "the past three days of when attacks have happened this isn't great also we",
    "start": "77130",
    "end": "82799"
  },
  {
    "text": "sometimes have sneaky enemies whereby they come in and in stealth and take our tower and we don't actually know an",
    "start": "82799",
    "end": "88650"
  },
  {
    "text": "attacks coming at that point now this can be thought of as signals being passed around by watchtowers back to our",
    "start": "88650",
    "end": "95369"
  },
  {
    "text": "town or it could also be thought of as Prometheus recording metrics back up to something like Ravana",
    "start": "95369",
    "end": "101520"
  },
  {
    "text": "now all these problems in the tower defense or a lot like we could have with this Prometheus setup I'm Dominic Greene",
    "start": "101520",
    "end": "109890"
  },
  {
    "text": "I am a software engineer improbable I work in a number of open-source projects such as Thanos goji OPC middleware and",
    "start": "109890",
    "end": "117750"
  },
  {
    "text": "go HTTP wires and also organized a number of meetups in London my name is",
    "start": "117750",
    "end": "123030"
  },
  {
    "text": "Lukas Edwin marine I'm a software engineer at Red Hat by way of core OS I'm working on the observability team on",
    "start": "123030",
    "end": "129030"
  },
  {
    "text": "all things Thor knows Prometheus and prometheus operator and without further ado today we're",
    "start": "129030",
    "end": "135600"
  },
  {
    "text": "introducing Thanos this is an open source project dedicated to providing a global query view of metrics providing a",
    "start": "135600",
    "end": "144720"
  },
  {
    "text": "scalable and highly available metric system and also long term retention with unlimited storage so a little bit about",
    "start": "144720",
    "end": "152270"
  },
  {
    "text": "the Thanos community this project was started two years ago by Fabien Reinhart",
    "start": "152270",
    "end": "157500"
  },
  {
    "text": "and Bartek latke atom probable and since then it's grown into a CNC F sandbox",
    "start": "157500",
    "end": "163440"
  },
  {
    "text": "project with eight core maintainer x' and hundreds of contributors and yeah",
    "start": "163440",
    "end": "169350"
  },
  {
    "text": "hopefully some of you will also become contributors in the future since then",
    "start": "169350",
    "end": "174540"
  },
  {
    "text": "it's also been adopted by a bunch of different companies so don't just take Tom's word for it or my word for it",
    "start": "174540",
    "end": "179600"
  },
  {
    "text": "we're running it but so are a lot of other people and today we're actually",
    "start": "179600",
    "end": "185340"
  },
  {
    "text": "going to start our discussion not with Sano's but with prometheus we're gonna specifically look at some key features",
    "start": "185340",
    "end": "190620"
  },
  {
    "text": "of how Prometheus works we're gonna look at some of the shortcomings of Prometheus and then we're going to look",
    "start": "190620",
    "end": "197700"
  },
  {
    "text": "at how we can use stano's to complement these shortcomings in Prometheus and fix",
    "start": "197700",
    "end": "202709"
  },
  {
    "text": "some of the scalability issues that we might run into okay start we kind of",
    "start": "202709",
    "end": "208380"
  },
  {
    "text": "want to have a shared understanding now this is a quote from the Google sre book it signifies that monitoring is that are",
    "start": "208380",
    "end": "215820"
  },
  {
    "text": "collecting aggregating and displaying real time quantitative data about a system so Prometheus and by extension",
    "start": "215820",
    "end": "222390"
  },
  {
    "text": "Thanos are monitoring systems we've built Thanos on top of the shoulders of giants so we've used all the components",
    "start": "222390",
    "end": "229620"
  },
  {
    "text": "Prometheus to build out a metric system in a distributed way",
    "start": "229620",
    "end": "234850"
  },
  {
    "text": "if you can put your hands up if you've heard of Prometheus before fantastic and keep your hands raised if",
    "start": "234850",
    "end": "241580"
  },
  {
    "text": "you use an introduction amazing and anyone using thon awesome production oh cool",
    "start": "241580",
    "end": "247460"
  },
  {
    "text": "nice a lot more than I expected so with Prometheus on your client side you",
    "start": "247460",
    "end": "253790"
  },
  {
    "text": "instrument a number of metrics you expose these via the Prometheus server which is an HTTP endpoint which then",
    "start": "253790",
    "end": "261170"
  },
  {
    "text": "Prometheus can grab these metrics from on the right hand side you can see a number of servers and on the left this",
    "start": "261170",
    "end": "267380"
  },
  {
    "text": "is Prometheus split down to four key components the scrape engine the query engine the compactor and the rules",
    "start": "267380",
    "end": "273500"
  },
  {
    "text": "engine and we're going to take these components and distribute them with Thanos cool all right so toss begin",
    "start": "273500",
    "end": "281840"
  },
  {
    "text": "let's just take a look at a really basic prometheus deployment in kubernetes where we have a number of micro services",
    "start": "281840",
    "end": "287150"
  },
  {
    "text": "being scraped by one prometheus server and let's think about what happens for",
    "start": "287150",
    "end": "292490"
  },
  {
    "text": "example if for any number of reasons our Prometheus goes down what happens to our",
    "start": "292490",
    "end": "297740"
  },
  {
    "text": "entire metric system and let's think about it more specifically what were to happen if Prometheus went down right in",
    "start": "297740",
    "end": "304280"
  },
  {
    "text": "the middle of an incident we're having a production outage and right when we need metrics most Prometheus is down at the",
    "start": "304280",
    "end": "310520"
  },
  {
    "text": "heart of it the issue is that Prometheus itself is not a highly available system so the way that we deal with this in",
    "start": "310520",
    "end": "316940"
  },
  {
    "text": "production oftentimes do we run Prometheus pairs right so we'll run two Promethea one next to each other where",
    "start": "316940",
    "end": "322070"
  },
  {
    "text": "both are scraping all of the services obviously one of the results of this",
    "start": "322070",
    "end": "327200"
  },
  {
    "text": "we're running two Promethea we're also using twice resources CPU memory disk but keep this in mind because we're",
    "start": "327200",
    "end": "334520"
  },
  {
    "text": "gonna come back to this set up a few more times another limitation of Prometheus today is a storage limitation",
    "start": "334520",
    "end": "341480"
  },
  {
    "text": "so by default Prometheus has as we all know a retention of 15 days right so if I ingest some metrics these samples are",
    "start": "341480",
    "end": "347810"
  },
  {
    "text": "kept in TS DB for 15 days and the question remains what if I want to query",
    "start": "347810",
    "end": "352970"
  },
  {
    "text": "data from several months ago or a year ago and moreover what happens if",
    "start": "352970",
    "end": "358880"
  },
  {
    "text": "something happens to my disk what are my fallacies and got scrubbed if for any enum reasons and I lose all my metrics",
    "start": "358880",
    "end": "363950"
  },
  {
    "text": "now what do I do at the end of the day we want to have some way buying long-term storage for our metrics",
    "start": "363950",
    "end": "370689"
  },
  {
    "text": "and some way to make sure that we don't lose our metrics and we need them the most",
    "start": "370689",
    "end": "377099"
  },
  {
    "text": "so not only that is when we start to scale Prometheus we tend to do it by a Prometheus Federation so we have a",
    "start": "380580",
    "end": "387129"
  },
  {
    "text": "monitoring cluster then federates out to a number of other things and at this point you need to serve recording rules",
    "start": "387129",
    "end": "393099"
  },
  {
    "text": "at a start you can't get the fine granular details that you can do from your Prometheus in your leaf clusters",
    "start": "393099",
    "end": "399699"
  },
  {
    "text": "and so at times your engineers end up going from the main cluster at the top out into the the leaf clusters to try",
    "start": "399699",
    "end": "405759"
  },
  {
    "text": "and get more detailed information and then as we scale up maybe you want the",
    "start": "405759",
    "end": "411279"
  },
  {
    "text": "Federation to have a che as well this point we're now scraping double amount data from the clusters at the bottom so",
    "start": "411279",
    "end": "418869"
  },
  {
    "text": "actually and then when they go hey che as well you're doing two four eight x amount data it's a lot of bandwidth for",
    "start": "418869",
    "end": "425469"
  },
  {
    "text": "wasting by just scraping metrics normally you have Federation's so that",
    "start": "425469",
    "end": "431349"
  },
  {
    "text": "you can propagate queries and use it with your profile setup this is relatively okay with three clusters you",
    "start": "431349",
    "end": "438819"
  },
  {
    "text": "can pull the data into memory it works fine Federation what effectively does is pulls all the data from those leaf",
    "start": "438819",
    "end": "444879"
  },
  {
    "text": "fermitas into memory to then be served again however ones when we scale up if we have",
    "start": "444879",
    "end": "450219"
  },
  {
    "text": "then n number of clusters that's all that memories coming into that federated",
    "start": "450219",
    "end": "455649"
  },
  {
    "text": "federated endpoint I'm going to blow up that instance if we're not careful right",
    "start": "455649",
    "end": "463179"
  },
  {
    "text": "so this is where Thomas comes in the whole point of Thanos is to try to",
    "start": "463179",
    "end": "468459"
  },
  {
    "text": "combat some of the issues that we see when we're stretching Prometheus to this really global scale when we're scraping",
    "start": "468459",
    "end": "474459"
  },
  {
    "text": "from hundreds thousands of different clusters the question is how can we make Prometheus scalable and highly available",
    "start": "474459",
    "end": "481499"
  },
  {
    "text": "so at its core Thanos consists of a number of",
    "start": "481499",
    "end": "486539"
  },
  {
    "text": "composable components that we can use to bring scalability H a long term",
    "start": "486539",
    "end": "491919"
  },
  {
    "text": "retention on top of any existing Prometheus setup with that in mind the",
    "start": "491919",
    "end": "497139"
  },
  {
    "text": "goals of Thanos is number one to be Prometheus compatible this is not just something that's convenience",
    "start": "497139",
    "end": "504130"
  },
  {
    "text": "this is actually strategic we want Thanos to be built on top of all the great design principles of prometheus to",
    "start": "504130",
    "end": "511150"
  },
  {
    "text": "leverage all of the knowledge and all the learnings from the last several years of running Prometheus and to leverage the Prometheus ecosystem number",
    "start": "511150",
    "end": "518349"
  },
  {
    "text": "two we want to have a global view of metrics a query time we want to be able to see metrics from all over different",
    "start": "518350",
    "end": "524620"
  },
  {
    "text": "clusters we want to have long-term retention so we want to be able to see",
    "start": "524620",
    "end": "529750"
  },
  {
    "text": "historical data from several years ago from all of our clusters but also we want to be able to see data down to the",
    "start": "529750",
    "end": "535840"
  },
  {
    "text": "individual sample even if it's from like two years ago and that brings us to the fourth point which is we want to provide",
    "start": "535840",
    "end": "542290"
  },
  {
    "text": "down sampling and compaction of this data so we can make our queries efficient so let's do a thought",
    "start": "542290",
    "end": "549160"
  },
  {
    "text": "experiment and let's think back to the federated Kriya setup that we were just discussing earlier at the heart of it",
    "start": "549160",
    "end": "555160"
  },
  {
    "text": "one of the issues with this setup is the fact that we're ingesting data from and different clusters into one single one",
    "start": "555160",
    "end": "561640"
  },
  {
    "text": "single instance and this can bring scalability issues but let's just think",
    "start": "561640",
    "end": "567070"
  },
  {
    "text": "for a minute what could we do if we invented a new component where instead of ingesting data from every single",
    "start": "567070",
    "end": "573670"
  },
  {
    "text": "cluster at the leaf we actually query data at runtime so for",
    "start": "573670",
    "end": "580900"
  },
  {
    "text": "example this querying service receives a request from a client it proxies it down to the leaf clusters joins the responses",
    "start": "580900",
    "end": "587830"
  },
  {
    "text": "and returns that to the client let's imagine something like this suddenly our service for querying becomes somewhat",
    "start": "587830",
    "end": "594730"
  },
  {
    "text": "more scalable right we can actually were not ingesting data from every single cluster so we are just dealing with",
    "start": "594730",
    "end": "599800"
  },
  {
    "text": "queries and this becomes somewhat tenable there is one limitation here one",
    "start": "599800",
    "end": "605530"
  },
  {
    "text": "big problem with this design is that every single time that we get a request we have to proxy it to every single",
    "start": "605530",
    "end": "611620"
  },
  {
    "text": "cluster imagine that we are querying for data for a time series it's only on cluster 1 we're nevertheless gonna proxy",
    "start": "611620",
    "end": "619270"
  },
  {
    "text": "that query to every single other cluster and that's really inefficient right so let's imagine for a minute if we could",
    "start": "619270",
    "end": "626440"
  },
  {
    "text": "instead add yet another component and what this did was a side card that was",
    "start": "626440",
    "end": "632140"
  },
  {
    "text": "sitting next to Prometheus and just announced hey these the metrics on my teas to be on this local cluster now Sano's or this new",
    "start": "632140",
    "end": "640670"
  },
  {
    "text": "component we're matching this query component can know ahead of time hey if I get a request for this metric I know which cluster I have to hit and we're no",
    "start": "640670",
    "end": "647780"
  },
  {
    "text": "longer having to make requests for every every single other cluster and we can make requires more efficient we can",
    "start": "647780",
    "end": "654710"
  },
  {
    "text": "optimize this a little bit more if you recall here we're exposing to NPA two",
    "start": "654710",
    "end": "660560"
  },
  {
    "text": "API endpoints one for the Prometheus query MPI and one for this new imaginary",
    "start": "660560",
    "end": "665720"
  },
  {
    "text": "announcing metrics API if we combine these into one single API that does both",
    "start": "665720",
    "end": "671630"
  },
  {
    "text": "querying and announcing labels we can also take advantage of this moment and say hey rather than exposing the the",
    "start": "671630",
    "end": "678440"
  },
  {
    "text": "query data the results using JSON what did we did it using protobuf in this way we reduced the load at the query side on",
    "start": "678440",
    "end": "684560"
  },
  {
    "text": "the query component because we no longer have to do all these extra allocations CPU blah blah and we can also reduce our",
    "start": "684560",
    "end": "689630"
  },
  {
    "text": "bandwidth usage so let's expose one G RPC API that does both querying and",
    "start": "689630",
    "end": "695570"
  },
  {
    "text": "announcing to make this more efficient so at this point we're effectively",
    "start": "695570",
    "end": "701600"
  },
  {
    "text": "designed the sanest query component and the final sidecar the query component",
    "start": "701600",
    "end": "706880"
  },
  {
    "text": "pans out and calls to each side car which then does a remote read to the local Prometheus exposing the",
    "start": "706880",
    "end": "713060"
  },
  {
    "text": "information about the data that's stored on that node now this communications are all done by G RPC and one of the most",
    "start": "713060",
    "end": "720080"
  },
  {
    "text": "important parts of tunnels in my opinion is the story P I so this API is what's exposed is a well as a subset of the RPC",
    "start": "720080",
    "end": "727190"
  },
  {
    "text": "so it's exposed by all the things within Thanos so here you can see the info",
    "start": "727190",
    "end": "732620"
  },
  {
    "text": "endpoint which gives us the metadata about the time and the kind of data that's on that node as well as the series which gives back a streamed",
    "start": "732620",
    "end": "739250"
  },
  {
    "text": "response allowing us to query they'll send that data more efficiently to to",
    "start": "739250",
    "end": "746060"
  },
  {
    "text": "get more information about the story pi Bartok and Frederick are going to be doing a deep dive tomorrow so head over",
    "start": "746060",
    "end": "751160"
  },
  {
    "text": "to that and to really dig into the store API so as it turns out we've also solved",
    "start": "751160",
    "end": "758540"
  },
  {
    "text": "the h a problem at least for the federated instance here now we have graph on a calling in I can round-robin",
    "start": "758540",
    "end": "764540"
  },
  {
    "text": "between to query components in that monitoring cluster means that if one goes down the other",
    "start": "764540",
    "end": "770290"
  },
  {
    "text": "one can still serve traffic and it's just proxy requests through we can also use the same thing to do hey che",
    "start": "770290",
    "end": "777700"
  },
  {
    "text": "Promethea in the leaf clusters here we can see that the story pi now knows",
    "start": "777700",
    "end": "783250"
  },
  {
    "text": "about many of the Prometheus's however we've just got the same problem as we",
    "start": "783250",
    "end": "788620"
  },
  {
    "text": "had with the federation earlier we now have to get back all the data from both of those side cars and the Prometheus",
    "start": "788620",
    "end": "795490"
  },
  {
    "text": "and within combining it in memory in the courier and send me back to crew fauna one if we have a component that could",
    "start": "795490",
    "end": "802420"
  },
  {
    "text": "combine multiple streams of data and feed it back what we do that's the query",
    "start": "802420",
    "end": "807700"
  },
  {
    "text": "component so what we can effectively do now is put a query component down in those leaf clusters along with the the",
    "start": "807700",
    "end": "814120"
  },
  {
    "text": "side cars this means that when when we call into the query er we can then merge",
    "start": "814120",
    "end": "819610"
  },
  {
    "text": "the two streams and send it back as I said before the story pi is really important in turn us and this the query",
    "start": "819610",
    "end": "825790"
  },
  {
    "text": "component along with a sidecar implement that API so we can just switch them in when we need to so here we've made a lot",
    "start": "825790",
    "end": "834970"
  },
  {
    "text": "of progress towards a scaleable meteor system but what about storage we said we also wanted to keep the data for longer",
    "start": "834970",
    "end": "841860"
  },
  {
    "text": "right so we already know of one really obvious",
    "start": "841860",
    "end": "847510"
  },
  {
    "text": "solution for storage and that's object storage this has the benefit of being both really cheap and on the other hand",
    "start": "847510",
    "end": "853750"
  },
  {
    "text": "super flexible and scalable right most cloud providers don't require you to pre allocate some amount of storage for your",
    "start": "853750",
    "end": "858940"
  },
  {
    "text": "object storage so we can actually upload blocks as we need and the storage will",
    "start": "858940",
    "end": "865630"
  },
  {
    "text": "just grow with us so we one solution that we can do is say",
    "start": "865630",
    "end": "870970"
  },
  {
    "text": "hey let's make our tano sidecar upload TS DB blocks as Prometheus produces",
    "start": "870970",
    "end": "876790"
  },
  {
    "text": "these TS DB blogs these immutable blocks we could have the sidecar upload them to object storage and now we suddenly have a way to keep long-term data so let's",
    "start": "876790",
    "end": "885130"
  },
  {
    "text": "make sure that you know if the storage on the clusters died we don't lose any of the blocks",
    "start": "885130",
    "end": "891180"
  },
  {
    "text": "however one of the things that's missing here is we need a way to expose the",
    "start": "891180",
    "end": "898480"
  },
  {
    "text": "long-term store data to the rest of to the query API so one thing that we could do is imagine",
    "start": "898480",
    "end": "903999"
  },
  {
    "text": "yet a new component that is a gateway to to this data stored in object storage we",
    "start": "903999",
    "end": "911259"
  },
  {
    "text": "already have a waiting we already have an API that we defined for querying and announcing the metrics that are stored",
    "start": "911259",
    "end": "917319"
  },
  {
    "text": "in some storage and that's the story API so let's make this component the gateway expose the same API and when it receives",
    "start": "917319",
    "end": "922929"
  },
  {
    "text": "a request for querying some metrics it simply reads those metrics from object storage and returns it to the query now",
    "start": "922929",
    "end": "932249"
  },
  {
    "text": "there is we've solved the question of how do we store long term storage how we",
    "start": "932249",
    "end": "938199"
  },
  {
    "text": "store long term metrics and also how do we query for long term metrics but one of the issues that comes with this great",
    "start": "938199",
    "end": "944410"
  },
  {
    "text": "new power is is the following imagine the week query for two years",
    "start": "944410",
    "end": "949419"
  },
  {
    "text": "worth of data of some time series that sample that 15-second interval this will be hundreds of thousands of data points",
    "start": "949419",
    "end": "955269"
  },
  {
    "text": "and this presents a number of problems number one we're doing a ton of disk reads we're using a ton of bandwidth and",
    "start": "955269",
    "end": "962470"
  },
  {
    "text": "then we're returning more data points that we probably have pixels on our dashboards so we're doing all this work",
    "start": "962470",
    "end": "967689"
  },
  {
    "text": "for no reason one thing that would be great is hey if we query for a ton of time for a really",
    "start": "967689",
    "end": "973329"
  },
  {
    "text": "long time range let's only use maybe one sample every hour so we want to do is",
    "start": "973329",
    "end": "978549"
  },
  {
    "text": "down sampling of our data for this reason let's invent yet another component called the compactor that what",
    "start": "978549",
    "end": "983889"
  },
  {
    "text": "it does is down samples all of the data that's stored in object storage and it also compacts multiple smaller TV blocks",
    "start": "983889",
    "end": "990220"
  },
  {
    "text": "into a fewer larger ones so we can make querying more efficient also if you're",
    "start": "990220",
    "end": "998019"
  },
  {
    "text": "interested in more and knowing about dense sampling and how that works again please see Bartok and Frederick's",
    "start": "998019",
    "end": "1004109"
  },
  {
    "text": "talk tomorrow at five twenty and they're gonna be talking about this cool so we've seen how we started out with a",
    "start": "1004109",
    "end": "1010139"
  },
  {
    "text": "federal federal rate of Prometheus we've gone from there to add the query component to allow us to proxy traffic",
    "start": "1010139",
    "end": "1016829"
  },
  {
    "text": "out to multiple side cars the side car then it's taking those requests and",
    "start": "1016829",
    "end": "1022859"
  },
  {
    "text": "proxxon it to a Prometheus with the remote Reid and uploading blocks into object storage at this point we've added",
    "start": "1022859",
    "end": "1029879"
  },
  {
    "text": "in the store gateway which can read data from the object storage and serve it back",
    "start": "1029879",
    "end": "1035209"
  },
  {
    "text": "using the same story pi and finally we've had the compactor which can dance sample and compress data now this works",
    "start": "1035210",
    "end": "1042319"
  },
  {
    "text": "great for improbable whereby we've got tens of clusters maybe up to 40 we can",
    "start": "1042320",
    "end": "1048350"
  },
  {
    "text": "add these story pies into our query components for example but as you get to",
    "start": "1048350",
    "end": "1054710"
  },
  {
    "text": "bigger scale this might not work for you especially if you're creating and destroying clusters on the fly right so",
    "start": "1054710",
    "end": "1060890"
  },
  {
    "text": "one of the things that we're doing at Red Hat is that we have thousands of open shift clusters that are sending",
    "start": "1060890",
    "end": "1065990"
  },
  {
    "text": "thousands of time series back to a monitoring cluster that we're running and one of the issues that um that the",
    "start": "1065990",
    "end": "1073160"
  },
  {
    "text": "traditional thanos setup presents is that we can't ask all of our customers that hey open up your Prometheus",
    "start": "1073160",
    "end": "1078950"
  },
  {
    "text": "endpoint for us so that we can query it number one that's unsafe and number two",
    "start": "1078950",
    "end": "1084140"
  },
  {
    "text": "even for us that would be really cumbersome because we have to manage configuration for every single customer cluster as it goes up and down so this",
    "start": "1084140",
    "end": "1091640"
  },
  {
    "text": "this model just simply won't work for us what we need to do is switch from a model where we're pulling metrics from",
    "start": "1091640",
    "end": "1098660"
  },
  {
    "text": "all of our customers clusters to one where the customers clusters are pushing metrics up to us luckily such a model",
    "start": "1098660",
    "end": "1105740"
  },
  {
    "text": "already exists in the previous ecosystem and this is called the Prometheus remote right API so let's do one more thought",
    "start": "1105740",
    "end": "1112790"
  },
  {
    "text": "experiment and let's say we want to invent one new component that is able to",
    "start": "1112790",
    "end": "1119420"
  },
  {
    "text": "accept remote write requests and what it does is that whenever sees a remote write request it stores these metrics",
    "start": "1119420",
    "end": "1126290"
  },
  {
    "text": "and some local TS DB that it's running in memory there now one thing obviously",
    "start": "1126290",
    "end": "1136220"
  },
  {
    "text": "we're going to need to do to make this component useful is it we need to expose the same store API that we implemented in the sidecar and the query and the",
    "start": "1136220",
    "end": "1143570"
  },
  {
    "text": "store gateway and this way we can query the metrics that are stored in the local API or in the local TS dB so so far so",
    "start": "1143570",
    "end": "1151100"
  },
  {
    "text": "good we now no longer require our customers to open up a port right they just remote write their own checks",
    "start": "1151100",
    "end": "1157040"
  },
  {
    "text": "directly to us and we no longer have to manage configuration about information about all the customers query endpoints",
    "start": "1157040",
    "end": "1162830"
  },
  {
    "text": "so so far we've made a lot of progress but as we start scaling this up from a",
    "start": "1162830",
    "end": "1168350"
  },
  {
    "text": "few clusters to n clusters we're gonna run into an issue this is looking a lot like the same Prometheus Federation setup",
    "start": "1168350",
    "end": "1174679"
  },
  {
    "text": "that we had at the beginning where we have n clusters of thousands of clusters that are all writing data into one",
    "start": "1174679",
    "end": "1180169"
  },
  {
    "text": "single process that's ingesting it this at one point is going to hit a limit and it's going to break for the same reasons so we need to do something to make the",
    "start": "1180169",
    "end": "1187490"
  },
  {
    "text": "system scalable one thing the one way that we can solve this is by turning our",
    "start": "1187490",
    "end": "1194570"
  },
  {
    "text": "new component into a hash ring so let's say we make this component a scalable hash ring that we can grow up and down",
    "start": "1194570",
    "end": "1200330"
  },
  {
    "text": "to suit our needs you know as load grows and these components will take care of forwarding requests to each other and we",
    "start": "1200330",
    "end": "1207230"
  },
  {
    "text": "can load balanced all incoming requests among them one detail that we have to make sure when we're creating this hash",
    "start": "1207230",
    "end": "1212450"
  },
  {
    "text": "ring is that we want to make sure that on all all samples from one given time",
    "start": "1212450",
    "end": "1219020"
  },
  {
    "text": "series always end up in the same replicas so that way the time series is is a is coherent in one single TS DB and",
    "start": "1219020",
    "end": "1227649"
  },
  {
    "text": "this will solve our this will solve the scalability question because now we can grow this hashing as we need but notably",
    "start": "1227649",
    "end": "1234980"
  },
  {
    "text": "this doesn't yet solve our high availability issue so imagine that one",
    "start": "1234980",
    "end": "1240950"
  },
  {
    "text": "of the replicas in this hash ring goes down this means that all time series that are destined for that replica will",
    "start": "1240950",
    "end": "1247700"
  },
  {
    "text": "no longer be able to be accepted so clearly we need to do something one way that we can address this issue is by",
    "start": "1247700",
    "end": "1253640"
  },
  {
    "text": "implementing replication and we can say for example hey all incoming time series have to be replicated to three different",
    "start": "1253640",
    "end": "1260090"
  },
  {
    "text": "replicas and unless they're accepted by two replicas we don't mark the requests as successful now we can suddenly",
    "start": "1260090",
    "end": "1266779"
  },
  {
    "text": "tolerate replicas failures and we can adjust the replication settings obviously we can say replicate to five",
    "start": "1266779",
    "end": "1272330"
  },
  {
    "text": "it must be at least three bla bla bla and this way um we have both scalability",
    "start": "1272330",
    "end": "1277610"
  },
  {
    "text": "and high availability so we've addressed on two of the concerns we still have a",
    "start": "1277610",
    "end": "1284960"
  },
  {
    "text": "question about storage how can we make the data that's stored on how the",
    "start": "1284960",
    "end": "1290840"
  },
  {
    "text": "metrics that are stored in these new components that are receiving metrics how can we make this also long-term and",
    "start": "1290840",
    "end": "1296330"
  },
  {
    "text": "durable luckily we've already addressed storage before with a thumb of sidecar and what we can do is say hey let's make",
    "start": "1296330",
    "end": "1301820"
  },
  {
    "text": "the same coin upload all the data to object storage so yeah here at this moment we can then",
    "start": "1301820",
    "end": "1308110"
  },
  {
    "text": "have that component upload its blocks much like the sidecar does and then add",
    "start": "1308110",
    "end": "1313220"
  },
  {
    "text": "back in the store component that we had earlier so this store gateway will proxy traffic and the query can only query the",
    "start": "1313220",
    "end": "1319880"
  },
  {
    "text": "hash ring where it can query out to object storage this sounds like it would work so now we can use that to get the",
    "start": "1319880",
    "end": "1327860"
  },
  {
    "text": "historical data as well and this effectively is a Prometheus remote",
    "start": "1327860",
    "end": "1334760"
  },
  {
    "text": "receiver so this allows us to use the remote write to store data and upload it to object storage you want to know more",
    "start": "1334760",
    "end": "1343160"
  },
  {
    "text": "about the remote receiver component mattias is giving a talk tomorrow and he's going to show how you can use it in",
    "start": "1343160",
    "end": "1348320"
  },
  {
    "text": "the wild so go along to that talk and heckle him so the last thing that we",
    "start": "1348320",
    "end": "1354470"
  },
  {
    "text": "haven't talked about though in the four quadrants we saw earlier was the fact that we could send alerts and do encoding rules at some point you may",
    "start": "1354470",
    "end": "1362360"
  },
  {
    "text": "want to record over multiple clusters a time series for instance the maximum number of players in a game server",
    "start": "1362360",
    "end": "1367610"
  },
  {
    "text": "across multiple clusters this is where we can use something a lot like Prometheus recording rules so we've",
    "start": "1367610",
    "end": "1374120"
  },
  {
    "text": "taken those and put them into the runoff rule however alerting rules we should",
    "start": "1374120",
    "end": "1380990"
  },
  {
    "text": "make sure we push right down stack more close to where we actually take the data",
    "start": "1380990",
    "end": "1387160"
  },
  {
    "text": "so at this point again we started out with Prometheus in almost like a federated way we've now added the query",
    "start": "1387160",
    "end": "1394610"
  },
  {
    "text": "endpoint which will query data anywhere we've added the remote receiver that we can do remote writes to again we've got",
    "start": "1394610",
    "end": "1402860"
  },
  {
    "text": "the object storage satisfied and then we've added a rule component to allow us",
    "start": "1402860",
    "end": "1408110"
  },
  {
    "text": "to add multiple metrics from clusters together or send alerts to alert manager all right so let's take one last look at",
    "start": "1408110",
    "end": "1416299"
  },
  {
    "text": "our functional diagram for Prometheus and see how this diagram changes now that we're adding in Thomas to the mix",
    "start": "1416299",
    "end": "1422840"
  },
  {
    "text": "so the first thing we did was we decoupled querying from scraping we saw",
    "start": "1422840",
    "end": "1429679"
  },
  {
    "text": "also that by running a stateless query component we're able to provide a highly available querying service",
    "start": "1429679",
    "end": "1436140"
  },
  {
    "text": "next we saw that by exposing metadata about what time series are stored where",
    "start": "1436140",
    "end": "1441250"
  },
  {
    "text": "we're able to make this querying more efficient and we also saw that we can provide an additional way to ingest metrics namely with remote right we also",
    "start": "1441250",
    "end": "1449170"
  },
  {
    "text": "saw that we can persist these metrics in object storage to provide long term storage for all of our data we saw how",
    "start": "1449170",
    "end": "1456880"
  },
  {
    "text": "we can query the data to sort an object storage using the store gateway and also how we can make this query more",
    "start": "1456880",
    "end": "1462760"
  },
  {
    "text": "efficient using a compactor and finally we briefly mentioned how we can use the",
    "start": "1462760",
    "end": "1468940"
  },
  {
    "text": "Thanos ruler to provide a learning rule sorry to provide recording rules for the",
    "start": "1468940",
    "end": "1475510"
  },
  {
    "text": "data that's being received from all of the different story api's and by doing",
    "start": "1475510",
    "end": "1481150"
  },
  {
    "text": "all of this we just built Thanos if you",
    "start": "1481150",
    "end": "1486190"
  },
  {
    "text": "want to give Stannis to try yourself and we've actually uploaded a snorer to",
    "start": "1486190",
    "end": "1491230"
  },
  {
    "text": "cata Koda so you don't actually have to download it and install it and run it you can go to this website and use the",
    "start": "1491230",
    "end": "1497230"
  },
  {
    "text": "online however it is relatively easy to download and run yourself and with that",
    "start": "1497230",
    "end": "1502600"
  },
  {
    "text": "thank you very much",
    "start": "1502600",
    "end": "1505679"
  },
  {
    "text": "so these are the other talks that we mentioned deep dive tomorrow afternoon and that's with the remote receive",
    "start": "1508919",
    "end": "1516519"
  },
  {
    "text": "component tomorrow as well and we're happy to take any questions you repeat",
    "start": "1516519",
    "end": "1528879"
  },
  {
    "text": "the question please remote receive is it being used at any scale in production at the moment yeah we're using it right now inside of Red",
    "start": "1528879",
    "end": "1535210"
  },
  {
    "text": "Hat and we're using it like we said in just data from thousands of open chip clusters and this is going to keep",
    "start": "1535210",
    "end": "1541299"
  },
  {
    "text": "growing we're gonna start doing tens of thousands of time series from different",
    "start": "1541299",
    "end": "1546970"
  },
  {
    "text": "clusters I'm making the renter run so",
    "start": "1546970",
    "end": "1559359"
  },
  {
    "text": "you've got long-term storage now where you can store for years or something but",
    "start": "1559359",
    "end": "1565059"
  },
  {
    "text": "let's say your storage is an infinite how do you then you know expire storage",
    "start": "1565059",
    "end": "1570190"
  },
  {
    "text": "our expire metrics from your from your store the moment correct me if I'm wrong I don't think there's anything that",
    "start": "1570190",
    "end": "1575919"
  },
  {
    "text": "automatically and expired storage for you however because it's in blob storage is literally just folders and files in",
    "start": "1575919",
    "end": "1582100"
  },
  {
    "text": "object storage so you can very quickly purge the their old ones if you wanted by our command I think it's something",
    "start": "1582100",
    "end": "1588789"
  },
  {
    "text": "that the team are looking into for the future but it's definitely not there at the moment right it's also really easy to clean up blocks that are stored in",
    "start": "1588789",
    "end": "1594850"
  },
  {
    "text": "object storage because the way that the Prometheus he has DV format works is that all blocks are named with a ul ID",
    "start": "1594850",
    "end": "1602440"
  },
  {
    "text": "that's a universally LexA graphically sortable ID and the first few bits of it",
    "start": "1602440",
    "end": "1609399"
  },
  {
    "text": "are a date so you can just like sort even an AWS or anywhere you want and say hey anything that's before this name",
    "start": "1609399",
    "end": "1616919"
  },
  {
    "text": "essentially like this lexographic like sorting just delete it or something",
    "start": "1616919",
    "end": "1622590"
  },
  {
    "text": "hey couldn't talk a little bit more about the configuration of two",
    "start": "1625410",
    "end": "1631810"
  },
  {
    "text": "Prometheus's which are still running within the customers cluster within kubernetes cluster and how do you",
    "start": "1631810",
    "end": "1639070"
  },
  {
    "text": "configure remote writes like what do you do with batches and for how long do",
    "start": "1639070",
    "end": "1644080"
  },
  {
    "text": "stored the data locally there right so when Prometheus when Prometheus has",
    "start": "1644080",
    "end": "1650050"
  },
  {
    "text": "remote writing to fanos this is actually just tailing the right ahead log so it's",
    "start": "1650050",
    "end": "1657220"
  },
  {
    "text": "not um it's not like remote writing the data that is already written two blocks on TTV it's actually remote writing what",
    "start": "1657220",
    "end": "1663070"
  },
  {
    "text": "it has in the right ahead log what else anything any red or any right remote",
    "start": "1663070",
    "end": "1670090"
  },
  {
    "text": "write requests that fail will get buffered in memory and preemie this will keep trying to retry those remote write",
    "start": "1670090",
    "end": "1676510"
  },
  {
    "text": "requests so take it from the client side is just the URL to upload exactly so remember so it's just a URL oh yeah I'm",
    "start": "1676510",
    "end": "1683560"
  },
  {
    "text": "sorry that was the question yeah so just the URL that you configure you can also just some other things if you want like relabeling configs and obviously some",
    "start": "1683560",
    "end": "1691120"
  },
  {
    "text": "like proxy and other settings and things like this yeah I went to the codec stack",
    "start": "1691120",
    "end": "1698230"
  },
  {
    "text": "can you give a brief or difference between codecs and why would I run codecs forces us so both of them are",
    "start": "1698230",
    "end": "1706840"
  },
  {
    "text": "quite similar and the communities actually work very closely together so actually I think the guys are going to",
    "start": "1706840",
    "end": "1713410"
  },
  {
    "text": "prove it tomorrow but there's a number of components within cortex that we will also be helping contribute to and use",
    "start": "1713410",
    "end": "1718450"
  },
  {
    "text": "and so they kind of complement each other cortex start off with the the push",
    "start": "1718450",
    "end": "1723610"
  },
  {
    "text": "based approach whereas then I'll start off with a pull based approach and I suppose it comes down to at the start it",
    "start": "1723610",
    "end": "1730570"
  },
  {
    "text": "came down to which approach you wanted to go with I suppose the solutions a bit more difficult now that we both have the",
    "start": "1730570",
    "end": "1736600"
  },
  {
    "text": "pull and push model yeah any other question",
    "start": "1736600",
    "end": "1742980"
  },
  {
    "text": "hi how does this work or does it at all with say glacier class storage for I",
    "start": "1749100",
    "end": "1757909"
  },
  {
    "text": "don't know if you think we have anything that writes the Glacial storage of the minute however I suppose you could set",
    "start": "1757909",
    "end": "1763710"
  },
  {
    "text": "something up in your object storage - yeah you think out you set the lifecycle rules to the bucket and the bucket will",
    "start": "1763710",
    "end": "1770269"
  },
  {
    "text": "yeah varying a little so I suppose you could click put in cold storage your",
    "start": "1770269",
    "end": "1777090"
  },
  {
    "text": "date if you don't want to read it so if you don't want aquaria because i'm not sure we could get the query to do that",
    "start": "1777090",
    "end": "1782340"
  },
  {
    "text": "yeah it'll probably be too expensive to be querying stuff that's in glacier",
    "start": "1782340",
    "end": "1787950"
  },
  {
    "text": "because the whole point is that you're not accessing it frequently right so you wouldn't want to put things there that you're actually gonna be accessing often",
    "start": "1787950",
    "end": "1793620"
  },
  {
    "text": "but what about for forensic investigation for something that might have happened you know to go good yeah",
    "start": "1793620",
    "end": "1800549"
  },
  {
    "text": "you would at work I mean you could definitely serve a cron job to copy the the blocks across interglacial storage",
    "start": "1800549",
    "end": "1807330"
  },
  {
    "text": "from the bucket yes I did to do that investigation you didn't have to just take it back out of glacial storage okay",
    "start": "1807330",
    "end": "1813840"
  },
  {
    "text": "thank you I think that's our time we're going to stick around for a while so if",
    "start": "1813840",
    "end": "1820200"
  },
  {
    "text": "you want to ask any more questions coming grabbers or any of the other members of the team thank you very much thank you",
    "start": "1820200",
    "end": "1825900"
  },
  {
    "text": "[Applause]",
    "start": "1825900",
    "end": "1829589"
  }
]