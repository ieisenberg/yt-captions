[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "all right welcome everyone my name is Sally I'm very lucky to introduce Rohan and Jose from Red Hat they're going to",
    "start": "30",
    "end": "7529"
  },
  {
    "text": "be talking about raw block volumes today so to put this into context for you if you're familiar with the kubernetes",
    "start": "7529",
    "end": "14179"
  },
  {
    "text": "volume subsystem what you're going to know is that for the most part the",
    "start": "14179",
    "end": "19980"
  },
  {
    "text": "system allows you to access volumes as files with a file system applied to it and you'll have a directory mounted into",
    "start": "19980",
    "end": "27660"
  },
  {
    "text": "your container and when you read and write files to that directory you know",
    "start": "27660",
    "end": "32850"
  },
  {
    "text": "it'll be available to you it'll be persisted out there are some subset of applications that don't like to interact",
    "start": "32850",
    "end": "38250"
  },
  {
    "text": "with volumes through a file system interface and they want to be able to have raw block access to those volumes",
    "start": "38250",
    "end": "45090"
  },
  {
    "text": "and so this new feature that is currently in beta raw block volumes is what they're gonna be talking about",
    "start": "45090",
    "end": "50879"
  },
  {
    "text": "today and they'll tell you all about it please give them a warm welcome thank you we appreciate the warmth that's a",
    "start": "50879",
    "end": "60570"
  },
  {
    "start": "60000",
    "end": "75000"
  },
  {
    "text": "little cold in here alright so fair warning this may contain",
    "start": "60570",
    "end": "66240"
  },
  {
    "text": "opinions speculations and bad jokes these are entirely our responsibility and our fault and do not represent the",
    "start": "66240",
    "end": "71460"
  },
  {
    "text": "views of red hat or the rook project alright introductions I'm Jose over there on the over there on",
    "start": "71460",
    "end": "79290"
  },
  {
    "start": "75000",
    "end": "143000"
  },
  {
    "text": "the left I've been in and around storage for about ten years currently I work in open shift container storage or OCS I",
    "start": "79290",
    "end": "87390"
  },
  {
    "text": "participate in six storage and I like hitting things mostly drums this over",
    "start": "87390",
    "end": "94380"
  },
  {
    "text": "here is Rohan he graduated from college in India in 2018 he did a google Summer",
    "start": "94380",
    "end": "100110"
  },
  {
    "text": "of Code with the CNC F and worked on adding an NFS operator to rook currently he also works with OCS focusing on the",
    "start": "100110",
    "end": "106860"
  },
  {
    "text": "rook upstream and if you can't tell by now he loves watching anime and riding motorbikes alright so here's a quick",
    "start": "106860",
    "end": "114780"
  },
  {
    "text": "overview of what we're gonna go through we're at the agenda right now we're gonna go over some of the basics of",
    "start": "114780",
    "end": "120270"
  },
  {
    "text": "storage and kubernetes do another quick overview of raw block TVs especially for those of you just filtering in and",
    "start": "120270",
    "end": "126290"
  },
  {
    "text": "introduction to rook and rook stuff then we'll get into the meat of it of how OS",
    "start": "126290",
    "end": "132180"
  },
  {
    "text": "D's war war have been provisioned and how they're being can be provisioned now and",
    "start": "132180",
    "end": "138460"
  },
  {
    "text": "then we're gonna have a nice little demo at the end all right so to set the stage",
    "start": "138460",
    "end": "144760"
  },
  {
    "start": "143000",
    "end": "258000"
  },
  {
    "text": "storage and kubernetes your basic workflow with storage D States involves",
    "start": "144760",
    "end": "150160"
  },
  {
    "text": "persistent volumes or Peavey's persistent volume claims or PVCs and",
    "start": "150160",
    "end": "155290"
  },
  {
    "text": "storage classes SCS the persistent volume represents a volume of storage in",
    "start": "155290",
    "end": "162040"
  },
  {
    "text": "a storage subsystem somewhere different storage backends defined what a volume represents but for the purposes of an",
    "start": "162040",
    "end": "171280"
  },
  {
    "text": "application running kubernetes they don't entirely care what's running down there and in particular applications",
    "start": "171280",
    "end": "178209"
  },
  {
    "text": "don't interact directly with PVS they interact with persistent volume claims or PVCs which represents a request to",
    "start": "178209",
    "end": "186640"
  },
  {
    "text": "use a piece of storage that is available in kubernetes in particular PBS are non namespaced",
    "start": "186640",
    "end": "192280"
  },
  {
    "text": "so their cluster wide but PVCs our namespace and then you have start classes another non namespace resource",
    "start": "192280",
    "end": "199959"
  },
  {
    "text": "that basically provides a point of access where PVCs can dynamically provision PBS for for their use this is",
    "start": "199959",
    "end": "211060"
  },
  {
    "text": "you know a relatively recent development whereas previously you'd have to",
    "start": "211060",
    "end": "216519"
  },
  {
    "text": "statically define your PBS and then create a PVC that directly referenced that pv now instead you create a PVC",
    "start": "216519",
    "end": "224140"
  },
  {
    "text": "that references the storage class which talks to the storage driver underneath and creates the corresponding PVC here's",
    "start": "224140",
    "end": "231760"
  },
  {
    "text": "a nice little graphic of that where the developer specifies the PVC and",
    "start": "231760",
    "end": "238530"
  },
  {
    "text": "operations or infrared ever specifies the storage class that the PVC is",
    "start": "238530",
    "end": "243850"
  },
  {
    "text": "submitted to it instructs the storage back-end to create the persistent volume at the end and then the persistent",
    "start": "243850",
    "end": "252220"
  },
  {
    "text": "volume is mounted by the application on whatever notes note it's running raw",
    "start": "252220",
    "end": "258910"
  },
  {
    "start": "258000",
    "end": "427000"
  },
  {
    "text": "block TVs so again why rob block TVs um it allows kubernetes to present storage",
    "start": "258910",
    "end": "265660"
  },
  {
    "text": "to containers without having a formatted file system on top of them so the reason for this is",
    "start": "265660",
    "end": "272500"
  },
  {
    "text": "that many applications like databases or software-defined storage can Leverett",
    "start": "272500",
    "end": "278440"
  },
  {
    "text": "storage directly at the block layer meaning that you can cut out a lot of abstraction and indirection that file",
    "start": "278440",
    "end": "285070"
  },
  {
    "text": "system present this allows this typically allows for more consistent IO",
    "start": "285070",
    "end": "290380"
  },
  {
    "text": "performance and low latency access to your storage and you can find a link to",
    "start": "290380",
    "end": "295500"
  },
  {
    "text": "more about how to use that down there the way you access this in your PBS and",
    "start": "295500",
    "end": "302710"
  },
  {
    "text": "PVCs is through a new field called volume mode this went beta and kubernetes 1.13 and it specifies whether",
    "start": "302710",
    "end": "309850"
  },
  {
    "text": "how a storage will be accessed whether with a file system or astrov lock as",
    "start": "309850",
    "end": "314860"
  },
  {
    "text": "mentioned a volume mode field is present in both TVs and PVCs and then you can",
    "start": "314860",
    "end": "320110"
  },
  {
    "text": "just leave it unspecified or specified volume mode file to go with the backwards-compatible default here's what",
    "start": "320110",
    "end": "329020"
  },
  {
    "text": "it looks like for volume mode file again this can be omitted it since it's just",
    "start": "329020",
    "end": "334330"
  },
  {
    "text": "the default and you see over there on the far left or stage left that you just",
    "start": "334330",
    "end": "342310"
  },
  {
    "text": "specify the volumes at the bottom with the PVC name and then you mount and then",
    "start": "342310",
    "end": "348039"
  },
  {
    "text": "you specify that per container into volume mount section volley mode block",
    "start": "348039",
    "end": "353199"
  },
  {
    "text": "looks much the same from the PV and PVC level but when you use your pod you",
    "start": "353199",
    "end": "358240"
  },
  {
    "text": "specify your PVC and then put it under the new a new volume device this section",
    "start": "358240",
    "end": "364180"
  },
  {
    "text": "which which tells kubernetes where in the container that volume device should",
    "start": "364180",
    "end": "370960"
  },
  {
    "text": "appear a quick note here if you're",
    "start": "370960",
    "end": "376539"
  },
  {
    "text": "familiar with this stuff at all you're familiar you're probably familiar with access modes volley mode and access mode",
    "start": "376539",
    "end": "381729"
  },
  {
    "text": "are not synonymous Nord related access modes includes things like read write",
    "start": "381729",
    "end": "386740"
  },
  {
    "text": "many and read write once that controls how many pods may access may attach a",
    "start": "386740",
    "end": "392680"
  },
  {
    "text": "PVC at any given time and whether or not whether or not how many of those pods can write to it",
    "start": "392680",
    "end": "399520"
  },
  {
    "text": "where they intersect is that certain storage drivers that that allow Rob",
    "start": "399520",
    "end": "406690"
  },
  {
    "text": "Locke volumes may allow various different access modes to their rob",
    "start": "406690",
    "end": "413590"
  },
  {
    "text": "block storage depending on what the technology supports this is usually a limitation of whatever technology is",
    "start": "413590",
    "end": "420310"
  },
  {
    "text": "used to attach the storage to the node the depart is running on all right now",
    "start": "420310",
    "end": "428020"
  },
  {
    "start": "427000",
    "end": "573000"
  },
  {
    "text": "rook and rook Seth what is rook we have",
    "start": "428020",
    "end": "433170"
  },
  {
    "text": "rook is storage operators for kubernetes it helps you automate various aspects of",
    "start": "433170",
    "end": "440290"
  },
  {
    "text": "your of maintaining your data plane from deployment to bootstrapping configuration and upgrade rook up rook",
    "start": "440290",
    "end": "448660"
  },
  {
    "text": "is a collection of operators which are a piece of software that implements the operator pattern for storage solutions",
    "start": "448660",
    "end": "456970"
  },
  {
    "text": "like obviously rook stuff here but also edge FS and Mineo in just a quick",
    "start": "456970",
    "end": "464500"
  },
  {
    "text": "overview the operator pattern allows you to define a desired state for any given",
    "start": "464500",
    "end": "471430"
  },
  {
    "text": "resource that you manage or CRD for instance you've probably heard dad before and then the operator works to",
    "start": "471430",
    "end": "477760"
  },
  {
    "text": "reconcile the actual state of the cluster to match the desired state basically it watches for changes in the",
    "start": "477760",
    "end": "484990"
  },
  {
    "text": "cluster or in your desired state configuration and applies changes to the",
    "start": "484990",
    "end": "490870"
  },
  {
    "text": "cluster to make the cluster state match the cluster desired State rooks F was is",
    "start": "490870",
    "end": "499890"
  },
  {
    "text": "is the attempt to bring SEF into containers to give you resilient",
    "start": "499890",
    "end": "504910"
  },
  {
    "text": "distributed storage that is native to kubernetes in particular it's self-healing can recover from a variety",
    "start": "504910",
    "end": "512169"
  },
  {
    "text": "of error conditions and because it's stuff it is highly scalable can run on",
    "start": "512169",
    "end": "519969"
  },
  {
    "text": "commodity hardware and it's fully open source here is just sort of a quick",
    "start": "519969",
    "end": "526390"
  },
  {
    "text": "overview of what a stuff cluster looks like you don't need to know too much",
    "start": "526390",
    "end": "531400"
  },
  {
    "text": "about the individual components that can that will be covered all these things will be covered in talks later this week be sure to attend",
    "start": "531400",
    "end": "538840"
  },
  {
    "text": "the rook intro and deep dive on Thursday if you're interested the part we care",
    "start": "538840",
    "end": "544900"
  },
  {
    "text": "about here is that SEF is divided into a number of demons that produce that have",
    "start": "544900",
    "end": "552820"
  },
  {
    "text": "several functions the one we care about are the OS DS that are towards the end in the top corner there",
    "start": "552820",
    "end": "559210"
  },
  {
    "text": "this stands for what is it objects or objects service demon of the stage",
    "start": "559210",
    "end": "564310"
  },
  {
    "text": "object storage demon and they are what represented the storage device into the",
    "start": "564310",
    "end": "569320"
  },
  {
    "text": "stuff cluster so now we get into oh s DS",
    "start": "569320",
    "end": "576150"
  },
  {
    "start": "573000",
    "end": "690000"
  },
  {
    "text": "how their provision how they were provisioned and how they can be provisioned now the way this works is",
    "start": "576150",
    "end": "585030"
  },
  {
    "text": "that traditionally OS DS use locally attached devices to your",
    "start": "585030",
    "end": "592030"
  },
  {
    "text": "kubernetes nodes so what you would do is that you would set up you know instances or physical machines with storage",
    "start": "592030",
    "end": "599110"
  },
  {
    "text": "devices and then you could specify how those storage devices would be used by",
    "start": "599110",
    "end": "606400"
  },
  {
    "text": "rooks F to build your SEF cluster at its most rudimentary configuration using",
    "start": "606400",
    "end": "612910"
  },
  {
    "text": "devices in this manner is really easy rook in Luke implements a auto discovery",
    "start": "612910",
    "end": "619630"
  },
  {
    "text": "feature that looks through all your kubernetes nodes or the kubernetes notes",
    "start": "619630",
    "end": "625240"
  },
  {
    "text": "you limit it to and finds available non",
    "start": "625240",
    "end": "631360"
  },
  {
    "text": "formatted empty storage devices and just consumes some or all of the storage",
    "start": "631360",
    "end": "637750"
  },
  {
    "text": "devices so at its most basic configuration you can just tell the storage subsystem to use all nodes in",
    "start": "637750",
    "end": "645340"
  },
  {
    "text": "the cluster and use any free devices it finds on those nodes there's obviously mostly ideal if you're",
    "start": "645340",
    "end": "653320"
  },
  {
    "text": "using a purely if you're using a kubernetes cluster purely to host your data plane but you can use a variety of",
    "start": "653320",
    "end": "660340"
  },
  {
    "text": "node selectors and taint in toleration x' to limit",
    "start": "660340",
    "end": "665810"
  },
  {
    "text": "where the rook SEF pods will run so",
    "start": "665810",
    "end": "670880"
  },
  {
    "text": "really all you have to do is define your storage nodes define your local devices and then rook takes care of the rest it",
    "start": "670880",
    "end": "678440"
  },
  {
    "text": "will prepare the storage devices for consumption by SEF and then start the",
    "start": "678440",
    "end": "683450"
  },
  {
    "text": "OSD pods on the nodes that contain those storage devices on the upside these",
    "start": "683450",
    "end": "691970"
  },
  {
    "start": "690000",
    "end": "1107000"
  },
  {
    "text": "things are easy to configure and for especially for most storage administrators coming into kubernetes",
    "start": "691970",
    "end": "698300"
  },
  {
    "text": "for the first time this is very familiar to them because they're very much used to attaching physical physical disk to",
    "start": "698300",
    "end": "704840"
  },
  {
    "text": "physical nodes and then consuming them especially if they're coming from the straight-set world and coming in to rook",
    "start": "704840",
    "end": "710780"
  },
  {
    "text": "this is very familiar to them and in particular disappoints a any type of device or appliance that linux support",
    "start": "710780",
    "end": "717560"
  },
  {
    "text": "since all it looks for is a device on the node on the other hand especially",
    "start": "717560",
    "end": "725090"
  },
  {
    "text": "for there's especially concerned for people deploying on cloud environments this typically requires specialized",
    "start": "725090",
    "end": "730760"
  },
  {
    "text": "nodes that may or may not be ideal when you're dealing when you're trying to scale out and deal with not-- deal with",
    "start": "730760",
    "end": "739940"
  },
  {
    "text": "node failures that may occur and it provides it enforces a rigid coupling",
    "start": "739940",
    "end": "746390"
  },
  {
    "text": "between compute and storage such that there's no real way to sort of easily",
    "start": "746390",
    "end": "752720"
  },
  {
    "text": "spin up new storage cattle basically so",
    "start": "752720",
    "end": "760270"
  },
  {
    "text": "we implemented a thing that ended up being called storage plus device sets",
    "start": "760270",
    "end": "765430"
  },
  {
    "text": "the procedure for setting this up is much the same as with local storage OS",
    "start": "765430",
    "end": "772940"
  },
  {
    "text": "DS except that now you define a desired amount of storage rather than the exact",
    "start": "772940",
    "end": "779660"
  },
  {
    "text": "devices you want to use but as far as rogue stuff is concerned these end up",
    "start": "779660",
    "end": "785510"
  },
  {
    "text": "being consumed in much the same way and present it to SEF in the same way so the main difference is that is in how you",
    "start": "785510",
    "end": "793070"
  },
  {
    "text": "actually define the devices to be consumed",
    "start": "793070",
    "end": "797680"
  },
  {
    "text": "Sturge plus devices were designed as a generic rook struct and as UNF you if",
    "start": "798100",
    "end": "804650"
  },
  {
    "text": "you'll remember rook is a collection of operators for multiple storage services",
    "start": "804650",
    "end": "809680"
  },
  {
    "text": "so the storage class device sets are not you know are fairly extensible and",
    "start": "809680",
    "end": "816290"
  },
  {
    "text": "feature-rich but some features of them are not used in Brookes F we just use a",
    "start": "816290",
    "end": "821960"
  },
  {
    "text": "small number we just basically use what we need to get stuff up and running so",
    "start": "821960",
    "end": "828500"
  },
  {
    "text": "the first thing we need to set is a name for your search class device set this is",
    "start": "828500",
    "end": "834260"
  },
  {
    "text": "used in part to generate unique and consistent PVC names for the PVCs that",
    "start": "834260",
    "end": "839390"
  },
  {
    "text": "will be consumed by the OS DS but it also helps internally to sort of to keep",
    "start": "839390",
    "end": "844640"
  },
  {
    "text": "track of which device belongs to which set because if you can see on the second",
    "start": "844640",
    "end": "850190"
  },
  {
    "text": "line there this is actually a list of storage class device sets you can",
    "start": "850190",
    "end": "855620"
  },
  {
    "text": "specify more than one set per storage cluster and we didn't want to necessarily go with a straight mat with",
    "start": "855620",
    "end": "861530"
  },
  {
    "text": "a width map to to set this up so we just",
    "start": "861530",
    "end": "867860"
  },
  {
    "text": "went with the list of named items now here we get to count count is the number",
    "start": "867860",
    "end": "874010"
  },
  {
    "text": "of devices in this set so if you set a count to 3 3 OS DS will be spun up in",
    "start": "874010",
    "end": "880010"
  },
  {
    "text": "association with this set you could also for instance specify three different sets with a count of 1",
    "start": "880010",
    "end": "886270"
  },
  {
    "text": "to also get three storage 3os Dee's running in your cluster and I'll explain",
    "start": "886270",
    "end": "893900"
  },
  {
    "text": "more about that in a little bit all right portable this is the big feature",
    "start": "893900",
    "end": "900250"
  },
  {
    "text": "for storage class device set meaning that PVCs are allowed to move between",
    "start": "900250",
    "end": "905990"
  },
  {
    "text": "nodes this is in particular an important configuration for Brooke stuff because",
    "start": "905990",
    "end": "911210"
  },
  {
    "text": "this is how you trigger the new behavior that I'll that will cut a new behavior",
    "start": "911210",
    "end": "917780"
  },
  {
    "text": "that we had to come up with that we'll cover in in a little bit in a little bit here and then volume clang templates",
    "start": "917780",
    "end": "924560"
  },
  {
    "text": "this is just a list of PVC templates that follow it's",
    "start": "924560",
    "end": "930090"
  },
  {
    "text": "it just imports the standard spec so if you've written any PVC if you've written",
    "start": "930090",
    "end": "935130"
  },
  {
    "text": "any PVC ya Mille this should look very familiar you specified the amount of storage you want the storage class you",
    "start": "935130",
    "end": "941820"
  },
  {
    "text": "want to provisioned against volley mode block as a requirement here and then any access modes that you want set for this",
    "start": "941820",
    "end": "948510"
  },
  {
    "text": "PVC you can see that that is also a list",
    "start": "948510",
    "end": "953960"
  },
  {
    "text": "at the moment we rooks F only supports setting one PVC but this is something we",
    "start": "953960",
    "end": "962010"
  },
  {
    "text": "may be looking to extend in the future for more advanced OSD configurations the",
    "start": "962010",
    "end": "969690"
  },
  {
    "text": "pros of this you can offload your device distribution to the kubernetes scheduler since these things are just running as",
    "start": "969690",
    "end": "977550"
  },
  {
    "text": "pods now this scheduler will take care of finding notes with appropriate resources and scheduling or moving pods",
    "start": "977550",
    "end": "985410"
  },
  {
    "text": "around as needed incidentally device migration between nodes is now possible",
    "start": "985410",
    "end": "991640"
  },
  {
    "text": "as I mentioned earlier stuff is a self-healing self-healing storage",
    "start": "991640",
    "end": "997290"
  },
  {
    "text": "service meaning that there's a lot there can be a lot of a lot of data traffic",
    "start": "997290",
    "end": "1003170"
  },
  {
    "text": "going around as it tries to rebalance how the various blocks are distributed",
    "start": "1003170",
    "end": "1008810"
  },
  {
    "text": "across the cluster if you have you know multiple replicas across multiple failure domains etc with device",
    "start": "1008810",
    "end": "1015470"
  },
  {
    "text": "migration as long as the device can migrate quickly enough Ceph will just",
    "start": "1015470",
    "end": "1022430"
  },
  {
    "text": "detect the device is back and not need to perform any and not need to perform",
    "start": "1022430",
    "end": "1028579"
  },
  {
    "text": "any data migration since it will just use the data that's",
    "start": "1028580",
    "end": "1035300"
  },
  {
    "text": "already present on the disk so no additional replication no need to move anything around and this works with just",
    "start": "1035300",
    "end": "1043490"
  },
  {
    "text": "any Rock raw block TV regardless of the storage driver underneath so as long as",
    "start": "1043490",
    "end": "1050420"
  },
  {
    "text": "your as long as your storage vendor supports using volume mode block on",
    "start": "1050420",
    "end": "1055490"
  },
  {
    "text": "there PBS we can consume it and of course this is shiny and new so it's",
    "start": "1055490",
    "end": "1061550"
  },
  {
    "text": "very exciting at least for me unfortunately it's a bit bulkier to deal",
    "start": "1061550",
    "end": "1067529"
  },
  {
    "text": "with than just than just using local storage devices your you need to have a",
    "start": "1067529",
    "end": "1072740"
  },
  {
    "text": "predefined storage class that allows you to access your desired block volumes",
    "start": "1072740",
    "end": "1079559"
  },
  {
    "text": "such as GP to storage class in AWS the",
    "start": "1079559",
    "end": "1085289"
  },
  {
    "text": "device support is not as robust as it is in just straight Linux we are limited",
    "start": "1085289",
    "end": "1090690"
  },
  {
    "text": "only to what we are limited to only what kubernetes supports and as you saw",
    "start": "1090690",
    "end": "1096419"
  },
  {
    "text": "earlier the configuration for this can get fairly extensive and it's not even",
    "start": "1096419",
    "end": "1101580"
  },
  {
    "text": "different so it's natural to be skeptical of these things all right and",
    "start": "1101580",
    "end": "1109020"
  },
  {
    "start": "1107000",
    "end": "1267000"
  },
  {
    "text": "now I'm gonna turn this over to Rohan to",
    "start": "1109020",
    "end": "1114090"
  },
  {
    "text": "go over some of the bumps in the road that we found along the way to implementing this so the first issue",
    "start": "1114090",
    "end": "1125490"
  },
  {
    "text": "which we faced was when we mount the block TVs on top of privileged containers so we won't find a block",
    "start": "1125490",
    "end": "1132510"
  },
  {
    "text": "babies there because the /dev is bind mounted to into the container and it",
    "start": "1132510",
    "end": "1137970"
  },
  {
    "text": "prevents given it is from presenting the block device at the desired path to solve this we use a little trick so we",
    "start": "1137970",
    "end": "1145260"
  },
  {
    "text": "use a non privileged init container to copy the device as the devices a file into the Linux system so that file was",
    "start": "1145260",
    "end": "1153419"
  },
  {
    "text": "copied to a shared directory which was shared between the inner container and the privileged container thanks to John",
    "start": "1153419",
    "end": "1158760"
  },
  {
    "text": "for this so if I look at the right-hand side bottom we have two volumes one is",
    "start": "1158760",
    "end": "1165059"
  },
  {
    "text": "the block TV that is set one def 0 and next we have in in today a directory so",
    "start": "1165059",
    "end": "1170580"
  },
  {
    "text": "if you look at the init container is basically just copying the block device",
    "start": "1170580",
    "end": "1176010"
  },
  {
    "text": "to the shared directory and on the left hand side we see we just have the shared",
    "start": "1176010",
    "end": "1181200"
  },
  {
    "text": "directory mounted to the privileged container so the next issue which we faced was the who is DS when we spin up",
    "start": "1181200",
    "end": "1189360"
  },
  {
    "text": "on the same node like we spin like 1000 is DS and 3 we're on the same node so one or two oh is this way failing",
    "start": "1189360",
    "end": "1196860"
  },
  {
    "text": "this was because when chef volume prepares the LVM so we have /dev mounted",
    "start": "1196860",
    "end": "1201899"
  },
  {
    "text": "and cuban it is monster block device as loopback device so when slash Davis",
    "start": "1201899",
    "end": "1207539"
  },
  {
    "text": "motors we have the original device of the node there as well as the loopback device so when we use a chef osg command",
    "start": "1207539",
    "end": "1214890"
  },
  {
    "text": "to start so it detects like that iridium is having two Peavey's linked to it one",
    "start": "1214890",
    "end": "1221519"
  },
  {
    "text": "is the loopback device and one is the actual devices there to simplify this to solve this we just pass the exact every",
    "start": "1221519",
    "end": "1229049"
  },
  {
    "text": "path that is slash def the name of the waging and then the name of the LV the",
    "start": "1229049",
    "end": "1234570"
  },
  {
    "text": "last problem which we faced was so G's we're clustering onto a same node suppose we way if we spend 1000 series",
    "start": "1234570",
    "end": "1241529"
  },
  {
    "text": "and we had three nodes so like five to six always DS we're coming out on the same node so suppose we have three",
    "start": "1241529",
    "end": "1248279"
  },
  {
    "text": "replicas for the OS days sewed and all they are on the same node and if the node goes down we lose our data so which",
    "start": "1248279",
    "end": "1254789"
  },
  {
    "text": "was which is undesired so to solve this we use placement affinities so this is a",
    "start": "1254789",
    "end": "1261059"
  },
  {
    "text": "basic placement affinity that will distribute the OECD's around nodes now",
    "start": "1261059",
    "end": "1267360"
  },
  {
    "text": "it's show time so for the time off we have an open ship cluster running upon AWS and why do we have open",
    "start": "1267360",
    "end": "1275549"
  },
  {
    "text": "well the main reason we have opened openshift cluster on is because Red Hat you know we we have a lot of internal",
    "start": "1275549",
    "end": "1282539"
  },
  {
    "text": "automation to make it easy to set up an AWS cluster and run openshift and in",
    "start": "1282539",
    "end": "1287760"
  },
  {
    "text": "particular we're going to be using a an aspect of open shift for machine",
    "start": "1287760",
    "end": "1293820"
  },
  {
    "text": "management which makes it easy to tear down and bring notes back up from the",
    "start": "1293820",
    "end": "1300090"
  },
  {
    "text": "open chef console cool so if you have T is the font okay for every one volts in",
    "start": "1300090",
    "end": "1307710"
  },
  {
    "text": "the back bigger",
    "start": "1307710",
    "end": "1313100"
  },
  {
    "text": "sorry you got black on white",
    "start": "1315940",
    "end": "1319779"
  },
  {
    "text": "why don't why don't ya good so we have",
    "start": "1326490",
    "end": "1332999"
  },
  {
    "text": "three OS is running on three different nodes if you see here it's one is on 1 7",
    "start": "1332999",
    "end": "1339269"
  },
  {
    "text": "8 1 is 1 1 6 3 1 is on 45 so we when we",
    "start": "1339269",
    "end": "1345450"
  },
  {
    "text": "check the Ceph health so we have 3 oasis 3 are up and 3 are in now we check the",
    "start": "1345450",
    "end": "1351570"
  },
  {
    "text": "nodes so here we have 4 worker nodes and",
    "start": "1351570",
    "end": "1359779"
  },
  {
    "text": "Aziz are on 3 of them",
    "start": "1359779",
    "end": "1363710"
  },
  {
    "text": "so now we're accessing the OpenShift machine API to look at the machines that",
    "start": "1368650",
    "end": "1374930"
  },
  {
    "text": "we have set up the main thing to note for machines is that these machines are",
    "start": "1374930",
    "end": "1380000"
  },
  {
    "text": "controlled by what's known as a machine set and a machine set basically is",
    "start": "1380000",
    "end": "1386900"
  },
  {
    "text": "allows you to just like like a replica",
    "start": "1386900",
    "end": "1392750"
  },
  {
    "text": "set for instance its job is to through the use of the through the use of the",
    "start": "1392750",
    "end": "1398720"
  },
  {
    "text": "machine set controller or the OpenShift machine api api operator is to make sure",
    "start": "1398720",
    "end": "1407870"
  },
  {
    "text": "that there are always enough machines to satisfy the needs of the machine set so what we're gonna do here is delete what",
    "start": "1407870",
    "end": "1414980"
  },
  {
    "text": "is delete one of the machines that's part of a machine set so that it goes down and another one is being brought up",
    "start": "1414980",
    "end": "1421370"
  },
  {
    "text": "but since we don't want to wait for the next machine to be brought up we're just going to evict all the pods from the",
    "start": "1421370",
    "end": "1428540"
  },
  {
    "text": "first machine we're deleting and they should be brought up elsewhere so we",
    "start": "1428540",
    "end": "1435290"
  },
  {
    "text": "have this OSD that is running on IP ending with 163 and here is the machine",
    "start": "1435290",
    "end": "1441290"
  },
  {
    "text": "for that one which has a pending which one is 6-3 I will just copy the name of",
    "start": "1441290",
    "end": "1447950"
  },
  {
    "text": "the machine and we'll do a delete machine name of the Machine and the",
    "start": "1447950",
    "end": "1457730"
  },
  {
    "text": "namespace",
    "start": "1457730",
    "end": "1460210"
  },
  {
    "text": "here we go so when I deleted the machine we see an OSD event into the pending",
    "start": "1465680",
    "end": "1471920"
  },
  {
    "text": "state this is the new Z that is coming up Juan Jose that went off it vanished",
    "start": "1471920",
    "end": "1477140"
  },
  {
    "text": "so it is the uzi that was there on the node which we deleted and that always reminded it to a new node which is one",
    "start": "1477140",
    "end": "1485210"
  },
  {
    "text": "7:18 in this case and we will just wait for it to come up here we can see one of",
    "start": "1485210",
    "end": "1491960"
  },
  {
    "text": "the moon Mons is down because one of the non node went down and 300 G's are there",
    "start": "1491960",
    "end": "1497870"
  },
  {
    "text": "and two are up and three are in so when this comes back to the running state we",
    "start": "1497870",
    "end": "1504140"
  },
  {
    "text": "will see that the health is fine it will take a while let's see do n do an OC",
    "start": "1504140",
    "end": "1516500"
  },
  {
    "text": "described on the OSD yes it's back up nope there goes yep so it will take a",
    "start": "1516500",
    "end": "1523340"
  },
  {
    "text": "while for the health to be back to normal and yeah here it is it's is going",
    "start": "1523340",
    "end": "1529310"
  },
  {
    "text": "to get normal after the moon is up I",
    "start": "1529310",
    "end": "1533800"
  },
  {
    "text": "think that's all dated yep yep so huzzah",
    "start": "1535300",
    "end": "1541400"
  },
  {
    "text": "we have our storage backup and that was all in under a minute yep alright so that's the end of the",
    "start": "1541400",
    "end": "1551960"
  },
  {
    "start": "1551000",
    "end": "1572000"
  },
  {
    "text": "demo pretty quick talk thank you [Applause]",
    "start": "1551960",
    "end": "1558630"
  },
  {
    "text": "now it's time for the questions yep if you have any questions feel free if you want us to go back and touch on",
    "start": "1560360",
    "end": "1566340"
  },
  {
    "text": "something a little deeper we're happy to do so we got the time for it and when",
    "start": "1566340",
    "end": "1574020"
  },
  {
    "start": "1572000",
    "end": "1592000"
  },
  {
    "text": "you were talking about the OS DS you mentioned about a manual auto discover",
    "start": "1574020",
    "end": "1579330"
  },
  {
    "text": "modes can you give us a use case for each one why would you need to discover",
    "start": "1579330",
    "end": "1585390"
  },
  {
    "text": "modes there why can't we just order disco why can't you what would it be Auto disco why would you need manual Oh",
    "start": "1585390",
    "end": "1592730"
  },
  {
    "start": "1592000",
    "end": "1656000"
  },
  {
    "text": "why would you arm as this is in particular if for some reason you have a",
    "start": "1592730",
    "end": "1600210"
  },
  {
    "text": "mix of storage devices on a given node that you want to make sure and you want",
    "start": "1600210",
    "end": "1605700"
  },
  {
    "text": "to make sure you only use a certain type of storage and not the other this comes",
    "start": "1605700",
    "end": "1611040"
  },
  {
    "text": "from this comes from the early days of how you run Seth and in particular you",
    "start": "1611040",
    "end": "1617850"
  },
  {
    "text": "could run multiple Seth clusters in the same kubernetes cluster so you could",
    "start": "1617850",
    "end": "1624870"
  },
  {
    "text": "have you could have the same node providing storage for different stuff clusters this is something that the rook",
    "start": "1624870",
    "end": "1631049"
  },
  {
    "text": "the rook project wanted to make sure that they that they supported but then",
    "start": "1631049",
    "end": "1637860"
  },
  {
    "text": "also it's a matter of it could also be a matter of security policy where you want to make sure that there is no like you",
    "start": "1637860",
    "end": "1645000"
  },
  {
    "text": "can you can add additional storage devices to the node but you don't want them necessarily automatically consumed",
    "start": "1645000",
    "end": "1651720"
  },
  {
    "text": "so you want to manually change the configuration before they're consumed I",
    "start": "1651720",
    "end": "1657740"
  },
  {
    "start": "1656000",
    "end": "1703000"
  },
  {
    "text": "you mentioned the need for an init container to get get a device into a",
    "start": "1658010",
    "end": "1664620"
  },
  {
    "text": "privileged pod is there a way to run OS",
    "start": "1664620",
    "end": "1670440"
  },
  {
    "text": "DS in an unprivileged fashion now that you can sort of directly get at the block devices there's not a limitation",
    "start": "1670440",
    "end": "1676200"
  },
  {
    "text": "of because we need to use LVM and nvm",
    "start": "1676200",
    "end": "1681390"
  },
  {
    "text": "works only on the privileged containers okay yeah there's conflicts between the",
    "start": "1681390",
    "end": "1689680"
  },
  {
    "text": "I think it's with the namespaces the security namespaces when using LVM inside a container versus on the host",
    "start": "1689680",
    "end": "1697300"
  },
  {
    "text": "itself we got someone back there with",
    "start": "1697300",
    "end": "1707620"
  },
  {
    "start": "1703000",
    "end": "1740000"
  },
  {
    "text": "Luke is it possible to run an hour more than one why OSD for each device at the",
    "start": "1707620",
    "end": "1716860"
  },
  {
    "text": "moment no you can only run one OSD per storage device okay so if I have like an",
    "start": "1716860",
    "end": "1722320"
  },
  {
    "text": "AI nvme or a SSB and that needs more OS DS to get the complete performance",
    "start": "1722320",
    "end": "1727390"
  },
  {
    "text": "that's not at the moment but we left room for that to be implemented down the",
    "start": "1727390",
    "end": "1733270"
  },
  {
    "text": "road okay cool thank you yeah if you want if you want to contribute patches",
    "start": "1733270",
    "end": "1738310"
  },
  {
    "text": "eagerly accepted hey so I could I didn't",
    "start": "1738310",
    "end": "1745720"
  },
  {
    "start": "1740000",
    "end": "1783000"
  },
  {
    "text": "understood when you said you were copying the data for the device could",
    "start": "1745720",
    "end": "1750730"
  },
  {
    "text": "you explain why do you want to copy the data so we can't have the block device in the privileged container so just to",
    "start": "1750730",
    "end": "1758410"
  },
  {
    "text": "make sure that we have the block device in the privileged container we were copying into a shared directory which was there in the init container as well",
    "start": "1758410",
    "end": "1764650"
  },
  {
    "text": "as the privileged container yeah in each connector and unique container was in on privileged container and that has the",
    "start": "1764650",
    "end": "1770740"
  },
  {
    "text": "blob device attached to it block PV yeah we're not copying the actual data on the block device we're copying the file that",
    "start": "1770740",
    "end": "1777730"
  },
  {
    "text": "represents the block device in Linux from one location to the other case you can do that yeah regarding the why do",
    "start": "1777730",
    "end": "1786970"
  },
  {
    "start": "1783000",
    "end": "1836000"
  },
  {
    "text": "you have to like you can add a capability right security context Forest",
    "start": "1786970",
    "end": "1792220"
  },
  {
    "text": "for the purpose of the LVM can you repeat the question so you said you want",
    "start": "1792220",
    "end": "1797920"
  },
  {
    "text": "to set privileged is equal to true for the container instead you can do it at",
    "start": "1797920",
    "end": "1803710"
  },
  {
    "text": "the granular level like you can add what the capabilities required for running",
    "start": "1803710",
    "end": "1809260"
  },
  {
    "text": "the ilbm yeah unfortunately we did try to narrow down just a set of",
    "start": "1809260",
    "end": "1815410"
  },
  {
    "text": "capabilities that we could specify to the container but we weren't getting anything that actually",
    "start": "1815410",
    "end": "1821149"
  },
  {
    "text": "LVM happy so the only thing we could find that would actually make LVM",
    "start": "1821149",
    "end": "1826729"
  },
  {
    "text": "function properly was to use privilege was to have it run as privileged one",
    "start": "1826729",
    "end": "1837289"
  },
  {
    "start": "1836000",
    "end": "1899000"
  },
  {
    "text": "more question so when you are running with kubernetes I mean when you're running surf with kubernetes if i OS B",
    "start": "1837289",
    "end": "1844460"
  },
  {
    "text": "fails then kubernetes might go on might try to restart it on some other node right but the device may not be the same",
    "start": "1844460",
    "end": "1852259"
  },
  {
    "text": "like now here I could have diverse PA but there it could be diversity II right how do we handle those kind of",
    "start": "1852259",
    "end": "1858710"
  },
  {
    "text": "situations so right now we are using the AWS EBS volumes because they are",
    "start": "1858710",
    "end": "1866299"
  },
  {
    "text": "portable quite portable and when they move from one node to another is the same device but in case of local storage",
    "start": "1866299",
    "end": "1872779"
  },
  {
    "text": "provisional we have some complications there and we are trying to get it fixed okay yeah so in particular kubernetes or",
    "start": "1872779",
    "end": "1881659"
  },
  {
    "text": "Rob lock creates a loopback device which it uses to present the storage into the container so we don't have to care where",
    "start": "1881659",
    "end": "1888619"
  },
  {
    "text": "it's located on the host itself because we just care about the volume device or the volume device file that is in the",
    "start": "1888619",
    "end": "1895969"
  },
  {
    "text": "container anyone else oh we got one over",
    "start": "1895969",
    "end": "1902690"
  },
  {
    "start": "1899000",
    "end": "1981000"
  },
  {
    "text": "here all the way on the other side of the room Thank You Saad",
    "start": "1902690",
    "end": "1910029"
  },
  {
    "text": "so sorry if I miss this but so the one once you create this F cluster the thing that's being exposed to the applications",
    "start": "1914830",
    "end": "1920770"
  },
  {
    "text": "that are consuming this are they consuming it as raw block devices or is there a file system created somewhere",
    "start": "1920770",
    "end": "1926770"
  },
  {
    "text": "and if so by whom they can but they don't have to so there's the distinction",
    "start": "1926770",
    "end": "1932830"
  },
  {
    "text": "between now that now you have to make the distinction between the PVCs Seth is consuming for its OS DS and a PV",
    "start": "1932830",
    "end": "1940540"
  },
  {
    "text": "and the PVCs it provides for applications these are two completely distinct sets so you know one uses the",
    "start": "1940540",
    "end": "1947740"
  },
  {
    "text": "underlying storage provider like EBS or or Azure discs or whatever and then",
    "start": "1947740",
    "end": "1953410"
  },
  {
    "text": "stuff just presents set volumes and if you're using RBD volumes you can consume",
    "start": "1953410",
    "end": "1959530"
  },
  {
    "text": "those as either raw block or file volumes so it will be some up to something above this to create those",
    "start": "1959530",
    "end": "1966730"
  },
  {
    "text": "carve them up create file systems or alvey's or whatever it is yep cool",
    "start": "1966730",
    "end": "1972670"
  },
  {
    "text": "although typically applications that would want to use raw block devices don't need to format their own file",
    "start": "1972670",
    "end": "1979150"
  },
  {
    "text": "system how does it handle multipath or",
    "start": "1979150",
    "end": "1988540"
  },
  {
    "start": "1981000",
    "end": "2005000"
  },
  {
    "text": "when you have multiple paths to the same device what does discovery do do we have",
    "start": "1988540",
    "end": "1994840"
  },
  {
    "text": "we don't have any of the rook maintainer x' here unfortunately that one I don't know my guess would be that are you",
    "start": "1994840",
    "end": "2003060"
  },
  {
    "text": "talking about the local device auto-discovery yeah fresh device is for",
    "start": "2003060",
    "end": "2009360"
  },
  {
    "start": "2005000",
    "end": "2031000"
  },
  {
    "text": "instance you just have dual ported drives and so the same device shows up",
    "start": "2009360",
    "end": "2014600"
  },
  {
    "text": "as two instances but it's the same worldwide device okay yeah I don't know",
    "start": "2014600",
    "end": "2024090"
  },
  {
    "text": "about that you can feel free to bring that up in the rook session later this week okay will do and what else this",
    "start": "2024090",
    "end": "2034080"
  },
  {
    "text": "side has been a little sparse anyone doesn't look like it so I think that's",
    "start": "2034080",
    "end": "2039700"
  },
  {
    "text": "it all right thank you [Applause]",
    "start": "2039700",
    "end": "2045170"
  }
]