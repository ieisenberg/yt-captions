[
  {
    "text": "hello everyone I'm Kasim fields and I am your track host today for the last",
    "start": "0",
    "end": "5759"
  },
  {
    "text": "session of the day congratulations to all of you for making it this far woo you are all the real MVPs so we are",
    "start": "5759",
    "end": "20070"
  },
  {
    "text": "here today for the last session of the performance track staying in tune",
    "start": "20070",
    "end": "25949"
  },
  {
    "text": "optimized kubernetes for stability and utilization by Randy Johnson and couchy",
    "start": "25949",
    "end": "32340"
  },
  {
    "text": "credit Krishna from VMware I'm here to remind you there will be a feedback",
    "start": "32340",
    "end": "37980"
  },
  {
    "text": "forum available after the session our speakers and we had the CNC F would really appreciate it if you would give",
    "start": "37980",
    "end": "43829"
  },
  {
    "text": "us your feedback I know they're really hard to remember to do but we would very much appreciate it and I will be running",
    "start": "43829",
    "end": "49410"
  },
  {
    "text": "around the microphone for QA at the end of the session now Randy and kashuk take",
    "start": "49410",
    "end": "54960"
  },
  {
    "text": "us away thank you so much for being here it's the last session like we mentioned",
    "start": "54960",
    "end": "63180"
  },
  {
    "text": "so this is we're going to talk about staying in tune optimizing your cumulus",
    "start": "63180",
    "end": "68400"
  },
  {
    "text": "clusters for stability and utilization it's a very overlooked topic and I think",
    "start": "68400",
    "end": "74040"
  },
  {
    "text": "whatever we learnt in field hearing as a field engineer at VMware we've tried to",
    "start": "74040",
    "end": "79560"
  },
  {
    "text": "put it in this topic and hopefully you'll take it back and implement and tune your clusters accordingly",
    "start": "79560",
    "end": "86310"
  },
  {
    "text": "so I'm Kyle shake Radhakrishnan I'm Randy Johnson so let's talk about others",
    "start": "86310",
    "end": "92640"
  },
  {
    "text": "I mean communities it has potential it's gone way beyond potential right now I",
    "start": "92640",
    "end": "98040"
  },
  {
    "text": "think the first step to have this increase beyond its potential is the",
    "start": "98040",
    "end": "104040"
  },
  {
    "text": "declarative API I think we all know like API is like the first-class citizen it's",
    "start": "104040",
    "end": "109619"
  },
  {
    "text": "all about software interfacing with software and then the next important",
    "start": "109619",
    "end": "114840"
  },
  {
    "text": "aspect of this is the control loop aspect I mean that's a self-healing side of things autonomously converging on",
    "start": "114840",
    "end": "121049"
  },
  {
    "text": "building blocks you're having rezulin see at the app and infra layers so these",
    "start": "121049",
    "end": "126899"
  },
  {
    "text": "are kind of the two main primitives but for a talk you're going to look at the bin packing as",
    "start": "126899",
    "end": "132900"
  },
  {
    "text": "which is very important and we'll be discussing a lot about limits and requests it's all about determining",
    "start": "132900",
    "end": "139200"
  },
  {
    "text": "where is the best part placement its approximation with like performance guarantee like we mentioned so this is",
    "start": "139200",
    "end": "146189"
  },
  {
    "text": "what we're going to be looking at all right no talk so we have to break these",
    "start": "146189",
    "end": "151859"
  },
  {
    "text": "invisible walls right I think we always have these default parameters that we set up and we tune our clusters that we",
    "start": "151859",
    "end": "158849"
  },
  {
    "text": "go through but we need to break these walls and optimize our clusters accordingly I think we don't know where",
    "start": "158849",
    "end": "166379"
  },
  {
    "text": "that ceiling is a rather invisible Wallis and we have to look at these clusters and then take these default",
    "start": "166379",
    "end": "174450"
  },
  {
    "text": "values that's been set up and then kind of change them so for the goals of this talk is the two main things that we're",
    "start": "174450",
    "end": "181290"
  },
  {
    "text": "trying to look at like we mentioned its stability and utilization they always",
    "start": "181290",
    "end": "186629"
  },
  {
    "text": "seem to be at odds with each other but we believe that with a solid understanding of these stability and",
    "start": "186629",
    "end": "193560"
  },
  {
    "text": "utilization we can achieve both which is always good for your business as well so",
    "start": "193560",
    "end": "200389"
  },
  {
    "text": "what are the challenges let's look at the challenges that we have I mean Kuban it is isn't configured for our app I",
    "start": "200840",
    "end": "207480"
  },
  {
    "text": "mean it's resources burst priority you're looking at a lot of these aspects",
    "start": "207480",
    "end": "213030"
  },
  {
    "text": "for when you're having app getting deployed onto your cube - cluster configuration is also very important and",
    "start": "213030",
    "end": "220049"
  },
  {
    "text": "I mean when we have your app deployed on your cumulus cluster your utilization is increasing the behavior of clusters",
    "start": "220049",
    "end": "227069"
  },
  {
    "text": "become even more complicated we enough in a talk we're going to depth about how",
    "start": "227069",
    "end": "233010"
  },
  {
    "text": "these behaviors are affect us or affect these clusters and like I said I mean",
    "start": "233010",
    "end": "239940"
  },
  {
    "text": "the app the utilize we have these apps are getting configured the utilization increases whenever utilization increases",
    "start": "239940",
    "end": "246599"
  },
  {
    "text": "we're always leading into disruptions there's expected disruptions that you can see and there's an unexpected",
    "start": "246599",
    "end": "252150"
  },
  {
    "text": "disruptions that you're probably not in going to encounter I mean you're gonna",
    "start": "252150",
    "end": "257760"
  },
  {
    "text": "encounter it but you're not aware of so how can we maintain stability when you have these disruptions so we always",
    "start": "257760",
    "end": "264389"
  },
  {
    "text": "believe that in this model that we talked about the target is",
    "start": "264389",
    "end": "269700"
  },
  {
    "text": "always moving you're always there is no like an acid state you're getting to to",
    "start": "269700",
    "end": "275490"
  },
  {
    "text": "be state where you're kind of the target is always in a moving state your app requirements change the underlying",
    "start": "275490",
    "end": "282210"
  },
  {
    "text": "infrastructure or the hardware is changing the only constant for all of us",
    "start": "282210",
    "end": "287250"
  },
  {
    "text": "which is why we are all here the Kuban it is is constant and binds the app and hardware and these are the challenges",
    "start": "287250",
    "end": "294750"
  },
  {
    "text": "that kind of like we're going to fix it the moving target that we talked talking about you're going to be optimizing your",
    "start": "294750",
    "end": "301320"
  },
  {
    "text": "app based on that target what can we do about it two things the limits and",
    "start": "301320",
    "end": "309000"
  },
  {
    "text": "requests we're looking at scheduling over committing and eviction then you're",
    "start": "309000",
    "end": "316350"
  },
  {
    "text": "also looking to look at allocatable capacity and we're going to look at why that is important in the allocatable",
    "start": "316350",
    "end": "321990"
  },
  {
    "text": "capacity you're looking at defaults the eviction threshold why cube system our cube and system",
    "start": "321990",
    "end": "327540"
  },
  {
    "text": "reserved our components are all important for you and how you should look at tuning these parameters Thanks",
    "start": "327540",
    "end": "337220"
  },
  {
    "text": "all right so we're gonna start with a simple example and introduce limits and requests and go over some of the",
    "start": "337220",
    "end": "343620"
  },
  {
    "text": "problems that those solve and then we're gonna tie that back to some of these other behaviors that are really",
    "start": "343620",
    "end": "348690"
  },
  {
    "text": "important within kubernetes as we approach utilization so here's our example node and its really simple known",
    "start": "348690",
    "end": "355650"
  },
  {
    "text": "right now it just has one replica of pod a deployed to it and there's a couple problems with this the first of which is",
    "start": "355650",
    "end": "362460"
  },
  {
    "text": "that when the scheduler placed this pod on this node it didn't do that based on any sort of information about the",
    "start": "362460",
    "end": "369150"
  },
  {
    "text": "resources that it needs and another problem that's even bigger than that is that the resource consumption of this",
    "start": "369150",
    "end": "375150"
  },
  {
    "text": "pod is unbounded so this pod is free to consume all of the available resources on this node so when we deploy a second",
    "start": "375150",
    "end": "384810"
  },
  {
    "text": "replica of this pod notice that the CPU utilization is split evenly so this is",
    "start": "384810",
    "end": "390750"
  },
  {
    "text": "because CPU is a compressible resource and that means that when the CPU is fully exhausted each of its consumed",
    "start": "390750",
    "end": "397440"
  },
  {
    "text": "murrs will be throttled so they'll be slowed down but memory on the other hand",
    "start": "397440",
    "end": "403050"
  },
  {
    "text": "is incompressible so when we run out of memory it's a really big problem and we",
    "start": "403050",
    "end": "408210"
  },
  {
    "text": "can't compress it in the same way that we could CPU so at this point there's a",
    "start": "408210",
    "end": "413820"
  },
  {
    "text": "couple things that can happen ideally one of these pods would be evicted",
    "start": "413820",
    "end": "418860"
  },
  {
    "text": "but since the known is fully utilized it's also possible that the eviction",
    "start": "418860",
    "end": "424380"
  },
  {
    "text": "process doesn't occur fast enough or that it doesn't occur at all so that's",
    "start": "424380",
    "end": "429690"
  },
  {
    "text": "problematic because at that point you'd be reliant on the nodes out of memory killer and we're not going to go into",
    "start": "429690",
    "end": "435570"
  },
  {
    "text": "the details on that but basically one thing that's possible is that the out of memory killer kills docker and if docker",
    "start": "435570",
    "end": "442200"
  },
  {
    "text": "goes down all the containers on this node will go down as well and there is a way to fix that and there are ways to",
    "start": "442200",
    "end": "448020"
  },
  {
    "text": "mitigate against this and we'll be getting into details on those but just to recap on the two types of resources",
    "start": "448020",
    "end": "454490"
  },
  {
    "text": "CPU not a huge deal if we run out of it because it'll be throttled it's a compressible resource but memory on the",
    "start": "454490",
    "end": "461730"
  },
  {
    "text": "other hand is problematic if we run out of it can think of memory as being incompressible like water so if you're",
    "start": "461730",
    "end": "469710"
  },
  {
    "text": "trying to compress it you're gonna have a bad time all right so what do we do about this",
    "start": "469710",
    "end": "476940"
  },
  {
    "text": "the first thing we should do is set up limits and a limit is just a way to specify an upper bound on resource",
    "start": "476940",
    "end": "484350"
  },
  {
    "text": "consumption so for CPU which is a compressible resource the limit is going to be the point at which CPU is",
    "start": "484350",
    "end": "490290"
  },
  {
    "text": "throttled it's gonna be the point at which this container is slowed down memory on the other hand because it is",
    "start": "490290",
    "end": "495930"
  },
  {
    "text": "incompressible when memory utilization reaches its limit this container is",
    "start": "495930",
    "end": "502410"
  },
  {
    "text": "going to be killed and restarted according to its restart policy",
    "start": "502410",
    "end": "507380"
  },
  {
    "text": "likewise similar to the specifying the upper bound on containers we can also specify a lower bound and we do this by",
    "start": "507950",
    "end": "515219"
  },
  {
    "text": "setting a request and we illustrate this with the dark green squares so it turns",
    "start": "515219",
    "end": "522539"
  },
  {
    "text": "out that even though these are really simple ideas just specifying an upper bound and lower",
    "start": "522539",
    "end": "527630"
  },
  {
    "text": "found on containers these have a huge effect on the overall behavior of our cluster so limits and requests lay the",
    "start": "527630",
    "end": "535310"
  },
  {
    "text": "groundwork for scheduling it affects how well informed the scheduler is to make a good decision on pod placement it",
    "start": "535310",
    "end": "542300"
  },
  {
    "text": "affects over commit and over commit is a process of assigning more resources to containers then the node actually has",
    "start": "542300",
    "end": "549080"
  },
  {
    "text": "available they affect eviction and eviction is the process of kicking pods",
    "start": "549080",
    "end": "554210"
  },
  {
    "text": "out of a node to reclaim resources and maintain stability these are the three",
    "start": "554210",
    "end": "559910"
  },
  {
    "text": "main processes that effects stability and utilization within a cluster and if we want to achieve both stability and",
    "start": "559910",
    "end": "565750"
  },
  {
    "text": "utilization we need to be aware of and understand these ideas alright so back",
    "start": "565750",
    "end": "572510"
  },
  {
    "text": "to scheduling so in kubernetes the scheduler filters nodes based on",
    "start": "572510",
    "end": "578000"
  },
  {
    "text": "requests and based on the request here it looks like another pod could fit on",
    "start": "578000",
    "end": "583370"
  },
  {
    "text": "this node but based on its utilization and our previous example we know that",
    "start": "583370",
    "end": "588650"
  },
  {
    "text": "another pod would not be successful on this node so what can we do about this one way that we can solve this issue is",
    "start": "588650",
    "end": "595790"
  },
  {
    "text": "by setting the request equal to the limit or at least closer to the limit",
    "start": "595790",
    "end": "600820"
  },
  {
    "text": "because the scheduler filters nodes based on requests it's going to compare this pods request to the remaining",
    "start": "600820",
    "end": "607940"
  },
  {
    "text": "allocatable capacity on this node and we'll get into allocatable a little bit",
    "start": "607940",
    "end": "612980"
  },
  {
    "text": "later but basically we're solving this problem by informing the scheduler that a second replica of this pod would not",
    "start": "612980",
    "end": "618680"
  },
  {
    "text": "fit on the first node and this seems a little odd why my scheduling be",
    "start": "618680",
    "end": "624410"
  },
  {
    "text": "implemented this way so one side effect of this is that this allows us to",
    "start": "624410",
    "end": "632300"
  },
  {
    "text": "implicitly manage over commit ourselves so over commit again is the process of",
    "start": "632300",
    "end": "637370"
  },
  {
    "text": "assigning more of a resource to pods then the node actually has available and",
    "start": "637370",
    "end": "642410"
  },
  {
    "text": "because the scheduler only considers requests not utilization or limits we",
    "start": "642410",
    "end": "648440"
  },
  {
    "text": "implicitly configure this with the difference between limits and requests and whether or not over commit is a good",
    "start": "648440",
    "end": "655610"
  },
  {
    "text": "idea for your app is really up to you but at a high level without over commit",
    "start": "655610",
    "end": "661170"
  },
  {
    "text": "which can be achieved by setting requests equal to limits your utilization will be a little bit lower",
    "start": "661170",
    "end": "666420"
  },
  {
    "text": "you would never run out of resources but you're basically buying stability at the expense of utilization with overcommit",
    "start": "666420",
    "end": "674940"
  },
  {
    "text": "if you allow requests to be less than limits you can achieve higher utilization you might run out of",
    "start": "674940",
    "end": "681180"
  },
  {
    "text": "resources and you're basically buying utilization at the expense of stability so quality of service is a class it's",
    "start": "681180",
    "end": "692100"
  },
  {
    "text": "assigned to pods based on their requests relative to limits and in each of our",
    "start": "692100",
    "end": "697380"
  },
  {
    "text": "previous examples we've had three different configurations of requests relative to limit so if you have no",
    "start": "697380",
    "end": "704730"
  },
  {
    "text": "limits or requests that would correspond to a best-effort quality of service and this is the lowest quality of service if",
    "start": "704730",
    "end": "711180"
  },
  {
    "text": "you have request set less than limits that would correspond to a burstable quality of service so the the middle",
    "start": "711180",
    "end": "718440"
  },
  {
    "text": "quality of service and then if you set your request equal to your limits then that would be a guaranteed quality of",
    "start": "718440",
    "end": "724170"
  },
  {
    "text": "service and guaranteed means that the pod would never be evicted because of",
    "start": "724170",
    "end": "729569"
  },
  {
    "text": "another pods resource consumption so",
    "start": "729569",
    "end": "735149"
  },
  {
    "text": "let's go over an example of eviction again eviction is the process of kicking pods off of a node in order to reclaim",
    "start": "735149",
    "end": "742199"
  },
  {
    "text": "resources and maintain stability so each of the letters here corresponds to a pod",
    "start": "742199",
    "end": "747569"
  },
  {
    "text": "consuming a varying amount of memory relative to its request and limit and",
    "start": "747569",
    "end": "755160"
  },
  {
    "text": "we're gonna use memory here as an example because as we've mentioned earlier running out of memory is the biggest risk to stability this animation",
    "start": "755160",
    "end": "765269"
  },
  {
    "text": "shows the order in which pods from the previous slide are prioritized for eviction so notice that the first four",
    "start": "765269",
    "end": "771750"
  },
  {
    "text": "pods all have a quality of service of best effort and the third and fourth",
    "start": "771750",
    "end": "777209"
  },
  {
    "text": "pods that are evicted actually have the same priority so when this happens the",
    "start": "777209",
    "end": "782339"
  },
  {
    "text": "quality of service is equivalent and the priority then they're ranked by the utilization and now with the burst of",
    "start": "782339",
    "end": "789360"
  },
  {
    "text": "all pods because we have a request and that we compare the utilization to even though these are the same priority",
    "start": "789360",
    "end": "795610"
  },
  {
    "text": "we compared the utilization relative to the request and then last would be a",
    "start": "795610",
    "end": "800680"
  },
  {
    "text": "guaranteed pod and again guaranteed pods will never be evicted because of another",
    "start": "800680",
    "end": "806350"
  },
  {
    "text": "pods resource consumption but if system Damon's were to burst then in that case",
    "start": "806350",
    "end": "811899"
  },
  {
    "text": "they would be evicted and in that sense you can also think of system Damon's as having a guaranteed quality of service",
    "start": "811899",
    "end": "820260"
  },
  {
    "text": "so by default limits and requests are optional and this is kind of because",
    "start": "820410",
    "end": "827860"
  },
  {
    "text": "there's not really a sensible default for these values it's entirely up to us to set these and even though there's no",
    "start": "827860",
    "end": "834760"
  },
  {
    "text": "error or even a warning if we deploy a pod without limits and requests they're still incredibly important so the way",
    "start": "834760",
    "end": "840970"
  },
  {
    "text": "that we enforce these and force defaults to exist on pods is by setting up a",
    "start": "840970",
    "end": "846970"
  },
  {
    "text": "limit range object and a limit range object will allow you to set a minimum maximum ratio of limit to request or",
    "start": "846970",
    "end": "854769"
  },
  {
    "text": "default values so when you set up this object the limit ranger admission controller will enforce all of these",
    "start": "854769",
    "end": "861070"
  },
  {
    "text": "rules on pods as they're deployed in your cluster and then we can also set up quotas at the namespace level and you",
    "start": "861070",
    "end": "868540"
  },
  {
    "text": "would use a resource quota object to do so all right now we're going to get into",
    "start": "868540",
    "end": "874810"
  },
  {
    "text": "allocatable and this is a really important concept to understand and I",
    "start": "874810",
    "end": "879910"
  },
  {
    "text": "think it's especially important that we as consumers of kubernetes think of",
    "start": "879910",
    "end": "884950"
  },
  {
    "text": "nodes in terms of allocatable capacity and not just total capacity so this is",
    "start": "884950",
    "end": "891670"
  },
  {
    "text": "important because it's how the scheduler filters nodes it looks at the remaining allocatable capacity for a resource and",
    "start": "891670",
    "end": "899079"
  },
  {
    "text": "compares that to a pods request allocatable is also the buffer in which",
    "start": "899079",
    "end": "905350"
  },
  {
    "text": "pod utilization can burst and we can also sorry and the edge of allocatable",
    "start": "905350",
    "end": "912790"
  },
  {
    "text": "where allocatable ends is one of the thresholds that can trigger pod eviction or throttling depending on the type of",
    "start": "912790",
    "end": "918970"
  },
  {
    "text": "resource and we can also think of allocatable more broadly as one of the main",
    "start": "918970",
    "end": "925630"
  },
  {
    "text": "you Nobles for adjusting stability versus utilization so you can imagine to",
    "start": "925630",
    "end": "931390"
  },
  {
    "text": "tune for stability we could decrease allocatable and when we do this this would result in less allocatable",
    "start": "931390",
    "end": "938320"
  },
  {
    "text": "capacity for the scheduler to place pods there would be less of a buffer for pods",
    "start": "938320",
    "end": "943480"
  },
  {
    "text": "to burst and there would be a greater buffer between the point at which eviction begins and the total capacity",
    "start": "943480",
    "end": "949870"
  },
  {
    "text": "of the node and this would make it less likely for Damon's to experience resource contention so likewise the",
    "start": "949870",
    "end": "956800"
  },
  {
    "text": "opposite is always true if we wanted to tune for utilization we could increase allocatable and again this would just",
    "start": "956800",
    "end": "962980"
  },
  {
    "text": "give more capacity for the scheduler to place pods more of a buffer for pods to burst and there would be a smaller",
    "start": "962980",
    "end": "969550"
  },
  {
    "text": "buffer between the eviction threshold and the actual capacity of the node and",
    "start": "969550",
    "end": "974730"
  },
  {
    "text": "this would of course make it a little bit more likely for resource contention to occur so overall as allocatable",
    "start": "974730",
    "end": "981430"
  },
  {
    "text": "shrinks we're tuning for stability and as it increases we're tuning for utilization so let's talk about what",
    "start": "981430",
    "end": "987160"
  },
  {
    "text": "allocatable constraints are because that's what allows us to tune a locatable alright so to start let's take",
    "start": "987160",
    "end": "995350"
  },
  {
    "text": "a step back and take another look at what nodes actually look like on a newly installed cluster so by default pods and",
    "start": "995350",
    "end": "1002370"
  },
  {
    "text": "Damons consume resources and we've kind of been glossing over this fact up until this point but now everything on the",
    "start": "1002370",
    "end": "1009120"
  },
  {
    "text": "left is going to represent utilization of pods and Damons and everything on the right is gonna represent allocatable",
    "start": "1009120",
    "end": "1016320"
  },
  {
    "text": "constraints so notice the only thing that we have on the right is an eviction threshold for memory and this threshold",
    "start": "1016320",
    "end": "1024360"
  },
  {
    "text": "applies to the total utilization of pods and Damons and when the total",
    "start": "1024360",
    "end": "1029670"
  },
  {
    "text": "utilization hits this point that's when eviction begins so one problem with this",
    "start": "1029670",
    "end": "1034980"
  },
  {
    "text": "is that pods and Damons are actually competing for resources the scheduler does not take into account resource",
    "start": "1034980",
    "end": "1041130"
  },
  {
    "text": "consumption of Damons so the way that we can mitigate this is by setting cube and",
    "start": "1041130",
    "end": "1048390"
  },
  {
    "text": "system reserved which are actually disabled by default so cube reserved is a way to set aside resources for",
    "start": "1048390",
    "end": "1054690"
  },
  {
    "text": "kubernetes related Damons like the cubelet and docker system reserved is for everything else",
    "start": "1054690",
    "end": "1059780"
  },
  {
    "text": "on the node and when we set these up these are factored into allocatable constraints and as that increases",
    "start": "1059780",
    "end": "1067100"
  },
  {
    "text": "allocatable decreases like we were discussing before so this is how we take into account daemon resource consumption",
    "start": "1067100",
    "end": "1073190"
  },
  {
    "text": "and prevent pods and Damons from competing for the same resources and again there's not really a default for",
    "start": "1073190",
    "end": "1080330"
  },
  {
    "text": "this either because there's not really a sensible default to set kubernetes is",
    "start": "1080330",
    "end": "1085910"
  },
  {
    "text": "unaware of the underlying hardware and our app requirements so again this is something that's really up to us to set",
    "start": "1085910",
    "end": "1092030"
  },
  {
    "text": "up in tune and just to recap for CPU the",
    "start": "1092030",
    "end": "1097460"
  },
  {
    "text": "scheduler uses allocatable to schedule based on CPU requests allocatable is the",
    "start": "1097460",
    "end": "1103040"
  },
  {
    "text": "buffer in which CPU utilization can burst and when utilization exceeds",
    "start": "1103040",
    "end": "1108860"
  },
  {
    "text": "allocatable pods will be throttled to prevent contention with Damons similar",
    "start": "1108860",
    "end": "1116120"
  },
  {
    "text": "story for memory and ephemeral storage the only difference here is that we include the eviction threshold in node",
    "start": "1116120",
    "end": "1122960"
  },
  {
    "text": "allocatable constraints and quick side note on that eviction only applies to",
    "start": "1122960",
    "end": "1128650"
  },
  {
    "text": "memory and ephemeral storage I believe right right now so I don't think you can currently trigger eviction based on pod",
    "start": "1128650",
    "end": "1135799"
  },
  {
    "text": "CPU utilization again so the edge of allocatable is a threshold for eviction",
    "start": "1135799",
    "end": "1142640"
  },
  {
    "text": "not throttling because this is a incompressible resource and one",
    "start": "1142640",
    "end": "1147740"
  },
  {
    "text": "important note here is that we now have two thresholds for eviction one is the",
    "start": "1147740",
    "end": "1154790"
  },
  {
    "text": "same one that we discussed earlier which is triggered by the total consumption of pods and nodes that's the addiction",
    "start": "1154790",
    "end": "1160850"
  },
  {
    "text": "threshold on the far right and another is the edge of allocatable and this one is triggered by the consumption of pods",
    "start": "1160850",
    "end": "1167650"
  },
  {
    "text": "and although we use CPU memory and ephemeral storage as examples those are",
    "start": "1167650",
    "end": "1173419"
  },
  {
    "text": "just what's currently supported and documented but it's also important to note that the concept of allocatable",
    "start": "1173419",
    "end": "1178790"
  },
  {
    "text": "could could potentially be generalized beyond these types of resources and just",
    "start": "1178790",
    "end": "1184190"
  },
  {
    "text": "keep this in mind and we could extend this API to someday account for any type of resource that we want",
    "start": "1184190",
    "end": "1189830"
  },
  {
    "text": "maybe GPUs example all right so this is a lot of",
    "start": "1189830",
    "end": "1196810"
  },
  {
    "text": "new information what are we supposed to do with it so there's some key questions",
    "start": "1196810",
    "end": "1202480"
  },
  {
    "text": "that we can ask ourself with this to inform how we set up our cluster the first of which is is our scheduler",
    "start": "1202480",
    "end": "1209920"
  },
  {
    "text": "prepared to make informed decisions so remember by default limits and",
    "start": "1209920",
    "end": "1215680"
  },
  {
    "text": "requests are optional the way to enforce that is by setting up a limit range object once we're using limits and",
    "start": "1215680",
    "end": "1222550"
  },
  {
    "text": "requests one of the most important things to consider is utilization relative to requests and this cuts both",
    "start": "1222550",
    "end": "1230980"
  },
  {
    "text": "ways when working with customers we found that if for example app teams",
    "start": "1230980",
    "end": "1235990"
  },
  {
    "text": "consistently overestimate how much of a resource their app needs the scheduler",
    "start": "1235990",
    "end": "1241390"
  },
  {
    "text": "will think that the cluster is full because it's comparing the pod request to the allocatable capacity of each node",
    "start": "1241390",
    "end": "1250440"
  },
  {
    "text": "but if you look at actual utilization then it's gonna be hovering around like five to ten percent but also the",
    "start": "1250440",
    "end": "1257290"
  },
  {
    "text": "opposite is also true so if utilization is considerably higher than your requests it will be consistently running up against the",
    "start": "1257290",
    "end": "1264610"
  },
  {
    "text": "limits in the best case because then that pod will be throttled and and the",
    "start": "1264610",
    "end": "1269620"
  },
  {
    "text": "blast radius of that will be limited to one pod but also it could run up against",
    "start": "1269620",
    "end": "1274660"
  },
  {
    "text": "eviction thresholds which would cause instability at the app layer and then the worst possible scenario is if",
    "start": "1274660",
    "end": "1280600"
  },
  {
    "text": "eviction isn't fast enough or it fails and then you're relying on the nodes out of memory killer is over commit",
    "start": "1280600",
    "end": "1290260"
  },
  {
    "text": "configured properly so if our limits are too high this could result in eviction",
    "start": "1290260",
    "end": "1296050"
  },
  {
    "text": "occurring too frequently and that's because the utilization would be running up against eviction thresholds if our",
    "start": "1296050",
    "end": "1303730"
  },
  {
    "text": "limits are too low this could result in containers restarting in the case of memory for example because those",
    "start": "1303730",
    "end": "1310240"
  },
  {
    "text": "containers would be killed and restarted or throttling in the case of CPU and how",
    "start": "1310240",
    "end": "1317290"
  },
  {
    "text": "close are we to triggering eviction so this question is a little bit more nuanced than just",
    "start": "1317290",
    "end": "1322870"
  },
  {
    "text": "looking at memory and available memory is important but that tells us when the",
    "start": "1322870",
    "end": "1327910"
  },
  {
    "text": "entire node is at risk so because we understand these eviction thresholds and these values are well",
    "start": "1327910",
    "end": "1335110"
  },
  {
    "text": "defined and known ahead of time we can be a little bit more proactive in our approach to monitoring so by knowing",
    "start": "1335110",
    "end": "1341680"
  },
  {
    "text": "exactly when eviction is going to occur we know when instability at the app level is going to occur not just",
    "start": "1341680",
    "end": "1347980"
  },
  {
    "text": "instability at the infrastructure level so again these two eviction thresholds",
    "start": "1347980",
    "end": "1354030"
  },
  {
    "text": "this one is for the allocatable capacity of pods and this one is for the eviction",
    "start": "1354030",
    "end": "1362200"
  },
  {
    "text": "threshold this is the first one that we went over and monitoring these two",
    "start": "1362200",
    "end": "1367690"
  },
  {
    "text": "eviction thresholds it can be really powerful because it informs our decision-making process moving forward",
    "start": "1367690",
    "end": "1373750"
  },
  {
    "text": "for how to - in our clusters but we also know the cause of eviction and that's",
    "start": "1373750",
    "end": "1379870"
  },
  {
    "text": "what allows us to tune it so we can build upon this further and ask ourselves alright what if we frequently",
    "start": "1379870",
    "end": "1387520"
  },
  {
    "text": "encounter eviction caused by utilizing allocatable well in that case we would",
    "start": "1387520",
    "end": "1393730"
  },
  {
    "text": "know that we need to tune our limits and requests to adjust our overcommit and if",
    "start": "1393730",
    "end": "1398860"
  },
  {
    "text": "we know that we are frequently encountering eviction caused by crossing the eviction threshold we know that we",
    "start": "1398860",
    "end": "1405670"
  },
  {
    "text": "needed to noob and system reserved so this isn't because we haven't set aside enough resources for our system Damons",
    "start": "1405670",
    "end": "1414420"
  },
  {
    "text": "and i know it's late in the day and i'm throwing a little bit of math at you but",
    "start": "1415440",
    "end": "1420910"
  },
  {
    "text": "this will be easy I promise so in all of our examples so far we've been considering allocatable at the node",
    "start": "1420910",
    "end": "1426880"
  },
  {
    "text": "level and it turns out we can generalize this a little bit further and think",
    "start": "1426880",
    "end": "1431980"
  },
  {
    "text": "about the allocatable capacity of our cluster so this can help us be a little",
    "start": "1431980",
    "end": "1437830"
  },
  {
    "text": "bit more proactive in how we think about failover for example so imagine if you have two nodes if neither node goes",
    "start": "1437830",
    "end": "1445450"
  },
  {
    "text": "above 50% utilization you know that one node failing wouldn't be a huge deal",
    "start": "1445450",
    "end": "1451090"
  },
  {
    "text": "because those workloads could be rescheduled on the other node and there would be a temporary disruption of",
    "start": "1451090",
    "end": "1456710"
  },
  {
    "text": "but those workloads would at least be able to be rescheduled and disclaimer on",
    "start": "1456710",
    "end": "1461990"
  },
  {
    "text": "this idea because this does assume that nodes are the same size and it doesn't account for things like affinity paints",
    "start": "1461990",
    "end": "1468470"
  },
  {
    "text": "and toleration x' so it's scheduling is a bit more complicated than this but the",
    "start": "1468470",
    "end": "1473840"
  },
  {
    "text": "point is if we if these values are well defined and known ahead of time why not plug them into monitoring tools to help",
    "start": "1473840",
    "end": "1481220"
  },
  {
    "text": "us be a little bit more proactive and this is one example of how we might do that and I forgot to advance the slides",
    "start": "1481220",
    "end": "1486740"
  },
  {
    "text": "but we could generalize this for an arbitrary number of failures so imagine if you know you had two nodes and one",
    "start": "1486740",
    "end": "1493850"
  },
  {
    "text": "failure this would be 50% as your upper bound for utilization or if you had 10 nodes and one failure this would be your",
    "start": "1493850",
    "end": "1500899"
  },
  {
    "text": "upper bound for utilization would be 90% and because our node allocatable",
    "start": "1500899",
    "end": "1506509"
  },
  {
    "text": "constraints are well defined and known ahead of time we can take that into account as well and be even more precise",
    "start": "1506509",
    "end": "1512690"
  },
  {
    "text": "in our estimate these are some of the defaults that we think it's really",
    "start": "1512690",
    "end": "1519590"
  },
  {
    "text": "important to be aware of so eviction hard this is the default eviction threshold that we talked about earlier",
    "start": "1519590",
    "end": "1526210"
  },
  {
    "text": "housekeeping interval is the interval in which cubelet takes a snapshot of resource utilization on the node so the",
    "start": "1526210",
    "end": "1533659"
  },
  {
    "text": "cubit doesn't actually have a real time understanding of resource consumption eviction pressure transition period is a",
    "start": "1533659",
    "end": "1541730"
  },
  {
    "text": "mouthful but it is also how long it takes for a node to be able to schedule pods after an eviction threshold is met",
    "start": "1541730",
    "end": "1548440"
  },
  {
    "text": "so I think that's a really important one to be aware of because it's five minutes so taking these three into consideration",
    "start": "1548440",
    "end": "1555740"
  },
  {
    "text": "here's what a worst-case scenario might look like and I know this from experience so let's say our workload",
    "start": "1555740",
    "end": "1562850"
  },
  {
    "text": "consumes more than a hundred MIDI bytes faster than ten seconds the eviction",
    "start": "1562850",
    "end": "1568999"
  },
  {
    "text": "process doesn't begin fast enough or maybe it doesn't happen fast at all now we rely on the nodes out of memory",
    "start": "1568999",
    "end": "1575929"
  },
  {
    "text": "killer to save us and in the worst case docker is killed by the out of memory",
    "start": "1575929",
    "end": "1582320"
  },
  {
    "text": "handler and every container on the node is taken down in the best case we can",
    "start": "1582320",
    "end": "1587659"
  },
  {
    "text": "reclaim resources on the and the note is stable again but we can't schedule anything on that note for",
    "start": "1587659",
    "end": "1593830"
  },
  {
    "text": "five minutes so in the meantime as these five minutes are ticking down pressure on the remaining nodes is increased due",
    "start": "1593830",
    "end": "1601060"
  },
  {
    "text": "to workloads being rescheduled on those nodes and again this raises the",
    "start": "1601060",
    "end": "1606250"
  },
  {
    "text": "possibility of this scenario occurring on every node which could potentially result in a cascading failure throughout",
    "start": "1606250",
    "end": "1612460"
  },
  {
    "text": "the cluster so I bring this up because at the very least you might want to",
    "start": "1612460",
    "end": "1618850"
  },
  {
    "text": "increase the eviction threshold the default there is only 100 megabytes and decrease the eviction pressure",
    "start": "1618850",
    "end": "1625180"
  },
  {
    "text": "transition period unless your apt and tolerate a five-minute downtime and for",
    "start": "1625180",
    "end": "1631870"
  },
  {
    "text": "a docker oh quick note on this so I've got implied defaults up here so if you don't pass any flags in the implied",
    "start": "1631870",
    "end": "1640630"
  },
  {
    "text": "defaults or effectively what is passed in but I included the flags up here in case you wanted to add these to cubelet",
    "start": "1640630",
    "end": "1647230"
  },
  {
    "text": "and tweak them but again if you don't do anything this is what they would look like so before we talk about docker",
    "start": "1647230",
    "end": "1655900"
  },
  {
    "text": "we've got flags to consider so cube reserved and system reserved we talked about these earlier these are how you",
    "start": "1655900",
    "end": "1661570"
  },
  {
    "text": "set aside system resources for danaans and eviction soft this one is similar to",
    "start": "1661570",
    "end": "1668380"
  },
  {
    "text": "the other eviction threshold that we talked about but this one is not considered well it's not accounted for",
    "start": "1668380",
    "end": "1675580"
  },
  {
    "text": "in allocatable constraints so it will not affect your allocatable capacity but",
    "start": "1675580",
    "end": "1682060"
  },
  {
    "text": "once this threshold is triggered there's a timer that starts ticking down and when that timer hits zero if the",
    "start": "1682060",
    "end": "1689380"
  },
  {
    "text": "eviction threshold is still being violated then eviction will begin so it's kind of a nicer fiction threshold",
    "start": "1689380",
    "end": "1696520"
  },
  {
    "text": "and the grace period below is how you would tune that timer that we talked",
    "start": "1696520",
    "end": "1702340"
  },
  {
    "text": "about and on to docker so as we were saying earlier by default when docker",
    "start": "1702340",
    "end": "1708580"
  },
  {
    "text": "terminates it'll kill all of the containers on that node and there is a feature to mitigate against this it's",
    "start": "1708580",
    "end": "1715300"
  },
  {
    "text": "called live restore again implied default if you don't do anything this is what it looks like so the default value",
    "start": "1715300",
    "end": "1721300"
  },
  {
    "text": "for this is false but if you do want to take advantage of that feature you'd have to set this flag to true well that brings us to the",
    "start": "1721300",
    "end": "1731350"
  },
  {
    "text": "summary again we looked at a lot of things one is the limits and requests and allocatable capacity as well in",
    "start": "1731350",
    "end": "1738820"
  },
  {
    "text": "limits and requests we went to scheduling the overcommit eviction and we also looked in the allocatable",
    "start": "1738820",
    "end": "1745270"
  },
  {
    "text": "capacity defaults eviction threshold and Kuban system and how you can go about",
    "start": "1745270",
    "end": "1750420"
  },
  {
    "text": "tuning those parameters for your own clusters again please keep in mind we've",
    "start": "1750420",
    "end": "1758980"
  },
  {
    "text": "done this for a lot of customers we every cluster is different every app that you deploy in a cluster is",
    "start": "1758980",
    "end": "1764740"
  },
  {
    "text": "different you'd have your own enrollments to think about but if you have more questions to ask us please",
    "start": "1764740",
    "end": "1771100"
  },
  {
    "text": "feel free to reach out to us and hopefully this was insightful and how you can optimize the clusters well thank",
    "start": "1771100",
    "end": "1789250"
  },
  {
    "text": "you for coming and hopefully everybody I'm sorry yeah questions questions I saw",
    "start": "1789250",
    "end": "1796870"
  },
  {
    "text": "this hand over here first so I'll run over here don't remember to do you don't forget to do your feedback forms also I",
    "start": "1796870",
    "end": "1804280"
  },
  {
    "text": "will not remember probably clearly thanks for the presentation I didn't",
    "start": "1804280",
    "end": "1811150"
  },
  {
    "text": "remember reading somewhere that we should avoid sitting CPU limits cuz of something with the CFS the computer",
    "start": "1811150",
    "end": "1817570"
  },
  {
    "text": "scheduler is that you know if that's still the case",
    "start": "1817570",
    "end": "1821309"
  },
  {
    "text": "[Applause]",
    "start": "1828070",
    "end": "1832639"
  },
  {
    "text": "well next question I see I saw your hand",
    "start": "1850679",
    "end": "1859210"
  },
  {
    "text": "earlier so I will come to you first so first of all thanks this is great",
    "start": "1859210",
    "end": "1866860"
  },
  {
    "text": "information that I ever one really needs to know even if they don't know they need to know it yet you focus plainly on",
    "start": "1866860",
    "end": "1873730"
  },
  {
    "text": "the node what what do you find is the best way to put some of those quotas on",
    "start": "1873730",
    "end": "1881980"
  },
  {
    "text": "say a namespace so that you don't have like capacity creep as as they keep",
    "start": "1881980",
    "end": "1888669"
  },
  {
    "text": "adding things looking at more of like a multi-tenant cluster yes so the resource",
    "start": "1888669",
    "end": "1897070"
  },
  {
    "text": "quota object that we brought up is namespace so that would apply at the namespace level is that what you're",
    "start": "1897070",
    "end": "1903610"
  },
  {
    "text": "getting at know it yes you would use",
    "start": "1903610",
    "end": "1911500"
  },
  {
    "text": "that more the question is how what are the tools to look to try to come up with",
    "start": "1911500",
    "end": "1919059"
  },
  {
    "text": "what that quota for the namespace should be based on all that pods running in",
    "start": "1919059",
    "end": "1924220"
  },
  {
    "text": "that namespace does that make sense okay yes so yes so to approach that I would",
    "start": "1924220",
    "end": "1935080"
  },
  {
    "text": "probably start with just monitoring tools to figure out what our requests and limits should be for the app so that",
    "start": "1935080",
    "end": "1940509"
  },
  {
    "text": "we know what appropriate settings are for that and then I'd say that quota is",
    "start": "1940509",
    "end": "1946720"
  },
  {
    "text": "really just gonna be based on that what do you think is appropriate for the apps in that namespace again it depends for",
    "start": "1946720",
    "end": "1954009"
  },
  {
    "text": "every enrollment that people quit but that's the tools that we have relied on so far to just set coders for",
    "start": "1954009",
    "end": "1959830"
  },
  {
    "text": "specific okay so you're saying just like can I add up all the kind of add up all",
    "start": "1959830",
    "end": "1967480"
  },
  {
    "text": "the limits and add up all the requests and that is the quota yeah it's a little bit of a guessing game for what the",
    "start": "1967480",
    "end": "1974440"
  },
  {
    "text": "right value is but you definitely have to have a feedback loop that involves monitoring tools mm-hm",
    "start": "1974440",
    "end": "1979840"
  },
  {
    "text": "so yeah deploy your app use monitoring tools figure out what appropriate values are and then just kind of iterate on",
    "start": "1979840",
    "end": "1985750"
  },
  {
    "text": "that it's always a constant like we've discuss about the moving target it's always a moving target and what you're",
    "start": "1985750",
    "end": "1991330"
  },
  {
    "text": "trying to do so you'd have to get it done in your testing state of cluster and then see what your app teams are",
    "start": "1991330",
    "end": "1998080"
  },
  {
    "text": "thinking about and then circular limits accordingly yeah essentially what we've been doing is kind of what you describe",
    "start": "1998080",
    "end": "2004409"
  },
  {
    "text": "describing looking at Prometheus for like the last week and looking at the historical data and try to to guess what",
    "start": "2004409",
    "end": "2011429"
  },
  {
    "text": "that is I was just wondering if there is maybe a tool out there that would help with that I don't think there is I",
    "start": "2011429",
    "end": "2017970"
  },
  {
    "text": "haven't seen one I just wanted wondering if you guys so this might not be",
    "start": "2017970",
    "end": "2023610"
  },
  {
    "text": "answering your question very well either but one tool to check out is vertical pod autoscaler it can help automate the",
    "start": "2023610",
    "end": "2030210"
  },
  {
    "text": "process of setting limits and requests but as far as what an appropriate quota",
    "start": "2030210",
    "end": "2035820"
  },
  {
    "text": "should be for the namespace I don't know of any tools for that well I was in the",
    "start": "2035820",
    "end": "2042480"
  },
  {
    "text": "floor I did see somebody called densify who is kind of doing the same thing about monitoring those specific and then",
    "start": "2042480",
    "end": "2048929"
  },
  {
    "text": "able to chill so you can probably check that out but we've relied from it mostly on the CNC of projects and monitoring",
    "start": "2048929",
    "end": "2055320"
  },
  {
    "text": "tools to get our things done okay thank",
    "start": "2055320",
    "end": "2060929"
  },
  {
    "text": "you okay I'll do this one first and I'll",
    "start": "2060929",
    "end": "2068530"
  },
  {
    "text": "come around the room to you I'm curious if you have any practical experience",
    "start": "2068530",
    "end": "2075399"
  },
  {
    "text": "with interactions between pots that have priests top hooks and how that plays",
    "start": "2075400",
    "end": "2083190"
  },
  {
    "text": "into like soft eviction and hard efficient limits so let me give you kind",
    "start": "2083190",
    "end": "2089320"
  },
  {
    "text": "of an example if I've got an application that's got a long-running priests top hook and the note goes into a soft",
    "start": "2089320",
    "end": "2095950"
  },
  {
    "text": "memory condition and breaches that threshold but doesn't and stays there and then other pots continue to consume",
    "start": "2095950",
    "end": "2103360"
  },
  {
    "text": "resources as that hook terminates and if you then breached the hard eviction",
    "start": "2103360",
    "end": "2111070"
  },
  {
    "text": "limit after that how the sequence of events would play out if you guys ever encountered that and the work that",
    "start": "2111070",
    "end": "2117580"
  },
  {
    "text": "you've done I have a general sense of how those interacts but I'm curious if",
    "start": "2117580",
    "end": "2122620"
  },
  {
    "text": "you ever ran into something like that so it's not something that I've encountered myself but I would imagine that it",
    "start": "2122620",
    "end": "2130420"
  },
  {
    "text": "sounds like it might be a good use case for the soft eviction threshold I'd imagine if you had a brief increase in",
    "start": "2130420",
    "end": "2137110"
  },
  {
    "text": "utilization and then maybe you could adjust that grace period to account for that might be something to to think",
    "start": "2137110",
    "end": "2144490"
  },
  {
    "text": "about yeah I hope that it helps answer the questions but overall no it's not something that I've encountered myself",
    "start": "2144490",
    "end": "2151770"
  },
  {
    "text": "so when I was reading up on how kubernetes handled limits and requests",
    "start": "2156270",
    "end": "2162730"
  },
  {
    "text": "one of the things I read source is probably relevant but anyways it sounded reasonably so I made an assumption that",
    "start": "2162730",
    "end": "2168340"
  },
  {
    "text": "was true is that when you set a limit and the process wants to go beyond that",
    "start": "2168340",
    "end": "2176380"
  },
  {
    "text": "if there is free CPU cycles kubernetes will allow it to use those so it allowed",
    "start": "2176380",
    "end": "2181750"
  },
  {
    "text": "to exceed the limit however i've sensing several cases where kubernetes actually",
    "start": "2181750",
    "end": "2187900"
  },
  {
    "text": "throttled on the limit even when CPU was idle so what is the intended function and behavior how it should Cooper nays",
    "start": "2187900",
    "end": "2194230"
  },
  {
    "text": "handle that so I believe it throttles at",
    "start": "2194230",
    "end": "2199780"
  },
  {
    "text": "the limit so it's a hard limit even if there's free CPU cycles I'm not totally",
    "start": "2199780",
    "end": "2206710"
  },
  {
    "text": "sure it sounds like this person has worked on this but it's a hard roof yeah",
    "start": "2206710",
    "end": "2237130"
  },
  {
    "text": "that's what I observed too which caught me by surprise but now that I know that I can accommodate",
    "start": "2237130",
    "end": "2244079"
  },
  {
    "text": "I believe this will be our last question",
    "start": "2247580",
    "end": "2253390"
  },
  {
    "text": "hey so thanks for giving this talk if you are on eks you should be aware that",
    "start": "2253570",
    "end": "2260690"
  },
  {
    "text": "the newest ami they just released on Tuesday actually sets the cube reserved and the daemon reserved by default this",
    "start": "2260690",
    "end": "2267830"
  },
  {
    "text": "actually caused a fire for us because cluster autoscaler which will add more nodes to your ASG doesn't take into",
    "start": "2267830",
    "end": "2275120"
  },
  {
    "text": "account if you have cube reserved actually said so pods would try to be scheduled onto these nodes it didn't do the math",
    "start": "2275120",
    "end": "2282440"
  },
  {
    "text": "thought that there is actually a larger the normal amount of memory spun up another node and then that failed it",
    "start": "2282440",
    "end": "2288950"
  },
  {
    "text": "said oh I just need another node added another node and you can see where this goes from here so yeah just be aware by",
    "start": "2288950",
    "end": "2297950"
  },
  {
    "text": "default it's not set unless you using the eks AMI so but thanks for giving",
    "start": "2297950",
    "end": "2303890"
  },
  {
    "text": "this talk this is great thank you thank you",
    "start": "2303890",
    "end": "2307570"
  }
]