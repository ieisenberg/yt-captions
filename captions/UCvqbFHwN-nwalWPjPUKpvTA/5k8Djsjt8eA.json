[
  {
    "text": "all right so hello everyone uh thank you for being here I'm super stoked and excited to be here uh my name is aik I",
    "start": "440",
    "end": "8040"
  },
  {
    "text": "am a data scientist at int this is my colleague Bala um you know who is a",
    "start": "8040",
    "end": "13120"
  },
  {
    "text": "senior staff software engineer um and today I'm going to talk about the AI driven Progressive delivery and um",
    "start": "13120",
    "end": "20880"
  },
  {
    "text": "really show how we uh we were able to achieve this purely using open source tools so our goal from this talk is to",
    "start": "20880",
    "end": "28599"
  },
  {
    "text": "give you a solid intuition behind the whole process uh you know so that you guys can implement this",
    "start": "28599",
    "end": "34280"
  },
  {
    "text": "yourself all right so with that so the agenda is that we're going to start with",
    "start": "34280",
    "end": "40280"
  },
  {
    "text": "what is Progressive delivery talk about that uh and then go straight away into AI based Progressive delivery uh and",
    "start": "40280",
    "end": "47280"
  },
  {
    "text": "then finally we will actually go a bit deeper into the multivariate anomal detection and like and also how to we",
    "start": "47280",
    "end": "54000"
  },
  {
    "text": "Implement that okay and then finally a demo all right and by the way I am having a bad throat so I sound funny so",
    "start": "54000",
    "end": "61120"
  },
  {
    "text": "please excuse me um okay so like at into it our mission is to power Prosperity uh",
    "start": "61120",
    "end": "67840"
  },
  {
    "text": "like you know around the world and if you haven't heard of into it uh we are actually a global fintech company that",
    "start": "67840",
    "end": "73759"
  },
  {
    "text": "is building an AI native development platform um we serve about 100 million",
    "start": "73759",
    "end": "79080"
  },
  {
    "text": "you know customers across our various Brands um namely Turbo Tax Credit kma QuickBooks and MailChimp uh we are",
    "start": "79080",
    "end": "85159"
  },
  {
    "text": "really excited to be here and uh you know as we are big users of open- source tools and we love giving back to the",
    "start": "85159",
    "end": "91479"
  },
  {
    "text": "open source community so a little bit more about our platform so our AI native uh development",
    "start": "91479",
    "end": "99040"
  },
  {
    "text": "platform is massive in scale okay and it like supports over four uh a million",
    "start": "99040",
    "end": "104399"
  },
  {
    "text": "models running in production every day and our dep velocity has increased 8X uh",
    "start": "104399",
    "end": "109840"
  },
  {
    "text": "you know over the past 4 years and the platform you Powers more than or you",
    "start": "109840",
    "end": "115040"
  },
  {
    "text": "know about around you know 60 billion machine learning predictions uh you know per day",
    "start": "115040",
    "end": "121240"
  },
  {
    "text": "and one of the most exciting Parts about working at into it is how much open source software we use to build our Dev",
    "start": "121240",
    "end": "128000"
  },
  {
    "text": "platform uh we received the end user award in 2019 and you know 2022 um and",
    "start": "128000",
    "end": "134000"
  },
  {
    "text": "we love to create an open source many projects here into it especially our gon approach Etc which I think many of you",
    "start": "134000",
    "end": "140480"
  },
  {
    "text": "know about so with that let's move on to the core part of our talk so what really is Progressive",
    "start": "140480",
    "end": "148120"
  },
  {
    "text": "delivery like I'm pretty sure many of if you know this but we'll just you know brush up on that so uh you know",
    "start": "148120",
    "end": "155160"
  },
  {
    "text": "Progressive delivery is basically a gradual release of a new version all right so uh and the reason we do it is",
    "start": "155160",
    "end": "162920"
  },
  {
    "text": "because it reduces the risks of bugs and failures and it reduces the impact of buyer experiences like for the end users",
    "start": "162920",
    "end": "169760"
  },
  {
    "text": "and customers so and it also gives us a quick roll back mechanism so if something goes wrong we can just roll it",
    "start": "169760",
    "end": "176159"
  },
  {
    "text": "back um and a very good example is the canary deployments right the canary",
    "start": "176159",
    "end": "181319"
  },
  {
    "text": "based roll backs and the canary based uh in Progressive delivery um and a very good example here",
    "start": "181319",
    "end": "188120"
  },
  {
    "text": "is the Argo rollouts which we have been using for a long time um and uh yeah so",
    "start": "188120",
    "end": "193720"
  },
  {
    "text": "you know for this purpose we use Argo rollouts as well uh okay now let's talk about why do",
    "start": "193720",
    "end": "201879"
  },
  {
    "text": "we need to incorporate Ai and ml uh you know into this whole system uh right but",
    "start": "201879",
    "end": "207760"
  },
  {
    "text": "first we need to see what is the diff so and what is the exact problem that we are trying to solve using a progressive",
    "start": "207760",
    "end": "212959"
  },
  {
    "text": "delivery system so let's first Define what change induced incidents are so we",
    "start": "212959",
    "end": "220120"
  },
  {
    "text": "saw that more than 1/3 of the P0 and P1 the most critical incidents add into it",
    "start": "220120",
    "end": "226720"
  },
  {
    "text": "were caused by changes now these changes could be anything it could be new features it could be uh like any kind of",
    "start": "226720",
    "end": "234640"
  },
  {
    "text": "bug fix that we wanted to add but it turned out to be a bug in itself so that",
    "start": "234640",
    "end": "240120"
  },
  {
    "text": "can happen and then um even like some simple dependency upgrades right it",
    "start": "240120",
    "end": "245319"
  },
  {
    "text": "could be some new version of maybe numai or you know pandas or anything right uh",
    "start": "245319",
    "end": "251480"
  },
  {
    "text": "but that could cause uh you know catastrophic failures um you know inside the in production system and well it can",
    "start": "251480",
    "end": "259519"
  },
  {
    "text": "be really avoided or the reduction in the impact can really be done if it's detected early and resolved early so how",
    "start": "259519",
    "end": "267040"
  },
  {
    "text": "do we prevent them um the most basic way as we all know is just a static",
    "start": "267040",
    "end": "272160"
  },
  {
    "text": "thresholding based roll back okay so what do we do so we basically set a hard",
    "start": "272160",
    "end": "277479"
  },
  {
    "text": "threshold like for every metric that you care about for your application so for example if you care about error rate you",
    "start": "277479",
    "end": "283600"
  },
  {
    "text": "can just say hey you set a hard deadline or a hard threshold for you know 4% error rate and that should be all okay",
    "start": "283600",
    "end": "290639"
  },
  {
    "text": "anything any point that goes above that threshold you just roll back automatically and you do that for every",
    "start": "290639",
    "end": "295880"
  },
  {
    "text": "single metric that you care about so maybe latency the request latency so 400 milliseconds is the heart threshold if",
    "start": "295880",
    "end": "302520"
  },
  {
    "text": "anything goes above that you roll back so that's all there is to it right and",
    "start": "302520",
    "end": "308919"
  },
  {
    "text": "it is a very quick and easy way to go about this but uh like of course there are some drawbacks the main drawback is",
    "start": "308919",
    "end": "316440"
  },
  {
    "text": "that every service is unique so what you have for your service the metrics and",
    "start": "316440",
    "end": "322360"
  },
  {
    "text": "the thresholds is completely different or can be completely different to my service so I can have an ml inference",
    "start": "322360",
    "end": "328319"
  },
  {
    "text": "service that can tolerate up to 1 second of latency because it's doing you know a lot of number crunching but you might",
    "start": "328319",
    "end": "335039"
  },
  {
    "text": "have a really Mission critical service where uh like even a 100 millisecond latency is disastrous all right so",
    "start": "335039",
    "end": "342759"
  },
  {
    "text": "that's one so that's why you will have different thresholds and then different metrics that make sense for your",
    "start": "342759",
    "end": "349319"
  },
  {
    "text": "application as well so you might think that error rate is not really important for you like for you may maybe some kind",
    "start": "349319",
    "end": "354720"
  },
  {
    "text": "of you know clickstream data might be important right so it really depends on the person and the application there and",
    "start": "354720",
    "end": "361360"
  },
  {
    "text": "this could be non-operational metrics as well this could be some kind of business metric that you care about okay that you know number of customers that really",
    "start": "361360",
    "end": "367960"
  },
  {
    "text": "clicked on to your login page it could be something like that um and then the one of the main",
    "start": "367960",
    "end": "375360"
  },
  {
    "text": "problems of static thresholding is that you you are trying to just detect Global anomalies so anomalies that you can",
    "start": "375360",
    "end": "382199"
  },
  {
    "text": "actually so if you zoom out a time series and you can just pinpoint there is an anomaly that's Global okay because",
    "start": "382199",
    "end": "387720"
  },
  {
    "text": "you can really see that something has gone horribly wrong and there is a you know High anomaly there right but not",
    "start": "387720",
    "end": "393360"
  },
  {
    "text": "all anomalies are Global um because you time series metrics are very seasonal so",
    "start": "393360",
    "end": "398680"
  },
  {
    "text": "it could be that you have daily seasonality even weekly seasonality all right so the traffic goes up in the day",
    "start": "398680",
    "end": "405240"
  },
  {
    "text": "and it falls into the night and similarly for the weekday and the weekends so there could be some",
    "start": "405240",
    "end": "410360"
  },
  {
    "text": "contextual anomalies happening within that you know particular time window so",
    "start": "410360",
    "end": "415599"
  },
  {
    "text": "that is something that the you know static thresholds I simply cannot uh really detect and then also multiple",
    "start": "415599",
    "end": "422440"
  },
  {
    "text": "metrics collectively determine the system Health okay it's not just one metric or you know if just one metric",
    "start": "422440",
    "end": "428879"
  },
  {
    "text": "goes wrong you just roll back everything no it's actually a collectively uh like a coherent",
    "start": "428879",
    "end": "434919"
  },
  {
    "text": "mechanism where all of these different metrics determine the system Health in a multivariate fashion okay and and of",
    "start": "434919",
    "end": "440720"
  },
  {
    "text": "course they they might not be uh like you know of of equal weightage it could",
    "start": "440720",
    "end": "446240"
  },
  {
    "text": "be that you you are saying that for your application that error rate is the most important you give like 30% weightage to",
    "start": "446240",
    "end": "452720"
  },
  {
    "text": "that right but for CPU and memory probably you don't care much so you just give like 10% there so that's also one",
    "start": "452720",
    "end": "458479"
  },
  {
    "text": "more thing all right so now with that I will move on to the multivariate anormal",
    "start": "458479",
    "end": "465319"
  },
  {
    "text": "detection now before that I just want to ask in the audience how many of you have background in data science and",
    "start": "465319",
    "end": "472840"
  },
  {
    "text": "ml okay very few but no worries uh my goal is to give you a broad intuition",
    "start": "472840",
    "end": "478120"
  },
  {
    "text": "behind the approach regardless all right um all right so now we're going to talk about the multivariate normal detection",
    "start": "478120",
    "end": "484280"
  },
  {
    "text": "but before that what is a multivariate metric or or I would say what are multiv metrics and how does it look like so you",
    "start": "484280",
    "end": "490800"
  },
  {
    "text": "can see this is how uh a typical Service uh I would say the golden signal metrics",
    "start": "490800",
    "end": "496560"
  },
  {
    "text": "look like all right so you can see that how different each metric are for example the error rate and latency",
    "start": "496560",
    "end": "503479"
  },
  {
    "text": "follow like a proper seasonality it's uh daily as well as weekly you can see that",
    "start": "503479",
    "end": "509280"
  },
  {
    "text": "um but error rate has a large amount of variance in the data so there is there's",
    "start": "509280",
    "end": "514680"
  },
  {
    "text": "a lot of spikes there so here as you can see this is this is an example of a contextual anomaly if you had a global",
    "start": "514680",
    "end": "520240"
  },
  {
    "text": "anomaly here okay it wouldn't be able to detect this contextual anomaly here right and then similarly you have CPU",
    "start": "520240",
    "end": "525760"
  },
  {
    "text": "and memory which show an upward Trend so these are different structures in the time series that are there and that we",
    "start": "525760",
    "end": "531120"
  },
  {
    "text": "need to tackle so before that we need to Define some requirements from both from the ml",
    "start": "531120",
    "end": "536760"
  },
  {
    "text": "perspective and from the engineering perspective all right right so first of all the ml model needs to be",
    "start": "536760",
    "end": "544720"
  },
  {
    "text": "completely unsupervised by unsupervised I mean that we don't have the luxury of Target labels in the data you know which",
    "start": "544720",
    "end": "552360"
  },
  {
    "text": "can specify if the data is anomalous or not so we don't we don't have that luxury so it's completely up to the",
    "start": "552360",
    "end": "557560"
  },
  {
    "text": "model to actually understand like from the data by itself and say whether that point is anomalous or not all right it",
    "start": "557560",
    "end": "564959"
  },
  {
    "text": "needs to of course it needs to handle multiple features right because we are talking about multivariant here um yeah",
    "start": "564959",
    "end": "571519"
  },
  {
    "text": "so it needs to understand the underlying structure of the time series The seasonality the trend uh the noise a little bit and then it should be fairly",
    "start": "571519",
    "end": "578519"
  },
  {
    "text": "quick to train the and the reason I say that is because we we need to deploy this pipeline or deploy these models",
    "start": "578519",
    "end": "585360"
  },
  {
    "text": "into different clusters okay so it needs to be fairly quick and there are many different applications running add into",
    "start": "585360",
    "end": "591120"
  },
  {
    "text": "it you know within those clusters okay um and you know storage is expensive so",
    "start": "591120",
    "end": "597240"
  },
  {
    "text": "we need to make sure that um it can perform well with not more than 8 days worth of data of training all right",
    "start": "597240",
    "end": "604399"
  },
  {
    "text": "and also generate interpretable anomaly scores I mean something that any end user can look at it and say okay the",
    "start": "604399",
    "end": "609839"
  },
  {
    "text": "anomaly looks high or the anomal looks good but not something like you know some ml engineer or or data scientist",
    "start": "609839",
    "end": "615839"
  },
  {
    "text": "like me needs to just go and debug that right um and it also you know it should",
    "start": "615839",
    "end": "621480"
  },
  {
    "text": "handle Auto model life cycle management so if a model goes taale or something like that it should be able to retrain",
    "start": "621480",
    "end": "627519"
  },
  {
    "text": "it and then have a fresh new model on that now from the engineering standpoint we also have a few requirements there so",
    "start": "627519",
    "end": "634399"
  },
  {
    "text": "first of all all of this whole thing is actually real time so there needs to be a streaming data processing system which",
    "start": "634399",
    "end": "640680"
  },
  {
    "text": "is very very important okay and it needs to support custom sources and syncs now why custom well I could I could want to",
    "start": "640680",
    "end": "648399"
  },
  {
    "text": "uh let's say I want to get the data from Prometheus or Thanos all right that that would be a custom source well you might",
    "start": "648399",
    "end": "654399"
  },
  {
    "text": "want to use a different source for that um and similarly for the syns I I might want to you know send the data out to",
    "start": "654399",
    "end": "660880"
  },
  {
    "text": "wfront or maybe Prometheus again back again or you know whatnot so that is very important and then the most",
    "start": "660880",
    "end": "666760"
  },
  {
    "text": "important is that the sliding window aggregation support now this is critical because the model needs to be fed",
    "start": "666760",
    "end": "673160"
  },
  {
    "text": "sequences of data for it to understand the underlying structure of the metric so if you just send it one data point it",
    "start": "673160",
    "end": "679560"
  },
  {
    "text": "just won't understand it because it just like cannot understand the you know the time series patterns uh you know of the",
    "start": "679560",
    "end": "686800"
  },
  {
    "text": "data okay so this is very important and we'll come to that and then it has to be lightweight I mean since we are deploying this to multiple",
    "start": "686800",
    "end": "693360"
  },
  {
    "text": "clusters it has to be lightweight um easy to deploy to multiple clusters and then we also need to choose the right",
    "start": "693360",
    "end": "699120"
  },
  {
    "text": "tool for Progressive delivery and in this case of course we used AR rollouts which worked really good so I'll explain",
    "start": "699120",
    "end": "706200"
  },
  {
    "text": "the concept from a broad level first and then we will keep going deeper and deeper okay",
    "start": "706200",
    "end": "713000"
  },
  {
    "text": "so so here you can see that I have a service and I have just picked the",
    "start": "713000",
    "end": "718040"
  },
  {
    "text": "golden signal metrics which is the the error rates latency CPU and the memory so this is plain and simple there's a",
    "start": "718040",
    "end": "724320"
  },
  {
    "text": "service which is generating some metrics in this case the golden signal metrics and then we are storing those metrics in",
    "start": "724320",
    "end": "729800"
  },
  {
    "text": "the Prometheus store okay fairly simple now let's take that and see what happens",
    "start": "729800",
    "end": "735199"
  },
  {
    "text": "during a progressive delivery Okay now what's going on is that same service is generating the metrics um uh and you",
    "start": "735199",
    "end": "743600"
  },
  {
    "text": "know saving to the Prometheus store but now during Progressive delivery it is now generating a second set of metrics",
    "start": "743600",
    "end": "749279"
  },
  {
    "text": "which is for the canary deployment okay so we have the stable and Canary so the stable is the current version and the",
    "start": "749279",
    "end": "755160"
  },
  {
    "text": "canary is the new version so we are generating the metrics uh the stable metrics and you saving that to the",
    "start": "755160",
    "end": "761279"
  },
  {
    "text": "Prometheus store and similarly for the canary and finally what happens is that we we actually pull those data into the",
    "start": "761279",
    "end": "767920"
  },
  {
    "text": "ml pip planine which we will dive deeper into and but it comes in two different payloads all right so we we actually get",
    "start": "767920",
    "end": "775360"
  },
  {
    "text": "the you know payload for the stable and Canary and and finally the expected",
    "start": "775360",
    "end": "780600"
  },
  {
    "text": "output is we generate anomaly scores for both of those payloads so we generate an anomaly score for the stable version and",
    "start": "780600",
    "end": "787320"
  },
  {
    "text": "also for the canary version all right so now we'll focus on a very",
    "start": "787320",
    "end": "792959"
  },
  {
    "text": "critical step of the whole pipeline which is the sliding window aggregation and uh remember so I talked about that",
    "start": "792959",
    "end": "799440"
  },
  {
    "text": "the model needs the data in a sequence for it to understand right so what we do here is let's assume that we have a",
    "start": "799440",
    "end": "805800"
  },
  {
    "text": "sliding window size of three all right and let's just assume for Simplicity we are just looking at two multivariate",
    "start": "805800",
    "end": "811839"
  },
  {
    "text": "metrics one is error rate and one is the latency okay so here how it goes so you",
    "start": "811839",
    "end": "817079"
  },
  {
    "text": "have the promethus store and we are you fetching the metric at time T which is",
    "start": "817079",
    "end": "822240"
  },
  {
    "text": "the current time let's uh look at that right so you have the error rate and the latency so we get a vector okay just a a",
    "start": "822240",
    "end": "829160"
  },
  {
    "text": "size of two then the sliding window reducer when you pass through it what you get is actually a matrix so you get",
    "start": "829160",
    "end": "836480"
  },
  {
    "text": "not only the data for the current time stamp but also at T minus 1 and tus2",
    "start": "836480",
    "end": "842759"
  },
  {
    "text": "that way we have a sliding window size of three and then finally this can be fed into the model so that it can",
    "start": "842759",
    "end": "848800"
  },
  {
    "text": "understand the data in a sequence all right so now we're going to go deeper",
    "start": "848800",
    "end": "855120"
  },
  {
    "text": "into the ml inference steps all right so um this might look",
    "start": "855120",
    "end": "860480"
  },
  {
    "text": "complex but trust me it is fairly simple I'll just go over each of them one by one so we have the sliding window",
    "start": "860480",
    "end": "866440"
  },
  {
    "text": "reducer from the last slide right and we are getting so you know and we're getting the data in a window fashion the",
    "start": "866440",
    "end": "872920"
  },
  {
    "text": "first thing that we do is we need to pre-process the data now why is that now the model usually requires the data to",
    "start": "872920",
    "end": "879560"
  },
  {
    "text": "be normalized between a certain range usually that is between 0 and 1 or maybe minus 1 to 1 all right that is that is",
    "start": "879560",
    "end": "886600"
  },
  {
    "text": "one thing the second thing is that the you know life is not that simple uh I mean there are so many spikes and all of",
    "start": "886600",
    "end": "893759"
  },
  {
    "text": "that in the data that we might need to smooth some of the spikes to just get the real structure out of it okay it",
    "start": "893759",
    "end": "900360"
  },
  {
    "text": "could it could be very spiky there right so that is optional but we you know we sometimes like to do that as well now",
    "start": "900360",
    "end": "906880"
  },
  {
    "text": "then that that pre-processed data passes into the main neural network now here we",
    "start": "906880",
    "end": "913160"
  },
  {
    "text": "we actually get the raw output from the model OKAY raw output meaning that it is it is basically an unbounded value um",
    "start": "913160",
    "end": "921120"
  },
  {
    "text": "and then it is very difficult to interpret what that means all right uh now it's very important to know that we",
    "start": "921120",
    "end": "926519"
  },
  {
    "text": "are using the same model to predict for for both the stable and the canary payloads that is that is critical here",
    "start": "926519",
    "end": "933279"
  },
  {
    "text": "all right and I'll come to the training later but that's what's happening here and then finally that raw output is sent",
    "start": "933279",
    "end": "938759"
  },
  {
    "text": "into the postprocess step where we are actually just normalizing the output score to be more human interpretable so",
    "start": "938759",
    "end": "945639"
  },
  {
    "text": "maybe you want to have a probability distribution maybe between 0 to 1 or you might want something like uh you know",
    "start": "945639",
    "end": "952839"
  },
  {
    "text": "something like 0 to 10 where 0 means uh you know it's not anomalous at all and 10 meaning it's the highest anomaly so",
    "start": "952839",
    "end": "959040"
  },
  {
    "text": "is a range of severity that you can get out of it all right um and then you get the single scalar value for each window",
    "start": "959040",
    "end": "965600"
  },
  {
    "text": "there all right now this is the inference flow now what happens the first time you onboard your application",
    "start": "965600",
    "end": "971880"
  },
  {
    "text": "well there is no model there right so what happens is that it checks for whether the model exists if it does not",
    "start": "971880",
    "end": "978839"
  },
  {
    "text": "so you know if it does not exist then it passes on to the trainer which trains the model from the historic data stored",
    "start": "978839",
    "end": "985279"
  },
  {
    "text": "in Prometheus remember we have been you know putting those data inside prome so we you know fetch 8 days worth of",
    "start": "985279",
    "end": "991920"
  },
  {
    "text": "historic data now here we are not training on maybe let's say you know stable and conary versions differently",
    "start": "991920",
    "end": "997040"
  },
  {
    "text": "no we are just training on the stable versions because that's the one that we care about right that's the pattern we",
    "start": "997040",
    "end": "1002279"
  },
  {
    "text": "want to we want our model to understand all right and then it's saved and then that's it and then the next time the",
    "start": "1002279",
    "end": "1009199"
  },
  {
    "text": "inference pipeline can take that model up all right so now I'll go a little bit about the model details um not very",
    "start": "1009199",
    "end": "1016680"
  },
  {
    "text": "detailed but a little bit about what we are doing there so the good thing is that all of these models are available",
    "start": "1016680",
    "end": "1021920"
  },
  {
    "text": "in numic uh okay which is open source so so these models are quick to train even",
    "start": "1021920",
    "end": "1028319"
  },
  {
    "text": "without gpus I mean that was one of the fundamental like things we wanted to achieve there um it's robust to",
    "start": "1028319",
    "end": "1034678"
  },
  {
    "text": "anomalies in the training data as I said the training data is not perfect there there are anomalies and there are no",
    "start": "1034679",
    "end": "1040000"
  },
  {
    "text": "labels associated with those anomalies that we can just remove them right because that's another engineering",
    "start": "1040000",
    "end": "1045079"
  },
  {
    "text": "effort so it's robust anomalies in the training data as well um you also have the mechanism to",
    "start": "1045079",
    "end": "1051760"
  },
  {
    "text": "feature way them right so uh maybe let's say latency is 40% U you know and then",
    "start": "1051760",
    "end": "1057799"
  },
  {
    "text": "you error rate is 40% as well and CPU in memory is 10 10% so you could do that or you could just keep it all even that's",
    "start": "1057799",
    "end": "1064520"
  },
  {
    "text": "up to you and then the anomaly scores are interpretable now why is that because we generate two kinds of anomaly",
    "start": "1064520",
    "end": "1071039"
  },
  {
    "text": "scores one is a unified anomaly score which means that for all of your metrics you're getting just one score output",
    "start": "1071039",
    "end": "1077640"
  },
  {
    "text": "okay so one score that that can determine your system health and then you also have a per metric score so",
    "start": "1077640",
    "end": "1082960"
  },
  {
    "text": "let's say you have 10 metrics that you're feeding into the model you will have you will have anomaly scores for all those 10 metrics that way you can go",
    "start": "1082960",
    "end": "1090200"
  },
  {
    "text": "back backtrack and see whether you know which metric is the one that is most",
    "start": "1090200",
    "end": "1095760"
  },
  {
    "text": "likely cause of anomaly all right and then so you know talk a little bit about",
    "start": "1095760",
    "end": "1101760"
  },
  {
    "text": "the network itself we use uh you know convolution new network and record NE networks so these are you know pretty",
    "start": "1101760",
    "end": "1108039"
  },
  {
    "text": "robust and and you know one of the best ways to model time series okay and uh",
    "start": "1108039",
    "end": "1114120"
  },
  {
    "text": "the way we do that is we use an autoencoder Network now I won't go deep into this but just understand this the",
    "start": "1114120",
    "end": "1120480"
  },
  {
    "text": "auto encoder has two components to it one is the encoder one is the decoder all right the encoder takes the input",
    "start": "1120480",
    "end": "1126880"
  },
  {
    "text": "data and tries to compress the data the input data into a compressed representation all right the decoder",
    "start": "1126880",
    "end": "1133200"
  },
  {
    "text": "takes that compressed representation and tries to reconstruct the original data in the same dimension but if the input",
    "start": "1133200",
    "end": "1139919"
  },
  {
    "text": "data has anomalies the Reconstruction would be very bad and that's the basis of the whole anomalies code that we have",
    "start": "1139919",
    "end": "1145000"
  },
  {
    "text": "here okay um if you didn't understand that don't worry about it um you can",
    "start": "1145000",
    "end": "1150679"
  },
  {
    "text": "actually come and talk to me there so now this is one example of the output and the most important thing to",
    "start": "1150679",
    "end": "1157679"
  },
  {
    "text": "note here is the feature scores okay so here in the feature scores you can see that we have different scores for latency CPU error rate and memory okay",
    "start": "1157679",
    "end": "1164720"
  },
  {
    "text": "and we have weighed them accordingly so this is latency is like 40% here and then you have a final unified score",
    "start": "1164720",
    "end": "1170679"
  },
  {
    "text": "which is a weighted average of these uh you know feature scores here so now I'll",
    "start": "1170679",
    "end": "1176520"
  },
  {
    "text": "hand it over to Bala here y thank you aik so let's dive into like a real world",
    "start": "1176520",
    "end": "1182919"
  },
  {
    "text": "how we can going to implement this all the concept into in so let me jump into that architecture",
    "start": "1182919",
    "end": "1189400"
  },
  {
    "text": "level so in int we are using a premius as a metrix collector the premi is going",
    "start": "1189400",
    "end": "1195159"
  },
  {
    "text": "to collect that all the service metrix every 30 seconds and inject into our AA pipeline in the a pipeline we have like",
    "start": "1195159",
    "end": "1202360"
  },
  {
    "text": "a window so there we will group that all the metrics into the Sur and roll out",
    "start": "1202360",
    "end": "1208159"
  },
  {
    "text": "hash and pass it into that ml steps like a pre-processing and inference once that",
    "start": "1208159",
    "end": "1214880"
  },
  {
    "text": "anomaly sore is generated in the postprocessing step and it will push it back to the preas so that Argo roll out",
    "start": "1214880",
    "end": "1223200"
  },
  {
    "text": "will query that unified anomal score during that Canary deployment and assess that how the new deployment will be good",
    "start": "1223200",
    "end": "1230600"
  },
  {
    "text": "or bad so one of the engineering requirement on the AI pipeline it need",
    "start": "1230600",
    "end": "1235840"
  },
  {
    "text": "to be realtime streaming so we are using a numa flow as a streaming",
    "start": "1235840",
    "end": "1242640"
  },
  {
    "text": "pipeline what is Numa flow the neaf flow is the in open source project which is a",
    "start": "1242640",
    "end": "1248360"
  },
  {
    "text": "a kubernetes native servus platform for running the scalable and reliable stream",
    "start": "1248360",
    "end": "1253600"
  },
  {
    "text": "processing engine it's a it's a totally very lightweight so you can run it in the Rasberry P or anything so that",
    "start": "1253600",
    "end": "1261880"
  },
  {
    "text": "that's a main requirement in that uh AA pipeline because we are going to deploy this AA pipeline that every cluster and",
    "start": "1261880",
    "end": "1268440"
  },
  {
    "text": "it need to be like a very cost efficient and very lightweight the second it's a language",
    "start": "1268440",
    "end": "1275520"
  },
  {
    "text": "agnostics because some of the ml VTX need to be implemented in the python and",
    "start": "1275520",
    "end": "1280679"
  },
  {
    "text": "some of the sync or Source need to be implemented in Java or um goang so the",
    "start": "1280679",
    "end": "1287559"
  },
  {
    "text": "numer flow is coming with the the popular language SDK like Java Python",
    "start": "1287559",
    "end": "1292799"
  },
  {
    "text": "goang and rest so you can easily implement it it's a very easy to implement and it's coming with a inbu",
    "start": "1292799",
    "end": "1299559"
  },
  {
    "text": "source and syns for the famous streaming tools like a Kofa NCH HTTP request in",
    "start": "1299559",
    "end": "1305720"
  },
  {
    "text": "our AI pipeline we are mainly using a HTTP source and sync so that it's",
    "start": "1305720",
    "end": "1311120"
  },
  {
    "text": "receiving that metrics from the premus using a remote wrer and the Numa is",
    "start": "1311120",
    "end": "1316520"
  },
  {
    "text": "coming with the out of box Auto scaling so every vertex will scale independently",
    "start": "1316520",
    "end": "1323480"
  },
  {
    "text": "based on that back pressure of the vertex and it's it's a lightweight so",
    "start": "1323480",
    "end": "1329080"
  },
  {
    "text": "that it's a very cost efficient too okay let's get into the fun part is",
    "start": "1329080",
    "end": "1335000"
  },
  {
    "text": "a demo everybody's waiting for that so let me uh lay down that what is my demo",
    "start": "1335000",
    "end": "1341080"
  },
  {
    "text": "plan I'm going to do the two demos one is like a live demo the second one is a",
    "start": "1341080",
    "end": "1346120"
  },
  {
    "text": "recorded demo the live demo I'm going to see the realtime a pipeline which is",
    "start": "1346120",
    "end": "1352720"
  },
  {
    "text": "currently getting the metrics from the preas and generating anomal score for each Services the second one is a",
    "start": "1352720",
    "end": "1359279"
  },
  {
    "text": "recorded one how the progressive deliver is happening on using the anomaly",
    "start": "1359279",
    "end": "1366240"
  },
  {
    "text": "score okay okay so this is the Numa flow U so",
    "start": "1366240",
    "end": "1372919"
  },
  {
    "text": "num us packed with lot of features so that will help",
    "start": "1372919",
    "end": "1377960"
  },
  {
    "text": "you to Main manage your pipeline as well as you can debug your pipeline you can see that this is the ml pipeline running",
    "start": "1377960",
    "end": "1385400"
  },
  {
    "text": "on the live cluster and you can see that you can easily find like how is your",
    "start": "1385400",
    "end": "1390760"
  },
  {
    "text": "health of your pipeline how is the message lagging is happening on the real",
    "start": "1390760",
    "end": "1397320"
  },
  {
    "text": "time and uh basically you can see that every ver vertex how much the processing",
    "start": "1397320",
    "end": "1404520"
  },
  {
    "text": "speed and how much Auto scaling is happening here so this this is the window vertex as a set we are we are",
    "start": "1404520",
    "end": "1412480"
  },
  {
    "text": "aggregating aggregating the all the Matrix with the previous data points So",
    "start": "1412480",
    "end": "1417520"
  },
  {
    "text": "currently we are using a window length is a 12 it's a real time so n to n minus",
    "start": "1417520",
    "end": "1423880"
  },
  {
    "text": "12 data points will be passed into the inference to generate anomal",
    "start": "1423880",
    "end": "1429520"
  },
  {
    "text": "score let me go to the inference you can see that so every every every uh data points",
    "start": "1429520",
    "end": "1438559"
  },
  {
    "text": "will be inferenced and generating anomaly score and you did I think you noticed here the Numa flow UI is giving",
    "start": "1438559",
    "end": "1445880"
  },
  {
    "text": "like a every vertices how much CPUs is utilizing how much memories utilizing",
    "start": "1445880",
    "end": "1451159"
  },
  {
    "text": "and you can see that all the container details all the memory utilization details here so that if any problem you",
    "start": "1451159",
    "end": "1457840"
  },
  {
    "text": "can you don't need to go to the cluster or anything you can just come to here and just uh debug it so finally we have",
    "start": "1457840",
    "end": "1464600"
  },
  {
    "text": "like a premus sync which is receiving that all the payload from the ml ml",
    "start": "1464600",
    "end": "1469679"
  },
  {
    "text": "vertices and pushing into the preas you can see that here it is",
    "start": "1469679",
    "end": "1475600"
  },
  {
    "text": "pushing that every anomal sare into the preas so that it will get ready for Argo roll out to cor and assess that okay",
    "start": "1475600",
    "end": "1484360"
  },
  {
    "text": "let's go to the second demo which is a recorded demo so I have a service",
    "start": "1484360",
    "end": "1490320"
  },
  {
    "text": "which is deployed into the TGO roll out and I have a analysis template which is",
    "start": "1490320",
    "end": "1496640"
  },
  {
    "text": "like a monitoring that anomaly score if it is greater than four it will be",
    "start": "1496640",
    "end": "1502360"
  },
  {
    "text": "failed and less than four will be passed so in int we are using anomalous score",
    "start": "1502360",
    "end": "1508159"
  },
  {
    "text": "range is 0 to 10 0 to 4 will be a good greater than four will be a",
    "start": "1508159",
    "end": "1515440"
  },
  {
    "text": "anomalous so I have a buggy image which is created create a lightly increased uh",
    "start": "1515480",
    "end": "1522880"
  },
  {
    "text": "error rate and latency because my servic is configured uh weightage of",
    "start": "1522880",
    "end": "1528960"
  },
  {
    "text": "error rate and latency is high like a 40% 40% and CPU and memory is",
    "start": "1528960",
    "end": "1534279"
  },
  {
    "text": "10% so I'm going to merge that new",
    "start": "1534279",
    "end": "1538799"
  },
  {
    "text": "image then Argo CD will immediately detect it and it will try to sync",
    "start": "1542399",
    "end": "1550840"
  },
  {
    "text": "it so you can see that once it syn you can immediately see that the canary pod will coming into",
    "start": "1555919",
    "end": "1563880"
  },
  {
    "text": "that so you can see that the canary new pod is created because I created like a 30% traffic routing and you can see that",
    "start": "1563880",
    "end": "1571120"
  },
  {
    "text": "analysis run was start running and uh looking into the anomal scores so I moved into the",
    "start": "1571120",
    "end": "1579640"
  },
  {
    "text": "dashboard you can see that currently that all the stable hashes stable anomal scores are coming into the dashboard you",
    "start": "1579640",
    "end": "1585919"
  },
  {
    "text": "can see that because the services running normally with uh low latency and",
    "start": "1585919",
    "end": "1590960"
  },
  {
    "text": "uh error rate you can see that there is no anomaly at all so it's all less than one so let's wait for that Canary part",
    "start": "1590960",
    "end": "1598880"
  },
  {
    "text": "to accept the traffic and start generating anom score you can see that the canary parts metries are start",
    "start": "1598880",
    "end": "1607440"
  },
  {
    "text": "coming let's wait for the CPU to come okay so if you see that as I said my",
    "start": "1607440",
    "end": "1613159"
  },
  {
    "text": "buggy image has like a slightly increased error rate and latency from the normal Serv",
    "start": "1613159",
    "end": "1619120"
  },
  {
    "text": "so you can see that the future each individual Futures anomaly was very high because it's it's not a normal pattern",
    "start": "1619120",
    "end": "1625520"
  },
  {
    "text": "it's a little bit anom anomalist so it's went up so that if you see that the unified anomaly score based on the",
    "start": "1625520",
    "end": "1631760"
  },
  {
    "text": "weightage it's increased the greater than four let's go to that",
    "start": "1631760",
    "end": "1638559"
  },
  {
    "text": "uh uh roll out to see that how it is detecting so an analysis run I'm going",
    "start": "1638559",
    "end": "1645520"
  },
  {
    "text": "there so you can see that which was already d Ed greater than four and it's already flagging like uh your imagees",
    "start": "1645520",
    "end": "1655398"
  },
  {
    "text": "failed so once it's three times it's failed they it will automatically revert",
    "start": "1655640",
    "end": "1661880"
  },
  {
    "text": "back your new image so you can see that it's already degraded and it's it's",
    "start": "1661880",
    "end": "1667039"
  },
  {
    "text": "failed that an analysis run and it is rolled back to your normal",
    "start": "1667039",
    "end": "1673320"
  },
  {
    "text": "image okay that's all I have",
    "start": "1673320",
    "end": "1679440"
  },
  {
    "text": "sorry let me go back to the",
    "start": "1681360",
    "end": "1685320"
  },
  {
    "text": "slide okay thank you guys uh I'm from Numa flow and num logic",
    "start": "1690320",
    "end": "1697720"
  },
  {
    "text": "team a we also y so this all the model and all the",
    "start": "1697720",
    "end": "1704360"
  },
  {
    "text": "pipelines and everything's open sourced you can go to the num appro gith up repo",
    "start": "1704360",
    "end": "1710279"
  },
  {
    "text": "and you can find this all the pipeline and all the models you can just uh",
    "start": "1710279",
    "end": "1716120"
  },
  {
    "text": "deploy into your cluster it will run out of box with Argo roll",
    "start": "1716120",
    "end": "1721600"
  },
  {
    "text": "out any questions all right thank you yeah",
    "start": "1722399",
    "end": "1729480"
  },
  {
    "text": "yes thank [Applause] you great presentation guys but how do",
    "start": "1730080",
    "end": "1736320"
  },
  {
    "text": "you account for",
    "start": "1736320",
    "end": "1740039"
  },
  {
    "text": "well so I can take that so yeah so the question is that how do you plan for um",
    "start": "1753279",
    "end": "1759200"
  },
  {
    "text": "you know something that you foresee is going to happen like on the metric side like for example if we know that a new",
    "start": "1759200",
    "end": "1765159"
  },
  {
    "text": "change is going to increase the memory and the know CPU there right so from an",
    "start": "1765159",
    "end": "1770240"
  },
  {
    "text": "mlop standpoint I think it's fairly straightforward that you discard that model okay because what you're basically",
    "start": "1770240",
    "end": "1776840"
  },
  {
    "text": "saying is that this new change is completely has a completely different distribution from what I had seen before",
    "start": "1776840",
    "end": "1784559"
  },
  {
    "text": "okay so you just discard that model and then it will do the retraining again on the like on the newer metrics that you",
    "start": "1784559",
    "end": "1791120"
  },
  {
    "text": "said uh but it might take maybe a day to you know get all those metrics and get a",
    "start": "1791120",
    "end": "1796480"
  },
  {
    "text": "reliable anomaly score there so we have had these uh these issues so we do have some fail safe checks that we also",
    "start": "1796480",
    "end": "1803600"
  },
  {
    "text": "incorporate like an ensemble model with with some kind of a static thresholding just to keep everything at check",
    "start": "1803600",
    "end": "1809960"
  },
  {
    "text": "there would you ever consider develop model lower",
    "start": "1809960",
    "end": "1816240"
  },
  {
    "text": "envir the lower environment the problem is that the metrics would be different so if let's say that I like talk about",
    "start": "1816279",
    "end": "1822360"
  },
  {
    "text": "the QA right the QA if your application is in QA it's not getting any traffic at all one thing that you could do though",
    "start": "1822360",
    "end": "1829360"
  },
  {
    "text": "having the same traffic in staging environment something like that and those same metrics you can actually",
    "start": "1829360",
    "end": "1834679"
  },
  {
    "text": "train that train your model there and just transfer that model into prod",
    "start": "1834679",
    "end": "1840760"
  },
  {
    "text": "yeah yeah thank you good question",
    "start": "1840760",
    "end": "1844640"
  },
  {
    "text": "though yeah hi model",
    "start": "1845960",
    "end": "1853360"
  },
  {
    "text": "yeah yeah sure yeah okay so the question is that um well these models seem very",
    "start": "1867639",
    "end": "1874080"
  },
  {
    "text": "complex why not just use some kind of an llm um and just let's say that this is",
    "start": "1874080",
    "end": "1879840"
  },
  {
    "text": "this is how the data looks like and how this will okay uh and first of all our models are much much more simpler than",
    "start": "1879840",
    "end": "1885840"
  },
  {
    "text": "nlm uh okay because in LL M by default is much much complex we just see from the outside and just an API that's why",
    "start": "1885840",
    "end": "1892559"
  },
  {
    "text": "it seems very um like simple right but um another thing is this is the first",
    "start": "1892559",
    "end": "1899200"
  },
  {
    "text": "thing we tried so after llm came out llms came out we first tried giving it a",
    "start": "1899200",
    "end": "1905000"
  },
  {
    "text": "Time series data and then uh you know seeing how the output looks like it's not good at all okay it just does not",
    "start": "1905000",
    "end": "1912159"
  },
  {
    "text": "understand numeric values that well at all um yes",
    "start": "1912159",
    "end": "1919799"
  },
  {
    "text": "yeah okay so the question is was there any kind of vector database usage uh",
    "start": "1925120",
    "end": "1931519"
  },
  {
    "text": "that can be that can be then used upon by an llm uh no there is no Vector",
    "start": "1931519",
    "end": "1937080"
  },
  {
    "text": "database because if you if you think about it if you need an embedding you're actually embedding some in textual",
    "start": "1937080",
    "end": "1942639"
  },
  {
    "text": "information right but that would come in handy as an as a downstream task okay",
    "start": "1942639",
    "end": "1951559"
  },
  {
    "text": "but not at the core functionality of the anomal detection if let's say you want you want to give out something of like a",
    "start": "1951559",
    "end": "1958080"
  },
  {
    "text": "like a humanly interpretable you know message saying that why and which metric is the most cause of a normally you",
    "start": "1958080",
    "end": "1964519"
  },
  {
    "text": "could do that for sure you could do that okay and that's something that we have been working on as well uh you know something that takes the output from our",
    "start": "1964519",
    "end": "1971679"
  },
  {
    "text": "models and then use them all right hope that was clear yeah",
    "start": "1971679",
    "end": "1978679"
  },
  {
    "text": "hey thanks for the talk um how widely have you rolled out this approach in your own organization and like how often",
    "start": "1978679",
    "end": "1985880"
  },
  {
    "text": "do you encounter false positives what is it so the question is",
    "start": "1985880",
    "end": "1991559"
  },
  {
    "text": "that how widely have we used this um in our own organization oh and and how much",
    "start": "1991559",
    "end": "1997000"
  },
  {
    "text": "of false positives have we seen do you want to take that yeah yeah so basically like uh we we deployed this pipeline to",
    "start": "1997000",
    "end": "2004840"
  },
  {
    "text": "the across that all the inter clusters like almost like 450 clusters we and we",
    "start": "2004840",
    "end": "2010240"
  },
  {
    "text": "are currently migrating that all the services which are the static thrust template into the things currently we",
    "start": "2010240",
    "end": "2016320"
  },
  {
    "text": "almost running like a 500 services on the aops based Progressive delivery so",
    "start": "2016320",
    "end": "2022519"
  },
  {
    "text": "far we don't see like we see that like 1% of the PSE positive but which is more",
    "start": "2022519",
    "end": "2031240"
  },
  {
    "text": "related to that uh the service which has like a typically like their error rate will be always high during the",
    "start": "2031240",
    "end": "2038600"
  },
  {
    "text": "deployment so that we are adjusting the window length for those services to accommodate that initial ramp up time",
    "start": "2038600",
    "end": "2046880"
  },
  {
    "text": "other than that it's working very well yeah so to add to that there is also one more thing that in in the analysis",
    "start": "2046880",
    "end": "2053520"
  },
  {
    "text": "template we have a setting where it says that okay anything greater than four you roll back like four meaning the anomaly",
    "start": "2053520",
    "end": "2060320"
  },
  {
    "text": "score well the users have the ability to just change that value we just give them",
    "start": "2060320",
    "end": "2065480"
  },
  {
    "text": "a default value that will just work 85% of the time right because that's all all",
    "start": "2065480",
    "end": "2070520"
  },
  {
    "text": "we can do as a default like template and application but then they can just go ahead and do numerous things one give uh",
    "start": "2070520",
    "end": "2079440"
  },
  {
    "text": "or like add their own custom metrics which makes sense for them most of the false positives actually come which we",
    "start": "2079440",
    "end": "2084839"
  },
  {
    "text": "have seen is because like for example maybe latency doesn't work for them at all or CPU and memor maybe it's a Java",
    "start": "2084839",
    "end": "2091240"
  },
  {
    "text": "service where there is always a you know always a spike and drop in the CPU and",
    "start": "2091240",
    "end": "2096280"
  },
  {
    "text": "memory whenever you know Whenever there is a new pod coming up so in those cases they just say hey this does not work for",
    "start": "2096280",
    "end": "2102599"
  },
  {
    "text": "us so we'll give us our own metric and the cool thing is that all of this works with their own custom metrics as well as",
    "start": "2102599",
    "end": "2108119"
  },
  {
    "text": "long as they have the data in Prometheus really cool thank you guys of course this everything is a conflict",
    "start": "2108119",
    "end": "2113720"
  },
  {
    "text": "based so you can configure like every service independently yeah please thanks for the yeah my question",
    "start": "2113720",
    "end": "2121599"
  },
  {
    "text": "is that is very yeah most of the time CES",
    "start": "2121599",
    "end": "2129280"
  },
  {
    "text": "are can add that will more than the Cent",
    "start": "2135520",
    "end": "2140599"
  },
  {
    "text": "the second part of that is that why you are only a not adding the or other kind",
    "start": "2140599",
    "end": "2146599"
  },
  {
    "text": "of signal which validate the function of the service you could catch that early in place",
    "start": "2146599",
    "end": "2153520"
  },
  {
    "text": "of okay Anis no yeah yeah you can you can run",
    "start": "2156480",
    "end": "2161920"
  },
  {
    "text": "that n number of analysis to compare like a multiple uh a scores and you can",
    "start": "2161920",
    "end": "2168920"
  },
  {
    "text": "run that perfect environment scor also like uh how that your stage environment",
    "start": "2168920",
    "end": "2174760"
  },
  {
    "text": "scores and all the things you can compare it no no but uh in's case we are just",
    "start": "2174760",
    "end": "2180880"
  },
  {
    "text": "started with a production environment not comparing anything else because in in the stage is mainly for like make",
    "start": "2180880",
    "end": "2188200"
  },
  {
    "text": "sure not getting the production traffics to assess that things so that's only the production things we are considering",
    "start": "2188200",
    "end": "2194720"
  },
  {
    "text": "yeah but you can do like any way you want it for your application okay there is there is nothing stopping you from",
    "start": "2194720",
    "end": "2200240"
  },
  {
    "text": "doing doing certain things like that and like to his first question about the the",
    "start": "2200240",
    "end": "2205720"
  },
  {
    "text": "different kinds of error rate you could do that because the service owner is responsible for getting those metrics",
    "start": "2205720",
    "end": "2211200"
  },
  {
    "text": "out right so if you're a service owner and you are saying that I have two different sets of error rate okay for",
    "start": "2211200",
    "end": "2217160"
  },
  {
    "text": "the you know for the client side and for the server side you could do that get it so get it out and all we need to do is",
    "start": "2217160",
    "end": "2222880"
  },
  {
    "text": "change in the configuration to get those metrics out you can add the weightage as well and it will just work seamlessly",
    "start": "2222880",
    "end": "2228200"
  },
  {
    "text": "there is nothing um weird about it to uh",
    "start": "2228200",
    "end": "2233400"
  },
  {
    "text": "yeah uh so you showed your running your anomaly detection on your stable version as well uh two questions do you use that",
    "start": "2233400",
    "end": "2240760"
  },
  {
    "text": "for just generic alerting on running services and how do you incorporate that",
    "start": "2240760",
    "end": "2246520"
  },
  {
    "text": "into there's an anomaly in the stable version at the same time you're deploying so your Canary has that how do",
    "start": "2246520",
    "end": "2252200"
  },
  {
    "text": "you deal with that yeah I mean great question so the question is that we we uh give out anomaly scores for Canary",
    "start": "2252200",
    "end": "2258359"
  },
  {
    "text": "and stable versions what do we use the stable version score for okay so there are I mean two uh reasons for it the",
    "start": "2258359",
    "end": "2265680"
  },
  {
    "text": "first reason is that the stable version score give the end users just a holistic health of of your system like it just",
    "start": "2265680",
    "end": "2273119"
  },
  {
    "text": "gives a just an anomaly score just to see that where where you can set your alerts your you pager duties and all of",
    "start": "2273119",
    "end": "2280079"
  },
  {
    "text": "that so that's that's great because you want you want to set that on the stable version not on the you know cantic version that's one the you know second",
    "start": "2280079",
    "end": "2286720"
  },
  {
    "text": "thing is that yes you could use that so during a deployment you can actually have both of them and then do a",
    "start": "2286720",
    "end": "2293760"
  },
  {
    "text": "downstream analysis saying that hey my canary score was high but if my stable",
    "start": "2293760",
    "end": "2299480"
  },
  {
    "text": "score was also High probably there is some problem in the upstream or Downstream and not my application okay",
    "start": "2299480",
    "end": "2306680"
  },
  {
    "text": "that is possible we have not done that yet but that's in our road map and that's a that's a great question we",
    "start": "2306680",
    "end": "2311760"
  },
  {
    "text": "could use that of course thank you yeah yeah",
    "start": "2311760",
    "end": "2318400"
  },
  {
    "text": "right right okay so in that case so you have to think about it how you can get those metrics in okay that's the only",
    "start": "2340240",
    "end": "2347680"
  },
  {
    "text": "constraint once the system has the required metrics for your new version you can just do it so if you can somehow",
    "start": "2347680",
    "end": "2355440"
  },
  {
    "text": "if you somehow get those data in in your cicd pipeline you could actually run that that's no problem yeah but you have",
    "start": "2355440",
    "end": "2362720"
  },
  {
    "text": "to be creative with that right and in intu we have like a per M which will",
    "start": "2362720",
    "end": "2368560"
  },
  {
    "text": "simulate that the production traffic so we are monitoring that one we are generating anomal score on the perf",
    "start": "2368560",
    "end": "2374640"
  },
  {
    "text": "environment to generating a load to make sure there is no anomaly but which is",
    "start": "2374640",
    "end": "2380319"
  },
  {
    "text": "not like AR like a progressive delivery we will deploy it and make sure the stable one was anomally",
    "start": "2380319",
    "end": "2388640"
  },
  {
    "text": "stable we are simulating the production traffics in the perf",
    "start": "2388640",
    "end": "2394640"
  },
  {
    "text": "environment okay okay thank you guys here",
    "start": "2395000",
    "end": "2402200"
  }
]