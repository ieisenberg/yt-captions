[
  {
    "start": "0",
    "end": "80000"
  },
  {
    "text": "hello everyone we hope you have an amazing day at kubecon and you have some spare energy to learn something new and exciting",
    "start": "0",
    "end": "6640"
  },
  {
    "text": "about prometheus project before we start though i would like to you know to you to know too that maintainers team",
    "start": "6640",
    "end": "13679"
  },
  {
    "text": "and community created so many materials about promotions already you can read right",
    "start": "13679",
    "end": "18960"
  },
  {
    "text": "you can find those on our website promoters.io you can find that on blog posts youtube other cubecons fast",
    "start": "18960",
    "end": "26800"
  },
  {
    "text": "cubecons and you know in to be honest like it's kind of hard to prepare a talk",
    "start": "26800",
    "end": "32960"
  },
  {
    "text": "a deep deck from cues that doesn't repeat some knowledge but fear not we found some cool area that might be a",
    "start": "32960",
    "end": "41040"
  },
  {
    "text": "little bit different for you uh this year so what we want to do we'll try to",
    "start": "41040",
    "end": "46079"
  },
  {
    "text": "give you an overview of all newer features and deployment possibilities that we have created to accommodate many",
    "start": "46079",
    "end": "52800"
  },
  {
    "text": "many various requirements from you know cloud native community that adopts uh",
    "start": "52800",
    "end": "59359"
  },
  {
    "text": "you know the technology the cloud the communities and and all the good is there um with more companies there are much",
    "start": "59359",
    "end": "66720"
  },
  {
    "text": "more um you know use cases and different uh integrations that we have to evolve",
    "start": "66720",
    "end": "72799"
  },
  {
    "text": "and this is what we want to show but first some short introduction i have chris with me",
    "start": "72799",
    "end": "80000"
  },
  {
    "start": "80000",
    "end": "80000"
  },
  {
    "text": "hello i am chris marchbanks and i've been a member of the prometheus team since 2019. i'm currently working at grafana",
    "start": "80000",
    "end": "86799"
  },
  {
    "text": "labs on engineering on some machine learning capabilities",
    "start": "86799",
    "end": "92079"
  },
  {
    "text": "in my free time i try and spend as much time as i can skiing and climbing with my wife in the mountains outside our",
    "start": "92079",
    "end": "97119"
  },
  {
    "text": "home in colorado well i miss king as well",
    "start": "97119",
    "end": "102159"
  },
  {
    "text": "hello my name is bartek plotka and i'm a principal software engineer at red hat i am from used maintainer for a bit and i",
    "start": "102159",
    "end": "109840"
  },
  {
    "text": "also co-founded thanos project which kind of is a product for scaling permit use i'm also cncf attack observability",
    "start": "109840",
    "end": "117280"
  },
  {
    "text": "tech lead and i also write a book with aurelie which is called efficient go",
    "start": "117280",
    "end": "124000"
  },
  {
    "text": "so before we jump into those advanced cases let's introduce prometheus to",
    "start": "124000",
    "end": "129039"
  },
  {
    "start": "125000",
    "end": "125000"
  },
  {
    "text": "maybe those new here who just want to learn about what it is and what it does and let's make it very briefly",
    "start": "129039",
    "end": "135680"
  },
  {
    "text": "very brief so technically speaking prometheus is just a single stateful go binary that we",
    "start": "135680",
    "end": "142560"
  },
  {
    "text": "deploy in one of uh or two replicas next to the workloads we want to monitor so",
    "start": "142560",
    "end": "147599"
  },
  {
    "text": "the things that that produce some some data some metrics that we want to observe and grab",
    "start": "147599",
    "end": "154560"
  },
  {
    "text": "so when you start this prometus server um with the typical configuration it will first reach the discovery",
    "start": "154560",
    "end": "161840"
  },
  {
    "text": "that you configure discovery service that you configured for example it might be a dns service it might be a",
    "start": "161840",
    "end": "167920"
  },
  {
    "text": "kubernetes api it might be ecs that scans through the virtual machines that",
    "start": "167920",
    "end": "173280"
  },
  {
    "text": "you have deployed and using the labeling mechanisms or various different",
    "start": "173280",
    "end": "178800"
  },
  {
    "text": "strategies it obtained the targets that we want to scrape that we want to",
    "start": "178800",
    "end": "185280"
  },
  {
    "text": "essentially monitor so with those targets we essentially reach them and pull metrics from those targets which",
    "start": "185280",
    "end": "192560"
  },
  {
    "text": "typically are just applications and we do that with every configured interval which",
    "start": "192560",
    "end": "198879"
  },
  {
    "text": "generally means something between one second and couple of minutes and we call this process a scrape right",
    "start": "198879",
    "end": "205840"
  },
  {
    "text": "and then scrape is essentially a very fast hdb call to the end point that those applications exposes",
    "start": "205840",
    "end": "212560"
  },
  {
    "text": "and it usually exposes those uh using the open metrics or prometheus",
    "start": "212560",
    "end": "218720"
  },
  {
    "text": "text exposition format and so this exposes essentially the current",
    "start": "218720",
    "end": "223760"
  },
  {
    "text": "values of the gouges and counters that are in the system right now it will also you know watch the service",
    "start": "223760",
    "end": "229040"
  },
  {
    "text": "discovery for any changes that might appear dynamically to make sure we always have the correct place we take",
    "start": "229040",
    "end": "236000"
  },
  {
    "text": "the metrics from and they will be the available data that we pulled can be then accessed through",
    "start": "236000",
    "end": "243040"
  },
  {
    "text": "different means yeah we have various permit to use apis rest apis including query api with flexible prom ql language",
    "start": "243040",
    "end": "250400"
  },
  {
    "text": "um you can then connect rafana for some dashboarding experience you can also configure your own recording rules so to",
    "start": "250400",
    "end": "257040"
  },
  {
    "text": "make some fast aggregations or alerting rules that will trigger an alert when for example from you see that there are",
    "start": "257040",
    "end": "263680"
  },
  {
    "text": "too many errors on your application others then are sent to the other manager for routing purposes and then",
    "start": "263680",
    "end": "270320"
  },
  {
    "text": "sent to your email client phone or or slack to notify you or or some",
    "start": "270320",
    "end": "276160"
  },
  {
    "text": "further machine processes so that was general you know overview of",
    "start": "276160",
    "end": "281600"
  },
  {
    "start": "279000",
    "end": "279000"
  },
  {
    "text": "promote use very basics in this talk however we want to look on various maybe more advanced use cases but actually i",
    "start": "281600",
    "end": "289600"
  },
  {
    "text": "want to highlight that those are getting more and more typical because you know",
    "start": "289600",
    "end": "294960"
  },
  {
    "text": "with uh advancements of of technology of cncf projects we can actually",
    "start": "294960",
    "end": "301199"
  },
  {
    "text": "you know abstract so much complexity in a in a faster way so you know you might hit those cases",
    "start": "301199",
    "end": "308080"
  },
  {
    "text": "pretty soon if you are even new to the to the cloud native ecosystem",
    "start": "308080",
    "end": "313840"
  },
  {
    "text": "so we will talk about uh scraping features backfilling signal correlation",
    "start": "313840",
    "end": "319440"
  },
  {
    "text": "multi-cluster agent mode and multi-tenancy let's go for it",
    "start": "319440",
    "end": "324880"
  },
  {
    "text": "yeah so first up the core prometheus the scraping is still adding more features to allow more",
    "start": "324880",
    "end": "331520"
  },
  {
    "text": "and more use cases i want to highlight a couple of those in the service discovery component",
    "start": "331520",
    "end": "338000"
  },
  {
    "text": "first off in the last six months six new service discovery implementations have been added to",
    "start": "338000",
    "end": "344080"
  },
  {
    "text": "prometheus i especially want to thank all of the contributors who have implemented them",
    "start": "344080",
    "end": "350240"
  },
  {
    "text": "these new integrations allow for more and more users to easily configure and get started with prometheus",
    "start": "350240",
    "end": "356479"
  },
  {
    "start": "356000",
    "end": "356000"
  },
  {
    "text": "i'm especially excited about the generic http service based service discovery to allow users the ability",
    "start": "356479",
    "end": "364000"
  },
  {
    "text": "to use service discovery for anything they can think of for example in the past i've had to use sidecars and",
    "start": "364000",
    "end": "369919"
  },
  {
    "text": "kubernetes operator to fairly distribute load among a fleet of prometheus instances",
    "start": "369919",
    "end": "375680"
  },
  {
    "text": "today that would be possible using only the http service discovery",
    "start": "375680",
    "end": "381840"
  },
  {
    "text": "with all of the new service discoveries and just the continual growth of highly dynamic systems",
    "start": "383360",
    "end": "389280"
  },
  {
    "text": "prometheus has also improved handling of potentially untrusted or misconfigured",
    "start": "389280",
    "end": "394479"
  },
  {
    "text": "targets first it is now possible to limit the size and the count of labels sort of",
    "start": "394479",
    "end": "401360"
  },
  {
    "text": "especially useful one time write a service writing arbitrary error messages into a label value sometimes as large as",
    "start": "401360",
    "end": "407600"
  },
  {
    "text": "100 kilobytes in a single label value quickly brought my prometheus instance",
    "start": "407600",
    "end": "415039"
  },
  {
    "text": "to its knees second you can also now limit the body size of a scrape which can catch some",
    "start": "415039",
    "end": "422000"
  },
  {
    "text": "high carnality use cases that may also cause issues in your prometheus instance and finally",
    "start": "422000",
    "end": "428240"
  },
  {
    "text": "starting in the most recent version of prometheus we're going to allow the configuration of",
    "start": "428240",
    "end": "434400"
  },
  {
    "text": "scrape intervals and timeouts via relabeling in your in your service discovery",
    "start": "434400",
    "end": "440720"
  },
  {
    "text": "this means things like a generic kubernetes job will be able to customize a scrape",
    "start": "440720",
    "end": "447360"
  },
  {
    "text": "timeout a scrape interval on each service or each pod without having to create more",
    "start": "447360",
    "end": "453280"
  },
  {
    "text": "and more complex configurations for one-off cases",
    "start": "453280",
    "end": "458560"
  },
  {
    "text": "combined all of these new service discovery integrations and just the additional control you have",
    "start": "458560",
    "end": "464960"
  },
  {
    "text": "over scraping your targets allow more use cases and more safety than ever before",
    "start": "464960",
    "end": "470560"
  },
  {
    "text": "now back to bartek to learn about the new backfilling capabilities in prometheus",
    "start": "470560",
    "end": "476879"
  },
  {
    "text": "thank you um so in this kind of um feature you know what is very very",
    "start": "476879",
    "end": "483840"
  },
  {
    "text": "important is that we need to understand that for some people like for most of the of the users",
    "start": "483840",
    "end": "489199"
  },
  {
    "text": "of of any observability solution data is precious because once collected you",
    "start": "489199",
    "end": "494800"
  },
  {
    "text": "usually cannot you know collect it again for the situation that happened before so sometimes you want to persist that",
    "start": "494800",
    "end": "501280"
  },
  {
    "text": "data even across different systems this is why there are more and more cases when users want to import",
    "start": "501280",
    "end": "508479"
  },
  {
    "text": "bigger data sets of you know serious metrics to promote use setups for example",
    "start": "508479",
    "end": "514320"
  },
  {
    "text": "it might be due to migrating from two promoters from other systems or even between prompt uses in a different",
    "start": "514320",
    "end": "520479"
  },
  {
    "text": "locations or maybe bring back up backed up data",
    "start": "520479",
    "end": "526240"
  },
  {
    "text": "when you lose your for example persistent disk and data backfilling was always possible",
    "start": "526240",
    "end": "532240"
  },
  {
    "text": "not maybe always but for for a couple of years already but yet only recently with community we made it much more",
    "start": "532240",
    "end": "538720"
  },
  {
    "text": "approachable so let me explain how you can achieve that and import large number of metric series today",
    "start": "538720",
    "end": "546560"
  },
  {
    "start": "546000",
    "end": "546000"
  },
  {
    "text": "so first comment that i would like to introduce or like the solution is um",
    "start": "546560",
    "end": "552560"
  },
  {
    "text": "maybe you are familiar with the prompt tool a prompt tool is like another binary that builds with the prometeuse",
    "start": "552560",
    "end": "558320"
  },
  {
    "text": "project and um it allows you to perform certain operations on your",
    "start": "558320",
    "end": "564959"
  },
  {
    "text": "local machine or on the server it's like a cli tool so we recently added a",
    "start": "564959",
    "end": "570959"
  },
  {
    "text": "command called tsdb create blocks from and then it allows you to",
    "start": "570959",
    "end": "576720"
  },
  {
    "text": "define create a tsdb blocks from different inputs a tsdb block is",
    "start": "576720",
    "end": "583279"
  },
  {
    "text": "essentially a data in a format that prometheus",
    "start": "583279",
    "end": "588959"
  },
  {
    "text": "understand and and uses to store metrics uh in an efficient way right so if we",
    "start": "588959",
    "end": "595519"
  },
  {
    "text": "can create in an easy way those maybe complex binary format blocks",
    "start": "595519",
    "end": "600880"
  },
  {
    "text": "from very easy uh formats like openmetrics or oriamo then we can you",
    "start": "600880",
    "end": "606480"
  },
  {
    "text": "know just create those and put it in output and then literally just copy or move those",
    "start": "606480",
    "end": "613360"
  },
  {
    "text": "blocks into your prometeurs local storage and promptus will immediately",
    "start": "613360",
    "end": "619519"
  },
  {
    "text": "load those and even start compacting or recompacting vertically for for efficiency purposes right so this is why",
    "start": "619519",
    "end": "626720"
  },
  {
    "text": "we create the the dcb create blocks from openmetrics where you provide open metric format",
    "start": "626720",
    "end": "633440"
  },
  {
    "text": "files for different the same time periods and you can really literally use something",
    "start": "633440",
    "end": "640000"
  },
  {
    "text": "something like fromtio's exposition formats open metrics and and pack multiple series for multiple you know",
    "start": "640000",
    "end": "646560"
  },
  {
    "text": "timestamps and efficiently create this db flow from that on your local setup or somewhere else and then move the blocks",
    "start": "646560",
    "end": "653279"
  },
  {
    "text": "so that's kind of the general backfilling solution but there is one thing more that we did so it's a common",
    "start": "653279",
    "end": "660880"
  },
  {
    "start": "658000",
    "end": "658000"
  },
  {
    "text": "thing that when you do recording rules or even alerting mostly for recording rules you are",
    "start": "660880",
    "end": "666800"
  },
  {
    "text": "recording those roles so evaluating the uh certain expression for longer time",
    "start": "666800",
    "end": "672640"
  },
  {
    "text": "and then it might be a you know it might be some destruction at some point of time like let's say for",
    "start": "672640",
    "end": "678800"
  },
  {
    "text": "three days uh your clusters were down for some reason or like your your machine with promotions were down and",
    "start": "678800",
    "end": "685600"
  },
  {
    "text": "you have a gap of three days now what what you can do right now you can backfill",
    "start": "685600",
    "end": "691600"
  },
  {
    "text": "rule backfill this data which means that you can perform the same recording rule but for",
    "start": "691600",
    "end": "696800"
  },
  {
    "text": "the past data and catch up and and build this gap of data and import directly",
    "start": "696800",
    "end": "703200"
  },
  {
    "text": "into the promoters again which is amazing because you can just fix that the previous gaps if you still",
    "start": "703200",
    "end": "709760"
  },
  {
    "text": "have a raw data for this moment right and this is especially important when",
    "start": "709760",
    "end": "715279"
  },
  {
    "text": "you have like distributed um rules system maybe with the cortex system and thanos but it might be useful for permit",
    "start": "715279",
    "end": "722480"
  },
  {
    "text": "us cases as well um so for this you use ddb create blocks from rules",
    "start": "722480",
    "end": "729480"
  },
  {
    "text": "thanks spartak next up there's another new feature in prometheus which is exemplars exemplars",
    "start": "731600",
    "end": "739040"
  },
  {
    "text": "provide a way to associate context or other information to your metrics and are commonly used",
    "start": "739040",
    "end": "745200"
  },
  {
    "text": "for adding tracing information to counters or histograms",
    "start": "745200",
    "end": "750880"
  },
  {
    "text": "so what does an exemplar look like here's an example of exemplars being exported using the open metrics format",
    "start": "751519",
    "end": "758800"
  },
  {
    "text": "we can see two exemplars one with a trace id of fast and one the trace id of slow but you can add any data that you would",
    "start": "758800",
    "end": "765360"
  },
  {
    "text": "like to these that is helpful for you in addition you're provided the value of",
    "start": "765360",
    "end": "770880"
  },
  {
    "text": "the observation of the exemplar as well as the time stamp for when that value was observed",
    "start": "770880",
    "end": "776800"
  },
  {
    "text": "these fields allow the prometheus ui and grafana to plot exemplars on time series",
    "start": "776800",
    "end": "782560"
  },
  {
    "text": "charts for example here's how exemplars are",
    "start": "782560",
    "end": "788320"
  },
  {
    "start": "785000",
    "end": "785000"
  },
  {
    "text": "represented in the prometheus user interface graphing the 99th percentile of request",
    "start": "788320",
    "end": "793839"
  },
  {
    "text": "latencies for cortex then you can click on any of the exemplars present in the graph to find a trace id that you can",
    "start": "793839",
    "end": "800639"
  },
  {
    "text": "look for in your favorite tracing tool if you filter the series to only look at certain pods or",
    "start": "800639",
    "end": "807519"
  },
  {
    "text": "error status codes you will also only get exemplars relevant to those series always matching",
    "start": "807519",
    "end": "814160"
  },
  {
    "text": "the context that you're looking at having the ability to quickly jump from an alert or a dashboard to an example",
    "start": "814160",
    "end": "820959"
  },
  {
    "text": "trace is very powerful and streamlines both incident and debugging workflows",
    "start": "820959",
    "end": "827959"
  },
  {
    "text": "then client library support for exemplars is still growing currently",
    "start": "828959",
    "end": "834160"
  },
  {
    "text": "both java and go fully support exemplars and python has an open pull request to add",
    "start": "834160",
    "end": "840000"
  },
  {
    "text": "basic support that will be merged soon in order to enable collecting exemplar",
    "start": "840000",
    "end": "845040"
  },
  {
    "text": "data in prometheus as well as sending it with remote write to other systems that might use that exemplar data",
    "start": "845040",
    "end": "852399"
  },
  {
    "text": "you can use the exemplar storage feature flag also note that exemplars are only",
    "start": "852399",
    "end": "859199"
  },
  {
    "text": "collected when using the open metrics exposition format openmetrics is negotiated by default",
    "start": "859199",
    "end": "865440"
  },
  {
    "text": "when prometheus scrapes a target but some libraries such as client goaling require feature flag to enable open",
    "start": "865440",
    "end": "872079"
  },
  {
    "text": "metrics i hope you find exemplars as useful as i do and now back to bartek to discuss",
    "start": "872079",
    "end": "878480"
  },
  {
    "text": "multi-cluster use cases for prometheus thank you chris yeah i mean exemplars",
    "start": "878480",
    "end": "885120"
  },
  {
    "text": "this is something i i really would love to use more and more these days it helps so much to correlate with",
    "start": "885120",
    "end": "892320"
  },
  {
    "text": "uh with traces or or really to get the workflow done much much",
    "start": "892320",
    "end": "897600"
  },
  {
    "text": "easier but now let's talk about very common thing that that are starting to well emerge",
    "start": "897600",
    "end": "903519"
  },
  {
    "text": "uh for many of us so you know we have we used to have just one big kubernetes clusters but nowadays with um you know",
    "start": "903519",
    "end": "911040"
  },
  {
    "text": "companies and startups building solutions you know that can spin up communities clusters in like minutes or",
    "start": "911040",
    "end": "917760"
  },
  {
    "text": "like even seconds like it's a very common that we have many of them like in red hat we have",
    "start": "917760",
    "end": "923680"
  },
  {
    "text": "dozens of thousands of them running around the world so how do you even you know cope with that",
    "start": "923680",
    "end": "930639"
  },
  {
    "text": "and and what are the monitoring challenges or deployment models that we can use with promote use",
    "start": "930639",
    "end": "937519"
  },
  {
    "text": "so you know so we have one cluster uh let's imagine it's like",
    "start": "937519",
    "end": "943759"
  },
  {
    "text": "you know in europe and it's kind of production we might have different environments you have applications and",
    "start": "943759",
    "end": "950000"
  },
  {
    "text": "then as we discussed there is a prominent use running there to monitor them and what you do if you have more of",
    "start": "950000",
    "end": "956320"
  },
  {
    "text": "them now something that i want to show as a kind of anti-pattern",
    "start": "956320",
    "end": "961519"
  },
  {
    "text": "never never do that essentially never try to put prominent use outside of those clusters in let's say another",
    "start": "961519",
    "end": "968160"
  },
  {
    "text": "region or another um another cluster some are very remotely in a different",
    "start": "968160",
    "end": "973920"
  },
  {
    "text": "network and and try to use scraping mechanism so you know very low latency 15 seconds",
    "start": "973920",
    "end": "980160"
  },
  {
    "text": "interval or even faster scrape between those zones right and the",
    "start": "980160",
    "end": "985600"
  },
  {
    "text": "reason is that you need to have like a very reliable network available reliable",
    "start": "985600",
    "end": "991920"
  },
  {
    "text": "situation to actually have reliable monitoring from that so you want to have the promotes very close to your",
    "start": "991920",
    "end": "998079"
  },
  {
    "text": "application or really the ingestion part the pulling part very close to the application this is why never do that",
    "start": "998079",
    "end": "1004000"
  },
  {
    "text": "never put primitives in a different failure domain but what you can do instead right so",
    "start": "1004000",
    "end": "1010480"
  },
  {
    "text": "generally what always was possible again not always but probably like three four",
    "start": "1010480",
    "end": "1015759"
  },
  {
    "start": "1012000",
    "end": "1012000"
  },
  {
    "text": "years was character federation and essentially what it does is that you",
    "start": "1015759",
    "end": "1021440"
  },
  {
    "text": "federate so you can say scrapes from one parameters on top of another right and",
    "start": "1021440",
    "end": "1028240"
  },
  {
    "text": "usually you do that only for a set of metrics with maybe even different interval",
    "start": "1028240",
    "end": "1034798"
  },
  {
    "text": "so you can actually group and aggregate data on on the",
    "start": "1034799",
    "end": "1040798"
  },
  {
    "text": "second layer and on some other cluster for example and be able to query that data from a",
    "start": "1040799",
    "end": "1046720"
  },
  {
    "text": "single place and have all the data there if for example your primitives will die on or your cluster totally die right you",
    "start": "1046720",
    "end": "1053679"
  },
  {
    "text": "still have data and um that's kind of a great approach but it has its",
    "start": "1053679",
    "end": "1060080"
  },
  {
    "text": "own trade-offs first of all it's a double scrape again it's again scraped across the domain so",
    "start": "1060080",
    "end": "1065120"
  },
  {
    "text": "it's not you know very reliable for all cases and um and overall you usually need to",
    "start": "1065120",
    "end": "1072480"
  },
  {
    "text": "have you cannot you know fetch all the data you have to fetch like very small",
    "start": "1072480",
    "end": "1079760"
  },
  {
    "text": "percentage of the data to to to make it even possible so there are limits to this to",
    "start": "1079760",
    "end": "1085760"
  },
  {
    "text": "the solution so what kind of you know we build or like community build um is for example",
    "start": "1085760",
    "end": "1093520"
  },
  {
    "start": "1088000",
    "end": "1088000"
  },
  {
    "text": "remote with integration the promotions so this allows systems like thanos to",
    "start": "1093520",
    "end": "1099440"
  },
  {
    "text": "just attach to the to the prometeurs so build we can kind of have a sidecar that kind of transform that",
    "start": "1099440",
    "end": "1106000"
  },
  {
    "text": "um data to some jrpc api which is kind of easier to use internals and then you can place some kind of querying component",
    "start": "1106000",
    "end": "1113120"
  },
  {
    "text": "aggregating aggregation component on global level to have to be able to query the data from multiple locations",
    "start": "1113120",
    "end": "1119440"
  },
  {
    "text": "multiple clusters and i'm talking about hundreds thousands and and every query will try to find a",
    "start": "1119440",
    "end": "1125919"
  },
  {
    "text": "proper place and find out to the relevant places for the data and aggregate and perform promptql",
    "start": "1125919",
    "end": "1131919"
  },
  {
    "text": "evaluation on top of that and this is thanks of the remote read protocol which allows fetching series",
    "start": "1131919",
    "end": "1138880"
  },
  {
    "text": "from promoters directly again the limitation at trade-off here is that you are",
    "start": "1138880",
    "end": "1145200"
  },
  {
    "text": "you are accessing the data which is still on those cluster kind of components on those clusters so when the",
    "start": "1145200",
    "end": "1152080"
  },
  {
    "text": "cluster is down or maybe network is unreliable between you and the cluster you don't have visibility of those",
    "start": "1152080",
    "end": "1157600"
  },
  {
    "text": "metrics the advantage is that you don't move any data right well you don't move persistently and you don't persist",
    "start": "1157600",
    "end": "1163840"
  },
  {
    "text": "anywhere you only move it when you need them right and generally to be to be honest like we",
    "start": "1163840",
    "end": "1170559"
  },
  {
    "text": "we love scraping a lot of collecting a lot of data but we don't use most of them so",
    "start": "1170559",
    "end": "1176000"
  },
  {
    "text": "this might be great solution for you but it's not only that i think what what",
    "start": "1176000",
    "end": "1182480"
  },
  {
    "start": "1180000",
    "end": "1180000"
  },
  {
    "text": "was what is getting more and more popular and what what people use is remote right which means that you switch",
    "start": "1182480",
    "end": "1189760"
  },
  {
    "text": "um kind of to the opposite uh model instead of you pulling of the data you are pushing it somewhere else and actually",
    "start": "1189760",
    "end": "1196559"
  },
  {
    "text": "persisting there for for uh you know forever sometimes if you have like a cheap storage so",
    "start": "1196559",
    "end": "1203760"
  },
  {
    "text": "prometheus then uses remote right feature to stream the data samples every sample or the samples you want to stream",
    "start": "1203760",
    "end": "1210480"
  },
  {
    "text": "so maybe portion of the data that you have inside cluster into them some other cluster and then there are different",
    "start": "1210480",
    "end": "1216720"
  },
  {
    "text": "many solutions that allow you to receive this data including thanos cortex and recently we",
    "start": "1216720",
    "end": "1223360"
  },
  {
    "text": "created even some system that allows you to to kind of configure that in a kind of sas product",
    "start": "1223360",
    "end": "1228960"
  },
  {
    "text": "observatorium but there are so many other solutions that you can have uh have them listed here",
    "start": "1228960",
    "end": "1235039"
  },
  {
    "text": "and um and you know that might be the same thing that that will work for you",
    "start": "1235039",
    "end": "1241200"
  },
  {
    "start": "1241000",
    "end": "1241000"
  },
  {
    "text": "beyond just those big clustered systems like cortex like thanos like things like that that",
    "start": "1243039",
    "end": "1248480"
  },
  {
    "text": "you're sending to if you don't have a ton of data or you only want to send a small subset of data similar to what you",
    "start": "1248480",
    "end": "1254960"
  },
  {
    "text": "do with a traditional federation is that native prometheus is the prometheus",
    "start": "1254960",
    "end": "1260720"
  },
  {
    "text": "binary can now be enabled to receive remote write requests this means all of your clusters can just",
    "start": "1260720",
    "end": "1268000"
  },
  {
    "text": "send a small amount of maybe the result of recording rules to a central prometheus where you can evaluate global",
    "start": "1268000",
    "end": "1275039"
  },
  {
    "text": "rules and global alerts an advantage to this is you get all of the nice remote right semantics with",
    "start": "1275039",
    "end": "1280960"
  },
  {
    "text": "retrying requests or things like that if there is some sort of network partition that occurs between your central",
    "start": "1280960",
    "end": "1287760"
  },
  {
    "text": "prometheus and your many clusters this also",
    "start": "1287760",
    "end": "1293440"
  },
  {
    "text": "feeds into edge clients which we'll talk to a little bit a little bit next",
    "start": "1293440",
    "end": "1300720"
  },
  {
    "text": "so what happens if you just can't store your metrics next to your next year apps",
    "start": "1301120",
    "end": "1306640"
  },
  {
    "text": "this happens in lightweight clusters running just like raspberry pi's iot machines",
    "start": "1306640",
    "end": "1312640"
  },
  {
    "text": "or maybe dynamic clusters that don't exist very long and don't have long don't have storage for prometheus or you",
    "start": "1312640",
    "end": "1320240"
  },
  {
    "text": "just don't have persistent disks wherever you're trying to collect metrics from",
    "start": "1320240",
    "end": "1325600"
  },
  {
    "text": "that can make a traditional prometheus architecture very challenging and many times it's",
    "start": "1325600",
    "end": "1331200"
  },
  {
    "text": "very hard to scrape data from an edge client or things like that",
    "start": "1331200",
    "end": "1337360"
  },
  {
    "text": "the upcoming prometheus agent seeks to solve these use cases",
    "start": "1337360",
    "end": "1343120"
  },
  {
    "start": "1341000",
    "end": "1341000"
  },
  {
    "text": "so prometheus agent is a mode in prometheus just configured with a flag",
    "start": "1343120",
    "end": "1348400"
  },
  {
    "text": "that you can run prometheus and all it does is collect data and forward that data via remote right to",
    "start": "1348400",
    "end": "1355440"
  },
  {
    "text": "another prometheus instance or any of the remote write receivers that we just saw on a previous slide",
    "start": "1355440",
    "end": "1362320"
  },
  {
    "text": "this new mode is based on the grafana agent and is contributed by grafana labs",
    "start": "1362320",
    "end": "1368480"
  },
  {
    "text": "running in agent mode will minimize the disk and memory consumed by prometheus while still having a write-ahead log for",
    "start": "1368480",
    "end": "1375600"
  },
  {
    "text": "reliable sample delivery feel free to follow pull request 8785",
    "start": "1375600",
    "end": "1381440"
  },
  {
    "text": "for updates as this work is completed then beyond the just agent mode and the",
    "start": "1381440",
    "end": "1387760"
  },
  {
    "text": "remote write receiving capabilities we're also working on improvements to the core server to better support",
    "start": "1387760",
    "end": "1394320"
  },
  {
    "text": "resource constraint environments for example there are some settings recently added",
    "start": "1394320",
    "end": "1400080"
  },
  {
    "text": "to prometheus to just lower the amount of disk space that prometheus needs to provision for",
    "start": "1400080",
    "end": "1405120"
  },
  {
    "text": "every block of data and we are welcome to any more any more",
    "start": "1405120",
    "end": "1410559"
  },
  {
    "text": "contributions that will help enable these low resource use cases",
    "start": "1410559",
    "end": "1417120"
  },
  {
    "text": "thanks that's pretty exciting so last but not least let's talk about use case that pops up um",
    "start": "1417120",
    "end": "1425360"
  },
  {
    "text": "here and there again which is multi-tenancy right which is essentially",
    "start": "1425360",
    "end": "1430799"
  },
  {
    "text": "you know what if we can you know reduce the cost of running",
    "start": "1430799",
    "end": "1436240"
  },
  {
    "text": "um from used servers by essentially running one of them for multiple teams that don't necessarily like each other",
    "start": "1436240",
    "end": "1443600"
  },
  {
    "text": "or like don't want to see each other later um how to enable that and",
    "start": "1443600",
    "end": "1450559"
  },
  {
    "text": "this is something that we try to evolve as well and i want to explain how you can",
    "start": "1450559",
    "end": "1455760"
  },
  {
    "start": "1455000",
    "end": "1455000"
  },
  {
    "text": "achieve that right now especially on the kubernetes system so you can you know the problem here",
    "start": "1455760",
    "end": "1463279"
  },
  {
    "text": "as you can imagine let's let's be specific is is that in the cluster you might have",
    "start": "1463279",
    "end": "1468880"
  },
  {
    "text": "you know the instances that you want to make sure they are only you know single or like",
    "start": "1468880",
    "end": "1474000"
  },
  {
    "text": "multiple replicas but kind of single element there and it you want to make this reusable for multiple teams that",
    "start": "1474000",
    "end": "1482080"
  },
  {
    "text": "and you want to kind of isolate their experience so but you know the problem is there are",
    "start": "1482080",
    "end": "1488000"
  },
  {
    "text": "lots of shared resources here in grafana you specif as you specify a set of dashboards you specify a certain",
    "start": "1488000",
    "end": "1494960"
  },
  {
    "text": "configure configuration you don't want multiple teams to access the same set of dashboards because they can impact each",
    "start": "1494960",
    "end": "1501520"
  },
  {
    "text": "other maybe you know create a dashboard the same name and suddenly everything will will uh not work for another team",
    "start": "1501520",
    "end": "1508640"
  },
  {
    "text": "the same with administrator and this is actually very critical you you kind of there is normally a a single",
    "start": "1508640",
    "end": "1516080"
  },
  {
    "text": "place of configuring routing and providers which tells you know from what alerts should be rooted to watch uh",
    "start": "1516080",
    "end": "1523039"
  },
  {
    "text": "notifier like maybe page page or duty or this kind of slack or or whatever you know if you",
    "start": "1523039",
    "end": "1530159"
  },
  {
    "text": "allow configuring this by multiple people that don't know each other and don't know each other kind of goals",
    "start": "1530159",
    "end": "1536640"
  },
  {
    "text": "then well we might have a clash here and similarly to prometheus itself where we",
    "start": "1536640",
    "end": "1541679"
  },
  {
    "text": "specify set of scrape targets so what should be monitored and we for what are alerting rules all of this if it's",
    "start": "1541679",
    "end": "1548559"
  },
  {
    "text": "configured from a single place then you know we might have a serious incidence uh when people are impacting each other",
    "start": "1548559",
    "end": "1555760"
  },
  {
    "text": "from the configuration standpoint so this is what uh you know prometheus operator is aiming to solve",
    "start": "1555760",
    "end": "1562240"
  },
  {
    "text": "and in the community's ecosystem um so you have promised use operator and graffana operator",
    "start": "1562240",
    "end": "1569039"
  },
  {
    "text": "which kind of recently emerged those servers they essentially define a",
    "start": "1569039",
    "end": "1576080"
  },
  {
    "text": "certain apis allowing each team to have their own set of you",
    "start": "1576080",
    "end": "1581520"
  },
  {
    "text": "know rules recording rules alerting rules service monitors which specifies what",
    "start": "1581520",
    "end": "1587200"
  },
  {
    "text": "things should be scraped what targets should be should be specified for promote use um and other manager",
    "start": "1587200",
    "end": "1592960"
  },
  {
    "text": "configurations so routings and and and those providers and dashboards you can have your own set of dashboards",
    "start": "1592960",
    "end": "1599679"
  },
  {
    "text": "and it's only your dashboards you don't need to see you know other teams resources so",
    "start": "1599679",
    "end": "1606400"
  },
  {
    "text": "by providing that through communities apis we have you know by default all the airbag and kind of access controls so we",
    "start": "1606400",
    "end": "1613520"
  },
  {
    "text": "make sure that you know this team can only access namespaces so resources so those dashboards and and configurations",
    "start": "1613520",
    "end": "1620080"
  },
  {
    "text": "that only relate to them and then we can use graphene operator and prompt use operator that will watch kubernetes apis",
    "start": "1620080",
    "end": "1626640"
  },
  {
    "text": "and essentially merge those things together so merge another configuration into one merge service monitors into one",
    "start": "1626640",
    "end": "1633679"
  },
  {
    "text": "script configuration of parameters and and inject that to the single resources like prometus prometheus another manager",
    "start": "1633679",
    "end": "1640159"
  },
  {
    "text": "and grafana and this is what we use in production right now in red hat and and and we really i mean",
    "start": "1640159",
    "end": "1646799"
  },
  {
    "text": "um recommend doing that if you have like multiple multiple teams",
    "start": "1646799",
    "end": "1653039"
  },
  {
    "text": "and you don't want to impact each other and there is also one important point which is at the end uh you know",
    "start": "1653039",
    "end": "1659279"
  },
  {
    "text": "configuration is one thing but second thing is how you access this data while",
    "start": "1659279",
    "end": "1664399"
  },
  {
    "text": "providing some data isolation you can do that with grafana thanks to the you know access control mechanism many of those",
    "start": "1664399",
    "end": "1671200"
  },
  {
    "text": "but promote use doesn't have that but it doesn't mean that you cannot do it because there are already community",
    "start": "1671200",
    "end": "1676640"
  },
  {
    "text": "driven and very well supported uh proxies for example prom label proxy that allows you to essentially inject a",
    "start": "1676640",
    "end": "1684240"
  },
  {
    "text": "certain label into the promql making sure you know your team only sees your",
    "start": "1684240",
    "end": "1689600"
  },
  {
    "text": "own data nothing else and we use that heavily on cortex system tunnel system so all",
    "start": "1689600",
    "end": "1695440"
  },
  {
    "text": "those systems that derive from promoters as well so it's like a very very flexible solution here and you know all",
    "start": "1695440",
    "end": "1701919"
  },
  {
    "text": "those things that we improved on the scraping part that that chris mentioned in the very beginning uh so making sure",
    "start": "1701919",
    "end": "1707919"
  },
  {
    "text": "resource constraints are are uh you know respected and and also uh",
    "start": "1707919",
    "end": "1714480"
  },
  {
    "text": "you know there are scrape limits and all of those um isolation mechanisms this helps as well because you also limit the",
    "start": "1714480",
    "end": "1721840"
  },
  {
    "text": "the possibility of um of you know one team providing you know so many metrics",
    "start": "1721840",
    "end": "1728399"
  },
  {
    "text": "or so broken metric uh metric endpoints that will you know be stopped and be",
    "start": "1728399",
    "end": "1733919"
  },
  {
    "text": "rate limited before it it kills the whole probability of server for example so those approaches are exactly for this",
    "start": "1733919",
    "end": "1739279"
  },
  {
    "text": "purpose for the multiple multi-tenancy cases so you have some links for those",
    "start": "1739279",
    "end": "1744799"
  },
  {
    "text": "operators if you want to check them out and you will you can you can totally um",
    "start": "1744799",
    "end": "1751039"
  },
  {
    "text": "check those out and help building and improving those as well but there are certain things that we",
    "start": "1751039",
    "end": "1757360"
  },
  {
    "start": "1755000",
    "end": "1755000"
  },
  {
    "text": "always you know evolve into and we have two that we are super exciting to excited to",
    "start": "1757360",
    "end": "1764399"
  },
  {
    "text": "announce and the third first thing is this in idea of ingestion scalability",
    "start": "1764399",
    "end": "1769760"
  },
  {
    "text": "automation what it means is that thanks to the agents mode that we kind of start",
    "start": "1769760",
    "end": "1774799"
  },
  {
    "text": "to uh you know implement and provide it means that in a sense ingestion so those",
    "start": "1774799",
    "end": "1780559"
  },
  {
    "text": "the scrape mechanism and service discovery is stateless so you know the data is pulled and immediately forwarded",
    "start": "1780559",
    "end": "1787039"
  },
  {
    "text": "to some uh remote server this allows us to scale them dynamically and using you know",
    "start": "1787039",
    "end": "1793840"
  },
  {
    "text": "horizontal auto scalers and communities or any other solution because suddenly",
    "start": "1793840",
    "end": "1799039"
  },
  {
    "text": "we can dynamically assign scrape targets uh into multiple agents so",
    "start": "1799039",
    "end": "1805440"
  },
  {
    "text": "you if you're probably if your cluster scales from one node to thousand we can dynamically you know uh also",
    "start": "1805440",
    "end": "1812640"
  },
  {
    "text": "scale those ingestion pipelines it's not like a ones promote you server that suddenly you need to have more of those",
    "start": "1812640",
    "end": "1819039"
  },
  {
    "text": "which which are stateful it's kind of harder to achieve so i'm super excited to see those uh mechanism and we we we",
    "start": "1819039",
    "end": "1826960"
  },
  {
    "text": "i hope we can also work with open telemetry to make that to make to improve that part as well",
    "start": "1826960",
    "end": "1832640"
  },
  {
    "text": "yeah we also have high density histograms that has work",
    "start": "1832640",
    "end": "1837760"
  },
  {
    "text": "going on right now to support the new histogram format will provide both higher resolution and higher accuracy of",
    "start": "1837760",
    "end": "1844799"
  },
  {
    "text": "histogram quantile calculations and a significant cost savings in terms of how expensive histograms are to store and",
    "start": "1844799",
    "end": "1851200"
  },
  {
    "text": "just how much data is in your prometheus tsdb",
    "start": "1851200",
    "end": "1857039"
  },
  {
    "text": "i highly recommend checking out just a few days ago at the day zero prom con event my colleagues ganesha dieter gave",
    "start": "1857039",
    "end": "1863760"
  },
  {
    "text": "a talk about the work they've done in this area um please check it out",
    "start": "1863760",
    "end": "1870240"
  },
  {
    "text": "and finally thank you for listening to our talk provided some links to reach out to the",
    "start": "1870240",
    "end": "1875760"
  },
  {
    "text": "overall prometheus community as well of each as well as each of us individually now we have a few minutes to answer any",
    "start": "1875760",
    "end": "1882159"
  },
  {
    "text": "questions that you have thank you thank you",
    "start": "1882159",
    "end": "1888039"
  }
]