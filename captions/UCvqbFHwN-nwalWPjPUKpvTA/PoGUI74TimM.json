[
  {
    "start": "0",
    "end": "113000"
  },
  {
    "text": "good afternoon everybody thanks for being here late on a Friday afternoon",
    "start": "60",
    "end": "5400"
  },
  {
    "text": "after everybody's brains are completely full but we survived the snow so we can",
    "start": "5400",
    "end": "11250"
  },
  {
    "text": "survive anything so I'm here to tell you a story take you",
    "start": "11250",
    "end": "17190"
  },
  {
    "text": "through the journey that credit camera has been on over the last couple of years from monolith to micro-services",
    "start": "17190",
    "end": "24359"
  },
  {
    "text": "like it says I'm Mason Jones staff",
    "start": "24359",
    "end": "29849"
  },
  {
    "text": "engineer heading up our infrastructure Services team we're the team that gets to play with all the fun stuff that",
    "start": "29849",
    "end": "35250"
  },
  {
    "text": "we've really been all hearing about this week docker communities linker D and so forth that I'll be talking a little more",
    "start": "35250",
    "end": "41219"
  },
  {
    "text": "about a little bit about Credit Karma our mission is really to make financial",
    "start": "41219",
    "end": "47969"
  },
  {
    "text": "progress possible for everybody we serve our over 75 million members by helping",
    "start": "47969",
    "end": "54390"
  },
  {
    "text": "them understand their credit score their credit history ways to improve things ways to find better financial",
    "start": "54390",
    "end": "60059"
  },
  {
    "text": "opportunities we're just over 800 employees now half in engineering but",
    "start": "60059",
    "end": "67320"
  },
  {
    "text": "when our story starts two years ago we were half the size 400 employees half in",
    "start": "67320",
    "end": "72720"
  },
  {
    "text": "engineering and we'd already been growing very very rapidly over the last",
    "start": "72720",
    "end": "78060"
  },
  {
    "text": "little while and we were facing a model with I'll take you through these steps",
    "start": "78060",
    "end": "84210"
  },
  {
    "text": "during the talk and kind of tell you about the various phases that we went",
    "start": "84210",
    "end": "89610"
  },
  {
    "text": "through but I'm actually curious first to kind of start off how many of you have a monolith that you're either",
    "start": "89610",
    "end": "96479"
  },
  {
    "text": "preparing to get out of or are in the process of trying to get out of yep it's",
    "start": "96479",
    "end": "102509"
  },
  {
    "text": "a really really common story these days how many of you have already done that and you're just here to gloat cool",
    "start": "102509",
    "end": "111090"
  },
  {
    "text": "congratulations our reason for moving to",
    "start": "111090",
    "end": "117460"
  },
  {
    "start": "113000",
    "end": "161000"
  },
  {
    "text": "microservices from the monolith there are lots of different possible reasons that you all may have for us it was",
    "start": "117460",
    "end": "125020"
  },
  {
    "text": "really to try to get teams independent we had learned as probably a lot of you have learned that engineering growth",
    "start": "125020",
    "end": "131200"
  },
  {
    "text": "paired with a monolith leads to great sadness and we wanted to get teams to",
    "start": "131200",
    "end": "136510"
  },
  {
    "text": "where they could really be independent do their thing move at their own speed not deal with the monolith deployment",
    "start": "136510",
    "end": "144190"
  },
  {
    "text": "cycle and the dangers that come from making changes to a monolith and we wanted teams to be able to experiment as",
    "start": "144190",
    "end": "151120"
  },
  {
    "text": "well it's it's much much harder to just try something out in a monolith because the side effects are unknown right so",
    "start": "151120",
    "end": "157960"
  },
  {
    "text": "that was really what we were after here and when we made the decision",
    "start": "157960",
    "end": "164400"
  },
  {
    "start": "161000",
    "end": "311000"
  },
  {
    "text": "micro-services is gonna be our future we tried to figure out how are we gonna get there",
    "start": "164400",
    "end": "169960"
  },
  {
    "text": "where do we start on this and we wanted to begin really with some baby steps we",
    "start": "169960",
    "end": "175000"
  },
  {
    "text": "had a few people in the company who had experience with doctor mainly in development environments and kind of playing with things not a lot of",
    "start": "175000",
    "end": "180700"
  },
  {
    "text": "experience running doctor in production so we wanted to play with that first feeler I had to figure out what was",
    "start": "180700",
    "end": "186370"
  },
  {
    "text": "going to bite us right so rather than doing the tempting thing which is to",
    "start": "186370",
    "end": "192340"
  },
  {
    "text": "identify the piece of our model if that's causing us the most pain that we really want to get out of there that we really want to to free from you know",
    "start": "192340",
    "end": "198610"
  },
  {
    "text": "from the the monolith itself and we chose a couple of really relatively",
    "start": "198610",
    "end": "205120"
  },
  {
    "text": "unimportant services don't tell the teams that own them and I said that but we really wanted to play it safe we knew",
    "start": "205120",
    "end": "212470"
  },
  {
    "text": "that we were gonna have a lot to learn we selected those and we then wanted to",
    "start": "212470",
    "end": "219160"
  },
  {
    "text": "use the tools that we already had in hand to get things started we run to",
    "start": "219160",
    "end": "224770"
  },
  {
    "text": "make this more fun on bare metal in our data centers we use salt to provision",
    "start": "224770",
    "end": "230080"
  },
  {
    "text": "our servers and we run CentOS so supervisor kind of comes with the package so we thought all right let's",
    "start": "230080",
    "end": "237100"
  },
  {
    "text": "see what we can do with those we're not gonna try to bite off more than we can chew we're not gonna grab an",
    "start": "237100",
    "end": "242970"
  },
  {
    "text": "orchestration system and spend six months trying to figure it out before we make any progress we really want to just start we want to get this going so this",
    "start": "242970",
    "end": "251760"
  },
  {
    "text": "seems kind of weird I'm sure but we said let's set up deployment of a service by",
    "start": "251760",
    "end": "257820"
  },
  {
    "text": "using salt to provision files onto the servers where we want to run it those files are supervisor configs telling it",
    "start": "257820",
    "end": "264840"
  },
  {
    "text": "do a docker run we provision those files we kick supervisors and it does a docker",
    "start": "264840",
    "end": "272430"
  },
  {
    "text": "run for us if that container dies supervisors are gonna restart it that's its job it does that pretty well now",
    "start": "272430",
    "end": "280040"
  },
  {
    "text": "provisioning files with salt and telling supervisor to do a docker run sounds really horrible and it kind of is but it",
    "start": "280040",
    "end": "286950"
  },
  {
    "text": "got the job done we were able to get some containers running get docker",
    "start": "286950",
    "end": "292590"
  },
  {
    "text": "installed get some experience with that try to upgrade docker fail horribly get",
    "start": "292590",
    "end": "298980"
  },
  {
    "text": "some experience with that we just we learned the hard way that upgrading",
    "start": "298980",
    "end": "305550"
  },
  {
    "text": "docker is not always as straightforward as you might expect but this did work for us so at the beginning of this year",
    "start": "305550",
    "end": "314220"
  },
  {
    "start": "311000",
    "end": "421000"
  },
  {
    "text": "then we've actually gotten to about 15 services or so running across 20 servers",
    "start": "314220",
    "end": "319290"
  },
  {
    "text": "in our data center using this mechanism we built some deployment tools that",
    "start": "319290",
    "end": "325440"
  },
  {
    "text": "would interact with salt use some weird ansible stuff as well get some files out there kick supervisor and allow teams to",
    "start": "325440",
    "end": "334110"
  },
  {
    "text": "deploy to update their images to restart the containers it's working so now we've",
    "start": "334110",
    "end": "341430"
  },
  {
    "text": "got some pretty good experience running the stuff in production we've figured out some logging and some monitoring and",
    "start": "341430",
    "end": "346890"
  },
  {
    "text": "and some of those important things and the teams that are building services are actually pretty happy with this they",
    "start": "346890",
    "end": "352050"
  },
  {
    "text": "don't see the kind of ugliness underneath and too much we do and we",
    "start": "352050",
    "end": "357060"
  },
  {
    "text": "feel it and we see a lot more services coming down the line and we knew that this is not a scalable thing for our",
    "start": "357060",
    "end": "363090"
  },
  {
    "text": "team to continue managing it's really obviously super inflexible architecture we're hard-coding where",
    "start": "363090",
    "end": "369400"
  },
  {
    "text": "these service instances running what servers have which ones we're human",
    "start": "369400",
    "end": "374410"
  },
  {
    "text": "orchestrators honest to god using a google spreadsheet to track here our servers here our services and here what",
    "start": "374410",
    "end": "381280"
  },
  {
    "text": "ports they're on it works but it's not great right so how are we gonna get past",
    "start": "381280",
    "end": "387669"
  },
  {
    "text": "this the other thing that was actually biting us that was causing more heartache to more people was that teams",
    "start": "387669",
    "end": "395169"
  },
  {
    "text": "would develop their service on their laptop they'd get it running they'd go to testing they do their integration",
    "start": "395169",
    "end": "401259"
  },
  {
    "text": "testing it would be working they would deploy a production and it wouldn't work the plumbing the moving parts the",
    "start": "401259",
    "end": "407470"
  },
  {
    "text": "configurations were really very different in those environments and that was not making anybody happy so we",
    "start": "407470",
    "end": "413470"
  },
  {
    "text": "wanted a way to give the same experience across all of our environments but again",
    "start": "413470",
    "end": "418539"
  },
  {
    "text": "baby steps what's the next thing that we needed to do we still got a monolith obviously but",
    "start": "418539",
    "end": "423580"
  },
  {
    "start": "421000",
    "end": "487000"
  },
  {
    "text": "we've got some services going now so that's pretty good the next thing to start to make things a little bit more",
    "start": "423580",
    "end": "430060"
  },
  {
    "text": "flexible a little bit more automatable a little bit more looking into the future",
    "start": "430060",
    "end": "436030"
  },
  {
    "text": "was service discovery so we're running console already in our infrastructure",
    "start": "436030",
    "end": "442840"
  },
  {
    "text": "for other reasons and it makes a terrific service discovery tour right so we modified our deployment tool such",
    "start": "442840",
    "end": "450490"
  },
  {
    "text": "that after getting the service onto the server and getting a config set up it",
    "start": "450490",
    "end": "456250"
  },
  {
    "text": "would tell console about it so now we've got something we can query tell us what's running where that's quite an",
    "start": "456250",
    "end": "462370"
  },
  {
    "text": "improvement and we've got some decent health checks because prior to this the supervisor would alert us if something",
    "start": "462370",
    "end": "468639"
  },
  {
    "text": "died and it restarted it but if the service stops responding but the process is still there pretty invisible to us we",
    "start": "468639",
    "end": "474370"
  },
  {
    "text": "would often not really know about it until someone reported that traffic was just not flowing properly so this is an",
    "start": "474370",
    "end": "480759"
  },
  {
    "text": "improvement but it's incremental right we're not any closer to a dynamic infrastructure so still got our monolith",
    "start": "480759",
    "end": "489789"
  },
  {
    "start": "487000",
    "end": "754000"
  },
  {
    "text": "got some services running we're registering them now we know where they're running we know if they're healthy that's pretty good so what can",
    "start": "489789",
    "end": "496449"
  },
  {
    "text": "we do next we know that we're moving towards orchestration at some point you know",
    "start": "496449",
    "end": "501740"
  },
  {
    "text": "we're moving towards it because there's a box that says so we wanted to leverage the service discovery now to actually",
    "start": "501740",
    "end": "508370"
  },
  {
    "text": "send requests intelligently based on it so dynamic routing is next this is what",
    "start": "508370",
    "end": "515209"
  },
  {
    "text": "our routing looked like at this point if the monolith wanted to make us call to a",
    "start": "515209",
    "end": "520339"
  },
  {
    "text": "service which is our most common case early on it would make the request to an",
    "start": "520339",
    "end": "525740"
  },
  {
    "text": "engine X proxy running on its server that nginx proxy would forward it to a",
    "start": "525740",
    "end": "530990"
  },
  {
    "text": "VIP load balancer hardware load balancer in our data center which has been manually configured to know where",
    "start": "530990",
    "end": "536420"
  },
  {
    "text": "instances of the service are not great the VIP would then send it to one of the",
    "start": "536420",
    "end": "543680"
  },
  {
    "text": "servers to an engine X proxy on that end the job of that nginx proxy makes a little more sense which was to allow",
    "start": "543680",
    "end": "549649"
  },
  {
    "text": "Bluegreen deployments our our team's wanted to be able to have traffic going",
    "start": "549649",
    "end": "556279"
  },
  {
    "text": "to their blue instance deployed to green and then ramp 1% of traffic to it and",
    "start": "556279",
    "end": "561620"
  },
  {
    "text": "get a sense of whether things were working because keep in mind we're trying to roll this out we've got these services going while we've got 75",
    "start": "561620",
    "end": "568040"
  },
  {
    "text": "million members using our site and we're starting to move some you know actually important functionality into these",
    "start": "568040",
    "end": "573260"
  },
  {
    "text": "services so one percent of traffic is enough to watch the logs get a sense of whether things are healthy or not we did",
    "start": "573260",
    "end": "580220"
  },
  {
    "text": "this by having our job in our deployment tool update the nginx config send a",
    "start": "580220",
    "end": "586730"
  },
  {
    "text": "signal to nginx to reload the config and then we could actually adjust the weighting on-the-fly and safely so",
    "start": "586730",
    "end": "594470"
  },
  {
    "text": "you're probably though looking at this and wondering why why is there an engine hex on the other side and the reason for",
    "start": "594470",
    "end": "600110"
  },
  {
    "text": "that is those green arrows represent traffic going between hosts in our data center the in Credit Karma security is",
    "start": "600110",
    "end": "607220"
  },
  {
    "text": "the first thing we think about at all times and one of our requirements is that all traffic between hosts must be",
    "start": "607220",
    "end": "612560"
  },
  {
    "text": "encrypted must be TLS in turn X is there to ensure that security and for",
    "start": "612560",
    "end": "619190"
  },
  {
    "text": "performance reasons I don't think I mentioned earlier that to make things more fun our monolith is not just any",
    "start": "619190",
    "end": "624230"
  },
  {
    "text": "monolith it's now a seven year old PHP model with PHP does not do a nice job of",
    "start": "624230",
    "end": "630740"
  },
  {
    "text": "hole connections open with the result that every service request would have to open",
    "start": "630740",
    "end": "636180"
  },
  {
    "text": "a TLS connection send the request wait for the response to tear it down and do that over and over again the latency was",
    "start": "636180",
    "end": "641850"
  },
  {
    "text": "not acceptable it was not working for us so we put an engine X proxy on that side",
    "start": "641850",
    "end": "647310"
  },
  {
    "text": "to maintain the TLS connection to the load balancers and that actually got the",
    "start": "647310",
    "end": "652350"
  },
  {
    "text": "job done but it was another piece of essentially hard-coded infrastructure that we were putting on our servers so",
    "start": "652350",
    "end": "660000"
  },
  {
    "text": "how do we get past those how can we make use of console now to actually do better and more flexible and actually",
    "start": "660000",
    "end": "666960"
  },
  {
    "text": "automatable eventually routing and linker D ended the picture at that point",
    "start": "666960",
    "end": "673040"
  },
  {
    "text": "this is probably the only audience that I don't have to explain the service master to because everybody's been",
    "start": "673040",
    "end": "678420"
  },
  {
    "text": "hearing about it for the last several days I think but in our case linker D represented a service national deployed",
    "start": "678420",
    "end": "685830"
  },
  {
    "text": "unto all of our servers talking to consul so it knows where things are this",
    "start": "685830",
    "end": "691050"
  },
  {
    "text": "removed the need for the manual zip load balancer configuration remove the need for that engine X because linker D",
    "start": "691050",
    "end": "697110"
  },
  {
    "text": "actually does TLS connections between all the hosts for us and it got us ready for actual orchestration because now",
    "start": "697110",
    "end": "704570"
  },
  {
    "text": "when we deploy a container somewhere it will register with service discovery and",
    "start": "704570",
    "end": "710310"
  },
  {
    "text": "something will make use of that to actually make sure requests reach it so at this point routing looks like this",
    "start": "710310",
    "end": "716460"
  },
  {
    "text": "instead which is a whole lot cleaner and a whole lot smarter the monolith sends a request to linker D",
    "start": "716460",
    "end": "722400"
  },
  {
    "text": "on its hosts I think he looks at that request and says you want service foo",
    "start": "722400",
    "end": "727500"
  },
  {
    "text": "great I know where those are I'll pick one I will send your request to a linker D on its host and that will then at its",
    "start": "727500",
    "end": "736260"
  },
  {
    "text": "end send to either blue or green and we modified our deployment job to do the",
    "start": "736260",
    "end": "741270"
  },
  {
    "text": "waiting between blue and green to make use of linker DS name or D component we",
    "start": "741270",
    "end": "746640"
  },
  {
    "text": "can use the API rather than actually touching configs and telling it to reload which is also much much nicer",
    "start": "746640",
    "end": "754310"
  },
  {
    "start": "754000",
    "end": "1158000"
  },
  {
    "text": "so we're making some progress here we've still got our monolith but we've got a bunch of services we've got service",
    "start": "755100",
    "end": "760320"
  },
  {
    "text": "discovery in place we've got dynamic routing making use of that so we can start to look at actual orchestration",
    "start": "760320",
    "end": "766020"
  },
  {
    "text": "now we had actually started to explore orchestration late last year late 2016",
    "start": "766020",
    "end": "774110"
  },
  {
    "text": "just to kind of get an idea where we were going to be headed in the future in case it might influence some of our",
    "start": "774110",
    "end": "780030"
  },
  {
    "text": "decisions along the way at that time the landscape was fairly different than it",
    "start": "780030",
    "end": "785370"
  },
  {
    "text": "is now we were looking at mezzos we had people with experienced resumes OHS we",
    "start": "785370",
    "end": "792120"
  },
  {
    "text": "looked at it and felt the marathon and Aurora for continued orchestration were really kind of not keeping up they they",
    "start": "792120",
    "end": "799230"
  },
  {
    "text": "weren't really evolving very much it didn't look like we could count on them continuing to move forward the way that",
    "start": "799230",
    "end": "805080"
  },
  {
    "text": "we wanted we looked at nomads from Hershey Corp which was really actually appealing very simple very small easy to",
    "start": "805080",
    "end": "811380"
  },
  {
    "text": "manage small sort of functionality but probably enough for us at the time but",
    "start": "811380",
    "end": "817080"
  },
  {
    "text": "it was very new and we couldn't really find good stories of people running it at scale so we weren't really confident",
    "start": "817080",
    "end": "825210"
  },
  {
    "text": "enough at the time we looked at docker swarm oddly enough kind of the same story not a lot of production usage at",
    "start": "825210",
    "end": "832140"
  },
  {
    "text": "scale and we were also a little leery of putting all of our eggs in one basket you know rocket containers look pretty",
    "start": "832140",
    "end": "837540"
  },
  {
    "text": "interesting too we weren't sure where things might be going so kubernetes at",
    "start": "837540",
    "end": "842700"
  },
  {
    "text": "the time was also kind of an open question it was moving super fast but where was it going it was kind of hard",
    "start": "842700",
    "end": "848820"
  },
  {
    "text": "to keep up with we weren't quite sure fast forward just six months only six",
    "start": "848820",
    "end": "854010"
  },
  {
    "text": "months early this spring mid-spring the landscape had changed and thankfully for",
    "start": "854010",
    "end": "861420"
  },
  {
    "text": "us actually the decision was super obvious and since we're all here we know what the decision was right which was",
    "start": "861420",
    "end": "868320"
  },
  {
    "text": "kubernetes it was a very easy decision at that point for us the the main deciders were number one the community",
    "start": "868320",
    "end": "874680"
  },
  {
    "text": "which as we all know from being here is ridiculously active and excited you know moving things forward which is",
    "start": "874680",
    "end": "881100"
  },
  {
    "text": "awesome the abstractions that kubernetes provides were perfect for us everything",
    "start": "881100",
    "end": "888000"
  },
  {
    "text": "from pods and demon sets and deployments and services all mapped beautifully to what we wanted to do and what we were",
    "start": "888000",
    "end": "893190"
  },
  {
    "text": "kind of trying to do on our own in a half-baked way and finally by the spring",
    "start": "893190",
    "end": "899400"
  },
  {
    "text": "it was really pretty easy to talk to people and find Carreras installations running at scale so we felt pretty",
    "start": "899400",
    "end": "905040"
  },
  {
    "text": "confident that it was gonna be able to move forward and grow with us the way we expected but having decided on",
    "start": "905040",
    "end": "911610"
  },
  {
    "text": "kubernetes we had to figure out how to install this thing and this was interesting because you run mini Cube",
    "start": "911610",
    "end": "917160"
  },
  {
    "text": "you're like hey this is awesome gonna go to production huh how do I do that and",
    "start": "917160",
    "end": "922740"
  },
  {
    "text": "to make it more fun we run on bare metal in our data centers but we also do run workloads in multiple clouds so we",
    "start": "922740",
    "end": "928830"
  },
  {
    "text": "needed something that would run pretty much everywhere so that ruled out cops no bare metal support there cube ATM is",
    "start": "928830",
    "end": "936000"
  },
  {
    "text": "coming along now but at the time it really wasn't ready wouldn't give us a full H Aiken config yet CD cluster all",
    "start": "936000",
    "end": "943140"
  },
  {
    "text": "the things that we needed to really be production ready tectonic was pretty cool but we can't run core OSN or data",
    "start": "943140",
    "end": "948360"
  },
  {
    "text": "center for various esoteric reasons we looked through dozens we created a",
    "start": "948360",
    "end": "954270"
  },
  {
    "text": "spreadsheet of installers and trying to figure out the feature set and what would do what it was kind of crazy but",
    "start": "954270",
    "end": "959760"
  },
  {
    "text": "we ended up choosing cos Matic which is was at the time just adopted by a prenda",
    "start": "959760",
    "end": "965550"
  },
  {
    "text": "a smaller active community but some support good documentation and most importantly it did what we needed it to",
    "start": "965550",
    "end": "971310"
  },
  {
    "text": "do we would run on bare metal it would run in the cloud it was pretty flexible for configuration we could actually set",
    "start": "971310",
    "end": "976350"
  },
  {
    "text": "up the same tool set so that we could stand up our bare metal cluster with a",
    "start": "976350",
    "end": "982140"
  },
  {
    "text": "full aj config but then stand up a mini one in the cloud really easily for testing using the same tools which was",
    "start": "982140",
    "end": "988410"
  },
  {
    "text": "awfully nice so we've got a cluster in production now and we're ready to start",
    "start": "988410",
    "end": "995580"
  },
  {
    "text": "using it and how do we do that because we're not going to tell the teams yeah can you stop developing for a couple of",
    "start": "995580",
    "end": "1001580"
  },
  {
    "text": "weeks while we pick up your services and move them over into communities and change all the tooling - so it's talking - proving any",
    "start": "1001580",
    "end": "1007210"
  },
  {
    "text": "and all this that was not going to work so we were really back to our initial baby steps pick a service that's not in",
    "start": "1007210",
    "end": "1014560"
  },
  {
    "text": "the critical path work with the team to move it over talk to them and say can",
    "start": "1014560",
    "end": "1020290"
  },
  {
    "text": "you work with us to move your service over and that was actually really really the easy part we talked to a bunch of",
    "start": "1020290",
    "end": "1025449"
  },
  {
    "text": "teams to say hey could we work with you to make me move your service as the first one into kubernetes and they were all like pick me pick me because I could",
    "start": "1025450",
    "end": "1032230"
  },
  {
    "text": "see that you know it was gonna make life so much better so that was really nice",
    "start": "1032230",
    "end": "1037740"
  },
  {
    "text": "the main question that we had to solve though was as removing some of our services into communities and we're",
    "start": "1037740",
    "end": "1043600"
  },
  {
    "text": "continuing to run our existing cluster how do we manage the routing we've got services calling each other in and out",
    "start": "1043600",
    "end": "1049810"
  },
  {
    "text": "and your service mesh is great if it's gonna run everywhere but is it going to",
    "start": "1049810",
    "end": "1054850"
  },
  {
    "text": "run everywhere and we lucked out in a big way here I'd love to say that we planned this ahead and we knew it was",
    "start": "1054850",
    "end": "1061270"
  },
  {
    "text": "coming but no we didn't we actually lucked out because link Reedy happened to have the capability to do this once",
    "start": "1061270",
    "end": "1068410"
  },
  {
    "text": "we started digging into things linker DS name or D component is what talks to",
    "start": "1068410",
    "end": "1075130"
  },
  {
    "text": "service discovery and linker D has the capability to talk to multiple neighbor DS so what we ended up with was a",
    "start": "1075130",
    "end": "1082120"
  },
  {
    "text": "cluster that looked like this where our docker cluster has an Emer D talking to console our kubernetes cluster has a",
    "start": "1082120",
    "end": "1089380"
  },
  {
    "text": "name or D talking to kubernetes the linker DS everywhere talked to both named or DS so they know about the Union",
    "start": "1089380",
    "end": "1095980"
  },
  {
    "text": "set of all of the services they know where they all are so if a service in",
    "start": "1095980",
    "end": "1102550"
  },
  {
    "text": "our docker cluster wants to make a call to one in the communities cluster it sends the request to its local linker D",
    "start": "1102550",
    "end": "1108250"
  },
  {
    "text": "that linker D looks at and says oh yeah you're looking for that service I know that they're a bunch of those over there",
    "start": "1108250",
    "end": "1113290"
  },
  {
    "text": "they happened to be in kubernetes I'll pick one I will send the request to the linker D on its worker which is",
    "start": "1113290",
    "end": "1120040"
  },
  {
    "text": "interesting because the linker D needs to be then a demon said node poor it needs to be kind of known and",
    "start": "1120040",
    "end": "1125500"
  },
  {
    "text": "addressable from outside because it's not just going straight to the pod obviously so the request gets to that",
    "start": "1125500",
    "end": "1130930"
  },
  {
    "text": "link or D that linker D does its thing sends to the blue or the green instance of the service just like before go in",
    "start": "1130930",
    "end": "1137830"
  },
  {
    "text": "the other direction essentially the same thing right the community service a request to its linker d-link or do you",
    "start": "1137830",
    "end": "1143140"
  },
  {
    "text": "look sad says you're looking for that service over there that's that's in that crusty old docker cluster but I'll get",
    "start": "1143140",
    "end": "1148419"
  },
  {
    "text": "your request there picks one sends requests a linker D on that server and",
    "start": "1148419",
    "end": "1153429"
  },
  {
    "text": "it gets to the service and this works at",
    "start": "1153429",
    "end": "1160179"
  },
  {
    "start": "1158000",
    "end": "1450000"
  },
  {
    "text": "this point now we're done right not really but we're really making progress",
    "start": "1160179",
    "end": "1167169"
  },
  {
    "text": "we've got orchestration we're in the process of migrating and we can see that",
    "start": "1167169",
    "end": "1173140"
  },
  {
    "text": "we're moving into the 21st century finally as far as running services there's one other element of this I",
    "start": "1173140",
    "end": "1179770"
  },
  {
    "text": "wanted to mention just because it's kind of an interesting and important one which is configuration of services",
    "start": "1179770",
    "end": "1184960"
  },
  {
    "text": "themselves in our old docker cluster what we did was leverage our old friend salt again the team has a config file",
    "start": "1184960",
    "end": "1192100"
  },
  {
    "text": "for their service it sync it when they deploy salt pulls that config file out",
    "start": "1192100",
    "end": "1197830"
  },
  {
    "text": "of get provisions it under the server that's part of the supervisor docker run command it mounts that config file into",
    "start": "1197830",
    "end": "1204250"
  },
  {
    "text": "the container and it's got its config that's pretty horrible so we took",
    "start": "1204250",
    "end": "1209470"
  },
  {
    "text": "advantage of the move to kubernetes to actually move things into a better state",
    "start": "1209470",
    "end": "1214900"
  },
  {
    "text": "where configuration data for services is now in console we've already got console",
    "start": "1214900",
    "end": "1220510"
  },
  {
    "text": "out there and we know how to run it and the secrets database passwords API tokens and so forth or in hashing core",
    "start": "1220510",
    "end": "1226059"
  },
  {
    "text": "fault now so when services start up they pull a config some console to pull the secrets from hash you've got vault and there",
    "start": "1226059",
    "end": "1232240"
  },
  {
    "text": "they go getting services that are started in communities the token that they need to",
    "start": "1232240",
    "end": "1240100"
  },
  {
    "text": "talk to vault is an interesting thing that's getting better with the latest versions of both vault and kubernetes but it does work so in the process of",
    "start": "1240100",
    "end": "1249700"
  },
  {
    "text": "doing all of this we obviously learned a ton and kind of some of the high points were number one we lucked out in our",
    "start": "1249700",
    "end": "1257500"
  },
  {
    "text": "decision to start small and simple for sure if we had gone into this taking the",
    "start": "1257500",
    "end": "1262690"
  },
  {
    "text": "most important piece of our monolith and trying to chip it off and setting up an orchestration framework and trying to",
    "start": "1262690",
    "end": "1268120"
  },
  {
    "text": "get that working it would have been six months before we made any progress before we learned anything and we",
    "start": "1268120",
    "end": "1273130"
  },
  {
    "text": "actually would have made the wrong decisions along the way without question secondly we really underestimated the",
    "start": "1273130",
    "end": "1280900"
  },
  {
    "text": "effort to integrate our existing toolset with kubernetes and for that matter liquidy and all of these other pieces as",
    "start": "1280900",
    "end": "1286750"
  },
  {
    "text": "well in kind of moving along all of these steps we were kind of uprooting a",
    "start": "1286750",
    "end": "1294280"
  },
  {
    "text": "bit of the development experience at each step along the way and trying to",
    "start": "1294280",
    "end": "1300430"
  },
  {
    "text": "modify deployment tooling as we go from weird salt and supervisor stuff into",
    "start": "1300430",
    "end": "1306280"
  },
  {
    "text": "kubernetes is a big thing and that impacts all of the environments from development to testing to production",
    "start": "1306280",
    "end": "1311980"
  },
  {
    "text": "along the way one of the things that I mentioned earlier is that we were trying to get to a state where the environments",
    "start": "1311980",
    "end": "1319060"
  },
  {
    "text": "were really the same the experience was the same running your service regardless of what environment you're in and naturally kubernetes being",
    "start": "1319060",
    "end": "1327100"
  },
  {
    "text": "configuration based being declarative made that much much easier and we actually had a slide but had to take it",
    "start": "1327100",
    "end": "1333340"
  },
  {
    "text": "out for time we lever to helm for this which is terrific helm let us just let have the developers provide a tiny",
    "start": "1333340",
    "end": "1340780"
  },
  {
    "text": "little config file we layer on all the environmental specific stuff depending on where it's being deployed to it just works and they",
    "start": "1340780",
    "end": "1347080"
  },
  {
    "text": "don't need to worry about all those nasty details so that was a big win but it took much much longer than we",
    "start": "1347080",
    "end": "1352330"
  },
  {
    "text": "expected to make all of the tooling work with all of this stuff I already kind of",
    "start": "1352330",
    "end": "1357880"
  },
  {
    "text": "mentioned our surprise at finding that we had dozens of installers to choose from and we had to figure out which one",
    "start": "1357880",
    "end": "1363790"
  },
  {
    "text": "was was gonna do the job and it feels to me like this is an opportunity for the community to really just rally behind",
    "start": "1363790",
    "end": "1369370"
  },
  {
    "text": "one make it you know the Installer to rule them all and just help with making",
    "start": "1369370",
    "end": "1374500"
  },
  {
    "text": "the onboarding of people starting out with communities much much easier I think that would be a great thing and",
    "start": "1374500",
    "end": "1381210"
  },
  {
    "text": "lastly but but not lastly is security security touched all of these things as",
    "start": "1381210",
    "end": "1387070"
  },
  {
    "text": "we were going along and as you're starting to move into distributed services and you're starting to look at",
    "start": "1387070",
    "end": "1393940"
  },
  {
    "text": "things like service meshes and all of these things security gets very interesting with linker D and the way",
    "start": "1393940",
    "end": "1401980"
  },
  {
    "text": "that we set up the service mesh with one instance purse server basically means that every server",
    "start": "1401980",
    "end": "1408490"
  },
  {
    "text": "can talk to every other server and you just have to kind of be okay with that if you're gonna be okay with that",
    "start": "1408490",
    "end": "1413910"
  },
  {
    "text": "alternately with you know service meshes you can obviously run one per pod in",
    "start": "1413910",
    "end": "1419230"
  },
  {
    "text": "communities wouldn't help us outside communities but it it would give us the the better path which is really where",
    "start": "1419230",
    "end": "1424330"
  },
  {
    "text": "we're gonna be moving to where you can lock down access between pods somewhat easily",
    "start": "1424330",
    "end": "1429340"
  },
  {
    "text": "and lastly running kubernetes involves like a dozen docker images",
    "start": "1429340",
    "end": "1436570"
  },
  {
    "text": "right for us we had to pull all of those down put them in our private registry scan them get them approved make sure",
    "start": "1436570",
    "end": "1442420"
  },
  {
    "text": "that they're cool with security and yeah move on from there which makes upgrades",
    "start": "1442420",
    "end": "1447910"
  },
  {
    "text": "tons of fun so we're still moving through this we've still got our",
    "start": "1447910",
    "end": "1454960"
  },
  {
    "start": "1450000",
    "end": "1950000"
  },
  {
    "text": "monolith we've accepted that we're gonna remodel it for a while but it's getting smaller rather than bigger we've got",
    "start": "1454960",
    "end": "1460330"
  },
  {
    "text": "more and more services going by the end of this year we will sorry by the end of 2018 we'll probably have a couple of",
    "start": "1460330",
    "end": "1466390"
  },
  {
    "text": "hundred services the way it's going but we've slowly moved our way through tooling understanding along the way what",
    "start": "1466390",
    "end": "1473470"
  },
  {
    "text": "we're trying to get out of it and we feel pretty good about where we're going and tools like kubernetes than greedy",
    "start": "1473470",
    "end": "1479560"
  },
  {
    "text": "and so forth have made it much much easier so hopefully some of what we've gone through some of our thoughts and",
    "start": "1479560",
    "end": "1485290"
  },
  {
    "text": "decisions may be useful for folks who are tackling this now and thank you very much",
    "start": "1485290",
    "end": "1491440"
  },
  {
    "text": "[Applause] it looks like I have a little bit of",
    "start": "1491440",
    "end": "1497710"
  },
  {
    "text": "time for questions if anybody has any",
    "start": "1497710",
    "end": "1501299"
  },
  {
    "text": "yeah the question is about how we're using helm and we have standardized on",
    "start": "1514049",
    "end": "1520929"
  },
  {
    "text": "all of this so that every service runs the same way and the developers don't",
    "start": "1520929",
    "end": "1526360"
  },
  {
    "text": "have to think about deployments services or any of those objects really all they know is that they've got a service",
    "start": "1526360",
    "end": "1532299"
  },
  {
    "text": "it's called this they need X number of instances of it running and for the most",
    "start": "1532299",
    "end": "1539049"
  },
  {
    "text": "part that's all they have to change we allow overrides for we run services in both Scala and node at this point so we",
    "start": "1539049",
    "end": "1545500"
  },
  {
    "text": "do allow overrides for things like max keeps eyes for Scala services just because the JVM is annoying so they can",
    "start": "1545500",
    "end": "1553330"
  },
  {
    "text": "put in some of those but then it's a very small file that they have to think about and then we layer on everything else at deployment time based on where",
    "start": "1553330",
    "end": "1561039"
  },
  {
    "text": "they're deploying to if it's going to to testing then you know one instance of it's going to production another and we",
    "start": "1561039",
    "end": "1566770"
  },
  {
    "text": "also have multiple data centers that we deploy to in parallel so we kind of take care of abstracting that so they don't",
    "start": "1566770",
    "end": "1572440"
  },
  {
    "text": "really have to think about it",
    "start": "1572440",
    "end": "1575250"
  },
  {
    "text": "so the question is about performance degradation using linker D we didn't actually now that said when we first",
    "start": "1582049",
    "end": "1590580"
  },
  {
    "text": "adopted it we were pretty primitive in our metrics so it was a little bit of best guess and",
    "start": "1590580",
    "end": "1596130"
  },
  {
    "text": "like yeah this feels pretty good but we've done you know measurements since then and it's it's really pretty minimal",
    "start": "1596130",
    "end": "1604110"
  },
  {
    "text": "yeah very acceptable for us no no",
    "start": "1604110",
    "end": "1613590"
  },
  {
    "text": "Hardware load balancers anymore which is really really nice because they actually do have api's but nobody quite knows how",
    "start": "1613590",
    "end": "1620309"
  },
  {
    "text": "to use them anything else so yeah",
    "start": "1620309",
    "end": "1628279"
  },
  {
    "text": "yeah the so the question is whether we started well whether these services were pieces of the monolith or new",
    "start": "1635470",
    "end": "1641420"
  },
  {
    "text": "functionality we purposely started with new functionality actually because we knew if we were trying to take existing",
    "start": "1641420",
    "end": "1647750"
  },
  {
    "text": "code and like put it in this new thing it was gonna be more difficult and ironically also we had as a company",
    "start": "1647750",
    "end": "1655610"
  },
  {
    "text": "started to adopt Scala a bit and we were building frameworks for these services",
    "start": "1655610",
    "end": "1661760"
  },
  {
    "text": "and we could have asked all of the developers you know we're thinking about just doing the framework for Scala and",
    "start": "1661760",
    "end": "1669320"
  },
  {
    "text": "abandoning a PHP how do you feel about that and maybe not surprisingly everybody said yeah we don't want to do",
    "start": "1669320",
    "end": "1675440"
  },
  {
    "text": "PHP anymore scholars great so it was all new stuff all all being written in Scala and as we've been carving pieces out of",
    "start": "1675440",
    "end": "1682040"
  },
  {
    "text": "the monolith and replacing them we've actually been wholesale rewriting them into Scala so we really have no PHP",
    "start": "1682040",
    "end": "1688070"
  },
  {
    "text": "services or anything in production yeah",
    "start": "1688070",
    "end": "1691360"
  },
  {
    "text": "yeah so the the culture question yeah I was surprised actually that it was",
    "start": "1698210",
    "end": "1704730"
  },
  {
    "text": "fairly easy we we have absolutely gone for the you build it you own it",
    "start": "1704730",
    "end": "1711230"
  },
  {
    "text": "mentality and there's been very little resistance to that because the the",
    "start": "1711230",
    "end": "1717360"
  },
  {
    "text": "trade-offs of not having to deal with the monolith are well worth it in everybody's minds we've obviously had to",
    "start": "1717360",
    "end": "1724710"
  },
  {
    "text": "train people on understanding pager duty and responding and all of this but the the harder thing that enables the",
    "start": "1724710",
    "end": "1733350"
  },
  {
    "text": "culture has been providing the tools that they need to be able to manage their services properly again going back",
    "start": "1733350",
    "end": "1740009"
  },
  {
    "text": "to security almost nobody our company has access to the production servers there is no association to production",
    "start": "1740009",
    "end": "1746340"
  },
  {
    "text": "ever for developers so we really had to approach this from the standpoint of",
    "start": "1746340",
    "end": "1751350"
  },
  {
    "text": "making sure they have their logs they have their metrics they have dashboards they have you know the alerting",
    "start": "1751350",
    "end": "1756629"
  },
  {
    "text": "capabilities and they have the ability to really go in and understand what's happening when they're certain that starts to act up and I'd love to say",
    "start": "1756629",
    "end": "1764129"
  },
  {
    "text": "that we've got there really well solved but we're still evolving it and probably will be for a while because that that",
    "start": "1764129",
    "end": "1769590"
  },
  {
    "text": "tooling can never be too good",
    "start": "1769590",
    "end": "1772610"
  },
  {
    "text": "we have a custom-built tool for deployment actually and again that goes",
    "start": "1776240",
    "end": "1781889"
  },
  {
    "text": "back to the security reason we've we've looked at things like well all the various tools that have been in the",
    "start": "1781889",
    "end": "1787830"
  },
  {
    "text": "sponsor area for doing kubernetes deployments and really none of them work for us because of the security access",
    "start": "1787830",
    "end": "1794820"
  },
  {
    "text": "problems or challenges I should say so we built our own it's it's kind of this",
    "start": "1794820",
    "end": "1800190"
  },
  {
    "text": "combination of Jenkins and ansible and everything flows through you know one",
    "start": "1800190",
    "end": "1805950"
  },
  {
    "text": "point that's very carefully monitored we log every single activity that it does",
    "start": "1805950",
    "end": "1810990"
  },
  {
    "text": "and so yeah we had to build our own that way it did get much much nicer with",
    "start": "1810990",
    "end": "1816200"
  },
  {
    "text": "communities that's for sure because we can actually just we wrote a little go utility that we just called deploy tool",
    "start": "1816200",
    "end": "1822960"
  },
  {
    "text": "because we're imaginative and it it's responsible for just taking a command line and then it calls the communities",
    "start": "1822960",
    "end": "1830580"
  },
  {
    "text": "API to do what we need and that also kind of limits access you can only do what that tool will let you do",
    "start": "1830580",
    "end": "1837450"
  },
  {
    "text": "and so from a security standpoint I lock things down a little bit more as well",
    "start": "1837450",
    "end": "1843320"
  },
  {
    "text": "yeah the development environment that is still in in flux right now honestly",
    "start": "1849800",
    "end": "1856930"
  },
  {
    "text": "because we've gone from a day where everybody could just run a copy of the",
    "start": "1856930",
    "end": "1863510"
  },
  {
    "text": "system on their laptop in a VM actually it's how we did it",
    "start": "1863510",
    "end": "1868820"
  },
  {
    "text": "and now with all of these services it's too big to run on a laptop so we've had",
    "start": "1868820",
    "end": "1874040"
  },
  {
    "text": "like everybody else I've talked to had to figure out how do you do development with a minimal set of the system",
    "start": "1874040",
    "end": "1880370"
  },
  {
    "text": "available and we're pretty much kind of saying do that as you like and you know",
    "start": "1880370",
    "end": "1889100"
  },
  {
    "text": "providing the tools to make it fairly easy and then figuring out which services we can provide in a shared way",
    "start": "1889100",
    "end": "1896180"
  },
  {
    "text": "up in the cloud so everybody kind of uses one instance of that one that only",
    "start": "1896180",
    "end": "1901670"
  },
  {
    "text": "works for some services that essentially kind of work in a multi-tenant way because otherwise as you know you you",
    "start": "1901670",
    "end": "1908210"
  },
  {
    "text": "know someone makes a call changes the data then I make a call it doesn't work anymore so we're still kind of evolving",
    "start": "1908210",
    "end": "1914900"
  },
  {
    "text": "that a bit but from from there we're",
    "start": "1914900",
    "end": "1920300"
  },
  {
    "text": "figuring developers are doing their unit testing they're running in their IDE they've got their stuff kind of working",
    "start": "1920300",
    "end": "1925940"
  },
  {
    "text": "and then we made it very easy to essentially push a button and spin up a tiny cluster basically one node cluster",
    "start": "1925940",
    "end": "1932930"
  },
  {
    "text": "for now in the cloud and then get a URL to be able to hit that I actually do Cuddy",
    "start": "1932930",
    "end": "1938900"
  },
  {
    "text": "integration testing with everything available",
    "start": "1938900",
    "end": "1942730"
  },
  {
    "text": "the data migrations as we're kind of tripping pieces off the monolith yeah it's painful we've done in a couple of",
    "start": "1949480",
    "end": "1958760"
  },
  {
    "start": "1950000",
    "end": "2029000"
  },
  {
    "text": "different ways actually primarily depending on the size of the data sometimes it's just too much to like do",
    "start": "1958760",
    "end": "1964760"
  },
  {
    "text": "during a maintenance window you know because we could in theory you can deploy the service the new service get",
    "start": "1964760",
    "end": "1970730"
  },
  {
    "text": "it ready wait for a maintenance window do a data migration and then turn it on and hope it works and see how that goes",
    "start": "1970730",
    "end": "1978320"
  },
  {
    "text": "but we've also done kind of rolling deployment of the service where",
    "start": "1978320",
    "end": "1985070"
  },
  {
    "text": "initially it'll be using the old table and it'll be reading and then it'll be double writing into a new database",
    "start": "1985070",
    "end": "1991279"
  },
  {
    "text": "because we have definitely gone for the no database is used by more than one",
    "start": "1991279",
    "end": "1997340"
  },
  {
    "text": "code base ever because then you've just got a different model ather and different looking monolith so there are",
    "start": "1997340",
    "end": "2004539"
  },
  {
    "text": "definitely cases where you've had to give it a couple of weeks of running until most of the data has been double",
    "start": "2004539",
    "end": "2010480"
  },
  {
    "text": "written and is in both places and then kind of do one last update during a maintenance window and then we're good",
    "start": "2010480",
    "end": "2015640"
  },
  {
    "text": "and we killed the old table but yeah it's it's painful sometimes alright I",
    "start": "2015640",
    "end": "2023950"
  },
  {
    "text": "think we're out of time thank you very much [Applause]",
    "start": "2023950",
    "end": "2030949"
  }
]