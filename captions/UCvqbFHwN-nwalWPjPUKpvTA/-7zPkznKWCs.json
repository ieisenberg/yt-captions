[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "all right welcome to the talk on operating enterprise great kubernetes clusters at salesforce on bare metal i'm anubhav",
    "start": "80",
    "end": "7600"
  },
  {
    "text": "dhoot senior director of engineering i manage a team that provides the kubernetes based platform for salesforce teams",
    "start": "7600",
    "end": "15120"
  },
  {
    "text": "am joined in this talk with mayankumar who is the architect on the same team",
    "start": "15120",
    "end": "20320"
  },
  {
    "text": "so who is this talk for this talk is for everyone who wants to operate their own",
    "start": "22240",
    "end": "27279"
  },
  {
    "text": "kubernetes clusters especially if you're doing it on bare metal we will explain our own experience",
    "start": "27279",
    "end": "33280"
  },
  {
    "text": "of doing it at salesforce this is really doing kubernetes the hard way as compared to the managed",
    "start": "33280",
    "end": "38879"
  },
  {
    "text": "kubernetes engines out there we'll do a deep dive into our architecture and how we operate our",
    "start": "38879",
    "end": "44719"
  },
  {
    "text": "stack over the last four and a half years we have developed a bunch of war stories of",
    "start": "44719",
    "end": "50320"
  },
  {
    "text": "issues and pitfalls that we face we would like to share with you hopefully you can learn from them as",
    "start": "50320",
    "end": "55920"
  },
  {
    "text": "well at the end we'll talk about our current and future investments",
    "start": "55920",
    "end": "62480"
  },
  {
    "text": "we hope at the end of this talk you will appreciate what does it take to run your own kubernetes clusters",
    "start": "62480",
    "end": "69680"
  },
  {
    "text": "so let's dive in so here is the overview of our entire",
    "start": "69680",
    "end": "75439"
  },
  {
    "start": "72000",
    "end": "72000"
  },
  {
    "text": "stack end to end as you can see we deploy using puppet on top of bare",
    "start": "75439",
    "end": "82720"
  },
  {
    "text": "metal so we deploy kubernetes docker city all through puppet on top of master and worker nodes",
    "start": "82720",
    "end": "90079"
  },
  {
    "text": "puppet internally we configure it to rely on systemd systemd is a component on the box that",
    "start": "90079",
    "end": "96000"
  },
  {
    "text": "is responsive to making sure the services keep running our customers or other salesforce teams",
    "start": "96000",
    "end": "103600"
  },
  {
    "text": "who want to just focus on their business logic and not worry about infrastructure so what we provide to our customers is a",
    "start": "103600",
    "end": "111200"
  },
  {
    "text": "github style deployment pipeline they check in their code into git along",
    "start": "111200",
    "end": "116719"
  },
  {
    "text": "with that they also check in a simplified manifest that describes their application the ci",
    "start": "116719",
    "end": "124000"
  },
  {
    "text": "pipeline runs it validates their application manifest packages it up",
    "start": "124000",
    "end": "129200"
  },
  {
    "text": "and deploys it to all clusters as a goal state the way we describe the goal state is in",
    "start": "129200",
    "end": "135120"
  },
  {
    "text": "the form of a crd for each application as you're all probably familiar crds as",
    "start": "135120",
    "end": "141520"
  },
  {
    "text": "a way to extend the kubernetes api we have extended the kubernetes api to",
    "start": "141520",
    "end": "147280"
  },
  {
    "text": "add infrastructure integrations such as pki cert configuration or custom",
    "start": "147280",
    "end": "152480"
  },
  {
    "text": "load balancing so when the goal state lands up in every",
    "start": "152480",
    "end": "157840"
  },
  {
    "text": "cluster we have an operator that keeps listening for this goal state",
    "start": "157840",
    "end": "163519"
  },
  {
    "text": "as soon as it gets a new goal state it'll go convert that crd into deployments",
    "start": "163519",
    "end": "169440"
  },
  {
    "text": "so that gives us our customers ability to deploy across the world across all of our data centers",
    "start": "169440",
    "end": "175599"
  },
  {
    "text": "within minutes over the last four and a half years our git mono repo where they check in",
    "start": "175599",
    "end": "182239"
  },
  {
    "text": "have already reached 100 000 commits last year alone we had 20 000 commands",
    "start": "182239",
    "end": "187519"
  },
  {
    "text": "so it's highly used inside our infrastructure so how is the puppet module that this",
    "start": "187519",
    "end": "194080"
  },
  {
    "text": "builds on implemented so we have built a puppet module that",
    "start": "194080",
    "end": "199599"
  },
  {
    "start": "196000",
    "end": "196000"
  },
  {
    "text": "lets you deploy kubernetes in a very simple way the declaration as i showed with just this two lines",
    "start": "199599",
    "end": "206560"
  },
  {
    "text": "you can say install kubernetes on my machine it takes in a flag which says do you",
    "start": "206560",
    "end": "211840"
  },
  {
    "text": "want the master or the worker node behavior let's see what's inside this",
    "start": "211840",
    "end": "219680"
  },
  {
    "text": "it's implemented in three layers from right to left the right post layer uses rpm to install",
    "start": "220080",
    "end": "227920"
  },
  {
    "text": "the binaries and as you said before it uses systemd configuration to manage the daemons so systemd is the",
    "start": "227920",
    "end": "235360"
  },
  {
    "text": "component will restart your service if it fails that layer is then used by the node",
    "start": "235360",
    "end": "241760"
  },
  {
    "text": "puppet class the node puppet class runs on every machine",
    "start": "241760",
    "end": "247439"
  },
  {
    "text": "it installs docker cubelet q proxy flannel or our custom sdn and ha proxy",
    "start": "247439",
    "end": "255599"
  },
  {
    "text": "why do we use ha proxy i'll cover that in the next slide",
    "start": "255599",
    "end": "260639"
  },
  {
    "text": "the master profit class takes in the node puppet class so it installs all of those services in",
    "start": "260639",
    "end": "266639"
  },
  {
    "text": "addition it also installs the hcd as well as the kubernetes control plane",
    "start": "266639",
    "end": "272400"
  },
  {
    "text": "that means api server controller manager and scheduler all as systemd services",
    "start": "272400",
    "end": "279599"
  },
  {
    "text": "so what are the features that we implemented inside our puppet module",
    "start": "280880",
    "end": "286560"
  },
  {
    "start": "281000",
    "end": "281000"
  },
  {
    "text": "first and foremost we completely automated the deployment of edge cd docker and kubernetes",
    "start": "286560",
    "end": "292639"
  },
  {
    "text": "and we also added a bunch of configuration flags for our services important things like cubelet arguments",
    "start": "292639",
    "end": "299199"
  },
  {
    "text": "and api server flags they're all configurable also we had to",
    "start": "299199",
    "end": "304240"
  },
  {
    "text": "implement features just for hcd and kubernetes for example the hcd service it waits before",
    "start": "304240",
    "end": "312080"
  },
  {
    "text": "for quorum to be formed before it proceeds the service account support in",
    "start": "312080",
    "end": "317120"
  },
  {
    "text": "kubernetes supports key rotation and there is one more detail that i did",
    "start": "317120",
    "end": "322160"
  },
  {
    "text": "not show in the previous slide so we were forced to run flannel on docker",
    "start": "322160",
    "end": "328080"
  },
  {
    "text": "but docker needs final running to start so how do we solve this so we ended up",
    "start": "328080",
    "end": "333919"
  },
  {
    "text": "running two instances of docker in every machine the first docker instance is called docker bootstrap",
    "start": "333919",
    "end": "340080"
  },
  {
    "text": "it runs flannel and sdn depending on the case and since flannel",
    "start": "340080",
    "end": "346000"
  },
  {
    "text": "needs that city it also installs that city on that machine now since flannel is already running now",
    "start": "346000",
    "end": "351840"
  },
  {
    "text": "the main docker can start and that's where we run kubernetes so that's a neat way we solve this problem",
    "start": "351840",
    "end": "359280"
  },
  {
    "text": "so to conclude how do we think about running kubernetes on puppet well the",
    "start": "359600",
    "end": "366000"
  },
  {
    "start": "360000",
    "end": "360000"
  },
  {
    "text": "good things are it's completely automated it's also declarative the same thing that we like about pub",
    "start": "366000",
    "end": "372400"
  },
  {
    "text": "kubernetes by using random start intervals on the",
    "start": "372400",
    "end": "378000"
  },
  {
    "text": "puppet region on every machine we can get some amount of staggering across machines but what are",
    "start": "378000",
    "end": "384720"
  },
  {
    "text": "the drawbacks this is not proper orchestration or any health mediation",
    "start": "384720",
    "end": "390720"
  },
  {
    "text": "also the iteration cycles are very expensive on puppet like any small change it takes in a lot",
    "start": "390720",
    "end": "396319"
  },
  {
    "text": "of effort to try it out and lastly the way modules are composed in puppet",
    "start": "396319",
    "end": "402319"
  },
  {
    "text": "any base module that you may not own maybe a common os library change can easily break",
    "start": "402319",
    "end": "408720"
  },
  {
    "text": "your module and you will hear about this more in the subsequent slide",
    "start": "408720",
    "end": "414800"
  },
  {
    "text": "so that concludes our deep dive into our puppet infrastructure now what did we have to do to",
    "start": "415360",
    "end": "421520"
  },
  {
    "start": "418000",
    "end": "418000"
  },
  {
    "text": "operationalize this stack we knew from the beginning we have to run",
    "start": "421520",
    "end": "426960"
  },
  {
    "text": "our control plane in high availability mode so we run multiple servers so our xcd",
    "start": "426960",
    "end": "432560"
  },
  {
    "text": "runs in quora mode but kubernetes for example the cubelet",
    "start": "432560",
    "end": "438080"
  },
  {
    "text": "cannot talk to multiple api servers so that's how we solve that was using",
    "start": "438080",
    "end": "443440"
  },
  {
    "text": "installing h a proxy on every machine so we configure cubelets to talk to the ha proxy as if it's talking to the api",
    "start": "443440",
    "end": "450000"
  },
  {
    "text": "server and then the ha proxy will load balance across the api servers",
    "start": "450000",
    "end": "455280"
  },
  {
    "text": "for security we hardened all our communication from service to service across machines using mtls",
    "start": "455280",
    "end": "461759"
  },
  {
    "text": "based on our custom pki configuration also we isolated two workloads that",
    "start": "461759",
    "end": "467919"
  },
  {
    "text": "should not be on the same machine by using node labels so we apply",
    "start": "467919",
    "end": "472960"
  },
  {
    "text": "labels on the nodes to group them into what we call as minion pools and when we create a deployment we use a",
    "start": "472960",
    "end": "479280"
  },
  {
    "text": "label selector to target one of those minion pools that lets us isolate which machines our",
    "start": "479280",
    "end": "484960"
  },
  {
    "text": "deployment can run on and conversely on the other way for each machine we apply our back",
    "start": "484960",
    "end": "491599"
  },
  {
    "text": "that limits what name spaces they can access also we have to monitor every piece of",
    "start": "491599",
    "end": "497039"
  },
  {
    "text": "our stack so we implemented what we call as watchdogs these are agents that monitor",
    "start": "497039",
    "end": "503759"
  },
  {
    "text": "infrastructure and alert us we also implemented a sequel based monitoring pipeline",
    "start": "503759",
    "end": "510479"
  },
  {
    "text": "that provides snapshot visibility and custom alerting and i'll go deeper in the next slide",
    "start": "510479",
    "end": "516640"
  },
  {
    "text": "we also open source loop last year this is a project that provides historical",
    "start": "516640",
    "end": "522000"
  },
  {
    "text": "visibility on kubernetes we believe there's no other tool available in the ecosystem that solves",
    "start": "522000",
    "end": "527519"
  },
  {
    "text": "this unique problem we also automated as many of the operations as possible",
    "start": "527519",
    "end": "533440"
  },
  {
    "text": "for example we reboot machines when things go bad on them",
    "start": "533440",
    "end": "538639"
  },
  {
    "text": "so now let's look into the monitoring stack for our infrastructure as i said we implement",
    "start": "539440",
    "end": "545440"
  },
  {
    "start": "540000",
    "end": "540000"
  },
  {
    "text": "what is called are basically agents that monitor one piece of the infrastructure",
    "start": "545440",
    "end": "551360"
  },
  {
    "text": "and emit an alert we have run watchdog itself on top of",
    "start": "551360",
    "end": "557200"
  },
  {
    "text": "kubernetes and we implemented a framework that makes it very easy for us to add another",
    "start": "557200",
    "end": "562320"
  },
  {
    "text": "watchdog this has features such as warm-up period so it waits for a failures to happen for",
    "start": "562320",
    "end": "569360"
  },
  {
    "text": "a little bit of time before it alerts us so it takes care of momentary blips it also has cooldown periods so once it",
    "start": "569360",
    "end": "575120"
  },
  {
    "text": "alerts us it waits for another certain configurable period of time before alerting us again",
    "start": "575120",
    "end": "580800"
  },
  {
    "text": "we also added snooze this is very helpful when you want the alerts to stop for a certain amount of time because you know",
    "start": "580800",
    "end": "587200"
  },
  {
    "text": "you about to fix something and if you forget to fix that or it takes longer he'll come back and alert you again to",
    "start": "587200",
    "end": "593519"
  },
  {
    "text": "remind you this need still needs to be fixed so watchdogs can alert us directly",
    "start": "593519",
    "end": "599440"
  },
  {
    "text": "through pagerduty as shown on the top but they also emit crds",
    "start": "599440",
    "end": "605200"
  },
  {
    "text": "these crds are coming from each of those data centers and we aggregate them using a kafka bus that",
    "start": "605200",
    "end": "611519"
  },
  {
    "text": "lands those data into sql and with this central sql we can write sql query based alerts and metrics",
    "start": "611519",
    "end": "619200"
  },
  {
    "text": "as well as provide dashboards for visibility to our customers so that concludes a deep dive in our",
    "start": "619200",
    "end": "626240"
  },
  {
    "text": "architecture next i'm going to hand it off to mayak who will cover some interesting war stories",
    "start": "626240",
    "end": "633839"
  },
  {
    "text": "thanks anu so war stories are super fun so let's look at some of these",
    "start": "635040",
    "end": "640399"
  },
  {
    "text": "war stories we have the first one is about perils of mounting host part",
    "start": "640399",
    "end": "646000"
  },
  {
    "start": "641000",
    "end": "641000"
  },
  {
    "text": "this wall story dates back to the days of kubernetes 1.3 our first encounter with kubernetes",
    "start": "646000",
    "end": "651920"
  },
  {
    "text": "itself we had made the decision to use it and we were still learning about the various knobs it offered",
    "start": "651920",
    "end": "658240"
  },
  {
    "text": "one of those knobs was hostpath one fine day we found that every rolling",
    "start": "658240",
    "end": "663680"
  },
  {
    "text": "update of a deployment could result in parts getting stuck in container creating",
    "start": "663680",
    "end": "669040"
  },
  {
    "text": "more debugging into cubelet logs showed teardown failures and volume setup failures",
    "start": "669040",
    "end": "674480"
  },
  {
    "text": "for secrets and empty darwinians further along following a series of breadcrumbs",
    "start": "674480",
    "end": "680000"
  },
  {
    "text": "in cubelet logs and running ls off searching through pop mount and using other linux tools",
    "start": "680000",
    "end": "687040"
  },
  {
    "text": "we found that a partner team had mounted the root file system using hostpath inside their deployment yes you heard",
    "start": "687040",
    "end": "694160"
  },
  {
    "text": "that right the same brute file system also has var lib cubelet which is the default path in which",
    "start": "694160",
    "end": "700959"
  },
  {
    "text": "cubelet mounts a secret and empty disk that meant that any part that landed on",
    "start": "700959",
    "end": "707839"
  },
  {
    "text": "the same host as the partner pod mounting the root file system would not be able to terminate traceable",
    "start": "707839",
    "end": "714720"
  },
  {
    "text": "so this is because that parts emptieder and secrets volume are also mounted inside the partner part and",
    "start": "714720",
    "end": "721040"
  },
  {
    "text": "any attempt to tear down those volumes will results will result in resource busy errors",
    "start": "721040",
    "end": "727920"
  },
  {
    "text": "we immediately ask the partner team to mount very specific paths they needed and went ahead and added additional",
    "start": "727920",
    "end": "734079"
  },
  {
    "text": "validation the validation to allow only a limited set of paths using host paths",
    "start": "734079",
    "end": "739839"
  },
  {
    "text": "we also went ahead and made read-only as the default way to mount host part while using hostpass",
    "start": "739839",
    "end": "747040"
  },
  {
    "text": "is an anti-pattern we all realize now back in the days this was a lesson learned that it can be dangerous to",
    "start": "747040",
    "end": "753279"
  },
  {
    "text": "expose all knobs of a new tool that we all didn't understand properly using host paths can also lead to",
    "start": "753279",
    "end": "759920"
  },
  {
    "text": "services that may work on one distribution of linux but not another",
    "start": "759920",
    "end": "765040"
  },
  {
    "text": "so let's move on to the next war story so this is about bridge networking failures",
    "start": "765040",
    "end": "770959"
  },
  {
    "start": "767000",
    "end": "767000"
  },
  {
    "text": "this story is also about our early days of kubernetes we saw a customer was complaining with",
    "start": "770959",
    "end": "776959"
  },
  {
    "text": "intermittent connectivity failures with little more debugging we found that",
    "start": "776959",
    "end": "782240"
  },
  {
    "text": "the failure only occurred when the client and server parts were residing on the same post",
    "start": "782240",
    "end": "788480"
  },
  {
    "text": "after a lot of debugging reading through comments upon comments and kubernetes issues we learned about the new concept called",
    "start": "788480",
    "end": "795600"
  },
  {
    "text": "hairpin mode pretty sure you have heard of it helping mode is setting on the bridge",
    "start": "795600",
    "end": "800880"
  },
  {
    "text": "interfaces that enables a packet received from an interface to be routed back on the same interface",
    "start": "800880",
    "end": "807440"
  },
  {
    "text": "the help and mode did not apply in this specific case because the client and server pods were",
    "start": "807440",
    "end": "813120"
  },
  {
    "text": "different so after some more debugging we found out about the ciscl kernel flag called",
    "start": "813120",
    "end": "819680"
  },
  {
    "text": "net bridge bridge nf call ip tables you can see it on the screen on the slides",
    "start": "819680",
    "end": "825120"
  },
  {
    "text": "this controls whether or not the packets traversing the bridge are sent to ip tables for processing",
    "start": "825120",
    "end": "832720"
  },
  {
    "text": "someone had said this to zero as an optimization in the common puppet module and hence preventing the packet from",
    "start": "832720",
    "end": "839120"
  },
  {
    "text": "reaching high details since the client and server parts on the same host were talking to each other",
    "start": "839120",
    "end": "845040"
  },
  {
    "text": "using cluster ip the packets needed to be processed by ip tables where the actual cluster ip magic",
    "start": "845040",
    "end": "851519"
  },
  {
    "text": "happens setting this to one fixed issue but it was a long painful debugging we learned from this",
    "start": "851519",
    "end": "858959"
  },
  {
    "text": "and added monitoring for this scenario in the form of a watchdog which would not only check the",
    "start": "858959",
    "end": "864399"
  },
  {
    "text": "cctl kernel flag but also the hairpin mode for all we eats in the process we obviously gained a lot",
    "start": "864399",
    "end": "872240"
  },
  {
    "text": "of iptable expertise so let's move on to our the third war",
    "start": "872240",
    "end": "878160"
  },
  {
    "start": "876000",
    "end": "876000"
  },
  {
    "text": "story this is about quorum members i'm pretty sure you've heard of that word the story is about an hcd outage we saw",
    "start": "878160",
    "end": "885920"
  },
  {
    "text": "in one of our clusters hcd state was wiped out on two of the three fcd members",
    "start": "885920",
    "end": "892320"
  },
  {
    "text": "and they restarted as a brand new cluster since the two fcd members were still in",
    "start": "892320",
    "end": "898160"
  },
  {
    "text": "quorum they convinced the third xcd member to join this new cluster and replicate their empty state to their",
    "start": "898160",
    "end": "905279"
  },
  {
    "text": "third member now the reason for why hcd got wiped out of its data is not clear but the reason",
    "start": "905279",
    "end": "912560"
  },
  {
    "text": "for why third member joined the new cluster and lose all its state was that flag",
    "start": "912560",
    "end": "919279"
  },
  {
    "text": "initial cluster state you can see it on the screen it was still set to new on all members",
    "start": "919279",
    "end": "925839"
  },
  {
    "text": "if this flag was set to existing the last hcd member would not have joined the new cluster",
    "start": "925839",
    "end": "932399"
  },
  {
    "text": "since it was already part of an existing cluster and we could have some chance in",
    "start": "932399",
    "end": "937519"
  },
  {
    "text": "recovering that data the xct cluster formation requires all members to be in new state",
    "start": "937519",
    "end": "943759"
  },
  {
    "text": "initially but must be switched to existing later to avoid this scenario we use the puppet",
    "start": "943759",
    "end": "950959"
  },
  {
    "text": "hiera mechanism to switch from new to existing in a month time frame",
    "start": "950959",
    "end": "956079"
  },
  {
    "text": "when the new data center is ready to go live so people in the public cloud with",
    "start": "956079",
    "end": "962480"
  },
  {
    "text": "managed cloud providers we will they will probably never get to experience this",
    "start": "962480",
    "end": "967759"
  },
  {
    "text": "but on bare metal we need to expect this some similar to how disk and network failures",
    "start": "967759",
    "end": "973279"
  },
  {
    "text": "are part of any distributed systems and be prepared for it this incident showed",
    "start": "973279",
    "end": "979040"
  },
  {
    "text": "how the flags are confusing and we started doing regular backups of our all our lcd data and also doing regular",
    "start": "979040",
    "end": "986639"
  },
  {
    "text": "game day exercises to try such scenarios and make sure that our run books are ready for dealing with this so let's move on to",
    "start": "986639",
    "end": "994720"
  },
  {
    "text": "our next war story and this war story is about memory lakes",
    "start": "994720",
    "end": "1002079"
  },
  {
    "start": "995000",
    "end": "995000"
  },
  {
    "text": "it's about the kubernetes cluster which was rapidly leaking pods you may ask how can you really leak pods",
    "start": "1002079",
    "end": "1010480"
  },
  {
    "text": "one day our on-call reported that the apa server machines were going unresponsive on looking at our graphs we",
    "start": "1010880",
    "end": "1018560"
  },
  {
    "text": "found their memory usage steadily increasing until they would become unresponsive",
    "start": "1018560",
    "end": "1024480"
  },
  {
    "text": "while debugging this we were manually restarting them to prevent the hc cluster from tipping over and lose score",
    "start": "1024480",
    "end": "1031280"
  },
  {
    "text": "we decided that in the interim to we should put a systemd memory limit for our api servers while we continue to",
    "start": "1031280",
    "end": "1037678"
  },
  {
    "text": "debug this further this at least stabilize the cluster since the systemd would restart the api",
    "start": "1037679",
    "end": "1043600"
  },
  {
    "text": "server before it became unresponsive further debugging led to a mutating",
    "start": "1043600",
    "end": "1049200"
  },
  {
    "text": "weapon that was incorrectly adding a json patch this json patch in trying to add a label",
    "start": "1049200",
    "end": "1056799"
  },
  {
    "text": "dropped all other labels since the labels were all gone the pod being created was not accepted",
    "start": "1056799",
    "end": "1064559"
  },
  {
    "text": "by the deployment controller and hence the deployment controller kept on creating new pods",
    "start": "1064559",
    "end": "1070799"
  },
  {
    "text": "to satisfy its desired goal state this continued until new pods filled up the entire api",
    "start": "1070799",
    "end": "1077039"
  },
  {
    "text": "server host memory and crashed it so this bug in our jason patch",
    "start": "1077039",
    "end": "1083280"
  },
  {
    "text": "it was his in it was hidden due to another kubernetes bulb that was re-merging the labels back and so we",
    "start": "1083280",
    "end": "1090640"
  },
  {
    "text": "never saw this bag until it got exposed after we upgraded",
    "start": "1090640",
    "end": "1095679"
  },
  {
    "text": "it to a new version of kubernetes we finally fixed the bug but there were a ton of learnings from this",
    "start": "1095679",
    "end": "1102240"
  },
  {
    "text": "and a lot of debugging for sure so mutating the books are a deadly weapon that must be constantly validated",
    "start": "1102240",
    "end": "1110160"
  },
  {
    "text": "and can read in a cluster also adding limits is better than rendering the cluster",
    "start": "1110160",
    "end": "1115600"
  },
  {
    "text": "unusable and this holds true for both hosts and parts",
    "start": "1115600",
    "end": "1121440"
  },
  {
    "text": "so now we will move on to our last war story and and we have some more but i think",
    "start": "1121440",
    "end": "1127600"
  },
  {
    "start": "1124000",
    "end": "1124000"
  },
  {
    "text": "this is all we have time for so this story is about our adventure in rolling out service accounts featuring",
    "start": "1127600",
    "end": "1133760"
  },
  {
    "text": "kubernetes on bare metal infrastructure we had just enabled service accounts and",
    "start": "1133760",
    "end": "1140480"
  },
  {
    "text": "puppet had begun rolling out this feature across our data centers puppet uses a carrying mechanism for our",
    "start": "1140480",
    "end": "1147760"
  },
  {
    "text": "bare metal hose based on their puppet roles for masters which includes api server controller",
    "start": "1147760",
    "end": "1153600"
  },
  {
    "text": "manager and schedulers we can read the changes in one master before rolling it out to all masters",
    "start": "1153600",
    "end": "1160160"
  },
  {
    "text": "one of our on calls noticed that the kubernetes parts were being deleted but were not coming",
    "start": "1160160",
    "end": "1166240"
  },
  {
    "text": "up the error on those pods was no api token found for service account default",
    "start": "1166240",
    "end": "1173200"
  },
  {
    "text": "we immediately recognize this as related to the service account feature further feature further debugging revealed that",
    "start": "1173200",
    "end": "1180559"
  },
  {
    "text": "there was ongoing patching which was bringing down nodes one by one",
    "start": "1180559",
    "end": "1185600"
  },
  {
    "text": "and hence the pods were getting evicted and recreated at the same time",
    "start": "1185600",
    "end": "1192720"
  },
  {
    "text": "the service account feature was being rolled out the puppet calorie mechanism had",
    "start": "1192720",
    "end": "1198000"
  },
  {
    "text": "resulted in a situation where a master node carrying the latest puppet code got all the service account flags for",
    "start": "1198000",
    "end": "1204880"
  },
  {
    "text": "api server and controller manager but the controller manager that was leader was running on an",
    "start": "1204880",
    "end": "1210799"
  },
  {
    "text": "older node so the older controller manager did not",
    "start": "1210799",
    "end": "1216000"
  },
  {
    "text": "ensure and you can see it on the screen so the older controller manager did not ensure that tokens for the reference",
    "start": "1216000",
    "end": "1222559"
  },
  {
    "text": "service account in the pod being created exists and hence the port being created",
    "start": "1222559",
    "end": "1229039"
  },
  {
    "text": "actually failed the admission check on the api server which you see on the left this happened because the api server",
    "start": "1229039",
    "end": "1236559"
  },
  {
    "text": "handling that request was a new api server in puppet can remote",
    "start": "1236559",
    "end": "1241840"
  },
  {
    "text": "and was validating the existence of the token we fast forwarded that feature so that",
    "start": "1241840",
    "end": "1248720"
  },
  {
    "text": "the puppet can remote is applied to all the hosts and that's all the issue we all learned",
    "start": "1248720",
    "end": "1254559"
  },
  {
    "text": "that our channeling mechanism is not the best way to roll out new kubernetes features especially the",
    "start": "1254559",
    "end": "1260559"
  },
  {
    "text": "ones which span across api servers and controller managers and more thought is needed",
    "start": "1260559",
    "end": "1267120"
  },
  {
    "text": "so this is all the ro stories we just talked about",
    "start": "1267120",
    "end": "1272320"
  },
  {
    "start": "1271000",
    "end": "1271000"
  },
  {
    "text": "now we are going to summarize um to summarize",
    "start": "1272320",
    "end": "1280159"
  },
  {
    "text": "rolling your own kubernetes on a bare metal non-vm infrastructure is hard and requires a",
    "start": "1280159",
    "end": "1286559"
  },
  {
    "text": "lot of expertise and investments hopefully we have shared some recipes today",
    "start": "1286559",
    "end": "1292000"
  },
  {
    "text": "that you can take home and apply to your own infrastructure we have invested in a fully automated",
    "start": "1292000",
    "end": "1298880"
  },
  {
    "text": "highly available and secure deployment pipeline for docker at cd and kubernetes infrastructure",
    "start": "1298880",
    "end": "1305039"
  },
  {
    "text": "across all our data centers which has evolved over the last 4.5 years",
    "start": "1305039",
    "end": "1310960"
  },
  {
    "text": "the whole infrastructure has tight integrations with networking security and monitoring we have",
    "start": "1310960",
    "end": "1318480"
  },
  {
    "text": "invested in watchdog monitoring alerting and visibility pipeline you saw some of that in anu",
    "start": "1318480",
    "end": "1324320"
  },
  {
    "text": "slide when he was diving deep into the monitoring watchdogs we have also invested in cost to serve",
    "start": "1324320",
    "end": "1330320"
  },
  {
    "text": "visibility at container name space and team level lastly",
    "start": "1330320",
    "end": "1335440"
  },
  {
    "text": "there is a robust on-call rotation and run books for all are on call which makes it easy for",
    "start": "1335440",
    "end": "1342400"
  },
  {
    "text": "our on-call engineers to debug and resolve problems even at 3 am in the",
    "start": "1342400",
    "end": "1347600"
  },
  {
    "text": "night we'd also like to mention some ongoing and future",
    "start": "1347600",
    "end": "1352640"
  },
  {
    "text": "investments so we are also building a pass for containers to accelerate the",
    "start": "1352640",
    "end": "1358640"
  },
  {
    "text": "adoption of public cloud and abstract enough of the infrastructure away",
    "start": "1358640",
    "end": "1363760"
  },
  {
    "text": "for developers who don't need to fully understand it we are also building a pass layer for",
    "start": "1363760",
    "end": "1369120"
  },
  {
    "text": "accessing cloud resources in a cloud neutral way and we will continue to talk about them",
    "start": "1369120",
    "end": "1374640"
  },
  {
    "text": "and open sources them when they are available we all earlier open source two of our",
    "start": "1374640",
    "end": "1380480"
  },
  {
    "text": "projects one is generic site car injector which enables you to declaratively inject side",
    "start": "1380480",
    "end": "1386640"
  },
  {
    "text": "cars meeting the need of a wide variety of our infrastructure teams this project is currently used in",
    "start": "1386640",
    "end": "1393200"
  },
  {
    "text": "production by more than seven teams for critical side cars like pki secrets",
    "start": "1393200",
    "end": "1398640"
  },
  {
    "text": "and kerberos and we have some interesting ideas in this space you will hear more about by the end of this year",
    "start": "1398640",
    "end": "1406400"
  },
  {
    "text": "so we also open source loop last year which is our history visualization tool for kubernetes resources and it helps in",
    "start": "1406400",
    "end": "1412960"
  },
  {
    "text": "debugging and monitoring anu touched upon these in his previous",
    "start": "1412960",
    "end": "1418159"
  },
  {
    "text": "slides as well if you want to help solve some really challenging problems",
    "start": "1418159",
    "end": "1423279"
  },
  {
    "text": "come join us anu and myself will be here for 10 more minutes and happy to take any questions you might have thank you",
    "start": "1423279",
    "end": "1432158"
  },
  {
    "text": "because you can hear us so we're going to try to answer all your",
    "start": "1432960",
    "end": "1438400"
  },
  {
    "text": "questions so do we how do we do rolling update of",
    "start": "1438400",
    "end": "1443679"
  },
  {
    "text": "our kubernetes clusters so as we said in the talk we do connecting",
    "start": "1443679",
    "end": "1449600"
  },
  {
    "text": "we basically means we pick one server out of our five control plane",
    "start": "1449600",
    "end": "1455520"
  },
  {
    "text": "machines to run the new version if everything looks good after a certain amount of time interval",
    "start": "1455520",
    "end": "1460799"
  },
  {
    "text": "then we start rolling the update to the other api servers that's why we run five api",
    "start": "1460799",
    "end": "1466000"
  },
  {
    "text": "servers that gives us a way to test on one and the remaining four are still part of the quorum and if something happens to",
    "start": "1466000",
    "end": "1472480"
  },
  {
    "text": "one it would not affect the others similarly for the minions we group them into a bunch of pools and",
    "start": "1472480",
    "end": "1479200"
  },
  {
    "text": "then we upgrade them anything you add",
    "start": "1479200",
    "end": "1487840"
  },
  {
    "text": "so i'll answer this one how do you use persistent volumes on bare metal the answer is we don't there was",
    "start": "1495679",
    "end": "1504559"
  },
  {
    "text": "i think persistent volumes are hard so most of our persistent infrastructure is not on kubernetes yet",
    "start": "1504559",
    "end": "1513039"
  },
  {
    "text": "yeah i think the next one is this how do you do no recovery and bare metal it's hard i mean that's the biggest problem with bare metal is you have no",
    "start": "1513760",
    "end": "1520640"
  },
  {
    "text": "way to quickly nuke a machine if something goes wrong so you really have to treat our api servers",
    "start": "1520640",
    "end": "1527520"
  },
  {
    "text": "as almost like pets like if something happens to one of our api servers out of the five then we want to get it",
    "start": "1527520",
    "end": "1532640"
  },
  {
    "text": "recovered very quickly similarly for our minions if we lose a bunch of minions that are affecting a",
    "start": "1532640",
    "end": "1538880"
  },
  {
    "text": "customer we haven't really prioritized them that's some of the challenges running on them",
    "start": "1538880",
    "end": "1544559"
  },
  {
    "text": "but i think to add to that uh we have a node rebooter right which is basically in order to recover",
    "start": "1545120",
    "end": "1552240"
  },
  {
    "text": "on bare metal we have an automated uh node rebuilder which based on some",
    "start": "1552240",
    "end": "1557279"
  },
  {
    "text": "health signals can actually go and determine that the node is unhealthy and needs",
    "start": "1557279",
    "end": "1563279"
  },
  {
    "text": "rebooting so that helps alleviate a lot of the challenges for diameter",
    "start": "1563279",
    "end": "1571679"
  },
  {
    "text": "somebody else has to use terraform or ansible there are other teams have tried ansible",
    "start": "1571679",
    "end": "1576720"
  },
  {
    "text": "nobody as we know has used it for kubernetes as you said in our talk like using puppet for kubernetes is a little",
    "start": "1576720",
    "end": "1583120"
  },
  {
    "text": "challenging because of lack of orchestration so if you had to start over again we probably might not",
    "start": "1583120",
    "end": "1588720"
  },
  {
    "text": "have picked puppet and might have looked at something else that gave us orchestration across",
    "start": "1588720",
    "end": "1594080"
  },
  {
    "text": "api servers if something bad happens we can stop the rolling update automatically",
    "start": "1594080",
    "end": "1599840"
  },
  {
    "text": "let's see so do you use the same hardware spec for both master and workflows the answer is yes yeah it's",
    "start": "1602400",
    "end": "1608880"
  },
  {
    "text": "exactly the same how do you use load balancer services so",
    "start": "1608880",
    "end": "1616000"
  },
  {
    "text": "we run our custom software load balancing stack that does uh proxying across our pods so",
    "start": "1616000",
    "end": "1622480"
  },
  {
    "text": "the custom load balancing stack can go talk to the api server grab the list of",
    "start": "1622480",
    "end": "1627600"
  },
  {
    "text": "ips of actual pods and since we run sdn stack that gives us flat ips across",
    "start": "1627600",
    "end": "1633440"
  },
  {
    "text": "pods and then metal it can redirect traffic from the proxy layer of the load balancer directly to the parts",
    "start": "1633440",
    "end": "1640399"
  },
  {
    "text": "and that's a much better way that we have found to run software load balancing on top of our infrastructure",
    "start": "1640399",
    "end": "1647440"
  },
  {
    "text": "what is your solution for managing endpoints and service mesh so i don't know if by end points you",
    "start": "1649440",
    "end": "1655279"
  },
  {
    "text": "mean the kubernetes endpoints um i don't know exactly what the",
    "start": "1655279",
    "end": "1660480"
  },
  {
    "text": "question is but i think we we do have a homegrown service mesh solution",
    "start": "1660480",
    "end": "1665520"
  },
  {
    "text": "and we are slowly migrating that to histo on first party bare metal and uh",
    "start": "1665520",
    "end": "1672399"
  },
  {
    "text": "i mean istio uses endpoints and uh service kubernetes services rely on",
    "start": "1672399",
    "end": "1677600"
  },
  {
    "text": "endpoints so if you can be more specific on that question that would help",
    "start": "1677600",
    "end": "1684080"
  },
  {
    "text": "and any more questions you can always come to slack after this talk ends with the slack channel hanging out",
    "start": "1685919",
    "end": "1692080"
  },
  {
    "text": "there to answer the questions",
    "start": "1692080",
    "end": "1697120"
  },
  {
    "text": "and how do you request for the question limit yeah it's basically we don't have a smart algorithm at the moment but we",
    "start": "1697120",
    "end": "1702640"
  },
  {
    "text": "do plan to implement something in the future that auto automates our",
    "start": "1702640",
    "end": "1711840"
  },
  {
    "text": "managing storage and bare metal as we said it's hard to do storage and then my room",
    "start": "1715919",
    "end": "1721440"
  },
  {
    "text": "teams apartheid teams have built their own custom orchestration using crds and controllers",
    "start": "1721440",
    "end": "1728240"
  },
  {
    "text": "to manage that",
    "start": "1728240",
    "end": "1731039"
  },
  {
    "text": "we do not use any dedicated storage for our cluster it's all on local disk on the developer",
    "start": "1736000",
    "end": "1741600"
  },
  {
    "text": "cluster that's why you use ha",
    "start": "1741600",
    "end": "1744880"
  },
  {
    "text": "then another question",
    "start": "1750559",
    "end": "1753840"
  },
  {
    "text": "most of these",
    "start": "1758240",
    "end": "1760960"
  },
  {
    "text": "so how long have you been running kubernetes on bare metal in production that we are about to complete like",
    "start": "1764720",
    "end": "1770159"
  },
  {
    "text": "probably five years now just started exactly around kubernetes 1.3 when",
    "start": "1770159",
    "end": "1777200"
  },
  {
    "text": "kubernetes deployment was still an alpha feature and we were thinking about whether to",
    "start": "1777200",
    "end": "1783279"
  },
  {
    "text": "bet it or bet on it or not but yeah now it's been around four and a half reaching five years",
    "start": "1783279",
    "end": "1791120"
  },
  {
    "text": "yeah we were part of the first coupon it is nice to see how the community has grown over the last five years just",
    "start": "1791279",
    "end": "1796559"
  },
  {
    "text": "exploded in front of our eyes",
    "start": "1796559",
    "end": "1800720"
  },
  {
    "text": "and what else yes so you want to take this one why do",
    "start": "1803039",
    "end": "1810880"
  },
  {
    "text": "you use bare metal at all why didn't you just use stronger vms",
    "start": "1810880",
    "end": "1815919"
  },
  {
    "text": "yeah we use bare metal for various reasons in our first party infrastructure i think using",
    "start": "1816080",
    "end": "1822399"
  },
  {
    "text": "a vms might be much easier but we didn't have that option when we started and that's what we used",
    "start": "1822399",
    "end": "1830240"
  },
  {
    "text": "what else i don't fully understand conversion",
    "start": "1835279",
    "end": "1840960"
  },
  {
    "text": "hyper converged bare metals so we basically use puppet on top of that metal",
    "start": "1840960",
    "end": "1846880"
  },
  {
    "text": "and it's we use a same skew for the entire data center",
    "start": "1846880",
    "end": "1852159"
  },
  {
    "text": "that keeps it simple for us to build our first quality of the infrastructure we don't use any specialized hardware",
    "start": "1852159",
    "end": "1858799"
  },
  {
    "text": "and we don't use anything public cloud for our first party infrastructure",
    "start": "1858799",
    "end": "1865840"
  },
  {
    "text": "[Laughter]",
    "start": "1873040",
    "end": "1877830"
  },
  {
    "text": "thanks a lot for attending this talk it's different experience for us doing it",
    "start": "1878399",
    "end": "1884480"
  },
  {
    "text": "live on this channel thank you",
    "start": "1884480",
    "end": "1891039"
  }
]