[
  {
    "text": "coming in joining in I think uh for the evening session",
    "start": "1400",
    "end": "6600"
  },
  {
    "text": "um and thanks to uh the cnca foundation for allowing me to come here and present to you on open data Hub uh how many of",
    "start": "6600",
    "end": "15599"
  },
  {
    "text": "you have worked on open data Hub here or know about heard about open open data",
    "start": "15599",
    "end": "22000"
  },
  {
    "text": "Hub okay then I think you in the right room because we need to know that okay so I'm uh part of I'm uh I'm",
    "start": "22000",
    "end": "30439"
  },
  {
    "text": "I'm from rat I'm a senior principal architect Focus predominantly on open shift AI That's a product which we have",
    "start": "30439",
    "end": "37680"
  },
  {
    "text": "from uh uh which is the Upstream is open data Hub basically okay and then we have",
    "start": "37680",
    "end": "42920"
  },
  {
    "text": "used that to create a product so I'm not going to talk about rad product at such let's focus on uh the solution which I",
    "start": "42920",
    "end": "49039"
  },
  {
    "text": "want to talk through okay and the open data Hub and uh how it is used in in",
    "start": "49039",
    "end": "54680"
  },
  {
    "text": "another open source product called open shift light speed as well okay with an integration of uh Rag llm and uh uh the",
    "start": "54680",
    "end": "63600"
  },
  {
    "text": "uh platform which we have I'm also going to talk about uh couple of personas uh",
    "start": "63600",
    "end": "69000"
  },
  {
    "text": "like admin Persona on how an administrator can actually set up a data science pipeline okay if they already",
    "start": "69000",
    "end": "75759"
  },
  {
    "text": "have a pipeline available with them right and uh the second Persona is a data scientist Persona uh how many of",
    "start": "75759",
    "end": "81560"
  },
  {
    "text": "you are data scientists or work with Okay cool so uh so what we do is we go",
    "start": "81560",
    "end": "86640"
  },
  {
    "text": "to the workbench and then we actually create a data science pipeline from there and it will just run and get you",
    "start": "86640",
    "end": "93720"
  },
  {
    "text": "what you are supposed to do right from the pipeline point of view so again uh so you don't have to worry",
    "start": "93720",
    "end": "102040"
  },
  {
    "text": "about how it's going to run in the back end uh right as a data scientist so",
    "start": "102040",
    "end": "107320"
  },
  {
    "text": "that's that's the whole purpose of what I'm going to try and show through open data Hub here and uh yeah that's uh and",
    "start": "107320",
    "end": "115040"
  },
  {
    "text": "I'm going to use generate as a framework or or an example here I'm going to use a lot of tools uh like uh Argo CD uh which",
    "start": "115040",
    "end": "123399"
  },
  {
    "text": "I have used to deploy the whole environment in our lab uh I am going to use uh tecton or Argo flow okay these",
    "start": "123399",
    "end": "131640"
  },
  {
    "text": "are all the workflows for the pipelines to be created from the data science point of view I'm going to use uh uh",
    "start": "131640",
    "end": "137720"
  },
  {
    "text": "kubernetes as a platform uh but I'm using open shift in this case Okay and",
    "start": "137720",
    "end": "143360"
  },
  {
    "text": "then uh I'm using Lang chain tool uh basically to create uh I'm using um uh",
    "start": "143360",
    "end": "149400"
  },
  {
    "text": "to do embeddings and things like that uh for now in this environment I'm using postas database uh with the vector",
    "start": "149400",
    "end": "156000"
  },
  {
    "text": "extensions uh but of course you can use M as well and there are a lot of examples where if you don't want to have",
    "start": "156000",
    "end": "162519"
  },
  {
    "text": "persistent storage you can have inmemory database as well some of them very work very fast and in terms of uh getting the",
    "start": "162519",
    "end": "169400"
  },
  {
    "text": "embeddings uh in into the inmemory database and then showing it to llm to actually get you what you need I'll",
    "start": "169400",
    "end": "175640"
  },
  {
    "text": "explain those things as well while we go through the process uh but I would like like to just how many of you are",
    "start": "175640",
    "end": "181879"
  },
  {
    "text": "implementing generative a in your organization or thinking of right everyone right most of you fantastic",
    "start": "181879",
    "end": "188519"
  },
  {
    "text": "okay now uh and uh so what happens right uh in in uh we talk about llm large",
    "start": "188519",
    "end": "195840"
  },
  {
    "text": "language models we'll talk about rag a bit uh we'll talk about uh high level architecture which uh I view like from",
    "start": "195840",
    "end": "202400"
  },
  {
    "text": "my perspective uh benefits of rag approach and then how to customize an",
    "start": "202400",
    "end": "207480"
  },
  {
    "text": "llm using rag uh we'll talk about open data Hub um it's an open source project",
    "start": "207480",
    "end": "214760"
  },
  {
    "text": "and uh it's actually an AI platform a very beautiful platform for you to actually go and understand and use it in",
    "start": "214760",
    "end": "222239"
  },
  {
    "text": "your environment then I'll we'll actually I want to focus more on the demo part here",
    "start": "222239",
    "end": "228760"
  },
  {
    "text": "demonstration uh I'm thinking of doing live demos I have some uh tabs here in my browser available in case if it",
    "start": "228760",
    "end": "236360"
  },
  {
    "text": "doesn't work at least I'll show you that hopefully it works here then I can show a little bit more about uh we can drive",
    "start": "236360",
    "end": "243120"
  },
  {
    "text": "deeper into what things are uh deployed as right from this uh specific solution point of view um and uh yeah you can",
    "start": "243120",
    "end": "252280"
  },
  {
    "text": "think this as a base model for you to do your development testing okay of a",
    "start": "252280",
    "end": "257600"
  },
  {
    "text": "generative AI with Rag and or a simple chat board if you want to create okay",
    "start": "257600",
    "end": "262880"
  },
  {
    "text": "and then it can be like a first step for you to go and understand how things work right how things are integrated and then",
    "start": "262880",
    "end": "268320"
  },
  {
    "text": "you can eventually improvise on that get better tool sets get a better better embedding algorithms based on your",
    "start": "268320",
    "end": "274800"
  },
  {
    "text": "requirements or better llms uh either so we we'll we'll discuss those few things as well here right now what is rag it's",
    "start": "274800",
    "end": "282479"
  },
  {
    "text": "basically a technique that enhances the capabilities of llm right so if you know llms are uh basically using set of data",
    "start": "282479",
    "end": "291000"
  },
  {
    "text": "uh and are trained on those specific data sets now if you want to use uh an",
    "start": "291000",
    "end": "296600"
  },
  {
    "text": "llm within your organization working on your data set right uh it will it doesn't understand because it's not",
    "start": "296600",
    "end": "302759"
  },
  {
    "text": "trained on your data set unless and you until you do a fine-tuning of those models using your data sets so so far I",
    "start": "302759",
    "end": "310400"
  },
  {
    "text": "I have spoken about three different terms right okay I have spoken about what rag uh with embeddings in it right",
    "start": "310400",
    "end": "316880"
  },
  {
    "text": "so that's where you actually split your your specific domain data and store it",
    "start": "316880",
    "end": "322440"
  },
  {
    "text": "in a vector database okay which is like embeddings uh second thing I spoke about",
    "start": "322440",
    "end": "327720"
  },
  {
    "text": "is uh fine-tuning right basically uh let's say you have a base",
    "start": "327720",
    "end": "333160"
  },
  {
    "text": "model or a base large language model you peel few of the upper uh layers from the",
    "start": "333160",
    "end": "340479"
  },
  {
    "text": "neural network create your own safe tensors out of that and basically you push that back into your uh S3 bucket",
    "start": "340479",
    "end": "347919"
  },
  {
    "text": "right and then you deploy your models using that that's fine tuning again in fine tuning you can use your data sets",
    "start": "347919",
    "end": "354440"
  },
  {
    "text": "uh for example uh we have something called um instruct lab now in instruct",
    "start": "354440",
    "end": "360240"
  },
  {
    "text": "lab you can provide you can do a Q&A right you can provide set of questions and answers to that and uh so it's like",
    "start": "360240",
    "end": "366599"
  },
  {
    "text": "let's say sample of five question and answers uh from your specific domain or 10 or 20 and then uh use a synthetic",
    "start": "366599",
    "end": "373440"
  },
  {
    "text": "data generator uh which can help you generate large amount of information based on those question sets and then",
    "start": "373440",
    "end": "379840"
  },
  {
    "text": "you can expose that uh to fine-tune your base model it can be a granite model it can be a llama model or or depending on",
    "start": "379840",
    "end": "386479"
  },
  {
    "text": "what you want to use for your specific domain set now this is what I'm talking about is file tuning where it actually",
    "start": "386479",
    "end": "392160"
  },
  {
    "text": "goes and and and add those uh specific uh tensors into your existing framework",
    "start": "392160",
    "end": "398240"
  },
  {
    "text": "right of the model and the third uh scenario which a lot of Enterprise would",
    "start": "398240",
    "end": "405280"
  },
  {
    "text": "like to avoid is to do a complete scratch uh training right of on your",
    "start": "405280",
    "end": "411080"
  },
  {
    "text": "specific data sets but uh so for example Lama uh model",
    "start": "411080",
    "end": "417080"
  },
  {
    "text": "right 80 billion parameters model so many big huge models are there right so to train those models it takes like lot",
    "start": "417080",
    "end": "423919"
  },
  {
    "text": "of CPUs and lot of resources and days right it's it's very costly Affair so that's why the fine tuning works okay",
    "start": "423919",
    "end": "430599"
  },
  {
    "text": "fine tuning also you cannot do like every day right if your data keeps changing and need to have a a chatboard",
    "start": "430599",
    "end": "437160"
  },
  {
    "text": "which reflects and responses to the data which is immediately like changed right okay for example today and I want to",
    "start": "437160",
    "end": "444160"
  },
  {
    "text": "have that information in my chat bar tomorrow it's the embedding is a good way to act actually push the data",
    "start": "444160",
    "end": "450879"
  },
  {
    "text": "everyday data or you can decide a frequency fine tuning is the next stage so you define decide or or Define when",
    "start": "450879",
    "end": "457360"
  },
  {
    "text": "you want to what frequency you want to do fine tuning and the training is is something if you cannot avoid okay try",
    "start": "457360",
    "end": "463080"
  },
  {
    "text": "to do a training also Define a frequency the cost matters right the time matters and and your resources and",
    "start": "463080",
    "end": "469560"
  },
  {
    "text": "the data scientist team also matters right and for for doing different type of",
    "start": "469560",
    "end": "475199"
  },
  {
    "text": "scenarios so that's that's the three different terms I wanted to Define basic basically embeddings fine tuning and",
    "start": "475199",
    "end": "482000"
  },
  {
    "text": "training um right now what we do we are talking about here is rag Ral augmented",
    "start": "482000",
    "end": "489000"
  },
  {
    "text": "generation right so it's basically it augments LM knowledge right with your",
    "start": "489000",
    "end": "494400"
  },
  {
    "text": "domain specific knowledge in your environment uh now if you see on the right side uh is that visible I think is",
    "start": "494400",
    "end": "501560"
  },
  {
    "text": "it visible at the end the picture okay cool great you guys have",
    "start": "501560",
    "end": "506919"
  },
  {
    "text": "good eyes I even for me it's not visible properly so uh let's see if this is working no",
    "start": "506919",
    "end": "515080"
  },
  {
    "text": "it's not so basically you uh I'm giving you chatbot example uh so you have a gradio which is a UI in in our case we",
    "start": "515080",
    "end": "522599"
  },
  {
    "text": "can do anything like we can do stimulate or any their application it goes uh basically you have a query now whatever",
    "start": "522599",
    "end": "529560"
  },
  {
    "text": "is your embeddings you stored in Vector database uh anyone uses Vector database",
    "start": "529560",
    "end": "534720"
  },
  {
    "text": "so far or rag okay um General Trend like do you guys if just show me hand if you",
    "start": "534720",
    "end": "540600"
  },
  {
    "text": "guys are using milis okay uh postgress with the vector",
    "start": "540600",
    "end": "545760"
  },
  {
    "text": "extensions oh many of them postp with any other like",
    "start": "545760",
    "end": "550920"
  },
  {
    "text": "redis okay cool in memory database redish right okay cool so uh so all",
    "start": "550920",
    "end": "559079"
  },
  {
    "text": "those things go into a vector database as embeddings uh and then um let's say you have a question the pro it goes into",
    "start": "559079",
    "end": "566320"
  },
  {
    "text": "prom template prom template is where you actually say okay you are you are a very sincere uh domain specific knowledge",
    "start": "566320",
    "end": "573640"
  },
  {
    "text": "expert to the llm and that guy will respond uh to you on that specific context right and then you provide your",
    "start": "573640",
    "end": "580200"
  },
  {
    "text": "embeddings so basically embeddings uh you have the syntax and the semantics",
    "start": "580200",
    "end": "585480"
  },
  {
    "text": "everything going into embeddings it also understands uh uh the it basically does",
    "start": "585480",
    "end": "592120"
  },
  {
    "text": "a similarity search based on what query you are having okay uh some of the embeddings use cosign search uh there",
    "start": "592120",
    "end": "598560"
  },
  {
    "text": "are different ways basically uh and you can use different types of embeddings uh that matters for depending what your",
    "start": "598560",
    "end": "605399"
  },
  {
    "text": "data set is so you need to be careful about selecting embeddings and how do you do text splitting whether the chunk",
    "start": "605399",
    "end": "611360"
  },
  {
    "text": "sizes are fine or not right okay and what is the best chunk size for your specific data sets so those things have to be defined before you actually move",
    "start": "611360",
    "end": "618279"
  },
  {
    "text": "your data into Vector database then uh once the data is there uh the vector the",
    "start": "618279",
    "end": "624279"
  },
  {
    "text": "embeddings uh the similarity search will be done based on what your query is from the the chatboard and what the vector",
    "start": "624279",
    "end": "630800"
  },
  {
    "text": "database data is coming out from that information along with the prompt template goes to llm and then you get an",
    "start": "630800",
    "end": "636079"
  },
  {
    "text": "answer based on your context or your domain specific information right so that's the basic architecture let me",
    "start": "636079",
    "end": "642639"
  },
  {
    "text": "move quickly again llms are powerful but their knowledge is limited to uh what",
    "start": "642639",
    "end": "648519"
  },
  {
    "text": "they trained on right so we had we use rack to uh set your uh get your domain",
    "start": "648519",
    "end": "653920"
  },
  {
    "text": "specific information again um some of the hallucination aspects are avoided uh",
    "start": "653920",
    "end": "659079"
  },
  {
    "text": "uh because of rag so that's also an important aspect which you need to uh",
    "start": "659079",
    "end": "664120"
  },
  {
    "text": "take care of uh there are other few few important",
    "start": "664120",
    "end": "670760"
  },
  {
    "text": "pointers here as well like rag is low cost compared to the timeconsuming L",
    "start": "670760",
    "end": "676279"
  },
  {
    "text": "models um so it it doesn't mean that I'm not saying that you shouldn't do training or tuning or uh embeddings",
    "start": "676279",
    "end": "682399"
  },
  {
    "text": "right you should basically figure out what is the best all three might be best approach for you okay but then Define on",
    "start": "682399",
    "end": "687839"
  },
  {
    "text": "the frequencies and the cost associated with that and the resources you have now this is how uh the first part Works",
    "start": "687839",
    "end": "694320"
  },
  {
    "text": "where basically you do your you load your data uh so you can actually take any documents or you can go and pull",
    "start": "694320",
    "end": "701399"
  },
  {
    "text": "data from websites right split your data that a lot of text Splitters available uh figure out what is the best Tex",
    "start": "701399",
    "end": "707800"
  },
  {
    "text": "spitter for your uh specific requirements Define the chunks okay store that in in a vector database",
    "start": "707800",
    "end": "714360"
  },
  {
    "text": "choice of vector database is also important right uh because over time your rag database Grows Right the vector",
    "start": "714360",
    "end": "720560"
  },
  {
    "text": "database so you you need to basically figure out how you want to do that and then uh this is a process right",
    "start": "720560",
    "end": "726399"
  },
  {
    "text": "basically load split embed and store and once you have this in Vector database so far there's nothing to do with LM right",
    "start": "726399",
    "end": "732760"
  },
  {
    "text": "you can just use Vector database to query and then get information right information right but it will be not in",
    "start": "732760",
    "end": "738000"
  },
  {
    "text": "a uh way which an llm is able to like generate and give it to you right it's",
    "start": "738000",
    "end": "743040"
  },
  {
    "text": "more of uh it will just do statistical analysis of the data which is which you",
    "start": "743040",
    "end": "748240"
  },
  {
    "text": "are passing as a query with what it has in Vector database with compare with some numbers using some uh specific uh",
    "start": "748240",
    "end": "755360"
  },
  {
    "text": "modeling techniques and then provide you outcome right now that goes as uh it",
    "start": "755360",
    "end": "760639"
  },
  {
    "text": "that is the information which is retrieved right in the next part uh remember the first diagram I showed so",
    "start": "760639",
    "end": "766360"
  },
  {
    "text": "what happens is when the question when there is an input question okay or or or a query which goes in uh basically it",
    "start": "766360",
    "end": "774000"
  },
  {
    "text": "goes into your vector database the data it it does it retries the information",
    "start": "774000",
    "end": "780120"
  },
  {
    "text": "gets to a pumpt and it that query goes to an llm and it generates an output answer right so that's that's a simple",
    "start": "780120",
    "end": "787079"
  },
  {
    "text": "high level architecture some of the business benefits uh these are Al these can also be use case in your",
    "start": "787079",
    "end": "792839"
  },
  {
    "text": "organizations right like content creation uh there is personalized recommendations um so recommendation",
    "start": "792839",
    "end": "799399"
  },
  {
    "text": "engine has already been there right for ages uh but that is more about predictive right but you can use that in combination with llm to do a generative",
    "start": "799399",
    "end": "807199"
  },
  {
    "text": "uh AI as well for your recommendations um to your internal folks or to your",
    "start": "807199",
    "end": "813040"
  },
  {
    "text": "customers uh then there's automated data analysis you can do efficient Knowledge Management very important uh a lot of uh",
    "start": "813040",
    "end": "820560"
  },
  {
    "text": "operations team they can actually push a lot of the logs with the results and the and the solutions they had right into",
    "start": "820560",
    "end": "827120"
  },
  {
    "text": "their uh Vector database and they can use an llm to query it okay so there is",
    "start": "827120",
    "end": "833320"
  },
  {
    "text": "an open shift light speed the project uh open source project again uh there uh we",
    "start": "833320",
    "end": "840480"
  },
  {
    "text": "are actually using we send the context let's say on kubernetes platform I have a chatbot",
    "start": "840480",
    "end": "847720"
  },
  {
    "text": "running it connects to my llm in the back end okay I can Define which llm I want to connect to and uh then I query",
    "start": "847720",
    "end": "856240"
  },
  {
    "text": "it send the Pod logs along with that send the Pod yaml status along with that and it that is the context it sets right",
    "start": "856240",
    "end": "863199"
  },
  {
    "text": "goes and queries the llm it has a it does Lama index and and has some local Vector database as well you can have",
    "start": "863199",
    "end": "869519"
  },
  {
    "text": "your own Vector database if you want to consistently store that and have your own knowledge base right and then you",
    "start": "869519",
    "end": "874800"
  },
  {
    "text": "get an uh get a response giving you an answer on what you need to do to fix that issue right on the specific P so",
    "start": "874800",
    "end": "881160"
  },
  {
    "text": "that's open shift light speed um if if you have time I'll show you that as well I have a tab on that um yeah and so",
    "start": "881160",
    "end": "890639"
  },
  {
    "text": "about open data Hub you go to open data hub.io so this is the project uh which",
    "start": "890639",
    "end": "896920"
  },
  {
    "text": "is an open source uh project basically what is open data Hub it's an AI platform designed for the hybrid Cloud",
    "start": "896920",
    "end": "903240"
  },
  {
    "text": "right uh I mean it works on open shift and open shift is like again works on hybrid Cloud right you can deploy open",
    "start": "903240",
    "end": "909480"
  },
  {
    "text": "shift anywhere so the same thing goes with open data Hub as well um it's B it helps provide you tools for your",
    "start": "909480",
    "end": "916519"
  },
  {
    "text": "training your serving as well as monitoring machine language uh learning models on site in public cloud or at",
    "start": "916519",
    "end": "923160"
  },
  {
    "text": "dedge right uh now the data scientist uh it it offers lot of things I can show",
    "start": "923160",
    "end": "928519"
  },
  {
    "text": "you the UI as well that will be better actually um uh but yeah it's meant for",
    "start": "928519",
    "end": "933639"
  },
  {
    "text": "the data scientist mlops Engineers they can collaborate on this do experimentation uh ensure that they",
    "start": "933639",
    "end": "939560"
  },
  {
    "text": "actually deploy the model in production here and then it scales right for them so there are run times like VM runtime",
    "start": "939560",
    "end": "946560"
  },
  {
    "text": "which we use here uh that's the runtime uh which uh Works along with so there are many run times which are supported",
    "start": "946560",
    "end": "952839"
  },
  {
    "text": "like there's an Intel open V run time which we support as well uh we support single model deployment on open data",
    "start": "952839",
    "end": "958800"
  },
  {
    "text": "have there is multimodel deployment as well depending on what models you're deploying generally llms very big right",
    "start": "958800",
    "end": "965560"
  },
  {
    "text": "most of the time you will in a single project in open shift you or in data science you will actually deploy a single",
    "start": "965560",
    "end": "971319"
  },
  {
    "text": "llm model uh it also helps take um to infuse",
    "start": "971319",
    "end": "977199"
  },
  {
    "text": "uh do infused AI infused applications right you can actually build a lot of them very quickly and uh and work with",
    "start": "977199",
    "end": "984440"
  },
  {
    "text": "the uh on the specific platform this is uh how it looks like",
    "start": "984440",
    "end": "990240"
  },
  {
    "text": "basically from the blocks uh if you see uh there's a hybrid crowd you have kubernetes then you have the open data",
    "start": "990240",
    "end": "996079"
  },
  {
    "text": "Hub operator uh which runs on kubernetes and that operator is responsible for deploying and maintaining all the",
    "start": "996079",
    "end": "1001800"
  },
  {
    "text": "components which are associated with open data Hub uh then on top of that you have monitoring alerting uh you also",
    "start": "1001800",
    "end": "1008240"
  },
  {
    "text": "have notebook controller which is mostly the workbench which the data scientist go and work on uh you will have model",
    "start": "1008240",
    "end": "1015000"
  },
  {
    "text": "serving and data science pipelines model serving is where you store your model and it can be served and um there is",
    "start": "1015000",
    "end": "1021880"
  },
  {
    "text": "data science pipeline this is predominantly focused on you as a data scientist or administrator can create",
    "start": "1021880",
    "end": "1027558"
  },
  {
    "text": "your own data science pipelines depending on what your workflow is whether you want to do data pre-processing whether you want to do uh",
    "start": "1027559",
    "end": "1033520"
  },
  {
    "text": "training whether you want to do model deployment or all sort of things right the complete pipeline",
    "start": "1033520",
    "end": "1039000"
  },
  {
    "text": "flow and then there is the is a very beautiful UI available uh for the dashboard and you can leverage storage",
    "start": "1039000",
    "end": "1045678"
  },
  {
    "text": "right and the for example we uh we can actually do open data Foundation here uh",
    "start": "1045679",
    "end": "1051520"
  },
  {
    "text": "as a storage which provides you with S3 which provides you with uh with block",
    "start": "1051520",
    "end": "1056799"
  },
  {
    "text": "device as well so depending on your persistent storage requirements you can actually uh do a set of PVCs on",
    "start": "1056799",
    "end": "1063200"
  },
  {
    "text": "kubernetes leveraging the open she data Foundation again that is uh based on Self Storage uh for the container",
    "start": "1063200",
    "end": "1070360"
  },
  {
    "text": "platforms right or the application platforms so that again is open source and available uh for you to actually",
    "start": "1070360",
    "end": "1077360"
  },
  {
    "text": "consume um uh anyone uses SE storage",
    "start": "1077360",
    "end": "1083120"
  },
  {
    "text": "here yeah okay cool there are a few nice to",
    "start": "1083120",
    "end": "1088640"
  },
  {
    "text": "see that right so uh that's about open data basically I wanted to show you a",
    "start": "1088640",
    "end": "1094240"
  },
  {
    "text": "basic rag implementation understand how the data injection happens okay and you",
    "start": "1094240",
    "end": "1099440"
  },
  {
    "text": "can use automation for the pipelines review how how the rag and llm can be used as an a assist with open shift",
    "start": "1099440",
    "end": "1106760"
  },
  {
    "text": "light speed uh so basically I'm talking about two things one is how you create a rag specific implementation and second",
    "start": "1106760",
    "end": "1113480"
  },
  {
    "text": "is if for a product company how you can actually embed uh uh these kind of systems right into your product Sals so",
    "start": "1113480",
    "end": "1120559"
  },
  {
    "text": "think from that those two different perspectives here uh and I have actually provided",
    "start": "1120559",
    "end": "1126880"
  },
  {
    "text": "there's a there's a Blog here uh which if you want how this specific thing is",
    "start": "1126880",
    "end": "1132120"
  },
  {
    "text": "implemented there's a detailed blog provided here then there is a deployment manifest which I have used to deploy",
    "start": "1132120",
    "end": "1137799"
  },
  {
    "text": "this specific environment which I showing you from the uh llm rack point of view so you can go to this uh if you",
    "start": "1137799",
    "end": "1144320"
  },
  {
    "text": "have open shift deployed with open data Hub you can actually um do an Argo",
    "start": "1144320",
    "end": "1149600"
  },
  {
    "text": "bootstrap and call this manifest and deploy this whole uh uh scenario which I'm describing here",
    "start": "1149600",
    "end": "1156400"
  },
  {
    "text": "right and it's all available um I mean you can just go and access it so this is the data science pipeline basically I",
    "start": "1156400",
    "end": "1162360"
  },
  {
    "text": "have simple pipeline uh do a data injection I query it I see the response time for that and then I sum the results",
    "start": "1162360",
    "end": "1169480"
  },
  {
    "text": "right that's the pipeline I have built here okay so so what happens right I",
    "start": "1169480",
    "end": "1175880"
  },
  {
    "text": "have this UI here I uh I think it is not yeah basically this is running using",
    "start": "1175880",
    "end": "1181840"
  },
  {
    "text": "gradio uh as a UI uh if you see here I am providing uh I'm asking a specific",
    "start": "1181840",
    "end": "1187440"
  },
  {
    "text": "question right provide location of cubec India now it doesn't actually give me an",
    "start": "1187440",
    "end": "1192720"
  },
  {
    "text": "answer because this guy is it doesn't have any context set right so what I do is I ingest the data",
    "start": "1192720",
    "end": "1199159"
  },
  {
    "text": "there's other llm which I'm running I inest the data has the same question right basically it is now able to tell",
    "start": "1199159",
    "end": "1205799"
  },
  {
    "text": "me as where it is and actually it gives me a link as well so that's how I",
    "start": "1205799",
    "end": "1211400"
  },
  {
    "text": "created this information uh using uh data injection right so basically I'm",
    "start": "1211400",
    "end": "1218240"
  },
  {
    "text": "providing the same query again here right and it kind of like gives me a",
    "start": "1218240",
    "end": "1224320"
  },
  {
    "text": "good result now if you see here this is uh my workbench",
    "start": "1224320",
    "end": "1230159"
  },
  {
    "text": "right okay I'm using U Lang chain here again",
    "start": "1230159",
    "end": "1236840"
  },
  {
    "text": "uh I'm using embeddings Vector store uh for storing I'm using a PG Vector database here so let me restart this I",
    "start": "1236840",
    "end": "1245679"
  },
  {
    "text": "think sorry I can't hear",
    "start": "1249559",
    "end": "1253320"
  },
  {
    "text": "you provide yeah I created what I did I went to cuon I created a PDF file and",
    "start": "1255120",
    "end": "1260960"
  },
  {
    "text": "then I pushed that PDF file along with the link I will show you that in the in the",
    "start": "1260960",
    "end": "1266159"
  },
  {
    "text": "workbench",
    "start": "1266159",
    "end": "1269159"
  },
  {
    "text": "um so this is my open data Hub basically right um it's the same interface uh so I",
    "start": "1277279",
    "end": "1283360"
  },
  {
    "text": "have this open shift data science pipeline uh projects here and then each project will have",
    "start": "1283360",
    "end": "1289039"
  },
  {
    "text": "multiple scenarios right for example yeah so it has data sence",
    "start": "1289039",
    "end": "1296200"
  },
  {
    "text": "pipeline model serving then there are a lot of things you can have from The Notebook images accelerator profiles",
    "start": "1296200",
    "end": "1302000"
  },
  {
    "text": "serving run times and things like that right",
    "start": "1302000",
    "end": "1307600"
  },
  {
    "text": "um yes you can do elastic as well I'm just opening this guys",
    "start": "1308720",
    "end": "1317880"
  },
  {
    "text": "so while it's coming up uh there is another scenario where if I in the Jupiter Hub so this is my Jupiter Hub",
    "start": "1325799",
    "end": "1332360"
  },
  {
    "text": "workbench basically I can go here and uh what I do is I'll let's say in this case",
    "start": "1332360",
    "end": "1338640"
  },
  {
    "text": "I want to uh provide a product specific version information right so I connect with my Vector database here uh I will",
    "start": "1338640",
    "end": "1346720"
  },
  {
    "text": "I'll set up the the database uh collection name here from the vector database point of view call in the M my",
    "start": "1346720",
    "end": "1352919"
  },
  {
    "text": "Lenin uh specific framework uh libraries um this is where I'm actually",
    "start": "1352919",
    "end": "1358320"
  },
  {
    "text": "passing the PDF information okay from I can either pull it from the website or I can store it locally and then it will",
    "start": "1358320",
    "end": "1364640"
  },
  {
    "text": "pull it locally from that right okay then I'm also having PDFs to URLs uh basically I'm using this to download all",
    "start": "1364640",
    "end": "1372120"
  },
  {
    "text": "the uh files and store it locally uh I'm setting up my websites here uh this is",
    "start": "1372120",
    "end": "1378080"
  },
  {
    "text": "more interesting part right basically I'm using recursive text character text splitter here to split the uh the PDF",
    "start": "1378080",
    "end": "1384600"
  },
  {
    "text": "document or whatever I have I Define the chunk size and Define the chunk overlaps right chunk overlap is like out of one24",
    "start": "1384600",
    "end": "1391440"
  },
  {
    "text": "characters 40 will be overlapped right uh with the next chunk so that's what the overlap size is in this case and uh",
    "start": "1391440",
    "end": "1400080"
  },
  {
    "text": "then basically I I I split that and uh this is what it shows me right as the split information um the second part is",
    "start": "1400080",
    "end": "1407919"
  },
  {
    "text": "the hugging face embeddings is what I'm using here so there are many embeddings here uh there is actually uh where is",
    "start": "1407919",
    "end": "1416200"
  },
  {
    "text": "that right uh there is a nice",
    "start": "1416200",
    "end": "1423600"
  },
  {
    "text": "uh yeah this is where you can actually go to mte that is massive text embedding",
    "start": "1425640",
    "end": "1431520"
  },
  {
    "text": "benchmarks okay it it lists all the uh uh embedding models and basically their",
    "start": "1431520",
    "end": "1436720"
  },
  {
    "text": "benchmarks uh for the one which in our case we should actually see the retrieval",
    "start": "1436720",
    "end": "1442480"
  },
  {
    "text": "uh there is this retrieval average okay this is what we should see from this specific use case point of",
    "start": "1442480",
    "end": "1449520"
  },
  {
    "text": "view okay and once we do that we actually",
    "start": "1449520",
    "end": "1456039"
  },
  {
    "text": "move all that those embeddings into the database okay and then query it right so it basically gets into and then we can",
    "start": "1456039",
    "end": "1462600"
  },
  {
    "text": "have similarity search and those kind of uh specific parameters set here",
    "start": "1462600",
    "end": "1469278"
  },
  {
    "text": "right now so this is my uh jupyter Hub notebook right now imagine I have like",
    "start": "1469440",
    "end": "1476480"
  },
  {
    "text": "there is this is inje then there is a query there is response time which I showed you right so basically I can go",
    "start": "1476480",
    "end": "1481720"
  },
  {
    "text": "here this is my alira notebook uh elira editor basically I can drag I can just",
    "start": "1481720",
    "end": "1487440"
  },
  {
    "text": "drop these programs in here connect with them specify the input and output and",
    "start": "1487440",
    "end": "1492640"
  },
  {
    "text": "once I just say uh once I run this right here okay using using this it will go",
    "start": "1492640",
    "end": "1499000"
  },
  {
    "text": "and basically go to a specific Pipeline and then run I already have this",
    "start": "1499000",
    "end": "1504360"
  },
  {
    "text": "executed here okay as the pipelines so if you see here these are all so there are two ways either I can do it through",
    "start": "1504360",
    "end": "1510720"
  },
  {
    "text": "alira so it shows me right okay as a data scientist that's what you can do actually okay and then you don't have to",
    "start": "1510720",
    "end": "1516159"
  },
  {
    "text": "worry about how the pipeline gets created on on the kubernetes platform scales and and runs and executes you",
    "start": "1516159",
    "end": "1521720"
  },
  {
    "text": "just have to worry about the outcome right for that then there is another way where you actually can import this pipeline so I can actually import say uh",
    "start": "1521720",
    "end": "1529159"
  },
  {
    "text": "test sanity check and then I can go in here then I say okay this is what I'm importing right and then import so this",
    "start": "1529159",
    "end": "1536360"
  },
  {
    "text": "will actually create this import uh data injection Pipeline and then I can go and view my runs as well here okay whether",
    "start": "1536360",
    "end": "1542880"
  },
  {
    "text": "it has completed or",
    "start": "1542880",
    "end": "1545840"
  },
  {
    "text": "not yes see it it status was green here for that now uh there is another aspect",
    "start": "1549120",
    "end": "1555520"
  },
  {
    "text": "to that U which I wanted to show you",
    "start": "1555520",
    "end": "1561640"
  },
  {
    "text": "yeah was that here in the data science pipeline right if you go here uh in the",
    "start": "1561640",
    "end": "1568799"
  },
  {
    "text": "edit so it actually supports two different uh pipelines",
    "start": "1568799",
    "end": "1574760"
  },
  {
    "text": "here it either supports arof flow here or it supports tecton so the these are",
    "start": "1575159",
    "end": "1582039"
  },
  {
    "text": "the two pipelines which it supports uh data science pipelines so you can use",
    "start": "1582039",
    "end": "1587120"
  },
  {
    "text": "any of this basically right either Argo or tecton so in the latest version the flow the move",
    "start": "1587120",
    "end": "1592520"
  },
  {
    "text": "movement is the so the recommendation is to go and work through arof flow but you can still use tectone as well so both",
    "start": "1592520",
    "end": "1598840"
  },
  {
    "text": "are available for now and now uh what we are doing I think I'm almost through",
    "start": "1598840",
    "end": "1604799"
  },
  {
    "text": "with this so so this is my open uh shift light",
    "start": "1604799",
    "end": "1610399"
  },
  {
    "text": "speed architecture uh so that was about the llm and how it sets sorry I want to show",
    "start": "1610399",
    "end": "1616919"
  },
  {
    "text": "more in terms of How It's deployed but because of the time I couldn't do it so this is our open shiet Virtual assist uh",
    "start": "1616919",
    "end": "1622279"
  },
  {
    "text": "leveraging the rag with llm so you see there is a rag database which can be Associated there's an llm uh which is",
    "start": "1622279",
    "end": "1629320"
  },
  {
    "text": "there which is outside of your open shift light speed right and it just has connectivity with your backend llm and",
    "start": "1629320",
    "end": "1635640"
  },
  {
    "text": "um then it goes into a corporate VPN where you have more information about how the things are from your specific",
    "start": "1635640",
    "end": "1642039"
  },
  {
    "text": "environment point of view there open Shi virtual assistant basically it helps you ease your user productivity increases",
    "start": "1642039",
    "end": "1648279"
  },
  {
    "text": "use e ease of use increase productivity and generate UI so this you can use it in your open shift environment right and",
    "start": "1648279",
    "end": "1655559"
  },
  {
    "text": "uh this is how basically it looks like where is that yeah so I I have this",
    "start": "1655559",
    "end": "1662440"
  },
  {
    "text": "broken pod here right this prod is not working in my open shift environment so basically I can say okay ask open shift",
    "start": "1662440",
    "end": "1669039"
  },
  {
    "text": "light speed so it will go to this chat board and I'm saying okay what is broken in this pod and please provide a fix for",
    "start": "1669039",
    "end": "1675919"
  },
  {
    "text": "it right I am sending it a context so it goes and sends a yaml status to",
    "start": "1675919",
    "end": "1681279"
  },
  {
    "text": "this right I can actually send uh additional information like I can send uh logs as well to that right through",
    "start": "1681279",
    "end": "1688880"
  },
  {
    "text": "this logs events yam yam stators and it will actually go and give you a",
    "start": "1688880",
    "end": "1694760"
  },
  {
    "text": "fantastic output as well as figure out so in this case basically it's saying why the prod is broken uh and then it",
    "start": "1694760",
    "end": "1702159"
  },
  {
    "text": "says okay what you need to do to fix the issue right so this is how you can use oh assistant right AI Bas assistant",
    "start": "1702159",
    "end": "1710200"
  },
  {
    "text": "to actually help you uh fix your issues in your open shift environment right and",
    "start": "1710200",
    "end": "1715399"
  },
  {
    "text": "so yeah that's that's what I wanted to show here as part of my presentation for",
    "start": "1715399",
    "end": "1721760"
  },
  {
    "text": "now and um yeah I think we have one more minute for query or question I think",
    "start": "1721760",
    "end": "1729000"
  },
  {
    "text": "four more minutes uh okay we start with your",
    "start": "1729000",
    "end": "1737679"
  },
  {
    "text": "oh what you can do is you can have a rag database associated with that okay and you can store that as embeddings in rag",
    "start": "1752840",
    "end": "1758880"
  },
  {
    "text": "database and then you configure uh in such a way that it will go and pull the context from the rag database along with",
    "start": "1758880",
    "end": "1765880"
  },
  {
    "text": "apprach same approach would matter yeah because we have a support it works with rag as well",
    "start": "1765880",
    "end": "1772398"
  },
  {
    "text": "yes you can uh what do you mean multiple emings",
    "start": "1772519",
    "end": "1779440"
  },
  {
    "text": "is yes yes so lock file here in this case I'm sending it through the context",
    "start": "1782679",
    "end": "1788320"
  },
  {
    "text": "itself okay and that becomes a a chat query and then I am actually um along",
    "start": "1788320",
    "end": "1794320"
  },
  {
    "text": "with that you can actually do a rag as well okay is what I'm thinking thinking of um sorry",
    "start": "1794320",
    "end": "1802840"
  },
  {
    "text": "yeah um here uh when you select a specific embedding okay and you you push",
    "start": "1807799",
    "end": "1814600"
  },
  {
    "text": "it into Vector database it will do indexing for you yes automatic but you can if you want to have proper indexing",
    "start": "1814600",
    "end": "1821720"
  },
  {
    "text": "right you can use your own indexing like Lama index you can use or some other mechanisms you can use to have proper indexing done as well",
    "start": "1821720",
    "end": "1829360"
  },
  {
    "text": "yeah yeah sorry what was your logic behind chunk overlap and",
    "start": "1829360",
    "end": "1834519"
  },
  {
    "text": "chunk why there's no logic in this case you need to basically try it out in your",
    "start": "1834519",
    "end": "1840080"
  },
  {
    "text": "environment and see what is the best suitable uh chunk size and the overlap size for your specific",
    "start": "1840080",
    "end": "1846080"
  },
  {
    "text": "data yeah",
    "start": "1846080",
    "end": "1849320"
  },
  {
    "text": "you yes no actually they are very different it's",
    "start": "1853720",
    "end": "1859760"
  },
  {
    "text": "a platform AI platform whereas Nim you can actually deploy it on open data Hub okay as a uh uh as a runtime so we have",
    "start": "1859760",
    "end": "1867919"
  },
  {
    "text": "different single model deployment multimodel deployment you can have a name deployment also in that okay same way and it has a runtime as well and",
    "start": "1867919",
    "end": "1874639"
  },
  {
    "text": "then in that you can actually U select when you deploy do the Nim you can actually enable Nim if you have",
    "start": "1874639",
    "end": "1880559"
  },
  {
    "text": "Enterprise license on this and you can go and uh download your new model uh",
    "start": "1880559",
    "end": "1885799"
  },
  {
    "text": "whatever model is supported in that name or if you don't want to do this you can just do a Nim deployment on open shift",
    "start": "1885799",
    "end": "1891559"
  },
  {
    "text": "basically either of that is possible sorry any other",
    "start": "1891559",
    "end": "1897159"
  },
  {
    "text": "questions yeah sorry I'll take I can't hear",
    "start": "1897159",
    "end": "1904279"
  },
  {
    "text": "you how are you doing similarity check that is actually",
    "start": "1904279",
    "end": "1910360"
  },
  {
    "text": "being done by the same uh embedding right the hugging face uh embedding which I use to push the data into Vector",
    "start": "1910360",
    "end": "1916880"
  },
  {
    "text": "I use the same okay when I'm doing uh extracting so basically whatever is the logic in that specific uh embedding uh",
    "start": "1916880",
    "end": "1924840"
  },
  {
    "text": "model it will actually use that logic okay so most of them I think some of them use cosine similarities okay uh yes",
    "start": "1924840",
    "end": "1933279"
  },
  {
    "text": "this is very in yes uh right now all the organizations",
    "start": "1933279",
    "end": "1940240"
  },
  {
    "text": "are trying to figure out how to basically push the data into Vector database and then try and see because uh",
    "start": "1940240",
    "end": "1948559"
  },
  {
    "text": "uh just an llm will not help so you need to have a rack database build a rag database that should be your own IP",
    "start": "1948559",
    "end": "1953880"
  },
  {
    "text": "basically okay and then uh play around basically with that okay that's the best way to",
    "start": "1953880",
    "end": "1960480"
  },
  {
    "text": "DOA yes yeah people are exploring that lot of customer exploring Enterprise are",
    "start": "1962159",
    "end": "1968360"
  },
  {
    "text": "exploring that yeah okay but this is the platform which can help actually do a lot of things for you that from that",
    "start": "1968360",
    "end": "1974200"
  },
  {
    "text": "point of view uh okay yes last question",
    "start": "1974200",
    "end": "1979840"
  },
  {
    "text": "okay so I actually showed you two different environments one was running with CU flow one was running with tecton",
    "start": "1984840",
    "end": "1991600"
  },
  {
    "text": "okay the older version has only tecton the newer one has both Q flow and",
    "start": "1991600",
    "end": "1997200"
  },
  {
    "text": "tectone I can have it yeah either of that okay uh I frankly I I'm still need",
    "start": "1997840",
    "end": "2004360"
  },
  {
    "text": "to figure out the differences between what will be better in what scenarios",
    "start": "2004360",
    "end": "2009639"
  },
  {
    "text": "okay generally I see the mlops flow from the app Dev point of view tecton is",
    "start": "2009639",
    "end": "2014960"
  },
  {
    "text": "better from the data science uh data science pipeline point of view uh the",
    "start": "2014960",
    "end": "2020360"
  },
  {
    "text": "Argo flow has lot of uh Point lot of scenarios which actually helps to",
    "start": "2020360",
    "end": "2026519"
  },
  {
    "text": "fine-tune or train or experiment those kind of things okay it it has very good uh integration with that",
    "start": "2026519",
    "end": "2035279"
  },
  {
    "text": "yeah so Argo flow is part of Argo flow basically you can Define the services in",
    "start": "2041399",
    "end": "2046799"
  },
  {
    "text": "open data Hub and those services will be enabled on your open shift if you have those operators installed like tekon",
    "start": "2046799",
    "end": "2053480"
  },
  {
    "text": "okay or Argo those kind of things okay I think time is over and we",
    "start": "2053480",
    "end": "2060000"
  },
  {
    "text": "need to like clear off the stage at least for myself we can do an offline discussion yeah thank you for joining",
    "start": "2060000",
    "end": "2066118"
  },
  {
    "text": "have a good day than",
    "start": "2066119",
    "end": "2070519"
  }
]