[
  {
    "text": "hey friends thanks for joining me for my talk my name is lee kapili i live in colorado",
    "start": "320",
    "end": "6799"
  },
  {
    "text": "enjoy parkour my dog's name is pepsi and i'm a filipino-american i",
    "start": "6799",
    "end": "13679"
  },
  {
    "text": "participate a lot with sick cluster life cycle in the kubernetes development groups if i",
    "start": "13679",
    "end": "18880"
  },
  {
    "text": "know you from there thanks so much for coming to support the talk and come and learn with me if i don't",
    "start": "18880",
    "end": "24720"
  },
  {
    "text": "know you yet please come and say hi my dms are open on twitter or you can hit me up on the kubernetes slack",
    "start": "24720",
    "end": "31119"
  },
  {
    "text": "i work with the developer developer experience team with weave works very lucky to be uh",
    "start": "31119",
    "end": "37520"
  },
  {
    "text": "friends when teammates with such wonderful people were the primary contributors to the",
    "start": "37520",
    "end": "43120"
  },
  {
    "text": "flux cd project but flexibility project does have a",
    "start": "43120",
    "end": "48320"
  },
  {
    "text": "open governance model so please come and build the best get ops tooling available with us we are helping people",
    "start": "48320",
    "end": "56160"
  },
  {
    "text": "adopt flux 2 lately there's a new set of apis and c or d driven controllers that",
    "start": "56160",
    "end": "62399"
  },
  {
    "text": "do a modular split of fetching and sync and apply to the cluster if you want to learn more about",
    "start": "62399",
    "end": "67760"
  },
  {
    "text": "implementing git ops from a cultural perspective with your teams and with your organization come check out our get",
    "start": "67760",
    "end": "73520"
  },
  {
    "text": "ops community website today i'm happy to be discussing some",
    "start": "73520",
    "end": "79200"
  },
  {
    "text": "strategies for multi-cluster routing and networking and before we just get into routing packets",
    "start": "79200",
    "end": "85040"
  },
  {
    "text": "we want to talk a little bit about rational for multiple clusters one of them might be workload proximity",
    "start": "85040",
    "end": "91200"
  },
  {
    "text": "say you're a break dancing apparel brand and you have huge user bases in korea and france you might want to",
    "start": "91200",
    "end": "97119"
  },
  {
    "text": "run some services for your webstore in those regions to reduce latency and improve reliability",
    "start": "97119",
    "end": "102880"
  },
  {
    "text": "and also maybe there's specific services to korea that aren't relevant in france similarly if you're",
    "start": "102880",
    "end": "108640"
  },
  {
    "text": "in that situation you might want to separate your failure domains so that say your cluster is failing in france",
    "start": "108640",
    "end": "114960"
  },
  {
    "text": "you can still do the special services to your users and your uh your fans and",
    "start": "114960",
    "end": "120079"
  },
  {
    "text": "stuff in korea another thing that we see often with kubernetes is like a desire to",
    "start": "120079",
    "end": "125520"
  },
  {
    "text": "split up compute so that the application is segregated to particular nodes",
    "start": "125520",
    "end": "130800"
  },
  {
    "text": "and you can do this with the kubernetes api but it's kind of complicated you have to use a combination of our",
    "start": "130800",
    "end": "137040"
  },
  {
    "text": "back name spaces network policy and the plot node selector admission",
    "start": "137040",
    "end": "142480"
  },
  {
    "text": "controller to decide that different name spaces can only schedule to subsets of label node",
    "start": "142480",
    "end": "150080"
  },
  {
    "text": "selectors so if you've got certain nodes labeled low latency that could be like say the entire",
    "start": "150080",
    "end": "155760"
  },
  {
    "text": "cluster or half of it but then a portion of that could be ephemeral and then others",
    "start": "155760",
    "end": "161200"
  },
  {
    "text": "could have a node pool for persistent storage you could slice this up in any number of ways using multiple name namespaces and the",
    "start": "161200",
    "end": "168160"
  },
  {
    "text": "admission controller for it super cool very flexible it's more flexible than the alternative which is",
    "start": "168160",
    "end": "174160"
  },
  {
    "text": "just to split up into two different control planes and separate clusters and give teams full access to that but",
    "start": "174160",
    "end": "180879"
  },
  {
    "text": "one is easier than the other arguably so this is the common reason that people will split uh their compute makes it",
    "start": "180879",
    "end": "188400"
  },
  {
    "text": "really simple and often those control planes are free so um one area where you start to hit",
    "start": "188400",
    "end": "193840"
  },
  {
    "text": "some not just complexity but actual constraints of the kubernetes api is with non-name spaceable objects when",
    "start": "193840",
    "end": "200159"
  },
  {
    "text": "you modify the api server with a custom resource definition then that's not something that can",
    "start": "200159",
    "end": "206000"
  },
  {
    "text": "easily be named spaced multiple people are going to be stepping on each other if you allow them to write crds potentially and",
    "start": "206000",
    "end": "212640"
  },
  {
    "text": "in addition some you know controller implementations might be using an api that doesn't",
    "start": "212640",
    "end": "217760"
  },
  {
    "text": "uh tenantize in a very secure way so you might be inspired to separate your cluster boundaries",
    "start": "217760",
    "end": "223760"
  },
  {
    "text": "uh beyond technical reasons maybe you just have social billing reporting reasons organizational",
    "start": "223760",
    "end": "229040"
  },
  {
    "text": "reasons uh one team might have more resources than another and one team might be incredibly",
    "start": "229040",
    "end": "234640"
  },
  {
    "text": "disenfranchised because of the latest reorganization um some other miscellaneous reasons that",
    "start": "234640",
    "end": "240080"
  },
  {
    "text": "i could think of maybe you're trying to use some novel services uh some novel features from a",
    "start": "240080",
    "end": "245120"
  },
  {
    "text": "kubernetes service provider say one cast provider does edge really well and one gives you secure enclaves",
    "start": "245120",
    "end": "251120"
  },
  {
    "text": "there's also hybrid cloud environments or you might be doing a migration in a hybrid cloud environment",
    "start": "251120",
    "end": "256799"
  },
  {
    "text": "need multiple clusters for that i've been in that situation before business to business networking say your",
    "start": "256799",
    "end": "262560"
  },
  {
    "text": "cluster you have this business relationship with another company that has a cluster with an",
    "start": "262560",
    "end": "268240"
  },
  {
    "text": "object store in it there's some valuable stuff in there that you have contractual access to how do you open up the network paths to",
    "start": "268240",
    "end": "273600"
  },
  {
    "text": "access those objects that object store over the network from your cluster",
    "start": "273600",
    "end": "278800"
  },
  {
    "text": "and then there's mergers and acquisitions which would naturally produce you know the need for um networking",
    "start": "278800",
    "end": "285120"
  },
  {
    "text": "between multiple distributed computers like kubernetes so you end up in this world you know there's like lots of little and big",
    "start": "285120",
    "end": "290560"
  },
  {
    "text": "clusters some of them are shared by multiple people others are running a single application 1000 times",
    "start": "290560",
    "end": "296080"
  },
  {
    "text": "in a bunch of retail stores or on trains and so you inevitably need to be routing",
    "start": "296080",
    "end": "302639"
  },
  {
    "text": "some packets between some of these clusters and the problem is that the basic unit of compute inside of kubernetes",
    "start": "302639",
    "end": "309199"
  },
  {
    "text": "you get an individual ip address for each one of these pods and so we have this service abstraction",
    "start": "309199",
    "end": "315919"
  },
  {
    "text": "that allows us to label select those pods and then keep an endpoint list up to date then other things can just watch the",
    "start": "315919",
    "end": "322320"
  },
  {
    "text": "endpoint list right so things like nodes cloud provider load balancers android's controllers they can stay up",
    "start": "322320",
    "end": "328400"
  },
  {
    "text": "to date with what the back ends are so that way when they become available they can get removed from the endpoint list and then you get a new pod with a brand",
    "start": "328400",
    "end": "334800"
  },
  {
    "text": "new ip address and as soon as it becomes ready then it gets added into the endpoint list but the problem is that those things",
    "start": "334800",
    "end": "341919"
  },
  {
    "text": "aren't the actual ip addresses in the cluster it's not just the pod ip addresses that matter",
    "start": "341919",
    "end": "347440"
  },
  {
    "text": "services have virtual ips nodes you might have a couple hundred of those they might have a bunch of mix there",
    "start": "347440",
    "end": "353759"
  },
  {
    "text": "might be multiple node pools with different configurations uh the cloud provider might be producing ip addresses for",
    "start": "353759",
    "end": "359919"
  },
  {
    "text": "multiple load balancers some of them being public on the internet and some of them being ip addresses in the vpc",
    "start": "359919",
    "end": "365680"
  },
  {
    "text": "your ingress controller may be fronted by an ip forwarding rule or some server inside of your cloud",
    "start": "365680",
    "end": "372400"
  },
  {
    "text": "provider's control plane or you know say you're using a bare metal solution that provides an ingress",
    "start": "372400",
    "end": "377600"
  },
  {
    "text": "controller ip from a node port powered node then the ip addresses are completely different",
    "start": "377600",
    "end": "383759"
  },
  {
    "text": "and it becomes you know a little bit difficult to track all of this ip information you know you're still declaring it but",
    "start": "383759",
    "end": "390000"
  },
  {
    "text": "it gets dynamically created by whatever the state of the environment is or however it was provisioned",
    "start": "390000",
    "end": "395440"
  },
  {
    "text": "and this is a lot of network turned to track so it's cool that oh sorry i forgot",
    "start": "395440",
    "end": "401680"
  },
  {
    "text": "about port maps between each of the ip address paths here there's also a potential for a",
    "start": "401680",
    "end": "408400"
  },
  {
    "text": "bunch of port mapping to occur so there's even more information that you're going to have to deal with and so the kubernetes api",
    "start": "408400",
    "end": "415199"
  },
  {
    "text": "lets you kind of encapsulate all of this network drift but then you have multiple clusters how do you resolve the drift between two",
    "start": "415199",
    "end": "422639"
  },
  {
    "text": "clusters say and your data center where you want your legacy apps to be able to access the services that you're now hosting",
    "start": "422639",
    "end": "428639"
  },
  {
    "text": "you know you have this really modern fast service discovery infrastructure inside of the cluster but",
    "start": "428639",
    "end": "434319"
  },
  {
    "text": "none of the rest of your holder infrastructure can keep up and even between multiple clusters now you have this problem like",
    "start": "434319",
    "end": "439840"
  },
  {
    "text": "how do you get things to talk to each other one kind of beginning solution the first",
    "start": "439840",
    "end": "445440"
  },
  {
    "text": "thing that people will be introduced to is to use something like service type node port or service type load balancer",
    "start": "445440",
    "end": "451840"
  },
  {
    "text": "this is a layer 4 abstraction that allows us to basically get a single ip address or ipv port",
    "start": "451840",
    "end": "459199"
  },
  {
    "text": "combo to map that forward or proxy traffic",
    "start": "459199",
    "end": "464400"
  },
  {
    "text": "to service virtual yps on each of the edge routers the nodes inside of the kubernetes cluster",
    "start": "464400",
    "end": "470479"
  },
  {
    "text": "and so basically we get a map of the service vip to either node ips or something that's provisioned by your cloud provider",
    "start": "470479",
    "end": "476319"
  },
  {
    "text": "or other tool set and then with that ip address if you're looking for a more declarative configuration",
    "start": "476319",
    "end": "482639"
  },
  {
    "text": "you can use controllers like external dns or cert manager to then get stable naming",
    "start": "482639",
    "end": "490400"
  },
  {
    "text": "and tls identities to mount within the workloads for those pawns",
    "start": "490400",
    "end": "496319"
  },
  {
    "text": "here we can see okay well if we spin up a load balancer that's the yellow thing inside of the bottom cluster",
    "start": "496639",
    "end": "502080"
  },
  {
    "text": "and we're able to reach out to it from those green back-end plots as well as from the legacy workloads or whatever running",
    "start": "502080",
    "end": "508560"
  },
  {
    "text": "inside of a data center or other infrastructure now reaching beyond just a single",
    "start": "508560",
    "end": "516159"
  },
  {
    "text": "service because load balancers and node ports they give us a single service abstraction ingress controllers allow us",
    "start": "516159",
    "end": "522959"
  },
  {
    "text": "to potentially route to many services and it does this because it's a reverse proxy",
    "start": "522959",
    "end": "529519"
  },
  {
    "text": "it's an api that describes how a reverse proxy should behave so then you have these ingress controllers implementing the ingress api",
    "start": "529519",
    "end": "536000"
  },
  {
    "text": "and we can route one network identity to many service backends based off of the content of the protocol that the client",
    "start": "536000",
    "end": "542880"
  },
  {
    "text": "is speaking so this allows us to do one-to-one and one-to-many setups i mean that in terms",
    "start": "542880",
    "end": "548880"
  },
  {
    "text": "of namespaces and ingress objects sometimes you can have a single ingress",
    "start": "548880",
    "end": "554800"
  },
  {
    "text": "network identity hooked up to a single ingress object in one namespace routing multiple",
    "start": "554800",
    "end": "560959"
  },
  {
    "text": "services in that name space other times you can collect multiple ingress objects from across all of the name",
    "start": "560959",
    "end": "567760"
  },
  {
    "text": "spaces in the cluster and have it link up to the same ingress controller it's primarily layer 7 abstraction but",
    "start": "567760",
    "end": "574560"
  },
  {
    "text": "some ingress controllers let you do layer four stuff at the same time and um it's really",
    "start": "574560",
    "end": "579839"
  },
  {
    "text": "important to note that these one-to-many setups that difference that i was talking about allow you to really reduce the external",
    "start": "579839",
    "end": "585680"
  },
  {
    "text": "network control or network churn that happens in terms of ips outside of your cluster so with",
    "start": "585680",
    "end": "593600"
  },
  {
    "text": "cloud provider ingresses you usually get to expose like some kind of internet facing or vpc accessible ip",
    "start": "593600",
    "end": "600880"
  },
  {
    "text": "address and it's usually per ingress so you can still route to multiple services within the same name space",
    "start": "600880",
    "end": "607440"
  },
  {
    "text": "but this tends to be a constraint of their implementation you can't just like route to everything inside of your cluster a pretty good design decision",
    "start": "607440",
    "end": "614880"
  },
  {
    "text": "but also that's a serious constraint that might not fit your use case and",
    "start": "614880",
    "end": "621200"
  },
  {
    "text": "these things are often very powerful because they're usually in some kind of horizontally scaled cloud provider",
    "start": "621200",
    "end": "627040"
  },
  {
    "text": "infrastructure but that can also make them incredibly expensive which can make them a prohibitive solution if you're trying",
    "start": "627040",
    "end": "633120"
  },
  {
    "text": "to just do low volume inter cluster traffic or something like that so if you find that that's not a good solution for you you might look at maybe",
    "start": "633120",
    "end": "639839"
  },
  {
    "text": "hosting your own ingress controller inside of your cluster there are a lot of third-party solutions",
    "start": "639839",
    "end": "645040"
  },
  {
    "text": "for this like contour traffic you know etc and genex ingress is so popular",
    "start": "645040",
    "end": "650800"
  },
  {
    "text": "and if you deploy these you often get a one-to-many setup so if you have a single ingress",
    "start": "650800",
    "end": "657279"
  },
  {
    "text": "controller all of the ingress objects could potentially set that ingress class and get",
    "start": "657279",
    "end": "662880"
  },
  {
    "text": "aggregated to that single reverse proxy and this composes really well you know",
    "start": "662880",
    "end": "668880"
  },
  {
    "text": "behind something like the layer 4 load balancer solution or a node port and you can start meshing",
    "start": "668880",
    "end": "675120"
  },
  {
    "text": "in between these ingresses and do some pretty interesting network topologies for declarative",
    "start": "675120",
    "end": "680240"
  },
  {
    "text": "configuration we use the same external dns insert manager to get stable dns",
    "start": "680240",
    "end": "685600"
  },
  {
    "text": "and cert manager actually gives us a bonus win here because the ingress api supports tls",
    "start": "685600",
    "end": "690640"
  },
  {
    "text": "termination which allows us to decouple tls certificates from the application",
    "start": "690640",
    "end": "696240"
  },
  {
    "text": "workloads you no longer have to restart your apps if you say want to change your domain name or add one or something like that or even just rotate the circuit",
    "start": "696240",
    "end": "703040"
  },
  {
    "text": "for automating the tls certs uh it can really help to use a wildcard dns plus the tls certificates this will",
    "start": "703040",
    "end": "710320"
  },
  {
    "text": "lower your deploy latency because then you don't have to constantly be requesting new tls",
    "start": "710320",
    "end": "715760"
  },
  {
    "text": "certificates and creating new dns records if you have a dns zone that's api",
    "start": "715760",
    "end": "722079"
  },
  {
    "text": "enabled this is ideal because lets you create the dns record and then also you can do something like acme v2",
    "start": "722079",
    "end": "729279"
  },
  {
    "text": "with let's encrypt to refresh that wildcard cert",
    "start": "729279",
    "end": "735519"
  },
  {
    "text": "here we can see an ingress with an external externally accessible internet",
    "start": "735600",
    "end": "741519"
  },
  {
    "text": "accessible ip that's just provided by the cloud provider and so as a natural consequence of being",
    "start": "741519",
    "end": "747360"
  },
  {
    "text": "on the internet suddenly anything with access to the internet can talk to it pods from inside a cluster workloads",
    "start": "747360",
    "end": "753839"
  },
  {
    "text": "inside of a data center as long as they have some route to the public internet then they can then get into the other cluster",
    "start": "753839",
    "end": "761279"
  },
  {
    "text": "here we have a different problem the ingress controller in the lower cluster is a self-hosted one so it doesn't have",
    "start": "761279",
    "end": "767519"
  },
  {
    "text": "an external ip address so we front it with a load balancer in",
    "start": "767519",
    "end": "772639"
  },
  {
    "text": "yellow and the load balancer provides a network id that then we can route traffic to using the pods and",
    "start": "772639",
    "end": "780000"
  },
  {
    "text": "workloads inside the data center you can see that if we start combining",
    "start": "780000",
    "end": "785200"
  },
  {
    "text": "uh ingresses in one cluster and ingresses in another cluster then you can get these one one to many",
    "start": "785200",
    "end": "790560"
  },
  {
    "text": "abstractions between many different uh topologies in your many different locations in your network",
    "start": "790560",
    "end": "796399"
  },
  {
    "text": "topology you get this kind of mesh behavior starting to form where you have a lot of access to things",
    "start": "796399",
    "end": "803600"
  },
  {
    "text": "that you've provisioned so super cool now one constraint i want you to notice here though is that there",
    "start": "803600",
    "end": "809519"
  },
  {
    "text": "isn't any ingress into the data center because for that you would need to say open up load balancer on your data",
    "start": "809519",
    "end": "816079"
  },
  {
    "text": "center side that's more traditional operations this also is an area where you could",
    "start": "816079",
    "end": "821120"
  },
  {
    "text": "start potentially thinking about a route sharing solution with route sharing",
    "start": "821120",
    "end": "827360"
  },
  {
    "text": "one of the primary goals here is to make the pod ip addresses that are constantly changing normally",
    "start": "827360",
    "end": "832639"
  },
  {
    "text": "inside of the kubernetes cluster natively routable not just on the nodes",
    "start": "832639",
    "end": "837760"
  },
  {
    "text": "the routers of the cluster but beyond the nodes beyond the cluster perimeter",
    "start": "837760",
    "end": "843600"
  },
  {
    "text": "one use case to think about here be like if you wanted to run an ingress controller outside the",
    "start": "843600",
    "end": "850480"
  },
  {
    "text": "cluster right actually run the ingress controller infrastructure outside the cluster then that ingress controller infra would",
    "start": "850480",
    "end": "857680"
  },
  {
    "text": "need to be able to route to the pod iap addresses that it reads from the api servers and pointless",
    "start": "857680",
    "end": "862800"
  },
  {
    "text": "and so like this is this is one use case another variation of this is if you were",
    "start": "862800",
    "end": "869279"
  },
  {
    "text": "to say want to run that ingress controller inside another cluster same thing but just a little bit more",
    "start": "869279",
    "end": "876079"
  },
  {
    "text": "cheeky and clever now it's not just about pod ips we also",
    "start": "876079",
    "end": "881839"
  },
  {
    "text": "might potentially want to be able to route something more useful like a service virtual ip",
    "start": "881839",
    "end": "888160"
  },
  {
    "text": "this is kind of tricky because the virtual ip doesn't exist in any one place it exists in the entire cluster",
    "start": "888160",
    "end": "894000"
  },
  {
    "text": "but it's possible to use the endpoints api to determine which nodes a",
    "start": "894000",
    "end": "900800"
  },
  {
    "text": "service has ready pods on if you can figure out which nodes those pods are running on",
    "start": "900800",
    "end": "906959"
  },
  {
    "text": "for that service then you can take all the node ips and advertise them as available routes for service vips",
    "start": "906959",
    "end": "914480"
  },
  {
    "text": "and you can even weight them and if you are putting equivalent routes into a routing table then it's important to know about this",
    "start": "914480",
    "end": "920720"
  },
  {
    "text": "concept of ecmp ecmp equal cost multi-path",
    "start": "920720",
    "end": "926240"
  },
  {
    "text": "routing this is supported by the linux kernel it's supported by a lot of standard networking hardware",
    "start": "926240",
    "end": "931839"
  },
  {
    "text": "if you have your own switches in your data center or co-location you might take a look at the manual and see",
    "start": "931839",
    "end": "937440"
  },
  {
    "text": "what is the ecmp behavior of the routing tables of that switch or other networking",
    "start": "937440",
    "end": "943279"
  },
  {
    "text": "equipment and um for instance i used to work with the switch",
    "start": "943279",
    "end": "948320"
  },
  {
    "text": "in the side of a kubernetes environment that you could advertise an arbitrary number of equivalent routes more or less uh",
    "start": "948320",
    "end": "954880"
  },
  {
    "text": "that routing table could get super long but only eight of those equivalent notes would be hashed and used at any one time",
    "start": "954880",
    "end": "961440"
  },
  {
    "text": "and the benefit of that is that say one of those routes actually became unavailable because we were rotating a",
    "start": "961440",
    "end": "967519"
  },
  {
    "text": "node out of the infrastructure it very quickly dropped out of the routing table as a healthy path",
    "start": "967519",
    "end": "973040"
  },
  {
    "text": "and then there would be load balancing and failover to other nodes because of ecmp",
    "start": "973040",
    "end": "979920"
  },
  {
    "text": "the protocols that are used to accomplish this kind of route sharing between things like routers are typically bgp and ospf these kinds",
    "start": "979920",
    "end": "987279"
  },
  {
    "text": "of protocols run the internet but they're not that scary the ospf is popular within private networks because",
    "start": "987279",
    "end": "993279"
  },
  {
    "text": "it is only for single autonomous systems whereas bgp is an inter-autonomous system protocol",
    "start": "993279",
    "end": "1000639"
  },
  {
    "text": "so you can deal with multiple autonomous systems sharing routes you can mix these protocols so that one",
    "start": "1000639",
    "end": "1006800"
  },
  {
    "text": "part of your network uses ospf and the other part uses bgp and the rats will transfer properly so here in the route sharing",
    "start": "1006800",
    "end": "1015440"
  },
  {
    "text": "diagram we need the routers of our kubernetes cluster as well as the routers of our data center to talk",
    "start": "1015440",
    "end": "1020959"
  },
  {
    "text": "in the kubernetes clusters i've just reiterating that the nodes are the routers of the kubernetes",
    "start": "1020959",
    "end": "1026959"
  },
  {
    "text": "cluster the nodes are what allow you to be able to figure out where the virtual mix of pods are",
    "start": "1026959",
    "end": "1034160"
  },
  {
    "text": "and they also are programmed by things like cooperation or your coup proxy replacement program",
    "start": "1034160",
    "end": "1039918"
  },
  {
    "text": "to have special ip tables or ipvs rules that will do the service load balancing",
    "start": "1039919",
    "end": "1046959"
  },
  {
    "text": "so very cool those nodes can talk between each other and share routes for pod ip addresses and service ips and the",
    "start": "1046959",
    "end": "1054480"
  },
  {
    "text": "nodes that back them and they can also share that same information with your",
    "start": "1054480",
    "end": "1059679"
  },
  {
    "text": "traditional routing infrastructure inside of your data centers or whatever network fabric you have",
    "start": "1059679",
    "end": "1066480"
  },
  {
    "text": "now for route sharing it's important to note that each cluster needs to have unique service subnets and pod subnets",
    "start": "1066480",
    "end": "1073919"
  },
  {
    "text": "basically you can't have collisions because an ip address can't live in multiple places you want to have a",
    "start": "1073919",
    "end": "1079600"
  },
  {
    "text": "unique route to be able to go there so each cluster has to have these separate subnets and",
    "start": "1079600",
    "end": "1084720"
  },
  {
    "text": "that can be a tricky challenge sometimes especially if you're traditionally deploying kubernetes with overlay",
    "start": "1084720",
    "end": "1089760"
  },
  {
    "text": "networks sometimes people will take a shortcut of using a default subnet and just having",
    "start": "1089760",
    "end": "1095679"
  },
  {
    "text": "it be the same in everywhere so uh this is something that you might want to think about that's definitely a constraint uh or",
    "start": "1095679",
    "end": "1103039"
  },
  {
    "text": "requirement of the solution for technologies that you might want to look at this is an incredibly short",
    "start": "1103039",
    "end": "1108240"
  },
  {
    "text": "non-exhaustive list calico is super popular it's based off of an older project called bird",
    "start": "1108240",
    "end": "1114080"
  },
  {
    "text": "a cube router is a very elegant single go program that does network policy",
    "start": "1114080",
    "end": "1119520"
  },
  {
    "text": "cni and a coup proxy replacement in a single program go check it out very very",
    "start": "1119520",
    "end": "1125200"
  },
  {
    "text": "cool um i've deployed both of those things but i haven't deployed romana which is just another bgp base",
    "start": "1125200",
    "end": "1130960"
  },
  {
    "text": "should go check that out for ospf go check out fr routing",
    "start": "1130960",
    "end": "1136160"
  },
  {
    "text": "um and the cooper mesh project which was using quagga before you also",
    "start": "1136160",
    "end": "1142640"
  },
  {
    "text": "just want to read the manual for your own rafter to see what kind of protocols it supports go take a look and mess with the stuff",
    "start": "1142640",
    "end": "1148640"
  },
  {
    "text": "really approachable it's not that scary as long as you're not at internet scale and accidentally taking",
    "start": "1148640",
    "end": "1154000"
  },
  {
    "text": "down you know somebody's network uh routes are great you know when we're",
    "start": "1154000",
    "end": "1160080"
  },
  {
    "text": "just working with ips but as people we want to configure things using dns right we want",
    "start": "1160080",
    "end": "1165120"
  },
  {
    "text": "names for services and when we talk about service vips like we don't think about the",
    "start": "1165120",
    "end": "1171200"
  },
  {
    "text": "actual ip address of that thing typically we think about resolving it using the",
    "start": "1171200",
    "end": "1176240"
  },
  {
    "text": "service name in the namespace and then the service cluster local domain this is only accessible typically within",
    "start": "1176240",
    "end": "1182640"
  },
  {
    "text": "the inside of a kubernetes cluster the service that allows you to do that is called core dns and it's very extensible and",
    "start": "1182640",
    "end": "1188960"
  },
  {
    "text": "configurable one thing to know about coordinates it's it's always the 10th ip address so it's",
    "start": "1188960",
    "end": "1195520"
  },
  {
    "text": "not just some random ip inside of the cluster it's the 10th one and if your service subnets are unique",
    "start": "1195520",
    "end": "1203039"
  },
  {
    "text": "and you are sharing the routes for services across your clusters then you should be",
    "start": "1203039",
    "end": "1208960"
  },
  {
    "text": "able to talk to the dns server of each cluster no matter where you are inside of your route sharing infrastructure so that would let you do",
    "start": "1208960",
    "end": "1215919"
  },
  {
    "text": "something like dns forwarding where say each cluster knows which zone it has and it also knows what all",
    "start": "1215919",
    "end": "1222559"
  },
  {
    "text": "of the zones of the other clusters dns servers are so say i'm in cluster one and i get a",
    "start": "1222559",
    "end": "1228480"
  },
  {
    "text": "request for cluster zero i can send that to cluster zero and get the answer back",
    "start": "1228480",
    "end": "1234960"
  },
  {
    "text": "in sort of this kind of dns mesh you could also just use an upstream dns server topology if you want to collect",
    "start": "1234960",
    "end": "1241039"
  },
  {
    "text": "them to a central place or advertise those things on the internet another common thing might be if you",
    "start": "1241039",
    "end": "1246400"
  },
  {
    "text": "have a private dns server or split horizon setup on your vpn then you could see how that",
    "start": "1246400",
    "end": "1252320"
  },
  {
    "text": "would enable you to use your route sharing infrastructure plus dns forwarding to access services by name from your",
    "start": "1252320",
    "end": "1259200"
  },
  {
    "text": "laptop or from workloads inside of your data center this is a super and fun environment to be in",
    "start": "1259200",
    "end": "1265039"
  },
  {
    "text": "i really miss being able to have access to this kind of infrastructure",
    "start": "1265039",
    "end": "1270720"
  },
  {
    "text": "and so because i missed that i built a demo",
    "start": "1270720",
    "end": "1277679"
  },
  {
    "text": "i would love for anybody to be able to play with this kind of infrastructure it's typically expensive and kind of a niche",
    "start": "1277679",
    "end": "1283039"
  },
  {
    "text": "opportunity it's hard to get an environment where you can play with this kind of thing without having some sort",
    "start": "1283039",
    "end": "1288320"
  },
  {
    "text": "of deadline or a cost factor so um why don't we get into an environment",
    "start": "1288320",
    "end": "1296080"
  },
  {
    "text": "that lets us play right if you clone my repository",
    "start": "1296080",
    "end": "1301360"
  },
  {
    "text": "it's at stealthy box slash multi-cluster dash githubs my name or my github handle",
    "start": "1301360",
    "end": "1308159"
  },
  {
    "text": "stealthy box then uh you can clone this repo and fork it",
    "start": "1308159",
    "end": "1313840"
  },
  {
    "text": "and then just run kind setup and kind load it'll take a few minutes",
    "start": "1313840",
    "end": "1319840"
  },
  {
    "text": "depending on your computer and what you already have downloaded this will get you three kind clusters",
    "start": "1319840",
    "end": "1327360"
  },
  {
    "text": "uh without any kind of cni setup at all and they will all have",
    "start": "1327360",
    "end": "1333840"
  },
  {
    "text": "configuration for different subnets for bots and sources you can see here cni is",
    "start": "1333840",
    "end": "1341440"
  },
  {
    "text": "the default cmi is disabled because we're going to use calico and then the load image script just",
    "start": "1341440",
    "end": "1346640"
  },
  {
    "text": "preloads everything for you so once you have this infrastructure",
    "start": "1346640",
    "end": "1352799"
  },
  {
    "text": "go ahead and just follow the readme we will",
    "start": "1352799",
    "end": "1359600"
  },
  {
    "text": "want to do a coupe cuddle apply to the kind cluster zero where we need",
    "start": "1359600",
    "end": "1367120"
  },
  {
    "text": "to bootstrap cni and so let's just expand the customization directory for the coop system name space",
    "start": "1367120",
    "end": "1374880"
  },
  {
    "text": "let's just do that real quick uh this needs to bootstrap flux uh or this needs to",
    "start": "1374880",
    "end": "1381280"
  },
  {
    "text": "bootstrap calico so that we can actually do the flux bootstrap which is the next thing that i'm going to suggest do",
    "start": "1381280",
    "end": "1388559"
  },
  {
    "text": "so in order to do that i have to export my github token uh this step is just",
    "start": "1388559",
    "end": "1395200"
  },
  {
    "text": "slightly different for me because i'm using a hub token here but you can just put your own in",
    "start": "1395200",
    "end": "1400799"
  },
  {
    "text": "here my hub token is modified to also have access to repos and ssh keys which is",
    "start": "1400799",
    "end": "1406320"
  },
  {
    "text": "two of the permissions that you'll need it's mentioned in the readme",
    "start": "1406320",
    "end": "1411280"
  },
  {
    "text": "so i'll do a flux bootstrap with my user stealthy box but it's a personal repository called",
    "start": "1412320",
    "end": "1418720"
  },
  {
    "text": "multi-cluster git ops and i want to sync the path config cluster 0 and apply it to the cluster so if you",
    "start": "1418720",
    "end": "1425840"
  },
  {
    "text": "look inside of config i have a folder called cluster 0 with a bunch of name spaces with a bunch of config in them so we'll get flux",
    "start": "1425840",
    "end": "1433279"
  },
  {
    "text": "bootstrapped ready to go and throw that off to the side uh here we can start talking a little",
    "start": "1433279",
    "end": "1439919"
  },
  {
    "text": "bit about what kind of configuration we're loading so inside of",
    "start": "1439919",
    "end": "1445840"
  },
  {
    "text": "the coop system namespace there is a customization that loads from",
    "start": "1446240",
    "end": "1452880"
  },
  {
    "text": "libraries the calico coordinates and surf deployments uh",
    "start": "1452880",
    "end": "1459440"
  },
  {
    "text": "calico this is just set up to do the subnet advertisement stuff",
    "start": "1459440",
    "end": "1464720"
  },
  {
    "text": "uh that's done with this patch right here so we advertise the cluster ips for the specific subnet of cluster zero and then",
    "start": "1464720",
    "end": "1472080"
  },
  {
    "text": "cluster one cluster two i'm disabling this just because uh it could break setups",
    "start": "1472080",
    "end": "1477200"
  },
  {
    "text": "depending on your computer now the surf configuration i want all of the nodes in",
    "start": "1477200",
    "end": "1482880"
  },
  {
    "text": "the cluster to join a surf cluster uh using multi-test you could also just use some ip",
    "start": "1482880",
    "end": "1489520"
  },
  {
    "text": "addresses like the eyepiece of your control plane notes those will change um you know your",
    "start": "1489520",
    "end": "1495039"
  },
  {
    "text": "workers and uh basically we'll use the surf cluster to solve the problem of network drift of node ip addresses so",
    "start": "1495039",
    "end": "1503039"
  },
  {
    "text": "each surf node can then know its network identity and advertise that to the rest of the surf",
    "start": "1503039",
    "end": "1508559"
  },
  {
    "text": "cluster and then if you look inside of the library server deployment",
    "start": "1508559",
    "end": "1514400"
  },
  {
    "text": "there is a query performance that the beautiful script i wrote that is actually a pretty",
    "start": "1514400",
    "end": "1520640"
  },
  {
    "text": "well uh functioning reconciler it even has like rachel exit and the",
    "start": "1520640",
    "end": "1526880"
  },
  {
    "text": "function of this bash script uh is to convert the surf members into bgp peers",
    "start": "1526880",
    "end": "1536000"
  },
  {
    "text": "and then also template the core files for the cluster to mesh between all of the other",
    "start": "1536000",
    "end": "1542720"
  },
  {
    "text": "clusters so uh you can you can read through this code it's item potent",
    "start": "1542720",
    "end": "1547919"
  },
  {
    "text": "uh was pretty fun exercise it's just jq bash coupe ctl calico ctl stuff",
    "start": "1547919",
    "end": "1555200"
  },
  {
    "text": "it was really fun to write this you can also figure out how to do graceful exits in bash this was a fun one to figure out with uh",
    "start": "1555200",
    "end": "1562080"
  },
  {
    "text": "traps and jobs that are being waited on and grabbing like the job list anyway",
    "start": "1562080",
    "end": "1569360"
  },
  {
    "text": "that's just fun easter eggs i'm sorry i apologize in advance uh for using that much bash but uh",
    "start": "1569360",
    "end": "1577200"
  },
  {
    "text": "basically the other secret kind of uh hat trick that happens here is that cluster one and cluster two have a very",
    "start": "1577200",
    "end": "1582880"
  },
  {
    "text": "similar setup in kube system uh in each of these clusters we also have a pod info deployment",
    "start": "1582880",
    "end": "1590720"
  },
  {
    "text": "just so that and a debug deployment so we can mess with the network and in the flux system apply directory",
    "start": "1590720",
    "end": "1598320"
  },
  {
    "text": "we have cluster 1 and cluster 2 apply which is using the new flux toolkit customization api",
    "start": "1598320",
    "end": "1604720"
  },
  {
    "text": "remote access feature to basically apply from this management",
    "start": "1604720",
    "end": "1610720"
  },
  {
    "text": "cluster all of the manifests in the git repository to the other ones so pretty cool stuff go check that out",
    "start": "1610720",
    "end": "1618240"
  },
  {
    "text": "with the single flux bootstrap done on our cluster we can start examining some",
    "start": "1618240",
    "end": "1625039"
  },
  {
    "text": "resources and see if some of our networking is working so",
    "start": "1625039",
    "end": "1630320"
  },
  {
    "text": "i think the first thing i'll look at is just do we have our bgp",
    "start": "1630320",
    "end": "1638840"
  },
  {
    "text": "peers so here we are in cluster zero we have bgp peers for cluster one cluster two",
    "start": "1638840",
    "end": "1645039"
  },
  {
    "text": "um because these are all appearing properly that means that the surf cluster is built",
    "start": "1645039",
    "end": "1650240"
  },
  {
    "text": "and um similarly i could change context to look at cluster one and you would see that it's",
    "start": "1650240",
    "end": "1656159"
  },
  {
    "text": "peering with cluster zero and cluster two so that's a really good sign the other",
    "start": "1656159",
    "end": "1661200"
  },
  {
    "text": "thing that we would just check is i wanna show you the core dns deployment it's slightly extended",
    "start": "1661200",
    "end": "1668240"
  },
  {
    "text": "so you can read the patches here but the core dns deployment",
    "start": "1668240",
    "end": "1675279"
  },
  {
    "text": "coupe system describe config maps prefixed with core",
    "start": "1675279",
    "end": "1683679"
  },
  {
    "text": "so this almost looks like a normal core file except uh in addition to the",
    "start": "1683679",
    "end": "1689600"
  },
  {
    "text": "cluster local domain for the kubernetes services we have extra cube zones from the environment",
    "start": "1689600",
    "end": "1694960"
  },
  {
    "text": "being templated in and then we also import any other core file snippets from etsy core dns.d we have separate config",
    "start": "1694960",
    "end": "1703120"
  },
  {
    "text": "maps then here's the coordinates configure that mounts to that location",
    "start": "1703120",
    "end": "1708960"
  },
  {
    "text": "this is being created by a surf quarry controller it's the bash script that we just read",
    "start": "1708960",
    "end": "1714640"
  },
  {
    "text": "we can see that we have additional coordinates configuration extending on that one from a separate config map",
    "start": "1714640",
    "end": "1720720"
  },
  {
    "text": "that is taking cluster 1.lan forwarding it to this place a specific 10th address in the 10101",
    "start": "1720720",
    "end": "1728960"
  },
  {
    "text": "service subnet and the 10th address in the 10102 service subnet for cluster2.lan",
    "start": "1728960",
    "end": "1735840"
  },
  {
    "text": "uh hopefully we won't have to restart core dns for this but having a few problems every now and then uh for coordinates env we can see that",
    "start": "1735840",
    "end": "1742559"
  },
  {
    "text": "extra coupe zones uh it sets cluster zero to that land so this core dns server knows that it's in",
    "start": "1742559",
    "end": "1749200"
  },
  {
    "text": "cluster zero and will respond not only to cluster.local but cluster0.land",
    "start": "1749200",
    "end": "1754240"
  },
  {
    "text": "for any of the service requests that we have so um if we get into our",
    "start": "1754240",
    "end": "1761440"
  },
  {
    "text": "debug pod ideally we should be able to say just check that coordinesses function",
    "start": "1761440",
    "end": "1769840"
  },
  {
    "text": "so we can look that pod info we can get the potty info service",
    "start": "1769840",
    "end": "1776559"
  },
  {
    "text": "that is from cluster.local it's the same ip address being returned",
    "start": "1777120",
    "end": "1783120"
  },
  {
    "text": "we should potentially be able to do cluster0.lan",
    "start": "1783120",
    "end": "1788159"
  },
  {
    "text": "and we can also get a completely different address for a completely different service in a different cluster",
    "start": "1789279",
    "end": "1795200"
  },
  {
    "text": "by changing to cluster1.lan you might also get lucky and be able to resolve a service for",
    "start": "1795200",
    "end": "1802279"
  },
  {
    "text": "cluster2.lan and if",
    "start": "1802279",
    "end": "1808559"
  },
  {
    "text": "all of our route sharing is working because we already know that dns already is functioning we should also be",
    "start": "1808559",
    "end": "1815039"
  },
  {
    "text": "able to curl from kind cluster 0 pod info in the default namespace of",
    "start": "1815039",
    "end": "1822399"
  },
  {
    "text": "cluster 2 at port 9890 and there we have cross cluster",
    "start": "1822399",
    "end": "1828799"
  },
  {
    "text": "communication without the use of any node ports or ingresses or load balancers purely route sharing",
    "start": "1828799",
    "end": "1837440"
  },
  {
    "text": "and dns forwarding so pretty cool stuff if you want to",
    "start": "1837440",
    "end": "1843200"
  },
  {
    "text": "take a look at more of what's happening under the hood i would highly i really like it",
    "start": "1843200",
    "end": "1850000"
  },
  {
    "text": "actually if you were to check out the project on github and fork it yourself and",
    "start": "1850000",
    "end": "1856799"
  },
  {
    "text": "you can replicate these results in your own environment uh the best part about this is that it",
    "start": "1856799",
    "end": "1862640"
  },
  {
    "text": "costs really nothing as long as your laptop is strong enough to run a few kind notes so super fun had a ton of fun putting",
    "start": "1862640",
    "end": "1870480"
  },
  {
    "text": "this demo together and would love to hear your feedback for sure so that's really cool",
    "start": "1870480",
    "end": "1878640"
  },
  {
    "text": "to me at least hope that you have some fun putting that together so just to kind of recap",
    "start": "1878640",
    "end": "1885600"
  },
  {
    "text": "in that example we just had a third cluster we had no upstream dns server or additional infra except for my laptop",
    "start": "1885600",
    "end": "1892320"
  },
  {
    "text": "which is not part of the routing mesh but we had three clusters that were doing dns forwarding and route sharing",
    "start": "1892320",
    "end": "1897840"
  },
  {
    "text": "between each other with several domains and then we had a pod info",
    "start": "1897840",
    "end": "1903440"
  },
  {
    "text": "deployment in each cluster and we were able to curl between them so pretty cool stuff",
    "start": "1903440",
    "end": "1909590"
  },
  {
    "text": "[Music] go check that out so for route sharing",
    "start": "1909590",
    "end": "1914799"
  },
  {
    "text": "and dns forwarding we can extend the native kubernetes service discovery and routing mesh",
    "start": "1914799",
    "end": "1921519"
  },
  {
    "text": "beyond the cluster but uh you do incur another single hop",
    "start": "1921519",
    "end": "1928799"
  },
  {
    "text": "in the service routing before doing it the services this way as an alternative you could try to have",
    "start": "1928799",
    "end": "1935360"
  },
  {
    "text": "service controllers you know programming each of the nodes across the cluster and doing route sharing in a little bit",
    "start": "1935360",
    "end": "1941919"
  },
  {
    "text": "of a different way you could eliminate a hop it is possible to do that kind of thing this is a",
    "start": "1941919",
    "end": "1947200"
  },
  {
    "text": "layer for abstraction so uh there's no like built-in encryption that's",
    "start": "1947200",
    "end": "1952240"
  },
  {
    "text": "happening you know as the packets transmit like between the clusters uh that's not",
    "start": "1952240",
    "end": "1957279"
  },
  {
    "text": "really like a like a given uh if you i you did see me disable the ip and ip",
    "start": "1957279",
    "end": "1964399"
  },
  {
    "text": "encapsulation that is a feature of calico if you have calico on both ends uh but yeah like",
    "start": "1964399",
    "end": "1973039"
  },
  {
    "text": "outside of that you're going to have to look into your implementation to figure out if you need to encrypt your packets say if",
    "start": "1973039",
    "end": "1979919"
  },
  {
    "text": "they're traveling across an untrusted network or the open internet this route sharing and like layer 4",
    "start": "1979919",
    "end": "1987760"
  },
  {
    "text": "adjacency and routability of pods is also typically a prerequisite for multi-cluster service",
    "start": "1987760",
    "end": "1994880"
  },
  {
    "text": "meshes so if you are interested in doing multi-cluster mesh you're going to have to know a little bit",
    "start": "1994880",
    "end": "2000720"
  },
  {
    "text": "about route sharing and this kind of infrastructure up some technologies again we have a non-exhaustive list but these are some",
    "start": "2000720",
    "end": "2007120"
  },
  {
    "text": "things that i think are fun uh with weave we have weavnet which is one of the older cni's",
    "start": "2007120",
    "end": "2012960"
  },
  {
    "text": "that's been around for quite a long time weavnet emulates a layer 2 network uh",
    "start": "2012960",
    "end": "2018640"
  },
  {
    "text": "using like mac addresses it's pretty pretty neat we've net allows multicast so you could",
    "start": "2018640",
    "end": "2025200"
  },
  {
    "text": "do something like the surf cluster that i have deployed on top of a weavnet",
    "start": "2025200",
    "end": "2030559"
  },
  {
    "text": "set of nodes and it's pretty easy uh we've done a previous talk before luke marsden demonstrated",
    "start": "2030559",
    "end": "2036880"
  },
  {
    "text": "we've net being meshed across multiple kubernetes clusters for pods apart routing uh for something a little bit more",
    "start": "2036880",
    "end": "2044159"
  },
  {
    "text": "interesting you could say create a wire guard network psyllium allows you to use wire guard",
    "start": "2044159",
    "end": "2051118"
  },
  {
    "text": "and they have some of the coolest edgiest non-standard multi-cluster mesh technology out there",
    "start": "2051119",
    "end": "2056398"
  },
  {
    "text": "right now go ahead and check out psyllium they actually have a route sharing uh network policy uh sharing",
    "start": "2056399",
    "end": "2063919"
  },
  {
    "text": "implementation using a bunch of scds instead of something like bgp and then",
    "start": "2063919",
    "end": "2069200"
  },
  {
    "text": "um the other super cool category of network tech that i think is out there right now",
    "start": "2069200",
    "end": "2075280"
  },
  {
    "text": "that you should look at are this category of two-way udp hole punching",
    "start": "2075280",
    "end": "2080638"
  },
  {
    "text": "nat traversing private networks so there will be typically this kind of like public",
    "start": "2080639",
    "end": "2086000"
  },
  {
    "text": "coordination or lighthouse server uh to use the tail scale and nebula terms",
    "start": "2086000",
    "end": "2091919"
  },
  {
    "text": "that allow nodes inside of private networks behind nat",
    "start": "2091919",
    "end": "2097118"
  },
  {
    "text": "to find each other over public networks and uh you can look at tail scale which",
    "start": "2097119",
    "end": "2103599"
  },
  {
    "text": "uses wire guard under the hood uh slack uses nebula for their own kind of uh corporate you know reasons",
    "start": "2103599",
    "end": "2111680"
  },
  {
    "text": "uh or for their workforce reasons and nebula uses the noise protocol to encrypt traffic and it's able to",
    "start": "2111680",
    "end": "2119520"
  },
  {
    "text": "do this kind of double mat traversal without any kind of public machines actually routing traffic through them zero tier is another",
    "start": "2119520",
    "end": "2126880"
  },
  {
    "text": "solution as well and their new version like nebula supports multicast but tail",
    "start": "2126880",
    "end": "2131920"
  },
  {
    "text": "scale does not support multi-class so uh as far as going further",
    "start": "2131920",
    "end": "2137359"
  },
  {
    "text": "uh beyond trying to defeat nat and encrypt your traffic on untrusted networks while still doing",
    "start": "2137359",
    "end": "2142480"
  },
  {
    "text": "fancy route sharing and dns forwarding uh we didn't talk much about how to do network policy between clusters uh as",
    "start": "2142480",
    "end": "2149839"
  },
  {
    "text": "far as i know this is pretty unexplored space i haven't played with the psyllium network policy stuff enough",
    "start": "2149839",
    "end": "2155119"
  },
  {
    "text": "to know but i think what i do know about psyllium is that it does tag network packets uh in a way where this could be possible",
    "start": "2155119",
    "end": "2161920"
  },
  {
    "text": "across the clusters uh for something more a theoretical",
    "start": "2161920",
    "end": "2166960"
  },
  {
    "text": "you could look at the well i shouldn't say theoretical because there's an implementation now go to kubernetes six mcs api you can",
    "start": "2166960",
    "end": "2174240"
  },
  {
    "text": "also read kep 1645 this is the multi-cluster services api",
    "start": "2174240",
    "end": "2179359"
  },
  {
    "text": "that allows um it just is a specification for which clusters can share about the endpoints lists of other",
    "start": "2179359",
    "end": "2185680"
  },
  {
    "text": "clusters so that's one way to figure out how to get coop proxy to get rid of that extra",
    "start": "2185680",
    "end": "2191440"
  },
  {
    "text": "hop coup proxy can now be mcs api aware for cross cluster service meshes you",
    "start": "2191440",
    "end": "2197920"
  },
  {
    "text": "might look at things like istio link or d maybe assembling your own kind of thing for a particular use case",
    "start": "2197920",
    "end": "2204320"
  },
  {
    "text": "also take a look at console connect and uh for inter cluster orchestration as a",
    "start": "2204320",
    "end": "2209599"
  },
  {
    "text": "final kind of tidbit of things to chew on um",
    "start": "2209599",
    "end": "2214720"
  },
  {
    "text": "you saw us with flux we were able to apply",
    "start": "2214720",
    "end": "2221920"
  },
  {
    "text": "a single bootstrapped gitops directory structure to multiple clusters and we",
    "start": "2221920",
    "end": "2228880"
  },
  {
    "text": "were using the customization toolkit api to control these supplies you can actually create health checks",
    "start": "2228880",
    "end": "2235920"
  },
  {
    "text": "and dependencies between individual customizations",
    "start": "2235920",
    "end": "2241119"
  },
  {
    "text": "within a set of toolkit controllers and if you were to deploy flagger",
    "start": "2241119",
    "end": "2249440"
  },
  {
    "text": "in each of the clusters using those customizations when we have the",
    "start": "2249520",
    "end": "2255920"
  },
  {
    "text": "k-status implementation of flagger finished you could actually do a cross cluster",
    "start": "2255920",
    "end": "2262640"
  },
  {
    "text": "not just a cross-cluster canary which is already possible and people are already doing that with istio multi-cluster mesh",
    "start": "2262640",
    "end": "2268240"
  },
  {
    "text": "and psyllium but you could actually orchestrate the dependencies between",
    "start": "2268240",
    "end": "2273359"
  },
  {
    "text": "those flagger canaries so if you're interested in canaries and service meshes and",
    "start": "2273359",
    "end": "2279920"
  },
  {
    "text": "traffic shifting and shaping beyond just the normal facilities that",
    "start": "2279920",
    "end": "2285280"
  },
  {
    "text": "you can do to accomplish zero downtime deployments and cross-cluster traffic with kubernetes route sharing and dns",
    "start": "2285280",
    "end": "2291040"
  },
  {
    "text": "forwarding uh check out flagger and check out dependency management with customizations in flux uh for today's",
    "start": "2291040",
    "end": "2298720"
  },
  {
    "text": "demo again you can go to the github repository and fork it and do it on your own uh play",
    "start": "2298720",
    "end": "2304960"
  },
  {
    "text": "with it on your machine submit issues or ping me on twitter if things are not working i really want to hear if you're just",
    "start": "2304960",
    "end": "2311359"
  },
  {
    "text": "learning something from this kind of environment because it took me a long time to be able to get into a place where i",
    "start": "2311359",
    "end": "2318240"
  },
  {
    "text": "could learn these skills and i would really like for the next kind of generation of practitioners",
    "start": "2318240",
    "end": "2324240"
  },
  {
    "text": "or people from different backgrounds even if you're very senior uh to learn about bgp and how to do rap",
    "start": "2324240",
    "end": "2330640"
  },
  {
    "text": "sharing in the context of kubernetes and cloud native applications uh hit me up i am available dms open",
    "start": "2330640",
    "end": "2338240"
  },
  {
    "text": "on twitter also my github if you want to follow me there and again the kubernetes slack as well",
    "start": "2338240",
    "end": "2344960"
  },
  {
    "text": "i'd really like to get to know you uh hear what you're doing with some of our technologies that we build that",
    "start": "2344960",
    "end": "2350079"
  },
  {
    "text": "weave or if you have a cool thing that you're doing with kubernetes also say hi in sick cluster life cycle and",
    "start": "2350079",
    "end": "2356720"
  },
  {
    "text": "thanks so much for coming to the talk see you later",
    "start": "2356720",
    "end": "2362800"
  }
]