[
  {
    "text": "well thank you all for coming turning",
    "start": "120",
    "end": "2280"
  },
  {
    "text": "out in the snow and the rain and the",
    "start": "2280",
    "end": "4240"
  },
  {
    "text": "wind that it took to get here from where",
    "start": "4240",
    "end": "6600"
  },
  {
    "text": "your hotels or airbnbs or wherever",
    "start": "6600",
    "end": "8320"
  },
  {
    "text": "you're staying um I'm and holler I have",
    "start": "8320",
    "end": "10639"
  },
  {
    "text": "the lofty title of a chief scientist at",
    "start": "10639",
    "end": "13120"
  },
  {
    "text": "a small company called alal and I",
    "start": "13120",
    "end": "15360"
  },
  {
    "text": "appreciate the chance to talk with you",
    "start": "15360",
    "end": "16800"
  },
  {
    "text": "today about skyray which is seamlessly",
    "start": "16800",
    "end": "19560"
  },
  {
    "text": "extending Cube Ray to multicluster",
    "start": "19560",
    "end": "22119"
  },
  {
    "text": "multicloud",
    "start": "22119",
    "end": "23680"
  },
  {
    "text": "operation so first a show of hands how",
    "start": "23680",
    "end": "25960"
  },
  {
    "text": "many people use have used",
    "start": "25960",
    "end": "27840"
  },
  {
    "text": "Ray so a fair number how about Cube Ray",
    "start": "27840",
    "end": "31080"
  },
  {
    "text": "I assume if you're here at the",
    "start": "31080",
    "end": "32480"
  },
  {
    "text": "conference you're using Ray you've used",
    "start": "32480",
    "end": "34079"
  },
  {
    "text": "Cube Ray all right cool um yeah so for",
    "start": "34079",
    "end": "36719"
  },
  {
    "text": "those who are less familiar with Ray",
    "start": "36719",
    "end": "38559"
  },
  {
    "text": "it's a unified framework that can scale",
    "start": "38559",
    "end": "41079"
  },
  {
    "text": "AI ml applications from running on a",
    "start": "41079",
    "end": "44079"
  },
  {
    "text": "laptop to running in a cluster and Cube",
    "start": "44079",
    "end": "46640"
  },
  {
    "text": "Ray allows the deletion creation and",
    "start": "46640",
    "end": "49360"
  },
  {
    "text": "scaling of Ray clusters Ray jobs and Ray",
    "start": "49360",
    "end": "52520"
  },
  {
    "text": "Services um on a kubernetes cluster so",
    "start": "52520",
    "end": "55440"
  },
  {
    "text": "kind of the diagram here of what the Cub",
    "start": "55440",
    "end": "58120"
  },
  {
    "text": "structure is is you've got your",
    "start": "58120",
    "end": "60000"
  },
  {
    "text": "kubernetes cluster on the bottom either",
    "start": "60000",
    "end": "61600"
  },
  {
    "text": "from your cloud or self-hosted and",
    "start": "61600",
    "end": "63840"
  },
  {
    "text": "you're running Cube Ray on top of it to",
    "start": "63840",
    "end": "65518"
  },
  {
    "text": "be able to run your ray clusters but",
    "start": "65519",
    "end": "68439"
  },
  {
    "text": "what if you have more than one",
    "start": "68439",
    "end": "70479"
  },
  {
    "text": "cluster so how many of you regularly use",
    "start": "70479",
    "end": "73600"
  },
  {
    "text": "multiple kubernetes",
    "start": "73600",
    "end": "75439"
  },
  {
    "text": "clusters okay pretty much most everybody",
    "start": "75439",
    "end": "78880"
  },
  {
    "text": "and then how about multiple",
    "start": "78880",
    "end": "80840"
  },
  {
    "text": "clouds okay so you know there's a lot of",
    "start": "80840",
    "end": "83840"
  },
  {
    "text": "reasons you're using multiple kubernetes",
    "start": "83840",
    "end": "85720"
  },
  {
    "text": "clusters you want to group the resources",
    "start": "85720",
    "end": "88079"
  },
  {
    "text": "by characteristics you want service",
    "start": "88079",
    "end": "90159"
  },
  {
    "text": "continuity so you want clusters in",
    "start": "90159",
    "end": "92280"
  },
  {
    "text": "different regions so if one region goes",
    "start": "92280",
    "end": "94040"
  },
  {
    "text": "down you don't lose your infrastructure",
    "start": "94040",
    "end": "96159"
  },
  {
    "text": "you might have clusters for different",
    "start": "96159",
    "end": "97640"
  },
  {
    "text": "purposes you have a production cluster",
    "start": "97640",
    "end": "99920"
  },
  {
    "text": "that you're tightly monitoring that",
    "start": "99920",
    "end": "101880"
  },
  {
    "text": "you're watching out for particular",
    "start": "101880",
    "end": "104119"
  },
  {
    "text": "resources that you've vetted as being",
    "start": "104119",
    "end": "106520"
  },
  {
    "text": "appropriate for production work and",
    "start": "106520",
    "end": "108479"
  },
  {
    "text": "you've got your development cluster with",
    "start": "108479",
    "end": "110079"
  },
  {
    "text": "a a lower qos uh maybe it's Dynamic and",
    "start": "110079",
    "end": "114479"
  },
  {
    "text": "so on so you've got that kind of reason",
    "start": "114479",
    "end": "116640"
  },
  {
    "text": "you have resource availability you have",
    "start": "116640",
    "end": "118479"
  },
  {
    "text": "a cluster in this region because gpus",
    "start": "118479",
    "end": "120439"
  },
  {
    "text": "are available there you have a cluster",
    "start": "120439",
    "end": "122360"
  },
  {
    "text": "in this region because it's cheaper and",
    "start": "122360",
    "end": "124439"
  },
  {
    "text": "of course there's a lot of reasons to",
    "start": "124439",
    "end": "125719"
  },
  {
    "text": "use uh multiple Cloud vendors you want",
    "start": "125719",
    "end": "128039"
  },
  {
    "text": "to avoid Cloud vendor lock in you want",
    "start": "128039",
    "end": "130160"
  },
  {
    "text": "to meet your customers where they are",
    "start": "130160",
    "end": "131800"
  },
  {
    "text": "maybe they have a preference for a",
    "start": "131800",
    "end": "133200"
  },
  {
    "text": "certain Cloud vendor um so the sort of",
    "start": "133200",
    "end": "136640"
  },
  {
    "text": "thinking about um multiple clusters and",
    "start": "136640",
    "end": "139680"
  },
  {
    "text": "multiple clouds brings in the idea of",
    "start": "139680",
    "end": "141920"
  },
  {
    "text": "Sky Computing which was described in a",
    "start": "141920",
    "end": "144480"
  },
  {
    "text": "recent uh paper from Berkeley talking",
    "start": "144480",
    "end": "147400"
  },
  {
    "text": "about what's needed to make Sky",
    "start": "147400",
    "end": "148959"
  },
  {
    "text": "Computing happen and one of the key",
    "start": "148959",
    "end": "150879"
  },
  {
    "text": "things is a commodity Cloud compute",
    "start": "150879",
    "end": "152720"
  },
  {
    "text": "layer now you could argue that",
    "start": "152720",
    "end": "154800"
  },
  {
    "text": "kubernetes is a commodity Cloud compute",
    "start": "154800",
    "end": "157040"
  },
  {
    "text": "layer but for that to really be credible",
    "start": "157040",
    "end": "159760"
  },
  {
    "text": "it needs to be almost as easy to use",
    "start": "159760",
    "end": "162080"
  },
  {
    "text": "multiple clusters as it is to use one",
    "start": "162080",
    "end": "165159"
  },
  {
    "text": "cluster um so that's the idea around",
    "start": "165159",
    "end": "167560"
  },
  {
    "text": "skyray skyray is trying to skyfy cubay",
    "start": "167560",
    "end": "171720"
  },
  {
    "text": "so it's trying to extend Cube's",
    "start": "171720",
    "end": "173560"
  },
  {
    "text": "operation from a single cluster",
    "start": "173560",
    "end": "175360"
  },
  {
    "text": "environment to a multicluster multicloud",
    "start": "175360",
    "end": "178280"
  },
  {
    "text": "operation and so this is achieved by",
    "start": "178280",
    "end": "180599"
  },
  {
    "text": "having um it work with a policy-driven",
    "start": "180599",
    "end": "184040"
  },
  {
    "text": "uh kubernetes fleet manager so the fleet",
    "start": "184040",
    "end": "187400"
  },
  {
    "text": "manager presents a kubernetes API to the",
    "start": "187400",
    "end": "190000"
  },
  {
    "text": "user and it interoperates with cuay the",
    "start": "190000",
    "end": "193440"
  },
  {
    "text": "fleet manager deploys Cub on each",
    "start": "193440",
    "end": "195680"
  },
  {
    "text": "cluster in a workload Set uh using a a",
    "start": "195680",
    "end": "199720"
  },
  {
    "text": "fleet manager policy to do so spread",
    "start": "199720",
    "end": "201760"
  },
  {
    "text": "duplicate then the fleet manager places",
    "start": "201760",
    "end": "204640"
  },
  {
    "text": "Cu uh clusters jobs and services that",
    "start": "204640",
    "end": "207560"
  },
  {
    "text": "arrive from users onto particular",
    "start": "207560",
    "end": "210319"
  },
  {
    "text": "workload clusters depending on policies",
    "start": "210319",
    "end": "212920"
  },
  {
    "text": "and depending on what it knows about",
    "start": "212920",
    "end": "214519"
  },
  {
    "text": "that workload cluster and then Cube Ray",
    "start": "214519",
    "end": "217319"
  },
  {
    "text": "on that workload cluster deals with the",
    "start": "217319",
    "end": "220040"
  },
  {
    "text": "things that are placed there just like",
    "start": "220040",
    "end": "221439"
  },
  {
    "text": "it would normally um and so there's a",
    "start": "221439",
    "end": "224480"
  },
  {
    "text": "bunch of uh you know uh kubernetes Fleet",
    "start": "224480",
    "end": "226799"
  },
  {
    "text": "managers available including carada open",
    "start": "226799",
    "end": "229400"
  },
  {
    "text": "cluster management and NOA some",
    "start": "229400",
    "end": "232480"
  },
  {
    "text": "examples so that's the vision of skyray",
    "start": "232480",
    "end": "235280"
  },
  {
    "text": "let's look how it operates in more",
    "start": "235280",
    "end": "237400"
  },
  {
    "text": "detail so basically cu Ray supports",
    "start": "237400",
    "end": "240599"
  },
  {
    "text": "three custom resource definitions the",
    "start": "240599",
    "end": "243400"
  },
  {
    "text": "ray cluster which lets you create a",
    "start": "243400",
    "end": "245480"
  },
  {
    "text": "cluster with specified resources and",
    "start": "245480",
    "end": "248040"
  },
  {
    "text": "attributes the ray job which creates a",
    "start": "248040",
    "end": "250760"
  },
  {
    "text": "ray cluster and then submits a job to",
    "start": "250760",
    "end": "253159"
  },
  {
    "text": "that cluster when it's ready and then",
    "start": "253159",
    "end": "255439"
  },
  {
    "text": "can optionally delete that cluster when",
    "start": "255439",
    "end": "257160"
  },
  {
    "text": "you no longer need it this is often used",
    "start": "257160",
    "end": "259400"
  },
  {
    "text": "for MLA training and the race service",
    "start": "259400",
    "end": "262960"
  },
  {
    "text": "which creates the cluster runs a rayer",
    "start": "262960",
    "end": "265840"
  },
  {
    "text": "deployment graph on that cluster Ray",
    "start": "265840",
    "end": "268199"
  },
  {
    "text": "Services have a bunch of features the",
    "start": "268199",
    "end": "270120"
  },
  {
    "text": "most notable for this talk is uh Race",
    "start": "270120",
    "end": "272400"
  },
  {
    "text": "Service autoscaling which involves the",
    "start": "272400",
    "end": "274199"
  },
  {
    "text": "idea that if the service has a bunch of",
    "start": "274199",
    "end": "277000"
  },
  {
    "text": "uh request queuing up it needs more",
    "start": "277000",
    "end": "279919"
  },
  {
    "text": "resources and so it automatically scales",
    "start": "279919",
    "end": "281800"
  },
  {
    "text": "up its requests to CU for more resources",
    "start": "281800",
    "end": "285400"
  },
  {
    "text": "any of these Cube Ray deployments",
    "start": "285400",
    "end": "286880"
  },
  {
    "text": "speaking of autoscaling can r with the",
    "start": "286880",
    "end": "288840"
  },
  {
    "text": "aray autoscaler which when it sees that",
    "start": "288840",
    "end": "291080"
  },
  {
    "text": "the ray job or service or cluster needs",
    "start": "291080",
    "end": "293880"
  },
  {
    "text": "more resources it automatically requests",
    "start": "293880",
    "end": "296680"
  },
  {
    "text": "them from the kubernetes cluster",
    "start": "296680",
    "end": "299759"
  },
  {
    "text": "So based on how Cube Ray works this is",
    "start": "299759",
    "end": "302160"
  },
  {
    "text": "how skyray Works in more detail the",
    "start": "302160",
    "end": "304680"
  },
  {
    "text": "fleet manager is watching all of its",
    "start": "304680",
    "end": "307280"
  },
  {
    "text": "workload clusters that it's managing",
    "start": "307280",
    "end": "309520"
  },
  {
    "text": "it's looking at it knows their name it",
    "start": "309520",
    "end": "311320"
  },
  {
    "text": "knows the cloud provider they come from",
    "start": "311320",
    "end": "313360"
  },
  {
    "text": "it knows the region they're in it knows",
    "start": "313360",
    "end": "315360"
  },
  {
    "text": "the available capacity they have it",
    "start": "315360",
    "end": "317520"
  },
  {
    "text": "knows labels that the operators put on",
    "start": "317520",
    "end": "319919"
  },
  {
    "text": "them it knows the kubernetes version",
    "start": "319919",
    "end": "321639"
  },
  {
    "text": "they're running it knows whether or not",
    "start": "321639",
    "end": "323360"
  },
  {
    "text": "they're running a cluster autoscaler Etc",
    "start": "323360",
    "end": "326319"
  },
  {
    "text": "so the fleet manager schedules CU in its",
    "start": "326319",
    "end": "328680"
  },
  {
    "text": "crds those three crds we just talked",
    "start": "328680",
    "end": "330840"
  },
  {
    "text": "about on all the workload clusters",
    "start": "330840",
    "end": "332880"
  },
  {
    "text": "according to a fleet manager policy that",
    "start": "332880",
    "end": "334639"
  },
  {
    "text": "says spread duplicate and then when jobs",
    "start": "334639",
    "end": "338240"
  },
  {
    "text": "clusters or service requests come in the",
    "start": "338240",
    "end": "340960"
  },
  {
    "text": "fleet manager schedules them on the",
    "start": "340960",
    "end": "343120"
  },
  {
    "text": "workload cluster and then as we said CU",
    "start": "343120",
    "end": "345840"
  },
  {
    "text": "handles it from",
    "start": "345840",
    "end": "347840"
  },
  {
    "text": "there um there's as I said many Fleet",
    "start": "347840",
    "end": "351400"
  },
  {
    "text": "managers available for our work we used",
    "start": "351400",
    "end": "353880"
  },
  {
    "text": "the Anova fleet manager it includes",
    "start": "353880",
    "end": "356360"
  },
  {
    "text": "several relevant capabilities that made",
    "start": "356360",
    "end": "358000"
  },
  {
    "text": "it a good choice for this work",
    "start": "358000",
    "end": "360160"
  },
  {
    "text": "it supports the spread duplicate policy",
    "start": "360160",
    "end": "362360"
  },
  {
    "text": "which we already talked about it spreads",
    "start": "362360",
    "end": "365080"
  },
  {
    "text": "it supports the uh specified cluster",
    "start": "365080",
    "end": "367919"
  },
  {
    "text": "policy and the priority policy which",
    "start": "367919",
    "end": "369560"
  },
  {
    "text": "we'll talk about during the course of",
    "start": "369560",
    "end": "371360"
  },
  {
    "text": "the of the talk and in particular it",
    "start": "371360",
    "end": "374680"
  },
  {
    "text": "supports an available capacity policy so",
    "start": "374680",
    "end": "377400"
  },
  {
    "text": "this policy says don't place this set of",
    "start": "377400",
    "end": "380160"
  },
  {
    "text": "jobs unless you can place them all",
    "start": "380160",
    "end": "382360"
  },
  {
    "text": "together uh at once so it's basically a",
    "start": "382360",
    "end": "385360"
  },
  {
    "text": "gang scheduling policy and so MLA AI",
    "start": "385360",
    "end": "388880"
  },
  {
    "text": "training jobs",
    "start": "388880",
    "end": "390199"
  },
  {
    "text": "uh often require gang scheduling because",
    "start": "390199",
    "end": "392720"
  },
  {
    "text": "there's a distributed training process",
    "start": "392720",
    "end": "395639"
  },
  {
    "text": "that needs the workers to make progress",
    "start": "395639",
    "end": "398360"
  },
  {
    "text": "in a coordinated fashion and in addition",
    "start": "398360",
    "end": "401440"
  },
  {
    "text": "Nova recognizes Ray cluster crds it",
    "start": "401440",
    "end": "404520"
  },
  {
    "text": "understands them it understands what the",
    "start": "404520",
    "end": "406400"
  },
  {
    "text": "worker and head requests for resources",
    "start": "406400",
    "end": "408919"
  },
  {
    "text": "are so it's able to understand whether",
    "start": "408919",
    "end": "410840"
  },
  {
    "text": "there's available capacity for the ray",
    "start": "410840",
    "end": "413120"
  },
  {
    "text": "items um another feature that's",
    "start": "413120",
    "end": "415319"
  },
  {
    "text": "interesting from the standpoint of",
    "start": "415319",
    "end": "416759"
  },
  {
    "text": "skyray is the Nova just in time clust",
    "start": "416759",
    "end": "419879"
  },
  {
    "text": "feature what that means is if a cluster",
    "start": "419879",
    "end": "422280"
  },
  {
    "text": "becomes idle Nova will uh basically uh",
    "start": "422280",
    "end": "426000"
  },
  {
    "text": "scale it to zero or delete it depending",
    "start": "426000",
    "end": "428000"
  },
  {
    "text": "on which option you prefer and then when",
    "start": "428000",
    "end": "430000"
  },
  {
    "text": "it becomes busy again when it's needed",
    "start": "430000",
    "end": "431759"
  },
  {
    "text": "again it will reconstitute",
    "start": "431759",
    "end": "435160"
  },
  {
    "text": "it the an addition Nova is autoscaler",
    "start": "435160",
    "end": "438440"
  },
  {
    "text": "aware that means it understands whether",
    "start": "438440",
    "end": "440639"
  },
  {
    "text": "the cluster the workload cluster is",
    "start": "440639",
    "end": "442240"
  },
  {
    "text": "running a cluster autoscaler it",
    "start": "442240",
    "end": "444160"
  },
  {
    "text": "currently recognizes the kubernetes",
    "start": "444160",
    "end": "446400"
  },
  {
    "text": "cluster autoscaler and the Luna cluster",
    "start": "446400",
    "end": "448960"
  },
  {
    "text": "autoscaler",
    "start": "448960",
    "end": "450120"
  },
  {
    "text": "and so if multiple clusters satisfy a",
    "start": "450120",
    "end": "452680"
  },
  {
    "text": "workload placement policy Nova tries to",
    "start": "452680",
    "end": "455599"
  },
  {
    "text": "place the workload on a cluster that",
    "start": "455599",
    "end": "457520"
  },
  {
    "text": "already has the resources available but",
    "start": "457520",
    "end": "459960"
  },
  {
    "text": "if there is no such cluster right now it",
    "start": "459960",
    "end": "462879"
  },
  {
    "text": "then chooses a Target cluster that's",
    "start": "462879",
    "end": "464800"
  },
  {
    "text": "running a cluster autoscaler with the",
    "start": "464800",
    "end": "466879"
  },
  {
    "text": "idea that that cluster autoscaler can",
    "start": "466879",
    "end": "468919"
  },
  {
    "text": "obtain the resources so for our Skyway",
    "start": "468919",
    "end": "471639"
  },
  {
    "text": "work we use the Luna smart cluster",
    "start": "471639",
    "end": "473720"
  },
  {
    "text": "autoscaler you see a little diagram at",
    "start": "473720",
    "end": "475840"
  },
  {
    "text": "the bottom of the screen about how",
    "start": "475840",
    "end": "477199"
  },
  {
    "text": "cluster uh autoscaler works with Luna it",
    "start": "477199",
    "end": "480319"
  },
  {
    "text": "looks at pending pods it can place them",
    "start": "480319",
    "end": "482479"
  },
  {
    "text": "together if they're small so they can",
    "start": "482479",
    "end": "484199"
  },
  {
    "text": "share a node or it can uh allocate",
    "start": "484199",
    "end": "486800"
  },
  {
    "text": "custom size uh nodes for them if they're",
    "start": "486800",
    "end": "489680"
  },
  {
    "text": "larger and since MLA workloads often",
    "start": "489680",
    "end": "493120"
  },
  {
    "text": "require expensive GPU resources for",
    "start": "493120",
    "end": "495520"
  },
  {
    "text": "certain um scenarios it can be nice to",
    "start": "495520",
    "end": "497680"
  },
  {
    "text": "have a cluster autoscaler running to",
    "start": "497680",
    "end": "499680"
  },
  {
    "text": "scale those resources up and down so now",
    "start": "499680",
    "end": "502919"
  },
  {
    "text": "let's look at a some examples of how",
    "start": "502919",
    "end": "504879"
  },
  {
    "text": "skyray can be used to achieve uh certain",
    "start": "504879",
    "end": "507520"
  },
  {
    "text": "policy objectives",
    "start": "507520",
    "end": "510319"
  },
  {
    "text": "um so to to recap we'll have the Nova",
    "start": "510319",
    "end": "513080"
  },
  {
    "text": "control plane running it will take",
    "start": "513080",
    "end": "516000"
  },
  {
    "text": "policies that um the operator wants to",
    "start": "516000",
    "end": "519080"
  },
  {
    "text": "apply to workloads it will use those",
    "start": "519080",
    "end": "521440"
  },
  {
    "text": "policies to schedule Cube Ray and its",
    "start": "521440",
    "end": "523320"
  },
  {
    "text": "crds and it will also use policies to",
    "start": "523320",
    "end": "525959"
  },
  {
    "text": "schedule the ray service Ray job and Ray",
    "start": "525959",
    "end": "528360"
  },
  {
    "text": "cruster custom resources so all the",
    "start": "528360",
    "end": "530839"
  },
  {
    "text": "scripts and kubernetes yaml and so on",
    "start": "530839",
    "end": "533399"
  },
  {
    "text": "that are used for these examples are",
    "start": "533399",
    "end": "535080"
  },
  {
    "text": "available in this open source repo",
    "start": "535080",
    "end": "537279"
  },
  {
    "text": "you'll see that some of the examples",
    "start": "537279",
    "end": "538600"
  },
  {
    "text": "will include Dynam dyamic allocation uh",
    "start": "538600",
    "end": "540920"
  },
  {
    "text": "one of them will include compound",
    "start": "540920",
    "end": "543920"
  },
  {
    "text": "AI so these are the six example use",
    "start": "543920",
    "end": "546839"
  },
  {
    "text": "cases we look at uh two of them are",
    "start": "546839",
    "end": "549040"
  },
  {
    "text": "training four of them are serving um I",
    "start": "549040",
    "end": "551760"
  },
  {
    "text": "tried to get a mix of things that I",
    "start": "551760",
    "end": "553839"
  },
  {
    "text": "thought customers often do along with a",
    "start": "553839",
    "end": "556440"
  },
  {
    "text": "mix of things that are show what's",
    "start": "556440",
    "end": "558360"
  },
  {
    "text": "possible if you were really interested",
    "start": "558360",
    "end": "559800"
  },
  {
    "text": "in pushing the envelope and I I hope",
    "start": "559800",
    "end": "562040"
  },
  {
    "text": "some of these match scenarios you have",
    "start": "562040",
    "end": "564519"
  },
  {
    "text": "but even if they don't these the the",
    "start": "564519",
    "end": "566720"
  },
  {
    "text": "infrastructure is flexible you can craft",
    "start": "566720",
    "end": "569000"
  },
  {
    "text": "policy that will match uh your",
    "start": "569000",
    "end": "571680"
  },
  {
    "text": "needs um for the Simplicity of",
    "start": "571680",
    "end": "573920"
  },
  {
    "text": "presentation all the scenario examples",
    "start": "573920",
    "end": "575760"
  },
  {
    "text": "only involve two clusters um and so",
    "start": "575760",
    "end": "578600"
  },
  {
    "text": "after the cube aray um has been deployed",
    "start": "578600",
    "end": "582920"
  },
  {
    "text": "you see this is what it looks like from",
    "start": "582920",
    "end": "584920"
  },
  {
    "text": "the Nova control plane standpoint it has",
    "start": "584920",
    "end": "587640"
  },
  {
    "text": "two Cube Ray operators one working in",
    "start": "587640",
    "end": "589920"
  },
  {
    "text": "each of the two uh workload clusters um",
    "start": "589920",
    "end": "593160"
  },
  {
    "text": "and all these uh examples use the",
    "start": "593160",
    "end": "595120"
  },
  {
    "text": "environment variable uh skyray path to",
    "start": "595120",
    "end": "597399"
  },
  {
    "text": "point at that open source repo that I",
    "start": "597399",
    "end": "600279"
  },
  {
    "text": "talked",
    "start": "600279",
    "end": "601120"
  },
  {
    "text": "about okay so the first example is that",
    "start": "601120",
    "end": "604079"
  },
  {
    "text": "you want to place a series of production",
    "start": "604079",
    "end": "606079"
  },
  {
    "text": "training jobs these jobs have proven",
    "start": "606079",
    "end": "608680"
  },
  {
    "text": "their worth in terms of business value",
    "start": "608680",
    "end": "611040"
  },
  {
    "text": "and so you really want to run these jobs",
    "start": "611040",
    "end": "613399"
  },
  {
    "text": "and so you have a static cluster that",
    "start": "613399",
    "end": "615760"
  },
  {
    "text": "maybe it's on premise or maybe it's",
    "start": "615760",
    "end": "617279"
  },
  {
    "text": "reserved Cloud resources and it's a",
    "start": "617279",
    "end": "620000"
  },
  {
    "text": "fixed size and if possible you want to",
    "start": "620000",
    "end": "621920"
  },
  {
    "text": "run the production training job there",
    "start": "621920",
    "end": "624480"
  },
  {
    "text": "but if it doesn't fit you're willing to",
    "start": "624480",
    "end": "626519"
  },
  {
    "text": "pay for on demand resources uh to handle",
    "start": "626519",
    "end": "629480"
  },
  {
    "text": "that training job because it's that",
    "start": "629480",
    "end": "631200"
  },
  {
    "text": "important to you so you have these two",
    "start": "631200",
    "end": "633279"
  },
  {
    "text": "clusters so we're going to use the",
    "start": "633279",
    "end": "635240"
  },
  {
    "text": "available capacity policy we already",
    "start": "635240",
    "end": "637000"
  },
  {
    "text": "talked about that will gang schedule uh",
    "start": "637000",
    "end": "639959"
  },
  {
    "text": "these training",
    "start": "639959",
    "end": "641440"
  },
  {
    "text": "jobs so as a proxy for our production",
    "start": "641440",
    "end": "644040"
  },
  {
    "text": "training workload we're going to use the",
    "start": "644040",
    "end": "645800"
  },
  {
    "text": "P torch image train Benchmark and for",
    "start": "645800",
    "end": "648440"
  },
  {
    "text": "each of the ray clusters we're going to",
    "start": "648440",
    "end": "649880"
  },
  {
    "text": "deploy will have a CPU head and two GPU",
    "start": "649880",
    "end": "653560"
  },
  {
    "text": "workers um we're going to deploy three",
    "start": "653560",
    "end": "655880"
  },
  {
    "text": "copies of this Ray job the first two",
    "start": "655880",
    "end": "658320"
  },
  {
    "text": "copies are going to fit",
    "start": "658320",
    "end": "659959"
  },
  {
    "text": "on the static cluster and the third copy",
    "start": "659959",
    "end": "661839"
  },
  {
    "text": "is not going to fit there so we're going",
    "start": "661839",
    "end": "663519"
  },
  {
    "text": "to schedule it on the dynamic cluster",
    "start": "663519",
    "end": "665720"
  },
  {
    "text": "and the Luda cluster autoscaler is going",
    "start": "665720",
    "end": "667560"
  },
  {
    "text": "to scale up to accommodate",
    "start": "667560",
    "end": "670120"
  },
  {
    "text": "it so this is the placement of those",
    "start": "670120",
    "end": "672959"
  },
  {
    "text": "three training jobs so as you can see if",
    "start": "672959",
    "end": "675320"
  },
  {
    "text": "you're a a cube Ray User it's just as",
    "start": "675320",
    "end": "677200"
  },
  {
    "text": "easy to deploy these training jobs to",
    "start": "677200",
    "end": "679600"
  },
  {
    "text": "the uh Nova endpoint as it would be to",
    "start": "679600",
    "end": "682079"
  },
  {
    "text": "deploy them directly to cube Ray so it's",
    "start": "682079",
    "end": "684600"
  },
  {
    "text": "kind of because there's a policy engine",
    "start": "684600",
    "end": "686440"
  },
  {
    "text": "behind this it makes things easy from",
    "start": "686440",
    "end": "688680"
  },
  {
    "text": "the standpoint of the",
    "start": "688680",
    "end": "690360"
  },
  {
    "text": "user um so what we see is the first two",
    "start": "690360",
    "end": "693519"
  },
  {
    "text": "training jobs start running right away",
    "start": "693519",
    "end": "695360"
  },
  {
    "text": "because they have sufficient resources",
    "start": "695360",
    "end": "696839"
  },
  {
    "text": "to run the third one is waiting for the",
    "start": "696839",
    "end": "699240"
  },
  {
    "text": "cluster autoscaler to bring up the",
    "start": "699240",
    "end": "700959"
  },
  {
    "text": "additional resources that are needed it",
    "start": "700959",
    "end": "703160"
  },
  {
    "text": "does that you see that in the middle and",
    "start": "703160",
    "end": "705399"
  },
  {
    "text": "then all three of the jobs can run to",
    "start": "705399",
    "end": "707279"
  },
  {
    "text": "completion and if the ray job three is",
    "start": "707279",
    "end": "710079"
  },
  {
    "text": "deleted that Dynamic uh scale up can be",
    "start": "710079",
    "end": "713320"
  },
  {
    "text": "scaled back down by uh",
    "start": "713320",
    "end": "715959"
  },
  {
    "text": "Luna so let's look at a second example",
    "start": "715959",
    "end": "718480"
  },
  {
    "text": "but in this example Le it's the same",
    "start": "718480",
    "end": "720480"
  },
  {
    "text": "training job as the first one but this",
    "start": "720480",
    "end": "722519"
  },
  {
    "text": "time we'll say that they're experimental",
    "start": "722519",
    "end": "724360"
  },
  {
    "text": "jobs so they haven't proven their",
    "start": "724360",
    "end": "726440"
  },
  {
    "text": "business value we don't want to pay for",
    "start": "726440",
    "end": "728440"
  },
  {
    "text": "any on demand resources for these but we",
    "start": "728440",
    "end": "730720"
  },
  {
    "text": "are willing to set up a cluster to run",
    "start": "730720",
    "end": "732480"
  },
  {
    "text": "these things in it's it's kind of a sunk",
    "start": "732480",
    "end": "734560"
  },
  {
    "text": "cost cluster that we're using for",
    "start": "734560",
    "end": "736560"
  },
  {
    "text": "experimental jobs and so in this case",
    "start": "736560",
    "end": "738880"
  },
  {
    "text": "we're not going to use the available",
    "start": "738880",
    "end": "740360"
  },
  {
    "text": "capacity policy we're going to use the",
    "start": "740360",
    "end": "742399"
  },
  {
    "text": "specified cluster placement policy to",
    "start": "742399",
    "end": "745000"
  },
  {
    "text": "place these jobs and so here you'd say",
    "start": "745000",
    "end": "748399"
  },
  {
    "text": "well hey if you're going to specifi the",
    "start": "748399",
    "end": "749800"
  },
  {
    "text": "cluster why not just run the job",
    "start": "749800",
    "end": "751320"
  },
  {
    "text": "directly on the cluster but the cool",
    "start": "751320",
    "end": "753320"
  },
  {
    "text": "thing about this layer of indirection is",
    "start": "753320",
    "end": "755399"
  },
  {
    "text": "you could later decide to run these jobs",
    "start": "755399",
    "end": "757199"
  },
  {
    "text": "on a different specified cluster and",
    "start": "757199",
    "end": "759199"
  },
  {
    "text": "Nova would take care of doing the",
    "start": "759199",
    "end": "761000"
  },
  {
    "text": "rescheduling and for future scheduling",
    "start": "761000",
    "end": "762880"
  },
  {
    "text": "so it's still nice to have the layer of",
    "start": "762880",
    "end": "764519"
  },
  {
    "text": "indirection so we're going to do fill no",
    "start": "764519",
    "end": "767800"
  },
  {
    "text": "spill so the same we're going to use the",
    "start": "767800",
    "end": "770040"
  },
  {
    "text": "same experimental training workload that",
    "start": "770040",
    "end": "772120"
  },
  {
    "text": "we used for the production training",
    "start": "772120",
    "end": "773480"
  },
  {
    "text": "workload and we're going to use the same",
    "start": "773480",
    "end": "775440"
  },
  {
    "text": "size Ray cluster but this time the three",
    "start": "775440",
    "end": "778040"
  },
  {
    "text": "copies of the job will all placed on the",
    "start": "778040",
    "end": "779959"
  },
  {
    "text": "static",
    "start": "779959",
    "end": "781079"
  },
  {
    "text": "cluster and we use the exact same",
    "start": "781079",
    "end": "783199"
  },
  {
    "text": "deployment because it's only the policy",
    "start": "783199",
    "end": "785480"
  },
  {
    "text": "engine that makes the outcome",
    "start": "785480",
    "end": "786720"
  },
  {
    "text": "differently so it's a it's super easy to",
    "start": "786720",
    "end": "789160"
  },
  {
    "text": "do the same thing and this time the two",
    "start": "789160",
    "end": "792360"
  },
  {
    "text": "jobs that are running on the static",
    "start": "792360",
    "end": "793639"
  },
  {
    "text": "cluster that fit are able to run to",
    "start": "793639",
    "end": "795839"
  },
  {
    "text": "completion while the third job is still",
    "start": "795839",
    "end": "798120"
  },
  {
    "text": "pending because there's not room on that",
    "start": "798120",
    "end": "799720"
  },
  {
    "text": "static cluster for it to run but if one",
    "start": "799720",
    "end": "802120"
  },
  {
    "text": "of those jobs that finishes is deleted",
    "start": "802120",
    "end": "804920"
  },
  {
    "text": "then the additional job that was waiting",
    "start": "804920",
    "end": "807320"
  },
  {
    "text": "becomes active and can run",
    "start": "807320",
    "end": "810120"
  },
  {
    "text": "so now let's switch gears and go to uh",
    "start": "810120",
    "end": "812880"
  },
  {
    "text": "serving and this is an example of",
    "start": "812880",
    "end": "815120"
  },
  {
    "text": "serving for production and serving for",
    "start": "815120",
    "end": "817959"
  },
  {
    "text": "development so when we serve for",
    "start": "817959",
    "end": "819959"
  },
  {
    "text": "production we want to create a static",
    "start": "819959",
    "end": "822320"
  },
  {
    "text": "cluster that has all the resources that",
    "start": "822320",
    "end": "824839"
  },
  {
    "text": "we think Maxim will need for serving and",
    "start": "824839",
    "end": "827120"
  },
  {
    "text": "production we want a fancier GPU uh that",
    "start": "827120",
    "end": "830279"
  },
  {
    "text": "we've decided is good enough for our um",
    "start": "830279",
    "end": "833160"
  },
  {
    "text": "model in production and we don't want to",
    "start": "833160",
    "end": "835279"
  },
  {
    "text": "do any auto scaling because we don't",
    "start": "835279",
    "end": "836680"
  },
  {
    "text": "want any latency during the production",
    "start": "836680",
    "end": "839399"
  },
  {
    "text": "serving so this is online serving for",
    "start": "839399",
    "end": "841639"
  },
  {
    "text": "development we're like oh you know",
    "start": "841639",
    "end": "843560"
  },
  {
    "text": "latency is fine these are developers",
    "start": "843560",
    "end": "845600"
  },
  {
    "text": "we're going to scale up when the",
    "start": "845600",
    "end": "847600"
  },
  {
    "text": "resources are needed and will scale up a",
    "start": "847600",
    "end": "849600"
  },
  {
    "text": "cheaper GPU instance than we're using in",
    "start": "849600",
    "end": "851959"
  },
  {
    "text": "production and so in this case we're",
    "start": "851959",
    "end": "853759"
  },
  {
    "text": "going to use a policy of labeling the",
    "start": "853759",
    "end": "855560"
  },
  {
    "text": "Clusters so we'll label the static",
    "start": "855560",
    "end": "857880"
  },
  {
    "text": "cluster as the production cluster we'll",
    "start": "857880",
    "end": "859920"
  },
  {
    "text": "label the dynamic cluster as the",
    "start": "859920",
    "end": "861480"
  },
  {
    "text": "development cluster we'll label the",
    "start": "861480",
    "end": "863440"
  },
  {
    "text": "placement jobs for serving and the",
    "start": "863440",
    "end": "865800"
  },
  {
    "text": "policy engine will match the labels and",
    "start": "865800",
    "end": "867759"
  },
  {
    "text": "put the job on the appropriate",
    "start": "867759",
    "end": "870240"
  },
  {
    "text": "cluster so in this case we'll run two",
    "start": "870240",
    "end": "873040"
  },
  {
    "text": "serving jobs um so the race service",
    "start": "873040",
    "end": "875920"
  },
  {
    "text": "that's that we'll use is the tech",
    "start": "875920",
    "end": "877920"
  },
  {
    "text": "summarizer model service so the ray",
    "start": "877920",
    "end": "880480"
  },
  {
    "text": "service Ray cluster will be a CPU head",
    "start": "880480",
    "end": "882839"
  },
  {
    "text": "in one GPU worker and we'll uh do the",
    "start": "882839",
    "end": "886320"
  },
  {
    "text": "deployments as outlined for production",
    "start": "886320",
    "end": "889279"
  },
  {
    "text": "and development so one will go on each",
    "start": "889279",
    "end": "891079"
  },
  {
    "text": "of the two clusters so we'll deploy The",
    "start": "891079",
    "end": "894079"
  },
  {
    "text": "Race Service uh in the production Nam",
    "start": "894079",
    "end": "896959"
  },
  {
    "text": "space with the it'll be under the",
    "start": "896959",
    "end": "898639"
  },
  {
    "text": "production",
    "start": "898639",
    "end": "899959"
  },
  {
    "text": "uh policy it'll get placed and we'll",
    "start": "899959",
    "end": "902440"
  },
  {
    "text": "validate that it serves correctly we'll",
    "start": "902440",
    "end": "905120"
  },
  {
    "text": "do the same thing for the development",
    "start": "905120",
    "end": "907920"
  },
  {
    "text": "service it gets deployed to the",
    "start": "907920",
    "end": "910720"
  },
  {
    "text": "development cluster and we validate that",
    "start": "910720",
    "end": "912920"
  },
  {
    "text": "it works as well uh there so we were",
    "start": "912920",
    "end": "915920"
  },
  {
    "text": "able to do uh these kinds of deployments",
    "start": "915920",
    "end": "918639"
  },
  {
    "text": "easily um almost as easily as deploying",
    "start": "918639",
    "end": "921759"
  },
  {
    "text": "directly to cuay and we were able to",
    "start": "921759",
    "end": "923959"
  },
  {
    "text": "save costs on the development cluster",
    "start": "923959",
    "end": "926120"
  },
  {
    "text": "versus the production cluster and if we",
    "start": "926120",
    "end": "928279"
  },
  {
    "text": "wanted to add more more development",
    "start": "928279",
    "end": "929560"
  },
  {
    "text": "clusters and more production clusters we",
    "start": "929560",
    "end": "931560"
  },
  {
    "text": "could add them and add a label to them",
    "start": "931560",
    "end": "933160"
  },
  {
    "text": "and they would instantly start to be",
    "start": "933160",
    "end": "934600"
  },
  {
    "text": "available for",
    "start": "934600",
    "end": "936920"
  },
  {
    "text": "workloads now let's look at another",
    "start": "936920",
    "end": "938800"
  },
  {
    "text": "serving example but this is for an llm",
    "start": "938800",
    "end": "941040"
  },
  {
    "text": "model and here we're going to look at a",
    "start": "941040",
    "end": "943160"
  },
  {
    "text": "multicloud example so in this example",
    "start": "943160",
    "end": "946079"
  },
  {
    "text": "we're going to use yet another policy",
    "start": "946079",
    "end": "948240"
  },
  {
    "text": "and this policy is the priority policy",
    "start": "948240",
    "end": "950720"
  },
  {
    "text": "and you can make a priority on any",
    "start": "950720",
    "end": "952399"
  },
  {
    "text": "attribute of a workload cluster but",
    "start": "952399",
    "end": "954319"
  },
  {
    "text": "we're going to make the priority based",
    "start": "954319",
    "end": "955839"
  },
  {
    "text": "on the cloud provider of the workload",
    "start": "955839",
    "end": "958199"
  },
  {
    "text": "cluster so we're going to use a cluster",
    "start": "958199",
    "end": "960480"
  },
  {
    "text": "uh prediction selection policy that says",
    "start": "960480",
    "end": "963199"
  },
  {
    "text": "schedule this on my preferred higher",
    "start": "963199",
    "end": "965839"
  },
  {
    "text": "priority cloud provider you know",
    "start": "965839",
    "end": "967800"
  },
  {
    "text": "workload cluster if possible otherwise",
    "start": "967800",
    "end": "970319"
  },
  {
    "text": "schedule it on my less preferred um uh",
    "start": "970319",
    "end": "973639"
  },
  {
    "text": "cloud provider and we're also going to",
    "start": "973639",
    "end": "976000"
  },
  {
    "text": "enable just in time um capability in",
    "start": "976000",
    "end": "979759"
  },
  {
    "text": "standby uh mode meaning it'll scale to",
    "start": "979759",
    "end": "982440"
  },
  {
    "text": "zero when the cluster is idle and you",
    "start": "982440",
    "end": "984399"
  },
  {
    "text": "may be thinking hey wait a minute the",
    "start": "984399",
    "end": "985759"
  },
  {
    "text": "cluster's never idle because cuay is",
    "start": "985759",
    "end": "988040"
  },
  {
    "text": "running on it but you know because we've",
    "start": "988040",
    "end": "990639"
  },
  {
    "text": "allocated cuay on every workload um",
    "start": "990639",
    "end": "993040"
  },
  {
    "text": "cluster but in that sense cubay is like",
    "start": "993040",
    "end": "995360"
  },
  {
    "text": "a demon set it's like a demon set for",
    "start": "995360",
    "end": "997480"
  },
  {
    "text": "the cluster that's there to handle Ray",
    "start": "997480",
    "end": "999920"
  },
  {
    "text": "jobs services and clusters that are",
    "start": "999920",
    "end": "1002560"
  },
  {
    "text": "deployed there in general it's not",
    "start": "1002560",
    "end": "1004279"
  },
  {
    "text": "needed and uh the just in time feature",
    "start": "1004279",
    "end": "1006920"
  },
  {
    "text": "has an option that says for these name",
    "start": "1006920",
    "end": "1009079"
  },
  {
    "text": "spaces don't let let them make me think",
    "start": "1009079",
    "end": "1011480"
  },
  {
    "text": "the cluster is busy so we'll turn that",
    "start": "1011480",
    "end": "1013120"
  },
  {
    "text": "on for CU so CU won't make the cluster",
    "start": "1013120",
    "end": "1015959"
  },
  {
    "text": "look busy so we're going to look at two",
    "start": "1015959",
    "end": "1018560"
  },
  {
    "text": "examp examples here with Priority One is",
    "start": "1018560",
    "end": "1020800"
  },
  {
    "text": "where the priorities are between two",
    "start": "1020800",
    "end": "1022440"
  },
  {
    "text": "cluster two uh uh Cloud providers that I",
    "start": "1022440",
    "end": "1025480"
  },
  {
    "text": "have that are both on demand resources",
    "start": "1025480",
    "end": "1027959"
  },
  {
    "text": "and the other one is going to be between",
    "start": "1027959",
    "end": "1029760"
  },
  {
    "text": "two Cloud providers where one of them is",
    "start": "1029760",
    "end": "1031558"
  },
  {
    "text": "a spot provider and the other one is uh",
    "start": "1031559",
    "end": "1034640"
  },
  {
    "text": "a regular cloud",
    "start": "1034640",
    "end": "1037000"
  },
  {
    "text": "provider so in the first example both my",
    "start": "1037000",
    "end": "1039959"
  },
  {
    "text": "eks and GK clusters are in standby so",
    "start": "1039959",
    "end": "1042678"
  },
  {
    "text": "they're scaled to zero so the only thing",
    "start": "1042679",
    "end": "1044438"
  },
  {
    "text": "that's running in the cloud is the",
    "start": "1044439",
    "end": "1045640"
  },
  {
    "text": "control plane so that's 10 cents an hour",
    "start": "1045640",
    "end": "1048160"
  },
  {
    "text": "and cu isn't keeping those clusters from",
    "start": "1048160",
    "end": "1050240"
  },
  {
    "text": "going to standby and now I'm going to",
    "start": "1050240",
    "end": "1052240"
  },
  {
    "text": "deploy my first llm and when I deploy",
    "start": "1052240",
    "end": "1055919"
  },
  {
    "text": "that since gke was my preferred uh cloud",
    "start": "1055919",
    "end": "1059120"
  },
  {
    "text": "provider the cluster that is uh belongs",
    "start": "1059120",
    "end": "1061919"
  },
  {
    "text": "to gke will be brought up my service",
    "start": "1061919",
    "end": "1064400"
  },
  {
    "text": "will start running there and I'll be",
    "start": "1064400",
    "end": "1066600"
  },
  {
    "text": "able to uh run the validate the",
    "start": "1066600",
    "end": "1068880"
  },
  {
    "text": "prediction is running now I want to run",
    "start": "1068880",
    "end": "1071600"
  },
  {
    "text": "a second llm model I will run it it will",
    "start": "1071600",
    "end": "1075240"
  },
  {
    "text": "get scheduled on eks this time because",
    "start": "1075240",
    "end": "1077320"
  },
  {
    "text": "my gke cluster is full it will validate",
    "start": "1077320",
    "end": "1081480"
  },
  {
    "text": "and now both clusters are active but if",
    "start": "1081480",
    "end": "1084039"
  },
  {
    "text": "I were to delete either of these llm",
    "start": "1084039",
    "end": "1086880"
  },
  {
    "text": "models uh the cluster would get",
    "start": "1086880",
    "end": "1090000"
  },
  {
    "text": "reclaimed here's a similar example but",
    "start": "1090000",
    "end": "1092520"
  },
  {
    "text": "with the Rackspace spot and if you guys",
    "start": "1092520",
    "end": "1094360"
  },
  {
    "text": "aren't familiar with this this is a",
    "start": "1094360",
    "end": "1095559"
  },
  {
    "text": "super cheap way to get uh spot instances",
    "start": "1095559",
    "end": "1099000"
  },
  {
    "text": "of fancy gpus but you know it's it's",
    "start": "1099000",
    "end": "1101440"
  },
  {
    "text": "doesn't come without risks so that you",
    "start": "1101440",
    "end": "1103240"
  },
  {
    "text": "may not be able to get the instance you",
    "start": "1103240",
    "end": "1104640"
  },
  {
    "text": "need so again I'm going to deploy my llm",
    "start": "1104640",
    "end": "1107880"
  },
  {
    "text": "hoping to get the spot instance cluster",
    "start": "1107880",
    "end": "1109960"
  },
  {
    "text": "to to handle it but if not I want it to",
    "start": "1109960",
    "end": "1112120"
  },
  {
    "text": "fall back to eks so same drill as before",
    "start": "1112120",
    "end": "1115799"
  },
  {
    "text": "uh with the same",
    "start": "1115799",
    "end": "1117960"
  },
  {
    "text": "outcome um next we're going to switch",
    "start": "1117960",
    "end": "1119960"
  },
  {
    "text": "gears I don't know how many of you have",
    "start": "1119960",
    "end": "1121600"
  },
  {
    "text": "PTSD from a kubernetes uh upgrade uh uh",
    "start": "1121600",
    "end": "1125600"
  },
  {
    "text": "experience okay a few of you all right I",
    "start": "1125600",
    "end": "1127480"
  },
  {
    "text": "know I do but um so this is looking at",
    "start": "1127480",
    "end": "1129600"
  },
  {
    "text": "um using the uh skyray to facilitate",
    "start": "1129600",
    "end": "1133000"
  },
  {
    "text": "kubernetes upgrade so we we ideally want",
    "start": "1133000",
    "end": "1135919"
  },
  {
    "text": "a kubernetes upgrade that causes no",
    "start": "1135919",
    "end": "1137720"
  },
  {
    "text": "downtime to our a I workloads and so the",
    "start": "1137720",
    "end": "1140960"
  },
  {
    "text": "idea here is that we're going to have a",
    "start": "1140960",
    "end": "1143120"
  },
  {
    "text": "cluster that's running our workload",
    "start": "1143120",
    "end": "1145600"
  },
  {
    "text": "we'll label that cluster and our policy",
    "start": "1145600",
    "end": "1148039"
  },
  {
    "text": "will be spread duplicate across all the",
    "start": "1148039",
    "end": "1151320"
  },
  {
    "text": "labeled clusters so originally I'm only",
    "start": "1151320",
    "end": "1153120"
  },
  {
    "text": "going to have one cluster with that",
    "start": "1153120",
    "end": "1154440"
  },
  {
    "text": "label but what I'm going to do is clone",
    "start": "1154440",
    "end": "1156840"
  },
  {
    "text": "that cluster this is a feature of um the",
    "start": "1156840",
    "end": "1160120"
  },
  {
    "text": "fleet manager and when I clone that",
    "start": "1160120",
    "end": "1162240"
  },
  {
    "text": "cluster the workload will now run on",
    "start": "1162240",
    "end": "1164840"
  },
  {
    "text": "both of the copies of the cluster the",
    "start": "1164840",
    "end": "1167799"
  },
  {
    "text": "new one with the new kubernetes version",
    "start": "1167799",
    "end": "1170240"
  },
  {
    "text": "the Clone will have a higher version and",
    "start": "1170240",
    "end": "1172039"
  },
  {
    "text": "the original one and of course my load",
    "start": "1172039",
    "end": "1173799"
  },
  {
    "text": "balancer will still be pointing at the",
    "start": "1173799",
    "end": "1175440"
  },
  {
    "text": "original cluster I as the operator will",
    "start": "1175440",
    "end": "1178000"
  },
  {
    "text": "make sure the new cluster is working",
    "start": "1178000",
    "end": "1179720"
  },
  {
    "text": "fine and when I'm happy I'll switch the",
    "start": "1179720",
    "end": "1181679"
  },
  {
    "text": "load balancer over and I'll take the",
    "start": "1181679",
    "end": "1183559"
  },
  {
    "text": "label off the old cluster which will",
    "start": "1183559",
    "end": "1185240"
  },
  {
    "text": "then be deleted by the um the Nova uh",
    "start": "1185240",
    "end": "1188960"
  },
  {
    "text": "fleet manager so this will be a instead",
    "start": "1188960",
    "end": "1190840"
  },
  {
    "text": "of the sort of suspend resume uh version",
    "start": "1190840",
    "end": "1194280"
  },
  {
    "text": "of uh just in time clusters this is a",
    "start": "1194280",
    "end": "1196600"
  },
  {
    "text": "delete recreate version so here I am",
    "start": "1196600",
    "end": "1199559"
  },
  {
    "text": "labeling the cluster spread duplicating",
    "start": "1199559",
    "end": "1202240"
  },
  {
    "text": "my workload onto that cluster deploying",
    "start": "1202240",
    "end": "1204799"
  },
  {
    "text": "the race service for the llm and",
    "start": "1204799",
    "end": "1207159"
  },
  {
    "text": "ensuring that it serves now I'm going to",
    "start": "1207159",
    "end": "1209640"
  },
  {
    "text": "make a copy of the cluster custom",
    "start": "1209640",
    "end": "1211960"
  },
  {
    "text": "resource that we'll have in the copy the",
    "start": "1211960",
    "end": "1215640"
  },
  {
    "text": "newer version of kubernetes in a unique",
    "start": "1215640",
    "end": "1217600"
  },
  {
    "text": "name for the new cluster and I'm going",
    "start": "1217600",
    "end": "1219720"
  },
  {
    "text": "to deploy it with Nova the fleet manager",
    "start": "1219720",
    "end": "1223280"
  },
  {
    "text": "will then see that there's a new cluster",
    "start": "1223280",
    "end": "1225760"
  },
  {
    "text": "that matches the label and that it needs",
    "start": "1225760",
    "end": "1227559"
  },
  {
    "text": "to start up the workload there so it",
    "start": "1227559",
    "end": "1229720"
  },
  {
    "text": "will start up the cluster and then start",
    "start": "1229720",
    "end": "1231320"
  },
  {
    "text": "the workload there then as I said you",
    "start": "1231320",
    "end": "1233480"
  },
  {
    "text": "can make sure it's you're happy with the",
    "start": "1233480",
    "end": "1235480"
  },
  {
    "text": "result and you can then take the label",
    "start": "1235480",
    "end": "1237960"
  },
  {
    "text": "off the old cluster when you are after",
    "start": "1237960",
    "end": "1240000"
  },
  {
    "text": "you've retargeted your load",
    "start": "1240000",
    "end": "1242679"
  },
  {
    "text": "balancer okay and our final example",
    "start": "1242679",
    "end": "1245320"
  },
  {
    "text": "we'll bring in we'll bring in a little",
    "start": "1245320",
    "end": "1246679"
  },
  {
    "text": "compound AI maybe this is the simplest",
    "start": "1246679",
    "end": "1248720"
  },
  {
    "text": "possible compound AI but this is an llm",
    "start": "1248720",
    "end": "1252080"
  },
  {
    "text": "plus retrieval augmented generation so",
    "start": "1252080",
    "end": "1254679"
  },
  {
    "text": "we're going to have the race Service uh",
    "start": "1254679",
    "end": "1257280"
  },
  {
    "text": "serving the llm and we're going to have",
    "start": "1257280",
    "end": "1259640"
  },
  {
    "text": "another service in front of that race",
    "start": "1259640",
    "end": "1261360"
  },
  {
    "text": "service that will take the user's query",
    "start": "1261360",
    "end": "1264080"
  },
  {
    "text": "look up a context in a vector database",
    "start": "1264080",
    "end": "1267240"
  },
  {
    "text": "uh provide the context and the query to",
    "start": "1267240",
    "end": "1269679"
  },
  {
    "text": "the llm get back the result and give it",
    "start": "1269679",
    "end": "1271640"
  },
  {
    "text": "back to the user and so we're going to",
    "start": "1271640",
    "end": "1273559"
  },
  {
    "text": "have one cluster that's dedicated to",
    "start": "1273559",
    "end": "1275240"
  },
  {
    "text": "serving it'll have GPU resources um and",
    "start": "1275240",
    "end": "1278520"
  },
  {
    "text": "it will also just because we wanted to",
    "start": "1278520",
    "end": "1280600"
  },
  {
    "text": "throw this in it will be running the",
    "start": "1280600",
    "end": "1282320"
  },
  {
    "text": "cuay autoscaler and the Luna autoscaler",
    "start": "1282320",
    "end": "1285000"
  },
  {
    "text": "so the cuay will scale up and the um",
    "start": "1285000",
    "end": "1287400"
  },
  {
    "text": "Luna Auto scaler will respond by adding",
    "start": "1287400",
    "end": "1289279"
  },
  {
    "text": "more resources to the cluster similarly",
    "start": "1289279",
    "end": "1292440"
  },
  {
    "text": "we have the comp we have the you know",
    "start": "1292440",
    "end": "1294720"
  },
  {
    "text": "ingestion cluster that's reading in our",
    "start": "1294720",
    "end": "1297279"
  },
  {
    "text": "data applying the embeddings and putting",
    "start": "1297279",
    "end": "1299559"
  },
  {
    "text": "it in the vector database and in our",
    "start": "1299559",
    "end": "1301600"
  },
  {
    "text": "case in this example the ingestion isn't",
    "start": "1301600",
    "end": "1303679"
  },
  {
    "text": "happening all the time let's say the",
    "start": "1303679",
    "end": "1305000"
  },
  {
    "text": "ingestion is only happening once a week",
    "start": "1305000",
    "end": "1307400"
  },
  {
    "text": "so that cluster which is CPU let's say",
    "start": "1307400",
    "end": "1310120"
  },
  {
    "text": "because our ingestion embedding function",
    "start": "1310120",
    "end": "1312840"
  },
  {
    "text": "can run well on CPU will be cranked up",
    "start": "1312840",
    "end": "1315360"
  },
  {
    "text": "when the ingestion is running and will",
    "start": "1315360",
    "end": "1317360"
  },
  {
    "text": "be uh put into stand by and deleted from",
    "start": "1317360",
    "end": "1319640"
  },
  {
    "text": "the cloud when it's not running so we'll",
    "start": "1319640",
    "end": "1321720"
  },
  {
    "text": "do the labels and we'll uh do the",
    "start": "1321720",
    "end": "1323799"
  },
  {
    "text": "policies here so here we are with the",
    "start": "1323799",
    "end": "1327080"
  },
  {
    "text": "Clusters and applying the labels we're",
    "start": "1327080",
    "end": "1330200"
  },
  {
    "text": "going to deploy our cubr service",
    "start": "1330200",
    "end": "1334120"
  },
  {
    "text": "validate that it runs well with the llm",
    "start": "1334120",
    "end": "1337080"
  },
  {
    "text": "we're going to run our ingestion job",
    "start": "1337080",
    "end": "1338880"
  },
  {
    "text": "which will bring our ingestion cluster",
    "start": "1338880",
    "end": "1340279"
  },
  {
    "text": "out of standby and run that job for us",
    "start": "1340279",
    "end": "1342120"
  },
  {
    "text": "and create the vector database and then",
    "start": "1342120",
    "end": "1344159"
  },
  {
    "text": "we'll crank up our Ray plus llm serving",
    "start": "1344159",
    "end": "1347120"
  },
  {
    "text": "job on the workload cluster so now we",
    "start": "1347120",
    "end": "1348799"
  },
  {
    "text": "have the serving cluster with the two",
    "start": "1348799",
    "end": "1350679"
  },
  {
    "text": "serving jobs on it and the ingestion",
    "start": "1350679",
    "end": "1352919"
  },
  {
    "text": "cluster gets spun down and so we can see",
    "start": "1352919",
    "end": "1355919"
  },
  {
    "text": "now that the ray plus llm query can be",
    "start": "1355919",
    "end": "1359039"
  },
  {
    "text": "answered from the database that we",
    "start": "1359039",
    "end": "1360760"
  },
  {
    "text": "ingested into our Vector database um so",
    "start": "1360760",
    "end": "1364240"
  },
  {
    "text": "again I feel like this is shows bti",
    "start": "1364240",
    "end": "1365919"
  },
  {
    "text": "cluster deployment can be almost as",
    "start": "1365919",
    "end": "1367360"
  },
  {
    "text": "simple as singer cluster deployment with",
    "start": "1367360",
    "end": "1369520"
  },
  {
    "text": "the right policies in",
    "start": "1369520",
    "end": "1371640"
  },
  {
    "text": "place so in conclusion uh skyray",
    "start": "1371640",
    "end": "1375720"
  },
  {
    "text": "seamlessly extends cubay for",
    "start": "1375720",
    "end": "1377440"
  },
  {
    "text": "multicluster multi Cloud scheduling",
    "start": "1377440",
    "end": "1379720"
  },
  {
    "text": "scenarios the scenarios we've shown try",
    "start": "1379720",
    "end": "1382240"
  },
  {
    "text": "to accomplish things like reducing",
    "start": "1382240",
    "end": "1384279"
  },
  {
    "text": "launch time increasing efficiency",
    "start": "1384279",
    "end": "1386720"
  },
  {
    "text": "managing costs enhancing robustness",
    "start": "1386720",
    "end": "1389720"
  },
  {
    "text": "facilitating cluster maintenance and so",
    "start": "1389720",
    "end": "1391880"
  },
  {
    "text": "on and we hope that skyray can help you",
    "start": "1391880",
    "end": "1394559"
  },
  {
    "text": "with your multicluster scenarios with CU",
    "start": "1394559",
    "end": "1398120"
  },
  {
    "text": "and uh please you know use the open-",
    "start": "1398120",
    "end": "1400159"
  },
  {
    "text": "source text uh you know scripts and yaml",
    "start": "1400159",
    "end": "1404039"
  },
  {
    "text": "and if you want to try skyray in",
    "start": "1404039",
    "end": "1405960"
  },
  {
    "text": "particular with this fleet manager and",
    "start": "1405960",
    "end": "1408240"
  },
  {
    "text": "this cluster Auto scaler of course it",
    "start": "1408240",
    "end": "1409799"
  },
  {
    "text": "would work with others uh there's three",
    "start": "1409799",
    "end": "1411520"
  },
  {
    "text": "versions available for you to give it a",
    "start": "1411520",
    "end": "1413279"
  },
  {
    "text": "try uh so thank you very much and I'm",
    "start": "1413279",
    "end": "1415840"
  },
  {
    "text": "happy to take any questions if there's",
    "start": "1415840",
    "end": "1417360"
  },
  {
    "text": "time um otherwise catch me in the hall",
    "start": "1417360",
    "end": "1421050"
  },
  {
    "text": "[Applause]",
    "start": "1421050",
    "end": "1424359"
  }
]