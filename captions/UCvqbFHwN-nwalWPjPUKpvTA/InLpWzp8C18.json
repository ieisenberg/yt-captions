[
  {
    "text": "we're gonna be talking about using pet set to automate PostgreSQL",
    "start": "469",
    "end": "5839"
  },
  {
    "text": "this is mainly you know about how pets that solves a whole bunch of problems for us in doing containerize databases",
    "start": "5839",
    "end": "15740"
  },
  {
    "text": "for anybody with anybody here at my presentation in London so in London I actually presented",
    "start": "15740",
    "end": "23390"
  },
  {
    "text": "the the early work in progress on this and this is actually pretty much the",
    "start": "23390",
    "end": "29400"
  },
  {
    "text": "completed thing so first of all I want to start out with a thank you to zalando",
    "start": "29400",
    "end": "35300"
  },
  {
    "text": "so lando is mega fashion retailer in europe and they've been doing a lot of",
    "start": "35300",
    "end": "42629"
  },
  {
    "text": "work on Patroni here and on automation and that sort of thing so the lion's share of what I would show you was done",
    "start": "42629",
    "end": "49170"
  },
  {
    "text": "by this on-land of folks so talk about pet set well actually no",
    "start": "49170",
    "end": "55230"
  },
  {
    "text": "we're not gonna talk about pet said when talk about stateful said anybody here missed the rename on that",
    "start": "55230",
    "end": "61370"
  },
  {
    "text": "the I mean the rename was a good idea pet set was a a working name and it was",
    "start": "61370",
    "end": "68909"
  },
  {
    "text": "never supposed to be the final name there's all about stateful containers so",
    "start": "68909",
    "end": "75750"
  },
  {
    "text": "how many people here know what pet said what stateful set does",
    "start": "75750",
    "end": "83090"
  },
  {
    "text": "okay and how many people here had never really heard of it before they looked at the program",
    "start": "83090",
    "end": "88640"
  },
  {
    "text": "everybody else is sort of in between so I will show you a little bit of that in",
    "start": "88640",
    "end": "94619"
  },
  {
    "text": "action now the reason I got involved with kubernetes to begin with is because",
    "start": "94619",
    "end": "101070"
  },
  {
    "text": "I wanted to automate Postgres that was the whole thing is I've been working on the posters project for 18 years and so",
    "start": "101070",
    "end": "108479"
  },
  {
    "text": "kubernetes comes along and I'm like oh wow a tool to solve a bunch of the automation problems that I have but the",
    "start": "108479",
    "end": "114689"
  },
  {
    "text": "tool wasn't quite ready to actually solve those automation problems now one",
    "start": "114689",
    "end": "119729"
  },
  {
    "text": "of the other projects has been working on is something called Patroni Patroni is a demon that managed as",
    "start": "119729",
    "end": "126509"
  },
  {
    "text": "Postgres and postcodes replication and postcodes replication clusters it's a way to",
    "start": "126509",
    "end": "132690"
  },
  {
    "text": "automate management - vossler's replication hi this is the thing the",
    "start": "132690",
    "end": "137819"
  },
  {
    "text": "zolina folks have put in so much work on it's an offshoot of composed iOS",
    "start": "137819",
    "end": "143900"
  },
  {
    "text": "thing but the important thing for you to know is it's a demon and it controls",
    "start": "143900",
    "end": "149940"
  },
  {
    "text": "Postgres and when you're launching containerized postcodes it runs as the primary process in the container",
    "start": "149940",
    "end": "156500"
  },
  {
    "text": "which is important for automation because it means that I know that if Patroni is not running then Postgres is",
    "start": "156500",
    "end": "163049"
  },
  {
    "text": "not running which eliminates a whole set of split brain cases so an Patroni works",
    "start": "163049",
    "end": "169950"
  },
  {
    "text": "in production you know running on the land oh and hundreds of database servers and that sort of thing running in a few",
    "start": "169950",
    "end": "175799"
  },
  {
    "text": "other sites but outside of kubernetes it's not not through kubernetes and so we actually want to automated this in",
    "start": "175799",
    "end": "182250"
  },
  {
    "text": "kubernetes now one of the important things is that here we're talking about single master replication with postcodes",
    "start": "182250",
    "end": "188519"
  },
  {
    "text": "the primary way to actually have a bunch of postman instances right you got one master and you have",
    "start": "188519",
    "end": "194750"
  },
  {
    "text": "zero too many replicas and this is still you know there's more advanced clustered",
    "start": "194750",
    "end": "202530"
  },
  {
    "text": "versions of post post and development but a lot of them are fairly alpha at this point plus for simple cases single",
    "start": "202530",
    "end": "209100"
  },
  {
    "text": "master replication is just easier to manage and easier to understand but that means that when we're automating it we",
    "start": "209100",
    "end": "214769"
  },
  {
    "text": "have to play this game called where's the master right because we want to deploy a bunch of identical Postma's",
    "start": "214769",
    "end": "221040"
  },
  {
    "text": "nodes and have the Postma's master just come up and the rest become replicas that master",
    "start": "221040",
    "end": "229560"
  },
  {
    "text": "then something to determine that and that's actually what Patroni does but when we were trying to automate what I",
    "start": "229560",
    "end": "236549"
  },
  {
    "text": "was trying to automate this through kubernetes I ran into a number of problems doing this in kubernetes 1.2",
    "start": "236549",
    "end": "243980"
  },
  {
    "text": "and I'll explain these problems to deal but this problem is basically amounted to node identity replication connections",
    "start": "243980",
    "end": "250380"
  },
  {
    "text": "connections to the master storage persistence and shard and",
    "start": "250380",
    "end": "257560"
  },
  {
    "text": "now that I'm actually working uncharted Postgres I don't I won't be showing you a working copy that today but I will",
    "start": "257560",
    "end": "262600"
  },
  {
    "text": "have one online next week the four started poster is shard",
    "start": "262600",
    "end": "267910"
  },
  {
    "text": "identity and these things were very hard to solvent in one point to this the",
    "start": "267910",
    "end": "273700"
  },
  {
    "text": "first thing was note identity and that he is that if you deployed a group of post res instances as a",
    "start": "273700",
    "end": "279419"
  },
  {
    "text": "replica set then you didn't really have a good way",
    "start": "279419",
    "end": "285520"
  },
  {
    "text": "to address them individually and that became a real problem because you",
    "start": "285520",
    "end": "291100"
  },
  {
    "text": "might want to say okay I want a load balance two replicas one through five but then replica six is my special",
    "start": "291100",
    "end": "297940"
  },
  {
    "text": "reporting replica because it's going to have the month end reports on it and I want to put anything else there and replica seven is gonna be supporting",
    "start": "297940",
    "end": "305229"
  },
  {
    "text": "continuous backup and so again I don't want to put any other load on that and there weren't any non hackish way is to",
    "start": "305229",
    "end": "312250"
  },
  {
    "text": "do this before stateful said because you didn't have an address that kubernetes understood that said I want this",
    "start": "312250",
    "end": "318850"
  },
  {
    "text": "particular replica so what we would and similar to this problem is something we",
    "start": "318850",
    "end": "324760"
  },
  {
    "text": "call replication identity which is that the current master needs to know the",
    "start": "324760",
    "end": "330160"
  },
  {
    "text": "identity of all the replicas that are connected to it among other things the master needs to",
    "start": "330160",
    "end": "335860"
  },
  {
    "text": "know if replicas get out of sync it needs to know what data it has to save for those out of sync replicas something",
    "start": "335860",
    "end": "341410"
  },
  {
    "text": "called replication slots which which actually has a more messy problem that'll explain it in a minute and so we",
    "start": "341410",
    "end": "347950"
  },
  {
    "text": "also need to the master also needs to have an identity for each of those replicas so the way that we tried to do",
    "start": "347950",
    "end": "354760"
  },
  {
    "text": "this under earlier versions of kubernetes was to just push the pod IP address to say okay we've got these pot",
    "start": "354760",
    "end": "361060"
  },
  {
    "text": "IP addresses they're unique you can actually connect to the individual nodes for that and so we're going to push the pot IP addresses and use that as our",
    "start": "361060",
    "end": "368050"
  },
  {
    "text": "de-facto node identity and hang everything else off of that well there a number of problems with",
    "start": "368050",
    "end": "373870"
  },
  {
    "text": "using the pot IP addresses one of them was that that doesn't really mesh with",
    "start": "373870",
    "end": "380080"
  },
  {
    "text": "kubernetes services so it meant that we couldn't actually Bernays services to funnel connections to the replicas",
    "start": "380080",
    "end": "387280"
  },
  {
    "text": "because kubernetes services don't pay any attention to the pot IP for a variety of reasons but then there were",
    "start": "387280",
    "end": "393850"
  },
  {
    "text": "also the thorny problems right if kubernetes needs to migrate stuff due to resource usage sometimes the IPS will",
    "start": "393850",
    "end": "402039"
  },
  {
    "text": "change the if the IP is change and you cycle",
    "start": "402039",
    "end": "407380"
  },
  {
    "text": "through your set IPS can get reused the there isn't really any understanding",
    "start": "407380",
    "end": "413860"
  },
  {
    "text": "of continuity that this IP address is associated with the particular post resistance and the it's stored database",
    "start": "413860",
    "end": "420389"
  },
  {
    "text": "I am that didn't help us to have a set",
    "start": "420389",
    "end": "426160"
  },
  {
    "text": "connection to just the master for readwrite connections there also were some other permutations",
    "start": "426160",
    "end": "433660"
  },
  {
    "text": "of it that were really messy to solve and will quiet hack as things and one of",
    "start": "433660",
    "end": "439360"
  },
  {
    "text": "these give you an illustration this is through something called replication slots so they do a replication slot in post",
    "start": "439360",
    "end": "445810"
  },
  {
    "text": "Prezi is you basically put every time the replica slips down new data from the master you know data that's changed",
    "start": "445810",
    "end": "452860"
  },
  {
    "text": "table updates and inserts and that sort of thing it puts a bookmark that tells the master where it left off in case it",
    "start": "452860",
    "end": "459010"
  },
  {
    "text": "loses the connection and that's important because if you do lose the connection between the replicas in the master you wanted to pick up when it's",
    "start": "459010",
    "end": "464830"
  },
  {
    "text": "left off so the master isn't saving any more data than it absolutely has to because it can if it saves too much",
    "start": "464830",
    "end": "471340"
  },
  {
    "text": "storage can really balloon well the problem is that if you have a lot of sort of intermediate failures and you",
    "start": "471340",
    "end": "478210"
  },
  {
    "text": "have replicas coming up and down all the time some of those IP addresses are going to change and so what happens he is what",
    "start": "478210",
    "end": "485620"
  },
  {
    "text": "was dot one goes down and it gets replaced with a new replica but that new",
    "start": "485620",
    "end": "491199"
  },
  {
    "text": "replicas and dot three well that replication slot for dot one still exists only it's not being used",
    "start": "491199",
    "end": "499050"
  },
  {
    "text": "and because it's not being used it's just accumulating data and so if you had",
    "start": "499050",
    "end": "504400"
  },
  {
    "text": "a series of failures that was causing you forcing you to bring down up and down a lot of replicas you actually end",
    "start": "504400",
    "end": "511509"
  },
  {
    "text": "up with a pileup of replication slots that are not and they're accumulating data on the master and if you have a",
    "start": "511509",
    "end": "517909"
  },
  {
    "text": "very high transaction rate you may have very little time to react to us and so then that forces you to have a watchdog",
    "start": "517910",
    "end": "524540"
  },
  {
    "text": "process that looks for replication slots that are not being used anymore and",
    "start": "524540",
    "end": "530180"
  },
  {
    "text": "deletes them so that they're not storing up that anymore but there's a problem with",
    "start": "530180",
    "end": "535280"
  },
  {
    "text": "learning that watchdog process if the watchdog process reacts too fast then the replica comes back and it can't get",
    "start": "535280",
    "end": "541460"
  },
  {
    "text": "data anymore because it's slot is gone so not really a solvable problem in the",
    "start": "541460",
    "end": "547700"
  },
  {
    "text": "context of kubernetes 1.2 and I feel kind of illustrates one of the the issues that we were having without stateful sets",
    "start": "547700",
    "end": "554440"
  },
  {
    "text": "so there were a lot of sort of workarounds in this one was that implementing this in like",
    "start": "554440",
    "end": "560330"
  },
  {
    "text": "1.2 we actually ended up failing over a lot more often than we really needed to you a custom proxy for handling master",
    "start": "560330",
    "end": "568700"
  },
  {
    "text": "etc connections was absolutely essential then that custom proxy actually has to read separate information that's outside",
    "start": "568700",
    "end": "575270"
  },
  {
    "text": "of kubernetes your discovery basically does not use qu Bernays discovery you have to create",
    "start": "575270",
    "end": "581630"
  },
  {
    "text": "headless services to front these things in the headless services connect to pod IPs and if anybody has done that before",
    "start": "581630",
    "end": "587660"
  },
  {
    "text": "you know it's a little unreliable again because those IPS can change around and",
    "start": "587660",
    "end": "593060"
  },
  {
    "text": "then you know I'll watchdog or cleanup process to actually deal with all of the sort of fallout for things that don't",
    "start": "593060",
    "end": "599150"
  },
  {
    "text": "just work and so it ends up being a relatively sort of complicated topology",
    "start": "599150",
    "end": "604940"
  },
  {
    "text": "even at on a minimal thing right you've gotta at least have in addition to kubernetes and a separate etcd or other",
    "start": "604940",
    "end": "611150"
  },
  {
    "text": "dcs cluster you have to have your post businesses running patron you have to have a custom proxy and you actually",
    "start": "611150",
    "end": "617030"
  },
  {
    "text": "have to have some kind of a cleanup watchdog service and in order for all of this to work",
    "start": "617030",
    "end": "622510"
  },
  {
    "text": "which is messy and and what I really hated about it is it meant that most of",
    "start": "622510",
    "end": "628520"
  },
  {
    "text": "my automating a post process was happening outside of kubernetes all the kubernetes was doing for me was making",
    "start": "628520",
    "end": "634430"
  },
  {
    "text": "sure that I had a certain number of Postgres containers up and running and the proc you know these other processes",
    "start": "634430",
    "end": "640100"
  },
  {
    "text": "were up and running but all the other animation took place outside of goober neighs who's not what I wanted out of kubernetes I wanted kubernetes to do the",
    "start": "640100",
    "end": "646070"
  },
  {
    "text": "automation for me so there are also some other issues storage",
    "start": "646070",
    "end": "652490"
  },
  {
    "text": "issues and that this was even worse and sort of fundamentally insoluble which is that if you wanted to have persistent",
    "start": "652490",
    "end": "659900"
  },
  {
    "text": "volumes to store your databases then there was no understanding that the",
    "start": "659900",
    "end": "666800"
  },
  {
    "text": "different replicas would have separate persistent volume claims unless you actually effectively did all that by",
    "start": "666800",
    "end": "673160"
  },
  {
    "text": "hand again which meant which didn't work for databases at all because right they all need read right to their own storage and",
    "start": "673160",
    "end": "680810"
  },
  {
    "text": "there was no opportunity to reuse a persistent volume those associated with particular replicas if that replica got",
    "start": "680810",
    "end": "686810"
  },
  {
    "text": "terminated and brought up on another node one of the ways we actually handled this",
    "start": "686810",
    "end": "693380"
  },
  {
    "text": "was to do daemon set a beaut we called demon set abuse or even though we have is a group of replicas we treated as a",
    "start": "693380",
    "end": "698570"
  },
  {
    "text": "daemon set and then his host path to actually join to some sort of network storage and therefore each node has",
    "start": "698570",
    "end": "704590"
  },
  {
    "text": "access to its own claim and network storage were to handle the volume management",
    "start": "704590",
    "end": "710060"
  },
  {
    "text": "basically in the application and I didn't want to think about doing",
    "start": "710060",
    "end": "715400"
  },
  {
    "text": "this charted Postgres under under kubernetes won't play - this is just be a nightmare yeah you know effectively you start",
    "start": "715400",
    "end": "722420"
  },
  {
    "text": "looking at you start saying maybe this will be easier to do without kubernetes it would be but now we have stateful set",
    "start": "722420",
    "end": "728450"
  },
  {
    "text": "and things are beautiful so",
    "start": "728450",
    "end": "733690"
  },
  {
    "text": "our architecture diagram gets a lot simpler under stateful set",
    "start": "733690",
    "end": "740710"
  },
  {
    "text": "for your single master Patroni Postgres right we need kubernetes and we need a",
    "start": "740710",
    "end": "746330"
  },
  {
    "text": "distributed configuration management cluster listen instance etcd and then we've got post quiz and number of post",
    "start": "746330",
    "end": "752390"
  },
  {
    "text": "questions this is running the Patroni daemon and they're communicating with the ECT and they're communicating with",
    "start": "752390",
    "end": "758300"
  },
  {
    "text": "kubernetes and that's all that you need so let's actually because this will take",
    "start": "758300",
    "end": "763910"
  },
  {
    "text": "a couple minutes to bring up so",
    "start": "763910",
    "end": "768759"
  },
  {
    "text": "so I've gotten my etcd cluster up but I do not have any posters up",
    "start": "774600",
    "end": "780350"
  },
  {
    "text": "so we're going to actually sort of start bringing it up will actually explain what's in it",
    "start": "785090",
    "end": "791150"
  },
  {
    "text": "so there are basically we've got five components here",
    "start": "793160",
    "end": "799770"
  },
  {
    "text": "we have again distributed configuration service cluster the headless service that you need to",
    "start": "799770",
    "end": "806130"
  },
  {
    "text": "create for a Repat set and the sustainer with patsaks don't show you that in a minute every stateful set a stateful set",
    "start": "806130",
    "end": "811320"
  },
  {
    "text": "of replicas running Patroni a master only service at a minimum you can also create a separate load balancing",
    "start": "811320",
    "end": "817710"
  },
  {
    "text": "service for reeds depending on your use case and then optionally a persistent",
    "start": "817710",
    "end": "823560"
  },
  {
    "text": "volume claim or president volume template which is how you use it in stateful set i for storage for durable",
    "start": "823560",
    "end": "832680"
  },
  {
    "text": "storage for the databases now the reason why i say that that's optional is that you don't always want",
    "start": "832680",
    "end": "838730"
  },
  {
    "text": "and largely what that depends on is how big your databases are going to be so",
    "start": "838730",
    "end": "847070"
  },
  {
    "text": "the so if you have a large database then",
    "start": "847070",
    "end": "853680"
  },
  {
    "text": "you're going to definitely want persistent volumes no one wants store that large database on persistent volumes so that you can actually reuse",
    "start": "853680",
    "end": "859080"
  },
  {
    "text": "the data and if you have a bunch of small databases it's actually a lot easier to use a ephemeral storage that",
    "start": "859080",
    "end": "865800"
  },
  {
    "text": "is node storage via empty there and big here is defined as how long does it",
    "start": "865800",
    "end": "872070"
  },
  {
    "text": "take you to make a copy of the database via the replication protocol right if",
    "start": "872070",
    "end": "877680"
  },
  {
    "text": "the answer is a very short time the answer is a couple of seconds then you're easier creating lots of ephemeral copies if the answer is an hour or more",
    "start": "877680",
    "end": "885930"
  },
  {
    "text": "then you're probably a lot better off using persistent volume clamps and that's just a you know sort of",
    "start": "885930",
    "end": "892890"
  },
  {
    "text": "application specific decision so let's actually show you that in terms of setup",
    "start": "892890",
    "end": "899270"
  },
  {
    "text": "so now by the way if you have any trouble seeing this on this if you go to",
    "start": "899270",
    "end": "906350"
  },
  {
    "text": "github.com J burkas atomic DB the code is actually",
    "start": "906350",
    "end": "912980"
  },
  {
    "text": "in there so you can get a closer look at it but I'll show you this here so I first we have to bring up an HDD",
    "start": "912980",
    "end": "919850"
  },
  {
    "text": "cluster and by the way I don't know if you've looked at like doing this under stateful set but this made doing doing",
    "start": "919850",
    "end": "925310"
  },
  {
    "text": "this particular action much less complicated like this is the entire business to bring up an EDC cluster",
    "start": "925310",
    "end": "930649"
  },
  {
    "text": "that's using empty ddr4 node data you know we've got our",
    "start": "930649",
    "end": "936670"
  },
  {
    "text": "hitless service which is a requirement of a pet set and",
    "start": "936670",
    "end": "942760"
  },
  {
    "text": "you know secrete this headless service requirement of a pet set we've got our secrets and then we have",
    "start": "942760",
    "end": "950110"
  },
  {
    "text": "the the actual pet which is still a pet set in the API we've renamed it but we",
    "start": "950110",
    "end": "956899"
  },
  {
    "text": "haven't actually released a code version with the change and so that's such a good example also for if you have to",
    "start": "956899",
    "end": "962630"
  },
  {
    "text": "bring up an ATC a 3-node easy cluster the particular thing that we actually get out of this out of stateful set is",
    "start": "962630",
    "end": "969820"
  },
  {
    "text": "when you do a number when we bring up a bunch of stateful set pods as",
    "start": "969820",
    "end": "977020"
  },
  {
    "text": "you'll see the our our node names here are predictably",
    "start": "977020",
    "end": "985760"
  },
  {
    "text": "incremental and because they're predictably incremental if I'm bringing up something like any TCG cluster then I",
    "start": "985760",
    "end": "994279"
  },
  {
    "text": "can actually give it an initial cluster configuration and pass that as an environment variable",
    "start": "994279",
    "end": "1000209"
  },
  {
    "text": "so that it will just come up rather than",
    "start": "1000209",
    "end": "1005339"
  },
  {
    "text": "to show the counter example go to I think that core OS in their site has a",
    "start": "1005339",
    "end": "1010690"
  },
  {
    "text": "pre stateful set example bringing up a three node etz cluster and it's several pages long yes I",
    "start": "1010690",
    "end": "1017970"
  },
  {
    "text": "don't recommend using cout burn the the e 2zii cluster that kubernetes uses for",
    "start": "1019589",
    "end": "1025030"
  },
  {
    "text": "its own management I don't recommend using the same one for apps bring up a second one for apps there's a couple of",
    "start": "1025030",
    "end": "1031449"
  },
  {
    "text": "reasons for that one is potentially performance because if kubernetes if the easy clusters so busy kubernetes can't",
    "start": "1031449",
    "end": "1039000"
  },
  {
    "text": "can't ask it things then your whole cluster is paralyzed and the big role in the security right is the I'm gonna give",
    "start": "1039000",
    "end": "1046808"
  },
  {
    "text": "you an example here actually the easy that kubernetes is running on here is actually secured",
    "start": "1046809",
    "end": "1052020"
  },
  {
    "text": "SSL etc and and I've actually brought up this one for the my demo here completely",
    "start": "1052020",
    "end": "1057520"
  },
  {
    "text": "unsecured so yeah the",
    "start": "1057520",
    "end": "1062640"
  },
  {
    "text": "so but that's etz and you came here to see Postgres so we've got a couple of",
    "start": "1062640",
    "end": "1068950"
  },
  {
    "text": "things for for sending in Postgres right we've got obviously you have to have some secrets",
    "start": "1068950",
    "end": "1074250"
  },
  {
    "text": "in a real production thing i would actually be using probably hashey core vault for for these for rotation etc the",
    "start": "1074250",
    "end": "1084300"
  },
  {
    "text": "yeah the things that secrets in kubernetes are really not that secret you know i mean they're base64 encoded",
    "start": "1084300",
    "end": "1090429"
  },
  {
    "text": "there's a lot of ways to get to the information there's no you know enforced rotation or anything like that",
    "start": "1090429",
    "end": "1097120"
  },
  {
    "text": "the something like vault actually can do that and one of the reasons i you like to use ball with post coasters that",
    "start": "1097120",
    "end": "1102550"
  },
  {
    "text": "vault has a plug-in for Postgres so that i can actually enforce credential rotation and have it work inside",
    "start": "1102550",
    "end": "1108190"
  },
  {
    "text": "Postgres the so and then we have our cluster and it's",
    "start": "1108190",
    "end": "1117100"
  },
  {
    "text": "actually quite simple so again first of all you create this headless service right we've got and an api version we've got our service",
    "start": "1117100",
    "end": "1125260"
  },
  {
    "text": "here these components are just labels to grab it and so and the important thing here",
    "start": "1125260",
    "end": "1132520"
  },
  {
    "text": "is we're selecting off the labels that we've got on the items and one of the",
    "start": "1132520",
    "end": "1139720"
  },
  {
    "text": "things that's actually I need to add a doc patch for cuz it's not it's there in the examples but it's not documented if",
    "start": "1139720",
    "end": "1145570"
  },
  {
    "text": "you're actually creating the headless service for a pet set cluster IP has to be none",
    "start": "1145570",
    "end": "1150780"
  },
  {
    "text": "we don't get a cluster IP from that the and it will just mysteriously not work",
    "start": "1150799",
    "end": "1156440"
  },
  {
    "text": "if you don't set that the and so then we've got our pet set configuration here",
    "start": "1156440",
    "end": "1161720"
  },
  {
    "text": "right and so we've got our basic declaration the labels and that sort of thing but part of it starts out your pets that starts out with we've got a",
    "start": "1161720",
    "end": "1168590"
  },
  {
    "text": "service name and that service name needs to relate to the head needs to be the same as the",
    "start": "1168590",
    "end": "1173779"
  },
  {
    "text": "headless service we've just created what's better to do them in in one go the number of initial replicas that we",
    "start": "1173779",
    "end": "1180590"
  },
  {
    "text": "have template with metadata and a bunch of",
    "start": "1180590",
    "end": "1186559"
  },
  {
    "text": "other things the and here's our container version and",
    "start": "1186559",
    "end": "1191960"
  },
  {
    "text": "then here's a whole bunch of environment variables I'm passing in in order to create Petronius initial configuration",
    "start": "1191960",
    "end": "1198950"
  },
  {
    "text": "which includes things like it needs to know where to find Det CD it needs to know what cluster it's a member of",
    "start": "1198950",
    "end": "1205570"
  },
  {
    "text": "it needs to I set some initial passwords and security information for Postgres",
    "start": "1205570",
    "end": "1211340"
  },
  {
    "text": "because although as you can't connect and then I need to actually have some",
    "start": "1211340",
    "end": "1217429"
  },
  {
    "text": "downward API information in order to do discovery because the nodes need to not",
    "start": "1217429",
    "end": "1223640"
  },
  {
    "text": "only be discoverable about the applications but they need to discover each other and so we need to pass that",
    "start": "1223640",
    "end": "1228710"
  },
  {
    "text": "that information which is all available via the download API I",
    "start": "1228710",
    "end": "1235450"
  },
  {
    "text": "Patroni actually has two ports five four three two zero recognizes the post press port",
    "start": "1235659",
    "end": "1240710"
  },
  {
    "text": "container port is that's actually the patron II API port which is important because we need",
    "start": "1240710",
    "end": "1248690"
  },
  {
    "text": "to be able to get information from that the nodes need to communicate with each other and then some volume mountains for",
    "start": "1248690",
    "end": "1254000"
  },
  {
    "text": "data etc and these are running FML because they",
    "start": "1254000",
    "end": "1260270"
  },
  {
    "text": "come up a lot faster if I do that so actually you know what I'm gonna kill these again because",
    "start": "1260270",
    "end": "1267580"
  },
  {
    "text": "because I want to show you another important thing about how stateful set works",
    "start": "1274400",
    "end": "1280450"
  },
  {
    "text": "so one of them's you notice what stateful set here is it actually one of",
    "start": "1281020",
    "end": "1286550"
  },
  {
    "text": "its features is it creates the nodes in numerical order so those numbers that",
    "start": "1286550",
    "end": "1291950"
  },
  {
    "text": "we're getting here petronia 0 p21 etc those are assigned by the stateful set API I didn't actually create those names",
    "start": "1291950",
    "end": "1299650"
  },
  {
    "text": "and so it'll bring them up there ok and so we've got all of them up now",
    "start": "1299650",
    "end": "1305870"
  },
  {
    "text": "and so then the other question becomes well where's the master right because part of this whole game is where's the master and so one of the things and this",
    "start": "1305870",
    "end": "1313130"
  },
  {
    "text": "is something that Alex and I think Alex at zalando figured",
    "start": "1313130",
    "end": "1318410"
  },
  {
    "text": "out is that in order to do mm-hmm all of",
    "start": "1318410",
    "end": "1324830"
  },
  {
    "text": "this nope they're not finished coming up yet okay",
    "start": "1324830",
    "end": "1331300"
  },
  {
    "text": "well",
    "start": "1333400",
    "end": "1336400"
  },
  {
    "text": "let's make sure we're not blowing up ah that's why hold on just a moment",
    "start": "1343580",
    "end": "1350020"
  },
  {
    "text": "see this is the problem with doing this multiple times",
    "start": "1350020",
    "end": "1354610"
  },
  {
    "text": "actually none the because I killed it off and brought",
    "start": "1389420",
    "end": "1394500"
  },
  {
    "text": "it back up it's trying to reinitialize the same cluster which it does not do in an automated way because",
    "start": "1394500",
    "end": "1401960"
  },
  {
    "text": "you could lose data if it if it we initialized a cluster that had already been previously initialized but that",
    "start": "1401960",
    "end": "1409740"
  },
  {
    "text": "means that I actually need to kill off",
    "start": "1409740",
    "end": "1412910"
  },
  {
    "text": "oh right",
    "start": "1423100",
    "end": "1426000"
  },
  {
    "text": "there we go so so in a couple of seconds that should",
    "start": "1442559",
    "end": "1449749"
  },
  {
    "text": "actually figure out that there we go it should figure out that there's no longer",
    "start": "1449749",
    "end": "1454940"
  },
  {
    "text": "a leader and so then when it figures out there's no longer leader it holds a leader election among the notes now when",
    "start": "1454940",
    "end": "1461269"
  },
  {
    "text": "you're doing an initial deployment if we hadn't had that initialized key blocking it would always end up being node zero",
    "start": "1461269",
    "end": "1466909"
  },
  {
    "text": "because that one comes up first right but in this case because we remove the key it holds an election and it's",
    "start": "1466909",
    "end": "1472220"
  },
  {
    "text": "largely timing based so if we actually look at this so this is what else figured out is that we can actually",
    "start": "1472220",
    "end": "1478970"
  },
  {
    "text": "abuse kubernetes annotations for our where's the master instead of requiring",
    "start": "1478970",
    "end": "1484759"
  },
  {
    "text": "an external proxy to do this so this Patroni rule tag",
    "start": "1484759",
    "end": "1490210"
  },
  {
    "text": "doesn't exist when we do the initial deployment and if you actually have a tag that doesn't exist when you do the",
    "start": "1490210",
    "end": "1496580"
  },
  {
    "text": "initial deployment then the individual application can use the",
    "start": "1496580",
    "end": "1502509"
  },
  {
    "text": "service API in order to add tags that it owns and",
    "start": "1502509",
    "end": "1508360"
  },
  {
    "text": "that means that we can it can basically set which one is",
    "start": "1508360",
    "end": "1514730"
  },
  {
    "text": "the master and which one's the replica now once we've set which one's the master and which one's the replica then",
    "start": "1514730",
    "end": "1520070"
  },
  {
    "text": "we want the applications to connect to it right because this headless service is really just for the nodes to communicate with each other we can",
    "start": "1520070",
    "end": "1525889"
  },
  {
    "text": "actually use it internally to communicate but it's a lot better to create a separate service for the applications so",
    "start": "1525889",
    "end": "1531220"
  },
  {
    "text": "the weed service is easy because the read service is let's load balance among",
    "start": "1531220",
    "end": "1536720"
  },
  {
    "text": "all of the nodes right the tricky one is the master service",
    "start": "1536720",
    "end": "1542440"
  },
  {
    "text": "that's the master service we actually create this selector that grabs that",
    "start": "1542440",
    "end": "1547730"
  },
  {
    "text": "role key",
    "start": "1547730",
    "end": "1550240"
  },
  {
    "text": "so it's actually",
    "start": "1553580",
    "end": "1556908"
  },
  {
    "text": "okay we've actually already got those up and running so",
    "start": "1561500",
    "end": "1567799"
  },
  {
    "text": "prove this is works I've got another container running with the posters client tools in there and we've got psql",
    "start": "1568250",
    "end": "1574590"
  },
  {
    "text": "here so we should be able to connect",
    "start": "1574590",
    "end": "1580460"
  },
  {
    "text": "so this shows that we're actually on a writable node the pages in recovery is a Maya replica it's our cryptic way of",
    "start": "1587280",
    "end": "1593610"
  },
  {
    "text": "saying in my replica so on a writable note here",
    "start": "1593610",
    "end": "1598190"
  },
  {
    "text": "now of course this would be no fun if we can't actually manipulate this so first let's actually scale this right",
    "start": "1598730",
    "end": "1604910"
  },
  {
    "text": "[Music]",
    "start": "1604910",
    "end": "1608019"
  },
  {
    "text": "oh right research name",
    "start": "1618270",
    "end": "1623450"
  },
  {
    "text": "so those will take a minute to come up there actually they're coming up really fast so the new ones that come up",
    "start": "1627559",
    "end": "1634710"
  },
  {
    "text": "obviously they come up as replicas and they join the masters part of the replica set they pull down a copy the database they start replication",
    "start": "1634710",
    "end": "1639950"
  },
  {
    "text": "automatically right but then the other fun thing is that of course Phil over needs to work",
    "start": "1639950",
    "end": "1646790"
  },
  {
    "text": "oh yes good point",
    "start": "1659800",
    "end": "1664200"
  },
  {
    "text": "and so hopes it's not what I want to",
    "start": "1664880",
    "end": "1670220"
  },
  {
    "text": "make me in history so this will take a few seconds to actually realize that the Masters gone",
    "start": "1670220",
    "end": "1678110"
  },
  {
    "text": "and to hold a new leader election the leader election operates in two stages basically first stage is standard leader",
    "start": "1678110",
    "end": "1683960"
  },
  {
    "text": "election via the dcs to see who grabs the leader lock second stage is checking the replication status of each node to",
    "start": "1683960",
    "end": "1690560"
  },
  {
    "text": "make sure that we're not electing a leader whose replication is broken or that's really far behind",
    "start": "1690560",
    "end": "1697420"
  },
  {
    "text": "there we go and we have a new master and the other thing that dopes",
    "start": "1697420",
    "end": "1703840"
  },
  {
    "text": "should happen is that then node one gets replaced automatically back patron Ian",
    "start": "1703840",
    "end": "1709070"
  },
  {
    "text": "then joins replication so that's not working it's pretty",
    "start": "1709070",
    "end": "1714950"
  },
  {
    "text": "awesome exit there's a few there's a few small issues that I'm going to go over but as far as automating single master",
    "start": "1714950",
    "end": "1721010"
  },
  {
    "text": "postcodes is now that we have stateful set it basically just works now you can add extra refinements on in real",
    "start": "1721010",
    "end": "1727190"
  },
  {
    "text": "production circumstance I would actually want things like an actual connection proxy for the applications and the",
    "start": "1727190",
    "end": "1733010"
  },
  {
    "text": "connection proxy would plug into the service and you know all of the other usual tooling you need like a backup",
    "start": "1733010",
    "end": "1739220"
  },
  {
    "text": "container to do automated backups and a bunch of other things but for a basic setup of hey I just want H a post press",
    "start": "1739220",
    "end": "1745100"
  },
  {
    "text": "it now all just works and kubernetes makes it happen which is awesome now there are a few things in this setup",
    "start": "1745100",
    "end": "1751220"
  },
  {
    "text": "that are not ideal one is if you're relying on DNS via",
    "start": "1751220",
    "end": "1756920"
  },
  {
    "text": "kubernetes services if your application relies on persistent connections of some kind or your connection pool relies on",
    "start": "1756920",
    "end": "1763580"
  },
  {
    "text": "persistent connections it's not this is really going to behave well when we do have those fail overs or when nodes get",
    "start": "1763580",
    "end": "1770060"
  },
  {
    "text": "migrated what it depends on your implementation",
    "start": "1770060",
    "end": "1775880"
  },
  {
    "text": "on your reconnection implementation and in that II is",
    "start": "1775880",
    "end": "1783550"
  },
  {
    "text": "that is any of this person connection is not going to look up the address well as long as it thinks it has a valid connection so one things can happen is",
    "start": "1783550",
    "end": "1791440"
  },
  {
    "text": "you don't actually get a failure until somebody tries to send data and then if",
    "start": "1791440",
    "end": "1798620"
  },
  {
    "text": "it's a badly programmed proxy like a lot of the Tomcat proxies I've seen it may not try to reconnect at any intelligent",
    "start": "1798620",
    "end": "1804680"
  },
  {
    "text": "way it may try to reconnect to a caste connection which is now wrong so you may need to actually do some",
    "start": "1804680",
    "end": "1811070"
  },
  {
    "text": "refactoring of your connection logic and applications for this to just work for you and like PG bouncer a lot of people",
    "start": "1811070",
    "end": "1817850"
  },
  {
    "text": "use the PG bouncer went back ends go wait for PG bouncer because PG bouncer is nice for scaling lots of connections for post Prez but when backends go away",
    "start": "1817850",
    "end": "1825230"
  },
  {
    "text": "for PG bouncer it tends to not try to replace them by lookup it tends to just",
    "start": "1825230",
    "end": "1830300"
  },
  {
    "text": "send you error messages which is really annoying also the the coop built-in kubernetes dns is not necessarily super",
    "start": "1830300",
    "end": "1837140"
  },
  {
    "text": "speedy compared to connecting like by IP address so there use some extra lag and forming",
    "start": "1837140",
    "end": "1843320"
  },
  {
    "text": "new connections but it's worth it for the use of management now so what I'm working on right now is working on",
    "start": "1843320",
    "end": "1849080"
  },
  {
    "text": "getting stateful set up with sharted post res I as well as multi master",
    "start": "1849080",
    "end": "1855110"
  },
  {
    "text": "synchronous replication so and I actually have to show off a demo those next week so at if PG thought Silicon",
    "start": "1855110",
    "end": "1862670"
  },
  {
    "text": "Valley so I will be if you follow up my blog or anything that sort of fuel you'll see this next week now there's",
    "start": "1862670",
    "end": "1869000"
  },
  {
    "text": "some other things in kubernetes to finish up here Oh",
    "start": "1869000",
    "end": "1874360"
  },
  {
    "text": "complex replication topologies so I want to have some asynchronous replicas and",
    "start": "1874930",
    "end": "1880010"
  },
  {
    "text": "some synchronous replicas and some cascading replicas that's that's a big to-do in the Patroni world to actually",
    "start": "1880010",
    "end": "1885920"
  },
  {
    "text": "support that in a meaningful way I'm you can kind of do it ad hoc but it becomes a big management headache and it is",
    "start": "1885920",
    "end": "1892700"
  },
  {
    "text": "something with people with actual production implementations need to have",
    "start": "1892700",
    "end": "1897730"
  },
  {
    "text": "um we do it depends I mean obviously if you're using a cloud host that supports dynamic purchases and volume claims then",
    "start": "1904419",
    "end": "1911059"
  },
  {
    "text": "it works right we're still waiting for a lot of stuff and I'll actually talk about some of that so here's some",
    "start": "1911059",
    "end": "1917059"
  },
  {
    "text": "buildings were waiting for the affinity anti affinity new affinity anti infinity stuff in 1.5 because we don't want to",
    "start": "1917059",
    "end": "1924080"
  },
  {
    "text": "have multiple replicas from the same database on the same node because then",
    "start": "1924080",
    "end": "1929659"
  },
  {
    "text": "you don't have a che [Music] the",
    "start": "1929659",
    "end": "1934960"
  },
  {
    "text": "stateful set Federation big",
    "start": "1934960",
    "end": "1939820"
  },
  {
    "text": "we're changing the the affinity anti infinity API and sort of syntax the",
    "start": "1940929",
    "end": "1946220"
  },
  {
    "text": "stuff that we had before with the annotations was really hard to make simple rules like don't put two nodes",
    "start": "1946220",
    "end": "1952879"
  },
  {
    "text": "from this stateful set on the same two items from the staple set on the same node",
    "start": "1952879",
    "end": "1958539"
  },
  {
    "text": "and it would often be ignored by the default scheduler frankly the but that's",
    "start": "1958539",
    "end": "1964970"
  },
  {
    "text": "all being fixed in 1.5 the I understand this work on stateful set",
    "start": "1964970",
    "end": "1971869"
  },
  {
    "text": "Federation to support general Federation you know we've general Federation in kubernetes now I have to exit to say hey I want two replicas in this data center",
    "start": "1971869",
    "end": "1978289"
  },
  {
    "text": "and two replicas in this data center two replicas in that data center the",
    "start": "1978289",
    "end": "1984850"
  },
  {
    "text": "so one of the things that we don't have in kubernetes in terms of compute",
    "start": "1985269",
    "end": "1990559"
  },
  {
    "text": "reservation in terms of saying hey you know these are our compute nodes and you",
    "start": "1990559",
    "end": "1996529"
  },
  {
    "text": "need to you know because because affinity is really the wrong way to do this really we should be able to for",
    "start": "1996529",
    "end": "2001600"
  },
  {
    "text": "things other than to shards from the same set we should be able to say hey this needs this many high ops and don't",
    "start": "2001600",
    "end": "2007600"
  },
  {
    "text": "over allocate storage for it and we don't have that in kubernetes and and",
    "start": "2007600",
    "end": "2012759"
  },
  {
    "text": "having databases in kubernetes are really showing that kind of need that also for persistent volumes I want to be",
    "start": "2012759",
    "end": "2018279"
  },
  {
    "text": "able to have more intelligent reclaimed policies because if a PVC has been idle for like",
    "start": "2018279",
    "end": "2024700"
  },
  {
    "text": "24 hours I probably don't want to reclaim it I probably want to spin up a new replica copy off the master",
    "start": "2024700",
    "end": "2031100"
  },
  {
    "text": "and if the reason why it's failing is that the data is corrupted I definitely don't want to reclaim it Plus for some",
    "start": "2031100",
    "end": "2037530"
  },
  {
    "text": "storage types I'd kind of like to be able to use host path which means using things like subdirectories under host",
    "start": "2037530",
    "end": "2042930"
  },
  {
    "text": "path and we really can't do that without reclaimed policies and then for like a sort of complicated use case that I'm",
    "start": "2042930",
    "end": "2048840"
  },
  {
    "text": "trying to enable is that for real production started post quiz for every shard you have one or more shadow shards",
    "start": "2048840",
    "end": "2054300"
  },
  {
    "text": "that are ready to step in if the primary shard dies for some reason well I need",
    "start": "2054300",
    "end": "2059310"
  },
  {
    "text": "to through stateful set actually be able to figure to be able to assign this those shadow shards and replace use them",
    "start": "2059310",
    "end": "2066210"
  },
  {
    "text": "to replace the primary shards and since I can't rename any of the stateful set instances I can't really do that right",
    "start": "2066210",
    "end": "2072780"
  },
  {
    "text": "now but but these are tweaks and and you know we'll get there in some form",
    "start": "2072780",
    "end": "2080540"
  },
  {
    "text": "so I've got a whole set of resources for this I actually have not pushed the current version the presentation about",
    "start": "2080540",
    "end": "2086638"
  },
  {
    "text": "do this right after that I Patroni if you want an actual like production",
    "start": "2086639",
    "end": "2093060"
  },
  {
    "text": "deployment of something like this because this one that I have there which is the code is up but it is meant as a",
    "start": "2093060",
    "end": "2098100"
  },
  {
    "text": "demo so it's easily comprehensible a production version would be like the helm chart for spy lo which is the",
    "start": "2098100",
    "end": "2103830"
  },
  {
    "text": "production wrapper for for Patroni which has all of the other tools and",
    "start": "2103830",
    "end": "2109680"
  },
  {
    "text": "components and sidecars and things and but if you want to look at the DEM of the more easily comprehensible demo it's",
    "start": "2109680",
    "end": "2115710"
  },
  {
    "text": "there on github so how much time to wait for questions yeah yeah okay awesome so here so we get",
    "start": "2115710",
    "end": "2125220"
  },
  {
    "text": "this on the wait how to turn this on your own switch for this",
    "start": "2125220",
    "end": "2131420"
  },
  {
    "text": "or is it just automatically on okay go ahead",
    "start": "2131420",
    "end": "2136910"
  },
  {
    "text": "yeah",
    "start": "2137170",
    "end": "2140170"
  },
  {
    "text": "whoa I'm the post ghost guy so I don't",
    "start": "2156910",
    "end": "2162020"
  },
  {
    "text": "actually know a lot about Maria DB so I don't I don't really have an opinion on that",
    "start": "2162020",
    "end": "2168430"
  },
  {
    "text": "versus RDS RDS is extremely popular when I was a",
    "start": "2168430",
    "end": "2173450"
  },
  {
    "text": "post growth consultant' we admit a lot of RDS instances for our customers but we still had to admit in them you",
    "start": "2173450",
    "end": "2181700"
  },
  {
    "text": "know as in our customers were still hiring us as consultants to admit their RDS instances so",
    "start": "2181700",
    "end": "2187240"
  },
  {
    "text": "I've never quite understood the appeal of RDS for that reason but then I'm very much a do it do-it-yourself type guy I",
    "start": "2187240",
    "end": "2194570"
  },
  {
    "text": "would say that in the current state like if you have an infrastructure based on kubernetes 1.4 or later that it's now",
    "start": "2194570",
    "end": "2202160"
  },
  {
    "text": "gotten so easy to actually do this yourself that it's probably gonna be easier for you to do it in kubernetes",
    "start": "2202160",
    "end": "2207500"
  },
  {
    "text": "than to do it via RDS so if you want to yeah go ahead and it",
    "start": "2207500",
    "end": "2213230"
  },
  {
    "text": "could you pass in the mic",
    "start": "2213230",
    "end": "2216340"
  },
  {
    "text": "yes",
    "start": "2220870",
    "end": "2223870"
  },
  {
    "text": "but are you",
    "start": "2232120",
    "end": "2234840"
  },
  {
    "text": "yeah when annexed right here",
    "start": "2246069",
    "end": "2250900"
  },
  {
    "text": "okay so right so so first question was when I have an opinion to using PG pool",
    "start": "2258849",
    "end": "2263869"
  },
  {
    "text": "and I've been working on automating post CRA's and post was H a failover for I",
    "start": "2263869",
    "end": "2270589"
  },
  {
    "text": "don't know like five years now and part of this has been a quest to avoid PG pool",
    "start": "2270589",
    "end": "2276369"
  },
  {
    "text": "so so that's basically my opinion of it is the the problem is that PG pool tries to be the Swiss Army knife of like",
    "start": "2276369",
    "end": "2282440"
  },
  {
    "text": "posters management it's connection manager and a cache and a replication manager and a query pre parser and",
    "start": "2282440",
    "end": "2290809"
  },
  {
    "text": "something else and it means that it doesn't do any of those things particularly well so I don't recommend using PGP pool if",
    "start": "2290809",
    "end": "2297170"
  },
  {
    "text": "you want a proxy to support your applications erik and peih-gee bouncer which is much simpler and i've been talking to the crunchy data",
    "start": "2297170",
    "end": "2304010"
  },
  {
    "text": "folks and they're actually working on a new much simpler go based proxy",
    "start": "2304010",
    "end": "2310390"
  },
  {
    "text": "that that they'll probably the thing to use once it's 1.0",
    "start": "2310390",
    "end": "2316450"
  },
  {
    "text": "yes the question is petronia automatically move with new posters versions or both",
    "start": "2321480",
    "end": "2327910"
  },
  {
    "text": "updates and the new version so this is two different questions it does not automatically move with updates it but",
    "start": "2327910",
    "end": "2333550"
  },
  {
    "text": "the thing is because failover is automatic what you do is you use a kubernetes rolling upgrade to operate",
    "start": "2333550",
    "end": "2340570"
  },
  {
    "text": "the containers to the new version of Postgres and failover will happen as you do the rolling upgrade",
    "start": "2340570",
    "end": "2346950"
  },
  {
    "text": "the and then doing major upgrades that's",
    "start": "2346950",
    "end": "2353830"
  },
  {
    "text": "waiting for post crews 10 after post was 10 you'll be able to do major upgrades via replication you can't right now",
    "start": "2353830",
    "end": "2360750"
  },
  {
    "text": "okay so we had a question over here",
    "start": "2360750",
    "end": "2364530"
  },
  {
    "text": "right so question is what sort of real production workloads are being run on",
    "start": "2379770",
    "end": "2384790"
  },
  {
    "text": "this because a lot of people don't think that you should run databases on kubernetes or in containers etc well in",
    "start": "2384790",
    "end": "2391780"
  },
  {
    "text": "terms of like the major production workload I'll let the gentleman from solando although he's not he's not running it",
    "start": "2391780",
    "end": "2398170"
  },
  {
    "text": "they're not running in Turku Bernays right now but they're definitely planning to doing the work we are running everything in dhaka already but",
    "start": "2398170",
    "end": "2403420"
  },
  {
    "text": "one-to-one with these two instances so we will have dedicated node pools on kubernetes and we'll definitely move",
    "start": "2403420",
    "end": "2408910"
  },
  {
    "text": "everything to grenadiers and there's a few hundred databases we are huge post quest fanboys so so we are not on",
    "start": "2408910",
    "end": "2415210"
  },
  {
    "text": "kubernetes right now in production but probably and for kube conan march will definitely have a lot of experience to",
    "start": "2415210",
    "end": "2421570"
  },
  {
    "text": "talk about yeah but we're already running in docker and so on yeah yes yeah",
    "start": "2421570",
    "end": "2427500"
  },
  {
    "text": "they're running a post post in docker on AWS whisper troll has a speedo but not on kubernetes yeah so it's not so",
    "start": "2427500",
    "end": "2434050"
  },
  {
    "text": "different but then we'll move in it yes yes oh",
    "start": "2434050",
    "end": "2441790"
  },
  {
    "text": "I don't know there's like 2 terabytes in total but okay are we at a time now no",
    "start": "2441790",
    "end": "2448970"
  },
  {
    "text": "we got a three more minutes so take take one more question here how do you operate this influence like",
    "start": "2448970",
    "end": "2456140"
  },
  {
    "text": "the use of pet sets and this sort of setup are you trying to create how do you operationalize these headsets",
    "start": "2456140",
    "end": "2463780"
  },
  {
    "text": "operators chorus recently really released operators where they do this",
    "start": "2463780",
    "end": "2469190"
  },
  {
    "text": "for and try to automate this for LCD and Prometheus and well those are the two",
    "start": "2469190",
    "end": "2474230"
  },
  {
    "text": "that they already grated I don't know if you it's fairly recently announced I mean yeah I'm not following the question",
    "start": "2474230",
    "end": "2479860"
  },
  {
    "text": "the maybe it's just cuz I don't actually didn't see that announcement cell",
    "start": "2479860",
    "end": "2488170"
  },
  {
    "text": "okay see the operators I know are",
    "start": "2490450",
    "end": "2496720"
  },
  {
    "text": "greater-than less-than between so so I don't know I I have not seen I did not see that release and so I",
    "start": "2496720",
    "end": "2503170"
  },
  {
    "text": "don't have an opinion on it if they can do etcd with it we can",
    "start": "2503170",
    "end": "2508450"
  },
  {
    "text": "probably do post quiz with it so what are whatever it is",
    "start": "2508450",
    "end": "2513240"
  },
  {
    "text": "so one last question yep",
    "start": "2513510",
    "end": "2520380"
  },
  {
    "text": "could you maybe talk about how what you're doing compares with like current CB DB is doing and kind of differences",
    "start": "2520620",
    "end": "2527950"
  },
  {
    "text": "or what those are so the question is what they already did versus what they're doing now so the the",
    "start": "2527950",
    "end": "2534160"
  },
  {
    "text": "crunchy DB reference deployment for post pres was built to work around the limitations I think of actually",
    "start": "2534160",
    "end": "2540490"
  },
  {
    "text": "kubernetes 1.1 and so you'll see their diagram I don't have it up here but",
    "start": "2540490",
    "end": "2547120"
  },
  {
    "text": "there's actually like 11 different kinds of containers a lot of which are sort of internal management containers and",
    "start": "2547120",
    "end": "2553030"
  },
  {
    "text": "proxies and they've got PG pool in the mix and that sort of thing and a lot of this because we couldn't do this stuff",
    "start": "2553030",
    "end": "2558340"
  },
  {
    "text": "right we couldn't manipulate the kubernetes dns to redirect it to the master and do all of these other things",
    "start": "2558340",
    "end": "2564330"
  },
  {
    "text": "the one of the things with stateful sets etc",
    "start": "2564330",
    "end": "2570090"
  },
  {
    "text": "particularly once one point four is supported with openshift they'll be able to simplify their architecture based on",
    "start": "2570090",
    "end": "2578110"
  },
  {
    "text": "using a lot of the same techniques I think they haven't decided yet whether they want to use there are several",
    "start": "2578110",
    "end": "2584080"
  },
  {
    "text": "demons that are similar to Patroni and I don't think they decided yet whether they want to use Patroni or one of the",
    "start": "2584080",
    "end": "2589300"
  },
  {
    "text": "others",
    "start": "2589300",
    "end": "2591570"
  },
  {
    "text": "yeah and when you would need that under this to write because you need something to run PG dump or to run a patr client",
    "start": "2596690",
    "end": "2603780"
  },
  {
    "text": "or whatever well cuz I'm just doing a demo I mean a",
    "start": "2603780",
    "end": "2610020"
  },
  {
    "text": "production one like I said look at look at silo and you've got all of these things yeah yeah the so",
    "start": "2610020",
    "end": "2618109"
  },
  {
    "text": "yeah so you would you know that would be included in this and that sort of thing it's the core failover and connection",
    "start": "2618109",
    "end": "2624660"
  },
  {
    "text": "management that's getting simple or everything else is pretty much the same okay and now we actually need to go so",
    "start": "2624660",
    "end": "2631140"
  },
  {
    "text": "if you have people with more questions I'm happy to take them out in the hall thank you very much",
    "start": "2631140",
    "end": "2636230"
  }
]