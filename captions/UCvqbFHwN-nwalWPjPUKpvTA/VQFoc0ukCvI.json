[
  {
    "text": "hello everyone I'm Frederick I work at Red Hat which was if you haven't heard",
    "start": "30",
    "end": "5970"
  },
  {
    "text": "previously or recently acquired by Red Hat know Korres was a quite pirate hat",
    "start": "5970",
    "end": "14269"
  },
  {
    "text": "yeah so today I want to talk about auto scaling your workload on on kubernetes",
    "start": "14269",
    "end": "22439"
  },
  {
    "text": "with the metrics from Prometheus but",
    "start": "22439",
    "end": "28289"
  },
  {
    "text": "what what do I want you to walk out of this room with in terms of information so I want to want everyone to understand",
    "start": "28289",
    "end": "36030"
  },
  {
    "text": "what is the officially recommended way to auto-scale workloads on kubernetes I",
    "start": "36030",
    "end": "42780"
  },
  {
    "text": "want to walk you through the history of this stack and this architecture and how",
    "start": "42780",
    "end": "49350"
  },
  {
    "text": "it has evolved over time to understand why we are where we are and then I want",
    "start": "49350",
    "end": "57480"
  },
  {
    "text": "to give everyone a future outlook of where I know things are going and some",
    "start": "57480",
    "end": "63600"
  },
  {
    "text": "predictions of where I think some things will go so without further ado let's",
    "start": "63600",
    "end": "69210"
  },
  {
    "text": "talk about auto scaling and first I want to make sure that we all understand the",
    "start": "69210",
    "end": "76770"
  },
  {
    "text": "terminology and have a common understanding of what we speak of when",
    "start": "76770",
    "end": "81930"
  },
  {
    "text": "we say auto scaling so on a very abstract sense we want to be able to",
    "start": "81930",
    "end": "87360"
  },
  {
    "text": "cover demand of our user facing applications or maybe not user facing",
    "start": "87360",
    "end": "92729"
  },
  {
    "text": "really anything that what our customer is whether that's actually an end user or some developer within our",
    "start": "92729",
    "end": "101070"
  },
  {
    "text": "infrastructure it doesn't really matter we want to be able to cover our demand based on our metrics and in order to be",
    "start": "101070",
    "end": "110729"
  },
  {
    "text": "able to do that we obviously need to collect those metrics but really",
    "start": "110729",
    "end": "116189"
  },
  {
    "text": "fundamentally why we why we do all of this is in order to cover our service",
    "start": "116189",
    "end": "122969"
  },
  {
    "text": "level objectives so things like latency of our AP is or the time within which we",
    "start": "122969",
    "end": "132450"
  },
  {
    "text": "want to or to our users that we work off items in a work queue for example whether",
    "start": "132450",
    "end": "138730"
  },
  {
    "text": "these are actually good at good examples depends on your individual problem but",
    "start": "138730",
    "end": "144579"
  },
  {
    "text": "that's generally what an SLO looks like of your service level agreement and more",
    "start": "144579",
    "end": "152319"
  },
  {
    "text": "often than not we can't actually directly say that we want to auto scale directly on our objective but we use",
    "start": "152319",
    "end": "158980"
  },
  {
    "text": "something called a service level indicator which as the name says",
    "start": "158980",
    "end": "164950"
  },
  {
    "text": "indicates that it influences our objective so an example of that could be",
    "start": "164950",
    "end": "170170"
  },
  {
    "text": "in order to process some requests within 5 minutes or something our queue length",
    "start": "170170",
    "end": "178150"
  },
  {
    "text": "needs to be worked off rapidly so we want to make sure that our queue length",
    "start": "178150",
    "end": "183370"
  },
  {
    "text": "is below 50 or something like that it's again whether that actually makes sense",
    "start": "183370",
    "end": "188440"
  },
  {
    "text": "in your case you need to evaluate that but you know your applications best so you just like with other monitoring need",
    "start": "188440",
    "end": "195579"
  },
  {
    "text": "to figure out what are the requirements of your users and you need to set set",
    "start": "195579",
    "end": "201669"
  },
  {
    "text": "those and collect those metrics in order to be able to know if you're performing the way that you're promising to perform",
    "start": "201669",
    "end": "208180"
  },
  {
    "text": "otherwise none of this really works and that's why we want to do auto scaling so",
    "start": "208180",
    "end": "214389"
  },
  {
    "text": "in terms of auto scaling there are two types of auto scaling and I will mostly",
    "start": "214389",
    "end": "219400"
  },
  {
    "text": "be focusing on the first one but I will still be explaining the differences so",
    "start": "219400",
    "end": "226079"
  },
  {
    "text": "there's horizontal auto scaling and more specifically in criminales we call this",
    "start": "226079",
    "end": "231250"
  },
  {
    "text": "the horizontal part okay autoscaler often abbreviated as the HPA and what",
    "start": "231250",
    "end": "237430"
  },
  {
    "text": "this means is in order to cover our demand we increase the number of replicas of our application and I think",
    "start": "237430",
    "end": "245199"
  },
  {
    "text": "this one is particularly important because although there is also vertical",
    "start": "245199",
    "end": "251379"
  },
  {
    "text": "pot auto scaling or just in general vertical auto scaling meaning we don't",
    "start": "251379",
    "end": "256389"
  },
  {
    "text": "increase the replicas of our application but we increase the individual size of",
    "start": "256389",
    "end": "261400"
  },
  {
    "text": "our replicas so while vertical part auto scaling is some is",
    "start": "261400",
    "end": "266860"
  },
  {
    "text": "very important as well it becomes impractical to scale beyond a certain",
    "start": "266860",
    "end": "272740"
  },
  {
    "text": "certain size because even though we have really giant servers out there with two",
    "start": "272740",
    "end": "278319"
  },
  {
    "text": "terabytes of RAM and gigantic amounts of CPU even those hit some physical limit",
    "start": "278319",
    "end": "284560"
  },
  {
    "text": "at some point and become impractical to scale out so what we often focus on is",
    "start": "284560",
    "end": "291509"
  },
  {
    "text": "horizontally scaling because within cloud infrastructure on with our cloud",
    "start": "291509",
    "end": "296650"
  },
  {
    "text": "providers we can basically treat it as endless resources unless we're maybe a",
    "start": "296650",
    "end": "302530"
  },
  {
    "text": "Google who scales and data centers not in instances but even that even on that",
    "start": "302530",
    "end": "310000"
  },
  {
    "text": "kind of level we we could do we could apply the same principles and just to",
    "start": "310000",
    "end": "315520"
  },
  {
    "text": "cover cover this again we are demand the",
    "start": "315520",
    "end": "321000"
  },
  {
    "text": "necessary amount here is calculated in terms of kubernetes the resource",
    "start": "321000",
    "end": "326349"
  },
  {
    "text": "requests and limits so now that we've covered auto scaling in abstract sense",
    "start": "326349",
    "end": "333729"
  },
  {
    "text": "let's look at how this was historically done on kubernetes and depending on the kubernetes version that you are running",
    "start": "333729",
    "end": "340000"
  },
  {
    "text": "you may already be using this stack and as you can tell by my language there is",
    "start": "340000",
    "end": "346210"
  },
  {
    "text": "something new coming up or already there so historically auto scaling was heavily",
    "start": "346210",
    "end": "353919"
  },
  {
    "text": "based around a component called heap stir in kubernetes and heaps there is a",
    "start": "353919",
    "end": "359379"
  },
  {
    "text": "monitoring system that collects metrics from your infrastructure mostly from a",
    "start": "359379",
    "end": "364479"
  },
  {
    "text": "component in your kubernetes cluster called C advisor which sits which is actually compiled into the couplet in",
    "start": "364479",
    "end": "371889"
  },
  {
    "text": "kubernetes and through that it gathers and infrastructure metrics about your",
    "start": "371889",
    "end": "377110"
  },
  {
    "text": "containers things like CPU memory usage and there's also some possibilities to",
    "start": "377110",
    "end": "383169"
  },
  {
    "text": "do custom metrics collection and then when hipster goes around and collect all",
    "start": "383169",
    "end": "388810"
  },
  {
    "text": "of these metrics it then writes it into some sink so into some other time series",
    "start": "388810",
    "end": "394960"
  },
  {
    "text": "database and when we wanted to consume this in",
    "start": "394960",
    "end": "401820"
  },
  {
    "text": "terms of horizontally scaling what we did is we created a horizontal part",
    "start": "401820",
    "end": "407040"
  },
  {
    "text": "autoscaler object and how this object works is that we have a reference to the",
    "start": "407040",
    "end": "413760"
  },
  {
    "text": "particular resource that we want to scale so the in this particular example we have a deployment but this could just",
    "start": "413760",
    "end": "420090"
  },
  {
    "text": "as well be a stateful set or really anything in kubernetes that has replicas",
    "start": "420090",
    "end": "426780"
  },
  {
    "text": "in it and actually this is generic in terms of kubernetes because anything that is scalable and we'll go into that",
    "start": "426780",
    "end": "433920"
  },
  {
    "text": "a bit more in detail later can be used here yeah and then the next fields I",
    "start": "433920",
    "end": "440820"
  },
  {
    "text": "think are obvious so we always want to make sure that at least five replicas are available of this and just in order",
    "start": "440820",
    "end": "447450"
  },
  {
    "text": "to not explode the resources that this needs we can also specify a maximum",
    "start": "447450",
    "end": "453240"
  },
  {
    "text": "replicas and within this design there",
    "start": "453240",
    "end": "458520"
  },
  {
    "text": "was only the possibility to do auto scaling based on CPU usage and more",
    "start": "458520",
    "end": "463830"
  },
  {
    "text": "specifically percentage so the resource requests in terms of CPU that we give it",
    "start": "463830",
    "end": "469680"
  },
  {
    "text": "if it has 70% in this case or more then",
    "start": "469680",
    "end": "474810"
  },
  {
    "text": "we start spinning up more parts of this deployment now this is this was really",
    "start": "474810",
    "end": "483270"
  },
  {
    "text": "cool we got this in criminales 1.2 and we could auto scale and in theory all",
    "start": "483270",
    "end": "488990"
  },
  {
    "text": "problems out there can be converted to a CPU bound problem now you probably",
    "start": "488990",
    "end": "496590"
  },
  {
    "text": "already understand some of the problems with this which is its while it's",
    "start": "496590",
    "end": "503400"
  },
  {
    "text": "theoretically possible it's not very practical to do this based on CPU usage",
    "start": "503400",
    "end": "508830"
  },
  {
    "text": "we may very well have something that is much more efficient when it's EEP memory",
    "start": "508830",
    "end": "515130"
  },
  {
    "text": "bound or really it's very hard to boil down our SL O's to something like CPU",
    "start": "515130",
    "end": "522780"
  },
  {
    "text": "percentage some organizations may be able to do this and it's great for those",
    "start": "522780",
    "end": "528150"
  },
  {
    "text": "organizations but it's very hard to get to that point to be able to do that and over time as",
    "start": "528150",
    "end": "535350"
  },
  {
    "text": "we developed hipster so hipster only started with a few theta betas things so",
    "start": "535350",
    "end": "543080"
  },
  {
    "text": "we were super excited when all these vendors came in and said yeah we want to do auto-scaling with and monitoring with",
    "start": "543080",
    "end": "550050"
  },
  {
    "text": "kubernetes as well and we said yeah all these vendors are behind this what we want to do this is really cool and as it",
    "start": "550050",
    "end": "557160"
  },
  {
    "text": "happens so often in open source projects and this is perfectly normal is that not",
    "start": "557160",
    "end": "563250"
  },
  {
    "text": "everybody maintains the code that they contribute and I think this was less a",
    "start": "563250",
    "end": "569250"
  },
  {
    "text": "problem of the community rather than us the developers of hipster who made the",
    "start": "569250",
    "end": "576029"
  },
  {
    "text": "architecture this way and so over time this created a lot of problems it caused",
    "start": "576029",
    "end": "581610"
  },
  {
    "text": "outages and kubernetes clusters monitoring is basically the most important component of your of your",
    "start": "581610",
    "end": "589380"
  },
  {
    "text": "infrastructure so it needs to be extremely stable Google actually has I don't know if they still do but I know",
    "start": "589380",
    "end": "595260"
  },
  {
    "text": "that they at least used to they have in terms of priority they have something",
    "start": "595260",
    "end": "601200"
  },
  {
    "text": "like medium high and monitoring so monitoring is the most important schedule scheduling priority there are",
    "start": "601200",
    "end": "609570"
  },
  {
    "text": "other organizations doing this and so we want to we want to make sure that",
    "start": "609570",
    "end": "614730"
  },
  {
    "text": "whatever we provide a super robust and what's also what also was strange was",
    "start": "614730",
    "end": "623070"
  },
  {
    "text": "because was that because of how hipster works that it it collects it must",
    "start": "623070",
    "end": "628350"
  },
  {
    "text": "collect the metrics and write it into some time series database the big the",
    "start": "628350",
    "end": "637380"
  },
  {
    "text": "big monitoring system that is super popular within kubernetes which is Prometheus fundamentally cannot work with this and then heaps are also",
    "start": "637380",
    "end": "646400"
  },
  {
    "text": "exposed metrics and Prometheus format but we can't use Prometheus so this was confusing to all of us so we we decided",
    "start": "646400",
    "end": "654270"
  },
  {
    "text": "about a year and a half ago that this pipeline needed a redesign and we wanted",
    "start": "654270",
    "end": "659820"
  },
  {
    "text": "to solve all of these problems that we had previously created ourselves with heaps there",
    "start": "659820",
    "end": "665160"
  },
  {
    "text": "so that we can go forward and just be productive to auto-scaling and stable",
    "start": "665160",
    "end": "671550"
  },
  {
    "text": "monitoring so one of those things that we wanted to achieve here is not be",
    "start": "671550",
    "end": "677610"
  },
  {
    "text": "bound to only be able to scale based on CPU we want to be able to auto scale on",
    "start": "677610",
    "end": "684000"
  },
  {
    "text": "arbitrary metrics and what we came up with are were the resource and custom",
    "start": "684000",
    "end": "689670"
  },
  {
    "text": "metrics api's within kubernetes and these were specified about a year and a",
    "start": "689670",
    "end": "695400"
  },
  {
    "text": "half ago and are actually available in all kubernetes clusters starting kubernetes 1.8 so most likely all of",
    "start": "695400",
    "end": "704520"
  },
  {
    "text": "your clusters already support these so",
    "start": "704520",
    "end": "710550"
  },
  {
    "text": "what's important to understand about these api's and we made this very very",
    "start": "710550",
    "end": "716280"
  },
  {
    "text": "much a goal of these api's is that they're just a specification of api's so",
    "start": "716280",
    "end": "723480"
  },
  {
    "text": "the individual implementations are up to the vendor to do so we don't have this",
    "start": "723480",
    "end": "728730"
  },
  {
    "text": "model again where there's one repository where everybody puts their integrations",
    "start": "728730",
    "end": "734250"
  },
  {
    "text": "and we again get unstable and to end tests in kubernetes and in general it",
    "start": "734250",
    "end": "739950"
  },
  {
    "text": "slows down the entire kubernetes development instead these vendors or the",
    "start": "739950",
    "end": "746520"
  },
  {
    "text": "community around these vendors or other crew open source projects need to write",
    "start": "746520",
    "end": "751710"
  },
  {
    "text": "these maintain these and maybe hand off ownership so this is it's more explicit",
    "start": "751710",
    "end": "758310"
  },
  {
    "text": "than whenever anyone is using creates one of these adapters that they're",
    "start": "758310",
    "end": "765420"
  },
  {
    "text": "responsible so that was important for us and something that's important to",
    "start": "765420",
    "end": "770790"
  },
  {
    "text": "understand when using these these api is only return a single value so the most recent value of that metric that you're",
    "start": "770790",
    "end": "778980"
  },
  {
    "text": "requesting so the resource metrics api",
    "start": "778980",
    "end": "783990"
  },
  {
    "text": "is actually very well defined about the metrics that it does expose so these are",
    "start": "783990",
    "end": "789480"
  },
  {
    "text": "something that we sometimes also refer to as the core metrics so things that every workload in your kubernetes",
    "start": "789480",
    "end": "795690"
  },
  {
    "text": "cluster has so CP you memory file system those kinds of",
    "start": "795690",
    "end": "800759"
  },
  {
    "text": "metrics and while today does only include CPU and memory we do want to",
    "start": "800759",
    "end": "807660"
  },
  {
    "text": "extend these but this is what we have today and what is stable today to be used and there is a canonical",
    "start": "807660",
    "end": "815850"
  },
  {
    "text": "implementation for this because the resource metrics API is actually important for kubernetes cluster to run",
    "start": "815850",
    "end": "821810"
  },
  {
    "text": "by itself so if you do things like coop CTL top or you want to also scale based",
    "start": "821810",
    "end": "828089"
  },
  {
    "text": "on CPU or metric on memory then you can do this consistently in every kubernetes",
    "start": "828089",
    "end": "834420"
  },
  {
    "text": "cluster and you don't need a monitoring vendor to do this and before we actually",
    "start": "834420",
    "end": "842129"
  },
  {
    "text": "implemented this we did the calculations and said ok kubernetes guarantees a",
    "start": "842129",
    "end": "847910"
  },
  {
    "text": "scalability of 5000 nodes with an average of 30 pods per node and if we",
    "start": "847910",
    "end": "854579"
  },
  {
    "text": "have about 10 metrics for each of those pods then we end up with and divide that",
    "start": "854579",
    "end": "860730"
  },
  {
    "text": "by the collection interval which is every 60 seconds the metric server would",
    "start": "860730",
    "end": "867149"
  },
  {
    "text": "go and scrape all of this information from the nodes then we end up with",
    "start": "867149",
    "end": "872480"
  },
  {
    "text": "25,000 metrics ingested per second which keeping this in memory is a very very",
    "start": "872480",
    "end": "878490"
  },
  {
    "text": "low amount of metrics and yeah so we went ahead and implemented this and it",
    "start": "878490",
    "end": "884939"
  },
  {
    "text": "performs really well and this is now the default implementation of the resource",
    "start": "884939",
    "end": "892050"
  },
  {
    "text": "metrics API in theory these vendors could still go ahead and implement these",
    "start": "892050",
    "end": "897379"
  },
  {
    "text": "this API but important is that when you",
    "start": "897379",
    "end": "902459"
  },
  {
    "text": "go on any kubernetes cluster we can be sure that everything works because this is the default thing that is deployed",
    "start": "902459",
    "end": "909329"
  },
  {
    "text": "for example gke employs this mini coop deploys and pretty much all of the other vendors of",
    "start": "909329",
    "end": "918059"
  },
  {
    "text": "managed kubernetes clusters do this and of course also most of the installation",
    "start": "918059",
    "end": "925009"
  },
  {
    "text": "tools do this if they don't shoot them a pull request because this is actually a requirement of",
    "start": "925009",
    "end": "931279"
  },
  {
    "text": "bananas cluster and then the custom metrics API was actually the thing that we really wanted out of this pipeline we",
    "start": "931279",
    "end": "938480"
  },
  {
    "text": "the semantics is are the same as with the resource metrics API which is you",
    "start": "938480",
    "end": "944269"
  },
  {
    "text": "get the latest sample of that time series that you're requesting but because this is custom by definition we",
    "start": "944269",
    "end": "952519"
  },
  {
    "text": "don't have any defined metrics for this and because it's very specific to how",
    "start": "952519",
    "end": "957889"
  },
  {
    "text": "you produce your metrics there cannot be a default implementation for this so",
    "start": "957889",
    "end": "964040"
  },
  {
    "text": "there must be an implement implementation by your vendor for your",
    "start": "964040",
    "end": "970240"
  },
  {
    "text": "monitoring system and with the custom metrics API each of your metrics",
    "start": "970240",
    "end": "976610"
  },
  {
    "text": "actually must be related somehow to a kubernetes object and we'll see later what that actually means",
    "start": "976610",
    "end": "982730"
  },
  {
    "text": "and because Prometheus was is so popular within kubernetes and I guess that's why",
    "start": "982730",
    "end": "988939"
  },
  {
    "text": "everyone is here today we started with Prometheus being the example",
    "start": "988939",
    "end": "996189"
  },
  {
    "text": "implementation but there are other implementations for other monitoring systems out there such as stackdriver",
    "start": "996189",
    "end": "1002709"
  },
  {
    "text": "and others and this is something very very new this is only an alpha where",
    "start": "1002709",
    "end": "1008110"
  },
  {
    "text": "whereas the other two api's are in beta and are available on all kubernetes",
    "start": "1008110",
    "end": "1013509"
  },
  {
    "text": "clusters the external metrics API is very similar to the custom metrics API",
    "start": "1013509",
    "end": "1019209"
  },
  {
    "text": "however it must not be related to a criminales object what this is useful",
    "start": "1019209",
    "end": "1024610"
  },
  {
    "text": "for is when you for example have some service that you're consuming by your",
    "start": "1024610",
    "end": "1029620"
  },
  {
    "text": "cloud provider let's say they have some sort of a message broker that you're",
    "start": "1029620",
    "end": "1036370"
  },
  {
    "text": "consuming and there's a queue length metric that that system exposes but it",
    "start": "1036370",
    "end": "1041740"
  },
  {
    "text": "doesn't run in your kubernetes cluster you can't so you can say my pod has this",
    "start": "1041740",
    "end": "1047199"
  },
  {
    "text": "metric so that's what this use case is for this is an alpha but there are",
    "start": "1047199",
    "end": "1053649"
  },
  {
    "text": "already some implementations out there for this so now that we understand what",
    "start": "1053649",
    "end": "1059919"
  },
  {
    "text": "auto scaling is on an abstract sense and we understand the current metrics",
    "start": "1059919",
    "end": "1067020"
  },
  {
    "text": "api's around kubernetes let's look at the concrete example with Prometheus so",
    "start": "1067020",
    "end": "1073470"
  },
  {
    "text": "to make sure that we all understand how Prometheus works and then how this entire system works as a whole let's",
    "start": "1073470",
    "end": "1081360"
  },
  {
    "text": "have a look at how Prometheus works so in Prometheus you your application keeps",
    "start": "1081360",
    "end": "1086700"
  },
  {
    "text": "its metrics in memory and this has very little overhead and basically in this",
    "start": "1086700",
    "end": "1096000"
  },
  {
    "text": "example we have started our application and it just just started so there were",
    "start": "1096000",
    "end": "1101220"
  },
  {
    "text": "requests count - this has not it hasn't received any traffic so requests count",
    "start": "1101220",
    "end": "1106710"
  },
  {
    "text": "is zero so Prometheus is configured to scrape those metrics every interval in",
    "start": "1106710",
    "end": "1112980"
  },
  {
    "text": "this case and this is the default every 15 seconds and when it does that it takes the metrics your application",
    "start": "1112980",
    "end": "1119400"
  },
  {
    "text": "exposes and writes it into its time series database and then we actually add",
    "start": "1119400",
    "end": "1125580"
  },
  {
    "text": "our application to a load balancer and it receives some traffic and then the",
    "start": "1125580",
    "end": "1130679"
  },
  {
    "text": "internal request counter is counted up and then the scrape interval has passed",
    "start": "1130679",
    "end": "1138120"
  },
  {
    "text": "and Prometheus will again scrape this information insert it into its time",
    "start": "1138120",
    "end": "1143400"
  },
  {
    "text": "series database and basically do that for eternity so let's make this use the",
    "start": "1143400",
    "end": "1151710"
  },
  {
    "text": "HPI let's make the HPI useless so in",
    "start": "1151710",
    "end": "1158250"
  },
  {
    "text": "order to be able to accommodate the features that we wanted to we had to create a new version of the horizontal",
    "start": "1158250",
    "end": "1164580"
  },
  {
    "text": "part or a scalar object because if you remember we were only able to specify",
    "start": "1164580",
    "end": "1170150"
  },
  {
    "text": "the CPU utilization so the first couple of fields that you're seeing here are",
    "start": "1170150",
    "end": "1176190"
  },
  {
    "text": "actually identical to the first version however the very thing that we wanted to change is now an array of metrics not a",
    "start": "1176190",
    "end": "1184140"
  },
  {
    "text": "single metric and we can specify multiple metrics about multiple types in",
    "start": "1184140",
    "end": "1191700"
  },
  {
    "text": "this case I'm showing something where I'm saying every pod in this associated with this object",
    "start": "1191700",
    "end": "1199620"
  },
  {
    "text": "exposes a metric called HTTP requests and actually the adapter gives us some",
    "start": "1199620",
    "end": "1209440"
  },
  {
    "text": "help here because the actual metric that my application exposes here is HTTP requests total but I'm not interested",
    "start": "1209440",
    "end": "1216850"
  },
  {
    "text": "really in terms of scaling about the total requests I'm interested in the",
    "start": "1216850",
    "end": "1222040"
  },
  {
    "text": "requests per second so it knows which ones of these are counters basically",
    "start": "1222040",
    "end": "1228880"
  },
  {
    "text": "converts those and then we can say what we want our target value for each part",
    "start": "1228880",
    "end": "1234310"
  },
  {
    "text": "to be so when this target value has exceeded that we spin up more replicas",
    "start": "1234310",
    "end": "1239350"
  },
  {
    "text": "of this in order to cover our demand and there are two types of thresholds we can",
    "start": "1239350",
    "end": "1246430"
  },
  {
    "text": "set here one is a is an actual value so in this case it makes sense to use a value but for something like CPU or",
    "start": "1246430",
    "end": "1253180"
  },
  {
    "text": "memory utilization we would again probably want something percentage based",
    "start": "1253180",
    "end": "1259260"
  },
  {
    "text": "so when the when the HPA actually does does this request the way this works is",
    "start": "1259260",
    "end": "1265660"
  },
  {
    "text": "that these api's are what we call aggregated api in kubernetes and what",
    "start": "1265660",
    "end": "1272320"
  },
  {
    "text": "this means is that when we create these actual instances so obviously there needs to be some application serving",
    "start": "1272320",
    "end": "1278440"
  },
  {
    "text": "these api's it registers them with the kubernetes api server and when then the",
    "start": "1278440",
    "end": "1285520"
  },
  {
    "text": "HPA just asks the kubernetes api server give me that value from the resource",
    "start": "1285520",
    "end": "1291400"
  },
  {
    "text": "metrics api the api server actually just basically proxies that to the actual",
    "start": "1291400",
    "end": "1297930"
  },
  {
    "text": "instance of the metric server if we're requesting the core the resource metrics",
    "start": "1297930",
    "end": "1303970"
  },
  {
    "text": "API and as we said earlier the metric server then goes and or actually",
    "start": "1303970",
    "end": "1311610"
  },
  {
    "text": "periodically goes and scrapes these metrics and in this case it will just immediately return whatever at pass in",
    "start": "1311610",
    "end": "1318580"
  },
  {
    "text": "memory about that requested metric now with the custom metrics API this is a",
    "start": "1318580",
    "end": "1324460"
  },
  {
    "text": "bit more complicated you need your actual monitoring system if you use",
    "start": "1324460",
    "end": "1329470"
  },
  {
    "text": "something like perme you may be running that within your actual cluster as well or not with if",
    "start": "1329470",
    "end": "1336519"
  },
  {
    "text": "you use some vendor you probably are not running that within your cluster but it's outside of your classroom then you",
    "start": "1336519",
    "end": "1342159"
  },
  {
    "text": "your custom metrics API would call out to that vendor and then return that",
    "start": "1342159",
    "end": "1348549"
  },
  {
    "text": "information and prometheus has all of this information because it goes through",
    "start": "1348549",
    "end": "1353769"
  },
  {
    "text": "your I mean you need to set up Prometheus correctly to do this but it's pretty easy to do that and you scrape",
    "start": "1353769",
    "end": "1360999"
  },
  {
    "text": "all of these pods and the metrics that they expose and that way then when you",
    "start": "1360999",
    "end": "1367299"
  },
  {
    "text": "when you'd perform a request against the custom metrics API it will perform a",
    "start": "1367299",
    "end": "1372399"
  },
  {
    "text": "query basically it is a translation layer which converts the custom metrics request into a Prometheus query and then",
    "start": "1372399",
    "end": "1380159"
  },
  {
    "text": "queries Prometheus and returns the result now in terms of the result the",
    "start": "1380159",
    "end": "1385330"
  },
  {
    "text": "example that I just showed how that actually internally works is that when",
    "start": "1385330",
    "end": "1391450"
  },
  {
    "text": "we say we want the HTTP requests rate of all of my parts of this associated",
    "start": "1391450",
    "end": "1398289"
  },
  {
    "text": "object then I then what the custom metrics adapter does here is it does a",
    "start": "1398289",
    "end": "1403839"
  },
  {
    "text": "rate so the inner function we can see here on the metric called HTTP requests",
    "start": "1403839",
    "end": "1409779"
  },
  {
    "text": "total and it makes sure to add a selector here to only select those parts that are actually relevant right now and",
    "start": "1409779",
    "end": "1416229"
  },
  {
    "text": "it applies range courious what we call this in Prometheus which means I want to",
    "start": "1416229",
    "end": "1422559"
  },
  {
    "text": "see the rate over the past five minutes and then it sums all of those by pods that's more not wouldn't necessarily be",
    "start": "1422559",
    "end": "1431249"
  },
  {
    "text": "important for this query but in general it doesn't hurt because if we have we can do some additional grouping when we",
    "start": "1431249",
    "end": "1441309"
  },
  {
    "text": "specify the metric we want to query and basically we could do something like requests per route or per method and",
    "start": "1441309",
    "end": "1449769"
  },
  {
    "text": "then we want to scale based on that so yeah so let's let's see this in action I",
    "start": "1449769",
    "end": "1457979"
  },
  {
    "text": "have obviously my blog running on kubernetes",
    "start": "1457979",
    "end": "1463600"
  },
  {
    "text": "and let's see nobody here in this room",
    "start": "1463600",
    "end": "1473029"
  },
  {
    "text": "has done any requests against my blog Sam so let's put some some load onto",
    "start": "1473029",
    "end": "1481399"
  },
  {
    "text": "this so I have prepared something that",
    "start": "1481399",
    "end": "1494059"
  },
  {
    "text": "will put some load onto this and I my HPA object looks very similar actually",
    "start": "1494059",
    "end": "1501679"
  },
  {
    "text": "exactly like what I just showed in my example so I'm now putting some load",
    "start": "1501679",
    "end": "1507440"
  },
  {
    "text": "onto my my blog and it creates metrics for all of the pods that are running my",
    "start": "1507440",
    "end": "1513620"
  },
  {
    "text": "blog and hopefully as we can see actually the object that I applied has a",
    "start": "1513620",
    "end": "1521299"
  },
  {
    "text": "max of 30 in case you all start ddossing my blog so hopefully we should now soon",
    "start": "1521299",
    "end": "1529309"
  },
  {
    "text": "start seeing that the traffic increases for my blog and then eventually that",
    "start": "1529309",
    "end": "1536350"
  },
  {
    "text": "epic replicas are increased because of that so we can already see that the",
    "start": "1536350",
    "end": "1545269"
  },
  {
    "text": "traffic increases and the horizontal",
    "start": "1545269",
    "end": "1551269"
  },
  {
    "text": "part autoscaler actually has a cool-down so it only scaled periodically so it",
    "start": "1551269",
    "end": "1558679"
  },
  {
    "text": "doesn't when when there's one spike up or down that it immediately tears down all the pods or immediately spins up",
    "start": "1558679",
    "end": "1565490"
  },
  {
    "text": "50,000 so it does this periodically and",
    "start": "1565490",
    "end": "1570669"
  },
  {
    "text": "for now we'll go back to the presentation and then we'll check in",
    "start": "1570669",
    "end": "1576080"
  },
  {
    "text": "later to see that this actually actually worked hopefully if not I have a",
    "start": "1576080",
    "end": "1581510"
  },
  {
    "text": "recording so now that we've seen all of this of the system what do I see what do",
    "start": "1581510",
    "end": "1589159"
  },
  {
    "text": "I already know is going to happen in this space and the in the near future after that I will make some predictions",
    "start": "1589159",
    "end": "1596120"
  },
  {
    "text": "of what I believe is going to happen so",
    "start": "1596120",
    "end": "1602050"
  },
  {
    "text": "the reason why we've been doing all of this work is because we felt heaps err",
    "start": "1602050",
    "end": "1607370"
  },
  {
    "text": "was not was not efficient enough of a solution for this so we just last week",
    "start": "1607370",
    "end": "1614810"
  },
  {
    "text": "have started formally the process the process to formally deprecated hipster",
    "start": "1614810",
    "end": "1620590"
  },
  {
    "text": "and pretty much within SiC instrumentation we believe that this is",
    "start": "1620590",
    "end": "1625700"
  },
  {
    "text": "the correct thing to do and actually as of kubernetes 1.10 all of the critical",
    "start": "1625700",
    "end": "1632000"
  },
  {
    "text": "things for a kubernetes cluster to operate have been migrated to use the",
    "start": "1632000",
    "end": "1637430"
  },
  {
    "text": "resource metrics api so we felt now is the time now that we have migrated",
    "start": "1637430",
    "end": "1643310"
  },
  {
    "text": "everything now's the time to deprecated the system and one and as I already",
    "start": "1643310",
    "end": "1651050"
  },
  {
    "text": "mentioned horizontal part auto scaling is really the default thing to do today",
    "start": "1651050",
    "end": "1657890"
  },
  {
    "text": "in kubernetes but it does it's not always the right thing so for databases for example it's very often important to",
    "start": "1657890",
    "end": "1664460"
  },
  {
    "text": "still increase vertically so there is a lot of work going on with this but",
    "start": "1664460",
    "end": "1670790"
  },
  {
    "text": "there's no the all of that work is still in an alpha and it only supports",
    "start": "1670790",
    "end": "1676370"
  },
  {
    "text": "Prometheus because it turns out a vertical part auto scaling is a lot more",
    "start": "1676370",
    "end": "1681650"
  },
  {
    "text": "complicated and a lot more fragile so there's a lot more thought that needs to",
    "start": "1681650",
    "end": "1686720"
  },
  {
    "text": "go into that as opposed to we have some rate limit per per pod and we just want",
    "start": "1686720",
    "end": "1694370"
  },
  {
    "text": "to increase that all the time because this would involve spinning up new pods and things like that so there needs to",
    "start": "1694370",
    "end": "1700190"
  },
  {
    "text": "be some more thought put into it and something really cool personally I think",
    "start": "1700190",
    "end": "1706760"
  },
  {
    "text": "is that very soon in criminales 111",
    "start": "1706760",
    "end": "1711980"
  },
  {
    "text": "we'll be able to scale custom resource definitions in kubernetes so in case you don't know custom resource definitions",
    "start": "1711980",
    "end": "1718970"
  },
  {
    "text": "are a mechanism in kubernetes to extend the kerbin ids api so for example I am",
    "start": "1718970",
    "end": "1725570"
  },
  {
    "text": "the maintainer of the Prometheus operator which allows you to create Prometheus objects within your",
    "start": "1725570",
    "end": "1732759"
  },
  {
    "text": "kubernetes cluster similar to how you create a deployment in your kubernetes cluster and this will allow us to auto",
    "start": "1732759",
    "end": "1740049"
  },
  {
    "text": "scale those kinds of resources and I think that's really exciting because it",
    "start": "1740049",
    "end": "1745389"
  },
  {
    "text": "means that we can very natively create these extensions to the kubernetes api",
    "start": "1745389",
    "end": "1750580"
  },
  {
    "text": "and have them behaved exactly the same way as all the other resources and I just want to give a shout out to Stefan",
    "start": "1750580",
    "end": "1756610"
  },
  {
    "text": "and Nikita who have been really hard of hard at work doing this so thanks thanks",
    "start": "1756610",
    "end": "1762249"
  },
  {
    "text": "to them we will be soon able to do this and one of the one other thing that",
    "start": "1762249",
    "end": "1769869"
  },
  {
    "text": "we're working on within SiC instrumentation is over the past year or",
    "start": "1769869",
    "end": "1777100"
  },
  {
    "text": "two or three we have realized within seconds fermentation that the metrics that we provide in urban Eddie's are not",
    "start": "1777100",
    "end": "1783909"
  },
  {
    "text": "as stable as we would like them to be so there are a number of proposals out there in order to improve this state so",
    "start": "1783909",
    "end": "1791230"
  },
  {
    "text": "that we can have a better confidence in the metrics that we're using today we'll",
    "start": "1791230",
    "end": "1797080"
  },
  {
    "text": "be there tomorrow yeah so I think while that's not super exciting in itself I",
    "start": "1797080",
    "end": "1803740"
  },
  {
    "text": "think it's really important for this entire ecosystem to be able to share all this knowledge because if a metric can",
    "start": "1803740",
    "end": "1810549"
  },
  {
    "text": "go away from one day to another then it's basically useless so those are the",
    "start": "1810549",
    "end": "1816070"
  },
  {
    "text": "things that I know are going to happen now some predictions that I think would",
    "start": "1816070",
    "end": "1821169"
  },
  {
    "text": "be cool to happen but I don't know if they're going to happen so I I think it would be really cool because we can auto",
    "start": "1821169",
    "end": "1827619"
  },
  {
    "text": "scale custom resources it would be really cool if we do the use the work",
    "start": "1827619",
    "end": "1832869"
  },
  {
    "text": "that some of the companies have been working on where they have where they describe a cluster with custom resource",
    "start": "1832869",
    "end": "1840159"
  },
  {
    "text": "definitions such as note pools that we can use all of these primitives that we",
    "start": "1840159",
    "end": "1845950"
  },
  {
    "text": "have and the mechanisms that are available in custom resources that we can automatically scale our clusters",
    "start": "1845950",
    "end": "1852610"
  },
  {
    "text": "with those primitives and today the cluster autoscaler is a separate component that actually does about a lot",
    "start": "1852610",
    "end": "1858820"
  },
  {
    "text": "of the very similar work as the horizontal part autoscaler does so I'm hoping that all of these projects",
    "start": "1858820",
    "end": "1866020"
  },
  {
    "text": "will one one day be combined and this goes very very much into the thought of",
    "start": "1866020",
    "end": "1874900"
  },
  {
    "text": "stable metrics but is a more broader thing happening in the monitoring space which is a standardization of all this",
    "start": "1874900",
    "end": "1881530"
  },
  {
    "text": "monitoring so there's a an effort called open metrics which is standardizing the",
    "start": "1881530",
    "end": "1887050"
  },
  {
    "text": "format basically that Prometheus has obviously there are some some changes",
    "start": "1887050",
    "end": "1892360"
  },
  {
    "text": "probably going to happen to it but it's based on that and what's really exciting",
    "start": "1892360",
    "end": "1897970"
  },
  {
    "text": "is that all of these monitoring vendors are coming together and working with the Prometheus team and other open source in",
    "start": "1897970",
    "end": "1904840"
  },
  {
    "text": "order to create a standard that all of these monitoring systems can then use because all kubernetes components are",
    "start": "1904840",
    "end": "1912070"
  },
  {
    "text": "already serving Prometheus style metrics so I think that's really cool and in within our bubble more or less there's",
    "start": "1912070",
    "end": "1921100"
  },
  {
    "text": "also some really awesome things happening like G RPC while HTTP metrics",
    "start": "1921100",
    "end": "1926350"
  },
  {
    "text": "are already pretty standardized in the Prometheus ecosystem G RPC gives us even",
    "start": "1926350",
    "end": "1931930"
  },
  {
    "text": "more structure as to how our HTTP handlers are registered and that gives",
    "start": "1931930",
    "end": "1937060"
  },
  {
    "text": "us the possibility to create dashboards alerting walls and auto scaling mechanisms that work across all of our",
    "start": "1937060",
    "end": "1944350"
  },
  {
    "text": "organizations so I think that's really powerful and together with that service",
    "start": "1944350",
    "end": "1949510"
  },
  {
    "text": "mesh really puts brings this to another level where every single pod within our",
    "start": "1949510",
    "end": "1956410"
  },
  {
    "text": "infrastructure exposes these metrics and they're all they're all the same width across our",
    "start": "1956410",
    "end": "1963730"
  },
  {
    "text": "entire cluster so that's really powerful for observability so really what all",
    "start": "1963730",
    "end": "1968920"
  },
  {
    "text": "this boils down to is we can build reusable alerts dashboards and why we're all here is to build reusable",
    "start": "1968920",
    "end": "1977490"
  },
  {
    "text": "auto-scaling and share all of this knowledge that we have running these applications that are now exposing very similar metrics in",
    "start": "1977490",
    "end": "1984880"
  },
  {
    "text": "order to serve our customers and fulfill our service level objectives so with",
    "start": "1984880",
    "end": "1991810"
  },
  {
    "text": "that let's have a look at our horizontal pot autoscaler again let's see if someone totally broke",
    "start": "1991810",
    "end": "1999489"
  },
  {
    "text": "everything nobody did cool so we now have more replicas of our of my blog",
    "start": "1999489",
    "end": "2009090"
  },
  {
    "text": "isn't that cool and we can see that we have sort of a stable request line which is around 2000",
    "start": "2009090",
    "end": "2017399"
  },
  {
    "text": "requests per second which is exactly the thing that we've specified yeah so that",
    "start": "2017399",
    "end": "2025080"
  },
  {
    "text": "worked awesome and with that thank you",
    "start": "2025080",
    "end": "2031210"
  },
  {
    "text": "[Applause]",
    "start": "2031210",
    "end": "2039349"
  }
]