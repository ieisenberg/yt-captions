[
  {
    "start": "0",
    "end": "30000"
  },
  {
    "text": "thanks for joining us today a John and I are here from the port in Denver Denver so that's beet port not B port and here",
    "start": "60",
    "end": "9269"
  },
  {
    "text": "we are to talk about batch encoding audio workloads on kubernetes had some",
    "start": "9269",
    "end": "14400"
  },
  {
    "text": "interesting use cases this year so my name is Lee Kapila I'm an infrastructure",
    "start": "14400",
    "end": "19410"
  },
  {
    "text": "engineer with B port we're on the same team together and you made me a see me around I helped participate with sig",
    "start": "19410",
    "end": "26250"
  },
  {
    "text": "cluster lifecycle you know collaborating on projects such as coop ADM trying to move the upstream of kubernetes forward",
    "start": "26250",
    "end": "33239"
  },
  {
    "text": "oh there's my twitter handle and stuff and picture of my dog also you guys",
    "start": "33239",
    "end": "39480"
  },
  {
    "text": "totally missed out I went to the parkour gym last night up in Redmond and it was a good time so hi I'm John silifke I'm",
    "start": "39480",
    "end": "47039"
  },
  {
    "start": "45000",
    "end": "71000"
  },
  {
    "text": "also an infrastructure engineer at Beatport I've worked for some much larger companies in the past doing",
    "start": "47039",
    "end": "53039"
  },
  {
    "text": "things in the observability space with open source tools I've also worked as a software engineer for object storage as",
    "start": "53039",
    "end": "59309"
  },
  {
    "text": "well and I might get of information at you here as well if you want to get in touch with me afterwards also a shout",
    "start": "59309",
    "end": "65518"
  },
  {
    "text": "out to former beat Porter's in the office thank you for coming so we replace the stuff you built",
    "start": "65519",
    "end": "72320"
  },
  {
    "start": "71000",
    "end": "107000"
  },
  {
    "text": "so quick show of hands with how many you guys this an electronic music okay cool",
    "start": "75350",
    "end": "81300"
  },
  {
    "text": "cool and how many of you people still download your music and keep it on your computer yeah yeah okay",
    "start": "81300",
    "end": "89550"
  },
  {
    "text": "so that's pretty much our market we're in a niche space nowadays with Spotify",
    "start": "89550",
    "end": "95040"
  },
  {
    "text": "and SoundCloud and everybody else but it's still a good market for us and our customers are mostly DJs at this point",
    "start": "95040",
    "end": "101640"
  },
  {
    "text": "they still like to download their music and curate it them themselves",
    "start": "101640",
    "end": "106549"
  },
  {
    "text": "so what is kind of the story about how we got here as it relates to DJ's we",
    "start": "106670",
    "end": "113220"
  },
  {
    "start": "107000",
    "end": "140000"
  },
  {
    "text": "started out in a very physical medium of having analog signals on vinyl platters",
    "start": "113220",
    "end": "118730"
  },
  {
    "text": "DJing at that time was literally getting together in mixing these two records",
    "start": "118730",
    "end": "124430"
  },
  {
    "text": "like this and cueing it up on the headphones and that was a big deal and then we finally got into the digital",
    "start": "124430",
    "end": "130320"
  },
  {
    "text": "space with CDs this was a little bit more of a compact format we were able to carry things around a lot easier you",
    "start": "130320",
    "end": "136290"
  },
  {
    "text": "could fit more of these in record stores but it was still physical finally we got",
    "start": "136290",
    "end": "141300"
  },
  {
    "start": "140000",
    "end": "229000"
  },
  {
    "text": "flash storage and you saw things like the iPod and these digital download services like iTunes and Beatport which",
    "start": "141300",
    "end": "148380"
  },
  {
    "text": "actually came about it around the same time and so this is where beat ports main business is and has been for about",
    "start": "148380",
    "end": "155580"
  },
  {
    "text": "the past decade we've been incredibly successful even every month still today you know our our record sales on in a",
    "start": "155580",
    "end": "162750"
  },
  {
    "text": "digital format they grow every month and people love our product because we know our customer know DJ's they like having",
    "start": "162750",
    "end": "170430"
  },
  {
    "text": "the asset and not only that but they need it to have like the license in order to perform in the biggest clubs in",
    "start": "170430",
    "end": "176070"
  },
  {
    "text": "the world and to be able to post process their audio with their own tools but has Beatport in an industry that really",
    "start": "176070",
    "end": "183270"
  },
  {
    "text": "hasn't pivoted or changed much right and now like we've seen the likes of soundcloud coming in and there's like a",
    "start": "183270",
    "end": "189030"
  },
  {
    "text": "new format of DJing starting to happen with creative sharing of audio and mixing you know some of you maybe have",
    "start": "189030",
    "end": "195480"
  },
  {
    "text": "been to Burning Man and you know there's a lot of creative sharing of audio happening there",
    "start": "195480",
    "end": "200810"
  },
  {
    "text": "how does Beatport take our legitimate business into the cloud and what does it mean to",
    "start": "200810",
    "end": "206630"
  },
  {
    "text": "use the cloud to provide new tools and new platforms new products and new ways",
    "start": "206630",
    "end": "211940"
  },
  {
    "text": "for people to more instantly and more excessively DJ in creative manners with",
    "start": "211940",
    "end": "216950"
  },
  {
    "text": "less of the arduous workflow of like custom like file naming and playlist",
    "start": "216950",
    "end": "222050"
  },
  {
    "text": "curation and stuff right like how do we make the experience more like this",
    "start": "222050",
    "end": "227540"
  },
  {
    "text": "century and that's the question that we asked yourself and that delivered some",
    "start": "227540",
    "end": "233480"
  },
  {
    "start": "229000",
    "end": "273000"
  },
  {
    "text": "interesting business goals there's some interesting business problems we decided on a couple of product directions some",
    "start": "233480",
    "end": "239510"
  },
  {
    "text": "really exciting stuff coming down the pipeline for DJ's it's really the next pivot in the format of how we perform",
    "start": "239510",
    "end": "246049"
  },
  {
    "text": "and play music but we have like eight five eight point five million tracks in our catalog right now and these new",
    "start": "246049",
    "end": "252950"
  },
  {
    "text": "products require all of our raw assets to be encoded in creative ways that can",
    "start": "252950",
    "end": "258350"
  },
  {
    "text": "be served in that instantaneous format and we're not gonna do that you know",
    "start": "258350",
    "end": "264290"
  },
  {
    "text": "with our petabytes storage cluster in our data center like we need to be able to do that at low latency all around the",
    "start": "264290",
    "end": "271280"
  },
  {
    "text": "world and so the business goal here that we're talking about what we've completed",
    "start": "271280",
    "end": "276950"
  },
  {
    "text": "and delivered it on was recalculating the BPM and doing key analysis on our",
    "start": "276950",
    "end": "283190"
  },
  {
    "text": "entire catalog and then during that process also taking the raw PCM frames from our waves in our petabytes storage",
    "start": "283190",
    "end": "290240"
  },
  {
    "text": "cluster in the data center and replicating that not only in our bare metal location but also into Google",
    "start": "290240",
    "end": "296720"
  },
  {
    "text": "Cloud in 128 kilobits per second with this AAC encoded format with more you",
    "start": "296720",
    "end": "303500"
  },
  {
    "text": "know formats to come and this system that we designed needed to be maintainable and pluggable in a way that",
    "start": "303500",
    "end": "308690"
  },
  {
    "text": "was going to be easy to maintain regardless of who was operating on the system so we're gonna be overloading a",
    "start": "308690",
    "end": "316669"
  },
  {
    "start": "314000",
    "end": "379000"
  },
  {
    "text": "couple terms here so I just want to make sure that everybody is using the same language so I've outlined some some",
    "start": "316669",
    "end": "323060"
  },
  {
    "text": "definitions here one is master when we talk about masters we're typically talking about our authoritative copy of",
    "start": "323060",
    "end": "329840"
  },
  {
    "text": "the PCM data the raw audio data a derivative asset for our purposes is",
    "start": "329840",
    "end": "335340"
  },
  {
    "text": "going to be anything that we derive from that master the back catalog is",
    "start": "335340",
    "end": "340560"
  },
  {
    "text": "basically the entire of our collection that petabyte of storage that Lee's talking about we kind of we track all of",
    "start": "340560",
    "end": "348690"
  },
  {
    "text": "our our releases our releases are a collection of all the tracks that we have in our catalog so these are just",
    "start": "348690",
    "end": "355710"
  },
  {
    "text": "logical groupings of those things and this is we're probably gonna mess this up but we do it all the time but for our",
    "start": "355710",
    "end": "363090"
  },
  {
    "text": "purposes a task is is any any set of work that we put into a pod will",
    "start": "363090",
    "end": "368970"
  },
  {
    "text": "probably end up using the term job at some point as well we'll try to differentiate from kubernetes jobs from",
    "start": "368970",
    "end": "374790"
  },
  {
    "text": "from tasks here yeah so some of our",
    "start": "374790",
    "end": "381990"
  },
  {
    "start": "379000",
    "end": "472000"
  },
  {
    "text": "constraints here we couldn't immediately migrate all of this work we had a lot of dependencies we have a data center that",
    "start": "381990",
    "end": "388770"
  },
  {
    "text": "we've been using for a very long time a lot of our assets were still in this data center so we needed a design a",
    "start": "388770",
    "end": "394950"
  },
  {
    "text": "system that was portable that we could kind of prototype out in the cloud which we ended up doing but ultimately run",
    "start": "394950",
    "end": "401700"
  },
  {
    "text": "that first iteration in our data center in a cost-effective way and then be able to later port that out",
    "start": "401700",
    "end": "408000"
  },
  {
    "text": "to the cloud without any additional work being done so some of our other",
    "start": "408000",
    "end": "413190"
  },
  {
    "text": "constraints were supporting these multiple modes of storage we have some Google Cloud Storage that we've been",
    "start": "413190",
    "end": "418860"
  },
  {
    "text": "using more recently we still have a giant NFS cluster that's hosting most of",
    "start": "418860",
    "end": "424020"
  },
  {
    "text": "our assets so we need to be mindful of these two different read and write modes going between locations we wanted to",
    "start": "424020",
    "end": "432570"
  },
  {
    "text": "minimize our round trips across the network from the data center to our points of presence in Google Cloud and",
    "start": "432570",
    "end": "439070"
  },
  {
    "text": "we needed to make sure the system was observable this is not as well documented or written about kind of how",
    "start": "439070",
    "end": "448140"
  },
  {
    "text": "you observe ephemeral workloads and keep track of those and be able to see things like failures and successes and make",
    "start": "448140",
    "end": "454740"
  },
  {
    "text": "sure that your your system is correct and it's doing what you wanted to do we want to be able to look",
    "start": "454740",
    "end": "460250"
  },
  {
    "text": "at concurrency see how many of these jobs we can complete in a given period",
    "start": "460250",
    "end": "466430"
  },
  {
    "text": "of time and minimize those errors so with this term that we kind of pointed",
    "start": "466430",
    "end": "473000"
  },
  {
    "start": "472000",
    "end": "549000"
  },
  {
    "text": "out which is what is a task we have some business logic right the first thing we need to do is we have to authenticate",
    "start": "473000",
    "end": "478880"
  },
  {
    "text": "with their systems because if there isn't the ability to have an authentication token would say the Beatport api it's not really a good",
    "start": "478880",
    "end": "486169"
  },
  {
    "text": "point in doing much work but say you're authenticated now you've got to fetch the asset right fetch the master from",
    "start": "486169",
    "end": "492650"
  },
  {
    "text": "wherever storage back-end you've currently plugged in to the workload and then once you have the asset in some",
    "start": "492650",
    "end": "499400"
  },
  {
    "text": "kind of local storage you can then run all of our vendor binaries some of those being from third-party companies that",
    "start": "499400",
    "end": "505729"
  },
  {
    "text": "we're partnered with in the industry for the improvements in BPM detection and key detection and then also you know",
    "start": "505729",
    "end": "512570"
  },
  {
    "text": "like minting waveforms so that you can visually interact with the track it's very important for a DJ to be able to",
    "start": "512570",
    "end": "517909"
  },
  {
    "text": "identify where the drop is in the track say right like that and then also like say ffmpeg you know it's open source",
    "start": "517909",
    "end": "524570"
  },
  {
    "text": "component and we're able to take advantage of to you know do correct transformations of PCM data into the",
    "start": "524570",
    "end": "531440"
  },
  {
    "text": "encoded formats that we need such as AAC and so you can see right here that we have like a very simple dag that's a",
    "start": "531440",
    "end": "537890"
  },
  {
    "text": "directed graph and the there's some serial components at the beginning of the business logic and then a fan-out",
    "start": "537890",
    "end": "544459"
  },
  {
    "text": "into components that are no longer dependent on each other at all and this can be represented very easily as a",
    "start": "544459",
    "end": "550910"
  },
  {
    "start": "549000",
    "end": "653000"
  },
  {
    "text": "single unit of compute as a pod and so the pod representation of this business logic it doesn't change really much at",
    "start": "550910",
    "end": "557540"
  },
  {
    "text": "all but you can see there that we have these additional volume amounts and it's important if you've never worked with",
    "start": "557540",
    "end": "562760"
  },
  {
    "text": "the NIC containers before to make note that the list of init containers in the pod actually runs in serial and this is",
    "start": "562760",
    "end": "569480"
  },
  {
    "text": "something that you can do to keep your workloads consistent you can say hey like run an authentication pod first and",
    "start": "569480",
    "end": "575839"
  },
  {
    "text": "then take that mutate something in a volume and then run another container on the contents of that volume right -",
    "start": "575839",
    "end": "581690"
  },
  {
    "text": "maybe fetch the authentication token for like getting something out of storage here you can also see that the read only",
    "start": "581690",
    "end": "589130"
  },
  {
    "text": "masters are only at guessable to the init containers and the necessary ones for pulling that asset",
    "start": "589130",
    "end": "594999"
  },
  {
    "text": "and that none of our vendor binaries actually have read access to our entire catalog which is a very important",
    "start": "594999",
    "end": "601300"
  },
  {
    "text": "security like it's a lease privilege model and the pods API allows us to implement that same thing the waveform",
    "start": "601300",
    "end": "609009"
  },
  {
    "text": "container in the containers array when it's running alongside with all of its friends the mountain named space inside",
    "start": "609009",
    "end": "614860"
  },
  {
    "text": "of that container is different than the mountain namespace of the mp3 encoding container and the AAC encoding container",
    "start": "614860",
    "end": "620860"
  },
  {
    "text": "is a least privileged of volume mount model same thing with you know if you wanted to use Google service accounts in",
    "start": "620860",
    "end": "627519"
  },
  {
    "text": "order to enforce privileges with with the container or sorry with the Google",
    "start": "627519",
    "end": "633370"
  },
  {
    "text": "storage buckets and so we had those things swappable with a feature flag and some some pod features so now that we",
    "start": "633370",
    "end": "640779"
  },
  {
    "text": "kind of like know how to actually represent the compute in a pod then the question becomes like how do you run 8",
    "start": "640779",
    "end": "647860"
  },
  {
    "text": "and 1/2 million of these over a long period of time in your cluster yeah so",
    "start": "647860",
    "end": "654759"
  },
  {
    "start": "653000",
    "end": "742000"
  },
  {
    "text": "you know all of this stuff is very simple we're using well-tested software we're not going out of our way to write",
    "start": "654759",
    "end": "660519"
  },
  {
    "text": "really complicated distributed systems and code we have stuff that works right these are things that we could run our",
    "start": "660519",
    "end": "666730"
  },
  {
    "text": "laptop we just want to be able to do it at scale and utilize our entire data center and use that compute without",
    "start": "666730",
    "end": "673779"
  },
  {
    "text": "writing a ton of code that we have to test right we have a business goal that we need to deliver on and not a lot of",
    "start": "673779",
    "end": "678790"
  },
  {
    "text": "time so what we decided to do is introduce a queue into our system for",
    "start": "678790",
    "end": "684790"
  },
  {
    "text": "publishing these jobs this is kind of important because we didn't want to rely entirely on the grantees api to store",
    "start": "684790",
    "end": "691899"
  },
  {
    "text": "this state right like we don't want to have a single point of failure and lose eight and a half million jobs that we",
    "start": "691899",
    "end": "698110"
  },
  {
    "text": "need to go and query our database for again so that was a conscious decision that we made so then the question we",
    "start": "698110",
    "end": "704620"
  },
  {
    "text": "have to ask ourselves is how are we going to take a simple message right like what is the asset we're working",
    "start": "704620",
    "end": "710499"
  },
  {
    "text": "with what do we want to derive from that asset and how can we get that into language that kubernetes understands and",
    "start": "710499",
    "end": "716139"
  },
  {
    "text": "allow kubernetes to handle the scheduling of those workloads so we came up with this thing that we wrote which is a",
    "start": "716139",
    "end": "723220"
  },
  {
    "text": "Python kubernetes client that is running as a daemon it's reading off this queue",
    "start": "723220",
    "end": "728790"
  },
  {
    "text": "it's able to interpret the intent of those messages and publish kubernetes",
    "start": "728790",
    "end": "734769"
  },
  {
    "text": "job objects to the API that's the section here and then ultimately these",
    "start": "734769",
    "end": "743980"
  },
  {
    "text": "end up this pods that get scheduled in our in our data center on non-homogeneous nodes you can also see",
    "start": "743980",
    "end": "753880"
  },
  {
    "text": "here were communicating with an external API that's storing some of the metadata that we compute we we do things like",
    "start": "753880",
    "end": "760860"
  },
  {
    "text": "checksums and metadata the BPM then key that Lee was talking about so what what",
    "start": "760860",
    "end": "768310"
  },
  {
    "start": "766000",
    "end": "804000"
  },
  {
    "text": "do these messages look like what is what is the minimum amount of information we need to give our system together to do",
    "start": "768310",
    "end": "773740"
  },
  {
    "text": "work so we have a request ID up here as well which is for tracing and then we",
    "start": "773740",
    "end": "780310"
  },
  {
    "text": "have a release ID which is our collection of tracks that this specific asset might be associated with we have",
    "start": "780310",
    "end": "786579"
  },
  {
    "text": "an asset ID which we use to communicate the unique identifiers to the API we",
    "start": "786579",
    "end": "792730"
  },
  {
    "text": "have an asset gooood which we actually used in our storage layout so this is important as well and then give me a",
    "start": "792730",
    "end": "798250"
  },
  {
    "text": "list of things I need to produce what are the different types of derivative assets that we want to be able to plug into this system and so we kind of",
    "start": "798250",
    "end": "806980"
  },
  {
    "start": "804000",
    "end": "858000"
  },
  {
    "text": "settled ok this seems like a pretty decent way to move forward right this was all just architecture and talk and",
    "start": "806980",
    "end": "812380"
  },
  {
    "text": "whiteboarding at that time and then we you know decided this is seems like it's",
    "start": "812380",
    "end": "817540"
  },
  {
    "text": "a pretty decent thing to try and prototype we need to build a system that",
    "start": "817540",
    "end": "822759"
  },
  {
    "text": "actually exhibits some of this behavior in order to determine whether or not like this is going to work as a",
    "start": "822759",
    "end": "828430"
  },
  {
    "text": "production system because we started looking around you know got on Google started reading blog posts and it",
    "start": "828430",
    "end": "834399"
  },
  {
    "text": "doesn't really seem like many people are at least writing about how they manage",
    "start": "834399",
    "end": "839680"
  },
  {
    "text": "batch workloads on kubernetes and this wasn't like something that we really knew was going to work",
    "start": "839680",
    "end": "845990"
  },
  {
    "text": "and we've got the architecture basically where you submit a bunch of messages to the queue and then the dispatcher mints",
    "start": "845990",
    "end": "852410"
  },
  {
    "text": "those messages into compute that's specified in kubernetes kubernetes does the rest now these were our hardware",
    "start": "852410",
    "end": "861500"
  },
  {
    "start": "858000",
    "end": "939000"
  },
  {
    "text": "assets it's a like we've got like basically one rack of servers in our",
    "start": "861500",
    "end": "867800"
  },
  {
    "text": "data center they're like from 2012 2006 let's show that says the del PE 29 50s",
    "start": "867800",
    "end": "873830"
  },
  {
    "text": "those things are tanks and if you've ever racked the hardware you can",
    "start": "873830",
    "end": "878930"
  },
  {
    "text": "understand why we might just have a bunch of old servers hanging around it's because these things were decommissioned",
    "start": "878930",
    "end": "884300"
  },
  {
    "text": "from production for good reasons like bad RAM slots failing dracs spooky stuff",
    "start": "884300",
    "end": "891680"
  },
  {
    "text": "it didn't want run a workload properly you know it just like things that you don't really want to run in production and we thought well could we cluster",
    "start": "891680",
    "end": "899149"
  },
  {
    "text": "these into like a kubernetes cluster I mean like these things do not have you",
    "start": "899149",
    "end": "904550"
  },
  {
    "text": "know like any business running kubernetes we decided to install send to",
    "start": "904550",
    "end": "909860"
  },
  {
    "text": "us on all of these things and start installing kubernetes start installing kubernetes this is a big deal you see",
    "start": "909860",
    "end": "917480"
  },
  {
    "text": "this is a bare-metal installation that requires network topology updates actually like sending people to the data",
    "start": "917480",
    "end": "923149"
  },
  {
    "text": "center to like cut cables plug-in mix rack things replace RAM slots this is a",
    "start": "923149",
    "end": "928279"
  },
  {
    "text": "serious undertaking and like when we decided to prototype the system when we have targeted into running this workload",
    "start": "928279",
    "end": "934220"
  },
  {
    "text": "this cluster didn't exist but if you",
    "start": "934220",
    "end": "940250"
  },
  {
    "start": "939000",
    "end": "1019000"
  },
  {
    "text": "remember from our business constraints we wanted to build on kubernetes so that we would have portability and the cool",
    "start": "940250",
    "end": "946970"
  },
  {
    "text": "thing is that we can go straight to GK and we could say hey like can I please have a 13 node cluster like can I please",
    "start": "946970",
    "end": "952730"
  },
  {
    "text": "start developing my software like can I see if this is going to be a reasonable way to achieve our business goal and",
    "start": "952730",
    "end": "958610"
  },
  {
    "text": "John and I alone were able to in a week provision the infrastructure provision",
    "start": "958610",
    "end": "964010"
  },
  {
    "text": "the H a revenue and Q cluster using helm and write a simple Python you know program to start consuming the messages",
    "start": "964010",
    "end": "971990"
  },
  {
    "text": "and start minting things into the kubernetes api and we were able to model that the workload was actually going to",
    "start": "971990",
    "end": "977839"
  },
  {
    "text": "schedule and meter performance needs and this was a week one word of warning is that I",
    "start": "977839",
    "end": "983510"
  },
  {
    "text": "would highly recommend that if you were going to consume the kubernetes api with",
    "start": "983510",
    "end": "989390"
  },
  {
    "text": "software please use a typed language such as typescript or golang or like something saying because using the",
    "start": "989390",
    "end": "997010"
  },
  {
    "text": "python kubernetes client was easy to iterate with because you didn't have to compile anything but like type mismatch",
    "start": "997010",
    "end": "1003340"
  },
  {
    "text": "errors such as accidentally swapping volume 4 volume mount like those would",
    "start": "1003340",
    "end": "1008440"
  },
  {
    "text": "sometimes submit to the API as valid API objects and then sometimes the program would blow up and the error messages are",
    "start": "1008440",
    "end": "1014590"
  },
  {
    "text": "like walls of JSON I would not recommend writing in Python and we're actually",
    "start": "1014590",
    "end": "1020770"
  },
  {
    "start": "1019000",
    "end": "1347000"
  },
  {
    "text": "going to demo a subset of our business logic right now with basically a system on gke so you can kind of show you how",
    "start": "1020770",
    "end": "1028300"
  },
  {
    "text": "it works a little bit here we have a storage bucket it is just a bunch of",
    "start": "1028300",
    "end": "1034990"
  },
  {
    "text": "test media we have folders with a bunch of WAV files this is raw PCM data we've",
    "start": "1034990",
    "end": "1042250"
  },
  {
    "text": "got an empty storage bucket called the BP coop con demo and P fours and then",
    "start": "1042250",
    "end": "1047860"
  },
  {
    "text": "here using this is Henning Jacob he's currently a solando I believe great",
    "start": "1047860",
    "end": "1054700"
  },
  {
    "text": "contributor in the community he built this front-end that basically shows you all of your nodes in the kubernetes",
    "start": "1054700",
    "end": "1059920"
  },
  {
    "text": "cluster so all these purple boxes are nodes in a gke cluster that we provision specifically for this purpose and all",
    "start": "1059920",
    "end": "1066460"
  },
  {
    "text": "the little green boxes that are currently highlighted are like things in coop systems that are being managed for",
    "start": "1066460",
    "end": "1071560"
  },
  {
    "text": "the kubernetes clusters such as calico pods and things like that so if I start",
    "start": "1071560",
    "end": "1076600"
  },
  {
    "text": "like filtering for things you can actually say hey like where's the dispatcher and there's a little green",
    "start": "1076600",
    "end": "1082690"
  },
  {
    "text": "pod that says yep that's a dispatcher and if I want to say hey where's my RabbitMQ cluster you can see that",
    "start": "1082690",
    "end": "1089110"
  },
  {
    "text": "there's three pods distributed in H a configuration across the cluster so this is kind of showing things",
    "start": "1089110",
    "end": "1096090"
  },
  {
    "text": "it's called coop ops view and you can install it with helman to your cluster really easily yeah so if I filter furan",
    "start": "1097280",
    "end": "1106280"
  },
  {
    "text": "code jobs and then I say hey in our namespace get me the list of pods I'm",
    "start": "1106280",
    "end": "1112309"
  },
  {
    "text": "just gonna get a shell right now in this pod normally we do this from just like a",
    "start": "1112309",
    "end": "1119510"
  },
  {
    "text": "laptop or a management machine - but what I'm gonna do right now is I'm going to use a script and a CSV file to just",
    "start": "1119510",
    "end": "1126260"
  },
  {
    "text": "submit a couple messages to the queue and once those messages are on the queue the dispatcher our little Python program",
    "start": "1126260",
    "end": "1132860"
  },
  {
    "text": "that's currently running right there on the pods list is going to see those messages on the queue and it's going to template a bunch of compute for the",
    "start": "1132860",
    "end": "1139549"
  },
  {
    "text": "kubernetes api so here's the CSV file",
    "start": "1139549",
    "end": "1149919"
  },
  {
    "text": "it's got 201 tracks in it each with a",
    "start": "1149919",
    "end": "1154970"
  },
  {
    "text": "unique good and then if I run publish on",
    "start": "1154970",
    "end": "1160100"
  },
  {
    "text": "that goods file you can see we submitted",
    "start": "1160100",
    "end": "1165770"
  },
  {
    "text": "200 messages to the queue here it's just a bunch of info and very shortly as long",
    "start": "1165770",
    "end": "1171020"
  },
  {
    "text": "as my could see tail proxy is still up yeah you're starting to see a bunch of a",
    "start": "1171020",
    "end": "1176470"
  },
  {
    "text": "very very like mixture see style pods starting to pop into the cluster right",
    "start": "1176470",
    "end": "1183400"
  },
  {
    "text": "now the UI isn't really built for this kind of like queueing use case because",
    "start": "1183400",
    "end": "1190070"
  },
  {
    "text": "right now there's a bunch of pending pods in the cluster and so it's actually kind of funny like they just start to",
    "start": "1190070",
    "end": "1195380"
  },
  {
    "text": "get in line over here and they're not currently scheduled in you know it's good that's kind of cute right but um so",
    "start": "1195380",
    "end": "1202010"
  },
  {
    "text": "right now we've got a couple bunch of kubernetes pods on the cluster well let's go ahead and describe what our workload looks like",
    "start": "1202010",
    "end": "1208870"
  },
  {
    "text": "check it out all right bunch of them put",
    "start": "1212300",
    "end": "1216790"
  },
  {
    "text": "whoops all right so we have this is not",
    "start": "1217840",
    "end": "1223610"
  },
  {
    "text": "precise about 126 of those things running right now and let's like maybe",
    "start": "1223610",
    "end": "1228680"
  },
  {
    "text": "describe the pods but also here we can",
    "start": "1228680",
    "end": "1234650"
  },
  {
    "text": "see okay in this container we've got a couple of volume mounts it's got some secrets and then here's like a shell",
    "start": "1234650",
    "end": "1242000"
  },
  {
    "text": "script you know because like we write - since we're infrastructure engineers see here you can see it's like running",
    "start": "1242000",
    "end": "1247700"
  },
  {
    "text": "ffmpeg and like stat you know it's getting like digests and putting things into a Google bucket with gsutil if you",
    "start": "1247700",
    "end": "1256280"
  },
  {
    "text": "look at the mounts it just has a local amount well that's because this container up here which is called fetch asset has a",
    "start": "1256280",
    "end": "1263690"
  },
  {
    "text": "mount or actually this just has a secret that allows it to copy things from the Google bucket and actually populate the",
    "start": "1263690",
    "end": "1270440"
  },
  {
    "text": "local storage bid all right so that's kind of how the compute workload is structured and if you see that these",
    "start": "1270440",
    "end": "1275990"
  },
  {
    "text": "pods in purple they're actually workloads that have completed computing their assets and so if I go to my",
    "start": "1275990",
    "end": "1282170"
  },
  {
    "text": "previously empty Google bucket I should now have tracks I've got some folders",
    "start": "1282170",
    "end": "1288730"
  },
  {
    "text": "this is why we did the sound check earlier let's make sure that we can actually play a song I hear that this is",
    "start": "1288730",
    "end": "1295040"
  },
  {
    "text": "like from Finland it's a new song has just dropped so hot right now",
    "start": "1295040",
    "end": "1301030"
  },
  {
    "text": "things like a you know this artist is not sure he's like he's really up-and-coming anything it's like I'm",
    "start": "1301160",
    "end": "1308180"
  },
  {
    "text": "gonna say oh wow yeah those are good sing I saw it",
    "start": "1308180",
    "end": "1315690"
  },
  {
    "text": "let's turn it out",
    "start": "1315690",
    "end": "1318200"
  },
  {
    "text": "you see they can really know our customers",
    "start": "1321300",
    "end": "1325890"
  },
  {
    "text": "[Music]",
    "start": "1331030",
    "end": "1335049"
  },
  {
    "text": "all right anyway it works so uh that's our demo there",
    "start": "1338560",
    "end": "1343750"
  },
  {
    "text": "thanks kubernetes so you know things go",
    "start": "1343750",
    "end": "1350780"
  },
  {
    "start": "1347000",
    "end": "1404000"
  },
  {
    "text": "beyond prototype quickly yeah so there's a lot of things that we need to look at",
    "start": "1350780",
    "end": "1355850"
  },
  {
    "text": "when we're designing this system that you might not immediately think about in other contexts our previous system was",
    "start": "1355850",
    "end": "1362660"
  },
  {
    "text": "long-running Damons that we're listening off of the cue they're each responsible for a specific task and like compressing",
    "start": "1362660",
    "end": "1370250"
  },
  {
    "text": "audio or calculating BPM so we were fetching the asset multiple times based on the job that was being performed this",
    "start": "1370250",
    "end": "1376670"
  },
  {
    "text": "is a lot different than approach right so we need to keep track of our pod failure rate how many of those are",
    "start": "1376670",
    "end": "1382100"
  },
  {
    "text": "succeeding how many completions we have per minute concurrency garbage collection was a big one I know there's",
    "start": "1382100",
    "end": "1388610"
  },
  {
    "text": "some efforts being made there now but what we ended up doing was writing a cron job to basically pull 4 completed",
    "start": "1388610",
    "end": "1394730"
  },
  {
    "text": "pods and then delete them and then we need to riku and record failed jobs we",
    "start": "1394730",
    "end": "1399950"
  },
  {
    "text": "need to know what went wrong with the system and how to diagnose it so this is",
    "start": "1399950",
    "end": "1406580"
  },
  {
    "start": "1404000",
    "end": "1645000"
  },
  {
    "text": "an example of the graph that we have in Griffin ax we decided to install the coop Prometheus stack this was actually",
    "start": "1406580",
    "end": "1412700"
  },
  {
    "text": "kind of to monitor some of our other workloads in kubernetes coop Prometheus is a solution from the quarry OS team",
    "start": "1412700",
    "end": "1419480"
  },
  {
    "text": "originally designed to help service their tecktonik kubernetes platform as a service product include Prometheus comes",
    "start": "1419480",
    "end": "1426650"
  },
  {
    "text": "with several things right out of the box you can install it as a helmet art from their git repository and it's got coop",
    "start": "1426650",
    "end": "1432440"
  },
  {
    "text": "state metrics which is an exporter that's designed to basically constantly pull the kubernetes api with the shared",
    "start": "1432440",
    "end": "1438470"
  },
  {
    "text": "informer and then expose things they're natively about the kubernetes api as metrics to Prometheus Prometheus in cou",
    "start": "1438470",
    "end": "1446780"
  },
  {
    "text": "Prometheus is actually operated by kubernetes operator we also get node",
    "start": "1446780",
    "end": "1451820"
  },
  {
    "text": "exporter that's scheduled as a daemon set across the entire cluster so that we can get native metrics about the Linux",
    "start": "1451820",
    "end": "1457580"
  },
  {
    "text": "machines that are running our cluster and then on top of just having prometheus operator with Prometheus",
    "start": "1457580",
    "end": "1464330"
  },
  {
    "text": "which is the data query engine we also have Griffon ax2 then have our persistent dashboard so",
    "start": "1464330",
    "end": "1469559"
  },
  {
    "text": "that we can actually put things together for a constant monitoring view of our cluster and here on the on this",
    "start": "1469559",
    "end": "1477059"
  },
  {
    "text": "dashboard we have encode pod phase counts alright so pause in a specific phase",
    "start": "1477059",
    "end": "1483899"
  },
  {
    "text": "such as pending or running or completed or failed or unknown luckily there's",
    "start": "1483899",
    "end": "1491639"
  },
  {
    "text": "none that are unknown on this graph and what you can see happening here is that",
    "start": "1491639",
    "end": "1498149"
  },
  {
    "text": "purple is the line that represents the total number of pods in our cluster you can see that's kind of like spiking up",
    "start": "1498149",
    "end": "1504830"
  },
  {
    "text": "right every couple of seconds and that's because the dispatcher the little Python",
    "start": "1504830",
    "end": "1511259"
  },
  {
    "text": "program that we wrote so watch the queue it's trying to keep a thousand pods in the cluster at any one time it's kind of",
    "start": "1511259",
    "end": "1517109"
  },
  {
    "text": "a number that we just picked for back pressure you'll know that if you've ever written a queueing system that you need to write back precious back pressure",
    "start": "1517109",
    "end": "1523799"
  },
  {
    "text": "algorithms right or some kind of back pressure threshold and so we're using the number of pods that are currently in",
    "start": "1523799",
    "end": "1529499"
  },
  {
    "text": "the API as a limiter on how many things we should pop up off the queue at any one moment we have the cron jobs in the",
    "start": "1529499",
    "end": "1537269"
  },
  {
    "text": "cluster to clean up things such as completions and failures and then you see this yellow line of pending pods",
    "start": "1537269",
    "end": "1543470"
  },
  {
    "text": "so when we immediately submit things to the cluster they're immediately going to be pending but as they start to get",
    "start": "1543470",
    "end": "1548669"
  },
  {
    "text": "scheduled and as they go into their container creation stage making sure that the docker images are pulled down",
    "start": "1548669",
    "end": "1554309"
  },
  {
    "text": "at the proper image tags then we're going to have running pods you actually see right here I'll point on my mouse",
    "start": "1554309",
    "end": "1560369"
  },
  {
    "text": "right there there's this little blue line that says that there's about 90",
    "start": "1560369",
    "end": "1566460"
  },
  {
    "text": "running pods in the cluster in one time this was 10 nodes in our data center and then we see our completion rate kind of",
    "start": "1566460",
    "end": "1572279"
  },
  {
    "text": "spiking every now and then right now this red bit right here at this time",
    "start": "1572279",
    "end": "1577499"
  },
  {
    "text": "this was an early snapshot this is the important bit about having persistent",
    "start": "1577499",
    "end": "1582929"
  },
  {
    "text": "metrics across your cluster at any one time we have a metrics from back when we had a failure of our API which is an",
    "start": "1582929",
    "end": "1588690"
  },
  {
    "text": "external system to this batch encoding workload and at this time we were",
    "start": "1588690",
    "end": "1594090"
  },
  {
    "text": "dealing with lots of different failures and so we wanted the failures to stop the system which meant that our resubmit cron job",
    "start": "1594090",
    "end": "1600080"
  },
  {
    "text": "was disabled and we had our failures spiked up hit the total pods threshold",
    "start": "1600080",
    "end": "1605510"
  },
  {
    "text": "and the dispatcher stopped submitting things to the cluster at this point I responded to the incident and basically",
    "start": "1605510",
    "end": "1612830"
  },
  {
    "text": "just ripped off all those jobs and put them back in the queue a fix the after I fixed the API so um that's a this",
    "start": "1612830",
    "end": "1621440"
  },
  {
    "text": "workload and kind of monitoring we wrote this query on kubernetes metrics not",
    "start": "1621440",
    "end": "1628160"
  },
  {
    "text": "anything specific to our system or anything that we had instrumented we're just running shell scripts and pots and",
    "start": "1628160",
    "end": "1633890"
  },
  {
    "text": "it was really cool to be able to just install Cooper Mathias and then suddenly get this kind of workload visibility in",
    "start": "1633890",
    "end": "1640010"
  },
  {
    "text": "the way that we were like running things so the advantage of using the kubernetes api um this one is a really funny graph",
    "start": "1640010",
    "end": "1648500"
  },
  {
    "start": "1645000",
    "end": "1725000"
  },
  {
    "text": "as well it's just system utilization so something we really struggled with even though our system was fast it didn't",
    "start": "1648500",
    "end": "1654800"
  },
  {
    "text": "seem to always be saturating our nodes for quite a while we were running at a",
    "start": "1654800",
    "end": "1660140"
  },
  {
    "text": "hundred course and I decided to basically stop the dispatcher which is to basically stop submitting things into",
    "start": "1660140",
    "end": "1667940"
  },
  {
    "text": "the cluster letting the queue just stay persistent and I modified the dispatcher to basically ignore resource",
    "start": "1667940",
    "end": "1674870"
  },
  {
    "text": "requirements so to submit compute workloads to kubernetes that didn't",
    "start": "1674870",
    "end": "1680000"
  },
  {
    "text": "actually ask for like two CPUs right or limit them at five CPUs and a gig of",
    "start": "1680000",
    "end": "1685700"
  },
  {
    "text": "memory and and then this happened and we don't know why this was this was a good",
    "start": "1685700",
    "end": "1691880"
  },
  {
    "text": "thing we had a hypothesis that maybe the that there was some temporal slicing from the the way that the C groups were",
    "start": "1691880",
    "end": "1698180"
  },
  {
    "text": "getting put together we thought maybe let's just remove the resource requirements it's actually talking with the solando folks and they found that",
    "start": "1698180",
    "end": "1704750"
  },
  {
    "text": "sometimes this helps with saturation and we jump up immediately to like 350 CPUs",
    "start": "1704750",
    "end": "1711080"
  },
  {
    "text": "which they're like oh yeah this is awesome like it's gonna go three times faster and that's not what happened they",
    "start": "1711080",
    "end": "1717980"
  },
  {
    "text": "didn't improve our throughput at all so we don't really know what happened but it was using a lot more CPU and we",
    "start": "1717980",
    "end": "1723200"
  },
  {
    "text": "reverted that change you've gotten like apparently know these",
    "start": "1723200",
    "end": "1728470"
  },
  {
    "start": "1725000",
    "end": "1855000"
  },
  {
    "text": "things about requests and limits but I have so many questions if you read the community's documentation about resource",
    "start": "1728470",
    "end": "1733869"
  },
  {
    "text": "requirements this is a huge area where we can improve because it's not incredibly clear when you're reading the",
    "start": "1733869",
    "end": "1739210"
  },
  {
    "text": "documentation there seems to be this conflation of pods and containers like that multiple you know a pod can have",
    "start": "1739210",
    "end": "1745539"
  },
  {
    "text": "multiple containers inside of it but then it will be talking be talking about a container see group and then swap to using pods and there may be some",
    "start": "1745539",
    "end": "1752590"
  },
  {
    "text": "semantic errors in this documentation it's really difficult to understand how things are structured and like say if I",
    "start": "1752590",
    "end": "1759789"
  },
  {
    "text": "provide one container that has resource requirements and then provide another container that doesn't like what happens to the pod see group and how does the",
    "start": "1759789",
    "end": "1767979"
  },
  {
    "text": "how do those see groups get shared across multiple CPUs if the pod see group is then bigger than a single",
    "start": "1767979",
    "end": "1773499"
  },
  {
    "text": "container or sorry big bigger than a single CPU the canonical folks have this",
    "start": "1773499",
    "end": "1779259"
  },
  {
    "text": "hacker noon article called job concurrency in kubernetes with lxd and",
    "start": "1779259",
    "end": "1784359"
  },
  {
    "text": "CPU pinning to the rescue actually really interesting I recommend reading it if this whole resource requirements",
    "start": "1784359",
    "end": "1791109"
  },
  {
    "text": "see group mapping conversation is peaking your interest because there's some nuances here with like even the CRI",
    "start": "1791109",
    "end": "1797830"
  },
  {
    "text": "runtime you pick like what's the docker default of like asking for 1100 Miller Coors the answer",
    "start": "1797830",
    "end": "1804429"
  },
  {
    "text": "that question is not simple and they also cite this guy a St grader who seems",
    "start": "1804429",
    "end": "1810039"
  },
  {
    "text": "to be from the lxd project probably one of their peers who has more detail about how lxd can handle these handles these",
    "start": "1810039",
    "end": "1816729"
  },
  {
    "text": "things and they actually suggest that for a workload similar to ours they were benchmarking video encoding was that the",
    "start": "1816729",
    "end": "1823779"
  },
  {
    "text": "best way to get through put in the system was to run several lxd kubernetes workers on a single bare metal node so",
    "start": "1823779",
    "end": "1831489"
  },
  {
    "text": "to run a kubernetes worker in an lxd container right and then to have that",
    "start": "1831489",
    "end": "1836679"
  },
  {
    "text": "lxd container run the Kubla inside multiple times per bare metal nodes so that the cpus would be pinned precisely",
    "start": "1836679",
    "end": "1843129"
  },
  {
    "text": "to the hardware underlying the actual compute and yeah this was something that",
    "start": "1843129",
    "end": "1849729"
  },
  {
    "text": "was not very simple luckily we were able to scream past our business requirements anyway",
    "start": "1849729",
    "end": "1855749"
  },
  {
    "start": "1855000",
    "end": "1986000"
  },
  {
    "text": "we also like ran into other kubernetes specific issues as well as several workload issues that we dealt with",
    "start": "1855749",
    "end": "1862779"
  },
  {
    "text": "internally but these are probably the more public ones that we definitely",
    "start": "1862779",
    "end": "1867789"
  },
  {
    "text": "wanted to take a note of in this talk so when we first built the system",
    "start": "1867789",
    "end": "1873220"
  },
  {
    "text": "everything was fine and peachy and happy and the prototype worked but then like as we started to low test the system say",
    "start": "1873220",
    "end": "1880539"
  },
  {
    "text": "you know put 10,000 jobs through the cluster or a hundred thousand tasks through the cluster that like all of a",
    "start": "1880539",
    "end": "1887559"
  },
  {
    "text": "sudden we'd be looking at our metrics and there would be gaps and you wouldn't really know why it was really hard to",
    "start": "1887559",
    "end": "1894519"
  },
  {
    "text": "correlate what was happening and then we looked at the pods in the monitoring namespace and we saw that the coop state",
    "start": "1894519",
    "end": "1901059"
  },
  {
    "text": "metrics pods on every or the single coop state metric pod in the cluster was in a crash loop well when you deploy coop",
    "start": "1901059",
    "end": "1908169"
  },
  {
    "text": "state metrics it has a sidecar inside of it that comes from the kubernetes",
    "start": "1908169",
    "end": "1913450"
  },
  {
    "text": "autoscaler repository it's called the add-on resizer or pod nanny some of you",
    "start": "1913450",
    "end": "1920619"
  },
  {
    "text": "may have interacted with this component before the pod nannies job is to take a per node resource requirement look at",
    "start": "1920619",
    "end": "1928299"
  },
  {
    "text": "the number of nodes in the cluster at any one time and basically if the number of nodes in the cluster changes it will",
    "start": "1928299",
    "end": "1934749"
  },
  {
    "text": "multiply that resource requirement and basically rewrite its pod definition so",
    "start": "1934749",
    "end": "1940090"
  },
  {
    "text": "it'll grant itself more resources in the kubernetes api as the cluster grows and shrinks which is a really interesting",
    "start": "1940090",
    "end": "1946059"
  },
  {
    "text": "way to run a workload unfortunately the per node resource requirements are kind of conservatively",
    "start": "1946059",
    "end": "1951850"
  },
  {
    "text": "tuned to normal usage of kubernetes and when you start putting as many objects",
    "start": "1951850",
    "end": "1957580"
  },
  {
    "text": "as we were putting in the API we were just like totally blowing this per node",
    "start": "1957580",
    "end": "1962889"
  },
  {
    "text": "number out of proportion we had to scale the coop state metrics per node resource",
    "start": "1962889",
    "end": "1968049"
  },
  {
    "text": "requirement using the helm values to probably about 20 times the size that",
    "start": "1968049",
    "end": "1973059"
  },
  {
    "text": "was in order to handle the number of objects we were putting into the cluster and that fixed our ability to view the",
    "start": "1973059",
    "end": "1978879"
  },
  {
    "text": "needed metrics to observe the system so the solution was to you know just scale the KSM helm valley",
    "start": "1978879",
    "end": "1985999"
  },
  {
    "text": "and this one is more interesting because the coop controller manager is a core",
    "start": "1985999",
    "end": "1991049"
  },
  {
    "start": "1986000",
    "end": "2066000"
  },
  {
    "text": "kubernetes component and so cook controller manager we're seeing basically right now that it times out on",
    "start": "1991049",
    "end": "1998279"
  },
  {
    "text": "the metrics endpoint in prometheus this is an interesting line like if there are any contributors in the room we need to",
    "start": "1998279",
    "end": "2004129"
  },
  {
    "text": "open an issue for this or I'm not sure if it's been fixed we're running coop 111 and we've just",
    "start": "2004129",
    "end": "2009739"
  },
  {
    "text": "recently released 113 but basically it seems to track metrics for every single",
    "start": "2009739",
    "end": "2016609"
  },
  {
    "text": "object in the cluster and we're not really sure it's unclear if there's a garbage collection or like time period",
    "start": "2016609",
    "end": "2024259"
  },
  {
    "text": "for this because we blow the number out of proportion and it takes like four",
    "start": "2024259",
    "end": "2030679"
  },
  {
    "text": "minutes to return this metrics list after running our workload for about three hours and basically the result is",
    "start": "2030679",
    "end": "2038119"
  },
  {
    "text": "that the Prometheus scraper setup is timing out and the only workaround that",
    "start": "2038119",
    "end": "2043340"
  },
  {
    "text": "we kind of have right now is to restart the coop controller manager this is a safe operation to perform you can just",
    "start": "2043340",
    "end": "2049250"
  },
  {
    "text": "like go delete the docker container backing the static pod on the control plane so I guess you can put that in a",
    "start": "2049250",
    "end": "2056000"
  },
  {
    "text": "cron job but we should fix this it",
    "start": "2056000",
    "end": "2061309"
  },
  {
    "text": "should be trivial I'm not sure if it's fixed in more recent versions of kubernetes or not yeah so you might have",
    "start": "2061309",
    "end": "2068240"
  },
  {
    "start": "2066000",
    "end": "2168000"
  },
  {
    "text": "noticed in Lee's demo there wasn't really any talk or kind of demonstration",
    "start": "2068240",
    "end": "2073368"
  },
  {
    "text": "of jobs there's a reason for that we ran into some issues early on kind of",
    "start": "2073369",
    "end": "2079940"
  },
  {
    "text": "pastor point where we were like okay let's shoot like 10,000 messages and create that many jobs we weren't seeing",
    "start": "2079940",
    "end": "2086779"
  },
  {
    "text": "issues the first time we started kind of a sustained load test of our system to",
    "start": "2086779",
    "end": "2092779"
  },
  {
    "text": "kind of you know kind of like do a pre trial of this long you know three months",
    "start": "2092779",
    "end": "2099559"
  },
  {
    "text": "of encoding we had to do we started run into issues with with jobs over time we",
    "start": "2099559",
    "end": "2105200"
  },
  {
    "text": "started to see list operations and delete operations things we needed to do",
    "start": "2105200",
    "end": "2110390"
  },
  {
    "text": "for basic garbage collection they were starting to degrade the more objects we were adding into the kubernetes api",
    "start": "2110390",
    "end": "2117609"
  },
  {
    "text": "and you know since we were using this back pressure mechanism we were essentially kind of piling up these",
    "start": "2117609",
    "end": "2123099"
  },
  {
    "text": "issues over time and this system would just stop completely and then we couldn't even go in and use KU cuddled",
    "start": "2123099",
    "end": "2129039"
  },
  {
    "text": "lead or anything like that to get the system back to a healthy state and allow this workload to keep going so we we",
    "start": "2129039",
    "end": "2136180"
  },
  {
    "text": "think it might have to do with the job controller and how it waits for its children pods to get cleaned up there",
    "start": "2136180",
    "end": "2142150"
  },
  {
    "text": "might be some additional logic that has to be done there we weren't sure and we still don't quite know yet we're still",
    "start": "2142150",
    "end": "2149260"
  },
  {
    "text": "sort of figuring it out there's a lot of variables we kind of had it like I said",
    "start": "2149260",
    "end": "2154779"
  },
  {
    "text": "we had a business problem to solve and not a lot of time so we decided not to dive into kubernetes internals here and",
    "start": "2154779",
    "end": "2161579"
  },
  {
    "text": "just go ahead and use regular old bare kubernetes pods so what are we losing",
    "start": "2161579",
    "end": "2172329"
  },
  {
    "start": "2168000",
    "end": "2214000"
  },
  {
    "text": "out on if we don't use jobs right these are sort of the basic features of jobs we have parallelism we have retries we",
    "start": "2172329",
    "end": "2180160"
  },
  {
    "text": "can do things over again it should they fail and we have completions right we don't really see a need to complete the",
    "start": "2180160",
    "end": "2188529"
  },
  {
    "text": "same job multiple times we get retries because we have this queuing system and we have an operator that will go in and",
    "start": "2188529",
    "end": "2195190"
  },
  {
    "text": "grab those failed jobs and put them into a dead letter q so we didn't need to worry about that and parallelism it was",
    "start": "2195190",
    "end": "2201460"
  },
  {
    "text": "just not something we needed for this encoding project I also point out that there is garbage collection I think",
    "start": "2201460",
    "end": "2207329"
  },
  {
    "text": "automatic garbage collection as of 1:12 alpha but like I said we're running at",
    "start": "2207329",
    "end": "2212380"
  },
  {
    "text": "111 at this point still so yeah this is how we're cleaning things up at the",
    "start": "2212380",
    "end": "2218349"
  },
  {
    "start": "2214000",
    "end": "2230000"
  },
  {
    "text": "moment we just have a cron job that checks for successful pods and we'll just go ahead and delete those and then",
    "start": "2218349",
    "end": "2223960"
  },
  {
    "text": "we will dead letter our failed stuff so we can add more work to our system so",
    "start": "2223960",
    "end": "2229828"
  },
  {
    "start": "2230000",
    "end": "2250000"
  },
  {
    "text": "yeah we're like I said we're not keeping state and kubernetes we're just gonna use rabbit to store all that work that",
    "start": "2230970",
    "end": "2236529"
  },
  {
    "text": "we need to do this is kept us out of trouble so far and a lot of this stuff",
    "start": "2236529",
    "end": "2242020"
  },
  {
    "text": "I've touched on we have a max number of pods and the API and we have some pretty good performance at this",
    "start": "2242020",
    "end": "2247880"
  },
  {
    "text": "point that we can live with we ran into",
    "start": "2247880",
    "end": "2253430"
  },
  {
    "start": "2250000",
    "end": "2468000"
  },
  {
    "text": "some other interesting things when we started to use the system this was actually independent of any kubernetes",
    "start": "2253430",
    "end": "2261260"
  },
  {
    "text": "centric api once we had like thousands of things in the API at any one moment",
    "start": "2261260",
    "end": "2267410"
  },
  {
    "text": "in time we actually had a coop CTL formatter issue that was plaguing us the default formatter would panic past like",
    "start": "2267410",
    "end": "2275270"
  },
  {
    "text": "nine hundred objects but somebody fixed it because now if you use a newer version of could see tail it doesn't",
    "start": "2275270",
    "end": "2281059"
  },
  {
    "text": "happen that was frustrating though because then you had to like pass your own format or flags in order to get it to work and it probably cost us like",
    "start": "2281059",
    "end": "2288859"
  },
  {
    "text": "four hours of our lives we also noticed some things about the API quality of",
    "start": "2288859",
    "end": "2295039"
  },
  {
    "text": "batch v1 jobs versus core pods obviously pods is like the basic unit of",
    "start": "2295039",
    "end": "2300589"
  },
  {
    "text": "kubernetes like it's one of the most well tested well exercise most written about like best documented regression",
    "start": "2300589",
    "end": "2307910"
  },
  {
    "text": "tested performance scale tested API that we have in the code based and it's a big",
    "start": "2307910",
    "end": "2313460"
  },
  {
    "text": "codebase but when you look at like the usage of batch people in jobs it's really difficult to find people who have",
    "start": "2313460",
    "end": "2320359"
  },
  {
    "text": "written about actually deploying that like a systems of production on the batch API like outside of just little",
    "start": "2320359",
    "end": "2326420"
  },
  {
    "text": "tiny workloads like Oh run a cron job here or run some jobs as part of a helm test pipeline or something like that",
    "start": "2326420",
    "end": "2332450"
  },
  {
    "text": "and we found that field selectors it's not like a like a very public or",
    "start": "2332450",
    "end": "2337789"
  },
  {
    "text": "documented thing that some of the fields in the jobs API actually have typos in",
    "start": "2337789",
    "end": "2342950"
  },
  {
    "text": "them and there's not as many of the field selectors in the jobs API as there",
    "start": "2342950",
    "end": "2348289"
  },
  {
    "text": "are on pods and that makes things really useful if you wanted like say put together a run book on like just using",
    "start": "2348289",
    "end": "2355579"
  },
  {
    "text": "coop CTL to respond to a system incident I actually have an example of something that we did with the pods API here's our",
    "start": "2355579",
    "end": "2362930"
  },
  {
    "text": "internal run book for the system we wrote a JQ program on the coop CTL JSON",
    "start": "2362930",
    "end": "2370220"
  },
  {
    "text": "output and so you can like basically parse in a container and container statuses and then get the",
    "start": "2370220",
    "end": "2377440"
  },
  {
    "text": "exit codes and put it out in a nice table for every single pot in your cluster this is a truncated version of",
    "start": "2377440",
    "end": "2382869"
  },
  {
    "text": "the output that says oh you know like you had some errors in your run-of-the-mill ian's things here's like",
    "start": "2382869",
    "end": "2388359"
  },
  {
    "text": "some audio a/c failures here's some BP API auth failures these pods are still in the cluster like go look at it and",
    "start": "2388359",
    "end": "2394779"
  },
  {
    "text": "fix your stuff and so switching to that API uh bought us some wins and started",
    "start": "2394779",
    "end": "2402849"
  },
  {
    "text": "some interesting conversations in C API machinery so thank you especially to George Jordan widget clearly you know",
    "start": "2402849",
    "end": "2411430"
  },
  {
    "text": "the performance of the pods API it's designed to scale to like 30,000 nodes so it passes our needs for completing a",
    "start": "2411430",
    "end": "2418960"
  },
  {
    "text": "couple of million executions but we found that the batch deletes we were not",
    "start": "2418960",
    "end": "2424329"
  },
  {
    "text": "able to keep a production system running with jobs and so we had to give up the",
    "start": "2424329",
    "end": "2429579"
  },
  {
    "text": "retry so silifke said the end result was that we were actually able to complete the encoding of our entire catalog of",
    "start": "2429579",
    "end": "2437500"
  },
  {
    "text": "ada in half a million tracks in about 80 days and that was 63 days of run time of",
    "start": "2437500",
    "end": "2444099"
  },
  {
    "text": "the system on the storage back-end there were some storage incidents and API issues and some deployment things where",
    "start": "2444099",
    "end": "2451329"
  },
  {
    "text": "we had to basically stop the workload in the cluster to free things up and because that was done in a kubernetes",
    "start": "2451329",
    "end": "2456700"
  },
  {
    "text": "native way like we could use the cluster to kind of perform other tasks and this",
    "start": "2456700",
    "end": "2461710"
  },
  {
    "text": "was a pretty successful project for our company so I know that when we wrote",
    "start": "2461710",
    "end": "2469240"
  },
  {
    "start": "2468000",
    "end": "2940000"
  },
  {
    "text": "this talk submission and when we submitted it we haven't actually built",
    "start": "2469240",
    "end": "2475720"
  },
  {
    "text": "the system yet or operated system in production right like we were just in",
    "start": "2475720",
    "end": "2481210"
  },
  {
    "text": "the middle of load testing and the title of this talk was encoding 250,000 songs",
    "start": "2481210",
    "end": "2486609"
  },
  {
    "text": "a day with batch v1 jobs and what we",
    "start": "2486609",
    "end": "2492130"
  },
  {
    "text": "were able to actually deliver after our estimations and running through all the problems that we kind of told you about",
    "start": "2492130",
    "end": "2498009"
  },
  {
    "text": "as well as some others was encoding 140,000 songs a day with core v1 pas",
    "start": "2498009",
    "end": "2505270"
  },
  {
    "text": "and this was this was a really successful project for us thanks for coming to listen to our story yeah this",
    "start": "2505270",
    "end": "2518890"
  },
  {
    "text": "is these are our twitter handles you can reach out to listen we're absolutely open for questions if we have a mic to",
    "start": "2518890",
    "end": "2523930"
  },
  {
    "text": "go around at all no we didn't repeat the question yeah yeah hey what's your question that's a really good way to do",
    "start": "2523930",
    "end": "2545800"
  },
  {
    "text": "that so the question is why use RabbitMQ in your data center and why not use",
    "start": "2545800",
    "end": "2552490"
  },
  {
    "text": "Google cloud functions to perform the compute workloads and this has to go back to a little bit of our business",
    "start": "2552490",
    "end": "2558820"
  },
  {
    "text": "constraints so the first thing is cost-effectiveness and the reuse of our",
    "start": "2558820",
    "end": "2564190"
  },
  {
    "text": "data center hardware the rapid answer to the RabbitMQ question is that we already",
    "start": "2564190",
    "end": "2569590"
  },
  {
    "text": "used rabbin and q for some other things and we know how to operate it but you know you could plug the dispatcher onto",
    "start": "2569590",
    "end": "2575800"
  },
  {
    "text": "pub/sub probably with some very simple Python client changes and then as far as",
    "start": "2575800",
    "end": "2580930"
  },
  {
    "text": "using the compute we would I you know really love to do service I think",
    "start": "2580930",
    "end": "2586869"
  },
  {
    "text": "there's that news company out in Britain who he chooses a serverless pipeline along with like starting out some video",
    "start": "2586869",
    "end": "2592480"
  },
  {
    "text": "files and they're able to like convert gigabytes of data and like half a second that's a really really fantastic",
    "start": "2592480",
    "end": "2597850"
  },
  {
    "text": "capability the cool thing about this system is that it is built on kubernetes like you could run it on and say a",
    "start": "2597850",
    "end": "2604000"
  },
  {
    "text": "server list kubernetes platform such as asher with their virtual couplet but",
    "start": "2604000",
    "end": "2610450"
  },
  {
    "text": "yeah we basically wanted to use a computer PI that could be run inside of",
    "start": "2610450",
    "end": "2615700"
  },
  {
    "text": "our data center service would be a great way to do this um we couldn't just move",
    "start": "2615700",
    "end": "2621040"
  },
  {
    "text": "a petabyte of our storage out of our data center like quickly though so no yeah yep",
    "start": "2621040",
    "end": "2628770"
  },
  {
    "text": "mm-hmm so the question is since you run a lot of short-term jobs did you learn",
    "start": "2637290",
    "end": "2643320"
  },
  {
    "text": "anything about operating DNS in kubernetes we run core DNS in our",
    "start": "2643320",
    "end": "2648810"
  },
  {
    "text": "cluster which is GA as of 111 our cluster is installed on bare metal as a single node control plane with Covidien",
    "start": "2648810",
    "end": "2655730"
  },
  {
    "text": "and DNS has been relatively boring we do",
    "start": "2655730",
    "end": "2661710"
  },
  {
    "text": "have our core DNS also advertising to our internal VPN and we have like",
    "start": "2661710",
    "end": "2667830"
  },
  {
    "text": "routable service IPS so that was done with a couple of core DNS plug-in modifications but we didn't run into any",
    "start": "2667830",
    "end": "2674730"
  },
  {
    "text": "performance issues and the number of DNS clusters in our DNS queries internal to",
    "start": "2674730",
    "end": "2680490"
  },
  {
    "text": "the cluster is relatively minimal because the API that it's talking to is run in a GK cluster external to the",
    "start": "2680490",
    "end": "2687750"
  },
  {
    "text": "workload does that answer your question cool hey we just have to have some stats",
    "start": "2687750",
    "end": "2697170"
  },
  {
    "text": "on this did you want to yeah it kind of varies wildly we have some tracks that",
    "start": "2697170",
    "end": "2702270"
  },
  {
    "text": "are like two minutes long we also still host like 60 minute extended mixes so it",
    "start": "2702270",
    "end": "2709830"
  },
  {
    "text": "can go anywhere from like two to three seconds to like 30 30 plus minutes",
    "start": "2709830",
    "end": "2716190"
  },
  {
    "text": "sometimes like an hour and for those really long extended mixes so I mean we",
    "start": "2716190",
    "end": "2722640"
  },
  {
    "text": "haven't done analysis and like removed outliers to figure that out but I would",
    "start": "2722640",
    "end": "2727710"
  },
  {
    "text": "say the average track like what we expect is like a five six minute track that takes about six or seven seconds",
    "start": "2727710",
    "end": "2734670"
  },
  {
    "text": "with with a single core yeah and the question was what was the average runtime of the word load um also we did",
    "start": "2734670",
    "end": "2740640"
  },
  {
    "text": "take this into account because we were worried our dispatcher like doesn't batches of submissions to kubernetes and",
    "start": "2740640",
    "end": "2747440"
  },
  {
    "text": "we were worried about like a stampeding herd kind of problem but because the our",
    "start": "2747440",
    "end": "2752730"
  },
  {
    "text": "songs on average are so varied in length it kind of tends to randomize out and",
    "start": "2752730",
    "end": "2757890"
  },
  {
    "text": "the workload becomes relatively stable after about an hour so yeah good question does that answer everything you're",
    "start": "2757890",
    "end": "2764070"
  },
  {
    "text": "looking for yeah hey what's your question",
    "start": "2764070",
    "end": "2772609"
  },
  {
    "text": "that somehow make that change",
    "start": "2784380",
    "end": "2789558"
  },
  {
    "text": "yes that's exactly what we're doing exactly yeah so the question is with",
    "start": "2794040",
    "end": "2801760"
  },
  {
    "text": "your RPC spec where you have a list of assets to produce can you dynamically change the workload in the dispatcher so",
    "start": "2801760",
    "end": "2808600"
  },
  {
    "text": "that it doesn't include all of the unnecessary containers and that's exactly how we built it so the",
    "start": "2808600",
    "end": "2814480"
  },
  {
    "text": "dispatcher will only mint a pod spec based off of what the RPC actually",
    "start": "2814480",
    "end": "2820090"
  },
  {
    "text": "specifies yes yeah the dispatcher generates the pod spec with the Python",
    "start": "2820090",
    "end": "2825700"
  },
  {
    "text": "kubernetes clients it's not from like a gamma file or something like that it is",
    "start": "2825700",
    "end": "2830740"
  },
  {
    "text": "not no another thing to point out too is we also map all of our different types",
    "start": "2830740",
    "end": "2836200"
  },
  {
    "text": "of encoding or derivitive assets to single purpose-built containers so we",
    "start": "2836200",
    "end": "2841750"
  },
  {
    "text": "have a docker registry and even for things like ffmpeg we don't kitchen-sink stuff so if you want to encode like AAC",
    "start": "2841750",
    "end": "2847690"
  },
  {
    "text": "files we have a special compiled binary for ffmpeg that only does AAC",
    "start": "2847690",
    "end": "2854340"
  },
  {
    "text": "the asset so that that's part of the round trip the network consideration is",
    "start": "2863110",
    "end": "2869860"
  },
  {
    "text": "like not pulling files over the network more than once and a single pod is",
    "start": "2869860",
    "end": "2875040"
  },
  {
    "text": "working on an asset and produces basically everything that you need for that asset at at one time and we didn't",
    "start": "2875040",
    "end": "2881860"
  },
  {
    "text": "find the need to do any caching beyond that because you're already preventing like seven or eight poles of the audio",
    "start": "2881860",
    "end": "2888790"
  },
  {
    "text": "file for different things oh yeah that's already cached on the",
    "start": "2888790",
    "end": "2895930"
  },
  {
    "text": "that's that's handled by docker right I",
    "start": "2895930",
    "end": "2902740"
  },
  {
    "text": "think we do if not present not present where is the good one doing this yeah so",
    "start": "2902740",
    "end": "2907990"
  },
  {
    "text": "that was the question about that was what settings are using for docker image pulse in your pod specs and if not",
    "start": "2907990",
    "end": "2914380"
  },
  {
    "text": "present will basically reuse the image it's important to tag your images properly us that you don't bust that",
    "start": "2914380",
    "end": "2921190"
  },
  {
    "text": "cache yeah good question oh cool anything else oh it's a decision",
    "start": "2921190",
    "end": "2928120"
  },
  {
    "text": "when asked something yeah cool thank you well well we're around you know so come chat with us and if you have any more",
    "start": "2928120",
    "end": "2934540"
  },
  {
    "text": "specific questions about your business problems we'd love to kind of chat about batch encoding on kubernetes thank you",
    "start": "2934540",
    "end": "2941700"
  }
]