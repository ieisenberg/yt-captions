[
  {
    "text": "okay thank you so much for joining from tuesday and our talk we are ultra",
    "start": "160",
    "end": "5839"
  },
  {
    "text": "excited to be here in person to talk about a non-trivial challenge of combining short living jobs and",
    "start": "5839",
    "end": "12000"
  },
  {
    "text": "serverless with cloud native pointering but before that short question to the audience uh",
    "start": "12000",
    "end": "18000"
  },
  {
    "text": "raise your hand if you know what the word fleeting means do you know what fleeting means",
    "start": "18000",
    "end": "24160"
  },
  {
    "text": "only couple of hands okay good thank you i was worried that everybody will know what this means because i didn't know",
    "start": "24160",
    "end": "30640"
  },
  {
    "text": "when i was proposing this talk with saswata because yeah i never heard this work in my life",
    "start": "30640",
    "end": "36079"
  },
  {
    "text": "so let's start with a short explanation so the dictionary says that fleeting",
    "start": "36079",
    "end": "41360"
  },
  {
    "text": "means something that passes very quickly and and generally is going sooner than you expect",
    "start": "41360",
    "end": "46960"
  },
  {
    "text": "and i found this amazing cartoon art by artist called mickey bach that explains",
    "start": "46960",
    "end": "52640"
  },
  {
    "text": "the word very well in my opinion he actually did a word a day cartoon series around 1940s",
    "start": "52640",
    "end": "58640"
  },
  {
    "text": "and i think i have to start doing those to kind of learn english better to be honest so in this cartoon we see a comparison called wilbur losing some",
    "start": "58640",
    "end": "66159"
  },
  {
    "text": "something that looked like a feast duel or fight and the woman watching this said that he stood up for a fitting",
    "start": "66159",
    "end": "72479"
  },
  {
    "text": "moment and i think this is pretty clear for me at least um hopefully for you as well but how this then can be applied to",
    "start": "72479",
    "end": "79759"
  },
  {
    "text": "metrics my colleagues as what i will explain that in a second in details but we could",
    "start": "79759",
    "end": "85360"
  },
  {
    "text": "summarize fleeting metrics as follows it's an ability to collect metrics or numeric values over time that aggregates",
    "start": "85360",
    "end": "92640"
  },
  {
    "text": "information from fleeting moments like wilbur uh will be a feast duel for",
    "start": "92640",
    "end": "97840"
  },
  {
    "text": "example imagine wilbur have a lot of those fights perhaps we want to learn with what ratio he is winning or losing",
    "start": "97840",
    "end": "104640"
  },
  {
    "text": "those fights or perhaps we want to know the median duration of those fights right per day",
    "start": "104640",
    "end": "110240"
  },
  {
    "text": "as you can imagine it's hard to get those information from our pool wielberg who potentially passed out already after",
    "start": "110240",
    "end": "116799"
  },
  {
    "text": "a lost fight and maybe forgets what what happened and generally this is what we",
    "start": "116799",
    "end": "122159"
  },
  {
    "text": "will be speaking about today so gathering metrics from those fleeting processes let's say short disclaimer uh",
    "start": "122159",
    "end": "129440"
  },
  {
    "text": "we won't solve all your problems there it's it's there is no silver bullet essentially but hopefully we'll get uh",
    "start": "129440",
    "end": "135280"
  },
  {
    "text": "will give you more context of what what are the options and why it's not trivial",
    "start": "135280",
    "end": "140319"
  },
  {
    "text": "the problem trivial problem and where are potential you know pitfalls of of this solution of this of the solutions",
    "start": "140319",
    "end": "146720"
  },
  {
    "text": "where are available in the space but before that short introduction together with me i have saswata on stage who flew",
    "start": "146720",
    "end": "152800"
  },
  {
    "text": "like over 20 hours with with two stops from india first time in europe um",
    "start": "152800",
    "end": "158100"
  },
  {
    "text": "[Applause]",
    "start": "158100",
    "end": "163200"
  },
  {
    "text": "so hi everyone no no try again uh hi everyone",
    "start": "163200",
    "end": "168959"
  },
  {
    "text": "my name is sasha mukherjee i'm a software engineering intern at red hat on the monitoring team and i was a gsoc",
    "start": "168959",
    "end": "175680"
  },
  {
    "text": "21 student developer under baltic's mentorship in the thames project i love",
    "start": "175680",
    "end": "181200"
  },
  {
    "text": "working with distributed systems and observability related technologies and when i'm not working i love to read and",
    "start": "181200",
    "end": "187840"
  },
  {
    "text": "go out for long walks and my name is bartek brodka and i'm",
    "start": "187840",
    "end": "193360"
  },
  {
    "text": "principal software engineer at red hat i maintain various projects in open source mostly written in golang including",
    "start": "193360",
    "end": "199680"
  },
  {
    "text": "prometheus and thanos and i'm cncf tag observability techlet and i also write a",
    "start": "199680",
    "end": "205200"
  },
  {
    "text": "book with aurelie called efficient go and i put way too much content about observability",
    "start": "205200",
    "end": "211599"
  },
  {
    "text": "so it's kind of related as well so yeah so sweater let's go",
    "start": "211599",
    "end": "216799"
  },
  {
    "text": "so for today's talk let us begin by trying to answer some initial questions",
    "start": "216799",
    "end": "222879"
  },
  {
    "text": "that are what exactly are short-living or fleeting jobs and why monitoring them is",
    "start": "222879",
    "end": "229519"
  },
  {
    "text": "so crucial and also what are the problems that we face when we try to",
    "start": "229519",
    "end": "234640"
  },
  {
    "text": "monitor them so so when we talk about monitoring we usually refer to the metric signal of",
    "start": "234640",
    "end": "241840"
  },
  {
    "text": "observability with the continuous growth of system complexities and the data that",
    "start": "241840",
    "end": "246959"
  },
  {
    "text": "we process every second we need monitoring to understand the state of our workloads and this is",
    "start": "246959",
    "end": "253680"
  },
  {
    "text": "accomplished by tools like prometheus designed to let us determine whether our systems are performing to expected",
    "start": "253680",
    "end": "260799"
  },
  {
    "text": "levels of service by collecting and preparing metrics and notifying humans about any problems and",
    "start": "260799",
    "end": "267120"
  },
  {
    "text": "the most common use case for these metrics is to construct drill down",
    "start": "267120",
    "end": "272320"
  },
  {
    "text": "visual dashboards and to alert and to trigger alerts when a system is behaving",
    "start": "272320",
    "end": "278400"
  },
  {
    "text": "anonymously so let's first understand how prometheus normally monitors your",
    "start": "278400",
    "end": "284080"
  },
  {
    "text": "workloads so in any process you can have numerous events occurring as it programmatically",
    "start": "284080",
    "end": "290320"
  },
  {
    "text": "does things that you needed to do you can even see that every single cpu instruction is an event and many of",
    "start": "290320",
    "end": "297440"
  },
  {
    "text": "these events or groups of events are of interest to people like us who are responsible for running these processes",
    "start": "297440",
    "end": "304479"
  },
  {
    "text": "as this can tell us if the system is behaving as we expect it to",
    "start": "304479",
    "end": "309600"
  },
  {
    "text": "and we want to generate numeric data from these events by aggregating them",
    "start": "309600",
    "end": "315120"
  },
  {
    "text": "over time since we know that we will need to alert analyze or debug the processes that we",
    "start": "315120",
    "end": "320960"
  },
  {
    "text": "run and prometheus scrapes these aggregations of event data from time to",
    "start": "320960",
    "end": "326479"
  },
  {
    "text": "time and generates a series of samples for us which are a pair of numeric",
    "start": "326479",
    "end": "332400"
  },
  {
    "text": "aggregation values and timestamps and these series are what we call as metrics",
    "start": "332400",
    "end": "338560"
  },
  {
    "text": "so with this pull based model of prometheus in mind let us try to understand how the infrastructure and",
    "start": "338560",
    "end": "344960"
  },
  {
    "text": "the functionality that we control has changed over time and why",
    "start": "344960",
    "end": "350400"
  },
  {
    "text": "our metrics are now fleeting in the past we would prepare physical servers for software to be deployed on",
    "start": "350400",
    "end": "357840"
  },
  {
    "text": "which would involve installing the operating system hardware upgrades uh device drivers and so on and so forth as",
    "start": "357840",
    "end": "365360"
  },
  {
    "text": "there was strong coupling between hardware and software this could be called as static infrastructure",
    "start": "365360",
    "end": "372240"
  },
  {
    "text": "afterwards virtual machines emerged so now you could deploy your software to a kind of simulated physical server this",
    "start": "372240",
    "end": "379440"
  },
  {
    "text": "provided a lot more flexibility and made deployments repeatable and started a shift towards dynamic infrastructure and",
    "start": "379440",
    "end": "387199"
  },
  {
    "text": "reduced the amount of functionality that the user needed to control",
    "start": "387199",
    "end": "392639"
  },
  {
    "text": "but there were still some overhead and limitations with this and that is when several containerization tools and",
    "start": "392639",
    "end": "399440"
  },
  {
    "text": "technologies started to be born and made it possible to run several different applications on the same",
    "start": "399440",
    "end": "404960"
  },
  {
    "text": "system without any of them interfering with each other the surface area of user control further",
    "start": "404960",
    "end": "411440"
  },
  {
    "text": "reduced with this and then we now have serverless which abstracts a lot of this away users now",
    "start": "411440",
    "end": "418639"
  },
  {
    "text": "no longer have to care about where their code is running or even when their code is running",
    "start": "418639",
    "end": "423759"
  },
  {
    "text": "as this taken care of by serverless solutions and even if it is not serverless",
    "start": "423759",
    "end": "429039"
  },
  {
    "text": "applications with predefined completion states have existed for a while now such as bad jobs",
    "start": "429039",
    "end": "435520"
  },
  {
    "text": "and cron jobs uh now this is sort of an oversimplification of a very very long",
    "start": "435520",
    "end": "441360"
  },
  {
    "text": "time line of changes but highlights a very important trend which is that users like you and me now have way less",
    "start": "441360",
    "end": "448000"
  },
  {
    "text": "control over the code that we run now this is especially relevant in monitoring as we do not have the full",
    "start": "448000",
    "end": "454720"
  },
  {
    "text": "overview of the situation in such environments we author the code but we",
    "start": "454720",
    "end": "459840"
  },
  {
    "text": "don't run it so what exactly is the nature of such",
    "start": "459840",
    "end": "464879"
  },
  {
    "text": "short living or fleeting workloads so when we talk about short-living jobs we usually indicate some processes like",
    "start": "464879",
    "end": "471599"
  },
  {
    "text": "serverless functions uh batches bat jobs or cron jobs or even ephemeral containers and when we talk",
    "start": "471599",
    "end": "478800"
  },
  {
    "text": "about such processes they usually usually are running and doing the work for a limited but arbitrary amount of",
    "start": "478800",
    "end": "485919"
  },
  {
    "text": "time but short living is kind of a wrong name here or a misnomer here we cannot define",
    "start": "485919",
    "end": "491440"
  },
  {
    "text": "a hard limit or a boundary for when a process goes from short to long",
    "start": "491440",
    "end": "496560"
  },
  {
    "text": "uh say if a job runs for 10 hours is it long or is it short so then how do we",
    "start": "496560",
    "end": "501680"
  },
  {
    "text": "make the distinction here well we can define it as a process that has a well-defined completion state any",
    "start": "501680",
    "end": "508319"
  },
  {
    "text": "process which has some completion state after which we don't expect it to perform any operations and it can be",
    "start": "508319",
    "end": "514800"
  },
  {
    "text": "terminated gracefully can be termed as a short living process the most interesting information for us is",
    "start": "514800",
    "end": "521200"
  },
  {
    "text": "usually after the completion of such a process uh which is what we usually want to enumerate as metrics",
    "start": "521200",
    "end": "528320"
  },
  {
    "text": "so now that we know what a fleeting workload really is let us try to understand how monitoring is different",
    "start": "528320",
    "end": "535279"
  },
  {
    "text": "based on the type of processes for longer living workloads which are by far the most common workload that is",
    "start": "535279",
    "end": "542080"
  },
  {
    "text": "monitored by prometheus like for example a web server it is relatively easy as it",
    "start": "542080",
    "end": "547839"
  },
  {
    "text": "can be scraped and metrics can be collected at any point of time until unless something goes wrong",
    "start": "547839",
    "end": "554320"
  },
  {
    "text": "as the recent aggregations of events are always there in the process itself",
    "start": "554320",
    "end": "559360"
  },
  {
    "text": "it can even have several fleeting processes inside of it but as long as",
    "start": "559360",
    "end": "564640"
  },
  {
    "text": "they are instrumented uh the aggregated data will can still be found within the longer living process and can be scraped",
    "start": "564640",
    "end": "571760"
  },
  {
    "text": "successfully by prometheus however for workloads uh composed of",
    "start": "571760",
    "end": "577120"
  },
  {
    "text": "only fleeting jobs prometheus pool based model becomes problematic and effective",
    "start": "577120",
    "end": "582399"
  },
  {
    "text": "monitoring cannot be achieved a scrape might miss the fleeting job completely",
    "start": "582399",
    "end": "587680"
  },
  {
    "text": "in which case it simply cannot collect any metrics and in the case it does coincide with a running fleeting job it",
    "start": "587680",
    "end": "593920"
  },
  {
    "text": "can only collect some of the partially aggregated data but will miss the rest of the events",
    "start": "593920",
    "end": "600000"
  },
  {
    "text": "just like wilbur who was a fleeting process but got knocked out and now we cannot know how many duals he had or how",
    "start": "600000",
    "end": "606720"
  },
  {
    "text": "many he won prometheus in a typical setup also has the inbuilt capability for monitoring",
    "start": "606720",
    "end": "613760"
  },
  {
    "text": "the health of the discovered or configured targets it does this by generating a dedicated time series",
    "start": "613760",
    "end": "620480"
  },
  {
    "text": "called up with job and instance labels and on each script it aggregate it",
    "start": "620480",
    "end": "625600"
  },
  {
    "text": "appends a sample to the series with a value of one if the instance was reachable and scraped",
    "start": "625600",
    "end": "631839"
  },
  {
    "text": "successfully but if the script fails and the instance is unreachable prometheus appends the up time series with the",
    "start": "631839",
    "end": "638160"
  },
  {
    "text": "value of zero instance is now down that is not possible with fleeting",
    "start": "638160",
    "end": "643440"
  },
  {
    "text": "processes because of the difficulties i mentioned when with regards to how pull based monitoring can scrape fleeting",
    "start": "643440",
    "end": "649920"
  },
  {
    "text": "processes so to summarize we have three major problems when it comes to",
    "start": "649920",
    "end": "656720"
  },
  {
    "text": "monitoring fleeting jobs using prometheus semantics fleeting processes can miss prometheus grapes completely or",
    "start": "656720",
    "end": "663920"
  },
  {
    "text": "partially health and success can't be deduced from upness of the process alone",
    "start": "663920",
    "end": "669440"
  },
  {
    "text": "and finally it's hard to aggregate data correctly while you're only aware about a single event which is a point that",
    "start": "669440",
    "end": "676079"
  },
  {
    "text": "we'll be talking about later on and before this talk we also discuss the same problems with open fast and key",
    "start": "676079",
    "end": "682320"
  },
  {
    "text": "native communities okay hello hello yes okay so what if you",
    "start": "682320",
    "end": "688959"
  },
  {
    "text": "are a promiscuous user or you want to be a prominent use user but you have a lot of those fleeting jobs like serverless",
    "start": "688959",
    "end": "695440"
  },
  {
    "text": "functions bad jobs or any kind of short living processes as we define them what do you do well initially there are two",
    "start": "695440",
    "end": "702399"
  },
  {
    "text": "categories of solution that are often mentioned maybe as an ultimate solutions but the truth is they're just you know",
    "start": "702399",
    "end": "709279"
  },
  {
    "text": "some options in the space and let's go through those and and those options solve some of the challenges saswada",
    "start": "709279",
    "end": "715440"
  },
  {
    "text": "mentioned with fleeting jobs so let's focus on the first one which is simply relying on",
    "start": "715440",
    "end": "722160"
  },
  {
    "text": "event-based observability and deriving metrics from it as we mentioned process our processes",
    "start": "722160",
    "end": "728160"
  },
  {
    "text": "are full of events and some of them are useful to observe and count so then we",
    "start": "728160",
    "end": "733200"
  },
  {
    "text": "can create alerts monitor dashboards and really use that for observability particularly when we look on butt jobs",
    "start": "733200",
    "end": "740560"
  },
  {
    "text": "and fleeting processes there are mostly about single events so it feels natural to have observability that collect logs",
    "start": "740560",
    "end": "748000"
  },
  {
    "text": "of events and there are of course many solutions for that like logging tracing and even",
    "start": "748000",
    "end": "754079"
  },
  {
    "text": "even dedicated solutions that allows us to capture information on the context about each of those events and forward",
    "start": "754079",
    "end": "760560"
  },
  {
    "text": "to some collectors and back in some storage with tracing it's even better because those things are linked together in a",
    "start": "760560",
    "end": "766399"
  },
  {
    "text": "structured way across you know request with even stored in just one place we",
    "start": "766399",
    "end": "773040"
  },
  {
    "text": "can process that information and provide really any metric we often call those jobs rulers aggregators generators",
    "start": "773040",
    "end": "780720"
  },
  {
    "text": "sometimes exporters and the ruler can then deliver those this data to promote use",
    "start": "780720",
    "end": "786399"
  },
  {
    "text": "um or promotes compatible system by allowing promote use or or some kind of collector to scrape it or maybe you know",
    "start": "786399",
    "end": "793440"
  },
  {
    "text": "backfill through usdb block or something like that to sum up with this solution we know about all the events so it's",
    "start": "793440",
    "end": "800240"
  },
  {
    "text": "kind of easy to aggregate and find common dimension uh for for your metrics and there's no",
    "start": "800240",
    "end": "806399"
  },
  {
    "text": "regular you know interval of you know scrape happening so process can have whatever lifetime a lifespan it might be",
    "start": "806399",
    "end": "813760"
  },
  {
    "text": "fleeting short medium long long living as long as we are able to deliver reliably this single trace from each",
    "start": "813760",
    "end": "820560"
  },
  {
    "text": "about each event to some back end we should be fine and there are many solutions in open",
    "start": "820560",
    "end": "826320"
  },
  {
    "text": "source that you can grab and they will do that work uh and it's still growing right so for example you could use",
    "start": "826320",
    "end": "832800"
  },
  {
    "text": "grafana loki with loki ruler you can have graphana tempo and",
    "start": "832800",
    "end": "838800"
  },
  {
    "text": "you know there is i think just i don't know like i seen that like two weeks ago there was like tempo",
    "start": "838800",
    "end": "844000"
  },
  {
    "text": "metric generator and there are ideas to to have some rulers as well um you could have some very you know nice collectors",
    "start": "844000",
    "end": "851839"
  },
  {
    "text": "on steroids like open telemetry like vector which can derive some metrics from the from the",
    "start": "851839",
    "end": "857680"
  },
  {
    "text": "um from the events that it is consuming and there's mtail which is kind of a",
    "start": "857680",
    "end": "863040"
  },
  {
    "text": "kind of old google project that yeah just parses some log line and produce metrics so there are some solutions",
    "start": "863040",
    "end": "868320"
  },
  {
    "text": "there are more and of course there are vendors who who try to solve that for for yeah for some uh some same amount of",
    "start": "868320",
    "end": "875199"
  },
  {
    "text": "money and their downside as everything and because of all the engineering is really",
    "start": "875199",
    "end": "880320"
  },
  {
    "text": "about trade-offs first one is enormous cost that really usually results from complexity of such system",
    "start": "880320",
    "end": "886720"
  },
  {
    "text": "you need to gather every interesting even from all fleeting jobs all fitting processes then process you know and",
    "start": "886720",
    "end": "893600"
  },
  {
    "text": "derive metrics from those and then so then you can alert and monitor and do uh",
    "start": "893600",
    "end": "898720"
  },
  {
    "text": "the whole observability story that's a serious traffic per second um just for",
    "start": "898720",
    "end": "904480"
  },
  {
    "text": "observability data and and and then store all this data in your in your back end now imagine yourself as function",
    "start": "904480",
    "end": "910959"
  },
  {
    "text": "does one thing one event and then admits perhaps few spawns tracing spawns about",
    "start": "910959",
    "end": "916079"
  },
  {
    "text": "the situation and wants to send it to some collector right that's probably a few kilobytes of data and some network",
    "start": "916079",
    "end": "922320"
  },
  {
    "text": "call and so it feel maybe okay from the first glance but serverless are meant to scale",
    "start": "922320",
    "end": "927600"
  },
  {
    "text": "from zero to like you know thousand qrs per second in milliseconds second span",
    "start": "927600",
    "end": "932959"
  },
  {
    "text": "and your receiving pipeline for those events have to have to scale in the same way and you have to pay for that that's",
    "start": "932959",
    "end": "939120"
  },
  {
    "text": "a lot of compute power and engineering time so it means larger cost not only that",
    "start": "939120",
    "end": "944480"
  },
  {
    "text": "but you have to store and reach index and process that data in your back end to derive meaningful information from it",
    "start": "944480",
    "end": "951360"
  },
  {
    "text": "metrics and you are in very much in the big data world this day so then you need to pay",
    "start": "951360",
    "end": "957440"
  },
  {
    "text": "and again on top of that also a metric system uh that will use those metrics so",
    "start": "957440",
    "end": "962560"
  },
  {
    "text": "generally it's more more costly solution right and at the end is very tempting to use this for all observability as a",
    "start": "962560",
    "end": "969839"
  },
  {
    "text": "silver bullet but it's not and i presented some ballpark calculation on the conference in uk two weeks ago",
    "start": "969839",
    "end": "976320"
  },
  {
    "text": "and just tracing back-end cost for some vendor you'll pay 30 times more than just for",
    "start": "976320",
    "end": "984079"
  },
  {
    "text": "your you know compute power for your application logic and i'm not counting here even additional cost of collecting",
    "start": "984079",
    "end": "990800"
  },
  {
    "text": "all of this and sending to this vendor right so you can imagine it can go very sideways if you really are",
    "start": "990800",
    "end": "997120"
  },
  {
    "text": "you know short of of of uh maybe i don't know you want to save some money right so it's kind of crazy and i actually use",
    "start": "997120",
    "end": "1004320"
  },
  {
    "text": "pretty cheap and nice vendor for this calculation and there are more expensive one and then of course there is solution in",
    "start": "1004320",
    "end": "1010240"
  },
  {
    "text": "tracing world which is called sampling it's a very popular technique of filtering uh information about events to",
    "start": "1010240",
    "end": "1016560"
  },
  {
    "text": "only send those are that are that matters like requests that were failing",
    "start": "1016560",
    "end": "1021600"
  },
  {
    "text": "and maybe very slow or maybe interesting for some monitoring purposes and sampling is really must have to have",
    "start": "1021600",
    "end": "1027839"
  },
  {
    "text": "like reasonable cost of tracing even nowadays but then it means we cannot really derive metrics from that very",
    "start": "1027839",
    "end": "1034240"
  },
  {
    "text": "easily because even storage or collector has only partial information of what happened like maybe 10 percent of things",
    "start": "1034240",
    "end": "1040640"
  },
  {
    "text": "so you don't have a big picture and it's kind of inaccurate right so it's right kind of defies the solution but in some",
    "start": "1040640",
    "end": "1047199"
  },
  {
    "text": "way having just the solution like full sampling solution for just fleeting jobs",
    "start": "1047199",
    "end": "1053440"
  },
  {
    "text": "which maybe is a small part of your overall system maybe makes sense maybe it's fair solution for you",
    "start": "1053440",
    "end": "1059600"
  },
  {
    "text": "but with complexity reliability is another problem and there is a reason why promote use is so popular it's just",
    "start": "1059600",
    "end": "1064960"
  },
  {
    "text": "a single binary that just works and close to the workloads and here there are so many stages that something can go",
    "start": "1064960",
    "end": "1071919"
  },
  {
    "text": "wrong and it makes your pipeline makes your monitoring just less reliable",
    "start": "1071919",
    "end": "1078080"
  },
  {
    "text": "and some of this you can mitigate of course by pushing more money and engineering time uh but yeah it's just a",
    "start": "1078080",
    "end": "1084160"
  },
  {
    "text": "trade-off and last but not the least something that is maybe overlooked is that we treat",
    "start": "1084160",
    "end": "1090640"
  },
  {
    "text": "serverless function as a single quick job and you really do really want to make another tcp connection to send",
    "start": "1090640",
    "end": "1096960"
  },
  {
    "text": "observability signal and wait for it and pay for that like price for function function depends on its execution time",
    "start": "1096960",
    "end": "1103200"
  },
  {
    "text": "like for example in lambda you pay for megabytes per second you use um so if",
    "start": "1103200",
    "end": "1108960"
  },
  {
    "text": "the majority of your your time spent on the quick function that and that does some logic is really spent on like",
    "start": "1108960",
    "end": "1115280"
  },
  {
    "text": "sending some observability information that that is not sustainable uh but also we are not experts in serverless jobs so",
    "start": "1115280",
    "end": "1122080"
  },
  {
    "text": "we are just looking that from monitoring perspective um there are other problems too like again you cannot buffer and batch this",
    "start": "1122080",
    "end": "1129600"
  },
  {
    "text": "monitoring you know data together to maybe send it more efficiently",
    "start": "1129600",
    "end": "1134640"
  },
  {
    "text": "so it's much more expensive and slower than if you would have uh you know maybe longer living job",
    "start": "1134640",
    "end": "1140559"
  },
  {
    "text": "and um and what do you do with um you know if tracing collector is down how do",
    "start": "1140559",
    "end": "1145840"
  },
  {
    "text": "you buffer those things doesn't have state so",
    "start": "1145840",
    "end": "1150880"
  },
  {
    "text": "uh yeah so let us take a look at another category of solutions that exist in the",
    "start": "1150880",
    "end": "1156799"
  },
  {
    "text": "ecosystem and allow us to push partially aggregated metrics instead of just",
    "start": "1156799",
    "end": "1162799"
  },
  {
    "text": "events and how this might be useful so let us start again at the beginning where we have a fleeting process we have",
    "start": "1162799",
    "end": "1170559"
  },
  {
    "text": "multiple fleeting processes that have some events internally that we are interested in however as normal",
    "start": "1170559",
    "end": "1177120"
  },
  {
    "text": "prometheus scrape does not work here we need to collect that data somehow",
    "start": "1177120",
    "end": "1182400"
  },
  {
    "text": "something that we can do is aggregate some of the recent data about these interesting events within the process",
    "start": "1182400",
    "end": "1188799"
  },
  {
    "text": "itself this partial aggregation cannot give us the full context that we need but can",
    "start": "1188799",
    "end": "1194960"
  },
  {
    "text": "only give us the very small context of events within a fleeting process",
    "start": "1194960",
    "end": "1200559"
  },
  {
    "text": "as prometheus is not a push based monitoring system we require some additional gateway or collector",
    "start": "1200559",
    "end": "1206799"
  },
  {
    "text": "component which can receive these partial metrics and might be able to aggregate it even further before",
    "start": "1206799",
    "end": "1213200"
  },
  {
    "text": "exposing it for prometheus to scrape and generate samples from so in theory with such a push based approach all of your",
    "start": "1213200",
    "end": "1220080"
  },
  {
    "text": "processes can push partial metrics uh which are essentially aggregations of their recent data collected from the",
    "start": "1220080",
    "end": "1226880"
  },
  {
    "text": "events within the process itself with the push to base approach your",
    "start": "1226880",
    "end": "1232159"
  },
  {
    "text": "fleeting processes can are no longer dependent on the time of a scrape fleeting jobs can simply flush their",
    "start": "1232159",
    "end": "1239039"
  },
  {
    "text": "metrics to the gateway as soon as they are completed and allow the gateway to",
    "start": "1239039",
    "end": "1244080"
  },
  {
    "text": "aggregate it further now discriminator aggregations with this approach are possible but let's try to",
    "start": "1244080",
    "end": "1250640"
  },
  {
    "text": "understand what uh what it is and why we need it so in this kind of scenario each of our",
    "start": "1250640",
    "end": "1257520"
  },
  {
    "text": "fleeting jobs can aggregate their internal events in their own process however as these aggregations can only",
    "start": "1257520",
    "end": "1264640"
  },
  {
    "text": "give us the very small context of a single fleeting workload we cannot use it as this is not an",
    "start": "1264640",
    "end": "1271120"
  },
  {
    "text": "overall view as you can see here with the fu counter metric uh which is sent by each fleeting",
    "start": "1271120",
    "end": "1276400"
  },
  {
    "text": "process after it has been incremented to values like one two or three",
    "start": "1276400",
    "end": "1282000"
  },
  {
    "text": "but for us to get an overall view the metric can be aggregated at the gateway or collector level to the value of six",
    "start": "1282000",
    "end": "1288640"
  },
  {
    "text": "and this is what we can term as a distributed aggregation however different types of aggregations",
    "start": "1288640",
    "end": "1294880"
  },
  {
    "text": "are needed for different types of metrics within prometheus for counters",
    "start": "1294880",
    "end": "1300880"
  },
  {
    "text": "metrics where all the labels match can just be added up so as you can see here once a counter is pushed twice we can",
    "start": "1300880",
    "end": "1308320"
  },
  {
    "text": "expose it with a value of two for histograms or even summaries",
    "start": "1308320",
    "end": "1314000"
  },
  {
    "text": "the buckets can be added up and if the bucket boundaries are mismatched the result can be the union of all buckets",
    "start": "1314000",
    "end": "1321200"
  },
  {
    "text": "so as you can see here with when two http request durations of pi seconds or 3.14 seconds are pushed",
    "start": "1321200",
    "end": "1328000"
  },
  {
    "text": "they get added to 6.28 and for gauges metrics can also be added",
    "start": "1328000",
    "end": "1334480"
  },
  {
    "text": "similarly to counters but we might want uh to provide different semantics for",
    "start": "1334480",
    "end": "1339760"
  },
  {
    "text": "aggregations for example this cpu temperature gauge would benefit from aggregating the last push temperature",
    "start": "1339760",
    "end": "1346559"
  },
  {
    "text": "instead of just simply adding up all of them so several open source solutions can",
    "start": "1346559",
    "end": "1352799"
  },
  {
    "text": "help us in building such a partial metric push based architecture for our fleeting processes all of them with",
    "start": "1352799",
    "end": "1360000"
  },
  {
    "text": "varying semantics implementations and configurations and each with its own trade-offs uh also functionality like",
    "start": "1360000",
    "end": "1367520"
  },
  {
    "text": "distributed aggregations are complex to implement so for example prometheus push",
    "start": "1367520",
    "end": "1372559"
  },
  {
    "text": "gateway supports push and cache functionality but that but not",
    "start": "1372559",
    "end": "1377919"
  },
  {
    "text": "distributed aggregations other solutions like we've works from aggregation gateway",
    "start": "1377919",
    "end": "1383679"
  },
  {
    "text": "does but aren't really customizable and there is even a new rust based uh aggregating gateway that does this via",
    "start": "1383679",
    "end": "1390400"
  },
  {
    "text": "labels and it's built by an awesome engineer at cloud fair called colin douche who is who we actually spoke to",
    "start": "1390400",
    "end": "1396480"
  },
  {
    "text": "before this talk and he is giving a kubecon talk about the same and you can rely on ecosystems of open",
    "start": "1396480",
    "end": "1402960"
  },
  {
    "text": "telemetry and statsd as well for partial metrics push-based architectures too",
    "start": "1402960",
    "end": "1408320"
  },
  {
    "text": "thus you need to carefully consider what fits your workloads now there are also problems around",
    "start": "1408320",
    "end": "1415039"
  },
  {
    "text": "metric stillness semantics metric stainless is not handled well by such push-based solutions solutions like",
    "start": "1415039",
    "end": "1421760"
  },
  {
    "text": "the push gateway never forget the time series that is pushed to it and will expose them forever to prometheus until",
    "start": "1421760",
    "end": "1428240"
  },
  {
    "text": "you manually delete it via the push gateways api which might make it frustrating to use",
    "start": "1428240",
    "end": "1434000"
  },
  {
    "text": "and there is this is also the case with aggregation gateway but it does not even have a metric deletion api this means",
    "start": "1434000",
    "end": "1441360"
  },
  {
    "text": "that to forget old or steel series you would simply have to restart your service",
    "start": "1441360",
    "end": "1446640"
  },
  {
    "text": "and some solutions like open telemetry allow instrumentation to tell when to delete the metrics via specialized otlp",
    "start": "1446640",
    "end": "1454159"
  },
  {
    "text": "protocols but it is not done automatically and if you would pull with prometheus it can cause issues",
    "start": "1454159",
    "end": "1461679"
  },
  {
    "text": "so we have to also in a way share cardinality between our fleeting processes",
    "start": "1461679",
    "end": "1467520"
  },
  {
    "text": "as each of them pushes some partial data often the same exact metric with the same labels but one process cannot see",
    "start": "1467520",
    "end": "1474960"
  },
  {
    "text": "how the other processors are aggregating their events or uh how they are pushing",
    "start": "1474960",
    "end": "1480799"
  },
  {
    "text": "as a result it becomes really easy to explode with cardinality with inconsistent labels and dimensions and",
    "start": "1480799",
    "end": "1487600"
  },
  {
    "text": "this can also lead to some pretty serious accuracy issues which can be really hard to",
    "start": "1487600",
    "end": "1492799"
  },
  {
    "text": "fix so with push reliability also takes a hit",
    "start": "1492799",
    "end": "1497840"
  },
  {
    "text": "push request may fail from your fleeting processes and the gateway not being scalable in the majority of cases might",
    "start": "1497840",
    "end": "1505360"
  },
  {
    "text": "become a bottleneck by dropping push requests or simply not being able to process them and getting oomed as one",
    "start": "1505360",
    "end": "1512080"
  },
  {
    "text": "metrics are pushed to them thus with such solutions uh there are way more things that can break and need to be",
    "start": "1512080",
    "end": "1518080"
  },
  {
    "text": "fixed so as bartek mentioned earlier push pushing in any form might mean an",
    "start": "1518080",
    "end": "1524559"
  },
  {
    "text": "enormous latency hint as you have to initialize some tcp or udp connection",
    "start": "1524559",
    "end": "1529760"
  },
  {
    "text": "while partially aggregated metrics are easier to push instead of actual event data this might already make it too slow",
    "start": "1529760",
    "end": "1537200"
  },
  {
    "text": "for your environment and we also need to worry about scalability",
    "start": "1537200",
    "end": "1542559"
  },
  {
    "text": "for many of the existing open source solutions the gateway or collector components are not horizontally scalable",
    "start": "1542559",
    "end": "1548720"
  },
  {
    "text": "like the prometheus push gateway or the aggregation gateway multiple instances often cannot be run together or",
    "start": "1548720",
    "end": "1555279"
  },
  {
    "text": "simultaneously so these components will likely become single points of failures in your monitoring architecture as you",
    "start": "1555279",
    "end": "1561919"
  },
  {
    "text": "start to push more and more metrics from your applications okay hello hello",
    "start": "1561919",
    "end": "1569039"
  },
  {
    "text": "can you hear me okay as we learned um the two mentioned solutions pushing events and uh pushing metrics they have",
    "start": "1569039",
    "end": "1576960"
  },
  {
    "text": "their you know smaller or bigger problems and the main theme is complexity and it might be fine for your",
    "start": "1576960",
    "end": "1582320"
  },
  {
    "text": "requirements by the way but maybe there is something simpler we could do i think once you saw by the way the",
    "start": "1582320",
    "end": "1587840"
  },
  {
    "text": "challenges with this push of events or metric at least for me it's easier to accept some kind of limitation",
    "start": "1587840",
    "end": "1593840"
  },
  {
    "text": "of pool based metrics and following that in our opinion there are two solutions and i think we are",
    "start": "1593840",
    "end": "1599200"
  },
  {
    "text": "over time so we have three minutes so i will just explain one solution and",
    "start": "1599200",
    "end": "1604640"
  },
  {
    "text": "so first of all reach close box monitoring and so generally the",
    "start": "1604640",
    "end": "1609840"
  },
  {
    "text": "open and closed box monitoring is something that you might know from previous white box and black box",
    "start": "1609840",
    "end": "1615279"
  },
  {
    "text": "monitoring same thing but different name because we are all moving to more inclusive language and premise is very",
    "start": "1615279",
    "end": "1620799"
  },
  {
    "text": "simple it's open box monitoring if the source of the information is directly from the source of the event for example",
    "start": "1620799",
    "end": "1627360"
  },
  {
    "text": "from wilbur directly he might be the best person to tell about some details and what he planned to do in his mind",
    "start": "1627360",
    "end": "1633039"
  },
  {
    "text": "and what went wrong close box monitoring is when you deliver similar or good enough information from",
    "start": "1633039",
    "end": "1639520"
  },
  {
    "text": "someone or something who participated in this event maybe computation maybe network call but they are not source of",
    "start": "1639520",
    "end": "1646399"
  },
  {
    "text": "it and they might just capture information on the way so for example the same questions around winning rate",
    "start": "1646399",
    "end": "1652080"
  },
  {
    "text": "and uh and duration we might ask uh wilbur's partner perhaps wife that you",
    "start": "1652080",
    "end": "1657760"
  },
  {
    "text": "know we're watching all those fights she might know not to know exactly what was on wilbur mind but she saw the whole",
    "start": "1657760",
    "end": "1663919"
  },
  {
    "text": "situation right and this is really some lesson from our requirements gathering discussions with openfast community it's",
    "start": "1663919",
    "end": "1670240"
  },
  {
    "text": "funny kind of we met with the maintainers and to uh you know including alex who is here and um we asked hey",
    "start": "1670240",
    "end": "1676720"
  },
  {
    "text": "with the promising use community what do you need what do you need to make open first better and we're like what do you mean we are happy i was like okay",
    "start": "1676720",
    "end": "1684559"
  },
  {
    "text": "why you are happy and the reason why they are happy is they build their monitoring",
    "start": "1684559",
    "end": "1689600"
  },
  {
    "text": "and they get this mentor monitoring information cheaply from surrounding of of those fleeting processes which also",
    "start": "1689600",
    "end": "1695200"
  },
  {
    "text": "mean there is no instrumentation manual instrumentation needed it's auto instrumented if you think about the",
    "start": "1695200",
    "end": "1700480"
  },
  {
    "text": "smart there are many long living processes around your fleet um for example orchestration system like",
    "start": "1700480",
    "end": "1706480"
  },
  {
    "text": "kubernetes you have cube state metrics so if you have bad job you don't need to instrument back job you can tell it's we",
    "start": "1706480",
    "end": "1712960"
  },
  {
    "text": "you know kind of successful rate or durations by just cube state metrics right the network proxies or gateways",
    "start": "1712960",
    "end": "1719679"
  },
  {
    "text": "are part of your request so you can deliver you know they can provide metrics about",
    "start": "1719679",
    "end": "1724880"
  },
  {
    "text": "uh from network traffic perspective so you can totally build you know red uh method monitoring like rates errors",
    "start": "1724880",
    "end": "1731760"
  },
  {
    "text": "duration all of it just you know from uh you know service meshes finally our os",
    "start": "1731760",
    "end": "1737360"
  },
  {
    "text": "knows ever kernel knows about everything every cpu instruction i o that is",
    "start": "1737360",
    "end": "1742480"
  },
  {
    "text": "happening with your serverless function so also memory usage and performance and so so everything else you can kind of",
    "start": "1742480",
    "end": "1748799"
  },
  {
    "text": "derive from existing tools like c groups ebpf the thing that you might miss is a it's",
    "start": "1748799",
    "end": "1755919"
  },
  {
    "text": "an open box monitoring so things that are with really within the context and logic and of the function itself but",
    "start": "1755919",
    "end": "1762559"
  },
  {
    "text": "because function is super super small the context is usually little very small",
    "start": "1762559",
    "end": "1767679"
  },
  {
    "text": "so the question is do you really need that right and second i don't think i have time for",
    "start": "1767679",
    "end": "1772960"
  },
  {
    "text": "that we can chat about that later but essentially it's like recommendation for serverless architecture as a user or",
    "start": "1772960",
    "end": "1779360"
  },
  {
    "text": "serverless you cannot do more um but we have to skip that unfortunately but all",
    "start": "1779360",
    "end": "1784559"
  },
  {
    "text": "of this yeah you know the recommendation i didn't mention and also the closed bus monitoring what you can do if you are",
    "start": "1784559",
    "end": "1790720"
  },
  {
    "text": "just a user of like lambda or amazon lambda or cloud run or any of those existing solutions right in some",
    "start": "1790720",
    "end": "1797760"
  },
  {
    "text": "way the reality is that you know clouds maybe cloud vendors want to lock you in",
    "start": "1797760",
    "end": "1803600"
  },
  {
    "text": "in some way so users are left without kind of possibility to use normal prompt use",
    "start": "1803600",
    "end": "1808799"
  },
  {
    "text": "metrics normal code registries and instrument normal instrumentation",
    "start": "1808799",
    "end": "1813919"
  },
  {
    "text": "and and those close box monitoring things so it's really about you know also incentivize those vendors to",
    "start": "1813919",
    "end": "1820559"
  },
  {
    "text": "integrate more with open source and cncf kind of monitoring so i guess it's some time for some feature requests and here",
    "start": "1820559",
    "end": "1827039"
  },
  {
    "text": "and there if you are users of those of those platforms so really yeah that's the some of it and",
    "start": "1827039",
    "end": "1834000"
  },
  {
    "text": "the big learning here is that there is no silver bullet here unfortunately both even-based solution pushing metrics uh",
    "start": "1834000",
    "end": "1841520"
  },
  {
    "text": "close box monitoring has their pros and cons and hopefully we learned today about",
    "start": "1841520",
    "end": "1846559"
  },
  {
    "text": "some trade volt trade-offs and implications and hopefully all of this will give this important debugging",
    "start": "1846559",
    "end": "1852159"
  },
  {
    "text": "monitoring and observability data to wilbur who can improve you know feast duels or ideally better negotiation",
    "start": "1852159",
    "end": "1858559"
  },
  {
    "text": "skills to solve disputes in a more friendly way and yeah that's it thank you very much",
    "start": "1858559",
    "end": "1865840"
  },
  {
    "text": "we have a cut-over of four minutes so maybe you can try one question while the others are getting set up",
    "start": "1871279",
    "end": "1878960"
  },
  {
    "text": "are you already cabled alex okay then we're going to use this time",
    "start": "1878960",
    "end": "1886159"
  },
  {
    "text": "yeah would it make sense to run something like the uh the push",
    "start": "1886159",
    "end": "1892559"
  },
  {
    "text": "gateway as a demon set and then on your uh then push to",
    "start": "1892559",
    "end": "1899440"
  },
  {
    "text": "basically local host you can you can answer that if you want",
    "start": "1899440",
    "end": "1904840"
  },
  {
    "text": "yeah yeah so as far as i understand it push gateway is not recommended for those",
    "start": "1904840",
    "end": "1912080"
  },
  {
    "text": "kind of solutions it does not have any such distributed aggregation capabilities so it's only recommended",
    "start": "1912080",
    "end": "1918559"
  },
  {
    "text": "for service level ephemeral bad jobs and running it on like a daemon set or even",
    "start": "1918559",
    "end": "1925039"
  },
  {
    "text": "multiple replicas of it isn't recommended i think there was some mailing list about it too where brian",
    "start": "1925039",
    "end": "1931039"
  },
  {
    "text": "commented that no it isn't safe to run it like that so yeah",
    "start": "1931039",
    "end": "1936640"
  },
  {
    "text": "any other questions",
    "start": "1936799",
    "end": "1940440"
  },
  {
    "text": "maybe you can give us uh two minutes on the worker approach then okay so",
    "start": "1948080",
    "end": "1953840"
  },
  {
    "text": "work our approach oh my god yeah i should pick it up actually do this i didn't know there is",
    "start": "1953840",
    "end": "1958880"
  },
  {
    "text": "enough time but anyway it's about making sure that there is",
    "start": "1958880",
    "end": "1965519"
  },
  {
    "text": "so usually you serverless platforms really architecture the serverless process those fleeting processes as",
    "start": "1965519",
    "end": "1971360"
  },
  {
    "text": "isolated process but if you move a little bit step back and really schedule those in in",
    "start": "1971360",
    "end": "1977440"
  },
  {
    "text": "multi-thread environment rather so have like as one bigger living long living process",
    "start": "1977440",
    "end": "1984799"
  },
  {
    "text": "that schedules those those functions and maybe only one type of the function",
    "start": "1984799",
    "end": "1991360"
  },
  {
    "text": "and then only you scale those long living processes that orchestrates those functions because those functions should",
    "start": "1991360",
    "end": "1996880"
  },
  {
    "text": "be super small there is overhead to even start the process um so it would be good because finally",
    "start": "1996880",
    "end": "2002880"
  },
  {
    "text": "you have some long-living worker that will share some state that then you can batch different things not only",
    "start": "2002880",
    "end": "2008080"
  },
  {
    "text": "monitoring uh but but you know maybe other state as well uh for for even for your logic right and um yeah but the",
    "start": "2008080",
    "end": "2015679"
  },
  {
    "text": "state of of the world right now is that you know they they natively use kubernetes so they would need to move a",
    "start": "2015679",
    "end": "2022320"
  },
  {
    "text": "little bit um away from that so you know it's just a recommendation from from",
    "start": "2022320",
    "end": "2027600"
  },
  {
    "text": "you're someone that looks at this architecture from monitoring perspective so yeah",
    "start": "2027600",
    "end": "2033840"
  },
  {
    "text": "thank you thank you thank you uh any more questions just pop in and slack uh three",
    "start": "2034880",
    "end": "2041360"
  },
  {
    "text": "promiscuously and i'm certain you'll answer them thank you [Applause]",
    "start": "2041360",
    "end": "2052719"
  }
]