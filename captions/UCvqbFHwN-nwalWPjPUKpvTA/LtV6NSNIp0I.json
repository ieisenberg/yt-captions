[
  {
    "text": "hey everyone thank you for joining uh uh Peter hunt I'm also a software",
    "start": "10260",
    "end": "15960"
  },
  {
    "text": "engineer at Red Hat working primarily on cryo but sometimes on keyboard and signode and sometimes run C podman",
    "start": "15960",
    "end": "22080"
  },
  {
    "text": "container related Technologies appreciate everyone coming out uh right before the party hope to get you guys",
    "start": "22080",
    "end": "27420"
  },
  {
    "text": "you all pumped up uh ready to go for this evening and we're gonna be talking",
    "start": "27420",
    "end": "32758"
  },
  {
    "text": "about cryo senior year um so I'm going to start off quick introduction uh what is cryo it gets you",
    "start": "32759",
    "end": "40980"
  },
  {
    "text": "know and cryo is an implementation of the kubernetes container runtime interface compliant with the open",
    "start": "40980",
    "end": "46980"
  },
  {
    "text": "container initiative that's a lot of jargon a lot of acronyms what you can",
    "start": "46980",
    "end": "52020"
  },
  {
    "text": "take away from that is it takes the spot and the stack that Docker used to occupy",
    "start": "52020",
    "end": "58500"
  },
  {
    "text": "um so it uh pulls the oci container images and starts the containers and pods it's",
    "start": "58500",
    "end": "64739"
  },
  {
    "text": "responsible for all the operation underneath the cubelet but before the oci container runtime",
    "start": "64739",
    "end": "71520"
  },
  {
    "text": "some design philosophies that cryo takes on is it's a balance of stability and features with a focus of security and",
    "start": "71520",
    "end": "79560"
  },
  {
    "text": "performance and specifically it's purpose built for kubernetes and we'll describe some of the consequences of",
    "start": "79560",
    "end": "85020"
  },
  {
    "text": "that later so here's a quick architectural diagram of what cryo is so you see on the left",
    "start": "85020",
    "end": "91080"
  },
  {
    "text": "here cry a cubelet talks via grpc to cryo cryo you know cubelet ask cryo to",
    "start": "91080",
    "end": "97920"
  },
  {
    "text": "pull an image or create a container or start a pod cryo's image and runtime",
    "start": "97920",
    "end": "103500"
  },
  {
    "text": "Services then are responsible for doing that underneath the so for the image",
    "start": "103500",
    "end": "109020"
  },
  {
    "text": "service Crow has a library containers image which actually does the pulling and then Crow has for the runtime",
    "start": "109020",
    "end": "116520"
  },
  {
    "text": "service it uses you know OSI runtime generation to actually generate the container spec and then uses an oci",
    "start": "116520",
    "end": "123000"
  },
  {
    "text": "runtime to actually start the container for a pod it uses cni to provision the",
    "start": "123000",
    "end": "128280"
  },
  {
    "text": "networking resources and underneath on the disk you know it uses container storage to allocate the you know the",
    "start": "128280",
    "end": "135300"
  },
  {
    "text": "disk resources like the copy on write file system cry also has an entity called kanman",
    "start": "135300",
    "end": "141420"
  },
  {
    "text": "which we'll be talking about a little bit later but it's a container monitor that actually pays attention to the life cycle of the container",
    "start": "141420",
    "end": "149300"
  },
  {
    "text": "why would anyone use cryo and why do we implore that you do um so cryo is we aim to make cryo secure",
    "start": "149400",
    "end": "157500"
  },
  {
    "text": "by default so you know we try to have a minimal attack Surface by reducing the",
    "start": "157500",
    "end": "163260"
  },
  {
    "text": "set of operations that cryo is responsible for so with other generic container run times as responsible for a",
    "start": "163260",
    "end": "169620"
  },
  {
    "text": "lot of a lot more operations in cryo which is only looking to satisfy the kubernetes CRI and the things that the",
    "start": "169620",
    "end": "176400"
  },
  {
    "text": "cubelet wants it to do so cryo doesn't need to build images or push images for",
    "start": "176400",
    "end": "182280"
  },
  {
    "text": "instance because all it cares about are the things that cubelet wants to be able to do cryo also prioritizes security features",
    "start": "182280",
    "end": "189599"
  },
  {
    "text": "and tries to get them consumable in a way that the kubernetes API can you know satisfy we had experimental support for",
    "start": "189599",
    "end": "196800"
  },
  {
    "text": "user namespaces for years before we just recently added it to the actual CRI and",
    "start": "196800",
    "end": "202379"
  },
  {
    "text": "that allowed folks to test it out and you know work out some issues with it beforehand and also you know it was",
    "start": "202379",
    "end": "209580"
  },
  {
    "text": "specified through annotation so it was actually consumable by uh you know in kubernetes we also ship with a smaller",
    "start": "209580",
    "end": "216239"
  },
  {
    "text": "capability set because we don't expect you know we want um the containers that are run in",
    "start": "216239",
    "end": "222180"
  },
  {
    "text": "production which is cryo's main priority to be as secure as possible",
    "start": "222180",
    "end": "227819"
  },
  {
    "text": "cry also aims to be as performant as possible specifically for kubernetes because crowd's behavior is customized",
    "start": "227819",
    "end": "234060"
  },
  {
    "text": "for kubernetes common operations are optimized for so you know currently with",
    "start": "234060",
    "end": "240180"
  },
  {
    "text": "the generic plague the Pod life cycle event generator the way that the cubelet",
    "start": "240180",
    "end": "245400"
  },
  {
    "text": "maintains the state of what the status of the pods and containers are is by frequently re-listing and asking cryo",
    "start": "245400",
    "end": "252540"
  },
  {
    "text": "hey what's my containers what are my containers what are my pods what are my containers and it does that over and over again fairly frequently to maintain",
    "start": "252540",
    "end": "258900"
  },
  {
    "text": "its state machine while we're improving that asynchronously throughout this time",
    "start": "258900",
    "end": "264020"
  },
  {
    "text": "cryo has optimized for that use case because we know qubit is going to be doing that a lot I also be talking about",
    "start": "264020",
    "end": "271199"
  },
  {
    "text": "another optimization that cryo another set of optimizations that cryo has made a little bit in a little bit",
    "start": "271199",
    "end": "279000"
  },
  {
    "text": "ultimately what we want for cryo is for it to be boring so you know even though we have plenty of these exciting",
    "start": "279000",
    "end": "284940"
  },
  {
    "text": "features that you know optimize for kubernetes what we want really is that an admin chooses cryo as their container",
    "start": "284940",
    "end": "292199"
  },
  {
    "text": "runtime after deliberating over the options and then promptly forgets about it because it's doing the job that it",
    "start": "292199",
    "end": "297660"
  },
  {
    "text": "needs to and really nothing and we're excited to announce that we",
    "start": "297660",
    "end": "304080"
  },
  {
    "text": "are in our senior year hopefully we have just recently applied for graduation and",
    "start": "304080",
    "end": "310320"
  },
  {
    "text": "uh you know we're excited we've been in the uh as the CNN",
    "start": "310320",
    "end": "315560"
  },
  {
    "text": "we cry was born in the kubernetes sandbox kubernetes incubator in 2016 and",
    "start": "315560",
    "end": "323520"
  },
  {
    "text": "then it was it moved up to a Sandbox project which it is now in 2019 and",
    "start": "323520",
    "end": "329699"
  },
  {
    "text": "we're excited you know we have a lot of production users that are using cryo openshift you know Susan was for a while",
    "start": "329699",
    "end": "337020"
  },
  {
    "text": "lift um and so we're we're happy with those relationships and we think that cryo is very ready for production now it",
    "start": "337020",
    "end": "343800"
  },
  {
    "text": "has been for a long time but I think we've really proven it at this point we got a security audit in the spring and",
    "start": "343800",
    "end": "350460"
  },
  {
    "text": "they had uh mostly pretty good things to say and you know the couple of bad things we fixed already so now it's all",
    "start": "350460",
    "end": "356880"
  },
  {
    "text": "good things um and so we're excited for graduation and hope to have that go through uh soon",
    "start": "356880",
    "end": "365280"
  },
  {
    "text": "so uh next up we are going to talk a little bit about um the container monitor that I",
    "start": "365280",
    "end": "370740"
  },
  {
    "text": "mentioned earlier kanman um and you're going to talk about a rewrite of kanman first off we're going",
    "start": "370740",
    "end": "377940"
  },
  {
    "text": "to start out what is kanman um kanman's a little helper agent that manages the life cycle of the container",
    "start": "377940",
    "end": "383340"
  },
  {
    "text": "specifically it actually starts the oci runtime process and it watches for the exit of the container it manages the",
    "start": "383340",
    "end": "390900"
  },
  {
    "text": "logs of the container takes the logs from standard out and writes them to disk",
    "start": "390900",
    "end": "396419"
  },
  {
    "text": "um there's one instance of kanman per container and also one per exact sync",
    "start": "396419",
    "end": "402539"
  },
  {
    "text": "request so if you have an exact probe there's actually a common one running underneath there that will describe why that's important later",
    "start": "402539",
    "end": "409440"
  },
  {
    "text": "um kanban is called by cryo uh over the CLI so it has this pretty large CLI uh",
    "start": "409440",
    "end": "418740"
  },
  {
    "text": "set now and it's currently written in C so we have a couple of reasons that we",
    "start": "418740",
    "end": "426360"
  },
  {
    "text": "want to go through the process of re-reading kanman um specifically some of the points that I",
    "start": "426360",
    "end": "432960"
  },
  {
    "text": "mentioned earlier and why the consequences of those like one container per kanman uh one come on per container",
    "start": "432960",
    "end": "439560"
  },
  {
    "text": "and per exact session means that there's a lot more process overhead than we'd ideally like everyone here knows now",
    "start": "439560",
    "end": "446099"
  },
  {
    "text": "that the standard unit of container is POD you know it's 2022 kubernetes is largely one so we're really thinking",
    "start": "446099",
    "end": "452940"
  },
  {
    "text": "about um uh we're really thinking about containers uh in groups",
    "start": "452940",
    "end": "460220"
  },
  {
    "text": "it's also kind of on a CLI based and we're really looking for a more like modern API mechanism than just passing",
    "start": "460220",
    "end": "468599"
  },
  {
    "text": "it through the CLI versus for instance we have an API version flag that actually uh",
    "start": "468599",
    "end": "475500"
  },
  {
    "text": "um we use to specify differing behavior for uh change that we made years ago but",
    "start": "475500",
    "end": "480720"
  },
  {
    "text": "you know because of API compatibility over the CLI that's a little bit more difficult than it is you know with the",
    "start": "480720",
    "end": "486000"
  },
  {
    "text": "more versioned you know you know smart IPC mechanism and you know it's a it's a bit of a",
    "start": "486000",
    "end": "492360"
  },
  {
    "text": "clunky program you know it's a little tough to work with sometimes it was written in C and it's you know started a long time ago and you know we it's",
    "start": "492360",
    "end": "498840"
  },
  {
    "text": "really served us well and it's very stable but you know we're looking forward and looking up and trying to do",
    "start": "498840",
    "end": "504479"
  },
  {
    "text": "new things what we want out of our container monitor is really we want one kanman per",
    "start": "504479",
    "end": "512339"
  },
  {
    "text": "pod because you know the unit of containers in 2022 is now pods",
    "start": "512339",
    "end": "518459"
  },
  {
    "text": "um we want an IPC API based mechanism to speak with common CLI is good and it",
    "start": "518459",
    "end": "525060"
  },
  {
    "text": "works but it's clunky and it can be better and we want a more modern language",
    "start": "525060",
    "end": "530519"
  },
  {
    "text": "um you know C has served us well but everyone knows the common pitfalls of c and a lot of the difficulties that can",
    "start": "530519",
    "end": "536880"
  },
  {
    "text": "arise from using it and so we want to kind of work around you know work around those by having a",
    "start": "536880",
    "end": "544140"
  },
  {
    "text": "language that works a little bit better so I am happy to introduce to you a",
    "start": "544140",
    "end": "550320"
  },
  {
    "text": "program that actually satisfies all of these constraints and more we have uh",
    "start": "550320",
    "end": "555839"
  },
  {
    "text": "kanman RS you know I originally it would have been cool if we could have named it podman because uh you know it's a pod",
    "start": "555839",
    "end": "564660"
  },
  {
    "text": "monitor after all but for some reason people said that that would be confusing um you know I wonder why so instead we",
    "start": "564660",
    "end": "571080"
  },
  {
    "text": "settled on kanman RS a rust implementation of kanman it currently covers the scope of the existing kanban",
    "start": "571080",
    "end": "578880"
  },
  {
    "text": "but with a couple of differences some highlights of common RS is it has we have a native golang client API which",
    "start": "578880",
    "end": "586620"
  },
  {
    "text": "wraps Captain Proto as a protocol cam Proto is an API framework that's used by cloudflare and",
    "start": "586620",
    "end": "593399"
  },
  {
    "text": "a couple of other folks that basically advertises zero serialization between",
    "start": "593399",
    "end": "598760"
  },
  {
    "text": "the uh different even different languages so we can speak between go and rust and it basically passes a block of",
    "start": "598760",
    "end": "605880"
  },
  {
    "text": "memory over and those are each and mapped into the different languages so it's very fast",
    "start": "605880",
    "end": "612180"
  },
  {
    "text": "and also has less complexity than something like you know grpc or tgrpc or something",
    "start": "612180",
    "end": "620100"
  },
  {
    "text": "it uh come on our restaurants on the Pod level not for each container",
    "start": "620100",
    "end": "625860"
  },
  {
    "text": "um which is you know exactly what we want now and it also supports having the exact",
    "start": "625860",
    "end": "631920"
  },
  {
    "text": "sessions within the container which is beneficial",
    "start": "631920",
    "end": "636959"
  },
  {
    "text": "um it aims to keep a low memory usage so kanman used something around two Megs",
    "start": "636959",
    "end": "642420"
  },
  {
    "text": "per container and we're aiming you know something between four and six it would be better if it was lower obviously and",
    "start": "642420",
    "end": "648300"
  },
  {
    "text": "we're working on that but we really want there to be no memory penalty for using you know I theoretically there should",
    "start": "648300",
    "end": "655440"
  },
  {
    "text": "not be a memory penalty for using rust and you know if you had a pod with you know two kanbans for one for each",
    "start": "655440",
    "end": "662579"
  },
  {
    "text": "container and then maybe you know a couple of con mounts for the execs we aim for common RS to be much less memory",
    "start": "662579",
    "end": "668940"
  },
  {
    "text": "than all of that would ultimately end up using common RS to be able to support pod level",
    "start": "668940",
    "end": "675660"
  },
  {
    "text": "um as you know to support all the containers in the Pod it's going to be multi-threaded where kanman used to be",
    "start": "675660",
    "end": "680940"
  },
  {
    "text": "single threaded or still single threaded um and we have some exciting features that we're able to enhance kanman RS",
    "start": "680940",
    "end": "687720"
  },
  {
    "text": "with because uh is written in Rust so it's a little bit easier for us to add new features to it",
    "start": "687720",
    "end": "694200"
  },
  {
    "text": "um to be able to use common RS in cryo all you have to do is add a drop in file",
    "start": "694200",
    "end": "699240"
  },
  {
    "text": "to crowds config directory and specify the runtime type as pod and then point",
    "start": "699240",
    "end": "705660"
  },
  {
    "text": "it to con Monarch the location of con man RS and then restart cryo and cryo",
    "start": "705660",
    "end": "710880"
  },
  {
    "text": "will come up use column on our acid hopefully you can also promptly forget about that choice",
    "start": "710880",
    "end": "717000"
  },
  {
    "text": "right now we have uh passing CRI tests and integration tests kubernetes and and",
    "start": "717000",
    "end": "722459"
  },
  {
    "text": "no tests basically all of the tests that cryo expects for you know a fully functioning",
    "start": "722459",
    "end": "728700"
  },
  {
    "text": "um you know to be fully compliant with kubernetes um common RS is passing now",
    "start": "728700",
    "end": "734820"
  },
  {
    "text": "um we're planning for integration into podman uh there's a couple more uh pieces that we need for podman which",
    "start": "734820",
    "end": "741000"
  },
  {
    "text": "we've not quite gotten to yet so we'll talk about that a little bit um we have RPM packages available",
    "start": "741000",
    "end": "747320"
  },
  {
    "text": "and we also have static binaries available for each commit so if you want to download and use it you can currently",
    "start": "747320",
    "end": "753959"
  },
  {
    "text": "and we're looking for adding more distributions in the future as we stabilize in the future I'd like to describe to",
    "start": "753959",
    "end": "761339"
  },
  {
    "text": "you a world in which uh we have all the pieces that kanman used to have so there's currently some gaps you know we",
    "start": "761339",
    "end": "767160"
  },
  {
    "text": "have all the functioning pieces we need for cryo but you know podman needs a little bit more needs an attached exec",
    "start": "767160",
    "end": "772920"
  },
  {
    "text": "session because podman the process goes away and they need to see something holding open that exact currently cryo",
    "start": "772920",
    "end": "778620"
  },
  {
    "text": "is doing that um we need checkpointing uh for podman and",
    "start": "778620",
    "end": "784200"
  },
  {
    "text": "also actually soon for cryo because we're adding checkpoint and restore um functionality we need uh support for",
    "start": "784200",
    "end": "792660"
  },
  {
    "text": "SEC comp notify to be able to be notified when a container uses Asis call",
    "start": "792660",
    "end": "798000"
  },
  {
    "text": "that it wasn't supposed to or wasn't expected to and the journal D log driver",
    "start": "798000",
    "end": "803040"
  },
  {
    "text": "which you know just adds compatibility to what podman currently uses",
    "start": "803040",
    "end": "808680"
  },
  {
    "text": "we also looking forward have some features that we're pretty excited about hidden name space holding is one that I",
    "start": "808680",
    "end": "814980"
  },
  {
    "text": "personally am really excited about So currently if you have a pod level pin namespace you need the infra container",
    "start": "814980",
    "end": "820440"
  },
  {
    "text": "to hold open that namespace because you need to have the pid1 in the namespace to stay alive for the duration of all of",
    "start": "820440",
    "end": "827519"
  },
  {
    "text": "the processes within the pin name space currently the infra container I mean it works this worked for a long time but it",
    "start": "827519",
    "end": "833459"
  },
  {
    "text": "adds a little bit of process complexity and overhead and it would be nice if we just had some process imagine what a",
    "start": "833459",
    "end": "840540"
  },
  {
    "text": "process that survived for the duration of the Pod and was able to hold the uh Pig namespace for it and con man RS",
    "start": "840540",
    "end": "847980"
  },
  {
    "text": "satisfies those requirements so I think it'd be cool to be able to have the pin name space held by kanman RS and then",
    "start": "847980",
    "end": "854100"
  },
  {
    "text": "we'd be able to drop the input container in all cases also looking for IPv6 uh port forward",
    "start": "854100",
    "end": "861300"
  },
  {
    "text": "support which is required by or desired bipod man there needs to be a running",
    "start": "861300",
    "end": "866459"
  },
  {
    "text": "process to keep uh port forward requests going in IPv6",
    "start": "866459",
    "end": "871560"
  },
  {
    "text": "uh we want a logging rate limiting um so and just additional log drivers in",
    "start": "871560",
    "end": "876779"
  },
  {
    "text": "general we have uh you know right now there's like the kubernetes log uh",
    "start": "876779",
    "end": "881820"
  },
  {
    "text": "format and we're looking towards Journal D but also we have the opportunity for a Json logging driver which podman has",
    "start": "881820",
    "end": "888720"
  },
  {
    "text": "wanted for years ever and has never actually gotten and so being written in a more modern",
    "start": "888720",
    "end": "894839"
  },
  {
    "text": "language it'll be easier to integrate some of these new features and open Telemetry tracing which I'm",
    "start": "894839",
    "end": "900300"
  },
  {
    "text": "actually excited to say we have experimental support for we're still kind of working through the details of",
    "start": "900300",
    "end": "905459"
  },
  {
    "text": "it um and I didn't have time to put together a demo of it and I I want to",
    "start": "905459",
    "end": "910760"
  },
  {
    "text": "but but open Telemetry tracing is coming and so you can actually track the",
    "start": "910760",
    "end": "917220"
  },
  {
    "text": "especially with the cubelets support for open Telemetry tracing and cryos you can track the life cycle of a container from",
    "start": "917220",
    "end": "925139"
  },
  {
    "text": "you know being created in the API server being registered in FCD being created by",
    "start": "925139",
    "end": "930480"
  },
  {
    "text": "the cubelet being created by cryo and then all the way down to kanman RS now which we think is exciting and opens up",
    "start": "930480",
    "end": "937199"
  },
  {
    "text": "some opportunities for being able to track the life cycle of your containers",
    "start": "937199",
    "end": "942240"
  },
  {
    "text": "and pods so that's uh come on RS we're pretty excited for it",
    "start": "942240",
    "end": "947940"
  },
  {
    "text": "um and now I'm going now for something completely different we're going to talk a little bit about some load optimizations and specifically some",
    "start": "947940",
    "end": "954779"
  },
  {
    "text": "better reporting mechanisms that cryo has added semi-recently so if you've",
    "start": "954779",
    "end": "959880"
  },
  {
    "text": "attended some of these talks before you might have seen me talking about situations of load and kubernetes and",
    "start": "959880",
    "end": "965880"
  },
  {
    "text": "specifically between cryo and cubelet but I'll give a refresher for anyone who's new here so the problem is in",
    "start": "965880",
    "end": "973320"
  },
  {
    "text": "situations of load um cryo and Cuba get into this bickering match where they have trouble syncing up",
    "start": "973320",
    "end": "979260"
  },
  {
    "text": "between so basically the cubelet needs to have a Timeout on each container and",
    "start": "979260",
    "end": "985440"
  },
  {
    "text": "pod creation request because it needs to know that when it sends out that request that request and it just disappear into",
    "start": "985440",
    "end": "990899"
  },
  {
    "text": "the void the problem about that is that cryo you know can take an undetermined",
    "start": "990899",
    "end": "996480"
  },
  {
    "text": "amount of time to create a pottery container especially under load usually there's some entity that is the bottleneck it's often the sdn or maybe",
    "start": "996480",
    "end": "1004519"
  },
  {
    "text": "the um maybe the disc i o um and so it cryo might not be able to",
    "start": "1004519",
    "end": "1013160"
  },
  {
    "text": "create that container in time and because of that Crown uh the cuboid bicker about trying to create that",
    "start": "1013160",
    "end": "1019279"
  },
  {
    "text": "specified resource Cupid's like hey please create this resource and crowd's like hey I'm working on it name is reserved gets this awkward situation",
    "start": "1019279",
    "end": "1026900"
  },
  {
    "text": "so this has been largely solved in cryo as of you know 119 or something like",
    "start": "1026900",
    "end": "1032178"
  },
  {
    "text": "that and I'll describe so it's quite old now but I'll describe the solution because we've added a little bit to that",
    "start": "1032179",
    "end": "1038480"
  },
  {
    "text": "solution so uh the solution to it is to fine-tune our Behavior to the cubelet which cry I",
    "start": "1038480",
    "end": "1044959"
  },
  {
    "text": "was able to do because it's you know only for the only client that cryo materially cares about is the cubelet",
    "start": "1044959",
    "end": "1050840"
  },
  {
    "text": "um and running you know containers in production so um the cube the the basic idea is to",
    "start": "1050840",
    "end": "1058280"
  },
  {
    "text": "finish creating the resource and then save it into the cube last again so uh walk us through that example so cubelet",
    "start": "1058280",
    "end": "1064580"
  },
  {
    "text": "asked Crow hey can you create me this pod and Kyle's like sure I'll Reserve this name so that no one else know cry",
    "start": "1064580",
    "end": "1071000"
  },
  {
    "text": "cuddle or someone can come in and try to take that pod from you at some point in that process maybe cryo",
    "start": "1071000",
    "end": "1076760"
  },
  {
    "text": "gets stuck on sdn because it's taking a real long time there's a lot of pause being created at the same time something",
    "start": "1076760",
    "end": "1083240"
  },
  {
    "text": "crowd takes too long and the cubelet times out Cuba's not sure whether the you know timeout is because the request",
    "start": "1083240",
    "end": "1090260"
  },
  {
    "text": "disappeared or because you know it um actually is just taking too long so just in case qubit re-requests hey can",
    "start": "1090260",
    "end": "1097400"
  },
  {
    "text": "you create me this pod the second routine is aware that the first routine is",
    "start": "1097400",
    "end": "1103880"
  },
  {
    "text": "working on creating that plot because it can tell that the name is already reserved so what it does is it waits and",
    "start": "1103880",
    "end": "1110380"
  },
  {
    "text": "basically tells the first routine hey when you're done creating that let me know because I'm trying to give it to",
    "start": "1110380",
    "end": "1116419"
  },
  {
    "text": "the cubelet eventually the bottleneck either clears up or you know the networking uh your",
    "start": "1116419",
    "end": "1124220"
  },
  {
    "text": "pot you're finally you know on gums and crowd's able to create the",
    "start": "1124220",
    "end": "1129820"
  },
  {
    "text": "resource so crowd that the first routine detects",
    "start": "1129820",
    "end": "1134840"
  },
  {
    "text": "that there was a timeout and um you know pings the second routine",
    "start": "1134840",
    "end": "1140179"
  },
  {
    "text": "saying hey I'm done my thing now and returns to the cubelet you know Cuba's not paying attention anymore it assumed",
    "start": "1140179",
    "end": "1146360"
  },
  {
    "text": "that the request disappeared so this dotted line represents Cuba's not really listening anymore but what it is",
    "start": "1146360",
    "end": "1151700"
  },
  {
    "text": "listening to is the second routine which has said you know hey I have this resource for you but you need to request",
    "start": "1151700",
    "end": "1158240"
  },
  {
    "text": "again the reason we do another round of requests even though the resources already made is because you know the",
    "start": "1158240",
    "end": "1164679"
  },
  {
    "text": "resource there might be a race between the Cuba timing out when cryo sends it",
    "start": "1164679",
    "end": "1169940"
  },
  {
    "text": "so we worry about you know having this resource disappear so instead we we time",
    "start": "1169940",
    "end": "1175220"
  },
  {
    "text": "out one more time in the cubelet and then um await the inevitable cubelet",
    "start": "1175220",
    "end": "1180799"
  },
  {
    "text": "re-request that resource from another cryo routine that routine sees that it",
    "start": "1180799",
    "end": "1187100"
  },
  {
    "text": "has the resource and it's able to return it no problem so this this was a good optimization",
    "start": "1187100",
    "end": "1194480"
  },
  {
    "text": "because it changed the situation where there used to be a lot of thrashing on the Node where container would be",
    "start": "1194480",
    "end": "1200419"
  },
  {
    "text": "created it would take some time and then it would have to be you know the the creation would time out and at the time",
    "start": "1200419",
    "end": "1207380"
  },
  {
    "text": "we actually removed the container because it aired out and that was bad because that actually made the the um",
    "start": "1207380",
    "end": "1215120"
  },
  {
    "text": "the resources be even more constrained so now you know we've improved the",
    "start": "1215120",
    "end": "1220820"
  },
  {
    "text": "situation a lot now we're basically returning the resource just about as soon as it's actually done",
    "start": "1220820",
    "end": "1227419"
  },
  {
    "text": "um another uh you know small piece of it is we're actually throttling the requests",
    "start": "1227419",
    "end": "1232700"
  },
  {
    "text": "from the qubit so if the cubelet had its way or the way that it used to work is you know if cryo was just like",
    "start": "1232700",
    "end": "1239419"
  },
  {
    "text": "immediately after getting the duplicated request hey I'm already working on it I'm gonna return an error to you cubelet",
    "start": "1239419",
    "end": "1245299"
  },
  {
    "text": "then the qubit and cry like cubelet would propagate through the event API like a whole bunch of like names",
    "start": "1245299",
    "end": "1251059"
  },
  {
    "text": "reserved name is reserve because cry would be returning these errors really quickly and Cuba is trying to create this pot as fast as it can",
    "start": "1251059",
    "end": "1258140"
  },
  {
    "text": "what because we know that the Cuba is going to re-request this object you know",
    "start": "1258140",
    "end": "1263960"
  },
  {
    "text": "as long as the air keeps being time out what crowd can do is it can actually throttle the cubelet and wait on in this",
    "start": "1263960",
    "end": "1271400"
  },
  {
    "text": "routine too saying hold on Cuba slow down we're working on it don't worry",
    "start": "1271400",
    "end": "1277039"
  },
  {
    "text": "um taking as long as it can so that reduces the number of events in the Pod API so",
    "start": "1277039",
    "end": "1283100"
  },
  {
    "text": "um that's very nice and reduces churn and makes the admins",
    "start": "1283100",
    "end": "1288679"
  },
  {
    "text": "not as scared because there's not a bajillion messages saying God's name is reserved",
    "start": "1288679",
    "end": "1294020"
  },
  {
    "text": "a new thing that we've done semi recently which I'm also very excited about is we return where the Potter",
    "start": "1294020",
    "end": "1300200"
  },
  {
    "text": "container creation is stuck so we used to just say this kind of generic error like famous Reserve we're working on a",
    "start": "1300200",
    "end": "1306260"
  },
  {
    "text": "cubelet chill out but now we're also keeping track of where in the container and pod creation process that resource",
    "start": "1306260",
    "end": "1313940"
  },
  {
    "text": "is stuck and that way we're able to return so when resource uh when you know",
    "start": "1313940",
    "end": "1319760"
  },
  {
    "text": "one of the one of the crowd routines that isn't the original one returns an errors saying",
    "start": "1319760",
    "end": "1325220"
  },
  {
    "text": "hey you know we're working on it but it's not done yet it also says and it's actually stuck at you know something",
    "start": "1325220",
    "end": "1331220"
  },
  {
    "text": "like currently at stage sandbox Network created so it's saying like you know sometime after the sandbox network was",
    "start": "1331220",
    "end": "1338179"
  },
  {
    "text": "created we're stuck or says something like you know sandbox storage creation so what that error would indicate to me",
    "start": "1338179",
    "end": "1345380"
  },
  {
    "text": "is that there's an iops throttling problem if it said stuck on Sandbox",
    "start": "1345380",
    "end": "1350539"
  },
  {
    "text": "Network creation I would guess that there's a Slowdown the bottleneck in the sdn",
    "start": "1350539",
    "end": "1356600"
  },
  {
    "text": "so basically this Improvement allows an admin to see uh you know the state of",
    "start": "1356600",
    "end": "1363559"
  },
  {
    "text": "the container or pod creation process like why it's taking so long and hopefully make redomediation stuff",
    "start": "1363559",
    "end": "1370520"
  },
  {
    "text": "Southern I'm very excited for it because in the past it took kind of just a lot of intimate knowledge with cryo and the",
    "start": "1370520",
    "end": "1377059"
  },
  {
    "text": "way and the timing of things to be able to actually tell which what's from which I ended up doing a lot of that work you",
    "start": "1377059",
    "end": "1382700"
  },
  {
    "text": "know for the people that I support so I'm excited that it'll be doing that for me",
    "start": "1382700",
    "end": "1389080"
  },
  {
    "text": "and uh next up we have renal talking about some six door stuff folks everyone must",
    "start": "1389840",
    "end": "1396679"
  },
  {
    "text": "have six tours well I'm happy to happy to cryo her support for six tours style signatures",
    "start": "1396679",
    "end": "1402260"
  },
  {
    "text": "so the containers image Library the trial uses for pulling images merge support for verifying those signatures",
    "start": "1402260",
    "end": "1408400"
  },
  {
    "text": "oddman fully supports it uh supportment can be used to sign and push those",
    "start": "1408400",
    "end": "1413840"
  },
  {
    "text": "images and the signatures to a registry and then you can use cryo to verify those signatures when you pull the",
    "start": "1413840",
    "end": "1420440"
  },
  {
    "text": "images however we need to improve the UI of what happens when a signature",
    "start": "1420440",
    "end": "1425480"
  },
  {
    "text": "verification fails so today the CRI present a distinguish between the types",
    "start": "1425480",
    "end": "1431480"
  },
  {
    "text": "of image pull errors so we'll do some option work so the cubic and distinguish and give the right message to the user",
    "start": "1431480",
    "end": "1438440"
  },
  {
    "text": "when signature verification fails instead of just a generic image will fall back",
    "start": "1438440",
    "end": "1444620"
  },
  {
    "text": "then we also did work so that the cryo release binaries are now signed using",
    "start": "1444620",
    "end": "1449780"
  },
  {
    "text": "cosine so you can verify them as well so next I'll cover some upcoming",
    "start": "1449780",
    "end": "1456500"
  },
  {
    "text": "features that intersect cryo and the work we're doing in signode so first of them is username spaces",
    "start": "1456500",
    "end": "1464179"
  },
  {
    "text": "so so far like the like this username is supported in the Linux kernel but",
    "start": "1464179",
    "end": "1469760"
  },
  {
    "text": "kubernetes has not been able to take advantage of it so Peter mentioned that we had annotations based support in cryo",
    "start": "1469760",
    "end": "1476419"
  },
  {
    "text": "but finally in 125 we got phase one alpha for username spaces merged into",
    "start": "1476419",
    "end": "1483080"
  },
  {
    "text": "kubernetes so this phase one supports uh stateless spots it means any pods that don't use",
    "start": "1483080",
    "end": "1490159"
  },
  {
    "text": "persistent volumes will work with username spaces so the supported volume types are like empty the secrets config",
    "start": "1490159",
    "end": "1496940"
  },
  {
    "text": "maps and so on so with username spaces you get an additional",
    "start": "1496940",
    "end": "1502159"
  },
  {
    "text": "layer of security in your parts so you can be root inside your pod while being",
    "start": "1502159",
    "end": "1508179"
  },
  {
    "text": "non-root on the host what it means is if the process is able to break outside of",
    "start": "1508179",
    "end": "1513740"
  },
  {
    "text": "the container it's not able to attack the host or other containers running on the Node so it's very useful and one",
    "start": "1513740",
    "end": "1521360"
  },
  {
    "text": "more Advantage is we are now able to like run any random image for from any registry that runs as",
    "start": "1521360",
    "end": "1529460"
  },
  {
    "text": "root by default and not have how to worry about changing it to be non-root",
    "start": "1529460",
    "end": "1535039"
  },
  {
    "text": "so kernel takes care of it for us so here's a simple example of how you",
    "start": "1535039",
    "end": "1541100"
  },
  {
    "text": "utilize this so you just add that host users equal to false in your pod spec and that will enable cubelet and cryo to",
    "start": "1541100",
    "end": "1548720"
  },
  {
    "text": "enable username spaces and you are in this username space part",
    "start": "1548720",
    "end": "1555020"
  },
  {
    "text": "so next up in this area we'll start adding support for other types of",
    "start": "1555020",
    "end": "1560600"
  },
  {
    "text": "volumes persistent volumes and that's something we need to tackle Upstream first and then make available",
    "start": "1560600",
    "end": "1568179"
  },
  {
    "text": "so checkpoint restore so this is another feature that was merged into the cubelet",
    "start": "1569419",
    "end": "1575059"
  },
  {
    "text": "so basically it uses the criu to checkpoint container state",
    "start": "1575059",
    "end": "1580100"
  },
  {
    "text": "it's only a cubelet API at the moment so there's no kubernetes API for it so if you want to use this feature you have to",
    "start": "1580100",
    "end": "1586279"
  },
  {
    "text": "hop on a node and directly hit the queue the cubelets endpoint to checkpoint the",
    "start": "1586279",
    "end": "1591860"
  },
  {
    "text": "container so the current use case that is being targeted is forensic analysis say you're",
    "start": "1591860",
    "end": "1598159"
  },
  {
    "text": "a bank and there is a bad actor that has uh in that's able to like break out of a",
    "start": "1598159",
    "end": "1605179"
  },
  {
    "text": "pod and it's trying to attack or do something bad on your note so what you can do is you can get on",
    "start": "1605179",
    "end": "1611120"
  },
  {
    "text": "that node and then checkpoint the state of that quad and then you can move that",
    "start": "1611120",
    "end": "1616880"
  },
  {
    "text": "checkpointed state to another node and then you can start that pod back up",
    "start": "1616880",
    "end": "1623179"
  },
  {
    "text": "again so you can analyze what's what was happening inside that ball and this can",
    "start": "1623179",
    "end": "1628520"
  },
  {
    "text": "happen without the knowledge of the attacker like they can continue being in that part",
    "start": "1628520",
    "end": "1633620"
  },
  {
    "text": "so this allows like this is like a security feature so other use cases of",
    "start": "1633620",
    "end": "1639020"
  },
  {
    "text": "checkpoint restore uh like faster startup for parts such as jvm so we know that Java takes time uh to start up",
    "start": "1639020",
    "end": "1646640"
  },
  {
    "text": "right so potentially we can checkpoint a pod after that startup phase is done and",
    "start": "1646640",
    "end": "1654080"
  },
  {
    "text": "then you can launch hundreds of copies of such pods right so that'll be a net Improvement in startup time",
    "start": "1654080",
    "end": "1661279"
  },
  {
    "text": "so this is also early and like we just merged the checkpoint support so next",
    "start": "1661279",
    "end": "1666679"
  },
  {
    "text": "we'll start tackling uh the restore support in the kubernetes API",
    "start": "1666679",
    "end": "1672639"
  },
  {
    "text": "and finally uh invented like so today the cubelet uses the generic plate what",
    "start": "1673279",
    "end": "1678919"
  },
  {
    "text": "is plague so it's a pod life cycle event generator so that's what is being used",
    "start": "1678919",
    "end": "1684740"
  },
  {
    "text": "by the cubelet to materialize the life cycle of a pot so once the pubelet",
    "start": "1684740",
    "end": "1689779"
  },
  {
    "text": "starts a pod it needs to be aware when it dies or it gets killed for some reason because you know whenever it dies",
    "start": "1689779",
    "end": "1696799"
  },
  {
    "text": "cubelet starts it back for you right and how does it know that it knows it",
    "start": "1696799",
    "end": "1702200"
  },
  {
    "text": "through the plague the way the generic flag works is the cubelet periodically realists all the pods and containers",
    "start": "1702200",
    "end": "1709820"
  },
  {
    "text": "from the runtime over CRI now the overhead of doing this is very high when",
    "start": "1709820",
    "end": "1714860"
  },
  {
    "text": "there are lots of faults like imagine you're trying to push the boundaries of how many pods you can run on a road you",
    "start": "1714860",
    "end": "1720200"
  },
  {
    "text": "are at 600 700 nodes and very frequently a cubelet is requesting this list of",
    "start": "1720200",
    "end": "1726200"
  },
  {
    "text": "pods from the runtime so this adds a lot of overhead to both the cubelet and the container runtime",
    "start": "1726200",
    "end": "1732380"
  },
  {
    "text": "so we are working on a feature called as evented plague that moves to a list",
    "start": "1732380",
    "end": "1738380"
  },
  {
    "text": "watch model like a kubernetes operator's work today so with that the cubelet will be able to",
    "start": "1738380",
    "end": "1744500"
  },
  {
    "text": "list the pods less frequently way less than it does now and then rely on events",
    "start": "1744500",
    "end": "1749539"
  },
  {
    "text": "being sent from the container runtime to generate the plague events so this will drastically reduce the",
    "start": "1749539",
    "end": "1756320"
  },
  {
    "text": "overhead of uh cubelet and runtime and hopefully we'll be able to run way more",
    "start": "1756320",
    "end": "1762260"
  },
  {
    "text": "pods than we do today so this work is targeted for 126 there",
    "start": "1762260",
    "end": "1767360"
  },
  {
    "text": "are PR's open and we are hopeful that it will get merged",
    "start": "1767360",
    "end": "1771760"
  },
  {
    "text": "so this brings us to the uh end of the talk thanks for joining us and we are happy to take questions",
    "start": "1772580",
    "end": "1779960"
  },
  {
    "text": "[Applause]",
    "start": "1779960",
    "end": "1785940"
  },
  {
    "text": "you also so in in a tragic event where conman uh dies before the container does",
    "start": "1803120",
    "end": "1809299"
  },
  {
    "text": "then also we don't catch the container's exit because kanman is the parent of the",
    "start": "1809299",
    "end": "1814460"
  },
  {
    "text": "container process so it's the one that can catch the Sig child that the kernel sends when the process ends cryo's not",
    "start": "1814460",
    "end": "1821000"
  },
  {
    "text": "you know at all at common actually demonizes so conman is a child that",
    "start": "1821000",
    "end": "1826100"
  },
  {
    "text": "system B not even of cryo so crowd can't catch those so um you know at one point we had a kanman",
    "start": "1826100",
    "end": "1832640"
  },
  {
    "text": "monitor but um con man we called it um but it was uh",
    "start": "1832640",
    "end": "1837740"
  },
  {
    "text": "racy and kind of difficult to work with so we've uh you know made it we worked",
    "start": "1837740",
    "end": "1842899"
  },
  {
    "text": "hard to make sure that conman isn't going to segfault or anything like that or exit before the container does and it actually can't be boom killed either",
    "start": "1842899",
    "end": "1849500"
  },
  {
    "text": "because that would be in a bad State we hope to we're looking at um situations PID fds would help with",
    "start": "1849500",
    "end": "1856340"
  },
  {
    "text": "that where we can actually you know crowd can keep track of the life cycle of the caught him on or common RS",
    "start": "1856340",
    "end": "1863059"
  },
  {
    "text": "instance if you could also maybe use EPF to catch uh you know kanban exits but we",
    "start": "1863059",
    "end": "1868580"
  },
  {
    "text": "don't currently have anything so the ideal is that that doesn't happen",
    "start": "1868580",
    "end": "1873580"
  },
  {
    "text": "right",
    "start": "1878240",
    "end": "1880720"
  },
  {
    "text": "as of now it does not uh it would I agree that that would be even more",
    "start": "1891140",
    "end": "1896419"
  },
  {
    "text": "problematic of a situation um and so we're you know it is on our radar trying to figure out the way to",
    "start": "1896419",
    "end": "1902960"
  },
  {
    "text": "actually pay attention you know I really look forward to Pitt fds in general konmon could also use them but um you",
    "start": "1902960",
    "end": "1909080"
  },
  {
    "text": "know before that maybe we'll come up with some ebpf thing so that like that risky situation doesn't come up",
    "start": "1909080",
    "end": "1916419"
  },
  {
    "text": "uh because I wanted to have a good resume uh no because um the well because it",
    "start": "1918679",
    "end": "1924860"
  },
  {
    "text": "like was a natural fit for kind of the the piece of the stack it was you know we already had conman NC but it would",
    "start": "1924860",
    "end": "1930500"
  },
  {
    "text": "have been kind of clunky to add you know um asynchrony and you know all of these things and go would be the go run time",
    "start": "1930500",
    "end": "1937820"
  },
  {
    "text": "uses too much memory for the number of con mounts that we want um so we felt like rust was a good fit for that and",
    "start": "1937820",
    "end": "1944720"
  },
  {
    "text": "you know I kind of feel like in general the LA the the layer of the stack that we're at I think we're going to start",
    "start": "1944720",
    "end": "1950480"
  },
  {
    "text": "seeing more projects being written in Rust because it's a pretty natural fit for like the space",
    "start": "1950480",
    "end": "1955820"
  },
  {
    "text": "I think the rust has a right mix of low level and high level so we can do all the low level things we were able to do",
    "start": "1955820",
    "end": "1962120"
  },
  {
    "text": "with C while also having access to like an RPC API like Captain Proto that we",
    "start": "1962120",
    "end": "1967760"
  },
  {
    "text": "can easily integrate while not paying a huge memory or CPU penalty that that was a motivation too",
    "start": "1967760",
    "end": "1976360"
  },
  {
    "text": "okay um we have attracted some uh interest in it um but not a ton of",
    "start": "1980659",
    "end": "1987020"
  },
  {
    "text": "contributors mostly people uh some people are looking at uh you know some issues and stuff but it's you know a",
    "start": "1987020",
    "end": "1993140"
  },
  {
    "text": "little slow going but we're hoping that also the integration of rust into you know this whole ecosystem will appeal to",
    "start": "1993140",
    "end": "1999200"
  },
  {
    "text": "people",
    "start": "1999200",
    "end": "2001380"
  },
  {
    "text": "so what was the motivation for using a cni for setting up cryo and what are",
    "start": "2006220",
    "end": "2012220"
  },
  {
    "text": "your thoughts on this cni conflicting with other netcons uh for say any other CNR networks and",
    "start": "2012220",
    "end": "2018880"
  },
  {
    "text": "I've seen this issue that the cryo cni conflicts a bit so so are you talking",
    "start": "2018880",
    "end": "2025240"
  },
  {
    "text": "about travel shipping or defaults yeah so I think that's just something that we ship so it's easy for you to get started",
    "start": "2025240",
    "end": "2032019"
  },
  {
    "text": "with cryo right so you can easily start up a local appluster and you'll be able to see your parts but that's something",
    "start": "2032019",
    "end": "2037840"
  },
  {
    "text": "we never recommend for production use cases and we make it easy for you to like Drop in any see an icon and the",
    "start": "2037840",
    "end": "2045159"
  },
  {
    "text": "binary and it should just work so that's more like a starter thing to get inside yeah and also actually now",
    "start": "2045159",
    "end": "2051520"
  },
  {
    "text": "the packages that we ship in all of the Upstream distros they don't actually require the container networking plugins",
    "start": "2051520",
    "end": "2058679"
  },
  {
    "text": "or if you're on Fedora you're able to actually install cryo without installing the container networking plugins",
    "start": "2058899",
    "end": "2065020"
  },
  {
    "text": "um so it's it's not required but you know we suggested if you don't have another solution that can come out of",
    "start": "2065020",
    "end": "2071200"
  },
  {
    "text": "the box and uh I've seen that the plug issues I see with kubernetes they are not very",
    "start": "2071200",
    "end": "2077260"
  },
  {
    "text": "descriptive so how would you go about digging in deep in those issues and",
    "start": "2077260",
    "end": "2082480"
  },
  {
    "text": "actually figuring out what's causing the plague error so I think like there are two things right like one thing Peter",
    "start": "2082480",
    "end": "2089260"
  },
  {
    "text": "mentioned is improving what we lock in the container runtimes and the second part of it like improving what we log on",
    "start": "2089260",
    "end": "2096398"
  },
  {
    "text": "the cubelet side of things like the whole cubelet code that manages sync pod",
    "start": "2096399",
    "end": "2101920"
  },
  {
    "text": "and generates all this is is hard and what we are hoping is with events like",
    "start": "2101920",
    "end": "2107500"
  },
  {
    "text": "we have some ideas on how to simplify it and like make it better and also folks",
    "start": "2107500",
    "end": "2112720"
  },
  {
    "text": "on the community folks on Google are also working on some documentation so more people can come up to speed",
    "start": "2112720",
    "end": "2119520"
  },
  {
    "text": "contribute and simplify that whole area yeah the",
    "start": "2119520",
    "end": "2125500"
  },
  {
    "text": "the existence of tracing support into Cuba we also had the idea we were talking about it today",
    "start": "2125500",
    "end": "2131619"
  },
  {
    "text": "like uh improv like instrumenting sync pod and you know or the um the Pod",
    "start": "2131619",
    "end": "2137200"
  },
  {
    "text": "workers that are actually doing the plague um you know generating and managing if",
    "start": "2137200",
    "end": "2143680"
  },
  {
    "text": "we were instrumenting them uh with open Telemetry like maybe that could also give some better insight into what's",
    "start": "2143680",
    "end": "2149859"
  },
  {
    "text": "going on there um you'll be able to watch a span and see Its Behavior so that might help too",
    "start": "2149859",
    "end": "2156520"
  },
  {
    "text": "so right now so cryo currently doesn't have open Telemetry support",
    "start": "2156520",
    "end": "2161560"
  },
  {
    "text": "um it so it does have some preliminary support it like I think we're still",
    "start": "2161560",
    "end": "2167320"
  },
  {
    "text": "working through trying to get like figure out exactly the granularity of spans we have it on you know we have a",
    "start": "2167320",
    "end": "2174099"
  },
  {
    "text": "number of functions that are like emitting them I think that's but it's in like the main branch it's like not yet",
    "start": "2174099",
    "end": "2179320"
  },
  {
    "text": "been released I mean kanman still has only it's only on like",
    "start": "2179320",
    "end": "2184380"
  },
  {
    "text": "0.3.0 or something like that so it's pretty young there could have been a demo that we did",
    "start": "2184380",
    "end": "2191560"
  },
  {
    "text": "today but I didn't have time to put it together where like you actually can't see the spans actually you might be able to find a image I have if I have enough",
    "start": "2191560",
    "end": "2197740"
  },
  {
    "text": "time or send it to you or something of like what that looks like for commentara so we're getting there but we're not quite there yet",
    "start": "2197740",
    "end": "2203740"
  },
  {
    "text": "thank you",
    "start": "2203740",
    "end": "2206280"
  },
  {
    "text": "with the introduction of uh con man RS is there planes to eventually deprecate kanman itself and then replace it yeah",
    "start": "2211599",
    "end": "2218200"
  },
  {
    "text": "yeah I would say long term you know assuming that we can you know make sure",
    "start": "2218200",
    "end": "2224800"
  },
  {
    "text": "that and I'm pretty sure that we could this can be the case but you know I just want to maintain that one of the main ideas is to make sure that the ultimate",
    "start": "2224800",
    "end": "2230859"
  },
  {
    "text": "memory usage is not egregiously more than it was with kanman assuming that",
    "start": "2230859",
    "end": "2236079"
  },
  {
    "text": "which I think will be the case then uh eventually we'll deprecate and remove kanman and it'll be convon RS moving",
    "start": "2236079",
    "end": "2241599"
  },
  {
    "text": "forward and then obviously this will move Upstream into like uh openshift right or Downstream yeah yeah we're yeah we're",
    "start": "2241599",
    "end": "2248680"
  },
  {
    "text": "working on pulling it down as well",
    "start": "2248680",
    "end": "2252480"
  },
  {
    "text": "I wanted to say very impressive work with uh kanman RS and I appreciate the description of why rust might be the",
    "start": "2260800",
    "end": "2267880"
  },
  {
    "text": "good tool for the job um could you show the slide again on how to switch the crio runtime",
    "start": "2267880",
    "end": "2274359"
  },
  {
    "text": "and then maybe elaborate on like what future scenarios people might",
    "start": "2274359",
    "end": "2279940"
  },
  {
    "text": "consider switching to conman RS to be an early adopter yeah so this um this slide is the one",
    "start": "2279940",
    "end": "2286839"
  },
  {
    "text": "you were looking for yeah um so there you go",
    "start": "2286839",
    "end": "2292000"
  },
  {
    "text": "um what so um why you would want to early adopt it so so what are the things that we kind",
    "start": "2292000",
    "end": "2297940"
  },
  {
    "text": "of triggered our desire to do this is we wanted better C group accounting of the",
    "start": "2297940",
    "end": "2303220"
  },
  {
    "text": "kanman resources so like you know in situations where you want to keep the kanman on a separate CPU set than you",
    "start": "2303220",
    "end": "2310240"
  },
  {
    "text": "know the container is running if you have a real-time pod that's doing some Network latency stuff you could you know",
    "start": "2310240",
    "end": "2315820"
  },
  {
    "text": "that was originally what kind of got us started thinking about having a pod level kanman um to be able to isolate",
    "start": "2315820",
    "end": "2321579"
  },
  {
    "text": "that so that's one use case um I mean you know we'd be happy if folks wanted to try out and let us know",
    "start": "2321579",
    "end": "2327160"
  },
  {
    "text": "you know even if you just want to be at the bleeding edge because you know that'll help us iterate on getting it",
    "start": "2327160",
    "end": "2332500"
  },
  {
    "text": "better and stuff I would say why else yeah I mean so one more thing",
    "start": "2332500",
    "end": "2340720"
  },
  {
    "text": "that we wanted to tackle is like right now when we do an exec right we are spawning a new common",
    "start": "2340720",
    "end": "2346180"
  },
  {
    "text": "and its study State on a node your execs are the most expensive operation that a",
    "start": "2346180",
    "end": "2353079"
  },
  {
    "text": "container runtime is doing so we want to reduce that process hop here so with common RS like there won't be an",
    "start": "2353079",
    "end": "2359380"
  },
  {
    "text": "additional common we'll talk over rpc2 kanban and it'll directly do a run C or a c Run exit drastically reducing your",
    "start": "2359380",
    "end": "2366760"
  },
  {
    "text": "CPU usage the the other other thing that I just remembered is actually it can help with CPU and memory accounting on",
    "start": "2366760",
    "end": "2373119"
  },
  {
    "text": "the Node so because we have multiple kanmans now it's hard for us to use the Pod overhead feature which was",
    "start": "2373119",
    "end": "2378640"
  },
  {
    "text": "originally made for Kata because you have like a VM that has some sort of standard amount of memory and CPU but",
    "start": "2378640",
    "end": "2384280"
  },
  {
    "text": "you know if we have a variable number of kanbans we can't really well guess how much memory and CPU they'll all use",
    "start": "2384280",
    "end": "2390400"
  },
  {
    "text": "because it's not proportional to the number of containers with pod a kanban RS it's one per pod we can guess like",
    "start": "2390400",
    "end": "2396640"
  },
  {
    "text": "okay it's not going to use more than eight Megs definitely so you know that's the pot overhead and uh then you can",
    "start": "2396640",
    "end": "2403480"
  },
  {
    "text": "more tightly fit your containers on the Node because you know they're not there's not this like mysterious amount",
    "start": "2403480",
    "end": "2409720"
  },
  {
    "text": "of memory that's used by kanman",
    "start": "2409720",
    "end": "2413160"
  },
  {
    "text": "questions over time well let's get out of here",
    "start": "2415599",
    "end": "2422440"
  },
  {
    "text": "thank you everyone",
    "start": "2422440",
    "end": "2424920"
  }
]