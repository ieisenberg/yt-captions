[
  {
    "text": "hi everyone it's very nice to be back um",
    "start": "719",
    "end": "4000"
  },
  {
    "text": "welcome to our talk Serving the Future",
    "start": "4000",
    "end": "6480"
  },
  {
    "text": "KSER's next chapter hosting LLMs and",
    "start": "6480",
    "end": "9040"
  },
  {
    "text": "Ginai",
    "start": "9040",
    "end": "10280"
  },
  {
    "text": "models my name is Alexa Griffith i am a",
    "start": "10280",
    "end": "13679"
  },
  {
    "text": "senior software engineer at Bloomberg",
    "start": "13679",
    "end": "15440"
  },
  {
    "text": "and I'm working on building our AI",
    "start": "15440",
    "end": "18000"
  },
  {
    "text": "inference",
    "start": "18000",
    "end": "19640"
  },
  {
    "text": "platform and my name is Tessa Fam i'm",
    "start": "19640",
    "end": "22480"
  },
  {
    "text": "also a senior software engineer on the",
    "start": "22480",
    "end": "24880"
  },
  {
    "text": "AI inference team at Bloomberg",
    "start": "24880",
    "end": "29079"
  },
  {
    "text": "today let's come join me and Alexa in",
    "start": "29359",
    "end": "31840"
  },
  {
    "text": "the KCON AI land to explore what new",
    "start": "31840",
    "end": "34239"
  },
  {
    "text": "features Kerf has to offer in the age of",
    "start": "34239",
    "end": "36640"
  },
  {
    "text": "Gen",
    "start": "36640",
    "end": "38960"
  },
  {
    "text": "AI ai is evolving really quickly and Gen",
    "start": "39559",
    "end": "42960"
  },
  {
    "text": "AI is transforming industries from",
    "start": "42960",
    "end": "45680"
  },
  {
    "text": "automating task creating content to",
    "start": "45680",
    "end": "48399"
  },
  {
    "text": "making smarter decisions and as demand",
    "start": "48399",
    "end": "51239"
  },
  {
    "text": "skyrockets companies are also rushing to",
    "start": "51239",
    "end": "54559"
  },
  {
    "text": "adopt AI into their products which drive",
    "start": "54559",
    "end": "57680"
  },
  {
    "text": "the need for even more powerful models",
    "start": "57680",
    "end": "61120"
  },
  {
    "text": "but with that growth comes big",
    "start": "61120",
    "end": "63960"
  },
  {
    "text": "challenges we're facing higher compute",
    "start": "63960",
    "end": "66560"
  },
  {
    "text": "cost scalability struggles and the",
    "start": "66560",
    "end": "69119"
  },
  {
    "text": "constant push to balance performance and",
    "start": "69119",
    "end": "72119"
  },
  {
    "text": "efficiency that is where KSER comes in",
    "start": "72119",
    "end": "75280"
  },
  {
    "text": "queserf simplifies AI deployment by",
    "start": "75280",
    "end": "77360"
  },
  {
    "text": "handling inference autoscaling and",
    "start": "77360",
    "end": "79840"
  },
  {
    "text": "resource management to make it easier",
    "start": "79840",
    "end": "81520"
  },
  {
    "text": "for businesses to scale and get the most",
    "start": "81520",
    "end": "83840"
  },
  {
    "text": "out of generative",
    "start": "83840",
    "end": "86719"
  },
  {
    "text": "models deploying Jai at scale comes with",
    "start": "86920",
    "end": "90560"
  },
  {
    "text": "a whole new level of complexity these",
    "start": "90560",
    "end": "93680"
  },
  {
    "text": "models are getting bigger and more",
    "start": "93680",
    "end": "95840"
  },
  {
    "text": "intricate which means they need",
    "start": "95840",
    "end": "97640"
  },
  {
    "text": "significant compute power and smart",
    "start": "97640",
    "end": "100400"
  },
  {
    "text": "scaling to keep up so the usual",
    "start": "100400",
    "end": "102960"
  },
  {
    "text": "challenges like scalability reliability",
    "start": "102960",
    "end": "105920"
  },
  {
    "text": "and latency become even tougher to deal",
    "start": "105920",
    "end": "108720"
  },
  {
    "text": "with when we're working with these",
    "start": "108720",
    "end": "111040"
  },
  {
    "text": "models um with this size and on top of",
    "start": "111040",
    "end": "114000"
  },
  {
    "text": "that figuring out how to use resources",
    "start": "114000",
    "end": "116799"
  },
  {
    "text": "efficiently without sacrificing",
    "start": "116799",
    "end": "118880"
  },
  {
    "text": "performance is still a major hurdle to",
    "start": "118880",
    "end": "122000"
  },
  {
    "text": "put it simply making Genai run smoothly",
    "start": "122000",
    "end": "124640"
  },
  {
    "text": "at scale takes more than just raw",
    "start": "124640",
    "end": "127040"
  },
  {
    "text": "compute power it takes the right",
    "start": "127040",
    "end": "129360"
  },
  {
    "text": "strategy",
    "start": "129360",
    "end": "131680"
  },
  {
    "text": "so how does KSER come in to solve these",
    "start": "131680",
    "end": "134680"
  },
  {
    "text": "challenges first of all KSERF sets out",
    "start": "134680",
    "end": "137440"
  },
  {
    "text": "to simplify AI model deployment on",
    "start": "137440",
    "end": "139440"
  },
  {
    "text": "Kubernetes by abstracting away from the",
    "start": "139440",
    "end": "142400"
  },
  {
    "text": "complexity of Kubernetes allowing",
    "start": "142400",
    "end": "144640"
  },
  {
    "text": "developers to focus on building models",
    "start": "144640",
    "end": "146720"
  },
  {
    "text": "without having to worry about managing",
    "start": "146720",
    "end": "148640"
  },
  {
    "text": "the infrastructure under the",
    "start": "148640",
    "end": "150520"
  },
  {
    "text": "hood kser supports multiple Genai and",
    "start": "150520",
    "end": "153920"
  },
  {
    "text": "machine learning frameworks making it",
    "start": "153920",
    "end": "155920"
  },
  {
    "text": "versatile and framework agnostic and in",
    "start": "155920",
    "end": "158319"
  },
  {
    "text": "the past year we've also added support",
    "start": "158319",
    "end": "160640"
  },
  {
    "text": "for OpenAI protocol so now it is",
    "start": "160640",
    "end": "163360"
  },
  {
    "text": "compatible with the OpenAI API as",
    "start": "163360",
    "end": "166360"
  },
  {
    "text": "well kser comes with native features to",
    "start": "166360",
    "end": "169599"
  },
  {
    "text": "monitor",
    "start": "169599",
    "end": "170680"
  },
  {
    "text": "performance automatically adjust scaling",
    "start": "170680",
    "end": "173120"
  },
  {
    "text": "based on demand and optimize inference",
    "start": "173120",
    "end": "175360"
  },
  {
    "text": "efficiency and finally it is now",
    "start": "175360",
    "end": "178800"
  },
  {
    "text": "equipped with Gen AI ready features",
    "start": "178800",
    "end": "181200"
  },
  {
    "text": "including LM metricbased autoscaling",
    "start": "181200",
    "end": "184200"
  },
  {
    "text": "motocaching multi-node inference which",
    "start": "184200",
    "end": "186879"
  },
  {
    "text": "will help reduce latency and improve",
    "start": "186879",
    "end": "188879"
  },
  {
    "text": "response times and a couple more",
    "start": "188879",
    "end": "191000"
  },
  {
    "text": "features so with all of these powerful",
    "start": "191000",
    "end": "193599"
  },
  {
    "text": "tools KSER can prove to be an ideal",
    "start": "193599",
    "end": "196000"
  },
  {
    "text": "solution for the next generation of AI",
    "start": "196000",
    "end": "198599"
  },
  {
    "text": "applications if you're new to Kserve",
    "start": "198599",
    "end": "201040"
  },
  {
    "text": "let's take a quick look and see what it",
    "start": "201040",
    "end": "203599"
  },
  {
    "text": "is and how it works",
    "start": "203599",
    "end": "206560"
  },
  {
    "text": "now if you've been working with Queser",
    "start": "206560",
    "end": "208560"
  },
  {
    "text": "for a while this is probably a diagram",
    "start": "208560",
    "end": "211120"
  },
  {
    "text": "that you've seen very often um on a high",
    "start": "211120",
    "end": "214319"
  },
  {
    "text": "level Queser runs on Kubernetes which",
    "start": "214319",
    "end": "216799"
  },
  {
    "text": "acts as an abstract level on top of the",
    "start": "216799",
    "end": "219040"
  },
  {
    "text": "hardware level which is GPUs and CPUs",
    "start": "219040",
    "end": "222480"
  },
  {
    "text": "that users would need to have it's also",
    "start": "222480",
    "end": "225519"
  },
  {
    "text": "built with Kative and STTO which are two",
    "start": "225519",
    "end": "228799"
  },
  {
    "text": "other open source projects to give it",
    "start": "228799",
    "end": "231280"
  },
  {
    "text": "serverless capabilities and a service",
    "start": "231280",
    "end": "233280"
  },
  {
    "text": "mesh layer and as you'll see later in",
    "start": "233280",
    "end": "236720"
  },
  {
    "text": "this talk what's great about Kserve is",
    "start": "236720",
    "end": "239200"
  },
  {
    "text": "how easily it is to integrate with other",
    "start": "239200",
    "end": "241680"
  },
  {
    "text": "open source tools to make it a powerful",
    "start": "241680",
    "end": "244080"
  },
  {
    "text": "and adaptable",
    "start": "244080",
    "end": "245959"
  },
  {
    "text": "solution kser serves as a standard model",
    "start": "245959",
    "end": "249439"
  },
  {
    "text": "inference platform and provides a",
    "start": "249439",
    "end": "251280"
  },
  {
    "text": "Kubernetes custom resource definition",
    "start": "251280",
    "end": "253680"
  },
  {
    "text": "called the infant service for serving",
    "start": "253680",
    "end": "256239"
  },
  {
    "text": "predictive and generative machine",
    "start": "256239",
    "end": "258079"
  },
  {
    "text": "learning models and thanks to it",
    "start": "258079",
    "end": "260239"
  },
  {
    "text": "serverless design Quesov works across",
    "start": "260239",
    "end": "262479"
  },
  {
    "text": "multiple ML frameworks to provide a",
    "start": "262479",
    "end": "264800"
  },
  {
    "text": "streamlined high performance inference",
    "start": "264800",
    "end": "268040"
  },
  {
    "text": "protocol and this whole architecture",
    "start": "268040",
    "end": "270960"
  },
  {
    "text": "works together to support pre and",
    "start": "270960",
    "end": "273880"
  },
  {
    "text": "post-processing inference monitoring",
    "start": "273880",
    "end": "276720"
  },
  {
    "text": "explanability and even advanced",
    "start": "276720",
    "end": "278720"
  },
  {
    "text": "deployments like canary rollouts",
    "start": "278720",
    "end": "280800"
  },
  {
    "text": "pipelines and model ensembles with",
    "start": "280800",
    "end": "282720"
  },
  {
    "text": "inference graph",
    "start": "282720",
    "end": "285759"
  },
  {
    "text": "so for a long time KSERF has been the",
    "start": "285759",
    "end": "288160"
  },
  {
    "text": "go-to open source solution for",
    "start": "288160",
    "end": "289919"
  },
  {
    "text": "predictive model serving with features",
    "start": "289919",
    "end": "292240"
  },
  {
    "text": "dedicated specifically to predictive",
    "start": "292240",
    "end": "294400"
  },
  {
    "text": "inference needs such as autoscaling",
    "start": "294400",
    "end": "297199"
  },
  {
    "text": "based on requests um including scale to",
    "start": "297199",
    "end": "300639"
  },
  {
    "text": "zero on CPU and GPU there's request",
    "start": "300639",
    "end": "304240"
  },
  {
    "text": "batching requests and response logging",
    "start": "304240",
    "end": "307360"
  },
  {
    "text": "security with a n.z you have basic",
    "start": "307360",
    "end": "310240"
  },
  {
    "text": "traffic uh basic traffic management and",
    "start": "310240",
    "end": "312960"
  },
  {
    "text": "also out of the box observability with",
    "start": "312960",
    "end": "315520"
  },
  {
    "text": "metrics and distributed",
    "start": "315520",
    "end": "318440"
  },
  {
    "text": "tracing however running generative",
    "start": "318440",
    "end": "321600"
  },
  {
    "text": "models is way more demanding than",
    "start": "321600",
    "end": "324000"
  },
  {
    "text": "traditional predictive inference mainly",
    "start": "324000",
    "end": "326479"
  },
  {
    "text": "because of the massive size of the",
    "start": "326479",
    "end": "328800"
  },
  {
    "text": "models and their complexity",
    "start": "328800",
    "end": "331440"
  },
  {
    "text": "large language models for example need",
    "start": "331440",
    "end": "333919"
  },
  {
    "text": "high power GPUs TPUs or even specialized",
    "start": "333919",
    "end": "337680"
  },
  {
    "text": "AI accelerators which makes it critical",
    "start": "337680",
    "end": "340240"
  },
  {
    "text": "to efficiently allocate compute",
    "start": "340240",
    "end": "342240"
  },
  {
    "text": "resources in order to balance out cost",
    "start": "342240",
    "end": "344880"
  },
  {
    "text": "and performance on top of that these",
    "start": "344880",
    "end": "348240"
  },
  {
    "text": "models must scale dynamically which",
    "start": "348240",
    "end": "351039"
  },
  {
    "text": "means that they must be able to handle",
    "start": "351039",
    "end": "352800"
  },
  {
    "text": "traffic spikes without overprovisioning",
    "start": "352800",
    "end": "355759"
  },
  {
    "text": "and adjust resources as demand",
    "start": "355759",
    "end": "357919"
  },
  {
    "text": "fluctuates",
    "start": "357919",
    "end": "359840"
  },
  {
    "text": "another big challenge is latency",
    "start": "359840",
    "end": "362560"
  },
  {
    "text": "especially for real-time applications",
    "start": "362560",
    "end": "364400"
  },
  {
    "text": "like chat bots where we need almost",
    "start": "364400",
    "end": "366800"
  },
  {
    "text": "instant responses and that is why we",
    "start": "366800",
    "end": "369360"
  },
  {
    "text": "need to optimize model loading caching",
    "start": "369360",
    "end": "372639"
  },
  {
    "text": "and request",
    "start": "372639",
    "end": "373960"
  },
  {
    "text": "routing and with some models now",
    "start": "373960",
    "end": "376720"
  },
  {
    "text": "reaching terabytes in scale storing and",
    "start": "376720",
    "end": "379360"
  },
  {
    "text": "loading them efficiently is also a",
    "start": "379360",
    "end": "381360"
  },
  {
    "text": "priority",
    "start": "381360",
    "end": "382880"
  },
  {
    "text": "and finally engineering teams often need",
    "start": "382880",
    "end": "385840"
  },
  {
    "text": "to manage multiple LLMs at once which",
    "start": "385840",
    "end": "388240"
  },
  {
    "text": "means unified model management is also",
    "start": "388240",
    "end": "390639"
  },
  {
    "text": "key to keep everything running smoothly",
    "start": "390639",
    "end": "393360"
  },
  {
    "text": "on your",
    "start": "393360",
    "end": "394680"
  },
  {
    "text": "platform so if you already have your",
    "start": "394680",
    "end": "397600"
  },
  {
    "text": "platform running on Kserve or looking to",
    "start": "397600",
    "end": "400000"
  },
  {
    "text": "integrate Kserve into your stack here",
    "start": "400000",
    "end": "402400"
  },
  {
    "text": "are some new features you will get from",
    "start": "402400",
    "end": "404319"
  },
  {
    "text": "KSER to support your genai model",
    "start": "404319",
    "end": "406479"
  },
  {
    "text": "deployment",
    "start": "406479",
    "end": "409280"
  },
  {
    "text": "first of all we have built-in",
    "start": "409280",
    "end": "411039"
  },
  {
    "text": "metricbased",
    "start": "411039",
    "end": "412199"
  },
  {
    "text": "autoscaling so instead of just",
    "start": "412199",
    "end": "414440"
  },
  {
    "text": "autoscaling based on requests each pause",
    "start": "414440",
    "end": "417520"
  },
  {
    "text": "capacity can now be determined using an",
    "start": "417520",
    "end": "420400"
  },
  {
    "text": "LM metric like token throughput or first",
    "start": "420400",
    "end": "423280"
  },
  {
    "text": "time uh or time to first",
    "start": "423280",
    "end": "425479"
  },
  {
    "text": "token we have implemented model caching",
    "start": "425479",
    "end": "429039"
  },
  {
    "text": "which utilizes local persistent volumes",
    "start": "429039",
    "end": "431919"
  },
  {
    "text": "to reduce download time of storage",
    "start": "431919",
    "end": "434919"
  },
  {
    "text": "initializer and if you're using Quaser",
    "start": "434919",
    "end": "437520"
  },
  {
    "text": "with VLM you can also get multi-node",
    "start": "437520",
    "end": "440479"
  },
  {
    "text": "inference and prom caching out of the",
    "start": "440479",
    "end": "443080"
  },
  {
    "text": "box we've also added OpenAI protocol",
    "start": "443080",
    "end": "446360"
  },
  {
    "text": "compatibility as mentioned before which",
    "start": "446360",
    "end": "449520"
  },
  {
    "text": "supports chat completion and embedding",
    "start": "449520",
    "end": "451880"
  },
  {
    "text": "task kerve's observability observability",
    "start": "451880",
    "end": "455360"
  },
  {
    "text": "is also enhanced with LM metrics like",
    "start": "455360",
    "end": "458560"
  },
  {
    "text": "again time to first token token",
    "start": "458560",
    "end": "460720"
  },
  {
    "text": "throughput etc and now the the tool can",
    "start": "460720",
    "end": "464800"
  },
  {
    "text": "be integrated with envoy gateway to",
    "start": "464800",
    "end": "467360"
  },
  {
    "text": "manage traffic more efficiently in",
    "start": "467360",
    "end": "469520"
  },
  {
    "text": "hybrid use cases where external models",
    "start": "469520",
    "end": "472319"
  },
  {
    "text": "are used alongside with self-hosted",
    "start": "472319",
    "end": "474840"
  },
  {
    "text": "models so with these added features Kerf",
    "start": "474840",
    "end": "478560"
  },
  {
    "text": "now supports both traditional and",
    "start": "478560",
    "end": "480720"
  },
  {
    "text": "generative inference",
    "start": "480720",
    "end": "484360"
  },
  {
    "text": "this is the full architecture of KER for",
    "start": "485840",
    "end": "490400"
  },
  {
    "text": "the age of Gen AI on the control plane",
    "start": "490400",
    "end": "493759"
  },
  {
    "text": "you see that we've added a model cache",
    "start": "493759",
    "end": "496240"
  },
  {
    "text": "controller and the Kada external scaler",
    "start": "496240",
    "end": "499599"
  },
  {
    "text": "and on the data plane there's optional",
    "start": "499599",
    "end": "502000"
  },
  {
    "text": "integration with Envoy Gateway to",
    "start": "502000",
    "end": "505199"
  },
  {
    "text": "enhance traffic routing as mentioned",
    "start": "505199",
    "end": "507360"
  },
  {
    "text": "before we've also introduced OpenAI",
    "start": "507360",
    "end": "510360"
  },
  {
    "text": "compatibility LM rate limiting and",
    "start": "510360",
    "end": "512880"
  },
  {
    "text": "intelligent model routing to optimize",
    "start": "512880",
    "end": "515560"
  },
  {
    "text": "performance now we'll dive into some of",
    "start": "515560",
    "end": "518320"
  },
  {
    "text": "the key features and see how they all",
    "start": "518320",
    "end": "520560"
  },
  {
    "text": "come together um and fit into this whole",
    "start": "520560",
    "end": "524920"
  },
  {
    "text": "architecture the first feature we want",
    "start": "524920",
    "end": "527279"
  },
  {
    "text": "to talk about is model",
    "start": "527279",
    "end": "529240"
  },
  {
    "text": "caching model sizes are growing",
    "start": "529240",
    "end": "532000"
  },
  {
    "text": "exponentially they used to just be a few",
    "start": "532000",
    "end": "534880"
  },
  {
    "text": "gigabytes but now they're reaching to",
    "start": "534880",
    "end": "536959"
  },
  {
    "text": "hundreds of gigabytes or even",
    "start": "536959",
    "end": "539320"
  },
  {
    "text": "terabytes that means downloading and",
    "start": "539320",
    "end": "541920"
  },
  {
    "text": "starting a model takes a lot of time and",
    "start": "541920",
    "end": "544880"
  },
  {
    "text": "when you're scaling up every second",
    "start": "544880",
    "end": "546560"
  },
  {
    "text": "counts so the faster model can load the",
    "start": "546560",
    "end": "549600"
  },
  {
    "text": "quicker your infant service can scale",
    "start": "549600",
    "end": "551839"
  },
  {
    "text": "and recover when needed that's why Kerf",
    "start": "551839",
    "end": "554800"
  },
  {
    "text": "has introduced a solution to cache",
    "start": "554800",
    "end": "557120"
  },
  {
    "text": "models locally which will cut down",
    "start": "557120",
    "end": "559200"
  },
  {
    "text": "startup times and make scaling and",
    "start": "559200",
    "end": "561360"
  },
  {
    "text": "autoscaling much more",
    "start": "561360",
    "end": "564560"
  },
  {
    "text": "efficient with Kserve local model cache",
    "start": "565240",
    "end": "568320"
  },
  {
    "text": "you simply can specify where your model",
    "start": "568320",
    "end": "570640"
  },
  {
    "text": "is stored and which node groups you",
    "start": "570640",
    "end": "572880"
  },
  {
    "text": "should cache it this cache works as a",
    "start": "572880",
    "end": "576160"
  },
  {
    "text": "Kubernetes custom resource so this whole",
    "start": "576160",
    "end": "578720"
  },
  {
    "text": "local model local model cache is a CRD",
    "start": "578720",
    "end": "582560"
  },
  {
    "text": "so when you create the custom resource",
    "start": "582560",
    "end": "584720"
  },
  {
    "text": "Quaser will be able to handle the rest",
    "start": "584720",
    "end": "587600"
  },
  {
    "text": "the model cache controller as you've",
    "start": "587600",
    "end": "589360"
  },
  {
    "text": "seen in the diagram before it will",
    "start": "589360",
    "end": "591600"
  },
  {
    "text": "launch download jobs to pull the model",
    "start": "591600",
    "end": "594800"
  },
  {
    "text": "from the model registry onto each node",
    "start": "594800",
    "end": "598320"
  },
  {
    "text": "and then it will set up persistent",
    "start": "598320",
    "end": "600120"
  },
  {
    "text": "volumes and attach them to inference",
    "start": "600120",
    "end": "602560"
  },
  {
    "text": "pots using",
    "start": "602560",
    "end": "604360"
  },
  {
    "text": "PVCs and a cherry on top is that the",
    "start": "604360",
    "end": "607920"
  },
  {
    "text": "model cache mechanism is self-healing",
    "start": "607920",
    "end": "611360"
  },
  {
    "text": "so you see in this diagram on each node",
    "start": "611360",
    "end": "614800"
  },
  {
    "text": "you have a model node agent and this",
    "start": "614800",
    "end": "618000"
  },
  {
    "text": "agent is present on every node and it",
    "start": "618000",
    "end": "621360"
  },
  {
    "text": "continuously cycles through all the",
    "start": "621360",
    "end": "623040"
  },
  {
    "text": "model caches and when it detects a",
    "start": "623040",
    "end": "625680"
  },
  {
    "text": "missing model it will automatically try",
    "start": "625680",
    "end": "628000"
  },
  {
    "text": "to redownload it from the model registry",
    "start": "628000",
    "end": "631040"
  },
  {
    "text": "so if the cache is wiped out for",
    "start": "631040",
    "end": "633120"
  },
  {
    "text": "whatever reason this agent will be able",
    "start": "633120",
    "end": "634800"
  },
  {
    "text": "to recover it for you",
    "start": "634800",
    "end": "638680"
  },
  {
    "text": "this entire integration is seamless",
    "start": "639120",
    "end": "642399"
  },
  {
    "text": "because KSERF can now detect local PVs",
    "start": "642399",
    "end": "645120"
  },
  {
    "text": "automatically so there is really no",
    "start": "645120",
    "end": "647360"
  },
  {
    "text": "extra work for users except to create",
    "start": "647360",
    "end": "649600"
  },
  {
    "text": "just a couple",
    "start": "649600",
    "end": "652480"
  },
  {
    "text": "CRS so this is what a YAML for a model",
    "start": "652920",
    "end": "657360"
  },
  {
    "text": "cache a local model cache CR looks like",
    "start": "657360",
    "end": "660800"
  },
  {
    "text": "um as you can see it's pretty concise",
    "start": "660800",
    "end": "663040"
  },
  {
    "text": "you just specify the name of the infant",
    "start": "663040",
    "end": "664959"
  },
  {
    "text": "service the spec is um you need to",
    "start": "664959",
    "end": "667920"
  },
  {
    "text": "specify the model size and the node",
    "start": "667920",
    "end": "669839"
  },
  {
    "text": "groups that this model should be cached",
    "start": "669839",
    "end": "671760"
  },
  {
    "text": "on and finally the model storage URI so",
    "start": "671760",
    "end": "675839"
  },
  {
    "text": "where it c it can be downloaded",
    "start": "675839",
    "end": "679000"
  },
  {
    "text": "from once this CR is created and then",
    "start": "679000",
    "end": "682399"
  },
  {
    "text": "the controller can detect the CR and it",
    "start": "682399",
    "end": "685839"
  },
  {
    "text": "will spin up download jobs and try to",
    "start": "685839",
    "end": "688320"
  },
  {
    "text": "download the model onto these nodes the",
    "start": "688320",
    "end": "690640"
  },
  {
    "text": "CR will be updated with a status and",
    "start": "690640",
    "end": "692640"
  },
  {
    "text": "this status indicates that the model has",
    "start": "692640",
    "end": "696320"
  },
  {
    "text": "been downloaded into four nodes so there",
    "start": "696320",
    "end": "699519"
  },
  {
    "text": "are four copies the name of the infant",
    "start": "699519",
    "end": "702680"
  },
  {
    "text": "service and what is the status of the",
    "start": "702680",
    "end": "705519"
  },
  {
    "text": "model cache on each",
    "start": "705519",
    "end": "708880"
  },
  {
    "text": "node now that we have model cache um",
    "start": "710040",
    "end": "714240"
  },
  {
    "text": "let's talk about autoscaling autoscaling",
    "start": "714240",
    "end": "716880"
  },
  {
    "text": "we already have autoscaling requestbased",
    "start": "716880",
    "end": "719079"
  },
  {
    "text": "autoscaling on KSERF but this is another",
    "start": "719079",
    "end": "722560"
  },
  {
    "text": "feature that has been enhanced to better",
    "start": "722560",
    "end": "724640"
  },
  {
    "text": "support generative model serving now",
    "start": "724640",
    "end": "727519"
  },
  {
    "text": "your infant service can scale",
    "start": "727519",
    "end": "729200"
  },
  {
    "text": "automatically based on a LML a LM um",
    "start": "729200",
    "end": "734720"
  },
  {
    "text": "specific metric like Q size KV cache",
    "start": "734720",
    "end": "737760"
  },
  {
    "text": "size or token throughput or you can also",
    "start": "737760",
    "end": "740079"
  },
  {
    "text": "specify a combination of these metrics",
    "start": "740079",
    "end": "743360"
  },
  {
    "text": "without giving too much away into the",
    "start": "743360",
    "end": "745360"
  },
  {
    "text": "implementation details because there is",
    "start": "745360",
    "end": "747519"
  },
  {
    "text": "a deep dive talk later on Thursday to",
    "start": "747519",
    "end": "750639"
  },
  {
    "text": "talk about how autoscaling is",
    "start": "750639",
    "end": "752800"
  },
  {
    "text": "implemented in Kserve um this is just a",
    "start": "752800",
    "end": "756079"
  },
  {
    "text": "high level how you can add autoscaling",
    "start": "756079",
    "end": "758399"
  },
  {
    "text": "into your service so here on the right",
    "start": "758399",
    "end": "761519"
  },
  {
    "text": "side you'll see uh the YAML or the CR",
    "start": "761519",
    "end": "764720"
  },
  {
    "text": "for the infant service and the extra",
    "start": "764720",
    "end": "767440"
  },
  {
    "text": "part that you need to specify is the",
    "start": "767440",
    "end": "771560"
  },
  {
    "text": "autoscaling part specifying the metric",
    "start": "771560",
    "end": "775360"
  },
  {
    "text": "um to trigger the autoscaling so for",
    "start": "775360",
    "end": "777440"
  },
  {
    "text": "example in this YAML uh we want to pull",
    "start": "777440",
    "end": "781120"
  },
  {
    "text": "the metric from open telemetry and we",
    "start": "781120",
    "end": "785279"
  },
  {
    "text": "want to uh sum up the GPU cache usage",
    "start": "785279",
    "end": "788079"
  },
  {
    "text": "percentage and take the average of it",
    "start": "788079",
    "end": "790560"
  },
  {
    "text": "over time and the time here is set to 60",
    "start": "790560",
    "end": "793839"
  },
  {
    "text": "seconds fixed and you specify the target",
    "start": "793839",
    "end": "797440"
  },
  {
    "text": "so once this number hits that target the",
    "start": "797440",
    "end": "800560"
  },
  {
    "text": "autoscaling will be triggered",
    "start": "800560",
    "end": "803470"
  },
  {
    "text": "[Music]",
    "start": "803470",
    "end": "805040"
  },
  {
    "text": "um and another thing is KSER's",
    "start": "805040",
    "end": "808240"
  },
  {
    "text": "autoscaling is highly responsive because",
    "start": "808240",
    "end": "810639"
  },
  {
    "text": "real-time metrics are now being pushed",
    "start": "810639",
    "end": "812639"
  },
  {
    "text": "directly to the autoscaler for faster",
    "start": "812639",
    "end": "814720"
  },
  {
    "text": "scaling",
    "start": "814720",
    "end": "817120"
  },
  {
    "text": "decisions okay so uh for predictive",
    "start": "818200",
    "end": "821519"
  },
  {
    "text": "inference we had a unified API in Kserve",
    "start": "821519",
    "end": "824560"
  },
  {
    "text": "we still have that but for generative",
    "start": "824560",
    "end": "827120"
  },
  {
    "text": "inference we added some open AI protocol",
    "start": "827120",
    "end": "829760"
  },
  {
    "text": "support as well so right now we support",
    "start": "829760",
    "end": "832880"
  },
  {
    "text": "three different endpoints the",
    "start": "832880",
    "end": "834680"
  },
  {
    "text": "completions chat completions and",
    "start": "834680",
    "end": "836800"
  },
  {
    "text": "embeddings endpoint we plan to add more",
    "start": "836800",
    "end": "840000"
  },
  {
    "text": "uh support as well",
    "start": "840000",
    "end": "841880"
  },
  {
    "text": "later let's talk a bit about multi-node",
    "start": "841880",
    "end": "844320"
  },
  {
    "text": "inference of VLM so for an H100 node for",
    "start": "844320",
    "end": "847519"
  },
  {
    "text": "example with eight GPUs let's say we",
    "start": "847519",
    "end": "849920"
  },
  {
    "text": "have 80 gigs of memory uh each so we",
    "start": "849920",
    "end": "852959"
  },
  {
    "text": "have around 650 gigabytes of memory",
    "start": "852959",
    "end": "855360"
  },
  {
    "text": "available this mean that this means that",
    "start": "855360",
    "end": "857519"
  },
  {
    "text": "the deepseek model that we showed in an",
    "start": "857519",
    "end": "859199"
  },
  {
    "text": "earlier slide that's going to reach",
    "start": "859199",
    "end": "860959"
  },
  {
    "text": "around one terabyte exceeds this memory",
    "start": "860959",
    "end": "863600"
  },
  {
    "text": "capacity for a single node so somehow we",
    "start": "863600",
    "end": "866320"
  },
  {
    "text": "need to support the ability to uh run",
    "start": "866320",
    "end": "869120"
  },
  {
    "text": "multi-node inference right so we do this",
    "start": "869120",
    "end": "871680"
  },
  {
    "text": "using uh ray cluster with VLM offering",
    "start": "871680",
    "end": "874560"
  },
  {
    "text": "both tensor and pipeline parallelism to",
    "start": "874560",
    "end": "876959"
  },
  {
    "text": "serve our multi-node inference feature",
    "start": "876959",
    "end": "879279"
  },
  {
    "text": "so basically each inference service",
    "start": "879279",
    "end": "881680"
  },
  {
    "text": "becomes like a super pod with uh which",
    "start": "881680",
    "end": "884160"
  },
  {
    "text": "manages array cluster fleet which has",
    "start": "884160",
    "end": "887199"
  },
  {
    "text": "can have a head and worker nodes um for",
    "start": "887199",
    "end": "889680"
  },
  {
    "text": "running this distributed",
    "start": "889680",
    "end": "892680"
  },
  {
    "text": "inference let's talk about another",
    "start": "892680",
    "end": "894639"
  },
  {
    "text": "feature that we add to enhance genai",
    "start": "894639",
    "end": "897120"
  },
  {
    "text": "workloads which is prompt caching so",
    "start": "897120",
    "end": "899920"
  },
  {
    "text": "again a single ho a single pod can't",
    "start": "899920",
    "end": "902240"
  },
  {
    "text": "hold uh all of the cache because it's",
    "start": "902240",
    "end": "904560"
  },
  {
    "text": "large so since our GPU memory is limited",
    "start": "904560",
    "end": "907839"
  },
  {
    "text": "uh we need to externalize this cache so",
    "start": "907839",
    "end": "910480"
  },
  {
    "text": "VLM which is as you all may know a high",
    "start": "910480",
    "end": "913120"
  },
  {
    "text": "performance inference engine that's uh",
    "start": "913120",
    "end": "915279"
  },
  {
    "text": "optimized for running large language",
    "start": "915279",
    "end": "916959"
  },
  {
    "text": "models uh supports prompt caching out of",
    "start": "916959",
    "end": "919760"
  },
  {
    "text": "the box with uh via KV cache so",
    "start": "919760",
    "end": "922800"
  },
  {
    "text": "basically KV cache stores key value",
    "start": "922800",
    "end": "925199"
  },
  {
    "text": "matrices from previous steps uh in a",
    "start": "925199",
    "end": "928000"
  },
  {
    "text": "transformer model to speed up our",
    "start": "928000",
    "end": "930560"
  },
  {
    "text": "inference by reusing computations",
    "start": "930560",
    "end": "934240"
  },
  {
    "text": "however KVach has some challenges with",
    "start": "934240",
    "end": "936800"
  },
  {
    "text": "growth and as we can see here um with",
    "start": "936800",
    "end": "939680"
  },
  {
    "text": "just a thousand or 10,000 tokens we have",
    "start": "939680",
    "end": "942800"
  },
  {
    "text": "16 gigs in the KV cache and as you can",
    "start": "942800",
    "end": "945519"
  },
  {
    "text": "see uh as you add more zeros it grows",
    "start": "945519",
    "end": "948079"
  },
  {
    "text": "exponentially as well so uh depending on",
    "start": "948079",
    "end": "951199"
  },
  {
    "text": "the sequence KV cache can grow to be",
    "start": "951199",
    "end": "954240"
  },
  {
    "text": "pretty large so we need to add another",
    "start": "954240",
    "end": "956560"
  },
  {
    "text": "solution um so looking forward this is",
    "start": "956560",
    "end": "958399"
  },
  {
    "text": "what we are planning that we're in",
    "start": "958399",
    "end": "960160"
  },
  {
    "text": "progress implementing uh is LM cache",
    "start": "960160",
    "end": "962800"
  },
  {
    "text": "with KV cache so what is LM cache lm",
    "start": "962800",
    "end": "965360"
  },
  {
    "text": "cache is an open- source system designed",
    "start": "965360",
    "end": "967440"
  },
  {
    "text": "to manage uh your KV cache efficiently",
    "start": "967440",
    "end": "969920"
  },
  {
    "text": "so this makes KV cache more scalable uh",
    "start": "969920",
    "end": "972639"
  },
  {
    "text": "by caching common input prefixes helping",
    "start": "972639",
    "end": "975759"
  },
  {
    "text": "to reduce these redundant computation",
    "start": "975759",
    "end": "977839"
  },
  {
    "text": "and to speed up uh repeated queries so",
    "start": "977839",
    "end": "980480"
  },
  {
    "text": "like things like document processing uh",
    "start": "980480",
    "end": "982880"
  },
  {
    "text": "chat history so this what this does is",
    "start": "982880",
    "end": "985440"
  },
  {
    "text": "it makes uh the access and storage of",
    "start": "985440",
    "end": "987440"
  },
  {
    "text": "caches faster especially for e these",
    "start": "987440",
    "end": "990480"
  },
  {
    "text": "things like long documents and",
    "start": "990480",
    "end": "992240"
  },
  {
    "text": "multi-turn conversations um what's great",
    "start": "992240",
    "end": "994800"
  },
  {
    "text": "about LM cache is that you can share and",
    "start": "994800",
    "end": "996880"
  },
  {
    "text": "store across multiple VLM instances you",
    "start": "996880",
    "end": "999839"
  },
  {
    "text": "can route queries to the instances that",
    "start": "999839",
    "end": "1001600"
  },
  {
    "text": "already hold the relative context and",
    "start": "1001600",
    "end": "1004000"
  },
  {
    "text": "the KV cache um this is shown to be able",
    "start": "1004000",
    "end": "1006880"
  },
  {
    "text": "to reduce time to first token by around",
    "start": "1006880",
    "end": "1009759"
  },
  {
    "text": "three to 10 times and save on the GPU",
    "start": "1009759",
    "end": "1012800"
  },
  {
    "text": "cycle",
    "start": "1012800",
    "end": "1014040"
  },
  {
    "text": "reduction so this is a very basic",
    "start": "1014040",
    "end": "1016880"
  },
  {
    "text": "architecture of using our LM workloads",
    "start": "1016880",
    "end": "1019600"
  },
  {
    "text": "with Kserve as you can see we have a",
    "start": "1019600",
    "end": "1021600"
  },
  {
    "text": "client application that hits some load",
    "start": "1021600",
    "end": "1023040"
  },
  {
    "text": "balancer maybe we have just some generic",
    "start": "1023040",
    "end": "1025438"
  },
  {
    "text": "gateway let's say envoy gateway and in",
    "start": "1025439",
    "end": "1028000"
  },
  {
    "text": "our inference cluster we're running some",
    "start": "1028000",
    "end": "1029438"
  },
  {
    "text": "self-hosted models say like llama",
    "start": "1029439",
    "end": "1032079"
  },
  {
    "text": "mistral like any self-hosted models so",
    "start": "1032079",
    "end": "1035120"
  },
  {
    "text": "let's talk about how we could improve",
    "start": "1035120",
    "end": "1037280"
  },
  {
    "text": "this basic uh architecture well the",
    "start": "1037280",
    "end": "1040400"
  },
  {
    "text": "first step we can do is implement the",
    "start": "1040400",
    "end": "1042400"
  },
  {
    "text": "envoy AI gateway with Kserve so what",
    "start": "1042400",
    "end": "1045120"
  },
  {
    "text": "does Envoy AI gateway uh give us it",
    "start": "1045120",
    "end": "1048319"
  },
  {
    "text": "gives us a seamless AI integration so it",
    "start": "1048319",
    "end": "1051120"
  },
  {
    "text": "enables hybrid cloud model serving very",
    "start": "1051120",
    "end": "1053200"
  },
  {
    "text": "easily we have this resilient routing",
    "start": "1053200",
    "end": "1055280"
  },
  {
    "text": "that ensures that we have high",
    "start": "1055280",
    "end": "1056799"
  },
  {
    "text": "availability smart traffic traffic",
    "start": "1056799",
    "end": "1058880"
  },
  {
    "text": "management to our models um also we have",
    "start": "1058880",
    "end": "1061440"
  },
  {
    "text": "tokenbased rate limiting so we have",
    "start": "1061440",
    "end": "1063120"
  },
  {
    "text": "tokenbased autoscaling in Kserve but",
    "start": "1063120",
    "end": "1065520"
  },
  {
    "text": "from the envoy AI gate gateway",
    "start": "1065520",
    "end": "1067200"
  },
  {
    "text": "perspective we can also implement um",
    "start": "1067200",
    "end": "1069280"
  },
  {
    "text": "tokenbased rate limiting rather than",
    "start": "1069280",
    "end": "1071280"
  },
  {
    "text": "just the normal request weight limiting",
    "start": "1071280",
    "end": "1073600"
  },
  {
    "text": "so this controls our request flow giving",
    "start": "1073600",
    "end": "1075679"
  },
  {
    "text": "us again optimized performance um again",
    "start": "1075679",
    "end": "1078080"
  },
  {
    "text": "we have enhanced observability and",
    "start": "1078080",
    "end": "1079679"
  },
  {
    "text": "metrics on this layer so standardized",
    "start": "1079679",
    "end": "1082400"
  },
  {
    "text": "logging monitoring reporting like cost",
    "start": "1082400",
    "end": "1084559"
  },
  {
    "text": "reporting with tokens um and now we can",
    "start": "1084559",
    "end": "1087280"
  },
  {
    "text": "more easily treat our LLM as a service",
    "start": "1087280",
    "end": "1089600"
  },
  {
    "text": "so we can deploy and scale our LMS very",
    "start": "1089600",
    "end": "1092080"
  },
  {
    "text": "easily on CL across cloud and prem",
    "start": "1092080",
    "end": "1094720"
  },
  {
    "text": "because we have this centralized and",
    "start": "1094720",
    "end": "1096880"
  },
  {
    "text": "unified API and authentication um for LM",
    "start": "1096880",
    "end": "1101039"
  },
  {
    "text": "so no matter what LM provider you're",
    "start": "1101039",
    "end": "1102799"
  },
  {
    "text": "using the access pattern is going to",
    "start": "1102799",
    "end": "1104640"
  },
  {
    "text": "look the same",
    "start": "1104640",
    "end": "1107840"
  },
  {
    "text": "so let's again look at if we implement",
    "start": "1107840",
    "end": "1110160"
  },
  {
    "text": "the envoy AI gateway um what our",
    "start": "1110160",
    "end": "1112400"
  },
  {
    "text": "architecture would look like so this is",
    "start": "1112400",
    "end": "1114880"
  },
  {
    "text": "something an architecture that uh that",
    "start": "1114880",
    "end": "1117679"
  },
  {
    "text": "we run as well so we have our AI",
    "start": "1117679",
    "end": "1120000"
  },
  {
    "text": "workloads that are you might have some",
    "start": "1120000",
    "end": "1122640"
  },
  {
    "text": "in a a hosted cluster like an that you",
    "start": "1122640",
    "end": "1125280"
  },
  {
    "text": "self-host um could be a managed cluster",
    "start": "1125280",
    "end": "1127520"
  },
  {
    "text": "on your own i mean it could be in AWS or",
    "start": "1127520",
    "end": "1129520"
  },
  {
    "text": "it could be on prem um and you can also",
    "start": "1129520",
    "end": "1132480"
  },
  {
    "text": "connect easily to any LM provider AWS",
    "start": "1132480",
    "end": "1135039"
  },
  {
    "text": "Bedrock Azure OpenAI and what's great",
    "start": "1135039",
    "end": "1137679"
  },
  {
    "text": "about uh the Envoy gateway is you get",
    "start": "1137679",
    "end": "1139840"
  },
  {
    "text": "all these features and from a client",
    "start": "1139840",
    "end": "1141280"
  },
  {
    "text": "perspective it's really easy you just",
    "start": "1141280",
    "end": "1143120"
  },
  {
    "text": "give the model name and based on the",
    "start": "1143120",
    "end": "1144559"
  },
  {
    "text": "route in the Envoy AI gateway it",
    "start": "1144559",
    "end": "1146480"
  },
  {
    "text": "automatically knows where to route it to",
    "start": "1146480",
    "end": "1148160"
  },
  {
    "text": "so it makes it super easy to um have a",
    "start": "1148160",
    "end": "1151840"
  },
  {
    "text": "platform and build a platform with",
    "start": "1151840",
    "end": "1153280"
  },
  {
    "text": "Kserve and other environments",
    "start": "1153280",
    "end": "1157360"
  },
  {
    "text": "so let's discuss another further",
    "start": "1157360",
    "end": "1159600"
  },
  {
    "text": "optimization for a component in our",
    "start": "1159600",
    "end": "1162200"
  },
  {
    "text": "architecture so if you are running",
    "start": "1162200",
    "end": "1164880"
  },
  {
    "text": "KServe and self-hosted models um you can",
    "start": "1164880",
    "end": "1168160"
  },
  {
    "text": "also implement this uh gateway API",
    "start": "1168160",
    "end": "1173000"
  },
  {
    "text": "extension so what does this do the",
    "start": "1173000",
    "end": "1175440"
  },
  {
    "text": "gateway AP API inference extension gives",
    "start": "1175440",
    "end": "1178240"
  },
  {
    "text": "you better metrics and insights um it",
    "start": "1178240",
    "end": "1180799"
  },
  {
    "text": "tracks cache location model placement",
    "start": "1180799",
    "end": "1183039"
  },
  {
    "text": "and pod capacity and this is uh a",
    "start": "1183039",
    "end": "1186240"
  },
  {
    "text": "Kubernetes native extension of the just",
    "start": "1186240",
    "end": "1190360"
  },
  {
    "text": "ingress API extension as well so this is",
    "start": "1190360",
    "end": "1194080"
  },
  {
    "text": "just built for genai models specific for",
    "start": "1194080",
    "end": "1197039"
  },
  {
    "text": "them as well so we get pod capacity uh",
    "start": "1197039",
    "end": "1200559"
  },
  {
    "text": "is another insight we get we get smarter",
    "start": "1200559",
    "end": "1202720"
  },
  {
    "text": "routing so on this level it routes",
    "start": "1202720",
    "end": "1204960"
  },
  {
    "text": "requests based on like loads optimized",
    "start": "1204960",
    "end": "1207039"
  },
  {
    "text": "for inference and not just roundroin um",
    "start": "1207039",
    "end": "1209520"
  },
  {
    "text": "it can route based on model name",
    "start": "1209520",
    "end": "1211120"
  },
  {
    "text": "upgrading from traditional pathbased",
    "start": "1211120",
    "end": "1213280"
  },
  {
    "text": "routing and also it can implement",
    "start": "1213280",
    "end": "1215520"
  },
  {
    "text": "serving priority so prioritize models",
    "start": "1215520",
    "end": "1217760"
  },
  {
    "text": "based on how critical they are giving",
    "start": "1217760",
    "end": "1219440"
  },
  {
    "text": "higher priorities to latency sensitive",
    "start": "1219440",
    "end": "1222240"
  },
  {
    "text": "task um also it helps again on this",
    "start": "1222240",
    "end": "1224880"
  },
  {
    "text": "level optimize resource use so it",
    "start": "1224880",
    "end": "1226960"
  },
  {
    "text": "improves things like efficiency scaling",
    "start": "1226960",
    "end": "1229120"
  },
  {
    "text": "and",
    "start": "1229120",
    "end": "1231159"
  },
  {
    "text": "latency again we're talking about",
    "start": "1231159",
    "end": "1233440"
  },
  {
    "text": "building KSERve and using it as a",
    "start": "1233440",
    "end": "1235440"
  },
  {
    "text": "platform and especially since LM are",
    "start": "1235440",
    "end": "1238000"
  },
  {
    "text": "getting larger and open-source models",
    "start": "1238000",
    "end": "1240159"
  },
  {
    "text": "are more common we're looking to kind of",
    "start": "1240159",
    "end": "1242159"
  },
  {
    "text": "shift to thinking of LMS as a service um",
    "start": "1242159",
    "end": "1245600"
  },
  {
    "text": "and at the in the hybrid cloud with",
    "start": "1245600",
    "end": "1247120"
  },
  {
    "text": "Envoy AI gateway architecture we show",
    "start": "1247120",
    "end": "1249440"
  },
  {
    "text": "that but also um as we know uh resources",
    "start": "1249440",
    "end": "1252720"
  },
  {
    "text": "can be limited so we want to be able to",
    "start": "1252720",
    "end": "1254640"
  },
  {
    "text": "provide our users our AI engineers with",
    "start": "1254640",
    "end": "1257200"
  },
  {
    "text": "a platform they can easily use that can",
    "start": "1257200",
    "end": "1258960"
  },
  {
    "text": "optimize help to optimize our resources",
    "start": "1258960",
    "end": "1260799"
  },
  {
    "text": "and use them better one way we can do",
    "start": "1260799",
    "end": "1262799"
  },
  {
    "text": "that is through batch inference um so",
    "start": "1262799",
    "end": "1265919"
  },
  {
    "text": "this is we again we would implement the",
    "start": "1265919",
    "end": "1268320"
  },
  {
    "text": "uh we'll we are implementing the um open",
    "start": "1268320",
    "end": "1270640"
  },
  {
    "text": "AI protocol with this to to have a batch",
    "start": "1270640",
    "end": "1273760"
  },
  {
    "text": "architecture so this just shows with",
    "start": "1273760",
    "end": "1275840"
  },
  {
    "text": "Kserve and Envoya gateway how you can",
    "start": "1275840",
    "end": "1278640"
  },
  {
    "text": "implement another architecture to help",
    "start": "1278640",
    "end": "1280240"
  },
  {
    "text": "optimize performance and help to serve",
    "start": "1280240",
    "end": "1282640"
  },
  {
    "text": "your uh LLMs as a",
    "start": "1282640",
    "end": "1286159"
  },
  {
    "text": "service again another thing that we are",
    "start": "1286840",
    "end": "1289840"
  },
  {
    "text": "um looking forward and in progress of",
    "start": "1289840",
    "end": "1291679"
  },
  {
    "text": "implementing another further",
    "start": "1291679",
    "end": "1293320"
  },
  {
    "text": "optimization so latency for geni models",
    "start": "1293320",
    "end": "1296159"
  },
  {
    "text": "isn't very predictable because there are",
    "start": "1296159",
    "end": "1298480"
  },
  {
    "text": "these two steps um that have two",
    "start": "1298480",
    "end": "1300480"
  },
  {
    "text": "different restrictions one's",
    "start": "1300480",
    "end": "1301760"
  },
  {
    "text": "computebound and the other is memory",
    "start": "1301760",
    "end": "1303440"
  },
  {
    "text": "bound prefill is computebound and",
    "start": "1303440",
    "end": "1305679"
  },
  {
    "text": "decoding um is also memory bound so",
    "start": "1305679",
    "end": "1308960"
  },
  {
    "text": "prefill needs GPU compute on the fly and",
    "start": "1308960",
    "end": "1312159"
  },
  {
    "text": "decoding has a slightly different need",
    "start": "1312159",
    "end": "1314320"
  },
  {
    "text": "it needs to access the KV cache uh and",
    "start": "1314320",
    "end": "1316559"
  },
  {
    "text": "use the results from the pre prefilling",
    "start": "1316559",
    "end": "1318640"
  },
  {
    "text": "stage so they do depend on each other",
    "start": "1318640",
    "end": "1320240"
  },
  {
    "text": "but they have different needs so we can",
    "start": "1320240",
    "end": "1322080"
  },
  {
    "text": "separate these two execute them",
    "start": "1322080",
    "end": "1324120"
  },
  {
    "text": "independently and uh on different GPUs",
    "start": "1324120",
    "end": "1327039"
  },
  {
    "text": "and this can help to increase our",
    "start": "1327039",
    "end": "1328720"
  },
  {
    "text": "performance our throughput optimize our",
    "start": "1328720",
    "end": "1331280"
  },
  {
    "text": "hardware for each stage so this is",
    "start": "1331280",
    "end": "1333520"
  },
  {
    "text": "another optimization that we're in",
    "start": "1333520",
    "end": "1335120"
  },
  {
    "text": "progress of",
    "start": "1335120",
    "end": "1338360"
  },
  {
    "text": "developing so that was a lot and just to",
    "start": "1338360",
    "end": "1341360"
  },
  {
    "text": "wrap it up uh some of our key lessons",
    "start": "1341360",
    "end": "1343520"
  },
  {
    "text": "learned and takeaway since we are",
    "start": "1343520",
    "end": "1345360"
  },
  {
    "text": "running this at um an enterprise scale",
    "start": "1345360",
    "end": "1348240"
  },
  {
    "text": "so latency versus throughput uh we like",
    "start": "1348240",
    "end": "1351120"
  },
  {
    "text": "to you know give our engine AI engineers",
    "start": "1351120",
    "end": "1353840"
  },
  {
    "text": "the knobs for them to tune uh to best",
    "start": "1353840",
    "end": "1356400"
  },
  {
    "text": "fit the needs of their models for",
    "start": "1356400",
    "end": "1358200"
  },
  {
    "text": "example depending on the model or the",
    "start": "1358200",
    "end": "1360400"
  },
  {
    "text": "use case there might be a large model",
    "start": "1360400",
    "end": "1362159"
  },
  {
    "text": "that some requests need to hit but",
    "start": "1362159",
    "end": "1363840"
  },
  {
    "text": "certain requests it might be better if",
    "start": "1363840",
    "end": "1365120"
  },
  {
    "text": "they hit a smaller model and help us to",
    "start": "1365120",
    "end": "1366880"
  },
  {
    "text": "optimize our performance so what we what",
    "start": "1366880",
    "end": "1369440"
  },
  {
    "text": "we're trying to do as a platform team",
    "start": "1369440",
    "end": "1371280"
  },
  {
    "text": "and with Kerve is just give them those",
    "start": "1371280",
    "end": "1373520"
  },
  {
    "text": "knobs so that they can tune based on",
    "start": "1373520",
    "end": "1375360"
  },
  {
    "text": "their needs based on their use case so",
    "start": "1375360",
    "end": "1377919"
  },
  {
    "text": "considering latency versus throughput",
    "start": "1377919",
    "end": "1379919"
  },
  {
    "text": "considering the trade-offs and giving",
    "start": "1379919",
    "end": "1381440"
  },
  {
    "text": "them the the the things they need to be",
    "start": "1381440",
    "end": "1383840"
  },
  {
    "text": "able to tune it correctly additionally",
    "start": "1383840",
    "end": "1386400"
  },
  {
    "text": "optimizations matter so again all of",
    "start": "1386400",
    "end": "1388960"
  },
  {
    "text": "these things are all of these features",
    "start": "1388960",
    "end": "1390320"
  },
  {
    "text": "are different optimizations kv caching",
    "start": "1390320",
    "end": "1392720"
  },
  {
    "text": "prompt caching model batching model",
    "start": "1392720",
    "end": "1394880"
  },
  {
    "text": "caching token based autoscaling tok to",
    "start": "1394880",
    "end": "1397200"
  },
  {
    "text": "tokenbased request limiting all are",
    "start": "1397200",
    "end": "1399840"
  },
  {
    "text": "things that can help us improve the",
    "start": "1399840",
    "end": "1401600"
  },
  {
    "text": "performance again we're starting to",
    "start": "1401600",
    "end": "1403840"
  },
  {
    "text": "shift to think of our LMS as a service",
    "start": "1403840",
    "end": "1405840"
  },
  {
    "text": "and how we can host them to have better",
    "start": "1405840",
    "end": "1408320"
  },
  {
    "text": "optimizations and better use of our",
    "start": "1408320",
    "end": "1410000"
  },
  {
    "text": "resources so AI gateway and KServe",
    "start": "1410000",
    "end": "1412799"
  },
  {
    "text": "together can enable this seamless",
    "start": "1412799",
    "end": "1414720"
  },
  {
    "text": "on-prem and hybrid experience with uh",
    "start": "1414720",
    "end": "1417120"
  },
  {
    "text": "model serving additionally observability",
    "start": "1417120",
    "end": "1420080"
  },
  {
    "text": "is key um so GP for GPU capacity",
    "start": "1420080",
    "end": "1423120"
  },
  {
    "text": "planning and benchmarking we need to",
    "start": "1423120",
    "end": "1424559"
  },
  {
    "text": "ensure efficiency and having these uh",
    "start": "1424559",
    "end": "1427200"
  },
  {
    "text": "different ways to observe our model",
    "start": "1427200",
    "end": "1428799"
  },
  {
    "text": "which all of these tools and features",
    "start": "1428799",
    "end": "1430240"
  },
  {
    "text": "give us are really helpful another thing",
    "start": "1430240",
    "end": "1433039"
  },
  {
    "text": "as a platform team that we've been",
    "start": "1433039",
    "end": "1434559"
  },
  {
    "text": "thinking about are the SLAs and SLOs's",
    "start": "1434559",
    "end": "1436400"
  },
  {
    "text": "for LMS there are certain things that",
    "start": "1436400",
    "end": "1438720"
  },
  {
    "text": "you need to consider like for time to",
    "start": "1438720",
    "end": "1441120"
  },
  {
    "text": "first token and all of these different",
    "start": "1441120",
    "end": "1442799"
  },
  {
    "text": "things that we need to consider as a",
    "start": "1442799",
    "end": "1444640"
  },
  {
    "text": "platform team how to create a very",
    "start": "1444640",
    "end": "1446320"
  },
  {
    "text": "reliable platform which we're doing a",
    "start": "1446320",
    "end": "1448400"
  },
  {
    "text": "talk on later tomorrow also",
    "start": "1448400",
    "end": "1452400"
  },
  {
    "text": "all right so if you're interested in",
    "start": "1453679",
    "end": "1455600"
  },
  {
    "text": "working with or contributing to the",
    "start": "1455600",
    "end": "1457600"
  },
  {
    "text": "project you can get started with these",
    "start": "1457600",
    "end": "1460360"
  },
  {
    "text": "resources and Bloomberg is also hiring",
    "start": "1460360",
    "end": "1463279"
  },
  {
    "text": "so come talk to us at our booth um in",
    "start": "1463279",
    "end": "1465440"
  },
  {
    "text": "the expo halls when the main conference",
    "start": "1465440",
    "end": "1467760"
  },
  {
    "text": "starts if you're interested in being",
    "start": "1467760",
    "end": "1469600"
  },
  {
    "text": "part of the AI journey at",
    "start": "1469600",
    "end": "1471799"
  },
  {
    "text": "Bloomberg right thank you for being here",
    "start": "1471799",
    "end": "1474480"
  },
  {
    "text": "for our talk and uh we have like one",
    "start": "1474480",
    "end": "1476799"
  },
  {
    "text": "minute left for maybe one question thank",
    "start": "1476799",
    "end": "1479440"
  },
  {
    "text": "you very much",
    "start": "1479440",
    "end": "1482840"
  },
  {
    "text": "thank you so we do have time for",
    "start": "1483919",
    "end": "1486400"
  },
  {
    "text": "questions so go ahead yeah",
    "start": "1486400",
    "end": "1490320"
  },
  {
    "text": "i I will just ask the next speakers to",
    "start": "1490320",
    "end": "1492480"
  },
  {
    "text": "come forward if possible so that we",
    "start": "1492480",
    "end": "1494080"
  },
  {
    "text": "speed up after yeah go ahead hi thanks",
    "start": "1494080",
    "end": "1496640"
  },
  {
    "text": "for the for the session um I my question",
    "start": "1496640",
    "end": "1500799"
  },
  {
    "text": "is you started showing the general case",
    "start": "1500799",
    "end": "1503840"
  },
  {
    "text": "serve diagram where K native plus is to",
    "start": "1503840",
    "end": "1506640"
  },
  {
    "text": "shown and then you introduced MboyI",
    "start": "1506640",
    "end": "1510000"
  },
  {
    "text": "gateway how would that fit into that",
    "start": "1510000",
    "end": "1512159"
  },
  {
    "text": "picture is like an alternative like",
    "start": "1512159",
    "end": "1513919"
  },
  {
    "text": "model mesh or",
    "start": "1513919",
    "end": "1517559"
  },
  {
    "text": "Oh you're asking how uh K and ITO fit in",
    "start": "1520240",
    "end": "1525039"
  },
  {
    "text": "uh yeah we still use Kave and IO under",
    "start": "1525039",
    "end": "1527360"
  },
  {
    "text": "the hood um yeah you want to uh for the",
    "start": "1527360",
    "end": "1531279"
  },
  {
    "text": "ARM serving we actually um moving away",
    "start": "1531279",
    "end": "1534240"
  },
  {
    "text": "from Kative so because uh Kative",
    "start": "1534240",
    "end": "1536720"
  },
  {
    "text": "traditionally is a request based",
    "start": "1536720",
    "end": "1538320"
  },
  {
    "text": "autoscaling um and uh for the ARMS we",
    "start": "1538320",
    "end": "1541520"
  },
  {
    "text": "are currently um moving to direction for",
    "start": "1541520",
    "end": "1544000"
  },
  {
    "text": "using ka currently um so that gives us a",
    "start": "1544000",
    "end": "1547360"
  },
  {
    "text": "lot more flexibility how we can p u",
    "start": "1547360",
    "end": "1550000"
  },
  {
    "text": "scale based on any of the metrics um if",
    "start": "1550000",
    "end": "1553440"
  },
  {
    "text": "that answers question",
    "start": "1553440",
    "end": "1557080"
  },
  {
    "text": "thank you",
    "start": "1559520",
    "end": "1562760"
  },
  {
    "text": "hi this is a great talk um it covered a",
    "start": "1563559",
    "end": "1566960"
  },
  {
    "text": "lot of the I think requirements for an",
    "start": "1566960",
    "end": "1569360"
  },
  {
    "text": "advanced inferencing server uh",
    "start": "1569360",
    "end": "1572080"
  },
  {
    "text": "inferencing platform i was curious when",
    "start": "1572080",
    "end": "1574640"
  },
  {
    "text": "it comes to uh Nvidia Dynamo is that",
    "start": "1574640",
    "end": "1578559"
  },
  {
    "text": "something that you see embracing that",
    "start": "1578559",
    "end": "1580559"
  },
  {
    "text": "technology since it's open source or do",
    "start": "1580559",
    "end": "1582240"
  },
  {
    "text": "you see doing something that's",
    "start": "1582240",
    "end": "1583520"
  },
  {
    "text": "complimentary or competitive with that",
    "start": "1583520",
    "end": "1587960"
  },
  {
    "text": "um at Bloomberg we do try to integrate",
    "start": "1588240",
    "end": "1590960"
  },
  {
    "text": "with uh and we implement this or we have",
    "start": "1590960",
    "end": "1594880"
  },
  {
    "text": "this value in KERT 2 we do try to",
    "start": "1594880",
    "end": "1598000"
  },
  {
    "text": "implement with uh open- source projects",
    "start": "1598000",
    "end": "1600320"
  },
  {
    "text": "like no VLM so um I'm not 100% sure but",
    "start": "1600320",
    "end": "1604400"
  },
  {
    "text": "I'm sure that we would try to integrate",
    "start": "1604400",
    "end": "1605840"
  },
  {
    "text": "if we can as long as everything works",
    "start": "1605840",
    "end": "1607919"
  },
  {
    "text": "out right",
    "start": "1607919",
    "end": "1611240"
  },
  {
    "text": "hi great talk um quick question uh do",
    "start": "1611840",
    "end": "1615760"
  },
  {
    "text": "you run single model multi-node",
    "start": "1615760",
    "end": "1619240"
  },
  {
    "text": "deployment workloads and if so um do you",
    "start": "1619240",
    "end": "1623679"
  },
  {
    "text": "support or planning to support some kind",
    "start": "1623679",
    "end": "1625840"
  },
  {
    "text": "of topology awareness",
    "start": "1625840",
    "end": "1629559"
  },
  {
    "text": "yes I mean yeah for example the deepseek",
    "start": "1634799",
    "end": "1637360"
  },
  {
    "text": "model it needs to be multiode uh uh so",
    "start": "1637360",
    "end": "1642159"
  },
  {
    "text": "um topology we currently don't quite",
    "start": "1642159",
    "end": "1645760"
  },
  {
    "text": "suppose so like are you talking about",
    "start": "1645760",
    "end": "1647200"
  },
  {
    "text": "the hosting model like uh",
    "start": "1647200",
    "end": "1650360"
  },
  {
    "text": "uh uh can you repeat the question again",
    "start": "1650360",
    "end": "1653440"
  },
  {
    "text": "sorry yes so so I'm wondering like for",
    "start": "1653440",
    "end": "1655919"
  },
  {
    "text": "single model multi-note types of",
    "start": "1655919",
    "end": "1659159"
  },
  {
    "text": "deployments do you have some kind of",
    "start": "1659159",
    "end": "1661440"
  },
  {
    "text": "mechanism so that the nodes",
    "start": "1661440",
    "end": "1663520"
  },
  {
    "text": "participating in inference they actually",
    "start": "1663520",
    "end": "1666720"
  },
  {
    "text": "have some kind of network locality so",
    "start": "1666720",
    "end": "1668960"
  },
  {
    "text": "they are in close proxim proximity to",
    "start": "1668960",
    "end": "1671279"
  },
  {
    "text": "each other um I think uh you should be",
    "start": "1671279",
    "end": "1674559"
  },
  {
    "text": "able to do using the Kubernetes like a",
    "start": "1674559",
    "end": "1676480"
  },
  {
    "text": "topology um what is that called uh",
    "start": "1676480",
    "end": "1679440"
  },
  {
    "text": "awareness like uh scheduling um if you",
    "start": "1679440",
    "end": "1682080"
  },
  {
    "text": "want to collocate um all the pods in on",
    "start": "1682080",
    "end": "1685279"
  },
  {
    "text": "the same node um for the especially the",
    "start": "1685279",
    "end": "1688080"
  },
  {
    "text": "the workers and head head workers head",
    "start": "1688080",
    "end": "1690399"
  },
  {
    "text": "node and the worker nodes right",
    "start": "1690399",
    "end": "1693360"
  },
  {
    "text": "cool thank you y All right so apologies",
    "start": "1693360",
    "end": "1696559"
  },
  {
    "text": "for the two remaining questions but we",
    "start": "1696559",
    "end": "1698559"
  },
  {
    "text": "do have to move to the next session so I",
    "start": "1698559",
    "end": "1700880"
  },
  {
    "text": "I suggest maybe you could if you can",
    "start": "1700880",
    "end": "1703120"
  },
  {
    "text": "come and and ask the questions directly",
    "start": "1703120",
    "end": "1704960"
  },
  {
    "text": "but let's thank again for the great talk",
    "start": "1704960",
    "end": "1707520"
  },
  {
    "text": "thank you",
    "start": "1707520",
    "end": "1710760"
  }
]