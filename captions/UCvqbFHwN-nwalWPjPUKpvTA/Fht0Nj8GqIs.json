[
  {
    "start": "0",
    "end": "95000"
  },
  {
    "text": "welcome everyone thank you for joining",
    "start": "80",
    "end": "2399"
  },
  {
    "text": "lesson learned",
    "start": "2399",
    "end": "3280"
  },
  {
    "text": "on running hadoop on kubernetes session",
    "start": "3280",
    "end": "6319"
  },
  {
    "text": "my name is chen chang i'm part of data",
    "start": "6319",
    "end": "9200"
  },
  {
    "text": "sre team at linkedin",
    "start": "9200",
    "end": "11040"
  },
  {
    "text": "our team manages entire offline data",
    "start": "11040",
    "end": "13840"
  },
  {
    "text": "infrastructure",
    "start": "13840",
    "end": "15040"
  },
  {
    "text": "including hadoop spark presto and many",
    "start": "15040",
    "end": "18480"
  },
  {
    "text": "others",
    "start": "18480",
    "end": "20000"
  },
  {
    "text": "kubernetes and linkedin started about a",
    "start": "20000",
    "end": "22640"
  },
  {
    "text": "year ago",
    "start": "22640",
    "end": "23680"
  },
  {
    "text": "our team was one of the first users and",
    "start": "23680",
    "end": "26880"
  },
  {
    "text": "our main use case is to launch ephemeral",
    "start": "26880",
    "end": "30000"
  },
  {
    "text": "hadoop cluster for development and",
    "start": "30000",
    "end": "32558"
  },
  {
    "text": "testing purposes",
    "start": "32559",
    "end": "34239"
  },
  {
    "text": "there are many lessons learned from our",
    "start": "34239",
    "end": "36320"
  },
  {
    "text": "journey and would like to share some of",
    "start": "36320",
    "end": "38480"
  },
  {
    "text": "the challenges",
    "start": "38480",
    "end": "40000"
  },
  {
    "text": "problems we encountered as well as",
    "start": "40000",
    "end": "42480"
  },
  {
    "text": "solutions",
    "start": "42480",
    "end": "43280"
  },
  {
    "text": "in order to successfully running hadoop",
    "start": "43280",
    "end": "46399"
  },
  {
    "text": "on kubernetes with overlay network",
    "start": "46399",
    "end": "51038"
  },
  {
    "text": "in this session i will cover hadoop",
    "start": "54000",
    "end": "56640"
  },
  {
    "text": "basics",
    "start": "56640",
    "end": "57520"
  },
  {
    "text": "for audiences who are not familiar with",
    "start": "57520",
    "end": "59680"
  },
  {
    "text": "hadoop and are briefly",
    "start": "59680",
    "end": "61520"
  },
  {
    "text": "touching on hadoop footprint at linkedin",
    "start": "61520",
    "end": "64720"
  },
  {
    "text": "i will talk about the initiatives for",
    "start": "64720",
    "end": "66720"
  },
  {
    "text": "this project",
    "start": "66720",
    "end": "67920"
  },
  {
    "text": "architecture overview and challenges",
    "start": "67920",
    "end": "70159"
  },
  {
    "text": "walkthrough",
    "start": "70159",
    "end": "71520"
  },
  {
    "text": "in addition i would like to share the",
    "start": "71520",
    "end": "74080"
  },
  {
    "text": "insights of",
    "start": "74080",
    "end": "74960"
  },
  {
    "text": "what currently we are working on and a",
    "start": "74960",
    "end": "77119"
  },
  {
    "text": "future role map",
    "start": "77119",
    "end": "78400"
  },
  {
    "text": "at the end it will be a 10 minute video",
    "start": "78400",
    "end": "80880"
  },
  {
    "text": "including spinning",
    "start": "80880",
    "end": "82000"
  },
  {
    "text": "up a secure hadoop cluster on kubernetes",
    "start": "82000",
    "end": "85360"
  },
  {
    "text": "doing some file operation running a",
    "start": "85360",
    "end": "88640"
  },
  {
    "text": "hadoop job as well as in place expansion",
    "start": "88640",
    "end": "94240"
  },
  {
    "text": "let's start what is hadoop hadoop is a",
    "start": "94560",
    "end": "98240"
  },
  {
    "start": "95000",
    "end": "201000"
  },
  {
    "text": "collection of open source software",
    "start": "98240",
    "end": "100960"
  },
  {
    "text": "designed to handle big data storage",
    "start": "100960",
    "end": "103280"
  },
  {
    "text": "and computation the main components of",
    "start": "103280",
    "end": "106079"
  },
  {
    "text": "hadoop",
    "start": "106079",
    "end": "106880"
  },
  {
    "text": "are hadoop distributed file system hdfs",
    "start": "106880",
    "end": "110560"
  },
  {
    "text": "which is a parsex-like file system in a",
    "start": "110560",
    "end": "113439"
  },
  {
    "text": "design",
    "start": "113439",
    "end": "114159"
  },
  {
    "text": "for distributed computing framework and",
    "start": "114159",
    "end": "117360"
  },
  {
    "text": "it is highly reliable and resilient",
    "start": "117360",
    "end": "120399"
  },
  {
    "text": "hdfs has three types of components",
    "start": "120399",
    "end": "123840"
  },
  {
    "text": "a h a pair of name nodes which holds",
    "start": "123840",
    "end": "126799"
  },
  {
    "text": "metadata",
    "start": "126799",
    "end": "127520"
  },
  {
    "text": "of the entire file system quorum of",
    "start": "127520",
    "end": "130239"
  },
  {
    "text": "general nodes",
    "start": "130239",
    "end": "131200"
  },
  {
    "text": "saves the incremental metadata for later",
    "start": "131200",
    "end": "134640"
  },
  {
    "text": "compaction and many data nodes that",
    "start": "134640",
    "end": "137200"
  },
  {
    "text": "store",
    "start": "137200",
    "end": "138000"
  },
  {
    "text": "the actual data above hdfs",
    "start": "138000",
    "end": "141360"
  },
  {
    "text": "there's a yarn it manages all",
    "start": "141360",
    "end": "143840"
  },
  {
    "text": "distributed compute resources",
    "start": "143840",
    "end": "145760"
  },
  {
    "text": "including cpu memory and gpu across",
    "start": "145760",
    "end": "149200"
  },
  {
    "text": "many worker nodes yarn has four type of",
    "start": "149200",
    "end": "152400"
  },
  {
    "text": "components",
    "start": "152400",
    "end": "153120"
  },
  {
    "text": "resources manager manages hardware",
    "start": "153120",
    "end": "155519"
  },
  {
    "text": "resources",
    "start": "155519",
    "end": "156480"
  },
  {
    "text": "schedules workflows and monitor jobs",
    "start": "156480",
    "end": "159200"
  },
  {
    "text": "history server archives",
    "start": "159200",
    "end": "161200"
  },
  {
    "text": "completed jobs and a proxy server",
    "start": "161200",
    "end": "163440"
  },
  {
    "text": "redirect user's request",
    "start": "163440",
    "end": "165360"
  },
  {
    "text": "to either resources manager or history",
    "start": "165360",
    "end": "167680"
  },
  {
    "text": "server depends on the job state",
    "start": "167680",
    "end": "170080"
  },
  {
    "text": "last is the node manager that is the one",
    "start": "170080",
    "end": "172720"
  },
  {
    "text": "actually",
    "start": "172720",
    "end": "173440"
  },
  {
    "text": "runs the task on top of",
    "start": "173440",
    "end": "176480"
  },
  {
    "text": "those there are many distributed",
    "start": "176480",
    "end": "178640"
  },
  {
    "text": "computing framework",
    "start": "178640",
    "end": "179840"
  },
  {
    "text": "such as map reduce spark presto",
    "start": "179840",
    "end": "183440"
  },
  {
    "text": "and many others last but not the least",
    "start": "183440",
    "end": "186959"
  },
  {
    "text": "is the client such as gateway",
    "start": "186959",
    "end": "190400"
  },
  {
    "text": "for cli and many other open source",
    "start": "190400",
    "end": "193519"
  },
  {
    "text": "schedulers such as azkaban",
    "start": "193519",
    "end": "196640"
  },
  {
    "text": "apache uzi and airflow",
    "start": "196640",
    "end": "200879"
  },
  {
    "start": "201000",
    "end": "301000"
  },
  {
    "text": "hadoop at linkedin linkedin is a large",
    "start": "201840",
    "end": "205200"
  },
  {
    "text": "hadoop shop",
    "start": "205200",
    "end": "206080"
  },
  {
    "text": "and we have been continuously investing",
    "start": "206080",
    "end": "208480"
  },
  {
    "text": "in hadoop for past 10 years",
    "start": "208480",
    "end": "210959"
  },
  {
    "text": "we are running one of the largest hadoop",
    "start": "210959",
    "end": "213280"
  },
  {
    "text": "environment in industry",
    "start": "213280",
    "end": "214959"
  },
  {
    "text": "we have more than 10 hadoop clusters and",
    "start": "214959",
    "end": "218080"
  },
  {
    "text": "all of them are running on bare metals",
    "start": "218080",
    "end": "220879"
  },
  {
    "text": "our largest hadoop",
    "start": "220879",
    "end": "222400"
  },
  {
    "text": "cluster is over 7000 physical servers",
    "start": "222400",
    "end": "225519"
  },
  {
    "text": "with storage capacity of 400 petabytes",
    "start": "225519",
    "end": "229040"
  },
  {
    "text": "half million cpu v cores and a 1.6",
    "start": "229040",
    "end": "232959"
  },
  {
    "text": "petabyte of memory and we are constantly",
    "start": "232959",
    "end": "236080"
  },
  {
    "text": "pushing the limit",
    "start": "236080",
    "end": "237120"
  },
  {
    "text": "by adding more nodes we have multiple",
    "start": "237120",
    "end": "239920"
  },
  {
    "text": "super large",
    "start": "239920",
    "end": "241040"
  },
  {
    "text": "clusters with over 4 000 nodes each",
    "start": "241040",
    "end": "244720"
  },
  {
    "text": "we serve thousands of users across",
    "start": "244720",
    "end": "247360"
  },
  {
    "text": "entire company",
    "start": "247360",
    "end": "248560"
  },
  {
    "text": "our average hdfs read and write",
    "start": "248560",
    "end": "250959"
  },
  {
    "text": "throughput",
    "start": "250959",
    "end": "251599"
  },
  {
    "text": "are over 600 gigabytes per second",
    "start": "251599",
    "end": "254400"
  },
  {
    "text": "respected",
    "start": "254400",
    "end": "255280"
  },
  {
    "text": "respectively and every day we have over",
    "start": "255280",
    "end": "258799"
  },
  {
    "text": "300 000 jobs with with 100 million",
    "start": "258799",
    "end": "262000"
  },
  {
    "text": "of with 100 millions of",
    "start": "262000",
    "end": "265840"
  },
  {
    "text": "containers running our hdfs is very well",
    "start": "265840",
    "end": "268560"
  },
  {
    "text": "tuned",
    "start": "268560",
    "end": "269120"
  },
  {
    "text": "and can easily handle more than 100 000",
    "start": "269120",
    "end": "272000"
  },
  {
    "text": "qps",
    "start": "272000",
    "end": "273280"
  },
  {
    "text": "our team also manages ldap",
    "start": "273280",
    "end": "276320"
  },
  {
    "text": "kerberos and dns infrastructures to",
    "start": "276320",
    "end": "278800"
  },
  {
    "text": "support",
    "start": "278800",
    "end": "279360"
  },
  {
    "text": "secure hadoop clusters our average ldap",
    "start": "279360",
    "end": "282800"
  },
  {
    "text": "kdc and the dns qps are 150 000",
    "start": "282800",
    "end": "287040"
  },
  {
    "text": "5000 and 95 000. our hadoop environment",
    "start": "287040",
    "end": "290479"
  },
  {
    "text": "generates huge network traffic with",
    "start": "290479",
    "end": "292479"
  },
  {
    "text": "average bandwidth of 15",
    "start": "292479",
    "end": "294720"
  },
  {
    "text": "per 15 terabits per",
    "start": "294720",
    "end": "298000"
  },
  {
    "text": "second",
    "start": "298000",
    "end": "300400"
  },
  {
    "start": "301000",
    "end": "352000"
  },
  {
    "text": "so as i mentioned linkedin has a large",
    "start": "302560",
    "end": "305919"
  },
  {
    "text": "in-house hadoop development and sre",
    "start": "305919",
    "end": "308400"
  },
  {
    "text": "teams",
    "start": "308400",
    "end": "309120"
  },
  {
    "text": "that are constantly working with open",
    "start": "309120",
    "end": "310800"
  },
  {
    "text": "source community contributing features",
    "start": "310800",
    "end": "313520"
  },
  {
    "text": "and bug fixes having hadoop ephemeral",
    "start": "313520",
    "end": "316240"
  },
  {
    "text": "cluster running on kubernetes",
    "start": "316240",
    "end": "318320"
  },
  {
    "text": "allow every engineer to have own",
    "start": "318320",
    "end": "321360"
  },
  {
    "text": "cluster for testing and development in",
    "start": "321360",
    "end": "323840"
  },
  {
    "text": "addition",
    "start": "323840",
    "end": "324720"
  },
  {
    "text": "the cluster is with running with",
    "start": "324720",
    "end": "327199"
  },
  {
    "text": "immutable container images",
    "start": "327199",
    "end": "329280"
  },
  {
    "text": "so the issue is always reproducible",
    "start": "329280",
    "end": "332720"
  },
  {
    "text": "this approach significantly boosts the",
    "start": "332720",
    "end": "335360"
  },
  {
    "text": "productivity and efficiency",
    "start": "335360",
    "end": "337280"
  },
  {
    "text": "with only 3 minutes to spin up a secure",
    "start": "337280",
    "end": "340320"
  },
  {
    "text": "hadoop",
    "start": "340320",
    "end": "341199"
  },
  {
    "text": "cluster comparing to hours or days",
    "start": "341199",
    "end": "344160"
  },
  {
    "text": "building a testing environment",
    "start": "344160",
    "end": "346639"
  },
  {
    "text": "on the bare metal let's have a quick",
    "start": "346639",
    "end": "349520"
  },
  {
    "text": "architecture",
    "start": "349520",
    "end": "350840"
  },
  {
    "text": "review it builds on top of",
    "start": "350840",
    "end": "355120"
  },
  {
    "start": "352000",
    "end": "401000"
  },
  {
    "text": "kubernetes overlay network each",
    "start": "355120",
    "end": "358000"
  },
  {
    "text": "component",
    "start": "358000",
    "end": "358639"
  },
  {
    "text": "requires a kubernetes service",
    "start": "358639",
    "end": "361840"
  },
  {
    "text": "to have a static and a dns resolvable",
    "start": "361840",
    "end": "364960"
  },
  {
    "text": "hostname",
    "start": "364960",
    "end": "365600"
  },
  {
    "text": "for communication between parts as well",
    "start": "365600",
    "end": "368240"
  },
  {
    "text": "as",
    "start": "368240",
    "end": "368800"
  },
  {
    "text": "service discovery for every admin notes",
    "start": "368800",
    "end": "371840"
  },
  {
    "text": "such as general node name node resources",
    "start": "371840",
    "end": "374479"
  },
  {
    "text": "manager",
    "start": "374479",
    "end": "375360"
  },
  {
    "text": "there is a service associated with that",
    "start": "375360",
    "end": "378479"
  },
  {
    "text": "with one two one mapping for worker",
    "start": "378479",
    "end": "380479"
  },
  {
    "text": "nodes which is a data node",
    "start": "380479",
    "end": "382479"
  },
  {
    "text": "and node manager it is n21 mapping",
    "start": "382479",
    "end": "385600"
  },
  {
    "text": "all data node or node managers shares",
    "start": "385600",
    "end": "388639"
  },
  {
    "text": "the same service so the graph here shows",
    "start": "388639",
    "end": "391759"
  },
  {
    "text": "all the communication paths in between",
    "start": "391759",
    "end": "393919"
  },
  {
    "text": "and all the",
    "start": "393919",
    "end": "395039"
  },
  {
    "text": "connections are through service endpoint",
    "start": "395039",
    "end": "399680"
  },
  {
    "start": "401000",
    "end": "480000"
  },
  {
    "text": "let's move to the challenges over the",
    "start": "402479",
    "end": "405360"
  },
  {
    "text": "course of the project we encountered",
    "start": "405360",
    "end": "407440"
  },
  {
    "text": "many challenges and i would like to",
    "start": "407440",
    "end": "409520"
  },
  {
    "text": "share some highlights",
    "start": "409520",
    "end": "410960"
  },
  {
    "text": "with everyone here first one is dns it",
    "start": "410960",
    "end": "414000"
  },
  {
    "text": "took us days",
    "start": "414000",
    "end": "415120"
  },
  {
    "text": "to find out a good solution without",
    "start": "415120",
    "end": "417840"
  },
  {
    "text": "compromising security",
    "start": "417840",
    "end": "419680"
  },
  {
    "text": "next is network by default there is a no",
    "start": "419680",
    "end": "422560"
  },
  {
    "text": "fixed",
    "start": "422560",
    "end": "423039"
  },
  {
    "text": "or predetermined hostname in parts or",
    "start": "423039",
    "end": "425919"
  },
  {
    "text": "deployments",
    "start": "425919",
    "end": "427039"
  },
  {
    "text": "over with overlay network",
    "start": "427039",
    "end": "430080"
  },
  {
    "text": "hadoop admin nodes requires",
    "start": "430080",
    "end": "433759"
  },
  {
    "text": "those fixed hostname to be listed in the",
    "start": "433759",
    "end": "436160"
  },
  {
    "text": "hadoop configuration before",
    "start": "436160",
    "end": "438639"
  },
  {
    "text": "before the deployment third is the",
    "start": "438639",
    "end": "441360"
  },
  {
    "text": "identity",
    "start": "441360",
    "end": "442080"
  },
  {
    "text": "in secure hadoop every part requires a",
    "start": "442080",
    "end": "445280"
  },
  {
    "text": "unique",
    "start": "445280",
    "end": "445840"
  },
  {
    "text": "kerberos credential for authentication",
    "start": "445840",
    "end": "448639"
  },
  {
    "text": "due to the unknown of ip addresses",
    "start": "448639",
    "end": "451039"
  },
  {
    "text": "or host name at launch time we are not",
    "start": "451039",
    "end": "453680"
  },
  {
    "text": "able to",
    "start": "453680",
    "end": "454240"
  },
  {
    "text": "pre-generate those last but not the",
    "start": "454240",
    "end": "457199"
  },
  {
    "text": "least is the orchestration",
    "start": "457199",
    "end": "458880"
  },
  {
    "text": "because there are strong dependencies",
    "start": "458880",
    "end": "461759"
  },
  {
    "text": "between hadoop components",
    "start": "461759",
    "end": "463440"
  },
  {
    "text": "for example hdfs name node must",
    "start": "463440",
    "end": "466960"
  },
  {
    "text": "start after all the general nodes are up",
    "start": "466960",
    "end": "469840"
  },
  {
    "text": "and running",
    "start": "469840",
    "end": "471199"
  },
  {
    "text": "now i'm going to walk you through every",
    "start": "471199",
    "end": "473520"
  },
  {
    "text": "problem in detail",
    "start": "473520",
    "end": "474720"
  },
  {
    "text": "with solution",
    "start": "474720",
    "end": "479840"
  },
  {
    "text": "first one is a dns hadoop worker nodes",
    "start": "481039",
    "end": "484240"
  },
  {
    "text": "which is a yarn node manager or hdfs",
    "start": "484240",
    "end": "486960"
  },
  {
    "text": "data node",
    "start": "486960",
    "end": "487919"
  },
  {
    "text": "talks with each other using hostnames by",
    "start": "487919",
    "end": "490639"
  },
  {
    "text": "default",
    "start": "490639",
    "end": "491440"
  },
  {
    "text": "there is a no global resolvable host",
    "start": "491440",
    "end": "493759"
  },
  {
    "text": "name associated with it",
    "start": "493759",
    "end": "495120"
  },
  {
    "text": "also it is impossible to have one",
    "start": "495120",
    "end": "497440"
  },
  {
    "text": "service",
    "start": "497440",
    "end": "498240"
  },
  {
    "text": "per work part because",
    "start": "498240",
    "end": "501360"
  },
  {
    "text": "it won't scale natively with kubernetes",
    "start": "501360",
    "end": "505039"
  },
  {
    "text": "second is insecure hadoop authentication",
    "start": "505039",
    "end": "507680"
  },
  {
    "text": "also require ip",
    "start": "507680",
    "end": "509360"
  },
  {
    "text": "reverse lookup matching nodes kerberos",
    "start": "509360",
    "end": "512000"
  },
  {
    "text": "principle for example yarn slash",
    "start": "512000",
    "end": "514159"
  },
  {
    "text": "underscore",
    "start": "514159",
    "end": "515039"
  },
  {
    "text": "host the underscore host will be",
    "start": "515039",
    "end": "517599"
  },
  {
    "text": "replaced by the host name",
    "start": "517599",
    "end": "519279"
  },
  {
    "text": "in the hadoop logic with hostname f",
    "start": "519279",
    "end": "522560"
  },
  {
    "text": "and a mismatch will obviously result",
    "start": "522560",
    "end": "524959"
  },
  {
    "text": "authentication failure",
    "start": "524959",
    "end": "526880"
  },
  {
    "text": "next is the webhdfs which is a rest api",
    "start": "526880",
    "end": "530720"
  },
  {
    "text": "endpoint for hdfs when name node sends",
    "start": "530720",
    "end": "534000"
  },
  {
    "text": "the redirect",
    "start": "534000",
    "end": "535360"
  },
  {
    "text": "request back to the client it contains a",
    "start": "535360",
    "end": "538240"
  },
  {
    "text": "hostname of",
    "start": "538240",
    "end": "539760"
  },
  {
    "text": "designated data node that going to be",
    "start": "539760",
    "end": "542480"
  },
  {
    "text": "serving the actual request",
    "start": "542480",
    "end": "544640"
  },
  {
    "text": "and the last is for map reduce or spark",
    "start": "544640",
    "end": "547600"
  },
  {
    "text": "jobs",
    "start": "547600",
    "end": "548320"
  },
  {
    "text": "application master and spark driver are",
    "start": "548320",
    "end": "551120"
  },
  {
    "text": "sharing their host names with the",
    "start": "551120",
    "end": "552800"
  },
  {
    "text": "workers",
    "start": "552800",
    "end": "554080"
  },
  {
    "text": "so the solution here is",
    "start": "554080",
    "end": "557120"
  },
  {
    "text": "by default the part does not provide a",
    "start": "557120",
    "end": "559680"
  },
  {
    "text": "global dns",
    "start": "559680",
    "end": "561600"
  },
  {
    "text": "dns reservable hostname the solution is",
    "start": "561600",
    "end": "563760"
  },
  {
    "text": "to using",
    "start": "563760",
    "end": "564640"
  },
  {
    "text": "stateful set with headless service",
    "start": "564640",
    "end": "568320"
  },
  {
    "text": "so it will provide every part with a",
    "start": "568320",
    "end": "570640"
  },
  {
    "text": "global resolvable",
    "start": "570640",
    "end": "572160"
  },
  {
    "text": "hostname unfortunately it does not solve",
    "start": "572160",
    "end": "574800"
  },
  {
    "text": "the problem",
    "start": "574800",
    "end": "576480"
  },
  {
    "text": "we found the hostname resolution in the",
    "start": "576480",
    "end": "579200"
  },
  {
    "text": "reverse lookup",
    "start": "579200",
    "end": "580320"
  },
  {
    "text": "was inconsistent as you can see my",
    "start": "580320",
    "end": "582880"
  },
  {
    "text": "example here",
    "start": "582880",
    "end": "584080"
  },
  {
    "text": "so when i do the hostname dash f it come",
    "start": "584080",
    "end": "587519"
  },
  {
    "text": "up with the hostname with part",
    "start": "587519",
    "end": "590800"
  },
  {
    "text": "name however when i do a",
    "start": "590800",
    "end": "593920"
  },
  {
    "text": "dns resolution on that it shows up as a",
    "start": "593920",
    "end": "597360"
  },
  {
    "text": "known record then i do a reverse lookup",
    "start": "597360",
    "end": "600480"
  },
  {
    "text": "of the pod ip addresses i get",
    "start": "600480",
    "end": "602560"
  },
  {
    "text": "a actual dns resolvable hostname",
    "start": "602560",
    "end": "607279"
  },
  {
    "text": "so the solution here is we injecting the",
    "start": "607440",
    "end": "611200"
  },
  {
    "text": "actual dns resolvable hostname",
    "start": "611200",
    "end": "613279"
  },
  {
    "text": "to the pods etc hosts file at startup",
    "start": "613279",
    "end": "615839"
  },
  {
    "text": "time",
    "start": "615839",
    "end": "616640"
  },
  {
    "text": "then it resolve all the dns issues we",
    "start": "616640",
    "end": "619200"
  },
  {
    "text": "have",
    "start": "619200",
    "end": "619680"
  },
  {
    "text": "to running actual workload on hadoop",
    "start": "619680",
    "end": "623040"
  },
  {
    "text": "as you can see i get the another example",
    "start": "623040",
    "end": "625120"
  },
  {
    "text": "here underneath it",
    "start": "625120",
    "end": "626320"
  },
  {
    "text": "is after we injecting the actual",
    "start": "626320",
    "end": "628720"
  },
  {
    "text": "resolvable hostname into etc",
    "start": "628720",
    "end": "630640"
  },
  {
    "text": "hosts file then all the forward and",
    "start": "630640",
    "end": "634000"
  },
  {
    "text": "reverse lookup are matching",
    "start": "634000",
    "end": "637040"
  },
  {
    "text": "next one is the network",
    "start": "637040",
    "end": "640079"
  },
  {
    "start": "638000",
    "end": "705000"
  },
  {
    "text": "there are many components in hadoop and",
    "start": "640079",
    "end": "642640"
  },
  {
    "text": "they are communicating with each other",
    "start": "642640",
    "end": "644320"
  },
  {
    "text": "while the cluster is up and running",
    "start": "644320",
    "end": "646320"
  },
  {
    "text": "like for example data nodes initiate the",
    "start": "646320",
    "end": "649200"
  },
  {
    "text": "connection to the name node",
    "start": "649200",
    "end": "651200"
  },
  {
    "text": "node manager initiates the talk to the",
    "start": "651200",
    "end": "654079"
  },
  {
    "text": "resources manager",
    "start": "654079",
    "end": "655440"
  },
  {
    "text": "and there are many intercommunication",
    "start": "655440",
    "end": "657200"
  },
  {
    "text": "between components as well",
    "start": "657200",
    "end": "658880"
  },
  {
    "text": "and hadoop configuration must be",
    "start": "658880",
    "end": "660480"
  },
  {
    "text": "pre-populated with those host names of",
    "start": "660480",
    "end": "663519"
  },
  {
    "text": "with those hostname of admin notes",
    "start": "663519",
    "end": "666640"
  },
  {
    "text": "in place before we start the service the",
    "start": "666640",
    "end": "669279"
  },
  {
    "text": "solution",
    "start": "669279",
    "end": "670079"
  },
  {
    "text": "is to creating a kubernetes service for",
    "start": "670079",
    "end": "673360"
  },
  {
    "text": "every hadoop",
    "start": "673360",
    "end": "674079"
  },
  {
    "text": "admin parts it provides a dns",
    "start": "674079",
    "end": "677120"
  },
  {
    "text": "resolvable and and structured host name",
    "start": "677120",
    "end": "680480"
  },
  {
    "text": "which can be defined",
    "start": "680480",
    "end": "681839"
  },
  {
    "text": "and populated in hadoop configuration",
    "start": "681839",
    "end": "684320"
  },
  {
    "text": "before the actual deployment",
    "start": "684320",
    "end": "685920"
  },
  {
    "text": "so i include a snippet of",
    "start": "685920",
    "end": "689200"
  },
  {
    "text": "configuration xml for hadoop name node",
    "start": "689200",
    "end": "692079"
  },
  {
    "text": "so as you can see here as we have",
    "start": "692079",
    "end": "694399"
  },
  {
    "text": "name node ha1 and ha2 and each of them",
    "start": "694399",
    "end": "697600"
  },
  {
    "text": "using the service endpoint",
    "start": "697600",
    "end": "702000"
  },
  {
    "text": "next one is identity",
    "start": "702000",
    "end": "706160"
  },
  {
    "start": "705000",
    "end": "784000"
  },
  {
    "text": "it only applies to the secure hadoop",
    "start": "706640",
    "end": "709519"
  },
  {
    "text": "cluster",
    "start": "709519",
    "end": "710079"
  },
  {
    "text": "with kerberos enabled every hadoop",
    "start": "710079",
    "end": "712720"
  },
  {
    "text": "component",
    "start": "712720",
    "end": "713360"
  },
  {
    "text": "requires a key tab file with a service",
    "start": "713360",
    "end": "717040"
  },
  {
    "text": "principle for example hdfs slash",
    "start": "717040",
    "end": "720480"
  },
  {
    "text": "dns resolvable host name hr kerberos rom",
    "start": "720480",
    "end": "724079"
  },
  {
    "text": "the host name has ip addresses embedded",
    "start": "724079",
    "end": "726959"
  },
  {
    "text": "and",
    "start": "726959",
    "end": "727279"
  },
  {
    "text": "it is impossible to pre-generate them",
    "start": "727279",
    "end": "730079"
  },
  {
    "text": "and",
    "start": "730079",
    "end": "730320"
  },
  {
    "text": "pushing in the pod our solution is to",
    "start": "730320",
    "end": "733360"
  },
  {
    "text": "introduce",
    "start": "733360",
    "end": "734000"
  },
  {
    "text": "a key tab delivery service running in",
    "start": "734000",
    "end": "736399"
  },
  {
    "text": "kubernetes",
    "start": "736399",
    "end": "737519"
  },
  {
    "text": "it authenticates the request from hadoop",
    "start": "737519",
    "end": "740079"
  },
  {
    "text": "parts",
    "start": "740079",
    "end": "740880"
  },
  {
    "text": "generating a qa only key tab and",
    "start": "740880",
    "end": "744240"
  },
  {
    "text": "and ascend back to the client in hadoop",
    "start": "744240",
    "end": "747120"
  },
  {
    "text": "pod",
    "start": "747120",
    "end": "747920"
  },
  {
    "text": "we have an init container to reconstruct",
    "start": "747920",
    "end": "751200"
  },
  {
    "text": "the host name",
    "start": "751200",
    "end": "752240"
  },
  {
    "text": "by replacing the underscore ip",
    "start": "752240",
    "end": "755519"
  },
  {
    "text": "with the actual dns resolvable hostname",
    "start": "755519",
    "end": "757839"
  },
  {
    "text": "and send the request",
    "start": "757839",
    "end": "759279"
  },
  {
    "text": "to the keytab delivery service",
    "start": "759279",
    "end": "761760"
  },
  {
    "text": "unfortunately",
    "start": "761760",
    "end": "763200"
  },
  {
    "text": "we don't have much time to go through",
    "start": "763200",
    "end": "764959"
  },
  {
    "text": "the authentication workflow",
    "start": "764959",
    "end": "766399"
  },
  {
    "text": "but luckily it uses the same",
    "start": "766399",
    "end": "768399"
  },
  {
    "text": "authenticate",
    "start": "768399",
    "end": "769600"
  },
  {
    "text": "authentication mechanism at linkedin",
    "start": "769600",
    "end": "771760"
  },
  {
    "text": "recently open source",
    "start": "771760",
    "end": "773200"
  },
  {
    "text": "project coupe to hadoop and now it is",
    "start": "773200",
    "end": "775760"
  },
  {
    "text": "available on github",
    "start": "775760",
    "end": "777279"
  },
  {
    "text": "please feel free reaching out to me if",
    "start": "777279",
    "end": "779440"
  },
  {
    "text": "you would like to know more",
    "start": "779440",
    "end": "782560"
  },
  {
    "start": "784000",
    "end": "855000"
  },
  {
    "text": "orchestration last is the orchestration",
    "start": "785040",
    "end": "788240"
  },
  {
    "text": "as i mentioned hadoop components have",
    "start": "788240",
    "end": "790880"
  },
  {
    "text": "have",
    "start": "790880",
    "end": "791279"
  },
  {
    "text": "hard dependencies in between for example",
    "start": "791279",
    "end": "793760"
  },
  {
    "text": "name node must",
    "start": "793760",
    "end": "794880"
  },
  {
    "text": "star after general nodes are up the",
    "start": "794880",
    "end": "797519"
  },
  {
    "text": "obvious solution",
    "start": "797519",
    "end": "798560"
  },
  {
    "text": "is to launching general node part first",
    "start": "798560",
    "end": "801200"
  },
  {
    "text": "then name node pause next",
    "start": "801200",
    "end": "802880"
  },
  {
    "text": "such approach usually results long",
    "start": "802880",
    "end": "805360"
  },
  {
    "text": "deployment time",
    "start": "805360",
    "end": "806800"
  },
  {
    "text": "and a complicated external logic to",
    "start": "806800",
    "end": "809519"
  },
  {
    "text": "orchestrate",
    "start": "809519",
    "end": "810399"
  },
  {
    "text": "deployment sequence what we come up with",
    "start": "810399",
    "end": "813440"
  },
  {
    "text": "is introducing a in a container",
    "start": "813440",
    "end": "816480"
  },
  {
    "text": "to constantly polling status",
    "start": "816480",
    "end": "819519"
  },
  {
    "text": "of depending service using service",
    "start": "819519",
    "end": "822480"
  },
  {
    "text": "discovery with part",
    "start": "822480",
    "end": "823920"
  },
  {
    "text": "env in a container what exists",
    "start": "823920",
    "end": "826720"
  },
  {
    "text": "successfully",
    "start": "826720",
    "end": "827600"
  },
  {
    "text": "when the depending service is up another",
    "start": "827600",
    "end": "829839"
  },
  {
    "text": "main container",
    "start": "829839",
    "end": "830880"
  },
  {
    "text": "can proceed with this implementation",
    "start": "830880",
    "end": "834000"
  },
  {
    "text": "we are able to launching all parts",
    "start": "834000",
    "end": "836639"
  },
  {
    "text": "simultaneously",
    "start": "836639",
    "end": "838000"
  },
  {
    "text": "and reduce cluster deployment time down",
    "start": "838000",
    "end": "840880"
  },
  {
    "text": "to two to three minutes",
    "start": "840880",
    "end": "843120"
  },
  {
    "text": "i include a example of in a container",
    "start": "843120",
    "end": "845600"
  },
  {
    "text": "here without holding the",
    "start": "845600",
    "end": "847199"
  },
  {
    "text": "status of hdfs journal node",
    "start": "847199",
    "end": "852079"
  },
  {
    "text": "so that is the end of our challenges and",
    "start": "852079",
    "end": "855360"
  },
  {
    "start": "855000",
    "end": "904000"
  },
  {
    "text": "what is the next so it is a great lesson",
    "start": "855360",
    "end": "858160"
  },
  {
    "text": "we learned",
    "start": "858160",
    "end": "858880"
  },
  {
    "text": "to having hadoop running on kubernetes",
    "start": "858880",
    "end": "861199"
  },
  {
    "text": "as testing and sandbox environment",
    "start": "861199",
    "end": "863440"
  },
  {
    "text": "we are almost done to have other big",
    "start": "863440",
    "end": "865440"
  },
  {
    "text": "data components",
    "start": "865440",
    "end": "866560"
  },
  {
    "text": "such as spark presto azkaban",
    "start": "866560",
    "end": "869839"
  },
  {
    "text": "as part of our ephemeral hadoop cluster",
    "start": "869839",
    "end": "872800"
  },
  {
    "text": "in meanwhile",
    "start": "872800",
    "end": "873839"
  },
  {
    "text": "we are also working on long-running",
    "start": "873839",
    "end": "875519"
  },
  {
    "text": "hadoop cluster on kubernetes",
    "start": "875519",
    "end": "877680"
  },
  {
    "text": "to replace integration environment that",
    "start": "877680",
    "end": "880320"
  },
  {
    "text": "is currently running on bare metals",
    "start": "880320",
    "end": "882560"
  },
  {
    "text": "and the number of issues we've",
    "start": "882560",
    "end": "884320"
  },
  {
    "text": "encountered are not less",
    "start": "884320",
    "end": "886160"
  },
  {
    "text": "but we do see 20 times faster deployment",
    "start": "886160",
    "end": "889519"
  },
  {
    "text": "speed",
    "start": "889519",
    "end": "890560"
  },
  {
    "text": "our next major goal will be kubernetes",
    "start": "890560",
    "end": "893839"
  },
  {
    "text": "custom",
    "start": "893839",
    "end": "894800"
  },
  {
    "text": "custom resources definition and hadoop",
    "start": "894800",
    "end": "897120"
  },
  {
    "text": "operator",
    "start": "897120",
    "end": "898160"
  },
  {
    "text": "hopefully we can open source those when",
    "start": "898160",
    "end": "900399"
  },
  {
    "text": "they are ready",
    "start": "900399",
    "end": "903040"
  },
  {
    "start": "904000",
    "end": "977000"
  },
  {
    "text": "the key takeaway for this presentation",
    "start": "904800",
    "end": "907360"
  },
  {
    "text": "will be",
    "start": "907360",
    "end": "907839"
  },
  {
    "text": "it opens the opportunity to combine",
    "start": "907839",
    "end": "910560"
  },
  {
    "text": "different workload",
    "start": "910560",
    "end": "911760"
  },
  {
    "text": "onto the same resources management",
    "start": "911760",
    "end": "914480"
  },
  {
    "text": "platform",
    "start": "914480",
    "end": "915360"
  },
  {
    "text": "i know a lot of company has separate",
    "start": "915360",
    "end": "917519"
  },
  {
    "text": "infrastructure",
    "start": "917519",
    "end": "918399"
  },
  {
    "text": "serving online and offline and the",
    "start": "918399",
    "end": "920880"
  },
  {
    "text": "online infrastructure usually idle or",
    "start": "920880",
    "end": "923040"
  },
  {
    "text": "less load",
    "start": "923040",
    "end": "923760"
  },
  {
    "text": "during off-peak hours leveraging those",
    "start": "923760",
    "end": "926560"
  },
  {
    "text": "idling resources",
    "start": "926560",
    "end": "927680"
  },
  {
    "text": "will easily save company millions of",
    "start": "927680",
    "end": "930240"
  },
  {
    "text": "hardware costs",
    "start": "930240",
    "end": "932000"
  },
  {
    "text": "second it may change the way how the big",
    "start": "932000",
    "end": "934720"
  },
  {
    "text": "data infrastructure are running",
    "start": "934720",
    "end": "936720"
  },
  {
    "text": "a lot of hadoop native distributed",
    "start": "936720",
    "end": "938839"
  },
  {
    "text": "framework are now",
    "start": "938839",
    "end": "940480"
  },
  {
    "text": "running natively on kubernetes for",
    "start": "940480",
    "end": "943120"
  },
  {
    "text": "example",
    "start": "943120",
    "end": "943920"
  },
  {
    "text": "spark and a linkedin open source samsa",
    "start": "943920",
    "end": "947199"
  },
  {
    "text": "and the last and",
    "start": "947199",
    "end": "950560"
  },
  {
    "text": "make hadoop to kubernetes transition",
    "start": "950560",
    "end": "952480"
  },
  {
    "text": "much easier which kind of resonant the",
    "start": "952480",
    "end": "954639"
  },
  {
    "text": "very first point",
    "start": "954639",
    "end": "957519"
  },
  {
    "text": "that is the end of the presentation",
    "start": "957920",
    "end": "961920"
  },
  {
    "text": "by the way there is another session",
    "start": "961920",
    "end": "963680"
  },
  {
    "text": "tomorrow presented by my colleagues aben",
    "start": "963680",
    "end": "966320"
  },
  {
    "text": "and",
    "start": "966320",
    "end": "966639"
  },
  {
    "text": "frank talking about petabyte scale",
    "start": "966639",
    "end": "969759"
  },
  {
    "text": "ai at linkedin so let's move uh jump",
    "start": "969759",
    "end": "973040"
  },
  {
    "text": "into",
    "start": "973040",
    "end": "973839"
  },
  {
    "text": "the demo session thank you",
    "start": "973839",
    "end": "978160"
  },
  {
    "start": "977000",
    "end": "1585000"
  },
  {
    "text": "a short video to demonstrate",
    "start": "980959",
    "end": "983680"
  },
  {
    "text": "bootstrapping a brand new ephemeral",
    "start": "983680",
    "end": "985759"
  },
  {
    "text": "hadoop cluster",
    "start": "985759",
    "end": "986880"
  },
  {
    "text": "including hdfs and a yarn on kubernetes",
    "start": "986880",
    "end": "990240"
  },
  {
    "text": "with overlay network",
    "start": "990240",
    "end": "993519"
  },
  {
    "text": "let's deploy a cluster now",
    "start": "994000",
    "end": "997120"
  },
  {
    "text": "cluster name is cryptcon 2020",
    "start": "997120",
    "end": "1001120"
  },
  {
    "text": "demo deploy",
    "start": "1001120",
    "end": "1004800"
  },
  {
    "text": "this is a wrapper and it does three",
    "start": "1006079",
    "end": "1008480"
  },
  {
    "text": "things first",
    "start": "1008480",
    "end": "1009440"
  },
  {
    "text": "it uses python ginger to templating",
    "start": "1009440",
    "end": "1011680"
  },
  {
    "text": "engine to",
    "start": "1011680",
    "end": "1012959"
  },
  {
    "text": "render many yaml files including pod",
    "start": "1012959",
    "end": "1016079"
  },
  {
    "text": "service stay for set definitions and",
    "start": "1016079",
    "end": "1019199"
  },
  {
    "text": "config maps second it does yaml file",
    "start": "1019199",
    "end": "1022240"
  },
  {
    "text": "validation",
    "start": "1022240",
    "end": "1023040"
  },
  {
    "text": "by yaml linked and last is to apply",
    "start": "1023040",
    "end": "1025839"
  },
  {
    "text": "those yaml files",
    "start": "1025839",
    "end": "1026959"
  },
  {
    "text": "onto kubernetes with a specific order",
    "start": "1026959",
    "end": "1030319"
  },
  {
    "text": "it starts with service then config map",
    "start": "1030319",
    "end": "1033438"
  },
  {
    "text": "and the last part state4 set each part",
    "start": "1033439",
    "end": "1036798"
  },
  {
    "text": "has at least",
    "start": "1036799",
    "end": "1037678"
  },
  {
    "text": "one in a container to fetch kerberos",
    "start": "1037679",
    "end": "1040880"
  },
  {
    "text": "key tab and some parts have",
    "start": "1040880",
    "end": "1044319"
  },
  {
    "text": "one additional in a container for",
    "start": "1044319",
    "end": "1046480"
  },
  {
    "text": "dependency check",
    "start": "1046480",
    "end": "1047839"
  },
  {
    "text": "for example in hdfs the name node",
    "start": "1047839",
    "end": "1051039"
  },
  {
    "text": "should start after three general nodes",
    "start": "1051039",
    "end": "1053760"
  },
  {
    "text": "are up and running",
    "start": "1053760",
    "end": "1055440"
  },
  {
    "text": "i have successfully applied all the yaml",
    "start": "1055440",
    "end": "1057840"
  },
  {
    "text": "files let's checking the part",
    "start": "1057840",
    "end": "1061520"
  },
  {
    "text": "let's check the config map",
    "start": "1062559",
    "end": "1067840"
  },
  {
    "text": "and let's check the service",
    "start": "1068320",
    "end": "1072080"
  },
  {
    "text": "they're all newly created let's back to",
    "start": "1073760",
    "end": "1077200"
  },
  {
    "text": "the part again",
    "start": "1077200",
    "end": "1081840"
  },
  {
    "text": "just under 110 seconds we have",
    "start": "1087200",
    "end": "1090559"
  },
  {
    "text": "15 parts up and running among those",
    "start": "1090559",
    "end": "1094320"
  },
  {
    "text": "15 parts eight of them for hdfs",
    "start": "1094320",
    "end": "1098000"
  },
  {
    "text": "and six of them for yarn",
    "start": "1098000",
    "end": "1101360"
  },
  {
    "text": "hdfs including three general notes",
    "start": "1101360",
    "end": "1104799"
  },
  {
    "text": "two name notes and three workers yarn",
    "start": "1104799",
    "end": "1107840"
  },
  {
    "text": "has",
    "start": "1107840",
    "end": "1108240"
  },
  {
    "text": "one resources manager one proxy server",
    "start": "1108240",
    "end": "1111039"
  },
  {
    "text": "one history server",
    "start": "1111039",
    "end": "1112320"
  },
  {
    "text": "along with three node managers",
    "start": "1112320",
    "end": "1115360"
  },
  {
    "text": "we have one more pod which is",
    "start": "1115360",
    "end": "1118559"
  },
  {
    "text": "the gateway we are using that to",
    "start": "1118559",
    "end": "1120880"
  },
  {
    "text": "interact with our",
    "start": "1120880",
    "end": "1122720"
  },
  {
    "text": "cluster run let's check our web ui",
    "start": "1122720",
    "end": "1127840"
  },
  {
    "text": "here is the job history web ui so it",
    "start": "1129600",
    "end": "1132720"
  },
  {
    "text": "archived",
    "start": "1132720",
    "end": "1133760"
  },
  {
    "text": "all the completed jobs",
    "start": "1133760",
    "end": "1137840"
  },
  {
    "text": "the resources manager web ui and we have",
    "start": "1137840",
    "end": "1140960"
  },
  {
    "text": "three",
    "start": "1140960",
    "end": "1141520"
  },
  {
    "text": "active nodes",
    "start": "1141520",
    "end": "1145200"
  },
  {
    "text": "and here is our name node 2",
    "start": "1145679",
    "end": "1149600"
  },
  {
    "text": "and name node 1 and they",
    "start": "1149600",
    "end": "1153039"
  },
  {
    "text": "are one active and one stand by",
    "start": "1153039",
    "end": "1157840"
  },
  {
    "text": "as i mentioned we have two name nodes",
    "start": "1159039",
    "end": "1161440"
  },
  {
    "text": "here for high availability",
    "start": "1161440",
    "end": "1163280"
  },
  {
    "text": "one is active and the other is standby",
    "start": "1163280",
    "end": "1166799"
  },
  {
    "text": "we are able to fail over between them",
    "start": "1166799",
    "end": "1169840"
  },
  {
    "text": "and all the client will automatically",
    "start": "1169840",
    "end": "1172880"
  },
  {
    "text": "connect to",
    "start": "1172880",
    "end": "1173679"
  },
  {
    "text": "the active name node for hdfs",
    "start": "1173679",
    "end": "1176080"
  },
  {
    "text": "transaction",
    "start": "1176080",
    "end": "1177440"
  },
  {
    "text": "during our startup process we have fell",
    "start": "1177440",
    "end": "1180880"
  },
  {
    "text": "over testing to making sure that it",
    "start": "1180880",
    "end": "1183679"
  },
  {
    "text": "works",
    "start": "1183679",
    "end": "1184720"
  },
  {
    "text": "now we have our hadoop cluster running",
    "start": "1184720",
    "end": "1188240"
  },
  {
    "text": "let's interact with the cluster using",
    "start": "1188240",
    "end": "1191120"
  },
  {
    "text": "our gateway",
    "start": "1191120",
    "end": "1193840"
  },
  {
    "text": "pod",
    "start": "1204840",
    "end": "1207840"
  },
  {
    "text": "let me become myself",
    "start": "1210799",
    "end": "1214399"
  },
  {
    "text": "so i'm accessing my home directory on",
    "start": "1216320",
    "end": "1219200"
  },
  {
    "text": "hdfs",
    "start": "1219200",
    "end": "1221840"
  },
  {
    "text": "and i got gss initiate failed because",
    "start": "1223360",
    "end": "1226480"
  },
  {
    "text": "we are running secure hadoop cluster and",
    "start": "1226480",
    "end": "1229120"
  },
  {
    "text": "we require",
    "start": "1229120",
    "end": "1230559"
  },
  {
    "text": "a valid kerberos tgt",
    "start": "1230559",
    "end": "1235840"
  },
  {
    "text": "so once i have a valid tgt",
    "start": "1238480",
    "end": "1241679"
  },
  {
    "text": "i'm able to browse my file system",
    "start": "1241679",
    "end": "1247279"
  },
  {
    "text": "so right now i don't have any files in",
    "start": "1247760",
    "end": "1249840"
  },
  {
    "text": "my home",
    "start": "1249840",
    "end": "1250799"
  },
  {
    "text": "directories let's touch a file",
    "start": "1250799",
    "end": "1263840"
  },
  {
    "text": "so i create a new text file called",
    "start": "1271200",
    "end": "1273280"
  },
  {
    "text": "coopercon 2020-demo.txt",
    "start": "1273280",
    "end": "1276960"
  },
  {
    "text": "that's uploading this file to",
    "start": "1276960",
    "end": "1280559"
  },
  {
    "text": "my home directory in hdfs",
    "start": "1280559",
    "end": "1291840"
  },
  {
    "text": "let's do a list again",
    "start": "1292720",
    "end": "1296158"
  },
  {
    "text": "as you can see my file has been",
    "start": "1299600",
    "end": "1301520"
  },
  {
    "text": "successfully",
    "start": "1301520",
    "end": "1303039"
  },
  {
    "text": "uploaded to hdfs",
    "start": "1303039",
    "end": "1306640"
  },
  {
    "text": "now let's submitting a job",
    "start": "1306640",
    "end": "1319840"
  },
  {
    "text": "it is a very simple pie calculation",
    "start": "1321919",
    "end": "1325280"
  },
  {
    "text": "job",
    "start": "1325280",
    "end": "1327520"
  },
  {
    "text": "usually this job should take no more",
    "start": "1328720",
    "end": "1330960"
  },
  {
    "text": "than 30 seconds",
    "start": "1330960",
    "end": "1332720"
  },
  {
    "text": "as you can see we have successfully",
    "start": "1332720",
    "end": "1334720"
  },
  {
    "text": "submitting the job to",
    "start": "1334720",
    "end": "1336000"
  },
  {
    "text": "our resources manager let's go the",
    "start": "1336000",
    "end": "1338799"
  },
  {
    "text": "resource manager page and",
    "start": "1338799",
    "end": "1340240"
  },
  {
    "text": "refresh here as you can see in my job",
    "start": "1340240",
    "end": "1343200"
  },
  {
    "text": "here's my job id",
    "start": "1343200",
    "end": "1344559"
  },
  {
    "text": "my username job name start time launch",
    "start": "1344559",
    "end": "1347039"
  },
  {
    "text": "time and",
    "start": "1347039",
    "end": "1347600"
  },
  {
    "text": "the state is running",
    "start": "1347600",
    "end": "1350880"
  },
  {
    "text": "and the stdl from the job submission is",
    "start": "1350880",
    "end": "1354000"
  },
  {
    "text": "constantly getting",
    "start": "1354000",
    "end": "1355760"
  },
  {
    "text": "is constantly getting updated with the",
    "start": "1355760",
    "end": "1357840"
  },
  {
    "text": "status and",
    "start": "1357840",
    "end": "1359520"
  },
  {
    "text": "here you go the job finish just under",
    "start": "1359520",
    "end": "1363240"
  },
  {
    "text": "27.5 seconds",
    "start": "1363240",
    "end": "1366320"
  },
  {
    "text": "let's refresh the page again as you can",
    "start": "1366320",
    "end": "1368559"
  },
  {
    "text": "see our job",
    "start": "1368559",
    "end": "1369760"
  },
  {
    "text": "now is in finished state let's go to the",
    "start": "1369760",
    "end": "1373600"
  },
  {
    "text": "job history",
    "start": "1373600",
    "end": "1374480"
  },
  {
    "text": "server so because this job is",
    "start": "1374480",
    "end": "1378320"
  },
  {
    "text": "a completed job that's why it's showing",
    "start": "1378320",
    "end": "1380480"
  },
  {
    "text": "up in our job history server",
    "start": "1380480",
    "end": "1382640"
  },
  {
    "text": "in job history server i can dig into",
    "start": "1382640",
    "end": "1386000"
  },
  {
    "text": "more details with the job like for",
    "start": "1386000",
    "end": "1387760"
  },
  {
    "text": "example i can see the log",
    "start": "1387760",
    "end": "1389840"
  },
  {
    "text": "for each running task right here",
    "start": "1389840",
    "end": "1393919"
  },
  {
    "text": "that can go logs that's all the logs",
    "start": "1393919",
    "end": "1397200"
  },
  {
    "text": "here",
    "start": "1397200",
    "end": "1399519"
  },
  {
    "text": "okay",
    "start": "1400840",
    "end": "1403840"
  },
  {
    "text": "the cluster initially started with three",
    "start": "1405840",
    "end": "1408400"
  },
  {
    "text": "workers",
    "start": "1408400",
    "end": "1409039"
  },
  {
    "text": "and in some use cases it required more",
    "start": "1409039",
    "end": "1412240"
  },
  {
    "text": "resources",
    "start": "1412240",
    "end": "1413120"
  },
  {
    "text": "to run large workload since worker nodes",
    "start": "1413120",
    "end": "1416720"
  },
  {
    "text": "are defined as stateful set we can",
    "start": "1416720",
    "end": "1419440"
  },
  {
    "text": "easily expanding them",
    "start": "1419440",
    "end": "1422720"
  },
  {
    "text": "cto scale replicas",
    "start": "1424000",
    "end": "1427600"
  },
  {
    "text": "equals to 10",
    "start": "1427600",
    "end": "1430640"
  },
  {
    "text": "state 4 set coupon",
    "start": "1430640",
    "end": "1434400"
  },
  {
    "text": "2020 demo",
    "start": "1434400",
    "end": "1438080"
  },
  {
    "text": "so we are expanding our hdfs data node",
    "start": "1438559",
    "end": "1442240"
  },
  {
    "text": "from",
    "start": "1442240",
    "end": "1442720"
  },
  {
    "text": "three parts to 10 parts",
    "start": "1442720",
    "end": "1446720"
  },
  {
    "text": "we are doing the same for the yarn no",
    "start": "1448559",
    "end": "1450480"
  },
  {
    "text": "manager",
    "start": "1450480",
    "end": "1452960"
  },
  {
    "text": "let's check in the pod",
    "start": "1453840",
    "end": "1459840"
  },
  {
    "text": "data nodes and node managers are spawned",
    "start": "1460320",
    "end": "1463679"
  },
  {
    "text": "in kubernetes cluster",
    "start": "1463679",
    "end": "1465360"
  },
  {
    "text": "and they will join automatically to yarn",
    "start": "1465360",
    "end": "1468880"
  },
  {
    "text": "and hdfs in next 30 seconds",
    "start": "1468880",
    "end": "1473840"
  },
  {
    "text": "under 30 seconds we have the additional",
    "start": "1475600",
    "end": "1478080"
  },
  {
    "text": "data nodes up and running and",
    "start": "1478080",
    "end": "1480320"
  },
  {
    "text": "as well as the additional no managers up",
    "start": "1480320",
    "end": "1483039"
  },
  {
    "text": "running let's go back to the web uin to",
    "start": "1483039",
    "end": "1485039"
  },
  {
    "text": "see",
    "start": "1485039",
    "end": "1485679"
  },
  {
    "text": "whether those nodes are joined our",
    "start": "1485679",
    "end": "1488000"
  },
  {
    "text": "cluster",
    "start": "1488000",
    "end": "1490480"
  },
  {
    "text": "to refresh the page okay so active nodes",
    "start": "1490960",
    "end": "1494799"
  },
  {
    "text": "from",
    "start": "1494799",
    "end": "1495360"
  },
  {
    "text": "3 to 10. check the hdfs",
    "start": "1495360",
    "end": "1501600"
  },
  {
    "text": "it also increased from 3 to",
    "start": "1501600",
    "end": "1504799"
  },
  {
    "text": "10 nodes usually when we're doing",
    "start": "1504799",
    "end": "1507760"
  },
  {
    "text": "expansion",
    "start": "1507760",
    "end": "1509360"
  },
  {
    "text": "in our data center with uh with the bare",
    "start": "1509360",
    "end": "1513039"
  },
  {
    "text": "metal setup",
    "start": "1513039",
    "end": "1514000"
  },
  {
    "text": "it usually take about hours to expand",
    "start": "1514000",
    "end": "1518320"
  },
  {
    "text": "and here we are talking about 30 seconds",
    "start": "1518320",
    "end": "1522640"
  },
  {
    "text": "okay after the testing is done cluster",
    "start": "1522640",
    "end": "1526080"
  },
  {
    "text": "can be easily",
    "start": "1526080",
    "end": "1527039"
  },
  {
    "text": "torn down",
    "start": "1527039",
    "end": "1529840"
  },
  {
    "text": "it deletes all the part stay for set",
    "start": "1539039",
    "end": "1542960"
  },
  {
    "text": "config map and services",
    "start": "1542960",
    "end": "1546559"
  },
  {
    "text": "in next minute or two other part",
    "start": "1546559",
    "end": "1549919"
  },
  {
    "text": "will be terminated",
    "start": "1549919",
    "end": "1557840"
  },
  {
    "text": "this concludes the video if you have any",
    "start": "1574640",
    "end": "1577840"
  },
  {
    "text": "questions please join the question",
    "start": "1577840",
    "end": "1580320"
  },
  {
    "text": "session",
    "start": "1580320",
    "end": "1580880"
  },
  {
    "text": "and i'm more than happy to answer thank",
    "start": "1580880",
    "end": "1583520"
  },
  {
    "text": "you",
    "start": "1583520",
    "end": "1585840"
  }
]