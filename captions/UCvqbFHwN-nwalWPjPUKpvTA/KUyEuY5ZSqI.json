[
  {
    "start": "0",
    "end": "108000"
  },
  {
    "text": "hello everyone and welcome to the cubicle talk on scaling keep flow for multi tenancy at spotify today jonathan",
    "start": "1439",
    "end": "9200"
  },
  {
    "text": "and i are going to share the story on how we scale up our cube flow platform to serve the growing number of ml teams",
    "start": "9200",
    "end": "15920"
  },
  {
    "text": "at spotify let's start with the introduction my name is cash dai i'm a senior ml info",
    "start": "15920",
    "end": "23600"
  },
  {
    "text": "engineer at spotify this is my teammate jonathan jean also a senior engineer you",
    "start": "23600",
    "end": "29359"
  },
  {
    "text": "will hear from him during the later part of this talk",
    "start": "29359",
    "end": "34160"
  },
  {
    "text": "so we are spotify if you haven't heard of us before we an audio streaming",
    "start": "34800",
    "end": "39920"
  },
  {
    "text": "service we launched in 2008 and now we have more than 365 million monthly active users",
    "start": "39920",
    "end": "48079"
  },
  {
    "text": "on our platform we have more than 70 million tracks and almost 3 million podcast titles",
    "start": "48079",
    "end": "54960"
  },
  {
    "text": "and our service is available in 178 markets all over the world",
    "start": "54960",
    "end": "61358"
  },
  {
    "text": "machine learning is at the heart of almost everything we do at spotify including recommending personalized",
    "start": "62559",
    "end": "68720"
  },
  {
    "text": "content on home page optimizing the ranking results from the search or",
    "start": "68720",
    "end": "73840"
  },
  {
    "text": "helping you discover and explore the music you haven't listened to before",
    "start": "73840",
    "end": "79119"
  },
  {
    "text": "it enables us to recommend artists playlists and podcasts to keep our users",
    "start": "79119",
    "end": "85439"
  },
  {
    "text": "active engaged and more likely to subscribe in the long term",
    "start": "85439",
    "end": "90640"
  },
  {
    "text": "to power ml products at spotify our team is building a standardized machine learning platform to provide our",
    "start": "90640",
    "end": "97840"
  },
  {
    "text": "engineers with tools and environment to quickly prototype experiment and",
    "start": "97840",
    "end": "102960"
  },
  {
    "text": "productionize their ml ideas",
    "start": "102960",
    "end": "107118"
  },
  {
    "start": "108000",
    "end": "108000"
  },
  {
    "text": "our platform internally known as qfloor platform consists of two major components",
    "start": "108720",
    "end": "114640"
  },
  {
    "text": "a python sdk for building ml workflow with tfx components and several managed kubeflow gke",
    "start": "114640",
    "end": "121920"
  },
  {
    "text": "clusters for mr pipeline executions for those who don't know tfx tensorflow",
    "start": "121920",
    "end": "128399"
  },
  {
    "text": "extended is a component-based ml framework around tens flow ecosystems",
    "start": "128399",
    "end": "134480"
  },
  {
    "text": "qflow is a set of machine learning toolkits on top of kubernetes on our platform we mainly use kubeflow",
    "start": "134480",
    "end": "142000"
  },
  {
    "text": "pipelines to orchestrate ml workflows built with tfx",
    "start": "142000",
    "end": "147520"
  },
  {
    "start": "148000",
    "end": "148000"
  },
  {
    "text": "this is a typical ml workflow and spotify as you can see it has a sequence of components representing different",
    "start": "148239",
    "end": "155200"
  },
  {
    "text": "steps in a machine learning pipeline it starts with future engineering future",
    "start": "155200",
    "end": "160400"
  },
  {
    "text": "collector and transform components assemble the raw features and transform them into the meaningful ones for model",
    "start": "160400",
    "end": "167360"
  },
  {
    "text": "training next we have stats and schema generation components to validate those features",
    "start": "167360",
    "end": "173760"
  },
  {
    "text": "and produce a schema file based on the future data stats",
    "start": "173760",
    "end": "178959"
  },
  {
    "text": "then we have the trainer component for model training and evaluate component for the model performance analysis",
    "start": "178959",
    "end": "185840"
  },
  {
    "text": "in the end hopefully everything looks good then we can deploy the model to production through the deployer",
    "start": "185840",
    "end": "192640"
  },
  {
    "text": "but of course in reality an actual ml pipeline can be much more complicated",
    "start": "192640",
    "end": "197680"
  },
  {
    "text": "than this example it also includes user custom components containing different",
    "start": "197680",
    "end": "202800"
  },
  {
    "text": "business logics after pipeline is authored teams can submit pipeline to our gk",
    "start": "202800",
    "end": "209920"
  },
  {
    "text": "cluster and it will get executed through keyflow pipelines on our cluster we installed sdo for",
    "start": "209920",
    "end": "216959"
  },
  {
    "text": "service discovery and authentication and now metadata start to track tfx",
    "start": "216959",
    "end": "222799"
  },
  {
    "text": "component executions and our own toolings for team management and metric monitoring",
    "start": "222799",
    "end": "229760"
  },
  {
    "text": "this q flow platform is extremely valuable since it helped us better manage our ml workloads and accelerates",
    "start": "229760",
    "end": "237200"
  },
  {
    "text": "the pace of model experimentation and rollout",
    "start": "237200",
    "end": "242239"
  },
  {
    "text": "after we understand a little bit more about how ml is done in spotify let's focus on the kubernetes part our cube",
    "start": "242879",
    "end": "250080"
  },
  {
    "text": "flow clusters for those who attended kubecon north america 2019",
    "start": "250080",
    "end": "256079"
  },
  {
    "text": "you might remember we shared our story on how we build and manage the cookbook clusters we first launched platform",
    "start": "256079",
    "end": "263759"
  },
  {
    "text": "let's do a quick recap here before i start diving into details i",
    "start": "263759",
    "end": "270960"
  },
  {
    "text": "want to remind everyone that everything we do at spotify is on google cloud because of this we are able to make a",
    "start": "270960",
    "end": "278720"
  },
  {
    "text": "lot of assumptions and utilize many google cloud services when we build our platform",
    "start": "278720",
    "end": "286639"
  },
  {
    "start": "288000",
    "end": "288000"
  },
  {
    "text": "our qr4 clusters are built through the same process the gtp resources are",
    "start": "288400",
    "end": "293520"
  },
  {
    "text": "created and managed by the terraform including gk clusters cloud sql",
    "start": "293520",
    "end": "298720"
  },
  {
    "text": "instances service accounts and more terraform is a tool for building",
    "start": "298720",
    "end": "303919"
  },
  {
    "text": "changing and versioning the infrastructure so the infrastructure can be traded as code",
    "start": "303919",
    "end": "309440"
  },
  {
    "text": "on the other hand queue flow related kubernetes resources are organized by customize and they are",
    "start": "309440",
    "end": "316080"
  },
  {
    "text": "deployed through the cube cutter customize decomposes resource files into",
    "start": "316080",
    "end": "321199"
  },
  {
    "text": "base and overlay files which allows us to create a customization layer on top of the open source solution",
    "start": "321199",
    "end": "329599"
  },
  {
    "start": "330000",
    "end": "330000"
  },
  {
    "text": "we first launched our platform in for 2019 and we had a beta release in 2020",
    "start": "330000",
    "end": "336720"
  },
  {
    "text": "we will go ga in early next year so far we have more than 60 teams and almost",
    "start": "336720",
    "end": "342320"
  },
  {
    "text": "600 users using our platform there were thirty thousand models trained and a hundred thousand pipeline",
    "start": "342320",
    "end": "349120"
  },
  {
    "text": "execution hours during our platform last year in average there are close 300 pipeline",
    "start": "349120",
    "end": "355039"
  },
  {
    "text": "runs every day it has been a great journey for us to",
    "start": "355039",
    "end": "360560"
  },
  {
    "text": "witness our platform's growth and be part of it in the following section jonathan and i",
    "start": "360560",
    "end": "366560"
  },
  {
    "text": "are going to talk about the challenges we are facing while we scale up our platform and how we address those",
    "start": "366560",
    "end": "372639"
  },
  {
    "text": "challenges more specifically we are going to cover",
    "start": "372639",
    "end": "378160"
  },
  {
    "start": "375000",
    "end": "375000"
  },
  {
    "text": "how do we support a growing number of ml teams on our platform to allow them to",
    "start": "378160",
    "end": "383199"
  },
  {
    "text": "operate in isolated and self-manageable environment second how do we deal with upstream",
    "start": "383199",
    "end": "390080"
  },
  {
    "text": "breaking changes by using the multi-class strategy then we are also going to cover our get",
    "start": "390080",
    "end": "396720"
  },
  {
    "text": "ops framework for continuous deployment to manage multiple clusters and complex",
    "start": "396720",
    "end": "401919"
  },
  {
    "text": "deployments in the end we will talk about cluster observability for targeted and uniform",
    "start": "401919",
    "end": "408560"
  },
  {
    "text": "reliability across multiple clusters let's start with team-based multi-tenant",
    "start": "408560",
    "end": "415840"
  },
  {
    "text": "spot when we started working with kubeflow there's no margin tendencies part",
    "start": "415840",
    "end": "422560"
  },
  {
    "start": "418000",
    "end": "418000"
  },
  {
    "text": "everything was running on a single namespace including kubeflow services and actual user pipelines in early 2020",
    "start": "422560",
    "end": "430479"
  },
  {
    "text": "multi-tenancy was introduced in qfloor pipelines and we were one of the first teams that worked with google to adopt",
    "start": "430479",
    "end": "437280"
  },
  {
    "text": "it into our own platform it's based on the open source version of qpro profile component",
    "start": "437280",
    "end": "444319"
  },
  {
    "text": "each profile corresponds to a namespace on a cluster and you can configure the owner contributors and service count",
    "start": "444319",
    "end": "451520"
  },
  {
    "text": "used for the namespace although it provided basic multi-tenancy",
    "start": "451520",
    "end": "456639"
  },
  {
    "text": "features it came with several drawbacks that didn't work well with our internal",
    "start": "456639",
    "end": "461840"
  },
  {
    "text": "team structure first the keyframe profiles owner has to be a real user but not a team represented by",
    "start": "461840",
    "end": "469520"
  },
  {
    "text": "group email this is not ideal because people are moving among teams",
    "start": "469520",
    "end": "475360"
  },
  {
    "text": "if the owner leaves the team the namespace needs to be reconstructed",
    "start": "475360",
    "end": "480800"
  },
  {
    "text": "second the contributor of a namespace has to be managed manually this is no",
    "start": "480800",
    "end": "486479"
  },
  {
    "text": "standard process to do that if we build a new cluster the owner of a namespace",
    "start": "486479",
    "end": "491840"
  },
  {
    "text": "would have to configure all contributors again third it doesn't support google group",
    "start": "491840",
    "end": "498479"
  },
  {
    "text": "and all contributors need to be added individually this is tedious especially",
    "start": "498479",
    "end": "503680"
  },
  {
    "text": "if you have a big team at spotify hmr team operates in its own",
    "start": "503680",
    "end": "509759"
  },
  {
    "text": "gcp project members in the team share the same access to the data and resources in the",
    "start": "509759",
    "end": "516000"
  },
  {
    "text": "project we need a team-based support in our cubeflow cluster",
    "start": "516000",
    "end": "522560"
  },
  {
    "text": "so that everyone in the team can have the access as soon as the namespace is created",
    "start": "522560",
    "end": "528399"
  },
  {
    "text": "meanwhile with this setup we can also conveniently obtain the",
    "start": "528399",
    "end": "533680"
  },
  {
    "text": "insights regarding team level operational metrics and resource consumptions to better understand and",
    "start": "533680",
    "end": "539839"
  },
  {
    "text": "manage our platform with this in mind we have developed our",
    "start": "539839",
    "end": "546399"
  },
  {
    "start": "543000",
    "end": "543000"
  },
  {
    "text": "own team management tooling for kubeflow cluster in our setup each team profile or",
    "start": "546399",
    "end": "552160"
  },
  {
    "text": "namespace is owned by a google group that corresponds to an internal ldap group",
    "start": "552160",
    "end": "557600"
  },
  {
    "text": "which removes the need of an individual user being the owner to support the team",
    "start": "557600",
    "end": "563120"
  },
  {
    "text": "concept in the kubeflow profile we create the customer resource definition to define our back and sto",
    "start": "563120",
    "end": "569600"
  },
  {
    "text": "rules for team members and additional contributors we also created a corresponding",
    "start": "569600",
    "end": "576160"
  },
  {
    "text": "kubernetes controller to support google group so it automatically extracts members",
    "start": "576160",
    "end": "581600"
  },
  {
    "text": "from the group and configures necessary permissions for them the controller also periodically syncs",
    "start": "581600",
    "end": "588160"
  },
  {
    "text": "with google group service so any changes in the team will be automatically reflect on cluster",
    "start": "588160",
    "end": "596000"
  },
  {
    "text": "lastly we use customize to manage our team configs and deploy them through githubs",
    "start": "596000",
    "end": "602079"
  },
  {
    "text": "an engineer in a team can submit a pull request to repo where all the profiles",
    "start": "602079",
    "end": "607200"
  },
  {
    "text": "are stored then the cd runs customize and verify the changes",
    "start": "607200",
    "end": "612959"
  },
  {
    "text": "once the pi is merged the master building invokes customize to render resources and deploy them to the",
    "start": "612959",
    "end": "619360"
  },
  {
    "text": "clusters this process also allows us to easily",
    "start": "619360",
    "end": "624480"
  },
  {
    "text": "reproduce and apply the same team namespace setup to a different cluster",
    "start": "624480",
    "end": "630399"
  },
  {
    "text": "we just simply need to add it to a list of clusters defined in a ci cd",
    "start": "630399",
    "end": "636640"
  },
  {
    "start": "637000",
    "end": "637000"
  },
  {
    "text": "let's take a look of an actual team profile example the name field defines design name for",
    "start": "637120",
    "end": "644160"
  },
  {
    "text": "the team profile and the namespace the group email field should be a group",
    "start": "644160",
    "end": "649839"
  },
  {
    "text": "email for the team that owns the profile the profile also requires gcp service",
    "start": "649839",
    "end": "656640"
  },
  {
    "text": "account to access datasets and gcp resources from its namespace it's bounded with a kubernetes service",
    "start": "656640",
    "end": "663600"
  },
  {
    "text": "account through gke workload identity contribute field defines a list of",
    "start": "663600",
    "end": "669680"
  },
  {
    "text": "additional users the team would like to give access to they can be individuals or groups",
    "start": "669680",
    "end": "676959"
  },
  {
    "text": "the last of quota override field allows team to override the default resource quota defined by the platform team but",
    "start": "677279",
    "end": "684240"
  },
  {
    "text": "is subjected to the administrator's approval",
    "start": "684240",
    "end": "689160"
  },
  {
    "text": "after team configuration is defined our tool automatically converts it into a",
    "start": "689519",
    "end": "694800"
  },
  {
    "text": "set of kubernetes resource files as you can see on the right side it has",
    "start": "694800",
    "end": "700079"
  },
  {
    "text": "a cube flow profile yaml and the example namespace folder in the namespace folder there are crd",
    "start": "700079",
    "end": "707200"
  },
  {
    "text": "resource file that defines outback and sto rules for team members and contributors",
    "start": "707200",
    "end": "713600"
  },
  {
    "text": "our controller will pick it up and set the permissions accordingly after it's deployed",
    "start": "713600",
    "end": "720160"
  },
  {
    "text": "it also has a limited range file that defines a defaulted resource request and limit for containers running in that",
    "start": "720160",
    "end": "727360"
  },
  {
    "text": "namespace meanwhile customization file assembles different",
    "start": "727360",
    "end": "733120"
  },
  {
    "text": "resource manifests needed for their namespace so they can be deployed altogether",
    "start": "733120",
    "end": "739600"
  },
  {
    "text": "this flexible structure also allows users to add more custom resources to",
    "start": "739600",
    "end": "745120"
  },
  {
    "text": "the team namespace if they are needed in the future",
    "start": "745120",
    "end": "749839"
  },
  {
    "text": "another topic i would like to talk about is our new kubeflow multi-class strategy",
    "start": "750560",
    "end": "756959"
  },
  {
    "text": "since the landscape of ml is involving so fast and we are constantly dealing",
    "start": "756959",
    "end": "762240"
  },
  {
    "text": "with the infrastructure upgrades as well as the braking changes coming with them",
    "start": "762240",
    "end": "768480"
  },
  {
    "start": "769000",
    "end": "769000"
  },
  {
    "text": "our initial platform setup consists of three clusters we have an experimental cluster for",
    "start": "769120",
    "end": "775120"
  },
  {
    "text": "internal infrastructure prototyping and testing a product cluster is for production",
    "start": "775120",
    "end": "780320"
  },
  {
    "text": "pipelines as well as ad hoc ml workloads we also have a dev cluster intended for",
    "start": "780320",
    "end": "786480"
  },
  {
    "text": "platform developers but it also serves as backup cluster when the product is under upgrade or maintainers",
    "start": "786480",
    "end": "794160"
  },
  {
    "text": "this setup makes rolling out a new version of queue flow painful and slow we usually first test it internally on",
    "start": "794160",
    "end": "801519"
  },
  {
    "text": "our experimental cluster then apply the changes to the dev and then wait for our users to upgrade the pipeline before we",
    "start": "801519",
    "end": "808880"
  },
  {
    "text": "could eventually roll it out to the proud the process usually takes weeks to complete",
    "start": "808880",
    "end": "815120"
  },
  {
    "text": "to support the roll out sometimes we need to handle breaking changes in our client sdk or install",
    "start": "815120",
    "end": "822160"
  },
  {
    "text": "multiple versions of qfloor services on cluster for the backward compatibility",
    "start": "822160",
    "end": "828160"
  },
  {
    "text": "on the other hand during a final production rollout we were under pressure to complete the upgrade asap to",
    "start": "828160",
    "end": "835760"
  },
  {
    "text": "minimize the user interruption even worse this approach forced the user",
    "start": "835760",
    "end": "841600"
  },
  {
    "text": "to upgrade their ml pipelines when we upgrade the infrastructure so teams couldn't choose to migrate based on",
    "start": "841600",
    "end": "848399"
  },
  {
    "text": "their own schedules as as a result we have implemented a",
    "start": "848399",
    "end": "854639"
  },
  {
    "start": "852000",
    "end": "852000"
  },
  {
    "text": "multi-class strategy for our kubeflow platform since our platform is made of a python",
    "start": "854639",
    "end": "860880"
  },
  {
    "text": "sdk and a manager cluster we decided to version our infrastructure along with",
    "start": "860880",
    "end": "866480"
  },
  {
    "text": "the sdk the pipeline is guaranteed to be able to run on a cluster with the same major",
    "start": "866480",
    "end": "873040"
  },
  {
    "text": "version of the sdks that views it the cluster installs seller of services",
    "start": "873040",
    "end": "878639"
  },
  {
    "text": "with a version compatible with each other as well as client sdk as shown in",
    "start": "878639",
    "end": "884000"
  },
  {
    "text": "the illustration suppose we are committed to support three major versions",
    "start": "884000",
    "end": "889360"
  },
  {
    "text": "we will have three dedicated clusters the pipeline is always submitted to the",
    "start": "889360",
    "end": "894399"
  },
  {
    "text": "cluster with the same major version in addition we have an extra on-demand",
    "start": "894399",
    "end": "899839"
  },
  {
    "text": "cluster as the backup configured through the terraform so it can be quickly booted up if one of our clusters goes",
    "start": "899839",
    "end": "907600"
  },
  {
    "text": "down so why is this better it first",
    "start": "907600",
    "end": "912800"
  },
  {
    "start": "909000",
    "end": "909000"
  },
  {
    "text": "encapsulates the entire q flow stack from the pipeline sdk to the execution",
    "start": "912800",
    "end": "917839"
  },
  {
    "text": "engine to ensure compatibility second it decouples the instructor upgrade cadence",
    "start": "917839",
    "end": "923920"
  },
  {
    "text": "from the pipeline upgrade cadence users can upgrade their pipelines based on their own priorities and schedules",
    "start": "923920",
    "end": "932639"
  },
  {
    "text": "it also well defines the platform's supporting scope users will have to upgrade their",
    "start": "932639",
    "end": "937920"
  },
  {
    "text": "pipelines before we deprecate the old cluster so we can clearly communicate with our users",
    "start": "937920",
    "end": "944800"
  },
  {
    "text": "last but not least we will no longer need to worry about breaking changes and backward compatibility it can be simply",
    "start": "944800",
    "end": "952320"
  },
  {
    "text": "addressed by offering users a new cluster but what does that mean for our team",
    "start": "952320",
    "end": "959199"
  },
  {
    "start": "957000",
    "end": "957000"
  },
  {
    "text": "this multi-class strategy definitely requires us to have more sophisticated",
    "start": "959199",
    "end": "964240"
  },
  {
    "text": "setup for our cluster management so we consolidated our terraform process",
    "start": "964240",
    "end": "970160"
  },
  {
    "text": "to encapsulate the entire cluster creation logic in one module so different clusters can be easily",
    "start": "970160",
    "end": "977040"
  },
  {
    "text": "critical we also developed a cube flow deployment",
    "start": "977040",
    "end": "982079"
  },
  {
    "text": "blueprints by taking the advantage of customize and get ops framework to",
    "start": "982079",
    "end": "987199"
  },
  {
    "text": "manage the kubernetes resources meanwhile we centralized ml workflow metadata in",
    "start": "987199",
    "end": "994079"
  },
  {
    "text": "our own metadata service so people can still compare the performance of the models produced from different clusters",
    "start": "994079",
    "end": "1002079"
  },
  {
    "text": "similarly we are also going to implement an aggregate experiment page so folks",
    "start": "1002079",
    "end": "1007680"
  },
  {
    "text": "can still view all the pipeline runs in one place",
    "start": "1007680",
    "end": "1012399"
  },
  {
    "text": "finally this is illustration of our multi-cluster-based q4 platform each qp4",
    "start": "1012800",
    "end": "1018720"
  },
  {
    "start": "1013000",
    "end": "1013000"
  },
  {
    "text": "cluster is built through our queue flow deployment blueprints on top of that we",
    "start": "1018720",
    "end": "1023839"
  },
  {
    "text": "have a centralized ml workflow manager service and a unified ui for pipeline",
    "start": "1023839",
    "end": "1028880"
  },
  {
    "text": "experiments and runs next i'll hand over the talk to my teammates jonathan",
    "start": "1028880",
    "end": "1035199"
  },
  {
    "text": "he will talk more in details about how we improve the queue flow deployment process for our new multi-class setup",
    "start": "1035199",
    "end": "1043520"
  },
  {
    "text": "hey everybody i'm jonathan jenn like how she said i'm going to talk about the work that we've done around deployment",
    "start": "1043919",
    "end": "1050880"
  },
  {
    "text": "monitoring and metrics in support of the multi-cluster strategy that you all just",
    "start": "1050880",
    "end": "1056559"
  },
  {
    "text": "heard about more broadly however we'll be focusing on derivative infrastructural challenges resulting",
    "start": "1056559",
    "end": "1062559"
  },
  {
    "text": "from that increased complexity and what we have done to address them",
    "start": "1062559",
    "end": "1068240"
  },
  {
    "start": "1068000",
    "end": "1068000"
  },
  {
    "text": "to start like keshi said we've increased our investment in terraform pretty significantly in the lead up to our",
    "start": "1068240",
    "end": "1073919"
  },
  {
    "text": "multi-cluster offering going all in on our infrastructure's code strategy this",
    "start": "1073919",
    "end": "1079039"
  },
  {
    "text": "has enabled us to tackle several new challenges that have cropped up as a result of multi-cluster for example now",
    "start": "1079039",
    "end": "1085280"
  },
  {
    "text": "that we have all these user-facing clusters how do we ensure consistency between them and in places where they do",
    "start": "1085280",
    "end": "1091760"
  },
  {
    "text": "need to diverge from the standard configuration in meaningful ways how do we manage those deviations sustainably",
    "start": "1091760",
    "end": "1098240"
  },
  {
    "text": "and systematically at the same time we have all these clusters now and that inherently introduces new",
    "start": "1098240",
    "end": "1104720"
  },
  {
    "text": "overhead and manual toil around the pretty fundamental task of applying changes to those clusters",
    "start": "1104720",
    "end": "1111600"
  },
  {
    "text": "lastly there is the ongoing challenge of managing these implicit dependencies between",
    "start": "1111600",
    "end": "1116960"
  },
  {
    "text": "kubernetes resources and kubeflow resources when deploying to those clusters as well",
    "start": "1116960",
    "end": "1122320"
  },
  {
    "text": "as any sort of bootstrapping processes that need to be run systematically on each new cluster to get it ready for",
    "start": "1122320",
    "end": "1129520"
  },
  {
    "text": "incoming user traffic and so on in response to all these new and",
    "start": "1129520",
    "end": "1135039"
  },
  {
    "start": "1133000",
    "end": "1133000"
  },
  {
    "text": "amplified challenges we decided to adopt argo cd as the framework around which to",
    "start": "1135039",
    "end": "1140160"
  },
  {
    "text": "base our cluster deployment operations moving forward this is a natural extension of our",
    "start": "1140160",
    "end": "1145280"
  },
  {
    "text": "existing github setup now instead of applying all of our changes by hand to each user facing cluster that we oversee",
    "start": "1145280",
    "end": "1152160"
  },
  {
    "text": "and maintaining a loose mental model of what dependencies exist between what resources and what follow-up steps need",
    "start": "1152160",
    "end": "1157919"
  },
  {
    "text": "to be taken to bootstrap we can encode all that very formally into our into our",
    "start": "1157919",
    "end": "1162960"
  },
  {
    "text": "argo cd setup this also brings with it several nice paradigms including deployment parameterization to really drive home",
    "start": "1162960",
    "end": "1169600"
  },
  {
    "text": "the idea that some clusters will inherently need to differ from their peers in different ways",
    "start": "1169600",
    "end": "1175919"
  },
  {
    "text": "now if we return back to this visualization keshie presented earlier of this fleet of user facing clusters",
    "start": "1175919",
    "end": "1182000"
  },
  {
    "start": "1176000",
    "end": "1176000"
  },
  {
    "text": "now rather than manage all this by hand in a very manual and toilsome process we",
    "start": "1182000",
    "end": "1187200"
  },
  {
    "text": "can now bring argo cd into the fray as our deployments broker in a sense now we",
    "start": "1187200",
    "end": "1193200"
  },
  {
    "text": "as cluster maintainers can make our requisite changes to the resource manifests in our deployment blueprints",
    "start": "1193200",
    "end": "1198720"
  },
  {
    "text": "repository as before these changes then in turn will get picked up by argo cd on our behalf to",
    "start": "1198720",
    "end": "1205360"
  },
  {
    "text": "then propagate accordingly to all these clusters this streamlines the deployment process",
    "start": "1205360",
    "end": "1210400"
  },
  {
    "text": "pretty significantly and it allows our team to focus on higher level requirements while offloading that rote",
    "start": "1210400",
    "end": "1216960"
  },
  {
    "text": "mechanical work of propagating these changes to argo cd more critically in doing so we ensure",
    "start": "1216960",
    "end": "1223440"
  },
  {
    "text": "greater reliability for users of this product and less chance of breakages or inconsistencies",
    "start": "1223440",
    "end": "1229440"
  },
  {
    "text": "as a result so with that let's shift focus a bit now and talk a little bit about",
    "start": "1229440",
    "end": "1235200"
  },
  {
    "text": "observability in this new multi-cluster world as adoption grows for our q field",
    "start": "1235200",
    "end": "1240799"
  },
  {
    "start": "1239000",
    "end": "1239000"
  },
  {
    "text": "platform with all these new clusters and new users the potential cost of any outage or inefficiency grows accordingly",
    "start": "1240799",
    "end": "1248000"
  },
  {
    "text": "the key insight that our team came to here is that users are not alerts and they should not act as an alerting",
    "start": "1248000",
    "end": "1253919"
  },
  {
    "text": "system what i mean by this is that our users should not be the first to know about any issues in our clusters and by",
    "start": "1253919",
    "end": "1260720"
  },
  {
    "text": "extension we as cluster owners and maintainers should know about those issues before anybody else",
    "start": "1260720",
    "end": "1266559"
  },
  {
    "text": "in other words if a user has been impacted by an outage or an inefficiency or a regression it's too late basically",
    "start": "1266559",
    "end": "1274159"
  },
  {
    "text": "and with multi-cluster we now have more complex of observability needs concretely we need to ensure parity in",
    "start": "1274159",
    "end": "1281520"
  },
  {
    "text": "instrumentation not just in existing clusters but in any new ones that we might spin up in the future they all",
    "start": "1281520",
    "end": "1287679"
  },
  {
    "text": "need the same guarantees the same instrumentation the same alerting and so on",
    "start": "1287679",
    "end": "1293840"
  },
  {
    "text": "to that effect we took further advantage of the infrastructure's code paradigm with our terraform setup more",
    "start": "1293840",
    "end": "1299840"
  },
  {
    "start": "1294000",
    "end": "1294000"
  },
  {
    "text": "fundamentally however we expanded our notion of infrastructure and what that really entails at first glance one might",
    "start": "1299840",
    "end": "1307360"
  },
  {
    "text": "consider infrastructure as solely the compute resources and the networking for example that's needed to run your",
    "start": "1307360",
    "end": "1313760"
  },
  {
    "text": "cluster and its constituent services but we argue that really it's it's so much more than that it includes not just the",
    "start": "1313760",
    "end": "1320159"
  },
  {
    "text": "cluster resources themselves but also all the configurations and auxiliary tooling that's needed in order to work",
    "start": "1320159",
    "end": "1326400"
  },
  {
    "text": "effectively with those clusters these might include alerts slos that you might want to track dashboards for real-time",
    "start": "1326400",
    "end": "1332880"
  },
  {
    "text": "outage triaging and on-call operations and any auxiliary deployments or sidecars that you might need in order to",
    "start": "1332880",
    "end": "1339840"
  },
  {
    "text": "supplement the core parts of your product offering",
    "start": "1339840",
    "end": "1345039"
  },
  {
    "text": "so with that going back to our terraform setup for a second on top of using terraform to configure and provision",
    "start": "1345039",
    "end": "1350720"
  },
  {
    "text": "compute resources node pools database instances and similar other more we'll",
    "start": "1350720",
    "end": "1356559"
  },
  {
    "text": "call it concrete infrastructural requirements we can extend that configurability to standardized",
    "start": "1356559",
    "end": "1362320"
  },
  {
    "text": "dashboards alert policies and formalized slos all tracked within gcp and all",
    "start": "1362320",
    "end": "1368000"
  },
  {
    "text": "configured using the official gcp terraform provider that's not all however the paradigm",
    "start": "1368000",
    "end": "1374159"
  },
  {
    "text": "becomes all the more powerful once you take fuller advantage of terraform modules to encapsulate all these",
    "start": "1374159",
    "end": "1380720"
  },
  {
    "text": "constructs in a parameterized logical representation of a single cluster with",
    "start": "1380720",
    "end": "1385840"
  },
  {
    "text": "that we can now stamp out new clusters much like creating new instances of a class or a struct in your typical",
    "start": "1385840",
    "end": "1391840"
  },
  {
    "text": "programming language but now they also come with batteries included essentially and all the requisite tooling on top of",
    "start": "1391840",
    "end": "1398240"
  },
  {
    "text": "the raw compute comes for free as well this entire pattern allows us to spin up and tear down entire cube load clusters",
    "start": "1398240",
    "end": "1405200"
  },
  {
    "text": "with ease and all this with ironclad guarantees that everything is set up for",
    "start": "1405200",
    "end": "1410559"
  },
  {
    "text": "each cluster exactly the way that it should be and we expect it to",
    "start": "1410559",
    "end": "1416080"
  },
  {
    "start": "1416000",
    "end": "1416000"
  },
  {
    "text": "speaking of observability however let's talk a little bit more specifically about slo tracking for our kubeflow",
    "start": "1416080",
    "end": "1422159"
  },
  {
    "text": "platform as the platform matures we have an increased need for visibility with",
    "start": "1422159",
    "end": "1427520"
  },
  {
    "text": "regards to performance stability and reliability this applies not just to us",
    "start": "1427520",
    "end": "1433120"
  },
  {
    "text": "as cluster owners but to our users as well they want to know and really they",
    "start": "1433120",
    "end": "1438159"
  },
  {
    "text": "deserve to know what kind of performance they can expect from our from our clusters if they're going to adopt the",
    "start": "1438159",
    "end": "1444080"
  },
  {
    "text": "adopt use themselves like i touched on earlier we take heavy",
    "start": "1444080",
    "end": "1449360"
  },
  {
    "text": "advantage of gcp's native tooling for slo tracking via terraform this tooling gives us a lot of nice",
    "start": "1449360",
    "end": "1455840"
  },
  {
    "text": "bells and whistles for slo tracking for example we can track what's called at the error budget and the burn rate which",
    "start": "1455840",
    "end": "1462000"
  },
  {
    "text": "gives us ready insight into how close we are at any given time to violating our slo we can also define alerts on top of",
    "start": "1462000",
    "end": "1469520"
  },
  {
    "text": "the error budget and the burn rate such that if we're starting to run low on error budget for example we can take",
    "start": "1469520",
    "end": "1475440"
  },
  {
    "text": "appropriate remediations to stay within budget at for that time frame and prevent ourselves in advance from",
    "start": "1475440",
    "end": "1481200"
  },
  {
    "text": "violating our own slows on that note let's talk about the",
    "start": "1481200",
    "end": "1486320"
  },
  {
    "start": "1485000",
    "end": "1485000"
  },
  {
    "text": "foundation of all these things matrix our product has the luxury of having a very extensive buffet of out-of-the-box",
    "start": "1486320",
    "end": "1493760"
  },
  {
    "text": "metrics to choose from and these are all provided by our respective dependencies including istio qflo and also kubernetes",
    "start": "1493760",
    "end": "1500960"
  },
  {
    "text": "itself via cube state metrics however we noticed pretty early on that a lot of these metrics in trying to be",
    "start": "1500960",
    "end": "1508000"
  },
  {
    "text": "as general as possible and avoid domain specificity often don't outright enable us to track",
    "start": "1508000",
    "end": "1514000"
  },
  {
    "text": "specifically what matters to us and what is critical to our product offering",
    "start": "1514000",
    "end": "1519120"
  },
  {
    "text": "and even then cases where it was in fact possible to do so oftentimes required these really lengthy really arcane",
    "start": "1519120",
    "end": "1525840"
  },
  {
    "text": "really verbose recording rules and extensive prom ql arithmetic to the point where maintainability became an",
    "start": "1525840",
    "end": "1531919"
  },
  {
    "text": "issue to that effect we implemented what we call very appropriately kubeflow state",
    "start": "1531919",
    "end": "1537760"
  },
  {
    "start": "1534000",
    "end": "1534000"
  },
  {
    "text": "metrics we took heavy inspiration from coop state metrics to create an analogous",
    "start": "1537760",
    "end": "1543360"
  },
  {
    "text": "custom metrics exporter specifically targeting keyflow use cases",
    "start": "1543360",
    "end": "1548880"
  },
  {
    "text": "specifically like q state metrics when deployed to our clusters kubeflow state metrics will",
    "start": "1548880",
    "end": "1554480"
  },
  {
    "text": "listen on kubernetes events such as pod creation or scheduling changes and translate those into appropriate product",
    "start": "1554480",
    "end": "1561520"
  },
  {
    "text": "specific prometheus metrics these might include tracking how long kubeflow pipeline pods take to start running how",
    "start": "1561520",
    "end": "1568640"
  },
  {
    "text": "long those pods stay on the cluster once execution concludes and so on we can",
    "start": "1568640",
    "end": "1574320"
  },
  {
    "text": "then use those metrics downstream in slos alerts dashboards and what have you",
    "start": "1574320",
    "end": "1579760"
  },
  {
    "text": "and since kubeflow itself is based on kubernetes this unlocks a very powerful usage",
    "start": "1579760",
    "end": "1585360"
  },
  {
    "text": "pattern in that we can effectively instrument and track q flow behavior via the native kubernetes api without once",
    "start": "1585360",
    "end": "1592559"
  },
  {
    "text": "ever needing to modify patch or fork kubeflow code itself",
    "start": "1592559",
    "end": "1598240"
  },
  {
    "text": "with all that i want to talk a little now about some of the key lessons that our team has learned throughout all this",
    "start": "1598240",
    "end": "1604159"
  },
  {
    "text": "work the first lesson and this is one that i touched on before is that your",
    "start": "1604159",
    "end": "1609279"
  },
  {
    "start": "1606000",
    "end": "1606000"
  },
  {
    "text": "infrastructure is more than just compute in particular it is in our opinion worth opening yourself to",
    "start": "1609279",
    "end": "1616080"
  },
  {
    "text": "thinking about your cluster infrastructure in more abstract areas getting into the habit of managing less",
    "start": "1616080",
    "end": "1621760"
  },
  {
    "text": "concrete aspects of your cluster operations like dashboards and alerts with the same amount of rigor will open",
    "start": "1621760",
    "end": "1628080"
  },
  {
    "text": "the door to surprising new opportunities for standardization and formalization",
    "start": "1628080",
    "end": "1634720"
  },
  {
    "text": "secondly i am a very strong proponent of investing in observability and reliability preemptively before you",
    "start": "1634720",
    "end": "1641919"
  },
  {
    "text": "really really truly desperately need it at the very least it's worth having a plan in place",
    "start": "1641919",
    "end": "1647279"
  },
  {
    "text": "at the end of the day users should be afforded the luxury of spending as little time as possible ideally never",
    "start": "1647279",
    "end": "1653840"
  },
  {
    "text": "having to think about the underlying infrastructure of their work investing in observability and",
    "start": "1653840",
    "end": "1659279"
  },
  {
    "text": "reliability in advance empowers teams like ours to let users focus on the problems that",
    "start": "1659279",
    "end": "1665520"
  },
  {
    "text": "truly matter to them and freeing them from concerns around flakiness inconsistency",
    "start": "1665520",
    "end": "1672399"
  },
  {
    "text": "or wondering if something is their fault or a bug in our in the queue for in the ql platform etc",
    "start": "1672399",
    "end": "1679200"
  },
  {
    "text": "and we delay those investments at our own risk they can be really difficult and time consuming to put in place",
    "start": "1679200",
    "end": "1685520"
  },
  {
    "text": "reactively after the fact and all the while users are being frustrated by outages confusing behavior",
    "start": "1685520",
    "end": "1693279"
  },
  {
    "text": "and continuing to lose confidence in your platform over time and lastly i want to encourage expanding",
    "start": "1693279",
    "end": "1700320"
  },
  {
    "start": "1698000",
    "end": "1698000"
  },
  {
    "text": "your notion of what your product truly is taking the spotify qflo platform as an",
    "start": "1700320",
    "end": "1706000"
  },
  {
    "text": "example we argue that our platform while it's based primarily around qflo is not",
    "start": "1706000",
    "end": "1712320"
  },
  {
    "text": "exclusively kubeflow itself really what we've done is treat kubeflow as an extensible foundation that we",
    "start": "1712320",
    "end": "1719279"
  },
  {
    "text": "molded customized extend it and continue to do so really to fit our ever-growing platform needs",
    "start": "1719279",
    "end": "1727200"
  },
  {
    "text": "and these include all the new functionality that we described such as multi-tenancy",
    "start": "1727200",
    "end": "1732880"
  },
  {
    "text": "multi-cluster support reliability instrumentation and custom metrics opening yourself up to the idea of",
    "start": "1732880",
    "end": "1740080"
  },
  {
    "text": "extending your foundation rather than being strictly tethered to it per se and really taking full advantage of",
    "start": "1740080",
    "end": "1745919"
  },
  {
    "text": "kubernetes as the common abstraction layer really opens the door to some very powerful and very compelling",
    "start": "1745919",
    "end": "1752640"
  },
  {
    "text": "usage patterns in closing keshie and i would like to give you all a preview of what's in",
    "start": "1752640",
    "end": "1758559"
  },
  {
    "text": "store for the qflo platform here at spotify first off we plan to increase our",
    "start": "1758559",
    "end": "1764159"
  },
  {
    "start": "1762000",
    "end": "1762000"
  },
  {
    "text": "investment in observability we're thinking of this in terms of both reliability engineering and also user",
    "start": "1764159",
    "end": "1769919"
  },
  {
    "text": "facing functionality in the former case we're looking at truly formalizing metrics as part of our",
    "start": "1769919",
    "end": "1775520"
  },
  {
    "text": "platform possibly taking advantage of prometheus operator to treat recording rules and prometheus alerts as managed",
    "start": "1775520",
    "end": "1782720"
  },
  {
    "text": "resources within our kubernetes clusters and we'll also be looking at ways to expose user pipeline metrics to users",
    "start": "1782720",
    "end": "1789760"
  },
  {
    "text": "themselves for integration into their own alerting and monitoring setups",
    "start": "1789760",
    "end": "1795840"
  },
  {
    "start": "1796000",
    "end": "1796000"
  },
  {
    "text": "we intend as well to increase our investment in what i like to call on cluster compute",
    "start": "1796000",
    "end": "1801600"
  },
  {
    "text": "currently the bulk of users qfl computations end up getting outsourced to higher level managed gcp services",
    "start": "1801600",
    "end": "1808720"
  },
  {
    "text": "such as data flow for data engineering and cloud ai platform for model training however this execution model does not",
    "start": "1808720",
    "end": "1815600"
  },
  {
    "text": "always mesh well with users maybe they're concerned about the cloud costs or their training needs are esoteric",
    "start": "1815600",
    "end": "1821919"
  },
  {
    "text": "enough that they would like fuller control over the execution model of their pipelines we'll be looking at the solutions for",
    "start": "1821919",
    "end": "1828399"
  },
  {
    "text": "users like those that will empower them with the flexibility to take advantage of training patterns that don't cleanly",
    "start": "1828399",
    "end": "1835360"
  },
  {
    "text": "fit into what into that user journey we also want to invest more in users",
    "start": "1835360",
    "end": "1842159"
  },
  {
    "start": "1840000",
    "end": "1840000"
  },
  {
    "text": "ability to manage the particulars of their key flow journey themselves basically",
    "start": "1842159",
    "end": "1847360"
  },
  {
    "text": "this might include management of service accounts permissions retention policies",
    "start": "1847360",
    "end": "1852399"
  },
  {
    "text": "and cluster quota overrides to allow them to temporarily take up or like consume more",
    "start": "1852399",
    "end": "1858960"
  },
  {
    "text": "resources at a time for intensive compute doing so would give advanced users greater control over their own platform",
    "start": "1858960",
    "end": "1865840"
  },
  {
    "text": "usage while still allowing newcomers to get quickly get started with sensible defaults that we provide for them",
    "start": "1865840",
    "end": "1873840"
  },
  {
    "text": "and lastly we believe very strongly in contributing back to the open source community that has empowered so much of",
    "start": "1873840",
    "end": "1880320"
  },
  {
    "start": "1874000",
    "end": "1874000"
  },
  {
    "text": "our own work case in point a lot of our work such as the kupelo centric metrics",
    "start": "1880320",
    "end": "1885600"
  },
  {
    "text": "exporter that i mentioned before has been designed from day one for generality and open source ability",
    "start": "1885600",
    "end": "1892960"
  },
  {
    "text": "as such we're actively looking at ways to open source some of our work around kubeflow in production",
    "start": "1892960",
    "end": "1898640"
  },
  {
    "text": "details are kind of fuzzy right now but there will hopefully be more to come and we'd love to share more with you in the",
    "start": "1898640",
    "end": "1904240"
  },
  {
    "text": "near future and that's all we had to talk to you about today thanks a bunch for listening",
    "start": "1904240",
    "end": "1910720"
  },
  {
    "text": "to us today if any of this work sounds interesting to you we welcome you to check out life spotify.com",
    "start": "1910720",
    "end": "1917279"
  },
  {
    "text": "for new opportunities around kubernetes cloud native computing and machine learning at spotify thanks again for",
    "start": "1917279",
    "end": "1923600"
  },
  {
    "text": "your time we'll be taking questions now",
    "start": "1923600",
    "end": "1928039"
  }
]