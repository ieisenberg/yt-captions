[
  {
    "text": "so hello everyone uh welcome to my talk where I'll be covering uh some recent",
    "start": "80",
    "end": "5359"
  },
  {
    "text": "developments uh in kubernetes and and within GK for um overload protection and",
    "start": "5359",
    "end": "11639"
  },
  {
    "text": "and prevention of um um various uh denal service um scenarios that we've been working",
    "start": "11639",
    "end": "17240"
  },
  {
    "text": "on uh so firstly just a little bit about myself um obviously I work on gke and I",
    "start": "17240",
    "end": "23119"
  },
  {
    "text": "joined Google about uh two years ago um prior to working at Google I've been I've been working on uh kubernetes",
    "start": "23119",
    "end": "30679"
  },
  {
    "text": "uh for about uh five to six years at this point uh so over the years I've",
    "start": "30679",
    "end": "36520"
  },
  {
    "text": "worked on um many areas of of the codebase uh but more recently I've been uh uh working U on some areas uh in Cube",
    "start": "36520",
    "end": "44600"
  },
  {
    "text": "API server specifically around uh API priority and fairness and kind of trying to operationalize this uh subsystem",
    "start": "44600",
    "end": "52239"
  },
  {
    "text": "within gke um and so that that'll be uh the focus for most of the",
    "start": "52239",
    "end": "57800"
  },
  {
    "text": "talk um but before we go there it's worth just kind of mentioning kind of the different types of uh denal service",
    "start": "57800",
    "end": "65439"
  },
  {
    "text": "that a kubernetes cluster can experience and I'm sure there's been like tons of talks about how you can brick your cluster and and and whatnot um and so",
    "start": "65439",
    "end": "72159"
  },
  {
    "text": "many times when we think about like denial surfice attacks we think about you know uh uh Bots flooding your",
    "start": "72159",
    "end": "79280"
  },
  {
    "text": "network with millions of packets per second or we think about you know security vulnerabilities being exploited",
    "start": "79280",
    "end": "84799"
  },
  {
    "text": "at scale and uh you know don't get me wrong like these types of attacks do happen but luckily for um myself and the",
    "start": "84799",
    "end": "93399"
  },
  {
    "text": "the the other Engineers that work on gke WE leverage uh many of Google's um",
    "start": "93399",
    "end": "98439"
  },
  {
    "text": "common infrastructure and services and so out of the box we get pretty good protection for for those types of um",
    "start": "98439",
    "end": "104640"
  },
  {
    "text": "attacks like really large scale um and yeah it turns out you know Google Google",
    "start": "104640",
    "end": "110119"
  },
  {
    "text": "infrastructure is fairly good at this kind of thing so uh many of these uh",
    "start": "110119",
    "end": "116399"
  },
  {
    "text": "denal service attacks um not like don't end up really being my problem right they're kind of like handled at a more",
    "start": "116399",
    "end": "123119"
  },
  {
    "text": "like common infrastructure layer and there's really smart teams of Engineers that kind of work on on these kind of",
    "start": "123119",
    "end": "129080"
  },
  {
    "text": "things for for our networks and our servers at Google and yeah there's like tons of uh recent block posts around",
    "start": "129080",
    "end": "135560"
  },
  {
    "text": "these kind of topics the most recent one around um the http2 rapid reset that was",
    "start": "135560",
    "end": "142120"
  },
  {
    "text": "pretty cool blog post which I I recommend um you check it out if you haven't um so From gk's perspective",
    "start": "142120",
    "end": "150519"
  },
  {
    "text": "the types of denial of service that we need to mainly be concerned with is uh denial of service at the application",
    "start": "150519",
    "end": "156959"
  },
  {
    "text": "layer or in this case the application being like kubernetes or specifically the the QB API server um so if we think",
    "start": "156959",
    "end": "164040"
  },
  {
    "text": "about like the possible types of um denial of service that like QBE API server can experience we can very",
    "start": "164040",
    "end": "170720"
  },
  {
    "text": "broadly break it down into kind of two categories the first is um you know the",
    "start": "170720",
    "end": "176280"
  },
  {
    "text": "a bad actor kind of gets access to QB API server and it starts kind of of exploiting um some like known bugs or",
    "start": "176280",
    "end": "182239"
  },
  {
    "text": "performance characteristics about the system and then tries to take down the API server um but you can argue this is",
    "start": "182239",
    "end": "188159"
  },
  {
    "text": "like very similar to basically like security exploit right like if if such a bug um was known we would probably treat",
    "start": "188159",
    "end": "195159"
  },
  {
    "text": "it like a cve and like go through the standard you know process to like quickly roll out cve fixes so I'm not",
    "start": "195159",
    "end": "200959"
  },
  {
    "text": "going to really touch on that topic all too much um and the second is is a user that is like unknowingly causing a",
    "start": "200959",
    "end": "207760"
  },
  {
    "text": "denial service scenario in their cluster and for this talk I'll be kind of mostly",
    "start": "207760",
    "end": "213000"
  },
  {
    "text": "um talking about those kind of scenarios and if you think about it it's actually not uh that surprising like especially",
    "start": "213000",
    "end": "219680"
  },
  {
    "text": "if you have like really really large um GK clusters that are shared across uh",
    "start": "219680",
    "end": "224799"
  },
  {
    "text": "many engineering teams it's it's not uncommon to see um these types of scenarios kind of happening uh so yeah",
    "start": "224799",
    "end": "231439"
  },
  {
    "text": "let's let's go through some uh one example that's not uh that's not uncommon uh for gke uh so let's say you",
    "start": "231439",
    "end": "238599"
  },
  {
    "text": "have like a you have a 5,000 note cluster uh in your organization you know so is shared across like many many",
    "start": "238599",
    "end": "245519"
  },
  {
    "text": "engineering teams in your big organization and you know one team maybe it's the monitoring team or some other",
    "start": "245519",
    "end": "252159"
  },
  {
    "text": "team they need to uh they want to like deploy this this new Damon set um and so",
    "start": "252159",
    "end": "258280"
  },
  {
    "text": "for starters like already this Damon set is going to run on every single node in your 5,000 no cluster so you're basically running 5,000 replicas of this",
    "start": "258280",
    "end": "265040"
  },
  {
    "text": "thing and yeah let's say the purpose of this D set is is for monitoring so the first thing is does is it needs to find",
    "start": "265040",
    "end": "272039"
  },
  {
    "text": "uh for every single replica it needs to find every pod that is running locally on the same node and then it uses the",
    "start": "272039",
    "end": "278360"
  },
  {
    "text": "pods IP and the status to to scrape the metrics and point and then it pushes it into like a centralized place so you can",
    "start": "278360",
    "end": "284360"
  },
  {
    "text": "you know use for your dashboards and whatever and then let's say this Damon Set uh doesn't follow like all the best",
    "start": "284360",
    "end": "291280"
  },
  {
    "text": "practices about kubernetes scalability so it doesn't use the listat pattern to fetch objects it doesn't use informers",
    "start": "291280",
    "end": "297440"
  },
  {
    "text": "and it instead what it does it like it has this like for Loop where it'll just like periodically list the objects that",
    "start": "297440",
    "end": "303039"
  },
  {
    "text": "it cares about uh and then even worse let's say you know it doesn't specify resource version so uh basically API",
    "start": "303039",
    "end": "309919"
  },
  {
    "text": "server every time it sees this request it needs to get all the the objects from ETD to get the latest version um and",
    "start": "309919",
    "end": "316560"
  },
  {
    "text": "then as the kind of the final nail in the coffin uh it'll use like a field selector uh to filter pods based on um",
    "start": "316560",
    "end": "323199"
  },
  {
    "text": "pods that are on the same node which actually requires API server to fetch all pods fromd because it actually",
    "start": "323199",
    "end": "329560"
  },
  {
    "text": "doesn't support uh filtering like SD objects based on on on specific",
    "start": "329560",
    "end": "334680"
  },
  {
    "text": "Fields um so yeah you can imagine like this type of uh scenario can like really",
    "start": "334680",
    "end": "339720"
  },
  {
    "text": "quickly snowball into like a full outage right especially if you have like 5,000 of these clients kind of just being",
    "start": "339720",
    "end": "345039"
  },
  {
    "text": "deployed at once and like like I make this kind of sound like a hypothetical scenario but it actually happens uh",
    "start": "345039",
    "end": "353000"
  },
  {
    "text": "quite frequently so like even for kind of like well-known third party packages",
    "start": "353000",
    "end": "358400"
  },
  {
    "text": "or thir third partyy ad add-ons um provided by vendors like it it's not uncommon to kind of um see these these",
    "start": "358400",
    "end": "366160"
  },
  {
    "text": "uh these types of these types clients um and so the goal for kubernetes is to",
    "start": "366160",
    "end": "371759"
  },
  {
    "text": "make it like we want to make kubernetes resilient enough so that in these types of scenarios it can kind of",
    "start": "371759",
    "end": "377280"
  },
  {
    "text": "automatically protect itself and preserve like specific capacity to uh handle or to be able to run like the the",
    "start": "377280",
    "end": "384199"
  },
  {
    "text": "core system components um so if we if we kind of uh",
    "start": "384199",
    "end": "389759"
  },
  {
    "text": "take a step back and look at the the history of kubernetes the very first uh mechanism that we introduced was",
    "start": "389759",
    "end": "396400"
  },
  {
    "text": "basically uh two flags in the API server the max requests and flight and the the max mutating requests and flight and",
    "start": "396400",
    "end": "403400"
  },
  {
    "text": "these two flags basically provided limits for the maximum amount of um read and write requests that you can process",
    "start": "403400",
    "end": "410639"
  },
  {
    "text": "at any given time um and in gke the first thing we had to do was we had to basically like",
    "start": "410639",
    "end": "416639"
  },
  {
    "text": "operationalize these U these flags in our Fleet um and the tricky thing is like you can't just like pick one value and just",
    "start": "416639",
    "end": "423360"
  },
  {
    "text": "have it work for the whole Fleet we have to basically uh tune this value so that",
    "start": "423360",
    "end": "428520"
  },
  {
    "text": "it's appropriate based on the capacity that that a cluster has um if you pick a",
    "start": "428520",
    "end": "433800"
  },
  {
    "text": "value too low you risk kind of preemptively um throttling your clients",
    "start": "433800",
    "end": "439199"
  },
  {
    "text": "excessively and you end up with like unused capacity in your cluster and if you set the value too high you risk",
    "start": "439199",
    "end": "445520"
  },
  {
    "text": "basically like overload like diluting the protection and then completely overloading your control plane um so",
    "start": "445520",
    "end": "451759"
  },
  {
    "text": "what we started with initially was very implementation so these are kind of just like madeup numbers the exact numbers",
    "start": "451759",
    "end": "457520"
  },
  {
    "text": "aren't really that important but we basically used like a linear function where for every uh CPU core we GI uh for",
    "start": "457520",
    "end": "464440"
  },
  {
    "text": "the control plane we add we would add 10 Max and flight requests for each core um",
    "start": "464440",
    "end": "470360"
  },
  {
    "text": "and uh this actually this approach actually got us pretty far like it actually worked fairly well but then it",
    "start": "470360",
    "end": "475560"
  },
  {
    "text": "it kind of started to have some limitations and so uh the the obvious",
    "start": "475560",
    "end": "480879"
  },
  {
    "text": "the the obvious big limitation is that like there's no concept of prioritization so you can still have",
    "start": "480879",
    "end": "487000"
  },
  {
    "text": "like complete overload situations just by like a handful of clients or in extreme cases like even one client can",
    "start": "487000",
    "end": "493479"
  },
  {
    "text": "basically use up all your Max and flight requests and basically make your brick",
    "start": "493479",
    "end": "499159"
  },
  {
    "text": "your cluster and and take it down and so this was obviously a problem and we we we want like the overall availability of",
    "start": "499159",
    "end": "505680"
  },
  {
    "text": "the system to be a little bit more robust uh and so this is where um APF comes into play uh but yeah really",
    "start": "505680",
    "end": "512719"
  },
  {
    "text": "quickly before I go into details I I do want to call out um some of the folks who've been driving this effort for for",
    "start": "512719",
    "end": "518120"
  },
  {
    "text": "multiple releases I I think in uh 129 which is the upcoming release we're",
    "start": "518120",
    "end": "523240"
  },
  {
    "text": "actually planning to uh promote the um API prior priority and fairness uh",
    "start": "523240",
    "end": "528600"
  },
  {
    "text": "feature to GA so that'll be a huge milestone for the team um these folks have been working on it for I think",
    "start": "528600",
    "end": "534360"
  },
  {
    "text": "almost like 12 releases now so a huge milestone and uh my main contribution to",
    "start": "534360",
    "end": "540160"
  },
  {
    "text": "this area is mostly um figuring out how to operationalize uh this in gke and kind",
    "start": "540160",
    "end": "546680"
  },
  {
    "text": "of just like throwing some bugs over the wall to them so they they did the hard work really um so yeah like in the very",
    "start": "546680",
    "end": "554440"
  },
  {
    "text": "initial implementations of APF we basically had um this system to Define",
    "start": "554440",
    "end": "559720"
  },
  {
    "text": "priority levels and then within each priority level uh API server is like managing cues to basically determine",
    "start": "559720",
    "end": "565760"
  },
  {
    "text": "like how much available capacity is left and whether it should either accept or drop new new requests um so so now we",
    "start": "565760",
    "end": "572720"
  },
  {
    "text": "have some guarantee in high load scenarios uh basically like when we when",
    "start": "572720",
    "end": "578160"
  },
  {
    "text": "API server gets overloaded we can be uh confident that you know some requests are reserved for the cuet some requests",
    "start": "578160",
    "end": "584480"
  },
  {
    "text": "are reserved for the scheduler and and and and whatnot and basically how we achieved this was we introduced a new",
    "start": "584480",
    "end": "590680"
  },
  {
    "text": "API Group flow control. kubernetes iio which basically introduced uh two apis",
    "start": "590680",
    "end": "597000"
  },
  {
    "text": "the priority level configuration and the flow schema so really quick recap for those who aren't familiar um priority",
    "start": "597000",
    "end": "603959"
  },
  {
    "text": "level configuration defines limits on the number of outstanding requests and limitations on the number of cued",
    "start": "603959",
    "end": "610200"
  },
  {
    "text": "requests and the flow schema is basically how you classify requests into those uh priority",
    "start": "610200",
    "end": "616880"
  },
  {
    "text": "levels uh so with the uh introduction of APF the kind of the first immediate",
    "start": "616880",
    "end": "622120"
  },
  {
    "text": "problem that we needed to needed to address was that each request was basically treated as one seat inside the",
    "start": "622120",
    "end": "629480"
  },
  {
    "text": "APF Q's um and and this was the case when the feature graduated to Beta in",
    "start": "629480",
    "end": "636760"
  },
  {
    "text": "1.20 but the problem is that in reality the amount of actual work that API server needs to do to process these um",
    "start": "636760",
    "end": "643760"
  },
  {
    "text": "requests actually varies like significantly um so it was important for us to have some kind of uh way to",
    "start": "643760",
    "end": "651560"
  },
  {
    "text": "associate like a more accurate cost to these requests and then apply it to the amount of seats that were picking up in",
    "start": "651560",
    "end": "658360"
  },
  {
    "text": "these in these cues use uh so over many releases we",
    "start": "658360",
    "end": "663920"
  },
  {
    "text": "introduced some heris to try to estimate the amount of work involved in a request",
    "start": "663920",
    "end": "669399"
  },
  {
    "text": "um so this was actually uh pretty like it was uh not that hard to implement",
    "start": "669399",
    "end": "675320"
  },
  {
    "text": "because we actually had very um uh we had the basic parameters all available",
    "start": "675320",
    "end": "680360"
  },
  {
    "text": "in the request metadata the problem is really figuring out like what are the actual right numbers um that can achieve",
    "start": "680360",
    "end": "686399"
  },
  {
    "text": "this so for single object get those still cost one seat which makes sense because we uh in the worst case scenario",
    "start": "686399",
    "end": "693040"
  },
  {
    "text": "we fetch the single object from SS and then we have to serialize the object and then return it to client all that um if",
    "start": "693040",
    "end": "699240"
  },
  {
    "text": "we look at a a list request um this could potentially list um uh all objects",
    "start": "699240",
    "end": "706279"
  },
  {
    "text": "of a resource in the cluster so the the amount of the cost has to be some function of the amount of objects that",
    "start": "706279",
    "end": "712560"
  },
  {
    "text": "we're going to potentially list uh in addition um we actually have to double",
    "start": "712560",
    "end": "718200"
  },
  {
    "text": "the cost if we list from Storage in instead of cache um and we can actually",
    "start": "718200",
    "end": "723360"
  },
  {
    "text": "determine this based on the the resource version of of the request um so the the formula for estimating uh list request",
    "start": "723360",
    "end": "730720"
  },
  {
    "text": "is uh n / 100 uh times two if resource version um or or it times two if it's",
    "start": "730720",
    "end": "740040"
  },
  {
    "text": "basically resource resource version is not zero so it was uh listed from uh",
    "start": "740040",
    "end": "745079"
  },
  {
    "text": "storage and where n is the number of objects that uh uh that's in",
    "start": "745079",
    "end": "750760"
  },
  {
    "text": "storage uh so watch is similar but but slightly different uh if watch request",
    "start": "750760",
    "end": "756639"
  },
  {
    "text": "specifies uh send initial events then we treat it like a list from cach because that's that's effectively what it's",
    "start": "756639",
    "end": "763000"
  },
  {
    "text": "doing otherwise we treat it as just costum one seed um mutating requests which includes",
    "start": "763000",
    "end": "769040"
  },
  {
    "text": "uh you know create delete update oh sorry create uh update and and delete yeah um is actually the most unintuitive",
    "start": "769040",
    "end": "775680"
  },
  {
    "text": "one because the cost to process a single muttin Quest is actually one but we have",
    "start": "775680",
    "end": "781519"
  },
  {
    "text": "to factor in the potential watch events that will be propagated um based on that",
    "start": "781519",
    "end": "786880"
  },
  {
    "text": "one mutating request so the more like Watchers you have for that object then",
    "start": "786880",
    "end": "792079"
  },
  {
    "text": "there's a kind of when you mutate the object you're going to send the watch events to all the all the clients that are watching for the object and so you",
    "start": "792079",
    "end": "798360"
  },
  {
    "text": "have to figure out like how to actually put a cost to that um propagated watch events uh so the formula we used for",
    "start": "798360",
    "end": "803959"
  },
  {
    "text": "mutating events was uh W ID 25 where W is the number of Watchers for that",
    "start": "803959",
    "end": "811600"
  },
  {
    "text": "object uh so uh how does the API server actually know the values of",
    "start": "811600",
    "end": "817160"
  },
  {
    "text": "nnw uh it's nothing really fancy we basically run like a small controller",
    "start": "817160",
    "end": "822600"
  },
  {
    "text": "inside the API server that is periodically checking for number of objects per resource in that CD and also",
    "start": "822600",
    "end": "828720"
  },
  {
    "text": "keeping track of of number of Watchers um watching for specific types of of of",
    "start": "828720",
    "end": "834519"
  },
  {
    "text": "resources and and name spaces Okay so so in practice these chur",
    "start": "834519",
    "end": "840199"
  },
  {
    "text": "sticks um they're not perfect but they're kind of a they're pretty significant improvement from just giving",
    "start": "840199",
    "end": "847040"
  },
  {
    "text": "like every single request the cost FL um and they they do have some some some",
    "start": "847040",
    "end": "852199"
  },
  {
    "text": "known limitations so starting with list requests um one limitation is that um we",
    "start": "852199",
    "end": "857959"
  },
  {
    "text": "don't actually factor in name spaces so the object tracker basically tracks total object count on a per resource",
    "start": "857959",
    "end": "864759"
  },
  {
    "text": "basis um and basically like you know if if you have like a thousand config maps",
    "start": "864759",
    "end": "871199"
  },
  {
    "text": "and they're all in you know like 900 are in one namespace and other 100 are divided across other namespaces if if",
    "start": "871199",
    "end": "878000"
  },
  {
    "text": "you list config maps in the namespace with like five objects uh we're always going to treat the amount of config Maps",
    "start": "878000",
    "end": "885120"
  },
  {
    "text": "as basically a thousand so like obviously um this is can be problematic but in practice it's actually not a big",
    "start": "885120",
    "end": "891720"
  },
  {
    "text": "deal because listing five config Maps even if we associate a high cost with it",
    "start": "891720",
    "end": "896920"
  },
  {
    "text": "because we process the request so quickly it usually ends up kind of being negligible but there are some cases",
    "start": "896920",
    "end": "902399"
  },
  {
    "text": "where where it has been problematic um another limitation is that we don't factor in the size of the",
    "start": "902399",
    "end": "907560"
  },
  {
    "text": "request because obviously the size determines like how much work is involved in in serialization so um",
    "start": "907560",
    "end": "912720"
  },
  {
    "text": "that's something we we currently don't factor in at all um for for mutating requests uh the watch tracker um I I",
    "start": "912720",
    "end": "920600"
  },
  {
    "text": "guess the biggest limitation is that it it's only tracking Watchers that are local to the same API server um and it's",
    "start": "920600",
    "end": "927199"
  },
  {
    "text": "not actually like communicating with the other API servers on the actual total amount of um watches so it's actually",
    "start": "927199",
    "end": "933839"
  },
  {
    "text": "not a complete accurate measurement of the actual like watch events that you propagate for for mutating requests and",
    "start": "933839",
    "end": "940120"
  },
  {
    "text": "then the other limitation is that um not all watch events are actually equal in work so we don't uh we don't actually",
    "start": "940120",
    "end": "946360"
  },
  {
    "text": "account for that like some watch events might trigger like more logical changes in your system whereas you know some",
    "start": "946360",
    "end": "951399"
  },
  {
    "text": "other watch events might just be like you know uh making a like just like sending a log event or something like",
    "start": "951399",
    "end": "957199"
  },
  {
    "text": "that so we don't actually care about um the actual work by watch events we just treat all watch events kind of kind of",
    "start": "957199",
    "end": "963480"
  },
  {
    "text": "the same uh so yeah let's go through some examples um let's say we have a list",
    "start": "963480",
    "end": "969120"
  },
  {
    "text": "request for config Maps uh in the default namespace so let's say a cluster has uh 600 config Maps but the request",
    "start": "969120",
    "end": "976360"
  },
  {
    "text": "specifies resource version zero which means this from cash so the cost is going to be um so earlier I mentioned we",
    "start": "976360",
    "end": "983319"
  },
  {
    "text": "basically limit the amount of seats to 10 so the cost is going to be the lower of either uh 10 uh or n over 100 so the",
    "start": "983319",
    "end": "992279"
  },
  {
    "text": "cost in this case is going to be six um if we take the same example and then we remove the resource version zero now",
    "start": "992279",
    "end": "999880"
  },
  {
    "text": "this becomes list from storage and so uh n over uh 100 * 2 is 12 but because we",
    "start": "999880",
    "end": "1006000"
  },
  {
    "text": "have that upper limit of 10 the cost is going to be 10 um so uh one problem we actually ran",
    "start": "1006000",
    "end": "1013079"
  },
  {
    "text": "into uh in gke so at some point we like upgraded to a version where list requests don't cost one anymore but now",
    "start": "1013079",
    "end": "1019759"
  },
  {
    "text": "they cost up to 10 uh we can have scenarios where the cost of a request is",
    "start": "1019759",
    "end": "1025000"
  },
  {
    "text": "actually more than the actual available capacity um so earlier we talked about",
    "start": "1025000",
    "end": "1030199"
  },
  {
    "text": "how priority levels they're given like some share of the total Max request in Flight based on the nominal concurrency",
    "start": "1030199",
    "end": "1036880"
  },
  {
    "text": "shares but if you configure like a really low Max in Flight request um you",
    "start": "1036880",
    "end": "1042798"
  },
  {
    "text": "you can end up with priority levels that have less than 10 seats but the but you can have a request come in that cost 10",
    "start": "1042799",
    "end": "1048480"
  },
  {
    "text": "seats and so you can have basically one request that effectively starves out the whole priority level uh and um basically",
    "start": "1048480",
    "end": "1055559"
  },
  {
    "text": "locks out the other clients from from at least until like the request completes uh and so more recently um we",
    "start": "1055559",
    "end": "1063880"
  },
  {
    "text": "kind of tuned this formula a little bit further such that we take um the smaller of either the estimated cost or 15% of",
    "start": "1063880",
    "end": "1073440"
  },
  {
    "text": "the uh total available seats um but we but we still apply this upper limit of",
    "start": "1073440",
    "end": "1078840"
  },
  {
    "text": "10 because we don't want like the the seats to kind of like you can have clusters that have like tens of",
    "start": "1078840",
    "end": "1084159"
  },
  {
    "text": "thousands of of of objects and we don't want any single request to basically cost more than 10 um and yeah this just",
    "start": "1084159",
    "end": "1092640"
  },
  {
    "text": "makes it more likely for a handful of clients or a handful of quests to basically like completely starve out a",
    "start": "1092640",
    "end": "1099200"
  },
  {
    "text": "priority level um so yeah this this tuning helped",
    "start": "1099200",
    "end": "1104559"
  },
  {
    "text": "quite a bit but if we go back to the way we configure like the max queston flight flag um and again these are just made up",
    "start": "1104559",
    "end": "1111679"
  },
  {
    "text": "numbers to illustrate a point we we started to notice this pattern where like control planes with really small",
    "start": "1111679",
    "end": "1117400"
  },
  {
    "text": "Max request and flight were still pretty um prone to premature rate limiting so",
    "start": "1117400",
    "end": "1123000"
  },
  {
    "text": "like they would start rate limiting clients thinking the cluster is overloaded but the cluster actually has like a lot of capacity left and then for",
    "start": "1123000",
    "end": "1130320"
  },
  {
    "text": "our like really large control planes like the control planes that are managing like your 5,000 to 15,000 node",
    "start": "1130320",
    "end": "1136640"
  },
  {
    "text": "clusters um they had too much Max request in flight so they they weren't actually providing like sufficient",
    "start": "1136640",
    "end": "1142720"
  },
  {
    "text": "overload protection and we had we could you could have scenarios where cluster gets overloaded by some clients that",
    "start": "1142720",
    "end": "1148559"
  },
  {
    "text": "aren't actually that important and so we started to think about um different ways we can calculate",
    "start": "1148559",
    "end": "1154640"
  },
  {
    "text": "the most appropriate values for Max request and flight and we landed on applying this like weighted function",
    "start": "1154640",
    "end": "1160720"
  },
  {
    "text": "such that the first few cores by the control plane give more max request in flight and then as you like add more uh",
    "start": "1160720",
    "end": "1168320"
  },
  {
    "text": "CPU capacity we kind of like taper it off and so when you get to like really large sizes you don't actually add more",
    "start": "1168320",
    "end": "1173919"
  },
  {
    "text": "max request in Flight if you give it more more capacity um uh so the the",
    "start": "1173919",
    "end": "1180159"
  },
  {
    "text": "effect of this is that we basically provide more upfront capacity if when we start the control plane smaller um and",
    "start": "1180159",
    "end": "1186960"
  },
  {
    "text": "and this way they're less likely to like lock out clients and rate limit clients prematurely and we also like restrict",
    "start": "1186960",
    "end": "1193840"
  },
  {
    "text": "the max requests in flight for really large control planes because you know these are typically your like really",
    "start": "1193840",
    "end": "1200440"
  },
  {
    "text": "large gka clusters and we need to always make sure that there's available capacity to scale the nodes and and we",
    "start": "1200440",
    "end": "1206840"
  },
  {
    "text": "don't want like non-system clients competing with the scheduler and and and the controller manager and like other",
    "start": "1206840",
    "end": "1212360"
  },
  {
    "text": "system clients that are trying to basically keep this like 5,000 node cluster running um so yeah this is BAS this is",
    "start": "1212360",
    "end": "1219919"
  },
  {
    "text": "basically to illustrate kind of what the max request and flight would look like as as a cluster kind of scales to more",
    "start": "1219919",
    "end": "1227159"
  },
  {
    "text": "notes okay so so far uh uh we",
    "start": "1227159",
    "end": "1232559"
  },
  {
    "text": "accomplished um we basically added like a mechanism to limit uh inflight requests using the max request inlight",
    "start": "1232559",
    "end": "1239200"
  },
  {
    "text": "flag and then we introduced this API to Define priorities and a way to classify",
    "start": "1239200",
    "end": "1244480"
  },
  {
    "text": "traffic into these priorities and then we introduced um work estimation and tuned that a bunch so that we make the",
    "start": "1244480",
    "end": "1250840"
  },
  {
    "text": "system more robust and it kind of more accurately measures the amount of work that's produced um by by incoming",
    "start": "1250840",
    "end": "1257679"
  },
  {
    "text": "requests so we're at a pretty good place at this point but there's this there's a really big underlying assumption that we've",
    "start": "1257679",
    "end": "1264400"
  },
  {
    "text": "been making so far that isn't always true which is that clients and the requests are always going to the correct",
    "start": "1264400",
    "end": "1271080"
  },
  {
    "text": "prior level right um and so this is where flow classification um is relevant",
    "start": "1271080",
    "end": "1276559"
  },
  {
    "text": "so earlier we briefly covered the the flow schema API which is basically a set",
    "start": "1276559",
    "end": "1281720"
  },
  {
    "text": "of rules and that Maps some parameters of a request into a priority level and if you don't have the correct flow",
    "start": "1281720",
    "end": "1287720"
  },
  {
    "text": "schema or an optimal configuration of flow schemas your system availability and performance is is going to suffer uh",
    "start": "1287720",
    "end": "1295520"
  },
  {
    "text": "and this is because we've designed these priority levels around specific clients and and and the types of requests that",
    "start": "1295520",
    "end": "1301600"
  },
  {
    "text": "we actually expect from those clients um but if these requests are not mapping correctly to those priority levels then",
    "start": "1301600",
    "end": "1307919"
  },
  {
    "text": "you know we're going to run into a lot of problems because we we we kind of designed the priority levels like we basically sharded capacity to these",
    "start": "1307919",
    "end": "1314400"
  },
  {
    "text": "priority levels based on what like you know specific types of clients and now now if we're not expecting that the the",
    "start": "1314400",
    "end": "1320440"
  },
  {
    "text": "shares of the priority levels are are incorrect uh and so uh kubernetes does",
    "start": "1320440",
    "end": "1326520"
  },
  {
    "text": "come with some uh sane default flow schemas which are designed to cover most cases uh so uh looking at U one example",
    "start": "1326520",
    "end": "1335039"
  },
  {
    "text": "uh the first one uh all clients authenticated with a kubernetes service account is going to use the workload low",
    "start": "1335039",
    "end": "1341200"
  },
  {
    "text": "priority level and like earlier in that like donut chart the workload load actually has the um most capacity and",
    "start": "1341200",
    "end": "1348919"
  },
  {
    "text": "this is because generally if you're running a pod with the mounted surface account that's going to be a controller and in large clusters you're going to",
    "start": "1348919",
    "end": "1355080"
  },
  {
    "text": "have lots of controllers running in this configuration which is why it has the most capacity um uh but if you're actually",
    "start": "1355080",
    "end": "1363039"
  },
  {
    "text": "running a surface account in the cube system namespace we actually treat it more of a system controller and so we we",
    "start": "1363039",
    "end": "1368279"
  },
  {
    "text": "put that into the workload high and workload high is basically like you can think of like workload low is all",
    "start": "1368279",
    "end": "1374080"
  },
  {
    "text": "controllers and workload high is all system controllers basically and uh yeah there's a bunch of other",
    "start": "1374080",
    "end": "1380039"
  },
  {
    "text": "rules similar to that but I'm not going to cover actually the last one is is",
    "start": "1380039",
    "end": "1385919"
  },
  {
    "text": "probably worth calling out so this is the global default priority level which is given a pretty small slice of the",
    "start": "1385919",
    "end": "1392640"
  },
  {
    "text": "cake and we reserve that for uh interactive clients like Cube control and and whatnot and this will be",
    "start": "1392640",
    "end": "1398400"
  },
  {
    "text": "important later for for example um yeah so in gke uh we have uh we have some",
    "start": "1398400",
    "end": "1405159"
  },
  {
    "text": "goals around uh flow classification uh so firstly we want to improve the uh",
    "start": "1405159",
    "end": "1411279"
  },
  {
    "text": "overall accuracy of FL classification which means we want to configure our flow schemas in such a way that",
    "start": "1411279",
    "end": "1417640"
  },
  {
    "text": "misclassifying flows is very unlikely and then secondly we want our",
    "start": "1417640",
    "end": "1423039"
  },
  {
    "text": "default configuration of flow schemas um and priority levels to work for every GK",
    "start": "1423039",
    "end": "1428559"
  },
  {
    "text": "user and so we want uh so we do allow customizing flow schemas and priority",
    "start": "1428559",
    "end": "1434159"
  },
  {
    "text": "levels with some guard rails but we want that to be a last resort um and we want it to be like a fairly low fairly rare",
    "start": "1434159",
    "end": "1441320"
  },
  {
    "text": "occurrence for um any GK user to have to kind of Tinker with flow schemas and priority levels um uh but in practice",
    "start": "1441320",
    "end": "1449679"
  },
  {
    "text": "this actually becomes um really hard to accomplish because of all the different way our customers set up the Clusters",
    "start": "1449679",
    "end": "1455960"
  },
  {
    "text": "and deploy their applications and all the different ways you can like authenticate and interact with your",
    "start": "1455960",
    "end": "1461000"
  },
  {
    "text": "cluster and so uh misclassification actually becomes uh uh kind of",
    "start": "1461000",
    "end": "1466799"
  },
  {
    "text": "inevitable at at to some extent um so yeah let's walk through some examples of",
    "start": "1466799",
    "end": "1472080"
  },
  {
    "text": "misclassification that can happen on GK so very common example is you run uh",
    "start": "1472080",
    "end": "1477440"
  },
  {
    "text": "some local tool on your laptop that generates a lot of traffic uh these clients will almost always use the",
    "start": "1477440",
    "end": "1483600"
  },
  {
    "text": "global default priority level that we kind of talked about earlier um and this is because usually when you run some",
    "start": "1483600",
    "end": "1490440"
  },
  {
    "text": "controller or or some tool on your laptop is going to authenticate uh Pro most likely it's",
    "start": "1490440",
    "end": "1496360"
  },
  {
    "text": "going to authenticate in the same way that you would like with your Cube control right it's going to point the cube config environment variable to your",
    "start": "1496360",
    "end": "1502559"
  },
  {
    "text": "Cube config um and it's going to uh but but the difference is that like it's going to generate a lot more traffic",
    "start": "1502559",
    "end": "1508520"
  },
  {
    "text": "than you would with Q control right like Q control is basically you know you run single commands that are you know list",
    "start": "1508520",
    "end": "1515000"
  },
  {
    "text": "pods or delete pods whatever but if you run like a dashboard for example using",
    "start": "1515000",
    "end": "1520399"
  },
  {
    "text": "the same Q config it's going to basically try to like list the whole world just to show you like a nice like",
    "start": "1520399",
    "end": "1525799"
  },
  {
    "text": "UI for your cluster right so very different traffic pattern compared to those two",
    "start": "1525799",
    "end": "1531120"
  },
  {
    "text": "things another example is um yeah like you run a controller on a GCM that's",
    "start": "1531120",
    "end": "1536799"
  },
  {
    "text": "like not part of your GK cluster uh that's also going to use Global default because it's uh probably going to be",
    "start": "1536799",
    "end": "1543000"
  },
  {
    "text": "authenticated using a Google service account and not a kubernetes service account so generally we want any nonstem",
    "start": "1543000",
    "end": "1548720"
  },
  {
    "text": "control non-system controllers to use um the workload low priority level like we discussed",
    "start": "1548720",
    "end": "1554080"
  },
  {
    "text": "earlier uh some third party add-ons will run controller in the cube system names space um which puts traffic uh in the",
    "start": "1554080",
    "end": "1562039"
  },
  {
    "text": "workload high priority level um and like that's obviously bad because you know well it's not always bad but it can be",
    "start": "1562039",
    "end": "1568799"
  },
  {
    "text": "bad because workload high is shared with the scheduler the controller manager and whatnot and you don't want to accidentally put a high load controller",
    "start": "1568799",
    "end": "1575919"
  },
  {
    "text": "in the same priority as those as those system components because that can really mess up your",
    "start": "1575919",
    "end": "1582039"
  },
  {
    "text": "cluster and so like yeah like these are just some examples but like you can think of more scenarios and like it it quickly becomes",
    "start": "1582039",
    "end": "1588120"
  },
  {
    "text": "impossible to cover all the different cases of of massification so instead of trying to chase all these Corner cases",
    "start": "1588120",
    "end": "1595200"
  },
  {
    "text": "we need to kind of make this the system a little bit more resilient to this misclassification and so this is where",
    "start": "1595200",
    "end": "1602000"
  },
  {
    "text": "uh priority borrowing becomes really important uh so this was introduced in uh 126 and as a name suggests uh",
    "start": "1602000",
    "end": "1609360"
  },
  {
    "text": "priority borrowing is a way for the API server to lend unused capacity from one",
    "start": "1609360",
    "end": "1614679"
  },
  {
    "text": "priority level to another so this allows for the the uh available capacity in any",
    "start": "1614679",
    "end": "1620520"
  },
  {
    "text": "priority level to be more flexible and it prevents uh few clients from starving",
    "start": "1620520",
    "end": "1625799"
  },
  {
    "text": "out entire uh priority levels and so the the neatest thing about uh borrowing is that it it",
    "start": "1625799",
    "end": "1633440"
  },
  {
    "text": "actually allows us to better better utilized unused capacity in the control plane uh but in the event of overload",
    "start": "1633440",
    "end": "1642120"
  },
  {
    "text": "everything's going to kind of self-correct to what whatever fixed capacity that we gave it so optimistic",
    "start": "1642120",
    "end": "1647760"
  },
  {
    "text": "Al it'll always try to use like all the unavailable capacity and the other priority levels and then if you kind of",
    "start": "1647760",
    "end": "1653240"
  },
  {
    "text": "reach a Tipping Point everything will kind of just be using the fixed priority that you've kind of assigned it um",
    "start": "1653240",
    "end": "1660200"
  },
  {
    "text": "ignoring kind of borrowing configurations um and so yeah for for reference uh within each priority level",
    "start": "1660200",
    "end": "1666519"
  },
  {
    "text": "there's two fields that you can tune um the borrowing limit percent which is the amount you can borrow from the other",
    "start": "1666519",
    "end": "1672000"
  },
  {
    "text": "priority levels and then lendable percent lendable percent is the amount other priority levels can borrow from",
    "start": "1672000",
    "end": "1677760"
  },
  {
    "text": "you right so we can kind of like tune these values so that uh for example like",
    "start": "1677760",
    "end": "1683120"
  },
  {
    "text": "we might not we actually there there's some priority levels where we absolutely don't want to let other party levels",
    "start": "1683120",
    "end": "1688159"
  },
  {
    "text": "borrow from it no matter what and so these are values that we can kind of tune um and yeah luckily in GK we",
    "start": "1688159",
    "end": "1694679"
  },
  {
    "text": "haven't actually had to tune this at all like the defaults have been working great but uh it's probably too too soon",
    "start": "1694679",
    "end": "1700440"
  },
  {
    "text": "to say at this point we might we might need to revisit this okay so uh think we running out",
    "start": "1700440",
    "end": "1706679"
  },
  {
    "text": "time so really quickly uh do you want to talk about web hooks so web hooks can",
    "start": "1706679",
    "end": "1712519"
  },
  {
    "text": "deny service your your control plane as many of you probably already know so",
    "start": "1712519",
    "end": "1718000"
  },
  {
    "text": "especially web hooks that use uh Wild Card matches right um and this is web",
    "start": "1718000",
    "end": "1723880"
  },
  {
    "text": "hooks are I'm very conflicted about web hooks because often times web hooks serve very valuable P purpose right",
    "start": "1723880",
    "end": "1730159"
  },
  {
    "text": "whether it's policy control or you know side car injection whatever and we can't",
    "start": "1730159",
    "end": "1736000"
  },
  {
    "text": "a lot of times like we can't just like turn them on off right um because you know you you're obviously you're installing the webbook for a valuable",
    "start": "1736000",
    "end": "1742279"
  },
  {
    "text": "reason um and also there's kind of this like shared responsibility between gke",
    "start": "1742279",
    "end": "1748120"
  },
  {
    "text": "and our customers like if you deploy web hook you're basically extending the control plane but like we're not we",
    "start": "1748120",
    "end": "1754360"
  },
  {
    "text": "can't manage the actual like services that are back in the web hook um so what we've done is we built some systems at",
    "start": "1754360",
    "end": "1760240"
  },
  {
    "text": "Google to provide some more um proactive uh insights and recommendations to customers to let them know about web",
    "start": "1760240",
    "end": "1766640"
  },
  {
    "text": "hooks that we think POS some potential risk um to the control plane um uh so if you use GK for",
    "start": "1766640",
    "end": "1775080"
  },
  {
    "text": "a while you you might actually be familiar with this so we actually do this for um for example for deprecated apis if your cluster is using some",
    "start": "1775080",
    "end": "1781679"
  },
  {
    "text": "deprecated API and you're about to upgrade to a version that removes those deprecated apis we actually have like a",
    "start": "1781679",
    "end": "1788000"
  },
  {
    "text": "bunch of UI that tells you like hey like you're using your clust we detected that your cluster is using a deprecated API",
    "start": "1788000",
    "end": "1794320"
  },
  {
    "text": "um and we're going to block your upgrades until you kind of resolve this and we kind of use that same system um",
    "start": "1794320",
    "end": "1799679"
  },
  {
    "text": "to to kind of look look at web Hooks and say like hey this web Hook is intercepting uh like leases in the cube",
    "start": "1799679",
    "end": "1806159"
  },
  {
    "text": "node lease uh namespace and you your web Hook is in the critical path of like node heartbeats for example and so we",
    "start": "1806159",
    "end": "1813000"
  },
  {
    "text": "actually um have this uh detection mechanism now to proactively let customers know about the web hooks um so",
    "start": "1813000",
    "end": "1819799"
  },
  {
    "text": "yeah this is the this example so yeah if we detect um unavail unavailability of a",
    "start": "1819799",
    "end": "1825279"
  },
  {
    "text": "service back in your web hook we have this UI that pops up um so the earlier",
    "start": "1825279",
    "end": "1830679"
  },
  {
    "text": "slide shows this is the I read I sorry I redacted a bunch of stuff um but this is like the kubernetes cluster list right",
    "start": "1830679",
    "end": "1838080"
  },
  {
    "text": "and then there's this notification column that has a bunch of like warnings so like uh the API deprecations would be",
    "start": "1838080",
    "end": "1846000"
  },
  {
    "text": "one example or like the recent do shim deprecation would be would be another one but we have a specific like warning",
    "start": "1846000",
    "end": "1851919"
  },
  {
    "text": "for web hooks now and if you click that it'll either show one of these two kind",
    "start": "1851919",
    "end": "1856960"
  },
  {
    "text": "of warnings one is uh if you reference a service for a webbook and that service was down for some amount of time we'll",
    "start": "1856960",
    "end": "1863440"
  },
  {
    "text": "actually tell you like hey you're you know you have a Web book in your cluster and the service is is not available and",
    "start": "1863440",
    "end": "1869360"
  },
  {
    "text": "so we we have some like docs to help you troubleshoot those kind of situations uh and then yeah if we detect web hooks",
    "start": "1869360",
    "end": "1875159"
  },
  {
    "text": "that intercept um what we deem as like system critical resources then we'll",
    "start": "1875159",
    "end": "1880559"
  },
  {
    "text": "we'll provide some recommendations to uh update your web hook to exclude those name spaces um so yeah Tech way is that web",
    "start": "1880559",
    "end": "1887960"
  },
  {
    "text": "hooks can take out take down your cluster and there are some best practices that you can follow so mainly",
    "start": "1887960",
    "end": "1894480"
  },
  {
    "text": "uh ensure the web hook has sufficient capacity and run multiple replicas if",
    "start": "1894480",
    "end": "1899799"
  },
  {
    "text": "you can uh don't intercept um system critical requests so some examples are",
    "start": "1899799",
    "end": "1906559"
  },
  {
    "text": "like lease traffic in Cube node lease or lease traffic in Cube system right because uh those are used for leader",
    "start": "1906559",
    "end": "1913159"
  },
  {
    "text": "election for uh system components um things like that and you can actually um",
    "start": "1913159",
    "end": "1918720"
  },
  {
    "text": "use a namespace selector field in the in the weapa configuration to actually exclude entire",
    "start": "1918720",
    "end": "1924120"
  },
  {
    "text": "namespaces and then more recently in 1.28 we introduced um match conditions",
    "start": "1924120",
    "end": "1929919"
  },
  {
    "text": "so you can use like cell expression rules to basically uh exclude um uh or",
    "start": "1929919",
    "end": "1936080"
  },
  {
    "text": "like you can basically use uh cell to basically like filter out um or decide",
    "start": "1936080",
    "end": "1941519"
  },
  {
    "text": "when you should invoke the webbook or not um and this is actually a feature that we've built with some some folks",
    "start": "1941519",
    "end": "1947440"
  },
  {
    "text": "working on eks because we're both looking into ways to reduce the blast radius of web",
    "start": "1947440",
    "end": "1954279"
  },
  {
    "text": "books and yeah that concludes my talk um yeah happy to take questions if there are",
    "start": "1954279",
    "end": "1960519"
  },
  {
    "text": "any oh thanks by the [Applause]",
    "start": "1960519",
    "end": "1968039"
  },
  {
    "text": "way sorry I like kind of rushed the talking to the end because I didn't want to run out of time for",
    "start": "1970159",
    "end": "1976039"
  },
  {
    "text": "questions",
    "start": "1976039",
    "end": "1979039"
  },
  {
    "text": "all right cool thanks for attending appreciate",
    "start": "1988919",
    "end": "1993840"
  },
  {
    "text": "it I'll also be like by the hallway if anyone wants to like chat about any of the stuff",
    "start": "1994320",
    "end": "2001919"
  }
]