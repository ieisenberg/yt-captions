[
  {
    "text": "hi everyone um thank you I know it's evening and thank you for joining the",
    "start": "80",
    "end": "6960"
  },
  {
    "text": "session today um today we're going to talk about Dynamic large scale sparkon",
    "start": "6960",
    "end": "12360"
  },
  {
    "text": "cuetes with Argo workflows and Argo events um before we kick off this",
    "start": "12360",
    "end": "19480"
  },
  {
    "text": "session I would like to check how many of you are currently running spark on cuetes",
    "start": "19480",
    "end": "25760"
  },
  {
    "text": "today yeah back uh how many of you are using work FL",
    "start": "25760",
    "end": "31320"
  },
  {
    "text": "today for any type of a right thank you very much that gives um probably you can",
    "start": "31320",
    "end": "37360"
  },
  {
    "text": "take away some of the uh uh points from this session today um my name is wuntu",
    "start": "37360",
    "end": "43760"
  },
  {
    "text": "I'm a principal Solutions architect working for AWS uh I'm specialized in data analytics and kubernetes and today",
    "start": "43760",
    "end": "51199"
  },
  {
    "text": "I have my colleague IDU hello everybody I'm very excited to be here my name is",
    "start": "51199",
    "end": "56559"
  },
  {
    "text": "zidu I'm a container specialist solution architect at AWS and um I'm I'm excited",
    "start": "56559",
    "end": "62760"
  },
  {
    "text": "to have this uh presentation together with my friend VAR in front of you all right thanks so",
    "start": "62760",
    "end": "69600"
  },
  {
    "text": "do uh without further Ado let's get started uh that's not working okay",
    "start": "69600",
    "end": "78720"
  },
  {
    "text": "sorry right um just a quick agenda uh we're going to talk about I give you a",
    "start": "78720",
    "end": "84640"
  },
  {
    "text": "little bit of intro for anyone who doesn't know about spark on cuetes but just a introduction to spark on",
    "start": "84640",
    "end": "90439"
  },
  {
    "text": "kubernetes and then we move on to why you need why customers want to run spark",
    "start": "90439",
    "end": "96040"
  },
  {
    "text": "on kubernetes and uh and we discuss about some of the best practices for running spark and",
    "start": "96040",
    "end": "101479"
  },
  {
    "text": "communities and especially and today's talk is about you know when it comes to",
    "start": "101479",
    "end": "108040"
  },
  {
    "text": "running a large scale spark on communities what kind of considerations and the best practices that you need to",
    "start": "108040",
    "end": "113880"
  },
  {
    "text": "uh consider before deploying your workloads onto kubernetes so we'll touch upon those best practices",
    "start": "113880",
    "end": "120799"
  },
  {
    "text": "um and uh we also talk about ago workl and ago events like you know some of these data pipelines that you're",
    "start": "120799",
    "end": "126680"
  },
  {
    "text": "building um you need to ensure that these data pipelines are running on specific schedulers workflow engines and",
    "start": "126680",
    "end": "134560"
  },
  {
    "text": "uh we will dive into the AG and Aro events and finally the demo we prepared a demo that will showcase how you can",
    "start": "134560",
    "end": "142040"
  },
  {
    "text": "actually create a dag with multiple spark jobs and Trigger those spark jobs using both spark operator and Spark",
    "start": "142040",
    "end": "148879"
  },
  {
    "text": "submit as well so that's what we are planning today so let's get started on the spark",
    "start": "148879",
    "end": "154720"
  },
  {
    "text": "conunity so as you see on the slide um as you all knows Apachi spark is a",
    "start": "154720",
    "end": "160720"
  },
  {
    "text": "distributed processing framework and which is mainly used for processing terabytes and even petabytes of data at",
    "start": "160720",
    "end": "167480"
  },
  {
    "text": "a very large scale with both unstructured even structured data for Apachi spark it comes with set of",
    "start": "167480",
    "end": "173599"
  },
  {
    "text": "liaries such as spark SQL uh ml streaming and Graphics is mainly for",
    "start": "173599",
    "end": "179280"
  },
  {
    "text": "various data data processing such as uh data processing machine learning and realtime data processing and graph data",
    "start": "179280",
    "end": "186239"
  },
  {
    "text": "processing so these set of libraries that you can use to process various set of data and you can run aachi spark on a",
    "start": "186239",
    "end": "192400"
  },
  {
    "text": "standalone machine such as your Windows machine or a laptop um but to run Apachi",
    "start": "192400",
    "end": "197920"
  },
  {
    "text": "spark on a distributed mode uh to process terabytes of data that is when apachi spark needs a resource manager",
    "start": "197920",
    "end": "204400"
  },
  {
    "text": "such as Hadoop you might be familiar uh Hadoop yon and Y is used as a resource",
    "start": "204400",
    "end": "210040"
  },
  {
    "text": "manager to actually distribute that uh executors across these instances and run",
    "start": "210040",
    "end": "215599"
  },
  {
    "text": "your workload uh but then uh in 2018 that's when uh Apachi spark added a support uh",
    "start": "215599",
    "end": "222920"
  },
  {
    "text": "for cuetes as a resource manager that means you can actually run uh spark",
    "start": "222920",
    "end": "228080"
  },
  {
    "text": "workloads onto kubernetes um now um you know uh with that",
    "start": "228080",
    "end": "233280"
  },
  {
    "text": "version uh that's when the shift started customers have started to think about um you know we want to migrate our existing",
    "start": "233280",
    "end": "239360"
  },
  {
    "text": "work Le to spark on cubes now and how matured it is and because of all the scalability features that uh cubes",
    "start": "239360",
    "end": "247480"
  },
  {
    "text": "offers so why spark on kubes and probably that's the common question that everybody asks and I know most of you",
    "start": "247480",
    "end": "254400"
  },
  {
    "text": "know about kubes because you here uh kubernetes it's very powerful container",
    "start": "254400",
    "end": "259759"
  },
  {
    "text": "orchestration tool which you can uh use and leverage some of these features one of them is dynamic scaling right you can",
    "start": "259759",
    "end": "266759"
  },
  {
    "text": "leverage you can run your spark work clouds bursty work clads from zero notes to thousand notes uh when you trigger",
    "start": "266759",
    "end": "273000"
  },
  {
    "text": "the job and you can scale down when you don't want to run the jobs using cluster autoscaler or Carpenter um in using the",
    "start": "273000",
    "end": "279560"
  },
  {
    "text": "Autos scaling features portability allows you to actually write your spark",
    "start": "279560",
    "end": "285000"
  },
  {
    "text": "job and containerize it and run it on any type of quetes Flavor of the cluster",
    "start": "285000",
    "end": "291039"
  },
  {
    "text": "right so uh you can use uh eks gke or even on Prim kubernetes clusters and",
    "start": "291039",
    "end": "297960"
  },
  {
    "text": "should work and resource isolation it's one of the key feature uh within the",
    "start": "297960",
    "end": "303160"
  },
  {
    "text": "spark like you can Define every single job can have its own CPU and memory",
    "start": "303160",
    "end": "308360"
  },
  {
    "text": "definition that way you are ensuring that your job doesn't take more than that CPU and memory that you allocated",
    "start": "308360",
    "end": "314080"
  },
  {
    "text": "so that gives you that resource isolation ensures that every job gets its own quota of the job and it's a",
    "start": "314080",
    "end": "320840"
  },
  {
    "text": "cloud diagnostic right so you can develop the script and if you think about migrating onto from on Prem to",
    "start": "320840",
    "end": "327840"
  },
  {
    "text": "more Cloud native tools like it you know eks or GK any other uh kubernetes uh",
    "start": "327840",
    "end": "334120"
  },
  {
    "text": "Solutions you can run it um and the multi- tency uh one of the key feature",
    "start": "334120",
    "end": "339600"
  },
  {
    "text": "about running spark on kuties is the multi tendency right so you can have namespace isolations multiple teams can",
    "start": "339600",
    "end": "345560"
  },
  {
    "text": "share the same cluster and run the multiple bords and running multiple versions of",
    "start": "345560",
    "end": "350880"
  },
  {
    "text": "the spark I think this is the one of the key feature um when you're using Hadoop you might have that situation that you",
    "start": "350880",
    "end": "356960"
  },
  {
    "text": "have to create a dedicated Hadoop cluster for every single spark version but with kubernetes you can use the same",
    "start": "356960",
    "end": "364080"
  },
  {
    "text": "kubernetes cluster such as eks in this case I'm talking about uh you can have multiple versions of the spark running",
    "start": "364080",
    "end": "369520"
  },
  {
    "text": "on the same communities that's great feature and the cncf ecosystem so now you want to monitor uh your spark jobs",
    "start": "369520",
    "end": "376319"
  },
  {
    "text": "and you want to create dashboards and you want to uh you know extract the logs and cncf ecosystem comes with a lot of",
    "start": "376319",
    "end": "383120"
  },
  {
    "text": "Open Source um add-ons which you can leverage to uh build a good ecosystem or",
    "start": "383120",
    "end": "389120"
  },
  {
    "text": "a full data Pipeline and cost optimization so all of these features actually drives the cost",
    "start": "389120",
    "end": "395639"
  },
  {
    "text": "optimization compared to other you know the traditional way of running on Hadoop uh spark on Hadoop uh just mainly the",
    "start": "395639",
    "end": "402720"
  },
  {
    "text": "autoscaling portability and multi tency right this is a simple uh uh",
    "start": "402720",
    "end": "409360"
  },
  {
    "text": "command like spark submit so it's no different from how you run on H Hadoop and if you see the master URL points to",
    "start": "409360",
    "end": "416080"
  },
  {
    "text": "the K API server so if you want to run your Park job and using this command you can run it on your local machine um and",
    "start": "416080",
    "end": "423960"
  },
  {
    "text": "and then points to the K8 server that can be any eks or any other kubernetes cluster and and the job will run let's",
    "start": "423960",
    "end": "431280"
  },
  {
    "text": "let's dig into how the job that runs on kubernetes so this is uh this is the",
    "start": "431280",
    "end": "438400"
  },
  {
    "text": "internals of how you can run spark on kuet like you how it works when you submit a job as you see on that on the",
    "start": "438400",
    "end": "444720"
  },
  {
    "text": "left hand side we have kubernetes control plane and the right hand side kubernetes data plane so when you submit",
    "start": "444720",
    "end": "450160"
  },
  {
    "text": "your spark submit from your local machine or from airflow or any other schedulers and the job will be submitted",
    "start": "450160",
    "end": "455720"
  },
  {
    "text": "to an API server and an API scheduler will schedule the driver pod that is the",
    "start": "455720",
    "end": "460879"
  },
  {
    "text": "first step uh once the driver pod is created but it's not showing there even there's a headless service also created",
    "start": "460879",
    "end": "467759"
  },
  {
    "text": "along with the driver part and the driver P will request scheduler to will",
    "start": "467759",
    "end": "473039"
  },
  {
    "text": "request API server to schedule the executors so how many other executors",
    "start": "473039",
    "end": "478240"
  },
  {
    "text": "and the scheduler will schedule those executors to you know and once these executors started running uh and these",
    "start": "478240",
    "end": "484479"
  },
  {
    "text": "executors will will make a connection to the driver using that headless service that's how the driver communicates with",
    "start": "484479",
    "end": "491039"
  },
  {
    "text": "the executors and send the task down to the executors and the jobs will run within individual executors so that",
    "start": "491039",
    "end": "497919"
  },
  {
    "text": "that's how Spar on kubernetes Works uh pretty much uh the communication end to",
    "start": "497919",
    "end": "504759"
  },
  {
    "text": "end but now as you see here this is a yamal file this is a spark operator but",
    "start": "504759",
    "end": "511039"
  },
  {
    "text": "within kubernetes World um everything that we want to run in a yaml declarator way right so what we have seen before is",
    "start": "511039",
    "end": "517959"
  },
  {
    "text": "a spark submit is a simple Json config with a CLI command but rather you want to Define your spark jobs in a simple",
    "start": "517959",
    "end": "524399"
  },
  {
    "text": "yaml file with you know the job name and as you see here the driver codes and",
    "start": "524399",
    "end": "529959"
  },
  {
    "text": "executor Cotes and number of executors that you need so this simplifies the whole process of running a spark job so",
    "start": "529959",
    "end": "535560"
  },
  {
    "text": "I can just simply write this job and then use CBE CTL to to apply and that",
    "start": "535560",
    "end": "540959"
  },
  {
    "text": "goes and runs your spark job on kubernetes right so how to uh get to",
    "start": "540959",
    "end": "547200"
  },
  {
    "text": "this point and how do we actually uh um you know leverage spark operator let's talk about a bit more about spark",
    "start": "547200",
    "end": "553360"
  },
  {
    "text": "operator so Google uh has created this uh new kubes operator for spark to",
    "start": "553360",
    "end": "560079"
  },
  {
    "text": "simplify that process so what it does differently from Spar submit let's take a look at this one here so you have the",
    "start": "560079",
    "end": "566519"
  },
  {
    "text": "created two um uh uh CDs which is spark application and scheduled spark",
    "start": "566519",
    "end": "572079"
  },
  {
    "text": "application and it comes with four main components one is controller submission job Runner and Spark pod Monitor and",
    "start": "572079",
    "end": "579200"
  },
  {
    "text": "then mutating admission web Hub so it's basically a controller uh which will uh look for the spark application object",
    "start": "579200",
    "end": "586240"
  },
  {
    "text": "yaml that you have seen before and then controller will ask the submission uh",
    "start": "586240",
    "end": "591480"
  },
  {
    "text": "submission job Runner to actually submit the job and submission job Runner what",
    "start": "591480",
    "end": "596519"
  },
  {
    "text": "it does is it converts that yaml into a spark submit assembles into a spark submit and then submits to kubernetes",
    "start": "596519",
    "end": "604200"
  },
  {
    "text": "API server that's which is similar to what you have seen with the spark submit so even spark operator is leveraging",
    "start": "604200",
    "end": "610360"
  },
  {
    "text": "spark submit behind the scenes once a job is submitted and Spark operator does",
    "start": "610360",
    "end": "616120"
  },
  {
    "text": "it in uh differently in two things one is Spar pod Monitor and the mutating",
    "start": "616120",
    "end": "621240"
  },
  {
    "text": "admission web hook so once a job is submitted uh you might need some storage you know mounting the volumes or",
    "start": "621240",
    "end": "627160"
  },
  {
    "text": "mounting configuration maps and so on so mutating admission web Hook is responsible before the Pod is persisted",
    "start": "627160",
    "end": "633800"
  },
  {
    "text": "it goes and creates the volumes and mounts the volumes and you know and ensure the Pod is created with all the",
    "start": "633800",
    "end": "640240"
  },
  {
    "text": "volumes and the SP uh spark pod monitor this ensure the status of driver status",
    "start": "640240",
    "end": "645959"
  },
  {
    "text": "executive status and keeps informing to the controller saying the job is running so if in any case if the job is failed",
    "start": "645959",
    "end": "652880"
  },
  {
    "text": "and the controller can restart the job so that's how the operator works and which is more a simplified way Than The",
    "start": "652880",
    "end": "659680"
  },
  {
    "text": "Spar submit but underneath it uses The Spar Spar",
    "start": "659680",
    "end": "665399"
  },
  {
    "text": "submit right so um the scaling spark and kuber",
    "start": "667240",
    "end": "673279"
  },
  {
    "text": "challeng some of the common challenges that we when we work with the customers they come back and say hey we tried",
    "start": "673279",
    "end": "678399"
  },
  {
    "text": "sparken cuetes but when when it comes to scaling when we go beyond 200 noes and even thousand notes and we we we hit all",
    "start": "678399",
    "end": "685200"
  },
  {
    "text": "sorts of issues and what kind of consideration we need to take and how Val ility consideration what kind of",
    "start": "685200",
    "end": "690440"
  },
  {
    "text": "logging and monitoring that we need to build when we run spark and kubernetes and choosing the right compute and",
    "start": "690440",
    "end": "696600"
  },
  {
    "text": "storage and what kind of network configurations that we need to Define and do we need to use specific B",
    "start": "696600",
    "end": "703639"
  },
  {
    "text": "schulist to run the job so let's talk about some of those",
    "start": "703639",
    "end": "709360"
  },
  {
    "text": "best practices uh that we talked about some of the challenges that we talked before um as you see here in this",
    "start": "709360",
    "end": "715560"
  },
  {
    "text": "diagram that we have um VPC CN I'm not sure how many of you heard the",
    "start": "715560",
    "end": "721160"
  },
  {
    "text": "networking within eks we use VPC cni um uh within AWS but uh the similar",
    "start": "721160",
    "end": "727120"
  },
  {
    "text": "networking will be um available in other Cloud providers as well so with the VPC cni we use a pod networking so so when",
    "start": "727120",
    "end": "734920"
  },
  {
    "text": "it comes to running spark on kubes and you want to run 50,000 Parts you need 50,000 IPS so which means you might hit",
    "start": "734920",
    "end": "742959"
  },
  {
    "text": "uh you might have heard um you know we've hit IP exhaustion issues so to avoid IP exhaustion issues and one the",
    "start": "742959",
    "end": "749199"
  },
  {
    "text": "main consideration or recommendation would we do is create to uh uh use a",
    "start": "749199",
    "end": "756279"
  },
  {
    "text": "non-routable IP secondary CER wrange to create two large subnets and each comes with a 65,000 IPS so in this",
    "start": "756279",
    "end": "763839"
  },
  {
    "text": "architecture you can go up to 120,000 Parts running on within the same eks cluster if you want to run a large",
    "start": "763839",
    "end": "773199"
  },
  {
    "text": "scale and then the future the next step is hey uh we want to try IP V6 I know we",
    "start": "774040",
    "end": "780600"
  },
  {
    "text": "secondary side range is an option but um this is something that we are looking at in the future where the customers move",
    "start": "780600",
    "end": "786399"
  },
  {
    "text": "to the IPv6 and it's a lot easier uh to work with so cuetes from version 1.23 it",
    "start": "786399",
    "end": "792959"
  },
  {
    "text": "supports IPv6 and even spark 340 added a support for IPv6 as well so all that you",
    "start": "792959",
    "end": "799680"
  },
  {
    "text": "need to do is the configuration that you see here when you submit the job and that allows you to leverage IP V6 and",
    "start": "799680",
    "end": "806040"
  },
  {
    "text": "avoids IP exhaustion you have a you know uh the large number of PS that you want to run within the same kubernetes",
    "start": "806040",
    "end": "813440"
  },
  {
    "text": "cluster right so this is one of the common issue when it comes to the code DNS um like if you're running a large",
    "start": "813440",
    "end": "820480"
  },
  {
    "text": "scale uh spark and kues and the first thing you might see an error something related to unknown host exception it's",
    "start": "820480",
    "end": "826920"
  },
  {
    "text": "because uh the nature of the spark is a bursty workload so you go and hit 10,000",
    "start": "826920",
    "end": "832040"
  },
  {
    "text": "parts and trying to spin up and trying to communicate with each other and and putting a pressure on the code DNS So to",
    "start": "832040",
    "end": "838279"
  },
  {
    "text": "avoid that we recommend using cluster proportional autoscaler with the Cod DNS which means when your kubernetes cluster",
    "start": "838279",
    "end": "845560"
  },
  {
    "text": "is growing uh and your code DNS Parts horizontally scales to support that uh the large scale kues class to and then",
    "start": "845560",
    "end": "852639"
  },
  {
    "text": "we also recommend using node local DNS cashing which is basically uh uh you",
    "start": "852639",
    "end": "857920"
  },
  {
    "text": "know having a cache in every single node so that it doesn't need to make DNS",
    "start": "857920",
    "end": "863440"
  },
  {
    "text": "resolution called to the Cod DNS ports uh it can refer to the local caching and reduces that amount of calls that makes",
    "start": "863440",
    "end": "870000"
  },
  {
    "text": "the CNS and improves the performance for the large scale cubetas clusters and finally the Pod anti-affinity just make",
    "start": "870000",
    "end": "876880"
  },
  {
    "text": "sure that you don't run uh more than two you know more than one part within the same note and these Cod DNS parts are",
    "start": "876880",
    "end": "883480"
  },
  {
    "text": "actually spread across all the nodes rather than running everything in a single",
    "start": "883480",
    "end": "888800"
  },
  {
    "text": "node right so one of the key uh important feature here is uh the storage",
    "start": "889240",
    "end": "894720"
  },
  {
    "text": "when it comes to the spark um you know you think about storage it's what kind of storage that we need to use and we",
    "start": "894720",
    "end": "901600"
  },
  {
    "text": "highly recommend using NVM SS based volumes uh because spark is a nature",
    "start": "901600",
    "end": "907000"
  },
  {
    "text": "does a lot of shuffling um and NVM based SS volumes as you see this is part of the E instance itself like uh when user",
    "start": "907000",
    "end": "915240"
  },
  {
    "text": "uh when the Spock job runs it goes and pulls the data from S3 bucket into that NVM SSD and these executors can uh uh",
    "start": "915240",
    "end": "922959"
  },
  {
    "text": "you know work with the data quite quickly without having to you know without having any latency so this gives",
    "start": "922959",
    "end": "928759"
  },
  {
    "text": "you high throughput low latency uh you know the uh performance for the spark",
    "start": "928759",
    "end": "936040"
  },
  {
    "text": "jobs but you also have another option where you can leverage the block block storage which is EBS volumes as well but",
    "start": "936040",
    "end": "942880"
  },
  {
    "text": "uh the only difference here is you get a variable throughput because e um the volume is external to the ec2 instance",
    "start": "942880",
    "end": "949920"
  },
  {
    "text": "and you have and EBS bandwidth uh B various based on the E2 instance that",
    "start": "949920",
    "end": "955759"
  },
  {
    "text": "you choose the smaller instance and the larger instance and but it also comes with other features like reusing",
    "start": "955759",
    "end": "962440"
  },
  {
    "text": "persistent volume claim like when you wanted to leverage re reuse the persistent volume claim is a feature",
    "start": "962440",
    "end": "968399"
  },
  {
    "text": "within kubernetes where if one of the node dies uh for some reason um you know",
    "start": "968399",
    "end": "974399"
  },
  {
    "text": "even if the node dies and the PO gets killed you can still keep the um the the",
    "start": "974399",
    "end": "979880"
  },
  {
    "text": "volumes that are connected to that old node and reuse those volumes with the",
    "start": "979880",
    "end": "984920"
  },
  {
    "text": "new node that comes up so basically it doesn't need to recompute it can use where it is",
    "start": "984920",
    "end": "991319"
  },
  {
    "text": "started give it to me yep you want to yeah thank you",
    "start": "991319",
    "end": "996440"
  },
  {
    "text": "hey now what options do we have when when we're running from the compute",
    "start": "996440",
    "end": "1002000"
  },
  {
    "text": "options when you're running the spark on the kues when you are deploying the spark ontic the executor can be",
    "start": "1002000",
    "end": "1007319"
  },
  {
    "text": "scheduled on the spot instances and the driver on the on demand instances and",
    "start": "1007319",
    "end": "1013279"
  },
  {
    "text": "they this in scheduling the executors on the spot instances this enables the",
    "start": "1013279",
    "end": "1018680"
  },
  {
    "text": "faster results by scaling and executors running on it and there is also another",
    "start": "1018680",
    "end": "1023880"
  },
  {
    "text": "way because another reason because if a p If a driver P it it will be run on the spot instances and the spot instances",
    "start": "1023880",
    "end": "1031000"
  },
  {
    "text": "gets terminated then the the all application fails and have to be resubmitted while if they that's why we",
    "start": "1031000",
    "end": "1038760"
  },
  {
    "text": "recommend that the driver should be always installed on the on demand instances the executors though even if",
    "start": "1038760",
    "end": "1045120"
  },
  {
    "text": "the the spotting get terminated the resiliency from the spark the driver",
    "start": "1045120",
    "end": "1050160"
  },
  {
    "text": "will create a new executor once a new spot instance is it is uh it is being",
    "start": "1050160",
    "end": "1055679"
  },
  {
    "text": "created and like this and like this you achieve the uh also the the cost",
    "start": "1055679",
    "end": "1061039"
  },
  {
    "text": "efficiency and um within your",
    "start": "1061039",
    "end": "1065919"
  },
  {
    "text": "cluster while running spark on the um on the cernes you can experience",
    "start": "1066160",
    "end": "1071240"
  },
  {
    "text": "challenges with the scalability and performance but with the carpenter the spark clusters can quickly can be",
    "start": "1071240",
    "end": "1077960"
  },
  {
    "text": "quickly scale and dynamically and in the same times you and the workloads meets",
    "start": "1077960",
    "end": "1084360"
  },
  {
    "text": "the the need that they want to and in the same time you achieve the resource availability and the cost efficiency",
    "start": "1084360",
    "end": "1090640"
  },
  {
    "text": "with with it and Carpenter is coming also with some uh features improvements to workflow",
    "start": "1090640",
    "end": "1097960"
  },
  {
    "text": "cons consolidation de-provisioning network no termination and further",
    "start": "1097960",
    "end": "1103240"
  },
  {
    "text": "improving the efficiency and the uh cost savings a PO unicorn that you'll see",
    "start": "1103240",
    "end": "1110600"
  },
  {
    "text": "actually in our demo uh it's a b sched purposely built for the kubernetes uh",
    "start": "1110600",
    "end": "1116200"
  },
  {
    "text": "and usually used into uh big data and ML",
    "start": "1116200",
    "end": "1121840"
  },
  {
    "text": "workloads and it is particularly useful for kubernetes with applications like uh",
    "start": "1121840",
    "end": "1126960"
  },
  {
    "text": "with the features like application aware scheduling where it is recognize the users the",
    "start": "1126960",
    "end": "1133159"
  },
  {
    "text": "applications and the cues the scheduling according with the uh submission order",
    "start": "1133159",
    "end": "1139799"
  },
  {
    "text": "it has also gang scheduling for the Pod placement and you will see you will see this also in the demo and a hierarchy",
    "start": "1139799",
    "end": "1147720"
  },
  {
    "text": "cues for builtin for optimized spark",
    "start": "1147720",
    "end": "1152559"
  },
  {
    "text": "performance um for the logging flu bit um kubernetes filter allow you to enrich",
    "start": "1153039",
    "end": "1160720"
  },
  {
    "text": "your log files with kubernetes metadata and if you are using the filters youing",
    "start": "1160720",
    "end": "1166280"
  },
  {
    "text": "the fil fluent bit filter extra filter config um you can get the metadata uh",
    "start": "1166280",
    "end": "1173600"
  },
  {
    "text": "from with with cuet instead of um quering the API very specific very",
    "start": "1173600",
    "end": "1180880"
  },
  {
    "text": "useful in when you have large custs because you don't put pressure on the API",
    "start": "1180880",
    "end": "1187039"
  },
  {
    "text": "anymore sorry S one more okay oh now getting into the Argo workflow this is a",
    "start": "1191080",
    "end": "1198280"
  },
  {
    "text": "dor trator a workflow engineer is the best way to run kubernetes um uh because",
    "start": "1198280",
    "end": "1204720"
  },
  {
    "text": "it is built for kubernetes all right it is very uh popular and very useful when",
    "start": "1204720",
    "end": "1210360"
  },
  {
    "text": "you're running ETL workloads or training jobs and you can orchestrate deployment",
    "start": "1210360",
    "end": "1216520"
  },
  {
    "text": "and then us it in your devops and CD pipelines with Argo workflows and the",
    "start": "1216520",
    "end": "1222559"
  },
  {
    "text": "Argo workflows it can be integrated with Argo events and the Argo events will will fetch any even",
    "start": "1222559",
    "end": "1229159"
  },
  {
    "text": "Source uh whatever it is CFA slack Web different web hooks you name it and",
    "start": "1229159",
    "end": "1235679"
  },
  {
    "text": "you'll see also in our example when you'll have like an external external source that will trigger an event and",
    "start": "1235679",
    "end": "1243799"
  },
  {
    "text": "consequently an orlow and a job and this is actually what I'm going",
    "start": "1243799",
    "end": "1249520"
  },
  {
    "text": "to show you to you in the demo today so in the demo we created an Amazon",
    "start": "1249520",
    "end": "1256000"
  },
  {
    "text": "cluster and install the ARG workflows in Argo events in their dedicated Nam spaces as you can see on the screen Argo",
    "start": "1256000",
    "end": "1263120"
  },
  {
    "text": "events and Argo Argo name Argo workflows in the same time as an an",
    "start": "1263120",
    "end": "1269120"
  },
  {
    "text": "outside Source okay we create we we have an Amazon sqsq that is created to",
    "start": "1269120",
    "end": "1275039"
  },
  {
    "text": "receive requests from users okay and the and the sqs Event Source that you going",
    "start": "1275039",
    "end": "1281279"
  },
  {
    "text": "to you'll see it in to the Argo events namespace object okay that is set up to",
    "start": "1281279",
    "end": "1287440"
  },
  {
    "text": "fetch the events from that uh sqsq that event can come from any like s 3ut",
    "start": "1287440",
    "end": "1295120"
  },
  {
    "text": "object notification not necessary from users yes which triggers this is only for the that we choose for the demo",
    "start": "1295120",
    "end": "1301559"
  },
  {
    "text": "purposes the sensors runs in ways for the certain condition to be made okay",
    "start": "1301559",
    "end": "1307159"
  },
  {
    "text": "and we will go through the how the sensor is built and when the the message is",
    "start": "1307159",
    "end": "1313880"
  },
  {
    "text": "received by The Event Source uh the sensor sees it and and creates an",
    "start": "1313880",
    "end": "1319360"
  },
  {
    "text": "workflow and the workflow through the spark operator um um create a spark job and uh",
    "start": "1319360",
    "end": "1328320"
  },
  {
    "text": "on in the spark team a name space obviously V mentioned earlier the",
    "start": "1328320",
    "end": "1334840"
  },
  {
    "text": "multi- tency so you can do like that in separate name spaces it allows you to build that end",
    "start": "1334840",
    "end": "1341360"
  },
  {
    "text": "to end data pipeline using Argo workflows and and you can drive those data pipelines using Argo events in this",
    "start": "1341360",
    "end": "1346919"
  },
  {
    "text": "case and you have spark op Ator Prometheus and Unicon sit in the within the same kues cluster um okay so and now",
    "start": "1346919",
    "end": "1354679"
  },
  {
    "text": "I am going to jump to the demo okay as you can see",
    "start": "1354679",
    "end": "1363039"
  },
  {
    "text": "here yeah we can off the demo first and then we can talk about yes I I will I",
    "start": "1364039",
    "end": "1369480"
  },
  {
    "text": "will start because it takes 2 minutes for the for the job okay and then I'm",
    "start": "1369480",
    "end": "1374919"
  },
  {
    "text": "going to uh I'm going to tell you exactly what I did all",
    "start": "1374919",
    "end": "1380600"
  },
  {
    "text": "right okay while what we do is uh triggering this job and it's basically",
    "start": "1380600",
    "end": "1385799"
  },
  {
    "text": "uh just simple yaml configuration that uh you can Define the whole dag uh in a",
    "start": "1385799",
    "end": "1391279"
  },
  {
    "text": "yaml file using augo workflows and you can build the most complex a workflow",
    "start": "1391279",
    "end": "1396440"
  },
  {
    "text": "similar to you might be using a airflow um for uh scheduling your data and ml",
    "start": "1396440",
    "end": "1402600"
  },
  {
    "text": "jobs and now kuet uh a workplace is a very lightweight workflow engine is more",
    "start": "1402600",
    "end": "1408799"
  },
  {
    "text": "powerful that you can build your end to end data pipelines and include the ml",
    "start": "1408799",
    "end": "1414240"
  },
  {
    "text": "pipelines part of the data pipelines and and use agus as your you know native",
    "start": "1414240",
    "end": "1419640"
  },
  {
    "text": "engine to run your jobs so and all that you will be doing is",
    "start": "1419640",
    "end": "1426279"
  },
  {
    "text": "defining um uh simple yaml files in this case there is a sensor yaml file that o",
    "start": "1426279",
    "end": "1432640"
  },
  {
    "text": "is going to show you uh in a minute once the job is triggered hello allow me to trigger the",
    "start": "1432640",
    "end": "1439200"
  },
  {
    "text": "job and then we'll come back to you and then show you to what exactly what I did",
    "start": "1439200",
    "end": "1445039"
  },
  {
    "text": "so we have sqsq created um is going to send a message uh to an sqsq to invoke",
    "start": "1445039",
    "end": "1452840"
  },
  {
    "text": "that pipeline one second if to make sure that I have the variables in place and yeah",
    "start": "1452840",
    "end": "1460720"
  },
  {
    "text": "it's all",
    "start": "1460720",
    "end": "1463120"
  },
  {
    "text": "good",
    "start": "1466799",
    "end": "1469799"
  },
  {
    "text": "I didn't copy the whole part of it that's make it more",
    "start": "1472720",
    "end": "1480720"
  },
  {
    "text": "interesting yeah so",
    "start": "1485799",
    "end": "1490080"
  },
  {
    "text": "um I think when it comes to the production that you have these workflows so the the message from the and now I",
    "start": "1492279",
    "end": "1498760"
  },
  {
    "text": "can increase it a bit again the message from the SKS was sent and we should have",
    "start": "1498760",
    "end": "1506360"
  },
  {
    "text": "a workflow triggered yeah just here I can see that",
    "start": "1506360",
    "end": "1512679"
  },
  {
    "text": "on the web UI which is AO workflow web UI here if you can expand that a little bit yes and until until let me let me",
    "start": "1512679",
    "end": "1521039"
  },
  {
    "text": "get the that's the sensor that's the sensor and need to log in again on this",
    "start": "1521039",
    "end": "1526840"
  },
  {
    "text": "one I think yep one",
    "start": "1526840",
    "end": "1530200"
  },
  {
    "text": "second ago workflows provides a really nice UI that you can expose that U are",
    "start": "1532000",
    "end": "1537200"
  },
  {
    "text": "using engine X and or backed by any uh application load balancer or a network",
    "start": "1537200",
    "end": "1542880"
  },
  {
    "text": "load balancer and and set up ad authentication with your own L",
    "start": "1542880",
    "end": "1548279"
  },
  {
    "text": "configuration to make it more accessible so our our uh our Event Source is sqs",
    "start": "1548279",
    "end": "1557159"
  },
  {
    "text": "Spark workflow okay was uh met the dependencies for the um uh for the",
    "start": "1557159",
    "end": "1563520"
  },
  {
    "text": "sensor and created a a spark workflow and this one it is just happening right",
    "start": "1563520",
    "end": "1569360"
  },
  {
    "text": "now as you can see okay so make it a bit bigger for you all right okay and the",
    "start": "1569360",
    "end": "1575919"
  },
  {
    "text": "the workflow has several parallel jobs so one job it was they some simple job",
    "start": "1575919",
    "end": "1582559"
  },
  {
    "text": "hello world job and from this two from this job okay another two jobs has been",
    "start": "1582559",
    "end": "1587919"
  },
  {
    "text": "created and at the end a bigger one and",
    "start": "1587919",
    "end": "1593120"
  },
  {
    "text": "you will see that in this uh in the same time because I'm going to move to the",
    "start": "1593120",
    "end": "1598559"
  },
  {
    "text": "CLI when the pods are being created into the spark te main Nam space and also in",
    "start": "1598559",
    "end": "1603799"
  },
  {
    "text": "the same time the carpenter will provision the nodes for the driver and",
    "start": "1603799",
    "end": "1609399"
  },
  {
    "text": "also the spot instances for the executors behind the scenes it is using unicor as a gang scheduling uh it's",
    "start": "1609399",
    "end": "1617039"
  },
  {
    "text": "using car to scale your notes so basically you have a empty cluster just a cuetes cluster running and once you",
    "start": "1617039",
    "end": "1623600"
  },
  {
    "text": "trigger the job uh which is AO worklow and it goes and Spins up the nodes and runs your spark job and then scale it",
    "start": "1623600",
    "end": "1629720"
  },
  {
    "text": "down to zero okay and this is the Event Source okay that is running into Argo",
    "start": "1629720",
    "end": "1635360"
  },
  {
    "text": "events Nam space and is looking for the U um um uh events that are to fetch from",
    "start": "1635360",
    "end": "1642880"
  },
  {
    "text": "Amazon sqs and then this is the sensor and I'll go briefly with with you and",
    "start": "1642880",
    "end": "1648880"
  },
  {
    "text": "the sensor has is triggering the Argo workflow here as you can see and this",
    "start": "1648880",
    "end": "1656559"
  },
  {
    "text": "one has par several parallel jobs one hello two Hello World jobs and one spark",
    "start": "1656559",
    "end": "1663240"
  },
  {
    "text": "operator job more complex jobs the jobs in the in this park operator uh uh jobs",
    "start": "1663240",
    "end": "1672399"
  },
  {
    "text": "they have the driver that needs to be installed with the uh through the",
    "start": "1672399",
    "end": "1677519"
  },
  {
    "text": "Carpenter on on demand distances as you can see in here and in the same time it",
    "start": "1677519",
    "end": "1683279"
  },
  {
    "text": "is and the Unicorn will take take care about the scheduling and in the same time the",
    "start": "1683279",
    "end": "1689440"
  },
  {
    "text": "executors the executors will be installed on the spot instances and the same is happening for",
    "start": "1689440",
    "end": "1695919"
  },
  {
    "text": "the bigger job but because I measure it and it takes exactly two minutes okay",
    "start": "1695919",
    "end": "1701000"
  },
  {
    "text": "and I have I think I'm already talking for two minutes about this thing so we should have these things happening here",
    "start": "1701000",
    "end": "1708720"
  },
  {
    "text": "so if I if I click that not viewer yes all right so these are the the first",
    "start": "1708720",
    "end": "1716519"
  },
  {
    "text": "three nodes okay they are the core nodes where the system is running and the",
    "start": "1716519",
    "end": "1723120"
  },
  {
    "text": "carpet created one node on demand node for the driver and two additional nodes",
    "start": "1723120",
    "end": "1730080"
  },
  {
    "text": "where the executors are running okay if I'm going to if I'm going to watch this",
    "start": "1730080",
    "end": "1736760"
  },
  {
    "text": "into the SP spark uh spark team a names",
    "start": "1736760",
    "end": "1742640"
  },
  {
    "text": "space okay I see that the first driver the the first driver into this workflow",
    "start": "1742640",
    "end": "1750279"
  },
  {
    "text": "this one from here spark operator uh Pi job has it is running",
    "start": "1750279",
    "end": "1756000"
  },
  {
    "text": "already and the executors are being created on the on the spot instances okay while the second job the",
    "start": "1756000",
    "end": "1765120"
  },
  {
    "text": "spark operator taxi job is waiting to finish the spark operative Pi",
    "start": "1765120",
    "end": "1771480"
  },
  {
    "text": "job okay and then to start running running again and the the executors from",
    "start": "1771480",
    "end": "1778240"
  },
  {
    "text": "the from the first one is terminating and the the the last job is being created as well yeah I think we are need",
    "start": "1778240",
    "end": "1785320"
  },
  {
    "text": "to the time uh sorry we took uh longer than that but if you have any quick questions uh we can take couple of",
    "start": "1785320",
    "end": "1791360"
  },
  {
    "text": "questions if you have y one question and then sorry they",
    "start": "1791360",
    "end": "1797880"
  },
  {
    "text": "can can they find you somewhere I'll be good we'll be standing outside so my",
    "start": "1797880",
    "end": "1803559"
  },
  {
    "text": "question is is anybody actually using IPv6 uh that's a good question um",
    "start": "1803559",
    "end": "1809960"
  },
  {
    "text": "there's a lot of customers are experimenting IPv6 at the moment uh it's because even though spark submit",
    "start": "1809960",
    "end": "1816240"
  },
  {
    "text": "supports uh there are other components which does not have support for IPv6 it's mainly validating what other tools",
    "start": "1816240",
    "end": "1823919"
  },
  {
    "text": "or add-ons that you want to run on kubernetes whether that's supports IPv6 or not say for example spark operator",
    "start": "1823919",
    "end": "1831240"
  },
  {
    "text": "does not support IPv6 because of that reasons customers are waiting until that support is available so so we're",
    "start": "1831240",
    "end": "1837679"
  },
  {
    "text": "thinking about that's a feature but once we have that you know we are going towards that way",
    "start": "1837679",
    "end": "1844240"
  }
]