[
  {
    "text": "hey everyone",
    "start": "399",
    "end": "1760"
  },
  {
    "text": "my name is radu and in the next nine and",
    "start": "1760",
    "end": "4400"
  },
  {
    "text": "a half minutes we're going to talk about",
    "start": "4400",
    "end": "5839"
  },
  {
    "text": "webassembly and machine learning and",
    "start": "5839",
    "end": "8320"
  },
  {
    "text": "we're going to have a quick look at wes",
    "start": "8320",
    "end": "10480"
  },
  {
    "text": "dnn and the new proposal for neural",
    "start": "10480",
    "end": "12719"
  },
  {
    "text": "networks in webassembly i'm a software",
    "start": "12719",
    "end": "14880"
  },
  {
    "text": "engineer i work at microsoft azure if",
    "start": "14880",
    "end": "17359"
  },
  {
    "text": "you've seen a bunch of deus labs",
    "start": "17359",
    "end": "18960"
  },
  {
    "text": "projects mentioned today i work on the",
    "start": "18960",
    "end": "20720"
  },
  {
    "text": "team",
    "start": "20720",
    "end": "21520"
  },
  {
    "text": "i also do a lot of work with the",
    "start": "21520",
    "end": "23119"
  },
  {
    "text": "bytecode alliance and the cncf",
    "start": "23119",
    "end": "25439"
  },
  {
    "text": "and tldr i'm interested in everything",
    "start": "25439",
    "end": "27840"
  },
  {
    "text": "that has to do with webassembly machine",
    "start": "27840",
    "end": "29519"
  },
  {
    "text": "learning distributed systems and",
    "start": "29519",
    "end": "31599"
  },
  {
    "text": "most often the combination of the of the",
    "start": "31599",
    "end": "34480"
  },
  {
    "text": "three",
    "start": "34480",
    "end": "35600"
  },
  {
    "text": "i'm a maintainer or contributor for",
    "start": "35600",
    "end": "37520"
  },
  {
    "text": "waggy crosslit hippo bindle the http",
    "start": "37520",
    "end": "40320"
  },
  {
    "text": "extension to azi uh and wes cnn",
    "start": "40320",
    "end": "44480"
  },
  {
    "text": "uh this talk and a lot of wes cnn has",
    "start": "44480",
    "end": "47039"
  },
  {
    "text": "been uh contributed by andrew brown and",
    "start": "47039",
    "end": "49360"
  },
  {
    "text": "mink u soon from intel",
    "start": "49360",
    "end": "51680"
  },
  {
    "text": "so this is uh",
    "start": "51680",
    "end": "53280"
  },
  {
    "text": "a shout out to andrew who's i think on a",
    "start": "53280",
    "end": "55360"
  },
  {
    "text": "sabbatical hope he's enjoying his time",
    "start": "55360",
    "end": "57920"
  },
  {
    "text": "off",
    "start": "57920",
    "end": "59359"
  },
  {
    "text": "uh",
    "start": "59359",
    "end": "60239"
  },
  {
    "text": "we're going to talk about a little bit",
    "start": "60239",
    "end": "61760"
  },
  {
    "text": "about webassembly and wazzy and then",
    "start": "61760",
    "end": "63680"
  },
  {
    "text": "machine learning in webassembly",
    "start": "63680",
    "end": "66159"
  },
  {
    "text": "i i paid very close attention no one",
    "start": "66159",
    "end": "69280"
  },
  {
    "text": "actually mentioned that webassembly is",
    "start": "69280",
    "end": "71360"
  },
  {
    "text": "neither web nor assembly today and that",
    "start": "71360",
    "end": "73920"
  },
  {
    "text": "is absolutely a given at any webassembly",
    "start": "73920",
    "end": "77040"
  },
  {
    "text": "conference so",
    "start": "77040",
    "end": "79759"
  },
  {
    "text": "i think",
    "start": "80400",
    "end": "81600"
  },
  {
    "text": "a lot of people talked about what web",
    "start": "81600",
    "end": "83119"
  },
  {
    "text": "assembly is and what webassembly isn't",
    "start": "83119",
    "end": "85759"
  },
  {
    "text": "but in short it's just a binary",
    "start": "85759",
    "end": "87439"
  },
  {
    "text": "instruction format for a web for a",
    "start": "87439",
    "end": "89200"
  },
  {
    "text": "stack-based virtual machine but in short",
    "start": "89200",
    "end": "91680"
  },
  {
    "text": "it just lets us run cool stuff on the",
    "start": "91680",
    "end": "94159"
  },
  {
    "text": "web that's not javascript only",
    "start": "94159",
    "end": "97200"
  },
  {
    "text": "but on top of webassembly it's",
    "start": "97200",
    "end": "99600"
  },
  {
    "text": "webassembly has no mention necessarily",
    "start": "99600",
    "end": "102560"
  },
  {
    "text": "of the web you can run",
    "start": "102560",
    "end": "104399"
  },
  {
    "text": "webassembly outside the browser pretty",
    "start": "104399",
    "end": "106479"
  },
  {
    "text": "well and that's where where wasd cam",
    "start": "106479",
    "end": "108799"
  },
  {
    "text": "comes in it's a capability oriented api",
    "start": "108799",
    "end": "111680"
  },
  {
    "text": "and essentially it's designed to",
    "start": "111680",
    "end": "113119"
  },
  {
    "text": "standardize the execution of webassembly",
    "start": "113119",
    "end": "115119"
  },
  {
    "text": "outside the browser",
    "start": "115119",
    "end": "117439"
  },
  {
    "text": "and what webassembly and wazi give us",
    "start": "117439",
    "end": "119920"
  },
  {
    "text": "outside the web is",
    "start": "119920",
    "end": "121840"
  },
  {
    "text": "a portable lightweight fast and secure",
    "start": "121840",
    "end": "124560"
  },
  {
    "text": "way of executing",
    "start": "124560",
    "end": "126079"
  },
  {
    "text": "semi-trusted or untrusted code",
    "start": "126079",
    "end": "130080"
  },
  {
    "text": "and if you're just getting started or",
    "start": "130080",
    "end": "132080"
  },
  {
    "text": "interested in getting in depth with uh",
    "start": "132080",
    "end": "134319"
  },
  {
    "text": "anything that has to do with wazi you",
    "start": "134319",
    "end": "136319"
  },
  {
    "text": "have to read the lin clark's code",
    "start": "136319",
    "end": "138400"
  },
  {
    "text": "cartoons they're probably the best",
    "start": "138400",
    "end": "140239"
  },
  {
    "text": "resource for anyone getting into",
    "start": "140239",
    "end": "142319"
  },
  {
    "text": "webassembly in wazi",
    "start": "142319",
    "end": "145360"
  },
  {
    "text": "why would we want to run",
    "start": "145599",
    "end": "148080"
  },
  {
    "text": "machine learning in wasm",
    "start": "148080",
    "end": "150000"
  },
  {
    "text": "well mostly for the same reasons we want",
    "start": "150000",
    "end": "151680"
  },
  {
    "text": "to run anything in webassembly we want",
    "start": "151680",
    "end": "154800"
  },
  {
    "text": "the portability that webassembly gives",
    "start": "154800",
    "end": "156560"
  },
  {
    "text": "us we want the flexibility to run the",
    "start": "156560",
    "end": "158720"
  },
  {
    "text": "same webassembly module anywhere and",
    "start": "158720",
    "end": "161599"
  },
  {
    "text": "specifically for machine learning we",
    "start": "161599",
    "end": "163519"
  },
  {
    "text": "want to run the same machine learning",
    "start": "163519",
    "end": "165760"
  },
  {
    "text": "model across different architectures we",
    "start": "165760",
    "end": "168879"
  },
  {
    "text": "also want to have language agnostic",
    "start": "168879",
    "end": "170959"
  },
  {
    "text": "access to different machine learning",
    "start": "170959",
    "end": "172959"
  },
  {
    "text": "runtimes and more importantly we don't",
    "start": "172959",
    "end": "175440"
  },
  {
    "text": "want to keep re-implementing the same",
    "start": "175440",
    "end": "177920"
  },
  {
    "text": "cpu instructions and machine learning",
    "start": "177920",
    "end": "179680"
  },
  {
    "text": "operators",
    "start": "179680",
    "end": "182319"
  },
  {
    "text": "but there are a couple of problems that",
    "start": "182560",
    "end": "184239"
  },
  {
    "text": "makes uh that make running machine",
    "start": "184239",
    "end": "186480"
  },
  {
    "text": "learning in webassembly today",
    "start": "186480",
    "end": "188480"
  },
  {
    "text": "not ideal uh first and most obvious is",
    "start": "188480",
    "end": "191680"
  },
  {
    "text": "we don't have gpu access in webassembly",
    "start": "191680",
    "end": "194000"
  },
  {
    "text": "runtimes",
    "start": "194000",
    "end": "195120"
  },
  {
    "text": "there's also no multi-threading or other",
    "start": "195120",
    "end": "197120"
  },
  {
    "text": "hardware acceleration",
    "start": "197120",
    "end": "198720"
  },
  {
    "text": "there are obvious missing",
    "start": "198720",
    "end": "201040"
  },
  {
    "text": "missing cpu instructions from",
    "start": "201040",
    "end": "202879"
  },
  {
    "text": "webassembly that",
    "start": "202879",
    "end": "204720"
  },
  {
    "text": "probably no one's going to add for each",
    "start": "204720",
    "end": "206480"
  },
  {
    "text": "machine learning framework and then in",
    "start": "206480",
    "end": "208799"
  },
  {
    "text": "general deploying machine learning in",
    "start": "208799",
    "end": "210720"
  },
  {
    "text": "production it's just difficult",
    "start": "210720",
    "end": "213920"
  },
  {
    "text": "so this is where was enn comes in the",
    "start": "213920",
    "end": "216319"
  },
  {
    "text": "web the wazi neural network proposal",
    "start": "216319",
    "end": "219599"
  },
  {
    "text": "it's a way of allowing us to",
    "start": "219599",
    "end": "222000"
  },
  {
    "text": "load a",
    "start": "222000",
    "end": "223120"
  },
  {
    "text": "neural network model into a runtime and",
    "start": "223120",
    "end": "226159"
  },
  {
    "text": "essentially run inferencing on that",
    "start": "226159",
    "end": "228239"
  },
  {
    "text": "particular neural network model",
    "start": "228239",
    "end": "230560"
  },
  {
    "text": "it's framework and format agnostic so if",
    "start": "230560",
    "end": "233760"
  },
  {
    "text": "you have an implementation you can run",
    "start": "233760",
    "end": "235439"
  },
  {
    "text": "it for",
    "start": "235439",
    "end": "236480"
  },
  {
    "text": "pytorch tensorflow or any other machine",
    "start": "236480",
    "end": "238720"
  },
  {
    "text": "learning framework or model you want to",
    "start": "238720",
    "end": "240319"
  },
  {
    "text": "run it for",
    "start": "240319",
    "end": "241680"
  },
  {
    "text": "it's much faster than running",
    "start": "241680",
    "end": "243200"
  },
  {
    "text": "inferencing in pure webassembly",
    "start": "243200",
    "end": "245439"
  },
  {
    "text": "and it's really portable we'll see a",
    "start": "245439",
    "end": "247519"
  },
  {
    "text": "recording of",
    "start": "247519",
    "end": "248959"
  },
  {
    "text": "the same machine learning model and",
    "start": "248959",
    "end": "251040"
  },
  {
    "text": "neural network and runtime on across",
    "start": "251040",
    "end": "253920"
  },
  {
    "text": "raspberry pi's and intel cpus and amd",
    "start": "253920",
    "end": "256400"
  },
  {
    "text": "and gpus as well",
    "start": "256400",
    "end": "258560"
  },
  {
    "text": "the current implementations are for the",
    "start": "258560",
    "end": "260880"
  },
  {
    "text": "openvino model and the onyx runtime",
    "start": "260880",
    "end": "263919"
  },
  {
    "text": "and",
    "start": "263919",
    "end": "265919"
  },
  {
    "text": "essentially it's a pretty simple api it",
    "start": "265919",
    "end": "268880"
  },
  {
    "text": "lets us load up an opaque byte array as",
    "start": "268880",
    "end": "271040"
  },
  {
    "text": "a neural network model we can initialize",
    "start": "271040",
    "end": "273520"
  },
  {
    "text": "the execution context and bind some",
    "start": "273520",
    "end": "275600"
  },
  {
    "text": "inputs as tensors",
    "start": "275600",
    "end": "277680"
  },
  {
    "text": "uh we can compute the inference and then",
    "start": "277680",
    "end": "279600"
  },
  {
    "text": "get the output sensor using a specific",
    "start": "279600",
    "end": "282880"
  },
  {
    "text": "get output result the api is written in",
    "start": "282880",
    "end": "285199"
  },
  {
    "text": "widex which means for anything that you",
    "start": "285199",
    "end": "288080"
  },
  {
    "text": "have either code gen or you want to",
    "start": "288080",
    "end": "291040"
  },
  {
    "text": "manually write bindings for widix you",
    "start": "291040",
    "end": "292880"
  },
  {
    "text": "can do it it's not fun",
    "start": "292880",
    "end": "295440"
  },
  {
    "text": "but in essence if you want to write the",
    "start": "295440",
    "end": "297680"
  },
  {
    "text": "low level implementation all you have to",
    "start": "297680",
    "end": "299440"
  },
  {
    "text": "do is",
    "start": "299440",
    "end": "300560"
  },
  {
    "text": "call the four main api functions that",
    "start": "300560",
    "end": "303840"
  },
  {
    "text": "was enan gives you and you can implement",
    "start": "303840",
    "end": "306000"
  },
  {
    "text": "that for any web assembly runtime",
    "start": "306000",
    "end": "309600"
  },
  {
    "text": "the interesting part is",
    "start": "309600",
    "end": "312080"
  },
  {
    "text": "uh",
    "start": "312080",
    "end": "313360"
  },
  {
    "text": "and here we're gonna see uh the same",
    "start": "313360",
    "end": "315840"
  },
  {
    "text": "neural network and the same runtime and",
    "start": "315840",
    "end": "317759"
  },
  {
    "text": "the same webassembly module run across",
    "start": "317759",
    "end": "320720"
  },
  {
    "text": "an insult macbook using the webassembly",
    "start": "320720",
    "end": "323360"
  },
  {
    "text": "runtime first using the wasm time",
    "start": "323360",
    "end": "327198"
  },
  {
    "text": "i hope that's visible",
    "start": "332639",
    "end": "336360"
  },
  {
    "text": "slides",
    "start": "347680",
    "end": "350160"
  },
  {
    "text": "okay",
    "start": "350160",
    "end": "351120"
  },
  {
    "text": "i'll just let it run essentially it",
    "start": "351120",
    "end": "352960"
  },
  {
    "text": "takes the same machine learning uh",
    "start": "352960",
    "end": "356000"
  },
  {
    "text": "model built with pytorch that does image",
    "start": "356000",
    "end": "360000"
  },
  {
    "text": "inferencing and now it runs it on a on a",
    "start": "360000",
    "end": "362319"
  },
  {
    "text": "raspberry pi",
    "start": "362319",
    "end": "363600"
  },
  {
    "text": "the great thing about this you can",
    "start": "363600",
    "end": "365120"
  },
  {
    "text": "cross-compile webassembly from anywhere",
    "start": "365120",
    "end": "368720"
  },
  {
    "text": "and",
    "start": "368720",
    "end": "370720"
  },
  {
    "text": "i mentioned earlier we essentially can",
    "start": "370720",
    "end": "373280"
  },
  {
    "text": "take the same model and the same",
    "start": "373280",
    "end": "376000"
  },
  {
    "text": "runtime and run it in something like",
    "start": "376000",
    "end": "378160"
  },
  {
    "text": "wacky",
    "start": "378160",
    "end": "381120"
  },
  {
    "text": "the one of the important things is",
    "start": "381120",
    "end": "383840"
  },
  {
    "text": "it's more performant than running pure",
    "start": "383840",
    "end": "385680"
  },
  {
    "text": "web assembly so you can have a look at",
    "start": "385680",
    "end": "388160"
  },
  {
    "text": "the uh at the benchmarks",
    "start": "388160",
    "end": "391360"
  },
  {
    "text": "the actual inference is really really",
    "start": "391360",
    "end": "393120"
  },
  {
    "text": "fast it's around two milliseconds tensor",
    "start": "393120",
    "end": "396479"
  },
  {
    "text": "preprocessing is the slower part it's",
    "start": "396479",
    "end": "399199"
  },
  {
    "text": "around ten times slower than uh the",
    "start": "399199",
    "end": "401360"
  },
  {
    "text": "actual inference",
    "start": "401360",
    "end": "402880"
  },
  {
    "text": "the other part is you still have to run",
    "start": "402880",
    "end": "404960"
  },
  {
    "text": "pre-processing in non-python which is",
    "start": "404960",
    "end": "407520"
  },
  {
    "text": "not ideal",
    "start": "407520",
    "end": "410479"
  },
  {
    "text": "one of the other really interesting",
    "start": "411440",
    "end": "412960"
  },
  {
    "text": "proposals we've seen",
    "start": "412960",
    "end": "414400"
  },
  {
    "text": "recently is was a parallel",
    "start": "414400",
    "end": "416720"
  },
  {
    "text": "and one of the ideas is",
    "start": "416720",
    "end": "418960"
  },
  {
    "text": "running the pre-processing using",
    "start": "418960",
    "end": "421919"
  },
  {
    "text": "wazi parallel",
    "start": "421919",
    "end": "424800"
  },
  {
    "text": "you can find most of the implementations",
    "start": "425840",
    "end": "427919"
  },
  {
    "text": "and resources about this",
    "start": "427919",
    "end": "429840"
  },
  {
    "text": "on github",
    "start": "429840",
    "end": "431599"
  },
  {
    "text": "any questions",
    "start": "431599",
    "end": "434840"
  },
  {
    "text": "yeah so i'm just curious uh you",
    "start": "437039",
    "end": "439440"
  },
  {
    "text": "mentioned that it's uh faster than",
    "start": "439440",
    "end": "441440"
  },
  {
    "text": "running standard web assembly",
    "start": "441440",
    "end": "443599"
  },
  {
    "text": "why is that is because you've been able",
    "start": "443599",
    "end": "445759"
  },
  {
    "text": "to strip out some of that stuff and you",
    "start": "445759",
    "end": "448400"
  },
  {
    "text": "have a better execution",
    "start": "448400",
    "end": "451440"
  },
  {
    "text": "go ahead and repeat the question uh the",
    "start": "452319",
    "end": "454160"
  },
  {
    "text": "question is why it's faster than pure",
    "start": "454160",
    "end": "455680"
  },
  {
    "text": "webassembly",
    "start": "455680",
    "end": "456960"
  },
  {
    "text": "essentially uh there are ways of running",
    "start": "456960",
    "end": "459360"
  },
  {
    "text": "inferencing in pure webassembly and the",
    "start": "459360",
    "end": "461199"
  },
  {
    "text": "way that usually done is a",
    "start": "461199",
    "end": "462960"
  },
  {
    "text": "re-implementation of the machine",
    "start": "462960",
    "end": "465120"
  },
  {
    "text": "learning framework that's compiled to",
    "start": "465120",
    "end": "467039"
  },
  {
    "text": "webassembly",
    "start": "467039",
    "end": "468639"
  },
  {
    "text": "where znn what it usually does is",
    "start": "468639",
    "end": "470639"
  },
  {
    "text": "essentially you can offload the",
    "start": "470639",
    "end": "472720"
  },
  {
    "text": "execution and the inference to the",
    "start": "472720",
    "end": "474479"
  },
  {
    "text": "runtime and the runtime has access to",
    "start": "474479",
    "end": "477199"
  },
  {
    "text": "hardware acceleration gpu or anything",
    "start": "477199",
    "end": "479360"
  },
  {
    "text": "like that so what what you do is through",
    "start": "479360",
    "end": "481440"
  },
  {
    "text": "the oasis n api you pass the model bytes",
    "start": "481440",
    "end": "485599"
  },
  {
    "text": "you pass the input as a tensor input the",
    "start": "485599",
    "end": "488400"
  },
  {
    "text": "runtime does the inferencing using gpu",
    "start": "488400",
    "end": "491039"
  },
  {
    "text": "access and then you get the tensor back",
    "start": "491039",
    "end": "495599"
  },
  {
    "text": "okay",
    "start": "497280",
    "end": "498479"
  },
  {
    "text": "uh thanks radio that was great thank you",
    "start": "498479",
    "end": "503720"
  }
]