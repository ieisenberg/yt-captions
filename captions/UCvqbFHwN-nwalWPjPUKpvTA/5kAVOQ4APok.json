[
  {
    "start": "0",
    "end": "34000"
  },
  {
    "text": "okay thank you all for thank you all for coming my name is Kehoe I work at AWS and then also working on a project",
    "start": "30",
    "end": "7020"
  },
  {
    "text": "called STD so today we will talk about",
    "start": "7020",
    "end": "18630"
  },
  {
    "text": "the project s CD which is now CN say project CN CF project and then we will",
    "start": "18630",
    "end": "24359"
  },
  {
    "text": "present some of the initial ideas that we are working on to improve our cluster operation of SC d so typical acidic acid",
    "start": "24359",
    "end": "36660"
  },
  {
    "text": "is distributed so typical city cluster is distributed over five to I mean three",
    "start": "36660",
    "end": "42270"
  },
  {
    "text": "to five not for high availability and then our city so a city server",
    "start": "42270",
    "end": "48870"
  },
  {
    "text": "implementer after consensus algorithm so raft is like a leader based so like so",
    "start": "48870",
    "end": "54629"
  },
  {
    "text": "in terms of a cap theorem a CDCP system so a city for high availability as it is",
    "start": "54629",
    "end": "61020"
  },
  {
    "text": "distributed while week I practice the consistency and then for tolerance so",
    "start": "61020",
    "end": "68280"
  },
  {
    "text": "what that means is cities to provide one logical cluster view of many physical",
    "start": "68280",
    "end": "73770"
  },
  {
    "text": "servers so as long as you maintain the connectivity with the quorum the cluster",
    "start": "73770",
    "end": "79830"
  },
  {
    "text": "should continue to work even with the faulty network so raft is little based",
    "start": "79830",
    "end": "86159"
  },
  {
    "text": "so data is replicated from leader to follower and then follower forward",
    "start": "86159",
    "end": "91259"
  },
  {
    "text": "proposals to leader and then what it",
    "start": "91259",
    "end": "96840"
  },
  {
    "text": "means in the raft course or implementer",
    "start": "96840",
    "end": "105869"
  },
  {
    "text": "after consensus algorithm so when you",
    "start": "105869",
    "end": "112530"
  },
  {
    "start": "110000",
    "end": "503000"
  },
  {
    "text": "operate city a lot of things can get thinking wrong so let's say they can go",
    "start": "112530",
    "end": "122700"
  },
  {
    "text": "this one so let's say we knew arrow a new member to next we know the cluster",
    "start": "122700",
    "end": "130160"
  },
  {
    "text": "or criticize becomes so when you add a new member to another cluster cluster size becomes from 3 to 4",
    "start": "130160",
    "end": "139810"
  },
  {
    "text": "so in this case so when you so let's say you had a new member to 3 node cluster",
    "start": "139810",
    "end": "145730"
  },
  {
    "text": "and they went leaders Network like so when you add a new member the new member joins the cluster with empty data which",
    "start": "145730",
    "end": "153379"
  },
  {
    "text": "means they require more update from the leader note and then then leaders",
    "start": "153379",
    "end": "158870"
  },
  {
    "text": "network becomes easily overloaded and then when the leaders Network pen this is all you stop the the leader heartbeat",
    "start": "158870",
    "end": "167959"
  },
  {
    "text": "may block will drop in the case the follower may election timeout and then a",
    "start": "167959",
    "end": "174409"
  },
  {
    "text": "trigger or new leadership election so in",
    "start": "174409",
    "end": "183980"
  },
  {
    "text": "a city is leader based so when you do not have a leader due to the leadership election se the cluster cannot process",
    "start": "183980",
    "end": "191209"
  },
  {
    "text": "any clamp requests so body so when you",
    "start": "191209",
    "end": "196639"
  },
  {
    "text": "operate a city cluster as it is distributed so you must be prepared for network partition so when the network",
    "start": "196639",
    "end": "206480"
  },
  {
    "text": "partition happens so in solving when you have our like leadership now a network",
    "start": "206480",
    "end": "212959"
  },
  {
    "text": "partition so in this the exterior of the cluster so you like as long as you",
    "start": "212959",
    "end": "221269"
  },
  {
    "text": "maintain the true active maintain the active quorum in the cluster",
    "start": "221269",
    "end": "226310"
  },
  {
    "text": "it's a city should continue to work so in this 3 node cluster is still the leaders partition still maintained so",
    "start": "226310",
    "end": "233030"
  },
  {
    "text": "active quorum so as this cluster should continue to work but what if the leaders",
    "start": "233030",
    "end": "241220"
  },
  {
    "text": "node becomes isolated so in this case leader does not maintain the reactive",
    "start": "241220",
    "end": "247010"
  },
  {
    "text": "quorum so leader like has to revert back to follower after election timeout and",
    "start": "247010",
    "end": "252590"
  },
  {
    "text": "then they can effect a cluster availability",
    "start": "252590",
    "end": "257409"
  },
  {
    "text": "but what if the leader what if we have both what if we have a total partition",
    "start": "260480",
    "end": "267120"
  },
  {
    "text": "and the membership reconfiguration so they would depends on the network topology also it depends on the cluster",
    "start": "267120",
    "end": "274350"
  },
  {
    "text": "size and they also depends on the size of quorum so when you add a new member to three node cluster cluster size",
    "start": "274350",
    "end": "281580"
  },
  {
    "text": "becomes 4 and then size of the column becomes from 2 to 3 and then in this",
    "start": "281580",
    "end": "288960"
  },
  {
    "text": "case we still have our so this is like 3 & 1 partition so photo 3 not partition",
    "start": "288960",
    "end": "294870"
  },
  {
    "text": "we still maintains two active quorum so leader does not lose the quorum so that",
    "start": "294870",
    "end": "301830"
  },
  {
    "text": "means like we still like can process the likely request but this is this is not",
    "start": "301830",
    "end": "310860"
  },
  {
    "text": "okay so this is true and to partition neither of the partition maintains two active quorum so this the leader will",
    "start": "310860",
    "end": "318450"
  },
  {
    "text": "soon election timeout and they revert back to follow work so they can affect",
    "start": "318450",
    "end": "325380"
  },
  {
    "text": "our cluster availability and then the client would need to wait until the new leader gets elected so what if network",
    "start": "325380",
    "end": "337470"
  },
  {
    "text": "of partition happens first and then new member gets added so in this case so",
    "start": "337470",
    "end": "345830"
  },
  {
    "text": "membership reconfiguration is a two-step process so you issue a member add comment first",
    "start": "345830",
    "end": "352020"
  },
  {
    "text": "and then you start a new sed process so before we run the member add comment we",
    "start": "352020",
    "end": "358020"
  },
  {
    "text": "had two active members out of three node cluster but after we run the member add",
    "start": "358020",
    "end": "364590"
  },
  {
    "text": "comment the cluster size becomes four and then in this case we only have our two active members out of four so little",
    "start": "364590",
    "end": "372810"
  },
  {
    "text": "lose the active quorum like from its partition and then revert",
    "start": "372810",
    "end": "377850"
  },
  {
    "text": "back to a follower again affect my cluster availability so these the same",
    "start": "377850",
    "end": "386880"
  },
  {
    "text": "logic applies to the one node cluster so let's say you error a new member to the one node cluster in the cluster size",
    "start": "386880",
    "end": "394030"
  },
  {
    "text": "comes to and then you only have so before you run the member at comment you only have a one active node one node",
    "start": "394030",
    "end": "402040"
  },
  {
    "text": "cluster which is okay but after you run the member add comment the cluster size becomes two and then you have only one",
    "start": "402040",
    "end": "408550"
  },
  {
    "text": "node out of two node cluster which means that you just lost a quorum after member add comment",
    "start": "408550",
    "end": "415530"
  },
  {
    "text": "so this getting worse if you paste or if you run the member add comment with the",
    "start": "420150",
    "end": "425440"
  },
  {
    "text": "wrong your air so let's say you run the member add comment with invalid your air",
    "start": "425440",
    "end": "430870"
  },
  {
    "text": "so the member add comment will still be applied for it a raft well the since the new member was",
    "start": "430870",
    "end": "439000"
  },
  {
    "text": "misconfigured in the first place don't you remember would never be able to join the cluster which means that the the",
    "start": "439000",
    "end": "446050"
  },
  {
    "text": "cluster will never be able to elect a new leader and then next since you lost",
    "start": "446050",
    "end": "451240"
  },
  {
    "text": "lost a leader you cannot even like revert membership reconfiguration either",
    "start": "451240",
    "end": "457440"
  },
  {
    "text": "so same applies to the multi node cluster so we have here neck three node",
    "start": "457830",
    "end": "464590"
  },
  {
    "text": "cluster and then you error you run the member add comment with invalid URL so",
    "start": "464590",
    "end": "471910"
  },
  {
    "text": "as soon as you applied on member edde comment the cluster size becomes 4 and",
    "start": "471910",
    "end": "477970"
  },
  {
    "text": "then corn size becomes 3 so here like in door number two we still have our 3",
    "start": "477970",
    "end": "483550"
  },
  {
    "text": "active node out of 4 node cluster well like this cluster can now only tolerate",
    "start": "483550",
    "end": "490090"
  },
  {
    "text": "one flaky member so even though one node going down can make the whole cluster",
    "start": "490090",
    "end": "495970"
  },
  {
    "text": "down because in that case we would only have our two active members our four node cluster so a city has become the",
    "start": "495970",
    "end": "505690"
  },
  {
    "start": "503000",
    "end": "629000"
  },
  {
    "text": "critical component in communities so we want to do better so we want to protect a city against miss configuration as I",
    "start": "505690",
    "end": "514030"
  },
  {
    "text": "just explained and they we also want to maintain the high availability during membership reconfiguration so a city",
    "start": "514030",
    "end": "522219"
  },
  {
    "text": "learner defined as a new node state in raft and the learner know",
    "start": "522220",
    "end": "527900"
  },
  {
    "text": "joins the cluster as a non-voting member so what that means it the loner node neither vote or does",
    "start": "527900",
    "end": "536930"
  },
  {
    "text": "not count towards the quorum and then what that means there isn't adding a loner node with none effect the cluster",
    "start": "536930",
    "end": "543290"
  },
  {
    "text": "availability as much so to simplify the",
    "start": "543290",
    "end": "550700"
  },
  {
    "text": "workflow you just run the member ed and yvette and then like once the loner",
    "start": "550700",
    "end": "558560"
  },
  {
    "text": "node is ready to be promoted you just want the member promote comment to promote the existing neck loner node to",
    "start": "558560",
    "end": "567050"
  },
  {
    "text": "the voting member so once promoted the lunar node now a",
    "start": "567050",
    "end": "574940"
  },
  {
    "text": "voting member would count would be counted towards the quorum and then the home site would change so for safety",
    "start": "574940",
    "end": "584390"
  },
  {
    "text": "checks so sed server will validate the promoter request for some safety checks",
    "start": "584390",
    "end": "589790"
  },
  {
    "text": "so in the case so let's say the lunar node has not caught up to the leaders log yet like still behind in the case",
    "start": "589790",
    "end": "597200"
  },
  {
    "text": "like SAT server will reject a promote request and to simplify simplify the",
    "start": "597200",
    "end": "606260"
  },
  {
    "text": "initial implementation the Lana would have only a limited set of features so you cannot transfer leadership to the",
    "start": "606260",
    "end": "612620"
  },
  {
    "text": "lunar node and then you cannot request the read or write from the lunar node so",
    "start": "612620",
    "end": "618290"
  },
  {
    "text": "this is only meant to serve as a naked standby note",
    "start": "618290",
    "end": "624070"
  },
  {
    "start": "629000",
    "end": "706000"
  },
  {
    "text": "thank you very much okay so just a quick raise of hands how many of you out there",
    "start": "629149",
    "end": "634709"
  },
  {
    "text": "have used @cd operator okay and how many are currently still using that see the",
    "start": "634709",
    "end": "641820"
  },
  {
    "text": "operator so a few last is anyone using it in production I saw someone okay all",
    "start": "641820",
    "end": "652260"
  },
  {
    "text": "right so I'm gonna be talking about persistent and durable at CB clusters using at CD operator so just kind of an",
    "start": "652260",
    "end": "659700"
  },
  {
    "text": "outline of what I'm gonna talk about we're gonna go over the definition of an operator and then we're going to go",
    "start": "659700",
    "end": "666060"
  },
  {
    "text": "through the critical components of the operator and we're also going to then",
    "start": "666060",
    "end": "671970"
  },
  {
    "text": "look at what ezio operator does great and then what really we can improve with",
    "start": "671970",
    "end": "677970"
  },
  {
    "text": "SCD operator and then finally just a roadmap of where the projects could be",
    "start": "677970",
    "end": "683160"
  },
  {
    "text": "going in the next few months as some people have probably noticed Etsy",
    "start": "683160",
    "end": "689100"
  },
  {
    "text": "operator was a little bit in an unmaintained State for a period of time and I've been working with some of our",
    "start": "689100",
    "end": "696149"
  },
  {
    "text": "team members to resolve that so hopefully with those seeing some improvements soon so thank you for your",
    "start": "696149",
    "end": "704579"
  },
  {
    "text": "patience okay so an operator is represents human operational knowledge",
    "start": "704579",
    "end": "710940"
  },
  {
    "start": "706000",
    "end": "792000"
  },
  {
    "text": "in software to reliably manage an application so what does that really",
    "start": "710940",
    "end": "717930"
  },
  {
    "text": "mean okay so in the case of the diagram",
    "start": "717930",
    "end": "723149"
  },
  {
    "text": "here we have a desired state we have current state and then we have our controller so the controller is",
    "start": "723149",
    "end": "729600"
  },
  {
    "text": "basically going to attempt to reconcile the difference between current state and desired state and to do that there's two",
    "start": "729600",
    "end": "736620"
  },
  {
    "text": "main components so the first component is the custom resource definition so the",
    "start": "736620",
    "end": "743130"
  },
  {
    "text": "CR D gives us access to gives us the capability to extend the kubernetes api",
    "start": "743130",
    "end": "749060"
  },
  {
    "text": "and this gives us access to custom resource objects much like the built-in",
    "start": "749060",
    "end": "757640"
  },
  {
    "text": "objects resources such as a pod this would be an example of a building",
    "start": "757640",
    "end": "764650"
  },
  {
    "text": "resource and the the second component is",
    "start": "764650",
    "end": "770120"
  },
  {
    "text": "the custom controller so the custom controller is the logic that combined",
    "start": "770120",
    "end": "776330"
  },
  {
    "text": "with the custom resource definition gives us access to a declaration here is",
    "start": "776330",
    "end": "788810"
  },
  {
    "text": "very powerful okay so to install it see",
    "start": "788810",
    "end": "795350"
  },
  {
    "start": "792000",
    "end": "835000"
  },
  {
    "text": "operators very straightforward this is literally just a clone from the the",
    "start": "795350",
    "end": "800960"
  },
  {
    "text": "repository I'm using the example this deployment if we were to apply and then",
    "start": "800960",
    "end": "808400"
  },
  {
    "text": "do a get against custom resources we would see that the package",
    "start": "808400",
    "end": "814480"
  },
  {
    "text": "programmatically defined three custom resources for us so at this point we",
    "start": "814480",
    "end": "819650"
  },
  {
    "text": "have access to SVD clusters we also have access to my CD restore and that's it e",
    "start": "819650",
    "end": "825440"
  },
  {
    "text": "backup and for the remainder of the talk I'm going to focus mostly on the that",
    "start": "825440",
    "end": "830870"
  },
  {
    "text": "city cluster resource alright so let's",
    "start": "830870",
    "end": "837170"
  },
  {
    "start": "835000",
    "end": "891000"
  },
  {
    "text": "go back to the diagram and now that we have our resources available to us we're",
    "start": "837170",
    "end": "843590"
  },
  {
    "text": "going to define a desired state so in this example we're going to do three node cluster and we're going to",
    "start": "843590",
    "end": "849830"
  },
  {
    "text": "specifically use at CD version 3.3 times",
    "start": "849830",
    "end": "856060"
  },
  {
    "text": "as far as the controller is concerned it's it's tasks are to populate the",
    "start": "856190",
    "end": "861590"
  },
  {
    "text": "configuration to facilitate a cluster and then also to to deploy the pods to",
    "start": "861590",
    "end": "867560"
  },
  {
    "text": "kubernetes as well the yellow representation of that definition is",
    "start": "867560",
    "end": "874870"
  },
  {
    "text": "this is this is what this would look like so you can see that we're making a call against the NC the cluster resource",
    "start": "874870",
    "end": "882410"
  },
  {
    "text": "in the spec we're defining the size of three and a specific version of that CD",
    "start": "882410",
    "end": "888380"
  },
  {
    "text": "as well so we were to apply this we could see if",
    "start": "888380",
    "end": "894480"
  },
  {
    "text": "we did and then did it get against pods we can see that there's three at CD",
    "start": "894480",
    "end": "899900"
  },
  {
    "text": "instances running so this is great this",
    "start": "899900",
    "end": "905310"
  },
  {
    "text": "is a very quick process and it worked as expected but we still want to do a",
    "start": "905310",
    "end": "910620"
  },
  {
    "text": "little bit of a sanity check to make sure this is a valid cluster so we just did a quick member list against one of",
    "start": "910620",
    "end": "918990"
  },
  {
    "text": "the pods and we can see that there are three members in the cluster and that",
    "start": "918990",
    "end": "924300"
  },
  {
    "text": "one of those members is leaders so this looks pretty sane so at this point at CD",
    "start": "924300",
    "end": "931430"
  },
  {
    "text": "operator has done done its job it has taken the desired state of a",
    "start": "931430",
    "end": "938160"
  },
  {
    "text": "three node cluster reconciled that against kubernetes to give the current",
    "start": "938160",
    "end": "943500"
  },
  {
    "text": "state of a three node cluster so now that we're at this point let's kind of",
    "start": "943500",
    "end": "948570"
  },
  {
    "text": "take a little bit of a look into how sed operator did this right so what were the",
    "start": "948570",
    "end": "954930"
  },
  {
    "text": "those steps so if we were to describe",
    "start": "954930",
    "end": "960510"
  },
  {
    "text": "the first pod that was initialized we'd see it's actually a single node cluster",
    "start": "960510",
    "end": "965570"
  },
  {
    "text": "which you know you may or may not have assumed so the reason for this is",
    "start": "965570",
    "end": "973260"
  },
  {
    "text": "because when we declare the resource we can declare a size and to scale up and",
    "start": "973260",
    "end": "978270"
  },
  {
    "text": "scale down the cluster we're going to use the cluster API so that would involve using a for example member add",
    "start": "978270",
    "end": "984960"
  },
  {
    "text": "right so this is how at C the operator scales cluster so if you look at the",
    "start": "984960",
    "end": "990930"
  },
  {
    "text": "second node you'll see that that increments and then of course the third would would do the same so this is it's",
    "start": "990930",
    "end": "998760"
  },
  {
    "text": "a smart right so in the case of a failure they can recover so that's",
    "start": "998760",
    "end": "1004930"
  },
  {
    "text": "that's work alright so we've went through kind of where at CD operator",
    "start": "1004930",
    "end": "1011510"
  },
  {
    "start": "1006000",
    "end": "1095000"
  },
  {
    "text": "does great as far as you know kind of easy task starting the three node cluster we",
    "start": "1011510",
    "end": "1017450"
  },
  {
    "text": "define a version that's pretty straightforward but now let's look at some failure cases so in the case of a",
    "start": "1017450",
    "end": "1023270"
  },
  {
    "text": "single failure the sed ed city still maintains quorum so we can still use the cluster",
    "start": "1023270",
    "end": "1030530"
  },
  {
    "text": "API so in this case the controller task would include first removing the failed",
    "start": "1030530",
    "end": "1038290"
  },
  {
    "text": "member removing the pod and then doing a",
    "start": "1038290",
    "end": "1043790"
  },
  {
    "text": "member add and and launching a new pod bringing the total number of active",
    "start": "1043790",
    "end": "1050210"
  },
  {
    "text": "members back to three and then on the",
    "start": "1050210",
    "end": "1057230"
  },
  {
    "text": "code end of things we can if we take a look the codes very easy to follow this is against this is a snippet from",
    "start": "1057230",
    "end": "1064250"
  },
  {
    "text": "cluster reconcile and if we take a look here we can see that we're actually doing calls to Etsy D using client v3 so",
    "start": "1064250",
    "end": "1073300"
  },
  {
    "text": "if you look at the top there you'll see this is actually a member add command",
    "start": "1073300",
    "end": "1078470"
  },
  {
    "text": "and then below that if you look at you'll see a create pod and if we were to kind of follow that back far enough",
    "start": "1078470",
    "end": "1084710"
  },
  {
    "text": "we would see eventually that this was a call to 2v1 pod so we're talking about using the kubernetes client go so that's",
    "start": "1084710",
    "end": "1098150"
  },
  {
    "start": "1095000",
    "end": "1118000"
  },
  {
    "text": "the operator has succeeded right has taken a failure state of a single node",
    "start": "1098150",
    "end": "1104420"
  },
  {
    "text": "that would have required manual interaction with the API and it has solved for that right so this is this is",
    "start": "1104420",
    "end": "1111950"
  },
  {
    "text": "a great example of what it does well so",
    "start": "1111950",
    "end": "1116799"
  },
  {
    "start": "1118000",
    "end": "1183000"
  },
  {
    "text": "so now let's kind of go through where this can go a little sideways so in the",
    "start": "1119680",
    "end": "1126050"
  },
  {
    "text": "case of a multi node failure then things get a little more complicated because we've lost quorum now we can no longer",
    "start": "1126050",
    "end": "1133100"
  },
  {
    "text": "interact with the API and get get out of this situation with a cluster API so",
    "start": "1133100",
    "end": "1140570"
  },
  {
    "text": "that to resolve this situation will require a snapshot restore so although",
    "start": "1140570",
    "end": "1146510"
  },
  {
    "text": "there is a restore operator with that's it e operator if there was to be",
    "start": "1146510",
    "end": "1154370"
  },
  {
    "text": "say all three nodes were to fail the the",
    "start": "1154370",
    "end": "1159450"
  },
  {
    "text": "data and the positive cells are ephemeral so if we were to lose all of that we would only the last known good",
    "start": "1159450",
    "end": "1166770"
  },
  {
    "text": "state of the data would be the last snapshot so there's a possibility of",
    "start": "1166770",
    "end": "1171900"
  },
  {
    "text": "some data loss there we're to use that wrong and the inevitable thing here is that this will",
    "start": "1171900",
    "end": "1177630"
  },
  {
    "text": "fail and as the operator cannot solve this problem by itself so how can we",
    "start": "1177630",
    "end": "1185430"
  },
  {
    "start": "1183000",
    "end": "1294000"
  },
  {
    "text": "make this better per system volume support so this is this was a big",
    "start": "1185430",
    "end": "1192560"
  },
  {
    "text": "addition so the initial support was added via PR 1861 and basically by",
    "start": "1192560",
    "end": "1199860"
  },
  {
    "text": "having persistent volume support we can have the data directory persist outside",
    "start": "1199860",
    "end": "1207090"
  },
  {
    "text": "of the lifecycle of the pod right and that's really important in this case because the data underlying is what at",
    "start": "1207090",
    "end": "1213900"
  },
  {
    "text": "CD needs to operate so in this case so now that's any",
    "start": "1213900",
    "end": "1222300"
  },
  {
    "text": "operator has another tool in its tool chain instead of just doing a restore or",
    "start": "1222300",
    "end": "1228980"
  },
  {
    "text": "do using the cluster API we can now look",
    "start": "1228980",
    "end": "1234690"
  },
  {
    "text": "at recycling the pods right so so how would that work so let's go back to the",
    "start": "1234690",
    "end": "1241920"
  },
  {
    "text": "multi node failure so in the case of this multi node failure at CD is lost",
    "start": "1241920",
    "end": "1248010"
  },
  {
    "text": "quorum and and this would as far as the",
    "start": "1248010",
    "end": "1253050"
  },
  {
    "text": "operator goes required some type of restore from snapshot but if we were to",
    "start": "1253050",
    "end": "1260730"
  },
  {
    "text": "actually just unmount the PVC from the pod restart all three pods with the same",
    "start": "1260730",
    "end": "1267720"
  },
  {
    "text": "configuration and name that CD would come back up and that's what we would",
    "start": "1267720",
    "end": "1273480"
  },
  {
    "text": "operate as if there was no problem because there isn't so that problem",
    "start": "1273480",
    "end": "1278580"
  },
  {
    "text": "would be solved so the result is a healthy cluster this is this is pretty big it's a lot more",
    "start": "1278580",
    "end": "1285630"
  },
  {
    "text": "complicated than it looks there's failure cases that we need to work through but you know it's it's pretty",
    "start": "1285630",
    "end": "1292860"
  },
  {
    "text": "exciting so just a few of the future",
    "start": "1292860",
    "end": "1298740"
  },
  {
    "start": "1294000",
    "end": "1329000"
  },
  {
    "text": "goals for the project so we are maintaining it more now we're hoping to get some of the features",
    "start": "1298740",
    "end": "1305120"
  },
  {
    "text": "there's a lot of outside APRs we're trying to work through of course we need to solve for corner cases in PV and PVC",
    "start": "1305120",
    "end": "1313460"
  },
  {
    "text": "failures and we definitely want to add learner support for durability and",
    "start": "1313460",
    "end": "1322190"
  },
  {
    "text": "efforts so upgrading a city is another",
    "start": "1322190",
    "end": "1332100"
  },
  {
    "text": "challenge when you work on you strip your systems so right now so SCD",
    "start": "1332100",
    "end": "1341970"
  },
  {
    "text": "so a city has a concept called a cluster version it is our stream value agreed by",
    "start": "1341970",
    "end": "1347640"
  },
  {
    "text": "the home like majority of the cluster so this is how a CD of weight works so leader first now ghetto ghetto",
    "start": "1347640",
    "end": "1356179"
  },
  {
    "text": "versions over budget value from each peer and then little picks the lowest",
    "start": "1356179",
    "end": "1361340"
  },
  {
    "text": "sovereign value out of all those virgins and then little pig store Louis over",
    "start": "1361340",
    "end": "1367740"
  },
  {
    "text": "virgin value as a cross diversion and the broadcast cluster version value to",
    "start": "1367740",
    "end": "1373650"
  },
  {
    "text": "the peers and then each peer now the follower we try to applied it cluster",
    "start": "1373650",
    "end": "1380340"
  },
  {
    "text": "version value to the server and then this will fail if the cluster version is",
    "start": "1380340",
    "end": "1385559"
  },
  {
    "text": "a downgrade to the server so in order to prevent or downgrade as of like STD 3.3",
    "start": "1385559",
    "end": "1392100"
  },
  {
    "text": "so the new sed member would fail to join the cluster if the cluster version",
    "start": "1392100",
    "end": "1398419"
  },
  {
    "text": "if the like new members of our version is lured in the cluster version so in",
    "start": "1398419",
    "end": "1405210"
  },
  {
    "text": "order to verify all these steps upgrade must happen incrementally so when you",
    "start": "1405210",
    "end": "1410640"
  },
  {
    "text": "upgrade its sed cluster you just upgrade one by one",
    "start": "1410640",
    "end": "1415610"
  },
  {
    "text": "so this is how a city so this is how like litter picks the cluster version so",
    "start": "1415950",
    "end": "1423190"
  },
  {
    "text": "first litter fetches the server versions from each peer and then in this case",
    "start": "1423190",
    "end": "1428700"
  },
  {
    "text": "leaders gonna pick the lowest like bottom value so when you have a three point three in the three point four the",
    "start": "1428700",
    "end": "1434950"
  },
  {
    "text": "cluster version will be selected and so a three point three so here we have our",
    "start": "1434950",
    "end": "1443590"
  },
  {
    "text": "new member with like software version 3.2 so if this member tried to join 3.2",
    "start": "1443590",
    "end": "1451090"
  },
  {
    "text": "3.3 cluster it would fail in order to prevent or downgrade so when you when",
    "start": "1451090",
    "end": "1461409"
  },
  {
    "start": "1458000",
    "end": "1504000"
  },
  {
    "text": "you upgrade City there are still a lot of things that can go wrong so we want to do better so we are introducing",
    "start": "1461409",
    "end": "1472239"
  },
  {
    "text": "introducing the downgrade feature so you when you run the sed Koro downgrade",
    "start": "1472239",
    "end": "1477549"
  },
  {
    "text": "command like we can temporarily while is this like lower version value like a",
    "start": "1477549",
    "end": "1483129"
  },
  {
    "text": "target version and then we're gonna provide API that you can check the progress of downgrade and then we were",
    "start": "1483129",
    "end": "1489669"
  },
  {
    "text": "we were also provide the comment that you can cancel the neck down great feature so when gia from Google is working on",
    "start": "1489669",
    "end": "1496149"
  },
  {
    "text": "this in there we gonna add this feature to the STD 3.4 all right so extensible",
    "start": "1496149",
    "end": "1507669"
  },
  {
    "start": "1504000",
    "end": "1542000"
  },
  {
    "text": "discovery so let CD offers various bootstrap options probably the most",
    "start": "1507669",
    "end": "1514419"
  },
  {
    "text": "interesting I think is SRV discovery but what we want to be able to do is support",
    "start": "1514419",
    "end": "1521649"
  },
  {
    "text": "multiple discovery mechanisms unfortunately doing that within the server really isn't maintainable so",
    "start": "1521649",
    "end": "1529389"
  },
  {
    "text": "we're looking at so basically what I'm talking about is there's some ideas for",
    "start": "1529389",
    "end": "1534989"
  },
  {
    "text": "for basically moving discovery mechanism out of the server and into the client",
    "start": "1534989",
    "end": "1542549"
  },
  {
    "start": "1542000",
    "end": "1597000"
  },
  {
    "text": "back to the SRV discovery SRV discovery is really interesting because you can",
    "start": "1543520",
    "end": "1550150"
  },
  {
    "text": "define the a lot of your information in",
    "start": "1550150",
    "end": "1555250"
  },
  {
    "text": "the SRV details of the DNS entries so at",
    "start": "1555250",
    "end": "1560890"
  },
  {
    "text": "that point you can minimize your configuration because that data will be",
    "start": "1560890",
    "end": "1566470"
  },
  {
    "text": "consumed during the SRV query the problem is we've gone probably 1/2 or",
    "start": "1566470",
    "end": "1573790"
  },
  {
    "text": "3/4 of the way if you look at the the highlighted areas there's still quite a bit of dynamic information that would",
    "start": "1573790",
    "end": "1580510"
  },
  {
    "text": "still need to be populated in this config right so for example we're listening to ABBA we may need to listen",
    "start": "1580510",
    "end": "1587470"
  },
  {
    "text": "to a specific IP so all this information needs to be populated in the config so",
    "start": "1587470",
    "end": "1593920"
  },
  {
    "text": "this is where we're looking that we can we can do better so this is an initial",
    "start": "1593920",
    "end": "1600400"
  },
  {
    "start": "1597000",
    "end": "1670000"
  },
  {
    "text": "proposal so it's still in its infancy but I hope to get something together in",
    "start": "1600400",
    "end": "1606610"
  },
  {
    "text": "the next few weeks on paper and basically the idea is name cluster in it",
    "start": "1606610",
    "end": "1613900"
  },
  {
    "text": "and basically how this works is first of all what if this was the configuration",
    "start": "1613900",
    "end": "1622810"
  },
  {
    "text": "required for SRV discovery if you take a look at this configuration it's completely static there's no there is",
    "start": "1622810",
    "end": "1629680"
  },
  {
    "text": "nothing identifying features of the node itself no IP domain names etc so this",
    "start": "1629680",
    "end": "1638350"
  },
  {
    "text": "greatly simplifies deployment when you are deploying this config you don't have to worry about the values and the reason",
    "start": "1638350",
    "end": "1648100"
  },
  {
    "text": "why is because the discovery has already occurred before @cd even starts so this",
    "start": "1648100",
    "end": "1653380"
  },
  {
    "text": "is done on a client level and then by having the code on the client side it's",
    "start": "1653380",
    "end": "1659020"
  },
  {
    "text": "easier to extend right so if you want to write your own discovery it's not that",
    "start": "1659020",
    "end": "1664090"
  },
  {
    "text": "big of a deal because the server doesn't have to support it so let's kind of go",
    "start": "1664090",
    "end": "1669100"
  },
  {
    "text": "through how that works so again this is generally at high level",
    "start": "1669100",
    "end": "1674750"
  },
  {
    "start": "1670000",
    "end": "1732000"
  },
  {
    "text": "the client would just have a single flag and this is basically those of you familiar with SRV discovery the client",
    "start": "1674750",
    "end": "1681920"
  },
  {
    "text": "already has this mechanism so we're just going to consume the results and from",
    "start": "1681920",
    "end": "1688850"
  },
  {
    "text": "that list we can we we are certainly smart enough to know which IP is ours",
    "start": "1688850",
    "end": "1694760"
  },
  {
    "text": "right or we can do a DNS query to see which which of this list it represents",
    "start": "1694760",
    "end": "1700580"
  },
  {
    "text": "this particular node so we can extract that information and then persist it to",
    "start": "1700580",
    "end": "1707630"
  },
  {
    "text": "the member bucket of the data store so once that's persisted on the server side",
    "start": "1707630",
    "end": "1713450"
  },
  {
    "text": "we can or something different but",
    "start": "1713450",
    "end": "1719110"
  },
  {
    "text": "basically just letting that CD know that those values could be at the store and",
    "start": "1719110",
    "end": "1725420"
  },
  {
    "text": "then if they aren't then fall back to the existing functionality so kind of a",
    "start": "1725420",
    "end": "1733610"
  },
  {
    "text": "visual of what I just kind of went through there we'll kind of show the different layers so at the top layer we",
    "start": "1733610",
    "end": "1740870"
  },
  {
    "text": "have our DNS as our V records okay and then as we move down into the client the",
    "start": "1740870",
    "end": "1748130"
  },
  {
    "text": "client is going to do a SRV query against the DMS and the results then",
    "start": "1748130",
    "end": "1753830"
  },
  {
    "text": "it's going to use to populate the member struct so this is very similar to what",
    "start": "1753830",
    "end": "1759920"
  },
  {
    "text": "happens in the server but we're going to also persist the client URL as well so",
    "start": "1759920",
    "end": "1767540"
  },
  {
    "text": "at this point we're gonna save that into the member bucket and and that's the",
    "start": "1767540",
    "end": "1773990"
  },
  {
    "text": "basic process so this is all great and",
    "start": "1773990",
    "end": "1781010"
  },
  {
    "start": "1778000",
    "end": "2191000"
  },
  {
    "text": "dandy but let's talk about a basic use case so if we were to go back to the operator example where we were deploying",
    "start": "1781010",
    "end": "1789440"
  },
  {
    "text": "an initial cluster and we had to wait for each node to increment well if we",
    "start": "1789440",
    "end": "1794630"
  },
  {
    "text": "wanted to provision if we wanted to provision new new data",
    "start": "1794630",
    "end": "1803049"
  },
  {
    "text": "layer as well you know this can all take time right so in this case with with the",
    "start": "1803049",
    "end": "1811830"
  },
  {
    "text": "with what I talked about we could do it in a NIC container and basically when the initialization took place we could",
    "start": "1811830",
    "end": "1819549"
  },
  {
    "text": "do our discovery and populate the data basically seed the data for the members",
    "start": "1819549",
    "end": "1825749"
  },
  {
    "text": "during the startup process and so at that point we could start all of the",
    "start": "1825749",
    "end": "1832720"
  },
  {
    "text": "nodes at the same time right so if this was five nodes if it was three we",
    "start": "1832720",
    "end": "1837909"
  },
  {
    "text": "wouldn't have to wait so this is just an example of a thank you",
    "start": "1837909",
    "end": "1848368"
  },
  {
    "text": "[Applause] yeah we have five minute for cleaning",
    "start": "1850240",
    "end": "1857070"
  },
  {
    "text": "well so the server of the client still needs to have that capability to basically start a raft node and and",
    "start": "1872070",
    "end": "1879190"
  },
  {
    "text": "persist to the dowser so you're doing the same operations of the server but",
    "start": "1879190",
    "end": "1884350"
  },
  {
    "text": "it's much more condensed right so it's it's not a full server implementation but as far as writing to the member",
    "start": "1884350",
    "end": "1891010"
  },
  {
    "text": "bucket doing a put you know that that would have to be part of the client",
    "start": "1891010",
    "end": "1897690"
  },
  {
    "text": "well because the underlying data is still intact right so as far as the data",
    "start": "1920529",
    "end": "1928279"
  },
  {
    "text": "directories are concerned they are on the persistent volume themselves right",
    "start": "1928279",
    "end": "1934009"
  },
  {
    "text": "so if the pod if the pods were to fail you know we can",
    "start": "1934009",
    "end": "1939679"
  },
  {
    "text": "basically the lifecycle of that pod isn't connected directly to the to the",
    "start": "1939679",
    "end": "1946129"
  },
  {
    "text": "persistent volume so if we were to restart those pods with the exact same names and the persistent volume per PBC",
    "start": "1946129",
    "end": "1956570"
  },
  {
    "text": "when that starts then the at CD as far",
    "start": "1956570",
    "end": "1962509"
  },
  {
    "text": "as its concerned would still be would run does that make sense I mean it doesn't know that that happened anyway",
    "start": "1962509",
    "end": "1988759"
  },
  {
    "text": "for day use case we have something some feature for the Chimaera maker so he has",
    "start": "1988759",
    "end": "1994789"
  },
  {
    "text": "like all like low latency so you just like consistently like watch from the existing cluster in there you just a",
    "start": "1994789",
    "end": "2000789"
  },
  {
    "text": "minute our data from the other cluster and then you can use there like a mirror like mirrored node and so forth back but",
    "start": "2000789",
    "end": "2008379"
  },
  {
    "text": "like in terms of your like deco replicated like the data center like deployment you can still tune though",
    "start": "2008379",
    "end": "2015820"
  },
  {
    "text": "like a heartbeat prymaat and the election time out so there are like many other ways to do it well photo like if",
    "start": "2015820",
    "end": "2022269"
  },
  {
    "text": "you have the lunar node you can also use like Lohner to have our like four pack like a node in different eras in",
    "start": "2022269",
    "end": "2031018"
  },
  {
    "text": "so I think they're good so do you mind",
    "start": "2056010",
    "end": "2062049"
  },
  {
    "text": "repeating that so what we are trying to",
    "start": "2062049",
    "end": "2070960"
  },
  {
    "text": "do is that there already we are trying to upgrade the HDD host vm so in order",
    "start": "2070960",
    "end": "2077740"
  },
  {
    "text": "to do and we have a - persistent volumes now to do that we unbounded persistent volume we upgrade our host vm and then",
    "start": "2077740",
    "end": "2085240"
  },
  {
    "text": "remount at at CD rebounded volume to the HDD and then try to do add member so",
    "start": "2085240",
    "end": "2093280"
  },
  {
    "text": "like we are trying to add it to at CD cluster which already has the data it's",
    "start": "2093280",
    "end": "2098950"
  },
  {
    "text": "not like a it's a new member with all the data exists king",
    "start": "2098950",
    "end": "2104309"
  },
  {
    "text": "right so so to answer yes so so basically we're not using this to extend",
    "start": "2116250",
    "end": "2122470"
  },
  {
    "text": "the cluster we're just we're basically in the case of a failed pod we can recycle the underlying dinosaur but",
    "start": "2122470",
    "end": "2130480"
  },
  {
    "text": "we're not extending the size of the cluster so we can't just replicate it n",
    "start": "2130480",
    "end": "2136840"
  },
  {
    "text": "times and then start at CD that wouldn't work because we still have to populate the one more question",
    "start": "2136840",
    "end": "2154950"
  },
  {
    "text": "I'm really just getting into this and there's definitely a ton of corner cases",
    "start": "2167010",
    "end": "2172390"
  },
  {
    "text": "even just in the last few weeks of work so I mean there are concerns so it's",
    "start": "2172390",
    "end": "2178540"
  },
  {
    "text": "something that you know that we'll have to solve around but yeah there is concern thank you",
    "start": "2178540",
    "end": "2189790"
  },
  {
    "text": "[Applause]",
    "start": "2189790",
    "end": "2193489"
  }
]