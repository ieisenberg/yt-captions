[
  {
    "text": "simple to use API and various to optimize distributed workloads so first of all let me speak about about data",
    "start": "160",
    "end": "6640"
  },
  {
    "text": "scientists uh so what we hear from them we've been working there for the last 7 to 8 years so what they really wants",
    "start": "6640",
    "end": "12719"
  },
  {
    "text": "they want quick ability to write ML code using native libraries like torch speed",
    "start": "12719",
    "end": "18160"
  },
  {
    "text": "jacks and scale it tricky part because it's kind of like to do multiple times sometimes",
    "start": "18160",
    "end": "25039"
  },
  {
    "text": "associated with the process so the problem is like they kind of like want to do multiple times without even",
    "start": "25039",
    "end": "30560"
  },
  {
    "text": "knowing it's running but the trick is like in reality there are a lot of things associated with this process for",
    "start": "30560",
    "end": "36640"
  },
  {
    "text": "example like they told us they need to know how to start environment how to configure Docker images how to configure",
    "start": "36640",
    "end": "43120"
  },
  {
    "text": "compute resources or data access sometimes they also need to learn more about API maybe they want to know about",
    "start": "43120",
    "end": "48920"
  },
  {
    "text": "Galing HPC technologies or maybe even know about resource cues so all this we",
    "start": "48920",
    "end": "54399"
  },
  {
    "text": "kind of talk infrastructure claim and the goal of us as a community how we can actually remove it because data",
    "start": "54399",
    "end": "60640"
  },
  {
    "text": "scientist stuck behind this group and right now we also live in a world of geni you know models become much more",
    "start": "60640",
    "end": "65920"
  },
  {
    "text": "complex we have like huge data set huge amount of data we have like all different frameworks the ecosystem is um",
    "start": "65920",
    "end": "74080"
  },
  {
    "text": "is just a lot technology API nickel blue how we can provide a unified way to give",
    "start": "74080",
    "end": "80000"
  },
  {
    "text": "them you know ability to interact with all those diversity of frameworks and in the same way adopting new technologies like uh uh API and other",
    "start": "80000",
    "end": "87520"
  },
  {
    "text": "things so what actually the end users want they just want a simplicity flexibility and scalability and they",
    "start": "87520",
    "end": "93840"
  },
  {
    "text": "don't want to learn anything about kubernetes so how we can abstract kubernetes um complexity from them so",
    "start": "93840",
    "end": "101200"
  },
  {
    "text": "for this we actually do this project called cubeflow trainer uh we introduced this project in the last coupon in",
    "start": "101200",
    "end": "108280"
  },
  {
    "text": "2024 so this project was kind of like a next generation of training operator which we started in 2017 right 2017 and",
    "start": "108280",
    "end": "116479"
  },
  {
    "text": "basically this is we kind of separate uh different services so we have a service for data science called train and we",
    "start": "116479",
    "end": "123200"
  },
  {
    "text": "have a service for devops engineers called training runtime so the difference is is training runtime is",
    "start": "123200",
    "end": "128239"
  },
  {
    "text": "like a blueprint that platform engineers can use to configure those uh those configurations and data science can work",
    "start": "128239",
    "end": "135200"
  },
  {
    "text": "with the one Python interface to interact with the jobs and then we're using the job set to actually perform distributed communications uh so with",
    "start": "135200",
    "end": "142840"
  },
  {
    "text": "that let me actually give a new thing that we're going to introduce in this session which is actually API runtime",
    "start": "142840",
    "end": "149840"
  },
  {
    "text": "and I think like uh the best thing to know about API is actually doing the demo about this new frameworks with MLX",
    "start": "149840",
    "end": "156959"
  },
  {
    "text": "and GP um so with that I'm going to show",
    "start": "156959",
    "end": "162080"
  },
  {
    "text": "you actually two notebooks uh the first one uh called distribute with MLX so for",
    "start": "162080",
    "end": "167760"
  },
  {
    "text": "those who not know MLX is the um framework specifically designed by we're",
    "start": "167760",
    "end": "173280"
  },
  {
    "text": "going through why we actually want to leverage it here because MLX using API for distributed communication and we try",
    "start": "173280",
    "end": "180640"
  },
  {
    "text": "to see how we can make it easier for folks to actually for scientists who want to run those code actually scale it",
    "start": "180640",
    "end": "186319"
  },
  {
    "text": "on top of um so the first thing I hope the demo will go well live let's see if",
    "start": "186319",
    "end": "191519"
  },
  {
    "text": "the internet is working actually so the first thing right what I need to do uh is I'm playing the role data scientist I",
    "start": "191519",
    "end": "198000"
  },
  {
    "text": "want to train so this is the simple CNN model which image classification example",
    "start": "198000",
    "end": "203440"
  },
  {
    "text": "the first thing I need to do I just need to initialize my trainer client and by point my trainer client to my local mini",
    "start": "203440",
    "end": "209760"
  },
  {
    "text": "platform so this is running locally in my mini cube on my machine without any you know other you know advanced compute",
    "start": "209760",
    "end": "215920"
  },
  {
    "text": "so next step is defining MLX code so this is pretty simple like if you know MLX API they have API for communication",
    "start": "215920",
    "end": "222319"
  },
  {
    "text": "like world size rank then I getting my data from amnest I distribute it across multiple partitions uh then I defining",
    "start": "222319",
    "end": "229599"
  },
  {
    "text": "my model again this is very simple example just using MLP model for um uh",
    "start": "229599",
    "end": "235120"
  },
  {
    "text": "image classification uh and then I defining my loss function my step and my training code here uh so this is again",
    "start": "235120",
    "end": "241519"
  },
  {
    "text": "native MLX code without any modification at the very end we export model back to disk because I want to do some evals",
    "start": "241519",
    "end": "247280"
  },
  {
    "text": "after this example is complete so what is the next step next step for me is actually uh get available runtimes so",
    "start": "247280",
    "end": "254400"
  },
  {
    "text": "remember we talked previously we have a runtime component so runtime you can think about like a template or blueprint that means that data scientist can use",
    "start": "254400",
    "end": "260880"
  },
  {
    "text": "to play with this uh so this is MLX runtime MLX distributed runtime which as",
    "start": "260880",
    "end": "266000"
  },
  {
    "text": "an entry point of API run and to use this runtime like what I just need to do I just have a one single simple API",
    "start": "266000",
    "end": "272880"
  },
  {
    "text": "called train and this API so this API actually comes from Cubeflow SDK so we",
    "start": "272880",
    "end": "277919"
  },
  {
    "text": "as a community kind of created this new SDK called Qflow SDK and they provide a Python interface for me as a data",
    "start": "277919",
    "end": "284479"
  },
  {
    "text": "scientist to interact with Kubernetes cluster so no Kubernetes at all like I",
    "start": "284479",
    "end": "290000"
  },
  {
    "text": "just you know I just say I have my function called ML MLX train I have my arguments like here I'm passing my model",
    "start": "290000",
    "end": "297199"
  },
  {
    "text": "path and I specify how many nodes I want to use to scale my MLX code also I can I",
    "start": "297199",
    "end": "304000"
  },
  {
    "text": "can install some packages in the runtime it could be cycl or pandas whatever I want to install to evaluate it and then this is like runtime reference to um to",
    "start": "304000",
    "end": "312240"
  },
  {
    "text": "my runtime here so this extra API generates some job id uh also we can",
    "start": "312240",
    "end": "317600"
  },
  {
    "text": "list all the jobs so you can see some of the jobs being created before this is the jobs we just started we can get all the steps from jobs as you can see using",
    "start": "317600",
    "end": "323680"
  },
  {
    "text": "three nodes so this is one of the launcher nodes to the worker nodes and all of them using like two CPUs devices",
    "start": "323680",
    "end": "329840"
  },
  {
    "text": "uh then we can get the locks from our code so right now because we do distribute training here uh actually",
    "start": "329840",
    "end": "335440"
  },
  {
    "text": "we're running uh 60,000 samples 60,000 images which is distributed across three",
    "start": "335440",
    "end": "340639"
  },
  {
    "text": "nodes so in total we perform DDP here so we're running as you can see uh 20,000",
    "start": "340639",
    "end": "347120"
  },
  {
    "text": "images uh pair like working pair like uh worker node uh so we is a very simple as",
    "start": "347120",
    "end": "354080"
  },
  {
    "text": "you can see training is happening um so it generates accuracy it's actually",
    "start": "354080",
    "end": "359120"
  },
  {
    "text": "running across all the available devices uh for me to work with also like I think important to mention if I have more more",
    "start": "359120",
    "end": "366000"
  },
  {
    "text": "like resources I can you know change number of nodes to 100 to 1,000 and it will automatically distribute my",
    "start": "366000",
    "end": "372400"
  },
  {
    "text": "function across all the available resources so let's see if training is complete all right so it's seven app",
    "start": "372400",
    "end": "379600"
  },
  {
    "text": "yeah so training is complete model has been saved to the disk let's try to see if this is available so this is my model",
    "start": "379600",
    "end": "385919"
  },
  {
    "text": "it's sitting right here uh okay so then you know what what data scientists usually do evals right evalations uh I",
    "start": "385919",
    "end": "392960"
  },
  {
    "text": "can pass some images from the test batches and I can see what kind of my model returns so this is like you know",
    "start": "392960",
    "end": "398639"
  },
  {
    "text": "our model pretty I would say right so we have like 40% of accuracy because we you know we you know don't train like a lot",
    "start": "398639",
    "end": "404880"
  },
  {
    "text": "of data but you know this model actually generates some outputs here you know I can do some relations and I can see how",
    "start": "404880",
    "end": "410000"
  },
  {
    "text": "it actually um been um calculated so very simple just writing native code",
    "start": "410000",
    "end": "415520"
  },
  {
    "text": "scale it uh but again it's very powerful because the next notebook what we're going to do we're going to fine-tune",
    "start": "415520",
    "end": "420560"
  },
  {
    "text": "tune T5 model using deep speed uh so for those who don't know deep speed is a framework built on top of torch to",
    "start": "420560",
    "end": "427039"
  },
  {
    "text": "optimize this for distributed training and here we're going to use uh eight varied GPUs uh on two nodes so it's",
    "start": "427039",
    "end": "434880"
  },
  {
    "text": "using GPUs to fine-tune T5 transformer uh first step similar to previous one we",
    "start": "434880",
    "end": "440560"
  },
  {
    "text": "just initializing the client passing the context to my GPU cluster uh specifying",
    "start": "440560",
    "end": "445680"
  },
  {
    "text": "my distributed environment specifying my data set here we're using VikiHow data set uh tokenized data set downloading",
    "start": "445680",
    "end": "452560"
  },
  {
    "text": "model tokenizer uh defining deep speed config so for those familiar with deep",
    "start": "452560",
    "end": "457840"
  },
  {
    "text": "speed deep speed provide a very nice way for data scientists to configure optimizer scheduulers uh microbatches so",
    "start": "457840",
    "end": "464400"
  },
  {
    "text": "I think they really familiar with those APIs again native native deep speed code",
    "start": "464400",
    "end": "469440"
  },
  {
    "text": "here and okay in the very end in the very end we exporting model back to S3",
    "start": "469440",
    "end": "475120"
  },
  {
    "text": "so this is my training function that I define uh saving checkpoint to the S3 getting available runtimes so because we",
    "start": "475120",
    "end": "481680"
  },
  {
    "text": "have GPU cluster here I have another runtimes so the first runtime is deep speed distributed second runtime is",
    "start": "481680",
    "end": "486960"
  },
  {
    "text": "torch distributed and as you can see this runtime has a four accelerators for GPUs for me also we have this API called",
    "start": "486960",
    "end": "494080"
  },
  {
    "text": "runtime packages so sometimes I want to know like what is inside the runtime and",
    "start": "494080",
    "end": "499280"
  },
  {
    "text": "what kind of packages it actually contains so this actually should uh tell",
    "start": "499280",
    "end": "504720"
  },
  {
    "text": "me I hope this will tell me or no I don't know let's see it should tell me what is instal inst installed inside the",
    "start": "504720",
    "end": "511240"
  },
  {
    "text": "runtime all right right all right all right all right all right all right all right all right all right all right all right right all right",
    "start": "511240",
    "end": "517039"
  },
  {
    "text": "right sometime it takes time you know to to process an image and get the data but",
    "start": "517080",
    "end": "522399"
  },
  {
    "text": "usually like you know this API should return me the the training job let me actually show you all right you know",
    "start": "522399",
    "end": "528959"
  },
  {
    "text": "it's always great to have a live demo but sometime you need to have a recording you know so all right so let",
    "start": "528959",
    "end": "534000"
  },
  {
    "text": "me let me show you what's actually it should do um basically this API so I",
    "start": "534000",
    "end": "539360"
  },
  {
    "text": "hope you can see it here all right not not here so okay train custom trainer",
    "start": "539360",
    "end": "546560"
  },
  {
    "text": "train job yeah this one all right so never mind live demo you know sometimes it",
    "start": "546560",
    "end": "553360"
  },
  {
    "text": "doesn't work so this API should actually return a list of available packages in",
    "start": "553360",
    "end": "558480"
  },
  {
    "text": "runtime uh so you can see this open API 4.1",
    "start": "558480",
    "end": "563760"
  },
  {
    "text": "Python version available packages at the bottom you can see the the deep speed",
    "start": "563760",
    "end": "569440"
  },
  {
    "text": "should be installed there we installed some packages here so me as you can understand what version of deep speed is",
    "start": "569440",
    "end": "575519"
  },
  {
    "text": "using uh what I can use to evaluate my model and you can see this demo was recorded in 4 a.m nice time to record a",
    "start": "575519",
    "end": "582080"
  },
  {
    "text": "demo so yeah so this is actually also available GPU devices for me so it says four Tesla V100 for me to play with and",
    "start": "582080",
    "end": "589519"
  },
  {
    "text": "the next step similar to previous examples what we need to do we need to take the function and call the one API",
    "start": "589519",
    "end": "596000"
  },
  {
    "text": "called train uh which basically passing this deep speed function we defined before how many nodes I want to use what",
    "start": "596000",
    "end": "603440"
  },
  {
    "text": "is my arguments to my function what is the input arguments to my training function like data set URL model name of",
    "start": "603440",
    "end": "610160"
  },
  {
    "text": "the bucket uh like T5 base um and uh um",
    "start": "610160",
    "end": "615760"
  },
  {
    "text": "yeah then we just you know me as a data scientist just need to execute this code this you know Python simple simple function similar to previous example we",
    "start": "615760",
    "end": "622160"
  },
  {
    "text": "can configure like you know version of the runtime packages like B3 or any other packages I want to install on top",
    "start": "622160",
    "end": "628800"
  },
  {
    "text": "on what is already exist in the runtime so this return the random job ID uh then",
    "start": "628800",
    "end": "634399"
  },
  {
    "text": "we can get list of all of our jobs we can actually get the the nodes so here because we're using two GPUs we return",
    "start": "634399",
    "end": "640399"
  },
  {
    "text": "two different nodes here and uh here it say four GPUs for every node",
    "start": "640399",
    "end": "646920"
  },
  {
    "text": "um and yeah so we're also using GPUs here we have the graphana dashboard available so this is the actually",
    "start": "646920",
    "end": "652640"
  },
  {
    "text": "utilization dashboard you can think about using DCGM exporter for this to show me like utilization of my uh",
    "start": "652640",
    "end": "658079"
  },
  {
    "text": "training uh so when yeah we can see some spikes here uh we can see like number of",
    "start": "658079",
    "end": "663360"
  },
  {
    "text": "GPUs so this is like very useful dashboard for data scientists to understand what's happening uh behind",
    "start": "663360",
    "end": "668720"
  },
  {
    "text": "the job um so right now because we're doing like tokenization we can see like on the right side it's not been still",
    "start": "668720",
    "end": "674800"
  },
  {
    "text": "like yet utilize the GPU resources here um so this like I just run the job",
    "start": "674800",
    "end": "680560"
  },
  {
    "text": "before we start this demo basically uh so we get again get job locks API return the locks for my for my training",
    "start": "680560",
    "end": "687120"
  },
  {
    "text": "function so because here we're using uh two nodes in four GPUs we do multiode uh",
    "start": "687120",
    "end": "694079"
  },
  {
    "text": "multiode multiGPU so in total we're using eight devices which means like in total we're using 160 samples across all",
    "start": "694079",
    "end": "701440"
  },
  {
    "text": "those accelerators uh here we can see like discover API settings um we should see like deep speed",
    "start": "701440",
    "end": "708800"
  },
  {
    "text": "configuration at the very bottom here yeah like this so similar configuration we said before",
    "start": "708800",
    "end": "715240"
  },
  {
    "text": "um and then at the very end we train the model so here we do fine-tuning of T5",
    "start": "715240",
    "end": "721200"
  },
  {
    "text": "and we just run this on a few appex and I think the training takes around you know 60 seconds um and every node every",
    "start": "721200",
    "end": "728639"
  },
  {
    "text": "working node process 160 uh uh 160 samples so right now if you go back to",
    "start": "728639",
    "end": "735440"
  },
  {
    "text": "the graph dashboard here we can see the spikes because GPU utilization go up and we can see like eight Nvidia GPU devices",
    "start": "735440",
    "end": "742320"
  },
  {
    "text": "that we use to fine-tune this LLM so and then I guess the next step",
    "start": "742320",
    "end": "748000"
  },
  {
    "text": "for us is to perform evaluations the very end um total training time is 69",
    "start": "748000",
    "end": "753959"
  },
  {
    "text": "seconds um and then we again we can download model back from S3 to the",
    "start": "753959",
    "end": "759279"
  },
  {
    "text": "notebook i didn't do this here because the model quite large it's like three three GB and we preform somewhere else",
    "start": "759279",
    "end": "765440"
  },
  {
    "text": "so T5 is very good for text summarization so it can do some summarize your data so we're taking the",
    "start": "765440",
    "end": "771200"
  },
  {
    "text": "information from um Qflow documentation qflow documentation which actually uh",
    "start": "771200",
    "end": "777040"
  },
  {
    "text": "summarize I ask my LLM to summarize the text at the bottom and performing output",
    "start": "777040",
    "end": "782079"
  },
  {
    "text": "like what is the Qflow trainer project is about yeah I think like right now yeah",
    "start": "782079",
    "end": "787760"
  },
  {
    "text": "we're just loading the model we fine-tune to the hugging face transformer and asking the transformer",
    "start": "787760",
    "end": "793440"
  },
  {
    "text": "to actually uh produce the output it takes some time for my machine",
    "start": "793440",
    "end": "799120"
  },
  {
    "text": "to process the model and actually generate the output from here um it should",
    "start": "799120",
    "end": "806320"
  },
  {
    "text": "be yeah it should take like a few seconds afterwards so uh yeah so Qflow",
    "start": "806360",
    "end": "811680"
  },
  {
    "text": "trainer is a comparency project designed for large language models so very simple right um but again like I think the goal",
    "start": "811680",
    "end": "818959"
  },
  {
    "text": "what we try to say here and let me go back to my official slide",
    "start": "818959",
    "end": "825160"
  },
  {
    "text": "so like what what what we try to say here um you see two examples right one is using MLX second is using deep speed",
    "start": "825160",
    "end": "832320"
  },
  {
    "text": "uh no Kubernetes at all I didn't even go to coupubectl like I don't want to know what is coupl like I'm a data scientist",
    "start": "832320",
    "end": "838079"
  },
  {
    "text": "right I forally scale MLX and deep speed using Qflow train job I didn't even know",
    "start": "838079",
    "end": "843680"
  },
  {
    "text": "anything about API because I just say I want to use you know API is a communication library but I don't really",
    "start": "843680",
    "end": "849199"
  },
  {
    "text": "uh configure it and this consistency between other environments like my local environment my cloud environment ment",
    "start": "849199",
    "end": "854720"
  },
  {
    "text": "all the infrastructure complexity is hidden which allows me to do rapid iteration so if you want to check this notebook this is the QR code for for",
    "start": "854720",
    "end": "861720"
  },
  {
    "text": "them i just want to spend maybe um a few minutes for quickly like QFL trainer V2",
    "start": "861720",
    "end": "867360"
  },
  {
    "text": "so this is the overall introduction by the way thank you CN for working on this new logo with this new logo for our",
    "start": "867360",
    "end": "873360"
  },
  {
    "text": "project and the goal is for this project is to allow data scientists to do training on Kubernetes in a very very",
    "start": "873360",
    "end": "879600"
  },
  {
    "text": "simple way so we connect all these libraries on top of cube on top of kubernetes with additional features like",
    "start": "879600",
    "end": "885760"
  },
  {
    "text": "multiode training fine-tuning elastic training gang scheduling without even worrying about kubernetes complexities",
    "start": "885760",
    "end": "892079"
  },
  {
    "text": "today we support um three runtimes which is torch deep speed and mallex we're",
    "start": "892079",
    "end": "897760"
  },
  {
    "text": "working on jacks and tensorflow support as well so folks can also leverage those runtimes for those frameworks and if you",
    "start": "897760",
    "end": "903440"
  },
  {
    "text": "want to learn about this project I just leave with the QR code for you as well so before I pass it to you to speak more",
    "start": "903440",
    "end": "908639"
  },
  {
    "text": "about HPC uh this is actually what's happening behind the scenes so demo looks extremely simple but behind the",
    "start": "908639",
    "end": "914880"
  },
  {
    "text": "scenes we orchestrating the entire API cluster to perform distributed communication so NPI require host file",
    "start": "914880",
    "end": "921040"
  },
  {
    "text": "SSH keys to create those training nodes and as you can see here we have job set",
    "start": "921040",
    "end": "927199"
  },
  {
    "text": "which generates two jobs we have a launcher and the node and the launcher creates the the launcher node which are",
    "start": "927199",
    "end": "932880"
  },
  {
    "text": "running the training as well and then it execute the NPI run command on the worker node so worker node can also",
    "start": "932880",
    "end": "938320"
  },
  {
    "text": "perform the the communication and in the NPI world every GPU is a slot uh so you",
    "start": "938320",
    "end": "944000"
  },
  {
    "text": "can see we have a total of eight GPUs and the variant we exporting world back to S back to S3 so data science can",
    "start": "944000",
    "end": "950160"
  },
  {
    "text": "perform evals so it's very complex like looking in this diagram but from the science perspective it just one API one",
    "start": "950160",
    "end": "956880"
  },
  {
    "text": "API one function and that's it so with that I will pass it to Ricky we will speak more about how we see this",
    "start": "956880",
    "end": "962480"
  },
  {
    "text": "transition from HPC to AI um moving forward",
    "start": "962480",
    "end": "968680"
  },
  {
    "text": "okay um thank you for uh describing Andre uh in my section uh we are going",
    "start": "970880",
    "end": "978240"
  },
  {
    "text": "to talk about what is uh transition from HPC to AI workload in my talk title and",
    "start": "978240",
    "end": "986720"
  },
  {
    "text": "what is a problem for that um today's model development and delivering",
    "start": "986720",
    "end": "992560"
  },
  {
    "text": "workflow has many steps uh as you can see in this slide uh it is from data",
    "start": "992560",
    "end": "999360"
  },
  {
    "text": "preparation to model",
    "start": "999360",
    "end": "1003040"
  },
  {
    "text": "serving however uh SPC focuses mostly only on model development tuning and",
    "start": "1004440",
    "end": "1013399"
  },
  {
    "text": "training in this case uh if we construct HPC dedicated infrastructure",
    "start": "1013399",
    "end": "1020399"
  },
  {
    "text": "orchestrator uh we probably need to multiple work scheduleuler and",
    "start": "1020399",
    "end": "1026360"
  },
  {
    "text": "infrastructure or orchestrators something like Kubernetes or uh some else uh that",
    "start": "1026360",
    "end": "1034720"
  },
  {
    "text": "will bring us increasing maintaining costs and ML engineers should understand",
    "start": "1034720",
    "end": "1040959"
  },
  {
    "text": "multiple user interfaces so we sometimes consider to",
    "start": "1040959",
    "end": "1048319"
  },
  {
    "text": "want to construct construct entire ML cycle on top of Kubernetes instead of",
    "start": "1048319",
    "end": "1055919"
  },
  {
    "text": "different work or scheduleuler and uh infrastructure",
    "start": "1055919",
    "end": "1061080"
  },
  {
    "text": "orchestration we re leberaging Kubernetes allows us to automated uh",
    "start": "1061080",
    "end": "1066640"
  },
  {
    "text": "infrastructure management and comprehensive user interface powered by",
    "start": "1066640",
    "end": "1071960"
  },
  {
    "text": "Kubernetes uh something like uh mitigating management cost and",
    "start": "1071960",
    "end": "1077679"
  },
  {
    "text": "leveraging self-heering or and more uh however uh for this transition uh we",
    "start": "1077679",
    "end": "1084320"
  },
  {
    "text": "have some problem so in the next section uh let me describe what is problem and",
    "start": "1084320",
    "end": "1090880"
  },
  {
    "text": "how to solve it uh by uh cube for",
    "start": "1090880",
    "end": "1096400"
  },
  {
    "text": "trainer first problem uh is uh we face we face uh migrating to",
    "start": "1097240",
    "end": "1106280"
  },
  {
    "text": "Kubernetes uh one day DevOps engineer um migrated job execution platform to",
    "start": "1106280",
    "end": "1113200"
  },
  {
    "text": "Kubernetes uh from scrum or some else and um they told it to data scientist uh",
    "start": "1113200",
    "end": "1121280"
  },
  {
    "text": "as you can see here uh at the at at that time uh data scientists are worried how",
    "start": "1121280",
    "end": "1128400"
  },
  {
    "text": "can they perform their jobs by same ways as previously in this case uh deop",
    "start": "1128400",
    "end": "1135039"
  },
  {
    "text": "devops engineers tells that they need to change their training code so that it",
    "start": "1135039",
    "end": "1141200"
  },
  {
    "text": "can adapt for the crowd native environment however uh as Andre uh",
    "start": "1141200",
    "end": "1147400"
  },
  {
    "text": "described uh in the demo part uh data scientists just want to perform",
    "start": "1147400",
    "end": "1154240"
  },
  {
    "text": "training code uh they do not want to care any infrastructure specification",
    "start": "1154240",
    "end": "1159840"
  },
  {
    "text": "change in their training code",
    "start": "1159840",
    "end": "1164360"
  },
  {
    "text": "so why does this happen let's consider why such problem",
    "start": "1165039",
    "end": "1171320"
  },
  {
    "text": "happen uh we can consider uh three layers uh for ML training job submission",
    "start": "1171320",
    "end": "1177799"
  },
  {
    "text": "environments uh data scientists are responsible for training calls and how",
    "start": "1177799",
    "end": "1183120"
  },
  {
    "text": "to use ML frameworks however when infrastructure layer is",
    "start": "1183120",
    "end": "1188840"
  },
  {
    "text": "changed these rayers got affections because those rayers fully rely on",
    "start": "1188840",
    "end": "1195280"
  },
  {
    "text": "infrastructure ray and a couple of training code executing specifications",
    "start": "1195280",
    "end": "1201039"
  },
  {
    "text": "are depends on each job scheduler specifications so uh those uh gaps and",
    "start": "1201039",
    "end": "1209600"
  },
  {
    "text": "overhead for data scientist so when DevOps engineers uh",
    "start": "1209600",
    "end": "1216320"
  },
  {
    "text": "migrate their training environment to Kubernetes they should consider the uh",
    "start": "1216320",
    "end": "1222080"
  },
  {
    "text": "another user interface actually even if they can",
    "start": "1222080",
    "end": "1227760"
  },
  {
    "text": "mitigate those problems by providing comprehensive user interface or DevOps",
    "start": "1227760",
    "end": "1233919"
  },
  {
    "text": "engineer manually adapts data scientist training code to Kubernetes we will face",
    "start": "1233919",
    "end": "1240640"
  },
  {
    "text": "other problems so let's see another problem by concrete use user",
    "start": "1240640",
    "end": "1248679"
  },
  {
    "text": "story another problem is um uh we face after they succeeded to migrate to",
    "start": "1248679",
    "end": "1256760"
  },
  {
    "text": "Kubernetes one day data scientists consider to want to use another new",
    "start": "1256760",
    "end": "1262679"
  },
  {
    "text": "stateofthe-art library or uh mechanism for their training cause uh however uh",
    "start": "1262679",
    "end": "1270640"
  },
  {
    "text": "as we face uh in previous program the training code should be adapted to cubar",
    "start": "1270640",
    "end": "1276960"
  },
  {
    "text": "environments it's crowd native uh so when data scientists consider",
    "start": "1276960",
    "end": "1282760"
  },
  {
    "text": "uh sorry uh however as we face in previous uh problem uh uh",
    "start": "1282760",
    "end": "1290400"
  },
  {
    "text": "uh okay so the their training code should be adapt and so we so when data",
    "start": "1290400",
    "end": "1296480"
  },
  {
    "text": "scientist uh consider uh interris new algorithm uh or sort of",
    "start": "1296480",
    "end": "1303200"
  },
  {
    "text": "rivalries uh uh DevOps engineer should support uh the mechanism in",
    "start": "1303200",
    "end": "1309360"
  },
  {
    "text": "infrastructure layer in kubernous layer uh that's not easy uh to address every",
    "start": "1309360",
    "end": "1315440"
  },
  {
    "text": "time every library every frameworks and so uh this is second",
    "start": "1315440",
    "end": "1323320"
  },
  {
    "text": "problem yes let's consider why does this",
    "start": "1323320",
    "end": "1328559"
  },
  {
    "text": "happen so the key problem is a different cycle between uh providing",
    "start": "1329640",
    "end": "1335559"
  },
  {
    "text": "state-ofthe-art ML framework and infrastructure changes the sort of ML",
    "start": "1335559",
    "end": "1342559"
  },
  {
    "text": "framework rivalry are rapidly development but it's not easy to",
    "start": "1342559",
    "end": "1348720"
  },
  {
    "text": "construct new infrastructure radar so how to migrate those how to mitigate do",
    "start": "1348720",
    "end": "1356000"
  },
  {
    "text": "how to mitigate to problems let me describe in the next slide um the mitigate solution is uh",
    "start": "1356000",
    "end": "1365600"
  },
  {
    "text": "rearing MPI uh like open MPI uh interface uh the NPI can filling up gaps",
    "start": "1365600",
    "end": "1373280"
  },
  {
    "text": "uh between job schedulers these advantage can help to mitigate problem",
    "start": "1373280",
    "end": "1379440"
  },
  {
    "text": "one which means data scientists do not need to mostly uh care infrastructure",
    "start": "1379440",
    "end": "1386360"
  },
  {
    "text": "change additionally NPI run or NPI exec or some NPI tools can perform arbitrary",
    "start": "1386360",
    "end": "1394520"
  },
  {
    "text": "commands which allows us to easily infrastructure problem debuging because",
    "start": "1394520",
    "end": "1401360"
  },
  {
    "text": "when they want to debug uh where come from the air in their training code uh",
    "start": "1401360",
    "end": "1407760"
  },
  {
    "text": "they can perform infrastructure air error verification commands like Nvidia",
    "start": "1407760",
    "end": "1414720"
  },
  {
    "text": "NCCL test or some tools uh in same interface",
    "start": "1414720",
    "end": "1421000"
  },
  {
    "text": "MPI this allows us to decouple of training code errors and infrastructure",
    "start": "1421000",
    "end": "1426159"
  },
  {
    "text": "error error easily however uh as you know um in",
    "start": "1426159",
    "end": "1433760"
  },
  {
    "text": "distributed training frameworks uh we have another mechanism like torch the torch is native PyTorch commands for",
    "start": "1433760",
    "end": "1440960"
  },
  {
    "text": "distributed training uh that is different approach uh opposed to MPI and actually Q for trainer supports",
    "start": "1440960",
    "end": "1449600"
  },
  {
    "text": "both tools in B2 API mpi is basically rancher based",
    "start": "1449600",
    "end": "1455440"
  },
  {
    "text": "mechanism aka uh center oriented but toran like um to torture take advantage",
    "start": "1455440",
    "end": "1464240"
  },
  {
    "text": "uh rancher rest mechanism aka distributing oriented both tools are helpful to",
    "start": "1464240",
    "end": "1471840"
  },
  {
    "text": "perform executing uh PyTorch based distributed training on the other hand",
    "start": "1471840",
    "end": "1477919"
  },
  {
    "text": "they both have pros and cons something like debugability and for",
    "start": "1477919",
    "end": "1485278"
  },
  {
    "text": "trainers however uh in the typical Raj model training environment we often face",
    "start": "1485320",
    "end": "1491840"
  },
  {
    "text": "infrastructure errors related to computing system and device uh during",
    "start": "1491840",
    "end": "1497840"
  },
  {
    "text": "training execution so I think the debugability is better than for triants since if we cannot uh",
    "start": "1497840",
    "end": "1505760"
  },
  {
    "text": "resolve those infrastructure errors emer engineer uh uh never execute their",
    "start": "1505760",
    "end": "1511679"
  },
  {
    "text": "training code actually we can mitigate uh this uh lack of for tolerancy",
    "start": "1511679",
    "end": "1519360"
  },
  {
    "text": "uh for MPI by horot or some tools uh this is the reason why we consider",
    "start": "1519360",
    "end": "1525919"
  },
  {
    "text": "MPI is useful for model training and finetuning for ranch uh training",
    "start": "1525919",
    "end": "1533760"
  },
  {
    "text": "environment however uh when we construct NPI environment the DevOps engineer need",
    "start": "1534360",
    "end": "1540320"
  },
  {
    "text": "to manually set up NPI worker nodes and insurance part communication and more",
    "start": "1540320",
    "end": "1547840"
  },
  {
    "text": "this is not tribal efforts and it's hard to address those for all no all jobs in",
    "start": "1547840",
    "end": "1555360"
  },
  {
    "text": "this case how to construct such environment on top of",
    "start": "1555360",
    "end": "1561120"
  },
  {
    "text": "Kubernetes uh actually uh we can consider uh two approaches um one is a",
    "start": "1562039",
    "end": "1568240"
  },
  {
    "text": "cube control execution pattern second is uh SSH pattern uh cube control pattern",
    "start": "1568240",
    "end": "1575919"
  },
  {
    "text": "execute MPI initialization um throughout cube API server which",
    "start": "1575919",
    "end": "1582080"
  },
  {
    "text": "means easily set up due to unnecessary additional setup however this typically",
    "start": "1582080",
    "end": "1588640"
  },
  {
    "text": "happens cubernetes control plane performance issues since this tries to",
    "start": "1588640",
    "end": "1594039"
  },
  {
    "text": "occupy an entire control plane processing abilities opposed to cube control",
    "start": "1594039",
    "end": "1600880"
  },
  {
    "text": "pattern we can consider SSH solution which is a slightly hard to construct um",
    "start": "1600880",
    "end": "1609440"
  },
  {
    "text": "due to necessary unnecessary additional state setups related to SSH uh setup",
    "start": "1609440",
    "end": "1616600"
  },
  {
    "text": "initialization but uh this SSH solution is safer for cubar control",
    "start": "1616600",
    "end": "1623320"
  },
  {
    "text": "planes so this is why we use cube for trainer um in the cube for trainer is",
    "start": "1623320",
    "end": "1630000"
  },
  {
    "text": "responsible for uh this SSH based MPI environment setup which allows DevOps",
    "start": "1630000",
    "end": "1636640"
  },
  {
    "text": "engineer to mitigate complicated setups an alternative as uh as an alternative",
    "start": "1636640",
    "end": "1646640"
  },
  {
    "text": "uh introducing QR uh MPR runtime uh we can consider leveraging directly use",
    "start": "1646640",
    "end": "1653440"
  },
  {
    "text": "cubernetes job or job set but those do",
    "start": "1653440",
    "end": "1658480"
  },
  {
    "text": "not automatically set up NPI environments which means data scientists need to manually specify infrastructure",
    "start": "1658480",
    "end": "1665919"
  },
  {
    "text": "parameters for NPI it's obviously not good user experience",
    "start": "1665919",
    "end": "1672879"
  },
  {
    "text": "so as Andre mentioned in uh what is cube for trainer section uh cube for trainer",
    "start": "1674559",
    "end": "1680000"
  },
  {
    "text": "is ro oriented resource model and automatically set up npi environments",
    "start": "1680000",
    "end": "1686320"
  },
  {
    "text": "which indicates data scientist can focus only on training code and uh training",
    "start": "1686320",
    "end": "1693039"
  },
  {
    "text": "parameter related uh infrastructure parameter like number of node uh number",
    "start": "1693039",
    "end": "1698559"
  },
  {
    "text": "of processes uh",
    "start": "1698559",
    "end": "1701919"
  },
  {
    "text": "something uh this is actual range of YAML configuration as you can see uh they do not need to specify",
    "start": "1703799",
    "end": "1710640"
  },
  {
    "text": "infrastructure parameters they just specify training parameters additionally uh we have",
    "start": "1710640",
    "end": "1717600"
  },
  {
    "text": "Python SDK as Andre uh showed in demo uh this allows data scientists more",
    "start": "1717600",
    "end": "1723840"
  },
  {
    "text": "Pythonic style uh job submission way opposed to YAML manifest style",
    "start": "1723840",
    "end": "1730240"
  },
  {
    "text": "okay uh in conclusion uh uh we introduced uh cube for trainer MP",
    "start": "1730240",
    "end": "1737279"
  },
  {
    "text": "runtime uh this allows data scientists and infrastructure engineers uh easily",
    "start": "1737279",
    "end": "1744720"
  },
  {
    "text": "scale up MPI environments or easily set up NPI environments and data scientists",
    "start": "1744720",
    "end": "1750480"
  },
  {
    "text": "can focus on training codes",
    "start": "1750480",
    "end": "1755240"
  },
  {
    "text": "so this is future work uh we planning to implement uh uh some feature uh so if",
    "start": "1756640",
    "end": "1764240"
  },
  {
    "text": "you have any interested uh you can check um this QR",
    "start": "1764240",
    "end": "1770320"
  },
  {
    "text": "code uh finally uh we have uh QR autom",
    "start": "1770600",
    "end": "1775840"
  },
  {
    "text": "and training working group and cubress batch working group uh those working",
    "start": "1775840",
    "end": "1780880"
  },
  {
    "text": "group uh are working tightly uh uh typically uh so uh if you are",
    "start": "1780880",
    "end": "1788880"
  },
  {
    "text": "interested in those problem or new feature uh you can join SH channel or uh",
    "start": "1788880",
    "end": "1796080"
  },
  {
    "text": "committee meeting some okay uh thank you for uh our talk",
    "start": "1796080",
    "end": "1802799"
  },
  {
    "text": "uh thank you thanks everyone",
    "start": "1802799",
    "end": "1807278"
  },
  {
    "text": "uh We don't have a lot of time for questions we happy to answer questions afterwards uh right do we have time or",
    "start": "1808360",
    "end": "1814919"
  },
  {
    "text": "no we don't have time all right so feel free to reach out to us happy to chat a bit more thanks around for time",
    "start": "1814919",
    "end": "1822799"
  }
]