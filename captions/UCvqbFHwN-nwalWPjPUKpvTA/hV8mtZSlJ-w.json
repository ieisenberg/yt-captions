[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "text": "hello everybody welcome in this is my talk called doing things Prometheus can't do with Prometheus my name is Tim",
    "start": "269",
    "end": "9599"
  },
  {
    "text": "Simmons I'm an engineer on the observability team at digitalocean my team primarily runs centralized",
    "start": "9599",
    "end": "15059"
  },
  {
    "text": "logging metrics and tracing infrastructure you can follow me on Twitter but I wouldn't recommend it",
    "start": "15059",
    "end": "20490"
  },
  {
    "text": "because that is roughly the content that you will be there for digitalocean is a cloud company that is focused on",
    "start": "20490",
    "end": "26849"
  },
  {
    "text": "simplicity we have everything you need to run the workloads that you want to run my primary job at do is caring for a",
    "start": "26849",
    "end": "34410"
  },
  {
    "text": "prometheus fleet that cares about metrics that kind of support the entire cloud first line in my CFP kind of this",
    "start": "34410",
    "end": "44700"
  },
  {
    "start": "40000",
    "end": "84000"
  },
  {
    "text": "a little was clickbait to try and get my talk accepted and entice you to come to",
    "start": "44700",
    "end": "52050"
  },
  {
    "text": "my talk nothing is like this cut-and-dried you know there's a lot of",
    "start": "52050",
    "end": "57930"
  },
  {
    "text": "nuance and saying something like that and like there might be like a cool tweet to do or whatever but like I you",
    "start": "57930",
    "end": "64198"
  },
  {
    "text": "know this is not that's nothing I could be easily convinced and I'm totally wrong here but I do kind of believe this",
    "start": "64199",
    "end": "70530"
  },
  {
    "text": "if you're willing to invest the time and energy in kind of understanding the tools and the outcomes that you're",
    "start": "70530",
    "end": "75630"
  },
  {
    "text": "looking for the old pillars that nobody is so fond of anymore I think they're",
    "start": "75630",
    "end": "81689"
  },
  {
    "text": "actually really really good and good enough for you the thing that is not good enough is us and this shouldn't",
    "start": "81689",
    "end": "89400"
  },
  {
    "start": "84000",
    "end": "152000"
  },
  {
    "text": "really come as a surprise to anybody right the problem with technology is always the people using it you know",
    "start": "89400",
    "end": "96060"
  },
  {
    "text": "people will say things like I don't have time to do good observability or you know I don't have time to learn these",
    "start": "96060",
    "end": "101460"
  },
  {
    "text": "complicated tools but these are the tools that are giving you insight into how the thing that like pays your bills",
    "start": "101460",
    "end": "108180"
  },
  {
    "text": "are performing so if there's anything that is you know complicated that is actually worth learning and",
    "start": "108180",
    "end": "114000"
  },
  {
    "text": "understanding isn't it being able to do this like understanding your customers experience I think if you spend a lot of",
    "start": "114000",
    "end": "121740"
  },
  {
    "text": "time making sure your customers are happy you know your existing customers I think the new ones will kind of figure themselves out I think the hardest",
    "start": "121740",
    "end": "129509"
  },
  {
    "text": "problem in the observability space is actually just convincing your boss your leader that it is worth doing and",
    "start": "129509",
    "end": "135660"
  },
  {
    "text": "worth doing well and the way that you can reliably serve customers and do this",
    "start": "135660",
    "end": "141240"
  },
  {
    "text": "is by intelligently instrumenting your software so you can see what's going on but to produce those best outcomes you",
    "start": "141240",
    "end": "147330"
  },
  {
    "text": "have to absolutely understand what is possible so that you can make the best inputs the goal here for us today is for",
    "start": "147330",
    "end": "155880"
  },
  {
    "start": "152000",
    "end": "199000"
  },
  {
    "text": "you all to start thinking about observability like I do at least for 30 minutes or so because I'm right and",
    "start": "155880",
    "end": "161580"
  },
  {
    "text": "hopefully that is something that is generally useful as you kind of leave here and go any lunch or whatever but I",
    "start": "161580",
    "end": "168570"
  },
  {
    "text": "think it will also kind of help you understand where I'm coming from when I start explaining how we've done some more advanced things with Prometheus and",
    "start": "168570",
    "end": "175260"
  },
  {
    "text": "that's probably the real reason that you're here anyway but while I was writing the talk and talking about those things I kind of realized there's a",
    "start": "175260",
    "end": "181410"
  },
  {
    "text": "common thread and it was more about how I'm approaching those problems and how how my team is approaching these",
    "start": "181410",
    "end": "186630"
  },
  {
    "text": "problems and I think that is something that is a lot more teachable in this kind of short format then trying to dive",
    "start": "186630",
    "end": "192990"
  },
  {
    "text": "really deep technically on something and have you kind of keep that all in your brain or reference with slides later but",
    "start": "192990",
    "end": "198150"
  },
  {
    "text": "you can totally do that just in case someone in the audience doesn't know what prometheus is or what metrics are I",
    "start": "198150",
    "end": "203730"
  },
  {
    "start": "199000",
    "end": "258000"
  },
  {
    "text": "want to do just a brief explanation metrics are real-time numeric measurements and they usually are",
    "start": "203730",
    "end": "209100"
  },
  {
    "text": "answering questions like how many things happened or how much is a thing is happening right now or how long",
    "start": "209100",
    "end": "214590"
  },
  {
    "text": "something took in code we often increment or set metrics anytime",
    "start": "214590",
    "end": "219660"
  },
  {
    "text": "something happens so if you have a system that is doing things every time something happens you're gonna set some sort of metric for it applications that",
    "start": "219660",
    "end": "227310"
  },
  {
    "text": "are utilizing Prometheus serve a literal web page with metrics and plain text",
    "start": "227310",
    "end": "232560"
  },
  {
    "text": "that you can go to and look at and that is called exporting metrics and I'm gonna I'm gonna talk about that quite a",
    "start": "232560",
    "end": "238230"
  },
  {
    "text": "bit so this is really useful because you can actually see you know what Prometheus is then gonna do is go in and",
    "start": "238230",
    "end": "244470"
  },
  {
    "text": "gobble up these metrics and you can see what Prometheus is ingesting and so",
    "start": "244470",
    "end": "252200"
  },
  {
    "text": "Prometheus will reach out gobble up those metrics and put them in a time series database that you can then go and query later looking at metrics and logs",
    "start": "252200",
    "end": "260250"
  },
  {
    "start": "258000",
    "end": "332000"
  },
  {
    "text": "and traces there are a couple of themes that kind of instantly emerge to me there are a lot of promises floating",
    "start": "260250",
    "end": "265849"
  },
  {
    "text": "around about people giving you observability for free or you know being able to add it on to something at any",
    "start": "265849",
    "end": "271879"
  },
  {
    "text": "time and you know get the kind of outcomes that you want in my experience when you just kind of rubs observability",
    "start": "271879",
    "end": "277550"
  },
  {
    "text": "on something like that you're not really gonna get the best the best outcome",
    "start": "277550",
    "end": "282590"
  },
  {
    "text": "these patterns and these tools need to be kind of thoroughly understood and then utilized thoughtfully this needs to",
    "start": "282590",
    "end": "289370"
  },
  {
    "text": "be something that is central to your software architecture and design you should be designing from the beginning",
    "start": "289370",
    "end": "294530"
  },
  {
    "text": "interactions that you can observe and test and that way you can have clear signals about what is going on there and",
    "start": "294530",
    "end": "301370"
  },
  {
    "text": "your performance and that is the best thing you can do to have maintainable software and to do that I mean I'm gonna",
    "start": "301370",
    "end": "308330"
  },
  {
    "text": "start to sound like a broken record but you have to spend the time to understand the tool you know doing things like",
    "start": "308330",
    "end": "313909"
  },
  {
    "text": "learning what to log and what not to log more importantly or what things you should be creating metrics for and then",
    "start": "313909",
    "end": "320330"
  },
  {
    "text": "learning different query languages for each of those things finding the balance of what you should be tracing and what you should not and when to sample when",
    "start": "320330",
    "end": "326840"
  },
  {
    "text": "to aggregate how to visualize all that stuff this all takes time and labor and",
    "start": "326840",
    "end": "332680"
  },
  {
    "start": "332000",
    "end": "400000"
  },
  {
    "text": "because of that time that it takes to kind of learn and understand this and do it well people are starting to talk",
    "start": "332680",
    "end": "337909"
  },
  {
    "text": "about going beyond you know we're gonna correlate these signals we're gonna build simple unifying tools some sort of",
    "start": "337909",
    "end": "343940"
  },
  {
    "text": "overarching thing that is going to make this all easier but you can't really do",
    "start": "343940",
    "end": "350060"
  },
  {
    "text": "that because these things are all so different and also kind of nuanced and it's hard to do any of that stuff well",
    "start": "350060",
    "end": "356419"
  },
  {
    "text": "without that core understanding and I don't want anybody to sleep on you know those old kind of fundamental pillar",
    "start": "356419",
    "end": "362000"
  },
  {
    "text": "concepts and wait for the next big thing because it's going to be built on this same stuff how about making sense here",
    "start": "362000",
    "end": "368029"
  },
  {
    "text": "because what I'm trying to say is that none of this is easy you heard it in the keynote yesterday you know it's it's not",
    "start": "368029",
    "end": "373430"
  },
  {
    "text": "easy and we're trying to make it easier but even with that top quality stuff that you're gonna get for free you still",
    "start": "373430",
    "end": "378620"
  },
  {
    "text": "have to understand how to transform that into a good result I want us here today",
    "start": "378620",
    "end": "383990"
  },
  {
    "text": "in a headspace where we are treating these tools like the critical infrastructure that they are not as something that comes later because that",
    "start": "383990",
    "end": "390319"
  },
  {
    "text": "is the way the way that we effectively maintain software and keep customers happy and keep dollars flowing getting",
    "start": "390319",
    "end": "396500"
  },
  {
    "text": "really good at this is worth the investment and it's really fun so Prometheus the reason you probably",
    "start": "396500",
    "end": "402840"
  },
  {
    "start": "400000",
    "end": "462000"
  },
  {
    "text": "added this talk to your schedule is because I was gonna be talk about doing fancy stuff with Prometheus and I will",
    "start": "402840",
    "end": "408300"
  },
  {
    "text": "but you need to be ready for a little bit of a letdown because the more I thought about this the more I realized",
    "start": "408300",
    "end": "414480"
  },
  {
    "text": "there isn't special sauce these are kind of logical thought processes that result from being really familiar with this",
    "start": "414480",
    "end": "420810"
  },
  {
    "text": "tool and these are hard problems to solve and the tools can't be good at everything that's why there's so many",
    "start": "420810",
    "end": "426060"
  },
  {
    "text": "tools so you kind of need to get good or at least fluent in all of them but you",
    "start": "426060",
    "end": "431820"
  },
  {
    "text": "know even just one will get you kind of a really good you know at least get you most of the way there having said that",
    "start": "431820",
    "end": "438540"
  },
  {
    "text": "though Prometheus is actually just about as versatile a piece of software as I have ever used knowing the supposed limitations that",
    "start": "438540",
    "end": "445470"
  },
  {
    "text": "people throw out I am constantly surprised with the how well it stands up",
    "start": "445470",
    "end": "450690"
  },
  {
    "text": "to just the just utter and complete punishment that we routinely inflict upon it and that's why I'm here because",
    "start": "450690",
    "end": "457139"
  },
  {
    "text": "I think people are glossing over how great it that we could have had this already so Prometheus is intentionally",
    "start": "457139",
    "end": "464520"
  },
  {
    "start": "462000",
    "end": "530000"
  },
  {
    "text": "not a distributed system it is beautiful in its simplicity in that way it's very easy to kind of set up and explore and",
    "start": "464520",
    "end": "470700"
  },
  {
    "text": "start to understand what the value you're gonna get out of it is in the future as long as resource usage is kept",
    "start": "470700",
    "end": "476190"
  },
  {
    "text": "in check the network is up and the things that are exporting those metrics are up you're basically good to go",
    "start": "476190",
    "end": "481820"
  },
  {
    "text": "sometimes though the network does break or queries make Prometheus sad or exporters go down so people want to",
    "start": "481820",
    "end": "488070"
  },
  {
    "text": "start talking about doing high availability Prometheus this is the kind of thing that can happen so I have an instance here that was running perfectly",
    "start": "488070",
    "end": "494310"
  },
  {
    "text": "well for a while it's a short there but you know this thing is running for weeks at a pretty constant resource",
    "start": "494310",
    "end": "499590"
  },
  {
    "text": "utilization then all of a sudden it doubles triples in resource usage",
    "start": "499590",
    "end": "505639"
  },
  {
    "text": "because somebody queried it big bad but if you've got ever and this is fine",
    "start": "505639",
    "end": "511889"
  },
  {
    "text": "but if you run out of CPU or you run out of memory to processes query then you can slow an instance down to the point",
    "start": "511889",
    "end": "518010"
  },
  {
    "text": "where it becomes unresponsive this is kind of the same the only thing that I see kind of routinely you know",
    "start": "518010",
    "end": "524430"
  },
  {
    "text": "operationally and running Prometheus for people is that sometimes you know people make it sad",
    "start": "524430",
    "end": "530630"
  },
  {
    "start": "530000",
    "end": "560000"
  },
  {
    "text": "so what happens when you do that is that you might end up with a gap in metrics when you're querying later because you",
    "start": "530630",
    "end": "536370"
  },
  {
    "text": "will put that server in a situation where it can't go out and reach for metrics anymore it also means you know a",
    "start": "536370",
    "end": "543180"
  },
  {
    "text": "gap in a graph if it's a normal day is maybe not such a big deal but when it when it is not scraping metrics like",
    "start": "543180",
    "end": "549420"
  },
  {
    "text": "that it is also not evaluating alerts so if you do have something terrible happen in that space not only will you not be",
    "start": "549420",
    "end": "556620"
  },
  {
    "text": "able to see it you won't get alerted for it and that is actually pretty bad the way that folks generally work around",
    "start": "556620",
    "end": "562350"
  },
  {
    "start": "560000",
    "end": "709000"
  },
  {
    "text": "this is by deploying replicas of Prometheus that scrape the same things and evaluate the same alerts ideally",
    "start": "562350",
    "end": "567750"
  },
  {
    "text": "they do this in kind of separate geographical locations you know kind of air-gap yourself from a basic network",
    "start": "567750",
    "end": "573959"
  },
  {
    "text": "failure this means that you need a way to switch between querying multiple instances of Prometheus at one time if",
    "start": "573959",
    "end": "580410"
  },
  {
    "text": "one of them has an issue and a proxy is a really good answer here so putting that in front of your H a replicas and",
    "start": "580410",
    "end": "585509"
  },
  {
    "text": "then you know you can switch if one of them is having problems you can run health checks on the graph endpoint in",
    "start": "585509",
    "end": "592410"
  },
  {
    "text": "Prometheus which is just a regular page that you would use to go and dork around in the UI and my experience that is a",
    "start": "592410",
    "end": "598769"
  },
  {
    "text": "pretty good proxy for is the Prometheus or not as I'm reading this I now realize",
    "start": "598769",
    "end": "604500"
  },
  {
    "text": "there is a status API that you could probably also query so maybe do that too ideally you could manually failover",
    "start": "604500",
    "end": "610649"
  },
  {
    "text": "those instances in your group so because maybe there is a network issue between",
    "start": "610649",
    "end": "616079"
  },
  {
    "text": "Prometheus and the things that it is scraping and you know about that because you've got an alert from somewhere but Prometheus is actually like hey I'm",
    "start": "616079",
    "end": "621839"
  },
  {
    "text": "doing Gucci man like there's less there's less metrics coming in and I'm actually happier but then you query and",
    "start": "621839",
    "end": "627540"
  },
  {
    "text": "say hey we're all my metrics if you've got a network segregated Prometheus somewhere that is not subject to that",
    "start": "627540",
    "end": "633209"
  },
  {
    "text": "issue you could go and poke your proxy as they actually go look over here we need to see what's going on and then",
    "start": "633209",
    "end": "638310"
  },
  {
    "text": "ideally you would not rotate back to that other instance that has those holes until the other one has problems so yeah",
    "start": "638310",
    "end": "644579"
  },
  {
    "text": "you would still have a hole in your metrics but like if you rotate seven days later the only time you're gonna see that holes if you're actually",
    "start": "644579",
    "end": "650310"
  },
  {
    "text": "looking that far back in time you also get a few things for free with a proxy like query logging although Prometheus",
    "start": "650310",
    "end": "657149"
  },
  {
    "text": "does do that now but I've been using it for a very long time and it has been very very useful to see who's being naughty and then",
    "start": "657149",
    "end": "664230"
  },
  {
    "text": "potentially something like rate-limiting - so this is a really high availability",
    "start": "664230",
    "end": "669360"
  },
  {
    "text": "right because there are there are situations where this could become unusable but you're not really going to",
    "start": "669360",
    "end": "676350"
  },
  {
    "text": "be able to tolerate a massive network failure in a system that uses the network to go out and reach out for metrics so deal with it I guess but you",
    "start": "676350",
    "end": "685050"
  },
  {
    "text": "can't minimize those gaps right and hopefully you can be able to find them you'll find those metrics that you need",
    "start": "685050",
    "end": "690480"
  },
  {
    "text": "somewhere if it's really important and you know as bad as I made that sound",
    "start": "690480",
    "end": "695639"
  },
  {
    "text": "I've been running from atheists pretty large infrastructure for years and there's only been a couple of cases that",
    "start": "695639",
    "end": "701490"
  },
  {
    "text": "I can remember where I didn't actually have something that somebody needed due to either a network or you know a bunch",
    "start": "701490",
    "end": "707639"
  },
  {
    "text": "of instances going down there are options though if you are kind of wanting to go harder in that in that way",
    "start": "707639",
    "end": "714600"
  },
  {
    "text": "there are things like say a nose query or promix Y and these promise to fill those gaps in your metrics you know even",
    "start": "714600",
    "end": "723600"
  },
  {
    "text": "if one of them goes down and you can have just like one thing that you're querying the way they do this is actually by fanning out queries so if",
    "start": "723600",
    "end": "730440"
  },
  {
    "text": "you have four instances that are scraping the exact same thing two of them go down this thing will actually",
    "start": "730440",
    "end": "735899"
  },
  {
    "text": "query all four see that there's a gap in the first two and then fill it in you know by deduplicating data and giving",
    "start": "735899",
    "end": "742019"
  },
  {
    "text": "you one solid graph there are some risks here though so you will have decreased query performance at least in a little",
    "start": "742019",
    "end": "748199"
  },
  {
    "text": "bit because there is CPU time spent once that proxy goes fans out and then kind of coalesce as those things together you",
    "start": "748199",
    "end": "755279"
  },
  {
    "text": "also can pay double the cost for latency of your query so say you do a big fat",
    "start": "755279",
    "end": "760620"
  },
  {
    "text": "query that returns like 40 Meg's of results maybe your network is slow maybe",
    "start": "760620",
    "end": "766470"
  },
  {
    "text": "it's not whatever that that result will go from Prometheus to this proxy do some",
    "start": "766470",
    "end": "771540"
  },
  {
    "text": "work correspondingly larger amount of work with the larger your data set is and then send it back along to you and",
    "start": "771540",
    "end": "777360"
  },
  {
    "text": "if you're running queries that are like 15 18 20 seconds you will kind of feel this a little bit more the other risk is",
    "start": "777360",
    "end": "785190"
  },
  {
    "text": "that if someone does a really big query or loads up a big fat killer dashboard over like eight weeks or something",
    "start": "785190",
    "end": "791680"
  },
  {
    "text": "these queries will fan out to all of your Prometheus and DDoS all of them and then you are creating that gap that you",
    "start": "791680",
    "end": "798310"
  },
  {
    "text": "are trying to avoid in the first place because none of your replicas are up there's also some increased operational",
    "start": "798310",
    "end": "803499"
  },
  {
    "text": "overhead you know this is another component to run and because Prometheus is generally so easy to operate this",
    "start": "803499",
    "end": "808660"
  },
  {
    "text": "thing can actually be as hard to operate as Prometheus itself but there are other benefits too so you can get something",
    "start": "808660",
    "end": "813670"
  },
  {
    "text": "like a global query view so you're not changing you know data sources in Griffin or trying to figure out which",
    "start": "813670",
    "end": "818860"
  },
  {
    "text": "Prometheus has the metrics that you're looking for and that is pretty cool let's talk about cardinality for a",
    "start": "818860",
    "end": "824829"
  },
  {
    "start": "822000",
    "end": "908000"
  },
  {
    "text": "second this is a quick refresher because this is not something that is intuitive for people when they start using Prometheus but it's probably the most",
    "start": "824829",
    "end": "831459"
  },
  {
    "text": "important thing for you to know every unique permutation of label values in Prometheus creates a new time series so",
    "start": "831459",
    "end": "838870"
  },
  {
    "text": "you can see in that example below I have three different time series because there are three different kind of combinations of potential metric label",
    "start": "838870",
    "end": "845709"
  },
  {
    "text": "values the official recommendation from Prometheus is that any individual query",
    "start": "845709",
    "end": "850959"
  },
  {
    "text": "should match hundreds and not thousands of time series so you see I've done a query that kind of matches three here",
    "start": "850959",
    "end": "857259"
  },
  {
    "text": "but you can maybe think about how if I was doing a sum across you know HTTP",
    "start": "857259",
    "end": "862449"
  },
  {
    "text": "requests across my entire application how this could easily you know find hundreds or thousands of time series you",
    "start": "862449",
    "end": "868589"
  },
  {
    "text": "can work out a query at a single point in time to figure out kind of how dangerous it's going to be so you can",
    "start": "868589",
    "end": "874420"
  },
  {
    "text": "see like how many times series are gonna be returned how long did your query take and you should do that before you go and graph something because the way to the",
    "start": "874420",
    "end": "880180"
  },
  {
    "text": "graph works is it evaluates your individual query at many many many points in time and then connects them",
    "start": "880180",
    "end": "886209"
  },
  {
    "text": "all together so if your grade takes 10 seconds when you evaluate it for the last 5 minutes it's not gonna work to you great if you're doing it over a week",
    "start": "886209",
    "end": "891809"
  },
  {
    "text": "so basically what you should do when you're creating metrics is to try and avoid labels that have unbounded",
    "start": "891809",
    "end": "896860"
  },
  {
    "text": "potential values and then watch the number of total labels that you put on a",
    "start": "896860",
    "end": "901990"
  },
  {
    "text": "metric because you've got a big metric already and then you add another label that just has ten values you just 10x the cardinality of that metric so if",
    "start": "901990",
    "end": "909279"
  },
  {
    "text": "you're following what I basically just told you to do is not create metrics when you're creating metrics in my",
    "start": "909279",
    "end": "914350"
  },
  {
    "text": "experience running Prometheus for people generally any query that is worth doing touches thousands of time series some of",
    "start": "914350",
    "end": "921670"
  },
  {
    "text": "them even hundreds of thousands so we've got to find a way to work with this luckily if you're querying under about",
    "start": "921670",
    "end": "927070"
  },
  {
    "text": "mm time series you're totally fine and then if you even go up to like 10,000 or so I'm not even worried about you going",
    "start": "927070",
    "end": "934390"
  },
  {
    "text": "beyond that though we might have to do some work if you're going to do those higher cardinality queries there are",
    "start": "934390",
    "end": "940060"
  },
  {
    "start": "937000",
    "end": "1125000"
  },
  {
    "text": "some things you can do operationally to Prometheus to help limit the worst of the pain so you need to have significant",
    "start": "940060",
    "end": "945610"
  },
  {
    "text": "amount of memory and CPU head room while prometheus is running a queried instance is a completely different animal like I",
    "start": "945610",
    "end": "951430"
  },
  {
    "text": "showed you earlier you can set a limit on how many samples I'm sorry I should",
    "start": "951430",
    "end": "957670"
  },
  {
    "text": "give you a guideline on head room I usually do 50% it still dies sometimes so you know it's just money right so you",
    "start": "957670",
    "end": "965200"
  },
  {
    "text": "know just keep on you're getting up or just run it in kubernetes and let it eat your entire cluster whatever you can set",
    "start": "965200",
    "end": "970990"
  },
  {
    "text": "a limit on how many samples a single query can load into memory so samples means individual values inside of the",
    "start": "970990",
    "end": "977470"
  },
  {
    "text": "individual time series so if you're evaluating a query that touches a thousand time series over two weeks it's",
    "start": "977470",
    "end": "982510"
  },
  {
    "text": "gonna have to evaluate that query potentially thousands of times so then you were you know multiplying that number of time series by a big number I",
    "start": "982510",
    "end": "989100"
  },
  {
    "text": "usually set this to like 50 million if somebody's doing something that is more than 50 million like I don't want them",
    "start": "989100",
    "end": "995950"
  },
  {
    "text": "to do it and prometheus generally will like kind of calculate the number of samples that it's gonna load it takes",
    "start": "995950",
    "end": "1002040"
  },
  {
    "text": "like five seconds or so and then go hold on Jack pretty sure and I usually say no",
    "start": "1002040",
    "end": "1008510"
  },
  {
    "text": "but I have been known onto occasion on occasion to bump up this limit for somebody who wants to do more you can",
    "start": "1008510",
    "end": "1016770"
  },
  {
    "text": "also set the limit on maximum concurrent queries that are being done in",
    "start": "1016770",
    "end": "1021930"
  },
  {
    "text": "Prometheus you should absolutely do this I usually set it to one per CPU core on the instance the reason for this is if",
    "start": "1021930",
    "end": "1029130"
  },
  {
    "text": "you are doing like 20 queries at a single point in time on a box that has like eight cores it is going to have to",
    "start": "1029130",
    "end": "1035579"
  },
  {
    "text": "do like a lot of context switching and it's not gonna go well it's better to wait the you know quarter of a second or",
    "start": "1035580",
    "end": "1042240"
  },
  {
    "text": "whatever for another query to finish and then execute your new one rather than just kind of doing them all at once what",
    "start": "1042240",
    "end": "1047790"
  },
  {
    "text": "usually happens when you do something like this without this limit is you're loading a big dashboard with a hundred queries on it and they will just all go",
    "start": "1047790",
    "end": "1054300"
  },
  {
    "text": "back I'll come back failing and then if you you know this limit then they can all just",
    "start": "1054300",
    "end": "1059500"
  },
  {
    "text": "roughly come in you know maybe two seconds later you should also shard",
    "start": "1059500",
    "end": "1064720"
  },
  {
    "text": "metrics or federate so at do we are a cloud with lots of regions and there are",
    "start": "1064720",
    "end": "1071680"
  },
  {
    "text": "times where people want to answer questions about the entire cloud unfortunately they're the you know it's",
    "start": "1071680",
    "end": "1077980"
  },
  {
    "text": "a big cloud so they're answering that question out of Prometheus would require a query across you know a giant global",
    "start": "1077980",
    "end": "1084520"
  },
  {
    "text": "set of data so I shard this stuff regionally this means that people are a",
    "start": "1084520",
    "end": "1089620"
  },
  {
    "text": "little bit sad at first because they're like well now I have to do my query for every region but you still get your",
    "start": "1089620",
    "end": "1095559"
  },
  {
    "text": "answer and that's better than no answer at all and it's honestly it's not that hard to stitch it all together like if",
    "start": "1095559",
    "end": "1100690"
  },
  {
    "text": "you really really care isn't it worth doing a little bit of work I don't know that's how I feel about it but you can",
    "start": "1100690",
    "end": "1106660"
  },
  {
    "text": "also federates oh if you you know still you could do a regional sharding and then create kind of a layered cake of",
    "start": "1106660",
    "end": "1113020"
  },
  {
    "text": "Prometheus where you are aggregating something at a regional level and then only putting that stuff in the next tier up so then you could do that global",
    "start": "1113020",
    "end": "1119770"
  },
  {
    "text": "query again just a little bit more you know kind of labor that you have to do but hey answering your question when you",
    "start": "1119770",
    "end": "1126730"
  },
  {
    "start": "1125000",
    "end": "1196000"
  },
  {
    "text": "start going really hard with queries you need to be kind of careful be careful with your aggregation so scope down to",
    "start": "1126730",
    "end": "1132250"
  },
  {
    "text": "only the things that you actually care about you know increase your step size of your query so that you have at first",
    "start": "1132250",
    "end": "1138070"
  },
  {
    "text": "the lowest resolution that you can stand and then you know the same thing with a smaller kind of time window and then",
    "start": "1138070",
    "end": "1143290"
  },
  {
    "text": "scale up so if you're gonna do some sort of big query over a week you know maybe start with an hour you know five minute",
    "start": "1143290",
    "end": "1149679"
  },
  {
    "text": "resolution and then kind of work your way up instead of just like punching it right in the mouth with the bigquery",
    "start": "1149679",
    "end": "1155790"
  },
  {
    "text": "there's another thing that you can do is you should be able to understand kind of",
    "start": "1155790",
    "end": "1161110"
  },
  {
    "text": "your query pattern and the questions that you're going to be trying to answer with these metrics and then create the",
    "start": "1161110",
    "end": "1166150"
  },
  {
    "text": "metrics and query to match so this is a lot of stuff to care about but again",
    "start": "1166150",
    "end": "1171640"
  },
  {
    "text": "you're doing something that's really hard so you have to care a little bit it's like you're supporting customer I don't know like what is more important",
    "start": "1171640",
    "end": "1178059"
  },
  {
    "text": "so doing these kind of things I routinely help folks query metrics that have hundreds of thousands of",
    "start": "1178059",
    "end": "1183520"
  },
  {
    "text": "time-series and if you listen to you know vendors and things this is something that you absolutely cannot do",
    "start": "1183520",
    "end": "1189010"
  },
  {
    "text": "it for me Thea's and I'm here to tell you they're lying it's just a little bit harder than it it's just a little bit",
    "start": "1189010",
    "end": "1194840"
  },
  {
    "text": "hard but it's fun I like to give you an example of crafting metrics that match",
    "start": "1194840",
    "end": "1200000"
  },
  {
    "start": "1196000",
    "end": "1266000"
  },
  {
    "text": "desired outcomes because this is something that I don't think people are doing enough so you've got some sort of web service that has potentially",
    "start": "1200000",
    "end": "1206390"
  },
  {
    "text": "thousands of endpoints that you might care about latency for you want to know per endpoint latency you want to know",
    "start": "1206390",
    "end": "1212990"
  },
  {
    "text": "latency per API version and then you want to know global latency just one single number you should create a",
    "start": "1212990",
    "end": "1219650"
  },
  {
    "text": "separate metric for each one of those questions so you will have one metric that has a label value for every",
    "start": "1219650",
    "end": "1226159"
  },
  {
    "text": "endpoint in your app and then if you tried to do a global you know latency calculation on that everything would",
    "start": "1226159",
    "end": "1232070"
  },
  {
    "text": "fall apart that's fine because now you have a separate metric for global latency that is tiny tiny tiny and it",
    "start": "1232070",
    "end": "1238399"
  },
  {
    "text": "comes back instantly and that's really good and then another one for you know some sort of group that you care about",
    "start": "1238399",
    "end": "1243700"
  },
  {
    "text": "this does create more metrics up front and it does increase your regular resource usage a little bit but you're",
    "start": "1243700",
    "end": "1249289"
  },
  {
    "text": "not going to blow anything out at query time and then you can actually answer all the questions that you want so you have to maintain a bit of cognitive load",
    "start": "1249289",
    "end": "1255110"
  },
  {
    "text": "you got to know what to query for what but I think it's a fair trade-off to being able to you know actually answer",
    "start": "1255110",
    "end": "1260480"
  },
  {
    "text": "your questions like I said spend a little time understand your tools because they're really important",
    "start": "1260480",
    "end": "1265990"
  },
  {
    "text": "long-term storage is something that people routinely ask for in prometheus",
    "start": "1265990",
    "end": "1272120"
  },
  {
    "start": "1266000",
    "end": "1343000"
  },
  {
    "text": "Prometheus pretty recently released disk backed retention so basically if you provision a big old disk on your",
    "start": "1272120",
    "end": "1277490"
  },
  {
    "text": "prometheus instance you can tell it to just fill up the entire disk with metrics and you know start deleting them",
    "start": "1277490",
    "end": "1282980"
  },
  {
    "text": "when it runs out of disk space you can do this with a block storage volume as well if you keep the maximum block size",
    "start": "1282980",
    "end": "1288679"
  },
  {
    "text": "that is the size that Prometheus size and time where Prometheus will compact your metrics down to if you keep that",
    "start": "1288679",
    "end": "1295039"
  },
  {
    "text": "relatively low like three to seven days when you run out of disk space and Prometheus needs to delete metrics it",
    "start": "1295039",
    "end": "1300289"
  },
  {
    "text": "will delete by block so if you you know can let this compact down like four weeks or something and then you run out",
    "start": "1300289",
    "end": "1305840"
  },
  {
    "text": "of disk space you're not deleting four weeks of disk space or metrics and then freeing up like half of your disk you",
    "start": "1305840",
    "end": "1312950"
  },
  {
    "text": "can kill little little tiny bits at a time this has a little bit more overhead",
    "start": "1312950",
    "end": "1317960"
  },
  {
    "text": "when you start querying but in my experience it's not so bad I turned this on in our infrastructure about",
    "start": "1317960",
    "end": "1323570"
  },
  {
    "text": "250 days ago so my experience doing really long-term queries is somewhat limited but doing kind of some of the",
    "start": "1323570",
    "end": "1329000"
  },
  {
    "text": "stuff I talked about before I've had like perfect easy success doing kind of",
    "start": "1329000",
    "end": "1334100"
  },
  {
    "text": "long-term queries and 250 days or a year or 2 years is a lot more than people need even just like four to eight weeks",
    "start": "1334100",
    "end": "1340610"
  },
  {
    "text": "is often enough for for every question there are new solutions in this space",
    "start": "1340610",
    "end": "1345800"
  },
  {
    "start": "1343000",
    "end": "1399000"
  },
  {
    "text": "though things like fan oohs m3 and cortex they offer long-term source as well as a lot",
    "start": "1345800",
    "end": "1351350"
  },
  {
    "text": "of other potential benefits so that like global query view I was talking about earlier multi-tenancy down sampling",
    "start": "1351350",
    "end": "1356720"
  },
  {
    "text": "compaction you know pushing metrics natively scaling horizontally these are",
    "start": "1356720",
    "end": "1361910"
  },
  {
    "text": "solving really big problems compared to just vanilla Prometheus and they come with a correspondingly large amount of",
    "start": "1361910",
    "end": "1367400"
  },
  {
    "text": "operational overhead so the cost to run any of these things is much much more than Prometheus and it needs to be",
    "start": "1367400",
    "end": "1373010"
  },
  {
    "text": "staffed appropriately if you're gonna do something like this my opinion is if you're looking at something like this just for long-term storage you should",
    "start": "1373010",
    "end": "1380270"
  },
  {
    "text": "think about whether you really need to keep that data long-term in the first place in my experience it's a thing people",
    "start": "1380270",
    "end": "1385850"
  },
  {
    "text": "think they want but almost never use and the problem is that especially if you're considering a big solution like one of",
    "start": "1385850",
    "end": "1391400"
  },
  {
    "text": "these big boys the expense and operations and money is a lot higher for",
    "start": "1391400",
    "end": "1397160"
  },
  {
    "text": "the relative utility that you're actually getting my recommendation for long-term storage is to throw that big",
    "start": "1397160",
    "end": "1403250"
  },
  {
    "start": "1399000",
    "end": "1449000"
  },
  {
    "text": "fat disc Prometheus and just let it go hog-wild if that doesn't work you know",
    "start": "1403250",
    "end": "1408290"
  },
  {
    "text": "you could think about setting up a long-term prometheus instance and federating only that kind of aggregated",
    "start": "1408290",
    "end": "1413840"
  },
  {
    "text": "long-term trending data that you really want to keep and if you do that and keep that data relatively small you know even",
    "start": "1413840",
    "end": "1420770"
  },
  {
    "text": "just like four four gigs of RAM in a you know 100 gig disk you're gonna keep that stuff for years but I say it again you",
    "start": "1420770",
    "end": "1428120"
  },
  {
    "text": "have to do that labor you have to think about the solution that you want and you know kind of care about your outcomes and then you've got a campaign for the",
    "start": "1428120",
    "end": "1434210"
  },
  {
    "text": "time to do that and ironically it might even be easier to convince some leaders that you should stand up a large you",
    "start": "1434210",
    "end": "1440120"
  },
  {
    "text": "know new cool system because it has kind of that cloud native clout but if you don't need all of that",
    "start": "1440120",
    "end": "1446300"
  },
  {
    "text": "you really shouldn't because it's gonna be very expensive once people start getting value out of metrics a common",
    "start": "1446300",
    "end": "1452600"
  },
  {
    "start": "1449000",
    "end": "1479000"
  },
  {
    "text": "idea that folks have is wanting to do machine learning or anomaly detection on and I'm sure this is possible I've heard",
    "start": "1452600",
    "end": "1458190"
  },
  {
    "text": "a lot of amazing stories about what machine learning can do I've personally never seen on even an outcome good or",
    "start": "1458190",
    "end": "1464400"
  },
  {
    "text": "bad this seems harder than it looks and it looks pretty freaking hard to me I'd",
    "start": "1464400",
    "end": "1469860"
  },
  {
    "text": "like to give you a couple of tips though so that you can do some basic kind of anomaly screening predictive analysis in Prometheus and then you can tell your",
    "start": "1469860",
    "end": "1475980"
  },
  {
    "text": "leaders you Turing machine learning and get yourself a big raise so this is a good example from the Prometheus blog",
    "start": "1475980",
    "end": "1482220"
  },
  {
    "start": "1479000",
    "end": "1525000"
  },
  {
    "text": "awhile ago so you've got a handful of instances of like a web application that are serving",
    "start": "1482220",
    "end": "1488160"
  },
  {
    "text": "you know kind of similar requests you can examine the group of latency metrics to find outlier instances that might be",
    "start": "1488160",
    "end": "1494430"
  },
  {
    "text": "running kind of slowly you can do this by checking out request durations average that are sticking out by two",
    "start": "1494430",
    "end": "1500460"
  },
  {
    "text": "standard deviations and then additionally you could mitigate any false positives if your data is really",
    "start": "1500460",
    "end": "1505620"
  },
  {
    "text": "closely clustered together by checking that the value must also be 20% higher",
    "start": "1505620",
    "end": "1511860"
  },
  {
    "text": "than average so this is an absolute unit of a query but you can break it down",
    "start": "1511860",
    "end": "1517530"
  },
  {
    "text": "into its component pieces fairly easily to kind of understand it you know spend an hour understand the tool you get it",
    "start": "1517530",
    "end": "1525410"
  },
  {
    "start": "1525000",
    "end": "1553000"
  },
  {
    "text": "another one I like to do is trying to find spikes and odd events so if you got",
    "start": "1525410",
    "end": "1530820"
  },
  {
    "text": "something that is normally very very consistent and you can examine some sort of recent amount of time like 5 minutes",
    "start": "1530820",
    "end": "1536640"
  },
  {
    "text": "or 10 minutes compared to the last 24 hours or so you can see if you've got some sort of change this is a great kind",
    "start": "1536640",
    "end": "1542580"
  },
  {
    "text": "of first alert if you were you know looking you're kind of starting on your journey toward you know doing this kind of thing and you just want to know when",
    "start": "1542580",
    "end": "1548790"
  },
  {
    "text": "something happened this is a good thing to kind of know what's going on there the only other thing that is better than",
    "start": "1548790",
    "end": "1555810"
  },
  {
    "start": "1553000",
    "end": "1592000"
  },
  {
    "text": "catching a problem right away though is catching it before it even happens the predict linear function in Prometheus",
    "start": "1555810",
    "end": "1561330"
  },
  {
    "text": "uses or it predicts a future value based on a backward-looking linear regression so the canonical example is alerting",
    "start": "1561330",
    "end": "1568980"
  },
  {
    "text": "you're gonna run out of disk space before you do imagine how useful that would be on like a database for example",
    "start": "1568980",
    "end": "1574550"
  },
  {
    "text": "this is great that whenever you have some sort of capacity or saturation metric you know it's a little bit",
    "start": "1574550",
    "end": "1580830"
  },
  {
    "text": "similar to the last example because usually something has started happening that has changed your future in a way that is that is undesirable but it's",
    "start": "1580830",
    "end": "1587520"
  },
  {
    "text": "good for longer-term stuff like a disk filling up release or like a TLS certificate expiring as well you can write an application that",
    "start": "1587520",
    "end": "1595680"
  },
  {
    "text": "exposes metrics for just about anything so in fact most of the ones that you would want to write probably already",
    "start": "1595680",
    "end": "1601560"
  },
  {
    "text": "exist out there a digitalocean we've written exporters to quantify capacity and our fleet in",
    "start": "1601560",
    "end": "1606870"
  },
  {
    "text": "various ways look at our events pipeline and look at individual customer or",
    "start": "1606870",
    "end": "1612450"
  },
  {
    "text": "customer resources and these are all things that are incredibly custom right only only they would be useful to us a",
    "start": "1612450",
    "end": "1618030"
  },
  {
    "text": "lot of times an exporter is a really good replacement for some sort of older monitoring tool that goes and like",
    "start": "1618030",
    "end": "1623670"
  },
  {
    "text": "tickles a bespoke bash script or command or something like that because you can standardize all that stuff into one",
    "start": "1623670",
    "end": "1629580"
  },
  {
    "text": "Prometheus place and you know you get a few things for free and having all of your stuff in one place and able to kind",
    "start": "1629580",
    "end": "1635550"
  },
  {
    "text": "of query and make dashboards off of and do alerts on is really really great there's one real gotcha that I've seen",
    "start": "1635550",
    "end": "1642060"
  },
  {
    "start": "1639000",
    "end": "1767000"
  },
  {
    "text": "by people a lot when writing exporters for the first time that I want to try and warn you about this isn't the go",
    "start": "1642060",
    "end": "1647610"
  },
  {
    "text": "library and it happens to you when you are kind of writing you're forced exporter and like reading a",
    "start": "1647610",
    "end": "1652740"
  },
  {
    "text": "documentation and doing it kind of the naive way basically what happens is you will create a new time series with some",
    "start": "1652740",
    "end": "1659040"
  },
  {
    "text": "combination of labels and you will you you will choose a label value that that represents something ephemeral so say",
    "start": "1659040",
    "end": "1665880"
  },
  {
    "text": "you're exporting metrics about a queue name that is only there for five minutes or so what will happen using this kind",
    "start": "1665880",
    "end": "1673590"
  },
  {
    "text": "of naive way that is very very easy to use them in the client is that metric",
    "start": "1673590",
    "end": "1678810"
  },
  {
    "text": "will then be present in the metrics endpoint that your application exposes as long as the application is up so",
    "start": "1678810",
    "end": "1685620"
  },
  {
    "text": "until it gets restarted so then you'll be clearing your stuff in Prometheus later or you know get an alert for",
    "start": "1685620",
    "end": "1690690"
  },
  {
    "text": "something you know some cue name or whatever you're like wow oh that shouldn't be like that you know that shouldn't be there and then we go one",
    "start": "1690690",
    "end": "1696810"
  },
  {
    "text": "look and it won't be there like why are my metrics wrong the workaround is to use a pattern called const metrics in",
    "start": "1696810",
    "end": "1703950"
  },
  {
    "text": "the go library this pattern essentially gathers the metrics at scrape time so",
    "start": "1703950",
    "end": "1709200"
  },
  {
    "text": "instead of kind of just you know bopping a little function or something inside your your code you can kind of either",
    "start": "1709200",
    "end": "1716990"
  },
  {
    "text": "just leave it to entirely a thing that happens in scrape time if it's really fast or you can keep the kind of",
    "start": "1716990",
    "end": "1722940"
  },
  {
    "text": "information that you want to be exporting metrics about in some sort of other abstraction in memory in your code and then build",
    "start": "1722940",
    "end": "1728700"
  },
  {
    "text": "them at the time that the scrape happens you should always be doing this when the",
    "start": "1728700",
    "end": "1734760"
  },
  {
    "text": "source of truth for the metrics that you're exporting is not your code itself so if you're doing something like you",
    "start": "1734760",
    "end": "1740910"
  },
  {
    "text": "know doing a database query and then kind of generating metrics from that or poking around in some sort of stats socket or writing an exporter for some",
    "start": "1740910",
    "end": "1746910"
  },
  {
    "text": "sort of large system that you're kind of you know just exporting some kind of health stuff about this is what you",
    "start": "1746910",
    "end": "1752040"
  },
  {
    "text": "should do I don't expect you to remember that what I would hope is that you kind of remember this moment and then maybe",
    "start": "1752040",
    "end": "1757770"
  },
  {
    "text": "when you're doing this someday you can like tweet at me or go read you know you'll remember constant ryx and you can",
    "start": "1757770",
    "end": "1763590"
  },
  {
    "text": "go like control of that and the docs and kind of see what I'm talking about Prometheus an alert manager can I go",
    "start": "1763590",
    "end": "1769890"
  },
  {
    "start": "1767000",
    "end": "1860000"
  },
  {
    "text": "together like PB&J so alert manager groups many many alerts from Prometheus and sends one single notification I like",
    "start": "1769890",
    "end": "1776130"
  },
  {
    "text": "creating team or service specific alerting levels so that like all of my team's debug alerts go to one slack",
    "start": "1776130",
    "end": "1782520"
  },
  {
    "text": "Channel and then all my team's critical alerts will go to page or duty and a slack channel something like that the",
    "start": "1782520",
    "end": "1788700"
  },
  {
    "text": "configuration for alert manager is very very powerful but it's a bit of a bear too right especially when you're doing something like global for an entire kind",
    "start": "1788700",
    "end": "1794970"
  },
  {
    "text": "of organization you see up on the right and that that tweet is actually my alert manager routing tree and you wouldn't",
    "start": "1794970",
    "end": "1802380"
  },
  {
    "text": "even know it's a tree because there are so many lines on it those are all the places that the alerts can possibly go",
    "start": "1802380",
    "end": "1807890"
  },
  {
    "text": "there's a lot of good literature out there about what you should alert on I'd",
    "start": "1807890",
    "end": "1815070"
  },
  {
    "text": "actually I don't know if I said this when you are creating kind of that alert manager config make an abstraction and",
    "start": "1815070",
    "end": "1821180"
  },
  {
    "text": "generate that config so like I don't I don't really have to care if my alert manager routing tree looks like a broken",
    "start": "1821180",
    "end": "1827970"
  },
  {
    "text": "record because I generate that config and it's fine an alert manager can totally handle it except in the UI it",
    "start": "1827970",
    "end": "1834510"
  },
  {
    "text": "does crash Chrome but that's fine there's some good literature out there about what should be alerted on I'm not",
    "start": "1834510",
    "end": "1840780"
  },
  {
    "text": "gonna like dive deep here because that could be a whole talk but alerts should prompt action and then have all the context attached to the alert necessary",
    "start": "1840780",
    "end": "1847380"
  },
  {
    "text": "to perform that action so write a playbook attached to your alert also if you're doing slack alerts use the tool",
    "start": "1847380",
    "end": "1853710"
  },
  {
    "text": "that Ullaeus wrote to create beautiful slack you just don't get like viscerally angry",
    "start": "1853710",
    "end": "1859019"
  },
  {
    "text": "whenever you see them the pinnacle of mindful observability is implementing s ellos so you should read the chapter in",
    "start": "1859019",
    "end": "1865409"
  },
  {
    "start": "1860000",
    "end": "1896000"
  },
  {
    "text": "the SRA book on it's really like the entire SRU book but essentially the idea is that you should be defining the type",
    "start": "1865409",
    "end": "1870720"
  },
  {
    "text": "of performance experience that your users can expect measure it and then take action to maintain that level of service this is the most important thing",
    "start": "1870720",
    "end": "1877740"
  },
  {
    "text": "to measure right so talking about being outcomes oriented this is the outcome for your customers this is the thing that you should be measuring but",
    "start": "1877740",
    "end": "1883710"
  },
  {
    "text": "quantifying it is actually perfectly really hard but it's an essential exercise and the first few times you do",
    "start": "1883710",
    "end": "1889529"
  },
  {
    "text": "it it might actually require some work you might even need to change the architecture of your application to be able to measure that thing in the right",
    "start": "1889529",
    "end": "1895200"
  },
  {
    "text": "place but once you've measured it you can then set an objective so this is often like some number of nines and then",
    "start": "1895200",
    "end": "1901980"
  },
  {
    "start": "1896000",
    "end": "1938000"
  },
  {
    "text": "you can give yourself an error budget so that would be like the amount of time that you're allowed to file your SLO you can use that then as a simple you know",
    "start": "1901980",
    "end": "1908759"
  },
  {
    "text": "this one simple thing as a as a factor in any decision-making that you might have to do so if you have a service that's consistently violating its SLO",
    "start": "1908759",
    "end": "1915389"
  },
  {
    "text": "and as an organization you decided the DES allows are important you can then make the leap that you should not be",
    "start": "1915389",
    "end": "1920909"
  },
  {
    "text": "doing any work that is not innovative fixing that problem this is a great way to push back on decision makers that",
    "start": "1920909",
    "end": "1926879"
  },
  {
    "text": "might be trying to push new work on you or new features or something because it's really hard to arguing it's not",
    "start": "1926879",
    "end": "1932039"
  },
  {
    "text": "achieving your SLO Prometheus can help you set this kind of system up and I want to show you briefly how to do that",
    "start": "1932039",
    "end": "1937940"
  },
  {
    "text": "so this is the 99th percentile query latency in micro me Thea stands for infrastructure over 24 hours measured",
    "start": "1937940",
    "end": "1944970"
  },
  {
    "start": "1938000",
    "end": "1972000"
  },
  {
    "text": "hourly say I wanted to define availability to be that 99% of my",
    "start": "1944970",
    "end": "1949980"
  },
  {
    "text": "requests complete in under a second I can express that in a Prometheus query as a binary 1 or 0 did this happen or",
    "start": "1949980",
    "end": "1956669"
  },
  {
    "text": "did this not happen for a time period using this less than boolean operator in Prometheus so you can see on the",
    "start": "1956669",
    "end": "1962220"
  },
  {
    "text": "previous graph there was one kind of hour where my latency my 99 percentile latency was over a second so now in this",
    "start": "1962220",
    "end": "1968970"
  },
  {
    "text": "graph it's a zero so I failed for that hour then you can use a Prometheus sub",
    "start": "1968970",
    "end": "1974369"
  },
  {
    "start": "1972000",
    "end": "1992000"
  },
  {
    "text": "query which are awesome to find that exact same data on that graph which was generated by you know evaluating one",
    "start": "1974369",
    "end": "1981480"
  },
  {
    "text": "query at 24 points in time and drawing all the lines together you can get all that data in one query so that you can",
    "start": "1981480",
    "end": "1987720"
  },
  {
    "text": "then use it in another later you see all those ones and zeroes there we can then take the average over",
    "start": "1987720",
    "end": "1993840"
  },
  {
    "start": "1992000",
    "end": "2085000"
  },
  {
    "text": "time of that data set and because it's expressed in those ones or zeros we can calculate a percentage of time that we met at RS although so if you're doing a",
    "start": "1993840",
    "end": "2000260"
  },
  {
    "text": "time based SLO this is a very easy way to do that so you can see that we actually had an availability of 95.8%",
    "start": "2000260",
    "end": "2005600"
  },
  {
    "text": "over those 24 hours so if we wanted 99% availability than we'd be in violation for simplicity here I evaluated the",
    "start": "2005600",
    "end": "2012680"
  },
  {
    "text": "queries every hour in real life you'd probably do it every few minutes or if any of you were doing like seven days you might go hourly or something like",
    "start": "2012680",
    "end": "2019100"
  },
  {
    "text": "that and intervals actually do matter because when I dropped it from evaluating - every hour - every minute my 95 percent",
    "start": "2019100",
    "end": "2025430"
  },
  {
    "text": "pass rate dropped to 71 so a little trick out there for you people trying to hack your SLO but like don't do that",
    "start": "2025430",
    "end": "2031370"
  },
  {
    "text": "actually because you're only hurting yourself a couple of more prom QL tricks for you in this example I'm not using a",
    "start": "2031370",
    "end": "2037640"
  },
  {
    "text": "histogram quantile to estimate my 99th percentile latency I set my SLO at one second and I am actually counting the",
    "start": "2037640",
    "end": "2044360"
  },
  {
    "text": "the requests that were less than or equal to a second and then my total number of requests so I can get a more exact number of requests that actually",
    "start": "2044360",
    "end": "2051860"
  },
  {
    "text": "took less than a second because I'm using that histogram bucket another problem that you might run into here is",
    "start": "2051860",
    "end": "2056990"
  },
  {
    "text": "that if you don't have any requests for five minutes you know you would be dividing by zero here which is not",
    "start": "2056990",
    "end": "2063020"
  },
  {
    "text": "allowed in Prometheus and it would break your entire query so using this little trick with or you can sanitize that time",
    "start": "2063020",
    "end": "2069110"
  },
  {
    "text": "where you had no requests as a one in your kind of binary zero to one by using any metric that is always extant and",
    "start": "2069110",
    "end": "2076490"
  },
  {
    "text": "then we can use the same pattern as we did in the previous example so we can calculate the percentage of time over the last week where latency was less",
    "start": "2076490",
    "end": "2082700"
  },
  {
    "text": "than one second and feed that into an SLO all right everybody we did it remember doing observability well",
    "start": "2082700",
    "end": "2089840"
  },
  {
    "start": "2085000",
    "end": "2136000"
  },
  {
    "text": "requires time and effort but your reliability is maybe the only thing that",
    "start": "2089840",
    "end": "2095000"
  },
  {
    "text": "is important enough to justify that kind of investment you need to do it anyway if you're gonna want the best if you",
    "start": "2095000",
    "end": "2100370"
  },
  {
    "text": "want the best outcomes because low effort inputs are not enough for your customers you should pick some tools and",
    "start": "2100370",
    "end": "2106190"
  },
  {
    "text": "get really good at using them I recommend from atheists because it's just excellent software and that's it if",
    "start": "2106190",
    "end": "2112760"
  },
  {
    "text": "you have questions you can come up and ask me afterwards over here or outside or something but I want to let you go to",
    "start": "2112760",
    "end": "2118040"
  },
  {
    "text": "lunch and I cannot take the risk of any of you embarrassing me in front of this entire so thanks for coming I appreciate your",
    "start": "2118040",
    "end": "2124190"
  },
  {
    "text": "time [Applause]",
    "start": "2124190",
    "end": "2129880"
  },
  {
    "text": "you [Applause]",
    "start": "2131880",
    "end": "2136209"
  }
]