[
  {
    "start": "0",
    "end": "115000"
  },
  {
    "text": "this is an impressive crowd I have to commend everybody for coming out people are still wandering in from the",
    "start": "0",
    "end": "6120"
  },
  {
    "text": "hallways which alright last day of the show in the afternoon everybody's still",
    "start": "6120",
    "end": "11429"
  },
  {
    "text": "caffeinated well at least this group is",
    "start": "11429",
    "end": "19580"
  },
  {
    "text": "see that guy need another cup cuz he's a little late I know I haven't started yet",
    "start": "20090",
    "end": "27720"
  },
  {
    "text": "because I'm still seeing more people coming down the hallway but I'll go ahead and get kinda started so I want to",
    "start": "27720",
    "end": "35130"
  },
  {
    "text": "say thank you to everybody coming out it's as the person standing here it's impressive to see the number of people",
    "start": "35130",
    "end": "41340"
  },
  {
    "text": "that want to learn the things that we've learned over the last year doing the work that we have to convert our massive",
    "start": "41340",
    "end": "49940"
  },
  {
    "text": "omnibus monolith into a cloud native chart and the changes that were actually required to make that migration now I",
    "start": "49940",
    "end": "58940"
  },
  {
    "text": "mentioned pitchforks here and I'll explain this pun has anybody ever went we have a great application it works",
    "start": "58940",
    "end": "65909"
  },
  {
    "text": "great we have hundreds of thousands of users now we got to do the new tech and then go do we have to do that from",
    "start": "65909",
    "end": "74490"
  },
  {
    "text": "scratch pitchfork okay we're gonna do this but now we're gonna have a higher",
    "start": "74490",
    "end": "79950"
  },
  {
    "text": "latency pitchfork okay we're gonna do this but now it's harder to maintain ten",
    "start": "79950",
    "end": "85500"
  },
  {
    "text": "bits for cos this is what I'm talking about we had to do this we had to do it",
    "start": "85500",
    "end": "93540"
  },
  {
    "text": "live and we had to do it without hiccups the users even knew that was hard",
    "start": "93540",
    "end": "102020"
  },
  {
    "text": "because at the same time as we're doing this our sass which ones the exact same",
    "start": "102020",
    "end": "108270"
  },
  {
    "text": "codebase had to be able to run at load which has not been the easiest thing in",
    "start": "108270",
    "end": "113640"
  },
  {
    "text": "the world but now that we're kind of tailed in let me cover Who I am my name",
    "start": "113640",
    "end": "121439"
  },
  {
    "start": "115000",
    "end": "206000"
  },
  {
    "text": "is Jason I'm a senior distribution engineer I have spent the last year",
    "start": "121439",
    "end": "127320"
  },
  {
    "text": "doing our cloud native helm charts and breaking out this gigantic pile of code",
    "start": "127320",
    "end": "132480"
  },
  {
    "text": "that may very complex very coordinated application sweet work I have a history",
    "start": "132480",
    "end": "139069"
  },
  {
    "text": "working with containers and commits into docker has anybody here ever heard of",
    "start": "139069",
    "end": "145670"
  },
  {
    "text": "the bridge IP flags no none of my networking guys here yeah I did that",
    "start": "145670",
    "end": "151190"
  },
  {
    "text": "because I couldn't deal with the Cisco VPN anymore but you know what",
    "start": "151190",
    "end": "156610"
  },
  {
    "text": "you're welcome flannel but yeah so last",
    "start": "156610",
    "end": "165170"
  },
  {
    "text": "September at the beginning my boss comes to me and goes okay so we've got this and we've got the existing omnibus in a",
    "start": "165170",
    "end": "173000"
  },
  {
    "text": "caperna t's chart but now we need to do it the right way I need you to start looking into it and then we fly to the",
    "start": "173000",
    "end": "179870"
  },
  {
    "text": "company summit and I spend the next five days in meetings discussing what we're",
    "start": "179870",
    "end": "185030"
  },
  {
    "text": "gonna have to do and everything after this is where this comes from so here's",
    "start": "185030",
    "end": "195200"
  },
  {
    "text": "the short summary I'm not gonna read that because I'm about to tell you it but if there's something in there that",
    "start": "195200",
    "end": "202069"
  },
  {
    "text": "screams that you please pay attention to the details so the real question people",
    "start": "202069",
    "end": "210650"
  },
  {
    "start": "206000",
    "end": "317000"
  },
  {
    "text": "don't realize is what is get loud now",
    "start": "210650",
    "end": "216190"
  },
  {
    "text": "I'm gonna explain to you from a couple of viewpoints the first of which being that right this is the marketing pitch",
    "start": "216190",
    "end": "224480"
  },
  {
    "text": "and I'll just put this up you can find this on our page but gitlab is the first single application to actually have the",
    "start": "224480",
    "end": "231590"
  },
  {
    "text": "entire DevOps lifecycle in a single interface we do concurrent DevOps",
    "start": "231590",
    "end": "238250"
  },
  {
    "text": "unlocking the organization's from all the constraints of the tool chains providing unmatched visibility higher",
    "start": "238250",
    "end": "245209"
  },
  {
    "text": "levels of efficiency of efficiency and comprehensive governance while we're at",
    "start": "245209",
    "end": "250430"
  },
  {
    "text": "it while we can make your lifecycle 200%",
    "start": "250430",
    "end": "255650"
  },
  {
    "text": "faster doing that is highly complex so",
    "start": "255650",
    "end": "260900"
  },
  {
    "text": "let's look at it from two viewpoints and I'll explain which one I'm gonna do you have the end user this is what you",
    "start": "260900",
    "end": "268280"
  },
  {
    "text": "see this is where everything you need is the end users view the interface itself",
    "start": "268280",
    "end": "274120"
  },
  {
    "text": "which we pride ourselves on making sure that you can do everything in one interface without having to jump around",
    "start": "274120",
    "end": "279860"
  },
  {
    "text": "without having to integrate all the things we can if you want but but then",
    "start": "279860",
    "end": "286250"
  },
  {
    "text": "again let's look at it another way in the crowd right now I have a bunch of",
    "start": "286250",
    "end": "291290"
  },
  {
    "text": "developers I have a bunch of engineers and I have a whole bunch of people that blend that together so let's look at how",
    "start": "291290",
    "end": "300290"
  },
  {
    "text": "the actual application works and what we had to do to make this a reality because",
    "start": "300290",
    "end": "307040"
  },
  {
    "text": "in the end I'm going to take all the parts and make it easy for you to put",
    "start": "307040",
    "end": "312320"
  },
  {
    "text": "them together in the stack for you to run it in the beginning it was simple it",
    "start": "312320",
    "end": "321520"
  },
  {
    "start": "317000",
    "end": "513000"
  },
  {
    "text": "was straightforward and it was Ruby on Rails on a single codebase and you",
    "start": "321520",
    "end": "327229"
  },
  {
    "text": "deployed it from source one gigantic code that's not the case these days now",
    "start": "327229",
    "end": "335000"
  },
  {
    "text": "we are still Ruby on Rails for the primary application but we've got sidekick now on the side we've got",
    "start": "335000",
    "end": "340820"
  },
  {
    "text": "giddily that I'll cover in a little bit that is a complete go service we actually have a shim proxy in front",
    "start": "340820",
    "end": "347360"
  },
  {
    "text": "called workhorse that's go binary that takes the heavy lifting away from Ruby so that we can make sure that our api's",
    "start": "347360",
    "end": "352580"
  },
  {
    "text": "are responsive the original focus was on",
    "start": "352580",
    "end": "357590"
  },
  {
    "text": "code hosting issues NMR's but on the previous slides I just showed you that we do way more than that these",
    "start": "357590",
    "end": "365000"
  },
  {
    "text": "days we did this and then we started",
    "start": "365000",
    "end": "370160"
  },
  {
    "text": "packaging this because doing everything from source is hard so we created the omnibus package and that slowly became",
    "start": "370160",
    "end": "378320"
  },
  {
    "text": "the gigantic monolith but it was",
    "start": "378320",
    "end": "384229"
  },
  {
    "text": "sensical because when we started we had a clear focus we had a group of people working a",
    "start": "384229",
    "end": "391099"
  },
  {
    "text": "single product to binding that and then we took that we bundled that and we shipped it and we shipped it and we",
    "start": "391099",
    "end": "397310"
  },
  {
    "text": "shipped it and we shipped it and all the twenties every month for the entire lifespan of this company we have done that that's",
    "start": "397310",
    "end": "404570"
  },
  {
    "text": "not been easy being a monolith made that something that was simple to do at scale",
    "start": "404570",
    "end": "412870"
  },
  {
    "text": "unfortunately there are some downsides that I'll cover in a minute cuz that's",
    "start": "412870",
    "end": "421190"
  },
  {
    "text": "the rest of that's all the bonus to having one large monolith what is it everything you need is everything you",
    "start": "421190",
    "end": "428150"
  },
  {
    "text": "see right the problem is while all of",
    "start": "428150",
    "end": "434960"
  },
  {
    "text": "these are advantages when you hit the current ecosystem and you look at what we have to do in the future being able",
    "start": "434960",
    "end": "443510"
  },
  {
    "text": "to control individual components is something we should be able to do but do we need to lock them all together in one",
    "start": "443510",
    "end": "449090"
  },
  {
    "text": "gigantic pile of blocks the good news is I can tell you this package will work on",
    "start": "449090",
    "end": "454910"
  },
  {
    "text": "any platform on any cloud under any distribution the bad news is how many",
    "start": "454910",
    "end": "460910"
  },
  {
    "text": "people want to manage fleets of VMs not really our thing anymore the thing grew",
    "start": "460910",
    "end": "470510"
  },
  {
    "text": "to the point that it was 1.5 gigs unpacked ok it's still that big it's",
    "start": "470510",
    "end": "477920"
  },
  {
    "text": "still usable still has all the features and all the configuration you download 500 Meg's as an installation package and",
    "start": "477920",
    "end": "485780"
  },
  {
    "text": "it unpacks to over a gig and a half and that's before you start putting data in because we have all of these things",
    "start": "485780",
    "end": "491230"
  },
  {
    "text": "because that package contains literally everything it requires to run our SAS",
    "start": "491230",
    "end": "498190"
  },
  {
    "text": "every component every configuration and every way you can put it together it's",
    "start": "498190",
    "end": "503570"
  },
  {
    "text": "good because we're able to use it we use it our customers use it the exact same way the problem is this thing is massive",
    "start": "503570",
    "end": "513789"
  },
  {
    "text": "so let me tell you a little bit of a story when it comes to growing pains",
    "start": "513790",
    "end": "519340"
  },
  {
    "text": "scaling and pushing the absolute boundaries of what you can do with a basic simple system now",
    "start": "519340",
    "end": "529170"
  },
  {
    "text": "one of the key components of what we've done over the years is get the trick is",
    "start": "529170",
    "end": "535980"
  },
  {
    "text": "get itself is the reason that moving to cloud native was hard but let me show",
    "start": "535980",
    "end": "544000"
  },
  {
    "text": "you why through a series of examples not from a consumer perspective but from the",
    "start": "544000",
    "end": "549130"
  },
  {
    "text": "inner workings of how it behaves itself so everybody know what it is and how it",
    "start": "549130",
    "end": "557200"
  },
  {
    "text": "actually works no do we all know how it works for us right you run a couple of",
    "start": "557200",
    "end": "563290"
  },
  {
    "text": "commands you push up to us everybody's happy UCI kicks off deploy app did you",
    "start": "563290",
    "end": "568630"
  },
  {
    "text": "do right but at the core of that command is how everything is handled how",
    "start": "568630",
    "end": "574930"
  },
  {
    "text": "everything is put together it works with snapshots of the entire file the sheer",
    "start": "574930",
    "end": "582250"
  },
  {
    "text": "number of files involved is every file you have and every version you've ever had and all the indexes and references",
    "start": "582250",
    "end": "587589"
  },
  {
    "text": "and everything else yes there are some optimizations that can be done but the",
    "start": "587589",
    "end": "592690"
  },
  {
    "text": "sheer thing is the more you have the harder it is because now you have to",
    "start": "592690",
    "end": "598420"
  },
  {
    "text": "read them all right to files easier 100",
    "start": "598420",
    "end": "604209"
  },
  {
    "text": "files a little harder a thousand files for 10 years oh boy",
    "start": "604209",
    "end": "610110"
  },
  {
    "text": "so let me give you an example has anybody ever checked out the Linux tree",
    "start": "610110",
    "end": "615839"
  },
  {
    "text": "yeah I got a couple of hands anybody else go to get a coffee while they do that right you check out that tree get",
    "start": "615839",
    "end": "626320"
  },
  {
    "text": "your coffee come back check out any branch I don't care what it is and then dip that",
    "start": "626320",
    "end": "631750"
  },
  {
    "text": "against current master how many files just got read on the file system right",
    "start": "631750",
    "end": "639810"
  },
  {
    "text": "all the files that are marked as different between the two of them and when you do diff that dip in for",
    "start": "639810",
    "end": "646630"
  },
  {
    "text": "information is not stored it's not greeting it and cutting it out it's running diff on all of those files",
    "start": "646630",
    "end": "653640"
  },
  {
    "text": "imagine how bad that gets when you have 10 million lines of code in a repository",
    "start": "654330",
    "end": "660220"
  },
  {
    "text": "that's 15 years old that's expensive in terms of performance now let's actually",
    "start": "660220",
    "end": "671140"
  },
  {
    "text": "go and make a branch make some changes stage and commit them right now you push",
    "start": "671140",
    "end": "678370"
  },
  {
    "text": "them up to your fork and now you go into add if you on an M R now it's my job to",
    "start": "678370",
    "end": "686110"
  },
  {
    "text": "do the thing that was already hard on your laptop right okay cool that's one",
    "start": "686110",
    "end": "695800"
  },
  {
    "text": "of you how about 10,000 people a second right",
    "start": "695800",
    "end": "705130"
  },
  {
    "start": "698000",
    "end": "928000"
  },
  {
    "text": "do you see where this is going suddenly it's harder but why is this the problem",
    "start": "705130",
    "end": "711150"
  },
  {
    "text": "traditional methods these types of requirements are the absolute heart of",
    "start": "711150",
    "end": "717610"
  },
  {
    "text": "the monoliths that we have but then we take that and multiply that not by 10,000 by a hundred thousand because all",
    "start": "717610",
    "end": "726160"
  },
  {
    "text": "of a sudden somebody started importing hundreds of thousands of repositories a day you all know when that was",
    "start": "726160",
    "end": "737250"
  },
  {
    "text": "so now we're getting to the really hard parts first off let's look at the traditional",
    "start": "737340",
    "end": "744930"
  },
  {
    "text": "methods we'll make more of them and we'll spread it out we'll sharp the disc figure out more of them okay that's not",
    "start": "744930",
    "end": "752490"
  },
  {
    "text": "working so we'll put them on SSDs cool okay now we're gonna raid 16 of these",
    "start": "752490",
    "end": "757650"
  },
  {
    "text": "things together because they're still not fast enough now we have the problem",
    "start": "757650",
    "end": "762839"
  },
  {
    "text": "of we have hundreds of things in the fleet accessing tens of machines that are massive with every disk we can throw",
    "start": "762839",
    "end": "770640"
  },
  {
    "text": "at it and it's still not keeping up because the traditional methods are a problem NFS is great when you've got 10",
    "start": "770640",
    "end": "785130"
  },
  {
    "text": "people or 100 people hey sis it means if I told you you had to manage an NFS server for 5,000 people or 50 for the",
    "start": "785130",
    "end": "792000"
  },
  {
    "text": "rest of your life which one do you pick exactly you pick the pitchfork and go",
    "start": "792000",
    "end": "799830"
  },
  {
    "text": "nope the beauty of NFS is it is capable it is",
    "start": "799830",
    "end": "805080"
  },
  {
    "text": "proven but it's not meant for this scale why because of the way it works we now",
    "start": "805080",
    "end": "812280"
  },
  {
    "text": "have a mount that has to be on every single node because our API code and our",
    "start": "812280",
    "end": "817380"
  },
  {
    "text": "web code and several other processes in the background have to be able to read these files because we're using Garrett",
    "start": "817380",
    "end": "824610"
  },
  {
    "text": "we're using Lib get we're using a bunch of other things it has to read them on the file system so now we've taken all",
    "start": "824610",
    "end": "831420"
  },
  {
    "text": "of this load for all of the disks we've shoved it out to all of the nodes and every time this one reads it it's got to",
    "start": "831420",
    "end": "838140"
  },
  {
    "text": "pull the whole file pull this whole file do a diff and pray to God it's not the linus tree it solved the problem but it",
    "start": "838140",
    "end": "848520"
  },
  {
    "text": "created another one now we're not as worried about how much this we can get",
    "start": "848520",
    "end": "854760"
  },
  {
    "text": "to or that all of them can get to it but now we've got disk i/o problems because",
    "start": "854760",
    "end": "861930"
  },
  {
    "text": "we've got everybody trying to read this disparate set of files now we've got all",
    "start": "861930",
    "end": "867780"
  },
  {
    "text": "of that traffic going across the at work and there's only so much you can fit through the tube and then of course",
    "start": "867780",
    "end": "874179"
  },
  {
    "text": "the whole problem with NFS system ends and if s falls over what happens so does",
    "start": "874179",
    "end": "880899"
  },
  {
    "text": "everything else okay so we have definitively found a",
    "start": "880899",
    "end": "887649"
  },
  {
    "text": "scaling limit now we can only push the traditional methods of up and out so far",
    "start": "887649",
    "end": "894149"
  },
  {
    "text": "before we realize that that's just not going to work because we don't have big",
    "start": "894149",
    "end": "899199"
  },
  {
    "text": "enough pipes end of line so now we've got all of this and we've just got more",
    "start": "899199",
    "end": "906819"
  },
  {
    "text": "of them and more of them and more of them and all of a sudden we need to add 15 nodes to the fleet and another 15",
    "start": "906819",
    "end": "913209"
  },
  {
    "text": "nodes to the fleet and another 15 nodes to the fleet to keep up with sudden user demand with every single time we have to",
    "start": "913209",
    "end": "920769"
  },
  {
    "text": "double something the choke points do not grow - they get tighter and tighter so",
    "start": "920769",
    "end": "929040"
  },
  {
    "start": "928000",
    "end": "1042000"
  },
  {
    "text": "we decided to take a second look at the problem and we began work on a project",
    "start": "929040",
    "end": "935709"
  },
  {
    "text": "that we call giddily and what we did was we took the API calls that you would",
    "start": "935709",
    "end": "941110"
  },
  {
    "text": "make to live get so the action will get mechanic's we're sending them over a G",
    "start": "941110",
    "end": "946629"
  },
  {
    "text": "RPC and then putting giddily on the actual file servers what we then do is",
    "start": "946629",
    "end": "954279"
  },
  {
    "text": "ask for a diff on whatever we want then we ask Italy for the response we no",
    "start": "954279",
    "end": "962619"
  },
  {
    "text": "longer need NFS now I can send a 1k packet get a 4k response instead of NFS",
    "start": "962619",
    "end": "969999"
  },
  {
    "text": "and reading 10,000 files we centralize everything across and this gives us the",
    "start": "969999",
    "end": "977410"
  },
  {
    "text": "ability to actually meet throughput because that pipe that's not getting any bigger suddenly has 1/10 of the traffic",
    "start": "977410",
    "end": "984459"
  },
  {
    "text": "going through it now we have more room just to be able to even get to the file servers plus we don't have NFS mounts on",
    "start": "984459",
    "end": "992919"
  },
  {
    "text": "everything so if one node goes out we don't lose half the fleet in an instant but we're also optimizing for the very",
    "start": "992919",
    "end": "1000629"
  },
  {
    "text": "specific problem of how do I get to get in the most performance way the answer is treat it",
    "start": "1000629",
    "end": "1009550"
  },
  {
    "text": "like a service or like a database we're still using the file system but now all",
    "start": "1009550",
    "end": "1015250"
  },
  {
    "text": "of the accesses to the files are on the node where we have the best performance we have the best caching and we don't",
    "start": "1015250",
    "end": "1022899"
  },
  {
    "text": "have to worry about the network at all okay so now we've managed to take the",
    "start": "1022899",
    "end": "1028480"
  },
  {
    "text": "monolith rip a chunk out make it something else and literally prop the thing up but how long are we going to be",
    "start": "1028480",
    "end": "1035079"
  },
  {
    "text": "able to do this scaling is still a problem even with this in play because we have all these other components so we",
    "start": "1035079",
    "end": "1044410"
  },
  {
    "start": "1042000",
    "end": "1195000"
  },
  {
    "text": "start there this provides us our first sign forward now we can look for our",
    "start": "1044410",
    "end": "1052570"
  },
  {
    "text": "other choke points traditional files are still there if you upload something file",
    "start": "1052570",
    "end": "1058900"
  },
  {
    "text": "system now chain NFS hasn't gone away completely yet there's a problem do we",
    "start": "1058900",
    "end": "1064809"
  },
  {
    "text": "really need to have an offense because somebody uploaded a cat picture come on",
    "start": "1064809",
    "end": "1071890"
  },
  {
    "text": "guys we can do better than that right so our next solution was to take everything",
    "start": "1071890",
    "end": "1078309"
  },
  {
    "text": "we've got as a traditional file that does not get and move it into object stores as an option why does this matter",
    "start": "1078309",
    "end": "1085410"
  },
  {
    "text": "because now I don't have to have the file system locally now I can take all",
    "start": "1085410",
    "end": "1090429"
  },
  {
    "text": "of these files hand it to a service that works well and we already have proven it and has the ability to run on Prem in a",
    "start": "1090429",
    "end": "1097330"
  },
  {
    "text": "cloud and any number of men and service providers now everybody knows this",
    "start": "1097330",
    "end": "1105250"
  },
  {
    "text": "writes pets cattle you should have all heard this term by now popularized by",
    "start": "1105250",
    "end": "1110260"
  },
  {
    "text": "CERN but in a nutshell anything that you can replace easily is cattle anything",
    "start": "1110260",
    "end": "1118780"
  },
  {
    "text": "that you have to care and feed for on a regular basis is a pet why does that",
    "start": "1118780",
    "end": "1124450"
  },
  {
    "text": "matter because that pet is your stateful",
    "start": "1124450",
    "end": "1129520"
  },
  {
    "text": "information our database for example now let me explain the problem with",
    "start": "1129520",
    "end": "1136909"
  },
  {
    "text": "configuring the omnibus at scale if I have hundreds of the MS I'm now",
    "start": "1136909",
    "end": "1142580"
  },
  {
    "text": "installing of the M and as the sage and an ansible client and installing the",
    "start": "1142580",
    "end": "1149750"
  },
  {
    "text": "package so now I have 20 gigs per VM I've got a gig that's gonna be put onto",
    "start": "1149750",
    "end": "1157309"
  },
  {
    "text": "it plus whatever is required as dependencies off of that and every single one of these knows has to download the latest package there's",
    "start": "1157309",
    "end": "1162679"
  },
  {
    "text": "waiting for 500 Meg's to arrive and then I can configure all the individual",
    "start": "1162679",
    "end": "1167720"
  },
  {
    "text": "components out of the omnibus which is great now I can kind of spread the load but it's still this big thing because",
    "start": "1167720",
    "end": "1174020"
  },
  {
    "text": "every single one of these no matter what will take at least two minutes to come up from the point I have the package",
    "start": "1174020",
    "end": "1180830"
  },
  {
    "text": "install two minutes to bring a note up that's not bad right unless you needed",
    "start": "1180830",
    "end": "1189080"
  },
  {
    "text": "those 15 nodes yesterday now not so much",
    "start": "1189080",
    "end": "1195760"
  },
  {
    "start": "1195000",
    "end": "1363000"
  },
  {
    "text": "so let's take that same thing break that massive stack and start ripping chunks",
    "start": "1195760",
    "end": "1201799"
  },
  {
    "text": "out and container rising them into smaller things that we can treat as individualized services and now that",
    "start": "1201799",
    "end": "1208580"
  },
  {
    "text": "we've started to pull some of that state out of the way they're no longer bound to the NFS on disk every component is",
    "start": "1208580",
    "end": "1216440"
  },
  {
    "text": "now separated and because it can be individually configured with only the thing it needs and I don't have to go",
    "start": "1216440",
    "end": "1222500"
  },
  {
    "text": "turn off and generate files for umpteen other things instead of being two minutes it can be as little as five",
    "start": "1222500",
    "end": "1229340"
  },
  {
    "text": "seconds if I need to bring up more web workers because all of a sudden our",
    "start": "1229340",
    "end": "1235460"
  },
  {
    "text": "api's are getting literally slammed because somebody decided to start forking Unreal Engine like crazy I can",
    "start": "1235460",
    "end": "1243200"
  },
  {
    "text": "do that now in less than a minute and I've got 30 of them there's a big difference there this is the promise",
    "start": "1243200",
    "end": "1251150"
  },
  {
    "text": "that we all want to say right so here's",
    "start": "1251150",
    "end": "1256460"
  },
  {
    "text": "a problem legacy debt who's ever heard that term",
    "start": "1256460",
    "end": "1262560"
  },
  {
    "text": "who has ever lived with it who's ever wished they weren't the cause of it yeah",
    "start": "1262560",
    "end": "1269870"
  },
  {
    "text": "been there that shared file system expectation it's a bugger because when",
    "start": "1269870",
    "end": "1279750"
  },
  {
    "text": "it was on one file system when we did distribute things as one big glob it was okay for the go binary to save it to",
    "start": "1279750",
    "end": "1288330"
  },
  {
    "text": "disk and then make an API call to the Ruby binary which then read it off of",
    "start": "1288330",
    "end": "1293850"
  },
  {
    "text": "disk and now I don't have to send it through IPC right but if they're in",
    "start": "1293850",
    "end": "1300090"
  },
  {
    "text": "separate containers and there's no shared disk and I can't do a shared disk",
    "start": "1300090",
    "end": "1308460"
  },
  {
    "text": "because if we do shared disk through rewrite many what's the major provider that will do that for us on every",
    "start": "1308460",
    "end": "1313920"
  },
  {
    "text": "platform anybody remember another three-letter problem so we have to face",
    "start": "1313920",
    "end": "1320580"
  },
  {
    "text": "down and fix those things then we have an interesting problem that was quite",
    "start": "1320580",
    "end": "1325980"
  },
  {
    "text": "fun to find workhorse remember that smart proxy I told you about it",
    "start": "1325980",
    "end": "1333090"
  },
  {
    "text": "apparently likes to talk to UNIX sockets not TCP we fixed it since obviously but",
    "start": "1333090",
    "end": "1342860"
  },
  {
    "text": "it's pretty amazing when you have a connection come in and then the moment you decide I'm gonna do it over TCP instead of Unix everybody becomes",
    "start": "1342860",
    "end": "1349350"
  },
  {
    "text": "localhost that's gonna make it walking hard great I've got 15 gigs of Splunk",
    "start": "1349350",
    "end": "1356130"
  },
  {
    "text": "and everybody's from the same machine oh right then we have another problem it's",
    "start": "1356130",
    "end": "1368040"
  },
  {
    "start": "1363000",
    "end": "1544000"
  },
  {
    "text": "deadly simple at that too time constraints which is what you have to",
    "start": "1368040",
    "end": "1375840"
  },
  {
    "text": "worry about your developers the development resources milestones roadmaps deliverables all those project",
    "start": "1375840",
    "end": "1383010"
  },
  {
    "text": "managers on your back saying hey the business still has to function while we",
    "start": "1383010",
    "end": "1388920"
  },
  {
    "text": "rework entire pieces of this that also means that new features are",
    "start": "1388920",
    "end": "1394909"
  },
  {
    "text": "going to continue coming because I'm not gonna be able to stop the guy who's doing the rails app and the go binaries",
    "start": "1394909",
    "end": "1400639"
  },
  {
    "text": "from actually shipping features that make it more performant better ad maven packages bring us all of the epic",
    "start": "1400639",
    "end": "1406999"
  },
  {
    "text": "stories etc etc I can't stop that it still has to function I still have a product to ship my customers still have",
    "start": "1406999",
    "end": "1412940"
  },
  {
    "text": "a product that they want to use and they want to get updates and security pages I can't just magically stop that to make my new thing we can't drop the ball none",
    "start": "1412940",
    "end": "1425989"
  },
  {
    "text": "of you realize that we've been doing this in the background the entire time that's the point we can't break existing",
    "start": "1425989",
    "end": "1433399"
  },
  {
    "text": "users and we can't have hiccups we have to think about everything ahead of time",
    "start": "1433399",
    "end": "1439220"
  },
  {
    "text": "plan well and execute but we've solved a",
    "start": "1439220",
    "end": "1446570"
  },
  {
    "text": "couple of old problems and moved into this cloud native thing and now we have new problems resources are dead simple",
    "start": "1446570",
    "end": "1456769"
  },
  {
    "text": "right everybody knows how much CPU this process needs I mean yes ok we know that",
    "start": "1456769",
    "end": "1464029"
  },
  {
    "text": "Unicorn is memory hungry we're aware right you've been telling us for years we use too much RAM we're working on it",
    "start": "1464029",
    "end": "1470869"
  },
  {
    "text": "but do we actually know what it is do you have the monitoring for your applications do you know how much CPU",
    "start": "1470869",
    "end": "1477379"
  },
  {
    "text": "you need when idle versus when there's 10 people versus literally some guy clicking around and if files because",
    "start": "1477379",
    "end": "1482989"
  },
  {
    "text": "he's one to look at what the kernel would like in 2 6 2 actually knowing",
    "start": "1482989",
    "end": "1490909"
  },
  {
    "text": "what your components need this is where monitoring comes into play you should have metrics you should have",
    "start": "1490909",
    "end": "1497989"
  },
  {
    "text": "performance data and have a clue what's going on the one thing you don't want to do is be like oh it only needs a gig a",
    "start": "1497989",
    "end": "1504950"
  },
  {
    "text": "ram and then you have an auto scaler set to trip when you use 50% average CPU",
    "start": "1504950",
    "end": "1511879"
  },
  {
    "text": "because you want to make sure you have enough and it spawns a whole bunch more pods and each pot actually uses 3 gigs",
    "start": "1511879",
    "end": "1518330"
  },
  {
    "text": "of ram and all of a sudden the oon kills the couplet and the entire cluster goes",
    "start": "1518330",
    "end": "1523389"
  },
  {
    "text": "from the internet thankfully we found that one in testing",
    "start": "1523389",
    "end": "1530039"
  },
  {
    "text": "then we also have to think about other things with a network right how do we deal with throughput do we know how much",
    "start": "1530550",
    "end": "1536559"
  },
  {
    "text": "those actually are how do we balance our services how do we ensure that those services are always up load balancing",
    "start": "1536559",
    "end": "1547210"
  },
  {
    "start": "1544000",
    "end": "1680000"
  },
  {
    "text": "what are we load balance what can we load balance can we use a layer seven",
    "start": "1547210",
    "end": "1552600"
  },
  {
    "text": "anybody think SSH is layer seven there's a hint do then we have to deal with which",
    "start": "1552600",
    "end": "1559600"
  },
  {
    "text": "providers now for us it's simple we know where we are but not everybody wants to use the same",
    "start": "1559600",
    "end": "1566650"
  },
  {
    "text": "load balancers that we want to use or the services that we use and since I'm making this for me but I'm making it for",
    "start": "1566650",
    "end": "1572710"
  },
  {
    "text": "all of my customers I have to remember that I have to make the entire thing",
    "start": "1572710",
    "end": "1577870"
  },
  {
    "text": "capable of using whatever my users decide to choose which means I have to support all the load balancers from all",
    "start": "1577870",
    "end": "1584380"
  },
  {
    "text": "the major cloud providers and figure out how to do it on metal it's a new option",
    "start": "1584380",
    "end": "1589870"
  },
  {
    "text": "thankfully then how do I deal with scaling I mentioned this a second ago",
    "start": "1589870",
    "end": "1595900"
  },
  {
    "text": "right maybe 50 percent for the thing that needs a lot of memory is a bad idea I thought the D percent was okay because",
    "start": "1595900",
    "end": "1602530"
  },
  {
    "text": "when I ran a QA test against it it didn't ever use more than 50 percent of one CPU apparently when I ran three more",
    "start": "1602530",
    "end": "1609580"
  },
  {
    "text": "it now used 115 percent and I had 16 pounds and it fell over again it's",
    "start": "1609580",
    "end": "1616270"
  },
  {
    "text": "important to know what things you can scale horizontally and which things you should just give more resources to in",
    "start": "1616270",
    "end": "1621940"
  },
  {
    "text": "the first place which things should be configurable and how to tweak them and",
    "start": "1621940",
    "end": "1627220"
  },
  {
    "text": "have a document to your users how to tweak them because their use case may not be exactly yours",
    "start": "1627220",
    "end": "1633370"
  },
  {
    "text": "so you should document exactly how you test it so that they can test it in",
    "start": "1633370",
    "end": "1639190"
  },
  {
    "text": "their load then the question comes down to automatic or manual kubernetes has",
    "start": "1639190",
    "end": "1644380"
  },
  {
    "text": "the wonderful ability to literally just COO control scale but then again you",
    "start": "1644380",
    "end": "1650200"
  },
  {
    "text": "have horizontal pod auto scalars and you have pod disruption budgets how much of that balance did you do maybe I",
    "start": "1650200",
    "end": "1656860"
  },
  {
    "text": "shouldn't scale my giddily notes out right if I inadvertently make too many databases I'm gonna piss things off but",
    "start": "1656860",
    "end": "1664480"
  },
  {
    "text": "all of my say registry nodes maybe if for some reason somebody has started pulling registry images every hour on",
    "start": "1664480",
    "end": "1671230"
  },
  {
    "text": "the hour from 100 machines baby every once in a while I should just let that",
    "start": "1671230",
    "end": "1676420"
  },
  {
    "text": "go ahead and spawn up and down to fit load right and you have resilience what",
    "start": "1676420",
    "end": "1685090"
  },
  {
    "start": "1680000",
    "end": "1734000"
  },
  {
    "text": "actually happens to your application when a node a whole know disappears off the cluster do you know do you know how",
    "start": "1685090",
    "end": "1692350"
  },
  {
    "text": "that behaves do you know which things shouldn't be on the same nodes so that that doesn't happen how does that",
    "start": "1692350",
    "end": "1700210"
  },
  {
    "text": "recover you should know these things and you think you do until you don't know",
    "start": "1700210",
    "end": "1705250"
  },
  {
    "text": "where your applications at because you don't know until it gets yet old traditional problems new ways of looking",
    "start": "1705250",
    "end": "1712900"
  },
  {
    "text": "at them yes but you also need new ways of examining them at the same time so",
    "start": "1712900",
    "end": "1718840"
  },
  {
    "text": "how to plan that out can actually be really hard and I'm not going to cover that because that's a whole other talk",
    "start": "1718840",
    "end": "1724500"
  },
  {
    "text": "some of that you may have gone to Marvin's talk yesterday and if you",
    "start": "1724500",
    "end": "1729880"
  },
  {
    "text": "didn't I suggest you catch it all the video recordings later now I kind of",
    "start": "1729880",
    "end": "1737080"
  },
  {
    "text": "buzzed through that but the three things I really want you to understand are",
    "start": "1737080",
    "end": "1742120"
  },
  {
    "text": "right here in simple terms in the beginning we were small and working",
    "start": "1742120",
    "end": "1749860"
  },
  {
    "text": "within a monolith was the right choice and I believe still today that's the",
    "start": "1749860",
    "end": "1756130"
  },
  {
    "text": "right choice for many people doing early work however while that provided us",
    "start": "1756130",
    "end": "1763780"
  },
  {
    "text": "those set of tools the point in which we needed to scale better and faster than",
    "start": "1763780",
    "end": "1768880"
  },
  {
    "text": "that tool was capable of doing is when we had to start actually growing beyond everything and think about it a new way",
    "start": "1768880",
    "end": "1775900"
  },
  {
    "text": "we saw the promises available to us through kubernetes the biggest thing to",
    "start": "1775900",
    "end": "1781870"
  },
  {
    "text": "remember is that these new ways of doing things will require the ways of looking at them be open to the fact of the way",
    "start": "1781870",
    "end": "1789040"
  },
  {
    "text": "you used to look at things may no longer be accurate and timely but remember that what you",
    "start": "1789040",
    "end": "1796460"
  },
  {
    "text": "did early on in the development lifecycle whatever your product is was it was the right choice at the time",
    "start": "1796460",
    "end": "1803500"
  },
  {
    "text": "because we're dealing with a piece of tech that goes almost as fast as get lab",
    "start": "1803500",
    "end": "1808790"
  },
  {
    "text": "features you could not have seen the future that you live in today learn from",
    "start": "1808790",
    "end": "1816980"
  },
  {
    "text": "what you've got in the past learn what's available now and work your way to it",
    "start": "1816980",
    "end": "1825129"
  },
  {
    "text": "okay",
    "start": "1827610",
    "end": "1830610"
  },
  {
    "text": "I do have a few minutes for questions in case anybody would like to ask them",
    "start": "1835100",
    "end": "1841960"
  },
  {
    "text": "the question was despite as careful and planning as we were did we end up with any big production customer oopsies",
    "start": "1851070",
    "end": "1858720"
  },
  {
    "text": "the answer is dole because we tested the daylights out of this first we deploy",
    "start": "1858720",
    "end": "1865220"
  },
  {
    "text": "believe it on all of these charts into review applications into a very large CI",
    "start": "1865220",
    "end": "1870810"
  },
  {
    "text": "cluster so we actually test everything about the charts at load for every single feature addition in the codebase",
    "start": "1870810",
    "end": "1878330"
  },
  {
    "text": "before it gets merged into our stable branch on the charts and before it gets released to a customer so when somebody",
    "start": "1878330",
    "end": "1884700"
  },
  {
    "text": "changes the way that a widget looks in the website it gets run through this in",
    "start": "1884700",
    "end": "1889860"
  },
  {
    "text": "our own CI any others",
    "start": "1889860",
    "end": "1897230"
  },
  {
    "text": "was our monolith in Bern at ease before we started okay so there's a fun story",
    "start": "1900590",
    "end": "1906389"
  },
  {
    "text": "about that one there is an older deprecated chart called the git lab omnibus chart because we literally took",
    "start": "1906389",
    "end": "1914009"
  },
  {
    "text": "that gigantic package and put it a docker shoved in a chart and went it'll",
    "start": "1914009",
    "end": "1919320"
  },
  {
    "text": "work it did technically work as long as",
    "start": "1919320",
    "end": "1925289"
  },
  {
    "text": "there was nothing else on the note so as long as you put taints on it that it got",
    "start": "1925289",
    "end": "1931350"
  },
  {
    "text": "one to all of its to itself and worked fine and the creation of that believe it",
    "start": "1931350",
    "end": "1936960"
  },
  {
    "text": "or not was three days on a bus at Mexico but my CEO made the mistake of",
    "start": "1936960",
    "end": "1943979"
  },
  {
    "text": "challenging us to do that three guys in three days on buses in Mexico made him",
    "start": "1943979",
    "end": "1951479"
  },
  {
    "text": "dance and I have the video to prove it anybody else",
    "start": "1951479",
    "end": "1958758"
  }
]