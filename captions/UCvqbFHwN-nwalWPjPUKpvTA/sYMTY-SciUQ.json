[
  {
    "start": "0",
    "end": "77000"
  },
  {
    "text": "alright thanks for thanks for coming everyone I'm Julius waltz one of the cofounders of the Prometheus monitoring",
    "start": "30",
    "end": "7500"
  },
  {
    "text": "system and this is really like an intro-level talk if you're already using Prometheus and you already kind of know",
    "start": "7500",
    "end": "14670"
  },
  {
    "text": "what it is maybe it's not so useful so I'm just giving you a chance to leave now anyway so in this talk I'm going to",
    "start": "14670",
    "end": "22560"
  },
  {
    "text": "just tell you what prometheus is what are the basic components why it's awesome why you should use it how it",
    "start": "22560",
    "end": "28800"
  },
  {
    "text": "came about and then you know hopefully we'll have some time for questions in",
    "start": "28800",
    "end": "33989"
  },
  {
    "text": "the end so what is prometheus Prometheus I would say is a tool set or",
    "start": "33989",
    "end": "40440"
  },
  {
    "text": "a whole ecosystem that cares about all the aspects of time series based",
    "start": "40440",
    "end": "45780"
  },
  {
    "text": "monitoring and alerting so we give you tools for getting metrics out of the",
    "start": "45780",
    "end": "51030"
  },
  {
    "text": "things you care about like processors and machines and networks and stuff but",
    "start": "51030",
    "end": "56070"
  },
  {
    "text": "then also infrastructure to collect those metrics to store them to make them",
    "start": "56070",
    "end": "61230"
  },
  {
    "text": "curable do useful stuff with their collected data and also you know besides",
    "start": "61230",
    "end": "66330"
  },
  {
    "text": "dashboards also offer alerting features and Prometheus is specifically",
    "start": "66330",
    "end": "71610"
  },
  {
    "text": "especially well-suited for dynamic cloud environments we do not do certain other",
    "start": "71610",
    "end": "80400"
  },
  {
    "text": "kinds of things just want to be clear on this Prometheus only does metrics that means like individual values that change",
    "start": "80400",
    "end": "87540"
  },
  {
    "text": "over time but that have some kind of time-series identifier we do not do logging where you want to store like all",
    "start": "87540",
    "end": "94560"
  },
  {
    "text": "the details of an individual event or request tracing so for logging you might",
    "start": "94560",
    "end": "100530"
  },
  {
    "text": "use something like elasticsearch for tracing maybe something like Zipkin and",
    "start": "100530",
    "end": "106710"
  },
  {
    "text": "we still recommend like having metrics logging and tracing in your infrastructure we allow you to define",
    "start": "106710",
    "end": "113700"
  },
  {
    "text": "alerting rules that could be possibly very complex but they're always very explicit so Prometheus doesn't do any",
    "start": "113700",
    "end": "120149"
  },
  {
    "text": "magic machine learning style anomaly detection just looking at your data or saying like Oh something looks really",
    "start": "120149",
    "end": "126780"
  },
  {
    "text": "weird here let's page someone also the parameter server itself is kind of",
    "start": "126780",
    "end": "133410"
  },
  {
    "text": "one node system and you can scale it manually but it's not a horizontally scalable system where you just press a",
    "start": "133410",
    "end": "139410"
  },
  {
    "text": "button add more machines and it kind of clouds up and that's also the storage by",
    "start": "139410",
    "end": "145290"
  },
  {
    "text": "default as it is infirmities itself is not a durable storage if you lose the",
    "start": "145290",
    "end": "150510"
  },
  {
    "text": "node that part of the storage is gone it's more of a really like real live monitoring system but we do have ways of",
    "start": "150510",
    "end": "157860"
  },
  {
    "text": "integrating long term storages that I'm going to mention later so we started",
    "start": "157860",
    "end": "164550"
  },
  {
    "start": "163000",
    "end": "249000"
  },
  {
    "text": "from Ethier's in 2012 at Sound Cloud out of a real need Matt and myself just came",
    "start": "164550",
    "end": "171240"
  },
  {
    "text": "from Google at that time and Sound Cloud was a company who already had in-house built like a cluster manager basically",
    "start": "171240",
    "end": "178350"
  },
  {
    "text": "before docker or kubernetes or Maysles really existed and you know they had",
    "start": "178350",
    "end": "184890"
  },
  {
    "text": "hundreds of micro services running on there and we needed something to monitor that I'll get to that in a second so we",
    "start": "184890",
    "end": "191130"
  },
  {
    "text": "started it at Sound Cloud brought it to relative maturity beginning of 2015 we",
    "start": "191130",
    "end": "198150"
  },
  {
    "text": "really published it took off how can use it all that now it's part of the cloud native computing foundation many",
    "start": "198150",
    "end": "204900"
  },
  {
    "text": "companies are using it it's kind of for many people it's almost like a default if you're using kubernetes it fits in",
    "start": "204900",
    "end": "210120"
  },
  {
    "text": "very well and yeah plenty of contributors big project maybe I should",
    "start": "210120",
    "end": "217830"
  },
  {
    "text": "say here also it's a bit special in comparison to some of the other CN CF",
    "start": "217830",
    "end": "223680"
  },
  {
    "text": "projects and that it didn't really have one engineering company that intentionally created this thing to put",
    "start": "223680",
    "end": "229739"
  },
  {
    "text": "in the open source world it was more of a byproduct like we created it because we needed it and then we thought oh",
    "start": "229739",
    "end": "235200"
  },
  {
    "text": "let's open sourced this but SoundCloud wasn't really you know in the business of creating open source infrastructure",
    "start": "235200",
    "end": "240959"
  },
  {
    "text": "software so now it's really one of the most independent software projects so",
    "start": "240959",
    "end": "250080"
  },
  {
    "start": "249000",
    "end": "295000"
  },
  {
    "text": "yeah the motivation was really SoundCloud already had this in-house build class 2 scheduler they had",
    "start": "250080",
    "end": "255120"
  },
  {
    "text": "hundreds of micro-services thousands of instances and we were just trying to figure out why SoundCloud was down all",
    "start": "255120",
    "end": "260970"
  },
  {
    "text": "the time or slow or whatever and it was very hard like if there's a latency spike using the existing monitoring to",
    "start": "260970",
    "end": "267240"
  },
  {
    "text": "stack then graphite stats D ganglia New Relic etc etc they either you know in terms of",
    "start": "267240",
    "end": "274919"
  },
  {
    "text": "the data model or in terms of the query language or the amount of history they could take into account didn't really",
    "start": "274919",
    "end": "280800"
  },
  {
    "text": "give us the the detailed insight that we needed to to figure out where exactly what was going wrong so finally we",
    "start": "280800",
    "end": "288449"
  },
  {
    "text": "decided to build a new solution which eventually became Prometheus to tackle all these issues I would say yeah let's",
    "start": "288449",
    "end": "296699"
  },
  {
    "start": "295000",
    "end": "458000"
  },
  {
    "text": "first have a look at the overall architecture you run one or multiple of",
    "start": "296699",
    "end": "303780"
  },
  {
    "text": "these central Prometheus servers that's block in the middle it's kind of the heart of the Prometheus ecosystem in",
    "start": "303780",
    "end": "310289"
  },
  {
    "text": "your company for example like every team could have one or every service depending on your organizational or",
    "start": "310289",
    "end": "315300"
  },
  {
    "text": "scaling needs you can configure these in scalable trees or other kind of you know",
    "start": "315300",
    "end": "322169"
  },
  {
    "text": "scalable architectures but let's just start with a single one you configure that Prometheus server to fetch metrics",
    "start": "322169",
    "end": "330770"
  },
  {
    "text": "over HTTP in a format that we define and the metrics can really come from any",
    "start": "330770",
    "end": "337110"
  },
  {
    "text": "kind of thing that can serve metrics over HTTP so you know typically we recommend you to if you have your own",
    "start": "337110",
    "end": "343680"
  },
  {
    "text": "applications like an own web app API server put metrics directly into your",
    "start": "343680",
    "end": "349500"
  },
  {
    "text": "own code white-box instrumentation that tells you exactly what's going on inside the application those are the best",
    "start": "349500",
    "end": "355860"
  },
  {
    "text": "metrics and then Prometheus can also go directly to your code and know it's like",
    "start": "355860",
    "end": "360960"
  },
  {
    "text": "oh you know if I cannot pull from your thing it's down and all these nice side effects for some things you cannot",
    "start": "360960",
    "end": "368340"
  },
  {
    "text": "directly do that you're not gonna like input Prometheus metrics into the Linux kernel or into the my sequel daemon",
    "start": "368340",
    "end": "375360"
  },
  {
    "text": "right and for that we have this exporter model where you run a sidecar job right next to the thing that has metrics in",
    "start": "375360",
    "end": "382319"
  },
  {
    "text": "some other kind of format already like the Linux kernel over the proc file system for example the node exporter",
    "start": "382319",
    "end": "389099"
  },
  {
    "text": "does that for host metrics or for Linux then Prometheus scrapes the node exporter and the node exporter in the",
    "start": "389099",
    "end": "394469"
  },
  {
    "text": "background just goes to the proc file system and some other to some other syscalls to figure out actual host",
    "start": "394469",
    "end": "400349"
  },
  {
    "text": "metrics translates them to prometheus metrics and shipped them back synchronously to",
    "start": "400349",
    "end": "405480"
  },
  {
    "text": "Prometheus Prometheus has a local time series database where it stores all these time series over time",
    "start": "405480",
    "end": "412290"
  },
  {
    "text": "it's a configurable retention could be like weeks sometimes months some extreme",
    "start": "412290",
    "end": "417720"
  },
  {
    "text": "people do years not really recommended then you can configure Prometheus to",
    "start": "417720",
    "end": "423980"
  },
  {
    "text": "calculate alerting rules over the collected data and the alert manager",
    "start": "423980",
    "end": "429210"
  },
  {
    "text": "component which is a separate binary then does the final dispatching of",
    "start": "429210",
    "end": "434510"
  },
  {
    "text": "notifications based on their details to chooses which team and what kind of notification mechanism you know slack",
    "start": "434510",
    "end": "441210"
  },
  {
    "text": "email page of duty etc it should go to and then you have you can integrate with",
    "start": "441210",
    "end": "447150"
  },
  {
    "text": "like Ravana and the HTTP API or you can have built-in UI and prometheus to you",
    "start": "447150",
    "end": "452640"
  },
  {
    "text": "know just play the data and dashboards or do other useful stuff with it I would",
    "start": "452640",
    "end": "459060"
  },
  {
    "start": "458000",
    "end": "514000"
  },
  {
    "text": "say these are my favorite selling points of Prometheus over other monitoring systems back in the day at least and",
    "start": "459060",
    "end": "465120"
  },
  {
    "text": "still nowadays I think the first to kind of go hand in hand",
    "start": "465120",
    "end": "470340"
  },
  {
    "text": "Prometheus has a dimensional data model that allows you to record in relatively",
    "start": "470340",
    "end": "475800"
  },
  {
    "text": "high detail for a time series based system what is happening where and then to make sense of that collected high",
    "start": "475800",
    "end": "482910"
  },
  {
    "text": "detail data we give you a powerful query language to compute whatever you kind of currently want to know about that data a",
    "start": "482910",
    "end": "491210"
  },
  {
    "text": "single primitive server is pretty simple to operate and still relatively you know",
    "start": "491210",
    "end": "496980"
  },
  {
    "text": "efficient I'll get to that later and the point that makes Prometheus",
    "start": "496980",
    "end": "503040"
  },
  {
    "text": "specifically work really well in dynamic cloud environments and cluster schedulers is the service discovery integration how it knows what it should",
    "start": "503040",
    "end": "510480"
  },
  {
    "text": "monitor all the time so I'm going to hop through all these",
    "start": "510480",
    "end": "515640"
  },
  {
    "start": "514000",
    "end": "743000"
  },
  {
    "text": "four points in row and go into some details first the data model you know",
    "start": "515640",
    "end": "520680"
  },
  {
    "text": "we'd store time series that's nothing revolutionary you know in all time",
    "start": "520680",
    "end": "528570"
  },
  {
    "text": "series have some kind of identifier and then they're just like turns them value times them value times them value",
    "start": "528570",
    "end": "534370"
  },
  {
    "text": "and in prometheus all the timestamps are in 64 in millisecond precision let's just keep it simple that's it that's all",
    "start": "534370",
    "end": "541150"
  },
  {
    "text": "you need for systems monitoring that's what we say the value type is always a float64 that works for pretty much",
    "start": "541150",
    "end": "548740"
  },
  {
    "text": "everything you can increment that like for I don't know forgot like 285 years 1",
    "start": "548740",
    "end": "553750"
  },
  {
    "text": "million increments per second before you reach the first non consecutive integer so at that point you should like reset",
    "start": "553750",
    "end": "560320"
  },
  {
    "text": "counters at some point or so so it doesn't really become a problem but yeah",
    "start": "560320",
    "end": "567460"
  },
  {
    "text": "so the the big difference is kind of how we identify time series compared to",
    "start": "567460",
    "end": "572470"
  },
  {
    "text": "systems that came before let's compare it to the graphite and stats the metrics",
    "start": "572470",
    "end": "578140"
  },
  {
    "text": "model where you had a long metric string that had some dots in the middle to two",
    "start": "578140",
    "end": "583540"
  },
  {
    "text": "separate components that form the implicit hierarchy in this example you have like an HTTP requests happening on",
    "start": "583540",
    "end": "591430"
  },
  {
    "text": "nginx servers on different hosts that's the IP component there and then",
    "start": "591430",
    "end": "596620"
  },
  {
    "text": "different paths and different status codes but the hierarchy here is implied",
    "start": "596620",
    "end": "601630"
  },
  {
    "text": "and implicit you know you need to kind of know what each component means but also you know why would the status code",
    "start": "601630",
    "end": "607930"
  },
  {
    "text": "be lower in a hierarchy than the path for example or why why wouldn't the host come first also if you add another",
    "start": "607930",
    "end": "615010"
  },
  {
    "text": "component would you add it in the end in the beginning somewhere in the middle it's kind of hard to extend as well in",
    "start": "615010",
    "end": "623220"
  },
  {
    "text": "Prometheus we go a bit of a different route where we put the metric name front",
    "start": "623220",
    "end": "628600"
  },
  {
    "text": "and center it's basically the central aspect of a system that you are monitoring like the number of HTTP",
    "start": "628600",
    "end": "634330"
  },
  {
    "text": "requests we have counted since a given process has started and you know the",
    "start": "634330",
    "end": "639940"
  },
  {
    "text": "process exposes that and then it can have can put key value pairs on that",
    "start": "639940",
    "end": "645370"
  },
  {
    "text": "metric to further DIF up differentiate dimensions on that metric for example",
    "start": "645370",
    "end": "651190"
  },
  {
    "text": "you know the process itself would add the path and the status labels onto",
    "start": "651190",
    "end": "656620"
  },
  {
    "text": "these metrics but then Prometheus when it scrapes the target also adds some labels of that target saying like this",
    "start": "656620",
    "end": "663160"
  },
  {
    "text": "came from this service or which we call job this job and this particular process",
    "start": "663160",
    "end": "668170"
  },
  {
    "text": "instance so you're getting some labels from the instrumentation in the process and some labels coming from the",
    "start": "668170",
    "end": "674230"
  },
  {
    "text": "Prometheus server because it knows what it actually just scraped and yeah this",
    "start": "674230",
    "end": "680260"
  },
  {
    "text": "gives you more explicit mapping of keys to values and there's no hierarchy anymore either now if you query that",
    "start": "680260",
    "end": "689800"
  },
  {
    "text": "becomes a bit more obvious y1 is nicer than the other in the kind of graphite model you have to know where to put",
    "start": "689800",
    "end": "696699"
  },
  {
    "text": "asterisks let's say you want to query all the HTTP requests for all the engine X's that end up with a 500 status code",
    "start": "696699",
    "end": "704970"
  },
  {
    "text": "would look like in the top four for graphite it's kind of implicit you need",
    "start": "704970",
    "end": "710740"
  },
  {
    "text": "to know what it all means for for Prometheus you just you start with a metric name and then you just add random",
    "start": "710740",
    "end": "716699"
  },
  {
    "text": "label filters you could also have negative or reg X filters here just",
    "start": "716699",
    "end": "721930"
  },
  {
    "text": "selecting the time series that you really care about and you don't even have to know what other labels exist on",
    "start": "721930",
    "end": "728410"
  },
  {
    "text": "those time series so really you want these kind of key value levels as",
    "start": "728410",
    "end": "733990"
  },
  {
    "text": "first-class citizens which also fits nicely with kubernetes and docker which",
    "start": "733990",
    "end": "739360"
  },
  {
    "text": "also have the same label concept so now we have collected all the useful data",
    "start": "739360",
    "end": "745540"
  },
  {
    "start": "743000",
    "end": "1122000"
  },
  {
    "text": "now let's do cool stuff with it Prometheus defines a new completely custom query language it's not sequel",
    "start": "745540",
    "end": "753220"
  },
  {
    "text": "based which was a bit of a like some people get surprised by that but we",
    "start": "753220",
    "end": "758470"
  },
  {
    "text": "think that what we have works better than a sequel based language for the kinds of computations we typically want",
    "start": "758470",
    "end": "764589"
  },
  {
    "text": "to do in time series based monitoring let's have a look at some simple",
    "start": "764589",
    "end": "769839"
  },
  {
    "text": "examples let's say we have this node exporter running on all our machines and the cluster and it tells us as one",
    "start": "769839",
    "end": "775990"
  },
  {
    "text": "metric for example like for every filesystem what's its total capacity and",
    "start": "775990",
    "end": "782230"
  },
  {
    "text": "then there will be all these labels on it the mount point and the device and which actual machine instance it is and",
    "start": "782230",
    "end": "787899"
  },
  {
    "text": "so on and now we can just ask Prometheus hey either give me everything then you would just list this metric name but",
    "start": "787899",
    "end": "794529"
  },
  {
    "text": "then we can add filters like do not give me the ones that are mounted on route divide that whole set of time series",
    "start": "794529",
    "end": "801970"
  },
  {
    "text": "by a billion to go from like bytes to roughly gigabytes and then well I only",
    "start": "801970",
    "end": "808689"
  },
  {
    "text": "want the ones that are actually larger than 100 gigabytes and this would give me all the partitions not mounted on",
    "start": "808689",
    "end": "813970"
  },
  {
    "text": "route larger than 100 gigabytes in all of my infrastructure and the output also contains the useful labels another",
    "start": "813970",
    "end": "823779"
  },
  {
    "text": "common thing you want to do in Prometheus is to calculate the ratio of two rates for example like you have your",
    "start": "823779",
    "end": "830049"
  },
  {
    "text": "error rate and you have a total rate like in this case and you want to you",
    "start": "830049",
    "end": "835689"
  },
  {
    "text": "know we want to know the ratio how many errors am I getting in comparison to the total and this is this is something you",
    "start": "835689",
    "end": "841479"
  },
  {
    "text": "can do with with a binary operator like a divided by year which isn't so",
    "start": "841479",
    "end": "846759"
  },
  {
    "text": "spectacular like if you sum over all the time series of all the rates that are bad and sum of all the ones that are",
    "start": "846759",
    "end": "852129"
  },
  {
    "text": "good and then divide you get a single number out mayor okay sometimes you want to split that up by by different",
    "start": "852129",
    "end": "859119"
  },
  {
    "text": "dimensions for example you want to keep the path you want to have the error rate ratio per request path then you could",
    "start": "859119",
    "end": "866529"
  },
  {
    "text": "add a modifier to this expression saying by path could add different labels in",
    "start": "866529",
    "end": "872559"
  },
  {
    "text": "there as well and then it would preserve this dimension and this is where some of the actual cool automatic join magic",
    "start": "872559",
    "end": "879849"
  },
  {
    "text": "happens in from QL if you have a binary operator between two whole sets of time",
    "start": "879849",
    "end": "886119"
  },
  {
    "text": "series then it will automatically join them up on identical label sets so you know the 500 error rate for the path on",
    "start": "886119",
    "end": "892959"
  },
  {
    "text": "the Left will divide it by the will be divided for the total rate for that same",
    "start": "892959",
    "end": "899049"
  },
  {
    "text": "path on the right and that gets propagated into the output so it's kind of this vector based arithmetic slang",
    "start": "899049",
    "end": "905049"
  },
  {
    "text": "guack you can do more fancy stuff for example you can track Layton sees with",
    "start": "905049",
    "end": "912099"
  },
  {
    "text": "histograms in your instrumentation and then use a query function doing career time to say like give me the 99th",
    "start": "912099",
    "end": "920369"
  },
  {
    "text": "percentile for example aggregated over all the instances and as averaged over a",
    "start": "920369",
    "end": "926409"
  },
  {
    "text": "5 minute time span so you can choose the percentile the aggregation level and the",
    "start": "926409",
    "end": "933369"
  },
  {
    "text": "overtime during Korea time and then you get these answers so these are just some examples",
    "start": "933369",
    "end": "940930"
  },
  {
    "text": "you can combine many more crazy constructs into very long crazy from QL",
    "start": "940930",
    "end": "948210"
  },
  {
    "text": "statements but you know this I think does for beginning now once you know a",
    "start": "948210",
    "end": "954160"
  },
  {
    "text": "bit of this language you can start looking at data with it either in this built-in expression browser in",
    "start": "954160",
    "end": "960220"
  },
  {
    "text": "parameters itself that's in the primitive server for example just looking at the current value of everything in your infrastructure",
    "start": "960220",
    "end": "966960"
  },
  {
    "text": "there's also a simple built-in graphing interface but then once you want to have",
    "start": "966960",
    "end": "972730"
  },
  {
    "text": "more bells and whistles nice dashboards that you can save and share with your colleagues of course you'll want to use CRO fauna",
    "start": "972730",
    "end": "978760"
  },
  {
    "text": "Accra fauna has native support for Prometheus backends so that's like what",
    "start": "978760",
    "end": "984190"
  },
  {
    "text": "we recommend for people to use for serious dashboarding and then the cool thing I think is that Prometheus also",
    "start": "984190",
    "end": "991720"
  },
  {
    "text": "integrates alerting in a unified way with the query language and based on the",
    "start": "991720",
    "end": "997270"
  },
  {
    "text": "data that is collected so you know in the Nagios style world you would run periodic checks on machines that don't",
    "start": "997270",
    "end": "1003839"
  },
  {
    "text": "really have much history they just check if something is currently running or the CPU is currently high or so in",
    "start": "1003839",
    "end": "1010320"
  },
  {
    "text": "prometheus it's it's a bit different we just collect we collect all the data as time series first and then you can",
    "start": "1010320",
    "end": "1016710"
  },
  {
    "text": "formulate any prompt qlae expression on top of that data that can take into account different amounts of histories",
    "start": "1016710",
    "end": "1023460"
  },
  {
    "text": "or also different aggregation levels you could say like is the whole service bad or is a particular process bad you can",
    "start": "1023460",
    "end": "1029640"
  },
  {
    "text": "decide on or something in between right and decide like what's the relevant alerting level and the way that works is",
    "start": "1029640",
    "end": "1036510"
  },
  {
    "text": "you say like in this example we say we want to create an alert many 500 errors",
    "start": "1036510",
    "end": "1041640"
  },
  {
    "text": "you configure that in your Prometheus server it will evaluate it every couple of seconds it will run this prompt ql",
    "start": "1041640",
    "end": "1047938"
  },
  {
    "text": "expression that is in yellow here and this is a repetition of something i had",
    "start": "1047939",
    "end": "1053610"
  },
  {
    "text": "earlier where it calculates like that per path error rate ratio i multiply it",
    "start": "1053610",
    "end": "1059490"
  },
  {
    "text": "by a hundred to get from a ratio to percentage and then I filter it to only get the paths that have more than 5%",
    "start": "1059490",
    "end": "1065730"
  },
  {
    "text": "error and in the lodging role in prometheus works this way like if there's no output",
    "start": "1065730",
    "end": "1071100"
  },
  {
    "text": "from an expression everything's good but any any time series that gets outputted",
    "start": "1071100",
    "end": "1076170"
  },
  {
    "text": "from the expression becomes an alert and then an alert manager we're premature",
    "start": "1076170",
    "end": "1081780"
  },
  {
    "text": "sends these individual alerts they can be grouped into single notifications like you know if you have an alert per",
    "start": "1081780",
    "end": "1089160"
  },
  {
    "text": "machine for example that's done and there's 100 machines down then a lot manager could like group that into a",
    "start": "1089160",
    "end": "1094740"
  },
  {
    "text": "single notification you can also say you know every output element has to be bad",
    "start": "1094740",
    "end": "1100050"
  },
  {
    "text": "for at least 5 minutes that's the statement the for statement there before",
    "start": "1100050",
    "end": "1105150"
  },
  {
    "text": "actually you know bugging someone around about it that's important for some kind of robustness and you can add extra",
    "start": "1105150",
    "end": "1111240"
  },
  {
    "text": "labels and annotations that can be used in a lot manager routing and in the",
    "start": "1111240",
    "end": "1116490"
  },
  {
    "text": "actual human readable snippets that you get in your alerts Prometheus is you",
    "start": "1116490",
    "end": "1125430"
  },
  {
    "text": "know single static go binary kind of server and you just run it on a machine it doesn't do any clustering because we",
    "start": "1125430",
    "end": "1132360"
  },
  {
    "text": "think that's really hard to do right and the clustering might also be the first",
    "start": "1132360",
    "end": "1137580"
  },
  {
    "text": "thing that breaks in the face of a network outage so you know if you want",
    "start": "1137580",
    "end": "1142620"
  },
  {
    "text": "to have high availability for alerting everything in parameters is built around high availability for alerting not",
    "start": "1142620",
    "end": "1148590"
  },
  {
    "text": "necessarily for infinite data retention or something but there we just if you want ha4 alerting you run two identical",
    "start": "1148590",
    "end": "1155880"
  },
  {
    "text": "Prometheus servers that don't know anything about each other but have the same scraping configuration and they",
    "start": "1155880",
    "end": "1161310"
  },
  {
    "text": "they might have slightly different scrape phases but conceptually they they collect the same data and they concur",
    "start": "1161310",
    "end": "1168060"
  },
  {
    "text": "compute the same alerting rules and communicate to the same alert manager cluster and then alert manager will",
    "start": "1168060",
    "end": "1174450"
  },
  {
    "text": "deduplicate based on the label sets so you don't actually get two notifications as long if both are still running yeah",
    "start": "1174450",
    "end": "1182000"
  },
  {
    "text": "but yeah like getting started with preemies is pretty simple there's like people running it on a Raspberry Pi at",
    "start": "1182000",
    "end": "1188130"
  },
  {
    "text": "home of course it gets more complicated once you have really big set ups and you need to think about scaling and topology",
    "start": "1188130",
    "end": "1194250"
  },
  {
    "text": "a single server is relatively efficient like we can do",
    "start": "1194250",
    "end": "1200160"
  },
  {
    "text": "yeah it depends on the hardware but if you have a big machine you can do a couple of millions of active series at a",
    "start": "1200160",
    "end": "1206340"
  },
  {
    "text": "given time and over a million samples ingested per second and then we store those samples as",
    "start": "1206340",
    "end": "1212940"
  },
  {
    "text": "roughly like something between one and two bytes per sample on disk it's a custom time series database so this is",
    "start": "1212940",
    "end": "1219600"
  },
  {
    "text": "good for keeping weeks months some people go out on a limb and keep like only few metrics but for years but then",
    "start": "1219600",
    "end": "1226500"
  },
  {
    "text": "we tell them like yeah there's no durability guarantees for people who do want to keep data more durably and",
    "start": "1226500",
    "end": "1232980"
  },
  {
    "text": "longer there's some decoupled remote storage support where you can tell Prometheus to ship all your all its",
    "start": "1232980",
    "end": "1238830"
  },
  {
    "text": "samples to remote endpoint and then that can choose to store it in a remote",
    "start": "1238830",
    "end": "1244050"
  },
  {
    "text": "system for example we have like in Flex DB supports that now there's yeah we",
    "start": "1244050",
    "end": "1249570"
  },
  {
    "text": "have adapters for graphite there's cortex from weave words there's several systems that like basically over ten",
    "start": "1249570",
    "end": "1256410"
  },
  {
    "text": "integrations in this space now where that allow you to write Prometheus samples to a scalable durable backends",
    "start": "1256410",
    "end": "1263550"
  },
  {
    "text": "system and some of them also allow you to read that data back through Prometheus again Prometheus as I",
    "start": "1263550",
    "end": "1272550"
  },
  {
    "text": "promised works really well in dynamic environments you know nowadays we have a on-demand VMS let's get up and down all",
    "start": "1272550",
    "end": "1279570"
  },
  {
    "text": "the time and disappear and then on top of that we have cluster schedulers like kubernetes and so on and then on top of",
    "start": "1279570",
    "end": "1286530"
  },
  {
    "text": "that we run hundreds of micro services and all those craziness and your monitoring system still needs to figure",
    "start": "1286530",
    "end": "1292350"
  },
  {
    "text": "out somehow what should currently be there what's this what's the source of truth that I should be expecting so that",
    "start": "1292350",
    "end": "1299490"
  },
  {
    "text": "I can tell you that something is not there for example right so how to make sense of all this we integrate with",
    "start": "1299490",
    "end": "1305520"
  },
  {
    "text": "service discovery in in prometheus so Prometheus can talk to various different",
    "start": "1305520",
    "end": "1311160"
  },
  {
    "text": "types of service discovery like DNS or kubernetes to know what should be",
    "start": "1311160",
    "end": "1316560"
  },
  {
    "text": "currently in your infrastructure and it pulls you know it uses that information",
    "start": "1316560",
    "end": "1322140"
  },
  {
    "text": "to to pull from those instances and then if it for example if the pool fails it",
    "start": "1322140",
    "end": "1328110"
  },
  {
    "text": "already has one automatic health signal that you can use that it records as a metric saying like this pool failed",
    "start": "1328110",
    "end": "1333779"
  },
  {
    "text": "that you could use in alerting or just looking at your infrastructure what's currently missing or down and if it's a",
    "start": "1333779",
    "end": "1340769"
  },
  {
    "text": "good service discovery like the kubernetes one it will also give you like really cool metadata about the things it has discovered like it will",
    "start": "1340769",
    "end": "1347519"
  },
  {
    "text": "tell you what kind of part it is that you're monitoring its like environment production type name engine X whatever",
    "start": "1347519",
    "end": "1354359"
  },
  {
    "text": "and then it you can choose if you want you to add that as labels to your time series so then you can actually select",
    "start": "1354359",
    "end": "1360539"
  },
  {
    "text": "and aggregate on that data so we support I think like roughly ten different types",
    "start": "1360539",
    "end": "1367589"
  },
  {
    "text": "of services coverage some are kind of lyric note level ones to discover you",
    "start": "1367589",
    "end": "1372749"
  },
  {
    "text": "know instances on machine providers a AWS as a Google others are more cluster",
    "start": "1372749",
    "end": "1378299"
  },
  {
    "text": "manager ones kubernetes obviously and then their generic ones DNS is pretty",
    "start": "1378299",
    "end": "1383729"
  },
  {
    "text": "agnostic and console zookeeper you can even build your own one if Prometheus doesn't support it yet with a file",
    "start": "1383729",
    "end": "1389789"
  },
  {
    "text": "watcher based interface so like in conclusion you know promises helps you",
    "start": "1389789",
    "end": "1395339"
  },
  {
    "text": "monitor and make sense of infrastructure that is dynamic and complex with these",
    "start": "1395339",
    "end": "1400859"
  },
  {
    "text": "four nice properties and yeah basically",
    "start": "1400859",
    "end": "1405950"
  },
  {
    "text": "that's that's it for today and I'm happy to take questions",
    "start": "1405950",
    "end": "1411440"
  },
  {
    "text": "yep [Applause]",
    "start": "1411440",
    "end": "1420399"
  },
  {
    "start": "1413000",
    "end": "1886000"
  },
  {
    "text": "yeah so there's like the alert itself",
    "start": "1433950",
    "end": "1440920"
  },
  {
    "text": "and then there's a final incident management portion which can happen outside of Prometheus the alerting in",
    "start": "1440920",
    "end": "1445930"
  },
  {
    "text": "Prometheus itself is really based on kind of a sliding window system where it really just checks is something bad",
    "start": "1445930",
    "end": "1452530"
  },
  {
    "text": "right now given the window of time that you're currently looking at in a sliding window fashion and if at some point the",
    "start": "1452530",
    "end": "1458470"
  },
  {
    "text": "badness shifts out of that window the alert will also resolve in prometheus unless you built like a fancy feedback",
    "start": "1458470",
    "end": "1465010"
  },
  {
    "text": "contraption where you record a new transceivers that feeds into you know this which gets a bit weird yeah okay",
    "start": "1465010",
    "end": "1478300"
  },
  {
    "text": "yeah so as long as the problem persists Prometheus will actually like every evaluation interval like which could be",
    "start": "1478300",
    "end": "1485770"
  },
  {
    "text": "15 or 30 seconds sent to alert manager like this problem is still active for this particular label combination and",
    "start": "1485770",
    "end": "1492760"
  },
  {
    "text": "then an alert manager you configure for a given grouping of alerts by labels",
    "start": "1492760",
    "end": "1500800"
  },
  {
    "text": "which you can choose their send like repeat this every two hours for example",
    "start": "1500800",
    "end": "1507700"
  },
  {
    "text": "or once per day if this problem still persists and you can also say they're",
    "start": "1507700",
    "end": "1512980"
  },
  {
    "text": "like for example if hundred hosts go down and you have an alert based on all",
    "start": "1512980",
    "end": "1519310"
  },
  {
    "text": "of them but you want to group them in a lot manager then you can say well you know maybe first I get alerts about 70",
    "start": "1519310",
    "end": "1525700"
  },
  {
    "text": "of them being down and then I don't want",
    "start": "1525700",
    "end": "1531400"
  },
  {
    "text": "to send out a notification yet because maybe 30 others will come just like a couple of seconds later so you could say",
    "start": "1531400",
    "end": "1537280"
  },
  {
    "text": "in a lot manager like wait 30 seconds in case there's more alert elements coming",
    "start": "1537280",
    "end": "1542710"
  },
  {
    "text": "into the same grouping so you can still group them together and these kind of features there but at some point if from",
    "start": "1542710",
    "end": "1549220"
  },
  {
    "text": "easiest things the problem is gone gone then it will like tell a lot manager oh it's resolved and it will also disappear",
    "start": "1549220",
    "end": "1555310"
  },
  {
    "text": "in alert manager yeah",
    "start": "1555310",
    "end": "1559020"
  },
  {
    "text": "yeah yeah yeah then if the problem still persists then it will you can get",
    "start": "1562160",
    "end": "1569190"
  },
  {
    "text": "repeated notifications through a lot manager yeah oh yeah and I will think about repeating questions actually yeah",
    "start": "1569190",
    "end": "1577940"
  },
  {
    "text": "yeah so question is yeah better to run prometheus inside the kubernetes cluster",
    "start": "1584510",
    "end": "1591960"
  },
  {
    "text": "or outside we recommend inside because then a lot of stuff gets taken care for",
    "start": "1591960",
    "end": "1597660"
  },
  {
    "text": "you automatically you get the service account stuff mounted in automatically",
    "start": "1597660",
    "end": "1602840"
  },
  {
    "text": "and it works network wise to reach all the pods automatically right so yeah we recommend that and then of course it's",
    "start": "1602840",
    "end": "1609810"
  },
  {
    "text": "nice to still maybe have some prometheus outside of the cluster or some other meta monitoring that checks whether the",
    "start": "1609810",
    "end": "1615330"
  },
  {
    "text": "one in the cluster actually works yeah",
    "start": "1615330",
    "end": "1619130"
  },
  {
    "text": "yeah yeah how many Prometheus services per instances to monitor I think it",
    "start": "1625010",
    "end": "1630630"
  },
  {
    "text": "really depends on how many the the the bottleneck is usually the number of time series so if your instances export like",
    "start": "1630630",
    "end": "1638550"
  },
  {
    "text": "five times here's each you can have either multiple thousands of instances",
    "start": "1638550",
    "end": "1644600"
  },
  {
    "text": "but if each instance has like ten thousand times here is that it exposes to Prometheus and you can have far fewer",
    "start": "1644600",
    "end": "1650220"
  },
  {
    "text": "you know the budgets that you have to think about most is a single primitive server can do if it's a big one can do",
    "start": "1650220",
    "end": "1657960"
  },
  {
    "text": "like maybe a couple of million of time series at a given time the time series is every unique combination of labels",
    "start": "1657960",
    "end": "1665070"
  },
  {
    "text": "that is currently relevant so metric name plus like all the you have to multiply to get like all the different",
    "start": "1665070",
    "end": "1671640"
  },
  {
    "text": "values that each label can have and multiply that for all the different possible labels to get the total number",
    "start": "1671640",
    "end": "1678240"
  },
  {
    "text": "of serious you're going to ingest and then times the number of instances or pods you have and that gives you an idea",
    "start": "1678240",
    "end": "1685200"
  },
  {
    "text": "of your budgeting there",
    "start": "1685200",
    "end": "1688580"
  },
  {
    "text": "yeah like it starts to slow down ingestion but you might also all just find that your queries don't return",
    "start": "1690970",
    "end": "1697040"
  },
  {
    "text": "anymore if you have like one very common error at the beginning for Prometheus users is like once they find out how",
    "start": "1697040",
    "end": "1703130"
  },
  {
    "text": "useful labels are they put information into the label values that is too",
    "start": "1703130",
    "end": "1708380"
  },
  {
    "text": "unbounded in cardinality like a user IP address or email or something and that could be millions of those right and",
    "start": "1708380",
    "end": "1713990"
  },
  {
    "text": "that gets multiplied with all the other cardinalities of all the other labels and then you have suddenly have like a billion time series and the Prometheus",
    "start": "1713990",
    "end": "1720620"
  },
  {
    "text": "server just instantly dies and then you can either ingest nor query or anything so yeah you will you'll get slow",
    "start": "1720620",
    "end": "1727309"
  },
  {
    "text": "ingestion or mist like failing scrapes and also slower queries or failing",
    "start": "1727309",
    "end": "1733070"
  },
  {
    "text": "queries or so at some point it'll just blow up yeah yeah",
    "start": "1733070",
    "end": "1746480"
  },
  {
    "text": "Rob med features for Prometheus in the year in a year from now or so so",
    "start": "1746480",
    "end": "1752919"
  },
  {
    "text": "Prometheus as a whole is probably not going to change that much anymore in in",
    "start": "1752919",
    "end": "1759470"
  },
  {
    "text": "terms of its scope and what it wants to do there's some things we have like we",
    "start": "1759470",
    "end": "1765350"
  },
  {
    "text": "have an official road map section on the site that has shrunk and shrunk and truck every time we have crossed out",
    "start": "1765350",
    "end": "1770630"
  },
  {
    "text": "some of those items one item that still on there is making better use of the metadata like when you instrument",
    "start": "1770630",
    "end": "1778460"
  },
  {
    "text": "something on the on the side of a process that exposes metrics it actually exposes still some metadata about the",
    "start": "1778460",
    "end": "1785270"
  },
  {
    "text": "metrics like what each metric means like help string and also whether it's a counter or gauge histogram or summary",
    "start": "1785270",
    "end": "1790730"
  },
  {
    "text": "metric and the server when it stores that information just the first thing it",
    "start": "1790730",
    "end": "1795770"
  },
  {
    "text": "does is throw that metadata away because we don't have any facility to store and make use of that yet and so one roadmap",
    "start": "1795770",
    "end": "1802940"
  },
  {
    "text": "item is still somehow temporarily aggregating at least that map that",
    "start": "1802940",
    "end": "1808820"
  },
  {
    "text": "metadata in memory and Prometheus so that we could give you some hints maybe if you're formulating a query and you're",
    "start": "1808820",
    "end": "1814549"
  },
  {
    "text": "doing and I can rate over a gauge we can tell you like oh you're trying to do something that might be not as you intended here rates can only be done",
    "start": "1814549",
    "end": "1821510"
  },
  {
    "text": "over a counter type metric or things like this that's one thing more and robust more robust remote",
    "start": "1821510",
    "end": "1828980"
  },
  {
    "text": "storage integrations is definitely a thing that's still being explored we had this initial interface where Prometheus",
    "start": "1828980",
    "end": "1834830"
  },
  {
    "text": "just sends samples in a in a real-time almost near time real-time kind of way",
    "start": "1834830",
    "end": "1840260"
  },
  {
    "text": "to a different endpoint that I talked about but then if you permit the server is down for or the network connection to",
    "start": "1840260",
    "end": "1847190"
  },
  {
    "text": "a hosted service that does the long-term storage for you is like down for 10 minutes and you just lose 10 minutes of",
    "start": "1847190",
    "end": "1852320"
  },
  {
    "text": "data and there's some other approaches that are more about shipping whole chunks from a Prometheus to some remote",
    "start": "1852320",
    "end": "1858560"
  },
  {
    "text": "system one of them is called Thanos and yeah I really recommend looking into",
    "start": "1858560",
    "end": "1864410"
  },
  {
    "text": "that one yeah I think those would be the main areas always looking for more",
    "start": "1864410",
    "end": "1869900"
  },
  {
    "text": "integrations and yeah but I think as a whole prometheus is pretty pretty set",
    "start": "1869900",
    "end": "1875030"
  },
  {
    "text": "the scope yep ok if you have multiple so",
    "start": "1875030",
    "end": "1889370"
  },
  {
    "start": "1886000",
    "end": "1981000"
  },
  {
    "text": "if one Prometheus server is no longer enough then as I said Prometheus doesn't",
    "start": "1889370",
    "end": "1895910"
  },
  {
    "text": "do automatic horizontally like push button scaling you have to kind of think about how you split up the load over",
    "start": "1895910",
    "end": "1904190"
  },
  {
    "text": "your Prometheus instances if you search for blog post I think Prometheus scaling and Federation that's a good overview",
    "start": "1904190",
    "end": "1910970"
  },
  {
    "text": "there's different strategies you could like one example is you have ten data centers one for me this is enough for",
    "start": "1910970",
    "end": "1916520"
  },
  {
    "text": "each data center but not for at all so you could have one Prometheus per data center or this could also be an in data",
    "start": "1916520",
    "end": "1922970"
  },
  {
    "text": "center thing that monitors all the detailed metrics within that data center",
    "start": "1922970",
    "end": "1928400"
  },
  {
    "text": "or that cluster or whatever your units are and then you have a global one that federates some of the aggregated stats",
    "start": "1928400",
    "end": "1936020"
  },
  {
    "text": "from the lower-level ones but doesn't have all the detail anymore so you have the global view but not all the detail",
    "start": "1936020",
    "end": "1941570"
  },
  {
    "text": "in the top and in the leaves of that tree you would have the very detailed instance level view but no longer the",
    "start": "1941570",
    "end": "1948080"
  },
  {
    "text": "global view that's the kind of hierarchical scaling there's some ways of doing limited types of horizontal",
    "start": "1948080",
    "end": "1954350"
  },
  {
    "text": "charting as well where you can like you have one service in one data center that's just 10,000 instances and",
    "start": "1954350",
    "end": "1961160"
  },
  {
    "text": "they cannot be scraped with one Prometheus you can configure 10 Prometheus and say like each of you you",
    "start": "1961160",
    "end": "1966350"
  },
  {
    "text": "only scrape a subset with like a more charting approach and then in a query you can still combine the data again",
    "start": "1966350",
    "end": "1971930"
  },
  {
    "text": "yeah I think we're probably out of time right yeah sorry cool thanks",
    "start": "1971930",
    "end": "1978820"
  },
  {
    "text": "[Applause]",
    "start": "1978820",
    "end": "1982950"
  }
]