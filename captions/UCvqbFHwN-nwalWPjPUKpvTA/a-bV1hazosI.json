[
  {
    "start": "0",
    "end": "13000"
  },
  {
    "text": "hi in this presentation i'm going to talk to you about how one can leverage legacy",
    "start": "160",
    "end": "5920"
  },
  {
    "text": "infrastructure that's diagnostic of kubernetes from within kubernetes to potentially discover",
    "start": "5920",
    "end": "11679"
  },
  {
    "text": "new physics my name is clemens and i'm a particle physicist at cern i'm",
    "start": "11679",
    "end": "17840"
  },
  {
    "start": "13000",
    "end": "63000"
  },
  {
    "text": "working on the cms experiment which is one of the experiments at the large hadron collider in the franco swiss area very close to",
    "start": "17840",
    "end": "25199"
  },
  {
    "text": "geneva and switzerland in this photo on the right hand side you can see me standing right next to the",
    "start": "25199",
    "end": "30240"
  },
  {
    "text": "accelerator so you can see in blue the dipole magnets these are there to actually bend the particle beams",
    "start": "30240",
    "end": "36079"
  },
  {
    "text": "so they go around in circle so and this circle actually has a circumference of 27 kilometers and it's a",
    "start": "36079",
    "end": "43120"
  },
  {
    "text": "kilometer underground and you can see here a lego model of the lhc obviously much",
    "start": "43120",
    "end": "48960"
  },
  {
    "text": "smaller but here's one of the experiments that's particularly interesting or dear to me which is here",
    "start": "48960",
    "end": "54160"
  },
  {
    "text": "in red that's the cms detector which is recording the collisions by the lhc",
    "start": "54160",
    "end": "59520"
  },
  {
    "text": "and i'm analyzing these data",
    "start": "59520",
    "end": "63359"
  },
  {
    "start": "63000",
    "end": "188000"
  },
  {
    "text": "so the field that i'm working is called high energy physics and short tap and it's all about trying to understand",
    "start": "65519",
    "end": "71280"
  },
  {
    "text": "the smallest building blocks of matter so as i said the particle detectors such as the cms detector",
    "start": "71280",
    "end": "76880"
  },
  {
    "text": "they record these collisions and the lhc actually provides up to 40 million collisions per second and the detector",
    "start": "76880",
    "end": "85360"
  },
  {
    "text": "you can imagine it as some kind of digital camera so that means it's actually taking",
    "start": "85360",
    "end": "91520"
  },
  {
    "text": "photos of of the collisions and that's happening 24 7 almost all year long",
    "start": "91520",
    "end": "97040"
  },
  {
    "text": "and you can see one of those photos that is taken or that has been taken on the right hand side so the particle",
    "start": "97040",
    "end": "103200"
  },
  {
    "text": "beams come in from both sides and then in the very center of the detector the particle beams are",
    "start": "103200",
    "end": "108560"
  },
  {
    "text": "brought into collision and then these sprays of particle develop here which are you know lots of particles",
    "start": "108560",
    "end": "116479"
  },
  {
    "text": "they're very close together and these particles that we are investigating actually um thinner than a hair",
    "start": "116479",
    "end": "123040"
  },
  {
    "text": "and the camera is actually a bit bigger than a standard digital camera it's uh so the cms",
    "start": "123040",
    "end": "129599"
  },
  {
    "text": "detector it weighs 14 000 tons as a height of 15 meters and a length of",
    "start": "129599",
    "end": "134720"
  },
  {
    "text": "21 meters and it has around 100 million channels so that's effectively around 100 megapixels",
    "start": "134720",
    "end": "142959"
  },
  {
    "text": "and these are particle um detectors i mean they're a bit better than your",
    "start": "142959",
    "end": "148160"
  },
  {
    "text": "digital camera which might also have 100 mega pixels because they can take 40 million seconds 40 million photos per second so because",
    "start": "148160",
    "end": "154319"
  },
  {
    "text": "there's one collision every 25 nanoseconds and this is so much data that we can actually not store this immediately",
    "start": "154319",
    "end": "160400"
  },
  {
    "text": "we have to um do something special we have to filter things as quickly as possible",
    "start": "160400",
    "end": "165519"
  },
  {
    "text": "largely based on hardware and then later on also based on on software and we can store up to thousands of such photos per second for",
    "start": "165519",
    "end": "172560"
  },
  {
    "text": "later analysis and this analysis since we're running all year long um is then a big data analysis so we are",
    "start": "172560",
    "end": "178800"
  },
  {
    "text": "actually analyzing tarot to pattern dates of petabytes of data using c plus plus",
    "start": "178800",
    "end": "184159"
  },
  {
    "text": "python and also shell scripts for the plumbing",
    "start": "184159",
    "end": "188640"
  },
  {
    "start": "188000",
    "end": "281000"
  },
  {
    "text": "for this kind of analysis we really need to have a big infrastructure and this is provided by the worldwide",
    "start": "189760",
    "end": "195360"
  },
  {
    "text": "lhc computing grid which consists of around 170 computer centers all around the world",
    "start": "195360",
    "end": "200720"
  },
  {
    "text": "you can see that on the right inside here so the data they're accumulated at sun so that's the tier zero and then they",
    "start": "200720",
    "end": "208239"
  },
  {
    "text": "are distributed and reconstructed and then distributed uh to the different sites all around the",
    "start": "208239",
    "end": "214400"
  },
  {
    "text": "world so you can see the tier one sites that have particularly high bandwidth links to the tier zero there you know for instance in russia",
    "start": "214400",
    "end": "221280"
  },
  {
    "text": "in italy and germany in the uk and the us and all kinds of places and",
    "start": "221280",
    "end": "227680"
  },
  {
    "text": "then there are around 160 further sites the so-called tier 2 sites that also you know have",
    "start": "227680",
    "end": "232959"
  },
  {
    "text": "parts of the data that's distributed so that we can send our analysis drops to the grid and analyze the",
    "start": "232959",
    "end": "239760"
  },
  {
    "text": "the the data where they're actually located and there are even more smaller batch farms so-called tier threes",
    "start": "239760",
    "end": "246159"
  },
  {
    "text": "they're then local and limited to those working at these for instance research institutions",
    "start": "246159",
    "end": "252000"
  },
  {
    "text": "and that and all these sites they're often already managed using kubernetes and at cern we have a slightly bigger",
    "start": "252000",
    "end": "258479"
  },
  {
    "text": "local cluster so uh we have a batch form that's uh running on ht condor a high throughput computing software for",
    "start": "258479",
    "end": "264479"
  },
  {
    "text": "batch computing and there we have 230 000 cores and that allows up to 150 000 jobs to",
    "start": "264479",
    "end": "271440"
  },
  {
    "text": "run simultaneously and that results in about or a maximum",
    "start": "271440",
    "end": "276639"
  },
  {
    "text": "of 1.4 million jobs that are completed per day",
    "start": "276639",
    "end": "282320"
  },
  {
    "start": "281000",
    "end": "370000"
  },
  {
    "text": "now the these um grid sites they are largely there for the you know large-scale",
    "start": "282320",
    "end": "287759"
  },
  {
    "text": "processing and most of these steps already um automated so you know from actually reconstructing",
    "start": "287759",
    "end": "294160"
  },
  {
    "text": "the data that we've taken and also having the associated simulation that we compare the data to",
    "start": "294160",
    "end": "300800"
  },
  {
    "text": "a process this is all automated but the part that is still pretty manual is actually the so-called high-level",
    "start": "300800",
    "end": "306800"
  },
  {
    "text": "physics analysis so this is some an analysis that tries to address a specific physics question so for",
    "start": "306800",
    "end": "312960"
  },
  {
    "text": "instance when we were searching for the higgs boson there were several analysis trying to find the higgs boson eventually it",
    "start": "312960",
    "end": "318560"
  },
  {
    "text": "worked out but this is something that is a somewhat manual process at this point because it needs a lot of",
    "start": "318560",
    "end": "324320"
  },
  {
    "text": "very detailed inputs that come that are provided from several groups and individuals from",
    "start": "324320",
    "end": "331039"
  },
  {
    "text": "within the collaboration so so for instance calibration constants and factors and corrections a certain",
    "start": "331039",
    "end": "337600"
  },
  {
    "text": "uncertainties etc they are all uh provided by the group and one has to collect them in order then to",
    "start": "337600",
    "end": "344639"
  },
  {
    "text": "provide or perform the final analysis so this is pretty uh complicated and therefore",
    "start": "344639",
    "end": "351680"
  },
  {
    "text": "challenging task to achieve and we can actually you know keep track",
    "start": "351680",
    "end": "357039"
  },
  {
    "text": "of all this thanks to version control so we're using git and so we can capture the implementation",
    "start": "357039",
    "end": "363440"
  },
  {
    "text": "and to execute these jobs we can also make use of software images so effectively docker containers in this case",
    "start": "363440",
    "end": "371360"
  },
  {
    "start": "370000",
    "end": "451000"
  },
  {
    "text": "the challenging part is now once you've captured the software so you have the software under version",
    "start": "371360",
    "end": "377199"
  },
  {
    "text": "control and you then build a container for instance using continuous integration uh we're using github mostly for that so",
    "start": "377199",
    "end": "384400"
  },
  {
    "text": "you have all the dependencies included in your container then you still need to know how you actually execute",
    "start": "384400",
    "end": "389840"
  },
  {
    "text": "this container or this image so we also need to capture the commands that's you know also doable but the big",
    "start": "389840",
    "end": "396800"
  },
  {
    "text": "challenge lies in actually capturing the full workflow so how do we connect the individual analysis steps",
    "start": "396800",
    "end": "402479"
  },
  {
    "text": "and for those there are several tools under investigation and some of them are already used by smaller groups i just want to point out",
    "start": "402479",
    "end": "407840"
  },
  {
    "text": "a few here so that's for instance a project at cern that's called rihanna that's a common workflow language",
    "start": "407840",
    "end": "413840"
  },
  {
    "text": "implementation it has a focus but it's also possible to use it for other things then",
    "start": "413840",
    "end": "420160"
  },
  {
    "text": "there's the adage which really focused on on uh only and then",
    "start": "420160",
    "end": "425599"
  },
  {
    "text": "but they're also tools that are very you know general purpose for instance luigi that has originally been developed",
    "start": "425599",
    "end": "432160"
  },
  {
    "text": "by spotify um and that's also being used by a few groups and i asked myself the question can we",
    "start": "432160",
    "end": "438960"
  },
  {
    "text": "actually do um this kind of high energy physics workflow processing in a cloud native way and for that for workflows and",
    "start": "438960",
    "end": "445680"
  },
  {
    "text": "kubernetes i found argo workflows and this seems to be a really nice tool to use",
    "start": "445680",
    "end": "452560"
  },
  {
    "start": "451000",
    "end": "551000"
  },
  {
    "text": "so now before i actually um go into implementing a workflow um from my hand of high energy physics",
    "start": "452560",
    "end": "459199"
  },
  {
    "text": "in argo i first wanted to talk to you about my cluster so at cern",
    "start": "459199",
    "end": "464400"
  },
  {
    "text": "we can actually provision clusters on-prem with openstack and then we have some",
    "start": "464400",
    "end": "471280"
  },
  {
    "text": "additional plugins that are somewhat certain specific so or high-energy physics specific so",
    "start": "471280",
    "end": "476479"
  },
  {
    "text": "there's a certain vm file system so that we can efficiently cache the software which is",
    "start": "476479",
    "end": "481520"
  },
  {
    "text": "you know quite a lot so it's actually gigabytes of software that we um need to use um for and um for the block",
    "start": "481520",
    "end": "488560"
  },
  {
    "text": "storage we have uh file system so that's that's uh in particular for the big data sets there for which we use eos",
    "start": "488560",
    "end": "495039"
  },
  {
    "text": "my cluster itself here is actually pretty small it has um four nodes with four cores each and",
    "start": "495039",
    "end": "500960"
  },
  {
    "text": "these have eight gigabytes of ram i have some object storage and also some block storage so as three",
    "start": "500960",
    "end": "507680"
  },
  {
    "text": "and seven s and 300 gigabytes are sufficient uh to store for instance the um output",
    "start": "507680",
    "end": "513120"
  },
  {
    "text": "um so that's the final product uh from the big data sets uh on on on disk for for later",
    "start": "513120",
    "end": "519760"
  },
  {
    "text": "immense you know analysis this cluster runs on kubernetes uh version 118.2",
    "start": "519760",
    "end": "526560"
  },
  {
    "text": "and um with this what happens inside the cluster i actually managed to buy githubs that's um and here i'm using argo cd",
    "start": "526560",
    "end": "535040"
  },
  {
    "text": "i can also put my secrets on the version control by using sops there's a barbicon barrican modification",
    "start": "535040",
    "end": "541279"
  },
  {
    "text": "so that's the secret manager for openstack and then i can deploy this using customize sops",
    "start": "541279",
    "end": "547040"
  },
  {
    "text": "there's a plugin for that with argo ct",
    "start": "547040",
    "end": "551920"
  },
  {
    "start": "551000",
    "end": "693000"
  },
  {
    "text": "now let's actually see uh how um you know a typical workflow would look",
    "start": "552800",
    "end": "558240"
  },
  {
    "text": "like when when when using argo and and here this particular example is",
    "start": "558240",
    "end": "564720"
  },
  {
    "text": "about searching for a new signal in the data triggered the workflow you can see it here is already running there's a blue",
    "start": "564720",
    "end": "571360"
  },
  {
    "text": "spinning wheel that shows that this is the currently um active step or the one that's not finished the",
    "start": "571360",
    "end": "577440"
  },
  {
    "text": "yellow ones are where the container is currently provisioning we're preparing a deer for the um you",
    "start": "577440",
    "end": "582640"
  },
  {
    "text": "know to get started for the output and then the workflow splits into several steps that are",
    "start": "582640",
    "end": "588560"
  },
  {
    "text": "related to the different processes that we're analyzing here and you can see that",
    "start": "588560",
    "end": "596800"
  },
  {
    "text": "we immediately scatter to a large number of jobs here so that's a nice feature of argo that",
    "start": "596800",
    "end": "602480"
  },
  {
    "text": "you don't have to determine how many jobs you will have in the end but you can dynamically generate this",
    "start": "602480",
    "end": "607680"
  },
  {
    "text": "and also you can directly observe what is happening which part of the",
    "start": "607680",
    "end": "613680"
  },
  {
    "text": "workflow is currently running you know what is the status of each of the steps",
    "start": "613680",
    "end": "619680"
  },
  {
    "text": "and you can also directly observe the logs so these are streamed uh directly into the user interface you",
    "start": "619680",
    "end": "626320"
  },
  {
    "text": "can see there's lots of shell scripting here the last command was a python script",
    "start": "626320",
    "end": "631519"
  },
  {
    "text": "and this workflow is already done and after this highly parallel step",
    "start": "631519",
    "end": "637760"
  },
  {
    "text": "that's usually the one that's computational most intensive we can gather our input so we have a couple of",
    "start": "637760",
    "end": "645120"
  },
  {
    "text": "merge steps then we can again scatter out to um steps where we for instance have to uh take",
    "start": "645120",
    "end": "650800"
  },
  {
    "text": "into account systematic uncertainties that use the same inputs but run a different variation so that's what something you",
    "start": "650800",
    "end": "657760"
  },
  {
    "text": "can see at the later stages and then at the very end we it all boils down to a single um step",
    "start": "657760",
    "end": "664800"
  },
  {
    "text": "that provides us with a final result that's the one that we're looking at here",
    "start": "664800",
    "end": "669920"
  },
  {
    "text": "and you can again see the logs this time they're a bit more interesting than before",
    "start": "669920",
    "end": "675040"
  },
  {
    "text": "so you can see there's a fit done to the data different processes here investigated",
    "start": "675040",
    "end": "681839"
  },
  {
    "text": "and then at the end we actually get the final result which is in this case a plot called postfit.pdf",
    "start": "681839",
    "end": "689839"
  },
  {
    "text": "all right that's the demo i cut it down from four minutes to two",
    "start": "695440",
    "end": "700959"
  },
  {
    "text": "minutes only and i told you at the very end that there's a plot that is produced by a fit in actually in",
    "start": "700959",
    "end": "709120"
  },
  {
    "text": "the end and um we're searching for new physics here and what you can see on the left hand",
    "start": "709120",
    "end": "716000"
  },
  {
    "text": "side in in this plot uh in the different colors the different processes that actually um you know would end up",
    "start": "716000",
    "end": "722639"
  },
  {
    "text": "in this particular signal region where we're searching for the signal and in the very center in dark green you see",
    "start": "722639",
    "end": "728800"
  },
  {
    "text": "something that's labeled as a signal and this is what a potential new physics signal would look like",
    "start": "728800",
    "end": "734560"
  },
  {
    "text": "and the black markers are actually the data so you can see already pre-fit so before any uh adjustment",
    "start": "734560",
    "end": "741760"
  },
  {
    "text": "uh the simulation that we produced matches nicely with the data",
    "start": "741760",
    "end": "747200"
  },
  {
    "text": "and now the question that we're asking us in a typical physics analysis is there any new physics signal right so we optimized",
    "start": "747200",
    "end": "754079"
  },
  {
    "text": "so that we can in simulation actually see the signal now we have to use the data to test if there is a new physics signal",
    "start": "754079",
    "end": "760320"
  },
  {
    "text": "so we're running this fit that allows um the the individual components to adjust to the data so the black",
    "start": "760320",
    "end": "766560"
  },
  {
    "text": "markers won't work but everything else is allowed to move and on the right hand side you actually see that",
    "start": "766560",
    "end": "771600"
  },
  {
    "text": "post fit so after the fit uh the data do not support the presence of the signal so in this case we have not discovered",
    "start": "771600",
    "end": "778800"
  },
  {
    "text": "physics and that's actually the case most of the time but i mean that's no reason to give up and",
    "start": "778800",
    "end": "784399"
  },
  {
    "text": "um no still confident that we'll find new physics in the near future",
    "start": "784399",
    "end": "790240"
  },
  {
    "start": "788000",
    "end": "853000"
  },
  {
    "text": "now let me take a step back and look again at the example workflow here so that's a screenshot from the demo",
    "start": "790240",
    "end": "796959"
  },
  {
    "text": "that we just saw and i should point out that uh you know this is really just a demo a realistic",
    "start": "796959",
    "end": "802639"
  },
  {
    "text": "physics analysis much more complex usually when i run you know the first step that",
    "start": "802639",
    "end": "808079"
  },
  {
    "text": "is here uh delightfully parallel as you can see so that has the large amounts of parallel jobs",
    "start": "808079",
    "end": "815839"
  },
  {
    "text": "i would usually run several hundreds of them and they would each run for several hours if not even days",
    "start": "815839",
    "end": "822000"
  },
  {
    "text": "depending on the complexity and the size of the data set that i'm analyzing what's what's really nice here about",
    "start": "822000",
    "end": "828320"
  },
  {
    "text": "argos that i actually uh could scatter out dynamically so i could create jobs i could define at the",
    "start": "828320",
    "end": "834480"
  },
  {
    "text": "very beginning i want um you know five jobs for this particular data set i want a certain other number for",
    "start": "834480",
    "end": "840160"
  },
  {
    "text": "for another step and argo does this and then at the end i can collect the information again and",
    "start": "840160",
    "end": "845519"
  },
  {
    "text": "as i mentioned in the video demo um there can be later parallel steps that i can pick up again",
    "start": "845519",
    "end": "851760"
  },
  {
    "text": "so this works really nicely now what isn't so nice is that life's a bit",
    "start": "851760",
    "end": "857760"
  },
  {
    "start": "853000",
    "end": "891000"
  },
  {
    "text": "unfair and realistic physics analysis workflow can actually not be run on my cluster",
    "start": "857760",
    "end": "862959"
  },
  {
    "text": "right so i mean my cluster has 16 cores and the hd condor batch farm it has 230 000",
    "start": "862959",
    "end": "870560"
  },
  {
    "text": "cores and 970 terabytes of memory so the question that i asked myself if if i want to be",
    "start": "870560",
    "end": "876720"
  },
  {
    "text": "able to run realistic uh cloud native workflows with argo um can i actually",
    "start": "876720",
    "end": "881760"
  },
  {
    "text": "somehow make use of these cars in hd condor and i you know because with",
    "start": "881760",
    "end": "887519"
  },
  {
    "text": "the 16 cores i said i i won't get very far and i found the solution to that and",
    "start": "887519",
    "end": "894320"
  },
  {
    "start": "891000",
    "end": "959000"
  },
  {
    "text": "that's what i call here the hd condor operator so the idea is to introduce an",
    "start": "894320",
    "end": "899600"
  },
  {
    "text": "htc job custom resource definition you can see the definition on the left hand side here so",
    "start": "899600",
    "end": "906240"
  },
  {
    "text": "there's an api version the kind is the custom resource definition and",
    "start": "906240",
    "end": "911600"
  },
  {
    "text": "then you know the name is htc job and the idea of this job is that it",
    "start": "911600",
    "end": "916959"
  },
  {
    "text": "should mimic a classical kubernetes job so it needs to somehow reflect uh running failed and succeeded status i",
    "start": "916959",
    "end": "923760"
  },
  {
    "text": "can do that via status properties of this resource and in addition i want",
    "start": "923760",
    "end": "929199"
  },
  {
    "text": "to be able to understand what's happening inside the hd conda batch cluster so i",
    "start": "929199",
    "end": "934240"
  },
  {
    "text": "added additional fields such as the job ids here that reflect the actual job ids within the hd",
    "start": "934240",
    "end": "940800"
  },
  {
    "text": "conda cluster so that i can also you know monitor and interact by using these job ids and this",
    "start": "940800",
    "end": "948079"
  },
  {
    "text": "is actually not work uh only by me but this has largely been done by tatters barack is a bioinformatics",
    "start": "948079",
    "end": "954480"
  },
  {
    "text": "student at vilnius university who interned with me at cern earlier this year",
    "start": "954480",
    "end": "960480"
  },
  {
    "start": "959000",
    "end": "1103000"
  },
  {
    "text": "now let's get back into um let's get down into the implementation",
    "start": "960480",
    "end": "965519"
  },
  {
    "text": "so when you have a custom resource definition you need to do something that acts on it and the operator uh so in kubernetes that's",
    "start": "965519",
    "end": "972720"
  },
  {
    "text": "done by an operator so the um and we looked around a bit what kind of operators we could use we looked into",
    "start": "972720",
    "end": "978880"
  },
  {
    "text": "metacontroller and then we found the operator sdk and that makes it really easy to get a kubernetes operator implemented",
    "start": "978880",
    "end": "985440"
  },
  {
    "text": "and running in the first place so we went with that now this operator needs to know about um",
    "start": "985440",
    "end": "991199"
  },
  {
    "text": "hd conda in one way or the other so i built a docker container that contains the hd condor client",
    "start": "991199",
    "end": "997120"
  },
  {
    "text": "and in addition we need to authenticate to this cluster so to hd condor and that's done",
    "start": "997120",
    "end": "1003680"
  },
  {
    "text": "via kerberos and i can create a kerberos token via the secrets that i can store",
    "start": "1003680",
    "end": "1010079"
  },
  {
    "text": "in my cluster the hd con operator itself is actually installed into this container",
    "start": "1010079",
    "end": "1016160"
  },
  {
    "text": "and in addition what this operator needs to know it needs to translate the information from the drop spec",
    "start": "1016160",
    "end": "1023680"
  },
  {
    "text": "into something that can be executed on hd condor so here um in particular the jobs are not",
    "start": "1023680",
    "end": "1029678"
  },
  {
    "text": "executed using a docker when running on hd condor but they're executed using singularity",
    "start": "1029679",
    "end": "1036000"
  },
  {
    "text": "so there's a translation step that um translates the image spec and the comma",
    "start": "1036000",
    "end": "1042480"
  },
  {
    "text": "the the script into something that is then effectively a singularity command with a",
    "start": "1042480",
    "end": "1047678"
  },
  {
    "text": "script attached to it and if we now look at the implementation in the boxes below",
    "start": "1047679",
    "end": "1052960"
  },
  {
    "text": "you can see that in my cluster i install the htc operator that knows about",
    "start": "1052960",
    "end": "1058000"
  },
  {
    "text": "the about hd conor and has my credentials when i create an htc job so it watches",
    "start": "1058000",
    "end": "1065520"
  },
  {
    "text": "the operator watches for them to be created and as soon as there is one it will use the api of htconder to to",
    "start": "1065520",
    "end": "1072240"
  },
  {
    "text": "submit jobs to the batch cluster then these batch drops on on hd condor",
    "start": "1072240",
    "end": "1077919"
  },
  {
    "text": "side will be executed and uh in addition to having the api implemented here",
    "start": "1077919",
    "end": "1083039"
  },
  {
    "text": "i also um have at the very end of my job uh web hook via cloud events that actually",
    "start": "1083039",
    "end": "1088400"
  },
  {
    "text": "tell my uh operator that a job is done and which one is the good job that is done in particular as well as",
    "start": "1088400",
    "end": "1095520"
  },
  {
    "text": "this exit code so that i can reduce the number of api queries from the htc operator to the batch cluster",
    "start": "1095520",
    "end": "1102880"
  },
  {
    "text": "okay so this is the htc job definition i created a job and then it submits something now i have to um",
    "start": "1104240",
    "end": "1109919"
  },
  {
    "text": "make this work with argo and the nice thing is that argo can actually manage any kind of",
    "start": "1109919",
    "end": "1115039"
  },
  {
    "text": "kubernetes resources so here's a step from from argo it's called generate batch",
    "start": "1115039",
    "end": "1120400"
  },
  {
    "text": "it has a couple of parameters among them for instance the number of jobs and then i can say i want to have a",
    "start": "1120400",
    "end": "1126000"
  },
  {
    "text": "resource and the action for this record and resource should be to create it and then i at the very",
    "start": "1126000",
    "end": "1132400"
  },
  {
    "text": "bottom you see the beginning of the manifest and you can see that the kind of resource that i want to be created is",
    "start": "1132400",
    "end": "1138799"
  },
  {
    "text": "an htc job so now what i need to do in addition is",
    "start": "1138799",
    "end": "1145280"
  },
  {
    "text": "i need to define some success condition that determines whether my jobs are all done or if any",
    "start": "1145280",
    "end": "1152240"
  },
  {
    "text": "of them failed so i define a success condition that the number of so the status field in my",
    "start": "1152240",
    "end": "1158240"
  },
  {
    "text": "custom resource definition needs to be equal to the number of jobs that i started with",
    "start": "1158240",
    "end": "1164000"
  },
  {
    "text": "and if any of the jobs failed then also this job is supposed to fail so that means now i can actually move",
    "start": "1164000",
    "end": "1170320"
  },
  {
    "text": "the long running steps into hd condor directly from my argo workflow",
    "start": "1170320",
    "end": "1175840"
  },
  {
    "start": "1175000",
    "end": "1468000"
  },
  {
    "text": "so what i'm going to do now is i'm going to run the same workflow as before",
    "start": "1175840",
    "end": "1180880"
  },
  {
    "text": "but moving this first step to hd condor and i'm going to speed this up a bit but",
    "start": "1180880",
    "end": "1186640"
  },
  {
    "text": "let's see how this goes okay so i'm submitting my job from the command line you can see it's",
    "start": "1186640",
    "end": "1192960"
  },
  {
    "text": "running the prepare deer step as before in the ui and now",
    "start": "1192960",
    "end": "1199520"
  },
  {
    "text": "there's actually a difference here you can see two spinning wheels after each other and this step",
    "start": "1199520",
    "end": "1204559"
  },
  {
    "text": "is actually now running on the batch so we're creating inside the cluster the custom resource",
    "start": "1204559",
    "end": "1210320"
  },
  {
    "text": "and therefore the operator will submit drops to the batch system and you can see that um here on the left-hand side",
    "start": "1210320",
    "end": "1215840"
  },
  {
    "text": "is already happening and also on the right-hand side there they're currently active zero drop id is none and as soon as the drops",
    "start": "1215840",
    "end": "1223919"
  },
  {
    "text": "are queued you actually see active four and you see the individual drop ids which you can compare to the left hand",
    "start": "1223919",
    "end": "1230000"
  },
  {
    "text": "side now the inside argo i defined a failure",
    "start": "1230000",
    "end": "1235039"
  },
  {
    "text": "condition a success condition so all the jobs need to be complete successfully so for things to move on",
    "start": "1235039",
    "end": "1242720"
  },
  {
    "text": "and i can actually investigate this in more detail if i look at my hd condor operator within",
    "start": "1242720",
    "end": "1250080"
  },
  {
    "text": "argo so here i'm looking um at the argo cd uh ui where i also see the logs",
    "start": "1250080",
    "end": "1256640"
  },
  {
    "text": "of the other parts and you can for instance here see",
    "start": "1256640",
    "end": "1262000"
  },
  {
    "text": "the job definition i submitted a job to big bird 10 [Music]",
    "start": "1262000",
    "end": "1267280"
  },
  {
    "text": "so this is the manifest for the job submission you can see there are",
    "start": "1267280",
    "end": "1272559"
  },
  {
    "text": "several of them because i actually submitted a total of four batches i also",
    "start": "1272559",
    "end": "1277679"
  },
  {
    "text": "have lots of um i have four custom resources now defined and now let's monitor what's happening",
    "start": "1277679",
    "end": "1283440"
  },
  {
    "text": "so the first job actually here already completed i can again check here the",
    "start": "1283440",
    "end": "1288559"
  },
  {
    "text": "htc operator now cloud events actually send us the job id and the return code from",
    "start": "1288559",
    "end": "1296919"
  },
  {
    "text": "generatepatchmc2 and that also then automatically triggers a transfer of the data from the batch",
    "start": "1296919",
    "end": "1303679"
  },
  {
    "text": "cluster into the pod file system and then directly into the shared storage that i",
    "start": "1303679",
    "end": "1310080"
  },
  {
    "text": "can use in subsequent steps inside the cluster again",
    "start": "1310080",
    "end": "1314960"
  },
  {
    "text": "now we just saw briefly uh the grafana monitoring but more interesting here is now",
    "start": "1315120",
    "end": "1320720"
  },
  {
    "text": "actually that also the first jobs are succeeding um in in for the cr",
    "start": "1320720",
    "end": "1326880"
  },
  {
    "text": "that i was monitoring and again we see that in the logs and grafana in green here",
    "start": "1326880",
    "end": "1334640"
  },
  {
    "text": "the drops that are running in yellow is a bit behind the ones that have been scheduled you",
    "start": "1334640",
    "end": "1341039"
  },
  {
    "text": "can actually see that all the jobs are running already now we're already at two succeeded jobs",
    "start": "1341039",
    "end": "1346799"
  },
  {
    "text": "and two active jobs and on the left-hand side you can see the total um 15 jobs seven have",
    "start": "1346799",
    "end": "1352080"
  },
  {
    "text": "completed eight are running um i can actually see now that uh 159.3",
    "start": "1352080",
    "end": "1358720"
  },
  {
    "text": "is the one that just completed and now",
    "start": "1358720",
    "end": "1365280"
  },
  {
    "text": "immediately after now we already had succeeded three that's another job that completed for",
    "start": "1365280",
    "end": "1372640"
  },
  {
    "text": "this custom resource that we were watching here and uh in a second or so will be it",
    "start": "1372640",
    "end": "1378559"
  },
  {
    "text": "succeeded for and that would automatically trigger the workflow to continue and you can see that at the",
    "start": "1378559",
    "end": "1383600"
  },
  {
    "text": "bottom here so we're already now moving on and all the subsequent steps here",
    "start": "1383600",
    "end": "1389280"
  },
  {
    "text": "are now happening inside the cluster and you see now that 15 out of 15 jobs have been completed",
    "start": "1389280",
    "end": "1394559"
  },
  {
    "text": "and again in the logs we see that all the data have been transferred and also",
    "start": "1394559",
    "end": "1400320"
  },
  {
    "text": "there's no success statement for the custom resource so argo continues",
    "start": "1400320",
    "end": "1405919"
  },
  {
    "text": "its job but the custom resource stays in place and all the other steps or as before um",
    "start": "1405919",
    "end": "1412720"
  },
  {
    "text": "pretty fast so there wasn't much to be done the images had been pulled already from the previous demo",
    "start": "1412720",
    "end": "1418320"
  },
  {
    "text": "so now we are already at the last step where we make the plot and that then concludes the workflow",
    "start": "1418320",
    "end": "1426320"
  },
  {
    "text": "now at the very end there's i said the custom resources remain so if i delete",
    "start": "1426320",
    "end": "1431520"
  },
  {
    "text": "the workflow also the resources that were created will be deleted",
    "start": "1431520",
    "end": "1436559"
  },
  {
    "text": "so you will now see on the top left that this will actually remove the drops from",
    "start": "1436559",
    "end": "1442320"
  },
  {
    "text": "the queue i left them there in case i wanted to transfer some data but i've done that automatically so now i'm at zero almost for the jobs",
    "start": "1442320",
    "end": "1450080"
  },
  {
    "text": "that are scheduled or even completed on the right-hand side you saw that now we get an error no more",
    "start": "1450080",
    "end": "1456400"
  },
  {
    "text": "htc drops are available and also the jobs are removed",
    "start": "1456400",
    "end": "1461919"
  },
  {
    "start": "1468000",
    "end": "1609000"
  },
  {
    "text": "all right so this worked really well as i said i sped up things a bit so this in total would have taken",
    "start": "1469440",
    "end": "1475039"
  },
  {
    "text": "something like 30 minutes just because um i'm not the only one running jobs inside the the cluster",
    "start": "1475039",
    "end": "1480720"
  },
  {
    "text": "um so and hd condor so such a cluster is not really made for short running jobs but the",
    "start": "1480720",
    "end": "1486400"
  },
  {
    "text": "short queue is actually something like 15 minutes and the default is something like eight hours so it's it's okay that things take some",
    "start": "1486400",
    "end": "1493039"
  },
  {
    "text": "time to to um to be scared to actually run um but if i run a real physics analysis you know",
    "start": "1493039",
    "end": "1500000"
  },
  {
    "text": "that would be perfectly fine with me but uh just to to close out um i i think you know what i managed to",
    "start": "1500000",
    "end": "1507440"
  },
  {
    "text": "show you is here that i um leveraged legacy infrastructure so you",
    "start": "1507440",
    "end": "1512480"
  },
  {
    "text": "know kubernetes agnostic computing infrastructure by means of a kubernetes custom resource definition",
    "start": "1512480",
    "end": "1518080"
  },
  {
    "text": "that is combined with an operator and the operator concept is extremely powerful for this purpose",
    "start": "1518080",
    "end": "1523919"
  },
  {
    "text": "and also i've i found that cloud native high energy physics workflows are possible using argos so",
    "start": "1523919",
    "end": "1530480"
  },
  {
    "text": "that's really nice and now there are a couple of steps still to be done so the htc operator",
    "start": "1530480",
    "end": "1536080"
  },
  {
    "text": "actually needs to be made a bit more flexible because when you choose cues in hd condor you can also directly translate",
    "start": "1536080",
    "end": "1542559"
  },
  {
    "text": "for instance compute requirements um so how long should your job run et cetera and so you're going to need to",
    "start": "1542559",
    "end": "1550720"
  },
  {
    "text": "translate what is there in kubernetes to something that is understandable or",
    "start": "1550720",
    "end": "1556240"
  },
  {
    "text": "understood uh by hd condor and of course it would be nice uh sooner or",
    "start": "1556240",
    "end": "1562320"
  },
  {
    "text": "later to also uh make use of these 170 grid sites that are available for me um",
    "start": "1562320",
    "end": "1568320"
  },
  {
    "text": "to use so the wlcg um so that's um on the roadmap uh but",
    "start": "1568320",
    "end": "1574640"
  },
  {
    "text": "actually if you want to see something more on this topic there there is a presentation by alessandra forte and lucas heinrich also",
    "start": "1574640",
    "end": "1581600"
  },
  {
    "text": "at cubicon here who actually talk about how they have their own grid site on the",
    "start": "1581600",
    "end": "1586880"
  },
  {
    "text": "on the laptop so i could in principle run my workflow in kubernetes then send it to the grid",
    "start": "1586880",
    "end": "1592480"
  },
  {
    "text": "where the grid would be their local laptop so that's fun isn't it all right so thanks",
    "start": "1592480",
    "end": "1598960"
  },
  {
    "text": "um to titles again uh for for his great work um i would also like to thank the cloud containers team at cern so that",
    "start": "1598960",
    "end": "1604960"
  },
  {
    "text": "means in particular thomas ricardo and spiros and um also um lucas and now i'm very happy to take your",
    "start": "1604960",
    "end": "1611679"
  },
  {
    "start": "1609000",
    "end": "1733000"
  },
  {
    "text": "questions so thank you for your attention",
    "start": "1611679",
    "end": "1615760"
  },
  {
    "text": "hello so um thanks again for for joining",
    "start": "1619600",
    "end": "1626000"
  },
  {
    "text": "i got a lot of questions in the chat i already tried to answer a couple of them",
    "start": "1626000",
    "end": "1632640"
  },
  {
    "text": "um i'll just go through them uh in the following probably the those that are you know",
    "start": "1632640",
    "end": "1639360"
  },
  {
    "text": "generally most uh interesting for other things you can uh you know chat with me in the cncf uh",
    "start": "1639360",
    "end": "1646080"
  },
  {
    "text": "slack afterwards um you can find me there i'll be monitoring the the machine learning um channel so",
    "start": "1646080",
    "end": "1653279"
  },
  {
    "text": "i mean that was a very uh general question on on you know general um information on cern",
    "start": "1653279",
    "end": "1658880"
  },
  {
    "text": "we actually have a our own top level um domain so you can just uh go to home.cern cern",
    "start": "1658880",
    "end": "1666159"
  },
  {
    "text": "and from there you find information in particular on uh the the cern experiments so the large",
    "start": "1666159",
    "end": "1672880"
  },
  {
    "text": "hadron collider and also the experiments at the large iron collider but also smaller experiments that are taking place",
    "start": "1672880",
    "end": "1679360"
  },
  {
    "text": "um that's then there was another question um on how",
    "start": "1679360",
    "end": "1686159"
  },
  {
    "text": "many kubernetes clusters are being used at cern and how many nodes are in the biggest cluster",
    "start": "1686159",
    "end": "1691760"
  },
  {
    "text": "i actually don't have a concrete answer to that um i mean i i know that there are",
    "start": "1691760",
    "end": "1697840"
  },
  {
    "text": "several hundred clusters but i think that most of them actually rather small and",
    "start": "1697840",
    "end": "1707039"
  },
  {
    "text": "i know that at least one of the clusters has around 1 000 uh cores so you usually have um",
    "start": "1707039",
    "end": "1713919"
  },
  {
    "text": "you know it depends on how these are set up uh some some you know we have different",
    "start": "1713919",
    "end": "1719760"
  },
  {
    "text": "flavors of of the vms that we use of the nodes so some of them have eight cores or others have four so",
    "start": "1719760",
    "end": "1727600"
  },
  {
    "text": "the standard would be four with eight gigabytes of ram",
    "start": "1727600",
    "end": "1732720"
  },
  {
    "text": "um then there was another question on the storage technology that we're using to",
    "start": "1732720",
    "end": "1738240"
  },
  {
    "start": "1733000",
    "end": "1826000"
  },
  {
    "text": "store all this data um and we're actually largely using eos",
    "start": "1738240",
    "end": "1743440"
  },
  {
    "text": "and i put the link to eos into the into the q a window you can also find it",
    "start": "1743440",
    "end": "1750240"
  },
  {
    "text": "by just searching for eos and cern so that's a technology that's been developed at cern",
    "start": "1750240",
    "end": "1757919"
  },
  {
    "text": "i think it's generally available and for long term preservation we",
    "start": "1757919",
    "end": "1763679"
  },
  {
    "text": "rely on tapes a lot so for data that aren't actively used they actually um copied",
    "start": "1763679",
    "end": "1769520"
  },
  {
    "text": "over to tape and then um you know archived and if you want to run on them this",
    "start": "1769520",
    "end": "1775200"
  },
  {
    "text": "what happened to me and the other day then it can easily take a few days until they're actually staged out to disc and this archiving",
    "start": "1775200",
    "end": "1783760"
  },
  {
    "text": "does not only happen at some but it also happens on the different grid sides so you can be uh lucky um or unlucky",
    "start": "1783760",
    "end": "1793120"
  },
  {
    "text": "then there was another question on uh having a link to the presentation so um i uploaded the presentation a pdf",
    "start": "1793679",
    "end": "1800880"
  },
  {
    "text": "of that um to the sket.com entry so you should be able to find it there and also edit the pdf with",
    "start": "1800880",
    "end": "1807200"
  },
  {
    "text": "the two links to the demos i just uploaded them to youtube because i wasn't sure",
    "start": "1807200",
    "end": "1813200"
  },
  {
    "text": "if the quality would be good enough but i think they showed up nicely so um yeah but",
    "start": "1813200",
    "end": "1819120"
  },
  {
    "text": "still if you want to rewatch them specifically um feel free to do though you can find them there",
    "start": "1819120",
    "end": "1824960"
  },
  {
    "text": "with the links um okay and then there are a couple of questions on the",
    "start": "1824960",
    "end": "1832640"
  },
  {
    "start": "1826000",
    "end": "1983000"
  },
  {
    "text": "workflows so the question was you know what is so specific",
    "start": "1832640",
    "end": "1838080"
  },
  {
    "text": "about the the uh high energy physics that that we might need uh separate tools so",
    "start": "1838080",
    "end": "1845440"
  },
  {
    "text": "in general i would say that we do not",
    "start": "1845440",
    "end": "1851600"
  },
  {
    "text": "really need dedicated tools so we could actually work with the existing tools but um yeah there are a lot of there's a lot",
    "start": "1851600",
    "end": "1859760"
  },
  {
    "text": "of custom tooling we're using also rather old systems um so you know we're always",
    "start": "1859760",
    "end": "1866080"
  },
  {
    "text": "a couple of uh years behind with the latest greatest software etc",
    "start": "1866080",
    "end": "1871600"
  },
  {
    "text": "um so uh you know we can work around that uh with containers uh but still you know that's the the the",
    "start": "1871600",
    "end": "1878240"
  },
  {
    "text": "custom tooling then um you know we you have to um do um authentication so we we have uh",
    "start": "1878240",
    "end": "1884720"
  },
  {
    "text": "certificates then we use kerberos in other places and all that all that and um so i'm",
    "start": "1884720",
    "end": "1892240"
  },
  {
    "text": "say one of the few people actually trying to um you know make these these",
    "start": "1892240",
    "end": "1899919"
  },
  {
    "text": "energy physics workflows accessible for uh people within the community and for that i",
    "start": "1899919",
    "end": "1906640"
  },
  {
    "text": "think it's really important to give them a really good experience so to make it as easy as possible for them",
    "start": "1906640",
    "end": "1912640"
  },
  {
    "text": "uh to uh you know get started with let's say cloud native or automated",
    "start": "1912640",
    "end": "1920080"
  },
  {
    "text": "workflows and also actually stay as close as possible to",
    "start": "1920080",
    "end": "1925440"
  },
  {
    "text": "let's say some some industry or open",
    "start": "1925440",
    "end": "1930480"
  },
  {
    "text": "source solution that can be used because um you know otherwise i would",
    "start": "1930480",
    "end": "1936399"
  },
  {
    "text": "have to do all the maintenance and that's uh probably not what i want to do but um yeah so",
    "start": "1936399",
    "end": "1941519"
  },
  {
    "text": "the custom tooling is mostly just around um for instance um the scatter gather",
    "start": "1941519",
    "end": "1947600"
  },
  {
    "text": "paradigm so we you know we have a couple of data sets but a data set actually contains a couple of um a thousand files and then",
    "start": "1947600",
    "end": "1955440"
  },
  {
    "text": "you want to parallelize this in a in a dynamic way so and then afterwards you wanna",
    "start": "1955440",
    "end": "1960960"
  },
  {
    "text": "uh collect this again but you don't want to collect all the files in one step but you want to do this in some kind of a",
    "start": "1960960",
    "end": "1966559"
  },
  {
    "text": "cascade and that's uh something that's maybe a bit more um have specific so if this",
    "start": "1966559",
    "end": "1972080"
  },
  {
    "text": "could be abstracted away from the user so that the physics analyst that's",
    "start": "1972080",
    "end": "1977279"
  },
  {
    "text": "something useful",
    "start": "1977279",
    "end": "1980880"
  },
  {
    "text": "okay i mean about the choice of the workflow tool",
    "start": "1982799",
    "end": "1989120"
  },
  {
    "start": "1983000",
    "end": "2015000"
  },
  {
    "text": "i just went with argo because i had you know i found it a few years back and and i was actually curious like",
    "start": "1989360",
    "end": "1996399"
  },
  {
    "text": "could we use it i mentioned in the presentation there are other tools for instance luigi and probably airflow is an equally good",
    "start": "1996399",
    "end": "2003360"
  },
  {
    "text": "um solution uh it just needs someone to actually um",
    "start": "2003360",
    "end": "2008559"
  },
  {
    "text": "give this a try and see um if that works for them um okay so",
    "start": "2008559",
    "end": "2016840"
  },
  {
    "start": "2015000",
    "end": "2065000"
  },
  {
    "text": "um so there are a couple of uh other",
    "start": "2016840",
    "end": "2023399"
  },
  {
    "text": "questions um so i mean at that so",
    "start": "2023399",
    "end": "2028880"
  },
  {
    "text": "there's a question whether we actually now completely switched to kubernetes and i'm actually actually",
    "start": "2028880",
    "end": "2034080"
  },
  {
    "text": "have to say we were only really getting started with kubernetes i mean that our id department is you know heavily",
    "start": "2034080",
    "end": "2040640"
  },
  {
    "text": "invested in in kubernetes but let's say all the jobs that we're running are still",
    "start": "2040640",
    "end": "2046240"
  },
  {
    "text": "running on uh hd condor or slurm or other platforms where we can execute",
    "start": "2046240",
    "end": "2051760"
  },
  {
    "text": "containers but we cannot really",
    "start": "2051760",
    "end": "2056638"
  },
  {
    "text": "we're not using kubernetes actually for for the workflows as i'm i'm doing it now",
    "start": "2058560",
    "end": "2064878"
  },
  {
    "text": "okay there there are a com a couple of other questions on for instance this k subs plug-in uh",
    "start": "2064879",
    "end": "2070800"
  },
  {
    "start": "2065000",
    "end": "2162000"
  },
  {
    "text": "for argo cd um and and other uh links so hit me up in the chat i can post them there i",
    "start": "2070800",
    "end": "2076398"
  },
  {
    "text": "don't think i will have uh time for that um maybe i take uh",
    "start": "2076399",
    "end": "2084560"
  },
  {
    "text": "like uh one last question so um so there was a question",
    "start": "2084560",
    "end": "2091520"
  },
  {
    "text": "like you know what is the what needs to be improved uh for kubernetes uh to make it easier um",
    "start": "2091520",
    "end": "2098079"
  },
  {
    "text": "for for for cern or for high energy physics in general and i said you know probably it's a multi-tenancy",
    "start": "2098079",
    "end": "2105680"
  },
  {
    "text": "and i i would say you know the the way that we do",
    "start": "2105680",
    "end": "2112480"
  },
  {
    "text": "authentication and authorization is also currently changing so so um in principle of course each user",
    "start": "2112480",
    "end": "2119839"
  },
  {
    "text": "should be able to create jobs in a cluster but for instance not delete some other users",
    "start": "2119839",
    "end": "2126000"
  },
  {
    "text": "job and so so in principle each user needs to have their own",
    "start": "2126000",
    "end": "2131280"
  },
  {
    "text": "uh dedicated namespace they need to be authenticated for instance via um oauth in one way or the other but",
    "start": "2131280",
    "end": "2137280"
  },
  {
    "text": "let's say most of the works actually happening in the terminal so this needs to be linked uh and that's uh currently giving me",
    "start": "2137280",
    "end": "2143920"
  },
  {
    "text": "some kind of headache even though i mean there are solutions but this needs to be figured out all right so um i see we're",
    "start": "2143920",
    "end": "2150400"
  },
  {
    "text": "out of time so thank you again very much uh for for joining this presentation hope you found it useful and",
    "start": "2150400",
    "end": "2155839"
  },
  {
    "text": "i'll be monitoring the slack chat for further questions so have a great",
    "start": "2155839",
    "end": "2161359"
  },
  {
    "text": "day thank you",
    "start": "2161359",
    "end": "2164799"
  }
]