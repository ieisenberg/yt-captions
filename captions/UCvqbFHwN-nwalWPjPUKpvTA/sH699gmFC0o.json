[
  {
    "text": "welcome to the session divide and conquer Master GPU partitioning and visualize savings with open cost my name",
    "start": "320",
    "end": "6440"
  },
  {
    "text": "is Casey U I'm a product manager on Azure kubernetes service team at Microsoft uh I work on everything cost",
    "start": "6440",
    "end": "12360"
  },
  {
    "text": "management optimization so really excited to talk about this topic with you uh unfortunately my Cobra Cent and",
    "start": "12360",
    "end": "18160"
  },
  {
    "text": "colleague Ally wasn't able to make it today so it's just",
    "start": "18160",
    "end": "23119"
  },
  {
    "text": "me all right so today we'll chat a bit about why people are even running gpus",
    "start": "23279",
    "end": "28320"
  },
  {
    "text": "uh what this looks like in the industry we'll layer in costs so when we think about gpus and costs what are some of",
    "start": "28320",
    "end": "33840"
  },
  {
    "text": "the challenges and issues that we're facing and then we'll talk about how to monitor GPU usage metrics uh visualize",
    "start": "33840",
    "end": "40399"
  },
  {
    "text": "your GPU costs and then ultimately Implement partitioning techniques to go and optimize your GPU",
    "start": "40399",
    "end": "46559"
  },
  {
    "text": "utilization uh and we'll tie it together in the end with a demo showing how to set up uh Nvidia GPU operator Implement",
    "start": "46559",
    "end": "54879"
  },
  {
    "text": "time slicing and then also leverage open cost",
    "start": "54879",
    "end": "60039"
  },
  {
    "text": "so as we all know gpus are scarce and expensive uh the scarcity often means that when we get a GPU we may want to",
    "start": "60039",
    "end": "67280"
  },
  {
    "text": "hold on to it even if we're not using it and if we're doing that if we have a GPU",
    "start": "67280",
    "end": "73040"
  },
  {
    "text": "we want to make sure we're extracting every penny every cent out of it that we can because they're so expensive uh so",
    "start": "73040",
    "end": "79640"
  },
  {
    "text": "really why why do we want to pay for these gpus if they're not going to be utilized and they're sitting their idle",
    "start": "79640",
    "end": "86079"
  },
  {
    "text": "GPU uh utilization this is the amount of time during which one or more kernels um",
    "start": "86079",
    "end": "91920"
  },
  {
    "text": "are are executing on the GPU and by default when kubernetes is scheduling GPU workloads uh it assigns a whole GPU",
    "start": "91920",
    "end": "99640"
  },
  {
    "text": "to a single workload or job exclusively this is called exclusive access uh and",
    "start": "99640",
    "end": "105600"
  },
  {
    "text": "with kubernetes uh in the Pod manifest when you're actually requesting your GPU",
    "start": "105600",
    "end": "110719"
  },
  {
    "text": "resource uh you have to request in whole integers so it's a little bit of a different model compared to CPUs where",
    "start": "110719",
    "end": "117079"
  },
  {
    "text": "you can request uh fractions of a CPU the exclusive access model means that",
    "start": "117079",
    "end": "124280"
  },
  {
    "text": "usually a workload is not going to use the entire gpus resources so a lot of",
    "start": "124280",
    "end": "129679"
  },
  {
    "text": "the utilization is actually going to be sitting their idle um this this means",
    "start": "129679",
    "end": "137000"
  },
  {
    "text": "you know when we think about at skill is this actually feasible is it economical to run AI workloads on gpus well it",
    "start": "137000",
    "end": "145400"
  },
  {
    "text": "absolutely can be if we you know Implement certain strategies but before we talk about the strategies uh let's",
    "start": "145400",
    "end": "152280"
  },
  {
    "text": "take a look at some data so in a recent Gartner Pier survey of over 500 Tech",
    "start": "152280",
    "end": "157879"
  },
  {
    "text": "leaders they found that over 48% of organizations are actually using kubernetes already to run their AI ml",
    "start": "157879",
    "end": "165840"
  },
  {
    "text": "workloads for AI and llms kubernetes really is the ideal platform to be",
    "start": "165840",
    "end": "171239"
  },
  {
    "text": "running these workloads why because kubernetes provide scalability uh",
    "start": "171239",
    "end": "177080"
  },
  {
    "text": "elasticity the ability to uh scale up and down automatically based on resource",
    "start": "177080",
    "end": "182599"
  },
  {
    "text": "demand is definitely a huge Advantage especially for training or apps that uh",
    "start": "182599",
    "end": "187879"
  },
  {
    "text": "require massive amounts of compute it kubernetes is declarative by Nature so",
    "start": "187879",
    "end": "192959"
  },
  {
    "text": "we communicate upfront how much resources are workloads require and then kubernetes will handle all that",
    "start": "192959",
    "end": "199319"
  },
  {
    "text": "scheduling uh which really reduces the operational overhead and burden that you know typical operations teams uh might",
    "start": "199319",
    "end": "207040"
  },
  {
    "text": "might experience and lastly kubernetes is dynamic and flexible containerizing",
    "start": "207040",
    "end": "212439"
  },
  {
    "text": "your AI apps means that you can uh there's increased extensibility and portability you can run your AI apps um",
    "start": "212439",
    "end": "219439"
  },
  {
    "text": "across clouds on public on Prem even in hybrid scenarios so let's sing to gpus more how",
    "start": "219439",
    "end": "227640"
  },
  {
    "text": "are they different from CPUs fundamentally they are doing different things because of how they're",
    "start": "227640",
    "end": "233599"
  },
  {
    "text": "architected and built CPUs are ubiquitous ubiquitous there isn't a single uh digital device that doesn't",
    "start": "233599",
    "end": "240920"
  },
  {
    "text": "have them CPUs are more general purpose too they're uh fast and versatile so",
    "start": "240920",
    "end": "246760"
  },
  {
    "text": "they can handle really complex processes with varied instructions versus gpus are",
    "start": "246760",
    "end": "252280"
  },
  {
    "text": "much more spe specialized um they're specialized processing units really optimized for performance parallel",
    "start": "252280",
    "end": "259400"
  },
  {
    "text": "processing throughput uh running thousands of operations at once",
    "start": "259400",
    "end": "264600"
  },
  {
    "text": "concurrently even if they are repetitive operations and additionally CPU",
    "start": "264600",
    "end": "269720"
  },
  {
    "text": "processes are serialized whereas gpus um processes run concurrently so if a GPU",
    "start": "269720",
    "end": "276080"
  },
  {
    "text": "is given a task it'll subdivide it into thousands of smaller tasks and then process them at the same time",
    "start": "276080",
    "end": "284639"
  },
  {
    "text": "concurrently now let's look at some common use cases uh in Industry gpus",
    "start": "289240",
    "end": "295199"
  },
  {
    "text": "since they're so suitable for handling large processes um made up of different parts they really excel at like",
    "start": "295199",
    "end": "302479"
  },
  {
    "text": "scientifical scientific calculation uh computationally complex Alor algorithms",
    "start": "302479",
    "end": "308320"
  },
  {
    "text": "inferencing training image processing and they're used across many different Industries and vertical as well from",
    "start": "308320",
    "end": "314600"
  },
  {
    "text": "Healthcare to gaming to fin serve uh you name it but as I mentioned before gpus are",
    "start": "314600",
    "end": "322240"
  },
  {
    "text": "expensive and costs can balloon really quickly so if I look at my infra or Cloud bill at the end of the day and see",
    "start": "322240",
    "end": "329319"
  },
  {
    "text": "how much my GPS actually cost me I might get that sticker shock and this is how I might feel uh this is not fine oh my",
    "start": "329319",
    "end": "336680"
  },
  {
    "text": "gosh why why are things on fire why are my costs so expensive what is my problem",
    "start": "336680",
    "end": "342560"
  },
  {
    "text": "so how can we go from this to this new mentality of it's fine everything will",
    "start": "342560",
    "end": "348600"
  },
  {
    "text": "be fine because my costs are now under control so let's see how we can get",
    "start": "348600",
    "end": "354360"
  },
  {
    "text": "there three three steps that we'll walk through today so monitoring your GPU costs with Nvidia G dcgm exporter second",
    "start": "354360",
    "end": "363440"
  },
  {
    "text": "visualizing GPU costs with open cost and then finally optimizing your costs you",
    "start": "363440",
    "end": "368520"
  },
  {
    "text": "can do that through right sizing your infrastructure but we'll be talking about uh partitioning",
    "start": "368520",
    "end": "374759"
  },
  {
    "text": "techniques so monitoring GPU metrics the Nvidia DC GPU manager dcgm for short is",
    "start": "374759",
    "end": "382759"
  },
  {
    "text": "a suite of tools for managing and monitoring your Nvidia GPU clusters uh",
    "start": "382759",
    "end": "388360"
  },
  {
    "text": "and DC environments it provides Health monitoring comprehensive diagnostics system alerts",
    "start": "388360",
    "end": "395520"
  },
  {
    "text": "uh governance policies and a whole bunch of great Telemetry metrics uh it is the",
    "start": "395520",
    "end": "400880"
  },
  {
    "text": "dcgm really is the foundation for a lot of GPU monitoring capabilities and the dcgm also",
    "start": "400880",
    "end": "408240"
  },
  {
    "text": "integrates into the kubernetes ecosystem through the dcgm exporter which provides",
    "start": "408240",
    "end": "413720"
  },
  {
    "text": "GPU Telemetry in containerized environments so the exporter will essentially expose GP metrics at a HTTP",
    "start": "413720",
    "end": "422199"
  },
  {
    "text": "metric endpoint and this means you can really seamlessly integrate into other monitoring or visualization tools open",
    "start": "422199",
    "end": "429800"
  },
  {
    "text": "source tools as well like Prometheus and grafana and you can run the dcgm exporter as a standalone container you",
    "start": "429800",
    "end": "437520"
  },
  {
    "text": "can also deploy it via the Nvidia DC GPU operator uh GPU operator uh and the GPU",
    "start": "437520",
    "end": "444960"
  },
  {
    "text": "operator is really great it's a tool that kind of automates the life cycle",
    "start": "444960",
    "end": "450120"
  },
  {
    "text": "um of the software and and manages the software required to use gpus with",
    "start": "450120",
    "end": "456639"
  },
  {
    "text": "kubernetes it also manages configures Provisions gpus to scale like other",
    "start": "456639",
    "end": "462960"
  },
  {
    "text": "resources in a kubernetes cluster the exporter also has some",
    "start": "462960",
    "end": "470039"
  },
  {
    "text": "really great metrics to help expose over or under utilization uh as well as GPU",
    "start": "470039",
    "end": "476240"
  },
  {
    "text": "node health so let's take a look at some of these metrics",
    "start": "476240",
    "end": "481440"
  },
  {
    "text": "here we have just outof boox Scana dashboard that is directly integrated",
    "start": "482199",
    "end": "487280"
  },
  {
    "text": "and pulling metrics from the dcgm EXP border um yeah this is great it's a standard out of box metric and we see a",
    "start": "487280",
    "end": "493879"
  },
  {
    "text": "whole plethora of different Telemetry specifically I can see my GPU",
    "start": "493879",
    "end": "500440"
  },
  {
    "text": "utilization not looking too good right now 0% utilization means it's not being",
    "start": "500440",
    "end": "506039"
  },
  {
    "text": "utilized well but I'm still having to pay for this expensive GPU not great uh I can also see memory utilization",
    "start": "506039",
    "end": "512560"
  },
  {
    "text": "percentage memory allocation temperature again a whole different metrics to understand how efficient my GPU",
    "start": "512560",
    "end": "519719"
  },
  {
    "text": "currently is uh and this in this case clearly not very",
    "start": "519719",
    "end": "525600"
  },
  {
    "text": "efficient here's another view a different set of metrics we see GPU",
    "start": "525600",
    "end": "530839"
  },
  {
    "text": "power usage power total GPU temperature all are quite important metrics to",
    "start": "530839",
    "end": "536760"
  },
  {
    "text": "monitor regularly to help you evaluate and respond to utilization Trends",
    "start": "536760",
    "end": "542320"
  },
  {
    "text": "anomalies to fine-tune and and optimize your your",
    "start": "542320",
    "end": "547920"
  },
  {
    "text": "performance now that we have monitoring in place we've seen our GPU metrics",
    "start": "548040",
    "end": "553240"
  },
  {
    "text": "let's actually visualize the costs so here we can use open cost open cost is",
    "start": "553240",
    "end": "559160"
  },
  {
    "text": "an open source vendor neutral cncf project uh actually just two or 3 weeks",
    "start": "559160",
    "end": "564360"
  },
  {
    "text": "ago it was promoted from CNC CF sandbox to incubation which which is quite",
    "start": "564360",
    "end": "569720"
  },
  {
    "text": "exciting for the whole Community I know we have some folks in the community here as part of open cost um open cost it",
    "start": "569720",
    "end": "576600"
  },
  {
    "text": "essentially provides a standardization and spec for measuring reporting",
    "start": "576600",
    "end": "581959"
  },
  {
    "text": "allocating and even visualizing your uh cluster infrastructure costs across",
    "start": "581959",
    "end": "587880"
  },
  {
    "text": "Cloud environments and with open costs you can really nicely slice and dice your",
    "start": "587880",
    "end": "594160"
  },
  {
    "text": "cluster cost by kubernetes abstractions like name space labels containers",
    "start": "594160",
    "end": "600160"
  },
  {
    "text": "any kubernetes construct essentially and we'll take a look at this in a demo pretty shortly how we can see costs with",
    "start": "600160",
    "end": "607560"
  },
  {
    "text": "the open cost API so we have monitoring visibility in",
    "start": "607560",
    "end": "614600"
  },
  {
    "text": "place and our thir M step is optimizing the costs GPU cost optimization is",
    "start": "614600",
    "end": "621800"
  },
  {
    "text": "really about maximizing your GPU efficiency while reducing the total cost of ownership for your GPU infrastructure",
    "start": "621800",
    "end": "629000"
  },
  {
    "text": "um but without compromising on performance so you may think that selecting the most powerful GPU or the",
    "start": "629000",
    "end": "635880"
  },
  {
    "text": "largest GPU instance type is the best but this isn't always the case this",
    "start": "635880",
    "end": "641360"
  },
  {
    "text": "often leads to unnecessary costs underutilized resources uh so right",
    "start": "641360",
    "end": "646600"
  },
  {
    "text": "sizing your GPU infrastructure is quite important so that you can balance your workloads needs in terms of memory",
    "start": "646600",
    "end": "653320"
  },
  {
    "text": "processing power Etc uh you can do this you know manually you can also leverage",
    "start": "653320",
    "end": "660040"
  },
  {
    "text": "Carpenter which is uh open source project as well to pick the most cost- effective skew for your GPU based on",
    "start": "660040",
    "end": "668279"
  },
  {
    "text": "your workloads requirements which is great and Cloud providers also offer quite a broad range of GP options from",
    "start": "668279",
    "end": "676720"
  },
  {
    "text": "the entry level ones that are better for running small inferencing workloads uh or smaller models all the way to higher",
    "start": "676720",
    "end": "683680"
  },
  {
    "text": "end gpus which are you know great for large scale training or large amounts of",
    "start": "683680",
    "end": "689399"
  },
  {
    "text": "data processing and then you can also leverage partitioning techniques uh to",
    "start": "689399",
    "end": "697079"
  },
  {
    "text": "combat underutilization and actually split up your GPU and share it across multiple",
    "start": "697079",
    "end": "704160"
  },
  {
    "text": "workloads so Nvidia supports several partitioning techniques to oversubscribe your gpus over subscription is kind of",
    "start": "704399",
    "end": "711639"
  },
  {
    "text": "like car pooling where you're packing more people or processes in this case into uh a single car or a GPU in this",
    "start": "711639",
    "end": "719279"
  },
  {
    "text": "this case to make it being used more efficiently uh GPU partitioning can",
    "start": "719279",
    "end": "725000"
  },
  {
    "text": "happen at the application Level it can also happen at the software and Hardware levels uh so let's specifically look at",
    "start": "725000",
    "end": "732440"
  },
  {
    "text": "the three the three in the middle so time slicing migs and",
    "start": "732440",
    "end": "737440"
  },
  {
    "text": "MPS time slicing this is the simplest approach uh this provides you the ability to run sever workloads",
    "start": "737639",
    "end": "744160"
  },
  {
    "text": "concurrently on the same GPU rather than having them spread out across multiple gpus like in the dedicated model up top",
    "start": "744160",
    "end": "751959"
  },
  {
    "text": "uh each process will be alternating in time exe in execution time uh in a",
    "start": "751959",
    "end": "757040"
  },
  {
    "text": "roundr in fashion and for each of the duration of the time slice they have full access to the GPU and then once the",
    "start": "757040",
    "end": "765040"
  },
  {
    "text": "time slice uh has is is completed the GPU will be relinquished and then",
    "start": "765040",
    "end": "770760"
  },
  {
    "text": "allocated to the next process in the queue uh when you are basically defining",
    "start": "770760",
    "end": "776720"
  },
  {
    "text": "uh time slicing in your config you'll Define a set of replicas that you want uh for your GPU so in this case since I",
    "start": "776720",
    "end": "784199"
  },
  {
    "text": "have three dedicated gpus for example I want my time slice GPU to have three",
    "start": "784199",
    "end": "789240"
  },
  {
    "text": "replicas uh and each slice will be handed out independently to a pod to run",
    "start": "789240",
    "end": "795000"
  },
  {
    "text": "the workload on it another cool thing about time slicing is that you can apply it to specific",
    "start": "795000",
    "end": "802199"
  },
  {
    "text": "nodes in your cluster it doesn't have to be all the nodes in your cluster so for example I can apply time slicing just to",
    "start": "802199",
    "end": "808639"
  },
  {
    "text": "my Tesla T4 GPU nodes only and not to my a100 or if there's specific GPU nodes",
    "start": "808639",
    "end": "815680"
  },
  {
    "text": "that are running specific uh processes they don't have to be time sliced as",
    "start": "815680",
    "end": "821680"
  },
  {
    "text": "well and one caveat to call out about time slicing um because each slice has",
    "start": "821680",
    "end": "827680"
  },
  {
    "text": "full access to the GPU there's not really a control over how much or how",
    "start": "827680",
    "end": "832720"
  },
  {
    "text": "many resources a process can request so if one process is requesting a a large",
    "start": "832720",
    "end": "838399"
  },
  {
    "text": "amount of resource it could lead to uh o pills or issues or other uh performance related issues for",
    "start": "838399",
    "end": "845920"
  },
  {
    "text": "the processes that are running in succession so that's just one caveat is that there can be some unpredictable uh",
    "start": "845920",
    "end": "853959"
  },
  {
    "text": "performance here and then we have MPS",
    "start": "853959",
    "end": "860320"
  },
  {
    "text": "multi-processing service this is a method of space partitioning here each",
    "start": "860320",
    "end": "865600"
  },
  {
    "text": "workload will remain resident on the GPU but it won't swapped out in terms of",
    "start": "865600",
    "end": "870759"
  },
  {
    "text": "execution time uh like in the time slicing model and here each fraction",
    "start": "870759",
    "end": "876800"
  },
  {
    "text": "only uses uh a portion of the gpu's memory and compute capability since it",
    "start": "876800",
    "end": "882040"
  },
  {
    "text": "is logically partitioned so we don't quite have the same uh unpredictable",
    "start": "882040",
    "end": "887759"
  },
  {
    "text": "performance like we did in the time slicing model and then migs are last uh",
    "start": "887759",
    "end": "893680"
  },
  {
    "text": "partition technique this enables a single GPU to be partitioned into separate GP instances up to seven",
    "start": "893680",
    "end": "901920"
  },
  {
    "text": "instances um and each are secure and isolated at the hardware level from each other uh so this is really great for M",
    "start": "901920",
    "end": "909240"
  },
  {
    "text": "multi-tenanted use cases uh it's similar to MPS and that the resources on the GPU",
    "start": "909240",
    "end": "915040"
  },
  {
    "text": "are space partitioned but each partition will get a dedicated portion of the gpus",
    "start": "915040",
    "end": "920480"
  },
  {
    "text": "computational resources memory so each can run independently without",
    "start": "920480",
    "end": "925600"
  },
  {
    "text": "interruption from the others possibly due to lack of resources so this is the",
    "start": "925600",
    "end": "931279"
  },
  {
    "text": "best strategy for a more predictable performance if that is uh important and",
    "start": "931279",
    "end": "937720"
  },
  {
    "text": "you can also combine strategies so you can combine migs and time slicing for example within uh a mig's partition you",
    "start": "937720",
    "end": "946240"
  },
  {
    "text": "can further leverage time slicing within a single partition so it's great that",
    "start": "946240",
    "end": "951639"
  },
  {
    "text": "you can kind of uh combine strategies to tailor to your",
    "start": "951639",
    "end": "956959"
  },
  {
    "text": "scenarios and now we have a demo so this demo will have three parts first we'll",
    "start": "956959",
    "end": "962600"
  },
  {
    "text": "create a kubernetes cluster set it up with a standard node pool as well as a GPU node pool with the GPU operator",
    "start": "962600",
    "end": "969360"
  },
  {
    "text": "enabled and then second we'll install Prometheus and open cost and show the",
    "start": "969360",
    "end": "974399"
  },
  {
    "text": "open cost API and how we can look at our GPU costs and then third we'll Implement",
    "start": "974399",
    "end": "980160"
  },
  {
    "text": "a Time slicing GPU and show how simple it can be to get set that to get set up",
    "start": "980160",
    "end": "986079"
  },
  {
    "text": "with that all right so here we are in our CLI I'm",
    "start": "986079",
    "end": "992920"
  },
  {
    "text": "creating a standard AHS cluster uh it'll have a node count of one just for this",
    "start": "992920",
    "end": "998560"
  },
  {
    "text": "demo purpose I'm going to create a namespace and deploy a very basic web",
    "start": "998560",
    "end": "1004519"
  },
  {
    "text": "app and then soon we'll see all of my cuse resources being created so this is",
    "start": "1004519",
    "end": "1010120"
  },
  {
    "text": "a very straightforward normal basic setup for a CPU",
    "start": "1010120",
    "end": "1015560"
  },
  {
    "text": "node then here I'm creating a second node pool in this node pool there will",
    "start": "1015560",
    "end": "1021040"
  },
  {
    "text": "be one GPU node uh standard nc4 as GPU and I'm going to actually skip the GPU",
    "start": "1021040",
    "end": "1028798"
  },
  {
    "text": "driver install because we'll see in a later step that with AKs at least the driver will be automatically installed",
    "start": "1028799",
    "end": "1035720"
  },
  {
    "text": "by default uh with the GPU operator and then I will add Nvidia",
    "start": "1035720",
    "end": "1043319"
  },
  {
    "text": "repo update and then I can finally install the Nvidia GPU operator",
    "start": "1043319",
    "end": "1050559"
  },
  {
    "text": "and now looking at my nodes I can see my GPU node is up and",
    "start": "1051320",
    "end": "1057039"
  },
  {
    "text": "running if I describe this node I'll see the various drivers device plugins uh",
    "start": "1057039",
    "end": "1065160"
  },
  {
    "text": "dcgm exporter they've all been deployed uh as part of the GPU",
    "start": "1065160",
    "end": "1073440"
  },
  {
    "text": "operator and then I can just go ahead and deploy my sample application",
    "start": "1074480",
    "end": "1079880"
  },
  {
    "text": "my GPU demo",
    "start": "1079880",
    "end": "1082760"
  },
  {
    "text": "app there'll be a few uh this is just a batch job and there'll be a few runs",
    "start": "1086000",
    "end": "1091640"
  },
  {
    "text": "that are happening if I check out the logs here",
    "start": "1091640",
    "end": "1097039"
  },
  {
    "text": "we'll see that there were 500 runs for this app that was using one GPU for its",
    "start": "1097039",
    "end": "1104600"
  },
  {
    "text": "computation and if we dig into some of the details we'll see the name up there Tesla",
    "start": "1104840",
    "end": "1110760"
  },
  {
    "text": "T4 uh was the GPU and we can see other details like total and free",
    "start": "1110760",
    "end": "1117400"
  },
  {
    "text": "memory so that was our first kind of basic setup and now I want to look at my",
    "start": "1119840",
    "end": "1127480"
  },
  {
    "text": "uh GPU costs so first we'll need to install",
    "start": "1127480",
    "end": "1133440"
  },
  {
    "text": "Prometheus and then we can go ahead and helmet install open cost very quick and",
    "start": "1133440",
    "end": "1141440"
  },
  {
    "text": "simple and once the open cost PO is ready we can just port",
    "start": "1145480",
    "end": "1151840"
  },
  {
    "text": "forward now if I go over to my browser um we're going to be looking at the open",
    "start": "1155840",
    "end": "1161440"
  },
  {
    "text": "cost allocations API uh so open cost does actually have a UI uh but some of",
    "start": "1161440",
    "end": "1166679"
  },
  {
    "text": "their G GPU metrics and and GPU cost capabilities are just uh available via",
    "start": "1166679",
    "end": "1173039"
  },
  {
    "text": "API at the moment which is still great you can you can programmatically pull this data into you know whatever",
    "start": "1173039",
    "end": "1178640"
  },
  {
    "text": "visualization analysis pipeline you you have kind of built whether that's um",
    "start": "1178640",
    "end": "1184440"
  },
  {
    "text": "powerbi or Prometheus grafana something else uh but the allocations API this um",
    "start": "1184440",
    "end": "1191640"
  },
  {
    "text": "allows you to query for cost and and resources allocated to kubernetes",
    "start": "1191640",
    "end": "1196679"
  },
  {
    "text": "workloads based on On Demand pricing by default you can configure your own custom specific pricing or integrate",
    "start": "1196679",
    "end": "1203480"
  },
  {
    "text": "into cloud provider uh Cost apis U but out of box this uh this is the",
    "start": "1203480",
    "end": "1210640"
  },
  {
    "text": "experience I'll show is just the on demand pricing So within this API 2 you spec",
    "start": "1210640",
    "end": "1217240"
  },
  {
    "text": "you specify a look pack window I specified the last seven days and then my aggregation is at the name Space",
    "start": "1217240",
    "end": "1224080"
  },
  {
    "text": "level but like I mentioned before you can aggregate any kubernetes primitive I can look per container per deployment",
    "start": "1224080",
    "end": "1230840"
  },
  {
    "text": "per service um but here I've just chosen per namespace and my resolution uh",
    "start": "1230840",
    "end": "1237240"
  },
  {
    "text": "window is 1 minute which is just the duration that is used for Prometheus queries so more frequent queries of",
    "start": "1237240",
    "end": "1245240"
  },
  {
    "text": "Prometheus just means a higher accuracy and here we'll take a look at a",
    "start": "1245240",
    "end": "1252039"
  },
  {
    "text": "few of our Nam spaces let me go to the GPU namespace",
    "start": "1252039",
    "end": "1259360"
  },
  {
    "text": "there we go our GPU app name space we can see details about the controller",
    "start": "1259559",
    "end": "1265600"
  },
  {
    "text": "kind was a job we see details about what the node skew type was andc uh 4 as like",
    "start": "1265600",
    "end": "1273000"
  },
  {
    "text": "we had set up and then since my job had just run for a few short minutes the the cost and",
    "start": "1273000",
    "end": "1280120"
  },
  {
    "text": "utilization data was actually quite low for my GPU uh for example it just ran",
    "start": "1280120",
    "end": "1285679"
  },
  {
    "text": "for 3 minutes I can see how much the average CPU request was how much CPU was",
    "start": "1285679",
    "end": "1291279"
  },
  {
    "text": "actually used and how that translates into a CPU cost and then similarly on",
    "start": "1291279",
    "end": "1297080"
  },
  {
    "text": "the GPU side I can see that I requested one GPU but what I actually used on the",
    "start": "1297080",
    "end": "1303039"
  },
  {
    "text": "GPU was a tiny fraction um here GPU cost is based on your uh real GPU utilization",
    "start": "1303039",
    "end": "1311799"
  },
  {
    "text": "not based on your GPU requests so even though the cost here seems very small",
    "start": "1311799",
    "end": "1317000"
  },
  {
    "text": "for this job in this namespace I still am actually paying for the entire GPU even though it's not being",
    "start": "1317000",
    "end": "1322919"
  },
  {
    "text": "utilized right now so that that Delta and this marginal cost versus the price",
    "start": "1322919",
    "end": "1328279"
  },
  {
    "text": "tag that I see on my bill that's that's opportunity to to save and to to close",
    "start": "1328279",
    "end": "1335520"
  },
  {
    "text": "that Gap all right and",
    "start": "1335520",
    "end": "1340600"
  },
  {
    "text": "finally let's take a look at implementing GPU partitioning specifically time",
    "start": "1340600",
    "end": "1347520"
  },
  {
    "text": "slicing so in our cluster we had our regular CPU node pool our GPU node pool",
    "start": "1347520",
    "end": "1353480"
  },
  {
    "text": "and now we're going to create a GPU time slicing node pool again it'll just have one node uh we'll skip GPU driver",
    "start": "1353480",
    "end": "1360760"
  },
  {
    "text": "install again and and same nc4 as uh node",
    "start": "1360760",
    "end": "1366919"
  },
  {
    "text": "type once this is up and running let's check our node not quite ready yet but",
    "start": "1371480",
    "end": "1376840"
  },
  {
    "text": "that's okay um we will look at our GPU operator pod and for this new node there",
    "start": "1376840",
    "end": "1382880"
  },
  {
    "text": "are actually several new containers uh that are running so the dcgm exporter",
    "start": "1382880",
    "end": "1388320"
  },
  {
    "text": "device plug-in driver Damon set these are all being initialized as part of the",
    "start": "1388320",
    "end": "1393600"
  },
  {
    "text": "GPU operator that we had installed",
    "start": "1393600",
    "end": "1398200"
  },
  {
    "text": "previously and then we'll just take a look at our name spaces and before we apply the time slicing config I just",
    "start": "1401799",
    "end": "1408200"
  },
  {
    "text": "want to show what the config actually looks like so I'm just applying this to the single uh GPU time slice node uh you",
    "start": "1408200",
    "end": "1415880"
  },
  {
    "text": "can specify for A1 100s for example your um time slicing and migs partitioning",
    "start": "1415880",
    "end": "1423080"
  },
  {
    "text": "technique or or kind of like how you want to set that up but since I don't have any A1 100s this config won't apply",
    "start": "1423080",
    "end": "1431000"
  },
  {
    "text": "uh but on the bottom I do have a section for Tesla T4 in my uh config I specify I",
    "start": "1431000",
    "end": "1438200"
  },
  {
    "text": "want want to split my Tesla T4 GPU into four replicas so it's pretty simple how you",
    "start": "1438200",
    "end": "1444760"
  },
  {
    "text": "actually uh Define how many replicas you want I will apply this time slicing",
    "start": "1444760",
    "end": "1452440"
  },
  {
    "text": "config and then one more step we have to do we have to patch the cluster policy a",
    "start": "1456480",
    "end": "1463600"
  },
  {
    "text": "device plug-in config name",
    "start": "1463600",
    "end": "1467960"
  },
  {
    "text": "and then our GPU time slice node is ready to",
    "start": "1468880",
    "end": "1474120"
  },
  {
    "text": "go we'll just add a label to",
    "start": "1474120",
    "end": "1478919"
  },
  {
    "text": "it and then finally we can describe that node and there we go we see in the",
    "start": "1483679",
    "end": "1490880"
  },
  {
    "text": "capacity and allocatable now we actually have four Nvidia gpus even though we've",
    "start": "1490880",
    "end": "1497440"
  },
  {
    "text": "only provisioned a single GPU in that initial",
    "start": "1497440",
    "end": "1503200"
  },
  {
    "text": "setup and we can also see the uh device plugin label that we had set for Tesla",
    "start": "1507840",
    "end": "1516159"
  },
  {
    "text": "T4 and now we can go and deploy uh and apply our sample application so this job",
    "start": "1516159",
    "end": "1524360"
  },
  {
    "text": "yaml the the main difference between the time slice version and the n non-time sliced application is that in the spec I",
    "start": "1524360",
    "end": "1532360"
  },
  {
    "text": "have defined paral parallelism uh three concurrent jobs to R run and completion I want three jobs",
    "start": "1532360",
    "end": "1540960"
  },
  {
    "text": "to be completed and then again the limit is",
    "start": "1540960",
    "end": "1548360"
  },
  {
    "text": "the same thing just the limit of one GPU and when we go and apply that the same",
    "start": "1548360",
    "end": "1553919"
  },
  {
    "text": "process will happen we can see that those jobs work",
    "start": "1553919",
    "end": "1559399"
  },
  {
    "text": "completed check out the logs and it looks almost the same pretty much the",
    "start": "1559399",
    "end": "1565679"
  },
  {
    "text": "same as my non-time sliced uh GPU however with the time slicing model I",
    "start": "1565679",
    "end": "1572000"
  },
  {
    "text": "could run more workloads on this GPU different sizes different amounts of",
    "start": "1572000",
    "end": "1577399"
  },
  {
    "text": "resources and this is different than my non-time slice GPU where this wouldn't have even been possible so just wanted",
    "start": "1577399",
    "end": "1585039"
  },
  {
    "text": "to show how quick and easy it is to get up and running with time slicing uh and some of our final key",
    "start": "1585039",
    "end": "1591760"
  },
  {
    "text": "takeaways so gpus are expensive but they can be feasible and economical to run um",
    "start": "1591760",
    "end": "1598159"
  },
  {
    "text": "if you implement some of these techniques monitoring invisibility is key because without these utilization",
    "start": "1598159",
    "end": "1604559"
  },
  {
    "text": "metrics or cost metrics we don't know what is being underutilized we don't know where to focus our efforts and and",
    "start": "1604559",
    "end": "1611640"
  },
  {
    "text": "keep teams accountable for their GPU spend uh exclusive access for a GPU is",
    "start": "1611640",
    "end": "1617919"
  },
  {
    "text": "the default so implementing partitioning is just one method to help us optimize",
    "start": "1617919",
    "end": "1623880"
  },
  {
    "text": "our gpus um but it can really help improve utilization and reduce costs some additional details about the",
    "start": "1623880",
    "end": "1631200"
  },
  {
    "text": "partitioning techniques so time slicing best for apps that are not latency sensitive or can or that can tolerate",
    "start": "1631200",
    "end": "1637679"
  },
  {
    "text": "Jitter migs best running multiple apps in parallel but very important if they",
    "start": "1637679",
    "end": "1643440"
  },
  {
    "text": "need to be resilient and have high qua then migs is a great option and then",
    "start": "1643440",
    "end": "1649159"
  },
  {
    "text": "finally uh MPS best for running apps in parallel that can maybe deal with some",
    "start": "1649159",
    "end": "1655200"
  },
  {
    "text": "amount of limited resiliency so that is all thank you so",
    "start": "1655200",
    "end": "1660880"
  },
  {
    "text": "much and I hope you have a great end of your cuon thank",
    "start": "1660880",
    "end": "1666330"
  },
  {
    "text": "[Applause]",
    "start": "1666330",
    "end": "1672609"
  },
  {
    "text": "you all right",
    "start": "1673240",
    "end": "1677440"
  },
  {
    "text": "thank",
    "start": "1680159",
    "end": "1682399"
  },
  {
    "text": "you so open cost only accounts for the cloud",
    "start": "1689000",
    "end": "1695279"
  },
  {
    "text": "providers instance cost right so there needs to be some more calculation to",
    "start": "1695279",
    "end": "1701679"
  },
  {
    "text": "correlate to the slice of the GPU if you wanted to do that kind of level of",
    "start": "1701679",
    "end": "1707360"
  },
  {
    "text": "financial account yes have you thought about how far you going to go I mean in the end you end up",
    "start": "1707360",
    "end": "1715200"
  },
  {
    "text": "with like how much did it cost per token presumably MH yeah I I know that there is some efforts on open cost side to",
    "start": "1715200",
    "end": "1722120"
  },
  {
    "text": "kind of explore support and cost in the time slice model right now it's more focused on the exclusive GPU and",
    "start": "1722120",
    "end": "1728960"
  },
  {
    "text": "Reporting those costs but yeah definitely time slicing is an area that is is being uh explored and would",
    "start": "1728960",
    "end": "1736200"
  },
  {
    "text": "definitely love to yeah collaborate see how we can make that happen yeah it's it's very cool",
    "start": "1736200",
    "end": "1743278"
  },
  {
    "text": "thanks okay that was exactly so yeah we are trying to make that work so it'll um",
    "start": "1743480",
    "end": "1748519"
  },
  {
    "text": "if anybody's interested like Casey said uh that's an area that myself and Alex from Cube Costa looking to make sure",
    "start": "1748519",
    "end": "1754880"
  },
  {
    "text": "that open cost supports that so any participation from the Microsoft side would be great as well uh be looking to",
    "start": "1754880",
    "end": "1760960"
  },
  {
    "text": "make that story better the the second question that I have is um uh the last time I had a chat with you and Bob about",
    "start": "1760960",
    "end": "1767840"
  },
  {
    "text": "uh the open cost add-on on AKs um we found out that it's not pushing to",
    "start": "1767840",
    "end": "1773159"
  },
  {
    "text": "promius but it's writing to a file based thing um the downside for us is that when we install our product on top of um",
    "start": "1773159",
    "end": "1780120"
  },
  {
    "text": "the the add-on we can we'll have to get uh we can access the open cost uh data",
    "start": "1780120",
    "end": "1786279"
  },
  {
    "text": "so we'd have to like uninstall that and install open cost uh because that you know we can't seem to access the",
    "start": "1786279",
    "end": "1792799"
  },
  {
    "text": "underlying uh mechanism right has there is there going to be any changes to that to either uh push it to Prometheus or um",
    "start": "1792799",
    "end": "1800960"
  },
  {
    "text": "give access to the the the file that you guys are putting the the raw open cost",
    "start": "1800960",
    "end": "1806039"
  },
  {
    "text": "data that's not something that we are well it's something that we can definitely explore um at the moment we",
    "start": "1806039",
    "end": "1813399"
  },
  {
    "text": "haven't Expos the open cost metric Prometheus and the native add-on uh just because of how we're doing",
    "start": "1813399",
    "end": "1819279"
  },
  {
    "text": "reconciliation with the Azure cost data um it's a little bit of a different process than how open cost does",
    "start": "1819279",
    "end": "1825960"
  },
  {
    "text": "reconciliation but definitely we can explore if that makes sense yeah 100%",
    "start": "1825960",
    "end": "1831440"
  },
  {
    "text": "because it results in a better customer experience and by the way great presentation I know Ali was on there you you did a great job thank you",
    "start": "1831440",
    "end": "1839799"
  }
]