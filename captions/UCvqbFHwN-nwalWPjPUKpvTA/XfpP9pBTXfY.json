[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "text": "all right hello and welcome to the talk about manual sffs share backups",
    "start": "399",
    "end": "6879"
  },
  {
    "text": "my name is robert and i work at cern as a junior fellow software engineer",
    "start": "6879",
    "end": "12799"
  },
  {
    "text": "and i mainly focus on storage integration into container environments",
    "start": "12799",
    "end": "18800"
  },
  {
    "text": "and their deployment etc so this is the agenda for the session",
    "start": "18800",
    "end": "24720"
  },
  {
    "start": "23000",
    "end": "23000"
  },
  {
    "text": "first we'll have a look at what kind of storage we want to actually",
    "start": "24720",
    "end": "30480"
  },
  {
    "text": "backup after that we'll briefly describe the individual components",
    "start": "30480",
    "end": "36880"
  },
  {
    "text": "right after that we'll have a look uh exactly how we want to carry out the backups what is the workflow we would",
    "start": "36880",
    "end": "43840"
  },
  {
    "text": "like to follow uh after that uh we'll have a peek into the",
    "start": "43840",
    "end": "49120"
  },
  {
    "text": "future that still needs to be done because as you'll see later on in the",
    "start": "49120",
    "end": "54559"
  },
  {
    "text": "in the presentation there are still couple of blocker issues that we are facing and those need to be resolved",
    "start": "54559",
    "end": "60559"
  },
  {
    "text": "first so i really take this whole talk more as a project update rather than a",
    "start": "60559",
    "end": "68720"
  },
  {
    "text": "showcase of a final ready-to-use product and in the end we will uh",
    "start": "68720",
    "end": "74240"
  },
  {
    "text": "conclude with the summary so quickly about cern itself it's the",
    "start": "74240",
    "end": "80640"
  },
  {
    "text": "european organization for nuclear research it's located in geneva switzerland at the swiss french border",
    "start": "80640",
    "end": "88720"
  },
  {
    "text": "it was founded in 1954 and its main mission is fundamental science which means",
    "start": "88720",
    "end": "94000"
  },
  {
    "text": "it's trying to answer such questions as what is the 96 percent of the universe made of for this dark matter dark energy",
    "start": "94000",
    "end": "101439"
  },
  {
    "text": "what was the state of matter just after the big bang and a lot more questions of",
    "start": "101439",
    "end": "106640"
  },
  {
    "text": "this kind to try to answer those questions cern has",
    "start": "106640",
    "end": "112240"
  },
  {
    "text": "built these very large machines called particle accelerators the one you can",
    "start": "112240",
    "end": "117759"
  },
  {
    "text": "see on the photo here is the largest of its kind it's called lhc",
    "start": "117759",
    "end": "123200"
  },
  {
    "text": "it's of a circular shape 27 kilometers in circumference",
    "start": "123200",
    "end": "129520"
  },
  {
    "text": "and it's placed in a tunnel 100 meters below ground and in this",
    "start": "129520",
    "end": "135360"
  },
  {
    "text": "in this tunnel there are two beams of protons being traveling uh in opposite directions and being",
    "start": "135360",
    "end": "142160"
  },
  {
    "text": "accelerated uh very close to the speed of light and they are made to be collided with one another",
    "start": "142160",
    "end": "149120"
  },
  {
    "text": "at uh at another kind of huge machines this is",
    "start": "149120",
    "end": "155280"
  },
  {
    "text": "this one is called a detector that is then able to monitor the",
    "start": "155280",
    "end": "161280"
  },
  {
    "text": "aftermath of the collision uh and this is basically what the physicists then",
    "start": "161280",
    "end": "167280"
  },
  {
    "text": "study and analyze at cern those detectors generate huge amounts of data",
    "start": "167280",
    "end": "173200"
  },
  {
    "text": "and even after all the filtering is done it's still tens of petabytes per year",
    "start": "173200",
    "end": "178400"
  },
  {
    "text": "that needs to be stored and analyzed and for that we have we are running",
    "start": "178400",
    "end": "184879"
  },
  {
    "text": "private cloud based on openstack this is a pretty recent screenshot of our graphone dashboard you",
    "start": "184879",
    "end": "191280"
  },
  {
    "text": "can see we have around 460 000 physical cores 87 000 vms running on",
    "start": "191280",
    "end": "199920"
  },
  {
    "text": "almost 58 000 hypervisors we are also running",
    "start": "199920",
    "end": "205040"
  },
  {
    "text": "around 360 kubernetes clusters we have openshift as well but",
    "start": "205040",
    "end": "210159"
  },
  {
    "text": "kubernetes leads by far in uh in the numbers for storage we have those kinds of",
    "start": "210159",
    "end": "216720"
  },
  {
    "start": "214000",
    "end": "214000"
  },
  {
    "text": "numbers uh block devices around 3.8 petabytes file",
    "start": "216720",
    "end": "222560"
  },
  {
    "text": "shares almost 900 era and object storage at around 48 terabytes",
    "start": "222560",
    "end": "229599"
  },
  {
    "text": "this is all backed by ceph it's mostly application and user data",
    "start": "229599",
    "end": "235599"
  },
  {
    "text": "for things like physics data machine learning models we use eos",
    "start": "235599",
    "end": "241680"
  },
  {
    "text": "this one picks at around 500 petabytes but this also includes the",
    "start": "241680",
    "end": "246879"
  },
  {
    "text": "tape archives so this is a lot of data but for the purposes of this",
    "start": "246879",
    "end": "253280"
  },
  {
    "text": "presentation we will only focus on the file shares so that's around 900 terabytes",
    "start": "253280",
    "end": "259120"
  },
  {
    "text": "but not all of this amount needs to be backed up because pretty big chunks of this amount is used",
    "start": "259120",
    "end": "266720"
  },
  {
    "text": "by ci by qa by testing environments and development environments so",
    "start": "266720",
    "end": "272639"
  },
  {
    "text": "we don't really need to backup those so we did some accounting and",
    "start": "272639",
    "end": "278639"
  },
  {
    "text": "by some estimates we have around 65 projects that are deploying",
    "start": "278639",
    "end": "283919"
  },
  {
    "text": "159 production kubernetes clusters and those",
    "start": "283919",
    "end": "289680"
  },
  {
    "text": "then store around 74 terabytes of manual sfs storage",
    "start": "289680",
    "end": "294880"
  },
  {
    "text": "so that's basically the kind of numbers we are uh looking at so um",
    "start": "294880",
    "end": "300400"
  },
  {
    "start": "296000",
    "end": "296000"
  },
  {
    "text": "i mentioned manula sfs shares it's in the title of the presentation as well so",
    "start": "300400",
    "end": "305600"
  },
  {
    "text": "what exactly are those suse is scalable distributed storage",
    "start": "305600",
    "end": "310880"
  },
  {
    "text": "system that we rely on heavily it's",
    "start": "310880",
    "end": "316560"
  },
  {
    "text": "it offers three interfaces for this storage in the same package object store called",
    "start": "316560",
    "end": "323840"
  },
  {
    "text": "blog devices rpds raiders blog devices and then shared file systems",
    "start": "323840",
    "end": "329840"
  },
  {
    "text": "called cfs and this is the last one is what we are focusing on",
    "start": "329840",
    "end": "336400"
  },
  {
    "text": "right now then we have the openstack manual which is shared file system service for",
    "start": "336400",
    "end": "342479"
  },
  {
    "text": "openstack and it's the",
    "start": "342479",
    "end": "347199"
  },
  {
    "text": "is the point of where basically the shared storage systems are able to uh interact with the",
    "start": "347600",
    "end": "354960"
  },
  {
    "text": "openstack cloud it supports a lot of different technologies around 35 of those both",
    "start": "354960",
    "end": "362240"
  },
  {
    "text": "proprietary and open source but for us the main use case for manula or the main",
    "start": "362240",
    "end": "368319"
  },
  {
    "text": "backend that we use it with is these ffs file systems",
    "start": "368319",
    "end": "374240"
  },
  {
    "text": "it also provides multi-tenancy quota management all very important features that we rely on",
    "start": "374800",
    "end": "383520"
  },
  {
    "text": "then lastly we have csi it's the container storage interface and uh it forms this uh",
    "start": "383840",
    "end": "392000"
  },
  {
    "text": "well it's an interface between some storage system and the container orchestrator for example kubernetes",
    "start": "392000",
    "end": "400240"
  },
  {
    "text": "and it allows storage vendors to write their own uh storage drivers and",
    "start": "400240",
    "end": "407759"
  },
  {
    "text": "those drivers are called csi drivers and uh basically it forms like a middleman",
    "start": "407759",
    "end": "414000"
  },
  {
    "text": "between the storage and the orchestrator so uh there is also some uh integration into the orchestrators",
    "start": "414000",
    "end": "421039"
  },
  {
    "text": "themselves so you can create pvcs and those pvcs are then fulfilled by",
    "start": "421039",
    "end": "427199"
  },
  {
    "text": "by whatever that particular csi driver is doing both cfs and manila have their own",
    "start": "427199",
    "end": "435759"
  },
  {
    "text": "csi drivers that we rely on in our container workloads so let's take a look",
    "start": "435759",
    "end": "442240"
  },
  {
    "text": "at them right now so for manual csi this is what the",
    "start": "442240",
    "end": "448479"
  },
  {
    "text": "structure looks like it's split into two main components",
    "start": "448479",
    "end": "455520"
  },
  {
    "text": "first one is controller plug-in that handles the cluster-wide operations like",
    "start": "455520",
    "end": "460639"
  },
  {
    "text": "creating pvc creating volumes at the manula",
    "start": "460639",
    "end": "466560"
  },
  {
    "text": "using the manual service at that particular storage backend and then the second component is",
    "start": "466560",
    "end": "474639"
  },
  {
    "text": "the node plugin which then handles all of the node local operations like mounting volume on a",
    "start": "474639",
    "end": "481759"
  },
  {
    "text": "node and then exposing it to the workloads on that node the important thing to note here is that",
    "start": "481759",
    "end": "488879"
  },
  {
    "text": "uh manual csi doesn't do the mounts by itself it relies on",
    "start": "488879",
    "end": "495680"
  },
  {
    "text": "other third-party csi drivers that are dedicated to",
    "start": "495680",
    "end": "501200"
  },
  {
    "text": "[Music] to whatever file system we are using",
    "start": "501200",
    "end": "507280"
  },
  {
    "text": "on that particular manual share so for example in case of cfs",
    "start": "507280",
    "end": "514320"
  },
  {
    "text": "the workflow is basically this when the kubrick tells manuela csi to",
    "start": "515279",
    "end": "521760"
  },
  {
    "text": "mount a volume then the csi driver asks manuela's release",
    "start": "521760",
    "end": "527630"
  },
  {
    "text": "[Music] what sort of information is needed to mounted share",
    "start": "527630",
    "end": "533760"
  },
  {
    "text": "in case of surface this is the uh monitor ips the root part of the volume",
    "start": "533760",
    "end": "540160"
  },
  {
    "text": "and cfx credentials and those are then forwarded to this ffs csi node plugin",
    "start": "540160",
    "end": "547040"
  },
  {
    "text": "which is a completely separate csi driver and it has",
    "start": "547040",
    "end": "552800"
  },
  {
    "text": "the tool the tools needed to actually carry out the mount on the node and",
    "start": "552800",
    "end": "559519"
  },
  {
    "text": "then expose it to the consumer port so",
    "start": "559519",
    "end": "565040"
  },
  {
    "start": "563000",
    "end": "563000"
  },
  {
    "text": "let's say we have a pvc with manual fast share",
    "start": "565040",
    "end": "570560"
  },
  {
    "text": "how would we go about backing it up this is the workflow we would like to follow",
    "start": "570560",
    "end": "577839"
  },
  {
    "text": "so it consists of six steps crossing the application creating a snapshot",
    "start": "578399",
    "end": "584320"
  },
  {
    "text": "unquestioning creating a volume from the snapshot then backing up this intermediate volume and removing",
    "start": "584320",
    "end": "592000"
  },
  {
    "text": "removing it along with the snapshot so let's break down those",
    "start": "592000",
    "end": "597519"
  },
  {
    "text": "steps one by one classic the application serves two",
    "start": "597519",
    "end": "602839"
  },
  {
    "text": "purposes uh first one is that it stops or pauses the application for",
    "start": "602839",
    "end": "608800"
  },
  {
    "text": "from processing any further requests this is because usually you don't want",
    "start": "608800",
    "end": "614880"
  },
  {
    "text": "to snapshot a live volume that's still being written to",
    "start": "614880",
    "end": "621040"
  },
  {
    "text": "uh at the same time as you are taking a snapshot of the data that you want to",
    "start": "621040",
    "end": "626160"
  },
  {
    "text": "backup the second point here is that",
    "start": "626160",
    "end": "632079"
  },
  {
    "text": "the application of which you are taking of which you are backing up the volumes",
    "start": "632079",
    "end": "639279"
  },
  {
    "text": "uh it might need to be aware of the fact that you are taking the snapshot because",
    "start": "639279",
    "end": "647519"
  },
  {
    "text": "it might store some in-memory buffers or caches that haven't been yet",
    "start": "647519",
    "end": "654000"
  },
  {
    "text": "written to the to the snapshot so it needs to flush",
    "start": "654000",
    "end": "659279"
  },
  {
    "text": "those caches and then you can take a snapshot of the volume otherwise",
    "start": "659279",
    "end": "665519"
  },
  {
    "text": "you could get inconsistent data this is of course very application specific and not all apps need that some",
    "start": "665519",
    "end": "672800"
  },
  {
    "text": "applications don't some applications would even consider this disruptive because",
    "start": "672800",
    "end": "680399"
  },
  {
    "text": "they might prefer availability and not being posed rather",
    "start": "680880",
    "end": "686839"
  },
  {
    "text": "than having consistent state on the disk being",
    "start": "686839",
    "end": "692720"
  },
  {
    "text": "snapshotted so we have created a snapshot now then",
    "start": "692720",
    "end": "699120"
  },
  {
    "text": "step number three unquestioning the application means resuming",
    "start": "699120",
    "end": "705440"
  },
  {
    "text": "resuming it and making it available to process the requests again",
    "start": "705440",
    "end": "712639"
  },
  {
    "text": "and now we can actually think about backing up the data that we have just snapshoted",
    "start": "712639",
    "end": "718560"
  },
  {
    "text": "the problem however is that you can't really backup a snapshot",
    "start": "718560",
    "end": "725199"
  },
  {
    "text": "because as far as csi and kubernetes are concerned",
    "start": "726079",
    "end": "731519"
  },
  {
    "text": "snapshots are these completely opaque",
    "start": "731519",
    "end": "736560"
  },
  {
    "text": "storage specific objects that you cannot really access you cannot access",
    "start": "736560",
    "end": "742720"
  },
  {
    "text": "the underlying data from within kubernetes what you can do though is create a volume from that",
    "start": "742720",
    "end": "750000"
  },
  {
    "text": "snapshot and then this volume you can actually mount somewhere and",
    "start": "750000",
    "end": "756000"
  },
  {
    "text": "walk the directory structure and copy uh the files from it into your backup",
    "start": "756000",
    "end": "762000"
  },
  {
    "text": "location for example an s3 bucket so that's number four",
    "start": "762000",
    "end": "769920"
  },
  {
    "text": "and step number five is uh basically just copying the data into the backup",
    "start": "770639",
    "end": "776079"
  },
  {
    "text": "location and lastly we can remove this intermediate volume and snapshot because they are no longer needed we have",
    "start": "776079",
    "end": "783200"
  },
  {
    "text": "written all the data we we meant to backup for restoration",
    "start": "783200",
    "end": "789600"
  },
  {
    "text": "the workflow is much more relaxed in terms of number of steps",
    "start": "789600",
    "end": "798000"
  },
  {
    "text": "we just have to download the data from the backup location and into the original volume and then we somehow",
    "start": "798000",
    "end": "804639"
  },
  {
    "text": "round the application this is of course uh very specific to each case",
    "start": "804639",
    "end": "810320"
  },
  {
    "text": "but in general as far as the volume backups are concerned this is",
    "start": "810320",
    "end": "818639"
  },
  {
    "text": "what we would do so what this means for manula and cfs",
    "start": "819199",
    "end": "824800"
  },
  {
    "text": "csi drivers is that as long as they",
    "start": "824800",
    "end": "829839"
  },
  {
    "text": "they have the capabilities to fulfill all of the actions we have seen",
    "start": "829839",
    "end": "835120"
  },
  {
    "text": "in the workflow users can choose whatever",
    "start": "835120",
    "end": "841360"
  },
  {
    "text": "backup solution they wish to use and as as long as this solution also",
    "start": "841519",
    "end": "846800"
  },
  {
    "text": "supports csi and is able to execute this workflow then we",
    "start": "846800",
    "end": "852720"
  },
  {
    "text": "are good to go uh we see that the workflow is very",
    "start": "852720",
    "end": "858890"
  },
  {
    "text": "[Applause] heavily reliant on snapshotting and the good news is that both of those",
    "start": "858890",
    "end": "866000"
  },
  {
    "text": "csi drivers support snapshots and creating snapshot creating volumes from snapshots",
    "start": "866000",
    "end": "872079"
  },
  {
    "text": "the bad news is however that this operation is extremely",
    "start": "872079",
    "end": "877920"
  },
  {
    "text": "expensive that's because ffs doesn't really know how to create a",
    "start": "877920",
    "end": "885920"
  },
  {
    "text": "thinly provisioned volume from a snapshot source so",
    "start": "886959",
    "end": "892720"
  },
  {
    "text": "what you would have to normally do is to copy the files",
    "start": "892720",
    "end": "898480"
  },
  {
    "text": "from the snapshot and into the target volume and as you can imagine that's very",
    "start": "898480",
    "end": "903839"
  },
  {
    "text": "inefficient so that's one of the things we've been working on uh",
    "start": "903839",
    "end": "910079"
  },
  {
    "text": "before cfs csi is to have the ability to create",
    "start": "910079",
    "end": "916720"
  },
  {
    "text": "volumes from snapshots in constant time technically this is",
    "start": "916720",
    "end": "922959"
  },
  {
    "text": "quite trivial because cfs exposes snapshots right there in",
    "start": "922959",
    "end": "929120"
  },
  {
    "text": "in the volume in this special dot snap folder and",
    "start": "929120",
    "end": "934880"
  },
  {
    "text": "then the individual then the subdirectories of this dot snap",
    "start": "934880",
    "end": "940959"
  },
  {
    "text": "folder uh are basically the the snapshots that",
    "start": "940959",
    "end": "946800"
  },
  {
    "text": "they have taken uh they are read only but that's fine uh so",
    "start": "946800",
    "end": "952560"
  },
  {
    "text": "what we do basically is just store reference to some particular",
    "start": "952560",
    "end": "959839"
  },
  {
    "text": "snapshot stored in the persistent volume object during the provisioning phase",
    "start": "959839",
    "end": "965360"
  },
  {
    "text": "and when it comes to mounting this volume we just navigate to the correct snapshot and",
    "start": "965360",
    "end": "974079"
  },
  {
    "text": "[Music] present that to the ports on the on the node",
    "start": "974280",
    "end": "981279"
  },
  {
    "text": "this feature is basically done and it will be part of the 3.7 release of surface csi scheduled",
    "start": "981279",
    "end": "987839"
  },
  {
    "text": "sometime next month so that's good similarly uh the same feature needs to",
    "start": "987839",
    "end": "995600"
  },
  {
    "text": "be implemented in manual csi and in this case the",
    "start": "995600",
    "end": "1001440"
  },
  {
    "text": "implementation of mounting the the snapshot is even easier made by the fact that",
    "start": "1001440",
    "end": "1010040"
  },
  {
    "text": "basically if this guy already implements the logic to mount cfs snapshot then",
    "start": "1010560",
    "end": "1017199"
  },
  {
    "text": "manual csi only needs to pass in the correct parameters to light surfaces i know that",
    "start": "1017199",
    "end": "1025038"
  },
  {
    "text": "yes mount this snapshot and that's it basically so in theory this is",
    "start": "1025039",
    "end": "1031520"
  },
  {
    "text": "very easy [Music] in practice this is also the",
    "start": "1031520",
    "end": "1037438"
  },
  {
    "text": "time where we hit our first roll roadblock issue and it's connected to the fact how",
    "start": "1037439",
    "end": "1044798"
  },
  {
    "text": "uh cfs exposes uh snapshots within this special.snap directory",
    "start": "1044799",
    "end": "1053720"
  },
  {
    "text": "and well because of a bug or",
    "start": "1054000",
    "end": "1059440"
  },
  {
    "text": "incorrect handling of of snapshots names we are basically unable to",
    "start": "1059440",
    "end": "1066880"
  },
  {
    "text": "access snapshot data within this dot snap directory uh",
    "start": "1066880",
    "end": "1072480"
  },
  {
    "text": "so we are sort of stuck at this point uh there are likely already uh",
    "start": "1072480",
    "end": "1078960"
  },
  {
    "text": "patches for for this issue upstream and it's big being uh worked on but",
    "start": "1078960",
    "end": "1084960"
  },
  {
    "text": "uh right now my nose is i cannot uh implement this feature",
    "start": "1084960",
    "end": "1091039"
  },
  {
    "start": "1091000",
    "end": "1091000"
  },
  {
    "text": "but as soon as this is done uh we can move to the next step which is then",
    "start": "1091039",
    "end": "1097280"
  },
  {
    "text": "uh choosing the the correct backup tool to",
    "start": "1097280",
    "end": "1104000"
  },
  {
    "text": "uh to carry out the backup procedures because",
    "start": "1104000",
    "end": "1110320"
  },
  {
    "text": "what we are doing is uh implementing uh",
    "start": "1110320",
    "end": "1116480"
  },
  {
    "text": "the functionality needed by the csi drivers the other part of the equation is to actually have some some tool to",
    "start": "1116480",
    "end": "1124559"
  },
  {
    "text": "actually execute the workflow of of the backup and",
    "start": "1124559",
    "end": "1130559"
  },
  {
    "text": "that's one of the tools you can see up here",
    "start": "1130960",
    "end": "1136160"
  },
  {
    "text": "some of them are proprietary some of them are open source",
    "start": "1136160",
    "end": "1141600"
  },
  {
    "text": "and the approach we are taking with our users is basically it's up to them whatever tool",
    "start": "1141919",
    "end": "1149760"
  },
  {
    "text": "they wish to to use because they differ all in quality in",
    "start": "1149760",
    "end": "1156160"
  },
  {
    "text": "the features they provide and it's really the users who know their",
    "start": "1156160",
    "end": "1161760"
  },
  {
    "text": "applications well and they can decide what features are",
    "start": "1161760",
    "end": "1167440"
  },
  {
    "text": "important for their use case right now we are",
    "start": "1167440",
    "end": "1173600"
  },
  {
    "text": "evaluating valero",
    "start": "1173919",
    "end": "1177559"
  },
  {
    "text": "it's an open source tool uh it provides scheduled backups pre and",
    "start": "1179200",
    "end": "1184320"
  },
  {
    "text": "post backup hooks data retention so uh things that our users would be",
    "start": "1184320",
    "end": "1190160"
  },
  {
    "text": "interested in and actually our colleagues from the drupal infrastructure team etc",
    "start": "1190160",
    "end": "1199039"
  },
  {
    "text": "they are already using valero without snapshots though because they cannot use them right now",
    "start": "1199039",
    "end": "1206000"
  },
  {
    "text": "and they were kind enough to share their experience experiences with us",
    "start": "1206000",
    "end": "1212159"
  },
  {
    "text": "so in general valero works very well uh",
    "start": "1212159",
    "end": "1219960"
  },
  {
    "text": "works well for when you want to backup the cluster resource definitions or the objects",
    "start": "1220720",
    "end": "1229200"
  },
  {
    "text": "but as soon as you want to backup persistent volumes and the underlying data",
    "start": "1229200",
    "end": "1235919"
  },
  {
    "text": "they point to that's when things start to get more interesting",
    "start": "1235919",
    "end": "1242080"
  },
  {
    "text": "so the rule is basically that if the back-end storage system is able to",
    "start": "1242080",
    "end": "1249039"
  },
  {
    "text": "upload the data to the backup location by itself without by",
    "start": "1249039",
    "end": "1255360"
  },
  {
    "text": "itself and in the background without you having to deploy some sort of",
    "start": "1255360",
    "end": "1260799"
  },
  {
    "text": "external tool within in your kubernetes cluster that does the copying for you so",
    "start": "1260799",
    "end": "1265919"
  },
  {
    "text": "as long as you don't need this tool uh you're fine and this is the case for ebs google press 10 disks",
    "start": "1265919",
    "end": "1273440"
  },
  {
    "text": "azure managed disks and similar",
    "start": "1273440",
    "end": "1278480"
  },
  {
    "text": "there are storage systems where this is not the case ceph is one of them",
    "start": "1278480",
    "end": "1284240"
  },
  {
    "text": "and if that's your case as well you you need",
    "start": "1284240",
    "end": "1290320"
  },
  {
    "text": "this external tool that does the coping from your volume and into for example s3",
    "start": "1290320",
    "end": "1295600"
  },
  {
    "text": "bucket in case of valero the tool is called drastic",
    "start": "1295600",
    "end": "1302400"
  },
  {
    "text": "and uh so it does what i just said copying file system into an s3 bucket",
    "start": "1302400",
    "end": "1309679"
  },
  {
    "text": "and also the other way around downloading the data from backhead into",
    "start": "1309679",
    "end": "1315679"
  },
  {
    "text": "a volume and it also provides deduplication uh",
    "start": "1315679",
    "end": "1322880"
  },
  {
    "text": "encryption or very cool features but",
    "start": "1322880",
    "end": "1328080"
  },
  {
    "text": "this is also the place where we",
    "start": "1328080",
    "end": "1332440"
  },
  {
    "text": "where we see most of the issues so the first one is large memory consumption",
    "start": "1333360",
    "end": "1339919"
  },
  {
    "text": "uh okay this is not",
    "start": "1339919",
    "end": "1345440"
  },
  {
    "text": "very visible but um so our colleagues from the drupal drupal",
    "start": "1345520",
    "end": "1350559"
  },
  {
    "text": "infratent have seen peaks of even 25 gigabytes of memory on the note",
    "start": "1350559",
    "end": "1358640"
  },
  {
    "text": "but as of valero 171 it's gotten significantly better",
    "start": "1358640",
    "end": "1363679"
  },
  {
    "text": "it's around 8 gigs and",
    "start": "1363679",
    "end": "1369440"
  },
  {
    "text": "but still this is expected because",
    "start": "1369440",
    "end": "1374720"
  },
  {
    "text": "rustic needs to do the deduplication and that takes a lot of memory to keep the",
    "start": "1374720",
    "end": "1380000"
  },
  {
    "text": "indices around so uh it's understandable but the problem is that the node on which uh the rustic pot",
    "start": "1380000",
    "end": "1389280"
  },
  {
    "text": "is running might not be large enough to carry out the backup",
    "start": "1389280",
    "end": "1394640"
  },
  {
    "text": "operation uh in at which point",
    "start": "1394640",
    "end": "1399919"
  },
  {
    "text": "or it just runs out of memory it is killed which brings us to the next issue",
    "start": "1399919",
    "end": "1405919"
  },
  {
    "text": "that failed backups stay failed there are no retries um",
    "start": "1405919",
    "end": "1411760"
  },
  {
    "text": "so when a when a rustic pod goes out of memory uh",
    "start": "1411760",
    "end": "1416880"
  },
  {
    "text": "it is killed and then it is restarted because valero deploys it as a",
    "start": "1416880",
    "end": "1422480"
  },
  {
    "text": "demand set so it is restarted right like any other demand support would be",
    "start": "1422480",
    "end": "1428400"
  },
  {
    "text": "and you would expect that rustic would be able to [Music]",
    "start": "1428400",
    "end": "1433840"
  },
  {
    "text": "continue where it left off before it was killed due to uh",
    "start": "1433840",
    "end": "1440159"
  },
  {
    "text": "running out of memory but this is not what we saw uh what we observed was basically it just got stuck",
    "start": "1440159",
    "end": "1448320"
  },
  {
    "text": "uh until the backup action failed uh backup action uh",
    "start": "1448320",
    "end": "1455279"
  },
  {
    "text": "timed out at which point the valero controller just move on to the",
    "start": "1455279",
    "end": "1462159"
  },
  {
    "text": "next backup item and this all was done",
    "start": "1462159",
    "end": "1467279"
  },
  {
    "text": "silently we didn't see any errors only after the whole backup job finished",
    "start": "1467279",
    "end": "1474320"
  },
  {
    "text": "and and that's when we could see the the issue but",
    "start": "1474320",
    "end": "1481200"
  },
  {
    "text": "it wasn't a very good user experience and then the last issue",
    "start": "1482159",
    "end": "1490000"
  },
  {
    "text": "we have seen is scaling issues and this is linked to the to the fact that",
    "start": "1490000",
    "end": "1496480"
  },
  {
    "text": "valero processes backup items in sequence one by one",
    "start": "1496480",
    "end": "1502400"
  },
  {
    "text": "and if you have a lot of pvcs and those all of those pvcs need to be copied out",
    "start": "1502400",
    "end": "1510240"
  },
  {
    "text": "using rustic then you can imagine you have you might have some problems so",
    "start": "1510240",
    "end": "1516559"
  },
  {
    "text": "again our colleagues from the drupal infra team uh they are managing around 1000 pvcs",
    "start": "1516559",
    "end": "1523520"
  },
  {
    "text": "for their infrastructure they are fairly",
    "start": "1523520",
    "end": "1528880"
  },
  {
    "text": "large and they wanted to have daily backups but because",
    "start": "1528880",
    "end": "1536070"
  },
  {
    "text": "[Music] the time it takes to back up the whole infrastructure takes",
    "start": "1536070",
    "end": "1541520"
  },
  {
    "text": "almost 48 hours they literally cannot have daily pickups",
    "start": "1541520",
    "end": "1547440"
  },
  {
    "text": "so where to go from here",
    "start": "1547600",
    "end": "1552158"
  },
  {
    "start": "1549000",
    "end": "1549000"
  },
  {
    "text": "it's not all that bad velero developers are working hard on",
    "start": "1553039",
    "end": "1558720"
  },
  {
    "text": "improving this all of those issues you can see it on their repository page",
    "start": "1558720",
    "end": "1564559"
  },
  {
    "text": "the community is very active and",
    "start": "1564559",
    "end": "1569600"
  },
  {
    "text": "from what i could see they are planning to improving their csi snap shooting",
    "start": "1569600",
    "end": "1575679"
  },
  {
    "text": "capabilities hopefully also providing support for the backup workflow we've seen a couple of",
    "start": "1575679",
    "end": "1582000"
  },
  {
    "text": "slides earlier after that they are",
    "start": "1582000",
    "end": "1587600"
  },
  {
    "text": "considering adding alternatives to rustic for example copia",
    "start": "1587600",
    "end": "1594159"
  },
  {
    "text": "and i'd like to point out that rustic is by no means like a",
    "start": "1594159",
    "end": "1599919"
  },
  {
    "text": "bad tool our theft team uses it uses it in their",
    "start": "1599919",
    "end": "1605200"
  },
  {
    "text": "uh internal operations on millions of files and it works just fine it's just",
    "start": "1605200",
    "end": "1611520"
  },
  {
    "text": "the nature of container environment is perhaps too",
    "start": "1611520",
    "end": "1616640"
  },
  {
    "text": "too volatile for for it to be running and",
    "start": "1616640",
    "end": "1622400"
  },
  {
    "text": "it's nice that we get to see some alternatives for the cases where",
    "start": "1622480",
    "end": "1628000"
  },
  {
    "text": "where it makes sense and because we",
    "start": "1628000",
    "end": "1634000"
  },
  {
    "start": "1632000",
    "end": "1632000"
  },
  {
    "text": "were pretty curious about this comparison between resting and copia we've made some benchmarks",
    "start": "1634000",
    "end": "1641360"
  },
  {
    "text": "those are very much just preliminary benchmarks on a small data set in very controlled",
    "start": "1641360",
    "end": "1649600"
  },
  {
    "text": "conditions so not really something to be taken [Music]",
    "start": "1649600",
    "end": "1656000"
  },
  {
    "text": "very seriously but it gives an idea of what things might look like once the",
    "start": "1656000",
    "end": "1661360"
  },
  {
    "text": "once valero receives this copia support so what we had was a volume with about",
    "start": "1661360",
    "end": "1668640"
  },
  {
    "text": "1.5 million files this was just uncompressed copies of the",
    "start": "1668640",
    "end": "1674080"
  },
  {
    "text": "linux kernel and it was the same linux kernel so majority of the files were well all the",
    "start": "1674080",
    "end": "1681120"
  },
  {
    "text": "files were the same which using which we could exercise this",
    "start": "1681120",
    "end": "1688159"
  },
  {
    "text": "deduplication feature and compare rustic to copia really well",
    "start": "1688159",
    "end": "1695120"
  },
  {
    "text": "and as you can see on the numbers uh just judging by the elapsed time copia",
    "start": "1695120",
    "end": "1703039"
  },
  {
    "text": "seems to perform better it's how it's pay spends",
    "start": "1703039",
    "end": "1708159"
  },
  {
    "text": "half the time of what the rustic needs to have to complete both backup and restore",
    "start": "1708159",
    "end": "1716080"
  },
  {
    "text": "the memory consumption is there is even larger difference i'm",
    "start": "1716080",
    "end": "1723039"
  },
  {
    "text": "not exactly sure what's going on here yet uh because it's quite significant and",
    "start": "1723039",
    "end": "1730880"
  },
  {
    "text": "the s3 bucket size is also lower in case of copia because copia",
    "start": "1730880",
    "end": "1736640"
  },
  {
    "text": "does maybe better splitting the data into objects and it also does compression so",
    "start": "1736640",
    "end": "1744000"
  },
  {
    "text": "that's one of the reasons why the bucket size in case of copy is smaller",
    "start": "1744000",
    "end": "1752240"
  },
  {
    "text": "and for resting the for restoring the story is very much the same",
    "start": "1752240",
    "end": "1759520"
  },
  {
    "start": "1759000",
    "end": "1759000"
  },
  {
    "text": "so to conclude what we wanted to achieve was providing our users with the ability",
    "start": "1759520",
    "end": "1767200"
  },
  {
    "text": "to have consistent backups for that you need snap shooting support and sadly",
    "start": "1767200",
    "end": "1773679"
  },
  {
    "text": "right now the snapshooting sport is not there yet but it's being addressed",
    "start": "1773679",
    "end": "1781679"
  },
  {
    "text": "so if you need back uh snapshot support as well",
    "start": "1781679",
    "end": "1786720"
  },
  {
    "text": "you you have to wait and so do we if you don't and your application is not that",
    "start": "1786720",
    "end": "1793600"
  },
  {
    "text": "sensitive to have in maybe inconsistent state on the",
    "start": "1793600",
    "end": "1799279"
  },
  {
    "text": "on the disk and that being backed up then you're good to go but",
    "start": "1799279",
    "end": "1804559"
  },
  {
    "text": "also being mindful of the limitations that we have experienced so",
    "start": "1804559",
    "end": "1809860"
  },
  {
    "text": "[Music] pretty large memory consumption and scaling issues",
    "start": "1809860",
    "end": "1816960"
  },
  {
    "text": "but in the end they are all being addressed so",
    "start": "1816960",
    "end": "1823360"
  },
  {
    "text": "uh we are looking forward to that all right so that's it uh thank you and are there any",
    "start": "1823360",
    "end": "1831440"
  },
  {
    "text": "questions [Applause]",
    "start": "1831440",
    "end": "1842619"
  },
  {
    "text": "there is mike by the way [Applause]",
    "start": "1847200",
    "end": "1856320"
  },
  {
    "text": "how do you acquire us your applications sorry uh how do you cover us and unquies",
    "start": "1856320",
    "end": "1861519"
  },
  {
    "text": "your application because there is no native support in kubernetes there is none but",
    "start": "1861519",
    "end": "1866960"
  },
  {
    "text": "in valero you can run [Music] bash script that does whatever you or",
    "start": "1866960",
    "end": "1873039"
  },
  {
    "text": "the application needs to be done for it requires so this is",
    "start": "1873039",
    "end": "1878880"
  },
  {
    "text": "the support is there in valero",
    "start": "1878880",
    "end": "1882799"
  },
  {
    "text": "i'm just wondering why you're not using the uh dsf",
    "start": "1887679",
    "end": "1893279"
  },
  {
    "text": "support for snapshots so mirroring to another ceph cluster",
    "start": "1893279",
    "end": "1898640"
  },
  {
    "text": "if i'm not mistaken this is not yet available in manila or is it",
    "start": "1899679",
    "end": "1906480"
  },
  {
    "text": "uh so we have uh manual developer right there",
    "start": "1906480",
    "end": "1913039"
  },
  {
    "text": "no sep supports um mirroring the snapshots to another cluster so right but in some in some",
    "start": "1913120",
    "end": "1920880"
  },
  {
    "text": "cases you might want to back up to some something else than safe",
    "start": "1920880",
    "end": "1926640"
  },
  {
    "text": "and in those cases you might need to just have some general tool that does",
    "start": "1926640",
    "end": "1933679"
  },
  {
    "text": "s3 bucket clone just saying it would save you",
    "start": "1933679",
    "end": "1939760"
  },
  {
    "text": "lots of time okay",
    "start": "1939760",
    "end": "1944600"
  },
  {
    "text": "hi uh it was not clear when you do a backups here right there was one backup but what about multiple backups like",
    "start": "1956000",
    "end": "1961919"
  },
  {
    "text": "scheduled backups are they incremental not incremental what are these different tools provide uh in related to",
    "start": "1961919",
    "end": "1968880"
  },
  {
    "text": "uh so both uh rustic and copia by default well",
    "start": "1968880",
    "end": "1975120"
  },
  {
    "text": "that's the only thing they can do is incremental backups and on the restore side uh",
    "start": "1975120",
    "end": "1981200"
  },
  {
    "text": "do they uh restore all the content before they uh allow application uh start",
    "start": "1981200",
    "end": "1991640"
  },
  {
    "text": "it depends but yes in general i you first need to",
    "start": "1991840",
    "end": "1997120"
  },
  {
    "text": "restore your your data before the application is able to be",
    "start": "1997120",
    "end": "2002320"
  },
  {
    "text": "[Music] presumed yeah because i uh one of the slides you mentioned like almost 65 terabyte of",
    "start": "2002320",
    "end": "2008640"
  },
  {
    "text": "data on one of the volume right so restoring that could take probably many hours so",
    "start": "2008640",
    "end": "2015679"
  },
  {
    "text": "yeah the all there is no other way i mean you lost your data you need to first to",
    "start": "2015679",
    "end": "2022720"
  },
  {
    "text": "get it back and only then continue but uh there are tools uh other than valero that enable",
    "start": "2022720",
    "end": "2031120"
  },
  {
    "text": "you to define your own workflows for example this one canister",
    "start": "2031120",
    "end": "2038000"
  },
  {
    "text": "which i found was very interesting project you can define your own workflow and",
    "start": "2038000",
    "end": "2046080"
  },
  {
    "text": "if your application has specific needs you can declare them in that workflow and",
    "start": "2046080",
    "end": "2054800"
  },
  {
    "text": "maybe that's the way okay thank you",
    "start": "2054800",
    "end": "2059639"
  },
  {
    "text": "hi um what do you do with application that you can't pause",
    "start": "2065359",
    "end": "2070960"
  },
  {
    "text": "like that's impossible to stop because they have to keep on processing",
    "start": "2070960",
    "end": "2076638"
  },
  {
    "text": "well then you might suffer from inconsistencies in the backups but",
    "start": "2077919",
    "end": "2083839"
  },
  {
    "text": "maybe having some backups are better than to have none maybe having",
    "start": "2083839",
    "end": "2089440"
  },
  {
    "text": "inconsistent backups is just what you have to deal with but yeah it's an issue",
    "start": "2089440",
    "end": "2095200"
  },
  {
    "text": "that we don't have clear answer to okay",
    "start": "2095200",
    "end": "2101720"
  },
  {
    "text": "thanks a lot for the presentation um my question would be have you built",
    "start": "2107040",
    "end": "2112240"
  },
  {
    "text": "any automation to verify backups like to verify that you can actually restore the",
    "start": "2112240",
    "end": "2118560"
  },
  {
    "text": "data after the backup so right now we are at the point of just",
    "start": "2118560",
    "end": "2124160"
  },
  {
    "text": "evaluating this in our kubernetes offerings so we don't have we are not at",
    "start": "2124160",
    "end": "2129920"
  },
  {
    "text": "the point of having a proper ci for this but",
    "start": "2129920",
    "end": "2135599"
  },
  {
    "text": "definitely this is something we would be aiming to have",
    "start": "2135599",
    "end": "2141640"
  },
  {
    "text": "okay i think we have actually run out of time 35 minutes",
    "start": "2149680",
    "end": "2156079"
  },
  {
    "text": "uh thank you for your attention and enjoy your lunch",
    "start": "2156079",
    "end": "2162599"
  }
]