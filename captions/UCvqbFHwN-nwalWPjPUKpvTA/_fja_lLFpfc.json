[
  {
    "text": "hello everyone and welcome to siggy scheduling deep dive I'm Bobbi salamat",
    "start": "89",
    "end": "5730"
  },
  {
    "text": "I'm here with Jonathan bass sorry he we are both software engineers at Google I'm also a co-lead of cig scheduling",
    "start": "5730",
    "end": "13740"
  },
  {
    "text": "today we are going to talk about some of the features of scheduler and also give you a general idea of what the scheduler",
    "start": "13740",
    "end": "20310"
  },
  {
    "text": "is hopefully these features are gonna be useful for you if you plan to run medium",
    "start": "20310",
    "end": "27119"
  },
  {
    "text": "to large scale workloads on kubernetes so with that let's take a look at what",
    "start": "27119",
    "end": "35790"
  },
  {
    "text": "kubernetes the scheduler is actually worth doing and what it is so kubernetes",
    "start": "35790",
    "end": "40980"
  },
  {
    "text": "is scheduler like many schedulers in many other cluster management service",
    "start": "40980",
    "end": "46320"
  },
  {
    "text": "service or software is responsible for placing pods on nodes during that",
    "start": "46320",
    "end": "54239"
  },
  {
    "text": "process it's actually uses various information and it's you it uses a",
    "start": "54239",
    "end": "60989"
  },
  {
    "text": "centralized architecture so in a centralized architecture actually kubernetes the scheduler must know about",
    "start": "60989",
    "end": "67409"
  },
  {
    "text": "everything in the cluster in order to make scheduling decisions so one option",
    "start": "67409",
    "end": "73710"
  },
  {
    "text": "to know about everything in a cluster is to go and ask from a like API server to",
    "start": "73710",
    "end": "80220"
  },
  {
    "text": "to give it all the information but it cannot be done for every single part otherwise performance would be very very",
    "start": "80220",
    "end": "87210"
  },
  {
    "text": "low so what it does it in fact is is that it caches a lot of information about the state of the cluster and in",
    "start": "87210",
    "end": "94680"
  },
  {
    "text": "order to keep this cache up-to-date it relies heavily on the events that it",
    "start": "94680",
    "end": "100259"
  },
  {
    "text": "receives from the API server so it receives a variance for new pods new nodes changes in the pods or changes in",
    "start": "100259",
    "end": "107610"
  },
  {
    "text": "the nodes or resources and many other things in the cluster so that those",
    "start": "107610",
    "end": "112920"
  },
  {
    "text": "events are a key part of the whole architecture of schedule and it's one of",
    "start": "112920",
    "end": "119520"
  },
  {
    "text": "the few components actually in kubernetes that relies heavily on on events and never resyncs it's",
    "start": "119520",
    "end": "125969"
  },
  {
    "text": "information with the API server now with this cache when a new pod arrives",
    "start": "125969",
    "end": "133800"
  },
  {
    "text": "caterer has to make decisions so all these parts usually go to the scheduler queue and schedule picks one part at a",
    "start": "133800",
    "end": "141870"
  },
  {
    "text": "time at least that's what it does today it might change in the future but that's what it does today it only picks one",
    "start": "141870",
    "end": "147390"
  },
  {
    "text": "part at a time and tries to find a proper note for that part when it tries",
    "start": "147390",
    "end": "153870"
  },
  {
    "text": "to find out now it goes through several stages or similar steps the first one is",
    "start": "153870",
    "end": "159210"
  },
  {
    "text": "to do a feasibility check of those nodes in a cluster in the feasibility check it",
    "start": "159210",
    "end": "165510"
  },
  {
    "text": "actually tries to filter out the nodes that are not possible candidates for running the pot alright so for example",
    "start": "165510",
    "end": "172740"
  },
  {
    "text": "in this slide node 1 and node n are not possible candidates node 2 3 & 4 are",
    "start": "172740",
    "end": "179610"
  },
  {
    "text": "possible candidates so it filters out the rest of the nodes and focuses on the nodes that are feasible for this part",
    "start": "179610",
    "end": "185550"
  },
  {
    "text": "and then in the next step it tries to find the best node among these possible candidates so this is step in kubernetes",
    "start": "185550",
    "end": "195360"
  },
  {
    "text": "the scheduler jargon we call it like priority functions it runs a bunch of priority functions this is a little bit",
    "start": "195360",
    "end": "201390"
  },
  {
    "text": "of unfortunate naming because priority can be confused with many other priority",
    "start": "201390",
    "end": "206580"
  },
  {
    "text": "in a system like pot fire it is another concept which is completely different",
    "start": "206580",
    "end": "212310"
  },
  {
    "text": "thing it has nothing to do with these priority functions it's essentially the priority functions for us do ranking of",
    "start": "212310",
    "end": "218850"
  },
  {
    "text": "these nodes and in the process of ranking scheduler tries to find the best node to run the part an example of these",
    "start": "218850",
    "end": "227430"
  },
  {
    "text": "ranking is that for example which one of these nodes has the largest number of images that their pod requires or",
    "start": "227430",
    "end": "236150"
  },
  {
    "text": "depending on the scheduling policy it may use like best fit or worst fit to",
    "start": "236150",
    "end": "242310"
  },
  {
    "text": "find the node that is better for running the pod so among these 3 nodes that we",
    "start": "242310",
    "end": "249270"
  },
  {
    "text": "have in in this slide note 3 happens to have the hi-c score so scheduler will",
    "start": "249270",
    "end": "258390"
  },
  {
    "text": "choose node 3 for running the pot right and then it goes to the last step of the",
    "start": "258390",
    "end": "264840"
  },
  {
    "text": "scheduling which is actually binding of a pod to that note in that process",
    "start": "264840",
    "end": "270960"
  },
  {
    "text": "scheduler doesn't do a whole lot other than updating pod spec with this note",
    "start": "270960",
    "end": "276960"
  },
  {
    "text": "name and what happens at that stage is that API server sends an event to the",
    "start": "276960",
    "end": "283080"
  },
  {
    "text": "corresponding cubelet on that node and tell that cubelet that you have a new",
    "start": "283080",
    "end": "288600"
  },
  {
    "text": "pod now cubelet goes and does its own checking and if it finds the pod as a",
    "start": "288600",
    "end": "294180"
  },
  {
    "text": "possible possible candidate for for itself you know it there or there or",
    "start": "294180",
    "end": "299430"
  },
  {
    "text": "change there are checks that could fail and cubelet could reject this part but if the checks pass then the cube later",
    "start": "299430",
    "end": "306930"
  },
  {
    "text": "starts running that part by preparing its containers and stuff like that this",
    "start": "306930",
    "end": "312690"
  },
  {
    "text": "is a pretty high-level overview of what the scheduler does when it tries to schedule a part with that I'm gonna hand",
    "start": "312690",
    "end": "320670"
  },
  {
    "text": "it over to Jonathan to talk about some of the features of kubernetes is scheduler that may be useful for you if",
    "start": "320670",
    "end": "327330"
  },
  {
    "text": "you run any production workload on kubernetes and then later on I'm gonna",
    "start": "327330",
    "end": "333150"
  },
  {
    "text": "talk about some of the more advanced features and give you a an overview of the roadmap hi so we don't have a lot of",
    "start": "333150",
    "end": "346800"
  },
  {
    "text": "time to get through a lot of these features we might not be able to go into the level of detail that we wanted to go",
    "start": "346800",
    "end": "352860"
  },
  {
    "text": "into so in the top corner we've left links and the slides or on the schedule",
    "start": "352860",
    "end": "358260"
  },
  {
    "text": "to all of the documentation for these features and we'll try to leave time for questions at the end or find us and talk",
    "start": "358260",
    "end": "365280"
  },
  {
    "text": "to us in person or on slack so several",
    "start": "365280",
    "end": "371190"
  },
  {
    "text": "different scenarios that you can use the scheduler to solve one of the simple",
    "start": "371190",
    "end": "379710"
  },
  {
    "text": "ones Oh before that we need to talk about how most of these features work so you're",
    "start": "379710",
    "end": "387030"
  },
  {
    "text": "probably familiar with labels these are key value pairs arbitrary key value",
    "start": "387030",
    "end": "392730"
  },
  {
    "text": "pairs that are on any API object you can use them to identify these objects and",
    "start": "392730",
    "end": "398540"
  },
  {
    "text": "then you can query the API server to get a specific set of those objects so in",
    "start": "398540",
    "end": "405320"
  },
  {
    "text": "all of these diagrams we use these circles with gears to represent pods so",
    "start": "405320",
    "end": "410390"
  },
  {
    "text": "in this example we have four pods all of them have the label app equals my app but they have these phase and roll",
    "start": "410390",
    "end": "417910"
  },
  {
    "text": "labels that are different so if you were to query pods with app equals my app you",
    "start": "417910",
    "end": "423830"
  },
  {
    "text": "would get all four but if you say app equals my app and role equals Fe you would only get these",
    "start": "423830",
    "end": "429350"
  },
  {
    "text": "two so given that how could we target a",
    "start": "429350",
    "end": "435140"
  },
  {
    "text": "specific set of nodes for your pod so we have something called node affinity",
    "start": "435140",
    "end": "440750"
  },
  {
    "text": "which uses one of these selectors to get a specific set of notes so in this case we require that our pod will run on a",
    "start": "440750",
    "end": "450200"
  },
  {
    "text": "node that matches this label selector so when we talk about filtering out the nodes node one would would get filtered",
    "start": "450200",
    "end": "457400"
  },
  {
    "text": "out as not a viable candidate for this pod to run on because it doesn't have a",
    "start": "457400",
    "end": "462500"
  },
  {
    "text": "own central sometimes you have related",
    "start": "462500",
    "end": "468530"
  },
  {
    "text": "services that you want to run with each other so for that we have pod affinity",
    "start": "468530",
    "end": "473630"
  },
  {
    "text": "pod affinity is a way to say in this case our service a wants to run close to",
    "start": "473630",
    "end": "481130"
  },
  {
    "text": "a pod with the label service B and what do we mean when we say close to is defined by this topology key when the",
    "start": "481130",
    "end": "488360"
  },
  {
    "text": "topology key is own we say we want our service to be in the same zone as any pod with label service equals B so here",
    "start": "488360",
    "end": "496880"
  },
  {
    "text": "we can see that the central zone has service B on node two but node 2 is out",
    "start": "496880",
    "end": "502130"
  },
  {
    "text": "of resources in this example and so node 3 is in the same zone and therefore the",
    "start": "502130",
    "end": "509630"
  },
  {
    "text": "pod can be scheduled",
    "start": "509630",
    "end": "512590"
  },
  {
    "text": "so the question was are there standard topology keys the answer is yes and so the full name is actually like",
    "start": "515180",
    "end": "520760"
  },
  {
    "text": "kubernetes do slash zone they are I'm pretty sure in the documentation but and",
    "start": "520760",
    "end": "527210"
  },
  {
    "text": "one of the other ones is in this next example you could specify hostname which",
    "start": "527210",
    "end": "532250"
  },
  {
    "text": "means I want to run on the same node as now in this case there's only one",
    "start": "532250",
    "end": "537680"
  },
  {
    "text": "service B and that node doesn't have any resources so our pod cannot be scheduled",
    "start": "537680",
    "end": "544450"
  },
  {
    "text": "here so instead of specifying this as a hard requirement it can also be",
    "start": "544450",
    "end": "549830"
  },
  {
    "text": "specified as a preference and so what you can do is this actually uses both to",
    "start": "549830",
    "end": "556400"
  },
  {
    "text": "say that our service requires to run in the same zone as service B but we prefer",
    "start": "556400",
    "end": "562340"
  },
  {
    "text": "to run on the same node as service B and since we cannot schedule on to the same",
    "start": "562340",
    "end": "568580"
  },
  {
    "text": "no to service B at least we can be in the same zone so another type of",
    "start": "568580",
    "end": "576980"
  },
  {
    "text": "affinity is pod anti affinity and this causes pods to be scheduled away from",
    "start": "576980",
    "end": "582380"
  },
  {
    "text": "each other the topology key is the same concept so here we're saying I want to",
    "start": "582380",
    "end": "587750"
  },
  {
    "text": "run my service on a node that does not have any pod with service equals a and",
    "start": "587750",
    "end": "593630"
  },
  {
    "text": "you'll notice in this example that service equals a is also on our own pods metadata so this is a self reference",
    "start": "593630",
    "end": "600530"
  },
  {
    "text": "saying I don't want two instances of my pod to run on the same node and this is",
    "start": "600530",
    "end": "607700"
  },
  {
    "text": "you know useful for something that you know you really want to load balanced and and not have you know double the",
    "start": "607700",
    "end": "614240"
  },
  {
    "text": "traffic going to one node but you might",
    "start": "614240",
    "end": "619340"
  },
  {
    "text": "still run into the case where a pod becomes unschedulable because of this and so this can also be set as",
    "start": "619340",
    "end": "625750"
  },
  {
    "text": "preference instead of a hard requirement",
    "start": "625750",
    "end": "630610"
  },
  {
    "text": "and the other important thing to note is that anti affinity pod anti affinity is",
    "start": "632170",
    "end": "638270"
  },
  {
    "text": "symmetric so a pod which is already scheduled on a node can affect the",
    "start": "638270",
    "end": "644210"
  },
  {
    "text": "scheduling of other pods on to that node so in this example we have a pod running on node two with anti affinity",
    "start": "644210",
    "end": "650680"
  },
  {
    "text": "against any other pod that doesn't have user equals a so this is kind of double",
    "start": "650680",
    "end": "660320"
  },
  {
    "text": "negatives but so the the pod the is user a here is saying I don't want any pods",
    "start": "660320",
    "end": "668150"
  },
  {
    "text": "on this node with me unless they also have the label user equals a so this user B pod with no affinity speck at all",
    "start": "668150",
    "end": "675260"
  },
  {
    "text": "cannot be scheduled on node two and I",
    "start": "675260",
    "end": "684050"
  },
  {
    "text": "think I will hand it off to Bobbie to talk about some other ways of scheduling",
    "start": "684050",
    "end": "690200"
  },
  {
    "text": "sure so some of the features that you heard about are useful in many many",
    "start": "690200",
    "end": "697250"
  },
  {
    "text": "scenarios and for many production workloads some of the features that I'm gonna talk about are probably more",
    "start": "697250",
    "end": "703520"
  },
  {
    "text": "interesting for a smaller group of people those who may probably run larger clusters one of those features is",
    "start": "703520",
    "end": "710480"
  },
  {
    "text": "actually taints and toleration things and toleration x' we use them for various things inside kubernetes but one",
    "start": "710480",
    "end": "719210"
  },
  {
    "text": "of the interesting examples when it comes to scheduling is actually to save",
    "start": "719210",
    "end": "724930"
  },
  {
    "text": "some of the nodes that have a special hardware for pods that actually need",
    "start": "724930",
    "end": "731000"
  },
  {
    "text": "those a special hardware in a heterogeneous cluster let's say you have",
    "start": "731000",
    "end": "736940"
  },
  {
    "text": "nodes some of them have for example GPU and in this example and some of them don't",
    "start": "736940",
    "end": "742100"
  },
  {
    "text": "right so a new pod arrives and the pod only requires four gigabytes of RAM",
    "start": "742100",
    "end": "747860"
  },
  {
    "text": "let's say all of the nodes in our cluster have four gigabytes of RAM only",
    "start": "747860",
    "end": "752960"
  },
  {
    "text": "the first one node one has GPU so scheduler if there is no particular",
    "start": "752960",
    "end": "760340"
  },
  {
    "text": "preference may happen to choose a node one for running this pod right and then",
    "start": "760340",
    "end": "765770"
  },
  {
    "text": "a second pod arrives a second later this pod requires four gigabytes of RAM but",
    "start": "765770",
    "end": "771830"
  },
  {
    "text": "it also requires four gig one GPU so since the node with GPU is already used",
    "start": "771830",
    "end": "779330"
  },
  {
    "text": "for running a part that didn't actually require G you we will might be able to schedule",
    "start": "779330",
    "end": "785120"
  },
  {
    "text": "this part at all so this part is gonna remain pending in order to avoid this scenario we can taint that note with GPU",
    "start": "785120",
    "end": "794110"
  },
  {
    "text": "with with a special change or if it actually could be any time this this is",
    "start": "794110",
    "end": "799370"
  },
  {
    "text": "like an arbitrary key value pair that you can pay you can put on that note and say for example key is GPU and I and I",
    "start": "799370",
    "end": "806750"
  },
  {
    "text": "want this to be to be saved preferably only for parts that require basically",
    "start": "806750",
    "end": "813680"
  },
  {
    "text": "GPU so here we actually set the effect as prefer block scheduling we have",
    "start": "813680",
    "end": "819050"
  },
  {
    "text": "different effects for example one can be no scheduling which means that only the",
    "start": "819050",
    "end": "824840"
  },
  {
    "text": "pods that we actually have the proper Toleration can be scheduled but in this case we are just saying that we be",
    "start": "824840",
    "end": "830990"
  },
  {
    "text": "prefer no pods to be scheduled on this node so and note that there are a pod",
    "start": "830990",
    "end": "837740"
  },
  {
    "text": "that arrives and needs four gigabytes of RAM is preferred to be scheduled on node",
    "start": "837740",
    "end": "843860"
  },
  {
    "text": "2 or node 3 if they they are full we will schedule it still on note 1 but",
    "start": "843860",
    "end": "850460"
  },
  {
    "text": "since these nodes are available now we scheduled a pattern let's say no to and now when that new pod arrives which",
    "start": "850460",
    "end": "858260"
  },
  {
    "text": "requires actually GPU since there is no other choice for this scheduler to pick",
    "start": "858260",
    "end": "863420"
  },
  {
    "text": "it actually picks the first node sometimes you may want to have higher guarantees in that case you may go and",
    "start": "863420",
    "end": "872090"
  },
  {
    "text": "actually put the effect as block is scheduling or no schedule in this case",
    "start": "872090",
    "end": "877700"
  },
  {
    "text": "the scheduler picks that node only if the pot has the proper toleration so in",
    "start": "877700",
    "end": "885500"
  },
  {
    "text": "this case we put the Toleration and this part that requires the GPU and scheduler",
    "start": "885500",
    "end": "891350"
  },
  {
    "text": "picks that part for picks this node with GPU for scheduling one thing that I",
    "start": "891350",
    "end": "896960"
  },
  {
    "text": "would like to add here is that there are ways to add these things or Toleration",
    "start": "896960",
    "end": "903290"
  },
  {
    "text": "automatically so for example and that you can have an admission controller in kubernetes that looks at the resource",
    "start": "903290",
    "end": "910610"
  },
  {
    "text": "requirements of that pod and adds the proper toleration the part automatically you don't need to",
    "start": "910610",
    "end": "917170"
  },
  {
    "text": "always do it manually but it also can be manual so if you're creating a",
    "start": "917170",
    "end": "923459"
  },
  {
    "text": "deployment you can put this toleration in the template of the pod and and",
    "start": "923459",
    "end": "930850"
  },
  {
    "text": "another feature is actually which is which we have it we've added recently is",
    "start": "930850",
    "end": "937680"
  },
  {
    "text": "priority and preemption one of the main costs of a cluster is",
    "start": "937680",
    "end": "945069"
  },
  {
    "text": "actually the amount of resources that are used in the cluster and those resources are usually directly directly",
    "start": "945069",
    "end": "953709"
  },
  {
    "text": "correspond to the number of nodes or amount of resources available on the nodes so a way of saving money in a",
    "start": "953709",
    "end": "960670"
  },
  {
    "text": "large cluster is to raise the resource utilization of each individual node in the cluster this feature that I'm going",
    "start": "960670",
    "end": "968199"
  },
  {
    "text": "to talk about was one of the most important features that help Google raise resource utilization in in borg in",
    "start": "968199",
    "end": "975850"
  },
  {
    "text": "google internal clusters basically in a cluster that all the parts have the same",
    "start": "975850",
    "end": "983079"
  },
  {
    "text": "priority when the nodes are full and a new pod arrives the pod cannot be scheduled anywhere but how do you",
    "start": "983079",
    "end": "990069"
  },
  {
    "text": "specify if some of these parts are more important than the others for example",
    "start": "990069",
    "end": "995379"
  },
  {
    "text": "you may run your CI CD along with your production workloads in the same cluster why would you do that because you want",
    "start": "995379",
    "end": "1003180"
  },
  {
    "text": "to raise resource utilization in your cluster but how do you specify which one",
    "start": "1003180",
    "end": "1009540"
  },
  {
    "text": "is more important that's how you spell you put actually priority on your parts in this case let's say that the pods",
    "start": "1009540",
    "end": "1019110"
  },
  {
    "text": "which like the red pods have the highest priority in your cluster the orange",
    "start": "1019110",
    "end": "1026280"
  },
  {
    "text": "parts are medium priority and the green pods are the lowest priority pods all right so you can put all of them in the",
    "start": "1026280",
    "end": "1032760"
  },
  {
    "text": "same cluster now a part with highest priority arrives the cluster is full",
    "start": "1032760",
    "end": "1039048"
  },
  {
    "text": "scheduler will never try node 3 because it's running also a high priority pod scheduler never preempts a part with the",
    "start": "1039049",
    "end": "1046048"
  },
  {
    "text": "same or higher priority but no 2 and no or possible candidates for running this",
    "start": "1046049",
    "end": "1051240"
  },
  {
    "text": "high-priority pod because they are running pods with lower priority than",
    "start": "1051240",
    "end": "1056280"
  },
  {
    "text": "this one scheduler looks at these two and it has various metrics but since",
    "start": "1056280",
    "end": "1062730"
  },
  {
    "text": "note one has the lowest priority pods that if removed can give enough",
    "start": "1062730",
    "end": "1068820"
  },
  {
    "text": "resources for to the schedule to schedule this high priority pod scheduler deletes those two pods or in",
    "start": "1068820",
    "end": "1076320"
  },
  {
    "text": "our technology preempts those pods giving them their graceful termination",
    "start": "1076320",
    "end": "1081419"
  },
  {
    "text": "period and after that graceful permeation period expires those pods are actually removed and now this higher",
    "start": "1081419",
    "end": "1088320"
  },
  {
    "text": "priority pod can be scheduled and this is node so why does this help with",
    "start": "1088320",
    "end": "1094940"
  },
  {
    "text": "raising utilization in your cluster so a lot of users today in absence of a",
    "start": "1094940",
    "end": "1101850"
  },
  {
    "text": "priority and preemption use a single workload or equally important workloads",
    "start": "1101850",
    "end": "1107730"
  },
  {
    "text": "in their cluster and rely on cluster autoscaler to scale up the cluster when there is demand for more resources but",
    "start": "1107730",
    "end": "1116400"
  },
  {
    "text": "an alternative is to actually combine various types of resources so that when",
    "start": "1116400",
    "end": "1123240"
  },
  {
    "text": "there are available resources you run your lower priority tasks and when there",
    "start": "1123240",
    "end": "1128400"
  },
  {
    "text": "is a need for more resources for let's say your production workloads you remove some of those lower priority tasks which",
    "start": "1128400",
    "end": "1135570"
  },
  {
    "text": "sometimes we call them like filler tasks from your nodes and use the resources for running your production workloads",
    "start": "1135570",
    "end": "1142640"
  },
  {
    "text": "this is also helpful if there is a sudden spike in the traffic of your",
    "start": "1142640",
    "end": "1149429"
  },
  {
    "text": "cluster for example suddenly there is a news about your website or your service",
    "start": "1149429",
    "end": "1154980"
  },
  {
    "text": "and people jump on their computers and want to check out your website or your",
    "start": "1154980",
    "end": "1160620"
  },
  {
    "text": "service and suddenly there is a huge spike so if you have a lot of available",
    "start": "1160620",
    "end": "1166169"
  },
  {
    "text": "resources used for let's say training your models or I don't know your CI CD",
    "start": "1166169",
    "end": "1172860"
  },
  {
    "text": "pipeline the new scheduler can react very quickly and remove some of these",
    "start": "1172860",
    "end": "1177870"
  },
  {
    "text": "workloads from your cluster and use it for running your more reduction the instances of your web",
    "start": "1177870",
    "end": "1185370"
  },
  {
    "text": "server or whatever the service you running autoscaler can help as well but",
    "start": "1185370",
    "end": "1190770"
  },
  {
    "text": "autoscaler has to first detect the the increasing in the spike in the cluster",
    "start": "1190770",
    "end": "1197400"
  },
  {
    "text": "and add new nodes which usually takes minutes so autoscaler is is has a little",
    "start": "1197400",
    "end": "1204450"
  },
  {
    "text": "bit higher delay and that's why priority and preemption can help you in this case",
    "start": "1204450",
    "end": "1211340"
  },
  {
    "text": "that's all of the features that we wanted to talk about today and in terms",
    "start": "1212000",
    "end": "1217950"
  },
  {
    "text": "of roadmap maraud map ahead priority and preemption has existed since 1.8 it is",
    "start": "1217950",
    "end": "1225150"
  },
  {
    "text": "still an alpha feature in 110 which is the current release up to scheduler we plan to move it to beta which which",
    "start": "1225150",
    "end": "1231090"
  },
  {
    "text": "means that it's going to be available by default and enabled by default in 111",
    "start": "1231090",
    "end": "1237210"
  },
  {
    "text": "and hopefully if things go well we can move it to GA in 112 yang scheduling is",
    "start": "1237210",
    "end": "1244890"
  },
  {
    "text": "another feature that which we're working on we are hoping to have an alpha version of gangue scheduling in 112 if",
    "start": "1244890",
    "end": "1253230"
  },
  {
    "text": "you have feedback about how we should do gang scheduling police and send them to",
    "start": "1253230",
    "end": "1258930"
  },
  {
    "text": "us we are designing the feature these days and we would like to hear as many",
    "start": "1258930",
    "end": "1264510"
  },
  {
    "text": "feedback as possible there are other performance improvements like equivalence cache that has been in alpha",
    "start": "1264510",
    "end": "1272210"
  },
  {
    "text": "for a while and we are moving it to beta in 1 111 which is going to be done on",
    "start": "1272210",
    "end": "1278010"
  },
  {
    "text": "next release a fennekin anti affinity has been in a beta feature hopefully we",
    "start": "1278010",
    "end": "1284250"
  },
  {
    "text": "can move it to AGI soon painting note by condition this is actually another",
    "start": "1284250",
    "end": "1291330"
  },
  {
    "text": "feature which require which uses things and toleration we are moving those to beta scheduling framework is a is a big",
    "start": "1291330",
    "end": "1299340"
  },
  {
    "text": "effort that we are also working on this is actually a good news for those who run like custom schedulers in their",
    "start": "1299340",
    "end": "1306690"
  },
  {
    "text": "clusters we are planning to move scheduler from their current from its current so like monolithic design into a",
    "start": "1306690",
    "end": "1315690"
  },
  {
    "text": "a more pluggable architecture so that various plugins can be added to the",
    "start": "1315690",
    "end": "1320970"
  },
  {
    "text": "scheduler in a easier way and removes the burden of keeping your customer",
    "start": "1320970",
    "end": "1326309"
  },
  {
    "text": "scheduler along with a defaulted scheduler and last but not least I would",
    "start": "1326309",
    "end": "1335190"
  },
  {
    "text": "like to thank our contributors who have contributed code from literally around the world various countries in Asia",
    "start": "1335190",
    "end": "1341970"
  },
  {
    "text": "Europe and American countries I really like to thank them some of them are",
    "start": "1341970",
    "end": "1347220"
  },
  {
    "text": "present in this room make sure to say hi and these are the three key points from",
    "start": "1347220",
    "end": "1354840"
  },
  {
    "text": "this talk with that we were happy to take questions",
    "start": "1354840",
    "end": "1359909"
  },
  {
    "text": "yes is there anything that you can",
    "start": "1359909",
    "end": "1368669"
  },
  {
    "text": "intelligently reschedule all the parts based on the workload demand like because I call it one of the work groups",
    "start": "1368669",
    "end": "1376019"
  },
  {
    "text": "in East you give you an example of Sto for instance sto mixer takes more",
    "start": "1376019",
    "end": "1382710"
  },
  {
    "text": "resources as you load up and the more load come in so if there is a feature in",
    "start": "1382710",
    "end": "1389700"
  },
  {
    "text": "the scheduler that can dynamically risk Aereo some of the parts which are overloaded that will really help",
    "start": "1389700",
    "end": "1396990"
  },
  {
    "text": "otherwise right now what will happen is some of the because the mixer where the part with the mixer it's there in the",
    "start": "1396990",
    "end": "1403860"
  },
  {
    "text": "node that node will become a bottleneck for rest of the workload pots all right",
    "start": "1403860",
    "end": "1409919"
  },
  {
    "text": "so basically I guess you like to have to run all of your parts with the same",
    "start": "1409919",
    "end": "1416429"
  },
  {
    "text": "priority but you still expect some level of intelligence into",
    "start": "1416429",
    "end": "1423018"
  },
  {
    "text": "suddenly some of them so it's not",
    "start": "1452239",
    "end": "1478979"
  },
  {
    "text": "exactly scheduler but kubernetes in general is going to have a feature similar to what you are asking basically",
    "start": "1478979",
    "end": "1486539"
  },
  {
    "text": "autoscaler is going to actually I guess is having this feature in alpha which is",
    "start": "1486539",
    "end": "1492359"
  },
  {
    "text": "basically vertical part auto scaling it increases the amount of resource",
    "start": "1492359",
    "end": "1497519"
  },
  {
    "text": "requirements for that part that now has a higher usage and we also have this",
    "start": "1497519",
    "end": "1503279"
  },
  {
    "text": "scheduler which is another incubator project the scheduler also looks at the",
    "start": "1503279",
    "end": "1509249"
  },
  {
    "text": "nodes which have let's say close to the capacity and remove some of the pods and when the pods are removed a scheduler",
    "start": "1509249",
    "end": "1516690"
  },
  {
    "text": "again schedules them elsewhere so basically a cooperation of various components in a cluster will result into",
    "start": "1516690",
    "end": "1524099"
  },
  {
    "text": "the feature that you you're asking yes I",
    "start": "1524099",
    "end": "1529099"
  },
  {
    "text": "have a tree of related questions about scheduler capacity is there documented",
    "start": "1530119",
    "end": "1538399"
  },
  {
    "text": "complexity of the scheduler it looks like what from what you said the cost of",
    "start": "1538399",
    "end": "1543690"
  },
  {
    "text": "scheduling one pod is going to be order of n log in and it so my question is is",
    "start": "1543690",
    "end": "1551549"
  },
  {
    "text": "that right or do you have a different answer and the related question is I saw",
    "start": "1551549",
    "end": "1557069"
  },
  {
    "text": "somewhere that there is a goal of being able to schedule I think it was not",
    "start": "1557069",
    "end": "1563279"
  },
  {
    "text": "forget the goal but is there a stated you know documented throughput achieved through",
    "start": "1563279",
    "end": "1568879"
  },
  {
    "text": "put on the scheduler yes so in terms of throughput we do definitely have some",
    "start": "1568879",
    "end": "1574369"
  },
  {
    "text": "consoles that actually you can go and look there are like scalability the team actually has these graphs that shows the",
    "start": "1574369",
    "end": "1582559"
  },
  {
    "text": "scheduler throughput but in terms of complexity it's really really hard to",
    "start": "1582559",
    "end": "1587570"
  },
  {
    "text": "come up with a single like oh of a login or anything like that the reason is that",
    "start": "1587570",
    "end": "1594379"
  },
  {
    "text": "it really depends on the features that or scheduling features that pods request",
    "start": "1594379",
    "end": "1600049"
  },
  {
    "text": "from the scheduler so a part that for example has pod inter pod affinity let's",
    "start": "1600049",
    "end": "1605330"
  },
  {
    "text": "say well go through more basically predicates that do more complex",
    "start": "1605330",
    "end": "1612440"
  },
  {
    "text": "operations or computations in the cluster and actually that could have been a lot worse than n log n it could",
    "start": "1612440",
    "end": "1619220"
  },
  {
    "text": "be like n cube or something like that I",
    "start": "1619220",
    "end": "1624008"
  },
  {
    "text": "guess it's there but uh but I don't have any on top of them I had any particular",
    "start": "1630070",
    "end": "1637399"
  },
  {
    "text": "link but I guess it's in the same performance dashboard the metric you're probably looking for I think it's called",
    "start": "1637399",
    "end": "1642649"
  },
  {
    "text": "scheduling end-to-end latency and it's maybe a histogram yeah but it also",
    "start": "1642649",
    "end": "1653210"
  },
  {
    "text": "depends you know it really depends on the size of your cluster as well so those are all run with a particular",
    "start": "1653210",
    "end": "1659059"
  },
  {
    "text": "cluster size and a particular number of pods in the cluster yeah go ahead with",
    "start": "1659059",
    "end": "1667669"
  },
  {
    "text": "the reason changes in couplet enriching and implementing priority and preemption can we expect quality of service classes",
    "start": "1667669",
    "end": "1674330"
  },
  {
    "text": "to go away no we were we're going to try to remain backward compatible as much as",
    "start": "1674330",
    "end": "1680960"
  },
  {
    "text": "we can but you're right there is there is somewhat of an overlap between these",
    "start": "1680960",
    "end": "1686059"
  },
  {
    "text": "two but they are not exactly addressing the same problem so they're going to",
    "start": "1686059",
    "end": "1691249"
  },
  {
    "text": "stay you can talk more about that offline because I guess",
    "start": "1691249",
    "end": "1696930"
  },
  {
    "text": "there has been a lot of discussions about that and so I wanted to follow up on the heterogeneous node scheduling",
    "start": "1696930",
    "end": "1704580"
  },
  {
    "text": "problem taking your GPU example can that extend to network capacity to quantity",
    "start": "1704580",
    "end": "1710880"
  },
  {
    "text": "and type of Nicks and physical Ethernet ports and can it extend to Nu IOA",
    "start": "1710880",
    "end": "1716540"
  },
  {
    "text": "awareness as well yes of course yes you",
    "start": "1716540",
    "end": "1721550"
  },
  {
    "text": "you will have multiple options when you create for example notes for your cluster if your nodes have special",
    "start": "1721550",
    "end": "1727770"
  },
  {
    "text": "hardware you contain them with various types of teens the teens are pretty generic these are just key value pairs",
    "start": "1727770",
    "end": "1735120"
  },
  {
    "text": "so kubernetes is not going to do it automatically for you it probably needs",
    "start": "1735120",
    "end": "1741840"
  },
  {
    "text": "some level of coding on your side to add properties for the nodes in your cluster",
    "start": "1741840",
    "end": "1748140"
  },
  {
    "text": "but once you do it yeah it works pretty much for any type of that I can I don't",
    "start": "1748140",
    "end": "1770010"
  },
  {
    "text": "know the answer basically what you are you're asking is very much node related",
    "start": "1770010",
    "end": "1776580"
  },
  {
    "text": "how know basically assigns these two ports to various pods in a cluster in",
    "start": "1776580",
    "end": "1784050"
  },
  {
    "text": "that particular node and I don't know the answer I'm not so sure if actually support it today or if you can put for",
    "start": "1784050",
    "end": "1792120"
  },
  {
    "text": "that particular requirement in the pod spec I am not aware of anything like that the question I have is is it",
    "start": "1792120",
    "end": "1801540"
  },
  {
    "text": "possible so currently the scheduler uses the number of CPUs in the machines and the parts back it looks for the CPU",
    "start": "1801540",
    "end": "1808680"
  },
  {
    "text": "requests to find the available node right we wanted to basically make use of",
    "start": "1808680",
    "end": "1814050"
  },
  {
    "text": "you know over-committing and not by using a limits and the request just by in terms of scheduler",
    "start": "1814050",
    "end": "1820890"
  },
  {
    "text": "bin packing more parts on to the machine and then the available resources there's",
    "start": "1820890",
    "end": "1826740"
  },
  {
    "text": "something community is looking for yeah so you know there are there are",
    "start": "1826740",
    "end": "1832450"
  },
  {
    "text": "multiple ways of achieving that one is to not specify any resource requirements",
    "start": "1832450",
    "end": "1838539"
  },
  {
    "text": "in your part right so that's basically the best effort parts that we have in in kubernetes and then scheduler assumes",
    "start": "1838539",
    "end": "1846070"
  },
  {
    "text": "that your pots do not need any particular amount of CPU or memory but",
    "start": "1846070",
    "end": "1851590"
  },
  {
    "text": "they actually do and once the early schedule they are they are going to be using some amount of CPU and you will",
    "start": "1851590",
    "end": "1859000"
  },
  {
    "text": "eventually achieve some level of over commitment on Anders notes and other possibilities to actually specify lower",
    "start": "1859000",
    "end": "1866760"
  },
  {
    "text": "requirements than the actual usage and you can specify a higher limit that's we",
    "start": "1866760",
    "end": "1872799"
  },
  {
    "text": "call burstable part cessation that also achieves over commitment of the CPU or",
    "start": "1872799",
    "end": "1879840"
  },
  {
    "text": "even memory but in case the memory it's a little bit harder because we cannot",
    "start": "1879840",
    "end": "1886389"
  },
  {
    "text": "really actually run more than more positive the amount of memory that we",
    "start": "1886389",
    "end": "1891970"
  },
  {
    "text": "have a node so you may end up having some of your parts in the field but CPU",
    "start": "1891970",
    "end": "1897490"
  },
  {
    "text": "is a more like compressible resource so it's easier to achieve over commitment",
    "start": "1897490",
    "end": "1902620"
  },
  {
    "text": "for CPU yes",
    "start": "1902620",
    "end": "1906419"
  },
  {
    "text": "no not really not at least today that that's a completely different model as",
    "start": "1916480",
    "end": "1922489"
  },
  {
    "text": "we have today well the question is are we ever going to consider distributed scheduling as",
    "start": "1922489",
    "end": "1929600"
  },
  {
    "text": "opposed to the centralized scheduling that we have today so that's a model effort for example maysa users and it's",
    "start": "1929600",
    "end": "1936320"
  },
  {
    "text": "a different model of scheduling we don't think that we are ready really to absorb",
    "start": "1936320",
    "end": "1942679"
  },
  {
    "text": "all of the changes there and it's a little hard given that the kubernetes api has",
    "start": "1942679",
    "end": "1949639"
  },
  {
    "text": "evolved over the assumption that you have a centralized scheduler it's a little harder to come up with them with",
    "start": "1949639",
    "end": "1957200"
  },
  {
    "text": "a central distributed the scheduler yes",
    "start": "1957200",
    "end": "1964090"
  },
  {
    "text": "so the question is that if there are many pending pods when are gonna if",
    "start": "1969730",
    "end": "1976429"
  },
  {
    "text": "they're gonna get drops eventually or are they gonna remain pending forever or what is the behavior the the behavior is",
    "start": "1976429",
    "end": "1983239"
  },
  {
    "text": "actually kubernetes schedule keeps them all in the queue as long as they exist",
    "start": "1983239",
    "end": "1988519"
  },
  {
    "text": "in the cluster so if if they are deleted at some point you are deleted from the queue as well but kubernetes scheduler",
    "start": "1988519",
    "end": "1995059"
  },
  {
    "text": "will keep retrying to schedule them until it actually can order deleted so",
    "start": "1995059",
    "end": "2002159"
  },
  {
    "text": "in the newer architecture of kubernetes is essentially as soon as an event",
    "start": "2002159",
    "end": "2007659"
  },
  {
    "text": "arrives that could possibly make those parts is scheduled about for example and a pod is deleted from the cluster and",
    "start": "2007659",
    "end": "2014859"
  },
  {
    "text": "releases some resources burnsy scheduled each retries those parts",
    "start": "2014859",
    "end": "2020669"
  },
  {
    "text": "right so yes so the question is sometimes when a part is getting retried",
    "start": "2050990",
    "end": "2057480"
  },
  {
    "text": "a few times the scheduler automatically automatically applies it back off to",
    "start": "2057480",
    "end": "2062638"
  },
  {
    "text": "that part once the part cannot be scheduled and as a result a part that has been longer in the queue probably",
    "start": "2062639",
    "end": "2069330"
  },
  {
    "text": "can take longer to schedule because of the back of time so kubernetes the newer",
    "start": "2069330",
    "end": "2075260"
  },
  {
    "text": "kubernetes the scheduler actually uses it it's a part of priority and",
    "start": "2075260",
    "end": "2080280"
  },
  {
    "text": "preemption which is not enabled by default but the newer mechanism actually is sorting pods based on their priority",
    "start": "2080280",
    "end": "2087240"
  },
  {
    "text": "so parts with higher priority are in front of the queue and today we don't have any backup mechanism for this newer",
    "start": "2087240",
    "end": "2094230"
  },
  {
    "text": "architecture we may add actually a backup mechanism for that as well to make sure we have a fair we give fair",
    "start": "2094230",
    "end": "2101490"
  },
  {
    "text": "scheduling opportunities to deposit in the queue but anyway the the idea here",
    "start": "2101490",
    "end": "2107190"
  },
  {
    "text": "is that the pods with highest priority get a highest chance of getting scheduled and those with the lowest",
    "start": "2107190",
    "end": "2112440"
  },
  {
    "text": "Peyer to go to the end of the queue so that behavior is going to change so we're at time but we're both around for",
    "start": "2112440",
    "end": "2120030"
  },
  {
    "text": "the rest of the con definitely find us and ask us more questions we're on slack we're on email thanks for",
    "start": "2120030",
    "end": "2126210"
  },
  {
    "text": "coming",
    "start": "2126210",
    "end": "2128480"
  }
]