[
  {
    "text": "okay good evening so uh today um I I'd like to share our experience with how we've adopted time series forecasting in",
    "start": "80",
    "end": "6480"
  },
  {
    "text": "the context of finops in kuber 0es uh starting all the way uh from a note level all the way up to our entire fleet",
    "start": "6480",
    "end": "13120"
  },
  {
    "text": "in order to reduce cost when managing large quantities of Parts my name is Irvin and unfortunately today I won't be",
    "start": "13120",
    "end": "19640"
  },
  {
    "text": "joined by my colleague Nicholas uh who was unable to attend Cube con today this time uh a large portion of today's",
    "start": "19640",
    "end": "25599"
  },
  {
    "text": "sharing was also prepared by him and I would be prep presenting his findings and insight on behalf of you guys to uh",
    "start": "25599",
    "end": "32119"
  },
  {
    "text": "today so just a quick introduction both of us are platform Engineers working within the engineering infrastructure",
    "start": "32120",
    "end": "38399"
  },
  {
    "text": "division at shy before we get started I would like",
    "start": "38399",
    "end": "44360"
  },
  {
    "text": "to give a brief introduction about our company so we're from shoy which is an e-commerce company operating in several",
    "start": "44360",
    "end": "50120"
  },
  {
    "text": "markets worldwide today we are the leading e-commerce uh platform in Southeast Asia",
    "start": "50120",
    "end": "55199"
  },
  {
    "text": "Taiwan and Brazil we are also the number one shopping app in these markets uh by aage monthly active users as well as",
    "start": "55199",
    "end": "62120"
  },
  {
    "text": "total time spent in app in shoy we've used kubernetes to",
    "start": "62120",
    "end": "67400"
  },
  {
    "text": "manage and orchestrate large numbers of ports that power the backend systems behind shopy today we have over 100,000",
    "start": "67400",
    "end": "74000"
  },
  {
    "text": "ports running across tens of thousands of notes distributed across multiple data centers worldwide and we also",
    "start": "74000",
    "end": "80479"
  },
  {
    "text": "expect the that these numbers will continue growing as the company grows in the years to",
    "start": "80479",
    "end": "86159"
  },
  {
    "text": "come unfortunately managing such large numbers of parts usually mean needing a large number of machines to support them",
    "start": "86159",
    "end": "92640"
  },
  {
    "text": "as well and by extension this would cost us millions of dollars per quarter as",
    "start": "92640",
    "end": "98360"
  },
  {
    "text": "such one of the main objectives in finops is to find solutions to optimize the usage of our existing resources in",
    "start": "98360",
    "end": "104399"
  },
  {
    "text": "order to continue supporting an Ever growing number of containers while minimizing the physical resource",
    "start": "104399",
    "end": "109439"
  },
  {
    "text": "requirements thus limiting the increase in financial cost involved from the underlying",
    "start": "109439",
    "end": "114719"
  },
  {
    "text": "infrastructure by observing resource utilization patterns with within our clusters we found several common cases",
    "start": "114719",
    "end": "121640"
  },
  {
    "text": "of resource wastage which could lead to uh under utilized resources so in shly our resources are",
    "start": "121640",
    "end": "128160"
  },
  {
    "text": "provisioned based on the requirement to support campaign events that happen around once a month and as such most of",
    "start": "128160",
    "end": "133480"
  },
  {
    "text": "our critical Services would need to have uh sufficient buffer in their resources and at the same time since we host our",
    "start": "133480",
    "end": "139840"
  },
  {
    "text": "machines on premise uh provisioning a large quantity of ports on such short notice is usually uh quite impractical",
    "start": "139840",
    "end": "147319"
  },
  {
    "text": "and hence we usually have to reserve some resources that need to remain unallocated in the event that we need to",
    "start": "147319",
    "end": "152959"
  },
  {
    "text": "massively scale up during unexpectedly large traffic spikes during these campaigns on on secondly uh most of our",
    "start": "152959",
    "end": "161319"
  },
  {
    "text": "services also exhibit usage patterns that follow uh that of our end users which are mostly concentrated in just a",
    "start": "161319",
    "end": "167000"
  },
  {
    "text": "handful of time zones across the Southeast Asian and Latin American time zones as such we usually see lower",
    "start": "167000",
    "end": "174239"
  },
  {
    "text": "utilization during off peak periods when our users are asleep which means that we usually have tens of thousands of CP few",
    "start": "174239",
    "end": "180040"
  },
  {
    "text": "cores just sitting idle during those times as you can see so how can we make our resources",
    "start": "180040",
    "end": "186239"
  },
  {
    "text": "more efficient so this is one approach we took from a note level and let's just try to visualize this from a diagram",
    "start": "186239",
    "end": "192200"
  },
  {
    "text": "here so in blue over here we can see that this is the actual CPU utilization",
    "start": "192200",
    "end": "197599"
  },
  {
    "text": "in use and the orange portion represents the allocated but currently unused",
    "start": "197599",
    "end": "202640"
  },
  {
    "text": "portion of the port CPU request and at the same time we also have a portion of",
    "start": "202640",
    "end": "207799"
  },
  {
    "text": "the node CPU resources that remain allocated so what we did is very simple",
    "start": "207799",
    "end": "214400"
  },
  {
    "text": "very simple we just monitored the amount of unused resources on the note and advertise this unused portion or the",
    "start": "214400",
    "end": "221319"
  },
  {
    "text": "orange portion via an extended resource so I'll just show an example shortly we can then make use of these reclaimed",
    "start": "221319",
    "end": "228200"
  },
  {
    "text": "resources to run much more workloads on top of just simply just the unallocated portion which is really",
    "start": "228200",
    "end": "234720"
  },
  {
    "text": "small so just this is just an example of what the advertise uh extended resource",
    "start": "234720",
    "end": "239959"
  },
  {
    "text": "might look like uh over here you can see that we advertise a b CPU of 30 cores",
    "start": "239959",
    "end": "245439"
  },
  {
    "text": "and then this way we can actually run ports that don't consume any uh allocatable CPU instead it consumes the",
    "start": "245439",
    "end": "251799"
  },
  {
    "text": "extended resource the B CPU resource that we have reclaimed earlier and uh",
    "start": "251799",
    "end": "256880"
  },
  {
    "text": "this way we actually can run more ports uh on top of what uh we currently can",
    "start": "256880",
    "end": "261959"
  },
  {
    "text": "offer but as I mentioned just now these are actually reclaimed resources and not",
    "start": "261959",
    "end": "268880"
  },
  {
    "text": "all suitable not all services are actually suitable to run on these sort of ephemeral resources so um even though",
    "start": "268880",
    "end": "276400"
  },
  {
    "text": "these resources are currently idle they might be needed in the future by the ports that originally requested them and",
    "start": "276400",
    "end": "282199"
  },
  {
    "text": "thus these resources should not be considered to be really um potentially shortlived in",
    "start": "282199",
    "end": "288400"
  },
  {
    "text": "nature so what we did in our company we introduced a new category of services known as batch services and these",
    "start": "288400",
    "end": "295720"
  },
  {
    "text": "workloads should be able to run on demand to make use of the idle resources that I described earlier that are",
    "start": "295720",
    "end": "301680"
  },
  {
    "text": "reclaimed from other user facing online services as compared to an online",
    "start": "301680",
    "end": "307199"
  },
  {
    "text": "service which low uh requires lower latency and high availability we expect that batch Services should be able to",
    "start": "307199",
    "end": "313240"
  },
  {
    "text": "tolerate much more disruptions and this may include lower availability or even",
    "start": "313240",
    "end": "319000"
  },
  {
    "text": "evictions some examples of uh B services in our company include Big Data spark",
    "start": "319000",
    "end": "324479"
  },
  {
    "text": "jobs as well as certain non-real time media transporting tasks so by downgrading suitable",
    "start": "324479",
    "end": "331560"
  },
  {
    "text": "services to this batch service level not only did we unlock a new class of resources that is much larger than",
    "start": "331560",
    "end": "337199"
  },
  {
    "text": "before that we can now tap into but this also helped us to free up uh the scars",
    "start": "337199",
    "end": "342479"
  },
  {
    "text": "uh resource for other online services to use in order to support the co the",
    "start": "342479",
    "end": "349440"
  },
  {
    "text": "collocation of both online and bch services on the same note we make use of several techniques uh that are available",
    "start": "349440",
    "end": "355240"
  },
  {
    "text": "in the Linux kernel so for one we set a lower cpu. weight",
    "start": "355240",
    "end": "360440"
  },
  {
    "text": "uh inside the C group uh for bat Services which ensures that other online services will always get the highest",
    "start": "360440",
    "end": "366840"
  },
  {
    "text": "share of CPU uh CPU time when it needs it we also made some adjustments to the",
    "start": "366840",
    "end": "374160"
  },
  {
    "text": "Linux kernel CPU scheduler actually which allows online cgroups to preempt",
    "start": "374160",
    "end": "379280"
  },
  {
    "text": "batch cgroups through a concept known as uh borrowed virtual time or BVT this way",
    "start": "379280",
    "end": "384560"
  },
  {
    "text": "we can still ensure that the latency SLA slos for online services uh can still be",
    "start": "384560",
    "end": "390360"
  },
  {
    "text": "retained and they can respond to burst of user traffic when it needs to in a timely manner at the expense of simply",
    "start": "390360",
    "end": "396520"
  },
  {
    "text": "just delaying the execution of the batch uh uh batch services in the CPU for a",
    "start": "396520",
    "end": "401800"
  },
  {
    "text": "short while since these resources are ephemeral they can be reclaimed by the",
    "start": "401800",
    "end": "408319"
  },
  {
    "text": "online services at any point in time as I described just now so in this example we can see that the online services",
    "start": "408319",
    "end": "414800"
  },
  {
    "text": "utilization as represented in blue could actually increase uh increase uh further",
    "start": "414800",
    "end": "422479"
  },
  {
    "text": "and this actually uh Corr results in the corresponding B CPU uh resource to",
    "start": "422479",
    "end": "427680"
  },
  {
    "text": "actually decrease so we already allocated the batch workload so what",
    "start": "427680",
    "end": "432840"
  },
  {
    "text": "what can we do now so when these batch workloads are uh because we we have the",
    "start": "432840",
    "end": "439800"
  },
  {
    "text": "kernel um uh features that we enabled so this might result in the batch workloads",
    "start": "439800",
    "end": "444960"
  },
  {
    "text": "to be throttled and they could actually be throttled for a really long time if the utilization of the online services",
    "start": "444960",
    "end": "451400"
  },
  {
    "text": "persist for a really long time and this might result in the bch workloads to fail to make forward progress and they",
    "start": "451400",
    "end": "457520"
  },
  {
    "text": "might even be hung indefinitely so what we'll need to do is we'll need to eventually evict these",
    "start": "457520",
    "end": "462840"
  },
  {
    "text": "badg workloads from the note so that we can run them on another note which has more idle resources available instead so",
    "start": "462840",
    "end": "469840"
  },
  {
    "text": "this presents our first challenge with using such resources if the not's resource utilization fluctuates very",
    "start": "469840",
    "end": "475800"
  },
  {
    "text": "wildly and frequently what we might observe is a high rate of eviction of batch workloads and frequent disruption",
    "start": "475800",
    "end": "481599"
  },
  {
    "text": "to uh what the batch PS are trying to do this is not ideal since they might not",
    "start": "481599",
    "end": "486759"
  },
  {
    "text": "actually get to get manage to get any work done if they are keep entering a schedule and EV schedule EV",
    "start": "486759",
    "end": "494120"
  },
  {
    "text": "cycle so because of this some of our batch service users uh also require a non-rival grace period so for example",
    "start": "494120",
    "end": "501720"
  },
  {
    "text": "our in-house Presto team requires an SLA that we need to provide a 60-minute graceful shutdown period uh in order to",
    "start": "501720",
    "end": "509080"
  },
  {
    "text": "minimize the interruption penalty on their users queries so uh just as an example if",
    "start": "509080",
    "end": "515080"
  },
  {
    "text": "these jobs were to be interrupted suddenly their users queries would need to be restarted all the way from the beginning and this would uh result in",
    "start": "515080",
    "end": "522279"
  },
  {
    "text": "throwing away all of the previous computation and waste all the resources that consume up to this point and this",
    "start": "522279",
    "end": "527800"
  },
  {
    "text": "whole exercise will be futile but on the other hand if we don't interrupt these these parts in time they might start to",
    "start": "527800",
    "end": "534000"
  },
  {
    "text": "be throttled indefinitely and because we set the cgroup settings we have no control over when uh we would actually",
    "start": "534000",
    "end": "542079"
  },
  {
    "text": "uh release the r houses back to them so even if we do wait for 1 hour before terminating the ports but if the port",
    "start": "542079",
    "end": "548440"
  },
  {
    "text": "doesn't actually get to use any CPU or use a very little amount of CPU this whole thing would be pointless and it's",
    "start": "548440",
    "end": "553720"
  },
  {
    "text": "simply prolonging the inevitable uh after 1 hour so some of the problems I've",
    "start": "553720",
    "end": "559959"
  },
  {
    "text": "mentioned earlier are scheduling challenges that can be addressed with a little bit of foresight into the",
    "start": "559959",
    "end": "565279"
  },
  {
    "text": "future so before we get into details about how we can do this through for casting to address the scheduling",
    "start": "565279",
    "end": "570880"
  },
  {
    "text": "challenges let's first dig a little bit into the theory about how we can do this we'll first consider two time",
    "start": "570880",
    "end": "577800"
  },
  {
    "text": "Horizons for forecasting the first of which is shortterm which focuses on the immediate Trend and the likely",
    "start": "577800",
    "end": "584079"
  },
  {
    "text": "trajectory of uh signal and the next is a long-term forecasting which focuses on the patterns such as cyclical changes in",
    "start": "584079",
    "end": "591760"
  },
  {
    "text": "the resource utilization each uh kind of forecasting can be used for different goals and",
    "start": "591760",
    "end": "598320"
  },
  {
    "text": "objectives we use short-term forecast primarily to solve the problem of flapping where metric May uh May",
    "start": "598320",
    "end": "604600"
  },
  {
    "text": "fluctuate up and down repeatedly our goal here is actually is to make the metric appear more smooth which will",
    "start": "604600",
    "end": "610480"
  },
  {
    "text": "help us to tolerate shli spikes in the metric for example like CPU usage and this can help to reduce the number of",
    "start": "610480",
    "end": "616839"
  },
  {
    "text": "unnecessary evictions on the other hand for long-term forecast our goal instead is to predict the long the future",
    "start": "616839",
    "end": "623360"
  },
  {
    "text": "resource usage of a given service and in so doing help the scheduler to make smarter placement decisions of reports",
    "start": "623360",
    "end": "629600"
  },
  {
    "text": "onto the note so let's start by look diving the",
    "start": "629600",
    "end": "635120"
  },
  {
    "text": "short-term forecasting in the graph above we can see that CPU usage might fluctuate very wildly uh the blue line",
    "start": "635120",
    "end": "642040"
  },
  {
    "text": "right and this might affect how we perform eviction and resource advertisement so what we need to do is",
    "start": "642040",
    "end": "648079"
  },
  {
    "text": "to find a way to smooth out smooth out the underlying signal or in this case the CPU usage so that we can avoid",
    "start": "648079",
    "end": "654480"
  },
  {
    "text": "evicting the workloads unnecessarily at the same time we'll need to distinguish short lift spikes",
    "start": "654480",
    "end": "660160"
  },
  {
    "text": "from long-term increases in CPU usage and we also need to detect this",
    "start": "660160",
    "end": "665200"
  },
  {
    "text": "quickly our chosen approach here is to make use of exponentially weighted moving average or ewma for short which",
    "start": "665200",
    "end": "672519"
  },
  {
    "text": "is a variant of the moving average but it's actually more sensitive to re uh",
    "start": "672519",
    "end": "677760"
  },
  {
    "text": "relatively recent changes as compared to say 5 minutes ago we use the ewma",
    "start": "677760",
    "end": "683480"
  },
  {
    "text": "together with a confidence band to perform anomaly detection so if the value is large than",
    "start": "683480",
    "end": "689560"
  },
  {
    "text": "our tolerated confidence interval of say three standard deviations this prompts a corresponding reaction such as to",
    "start": "689560",
    "end": "696440"
  },
  {
    "text": "increase or decrease the amount of advertised resources or to perform an eviction",
    "start": "696440",
    "end": "701480"
  },
  {
    "text": "accordingly using the ewma is superior to a simple moving average since it will be able to detect genuine increases much",
    "start": "701480",
    "end": "708079"
  },
  {
    "text": "more quickly as compared to just a simple moving average I also mentioned earlier that",
    "start": "708079",
    "end": "713680"
  },
  {
    "text": "some of our users the Presto team that I mentioned just now also require longer graceful shutdown periods of 1 hour we",
    "start": "713680",
    "end": "720880"
  },
  {
    "text": "can't use a short-term forecast for this because now we actually have to predict an hour into the future and instead",
    "start": "720880",
    "end": "727320"
  },
  {
    "text": "we'll need to adopt a different kind of forecasting method uh as you can tell I'm going into the long-term forecasting",
    "start": "727320",
    "end": "733519"
  },
  {
    "text": "section and to find something that is aware of the long-term Trend the seasonality and cyclical properties of",
    "start": "733519",
    "end": "740040"
  },
  {
    "text": "an online services utilization so unlike a short-term forecast uh the long-term forecast is",
    "start": "740040",
    "end": "745959"
  },
  {
    "text": "able to help the scheduler make smarter decisions in advance such that we can place the batch PS to ensure that within",
    "start": "745959",
    "end": "753839"
  },
  {
    "text": "the next 1 hour at least the batch parts are able to run to completion without having to suffer from throttling or",
    "start": "753839",
    "end": "759480"
  },
  {
    "text": "eviction so that's the whole idea let's Dive Right into how we can perform long-term",
    "start": "759480",
    "end": "765600"
  },
  {
    "text": "forecasting so if we just look at the utilization of the CPU utilization of a",
    "start": "765600",
    "end": "771519"
  },
  {
    "text": "node this actually might not be very useful because containers might be or",
    "start": "771519",
    "end": "776639"
  },
  {
    "text": "ports might be added or removed from a node we cannot simply just take the notes past utilization to predict the future utilization since it would be",
    "start": "776639",
    "end": "783839"
  },
  {
    "text": "incorrect whenever we just we whenever we add on port or ports are removed from the note as such we need to break down",
    "start": "783839",
    "end": "790199"
  },
  {
    "text": "the constituents of the noes utilization which might include uh online service",
    "start": "790199",
    "end": "795519"
  },
  {
    "text": "ports uh other B jobs that are running on it as well as other supporting services that could include uh container",
    "start": "795519",
    "end": "802120"
  },
  {
    "text": "D cuet and kernel which uh we call demon Services over here so after breaking down uh breaking",
    "start": "802120",
    "end": "809519"
  },
  {
    "text": "down these parts we can now analyze the pattern on the per service level for CPU",
    "start": "809519",
    "end": "814839"
  },
  {
    "text": "usage it's quite common for services to have usage patterns that are periodic in nature and uh changes throughout the day",
    "start": "814839",
    "end": "822880"
  },
  {
    "text": "for instance uh there may be a lot more users on the shopy app in our case during the lunch and evening periods and",
    "start": "822880",
    "end": "829120"
  },
  {
    "text": "lesser users during office hours or at night when they are sleeping some of our services may serve",
    "start": "829120",
    "end": "835320"
  },
  {
    "text": "users from a single time zone while other services may serve users across m multiple time zones and those Services",
    "start": "835320",
    "end": "841279"
  },
  {
    "text": "might see multiple Peaks for such Services their CPU usage often reflects",
    "start": "841279",
    "end": "846839"
  },
  {
    "text": "the real world users Behavior patterns and would be affected by real world events which could be seasonal and this",
    "start": "846839",
    "end": "853440"
  },
  {
    "text": "could include uh weekends monthly campaigns public holidays or even",
    "start": "853440",
    "end": "859320"
  },
  {
    "text": "elections so now that we are familiar with the characteristics of resource resource usage how do we go about go",
    "start": "859320",
    "end": "865600"
  },
  {
    "text": "about forecasting it so we may know the periodicity of certain resource usage patterns in the",
    "start": "865600",
    "end": "872800"
  },
  {
    "text": "case of CPU which largely follows user behavior on the daily basis one can assume that CPU usage has a periodicity",
    "start": "872800",
    "end": "879079"
  },
  {
    "text": "of 24 hours and as such one very simple solution is to Simply reuse the past",
    "start": "879079",
    "end": "884320"
  },
  {
    "text": "days uh value for predicting the current or the Future Days value so while this",
    "start": "884320",
    "end": "890360"
  },
  {
    "text": "might seem very extremely simplistic and naive this is actually the basic idea behind our forecasting method in",
    "start": "890360",
    "end": "896920"
  },
  {
    "text": "production what we would do is to actually take the last 14 days of data to predict the utilization for the next",
    "start": "896920",
    "end": "903040"
  },
  {
    "text": "24 hours we may also need to deal with imperfections in data collection as we",
    "start": "903040",
    "end": "910160"
  },
  {
    "text": "are currently using predas for uh data collection uh this collects data by scraping the metrix endpoint but what if",
    "start": "910160",
    "end": "917040"
  },
  {
    "text": "the endpoint was temporarily unavailable so if there was a timeout for example we might not have an observation for the",
    "start": "917040",
    "end": "923000"
  },
  {
    "text": "time period but fortunately we can make use of uh regular data imputation methods such as backfill interpolation",
    "start": "923000",
    "end": "929639"
  },
  {
    "text": "and forward fill to fill up these gaps so what we do is we make use of den",
    "start": "929639",
    "end": "936360"
  },
  {
    "text": "noising approaches to handle outliers and noise to provide a more smooth forecast since we have data that is",
    "start": "936360",
    "end": "942519"
  },
  {
    "text": "known to be periodic in nature what we did is to adopt the fer transform for this",
    "start": "942519",
    "end": "948360"
  },
  {
    "text": "approach so in order to denoise data what we did is we first use uh fft or",
    "start": "948360",
    "end": "954279"
  },
  {
    "text": "the fast for transform function to transform data into its frequency domain within the frequency domain over here uh",
    "start": "954279",
    "end": "962000"
  },
  {
    "text": "noisy signals or high frequencies that we saw previously would actually be",
    "start": "962000",
    "end": "967120"
  },
  {
    "text": "transformed into regions of lower amplitude so we can actually remove",
    "start": "967120",
    "end": "972720"
  },
  {
    "text": "these low amplitudes via a low pass filter and then transform the signal back using the inverse fft function to",
    "start": "972720",
    "end": "979639"
  },
  {
    "text": "transform the to to get back a d noise signal in the uh from the time domain so we use the uh for transform",
    "start": "979639",
    "end": "987759"
  },
  {
    "text": "approach not just for Den noising but in fact it also helps us to forecast signals with any periodicity not just 24",
    "start": "987759",
    "end": "993319"
  },
  {
    "text": "hours or daily uh periods for example a service might have a period of like a",
    "start": "993319",
    "end": "999880"
  },
  {
    "text": "weekly pattern inste and using fft we can actually predict the future value of any periodic signal once we're able to",
    "start": "999880",
    "end": "1006279"
  },
  {
    "text": "break down the signal in these parts and as uh you might know so um every signal",
    "start": "1006279",
    "end": "1012440"
  },
  {
    "text": "consists of many sine waves of varying frequencies amplitudes and phases and fft helped us do just that so so this",
    "start": "1012440",
    "end": "1019240"
  },
  {
    "text": "essentially means that using the fft approach we can generate forecasts for different Services even without knowing their periodicities in advance",
    "start": "1019240",
    "end": "1025240"
  },
  {
    "text": "regardless if it is an hourly daily or even weekly",
    "start": "1025240",
    "end": "1030798"
  },
  {
    "text": "pattern uh we also needed to take care when consuming forast to handle sudden spikes or breaks in between forecast",
    "start": "1031559",
    "end": "1038280"
  },
  {
    "text": "boundaries the jump in values between two distinct forecasts uh May introduce",
    "start": "1038280",
    "end": "1043400"
  },
  {
    "text": "certain artifacts that didn't exist in the underly signal or this is what we call Phantom spikes",
    "start": "1043400",
    "end": "1049919"
  },
  {
    "text": "so how we avoided this is to overlap multiple time series of separate prediction Cycles over each other and",
    "start": "1049919",
    "end": "1055559"
  },
  {
    "text": "use interpolation to Simply uh gradually transition values from one time series",
    "start": "1055559",
    "end": "1060640"
  },
  {
    "text": "into another so the result is we'll get a single forecast that is a very smooth and removes all of the Phantom spikes",
    "start": "1060640",
    "end": "1066880"
  },
  {
    "text": "that I mentioned earlier so with all this in hand we can now go back to our favorite example as I",
    "start": "1066880",
    "end": "1073480"
  },
  {
    "text": "shared earlier uh whether where our best jobs that need to run Presto queries need a lot long graceful shutdown period",
    "start": "1073480",
    "end": "1080039"
  },
  {
    "text": "of 60 Minutes to avoid wasted computation well now we have a long-term forecast available and we can see that",
    "start": "1080039",
    "end": "1086840"
  },
  {
    "text": "for example within the next hour uh our forecast tells us that the node is",
    "start": "1086840",
    "end": "1091880"
  },
  {
    "text": "expected to have a sharp increase in usage due to a predicted CPU usage Spike",
    "start": "1091880",
    "end": "1097080"
  },
  {
    "text": "for a particular service B yeah so on the scheduler this means that we should refrain from putting new",
    "start": "1097080",
    "end": "1103000"
  },
  {
    "text": "batch pods onto the not at this point in time and our forecast tells us that this is there is a strong likelihood that",
    "start": "1103000",
    "end": "1108280"
  },
  {
    "text": "those parts will be evicted within the next hour so for the existing batch PS that were already present on this note the",
    "start": "1108280",
    "end": "1114919"
  },
  {
    "text": "graceful shutdown period uh sorry a graceful shutdown procedure can be proceeded can be triggered on the batch",
    "start": "1114919",
    "end": "1121400"
  },
  {
    "text": "ports to start relocating them to other notes which give them gives them actually a time out of 1 hour before",
    "start": "1121400",
    "end": "1127000"
  },
  {
    "text": "they get forcefully evicted from the note before the predicted search will actually start to",
    "start": "1127000",
    "end": "1132400"
  },
  {
    "text": "happen so this uh how do we actually Implement all of this in production so this is our very brief architecture",
    "start": "1132400",
    "end": "1139559"
  },
  {
    "text": "diagram first we'll need to collect all of the container and note resource utilization metrics via predas as I",
    "start": "1139559",
    "end": "1145679"
  },
  {
    "text": "described earlier this data can then be ingested into a data warehouse data warehouse where it can be accessed",
    "start": "1145679",
    "end": "1152360"
  },
  {
    "text": "through Hive tables we can then make use of data pipelines using spark to perform batch",
    "start": "1152360",
    "end": "1158720"
  },
  {
    "text": "processing on the captured metrix where the forecasting functions can then be implemented in the form of a user",
    "start": "1158720",
    "end": "1164720"
  },
  {
    "text": "defined function or a UDF then the forecast are then put back in into the hi tables where they can be queried",
    "start": "1164720",
    "end": "1170440"
  },
  {
    "text": "again later so how all of this ties in the kubernetes we in Cube schedule",
    "start": "1170440",
    "end": "1176240"
  },
  {
    "text": "itself we have implemented a custom plugin that can read the forecast data stored on the hi table previously and",
    "start": "1176240",
    "end": "1182440"
  },
  {
    "text": "then we adjust the filter stage to exclude nodes that that have a high risk of eviction as I described",
    "start": "1182440",
    "end": "1190559"
  },
  {
    "text": "previously so I've explained how we've managed to come up with a somewhat simple model that allowed us to perform",
    "start": "1191360",
    "end": "1197240"
  },
  {
    "text": "long-term forecasting to predict the future usage for a particular service but can we do even",
    "start": "1197240",
    "end": "1203600"
  },
  {
    "text": "better while our simplistic and naive model can have uh decent accuracy to",
    "start": "1203600",
    "end": "1208799"
  },
  {
    "text": "predict the actual utilization we found several limitations that that can be easily overcome so there are some notice",
    "start": "1208799",
    "end": "1216360"
  },
  {
    "text": "noticeable deviations that you can see over here between the green and blue lines uh between the forecaster and",
    "start": "1216360",
    "end": "1221480"
  },
  {
    "text": "actual observations and this shows much room for improvement firstly this model is not",
    "start": "1221480",
    "end": "1226919"
  },
  {
    "text": "able to react to changes in trends quickly because it assumes a perfectly repeating pattern and thus we can",
    "start": "1226919",
    "end": "1232679"
  },
  {
    "text": "observe an error that could potentially last for a few days uh even after a Services utilization has drastically",
    "start": "1232679",
    "end": "1239559"
  },
  {
    "text": "changed another significant limitation is that this model can't handle seasonalities of longer durations uh for",
    "start": "1239559",
    "end": "1246600"
  },
  {
    "text": "example there could be a monthly campaign or public holidays that happen once a year so we currently use the past",
    "start": "1246600",
    "end": "1253120"
  },
  {
    "text": "14 days worth of data as our input and we would have to massively scale up the amount of input data to be processed",
    "start": "1253120",
    "end": "1258960"
  },
  {
    "text": "just to support longer term seasonal patterns and it's currently impr practical if we need to do this for every single microservice in the",
    "start": "1258960",
    "end": "1265919"
  },
  {
    "text": "company we've also considered other several complex models available in the community and one such model we tried",
    "start": "1265919",
    "end": "1271559"
  },
  {
    "text": "out was uh profit and open source forecasting model released by meta as we",
    "start": "1271559",
    "end": "1277200"
  },
  {
    "text": "can see profit was able to start accounting for newer Trends much earlier than our naive fft model and thus",
    "start": "1277200",
    "end": "1283760"
  },
  {
    "text": "minimizing the amount of days that the forecasted value was mispredicted",
    "start": "1283760",
    "end": "1289039"
  },
  {
    "text": "but that being said although these so-called more complex models can offer Superior accuracy and able to tolerate",
    "start": "1289039",
    "end": "1295600"
  },
  {
    "text": "TR changes in underline Trends better they often much costlier and the performance of the prediction step does",
    "start": "1295600",
    "end": "1301320"
  },
  {
    "text": "not scale as well this because most of the static St sorry statistical models",
    "start": "1301320",
    "end": "1308320"
  },
  {
    "text": "are not Global in scope and thus every single unique microservice needs a separate training and fitting cycle and",
    "start": "1308320",
    "end": "1313919"
  },
  {
    "text": "this has to be repeated tens of thousands of times for every single unique microservice that we have",
    "start": "1313919",
    "end": "1320080"
  },
  {
    "text": "so another popular growing option is to leverage machine learning or deep learning models such as Transformers",
    "start": "1320080",
    "end": "1326200"
  },
  {
    "text": "long short-term memory or even linear models to perform forecasting so unlike statistical models which I've covered up",
    "start": "1326200",
    "end": "1333480"
  },
  {
    "text": "to now ML and DL models can be trained on a wider variety of data and the same",
    "start": "1333480",
    "end": "1338640"
  },
  {
    "text": "model can be used to perform inferences on more than one service with reasonable",
    "start": "1338640",
    "end": "1344120"
  },
  {
    "text": "accuracy let's see how we can utilize DL models for our use case",
    "start": "1344120",
    "end": "1349799"
  },
  {
    "text": "we we must note that for a more complex model this will take more time to train given the same training data set as",
    "start": "1349799",
    "end": "1355840"
  },
  {
    "text": "compared to a simple model and based on our experience uh using a more complex model does not necessarily give rise to",
    "start": "1355840",
    "end": "1362000"
  },
  {
    "text": "better performance than a simpler model for D models we also interested in",
    "start": "1362000",
    "end": "1367039"
  },
  {
    "text": "to see how well the model can generalize or in other words how well it can handle data it has not seen before additionally",
    "start": "1367039",
    "end": "1373559"
  },
  {
    "text": "we also hope that it's able to handle forecast of any arbitrary time period and should not be restricted to the",
    "start": "1373559",
    "end": "1379480"
  },
  {
    "text": "shorter time windows that we restricted to in our statistical models previously this uh pre-train model can",
    "start": "1379480",
    "end": "1387039"
  },
  {
    "text": "then be packaged and used for model inference later such as part of a data pipeline for generating the",
    "start": "1387039",
    "end": "1393039"
  },
  {
    "text": "forecast working with Dr models also requires some development costs for instance uh to figure out what kind of",
    "start": "1393039",
    "end": "1399640"
  },
  {
    "text": "models we need and what hyper parameters to use so what we can do is to automate part of these chores using uh certain",
    "start": "1399640",
    "end": "1405840"
  },
  {
    "text": "tools like R tune in order to test hyper parameters in a distributed manner we also found that uh using",
    "start": "1405840",
    "end": "1412840"
  },
  {
    "text": "covariates can help to improve the accuracy of the prediction which in our case helps us to inject additional",
    "start": "1412840",
    "end": "1418440"
  },
  {
    "text": "correlated variables into the model to make a better prediction so for us a good example of a useful covariant is uh",
    "start": "1418440",
    "end": "1425000"
  },
  {
    "text": "campaign events so uh where we observe that utilization utilization is very",
    "start": "1425000",
    "end": "1430360"
  },
  {
    "text": "much different from the regular scenario on the daily basis since we know in advance When Future campaigns will be",
    "start": "1430360",
    "end": "1436640"
  },
  {
    "text": "held this campaign uh this model can then account for the anomalies AO automatically and adjust the forecasted",
    "start": "1436640",
    "end": "1443000"
  },
  {
    "text": "result automatically so now let's recreate our previous forecasting architecture using",
    "start": "1443000",
    "end": "1448679"
  },
  {
    "text": "Dr models we actually don't need to make too many changes to our original architecture so over here we simply just",
    "start": "1448679",
    "end": "1455600"
  },
  {
    "text": "replace the forecasting function with a call to a UDF that will perform the model inference instead the training",
    "start": "1455600",
    "end": "1462400"
  },
  {
    "text": "step is decoupled from the actual forecasting pipeline we can do the training using py toch on a distributed",
    "start": "1462400",
    "end": "1467799"
  },
  {
    "text": "cluster using gpus and Export the model into a portable format using Onyx and this can be directly used for online",
    "start": "1467799",
    "end": "1474520"
  },
  {
    "text": "prediction we can also periodically retrain the model in order to take account into recent changes to service",
    "start": "1474520",
    "end": "1479799"
  },
  {
    "text": "patterns over time so based on some of our very early",
    "start": "1479799",
    "end": "1484880"
  },
  {
    "text": "findings using some of these models we were able to get pretty good results for forecasting CPU utilization but we will",
    "start": "1484880",
    "end": "1491279"
  },
  {
    "text": "need much more fine-tuning to outperform the the statistical models that we currently have in",
    "start": "1491279",
    "end": "1497320"
  },
  {
    "text": "production so with that we uh evaluated several forecasting models including",
    "start": "1497320",
    "end": "1502399"
  },
  {
    "text": "both uh statistical and dbased models uh using the models we've tested",
    "start": "1502399",
    "end": "1507880"
  },
  {
    "text": "all of them have comparable accuracy but the more complex models we've evaluated would handle anomalies or changes and",
    "start": "1507880",
    "end": "1513799"
  },
  {
    "text": "Trend much quicker uh than the simpler models that I described another additional thing is",
    "start": "1513799",
    "end": "1520240"
  },
  {
    "text": "that stat statistical models are also easier to reason about and th easier to deug and this turns out it actually",
    "start": "1520240",
    "end": "1526840"
  },
  {
    "text": "makes it much simpler to use and push the production to handle the most common scenarios and that's why it's in production right now certain models also",
    "start": "1526840",
    "end": "1534159"
  },
  {
    "text": "have a prohibitively higher cost during inference which makes it costlier when working with thousands of unique time",
    "start": "1534159",
    "end": "1540399"
  },
  {
    "text": "series so despite its Simplicity we've actually found much greater success in using uh the naive fft models in",
    "start": "1540399",
    "end": "1547240"
  },
  {
    "text": "production at the moment but we also currently quickly running into certain roadblocks that can only be solved by",
    "start": "1547240",
    "end": "1553240"
  },
  {
    "text": "fine-tuning these more complex models to overcome them in the future so far we've covered how we can",
    "start": "1553240",
    "end": "1560440"
  },
  {
    "text": "make use of forecasting of uh note utilization for the purposes of running batch workloads on a single note but",
    "start": "1560440",
    "end": "1567200"
  },
  {
    "text": "there's a lot more we can do with these powerful techniques to forecast a Services utilization in the future and",
    "start": "1567200",
    "end": "1573559"
  },
  {
    "text": "this allows us to achieve higher resource utilization improve resource density across our entire fleet uh at",
    "start": "1573559",
    "end": "1579200"
  },
  {
    "text": "the same time while minimizing business impact so in kubernetes we know that we",
    "start": "1579200",
    "end": "1584880"
  },
  {
    "text": "can oversell Resources by setting the resource request lower than the limit these resources are configured by",
    "start": "1584880",
    "end": "1590600"
  },
  {
    "text": "developers in charge of the service and this uh in uh and this gives rise uh to",
    "start": "1590600",
    "end": "1596360"
  },
  {
    "text": "challenges for us as platform maintainers in our company so for one in shoy the business pic are often risk",
    "start": "1596360",
    "end": "1603640"
  },
  {
    "text": "Avers and would very much prefer to overr provision their resources especially for a new service which might",
    "start": "1603640",
    "end": "1609039"
  },
  {
    "text": "not yet be fully optimized and since they are focus on stability pic would",
    "start": "1609039",
    "end": "1614480"
  },
  {
    "text": "rather avoid changing these configurations even after their service was optimized in the future because you",
    "start": "1614480",
    "end": "1620440"
  },
  {
    "text": "know if it aim broke why fix it you know additionally microservices in our company are also partitioned by the",
    "start": "1620440",
    "end": "1626640"
  },
  {
    "text": "Target region that it serves and it's such a larger market like Indonesia would require much more resources than",
    "start": "1626640",
    "end": "1632039"
  },
  {
    "text": "say Singapore however pics tend to reuse the same configuration across all regions resulting in certain regions",
    "start": "1632039",
    "end": "1638799"
  },
  {
    "text": "that are much are very much over provision since we're approaching",
    "start": "1638799",
    "end": "1644279"
  },
  {
    "text": "kubernetes scheduling via a data driven perspective let's do this in a smarter way",
    "start": "1644279",
    "end": "1649520"
  },
  {
    "text": "instead since we have access to the historical metrics for each service we can actually generate long-term forecast",
    "start": "1649520",
    "end": "1654760"
  },
  {
    "text": "for each of them to automatically derive the requests and limits for the service and we don't have to depend on the",
    "start": "1654760",
    "end": "1660240"
  },
  {
    "text": "inconsistent configuration from the application pic's with this approach the amount of resources allocated can slowly",
    "start": "1660240",
    "end": "1667399"
  },
  {
    "text": "approach the true utilization of the service and this improves the resource uh efficiency of our Fleet over",
    "start": "1667399",
    "end": "1673279"
  },
  {
    "text": "time to refine this even further we can consider the resource utilization of of",
    "start": "1673279",
    "end": "1678320"
  },
  {
    "text": "services throughout periods of the day so remember earlier I mentioned that",
    "start": "1678320",
    "end": "1683880"
  },
  {
    "text": "our end user facing services are very much mirror the behavior of our users in the real world so this results in very",
    "start": "1683880",
    "end": "1689919"
  },
  {
    "text": "different usage patterns uh throughout the day across different services that service different markets across",
    "start": "1689919",
    "end": "1695039"
  },
  {
    "text": "different time zones as such certain Services May face pck usage at different times from others due to differing time",
    "start": "1695039",
    "end": "1701320"
  },
  {
    "text": "zones and we can then exploit this fact by placing services that serve different",
    "start": "1701320",
    "end": "1706480"
  },
  {
    "text": "time zones on the same note and this uh allows us to use a more aggressive over",
    "start": "1706480",
    "end": "1711840"
  },
  {
    "text": "sale factor and this can be done using extended resources as I show over",
    "start": "1711840",
    "end": "1717799"
  },
  {
    "text": "here we can avoid packing too many services that Peak at the same time on the same note and thus uh oversell the",
    "start": "1718799",
    "end": "1724679"
  },
  {
    "text": "resources instead to other services that are not expected a Peak at the same time this information can all be derived from",
    "start": "1724679",
    "end": "1730080"
  },
  {
    "text": "the long-term forecast of the services utilization that I described earlier we can also use our forecast to",
    "start": "1730080",
    "end": "1736640"
  },
  {
    "text": "improve our HPA or horizontal Port Auto scaling so typically the HPA depends on",
    "start": "1736640",
    "end": "1742360"
  },
  {
    "text": "a specific CPU utilization Target to be hit before scaling up the number of replicas so in some cases this might",
    "start": "1742360",
    "end": "1749960"
  },
  {
    "text": "already be too late and uh the business impacts might actually already be start to be noticed since we know the",
    "start": "1749960",
    "end": "1755679"
  },
  {
    "text": "utilization forecast for a service beforehand this allows us to easily predict the number of replicas that are",
    "start": "1755679",
    "end": "1761120"
  },
  {
    "text": "needed to keep the CPU utilization within this target range and this allows us to preemptively scale up scale",
    "start": "1761120",
    "end": "1767399"
  },
  {
    "text": "service in advance so that any large  in CPU during regular usage Peaks won't cause any noticeable degradation",
    "start": "1767399",
    "end": "1773320"
  },
  {
    "text": "to the user experience and now that we're using forast projections instead of the",
    "start": "1773320",
    "end": "1779360"
  },
  {
    "text": "developers configured settings we now face a potential problem when services are updated with new features or start",
    "start": "1779360",
    "end": "1786320"
  },
  {
    "text": "serving more user traffic or get Sunset over time their utilization might also dramatically change over time as such if",
    "start": "1786320",
    "end": "1792960"
  },
  {
    "text": "there's any changes to the projected uh resource utilization we'll also need to modify by the port resource request at",
    "start": "1792960",
    "end": "1799440"
  },
  {
    "text": "the same time thankfully uh in place updates of Port resources are supported since kubernetes 1.27 and this allows us",
    "start": "1799440",
    "end": "1806799"
  },
  {
    "text": "to change uh the resource spec for existing ports without needing to do a costly redeployment additionally to avoid",
    "start": "1806799",
    "end": "1813720"
  },
  {
    "text": "frequent flapping of the ports resource values that may result in uh frequent rescheduling we can also leverage the",
    "start": "1813720",
    "end": "1819320"
  },
  {
    "text": "short-term forecast techniques I described earlier so in the event that vertically",
    "start": "1819320",
    "end": "1826240"
  },
  {
    "text": "resizing the port is not feasible we can then relocate the port to another machine we can make use of a d to",
    "start": "1826240",
    "end": "1832720"
  },
  {
    "text": "explicitly remove the port after verifying that we can move it",
    "start": "1832720",
    "end": "1837799"
  },
  {
    "text": "elsewhere since we now have a much greater insight into the future of uh ports utilization this allows us to",
    "start": "1838760",
    "end": "1845320"
  },
  {
    "text": "further improve note note density on as a whole on in the whole cluster so I'm",
    "start": "1845320",
    "end": "1850720"
  },
  {
    "text": "sure many of you are familiar with cluster autoscaler which can help to scale down notes that are not uh needed",
    "start": "1850720",
    "end": "1856279"
  },
  {
    "text": "but yet all parts resource request can still be satisfied using our projected forecast this helps us to reduce the",
    "start": "1856279",
    "end": "1862399"
  },
  {
    "text": "total number of resources that need to be allocated but for us at shoy where where",
    "start": "1862399",
    "end": "1868080"
  },
  {
    "text": "we make use heavy use of on pray machines we can't easily reap the benefits of decommissioning a node as",
    "start": "1868080",
    "end": "1873360"
  },
  {
    "text": "easily as compared to using the cloud so what else can we do so for on Prem scenarios we actually",
    "start": "1873360",
    "end": "1879200"
  },
  {
    "text": "start to be creative and we actually start to uh set some uh we we instead we",
    "start": "1879200",
    "end": "1886799"
  },
  {
    "text": "control the C state of the CP pu and this allows it to downclock and enter a lower energy consumption state so this",
    "start": "1886799",
    "end": "1893840"
  },
  {
    "text": "is actually beneficial because we can't afford to completely shut down the machines that are not needed because it",
    "start": "1893840",
    "end": "1899000"
  },
  {
    "text": "may take a couple of minutes to start back up what if it doesn't start back up you know yeah so uh if it's not really",
    "start": "1899000",
    "end": "1904799"
  },
  {
    "text": "suitable for Real Time reaction to scale up events uh our findings is that this can",
    "start": "1904799",
    "end": "1910399"
  },
  {
    "text": "save up to 60 wats per machine and when applied to thousands of machines across our Fleet this is quite",
    "start": "1910399",
    "end": "1916360"
  },
  {
    "text": "significant and aside from putting the machines in low power State there's also other things that we can do with uh",
    "start": "1916360",
    "end": "1921600"
  },
  {
    "text": "these unused machines so we could temporarily convert the node into use for other bare meal applications or for",
    "start": "1921600",
    "end": "1928360"
  },
  {
    "text": "other uh nonnes applications so this actually allows us to share resources with other teams not yet running on",
    "start": "1928360",
    "end": "1934679"
  },
  {
    "text": "kubernetes without having to Bear The Upfront cost of them migrating to kubernetes just yet we can also choose",
    "start": "1934679",
    "end": "1940840"
  },
  {
    "text": "to Simply shut down the machine uh but as I mentioned earlier this uh requires a much longer lead time so this is where",
    "start": "1940840",
    "end": "1948320"
  },
  {
    "text": "forecasting can actually come in useful I just skip this",
    "start": "1948320",
    "end": "1954120"
  },
  {
    "text": "slide so how can we detect when uh we need to scale these notes back up for",
    "start": "1954120",
    "end": "1959639"
  },
  {
    "text": "one we can use the cube schedular backlog or the number of pending ports to detect that the cluster is indeed",
    "start": "1959639",
    "end": "1964840"
  },
  {
    "text": "star of resources however this might often times be too late so what we also do is to monitor the allocation ratio of",
    "start": "1964840",
    "end": "1972200"
  },
  {
    "text": "CPU and memory resources and always retain a small buffer that can be used for Port scheduling at all times and to",
    "start": "1972200",
    "end": "1978559"
  },
  {
    "text": "reclaim the notes we can quickly convert a note back from the low power State back to the normal C State additionally",
    "start": "1978559",
    "end": "1985200"
  },
  {
    "text": "we and also start to boot up back some of the machines that we may have powered off",
    "start": "1985200",
    "end": "1990320"
  },
  {
    "text": "earlier and since there's a pretty high overhead when switching the node between low power State and normal State we want",
    "start": "1990320",
    "end": "1996039"
  },
  {
    "text": "to reduce occurrence of this happening so there are certain scenarios which are fairly predictable uh and this we can",
    "start": "1996039",
    "end": "2003000"
  },
  {
    "text": "take from our service utilization forecast so this also allows us we can predict to predict the note scale up",
    "start": "2003000",
    "end": "2009240"
  },
  {
    "text": "events in advance and this also gives us a great period that we can use to handle any note reboot or note provisioning",
    "start": "2009240",
    "end": "2015440"
  },
  {
    "text": "procedures before the ports are ready to be scheduled so if you've been paying close attention you might notice that",
    "start": "2015440",
    "end": "2021240"
  },
  {
    "text": "this graph is very similar to the same graph that I showed earlier and basically that's the idea of this",
    "start": "2021240",
    "end": "2027039"
  },
  {
    "text": "sharing so to wrap up we have explored various ways to leverage forecasting for bch and online services from a note",
    "start": "2027039",
    "end": "2033240"
  },
  {
    "text": "level all the way up to techniques for improving resource efficiency at the cluster level",
    "start": "2033240",
    "end": "2038440"
  },
  {
    "text": "we've started by showing how we can make use of short-term forecasting to reduce volatility of the input signal in order",
    "start": "2038440",
    "end": "2044200"
  },
  {
    "text": "to minimize unnecessary reactions such as evictions uh in response to short-term fluctuations we also showed how we have",
    "start": "2044200",
    "end": "2051919"
  },
  {
    "text": "uh used long-term forecasting to handle reactions that need sometime to take effect such as graceful shutdown of B",
    "start": "2051919",
    "end": "2058000"
  },
  {
    "text": "jobs and the Reclamation of nodes from a low power state or from a power off State and through these forecasting",
    "start": "2058000",
    "end": "2064320"
  },
  {
    "text": "techniques we have also demonstrated various ways to improve the resource SCE density through automating oversell and",
    "start": "2064320",
    "end": "2070000"
  },
  {
    "text": "cluster Auto scaling so with that I'd like to end today's sharing so I hope you learned something today about how",
    "start": "2070000",
    "end": "2076280"
  },
  {
    "text": "you can make use of forecasting techniques to improve cost efficiency in your organization if you would like to",
    "start": "2076280",
    "end": "2081440"
  },
  {
    "text": "leave any feedback on this session you can do so by scanning this QR code and if you have further questions feel free",
    "start": "2081440",
    "end": "2086679"
  },
  {
    "text": "to reach out to me on the cncf slack yeah so thank",
    "start": "2086679",
    "end": "2092240"
  },
  {
    "text": "you I think we have time for one",
    "start": "2096640",
    "end": "2105200"
  },
  {
    "text": "question uh hi first of all amazing speech thank you so much uh very",
    "start": "2122599",
    "end": "2127760"
  },
  {
    "text": "relevant right now um you said that you use the statistical model profit did you try the neural",
    "start": "2127760",
    "end": "2135520"
  },
  {
    "text": "profit yeah actually we did so uh actually my teammate Nicholas tried out neuro profit but unfortunately he's not",
    "start": "2135760",
    "end": "2141280"
  },
  {
    "text": "here today and I think he's much more familiar with the results uh I think we still need much more time to fine-tune",
    "start": "2141280",
    "end": "2147800"
  },
  {
    "text": "the ne profit because I think the performance overhead was non- negligible for us to actually use in production at",
    "start": "2147800",
    "end": "2153280"
  },
  {
    "text": "the moment yeah if I remember yeah if I remember correctly the neural perfect is",
    "start": "2153280",
    "end": "2159720"
  },
  {
    "text": "the yeah more advanced uh than the original perfect and actually uh the the",
    "start": "2159720",
    "end": "2165319"
  },
  {
    "text": "result showing the slider we actually we based on the neeral prefect to do do the",
    "start": "2165319",
    "end": "2171119"
  },
  {
    "text": "prediction yeah yeah but Nicholas can unfortunately he's not here I cannot",
    "start": "2171119",
    "end": "2176480"
  },
  {
    "text": "share more detail and code to you yeah you can reach reach on S and I think he'll reply",
    "start": "2176480",
    "end": "2183880"
  },
  {
    "text": "there so yeah I I was also going to ask something about forecasting did you",
    "start": "2186560",
    "end": "2192119"
  },
  {
    "text": "tried reinforcement learning instead maybe to just first on the available",
    "start": "2192119",
    "end": "2198440"
  },
  {
    "text": "statistical data then while it is uh working on the production it can also",
    "start": "2198440",
    "end": "2204319"
  },
  {
    "text": "continue learning like when it fails it's a punishment When It Go does well",
    "start": "2204319",
    "end": "2210319"
  },
  {
    "text": "does well so yeah thanks for your question I think for reinforcement learning it's uh that",
    "start": "2210319",
    "end": "2216200"
  },
  {
    "text": "one is more like agent based so it's kind of like what what do you do when you react to a certain scenario right",
    "start": "2216200",
    "end": "2221880"
  },
  {
    "text": "but for us we're more interested in actually just predicting the future so it's actually more uh it's more suitable",
    "start": "2221880",
    "end": "2227400"
  },
  {
    "text": "to use forecasting techniques but actually how over here my manager he actually uh have he actually had another",
    "start": "2227400",
    "end": "2233280"
  },
  {
    "text": "sharing which relates to reinforcement learning so maybe you can share yeah we have a sharing a couple days ago but is",
    "start": "2233280",
    "end": "2239839"
  },
  {
    "text": "in Co AI days is interest this how we use reinforced learning to uh disc",
    "start": "2239839",
    "end": "2244960"
  },
  {
    "text": "scarying yeah because I think there there are fundamental difference between",
    "start": "2244960",
    "end": "2250640"
  },
  {
    "text": "reinforced learning and uh uh no normal",
    "start": "2250640",
    "end": "2256400"
  },
  {
    "text": "uh like yeah forecasting because forting is just predition but for the re iners",
    "start": "2256400",
    "end": "2261800"
  },
  {
    "text": "learning is made decision in will change the state so it's the fundamental difference so yeah for the forecasting",
    "start": "2261800",
    "end": "2269000"
  },
  {
    "text": "May suit no suitable for the reinforcement have much more",
    "start": "2269000",
    "end": "2275280"
  },
  {
    "text": "accurate um yeah uh forecasting we can just uh yeah adopt",
    "start": "2275280",
    "end": "2281560"
  },
  {
    "text": "more urate time serious uh models instead of using the reinforced learning",
    "start": "2281560",
    "end": "2287319"
  },
  {
    "text": "yeah reinforced learning yeah what what we try to change in the reinforced learning is the agent",
    "start": "2287319",
    "end": "2293480"
  },
  {
    "text": "usually ations U maybe more suitable for the scare or disc",
    "start": "2293480",
    "end": "2300440"
  },
  {
    "text": "yeah okay um I think I need to return the mic right now so if you have any questions I I'll be right here so you",
    "start": "2300440",
    "end": "2306000"
  },
  {
    "text": "can answer to answer any queries yeah thanks",
    "start": "2306000",
    "end": "2310960"
  }
]