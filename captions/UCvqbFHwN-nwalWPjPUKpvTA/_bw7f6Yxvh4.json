[
  {
    "text": "hello everyone welcome to the one of the last sessions of this kubecon thanks for sticking it",
    "start": "4799",
    "end": "11760"
  },
  {
    "text": "up today's session is going to be on smarter golden signals",
    "start": "11760",
    "end": "18359"
  },
  {
    "text": "my name is Anusha raghunathan and my co-presenter is venkat gunapati and both",
    "start": "18359",
    "end": "24300"
  },
  {
    "text": "of us are principal Engineers working on the kubernetes platform infrastructure at Intuit",
    "start": "24300",
    "end": "31580"
  },
  {
    "text": "today we'll be talking about what we are here for which is smarter",
    "start": "32540",
    "end": "39780"
  },
  {
    "text": "golden signals why we went about exploring this project",
    "start": "39780",
    "end": "46399"
  },
  {
    "text": "what does it mean to have cluster golden signals",
    "start": "46440",
    "end": "51559"
  },
  {
    "text": "anomaly detection for a kubernetes cluster and tools that we explored",
    "start": "52739",
    "end": "59460"
  },
  {
    "text": "we'll introduce you to Numa praj a new open source project that has been incubated at introit",
    "start": "59460",
    "end": "67140"
  },
  {
    "text": "and finally finish it off with a demo and takeaways",
    "start": "67140",
    "end": "73100"
  },
  {
    "text": "for those who don't know about Intuit Intuit is a fintech platform company",
    "start": "76799",
    "end": "82439"
  },
  {
    "text": "that's popularly known for building Financial products and services around",
    "start": "82439",
    "end": "88500"
  },
  {
    "text": "tax prep and filing accounting and payroll such as QuickBooks",
    "start": "88500",
    "end": "94680"
  },
  {
    "text": "credit score analysis using Credit Karma and small medium business marketing",
    "start": "94680",
    "end": "100860"
  },
  {
    "text": "tools with MailChimp and all of these Financial Services run on our",
    "start": "100860",
    "end": "108079"
  },
  {
    "text": "kubernetes-based infrastructure now if you want to take look at the",
    "start": "108079",
    "end": "115259"
  },
  {
    "text": "numbers at a glance we run about 275 kubernetes clusters",
    "start": "115259",
    "end": "121500"
  },
  {
    "text": "they are mid-size clusters that have about two twenty thousand plus namespaces",
    "start": "121500",
    "end": "128459"
  },
  {
    "text": "and they serve about 2500 Production Services I wanna call",
    "start": "128459",
    "end": "134400"
  },
  {
    "text": "out that these are just Production Services and we have a lot of pre-prod environments for QA perf testing and",
    "start": "134400",
    "end": "140160"
  },
  {
    "text": "whatnot and this is about 900 developer teams",
    "start": "140160",
    "end": "146840"
  },
  {
    "text": "and serving about 6 000 developers more than six thousand developers and some of",
    "start": "146840",
    "end": "152040"
  },
  {
    "text": "this is also seasonal traffic that will go up more than what these numbers are",
    "start": "152040",
    "end": "158900"
  },
  {
    "text": "and if you look at what a platform engineer does to observe the kubernetes",
    "start": "159780",
    "end": "165720"
  },
  {
    "text": "fleet that I was just mentioning we have several different components in",
    "start": "165720",
    "end": "171060"
  },
  {
    "text": "a kubernetes cluster that we monitor at a pretty high level we have the node",
    "start": "171060",
    "end": "178680"
  },
  {
    "text": "level components for CPU memory disk Network processes",
    "start": "178680",
    "end": "184319"
  },
  {
    "text": "then we have the kubernetes components themselves",
    "start": "184319",
    "end": "189840"
  },
  {
    "text": "and then follow that up with pod state information and finally some synthetic",
    "start": "189840",
    "end": "195900"
  },
  {
    "text": "monitoring that we run and synthetic monitoring is mainly for launching on-demand tests so that we can make sure",
    "start": "195900",
    "end": "202560"
  },
  {
    "text": "that maybe a particular workload is running particular type of networking workload is running and so on",
    "start": "202560",
    "end": "208920"
  },
  {
    "text": "so what are the metric sources for all of these for your node information we use",
    "start": "208920",
    "end": "215879"
  },
  {
    "text": "Telegraph for kubernetes we use Prometheus for pod State we use Coop",
    "start": "215879",
    "end": "221519"
  },
  {
    "text": "State metrics and for synthetic monitoring we use a tool called active monitor which is",
    "start": "221519",
    "end": "227700"
  },
  {
    "text": "incubated at intuitiveit using under the Keiko paraj umbrella of projects",
    "start": "227700",
    "end": "233700"
  },
  {
    "text": "and all of these generate alerts and when they do generate alerts the",
    "start": "233700",
    "end": "240540"
  },
  {
    "text": "platform engineer or the SRE that's on call is getting an overdose of all of these alerts",
    "start": "240540",
    "end": "247440"
  },
  {
    "text": "and they're frantically looking at a bunch of run books looking at dashboards",
    "start": "247440",
    "end": "253080"
  },
  {
    "text": "making sure that things are working okay trying to mitigate and remediate as we go",
    "start": "253080",
    "end": "258900"
  },
  {
    "text": "now note that as I mentioned earlier the scale is quite High there are about 275 plus clusters and",
    "start": "258900",
    "end": "267540"
  },
  {
    "text": "each cluster generates about 100 plus alerts so you do the math it's",
    "start": "267540",
    "end": "272880"
  },
  {
    "text": "it's going to be not too happy our platform engineer is getting a little overwhelmed here",
    "start": "272880",
    "end": "280080"
  },
  {
    "text": "now to make three things a little interesting let's throw in an incident who here likes to be on an incident call",
    "start": "280080",
    "end": "288600"
  },
  {
    "text": "okay nobody you do you work for beta Duty",
    "start": "288600",
    "end": "293960"
  },
  {
    "text": "so um now the platform engineer not only has to worry about mitigation but they're",
    "start": "299520",
    "end": "306780"
  },
  {
    "text": "they're also concerned about does this does does these services that",
    "start": "306780",
    "end": "311940"
  },
  {
    "text": "are impacted running on clusters that are healthy or unhealthy",
    "start": "311940",
    "end": "317820"
  },
  {
    "text": "they also need to understand whether this incident is this a service issue or is this a platform issue",
    "start": "317820",
    "end": "326180"
  },
  {
    "text": "and they also need to understand what the blast radius of the incident is",
    "start": "326460",
    "end": "331860"
  },
  {
    "text": "if this service is down in one cluster is it going to affect the rest of the",
    "start": "331860",
    "end": "336900"
  },
  {
    "text": "Clusters as well is it just a matter of time",
    "start": "336900",
    "end": "341240"
  },
  {
    "text": "that's how they're feeling right now they are not just a little over them they're literally drowning in alerts",
    "start": "342419",
    "end": "349620"
  },
  {
    "text": "I I'm sure many of you can relate to this because that's how I feel when I go on call",
    "start": "349620",
    "end": "355759"
  },
  {
    "text": "and what the platform engineer really wants is",
    "start": "355759",
    "end": "362340"
  },
  {
    "text": "very simple ways to reduce mttd and mttr",
    "start": "362340",
    "end": "369660"
  },
  {
    "text": "they want less positive less false positives and less false negatives when",
    "start": "369660",
    "end": "375180"
  },
  {
    "text": "I get an alert I want to be actually only alerted when there is a real problem and",
    "start": "375180",
    "end": "381419"
  },
  {
    "text": "I also want a few good quality signals I don't I want a filter signal from noise",
    "start": "381419",
    "end": "388319"
  },
  {
    "text": "and I might also like some tulips so let's talk about cluster golden",
    "start": "388319",
    "end": "394500"
  },
  {
    "text": "signals that's the motivation behind us exploring this concept of cluster golden signals now golden signals Is Not A New",
    "start": "394500",
    "end": "402000"
  },
  {
    "text": "Concept when you have a service whether it's a microservice running on kubernetes or a service running",
    "start": "402000",
    "end": "407039"
  },
  {
    "text": "elsewhere the health of a service can be determined using four metrics four sort",
    "start": "407039",
    "end": "414180"
  },
  {
    "text": "of signals high level signals Google SRE handbook released this a few years ago as some of you might be familiar with",
    "start": "414180",
    "end": "420440"
  },
  {
    "text": "and they talk about these four golden pillars which are error rates latency",
    "start": "420440",
    "end": "428180"
  },
  {
    "text": "saturation and requests or traffic so with getting a good few good signals",
    "start": "428180",
    "end": "435240"
  },
  {
    "text": "from on all these four pillars you can actually determine whether whether a service is healthy or not so we realized",
    "start": "435240",
    "end": "442380"
  },
  {
    "text": "that hey you know it applies to Services can be mapped that through kubernetes clusters as well",
    "start": "442380",
    "end": "449599"
  },
  {
    "text": "so as a service owner that's running on the kubernetes power infrastructure",
    "start": "449759",
    "end": "455580"
  },
  {
    "text": "the service owners concerns yes we do offer a lot of belts and whistles but at the end of the day they care about three",
    "start": "455580",
    "end": "462060"
  },
  {
    "text": "very high level fundamental things they care about availability",
    "start": "462060",
    "end": "468060"
  },
  {
    "text": "they care about scale and they care about correctness so we were looking to",
    "start": "468060",
    "end": "473520"
  },
  {
    "text": "see if we can map this cleanly to kubernetes components and we realized that yes we can",
    "start": "473520",
    "end": "480060"
  },
  {
    "text": "for availability you can map it to the control plane and you can probe a few things in there",
    "start": "480060",
    "end": "486680"
  },
  {
    "text": "along with networking which is pretty fundamental to availability and then for scale you can map it pretty",
    "start": "486680",
    "end": "493259"
  },
  {
    "text": "cleanly to clustered Auto scaler horizontal pod Auto scalar and vpa if you have implemented vpa",
    "start": "493259",
    "end": "500099"
  },
  {
    "text": "and finally you can map correctness to Cluster Authentication",
    "start": "500099",
    "end": "506340"
  },
  {
    "text": "cluster networking again in terms of latency and packet loss and maybe you",
    "start": "506340",
    "end": "511500"
  },
  {
    "text": "have some very custom cluster add-ons that actually give you some of the correctness in our in our case all of",
    "start": "511500",
    "end": "518039"
  },
  {
    "text": "the kubernetes Clusters we slap on about like two dozen plus cluster add-ons that",
    "start": "518039",
    "end": "524099"
  },
  {
    "text": "are helpful for day two operations for security compliance and and so on",
    "start": "524099",
    "end": "531259"
  },
  {
    "text": "so let's take a look at what these golden signals",
    "start": "531899",
    "end": "537959"
  },
  {
    "text": "translate to for us so every golden signal that emanates",
    "start": "537959",
    "end": "543180"
  },
  {
    "text": "from a kubernetes cluster the top Uber golden signals can have three distinct",
    "start": "543180",
    "end": "548279"
  },
  {
    "text": "States they can either be healthy which means that all the components that we are",
    "start": "548279",
    "end": "553380"
  },
  {
    "text": "probing for are healthy they can be in a degraded state which means that e like all of the critical",
    "start": "553380",
    "end": "560040"
  },
  {
    "text": "components should be healthy even if one of them turned to be a degraded State then we mark them as degraded",
    "start": "560040",
    "end": "566160"
  },
  {
    "text": "and finally critical then even one of these components again turn critical than the entire",
    "start": "566160",
    "end": "572940"
  },
  {
    "text": "Uber signal is marked critical now let's take a look at what the error",
    "start": "572940",
    "end": "580440"
  },
  {
    "text": "golden signal looks like for us this is a sample Prometheus rule that we've",
    "start": "580440",
    "end": "586500"
  },
  {
    "text": "written which is basically an aggregation of all of the golden signals that we cared about so for example Auto",
    "start": "586500",
    "end": "592920"
  },
  {
    "text": "scaler networking and and so on and you will see that the aggregation",
    "start": "592920",
    "end": "598440"
  },
  {
    "text": "rule you can actually customize it to how you need it but in our case we just aggregated and normalized it over a",
    "start": "598440",
    "end": "605640"
  },
  {
    "text": "particular number range now let's take a closer look at the",
    "start": "605640",
    "end": "611820"
  },
  {
    "text": "individual golden signal for a particular component in this case we're talking about node",
    "start": "611820",
    "end": "618480"
  },
  {
    "text": "local DNS nor local DNS as we all know is very crucial for making your DNS lookups",
    "start": "618480",
    "end": "625160"
  },
  {
    "text": "within your services and if that fails then you can be rest assured that your",
    "start": "625160",
    "end": "630540"
  },
  {
    "text": "availability and your correctness are going to go down so what we do is node local DNS has we",
    "start": "630540",
    "end": "636839"
  },
  {
    "text": "have Prometheus rules that make sure that node local DNS is",
    "start": "636839",
    "end": "641880"
  },
  {
    "text": "success slas are being met and how do we calculate the success SLA",
    "start": "641880",
    "end": "647100"
  },
  {
    "text": "so we you we look at the response total and then we calculate the any surf fail",
    "start": "647100",
    "end": "652980"
  },
  {
    "text": "errors over a preset period of time so for example in the last five minutes",
    "start": "652980",
    "end": "658019"
  },
  {
    "text": "how many serve fail errors did we get over the total number of responses and that's going to be your error rate and",
    "start": "658019",
    "end": "664380"
  },
  {
    "text": "then we subtract 100 from that and that's going to be a success rate SLA and a simple way to bucket this would be",
    "start": "664380",
    "end": "670620"
  },
  {
    "text": "hey if your success rate is over 99 then you're healthy if you're below 95 you're unhealthy if you're anywhere in between",
    "start": "670620",
    "end": "676800"
  },
  {
    "text": "then you are in a degraded state another way to look at error rates is",
    "start": "676800",
    "end": "683100"
  },
  {
    "text": "using error counts so some of these components that that we monitor add-ons or whatnot come up with",
    "start": "683100",
    "end": "691140"
  },
  {
    "text": "error counts instead of error rate so for example we have a CNA component it's",
    "start": "691140",
    "end": "696360"
  },
  {
    "text": "from AWS and there are three distinct components that make up the cni health",
    "start": "696360",
    "end": "703980"
  },
  {
    "text": "one is the AWS API error count another is the ipamd error count and then the",
    "start": "703980",
    "end": "711240"
  },
  {
    "text": "final one is the part eni error count so what we do is basically we look at the error counts for all of these components",
    "start": "711240",
    "end": "717180"
  },
  {
    "text": "and then we're like okay fine if the error count for this particular component over the last five minutes is",
    "start": "717180",
    "end": "724320"
  },
  {
    "text": "lesser than two then hey you know it reconciled eventually and then yes it's going to be okay if it's over five then",
    "start": "724320",
    "end": "732600"
  },
  {
    "text": "hey it's been reconciling but never got to get out of that error so hey you know maybe it's unhealthy anywhere between",
    "start": "732600",
    "end": "739860"
  },
  {
    "text": "two and five may be degraded so we came up with these rules and that's that's what the cni looks",
    "start": "739860",
    "end": "747420"
  },
  {
    "text": "like and we did this for pretty much all of the critical components that we thought were responsible for the error",
    "start": "747420",
    "end": "754200"
  },
  {
    "text": "rate and we rolled it out and we found a few things",
    "start": "754200",
    "end": "761000"
  },
  {
    "text": "we thought all of our clusters were one size fits all as far as error rates were concerned",
    "start": "762240",
    "end": "767880"
  },
  {
    "text": "and we found they weren't in fact they come in all sorts of shapes",
    "start": "767880",
    "end": "774899"
  },
  {
    "text": "and sizes and workloads some clusters like the TurboTax clusters",
    "start": "774899",
    "end": "780060"
  },
  {
    "text": "are very seasonal they are super busy from January through April of every year when tax Peak is really high in the U.S",
    "start": "780060",
    "end": "787320"
  },
  {
    "text": "and some have variable workloads throughout the day for example QuickBooks it's our accounting software",
    "start": "787320",
    "end": "793320"
  },
  {
    "text": "so people log in at 9am log off at 5 PM so it's super busy during U.S Day times",
    "start": "793320",
    "end": "799560"
  },
  {
    "text": "and then it's like super not so busy in the evenings and then there are specific",
    "start": "799560",
    "end": "804720"
  },
  {
    "text": "platform clusters that run our stream processing built build build processing and machine learning workloads that have",
    "start": "804720",
    "end": "812880"
  },
  {
    "text": "like super high volume and but they're not long lived pods they're more like",
    "start": "812880",
    "end": "818220"
  },
  {
    "text": "jobs that get scheduled and then they basically go down so every cluster is so",
    "start": "818220",
    "end": "824820"
  },
  {
    "text": "unique in their workloads so how are our static Prometheus thresholds going to",
    "start": "824820",
    "end": "831300"
  },
  {
    "text": "work for these Dynamic changing workloads right so what we what we ended",
    "start": "831300",
    "end": "837120"
  },
  {
    "text": "up having was hey the concept of cluster golden signals worked out fine but it's not going to work with static",
    "start": "837120",
    "end": "844380"
  },
  {
    "text": "thresholds because every cluster is unique",
    "start": "844380",
    "end": "848899"
  },
  {
    "text": "so then we said how are we going to do this without static thresholds and",
    "start": "849420",
    "end": "856440"
  },
  {
    "text": "that's when we started doing exploring anomaly detection and anomaly detection",
    "start": "856440",
    "end": "861959"
  },
  {
    "text": "is basically and I'm gonna code a systic blog here that specifically I like this",
    "start": "861959",
    "end": "867060"
  },
  {
    "text": "definition it says anomaly refers to an outlier in a given data set that's",
    "start": "867060",
    "end": "872519"
  },
  {
    "text": "pulled specific to an environment and it's a deviation from a confirmed pattern and anomaly detection is about",
    "start": "872519",
    "end": "880160"
  },
  {
    "text": "identifying or not these anomalous observations so a set of data points will",
    "start": "880160",
    "end": "885660"
  },
  {
    "text": "conductively help detect anomalies so we said okay let's explore anomaly detection where we can specifically look",
    "start": "885660",
    "end": "892680"
  },
  {
    "text": "at signals in the cluster and say hey yeah this is this works for this cluster and this doesn't work for another",
    "start": "892680",
    "end": "897720"
  },
  {
    "text": "cluster and also note that the 275 clusters that I'm talking about is a mix of prod and pre-prod environments so of",
    "start": "897720",
    "end": "905160"
  },
  {
    "text": "course like we all know prod clusters are not very tolerant to errors or latency issues whereas your pre-prod",
    "start": "905160",
    "end": "912120"
  },
  {
    "text": "environments are okay and and and so on so we wanted to also have that error",
    "start": "912120",
    "end": "917579"
  },
  {
    "text": "tolerance treated differently so we explored the concept of z-scores",
    "start": "917579",
    "end": "923279"
  },
  {
    "text": "for anomaly detection now Z scores as you might all um probably know is pretty standard uh",
    "start": "923279",
    "end": "929940"
  },
  {
    "text": "statistical model to detect uh the the it's basically a mean based approach",
    "start": "929940",
    "end": "935040"
  },
  {
    "text": "where you have a normal distribution of data and then you figure out the average and you get a z-score for a particular a",
    "start": "935040",
    "end": "940740"
  },
  {
    "text": "point in time and it's based on the data set that has already existed so you get a Baseline and then you say hey this new",
    "start": "940740",
    "end": "947399"
  },
  {
    "text": "data that I am getting is does that look like any of my old data or not and the",
    "start": "947399",
    "end": "953399"
  },
  {
    "text": "thing with the z-score calculation is that you can do you can get a z-score metric for the current data point using",
    "start": "953399",
    "end": "960600"
  },
  {
    "text": "a pretty standard formula so it's basically the current metric value minus",
    "start": "960600",
    "end": "965880"
  },
  {
    "text": "the average overtime divided by the standard deviation and it works pretty well like you can",
    "start": "965880",
    "end": "972480"
  },
  {
    "text": "say hey an anomaly is detected if the value is between uh it it's over the",
    "start": "972480",
    "end": "978779"
  },
  {
    "text": "three to minus three range it's normal if it's below one and minus one and if",
    "start": "978779",
    "end": "984240"
  },
  {
    "text": "it's uh it's it's slightly anomalous if it's over 2 and minus two so that's",
    "start": "984240",
    "end": "990240"
  },
  {
    "text": "that's the standard mapping that you get from from z-score you can look up any",
    "start": "990240",
    "end": "995399"
  },
  {
    "text": "statistical uh model paper and they'll tell you more details about what these scores are",
    "start": "995399",
    "end": "1001279"
  },
  {
    "text": "now the problem with z-score yes so we explore this we experimented with some of our error metrics that we wanted to",
    "start": "1001279",
    "end": "1008060"
  },
  {
    "text": "propagate as cluster golden signals and here's what happened there were several Pros it's very well",
    "start": "1008060",
    "end": "1014480"
  },
  {
    "text": "understood these scores are very well understood and uh they provide cluster specific anomaly",
    "start": "1014480",
    "end": "1021680"
  },
  {
    "text": "detection yes and guess what it's also available as part of Prometheus as a native primitive",
    "start": "1021680",
    "end": "1028520"
  },
  {
    "text": "so you can actually write alert rules pretty straightforward well what are the cons",
    "start": "1028520",
    "end": "1033938"
  },
  {
    "text": "z-score actually assumes a normal distribution so it assumes that there's going to be a",
    "start": "1033939",
    "end": "1040699"
  },
  {
    "text": "bell curve and uh if but in real life bell curve data doesn't exist all the",
    "start": "1040699",
    "end": "1046339"
  },
  {
    "text": "time in fact real world data is very different from from a bell curve so what happens is when there is a slope then",
    "start": "1046339",
    "end": "1052280"
  },
  {
    "text": "like when your data is actually trending on the downward Spike and there is an anomaly there z-score just averages",
    "start": "1052280",
    "end": "1057740"
  },
  {
    "text": "everything and says oh okay yeah you don't have an anomaly because you know like you're averaged over the last five minutes is going to be",
    "start": "1057740",
    "end": "1065200"
  },
  {
    "text": "not so different from this the spike and then so it doesn't detect the anomaly there so it's perfectly good for a",
    "start": "1065200",
    "end": "1071960"
  },
  {
    "text": "normal distribution but but anything else it's going to have issues identifying the anomaly",
    "start": "1071960",
    "end": "1078860"
  },
  {
    "text": "and the second yeah so the this is what the gaussian distribution I was talking about and um in if you want to overcome",
    "start": "1078860",
    "end": "1085220"
  },
  {
    "text": "this gaussian distribution issue then you have to get data from weeks and weeks worth so that you can say okay so",
    "start": "1085220",
    "end": "1091940"
  },
  {
    "text": "we can average it over and over and then we have to store the data over several weeks and then we have to understand",
    "start": "1091940",
    "end": "1098600"
  },
  {
    "text": "this a lot better and honestly this was just not working for us so we were looking for an anomaly",
    "start": "1098600",
    "end": "1104840"
  },
  {
    "text": "detection tool for cluster golden signals on kubernetes that that was",
    "start": "1104840",
    "end": "1109940"
  },
  {
    "text": "Reliant and reliable and at good technical fit and that's how we landed on Numa praj",
    "start": "1109940",
    "end": "1118640"
  },
  {
    "text": "so Numa paraj is a open source project that has been incubated at Intuit and it",
    "start": "1118640",
    "end": "1126320"
  },
  {
    "text": "is meant for real-time analytics and AI Ops on kubernetes and uh since they are",
    "start": "1126320",
    "end": "1132740"
  },
  {
    "text": "part of the larger platform team that we are we we are part of as well we work",
    "start": "1132740",
    "end": "1138740"
  },
  {
    "text": "closely with them to basically get the cluster good and signals anomaly detection for those",
    "start": "1138740",
    "end": "1144559"
  },
  {
    "text": "now Numa abroad has a few different projects but the main core ones are Numa flow",
    "start": "1144559",
    "end": "1151820"
  },
  {
    "text": "which is massively which handles massively High real-time jobs and Street",
    "start": "1151820",
    "end": "1157400"
  },
  {
    "text": "and it's a stream processing engine and Numa logic which is a collection of ml models and libraries that will help",
    "start": "1157400",
    "end": "1164240"
  },
  {
    "text": "with anomaly detection now let me walk you through what uh our",
    "start": "1164240",
    "end": "1169880"
  },
  {
    "text": "cluster install and AI Ops pipeline looks like so over here we can see the metrics that",
    "start": "1169880",
    "end": "1177919"
  },
  {
    "text": "we were interested in we were just looking at in the examples the node local DNS the cnia error counts and",
    "start": "1177919",
    "end": "1184340"
  },
  {
    "text": "whatnot and you can actually bucket all of those as part of your aggregation Rule and push it down to Prometheus in",
    "start": "1184340",
    "end": "1190039"
  },
  {
    "text": "fact Prometheus scrape interval is 30 seconds for us so it scrapes one data point every 30 seconds and then",
    "start": "1190039",
    "end": "1198919"
  },
  {
    "text": "what we have over here is an AA Ops namespace that we created in",
    "start": "1198919",
    "end": "1205160"
  },
  {
    "text": "every cluster and install the Numa flow controller in it now the controller itself installs a",
    "start": "1205160",
    "end": "1212360"
  },
  {
    "text": "few special crds for pipeline we'll talk about the pipeline in a bit for pipeline",
    "start": "1212360",
    "end": "1217640"
  },
  {
    "text": "vertices and there are buffers for each step of the pipeline to make sure that you can there's flow control between",
    "start": "1217640",
    "end": "1223160"
  },
  {
    "text": "them and uh we'll talk about the pipe Legacy pipeline how the pipeline looks so this",
    "start": "1223160",
    "end": "1230360"
  },
  {
    "text": "thing in purple is consists of all of the pipeline stages Prometheus ingests the metrics into our",
    "start": "1230360",
    "end": "1238820"
  },
  {
    "text": "pipeline the first stage of the pipeline input is a window step now each metric",
    "start": "1238820",
    "end": "1244940"
  },
  {
    "text": "comes into our pipeline as one data point however the ml models that are",
    "start": "1244940",
    "end": "1250039"
  },
  {
    "text": "acting to do the anomaly detection require a window a sliding window of data so they don't just act on one they",
    "start": "1250039",
    "end": "1257419"
  },
  {
    "text": "require like a data set so this step actually works in gaining",
    "start": "1257419",
    "end": "1262940"
  },
  {
    "text": "all of those data metrics and then window doing the windowing",
    "start": "1262940",
    "end": "1268000"
  },
  {
    "text": "then we have the preprocess step which actually makes the input metrics consumable by the ml models this part is",
    "start": "1268280",
    "end": "1276140"
  },
  {
    "text": "where any of the Transformations that are required are done and then there is the inference stage",
    "start": "1276140",
    "end": "1282380"
  },
  {
    "text": "which does the prediction for the anomaly detection threshold stage actually does the raw",
    "start": "1282380",
    "end": "1289700"
  },
  {
    "text": "anomaly score calculation based on a previous set of threshold ml models",
    "start": "1289700",
    "end": "1296620"
  },
  {
    "text": "and from threshold we go into post-processing which actually does the",
    "start": "1297020",
    "end": "1302780"
  },
  {
    "text": "normalization of the raw anomaly scores to a normalized range from 0 to 10 and",
    "start": "1302780",
    "end": "1309200"
  },
  {
    "text": "the general idea behind the 0 to 10 mapping is that from 0 to 3 your data",
    "start": "1309200",
    "end": "1315620"
  },
  {
    "text": "points are normally behaving nothing anomalous about it from three to seven",
    "start": "1315620",
    "end": "1321860"
  },
  {
    "text": "there is slightly anomalous behavior and then from 10 from 7 to 10 it's really",
    "start": "1321860",
    "end": "1327020"
  },
  {
    "text": "anomalous it's better to actually start alerting the right people in fact we",
    "start": "1327020",
    "end": "1332539"
  },
  {
    "text": "have a op systems in Intuit for services which automatically create an incident",
    "start": "1332539",
    "end": "1338780"
  },
  {
    "text": "if the anomaly scores over go over seven is if it's between 7 and 10.",
    "start": "1338780",
    "end": "1345200"
  },
  {
    "text": "and then this is also the stage where we push that anomaly score back to Prometheus so we have a Prometheus",
    "start": "1345200",
    "end": "1351620"
  },
  {
    "text": "Pusher that pushes the anomaly score back into Prometheus and then we have the training stage which is what trains",
    "start": "1351620",
    "end": "1358340"
  },
  {
    "text": "all of the ml models and then the the pre-process the",
    "start": "1358340",
    "end": "1363740"
  },
  {
    "text": "threshold and the actual neural network that's training the data all of that is stored in the ml flow uh storage that is",
    "start": "1363740",
    "end": "1371659"
  },
  {
    "text": "used back again for inference so that's overall the the AI Ops Pipeline and how",
    "start": "1371659",
    "end": "1379640"
  },
  {
    "text": "it it works now talking a bit more about Numa flow you",
    "start": "1379640",
    "end": "1386000"
  },
  {
    "text": "can find more details in the GitHub page and like I mentioned there are a bunch",
    "start": "1386000",
    "end": "1393080"
  },
  {
    "text": "of CRS that are installed in the cluster and a bunch of other like stateful",
    "start": "1393080",
    "end": "1398559"
  },
  {
    "text": "basically there's a readers deployment and then there is an interstate buffer service that is installed as part of the",
    "start": "1398559",
    "end": "1404179"
  },
  {
    "text": "aops namespace that helps with the different pipeline operations",
    "start": "1404179",
    "end": "1409460"
  },
  {
    "text": "and it's it's install installs in a few minutes like lesser than like three four minutes",
    "start": "1409460",
    "end": "1415580"
  },
  {
    "text": "and uh so far are the results have been pretty effective we've been able to get good results for",
    "start": "1415580",
    "end": "1422539"
  },
  {
    "text": "a class and we will see this in a demo where the we don't need to worry about static thresholds we can actually just",
    "start": "1422539",
    "end": "1429860"
  },
  {
    "text": "push the data that we need as far as metrics are concerned to our enuma proj namespace and the namespace actually",
    "start": "1429860",
    "end": "1436179"
  },
  {
    "text": "detects anomalies depending on the Clusters Baseline behaviors",
    "start": "1436179",
    "end": "1442820"
  },
  {
    "text": "and there is a UI that's available using a simple port forward in your cluster",
    "start": "1442820",
    "end": "1449419"
  },
  {
    "text": "now one one thing that you might be wondering is hey you know I'm a kubernetes engineer and uh I don't have",
    "start": "1449419",
    "end": "1457940"
  },
  {
    "text": "much of an experience do I need to understand ml so I when I started using this project I had the same set of",
    "start": "1457940",
    "end": "1464600"
  },
  {
    "text": "questions as well so I just put together a FAQ in case you're like me",
    "start": "1464600",
    "end": "1469640"
  },
  {
    "text": "so do you need ml experience to use this now but it might be just good to go and",
    "start": "1469640",
    "end": "1476179"
  },
  {
    "text": "look at the GitHub repo just to get an understanding look at our slack channels and like engage with us as far as",
    "start": "1476179",
    "end": "1482539"
  },
  {
    "text": "Community engagement goes and then what tell me a little bit about",
    "start": "1482539",
    "end": "1487700"
  },
  {
    "text": "the ml model how does it work so the model is actually an auto encoder",
    "start": "1487700",
    "end": "1493760"
  },
  {
    "text": "machine learning model and what it does is the way it does anomaly detection is",
    "start": "1493760",
    "end": "1499640"
  },
  {
    "text": "there is some sort of transformation of these data points and the when the",
    "start": "1499640",
    "end": "1505580"
  },
  {
    "text": "encoder and decoder cannot actually take this data set and be able to recognize it within within what is expected it",
    "start": "1505580",
    "end": "1512720"
  },
  {
    "text": "produces an anomaly and what is the purpose of retaining",
    "start": "1512720",
    "end": "1518840"
  },
  {
    "text": "models so basically what we do is we retain about five uh version for five",
    "start": "1518840",
    "end": "1524539"
  },
  {
    "text": "different ml models at any point in time and uh here is why",
    "start": "1524539",
    "end": "1529760"
  },
  {
    "text": "let's say you have an incident and that affects a few of your clusters and the incident is is not resolved let's say",
    "start": "1529760",
    "end": "1537740"
  },
  {
    "text": "for a few hours right now your ml models are getting trained on this bad data so",
    "start": "1537740",
    "end": "1544100"
  },
  {
    "text": "it's going to assume that this bad data is not anomalous anymore in fact it's going to think that that's normal",
    "start": "1544100",
    "end": "1549440"
  },
  {
    "text": "behavior right so you want to be able to throw away the ml model that's being",
    "start": "1549440",
    "end": "1555260"
  },
  {
    "text": "trained on bad data and you can use the previous model as a backup",
    "start": "1555260",
    "end": "1561158"
  },
  {
    "text": "and what is the model training frequency the a up systems at Intuit have about an",
    "start": "1562159",
    "end": "1568760"
  },
  {
    "text": "eight hour training frequency so every eight hours we we assume that there will",
    "start": "1568760",
    "end": "1574580"
  },
  {
    "text": "be some sort of data drift so your model has to get retrained on new data this is",
    "start": "1574580",
    "end": "1580039"
  },
  {
    "text": "just to make sure that we can we we don't assume that all data is going to be the same even for a specific cluster",
    "start": "1580039",
    "end": "1586039"
  },
  {
    "text": "over time and is there a UI to observe the ml flow",
    "start": "1586039",
    "end": "1592520"
  },
  {
    "text": "and the Machine learning side of things yeah so this is what it looks like you can hit it on like basically again it's",
    "start": "1592520",
    "end": "1599000"
  },
  {
    "text": "like a service it's uh you can do port forward on your local cluster and you",
    "start": "1599000",
    "end": "1604400"
  },
  {
    "text": "will get a nice UI with a lot of details about the ml model this is just one snapshot",
    "start": "1604400",
    "end": "1609919"
  },
  {
    "text": "that I took of the node local DNS that I was talking about and uh you can see",
    "start": "1609919",
    "end": "1615020"
  },
  {
    "text": "that there are five models that have been retained for this particular metric and uh only one of them is in production",
    "start": "1615020",
    "end": "1621740"
  },
  {
    "text": "the other ones are all archived and the way to retrigger a training is to basically change the stage from",
    "start": "1621740",
    "end": "1628279"
  },
  {
    "text": "production to archived and then it'll trigger a new ml model creation",
    "start": "1628279",
    "end": "1636679"
  },
  {
    "text": "so having said that let's take a look at a demo from venkata",
    "start": "1636679",
    "end": "1643840"
  },
  {
    "text": "my name is",
    "start": "1649100",
    "end": "1652240"
  },
  {
    "text": "looks like I have to switch screens",
    "start": "1659900",
    "end": "1665919"
  },
  {
    "text": "yeah just take away just one last night",
    "start": "1683620",
    "end": "1688179"
  },
  {
    "text": "okay um sorry about that little snaffle there",
    "start": "1766580",
    "end": "1774158"
  },
  {
    "text": "I didn't quit working kubernetes platform today I'm gonna quickly show you a demo on how we are leveraging",
    "start": "1807620",
    "end": "1814899"
  },
  {
    "text": "pneuma flow to detect DNS anomalies let me share my screen and quickly walk",
    "start": "1814899",
    "end": "1820820"
  },
  {
    "text": "through the setup",
    "start": "1820820",
    "end": "1823658"
  },
  {
    "text": "yeah okay taking a look at this screen right so we",
    "start": "1827059",
    "end": "1832460"
  },
  {
    "text": "have left side there is a cluster one on the right side there is cluster 2. and",
    "start": "1832460",
    "end": "1837679"
  },
  {
    "text": "what we are looking at here is the DNS metric on how much success rate we are",
    "start": "1837679",
    "end": "1842720"
  },
  {
    "text": "seeing in the cluster now this data is backed by amount of",
    "start": "1842720",
    "end": "1847820"
  },
  {
    "text": "requests that are we are getting into the cluster for DNS calls and amount of",
    "start": "1847820",
    "end": "1852919"
  },
  {
    "text": "server fails that we are seeing um in the cluster so server fails are",
    "start": "1852919",
    "end": "1859039"
  },
  {
    "text": "super critical in DNS and those happens when the one of the backends of the DNS infrastructure is not performing well or",
    "start": "1859039",
    "end": "1865820"
  },
  {
    "text": "being down now what we are seeing here is that one of the Clusters shows the",
    "start": "1865820",
    "end": "1871820"
  },
  {
    "text": "behavior where between 83 and 84 success rate but whereas other clusters shows",
    "start": "1871820",
    "end": "1877220"
  },
  {
    "text": "100 success rate now in order to generate an alert for this in a in a large Fleet of clusters",
    "start": "1877220",
    "end": "1884720"
  },
  {
    "text": "it gets really cumbersome really hard because you need so let's say this 84 percent is acceptable in a pre-prod",
    "start": "1884720",
    "end": "1890960"
  },
  {
    "text": "environment or like a test cluster where you're constantly doing experimentation",
    "start": "1890960",
    "end": "1896059"
  },
  {
    "text": "and this is okay in that environment now in order to generate as a platform team",
    "start": "1896059",
    "end": "1901520"
  },
  {
    "text": "to generate an alert that says hey right now the DNS is bad or we need to take an action right now right now",
    "start": "1901520",
    "end": "1909320"
  },
  {
    "text": "because these numbers are so drastically different whereas one is 84 percent another is 100 and then in production",
    "start": "1909320",
    "end": "1916340"
  },
  {
    "text": "environment let's say on the right side cluster here where you cannot even take five percent impact and that's really",
    "start": "1916340",
    "end": "1921799"
  },
  {
    "text": "big big deal right so now how do we come up with an alert that would satisfy both",
    "start": "1921799",
    "end": "1926840"
  },
  {
    "text": "right so now what you end up doing generally is that you General generally creating every every single cluster has",
    "start": "1926840",
    "end": "1933919"
  },
  {
    "text": "a separate alert and they have a manual thresholds or something like that now how is that a way to avoid that and",
    "start": "1933919",
    "end": "1940640"
  },
  {
    "text": "then say hey I will create a single alert that says that will give me an Insight on kind of a golden signal for",
    "start": "1940640",
    "end": "1946760"
  },
  {
    "text": "this right now what in this demo what I'm showing you is that the data here it",
    "start": "1946760",
    "end": "1953179"
  },
  {
    "text": "is generated from node local DNS is gonna be sent to pneuma flow",
    "start": "1953179",
    "end": "1958220"
  },
  {
    "text": "um pneuma flow will take this channel data and then generate an anomaly a",
    "start": "1958220",
    "end": "1963740"
  },
  {
    "text": "metric effect right and it is using a ml model behind the scene based on the data we prompt and then it is generating",
    "start": "1963740",
    "end": "1970100"
  },
  {
    "text": "anomaly score for us and which is the same here as well what we are trying to at least illustrate here is that if the",
    "start": "1970100",
    "end": "1975919"
  },
  {
    "text": "number is below 3 that is good and if we ever breach 3 that means there is",
    "start": "1975919",
    "end": "1981260"
  },
  {
    "text": "something wrong and somebody need to take an action and take a look at quickly right now I'm gonna introduce",
    "start": "1981260",
    "end": "1987860"
  },
  {
    "text": "um some failures into the right side cluster which is cluster 2 and see what",
    "start": "1987860",
    "end": "1993200"
  },
  {
    "text": "happens to the anomaly metric what we expect to see is that it needs go up",
    "start": "1993200",
    "end": "1998419"
  },
  {
    "text": "from whatever right now it is at one to more than three so that's your trigger",
    "start": "1998419",
    "end": "2004000"
  },
  {
    "text": "on alert that says hey there is something wrong it is no longer 100 and whereas if we introduce",
    "start": "2004000",
    "end": "2011440"
  },
  {
    "text": "um whereas this is not 100 already and this is okay meaning this cluster of",
    "start": "2011440",
    "end": "2016480"
  },
  {
    "text": "this behavior is okay right so you can always retrain these models and say hey let's say there is a bug and this is not",
    "start": "2016480",
    "end": "2023260"
  },
  {
    "text": "actually correct and we've addressed it and right now it's 100 now you can always retrain the pneumo flow models",
    "start": "2023260",
    "end": "2030640"
  },
  {
    "text": "and then get um get accurate alerts right so so let me quickly induce some failures here I'm",
    "start": "2030640",
    "end": "2039100"
  },
  {
    "text": "gonna just I just started the failure right now it will take about 30 seconds to reflect",
    "start": "2039100",
    "end": "2046720"
  },
  {
    "text": "um after some time you can clearly see that it's no longer at 100 cluster 2 has",
    "start": "2046720",
    "end": "2052358"
  },
  {
    "text": "now six percentage error rate most importantly you can also see that anomaly now went up to 10 whereas in",
    "start": "2052359",
    "end": "2060760"
  },
  {
    "text": "cluster one you always had about 17 failure and we never went beyond three",
    "start": "2060760",
    "end": "2067179"
  },
  {
    "text": "we already concluded that going Beyond three is bad and somebody need to take a",
    "start": "2067179",
    "end": "2072220"
  },
  {
    "text": "look and clearly cluster one in this case even though the failure is only six percent you would generate an alert this",
    "start": "2072220",
    "end": "2079240"
  },
  {
    "text": "kind of show you how you can leverage pneuma flow to generate alerts across large Fleet of clusters",
    "start": "2079240",
    "end": "2086020"
  },
  {
    "text": "this concludes the demo thank you very much",
    "start": "2086020",
    "end": "2090540"
  },
  {
    "text": "[Applause] so just to summarize what venkat was",
    "start": "2095620",
    "end": "2101020"
  },
  {
    "text": "saying is we had two clusters with different workloads the one on the right was more prone to errors because let's",
    "start": "2101020",
    "end": "2108880"
  },
  {
    "text": "say it's a pre-prod testing proof environment so the error rate was always around hovering around 13 to 17 percent",
    "start": "2108880",
    "end": "2116020"
  },
  {
    "text": "but the one on the left was a broad environment which had no errors and he",
    "start": "2116020",
    "end": "2122260"
  },
  {
    "text": "introduced the exact same number of Errors for the exact same amount of time",
    "start": "2122260",
    "end": "2127540"
  },
  {
    "text": "in both the Clusters and noted that the one on the right the there was it wasn't",
    "start": "2127540",
    "end": "2133480"
  },
  {
    "text": "showing any anomalous Behavior it was the anomaly score was between one and three but the one on the left it",
    "start": "2133480",
    "end": "2140320"
  },
  {
    "text": "immediately shot up to an anomaly score of 10 which shows that in a production environment we are not tolerant to it",
    "start": "2140320",
    "end": "2146440"
  },
  {
    "text": "and it was not a baseline Behavior so you can actually don't need to worry about static thresholds and you can",
    "start": "2146440",
    "end": "2153820"
  },
  {
    "text": "actually Implement golden signals using pneumaproj well that concludes the main takeaway is",
    "start": "2153820",
    "end": "2160359"
  },
  {
    "text": "go check out numapraj it's a pretty cool project engaged with the community and",
    "start": "2160359",
    "end": "2165520"
  },
  {
    "text": "uh start implementing cluster golden signals if your platform Engineers are getting overwhelmed and you're getting",
    "start": "2165520",
    "end": "2171940"
  },
  {
    "text": "alert fatigue and burdened by uh on call thank you very much",
    "start": "2171940",
    "end": "2178440"
  }
]