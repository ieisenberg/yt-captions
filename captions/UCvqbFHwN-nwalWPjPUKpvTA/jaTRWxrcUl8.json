[
  {
    "start": "0",
    "end": "42000"
  },
  {
    "text": "hello everyone my name is eduardo silva and welcome to this session called high",
    "start": "240",
    "end": "5440"
  },
  {
    "text": "throughput plus love resource usage and login journey as you can see my email is it's on the",
    "start": "5440",
    "end": "12160"
  },
  {
    "text": "first slide so feel free to reach out anytime for questions or any kind of follow-up that you wanted to do",
    "start": "12160",
    "end": "18640"
  },
  {
    "text": "as i say my name is eduardo i'm mostly on twitter and github and the main ether and as the founder of kalitia",
    "start": "18640",
    "end": "25680"
  },
  {
    "text": "which is a company which provides full support and products on top of the fluency and fluid bit",
    "start": "25680",
    "end": "31439"
  },
  {
    "text": "ecosystem feel free to reach out through the website too and also the creator and maintainer of",
    "start": "31439",
    "end": "37360"
  },
  {
    "text": "fluid project which is part of the cncf fluency project so in general everybody wants",
    "start": "37360",
    "end": "45039"
  },
  {
    "start": "42000",
    "end": "97000"
  },
  {
    "text": "performance right if you want to see high throughput then you think about performance but",
    "start": "45039",
    "end": "51680"
  },
  {
    "text": "everything has a little cost right so and when people say performance is",
    "start": "51680",
    "end": "56879"
  },
  {
    "text": "sometimes think about what's my the number of records or events that i can process by a unit of",
    "start": "56879",
    "end": "63840"
  },
  {
    "text": "time right and after that you start realizing that maybe the setup that you have",
    "start": "63840",
    "end": "68880"
  },
  {
    "text": "or the strategy that your tool is using might not be conformant or maybe it's not following the best practices from a",
    "start": "68880",
    "end": "75200"
  },
  {
    "text": "configuration perspective if you think about hey i want to consume",
    "start": "75200",
    "end": "80320"
  },
  {
    "text": "low cpu or i don't want to exceed this amount of memory and that is it's really hard to come up",
    "start": "80320",
    "end": "86560"
  },
  {
    "text": "with a with the ideal solution but i can talk about how do we solve",
    "start": "86560",
    "end": "92159"
  },
  {
    "text": "this problem in in fluent bed and where are we going with this",
    "start": "92159",
    "end": "97520"
  },
  {
    "start": "97000",
    "end": "132000"
  },
  {
    "text": "so our journey started a years ago we had this project called a fluentd",
    "start": "97520",
    "end": "104960"
  },
  {
    "text": "and fluency is really good for service it's really good to aggregate information we have a huge ecosystem right where",
    "start": "104960",
    "end": "112079"
  },
  {
    "text": "with people and companies has contributed more than a thousand of plugins and it's really great",
    "start": "112079",
    "end": "117759"
  },
  {
    "text": "so i'm saying this because i'm maybe i would say that eighty percent of you may be familiar with fluently which is a",
    "start": "117759",
    "end": "123200"
  },
  {
    "text": "graduated cncf project but also here we're going to explain and talk about the journey",
    "start": "123200",
    "end": "128879"
  },
  {
    "text": "of the sub project of it which is part of the ecosystem and when talking about login um",
    "start": "128879",
    "end": "135920"
  },
  {
    "start": "132000",
    "end": "240000"
  },
  {
    "text": "there's a couple of things that we need to clarify right logging by itself is not cheap and we",
    "start": "135920",
    "end": "142239"
  },
  {
    "text": "need to understand that logging in the past used to be pretty simple right an application just ship a message this",
    "start": "142239",
    "end": "149520"
  },
  {
    "text": "message message got stored in the file system or maybe it uses ziploc or syslog or any or systemd or any kind",
    "start": "149520",
    "end": "156640"
  },
  {
    "text": "of service to handle the message for it right and most of these messages are",
    "start": "156640",
    "end": "162319"
  },
  {
    "text": "text-based but if you think about what is the end of this message is that it allows you to perform some",
    "start": "162319",
    "end": "169200"
  },
  {
    "text": "data analysis and to do that you need to take all these messages and centralize them",
    "start": "169200",
    "end": "174720"
  },
  {
    "text": "in a database so then you can create some kind of schema or run some queries on top of it it",
    "start": "174720",
    "end": "180319"
  },
  {
    "text": "doesn't matter if you store in a sql or non-sql database or document oriented",
    "start": "180319",
    "end": "185440"
  },
  {
    "text": "actually the pattern is pretty much the same you want to have some structure which is on top of the text message that",
    "start": "185440",
    "end": "191680"
  },
  {
    "text": "was generated at the beginning and one of the challenges if you think about throw booth performance",
    "start": "191680",
    "end": "197840"
  },
  {
    "text": "is that every time we have more data because we have more applications right everybody is now deploying",
    "start": "197840",
    "end": "203200"
  },
  {
    "text": "microservices decoupling everything and it's really hard to keep control of",
    "start": "203200",
    "end": "208319"
  },
  {
    "text": "what's the logging rate of if application or what kind of messages this is spending right",
    "start": "208319",
    "end": "214400"
  },
  {
    "text": "sometimes you don't just care about specific messages like info warning errors but maybe you don't",
    "start": "214400",
    "end": "220319"
  },
  {
    "text": "want to debug messages maybe you can raise your hand if you have been managed some system",
    "start": "220319",
    "end": "226480"
  },
  {
    "text": "where a developer by music just enable the debug mode debug mode for one application and you",
    "start": "226480",
    "end": "232720"
  },
  {
    "text": "start to see all this increase of load of lo of log messages in your pipeline",
    "start": "232720",
    "end": "238400"
  },
  {
    "text": "which is quite normal but that affects performance right because that is our goal and as i said later so if we think about",
    "start": "238400",
    "end": "245439"
  },
  {
    "start": "240000",
    "end": "255000"
  },
  {
    "text": "the workflow or how this work together is that we have this application",
    "start": "245439",
    "end": "250959"
  },
  {
    "text": "that generates generate a simple log message and sometimes multiple of them as an",
    "start": "250959",
    "end": "257280"
  },
  {
    "start": "255000",
    "end": "355000"
  },
  {
    "text": "example let's take this simple message that it comes from a nginx nginx log access",
    "start": "257280",
    "end": "263680"
  },
  {
    "text": "law actually what it's doing is just writing a message with specified the id address",
    "start": "263680",
    "end": "269040"
  },
  {
    "text": "a timestamp on when this a request was generated plus the information from the protocol",
    "start": "269040",
    "end": "275600"
  },
  {
    "text": "the method the uri the protocol version the response site from the server",
    "start": "275600",
    "end": "280960"
  },
  {
    "text": "blah blah blah but this log message also is not unique right it could be different",
    "start": "280960",
    "end": "286880"
  },
  {
    "text": "sometimes applications generate messages in different formats even coming from the same application",
    "start": "286880",
    "end": "292960"
  },
  {
    "text": "sometimes you have multi-line application or think about stack traces right so you can think that",
    "start": "292960",
    "end": "298720"
  },
  {
    "text": "this thing is a bit complex and when you have this application generating this message",
    "start": "298720",
    "end": "304160"
  },
  {
    "text": "i would say that eighty percent of the time is just a rough text message it doesn't",
    "start": "304160",
    "end": "310320"
  },
  {
    "text": "have any structure it doesn't have any schema nowadays we are seeing people and",
    "start": "310320",
    "end": "315600"
  },
  {
    "text": "companies trying to accomplish let's try to log with json or trying to unify",
    "start": "315600",
    "end": "320880"
  },
  {
    "text": "a specific format but in reality that that effort has been around for years",
    "start": "320880",
    "end": "326960"
  },
  {
    "text": "and it is really hard to say that the industry is going to align to one specific format",
    "start": "326960",
    "end": "334080"
  },
  {
    "text": "it's really hard it doesn't happen in the last 20 years right but i would say that nowadays because of",
    "start": "334080",
    "end": "340400"
  },
  {
    "text": "parts in um capabilities maybe json is like the middle point between",
    "start": "340400",
    "end": "347440"
  },
  {
    "text": "all the options available i'm not saying that the best is really slow",
    "start": "347440",
    "end": "352639"
  },
  {
    "text": "but at least it's something that we can start with right and when we generate this message",
    "start": "352639",
    "end": "358400"
  },
  {
    "start": "355000",
    "end": "412000"
  },
  {
    "text": "this message gets list being handled by syslog rc slope or systemd",
    "start": "358400",
    "end": "363840"
  },
  {
    "text": "any kind of services running in the in the system and now we can have many of them but",
    "start": "363840",
    "end": "370479"
  },
  {
    "text": "also we think how this works because our goal is to perform data analysis is that we have all these files with all",
    "start": "370479",
    "end": "377199"
  },
  {
    "text": "these records maybe from different applications maybe from the same one so we need to have this kind of engine",
    "start": "377199",
    "end": "383199"
  },
  {
    "text": "that is able to listen for these messages or realize that they are rare and then",
    "start": "383199",
    "end": "388240"
  },
  {
    "text": "be able to take them and send it out to any destination like amazon 3",
    "start": "388240",
    "end": "394160"
  },
  {
    "text": "elastic stackdriver splunk or any kind of service right nowadays we have multiple options",
    "start": "394160",
    "end": "400479"
  },
  {
    "text": "in the market and this is just an example one of them there's no preference from",
    "start": "400479",
    "end": "405919"
  },
  {
    "text": "from my perspective actually every user might choose their own vendor or vendors for each one",
    "start": "405919",
    "end": "412960"
  },
  {
    "text": "and in the engine side there's a couple of steps it's not just take the data in and send",
    "start": "412960",
    "end": "418400"
  },
  {
    "text": "the data out if you think about throughput right that is quite easy and that is why quite fast",
    "start": "418400",
    "end": "424319"
  },
  {
    "text": "right there's no so much muscle work on that take the drains and the data out",
    "start": "424319",
    "end": "430639"
  },
  {
    "text": "but actually a real load processing load processor needs to come build with different kind of things like for",
    "start": "430639",
    "end": "436800"
  },
  {
    "text": "example collect blocks that come from different sources not just a file maybe from systemd from tcp udp",
    "start": "436800",
    "end": "445039"
  },
  {
    "text": "take this data apply or optionally try to apply some parsing so to convert it on",
    "start": "445039",
    "end": "450240"
  },
  {
    "text": "a circular method to a structure maybe to json or maybe some binary format that we can",
    "start": "450240",
    "end": "456319"
  },
  {
    "text": "deal with internally and sometimes most of these messages also need to have some",
    "start": "456319",
    "end": "461360"
  },
  {
    "text": "metadata on it if you are receiving some messages over tcp likely you would like to write the ip",
    "start": "461360",
    "end": "468400"
  },
  {
    "text": "address from the where this message is coming from now if you are wearing kubecon right",
    "start": "468400",
    "end": "474400"
  },
  {
    "text": "so if you think about messages coming from your port you would like to have your labels inside the same record so then you can",
    "start": "474400",
    "end": "481520"
  },
  {
    "text": "group them and perform your query without any problems without losing context but also there are other cases where you",
    "start": "481520",
    "end": "488319"
  },
  {
    "text": "want to perform some data reduction right if you're trapping a lot of messages",
    "start": "488319",
    "end": "493840"
  },
  {
    "text": "and these messages have like a as i said the early information developed messages they",
    "start": "493840",
    "end": "500639"
  },
  {
    "text": "maybe don't want them right so you want to make sure that every everything that comes as a debug",
    "start": "500639",
    "end": "506319"
  },
  {
    "text": "messages in your pipeline just drop it right if you think about a storage that charge you the build right",
    "start": "506319",
    "end": "514399"
  },
  {
    "text": "because of the amount of data that you ingest right you're maybe interested in to perform data reduction",
    "start": "514399",
    "end": "520159"
  },
  {
    "text": "and not send the whole information also as part of the role of the log",
    "start": "520159",
    "end": "525360"
  },
  {
    "text": "processor is to perform buffering and buffering is the capability to take this data and optionally maybe",
    "start": "525360",
    "end": "531600"
  },
  {
    "text": "store it perceived in a persistent way on this because you need to restart the service or maybe the subject just",
    "start": "531600",
    "end": "537360"
  },
  {
    "text": "crashed or you have to get a power failure or nettle audit so you want to make sure that you don't",
    "start": "537360",
    "end": "542800"
  },
  {
    "text": "lose any data and finally be able to send this information to",
    "start": "542800",
    "end": "548720"
  },
  {
    "text": "a different destination and these destinations are important and",
    "start": "548720",
    "end": "554320"
  },
  {
    "text": "some of them are just to handle the information as a binary globs right because i just want",
    "start": "554320",
    "end": "559920"
  },
  {
    "text": "to store the information that i'm getting but for example in the case of a slash you just care about how",
    "start": "559920",
    "end": "565360"
  },
  {
    "text": "can i query my documents ideally i want to have all my documents in elastic with a good structure in the json map",
    "start": "565360",
    "end": "572560"
  },
  {
    "text": "so i can query or have kind of index through them right same thing happen with stackdriver",
    "start": "572560",
    "end": "578640"
  },
  {
    "text": "and same thing happens with a spline class others",
    "start": "578640",
    "end": "584640"
  },
  {
    "text": "in general you can see that we have an input with different sources we have this",
    "start": "584640",
    "end": "590320"
  },
  {
    "text": "engine in the middle and we have the output so explaining these concepts are is",
    "start": "590320",
    "end": "595360"
  },
  {
    "text": "really important to understand performance right because we need to realize what how this works behind the scenes",
    "start": "595360",
    "end": "602399"
  },
  {
    "text": "right so in general simplifying everything is an input and an output",
    "start": "602399",
    "end": "608160"
  },
  {
    "start": "603000",
    "end": "630000"
  },
  {
    "text": "but we need to explain what is happening inside the input for example inside the input you have a",
    "start": "608160",
    "end": "613600"
  },
  {
    "text": "lot of io io means maybe you are opening a file from from the disk right or maybe you are",
    "start": "613600",
    "end": "620480"
  },
  {
    "text": "listening from a new network connection or maybe you are collecting metrics locally from the prog file system or",
    "start": "620480",
    "end": "627519"
  },
  {
    "text": "just receiving them from a third-party service in the engine has a",
    "start": "627519",
    "end": "632560"
  },
  {
    "start": "630000",
    "end": "676000"
  },
  {
    "text": "lot of work as i've explained it earlier it's like you have to parse information you have to offer options to filter this data",
    "start": "632560",
    "end": "639600"
  },
  {
    "text": "like enrich it or discard data also to serialize this information because",
    "start": "639600",
    "end": "644640"
  },
  {
    "text": "if you get the data as raw data or binary data and you're going to do all these parts in this field you need to have a unified",
    "start": "644640",
    "end": "652000"
  },
  {
    "text": "model for your data right and having a binary format is ideal",
    "start": "652000",
    "end": "657040"
  },
  {
    "text": "to do any kind of data processing this buffering to store the data so you don't lose it",
    "start": "657040",
    "end": "662720"
  },
  {
    "text": "and be able to have a routine logic to decide how to send my information that is",
    "start": "662720",
    "end": "668959"
  },
  {
    "text": "coming from a specific source to a different definition based in a pattern how to schedule",
    "start": "668959",
    "end": "674320"
  },
  {
    "text": "requires and so on and in the output side means that how do i send this deformation but i",
    "start": "674320",
    "end": "681920"
  },
  {
    "start": "676000",
    "end": "743000"
  },
  {
    "text": "just got in my engine to a third-party service it's like i need to deal with network setup",
    "start": "681920",
    "end": "687680"
  },
  {
    "text": "with payload formatting because every output destination expects a different payload format most",
    "start": "687680",
    "end": "693760"
  },
  {
    "text": "of them are working with json nowadays but for example difference is kaka right",
    "start": "693760",
    "end": "698959"
  },
  {
    "text": "kite has in its own format for the network but all of the others elastic and",
    "start": "698959",
    "end": "705760"
  },
  {
    "text": "azure log analytics and even splunk use json right and the output site",
    "start": "705760",
    "end": "712160"
  },
  {
    "text": "needs to deal with delivering content in different formats i'll also be able to understand",
    "start": "712160",
    "end": "718959"
  },
  {
    "text": "if i'm sending this information i need to expect for a specific return code because",
    "start": "718959",
    "end": "724079"
  },
  {
    "text": "if something failure and the server maybe says you know what i could not use process with information we need to reply",
    "start": "724079",
    "end": "731440"
  },
  {
    "text": "the engine needs to be able to retry through a scheduler well something that started as an input",
    "start": "731440",
    "end": "738880"
  },
  {
    "text": "output has a lot of tasks that works internally and now i'm going to introduce fluid i",
    "start": "738880",
    "end": "746480"
  },
  {
    "text": "know that just talking about it took it about fluently and i'm sure that you are really familiar with this actually",
    "start": "746480",
    "end": "753519"
  },
  {
    "text": "fluently and blue and bit are from the same family from the cncf and fluent bait is a cncf sub project",
    "start": "753519",
    "end": "760720"
  },
  {
    "text": "under the umbrella of a fluentd and the the good thing about",
    "start": "760720",
    "end": "766079"
  },
  {
    "text": "fluid is that it was always designed to have higher performance with fluency",
    "start": "766079",
    "end": "771680"
  },
  {
    "text": "this but also included in ruby and being written in ruby has some downsides from a scripting language",
    "start": "771680",
    "end": "778720"
  },
  {
    "text": "right it's really easy to extend it's really easy to skill but if you want to optimize some",
    "start": "778720",
    "end": "783760"
  },
  {
    "text": "resources it's really hard and i would say that it's not just ruby but any kind of language",
    "start": "783760",
    "end": "790399"
  },
  {
    "text": "that start implementing a system level application that start coupling a bunch of",
    "start": "790399",
    "end": "796000"
  },
  {
    "text": "dependencies right actually you start focusing mostly on usability or how to extend it",
    "start": "796000",
    "end": "802240"
  },
  {
    "text": "that optimizing every single component for performance a fluid is written in c language and we",
    "start": "802240",
    "end": "809040"
  },
  {
    "text": "try to reduce as much as possible the dependencies and try to build our own architecture",
    "start": "809040",
    "end": "814720"
  },
  {
    "text": "and deal with most of the staff on our own and i think that from a",
    "start": "814720",
    "end": "819760"
  },
  {
    "text": "community perspective we have get a really good feedback and really good traction from this",
    "start": "819760",
    "end": "825680"
  },
  {
    "text": "nowadays the users use fluency majority of them while others are using fluent bed and we",
    "start": "825680",
    "end": "831680"
  },
  {
    "text": "have a third option which both are using but they are using both there is a fluent bed and they are using",
    "start": "831680",
    "end": "837040"
  },
  {
    "text": "fluency together so let's talk more about the pipeline internals we",
    "start": "837040",
    "end": "844639"
  },
  {
    "start": "840000",
    "end": "858000"
  },
  {
    "text": "call a pipeline so everything that where it flows data",
    "start": "844639",
    "end": "849680"
  },
  {
    "text": "right we're talking about logging we're talking about technology right we're doing that kind of pipes so understanding how the data flows",
    "start": "849680",
    "end": "856320"
  },
  {
    "text": "is really interesting now if we talk about system code if we analyze in general all the input",
    "start": "856320",
    "end": "863120"
  },
  {
    "start": "858000",
    "end": "893000"
  },
  {
    "text": "interfaces to collect data from iodized metrics and so on you ended up using system",
    "start": "863120",
    "end": "868639"
  },
  {
    "text": "codes like open read socket list and bind and both functions to perform a memory",
    "start": "868639",
    "end": "874079"
  },
  {
    "text": "allocation and memory management in general this is pretty fine",
    "start": "874079",
    "end": "879279"
  },
  {
    "text": "this is not a big problem i would say that the most critical part is how do",
    "start": "879279",
    "end": "884320"
  },
  {
    "text": "you manage the memory while consuming data while processing the data and while sending this",
    "start": "884320",
    "end": "889839"
  },
  {
    "text": "information out a part of the",
    "start": "889839",
    "end": "895680"
  },
  {
    "start": "893000",
    "end": "977000"
  },
  {
    "text": "engine we can call it like a processor here we're split splitting the exclamation in two parts",
    "start": "895680",
    "end": "902320"
  },
  {
    "text": "actually the engine needs to be able to do parsing right because we mentioned that we want to have this constructor",
    "start": "902320",
    "end": "908000"
  },
  {
    "text": "information to a structure format right so we take it and could take a json",
    "start": "908000",
    "end": "913440"
  },
  {
    "text": "parser json or apply a regular expression on top of this side data because my data is quite custom but i",
    "start": "913440",
    "end": "920560"
  },
  {
    "text": "know how to group them right as the nginx example we know that the first field is",
    "start": "920560",
    "end": "925760"
  },
  {
    "text": "an ip address the second one is the timestamp and so on all lock formats for kind of",
    "start": "925760",
    "end": "931519"
  },
  {
    "text": "goal and messages csv etc also as part of filtering you might want",
    "start": "931519",
    "end": "937839"
  },
  {
    "text": "to do some kind of data enrichment in kubernetes of course you want to obtain all your labels or sometimes your",
    "start": "937839",
    "end": "943600"
  },
  {
    "text": "annotations through every single log record but also you want to do some kind of the",
    "start": "943600",
    "end": "948639"
  },
  {
    "text": "exclusion based on some patterns and internally i would say in our case",
    "start": "948639",
    "end": "955839"
  },
  {
    "text": "we serialize all our information every single event doesn't matter if it's a metric or a log",
    "start": "955839",
    "end": "961120"
  },
  {
    "text": "record we use message pack which is a kind of a similar to a json binary but quite",
    "start": "961120",
    "end": "967759"
  },
  {
    "text": "more performant and for baffling mechanism we offer memory and this which we are",
    "start": "967759",
    "end": "973600"
  },
  {
    "text": "going to explain in a few minutes also as part of this processing and work",
    "start": "973600",
    "end": "981519"
  },
  {
    "start": "977000",
    "end": "1052000"
  },
  {
    "text": "we need to be able to route this data right we have a logic to roll this data",
    "start": "981519",
    "end": "986959"
  },
  {
    "text": "meaning like for every data that comes from a different from a specific source you want to send it one example elastic",
    "start": "986959",
    "end": "994560"
  },
  {
    "text": "search but also you want to archive all the information to amazon f3",
    "start": "994560",
    "end": "999839"
  },
  {
    "text": "yeah this is quite common example and we have found some cases also that a enterprise companies are using splunk",
    "start": "999839",
    "end": "1008320"
  },
  {
    "text": "heavily and they are deciding to just send to splunk not 100 of the data but maybe 50",
    "start": "1008320",
    "end": "1015519"
  },
  {
    "text": "right which is more critical information that you are going to run queries over it and all of the other two archive systems",
    "start": "1015519",
    "end": "1023279"
  },
  {
    "text": "right and also as part of this processing agent we have all the scheduling",
    "start": "1023279",
    "end": "1028798"
  },
  {
    "text": "we have our own schedule because if you are collecting data you need to have a lot of timers",
    "start": "1028799",
    "end": "1034319"
  },
  {
    "text": "to say when to collect data from which place and which kind of collector but also have",
    "start": "1034319",
    "end": "1040079"
  },
  {
    "text": "this logic that if some oppo destination failed and asked if to retry something",
    "start": "1040079",
    "end": "1046000"
  },
  {
    "text": "be able to handle that task and implement a reply in place",
    "start": "1046000",
    "end": "1053679"
  },
  {
    "start": "1052000",
    "end": "1138000"
  },
  {
    "text": "as part of the output side of this pipeline we mentioned that everything is about",
    "start": "1053679",
    "end": "1059440"
  },
  {
    "text": "also network setup so we need to perform dns lookup connect to an endpoint",
    "start": "1059440",
    "end": "1064799"
  },
  {
    "text": "and you want to implement of course nowadays um tls so actually as part of that process you",
    "start": "1064799",
    "end": "1070160"
  },
  {
    "text": "want to handle the tls hunter and ideally you want to maintain this a",
    "start": "1070160",
    "end": "1075440"
  },
  {
    "text": "connection like with keep alive right if you are closing a connection and open a new",
    "start": "1075440",
    "end": "1081120"
  },
  {
    "text": "connection every single time that will be really expensive because the tls handshake process is really expensive",
    "start": "1081120",
    "end": "1087280"
  },
  {
    "text": "because of the round trips we mentioned that also internally we",
    "start": "1087280",
    "end": "1092320"
  },
  {
    "text": "have all this data in binary format right and this binary format for us is message back",
    "start": "1092320",
    "end": "1098160"
  },
  {
    "text": "but when you are talking to elasticsearch elasticsearch does not understand message back it",
    "start": "1098160",
    "end": "1103840"
  },
  {
    "text": "understands json maps or well any kind of json object so what you have to do",
    "start": "1103840",
    "end": "1109679"
  },
  {
    "text": "or what we do in our elastic search connector is take this message back binary message",
    "start": "1109679",
    "end": "1115840"
  },
  {
    "text": "and then send convert it back to json and then send it over the network right so we",
    "start": "1115840",
    "end": "1122240"
  },
  {
    "text": "need to do the payload delivery write all the data make sure that all data was",
    "start": "1122240",
    "end": "1127679"
  },
  {
    "text": "delivered check for return status and report to the engine what was the final a status of about all",
    "start": "1127679",
    "end": "1135200"
  },
  {
    "text": "these processing so now about optimizing all this",
    "start": "1135200",
    "end": "1143200"
  },
  {
    "text": "data process and io in general it's really interesting to understand how things works you know",
    "start": "1143200",
    "end": "1151360"
  },
  {
    "text": "at a deep level so as i said for to manage messages internally we use",
    "start": "1151360",
    "end": "1157440"
  },
  {
    "text": "a binary representation of the data using messenger pack and also",
    "start": "1157440",
    "end": "1162799"
  },
  {
    "text": "when dealing with how to handle this information one is sterilized internally at runtime we have a really",
    "start": "1162799",
    "end": "1170320"
  },
  {
    "text": "good a implementation with buffer management with an either mechanism with memory and a file system",
    "start": "1170320",
    "end": "1177280"
  },
  {
    "text": "we group all the records in chance and this is something i'm going to explain in the next slide",
    "start": "1177280",
    "end": "1184559"
  },
  {
    "start": "1184000",
    "end": "1272000"
  },
  {
    "text": "about data serialization and message back here in the screen we have a table which",
    "start": "1185120",
    "end": "1190840"
  },
  {
    "text": "compares what how many bytes compares using json",
    "start": "1190840",
    "end": "1196080"
  },
  {
    "text": "versus message back right for example for a new message to know in json is means four",
    "start": "1196080",
    "end": "1202320"
  },
  {
    "text": "bytes in message pack is just one if we go to a more critical example with a map which is",
    "start": "1202320",
    "end": "1208159"
  },
  {
    "text": "in the last line right with a key called 40 and when a value knew actually it used 11 bytes in message",
    "start": "1208159",
    "end": "1216080"
  },
  {
    "text": "pack that is 5 bytes this is just a comparison of how one",
    "start": "1216080",
    "end": "1221679"
  },
  {
    "text": "format works against the other right of course message pack",
    "start": "1221679",
    "end": "1226799"
  },
  {
    "text": "is quite small than a json payload right but i would say that this is one of the advantages but",
    "start": "1226799",
    "end": "1233039"
  },
  {
    "text": "the other advantage is that in message pack you don't need to parse every single",
    "start": "1233039",
    "end": "1238240"
  },
  {
    "text": "byte per byte right in json you have to do it you have to create some kind of index across all your map and then you",
    "start": "1238240",
    "end": "1245919"
  },
  {
    "text": "know you can know where the data start and where the data begins and ends in message pack you can know",
    "start": "1245919",
    "end": "1252000"
  },
  {
    "text": "that you have an array beforehand with x number of items so you can jump between them",
    "start": "1252000",
    "end": "1257679"
  },
  {
    "text": "so when you are dealing with data processing where you are removing some message sorry so key or adding more key or doing",
    "start": "1257679",
    "end": "1265200"
  },
  {
    "text": "some kind of any kind of modification is quite more performant and easy to do it",
    "start": "1265200",
    "end": "1272000"
  },
  {
    "start": "1272000",
    "end": "1327000"
  },
  {
    "text": "internally when we get this data we talk about data serialization now imagine that we got a text message",
    "start": "1272640",
    "end": "1280000"
  },
  {
    "text": "as a json map what we do is convert this json to message back",
    "start": "1280000",
    "end": "1285919"
  },
  {
    "text": "and then we have this notion of stack attack is just like a label so for every",
    "start": "1285919",
    "end": "1292159"
  },
  {
    "text": "information that comes from a specific source that has the same time this the same tag",
    "start": "1292159",
    "end": "1298240"
  },
  {
    "text": "they are all grouped together under the same chunk right and a challenge is just",
    "start": "1298240",
    "end": "1304640"
  },
  {
    "text": "an amount of bytes that stored many records right usually we handle chunks around",
    "start": "1304640",
    "end": "1310720"
  },
  {
    "text": "two megabytes which is a has been really efficient for most of the use cases",
    "start": "1310720",
    "end": "1318000"
  },
  {
    "text": "and so consider the chunk which is a more critical unit of data granular data that we have in the fluid",
    "start": "1318000",
    "end": "1325280"
  },
  {
    "text": "pipeline now how this operates for example in memory",
    "start": "1325280",
    "end": "1331600"
  },
  {
    "start": "1327000",
    "end": "1362000"
  },
  {
    "text": "when the engine starts getting data what we do is to create just one chunky memory right and what we",
    "start": "1331600",
    "end": "1339360"
  },
  {
    "text": "do is start appending the events right on that tank until that chunk gets",
    "start": "1339360",
    "end": "1344960"
  },
  {
    "text": "to two megabytes and that we simply create if we get more deformation",
    "start": "1344960",
    "end": "1350320"
  },
  {
    "text": "we just get a new chart and we do the same procedure it doesn't",
    "start": "1350320",
    "end": "1355679"
  },
  {
    "text": "matter if we have the same tag or not we we just create blocks of times a",
    "start": "1355679",
    "end": "1361039"
  },
  {
    "text": "in memory until one point we have a bunch of chunks linked in memory",
    "start": "1361039",
    "end": "1367200"
  },
  {
    "start": "1362000",
    "end": "1425000"
  },
  {
    "text": "which is pretty efficient is pretty fast actually memory buffering is the fastest",
    "start": "1367200",
    "end": "1372840"
  },
  {
    "text": "implementation but there's one downside right it's not persistent it's not persistent so what would happen",
    "start": "1372840",
    "end": "1380080"
  },
  {
    "text": "if our service is restarted what will happen is start with crashes yeah you're going to lose that data and",
    "start": "1380080",
    "end": "1386159"
  },
  {
    "text": "also there's one more critical path if you're running in containers likely your container will have a limit",
    "start": "1386159",
    "end": "1392080"
  },
  {
    "text": "about how much memory that process in this case fluid can consume and if you go on over that",
    "start": "1392080",
    "end": "1399200"
  },
  {
    "text": "right well you will know you reach the limit the kernel is going to kill your",
    "start": "1399200",
    "end": "1404320"
  },
  {
    "text": "your pot or your container so there's a kind of a special",
    "start": "1404320",
    "end": "1410400"
  },
  {
    "text": "situations where you just want to run in memory right but i would say ninety percent of the time you want to",
    "start": "1410400",
    "end": "1417679"
  },
  {
    "text": "use a secondary a option which so a second as a",
    "start": "1417679",
    "end": "1422799"
  },
  {
    "text": "secondary option with which is a file system in the opposite is what it does in the file system if",
    "start": "1422799",
    "end": "1429919"
  },
  {
    "text": "this even is uh not storing the things in memory means that we're going to start",
    "start": "1429919",
    "end": "1435039"
  },
  {
    "text": "start throwing the things in the file system right so but this is of course is most",
    "start": "1435039",
    "end": "1441360"
  },
  {
    "text": "intensive on io but here we have a couple of optimizations on how do we find the chunks on how do",
    "start": "1441360",
    "end": "1447919"
  },
  {
    "text": "we store them right so every time that we get more information we are getting a new chunk",
    "start": "1447919",
    "end": "1453600"
  },
  {
    "text": "and just appending this information to the and to what point that we have many",
    "start": "1453600",
    "end": "1459840"
  },
  {
    "text": "chunks in the file system but there's a curious thing here here",
    "start": "1459840",
    "end": "1465279"
  },
  {
    "text": "this is a couple of a simple example where we have five files right five chunks but if you think about that we have 2000",
    "start": "1465279",
    "end": "1472400"
  },
  {
    "text": "30 000 because maybe you were not able to send the data because the destination is down",
    "start": "1472400",
    "end": "1477760"
  },
  {
    "text": "right you will say hey but i'm going to have all these channels open i'm going to use a single file",
    "start": "1477760",
    "end": "1484240"
  },
  {
    "text": "descriptor for each one and i would say that now we are going to talk most about",
    "start": "1484240",
    "end": "1489520"
  },
  {
    "text": "a databases concept right we don't use any external database",
    "start": "1489520",
    "end": "1495200"
  },
  {
    "text": "right we implemented our own data to for this specific use case actually we think",
    "start": "1495200",
    "end": "1500559"
  },
  {
    "text": "that databases are really good and butter proof but they are for generic purposes right",
    "start": "1500559",
    "end": "1508000"
  },
  {
    "text": "here we are dealing with this kind of a log information that we need to group them in a different way and we need to deal",
    "start": "1508000",
    "end": "1515120"
  },
  {
    "text": "with file system memory but based on our pipeline logic and not a generic",
    "start": "1515120",
    "end": "1522480"
  },
  {
    "text": "database logic i'm not paying the databases are bad i'm saying that for our purposes",
    "start": "1522480",
    "end": "1527760"
  },
  {
    "text": "and performance gains uh our solution this solution that we implemented works",
    "start": "1527760",
    "end": "1533520"
  },
  {
    "text": "quite good and now we're going to jump into a more elaborate complex and and this is what",
    "start": "1533520",
    "end": "1541200"
  },
  {
    "text": "people really use in production actually it's not just a memory of file system what we have is",
    "start": "1541200",
    "end": "1548240"
  },
  {
    "text": "like an hybrid mechanism where all the data that we are creating of course goes to memory but in the",
    "start": "1548240",
    "end": "1555840"
  },
  {
    "text": "pipeline which says for this input source of data for example you are taking log files or",
    "start": "1555840",
    "end": "1561919"
  },
  {
    "text": "listening for messages you can say just a key app in memory",
    "start": "1561919",
    "end": "1567360"
  },
  {
    "text": "we have the concept of up and down you can see in the screen that we have in different colors in gray for down and",
    "start": "1567360",
    "end": "1573520"
  },
  {
    "text": "a light green for apps means that everything that is up in memory",
    "start": "1573520",
    "end": "1579600"
  },
  {
    "text": "is ready to be used or maybe it's a change that you can append more data right but we protect the memory",
    "start": "1579600",
    "end": "1586240"
  },
  {
    "text": "saying that hey you can have a memory up to for an example a hundred of chunks right",
    "start": "1586240",
    "end": "1592240"
  },
  {
    "text": "and if each chunk is in two megabytes well you can make you can make the map your own right but",
    "start": "1592240",
    "end": "1598880"
  },
  {
    "text": "after that we start storing all the chunks in the file system but how do we correlate this between all",
    "start": "1598880",
    "end": "1606559"
  },
  {
    "text": "the chunks that are in memory plus file system actually what we do is to use a",
    "start": "1606559",
    "end": "1612640"
  },
  {
    "text": "memory mapped files which is a really common implementation for databases where we",
    "start": "1612640",
    "end": "1618400"
  },
  {
    "text": "map a file that exists in the file system as if it were in memory and actually",
    "start": "1618400",
    "end": "1624720"
  },
  {
    "text": "that is really really performant right we are not using common system calls like read and write because we try to avoid",
    "start": "1624720",
    "end": "1631520"
  },
  {
    "text": "the the context copy of the data between the kernel and user space so using memory mapped files is quite",
    "start": "1631520",
    "end": "1638399"
  },
  {
    "text": "fast and we can have the control of how the data flows and if we get",
    "start": "1638399",
    "end": "1644080"
  },
  {
    "text": "ingested for example imagine that the output side is down and we're getting more data in",
    "start": "1644080",
    "end": "1650559"
  },
  {
    "text": "you are going to face this concept of back pressure right if you don't have a half ice system you're going to store",
    "start": "1650559",
    "end": "1655679"
  },
  {
    "text": "all your data memory and at some point your process is going to crash but what we do is study memory as much",
    "start": "1655679",
    "end": "1661520"
  },
  {
    "text": "as we can based on our own configuration limits and after that continue writing to file this now if you",
    "start": "1661520",
    "end": "1669120"
  },
  {
    "text": "look at the image you will see that in the file system part or on the disk you will see that some blocks are gray",
    "start": "1669120",
    "end": "1675840"
  },
  {
    "text": "and others are lightweight on this case on create we are saying that these chunks",
    "start": "1675840",
    "end": "1681600"
  },
  {
    "text": "are not up in memory they are just in the file system and when the memory part gets",
    "start": "1681600",
    "end": "1688720"
  },
  {
    "text": "empty it cannot get empty but start delivering the chunks we get more room more space to work on",
    "start": "1688720",
    "end": "1694559"
  },
  {
    "text": "the pipeline we just take the chunks and load them up in memory and this has been implemented for almost",
    "start": "1694559",
    "end": "1701120"
  },
  {
    "text": "two years and being a very scalable designs where we can have",
    "start": "1701120",
    "end": "1706480"
  },
  {
    "text": "i don't know 200k messages per second without any problem but you can increase that based on configuration and your own",
    "start": "1706480",
    "end": "1714159"
  },
  {
    "text": "there are many variables in your environment and then one thing about this all this data",
    "start": "1714159",
    "end": "1719679"
  },
  {
    "text": "processing for example with binary representation we can get the cpu and the cpu consumption low right also",
    "start": "1719679",
    "end": "1727919"
  },
  {
    "text": "all these i o to these that all these hundred memory is quite more performant and it's a really",
    "start": "1727919",
    "end": "1733840"
  },
  {
    "text": "scalable design right now this is butter proof and this is fluent is quite useful heavily on",
    "start": "1733840",
    "end": "1742080"
  },
  {
    "text": "most of my cloud providers like google aws microsoft azure and as a team we have",
    "start": "1742080",
    "end": "1749039"
  },
  {
    "text": "this kind of companies that work together actually we have weekly meetings with most of them",
    "start": "1749039",
    "end": "1755039"
  },
  {
    "text": "where they contribute also to the project they are part of fluent bed and they're trying to improve the whole",
    "start": "1755039",
    "end": "1760080"
  },
  {
    "text": "uh fluent ecosystem nowadays and this is scalable design",
    "start": "1760080",
    "end": "1765760"
  },
  {
    "text": "i would say that is most a scalable and cheap implementation in the market",
    "start": "1765760",
    "end": "1771360"
  },
  {
    "text": "right now you might find another solutions um but you will find that one thing is data",
    "start": "1771360",
    "end": "1778080"
  },
  {
    "text": "rate until this data consumption right so if you want to send the data fast enough",
    "start": "1778080",
    "end": "1783760"
  },
  {
    "text": "yeah just support the file disk right if you want to if you want to always",
    "start": "1783760",
    "end": "1791200"
  },
  {
    "text": "have a low cpu a low memory consumption maybe you can slow down ingestion do some dropping",
    "start": "1791200",
    "end": "1798000"
  },
  {
    "text": "also a one good thing to mention for everybody is like don't trust what i'm saying i would say",
    "start": "1798000",
    "end": "1804320"
  },
  {
    "text": "that in general when running these kind of things you should run your own benchmark and don't trust in benchmarks that are",
    "start": "1804320",
    "end": "1810799"
  },
  {
    "text": "published on the website right neither trust in our own ventures you might everybody has not had their",
    "start": "1810799",
    "end": "1816960"
  },
  {
    "text": "own baseline but we encourage every single user to run your own benchmarks because",
    "start": "1816960",
    "end": "1822080"
  },
  {
    "text": "every single configuration is different every use case is different",
    "start": "1822080",
    "end": "1827840"
  },
  {
    "start": "1828000",
    "end": "2104000"
  },
  {
    "text": "now as the next part we are going to do a quick demo on how fluent bit operates and with the",
    "start": "1830799",
    "end": "1837279"
  },
  {
    "text": "time we are going to just explain a little bit of the configuration a how do we achieve to sending just",
    "start": "1837279",
    "end": "1844399"
  },
  {
    "text": "something conservative numbers like 100 000 records per second which is most suitable for all cases um",
    "start": "1844399",
    "end": "1851919"
  },
  {
    "text": "actually that configuration is using file system buffering right together",
    "start": "1851919",
    "end": "1857200"
  },
  {
    "text": "with this hybrid mechanism so i'm going to switch back to the terminal",
    "start": "1857200",
    "end": "1862799"
  },
  {
    "text": "now in my left pane here i have the mi fluid configuration this",
    "start": "1862799",
    "end": "1868880"
  },
  {
    "text": "is a bare metal server on a different place and we are going to do is basically just tell one log file and send the data",
    "start": "1868880",
    "end": "1877279"
  },
  {
    "text": "in json format to a remote http endpoint right the remote http endpoint",
    "start": "1877279",
    "end": "1883760"
  },
  {
    "text": "is here on the right and it's just a basic a tls with http",
    "start": "1883760",
    "end": "1889360"
  },
  {
    "text": "server that is able to understand the json records and also expose a prometheus matrix okay",
    "start": "1889360",
    "end": "1897279"
  },
  {
    "text": "e and under this third one i'm going to run the load generator which is a script that will generate",
    "start": "1897279",
    "end": "1904240"
  },
  {
    "text": "around a thousand lines per second right each one will have one kilobyte",
    "start": "1904240",
    "end": "1911200"
  },
  {
    "text": "okay so to get started i'm going to run flue embed and now the configuration file",
    "start": "1911200",
    "end": "1920960"
  },
  {
    "text": "but let me check that i did a full cleanup of everything okay we're good to go",
    "start": "1920960",
    "end": "1928559"
  },
  {
    "text": "now i'm going to start prometheus because we are interested in to collect the metrics",
    "start": "1928559",
    "end": "1934880"
  },
  {
    "text": "and run the http server and as soon as i start the load generator here",
    "start": "1934880",
    "end": "1940799"
  },
  {
    "text": "we are going to see that the data the count of records will start increasing on the right so let's start",
    "start": "1940799",
    "end": "1949600"
  },
  {
    "text": "you will see also that in the fluid pane right here we're seeing a bunch of status messages",
    "start": "1949600",
    "end": "1956720"
  },
  {
    "text": "that the data was processed and now also you can realize on the right",
    "start": "1956720",
    "end": "1962000"
  },
  {
    "text": "that the number of records keep increasing and actually it's increasing quite constantly for a",
    "start": "1962000",
    "end": "1968480"
  },
  {
    "text": "hundred thousand records so let's verify how everything is working on the premier side so i'm going to my",
    "start": "1968480",
    "end": "1976320"
  },
  {
    "text": "web browser and start typing the address of prometus endpoint okay",
    "start": "1976320",
    "end": "1982640"
  },
  {
    "text": "i'm here i'm going to write my prongql query which will be fluent underscored records total i'm going to",
    "start": "1982640",
    "end": "1990000"
  },
  {
    "text": "use a five minutes and we're going to graph this information",
    "start": "1990000",
    "end": "1996960"
  },
  {
    "text": "we're going to get down so we can just get the new data that we're getting now or just the last",
    "start": "1996960",
    "end": "2003679"
  },
  {
    "text": "minute okay so this is the information that we are processing at the moment",
    "start": "2003679",
    "end": "2008720"
  },
  {
    "text": "actually it's quite constant this was the start of the process if we continue running the query we will",
    "start": "2008720",
    "end": "2014960"
  },
  {
    "text": "see that we are constantly around 100 000 messages per second usually on",
    "start": "2014960",
    "end": "2020880"
  },
  {
    "text": "this scenario a we don't see any back pressure we don't see any problems",
    "start": "2020880",
    "end": "2026799"
  },
  {
    "text": "we might see sometimes that the data goes down for a few seconds but could be because of networking but",
    "start": "2026799",
    "end": "2033360"
  },
  {
    "text": "then usually it recovers a without problem okay in the normal scenario like in a",
    "start": "2033360",
    "end": "2040559"
  },
  {
    "text": "kubernetes cluster a delivery data rate will be different actually you might",
    "start": "2040559",
    "end": "2046000"
  },
  {
    "text": "expect to have a i don't know a couple of nodes sending data to your own database right",
    "start": "2046000",
    "end": "2052560"
  },
  {
    "text": "it could be elastic spline but there's no like generic setup for each one",
    "start": "2052560",
    "end": "2058398"
  },
  {
    "text": "to achieve a perfect rate actually it's quite complex to determine what would be the ideal requirements",
    "start": "2058399",
    "end": "2064878"
  },
  {
    "text": "from a hardware perspective but we can see that with this way we can a process data without",
    "start": "2064879",
    "end": "2071118"
  },
  {
    "text": "any major problem okay so we're going to get back here to",
    "start": "2071119",
    "end": "2077040"
  },
  {
    "text": "the terminal and we see that everything keeps running without any major problem",
    "start": "2077040",
    "end": "2084560"
  },
  {
    "text": "okay so i think that we are good to go with this and i think that now it's time for some",
    "start": "2084560",
    "end": "2092320"
  },
  {
    "text": "q a some questions so please feel free to write your questions",
    "start": "2092320",
    "end": "2099040"
  },
  {
    "text": "right now in the chat or live during this session thank you so much",
    "start": "2099040",
    "end": "2106400"
  }
]