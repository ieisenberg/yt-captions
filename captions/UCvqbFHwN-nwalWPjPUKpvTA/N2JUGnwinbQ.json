[
  {
    "start": "0",
    "end": "174000"
  },
  {
    "text": "hi everyone and thank you for coming to stories from the playbook I'm Tina and",
    "start": "0",
    "end": "5970"
  },
  {
    "text": "this is Fred and we are Google coupe Nettie's engine site reliability engineers from London and today we're",
    "start": "5970",
    "end": "13769"
  },
  {
    "text": "here to talk to you about how we approach the challenge of managing the fleet of Google kubernetes engine hosted",
    "start": "13769",
    "end": "21270"
  },
  {
    "text": "masters so when you run a coop Nettie's cluster on gke we take care of the",
    "start": "21270",
    "end": "28529"
  },
  {
    "text": "master VM making sure that all of the core control plane components are up and",
    "start": "28529",
    "end": "33930"
  },
  {
    "text": "running we also control the upgrade cycle of the master VM and we foot the",
    "start": "33930",
    "end": "39570"
  },
  {
    "text": "bill for hosting the master on GCP so in this talk we're hoping to give you an",
    "start": "39570",
    "end": "45840"
  },
  {
    "text": "insight into how we do the alerting and incident management for gke as well as",
    "start": "45840",
    "end": "52710"
  },
  {
    "text": "give an overview of some recent outages most commonly seen tickets which might",
    "start": "52710",
    "end": "58289"
  },
  {
    "text": "be interesting for you as a gk customer or some general good practice tips if",
    "start": "58289",
    "end": "63870"
  },
  {
    "text": "you're running your own cluster but before we dive into the gke part what is",
    "start": "63870",
    "end": "69680"
  },
  {
    "text": "sre so site reliability engineering our main goal is to work with developers and",
    "start": "69680",
    "end": "78530"
  },
  {
    "text": "customers I suppose to help protect the service level objectives that we offer",
    "start": "78530",
    "end": "83850"
  },
  {
    "text": "to gke customers so we do this by holding the pager for the product but",
    "start": "83850",
    "end": "91979"
  },
  {
    "text": "also by working on new features and release processes and helping to review",
    "start": "91979",
    "end": "98159"
  },
  {
    "text": "design Docs or to make sure that if they follow the principle of reliability and",
    "start": "98159",
    "end": "105540"
  },
  {
    "text": "we also work closely not only with dev teams but also with other sre teams in",
    "start": "105540",
    "end": "110610"
  },
  {
    "text": "Google that gke is reliant on and in London and we have the ads cloud and",
    "start": "110610",
    "end": "117270"
  },
  {
    "text": "mobile as well as internal and traffic and networking teams and the ultimate",
    "start": "117270",
    "end": "122610"
  },
  {
    "text": "aim of all sre teams is to automate themselves out of a job so this is much",
    "start": "122610",
    "end": "129330"
  },
  {
    "text": "in keeping with the self-healing aasif ii behind kubernetes the components of the system can detect and",
    "start": "129330",
    "end": "136829"
  },
  {
    "text": "heal themselves without manual intervention and this means we need to iterate on the pages and the tickets",
    "start": "136829",
    "end": "143459"
  },
  {
    "text": "that we see to make sure that they don't happen repeatedly so for gke we have two",
    "start": "143459",
    "end": "150239"
  },
  {
    "text": "teams one in Seattle and one in London providing 24/7 coverage for both gke and",
    "start": "150239",
    "end": "156930"
  },
  {
    "text": "also appengine flexible environment so actually our team looks after all of the managed compute products on Google cloud",
    "start": "156930",
    "end": "164970"
  },
  {
    "text": "platform and we on-boarded gke as a product around 18 months ago so to give",
    "start": "164970",
    "end": "170730"
  },
  {
    "text": "you an idea of some of the challenges we face in numbers as I said we are we're one team but we have two shots globally",
    "start": "170730",
    "end": "178489"
  },
  {
    "start": "174000",
    "end": "174000"
  },
  {
    "text": "and we're supporting a cloud product that's growing exponentially both in terms of the number of users and also",
    "start": "178489",
    "end": "185010"
  },
  {
    "text": "the number of cloud locations and we're backed by an open-source project with",
    "start": "185010",
    "end": "190739"
  },
  {
    "text": "more than a thousand contributors we have a pretty rapid release cycle its",
    "start": "190739",
    "end": "196500"
  },
  {
    "text": "weekly and the rollout occurs over four days across the different cloud",
    "start": "196500",
    "end": "202169"
  },
  {
    "text": "locations and we're supporting the latest 3 minor kubernetes versions so in",
    "start": "202169",
    "end": "210629"
  },
  {
    "text": "almost every release we're doing one of the following things either making a new",
    "start": "210629",
    "end": "215909"
  },
  {
    "text": "minor version available as a preview early access version bumping a previous",
    "start": "215909",
    "end": "223590"
  },
  {
    "text": "preview version to a generally available version once we've seen enough stability",
    "start": "223590",
    "end": "230150"
  },
  {
    "text": "or promoting a previously generally available version to the default version",
    "start": "230150",
    "end": "235919"
  },
  {
    "text": "for all newly created clusters again once we have enough confidence or mass",
    "start": "235919",
    "end": "241199"
  },
  {
    "text": "migrating clusters from the oldest version in order to maintain this",
    "start": "241199",
    "end": "247409"
  },
  {
    "text": "sliding window of three minor versions so one of the central tenets of site",
    "start": "247409",
    "end": "253949"
  },
  {
    "text": "reliability engineering is that the headcount of SRS should not grow linearly with the number of developers",
    "start": "253949",
    "end": "260940"
  },
  {
    "text": "or the number of new features so we're going to try to talk to you about how we approach the challenge of maintaining",
    "start": "260940",
    "end": "267180"
  },
  {
    "text": "a 99.5 percent availability SLO for zonal clusters in light of all of these",
    "start": "267180",
    "end": "273870"
  },
  {
    "text": "moving parts okay so you're here for",
    "start": "273870",
    "end": "279120"
  },
  {
    "start": "276000",
    "end": "276000"
  },
  {
    "text": "some stories from the playbook so it's probably a good idea to make sure we start off on the same page",
    "start": "279120",
    "end": "284370"
  },
  {
    "text": "describe briefly what a playbook actually is I was hoping the sre book would give a nice 6 ink definition of",
    "start": "284370",
    "end": "290760"
  },
  {
    "text": "what that is but it doesn't say much beyond this which is fairly light on the subject talking about best practices and",
    "start": "290760",
    "end": "297330"
  },
  {
    "text": "reducing your mean time to recovery so if we look at GK specifically we have a",
    "start": "297330",
    "end": "302760"
  },
  {
    "text": "player country for every alert defined on the system so that might be high error rate on an API server or a cluster",
    "start": "302760",
    "end": "309330"
  },
  {
    "text": "being unavailable for too long and that entry documents the signs and symptoms to look for when an alert fires and also",
    "start": "309330",
    "end": "316860"
  },
  {
    "text": "the possible procedures to help mitigate and eventually fix the issue play books",
    "start": "316860",
    "end": "322920"
  },
  {
    "text": "are a great way of sharing information between engineers on a team and also reducing the cognitive load of the on",
    "start": "322920",
    "end": "328890"
  },
  {
    "text": "call or carrying the pedra quick and fluent ation detail our play books are",
    "start": "328890",
    "end": "335370"
  },
  {
    "text": "hosted in a number of redundant places so they're easily accessible even if a wide if there's a wide outage in Google",
    "start": "335370",
    "end": "343550"
  },
  {
    "text": "if we look briefly at what an idealized play book entry might look like in this",
    "start": "343550",
    "end": "349560"
  },
  {
    "text": "case city with HCD we can see some signs and symptoms down the left hand side and some procedures to fix them on the right",
    "start": "349560",
    "end": "357300"
  },
  {
    "text": "hand side we're going to come back to this in more detail so I'll move on for now as I mentioned the core part of an",
    "start": "357300",
    "end": "366810"
  },
  {
    "text": "effective player country is the signs and symptoms to look for when fires and",
    "start": "366810",
    "end": "372510"
  },
  {
    "start": "368000",
    "end": "368000"
  },
  {
    "text": "for that you need some good insights into how your system is performing in the case of gke we have we care mainly",
    "start": "372510",
    "end": "378720"
  },
  {
    "text": "about the health of the GK system overall so that's the systems that serve",
    "start": "378720",
    "end": "384060"
  },
  {
    "text": "the cost cluster lifecycle and we also care about the health of each and every cluster running within the fleet so we",
    "start": "384060",
    "end": "391620"
  },
  {
    "text": "gather metrics for both of those aspects showing the request rate for the control",
    "start": "391620",
    "end": "397140"
  },
  {
    "text": "plane API so for example the load on a particular master instance these are all shown on dashboards so",
    "start": "397140",
    "end": "404940"
  },
  {
    "text": "again there's one for each cluster control plane service and per cluster",
    "start": "404940",
    "end": "410040"
  },
  {
    "text": "dashboard so we can see exactly what's going on on each individual cluster and we aggregate those all together to",
    "start": "410040",
    "end": "415680"
  },
  {
    "text": "create a fleet-wide view of different clusters so we can see what's going on",
    "start": "415680",
    "end": "423630"
  },
  {
    "text": "between different kubernetes versions and also across different cloud regions finally we have a set of probers and",
    "start": "423630",
    "end": "430830"
  },
  {
    "text": "these give a critical black box view of the performance of the system as a user would see it so for example in gke we",
    "start": "430830",
    "end": "438540"
  },
  {
    "text": "have a proba that continually creates and deletes clusters and make sure that works and others that make sure metrics",
    "start": "438540",
    "end": "444840"
  },
  {
    "text": "are being exported as expected so how",
    "start": "444840",
    "end": "452610"
  },
  {
    "start": "450000",
    "end": "450000"
  },
  {
    "text": "did we gather the data that affords us this availability what this observability into the availability and",
    "start": "452610",
    "end": "459240"
  },
  {
    "text": "the state of gke clusters so in every cloud location we run a job outside of",
    "start": "459240",
    "end": "466140"
  },
  {
    "text": "Google cloud platform called the monitoring server and this collects metrics metadata and logs from each",
    "start": "466140",
    "end": "473760"
  },
  {
    "text": "cluster so our SLO definition of Master availability is based on the core",
    "start": "473760",
    "end": "480539"
  },
  {
    "text": "component statuses returning healthy for every component and if availability is",
    "start": "480539",
    "end": "486720"
  },
  {
    "text": "down for even a short period of time the monitoring server will enact an",
    "start": "486720",
    "end": "492389"
  },
  {
    "text": "automatic repair by recreating the master VM and also based on the number",
    "start": "492389",
    "end": "499229"
  },
  {
    "text": "of nodes that the cluster is running we can also automatically resize the master",
    "start": "499229",
    "end": "504990"
  },
  {
    "text": "VM so that it can handle higher workloads if these actions don't work and",
    "start": "504990",
    "end": "510930"
  },
  {
    "text": "availability remains down for too long or if some other metrics reach some",
    "start": "510930",
    "end": "517650"
  },
  {
    "text": "undesired status that like under some predefined conditions such as at CD",
    "start": "517650",
    "end": "524070"
  },
  {
    "text": "backups running to old or master VM discs to full an alert is sent to the on",
    "start": "524070",
    "end": "530730"
  },
  {
    "text": "caller so depending on the severity of the condition this can either be a ticket or it can be a page",
    "start": "530730",
    "end": "537540"
  },
  {
    "text": "that requires an immediate response and this might be location specific version",
    "start": "537540",
    "end": "542880"
  },
  {
    "text": "specific or cluster specific so each and every cluster has the ability to page us",
    "start": "542880",
    "end": "549410"
  },
  {
    "text": "individually so that's why we care so much about the reliability of the system and also besides our monitoring",
    "start": "549410",
    "end": "556680"
  },
  {
    "text": "infrastructure we can also receive tickets from our frontline support team if they're seeing urgent customer issues",
    "start": "556680",
    "end": "565100"
  },
  {
    "start": "565000",
    "end": "565000"
  },
  {
    "text": "so we thought we'd give some examples of the most commonly seen tickets and",
    "start": "565100",
    "end": "571350"
  },
  {
    "text": "alerts that we've had over the past year some of these have actually been successfully automated away so to start",
    "start": "571350",
    "end": "579300"
  },
  {
    "text": "off I'm sure we're not the only ones who've been the victims of golang panics due to nil pointer dereference and so a",
    "start": "579300",
    "end": "587399"
  },
  {
    "text": "few months ago we saw this a few times in the 1/7 controller manager code which",
    "start": "587399",
    "end": "593519"
  },
  {
    "text": "results in the controller manager crash looping and sometimes it's due to code that was committed in the open source",
    "start": "593519",
    "end": "600089"
  },
  {
    "text": "project actually a few versions ago but it's on a rare execution pass but due to",
    "start": "600089",
    "end": "606420"
  },
  {
    "text": "the sheer number of clusters that we manage it gets triggered by an edge case and wakes up and on caller with a page",
    "start": "606420",
    "end": "614120"
  },
  {
    "text": "in a couple of cases our on-call was able to submit a PR to fix the issue and",
    "start": "614120",
    "end": "620220"
  },
  {
    "text": "then we just need to wait for a new kubernetes patch release in order to migrate the problematic cluster onto it",
    "start": "620220",
    "end": "627120"
  },
  {
    "text": "and I think we're quite lucky in SRE that so many Googlers are open-source",
    "start": "627120",
    "end": "632220"
  },
  {
    "text": "contributors so we have an escalation path to find fixes and submit PRS for",
    "start": "632220",
    "end": "638240"
  },
  {
    "text": "issues such as this this little robot",
    "start": "638240",
    "end": "643470"
  },
  {
    "text": "character represents the service accounts which GK uses to interact with",
    "start": "643470",
    "end": "649350"
  },
  {
    "text": "clusters as they're running and there's one of these in every customer project it's also possible that a customer can",
    "start": "649350",
    "end": "656010"
  },
  {
    "text": "disable these for whatever reason and this limits the ability of the master",
    "start": "656010",
    "end": "662430"
  },
  {
    "text": "instances to manage the cluster and also the ability of the control plane to interact",
    "start": "662430",
    "end": "667689"
  },
  {
    "text": "with the cluster in general nowadays if that happens we don't get a page we move",
    "start": "667689",
    "end": "673389"
  },
  {
    "text": "the cluster into a specific state and notify the customer that this has happened and that they need to solve the",
    "start": "673389",
    "end": "679569"
  },
  {
    "text": "problem if they want to buy real abling the service accounts and then fluently",
    "start": "679569",
    "end": "685930"
  },
  {
    "text": "you might be surprised to see that or may not be surprised to see that on this slide and so we use this to export logs",
    "start": "685930",
    "end": "692439"
  },
  {
    "text": "from both the master and the node onto our logging API and this creates an",
    "start": "692439",
    "end": "698499"
  },
  {
    "text": "alert because at some point in time it was the largest user of CPU and even",
    "start": "698499",
    "end": "705670"
  },
  {
    "text": "though it's not a core component in itself we definitely don't want it to starve other critical processes on the",
    "start": "705670",
    "end": "712509"
  },
  {
    "text": "master but it's also vital that we have these logs for debugging purposes so we",
    "start": "712509",
    "end": "718120"
  },
  {
    "text": "alert when this add-on is not running properly we also see quite a few",
    "start": "718120",
    "end": "725199"
  },
  {
    "text": "instances of clusters becoming unavailable due to the workload that is being run on them often this is simply",
    "start": "725199",
    "end": "734079"
  },
  {
    "text": "due to some load testing where our customers kicking the tires a little too vigorously on the cluster sometimes it's",
    "start": "734079",
    "end": "741069"
  },
  {
    "text": "due to a slightly pathological configuration of the workload for instance one namespace per pod or simply",
    "start": "741069",
    "end": "746860"
  },
  {
    "text": "too many jobs simultaneously submitted this these conditions can combine",
    "start": "746860",
    "end": "752230"
  },
  {
    "text": "together to essentially denial-of-service the API server and cause other critical core components such as the scheduler and the controller",
    "start": "752230",
    "end": "758500"
  },
  {
    "text": "manager to start crash leaving and that can can result in an unhealthy cluster",
    "start": "758500",
    "end": "764250"
  },
  {
    "text": "and finally everyone's favorite distributed key-value store at CD let's",
    "start": "764250",
    "end": "771069"
  },
  {
    "text": "go back to the playbook example that Fred showed earlier so this is obviously",
    "start": "771069",
    "end": "776679"
  },
  {
    "start": "773000",
    "end": "773000"
  },
  {
    "text": "not a comprehensive example of a playbook entry for at CD it's a snapshot",
    "start": "776679",
    "end": "782379"
  },
  {
    "text": "of what our playbook looked like when we were experiencing two symptoms",
    "start": "782379",
    "end": "788199"
  },
  {
    "text": "repeatedly and I think it was the most common cause of pages a few months ago",
    "start": "788199",
    "end": "794980"
  },
  {
    "text": "so first of all MVCC database size exceeded so this is referring to bolt DB",
    "start": "794980",
    "end": "801430"
  },
  {
    "text": "and the multi-version concurrency control feature that's backing at CD and",
    "start": "801430",
    "end": "806710"
  },
  {
    "text": "this is what allows that CD to handle one right transaction and many reed transactions concurrently so what we",
    "start": "806710",
    "end": "814660"
  },
  {
    "text": "were seeing was that there was a bug in the version of @ CD that we were running which meant that this database would",
    "start": "814660",
    "end": "822100"
  },
  {
    "text": "expand and become increasingly defragmented if you happen to have a",
    "start": "822100",
    "end": "827680"
  },
  {
    "text": "reasonably long read transaction during which many writes occur and the chances",
    "start": "827680",
    "end": "832750"
  },
  {
    "text": "of this really increase if your cluster workload is relatively high so short",
    "start": "832750",
    "end": "839170"
  },
  {
    "text": "term fix for the on caller who gets paged was to of course check the playbook country then log in to the",
    "start": "839170",
    "end": "846190"
  },
  {
    "text": "container and then call a manual at CD compact or defrag command and or resize",
    "start": "846190",
    "end": "854500"
  },
  {
    "text": "the machine disk longer-term the fix that we actually went forward with",
    "start": "854500",
    "end": "859630"
  },
  {
    "text": "actually run an extra component on the master that were just simply periodically effective another symptom",
    "start": "859630",
    "end": "873370"
  },
  {
    "text": "that we saw a few times was crash looping at CD and this was triggered by",
    "start": "873370",
    "end": "879930"
  },
  {
    "text": "bug in the writer head log so what happened was that once at CD crashed it",
    "start": "879930",
    "end": "886990"
  },
  {
    "text": "would replay it's right ahead blog processing all log entries in order",
    "start": "886990",
    "end": "892740"
  },
  {
    "text": "which can include some long-running reads and this could take much longer",
    "start": "892740",
    "end": "898810"
  },
  {
    "text": "than the 15 initial delay seconds setting that we had for the at CD",
    "start": "898810",
    "end": "904960"
  },
  {
    "text": "containers liveness probe health check setting and this meant that cubelet",
    "start": "904960",
    "end": "910780"
  },
  {
    "text": "would cause at CD to restart again and therefore result in this crash looping",
    "start": "910780",
    "end": "916600"
  },
  {
    "text": "behavior and this was fixed by the on caller simply removing the initial delay",
    "start": "916600",
    "end": "922300"
  },
  {
    "text": "second setting from the health check and this meant that city had an",
    "start": "922300",
    "end": "928170"
  },
  {
    "text": "indefinitely long time to restart but the underlying fix was made in a later",
    "start": "928170",
    "end": "933299"
  },
  {
    "text": "version of a vet CD and the last section on vida election issues is an example of",
    "start": "933299",
    "end": "940739"
  },
  {
    "text": "a preemptive playbook entry so in December 2017 we launched high",
    "start": "940739",
    "end": "946709"
  },
  {
    "text": "availability clusters as a product in gke in beta and this is where a cluster is",
    "start": "946709",
    "end": "953369"
  },
  {
    "text": "backed by three masters across three different zones in one region so",
    "start": "953369",
    "end": "958619"
  },
  {
    "text": "previously all gke clusters have just one hosted master so it's only with this",
    "start": "958619",
    "end": "964829"
  },
  {
    "text": "high availability clusters product that we're executing the leader election code",
    "start": "964829",
    "end": "970169"
  },
  {
    "text": "paths in @cd so this is an example of some steps and that the on caller can",
    "start": "970169",
    "end": "977040"
  },
  {
    "text": "take if they discover and leader election issues okay so here are some",
    "start": "977040",
    "end": "984600"
  },
  {
    "start": "982000",
    "end": "982000"
  },
  {
    "text": "other common procedures that we'll use that are somewhat generic but apply in a",
    "start": "984600",
    "end": "991139"
  },
  {
    "text": "few cases the first is simply to roll back this is something we can do with",
    "start": "991139",
    "end": "996480"
  },
  {
    "text": "the GK control plane quite simply if there's a regression introduced with the new release you simply go back to the",
    "start": "996480",
    "end": "1001549"
  },
  {
    "text": "old one it's less easy for kubernetes specific issues if we're rolling out a",
    "start": "1001549",
    "end": "1008269"
  },
  {
    "text": "new patch version or a new minor version we can't really roll that back because we might be taking away features from a",
    "start": "1008269",
    "end": "1014419"
  },
  {
    "text": "customer or reintroducing bugs that were fixed in that new patch release so often what we'll do is cut a new release",
    "start": "1014419",
    "end": "1021350"
  },
  {
    "text": "internally with the fix roll that out and then upstream the patches to the relevant open source repo another option",
    "start": "1021350",
    "end": "1029750"
  },
  {
    "text": "we have for clusters specific alert is simply to resize the master VM that are",
    "start": "1029750",
    "end": "1035389"
  },
  {
    "text": "used for that cluster to a larger size and often if the cluster is overloaded and that's causing health issues the",
    "start": "1035389",
    "end": "1042288"
  },
  {
    "text": "larger size will give it the space to breathe and become healthy again we also",
    "start": "1042289",
    "end": "1048830"
  },
  {
    "text": "have the ability to tweak our on alert is configured we use this sparingly but if we think an alert is misfiring",
    "start": "1048830",
    "end": "1055100"
  },
  {
    "text": "then we can change the conditions on the which should shoot fire and hopefully reduce the that the pager is making that way for",
    "start": "1055100",
    "end": "1062360"
  },
  {
    "text": "example recently we had a configured such that if a single request that it",
    "start": "1062360",
    "end": "1067730"
  },
  {
    "text": "was monitoring failed it would fire that's clearly not a scalable situation to be in",
    "start": "1067730",
    "end": "1072880"
  },
  {
    "text": "finally we also have the option to simply silence the page we'll do this",
    "start": "1072880",
    "end": "1078800"
  },
  {
    "text": "very sparingly but if we've investigated the alert to its full extent and believe",
    "start": "1078800",
    "end": "1084050"
  },
  {
    "text": "we can't fix it then we'll silence it we'll use this in the case of the controller managing",
    "start": "1084050",
    "end": "1090230"
  },
  {
    "text": "controller manager crash looping for example where we knew that was a fix imminent but there was nothing we could",
    "start": "1090230",
    "end": "1095870"
  },
  {
    "text": "do at the time so what can you do to",
    "start": "1095870",
    "end": "1101180"
  },
  {
    "text": "make sure your cluster stays happy and any on caller behind it stays happier as well first and foremost it's important",
    "start": "1101180",
    "end": "1108110"
  },
  {
    "text": "that your cluster is provisioned for the workload that it's expected to run this goes for nodes but also the manage the",
    "start": "1108110",
    "end": "1114590"
  },
  {
    "text": "masters if you're managing those inso yourselves also the disks backing the",
    "start": "1114590",
    "end": "1120320"
  },
  {
    "text": "underlying at CD databases need to be well provisioned if there's any throttling there and the discs become a",
    "start": "1120320",
    "end": "1126470"
  },
  {
    "text": "bottleneck that can be detrimental to the cluster performance as well you can",
    "start": "1126470",
    "end": "1132320"
  },
  {
    "text": "use resource limits on your pod definitions to make sure the scheduler can do a good job of not overloading the",
    "start": "1132320",
    "end": "1138740"
  },
  {
    "text": "cluster and this should be a no-brainer but frequent at CD backups is a",
    "start": "1138740",
    "end": "1144260"
  },
  {
    "text": "definitely a good idea we do this in the order of minutes on every cluster and there's an alert that goes off if these",
    "start": "1144260",
    "end": "1150800"
  },
  {
    "text": "become too stale we also recommend keeping your cluster up-to-date as much",
    "start": "1150800",
    "end": "1156260"
  },
  {
    "text": "as possible up to the latest patch version if not minor version and GK can really help",
    "start": "1156260",
    "end": "1161570"
  },
  {
    "text": "here obviously also if you are using gke please make sure your service accounts",
    "start": "1161570",
    "end": "1166940"
  },
  {
    "text": "for a name remain enabled we've talked",
    "start": "1166940",
    "end": "1173090"
  },
  {
    "start": "1171000",
    "end": "1171000"
  },
  {
    "text": "to you about some of the routine alerts and where there is a solution in the",
    "start": "1173090",
    "end": "1179120"
  },
  {
    "text": "playbook but sometimes a large incident large impact occurs and that must be",
    "start": "1179120",
    "end": "1187040"
  },
  {
    "text": "route cost for the first time so in sorry for incident management and",
    "start": "1187040",
    "end": "1193190"
  },
  {
    "text": "follow a mantra of coordinate communication and control and we have an",
    "start": "1193190",
    "end": "1199850"
  },
  {
    "text": "established protocol in a crisis so on caller's are encouraged to pull in other",
    "start": "1199850",
    "end": "1205220"
  },
  {
    "text": "team members for help and assign defined roles so there should be an incident",
    "start": "1205220",
    "end": "1210740"
  },
  {
    "text": "commander to manage and coordinate efforts operations lead and a",
    "start": "1210740",
    "end": "1216080"
  },
  {
    "text": "communications lead so we have established communications channels to",
    "start": "1216080",
    "end": "1221920"
  },
  {
    "text": "identify the impact and notify customers via the support channels there's also a",
    "start": "1221920",
    "end": "1227990"
  },
  {
    "text": "scalation paths to other parts of cloud and internal teams as well as our",
    "start": "1227990",
    "end": "1233480"
  },
  {
    "text": "developers he maintained their own on-call rotation for the different subsystems within gke and kubernetes",
    "start": "1233480",
    "end": "1240760"
  },
  {
    "text": "then after the incidents it's very important to write a post-mortem that we can learn from and it's key that it's",
    "start": "1240760",
    "end": "1247940"
  },
  {
    "text": "blameless so an important tenet of sree culture is that the fault lies with the",
    "start": "1247940",
    "end": "1253970"
  },
  {
    "text": "underlying production systems and not with well-meaning humans and most",
    "start": "1253970",
    "end": "1259070"
  },
  {
    "text": "importantly we need to execute on action items to ensure that the outage is not",
    "start": "1259070",
    "end": "1264770"
  },
  {
    "text": "repeated so that's the theory of",
    "start": "1264770",
    "end": "1270550"
  },
  {
    "start": "1268000",
    "end": "1268000"
  },
  {
    "text": "incident management when I'm going to look at three different incidents that occurred recently and how we dealt with",
    "start": "1270550",
    "end": "1276740"
  },
  {
    "text": "them the first one is to do with docker which of course runs on every cluster",
    "start": "1276740",
    "end": "1282860"
  },
  {
    "text": "master and every node this started with the release of a new OS image underlying",
    "start": "1282860",
    "end": "1288680"
  },
  {
    "text": "the nodes and that started to roll out and it was noticed that if the docker daemon restarted on that node then pods",
    "start": "1288680",
    "end": "1295370"
  },
  {
    "text": "would no longer be scheduled on it it's obviously not great for the health of the cluster its workload so the first",
    "start": "1295370",
    "end": "1302060"
  },
  {
    "text": "thing to do is to try and mitigate the problem and in this case it was quite simple we roll back to using the previous OS image for new nodes and",
    "start": "1302060",
    "end": "1309280"
  },
  {
    "text": "downgrade any nodes that had been pushed forward so the problems mitigated but we",
    "start": "1309280",
    "end": "1316040"
  },
  {
    "text": "still don't know exactly what happened and that's when we start looking for a root cause so we knew it was something",
    "start": "1316040",
    "end": "1321410"
  },
  {
    "text": "to do with the new OS image so we're able to essentially diff the configurations between the",
    "start": "1321410",
    "end": "1326870"
  },
  {
    "text": "and the new one and that's where we see the doc alive restore flag come into play so then we're pretty suspicious of",
    "start": "1326870",
    "end": "1336140"
  },
  {
    "text": "that the the life restore flag so we roll out a new image with that disabled and everything seems to be fine again so",
    "start": "1336140",
    "end": "1344090"
  },
  {
    "text": "what went well here well we were able to spot the issue pretty quickly before the newest image got rolled out too far and",
    "start": "1344090",
    "end": "1351560"
  },
  {
    "text": "we were able to downgrade affected modes to a previous OS image where possible",
    "start": "1351560",
    "end": "1361180"
  },
  {
    "text": "obviously what went badly well those lack of testing here this should never really have got out into",
    "start": "1361180",
    "end": "1366910"
  },
  {
    "text": "into the live world and in some cases",
    "start": "1366910",
    "end": "1372590"
  },
  {
    "text": "where customers didn't have Auto upgrade enabled we had to ask them to downgrade specifically we actually got very lucky",
    "start": "1372590",
    "end": "1379820"
  },
  {
    "text": "here as well because there were no alerts firing at this point we were notified by one of our node team",
    "start": "1379820",
    "end": "1386120"
  },
  {
    "text": "engineers that this was an issue and yes we were able to solve it from their",
    "start": "1386120",
    "end": "1391340"
  },
  {
    "text": "notification this outage was one of our",
    "start": "1391340",
    "end": "1398360"
  },
  {
    "start": "1395000",
    "end": "1395000"
  },
  {
    "text": "greatest hits arguably one of the longest and most visible outages for gke",
    "start": "1398360",
    "end": "1404800"
  },
  {
    "text": "so far fingers crossed so this started off with our create cluster probers for",
    "start": "1404800",
    "end": "1413510"
  },
  {
    "text": "the entire europe geo firing as in they were not being able to successfully",
    "start": "1413510",
    "end": "1419990"
  },
  {
    "text": "create clusters and our logs indicated that this was a real customer issue and not just a problem with our prober",
    "start": "1419990",
    "end": "1427580"
  },
  {
    "text": "projects so some initial investigation showed that no other Google cloud platform services were affected",
    "start": "1427580",
    "end": "1434900"
  },
  {
    "text": "so they once so it wasn't an issue with the network or quota it was a gke isolated incident",
    "start": "1434900",
    "end": "1441490"
  },
  {
    "text": "cluster creation was failing when nodes were attempting to register with the",
    "start": "1441490",
    "end": "1446930"
  },
  {
    "text": "master and after a few more hours of root causing we finally found that the",
    "start": "1446930",
    "end": "1453140"
  },
  {
    "text": "error message indicated that the certificate signing module was the culprit",
    "start": "1453140",
    "end": "1459210"
  },
  {
    "text": "so the certificate signer was trying to pull an external image from docker hub",
    "start": "1459210",
    "end": "1466020"
  },
  {
    "text": "also appeared and the image was corrupted so in the absence of an",
    "start": "1466020",
    "end": "1471900"
  },
  {
    "text": "escalation path to docker hub asari we first explored the mitigation option of",
    "start": "1471900",
    "end": "1479060"
  },
  {
    "text": "configuring clusters to pull from a different location where images were still healthy",
    "start": "1479060",
    "end": "1485120"
  },
  {
    "text": "so this required us to rebuild GAE binaries which is something that takes",
    "start": "1485120",
    "end": "1490280"
  },
  {
    "text": "over an hour in the middle of doing this we discovered that the images were",
    "start": "1490280",
    "end": "1496440"
  },
  {
    "text": "actually coming from an internal Google container registry mirror so actually",
    "start": "1496440",
    "end": "1501720"
  },
  {
    "text": "what we needed to do was purge the internal mirrors cached image so in",
    "start": "1501720",
    "end": "1508230"
  },
  {
    "text": "terms of what went well we had pretty good metrics that assess the proportion of the impact and unfortunately it was",
    "start": "1508230",
    "end": "1515670"
  },
  {
    "text": "pretty large we were able to pull in help from the incident response team who",
    "start": "1515670",
    "end": "1521100"
  },
  {
    "text": "are an internal team of experienced Google sres that help provide emergency",
    "start": "1521100",
    "end": "1526950"
  },
  {
    "text": "assistance in the case of a major outage and we were pretty lucky that once we",
    "start": "1526950",
    "end": "1532440"
  },
  {
    "text": "root cause the problem the Google container registry team had a big button mitigation that meant we were able to",
    "start": "1532440",
    "end": "1540660"
  },
  {
    "text": "easily purge the problematic image so in terms of what went badly",
    "start": "1540660",
    "end": "1545820"
  },
  {
    "text": "so gke has many dependencies which meant lots of paths to debug so it actually",
    "start": "1545820",
    "end": "1551100"
  },
  {
    "text": "took quite a long time to find the log entry that suggested it was the",
    "start": "1551100",
    "end": "1556320"
  },
  {
    "text": "certificate signer at fault this means that our most significant action item",
    "start": "1556320",
    "end": "1561990"
  },
  {
    "text": "was to create a more comprehensive list of our dependencies this is still quite",
    "start": "1561990",
    "end": "1568230"
  },
  {
    "text": "long and create monitoring that shows the steps with success and failure while",
    "start": "1568230",
    "end": "1573870"
  },
  {
    "text": "we're bootstrapping a cluster and even though we didn't need the binary rebuild",
    "start": "1573870",
    "end": "1579570"
  },
  {
    "text": "process we found that we needed to create a fast-track emergency rollout",
    "start": "1579570",
    "end": "1584880"
  },
  {
    "text": "workflow so in case of any future outages we can roll out a fix in a more speedy",
    "start": "1584880",
    "end": "1591270"
  },
  {
    "text": "fashion the final incident we're going",
    "start": "1591270",
    "end": "1597060"
  },
  {
    "start": "1594000",
    "end": "1594000"
  },
  {
    "text": "to talk about is to do with the NS mask vulnerabilities it fits really poorly",
    "start": "1597060",
    "end": "1602910"
  },
  {
    "text": "into this instant format that we came up with but it's an interesting one to talk about nonetheless but a background to",
    "start": "1602910",
    "end": "1610410"
  },
  {
    "text": "start with DNS mask runs as part of the cube DNS add-on on every class of the GK",
    "start": "1610410",
    "end": "1617030"
  },
  {
    "text": "manages and we were notified by our internal security team that there were a",
    "start": "1617030",
    "end": "1622980"
  },
  {
    "text": "number of vulnerabilities in DNS masks from our point of view there were no",
    "start": "1622980",
    "end": "1628260"
  },
  {
    "text": "symptoms that's kind of a given with security vulnerabilities like this and there was no real mitigation here we",
    "start": "1628260",
    "end": "1634680"
  },
  {
    "text": "couldn't really roll back to a previous version of DNS mask which would be our usual strategy and we couldn't stop DNS",
    "start": "1634680",
    "end": "1641100"
  },
  {
    "text": "mask running on all of our clusters because that would probably make no one happy so what did we do we cut GK",
    "start": "1641100",
    "end": "1650430"
  },
  {
    "text": "specific kubernetes release versions with fixes included in them and rolled",
    "start": "1650430",
    "end": "1656340"
  },
  {
    "text": "those out just before these vulnerabilities were publicly disclosed we had to do this internally because we",
    "start": "1656340",
    "end": "1663120"
  },
  {
    "text": "couldn't leak the presence of these vulnerabilities before public disclosure but after we were able to upstream the",
    "start": "1663120",
    "end": "1670920"
  },
  {
    "text": "patches to the relevant repos everything here went surprisingly smoothly we were",
    "start": "1670920",
    "end": "1677550"
  },
  {
    "text": "able to do the rollout and fix the vulnerabilities without leaking the presence of them the outside world",
    "start": "1677550",
    "end": "1685190"
  },
  {
    "text": "however we were in the middle of doing another minor version upgrade and that",
    "start": "1685190",
    "end": "1690600"
  },
  {
    "text": "meant we did two upgrades in the same week and some customers did notice this change the method of creating these",
    "start": "1690600",
    "end": "1698400"
  },
  {
    "text": "patch releases is pretty heavyweight as well we had to do it for every version that we were managing and it's a pretty slow process just to change the version",
    "start": "1698400",
    "end": "1705060"
  },
  {
    "text": "essentially of one container running across our cluster masters so when I'm currently working on making that more",
    "start": "1705060",
    "end": "1712050"
  },
  {
    "text": "easy to change of course we've got incredibly lucky here that a we were able to pull this out with no one",
    "start": "1712050",
    "end": "1718620"
  },
  {
    "text": "noticing and without Lee the presence of it all so that it wasn't leaked by someone else so we had to do",
    "start": "1718620",
    "end": "1725020"
  },
  {
    "text": "an expedited rollout and primarily that this was discovered by our own internal",
    "start": "1725020",
    "end": "1730120"
  },
  {
    "text": "team so we weren't rushing to catch up",
    "start": "1730120",
    "end": "1734910"
  },
  {
    "start": "1736000",
    "end": "1736000"
  },
  {
    "text": "so finally when we're not putting out fires what are we doing so right now",
    "start": "1736110",
    "end": "1741940"
  },
  {
    "text": "we're very focused on making sure that the support model for gke scales as the",
    "start": "1741940",
    "end": "1748210"
  },
  {
    "text": "products continues to experience exponential growth in usage one of the",
    "start": "1748210",
    "end": "1753790"
  },
  {
    "text": "ways that we're looking into doing that is implementing a problem fingerprinting",
    "start": "1753790",
    "end": "1759130"
  },
  {
    "text": "tool that will identify a smoking gun from panics and stack traces for",
    "start": "1759130",
    "end": "1767040"
  },
  {
    "text": "reoccurring issues across the fleet and as discussed earlier our standard repair",
    "start": "1767040",
    "end": "1772600"
  },
  {
    "text": "procedure is an entire master vm recreate so while this is very effective it's a very big hammer and we want to do",
    "start": "1772600",
    "end": "1780850"
  },
  {
    "text": "this in the smarter way and really pinpoint the individual failing components and we also want to make the",
    "start": "1780850",
    "end": "1788380"
  },
  {
    "text": "resizing of the master vm a little bit smarter too we're looking into using the cloud",
    "start": "1788380",
    "end": "1794800"
  },
  {
    "text": "recommendations API that comes with Google cloud platform that gives suggestions on the CPU and memory usage",
    "start": "1794800",
    "end": "1802810"
  },
  {
    "text": "and rather than using a heuristic that's based on the number of nodes we're also",
    "start": "1802810",
    "end": "1809140"
  },
  {
    "text": "working on the private clusters product which launched in beta recently so the",
    "start": "1809140",
    "end": "1815290"
  },
  {
    "text": "challenge here for sorries is to again provide a similar gke support experience",
    "start": "1815290",
    "end": "1820690"
  },
  {
    "text": "when a customer's cluster is not visible on the public Internet and finally we",
    "start": "1820690",
    "end": "1827770"
  },
  {
    "text": "really we're looking into how we can contribute to cluster API so this is an",
    "start": "1827770",
    "end": "1833740"
  },
  {
    "text": "open source kubernetes project and it's an initiative to introduce some management api's to enable common",
    "start": "1833740",
    "end": "1841540"
  },
  {
    "text": "cluster lifecycle operations such as install upgrade repair delete for",
    "start": "1841540",
    "end": "1847690"
  },
  {
    "text": "masters and and I think sre involvement here would involve us taking some of the things",
    "start": "1847690",
    "end": "1854370"
  },
  {
    "text": "we've learnt about machine management and trying to apply that in open source with the view that we can use things",
    "start": "1854370",
    "end": "1862140"
  },
  {
    "text": "like the cluster well the cost API and things like our internal tools such as",
    "start": "1862140",
    "end": "1867600"
  },
  {
    "text": "the monitoring server instead so if you're interested in any of these and please do come and talk to us after",
    "start": "1867600",
    "end": "1874530"
  },
  {
    "text": "we'll hang around for a while after the talk and you can find us at the Google cloud booth later and we'd also really",
    "start": "1874530",
    "end": "1881340"
  },
  {
    "text": "be interested to hear about any sharp edges or outages and learning",
    "start": "1881340",
    "end": "1886830"
  },
  {
    "text": "experiences you've experienced and perhaps howhow you guys do alerting a",
    "start": "1886830",
    "end": "1891960"
  },
  {
    "text": "modern Trinity so and thanks for listening and if we have time we can take a couple of questions or we don't",
    "start": "1891960",
    "end": "1897540"
  },
  {
    "text": "want to keep you from from lunch [Applause]",
    "start": "1897540",
    "end": "1905960"
  }
]