[
  {
    "text": "thank you for turning up at uh nearly half five on uh on the second day of the",
    "start": "599",
    "end": "5799"
  },
  {
    "text": "conference we appreciate it uh yeah we're going to go through some of the",
    "start": "5799",
    "end": "11040"
  },
  {
    "text": "Sig off scaling updates uh over the past uh six months since last coupon uh as",
    "start": "11040",
    "end": "17520"
  },
  {
    "text": "well as some feature highlights um of improvement work that we've gone through uh we'll give a quick introduction to",
    "start": "17520",
    "end": "24640"
  },
  {
    "text": "the Sig for those who aren't familiar uh the sub projects that we uh are responsible for what they're the what",
    "start": "24640",
    "end": "31599"
  },
  {
    "text": "they do the problems they cover uh Etc uh we will cover the adoption of",
    "start": "31599",
    "end": "38920"
  },
  {
    "text": "carpenter uh one of the the big things that's been happening over the past uh sort of time since the last cucon uh",
    "start": "38920",
    "end": "46360"
  },
  {
    "text": "some updates on cluster scaler and uh also quick update on vpa uh future work",
    "start": "46360",
    "end": "53719"
  },
  {
    "text": "that the Sig would like to uh do this is also your opportunity to get involved in the Sig uh and yeah further areas where",
    "start": "53719",
    "end": "61840"
  },
  {
    "text": "you can help um so in terms of the the areas covered by the S um and this is where",
    "start": "61840",
    "end": "69400"
  },
  {
    "text": "Sig naming might get a bit confusing because you might look at Sig autoscaling and then also Sig uh",
    "start": "69400",
    "end": "75680"
  },
  {
    "text": "scalability and think what's the difference there um so Sig Al scaling is",
    "start": "75680",
    "end": "80759"
  },
  {
    "text": "uh primarily focused on scaling of clusters and nodes but and workloads um",
    "start": "80759",
    "end": "86560"
  },
  {
    "text": "so we have a few different sub projects that are uh intended to address these in different dimensions different problem",
    "start": "86560",
    "end": "92759"
  },
  {
    "text": "spaces um so in terms of the scaling of clusters and nodes that's primarily",
    "start": "92759",
    "end": "97920"
  },
  {
    "text": "cluster autoscaler has historically been the the project that the seon that covered this um we are also in the",
    "start": "97920",
    "end": "104439"
  },
  {
    "text": "process of adopting uh adopting Carpenter uh this is a project you might have heard mentioned if you were",
    "start": "104439",
    "end": "109799"
  },
  {
    "text": "previously in the uh in the previous talk in this room um it's been talked about past uh cuon as well this this",
    "start": "109799",
    "end": "117719"
  },
  {
    "text": "also aims it's another project aiming to solve the problem of scaling clusters and nodes and in terms of scaling",
    "start": "117719",
    "end": "123960"
  },
  {
    "text": "workloads we've got horizontal scaling vertical scaling uh multi-dimensional",
    "start": "123960",
    "end": "129599"
  },
  {
    "text": "scaling um as well as a couple of other projects the balancer and addon resizer",
    "start": "129599",
    "end": "134760"
  },
  {
    "text": "and I'll give you a quick overview of each of them um so cluster autoscaler it's responsible for monitoring for",
    "start": "134760",
    "end": "141360"
  },
  {
    "text": "unschedulable pods Provisions nodes in response um I'll cover this in a bit",
    "start": "141360",
    "end": "147120"
  },
  {
    "text": "more detail on how the how cluster a scale and carp differ um when talking about the adoption of that uh but it",
    "start": "147120",
    "end": "154040"
  },
  {
    "text": "also uh is responsible for removing underutilized nodes uh respecting pdps and constraints but you know everyone's",
    "start": "154040",
    "end": "160959"
  },
  {
    "text": "focused on uh making their clusters more sustainable making their clusters cost",
    "start": "160959",
    "end": "166680"
  },
  {
    "text": "less um and that's that's how cluster ult scalars to help there so you you have your workloads scaling down uh",
    "start": "166680",
    "end": "173400"
  },
  {
    "text": "times a low traffic your nodes become underly utilized and cluster scaler takes care of that for you um and it",
    "start": "173400",
    "end": "179760"
  },
  {
    "text": "performs scheduling simulations based on the declared uh unschedulable pod requests so it looks at all the",
    "start": "179760",
    "end": "185599"
  },
  {
    "text": "unschedulable pods currently in the cluster figures out what those what resources all of those pods will need to",
    "start": "185599",
    "end": "192440"
  },
  {
    "text": "uh be satisfied including constraints based on them and then Provisions the nodes to meet those uh",
    "start": "192440",
    "end": "199200"
  },
  {
    "text": "requirements Carpenter um Works quite similarly in some ways uh",
    "start": "199200",
    "end": "205280"
  },
  {
    "text": "different in others but it also monitors for schedulable pods provision nodes and response um it's slightly different in",
    "start": "205280",
    "end": "212200"
  },
  {
    "text": "the way it uh performs uh scal down of underutilized nodes it's also able to uh",
    "start": "212200",
    "end": "218760"
  },
  {
    "text": "look at nodes where if it removed all the pods onto other uh a different node",
    "start": "218760",
    "end": "225480"
  },
  {
    "text": "that it brings up it's able to do that so if it thinks oh I can replace this one big node with a smaller cheaper node",
    "start": "225480",
    "end": "233560"
  },
  {
    "text": "um it's capable of doing that um and it supports standard scheduling constraints for node selection as well",
    "start": "233560",
    "end": "240400"
  },
  {
    "text": "uh in terms of workload scaling horizontal pod autoscaler is generally the one people are most familiar with um",
    "start": "240400",
    "end": "246560"
  },
  {
    "text": "it increases and decreases the desired replicas to um achieve targets um so you",
    "start": "246560",
    "end": "251680"
  },
  {
    "text": "can see a sort of example on the side uh where we're saying a t the target utilization of this service is is lower",
    "start": "251680",
    "end": "258919"
  },
  {
    "text": "than all the current pods and in this case the the HPA would scale up the desired number up um it can scale on a",
    "start": "258919",
    "end": "266880"
  },
  {
    "text": "number of different uh Dimensions so it's resource resource metrics CPU and memory um currently that defaults summed",
    "start": "266880",
    "end": "274880"
  },
  {
    "text": "across the Pod cover uh that in a bit more detail when we talk about future work we'd like to do um it can also look",
    "start": "274880",
    "end": "281840"
  },
  {
    "text": "at custom metrics that you can expose through a number of different uh methods",
    "start": "281840",
    "end": "288000"
  },
  {
    "text": "but so things like QPS uh going into p uh going into pods um and then finally external",
    "start": "288000",
    "end": "294800"
  },
  {
    "text": "metrics so these are things that are not paod metrics so if you're looking at at a workload consuming from a q for",
    "start": "294800",
    "end": "302000"
  },
  {
    "text": "instance in your cloud provider of choice you can choose to scale on that so if you see the Q length increasing",
    "start": "302000",
    "end": "307280"
  },
  {
    "text": "you can uh increase the number of replicas um and you can have uh configurable scaling behaviors as well",
    "start": "307280",
    "end": "313840"
  },
  {
    "text": "so you can have fast scale up slow scale down you can uh configure that per",
    "start": "313840",
    "end": "321840"
  },
  {
    "text": "workload uh vertical pod autoscaler um so this is this is aiming to right siize",
    "start": "321840",
    "end": "327319"
  },
  {
    "text": "your pods in terms of their CPU memory requests um so you're looking at the",
    "start": "327319",
    "end": "333840"
  },
  {
    "text": "Historical resource usage of a pod and saying that that pod has always only",
    "start": "333840",
    "end": "339759"
  },
  {
    "text": "used 25% of the CPU that it's got I'm going to decrease the amount of CPU that",
    "start": "339759",
    "end": "345280"
  },
  {
    "text": "that that workload is requesting um and then uh bring that down um and it's",
    "start": "345280",
    "end": "350360"
  },
  {
    "text": "based on resource data so CPU usage memory usage and um events as well so if",
    "start": "350360",
    "end": "355600"
  },
  {
    "text": "you've got a one where you're looking say a Prometheus instance that you're uh having scale up and down it will also",
    "start": "355600",
    "end": "363080"
  },
  {
    "text": "monitor for those zo events and take that into account um so it recommends those pod",
    "start": "363080",
    "end": "368880"
  },
  {
    "text": "sizes to keep their real usage well within the uh requested pod capacity uh and you're able to run it in a",
    "start": "368880",
    "end": "375440"
  },
  {
    "text": "effectively dry run mode um so that you can look at those recommendations what the vpa would suggest and then evaluate",
    "start": "375440",
    "end": "382840"
  },
  {
    "text": "whether you want to apply them um multi-dial podor scaler so this is uh",
    "start": "382840",
    "end": "390720"
  },
  {
    "text": "this allows the combination of HPA and vpa scaling on a single workload so historically uh it was recommended",
    "start": "390720",
    "end": "397520"
  },
  {
    "text": "against running uh the HPA and vpa looking at the same resource metrics uh",
    "start": "397520",
    "end": "402639"
  },
  {
    "text": "that could result in uh slightly odd edge cases where you ended up with a lot",
    "start": "402639",
    "end": "407759"
  },
  {
    "text": "of very small replicas because you the vpa was uh the HPA would scale a",
    "start": "407759",
    "end": "414360"
  },
  {
    "text": "workload out because you were using a lot of CPU and then the vpa once that uh C CPU started falling would start",
    "start": "414360",
    "end": "420680"
  },
  {
    "text": "scaling it uh scaling the resources down on those pods um it's also designed from",
    "start": "420680",
    "end": "426280"
  },
  {
    "text": "the ground up to be extensible so it'll allow users to insert their own recommender so if you're wanting to",
    "start": "426280",
    "end": "432160"
  },
  {
    "text": "encapsulate some business logic because you've got a private pricing plan or something like that uh you will be able",
    "start": "432160",
    "end": "437840"
  },
  {
    "text": "to do that you you would write your own uh recommender and then just have the H",
    "start": "437840",
    "end": "443960"
  },
  {
    "text": "multi-dimensional pod aosc call like that um and you might have heard this refer to uh during a talk earlier uh",
    "start": "443960",
    "end": "451560"
  },
  {
    "text": "sorry yesterday um talking about that project uh balancer so this is uh",
    "start": "451560",
    "end": "457919"
  },
  {
    "text": "intended to solve a problem of like uh how how do you ensure that you're getting equal spreading of uh pods if",
    "start": "457919",
    "end": "464319"
  },
  {
    "text": "you're running a cluster across three zones in a region how do you ensure that those pods are um uh equally spread and",
    "start": "464319",
    "end": "472000"
  },
  {
    "text": "remain balanced as you start scaling up and down um and other business problems",
    "start": "472000",
    "end": "477560"
  },
  {
    "text": "so if you're looking to say be cost effective and using preemptible or spot",
    "start": "477560",
    "end": "482599"
  },
  {
    "text": "instances um but you still want some of your pods on uh non preemptible",
    "start": "482599",
    "end": "489400"
  },
  {
    "text": "instances in case that you start losing that capacity how how do you do that um",
    "start": "489400",
    "end": "495520"
  },
  {
    "text": "and you know how do you consume node types where you've got a negotiated r or discount first um and then also ver",
    "start": "495520",
    "end": "502840"
  },
  {
    "text": "perform the other forms of aut scaling I've already mentioned and make the ca work well with them um and finally add on resizer so",
    "start": "502840",
    "end": "510199"
  },
  {
    "text": "this is this is the simplest project we have in the the Sig quite some way it's it's vertically scaling a Singleton pod",
    "start": "510199",
    "end": "517279"
  },
  {
    "text": "proportionally to the scale of the cluster so it's useful for components where the resource used needs scale",
    "start": "517279",
    "end": "523080"
  },
  {
    "text": "linearly or exponentially potentially with the size of the cluster so things like metric server Etc and you can use",
    "start": "523080",
    "end": "528839"
  },
  {
    "text": "nodes or containers As the metric to drive this scaling uh so yeah those are those are the",
    "start": "528839",
    "end": "536000"
  },
  {
    "text": "projects we own um I mentioned that the Sig has now Tak taken uh is taking",
    "start": "536000",
    "end": "541040"
  },
  {
    "text": "ownership of carpenter so the Sig has been discussing this with uh Carpenter the carpenter uh",
    "start": "541040",
    "end": "548959"
  },
  {
    "text": "maintainers for quite some time so it's an alternative approach to Cluster and node Auto scaling from cluster",
    "start": "548959",
    "end": "554600"
  },
  {
    "text": "autoscaler it's been developed in the open from the beginning um under aws's stewardship to this point um it's",
    "start": "554600",
    "end": "561880"
  },
  {
    "text": "deliberately vendor neutral however so it's it's been designed um from the start to be vendor neutral allow Prov",
    "start": "561880",
    "end": "569200"
  },
  {
    "text": "wider implementations to be developed as required um and over the past year we've",
    "start": "569200",
    "end": "574320"
  },
  {
    "text": "been discussing okay how do how do we adopt this into the Sig without uh resulting in confusion for end users we",
    "start": "574320",
    "end": "582959"
  },
  {
    "text": "don't want end users turning up and going oh I don't understand which of these projects I would choose uh why why",
    "start": "582959",
    "end": "588680"
  },
  {
    "text": "do you have two two solutions to this uh two different solutions to the same problem um and also to enable uh",
    "start": "588680",
    "end": "596880"
  },
  {
    "text": "principle leas surprise so where are the two things have the same behaviors we want to make it as easy as possible for",
    "start": "596880",
    "end": "603200"
  },
  {
    "text": "people to configure it and have it work the same across both um it currently has",
    "start": "603200",
    "end": "608600"
  },
  {
    "text": "AWS unsurprisingly but also your implementations this was announced uh on",
    "start": "608600",
    "end": "614200"
  },
  {
    "text": "Monday but made public uh the repo has been made public yesterday so you can go and check out as yours uh implementation",
    "start": "614200",
    "end": "621040"
  },
  {
    "text": "of this as well um and we've agreed on the adoption now of the the project um",
    "start": "621040",
    "end": "626279"
  },
  {
    "text": "so this is the core repo this is library that then is consumed by these",
    "start": "626279",
    "end": "631560"
  },
  {
    "text": "implementations by AWS is your other Cloud providers potentially going forward um so that that repo is in the",
    "start": "631560",
    "end": "638959"
  },
  {
    "text": "process of being migrated over to kubernetes infrastructure and then uh s",
    "start": "638959",
    "end": "644600"
  },
  {
    "text": "government the one thing to note here is it's not replacing cluster autoscaler",
    "start": "644600",
    "end": "649639"
  },
  {
    "text": "both projects will continue to be managed by the Sig it is an alternative approach to uh addressing the same",
    "start": "649639",
    "end": "655839"
  },
  {
    "text": "problem so there's there are some differences uh um in their approaches um",
    "start": "655839",
    "end": "661560"
  },
  {
    "text": "but there's also areas where the two projects have different methods annotations to achieve the same name um",
    "start": "661560",
    "end": "667560"
  },
  {
    "text": "so cluster autoscaler is node group focused um so when it's uh scaling",
    "start": "667560",
    "end": "672639"
  },
  {
    "text": "things up and down it's looking at uh a node group uh in its internal",
    "start": "672639",
    "end": "677839"
  },
  {
    "text": "implementation and assuming that if it scales it up it will get a new thing that looks like the existing things in",
    "start": "677839",
    "end": "684200"
  },
  {
    "text": "that node group um the majority of cloud providers are also baked in we've currently got 27 cloud provider",
    "start": "684200",
    "end": "690480"
  },
  {
    "text": "implementations bed in as well as an external grpc cloud provider um to allow",
    "start": "690480",
    "end": "695760"
  },
  {
    "text": "outband extension um there was a talk earlier today talking about how this had been used with uh virtual cuet um and it",
    "start": "695760",
    "end": "704240"
  },
  {
    "text": "but it also allows again that encapsulation of business logic if you were wanting to create a cloud provider",
    "start": "704240",
    "end": "710160"
  },
  {
    "text": "implementation uh where you were had some some business logic around what nodes you wanted to prepare there um it",
    "start": "710160",
    "end": "717639"
  },
  {
    "text": "also has uh highlight configurable scaling behaviors via thresholds and scam and falls in the",
    "start": "717639",
    "end": "723200"
  },
  {
    "text": "primary um Carpenter however uh creates specific nodes so you create uh the the",
    "start": "723200",
    "end": "731639"
  },
  {
    "text": "custom resources that drive it and then it uses that to decide what nodes to",
    "start": "731639",
    "end": "737920"
  },
  {
    "text": "create within constraints that you provide it and create specific nodes rather than uh manipulating an O",
    "start": "737920",
    "end": "745360"
  },
  {
    "text": "group um as I said the cor is core is just released as a library to be",
    "start": "745360",
    "end": "750680"
  },
  {
    "text": "consumed by cloud provider implementations so the seg is not going to be publishing images of the carpenter",
    "start": "750680",
    "end": "757440"
  },
  {
    "text": "core it's just a library that we can then be consumed by um those creating",
    "start": "757440",
    "end": "764120"
  },
  {
    "text": "implementations um and as I said AWS and Z already have those implementations there um it's also able to perform",
    "start": "764120",
    "end": "772199"
  },
  {
    "text": "consolidation downscaling so if you've got all of your pods currently on a",
    "start": "772199",
    "end": "777240"
  },
  {
    "text": "large expensive node your workload has then scaled down um",
    "start": "777240",
    "end": "783079"
  },
  {
    "text": "or has uh say been terminated and then those pods can be removed onto the spare",
    "start": "783079",
    "end": "789760"
  },
  {
    "text": "capacity that are is there on existing nodes in the cluster but maybe there's",
    "start": "789760",
    "end": "795440"
  },
  {
    "text": "still a few pods that wouldn't be able to be moved and it will evaluate going okay I can but I can place those all on",
    "start": "795440",
    "end": "802160"
  },
  {
    "text": "a smaller uh cheaper node it will do that for you um and it also has the",
    "start": "802160",
    "end": "807440"
  },
  {
    "text": "concept of drift detection um so those of you who are responsible for managing clusters might uh have to say roll out",
    "start": "807440",
    "end": "815440"
  },
  {
    "text": "Ami updates every month Carpenter is able to uh perform drift detection see",
    "start": "815440",
    "end": "821560"
  },
  {
    "text": "that oh a new version of that Ami a new version of that Ami has been released I can do back replace that in the",
    "start": "821560",
    "end": "828399"
  },
  {
    "text": "background um and there's there's uh it can also do this via TTL so say for",
    "start": "828399",
    "end": "833680"
  },
  {
    "text": "instance you know that your nodes for whatever you want to rotate them every 24 hours just in case uh you can do that",
    "start": "833680",
    "end": "841480"
  },
  {
    "text": "as well um so the the next steps for the adoption however uh we're working on",
    "start": "841480",
    "end": "847720"
  },
  {
    "text": "adopting the existing Carpenter processes under kubernetes governance processes so Carpenter for a long time",
    "start": "847720",
    "end": "853639"
  },
  {
    "text": "has a public office hours they've been run uh completely open however we need",
    "start": "853639",
    "end": "859120"
  },
  {
    "text": "to migrate that into the existing sort of kubernetes Sig setup to ensure that we're meeting all the requirements of",
    "start": "859120",
    "end": "865759"
  },
  {
    "text": "The Wider kubernetes governance um and we've also got uh agreed work on",
    "start": "865759",
    "end": "871160"
  },
  {
    "text": "continuing to standardize across the node of to scaling projects um so there's a link to Doc there um that",
    "start": "871160",
    "end": "877199"
  },
  {
    "text": "Jonathan from AWS has put together and agreed with the Sig um that is basically",
    "start": "877199",
    "end": "882839"
  },
  {
    "text": "setting out the short medium and longer term work that we want to do uh with the carpenter community and uh s scaling",
    "start": "882839",
    "end": "890680"
  },
  {
    "text": "Community to move that forward okay thank you guys so as guys",
    "start": "890680",
    "end": "898519"
  },
  {
    "text": "said said cluster autoscaler is not going anywhere and it is also developed",
    "start": "898519",
    "end": "903600"
  },
  {
    "text": "in the meantime as we are adopting Carpenter and new features are coming there for example one of the",
    "start": "903600",
    "end": "911240"
  },
  {
    "text": "long-standing problems of clust autoscaler was the performance of uh",
    "start": "911240",
    "end": "917160"
  },
  {
    "text": "node draining so in cluster some of the nodes after it sometime may be not that",
    "start": "917160",
    "end": "924839"
  },
  {
    "text": "much utilized as they used to be before and cluster scaler will try to compact",
    "start": "924839",
    "end": "930800"
  },
  {
    "text": "them meaning move existing parts to some other nodes and delete the just emptied",
    "start": "930800",
    "end": "937759"
  },
  {
    "text": "uh notes however for quite a long time this was not a parallel process of",
    "start": "937759",
    "end": "945000"
  },
  {
    "text": "cluster autoscaler was deleting one note at a time and even if it tried hard to",
    "start": "945000",
    "end": "950040"
  },
  {
    "text": "be as fast as possible the limitation of a single threat was hitting larger",
    "start": "950040",
    "end": "955759"
  },
  {
    "text": "clusters however in 126 we finally introduced parallel drain process uh to",
    "start": "955759",
    "end": "964079"
  },
  {
    "text": "accelerate it now cluster autoscaler will be able to delete multiple nodes at a time and uh do it quite quickly while",
    "start": "964079",
    "end": "974360"
  },
  {
    "text": "maintaining a graceful termination period for the pods and making sure that all the pods have a",
    "start": "974360",
    "end": "981440"
  },
  {
    "text": "place to go uh so far that was an optional Behavior but with 128",
    "start": "981440",
    "end": "989120"
  },
  {
    "text": "and that is going to be uh default so this thing will hopefully uh",
    "start": "989120",
    "end": "996600"
  },
  {
    "text": "close this longstanding Gap in cluster autoscaler performance and you will get",
    "start": "996600",
    "end": "1002680"
  },
  {
    "text": "your cluster shrink quicker saving you money uh in uh 127 we also added",
    "start": "1002680",
    "end": "1009959"
  },
  {
    "text": "multiple improvements to this process so if you tested in 126 it should behave",
    "start": "1009959",
    "end": "1015759"
  },
  {
    "text": "much more robust and quicker in the newest",
    "start": "1015759",
    "end": "1021040"
  },
  {
    "text": "releases okay uh ongoing work is also like handling uh demon set properly so while",
    "start": "1021040",
    "end": "1031000"
  },
  {
    "text": "draining node it is important that we remove pods in some reasonable order on",
    "start": "1031000",
    "end": "1040079"
  },
  {
    "text": "the note there might be some Demon running that provide you with uh lock pushing capability it would be very bad",
    "start": "1040079",
    "end": "1048038"
  },
  {
    "text": "if we delete this uh demon set pods before deleting the actual nodes in that",
    "start": "1048039",
    "end": "1054880"
  },
  {
    "text": "case the logs that were written in the very last moments of the P life cycle",
    "start": "1054880",
    "end": "1060080"
  },
  {
    "text": "could be lost however uh we did some improvements there and now",
    "start": "1060080",
    "end": "1066840"
  },
  {
    "text": "it is landing in 129 and we will be doing no draining based on the priority",
    "start": "1066840",
    "end": "1074120"
  },
  {
    "text": "so to we will make sure that every single bit of logging that you put in uh",
    "start": "1074120",
    "end": "1081159"
  },
  {
    "text": "your application while being shut down is correctly pushed and we will you will",
    "start": "1081159",
    "end": "1086960"
  },
  {
    "text": "not lose any data one big dor that we started",
    "start": "1086960",
    "end": "1094039"
  },
  {
    "text": "in SE AOS scaling was a provisioning request it is an API to ask cluster",
    "start": "1094039",
    "end": "1102320"
  },
  {
    "text": "autoscaler or actually any other uh Autos scaling that might be in place for",
    "start": "1102320",
    "end": "1107720"
  },
  {
    "text": "example carpet to ensure that there is space for some",
    "start": "1107720",
    "end": "1113880"
  },
  {
    "text": "set of pts currently cluster of the scaler as a carpenter works with pending pots so in",
    "start": "1113880",
    "end": "1119960"
  },
  {
    "text": "order to trigger scale up you need to create the SPs in the first place that",
    "start": "1119960",
    "end": "1125120"
  },
  {
    "text": "may be good for uh serving workloads that are okay with getting half of the",
    "start": "1125120",
    "end": "1132640"
  },
  {
    "text": "needed pods on scale up but it is bad for things like machine learning which",
    "start": "1132640",
    "end": "1138840"
  },
  {
    "text": "require all pods to be present in order to start their computation having only a",
    "start": "1138840",
    "end": "1145880"
  },
  {
    "text": "fraction of them will cause more harm than good the workload will not start uh",
    "start": "1145880",
    "end": "1153919"
  },
  {
    "text": "in that case and you will be paying on your cloud provider for this half provisioned",
    "start": "1153919",
    "end": "1161880"
  },
  {
    "text": "resources and well it will raise your uh monthly bills however with provisioning",
    "start": "1161880",
    "end": "1168760"
  },
  {
    "text": "request we are trying to address this issue so we establish an API for which",
    "start": "1168760",
    "end": "1174240"
  },
  {
    "text": "you will tell us how many parts and in what shape you would like to",
    "start": "1174240",
    "end": "1180159"
  },
  {
    "text": "provision this thing will be uh done by provisioning request you create a",
    "start": "1180159",
    "end": "1185919"
  },
  {
    "text": "provisioning request you put play you put count and cluster autoscaler",
    "start": "1185919",
    "end": "1191559"
  },
  {
    "text": "responsibility is to let you know when cloud provider gives a green light for",
    "start": "1191559",
    "end": "1198520"
  },
  {
    "text": "the notes needed for these spots uh will come so you create provisioning request",
    "start": "1198520",
    "end": "1205600"
  },
  {
    "text": "you wait only uh when Cloud providers uh cloud provider says it is okay I've got",
    "start": "1205600",
    "end": "1212400"
  },
  {
    "text": "capacity I will give you this 200 of the most expensive",
    "start": "1212400",
    "end": "1217760"
  },
  {
    "text": "gpus uh when the cluster when cloud provider says that we will cluster autoscaler will change the status of",
    "start": "1217760",
    "end": "1224880"
  },
  {
    "text": "provisioning request and you will know that you could start your uh buge workload because the capacity will be",
    "start": "1224880",
    "end": "1233720"
  },
  {
    "text": "there uh so there there is a question how and what we will be actually",
    "start": "1234960",
    "end": "1241679"
  },
  {
    "text": "providing and this is defined by a think called provisioning class so right now",
    "start": "1241679",
    "end": "1248840"
  },
  {
    "text": "there are three classes that are there or are shortly coming the first one is",
    "start": "1248840",
    "end": "1255640"
  },
  {
    "text": "to just ask cluster AOS scaler whether it has currently the capacity for a",
    "start": "1255640",
    "end": "1261720"
  },
  {
    "text": "particular set of Parts uh cluster autoscaler will say yeah I have it or no I don't have it",
    "start": "1261720",
    "end": "1269320"
  },
  {
    "text": "based on what is there uh without trying to scale the things up then we will have",
    "start": "1269320",
    "end": "1275279"
  },
  {
    "text": "a generic scale up that will do its best to give you some level of atomicity and",
    "start": "1275279",
    "end": "1283120"
  },
  {
    "text": "cost control with this scale up however this uh",
    "start": "1283120",
    "end": "1288559"
  },
  {
    "text": "ER type of provisioning is not integrated with any goodies that your",
    "start": "1288559",
    "end": "1293840"
  },
  {
    "text": "cloud provider might have for example an ability to ask it directly whether it",
    "start": "1293840",
    "end": "1300400"
  },
  {
    "text": "could give you 100 uh notes or not so",
    "start": "1300400",
    "end": "1305799"
  },
  {
    "text": "why do we have this thing because we have as gu said 27 Cloud providers on",
    "start": "1305799",
    "end": "1311400"
  },
  {
    "text": "board and it will take time for them to integrate with the provisioning request",
    "start": "1311400",
    "end": "1317080"
  },
  {
    "text": "and modify their integration so that it is handled the best way possible in the meantime we will try to do our best to",
    "start": "1317080",
    "end": "1324679"
  },
  {
    "text": "give you some level of atomicity some level of control over provisioning and uh for uh Google Cloud",
    "start": "1324679",
    "end": "1333799"
  },
  {
    "text": "we already have uh this type of uh functionality where you can ask for",
    "start": "1333799",
    "end": "1341080"
  },
  {
    "text": "resources and you will get them in atomic way so we are TR blazing a with Google however we will be more than",
    "start": "1341080",
    "end": "1347679"
  },
  {
    "text": "happy to accept contribution from any Cloud we are integrated in uh so that",
    "start": "1347679",
    "end": "1353760"
  },
  {
    "text": "the users of cluster autoscalers get the uh batch AIML friendly behavior in their",
    "start": "1353760",
    "end": "1361480"
  },
  {
    "text": "cluster autoscaling and because now we have Carpenter on board hopefully this",
    "start": "1361480",
    "end": "1366760"
  },
  {
    "text": "functionality will also come there okay and uh B workload are not the",
    "start": "1366760",
    "end": "1374240"
  },
  {
    "text": "only use case there G is not the only use case it is Al Al useful for making the scale up faster if you",
    "start": "1374240",
    "end": "1382320"
  },
  {
    "text": "know already that you will be creating 10,000 pots for whatever reason you could Issue provisioning request and uh",
    "start": "1382320",
    "end": "1388919"
  },
  {
    "text": "while creating pods the provisioning request will let autoscaler know that these pods are coming in that quantity",
    "start": "1388919",
    "end": "1394720"
  },
  {
    "text": "and cluster autoscaler can do a single scale up instead of doing like a step function which will take more",
    "start": "1394720",
    "end": "1402159"
  },
  {
    "text": "time uh okay so there are also couple of minor improvements coming to Cluster",
    "start": "1403559",
    "end": "1409279"
  },
  {
    "text": "autoscaler cluster autoscaler will not do a partial scale up if calculating scale UPS helping C pods will take too",
    "start": "1409279",
    "end": "1415480"
  },
  {
    "text": "long often times cluster AOS scal is bombarded with huge amount of",
    "start": "1415480",
    "end": "1420799"
  },
  {
    "text": "parts and all of the computation can evaluation of simulation take too long",
    "start": "1420799",
    "end": "1425919"
  },
  {
    "text": "and uh cluster liveness probe fail and due to that cluster scale crashes and",
    "start": "1425919",
    "end": "1432960"
  },
  {
    "text": "it's restarted and getting into some unpleasant Loop now we have more control over execu ution and we limit how much",
    "start": "1432960",
    "end": "1439720"
  },
  {
    "text": "competition are done so that we don't crash we don't uh uh break Under",
    "start": "1439720",
    "end": "1446320"
  },
  {
    "text": "Pressure maybe not the best decision will be made but we will move forward and cluster autoscaler will work with",
    "start": "1446320",
    "end": "1454559"
  },
  {
    "text": "huge amounts of pots coming in okay vertical pot autoscaler finally",
    "start": "1454559",
    "end": "1461640"
  },
  {
    "text": "got is 1.0.0 release uh after a couple years and new",
    "start": "1461640",
    "end": "1468279"
  },
  {
    "text": "additional features are the control over how we enhance and expand memory requests",
    "start": "1468279",
    "end": "1476799"
  },
  {
    "text": "after we notice uh out of memory exceptions happening and we also added",
    "start": "1476799",
    "end": "1482480"
  },
  {
    "text": "their uh ability to specify that you would like uh to change CPU in integer",
    "start": "1482480",
    "end": "1490600"
  },
  {
    "text": "increment so that you have integer number of CPUs only by default uh",
    "start": "1490600",
    "end": "1496399"
  },
  {
    "text": "vertical put AO scaler tries to give you uh the number that is as close to the",
    "start": "1496399",
    "end": "1501520"
  },
  {
    "text": "actual usage sometimes it results in fractional numbers which are uh okay for",
    "start": "1501520",
    "end": "1507679"
  },
  {
    "text": "most of the users but there are use cases where you would like to have uh integer number of uh",
    "start": "1507679",
    "end": "1514279"
  },
  {
    "text": "CPUs we have couple of additional efforts in progress uh one of them is uh",
    "start": "1514279",
    "end": "1521480"
  },
  {
    "text": "to control vpa uh eviction Behavior based on the scaling Direction so for example SC only",
    "start": "1521480",
    "end": "1528279"
  },
  {
    "text": "up not scale down always consume more uh don't and have less disruptions that",
    "start": "1528279",
    "end": "1535720"
  },
  {
    "text": "didn't make to the 1.0 cut but it's already merged so it should get into very very next release and the second",
    "start": "1535720",
    "end": "1543760"
  },
  {
    "text": "thing that we have been dreaming about for quite a long is using the in place",
    "start": "1543760",
    "end": "1550919"
  },
  {
    "text": "uh pot upgrades so previously whenever we wanted to change the Potter Rec Quest",
    "start": "1550919",
    "end": "1558159"
  },
  {
    "text": "we had to restart the pots which was okay for some of the workloads but some of the workloads could not tolerate it",
    "start": "1558159",
    "end": "1564559"
  },
  {
    "text": "and it was very disruptive with it with the pot resize support finally",
    "start": "1564559",
    "end": "1570919"
  },
  {
    "text": "getting to kubernetes we hope to have it uh implemented in vertical Port of the",
    "start": "1570919",
    "end": "1577279"
  },
  {
    "text": "scalar soon so okay so future work",
    "start": "1577279",
    "end": "1584000"
  },
  {
    "text": "uh what we have there so if you want to help us or if you are interested in what",
    "start": "1584000",
    "end": "1589080"
  },
  {
    "text": "is coming close uh we should have container resource metrics Auto scaling",
    "start": "1589080",
    "end": "1596120"
  },
  {
    "text": "shortly mve to GA we should finally have scaling up from zero in HPA based on",
    "start": "1596120",
    "end": "1602880"
  },
  {
    "text": "external metrics uh we uh hope to have",
    "start": "1602880",
    "end": "1609640"
  },
  {
    "text": "uh this in place update support in uh vpa and finally land the",
    "start": "1609640",
    "end": "1617679"
  },
  {
    "text": "multidimensional po AOS scaler implementation so this available for you to",
    "start": "1617679",
    "end": "1623080"
  },
  {
    "text": "try okay thanks uh are there any questions and before we proceed to the",
    "start": "1623080",
    "end": "1628440"
  },
  {
    "text": "questions we do have a weekly s meeting every Monday that's 8 a.m. U Chicago",
    "start": "1628440",
    "end": "1637559"
  },
  {
    "text": "time s remember 6 4 P.M European time",
    "start": "1637559",
    "end": "1642679"
  },
  {
    "text": "and you are more than welcome to come to this meeting let us know more about your problems with Autos scaling your ideas",
    "start": "1642679",
    "end": "1649760"
  },
  {
    "text": "and we will we are there every week to help you with your uh Auto scaling",
    "start": "1649760",
    "end": "1656000"
  },
  {
    "text": "issues thank",
    "start": "1656000",
    "end": "1658720"
  },
  {
    "text": "you and we have a mic over there for",
    "start": "1663080",
    "end": "1668039"
  },
  {
    "text": "questions hello uh so I have a question about crust out scale and carpent like",
    "start": "1669360",
    "end": "1676039"
  },
  {
    "text": "difference I mean uh like is there any CER difference in",
    "start": "1676039",
    "end": "1682200"
  },
  {
    "text": "use cases between them I mean like when you how to know when to use which",
    "start": "1682200",
    "end": "1691320"
  },
  {
    "text": "one we we are using CR scale country and but should we like move to Carpenter or",
    "start": "1691320",
    "end": "1699840"
  },
  {
    "text": "like stick to the CR scale how do you know that okay so first of all what",
    "start": "1699840",
    "end": "1705000"
  },
  {
    "text": "cloud are you on first question what cloud are you on",
    "start": "1705000",
    "end": "1710679"
  },
  {
    "text": "gcp sorry gcp gcp so yeah right now there is no implementation of carpenter",
    "start": "1710679",
    "end": "1718000"
  },
  {
    "text": "for gcp so cluster autoscaler is your way to proceed yep in future maybe there",
    "start": "1718000",
    "end": "1724120"
  },
  {
    "text": "will be implementation of uh uh uh GC of",
    "start": "1724120",
    "end": "1729440"
  },
  {
    "text": "Carpenter on gcp right now there is no Carpenter as SK set currently has uh two",
    "start": "1729440",
    "end": "1736200"
  },
  {
    "text": "integration uh AWS which has been there from the very very beginning because the project was",
    "start": "1736200",
    "end": "1741880"
  },
  {
    "text": "started by AWS and just announced Azure implementation so if you are not on any",
    "start": "1741880",
    "end": "1747679"
  },
  {
    "text": "of these clouds cluster autoscaler is uh the way to go if you are on any of these",
    "start": "1747679",
    "end": "1754519"
  },
  {
    "text": "clouds then you have to consider how you uh manage your nodes what functionality",
    "start": "1754519",
    "end": "1760440"
  },
  {
    "text": "do you want to have do you want to have node groups and have this uh uh behavior",
    "start": "1760440",
    "end": "1767000"
  },
  {
    "text": "that has been kubernetes for a while where you have you know groups and you scale",
    "start": "1767000",
    "end": "1774320"
  },
  {
    "text": "groups up and down or you want to have completely customized nodes and you have",
    "start": "1774320",
    "end": "1779440"
  },
  {
    "text": "want to have more control over what is being created and when then maybe",
    "start": "1779440",
    "end": "1784640"
  },
  {
    "text": "Carpenter is a choice however we are trying to make this uh projects uh",
    "start": "1784640",
    "end": "1792120"
  },
  {
    "text": "converge to some extent so that uh you you will be it will be easy to switch",
    "start": "1792120",
    "end": "1799760"
  },
  {
    "text": "from one to another assuming that both provide uh the implementation for your",
    "start": "1799760",
    "end": "1805880"
  },
  {
    "text": "Cloud so we are aligning the apis we are aligning annotations we are aligning the",
    "start": "1805880",
    "end": "1812039"
  },
  {
    "text": "namings like how do we call scal down or is it compaction def fragmentation or",
    "start": "1812039",
    "end": "1817840"
  },
  {
    "text": "whatever so we are trying to make it easier for you to understand what are",
    "start": "1817840",
    "end": "1823159"
  },
  {
    "text": "the difference and also easier for you to switch between Windows if any of them",
    "start": "1823159",
    "end": "1829200"
  },
  {
    "text": "more fits your needs at a given time So eventually it's the choice of how you",
    "start": "1829200",
    "end": "1835039"
  },
  {
    "text": "want to manage it's hopefully not the choice of like that one is worse or better than the other we we have both of",
    "start": "1835039",
    "end": "1842279"
  },
  {
    "text": "our new projects and the old one as well so uh we will be developing both and",
    "start": "1842279",
    "end": "1849120"
  },
  {
    "text": "it's for you to choose how you want to manage stuff and that's that that's kind",
    "start": "1849120",
    "end": "1854880"
  },
  {
    "text": "of like the main difference okay in PR principle they the both project do more",
    "start": "1854880",
    "end": "1860279"
  },
  {
    "text": "or less the same they create nodes for uh pending pods and delete nodes for",
    "start": "1860279",
    "end": "1867360"
  },
  {
    "text": "delete nodes that are not used and all right thanks",
    "start": "1867360",
    "end": "1874159"
  },
  {
    "text": "welcome uh so a common or so so with the cluster Auto",
    "start": "1874960",
    "end": "1881559"
  },
  {
    "text": "scalar and I I think Carpenter I'm not sure um in order to determine",
    "start": "1881559",
    "end": "1887960"
  },
  {
    "text": "which H or how to scale up um they need to run simulations of the cube scheduler",
    "start": "1887960",
    "end": "1894360"
  },
  {
    "text": "um but that gets complicated when you are running a custom scheduler or you've",
    "start": "1894360",
    "end": "1899639"
  },
  {
    "text": "extended the the cube scheduler either with custom configuration or you've written custom plugins do you have",
    "start": "1899639",
    "end": "1905799"
  },
  {
    "text": "recommendations on how to use cluster autoscaler with and maybe Carpenter with a custom Schuler it depends on how",
    "start": "1905799",
    "end": "1913320"
  },
  {
    "text": "custom your custom Schuler is if it's doing completely different things then the",
    "start": "1913320",
    "end": "1919159"
  },
  {
    "text": "regular uh uh the regular SCH then you have a problem because right now we cannot",
    "start": "1919159",
    "end": "1925519"
  },
  {
    "text": "support a completely arbitrary uh logic and placement we do",
    "start": "1925519",
    "end": "1930679"
  },
  {
    "text": "simulations there to uh be sure that after we create a note the the PO will",
    "start": "1930679",
    "end": "1937360"
  },
  {
    "text": "go there if you have completely custom logic then I'm not directly seeing how",
    "start": "1937360",
    "end": "1942760"
  },
  {
    "text": "we could do it we cannot talk to a custom scheduler because because the API",
    "start": "1942760",
    "end": "1947880"
  },
  {
    "text": "first of all would be complicated and we do so many simulations on scale up and scale Downs that the communication",
    "start": "1947880",
    "end": "1953799"
  },
  {
    "text": "between two processes running possibly on different nodes would kill the performance on bigger clusters so",
    "start": "1953799",
    "end": "1961880"
  },
  {
    "text": "unfortunate for now the answer is try to be as close as uh regular Cube schuer as",
    "start": "1961880",
    "end": "1971480"
  },
  {
    "text": "possible you can have different node ranking functions but try to keep keep",
    "start": "1971480",
    "end": "1977519"
  },
  {
    "text": "basic constraints when the pot can fit into node or not aligned with Cube Schuler if you",
    "start": "1977519",
    "end": "1985919"
  },
  {
    "text": "have if you try to be as close as possible then probably it wouldn't matter that you have a custom schuer",
    "start": "1985919",
    "end": "1992159"
  },
  {
    "text": "running together with Carpenter and cluster SC if you stray too much then uh",
    "start": "1992159",
    "end": "1997760"
  },
  {
    "text": "one thing will think differently than the other and you will get into various types of problems",
    "start": "1997760",
    "end": "2005600"
  },
  {
    "text": "thank hey uh thank you both so much for the talk um in place vertical pod Auto",
    "start": "2005600",
    "end": "2010840"
  },
  {
    "text": "scaling something we're very excited about could you talk a little bit more about um like the status of that project",
    "start": "2010840",
    "end": "2017480"
  },
  {
    "text": "and maybe where support could be helpful uh so we have a cap that defines",
    "start": "2017480",
    "end": "2022639"
  },
  {
    "text": "what will be doing we have some people working on it uh so it depends on where",
    "start": "2022639",
    "end": "2028799"
  },
  {
    "text": "these people uh will finish if you have strong interest and we will be more than",
    "start": "2028799",
    "end": "2034960"
  },
  {
    "text": "uh will be more than welcome to come to seek can help us with implementing it thanks I think just just to cover extra",
    "start": "2034960",
    "end": "2042559"
  },
  {
    "text": "on that um the the wider work of like the node uh sign like that that work is",
    "start": "2042559",
    "end": "2049520"
  },
  {
    "text": "progressing but that's that's not actually like our our work is to take",
    "start": "2049520",
    "end": "2054560"
  },
  {
    "text": "advantage of it but I think it's still Alpha is the last I heard like the the",
    "start": "2054560",
    "end": "2060760"
  },
  {
    "text": "actual node in place update so you would need a cluster that supports the behavior to take advantage of the vpa",
    "start": "2060760",
    "end": "2068079"
  },
  {
    "text": "implementation of it got okay thank you no worries hey big fan of your guys work",
    "start": "2068079",
    "end": "2075480"
  },
  {
    "text": "first off uh but my question is kind of I guess it's HPA and vpa uh but also",
    "start": "2075480",
    "end": "2081358"
  },
  {
    "text": "falls in love with the multi-dimensional uh autoscaler do you guys have any use cases where people have used both of",
    "start": "2081359",
    "end": "2088358"
  },
  {
    "text": "them on uh metrics like CPU and memory because I noticed on the GitHub page it said to not use uh vpa with HPA if",
    "start": "2088359",
    "end": "2095638"
  },
  {
    "text": "you're going to be doing uh scaling off of those resource metrics uh because",
    "start": "2095639",
    "end": "2101240"
  },
  {
    "text": "right now we're in a stage where we want to optimize the resource usage we don't want to over provision um so would there",
    "start": "2101240",
    "end": "2107760"
  },
  {
    "text": "be a case where you would put in dry run mode on vpa to get an idea of the resource metrics to use and leave HPA on",
    "start": "2107760",
    "end": "2113800"
  },
  {
    "text": "or does HPA kind of skew uh the metrics vpa looks at so the problem with running",
    "start": "2113800",
    "end": "2120359"
  },
  {
    "text": "HPA and vpa on the same set of metrics is that by default HPA uses percentage",
    "start": "2120359",
    "end": "2128040"
  },
  {
    "text": "Target so you specify that you want your pods to be utilized up to let's say 50%",
    "start": "2128040",
    "end": "2134400"
  },
  {
    "text": "if it goes up you you create more pods if it goes down you uh reduce the number",
    "start": "2134400",
    "end": "2140520"
  },
  {
    "text": "of pots vpa changes the size of pots so",
    "start": "2140520",
    "end": "2145599"
  },
  {
    "text": "now this 50% means slightly different thing and if it by any chance it grows",
    "start": "2145599",
    "end": "2153480"
  },
  {
    "text": "it too much for example Beyond this scalability limit of your pods then for",
    "start": "2153480",
    "end": "2159079"
  },
  {
    "text": "example you may always be below 50% even though your PS are trying very",
    "start": "2159079",
    "end": "2166240"
  },
  {
    "text": "very hard to uh get res uh to process",
    "start": "2166240",
    "end": "2171599"
  },
  {
    "text": "the re request what will happen if you are always below uh the target your",
    "start": "2171599",
    "end": "2178079"
  },
  {
    "text": "deployment will shrink and it will shrink up to minimal size because it will always require",
    "start": "2178079",
    "end": "2186040"
  },
  {
    "text": "fewer your pods so in order to uh have it supported you should use",
    "start": "2186040",
    "end": "2193800"
  },
  {
    "text": "something that doesn't connect vpa and uh HPA that close together yeah where if",
    "start": "2193800",
    "end": "2202240"
  },
  {
    "text": "you want to use CPU you could try using absolute value so then uh the absolute value will",
    "start": "2202240",
    "end": "2210720"
  },
  {
    "text": "be kind of independent from the size of the pods because as as long as the size",
    "start": "2210720",
    "end": "2217319"
  },
  {
    "text": "of p is bigger as of the uh absolute value this is okay so but you need to be",
    "start": "2217319",
    "end": "2225359"
  },
  {
    "text": "very careful with that because there are a couple of edge cases there we recommend using slightly",
    "start": "2225359",
    "end": "2232560"
  },
  {
    "text": "different uh metrics for HPA for example number of incoming qpss which has of",
    "start": "2232560",
    "end": "2239680"
  },
  {
    "text": "course uh uh strong correlation with CPU usage however it's more more kind of",
    "start": "2239680",
    "end": "2245200"
  },
  {
    "text": "independent from it and then uh some strange interaction between HPA and vpa",
    "start": "2245200",
    "end": "2252480"
  },
  {
    "text": "are less likely to happen okay thank you so",
    "start": "2252480",
    "end": "2257559"
  },
  {
    "text": "much thank you for the thank you for all the work and thank you for the talk I I have a question from a enduser",
    "start": "2257920",
    "end": "2264720"
  },
  {
    "text": "perspective how would they think about whether to use Autos skating versus some of the serverless offerings from the",
    "start": "2264720",
    "end": "2271640"
  },
  {
    "text": "cloud like you know gke autopilot or like uh fargate",
    "start": "2271640",
    "end": "2277319"
  },
  {
    "text": "uh I cannot speak uh for fargate I can speak for GK autopilot GK autopilot",
    "start": "2277319",
    "end": "2285000"
  },
  {
    "text": "underneath uh uses cluster autoscaler so it solves uh the problem of configuring",
    "start": "2285000",
    "end": "2292839"
  },
  {
    "text": "cluster autoscaler for you so if you want to have more control over your",
    "start": "2292839",
    "end": "2298000"
  },
  {
    "text": "cluster then you use GK classic if you want Google to handle your Autos scaling",
    "start": "2298000",
    "end": "2305720"
  },
  {
    "text": "for you then you can use autopilot eventually both GK and autopilot will",
    "start": "2305720",
    "end": "2313640"
  },
  {
    "text": "run more or less the same Autos scaling code Autos scaling will happen for you",
    "start": "2313640",
    "end": "2319000"
  },
  {
    "text": "but with autopilot you have one problem less and with GK classic you have one",
    "start": "2319000",
    "end": "2324480"
  },
  {
    "text": "knob to control more I I think uh my experience personal",
    "start": "2324480",
    "end": "2330319"
  },
  {
    "text": "experience at work is is more like fargate you're always giving up some",
    "start": "2330319",
    "end": "2336200"
  },
  {
    "text": "control control for those um those serverless offerings um and certainly in",
    "start": "2336200",
    "end": "2341920"
  },
  {
    "text": "our use case we we need some of that control that we've been giving up therefore for us for most workloads",
    "start": "2341920",
    "end": "2347960"
  },
  {
    "text": "fargate isn't suitable and there are there are certainly workloads and like",
    "start": "2347960",
    "end": "2353520"
  },
  {
    "text": "talk to talk to other users of it that would be able to use those",
    "start": "2353520",
    "end": "2358880"
  },
  {
    "text": "um there's there's always going to be tradeoffs though like I I think it's it's always just a matter of evaluating",
    "start": "2358880",
    "end": "2365520"
  },
  {
    "text": "for your workload whether you can tolerate you know what the limitations of said serverless offering are um and",
    "start": "2365520",
    "end": "2372280"
  },
  {
    "text": "actually whether it might whether it will definitely result in costs okay thank",
    "start": "2372280",
    "end": "2379800"
  },
  {
    "text": "you okay looks like we don't have any more questions if you want to ask us",
    "start": "2380319",
    "end": "2386480"
  },
  {
    "text": "some more private questions we will be hanging out around for a moment in otherwise thank you for joining and",
    "start": "2386480",
    "end": "2392680"
  },
  {
    "text": "hopefully you will have the great last day of ccon tomorrow",
    "start": "2392680",
    "end": "2398400"
  },
  {
    "text": "you",
    "start": "2400359",
    "end": "2403359"
  }
]