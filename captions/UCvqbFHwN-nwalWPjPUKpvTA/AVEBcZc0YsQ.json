[
  {
    "text": "all right hello and welcome to my talk I'm uh Daniel Brookman I work at",
    "start": "80",
    "end": "5600"
  },
  {
    "text": "ISO veent I work on itself and ebpf in the Linux kernel I co maintain it",
    "start": "5600",
    "end": "12320"
  },
  {
    "text": "there and have been cont contributing for a very long time um today the talk",
    "start": "12320",
    "end": "17359"
  },
  {
    "text": "is about turning Up Performance to1 um I will talk about cium a new weef um",
    "start": "17359",
    "end": "23800"
  },
  {
    "text": "device replacement uh that we built and yeah and going back with TCP so really the",
    "start": "23800",
    "end": "32200"
  },
  {
    "text": "goal or experiment I had for this talk was basically I mean wouldn't it be nice to just turn on the volume knob and",
    "start": "32200",
    "end": "38680"
  },
  {
    "text": "improve your performance unfortunately it's not always that nice but uh or always that easy um but the question was",
    "start": "38680",
    "end": "45800"
  },
  {
    "text": "really like what would it take to really get to maximum performance and yeah first the question",
    "start": "45800",
    "end": "52640"
  },
  {
    "text": "of why why is it relevant in the first place so the use use cases you might have might differ so for example one",
    "start": "52640",
    "end": "59079"
  },
  {
    "text": "could be scale you're adding more workloads to your kubernetes environments you're adding you're connecting multiple clusters in a mesh",
    "start": "59079",
    "end": "65439"
  },
  {
    "text": "and therefore also the the traffic uh that is being pushed around increases",
    "start": "65439",
    "end": "70520"
  },
  {
    "text": "maybe maybe sustainability to better make use of your existing infrastructure or to reduce off for on Prem costs or",
    "start": "70520",
    "end": "78280"
  },
  {
    "text": "performance uh performance-wise to reduce the RPC workload latencies or to",
    "start": "78280",
    "end": "83600"
  },
  {
    "text": "better cope with potentially escalating bulk data demands that you have maybe from AI or machine learning",
    "start": "83600",
    "end": "91320"
  },
  {
    "text": "workloads uh actually for the latter there's quite a big push in the industry right now uh we see new Nicks coming up",
    "start": "91320",
    "end": "98720"
  },
  {
    "text": "with 800 gabit and Beyond um you know like a big uh hype around a Ai and and",
    "start": "98720",
    "end": "106399"
  },
  {
    "text": "and machine learning to push uh data center Innovations uh the hyperscalers are uh",
    "start": "106399",
    "end": "113680"
  },
  {
    "text": "you know increasing their capacity and we even see switches coming up to the",
    "start": "113680",
    "end": "119079"
  },
  {
    "text": "market with 50 1.2 terabit per second which is really crazy um so the question here coming",
    "start": "119079",
    "end": "126759"
  },
  {
    "text": "back to the kubernetes world is uh how would such a platform look like that would potentially be able to address",
    "start": "126759",
    "end": "133520"
  },
  {
    "text": "those future demands and and the more practical question uh is how like what",
    "start": "133520",
    "end": "139599"
  },
  {
    "text": "can we benefit from it today and especially without having to rewrite existing applications of",
    "start": "139599",
    "end": "147040"
  },
  {
    "text": "course if you look at the standard kubernetes architecture or um setup so you have",
    "start": "147040",
    "end": "154040"
  },
  {
    "text": "your host there's a CET running Cube proxy running and if you deploy celum as",
    "start": "154040",
    "end": "159840"
  },
  {
    "text": "a cni for example there's a cium agent uh like the the demon itself and a cni",
    "start": "159840",
    "end": "167440"
  },
  {
    "text": "plugin so whenever there's a new pod that is being spawned up it will",
    "start": "167440",
    "end": "172519"
  },
  {
    "text": "basically give a handle of the network namespace of the part to the cni plugin",
    "start": "172519",
    "end": "178599"
  },
  {
    "text": "the cni plugin will talk through RPC to the agent and eventually it will set",
    "start": "178599",
    "end": "184080"
  },
  {
    "text": "up networking devices IP addressing routing and in our",
    "start": "184080",
    "end": "189879"
  },
  {
    "text": "case also BPF programs and then when traffic is uh",
    "start": "189879",
    "end": "196040"
  },
  {
    "text": "going in and out of the Pod it will basically use the the upper stack um IP forwarding layer net filter routing and",
    "start": "196040",
    "end": "203440"
  },
  {
    "text": "so on um yeah there are couple of problems",
    "start": "203440",
    "end": "208720"
  },
  {
    "text": "with that if you look at scalability and performance aspects of such a setup uh",
    "start": "208720",
    "end": "213799"
  },
  {
    "text": "one is the Q proxy scalability so if you have a lot of services um you run into",
    "start": "213799",
    "end": "220159"
  },
  {
    "text": "issues but also routing uh through the upper stack which may not necessarily be very obvious um there are potential",
    "start": "220159",
    "end": "227439"
  },
  {
    "text": "reasons so maybe initially you deployed a cluster and you just went with the defaults defaults are very conservative",
    "start": "227439",
    "end": "232879"
  },
  {
    "text": "so that you can run on the um on a big variety of uh environments all the",
    "start": "232879",
    "end": "239439"
  },
  {
    "text": "current pels and so on or maybe you have custom net filter rules uh installed so",
    "start": "239439",
    "end": "244879"
  },
  {
    "text": "you have to go to net filter um or you cannot replace Q proxy for one reason or",
    "start": "244879",
    "end": "250239"
  },
  {
    "text": "another uh with a Q proxy I mean we probably many of you know it from from the bugging uh troubleshooting and",
    "start": "250239",
    "end": "257160"
  },
  {
    "text": "production um so you know when when you have a lot of services then it's like a linear walk uh trying to match uh one of",
    "start": "257160",
    "end": "265240"
  },
  {
    "text": "the service topples so it can it can be quite some overhead upper stack um",
    "start": "265240",
    "end": "271440"
  },
  {
    "text": "there's something when the packet leaves the part on the erress direction when you go to the upper stack there's",
    "start": "271440",
    "end": "277160"
  },
  {
    "text": "something in the kernel that is called SKB orphan uh for example when you have TCP traffic there that is basically",
    "start": "277160",
    "end": "283880"
  },
  {
    "text": "there to you know tell the TCP stack that the network packet already left the",
    "start": "283880",
    "end": "289800"
  },
  {
    "text": "Noe and as you can see it's not actually happen it's not actually the case what because the packet is still inside the",
    "start": "289800",
    "end": "295720"
  },
  {
    "text": "host name space and that is essentially there and and it's hard to remove because of net filters or net filter T",
    "start": "295720",
    "end": "303240"
  },
  {
    "text": "proxy relies on it um but doing that too soon before the packet actually left the",
    "start": "303240",
    "end": "309160"
  },
  {
    "text": "Noe uh basically breaks TCP back pressure because TC the TCP stack things",
    "start": "309160",
    "end": "315400"
  },
  {
    "text": "packet already left the note I can push more and because of that you can evade the sand prer limits and when you look",
    "start": "315400",
    "end": "322800"
  },
  {
    "text": "at the uh performance uh what you can see here on the right side the yellow",
    "start": "322800",
    "end": "328080"
  },
  {
    "text": "bar is when the when the application is inside the host itself and you do like a",
    "start": "328080",
    "end": "333360"
  },
  {
    "text": "TCP stream workload test with a 100 Gig Nick um overwire so I had like two",
    "start": "333360",
    "end": "340120"
  },
  {
    "text": "machines back to back um with the 8K MTU",
    "start": "340120",
    "end": "345520"
  },
  {
    "text": "you reach 100 gbit per second uh but if you look at uh the upper stack forwarding with the weave devices it's",
    "start": "345520",
    "end": "351800"
  },
  {
    "text": "not that great um it's 63 gbit and the reason is because TCP back pressure",
    "start": "351800",
    "end": "358319"
  },
  {
    "text": "breaks so the question is really can we achieve the same for kubernetes parts as",
    "start": "358319",
    "end": "363600"
  },
  {
    "text": "well um the answer is yes uh and I will take you to this journey that we did um",
    "start": "363600",
    "end": "372440"
  },
  {
    "text": "so yeah com coming back um in in our journey initially uh when we uh worked",
    "start": "372440",
    "end": "379240"
  },
  {
    "text": "in syum in the early days the first thing we did was uh replacing the Q proxy component uh with the BPF based",
    "start": "379240",
    "end": "387080"
  },
  {
    "text": "implementation in order to be better scalable that that covers all the",
    "start": "387080",
    "end": "392360"
  },
  {
    "text": "kubernetes service types um for the north south direction we have a per packet uh load balancing uh at the TC",
    "start": "392360",
    "end": "400720"
  },
  {
    "text": "BPF layer what you can see here uh agent when it when it spawns up it's",
    "start": "400720",
    "end": "406120"
  },
  {
    "text": "attaching BPF programs on the physical devices in the TC BPF layer of the deack",
    "start": "406120",
    "end": "412599"
  },
  {
    "text": "in east west uh we got rid of the per packet net and are doing the backend",
    "start": "412599",
    "end": "417840"
  },
  {
    "text": "selection at the connect time and we also have Mark left and host Port support the next logical thing after",
    "start": "417840",
    "end": "425240"
  },
  {
    "text": "that was basically to add support for xtp based service load balancing given",
    "start": "425240",
    "end": "430599"
  },
  {
    "text": "we already had the load balancer in the TC side now it's it was just a matter of porting that over to xtp xtp is a um",
    "start": "430599",
    "end": "439360"
  },
  {
    "text": "it's called Express data path and it's basically an attachment point inside the driver where you can build high",
    "start": "439360",
    "end": "444440"
  },
  {
    "text": "performance load balancing um so that so that essentially can Co collocate easily",
    "start": "444440",
    "end": "451400"
  },
  {
    "text": "collocate workloads and still have the high performance aspect uh um to better",
    "start": "451400",
    "end": "457360"
  },
  {
    "text": "scale out that also covers all the kubernetes services and we also have markli and DSR",
    "start": "457360",
    "end": "464000"
  },
  {
    "text": "support for this there was a nice blog post on theum uh website with a",
    "start": "464000",
    "end": "470240"
  },
  {
    "text": "production U graph so what you can see here from uh is a test run where",
    "start": "470240",
    "end": "477080"
  },
  {
    "text": "initially there was I ipvs in produ ction then it got moved over to a single",
    "start": "477080",
    "end": "482759"
  },
  {
    "text": "Noe for the layer for low balancing with xtp and what you can see is the here the CPU overhead so it's just really minimal",
    "start": "482759",
    "end": "490240"
  },
  {
    "text": "and once it was taken out again from production and moved back to ipvs actually two notes handling that",
    "start": "490240",
    "end": "496440"
  },
  {
    "text": "production traffic it it went really high again so xtp really allows for low overhead because it's so early uh inside",
    "start": "496440",
    "end": "504440"
  },
  {
    "text": "the receive path The Next Step that um we thought um",
    "start": "504440",
    "end": "511080"
  },
  {
    "text": "was is really useful performance-wise was to add a bandwidth manager to",
    "start": "511080",
    "end": "516479"
  },
  {
    "text": " um the the idea of the bandwidth manager is basically that we support ESS",
    "start": "516479",
    "end": "522760"
  },
  {
    "text": "bandwidth limits in cium so that you can say okay for this kind of part I want to",
    "start": "522760",
    "end": "527959"
  },
  {
    "text": "have 50 megabit for other parts 100 whatever so that you have a scalable erress rate limiting and the way this",
    "start": "527959",
    "end": "535640"
  },
  {
    "text": "basically works is that the agent it sets up um on a on a Nick you typically have multiq on the on the transmit side",
    "start": "535640",
    "end": "543880"
  },
  {
    "text": "and it sets up the fq Schuler it's called Fair Q scheder in the kernel and",
    "start": "543880",
    "end": "550079"
  },
  {
    "text": "we have a BPF program which basically tells that scheduler the departure time",
    "start": "550079",
    "end": "555800"
  },
  {
    "text": "of the packet so that you can configure so that you can set this departure time",
    "start": "555800",
    "end": "560839"
  },
  {
    "text": "based on the rate you want to have and this allows for uh potentially lockless",
    "start": "560839",
    "end": "567720"
  },
  {
    "text": "uh like lockless rate limiting and what you can see here uh in this graph is on the yellow",
    "start": "567720",
    "end": "576600"
  },
  {
    "text": "bar is like the the classical way uh to to achieve rate limiting for example",
    "start": "576600",
    "end": "582800"
  },
  {
    "text": "hierarchical token bucket um that's the usual way in the Linux kernel and thanks",
    "start": "582800",
    "end": "588399"
  },
  {
    "text": "to the fq Q disk and setting the time stamps in ebpf the earliest departure",
    "start": "588399",
    "end": "593640"
  },
  {
    "text": "time uh you can get a you know more than 4X better P99 latency for that so it's",
    "start": "593640",
    "end": "599079"
  },
  {
    "text": "it's really nice uh in this aspect it's not the only advantage that uh that this",
    "start": "599079",
    "end": "605079"
  },
  {
    "text": "uh the bandwith manager provides because um one thing that we added in",
    "start": "605079",
    "end": "611200"
  },
  {
    "text": "the colonel 519 uh colleague from Facebook and and myself we run some",
    "start": "611200",
    "end": "617000"
  },
  {
    "text": "experiments and fixed that in this deck was that basically when you have applications inside the port and you",
    "start": "617000",
    "end": "623320"
  },
  {
    "text": "would like to use bbr congestion control that's the congestion control algorithm for TCP which was developed by Google um",
    "start": "623320",
    "end": "630720"
  },
  {
    "text": "it's called I think bottleneck bandwidth and rtt so it uses a different",
    "start": "630720",
    "end": "635839"
  },
  {
    "text": "congestion signal uh than the default cubic one which is loss based um the",
    "start": "635839",
    "end": "642800"
  },
  {
    "text": "problem here is it was not possible before to use that for parts because whenever a network packet traverses the",
    "start": "642800",
    "end": "649720"
  },
  {
    "text": "network namespace the time stamp the departure Tim stamp that the congestion",
    "start": "649720",
    "end": "654800"
  },
  {
    "text": "control algorithm would set is basically being cleared um and the fix and the",
    "start": "654800",
    "end": "660079"
  },
  {
    "text": "kernel that we did basically then unlocked this and then you can use bbr",
    "start": "660079",
    "end": "665959"
  },
  {
    "text": "uh from parts and benefit from that and fq scheduler from the bandwith manager",
    "start": "665959",
    "end": "671800"
  },
  {
    "text": "is basically able to then schedule the packet at the right time at cucon uh",
    "start": "671800",
    "end": "677639"
  },
  {
    "text": "some um years ago I did a demo where I compared bbr versus the default cubic uh",
    "start": "677639",
    "end": "684800"
  },
  {
    "text": "over a lossi network for example when you're consuming services from a kubernetes cluster from mobile device or",
    "start": "684800",
    "end": "692079"
  },
  {
    "text": "or elsewhere or like over Wi-Fi for example uh we had a streaming demo where the streaming application was inside the",
    "start": "692079",
    "end": "698800"
  },
  {
    "text": "Pod the server and over the lossi network what you could see is that the",
    "start": "698800",
    "end": "704240"
  },
  {
    "text": "bbr with BPR it was staying in high definition the video and with the default it was falling back to the low",
    "start": "704240",
    "end": "710800"
  },
  {
    "text": "resolution because the uh the the TCP congestion window was reduced too",
    "start": "710800",
    "end": "717200"
  },
  {
    "text": "aggressively um so that's the bandwidth manager the next uh feature that we uh",
    "start": "717200",
    "end": "724920"
  },
  {
    "text": "added to cium was is called BPF host routing so the whole idea there is",
    "start": "724920",
    "end": "730480"
  },
  {
    "text": "basically well don't use the upper stack for routing uh we can do all the routing in TC BPF",
    "start": "730480",
    "end": "737240"
  },
  {
    "text": "layer and we can also make use of you know the kernel facilities like the the",
    "start": "737240",
    "end": "742399"
  },
  {
    "text": "kernel routing table and so on out of BPF uh because it's there as a helper to",
    "start": "742399",
    "end": "747639"
  },
  {
    "text": "utilize so your routing demons can still work and um uh you can benefit from this",
    "start": "747639",
    "end": "755240"
  },
  {
    "text": "uh without go having to go to the upper stack uh so there's a two new helpers in",
    "start": "755240",
    "end": "760399"
  },
  {
    "text": "BPF on the Kernel side that they added one is called BPF redirect Pier the other one is called BPF redirect",
    "start": "760399",
    "end": "766600"
  },
  {
    "text": "neighbor uh the redirect Pier is basically for The Logical in Ingress path into the Pod it allows a fast uh",
    "start": "766600",
    "end": "774639"
  },
  {
    "text": "network namespace switch from the Ingress side to the Ingress side of the weef device basically um what happens here",
    "start": "774639",
    "end": "781800"
  },
  {
    "text": "internally in the kernel is it just resets the device pointers from the physical device to The Weave device that",
    "start": "781800",
    "end": "787720"
  },
  {
    "text": "is inside the Pod and then there's another loop in the main receive Loop in the kernel instead of having to go to",
    "start": "787720",
    "end": "794560"
  },
  {
    "text": "The Reef device where it would NQ uh the packet in a per CPU backlog q and it",
    "start": "794560",
    "end": "799720"
  },
  {
    "text": "will add more latency um and it and it and it is lower for so so that's",
    "start": "799720",
    "end": "806120"
  },
  {
    "text": "basically the the kernel internals if you're it um the ER side so the new",
    "start": "806120",
    "end": "813959"
  },
  {
    "text": "helper here would basically allow you to inject the packet into the layer two of the kernel in into the neighboring",
    "start": "813959",
    "end": "820639"
  },
  {
    "text": "subsystem and it would help to resolve you know the the Mac addresses for the",
    "start": "820639",
    "end": "825880"
  },
  {
    "text": "source and destination Mac addresses and this can be combined with the FIB lookup that is in BPF uh inside the kernel um",
    "start": "825880",
    "end": "833680"
  },
  {
    "text": "yeah to basically um achieve the the outgoing path as well and the",
    "start": "833680",
    "end": "839279"
  },
  {
    "text": "interesting thing here is uh given this bypasses the upper stack there's also not this problem that",
    "start": "839279",
    "end": "845680"
  },
  {
    "text": "I mentioned earlier where you would skp orphan the packet so basically this fixes the TCP back pressure issue um",
    "start": "845680",
    "end": "853199"
  },
  {
    "text": "that was there before so this is how the complete picture looks here and if you look into the performance uh numbers",
    "start": "853199",
    "end": "860519"
  },
  {
    "text": "again this allows to push the the TCP bulk stream workload to 90 gbit per",
    "start": "860519",
    "end": "866880"
  },
  {
    "text": "second instead of 60 so this gives a huge performance Advantage it's still not perfect what",
    "start": "866880",
    "end": "873560"
  },
  {
    "text": "you can see here it's not 100% on par with what you have if as if the application is inside the host itself um",
    "start": "873560",
    "end": "881279"
  },
  {
    "text": "so there are more pieces to the puzzle um the next one uh that uh I",
    "start": "881279",
    "end": "887360"
  },
  {
    "text": "added into the kernel um this summer actually is called tcx so tcx is in",
    "start": "887360",
    "end": "893720"
  },
  {
    "text": "short um called an Express uh data path for TC TC so basically a new design for",
    "start": "893720",
    "end": "900600"
  },
  {
    "text": "the TC data path why uh because it is really crucial it's it's the foundation",
    "start": "900600",
    "end": "907040"
  },
  {
    "text": "for cium where we attach everything um uh like in in in in the in the main",
    "start": "907040",
    "end": "914079"
  },
  {
    "text": "data path that we have in in in the kernel and to make it more efficient to",
    "start": "914079",
    "end": "919320"
  },
  {
    "text": "make it more modern and also more robust for example we see more and more users that are using the uh TC data path so",
    "start": "919320",
    "end": "927759"
  },
  {
    "text": "when there are multiple users so that they don't step onto each other uh that's that got support with BPF link",
    "start": "927759",
    "end": "934000"
  },
  {
    "text": "which is a BPF concept that hasn't been added uh to TC given its",
    "start": "934000",
    "end": "940680"
  },
  {
    "text": "internals um more efficient fast path and then also dependency controls where",
    "start": "940680",
    "end": "946199"
  },
  {
    "text": "you can say Okay I want to attach my program before or after that other program so that you have like a relative",
    "start": "946199",
    "end": "952120"
  },
  {
    "text": "attachment point and it's from a user perspective nicer than what you had before with the old style TC so yeah",
    "start": "952120",
    "end": "959759"
  },
  {
    "text": "like like back in the days when in 2015 uh I added the initial um support",
    "start": "959759",
    "end": "967680"
  },
  {
    "text": "for TC BPF it was like the most natural fit at the time but now like uh making",
    "start": "967680",
    "end": "973319"
  },
  {
    "text": "this more modern was a it was a yeah good point in time to actually tackle",
    "start": "973319",
    "end": "978720"
  },
  {
    "text": "that with that um uh I also added a framework inside the kernel which is",
    "start": "978720",
    "end": "984160"
  },
  {
    "text": "called BPF Moc so like a multi- attach uh layer where multiple uh BPF programs",
    "start": "984160",
    "end": "991120"
  },
  {
    "text": "can be attached in an efficient way in an array an array is more cach line friendly than a linked list for example",
    "start": "991120",
    "end": "997839"
  },
  {
    "text": "where you would have more cach misses and this framework the goal of this was basically to provide a common you know",
    "start": "997839",
    "end": "1004240"
  },
  {
    "text": "look and feel from an API perspective because the idea was not to just have it",
    "start": "1004240",
    "end": "1009639"
  },
  {
    "text": "for tcx but also have it in other attached locations for example xtp um in",
    "start": "1009639",
    "end": "1016360"
  },
  {
    "text": "in the future so that we support multiple programs at the xtp layer uh netkit devices this is something that I",
    "start": "1016360",
    "end": "1022759"
  },
  {
    "text": "will mention later and uh cgroups and and so on um so just to give you the",
    "start": "1022759",
    "end": "1030678"
  },
  {
    "text": "picture this is how it looked like old style like the classic uh TC um uh you",
    "start": "1030679",
    "end": "1037120"
  },
  {
    "text": "know example picture where you have this you know fake qis on in Ingress and egress which actually doesn't do any you",
    "start": "1037120",
    "end": "1043319"
  },
  {
    "text": "know queuing in that sense uh it just basically is a container structure for",
    "start": "1043319",
    "end": "1049280"
  },
  {
    "text": "BPF programs or like for the old style uh TC that was there also before BPF and",
    "start": "1049280",
    "end": "1056559"
  },
  {
    "text": "this got really simplified into more efficient array and then like an efficient entry point for for",
    "start": "1056559",
    "end": "1064360"
  },
  {
    "text": "BPF and with that like from a from a micro Benchmark perspective uh the entry into a BPF",
    "start": "1064360",
    "end": "1071120"
  },
  {
    "text": "program got reduced into half uh so we could cut the Cycles into half when the cach is hot and I think the benefits",
    "start": "1071120",
    "end": "1077480"
  },
  {
    "text": "will be uh even better when you don't always have a hot um the data the data",
    "start": "1077480",
    "end": "1083240"
  },
  {
    "text": "hot in the cache so yeah um so now that it's moved to tcx",
    "start": "1083240",
    "end": "1090960"
  },
  {
    "text": "for the physical devices for the weef devices the next step in that",
    "start": "1090960",
    "end": "1096440"
  },
  {
    "text": "Journey that got merged recently is basically to replace the weave devices",
    "start": "1096440",
    "end": "1101720"
  },
  {
    "text": "uh we uh replaced this with something that is called netkit um and",
    "start": "1101720",
    "end": "1108840"
  },
  {
    "text": "the whole idea here is basically that uh for Reef devices there's still this",
    "start": "1108840",
    "end": "1115600"
  },
  {
    "text": "point where we have the logical eress path uh for traffic going out of PODS",
    "start": "1115600",
    "end": "1121880"
  },
  {
    "text": "where it has to take a uh Q like a per backlog queue um this is something that",
    "start": "1121880",
    "end": "1128480"
  },
  {
    "text": "is internal to The Reef driver in the kernel but it adds latency and the whole idea is basically",
    "start": "1128480",
    "end": "1134720"
  },
  {
    "text": "to move that BPF program uh that we have filium that is attached on the host side",
    "start": "1134720",
    "end": "1141000"
  },
  {
    "text": "of the weef device into the part but into the device in the part so that uh",
    "start": "1141000",
    "end": "1146880"
  },
  {
    "text": "applications they cannot unload this uh and the we move the BPF program closer",
    "start": "1146880",
    "end": "1153919"
  },
  {
    "text": "to the source and in the BPF program uh when we",
    "start": "1153919",
    "end": "1159200"
  },
  {
    "text": "know that we want to push the traffic out of the node uh the whole ideas then we can directly forward it without",
    "start": "1159200",
    "end": "1165159"
  },
  {
    "text": "having to go to this per CPU backlog queue so we can remove this uh this",
    "start": "1165159",
    "end": "1170640"
  },
  {
    "text": "additional this artificial inefficiency basically and the other thing that that",
    "start": "1170640",
    "end": "1177440"
  },
  {
    "text": "that was interesting while working on this is yeah why not make this an L3 device from the beginning right because",
    "start": "1177440",
    "end": "1183600"
  },
  {
    "text": "weave is a L2 so you need to have some op resolver as well but we can just get",
    "start": "1183600",
    "end": "1188960"
  },
  {
    "text": "rid of this as well so this got merged just recently not too long ago and it's",
    "start": "1188960",
    "end": "1196559"
  },
  {
    "text": "interesting to compare this to we and and also ipv Lan how does it basically",
    "start": "1196559",
    "end": "1201679"
  },
  {
    "text": "uh stand against those two so we as I mentioned this L2 ipv Lan L3 and we also",
    "start": "1201679",
    "end": "1207240"
  },
  {
    "text": "wanted to have this as an L3 device so that you don't need to resolve ARP uh from the from the Pod side",
    "start": "1207240",
    "end": "1214440"
  },
  {
    "text": "um for weef and netkit this is both like a like a like main device and a peer",
    "start": "1214440",
    "end": "1220559"
  },
  {
    "text": "device ipv land Works a little bit different so you have to make one of your physical devices the master device",
    "start": "1220559",
    "end": "1227440"
  },
  {
    "text": "and ipv LAN and then you can add ipv land slave devices uh which you then move into the network",
    "start": "1227440",
    "end": "1233520"
  },
  {
    "text": "namespaces but uh yeah the BPF program uh like the programming for IP VLAN is a",
    "start": "1233520",
    "end": "1241159"
  },
  {
    "text": "bit of a hassle because in we once had ipv L support but later on we",
    "start": "1241159",
    "end": "1246640"
  },
  {
    "text": "removed it again but basically when you want to do policy enforcement um we back then we added the",
    "start": "1246640",
    "end": "1253840"
  },
  {
    "text": "BPF program into the IP VLAN uh device that is inside the part but it is a",
    "start": "1253840",
    "end": "1259120"
  },
  {
    "text": "problem right because an application could just remove it again if it has the rights to do so and this is something",
    "start": "1259120",
    "end": "1265799"
  },
  {
    "text": "that we definitely wanted to avoid or um if you don't add this inside the Pod",
    "start": "1265799",
    "end": "1273600"
  },
  {
    "text": "then the normal default IP VLAN mode it it could be that this IP VLAN device",
    "start": "1273600",
    "end": "1279320"
  },
  {
    "text": "could just talk to this IP VLAN device directly without going somehow to the host so you cannot do any policy",
    "start": "1279320",
    "end": "1285039"
  },
  {
    "text": "enforcement so this is another hassle so essentially you have to add this to the",
    "start": "1285039",
    "end": "1290520"
  },
  {
    "text": "physical device and attach your BPF program there for all the different",
    "start": "1290520",
    "end": "1297279"
  },
  {
    "text": "parts um but then the problem is users they don't only have one physical device",
    "start": "1297279",
    "end": "1302799"
  },
  {
    "text": "I mean this is one use case but there can also be multiple ones and then you have to pick and then you have to make",
    "start": "1302799",
    "end": "1308200"
  },
  {
    "text": "then you have to add for example a dummy device and make this your master device and then it it gets complicated very",
    "start": "1308200",
    "end": "1314919"
  },
  {
    "text": "quickly so basically what we wanted to have this nice uh model for the reef",
    "start": "1314919",
    "end": "1319960"
  },
  {
    "text": "devices where you have one in the host one in the Pod and it's quite flexible so we also added we also had this for",
    "start": "1319960",
    "end": "1327080"
  },
  {
    "text": "netkit and yeah then what I mentioned it allows this fast network namespace",
    "start": "1327080",
    "end": "1333000"
  },
  {
    "text": "switch and egress um you can combine this with the FIP lookup in the case of",
    "start": "1333000",
    "end": "1338520"
  },
  {
    "text": "Ip Von uh it is I would say slightly",
    "start": "1338520",
    "end": "1343720"
  },
  {
    "text": "less efficient in this record because if you look into the ipv L code it has an internal FIP that is not using the",
    "start": "1343720",
    "end": "1351120"
  },
  {
    "text": "kernel FIP uh just for internal routing whether a packet has to go to outside of",
    "start": "1351120",
    "end": "1357400"
  },
  {
    "text": "the node or to another IP VLAN device in a different part so it has like an",
    "start": "1357400",
    "end": "1363320"
  },
  {
    "text": "internal hash table where it does the lookup and then when you want actually want to Route the traffic out of the",
    "start": "1363320",
    "end": "1368520"
  },
  {
    "text": "node there's an additional kernel FIP lookup so you can get rid of those two",
    "start": "1368520",
    "end": "1373880"
  },
  {
    "text": "and only have the one in netkit or if you have a use case where you actually don't need to do a FIP lookup you can",
    "start": "1373880",
    "end": "1382279"
  },
  {
    "text": "just direct it out of the note uh without having to do that so you're more flexible in this in this",
    "start": "1382279",
    "end": "1389600"
  },
  {
    "text": "sense so it basically takes the Best of Both Worlds and if you look at the",
    "start": "1389600",
    "end": "1397159"
  },
  {
    "text": "performance side if you look at the flame graphs the problem that you sometimes see under",
    "start": "1397159",
    "end": "1402559"
  },
  {
    "text": "pressure with we devices so basically here you have an application it's it's sending something",
    "start": "1402559",
    "end": "1408559"
  },
  {
    "text": "uh it will at some point Quee the packet to the per backlog queue what I mentioned earlier and then another",
    "start": "1408559",
    "end": "1415720"
  },
  {
    "text": "thread will pick it up and process it from there and then send it out right so",
    "start": "1415720",
    "end": "1421000"
  },
  {
    "text": "often times this can be deferred to the kernel soft AQ demon and that's not really nice uh so",
    "start": "1421000",
    "end": "1428559"
  },
  {
    "text": "basically what we want is what is shown here on the right side in case of netkit",
    "start": "1428559",
    "end": "1434000"
  },
  {
    "text": "um that it remains all the way in the process context uh when you want to send",
    "start": "1434000",
    "end": "1440000"
  },
  {
    "text": "the packet out of the node so that also the process scheduler in the kernel can account that time better and can make",
    "start": "1440000",
    "end": "1447120"
  },
  {
    "text": "better scheduling decisions and if you look at the performance now we see you you see this",
    "start": "1447120",
    "end": "1454080"
  },
  {
    "text": "purple bar that is added and you can see okay now the trut is as high as it is on",
    "start": "1454080",
    "end": "1459600"
  },
  {
    "text": "the on on the host right so it gives you the the full uh Power same for latency",
    "start": "1459600",
    "end": "1467440"
  },
  {
    "text": "um 99 latency is as low as it is on the host so it is um",
    "start": "1467440",
    "end": "1474759"
  },
  {
    "text": "yeah all right so now we have a zero overhead for networking for parts so the",
    "start": "1480159",
    "end": "1485760"
  },
  {
    "text": "question is can we push can we push even further we not stopping",
    "start": "1485760",
    "end": "1491480"
  },
  {
    "text": "here uh so there's an interesting technology that was added uh to the colonel from from Google engineers uh",
    "start": "1491480",
    "end": "1498279"
  },
  {
    "text": "from the TCP maintainers uh technology it's called Big TCP and that was initially merged for",
    "start": "1498279",
    "end": "1506600"
  },
  {
    "text": "IPv6 first uh I mean because most of the I think Google traffic is running on on",
    "start": "1506600",
    "end": "1512679"
  },
  {
    "text": "IPv6 it was merged in 519 and later on it also got added for ipv4 I mean uh for",
    "start": "1512679",
    "end": "1518000"
  },
  {
    "text": "the rest of the world um and the interesting thing the",
    "start": "1518000",
    "end": "1523720"
  },
  {
    "text": "the idea on on big TCP is basically to um aggregate more and and and to be able",
    "start": "1523720",
    "end": "1530360"
  },
  {
    "text": "to better cope with uh with the demands on future Nicks right so 200 400 GB and",
    "start": "1530360",
    "end": "1537279"
  },
  {
    "text": "Beyond so with those speeds um inum uh we added support for this because I",
    "start": "1537279",
    "end": "1543880"
  },
  {
    "text": "think it's a feature I mean that is very interesting and that that gives you a nice performance boost um there's",
    "start": "1543880",
    "end": "1551760"
  },
  {
    "text": "actually no changes that you need to do on your network on the MTU uh all of this is basically done on the Local Host",
    "start": "1551760",
    "end": "1559679"
  },
  {
    "text": "and uh so it will try to aggregate for many small packets like a big packet and",
    "start": "1559679",
    "end": "1565200"
  },
  {
    "text": "push it up and when we added this uh for ipv4 uh like some of the drivers they",
    "start": "1565200",
    "end": "1572480"
  },
  {
    "text": "have different limits um that they Expose and there was a nice uh uh",
    "start": "1572480",
    "end": "1579279"
  },
  {
    "text": "comment from an from an Intel engineer uh when we got the IC support added",
    "start": "1579279",
    "end": "1585159"
  },
  {
    "text": "which is like 400 gbit NX um it actually improved their request response rate",
    "start": "1585159",
    "end": "1590200"
  },
  {
    "text": "when you have it off versus have it on by 75% so it is is really impressive um the way this works as I",
    "start": "1590200",
    "end": "1598200"
  },
  {
    "text": "mentioned right so this is a typical picture of the networking stack um what",
    "start": "1598200",
    "end": "1604000"
  },
  {
    "text": "you can see here in those red box is the go engine uh it's called generic receive",
    "start": "1604000",
    "end": "1609440"
  },
  {
    "text": "offload and basically what this is trying to do is it will take mtus sized",
    "start": "1609440",
    "end": "1614760"
  },
  {
    "text": "packets when you have a TCP packet stream and it will aggregate small packets into a bigger packet here it's",
    "start": "1614760",
    "end": "1621679"
  },
  {
    "text": "64k and will push only one big packet to the upper stack so that you can save",
    "start": "1621679",
    "end": "1627559"
  },
  {
    "text": "resources that you don't have to process something end times just once and the same is also the case for the equest",
    "start": "1627559",
    "end": "1635320"
  },
  {
    "text": "path of the deck so it will push down a super sized packet and then either in",
    "start": "1635320",
    "end": "1641600"
  },
  {
    "text": "software but most network cards support this also in Hardware this is called TSO",
    "start": "1641600",
    "end": "1647240"
  },
  {
    "text": "the transmit segmentation offload it will then chunk up this packet and push it out in small",
    "start": "1647240",
    "end": "1653799"
  },
  {
    "text": "packets and yeah and then when you receive it somewhere else on the on the Node the",
    "start": "1653799",
    "end": "1660960"
  },
  {
    "text": "same happens again right you oval tra your Aggregate and so and then push it up the deack the whole problem with this",
    "start": "1660960",
    "end": "1667360"
  },
  {
    "text": "is the 64k is basically an upper size limit because what the Kel is trying to",
    "start": "1667360",
    "end": "1672640"
  },
  {
    "text": "do is it like the IP header has a total header length and that's 16 bit and",
    "start": "1672640",
    "end": "1678360"
  },
  {
    "text": "therefore the upper size limit is 64k um and big TCP basically has a way to",
    "start": "1678360",
    "end": "1685919"
  },
  {
    "text": "overcome this because for example for IPv6 it will insert a hop by hop header",
    "start": "1685919",
    "end": "1691640"
  },
  {
    "text": "in the kernel uh as a jumo gram and then you have 32bit length so you can have",
    "start": "1691640",
    "end": "1697519"
  },
  {
    "text": "much bigger aggregation limits and yeah what we did in celum uh",
    "start": "1697519",
    "end": "1705960"
  },
  {
    "text": "this is in 114 it supports both ipv4 and IPv6 we did some measurements and",
    "start": "1705960",
    "end": "1711880"
  },
  {
    "text": "192k was a uh sweet spot where you had most of the performance gains that we",
    "start": "1711880",
    "end": "1718519"
  },
  {
    "text": "saw um and that's what basically cium will",
    "start": "1718519",
    "end": "1723880"
  },
  {
    "text": "set up underneath so it will it will set up the physical devices for for for that aggregation and also all the devices on",
    "start": "1723880",
    "end": "1730279"
  },
  {
    "text": "the parts right and yeah it has probing as well because",
    "start": "1730279",
    "end": "1737159"
  },
  {
    "text": "uh the melanox Nicks that we have uh they have much higher limit for the",
    "start": "1737159",
    "end": "1742960"
  },
  {
    "text": "transmit segmentation offload uh so 192k can easily be supported for the Intel",
    "start": "1742960",
    "end": "1749279"
  },
  {
    "text": "ones they are slightly lower they have 128k uh so that's why we added probing",
    "start": "1749279",
    "end": "1754960"
  },
  {
    "text": "uh to that so that it lowers this slightly to still support other Nicks as well",
    "start": "1754960",
    "end": "1761519"
  },
  {
    "text": "and yeah so the running performance benchmarks uh the blue bar what you can",
    "start": "1761519",
    "end": "1767080"
  },
  {
    "text": "see here is the latency in micros seconds uh so we can from having it",
    "start": "1767080",
    "end": "1773600"
  },
  {
    "text": "disabled to having it enabled gives a 2X better P99 latency so even the request",
    "start": "1773600",
    "end": "1779640"
  },
  {
    "text": "response type workloads they benefit from that uh the TCP stream workloads um",
    "start": "1779640",
    "end": "1785039"
  },
  {
    "text": "as well I mean yeah it would be nice to have like a larger ni to to to to test",
    "start": "1785039",
    "end": "1791159"
  },
  {
    "text": "out um but you can definitely see the improvements and the transactions per",
    "start": "1791159",
    "end": "1797039"
  },
  {
    "text": "second from for a request response type workload they also improve very",
    "start": "1797039",
    "end": "1802080"
  },
  {
    "text": "significantly so this is a really nice feature um and it's it's a low hanging",
    "start": "1802080",
    "end": "1807919"
  },
  {
    "text": "fruit right because it's a uh to to enable it so yeah that's basically the",
    "start": "1807919",
    "end": "1815640"
  },
  {
    "text": "the complete picture of our journey uh in the future we are also looking into",
    "start": "1815640",
    "end": "1822440"
  },
  {
    "text": "other features that are currently that that have been merged just recently",
    "start": "1822440",
    "end": "1827960"
  },
  {
    "text": "or that they merged more in the future just recently there's the TCP microsc time stamp resolution so that is",
    "start": "1827960",
    "end": "1834360"
  },
  {
    "text": "interesting as well um I think we will I'm very uh I will look into that to add",
    "start": "1834360",
    "end": "1841399"
  },
  {
    "text": "this into as well um and then bbr V3 so that's basically an improved uh",
    "start": "1841399",
    "end": "1848240"
  },
  {
    "text": "version of uh the bbr TCB congestion control to basically address the the the",
    "start": "1848240",
    "end": "1854320"
  },
  {
    "text": "relatively High retransmission rate um so there's still some discussions but I think this will be as far as I'm aware",
    "start": "1854320",
    "end": "1861480"
  },
  {
    "text": "upstreamed uh very soon so yeah to conclude uh yeah know so",
    "start": "1861480",
    "end": "1868840"
  },
  {
    "text": "with some of the improvements we did from the eppf and side we can get rid of the netns of the network",
    "start": "1868840",
    "end": "1874799"
  },
  {
    "text": "namespace overhead for kubernetes ports and to be able to better deal with",
    "start": "1874799",
    "end": "1881000"
  },
  {
    "text": "100 gbit and Beyond there's big TCP which is a very interesting feature um",
    "start": "1881000",
    "end": "1886679"
  },
  {
    "text": "where you don't have to change the the the network or anything like that but you can just benefit from it uh with a",
    "start": "1886679",
    "end": "1894039"
  },
  {
    "text": "with the software change and to go even higher than that",
    "start": "1894039",
    "end": "1899720"
  },
  {
    "text": "even beyond that uh that's still work in progress on the Kernel side uh that",
    "start": "1899720",
    "end": "1905760"
  },
  {
    "text": "would very likely be t uh TCP zero copy because if you look into the uh",
    "start": "1905760",
    "end": "1911679"
  },
  {
    "text": "remaining uh performance uh uh overhead when you only",
    "start": "1911679",
    "end": "1917519"
  },
  {
    "text": "have big TCP then it's mostly on the copy site to copy the data from the kernel to the host and then TCP zero",
    "start": "1917519",
    "end": "1924000"
  },
  {
    "text": "copy comes into play right um we had in Paris few weeks ago a netcon workshop",
    "start": "1924000",
    "end": "1932120"
  },
  {
    "text": "where the you know kernel developers from the community met up and um yeah so",
    "start": "1932120",
    "end": "1938120"
  },
  {
    "text": "there are some interesting features but they are still in the works right so for TCP zero copy what would be needed is a",
    "start": "1938120",
    "end": "1944799"
  },
  {
    "text": "support for header data split in the kernel right now it doesn't really have a good framework to configure it or to even see",
    "start": "1944799",
    "end": "1951720"
  },
  {
    "text": "where the devices support this um this is going to come header data split is",
    "start": "1951720",
    "end": "1957279"
  },
  {
    "text": "needed because you want to separate the headers on one page and then the data that you then want to memory map uh to",
    "start": "1957279",
    "end": "1964519"
  },
  {
    "text": "an application to on like to to page aligned uh you know Pages uh for the",
    "start": "1964519",
    "end": "1972039"
  },
  {
    "text": "application um then the combination of big TCP and TCP zero copy is really",
    "start": "1972039",
    "end": "1977480"
  },
  {
    "text": "interesting because this gives a very low overhead for transferring megabytes",
    "start": "1977480",
    "end": "1982679"
  },
  {
    "text": "of data um and then the the the like",
    "start": "1982679",
    "end": "1989080"
  },
  {
    "text": "then what is really interesting for AI and machine learning workloads is",
    "start": "1989080",
    "end": "1994559"
  },
  {
    "text": "basically to memory map this into the GPU memory right so that you don't have",
    "start": "1994559",
    "end": "2000240"
  },
  {
    "text": "this detour through the host through the host CPU um there's also patches that",
    "start": "2000240",
    "end": "2005480"
  },
  {
    "text": "are right now under this discussion in the colel meing list but yeah",
    "start": "2005480",
    "end": "2011760"
  },
  {
    "text": "so with that uh my talk is concluded and if you have any questions I'm happy to",
    "start": "2011760",
    "end": "2017320"
  },
  {
    "text": "take them thank [Applause]",
    "start": "2017320",
    "end": "2029210"
  },
  {
    "text": "you so a lot of the uh optimizations that you mentioned today require um",
    "start": "2031600",
    "end": "2037919"
  },
  {
    "text": "relatively recent kernel versions does cium detect which features are available",
    "start": "2037919",
    "end": "2044399"
  },
  {
    "text": "based on kernel version or does it have like much stricter minimum kernel requirements now yeah I mean like the",
    "start": "2044399",
    "end": "2050398"
  },
  {
    "text": "minimum kernel requirements for celium is right now 419 so it's really old I mean we like",
    "start": "2050399",
    "end": "2057960"
  },
  {
    "text": "before that we had 4.9 we recently bumped that because 4.9 is um I think",
    "start": "2057960",
    "end": "2063158"
  },
  {
    "text": "out of out of date I mean it's super old um but yeah I mean we basically uh",
    "start": "2063159",
    "end": "2069760"
  },
  {
    "text": "detect whether a feature can be used and it's also up to the user like depending",
    "start": "2069760",
    "end": "2075200"
  },
  {
    "text": "on what kind of setup they or like what kind of constraints they have and so on so you can opt into",
    "start": "2075200",
    "end": "2080839"
  },
  {
    "text": "this yeah thanks for your presentation you",
    "start": "2080839",
    "end": "2088040"
  },
  {
    "text": "the big TCP example you say you turn it on on off course we're on uh",
    "start": "2088040",
    "end": "2093638"
  },
  {
    "text": "114.3 um how do you turn it on and off cuz Val of Yama what flag are we looking",
    "start": "2093639",
    "end": "2099200"
  },
  {
    "text": "for okay uh uh so it it it should be in the documentation of celium there's a flag",
    "start": "2099200",
    "end": "2106599"
  },
  {
    "text": "and Helm and I don't have it in my head right now um but I have linked it here",
    "start": "2106599",
    "end": "2112240"
  },
  {
    "text": "uh so it's actually in the slides if you want to look it up so yeah uh yeah I have one question uh so",
    "start": "2112240",
    "end": "2120000"
  },
  {
    "text": "the standard use case is to have like cetes uh in a VM",
    "start": "2120000",
    "end": "2125640"
  },
  {
    "text": "right uh uh in in a public Cloud so usually does all this somehow uh work",
    "start": "2125640",
    "end": "2134520"
  },
  {
    "text": "there as well yeah it does it does and um so for I mean like even the xtp based",
    "start": "2134520",
    "end": "2142359"
  },
  {
    "text": "load balancing a lot of public Cloud providers they offer sov based networking and then you get for example",
    "start": "2142359",
    "end": "2148160"
  },
  {
    "text": "for the melanox or Intel you can attach xtp natively there so that's no problem",
    "start": "2148160",
    "end": "2154119"
  },
  {
    "text": "okay that's exactly what I was looking for and all the improvements that I mentioned here like with the weave",
    "start": "2154119",
    "end": "2159920"
  },
  {
    "text": "device replacement bandwidth manager and so on and so forth also big TCP they",
    "start": "2159920",
    "end": "2165160"
  },
  {
    "text": "also work all on the public Cloud so yeah hi um same question what about",
    "start": "2165160",
    "end": "2173680"
  },
  {
    "text": "VMware again any issues with all these uh options on uh open V switch with",
    "start": "2173680",
    "end": "2180839"
  },
  {
    "text": "VMware oh um so the open V switch is uh",
    "start": "2180839",
    "end": "2187079"
  },
  {
    "text": "I mean yeah yeah with open V switch you I'm not I mean doesn't support open",
    "start": "2187079",
    "end": "2192520"
  },
  {
    "text": "V switch right so open V switch is a different data path uh technology",
    "start": "2192520",
    "end": "2198359"
  },
  {
    "text": "compared to ebpf uh but I presume maybe you mean you you meant open shift right",
    "start": "2198359",
    "end": "2204040"
  },
  {
    "text": "if I'm okay we we can take it offline it's it's",
    "start": "2204040",
    "end": "2210359"
  },
  {
    "text": "fine I think the question was about uh viralization device or network devices",
    "start": "2210359",
    "end": "2215520"
  },
  {
    "text": "most of those features don't",
    "start": "2215520",
    "end": "2218680"
  },
  {
    "text": "you mean the sov that did you require native rization Network rization device",
    "start": "2221520",
    "end": "2227119"
  },
  {
    "text": "in V spere doesn't support any of those features okay I I'm not too familiar with vhere to be honest um",
    "start": "2227119",
    "end": "2236000"
  },
  {
    "text": "but the xtp one is is is the only one where you have specific Nick requirements and yeah it doesn't support",
    "start": "2236000",
    "end": "2243119"
  },
  {
    "text": "it so if you want to access that on you need to set up your Mach s okay",
    "start": "2243119",
    "end": "2250720"
  },
  {
    "text": "can yeah all",
    "start": "2250720",
    "end": "2254359"
  },
  {
    "text": "right hi thanks for the presentation so my question is like if you have a multi-",
    "start": "2257920",
    "end": "2263240"
  },
  {
    "text": "tency and we are doing stuff ebf ebbf on the Kernel right are there any security",
    "start": "2263240",
    "end": "2269040"
  },
  {
    "text": "concerns there I mean like so so ebpf itself is",
    "start": "2269040",
    "end": "2274720"
  },
  {
    "text": "set up from syum C I right and there's there's one cni running there and that's setting up the programs inside the host",
    "start": "2274720",
    "end": "2282000"
  },
  {
    "text": "ebpf programs when they are pushed into the kernel they're being uh verified for",
    "start": "2282000",
    "end": "2287040"
  },
  {
    "text": "safety so that you don't do harm to the kernel so basically the verifier will do",
    "start": "2287040",
    "end": "2293839"
  },
  {
    "text": "a static analysis of the program to understand what the program is doing and reject it um",
    "start": "2293839",
    "end": "2301560"
  },
  {
    "text": "and that is basically also restricted to root only so basically um applications",
    "start": "2301560",
    "end": "2307640"
  },
  {
    "text": "in the in the cube system um namespace will be able to use it such as for",
    "start": "2307640",
    "end": "2313920"
  },
  {
    "text": "example but other application Parts not really so yeah thank",
    "start": "2313920",
    "end": "2322520"
  },
  {
    "text": "you all right one more",
    "start": "2323119",
    "end": "2328838"
  },
  {
    "text": "question uh one for presentation and uh as I know the big TC C uh IPv6 will set",
    "start": "2329440",
    "end": "2339400"
  },
  {
    "text": "the lens field to larrow in the uh header and add some extension header in",
    "start": "2339400",
    "end": "2345440"
  },
  {
    "text": "the uh IP video uh header uh uh so if if",
    "start": "2345440",
    "end": "2351560"
  },
  {
    "text": "uh a server run outside the cluster and no big TCP uh if uh and the client run",
    "start": "2351560",
    "end": "2359240"
  },
  {
    "text": "in the p in the cluster and uh uh big big TCP is enabl can you can can learn",
    "start": "2359240",
    "end": "2367440"
  },
  {
    "text": "uh communicate with each other and if if if not how to solve this issue can they",
    "start": "2367440",
    "end": "2373200"
  },
  {
    "text": "can can they do what can you repat the last can they communicate with each other oh yeah yeah this uh extension",
    "start": "2373200",
    "end": "2379839"
  },
  {
    "text": "header that is being added is only local on the Hol right so it's not going out to the network so this is really only in",
    "start": "2379839",
    "end": "2387000"
  },
  {
    "text": "order to push a larger packet up to stack but other than that um it's not",
    "start": "2387000",
    "end": "2393280"
  },
  {
    "text": "going onto the wire oh thank you so I I I found Crum document so I ask okay",
    "start": "2393280",
    "end": "2400319"
  },
  {
    "text": "today yeah thank you hi uh thanks for your talk uh quick",
    "start": "2400319",
    "end": "2406920"
  },
  {
    "text": "question about Express data path um versus sort of ebpf what led to your decision to use that as a load balancing",
    "start": "2406920",
    "end": "2415040"
  },
  {
    "text": "um piece yeah so the express data path I mean that's basically an attachment layer for ebpf inside the driver and I",
    "start": "2415040",
    "end": "2423119"
  },
  {
    "text": "mean all the major drivers by now do support it um so I mean given we",
    "start": "2423119",
    "end": "2429119"
  },
  {
    "text": "initially implemented this uh like the service L balancing and so on with the TC BPF the xtp uh extension to be able",
    "start": "2429119",
    "end": "2437280"
  },
  {
    "text": "to use that was a natural fit uh given you can benefit so much from it uh",
    "start": "2437280",
    "end": "2443200"
  },
  {
    "text": "because the overhead is so low and yeah what what what why I mean because you can move this closer to the closer to",
    "start": "2443200",
    "end": "2450440"
  },
  {
    "text": "the source right so that's like really the first possible point in the software stack where something can be processed",
    "start": "2450440",
    "end": "2456880"
  },
  {
    "text": "where something gets picked up from the driver from the receive rings and you can run and do some actions with it so",
    "start": "2456880",
    "end": "2463520"
  },
  {
    "text": "yeah it was a natural",
    "start": "2463520",
    "end": "2467319"
  },
  {
    "text": "fit oh yeah absolutely yeah I was just under the impression that was sort of like an Intel um Express data path piece",
    "start": "2469480",
    "end": "2476400"
  },
  {
    "text": "ah no no no I mean all major vendors they support xtp also Cloud providers as",
    "start": "2476400",
    "end": "2481839"
  },
  {
    "text": "well okay gotcha okay cool that that makes sense then thank you okay perfect",
    "start": "2481839",
    "end": "2487119"
  },
  {
    "text": "all right thank you very much",
    "start": "2487119",
    "end": "2492599"
  }
]