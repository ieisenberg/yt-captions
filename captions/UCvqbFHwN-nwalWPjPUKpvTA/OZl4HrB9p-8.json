[
  {
    "start": "0",
    "end": "62000"
  },
  {
    "text": "welcome everybody we are going to talk about with tests the name of our talk is",
    "start": "260",
    "end": "6569"
  },
  {
    "text": "failure is always an option what we mean by that is that when you're running",
    "start": "6569",
    "end": "12210"
  },
  {
    "text": "distributed systems number two has a",
    "start": "12210",
    "end": "18449"
  },
  {
    "text": "story behind it sorry failure is always an option okay",
    "start": "18449",
    "end": "29900"
  },
  {
    "text": "so what I mean by that is that when you are running of a distributed system",
    "start": "32239",
    "end": "38450"
  },
  {
    "text": "network partitions happen parts get evicted and failures happen and your",
    "start": "38450",
    "end": "46640"
  },
  {
    "text": "systems need to look at need to deal with it that's what we are going to talk about how to build a resilient global",
    "start": "46640",
    "end": "53070"
  },
  {
    "text": "database platform using the test so walk away over there okay so you're not near",
    "start": "53070",
    "end": "60090"
  },
  {
    "text": "these mics yeah my name is de pena I am the CEO of planet-scale which is a",
    "start": "60090",
    "end": "66900"
  },
  {
    "start": "62000",
    "end": "148000"
  },
  {
    "text": "company that helps people run Windows clusters I am the CEO of planet-scale",
    "start": "66900",
    "end": "76430"
  },
  {
    "text": "sugu who is here who is the main developer for Vitesse and I used to work",
    "start": "76430",
    "end": "82200"
  },
  {
    "text": "together at YouTube where witness was born to scale YouTube's databases I ran",
    "start": "82200",
    "end": "87930"
  },
  {
    "text": "YouTube's asari and TB teams that operated the Vitesse clusters",
    "start": "87930",
    "end": "93420"
  },
  {
    "text": "with me my co-speaker is that Kozlowski",
    "start": "93420",
    "end": "99189"
  },
  {
    "text": "dan and I work together at in Washington DC at the United States digital service",
    "start": "99189",
    "end": "104560"
  },
  {
    "text": "which is a small government department that helps the US government to run",
    "start": "104560",
    "end": "111369"
  },
  {
    "text": "their IT more efficiently and dan used to be the chief cloud architect for the",
    "start": "111369",
    "end": "119200"
  },
  {
    "text": "Veterans Administration in the United States if you just go by the budget that would be in top 50 companies in the",
    "start": "119200",
    "end": "127840"
  },
  {
    "text": "world or something like that when I asked them to come and work with me",
    "start": "127840",
    "end": "132940"
  },
  {
    "text": "building cannon scale he had one condition and that was that he needed he wanted to choose his own title so let me",
    "start": "132940",
    "end": "140140"
  },
  {
    "text": "present the Minister of engineering at pilot-scale all right so let's start off",
    "start": "140140",
    "end": "150970"
  },
  {
    "start": "148000",
    "end": "600000"
  },
  {
    "text": "by talking about witness all right the witness architecture so the reason that I wanted to start out here is that first",
    "start": "150970",
    "end": "157690"
  },
  {
    "text": "let me describe to you how witness works and then we can think about how to run",
    "start": "157690",
    "end": "163269"
  },
  {
    "text": "it a in kubernetes be in multiple kubernetes clusters across multiple data",
    "start": "163269",
    "end": "169600"
  },
  {
    "text": "centers right so what you see on the left hand side of this line are the app",
    "start": "169600",
    "end": "176950"
  },
  {
    "text": "servers these are running these are stateless servers presumably running in their own thoughts some bad jobs that",
    "start": "176950",
    "end": "184209"
  },
  {
    "text": "are running every night so on and so forth all connect to an elf or load",
    "start": "184209",
    "end": "190090"
  },
  {
    "text": "balancer which are front which are backed by many many instances of BTK",
    "start": "190090",
    "end": "196959"
  },
  {
    "text": "process okay so we TK to the state this proxy that is approve my sequel parcel built into it and that speaks the",
    "start": "196959",
    "end": "203850"
  },
  {
    "text": "my sequel binary fellow to the load balancer as far as they are concerned",
    "start": "203850",
    "end": "209610"
  },
  {
    "text": "they are connected to a single humongous my sequel database right so maybe with",
    "start": "209610",
    "end": "217050"
  },
  {
    "text": "many VT gates all stateless they connect in turn to the multiple sharks that we",
    "start": "217050",
    "end": "224700"
  },
  {
    "text": "present the key space so key space with a single shot is exactly identical to a",
    "start": "224700",
    "end": "229710"
  },
  {
    "text": "database but when you shot it you have n shards all the same schema that",
    "start": "229710",
    "end": "236430"
  },
  {
    "text": "represent the key space okay each shark is a my sequel cluster with one master",
    "start": "236430",
    "end": "242790"
  },
  {
    "text": "and multiple replicas and what is not represented here is the dimension of",
    "start": "242790",
    "end": "247980"
  },
  {
    "text": "multiple cells so if it s supports this notion of cells and the cell is nothing",
    "start": "247980",
    "end": "255420"
  },
  {
    "text": "but a failure to it right in the data center or lack might here maybe in a",
    "start": "255420",
    "end": "261900"
  },
  {
    "text": "cloud provider and availability zone might be a cell or a region might be yourself so depending on how you are",
    "start": "261900",
    "end": "268830"
  },
  {
    "text": "architecting your environment a cell is a failure domain right so in every shot",
    "start": "268830",
    "end": "277140"
  },
  {
    "text": "the master and the replicas can be distributed across multiple cells so",
    "start": "277140",
    "end": "282270"
  },
  {
    "text": "witness knows about cells from ground up right and the reason that witness knows about all this and is so good at running",
    "start": "282270",
    "end": "289530"
  },
  {
    "text": "in kubernetes is because in Google it used to run born which was the precursor",
    "start": "289530",
    "end": "296340"
  },
  {
    "text": "to kubernetes in poo right so topo",
    "start": "296340",
    "end": "302100"
  },
  {
    "text": "seller is basically a where all the cluster topology is stored and when it",
    "start": "302100",
    "end": "307350"
  },
  {
    "text": "is typically it's an HDD quorum but it can be a zookeeper core American bacon so core we support all three because",
    "start": "307350",
    "end": "313800"
  },
  {
    "text": "again in I used to use something quite heavy which is Google's boxer VC TLD is the",
    "start": "313800",
    "end": "321490"
  },
  {
    "text": "control panel the control panel gives you a web UI I think that will be",
    "start": "321490",
    "end": "328210"
  },
  {
    "text": "showing us that web UI and it also you can also send command like command lines",
    "start": "328210",
    "end": "334270"
  },
  {
    "text": "to it right the same commands that you can use the web UI to execute you can also send them through a command like",
    "start": "334270",
    "end": "340630"
  },
  {
    "text": "multiple command line means so that's witnessed in a nutshell also",
    "start": "340630",
    "end": "347020"
  },
  {
    "text": "want to mention I just want one second that focus ever is not part of the query",
    "start": "347020",
    "end": "352509"
  },
  {
    "text": "path right so topo sever has that has the information about how a particular",
    "start": "352509",
    "end": "357970"
  },
  {
    "text": "space is Charlotte as well as the information about the cluster topology but all that information is cached by",
    "start": "357970",
    "end": "366009"
  },
  {
    "text": "these v3 gates so that even if your proposal goes down your query path doesn't surface next night so wipe it s",
    "start": "366009",
    "end": "377710"
  },
  {
    "text": "so we are going to do this global resilient database using witness why is",
    "start": "377710",
    "end": "382990"
  },
  {
    "text": "Britain's ideally suited for a system like this as I said vp8 is a stateless",
    "start": "382990",
    "end": "389169"
  },
  {
    "text": "entry point that allows scaling if you suddenly need to add a thousand more app",
    "start": "389169",
    "end": "395530"
  },
  {
    "text": "servers you increase your meeting it's 200 or something like that and your system scales right there is no",
    "start": "395530",
    "end": "401949"
  },
  {
    "text": "connection number of issues and so on you can depending on what you want you can scale up and scale",
    "start": "401949",
    "end": "408299"
  },
  {
    "text": "down the number of readings every my Sigma T in the system gets a little",
    "start": "408299",
    "end": "414569"
  },
  {
    "text": "tablet process which is a mind that for that my sleep and we T tablet also has a",
    "start": "414569",
    "end": "420689"
  },
  {
    "text": "full my sequel powers are built into it we T tablet a does things like",
    "start": "420689",
    "end": "426679"
  },
  {
    "text": "connection put into the mice equally it puts a timeout on every query it puts a",
    "start": "426679",
    "end": "433079"
  },
  {
    "text": "timeout on every transaction it has things like hot draw protection built into it so in general you're my security",
    "start": "433079",
    "end": "439679"
  },
  {
    "text": "is a lot more robust running underneath the supervision of within tablet than",
    "start": "439679",
    "end": "445409"
  },
  {
    "text": "otherwise so even for a single non child and my sequel cluster it will perform a",
    "start": "445409",
    "end": "450809"
  },
  {
    "text": "lot better underneath bit as the other way so weekly tablet is really useful basically",
    "start": "450809",
    "end": "457019"
  },
  {
    "text": "all of you probably have had experienced whether in experienced developer writes",
    "start": "457019",
    "end": "465299"
  },
  {
    "text": "a query that brings down the home icicle database query of death that pretty much does not happen 100 winners because will",
    "start": "465299",
    "end": "472949"
  },
  {
    "text": "kill you where is that are returning to many roles or that everything for too long right I see a lot of knowing smiles here",
    "start": "472949",
    "end": "486440"
  },
  {
    "text": "medic backups and restores so when you have multiple my signal clusters you",
    "start": "486440",
    "end": "491940"
  },
  {
    "text": "need some automation and health around managing backups and restores and also a process for building them for bringing",
    "start": "491940",
    "end": "499050"
  },
  {
    "text": "up new tablets which doesn't involve database administrator figuring out how",
    "start": "499050",
    "end": "505410"
  },
  {
    "text": "to bring up a new tablet and so on so there is automation built-in Fitness which figures out for I am a tablet I",
    "start": "505410",
    "end": "513149"
  },
  {
    "text": "belong to this shard of this key space the last backup was taken for this shot",
    "start": "513149",
    "end": "519029"
  },
  {
    "text": "and this key space at such-and-such a time this is the s3 bucket name for that let",
    "start": "519029",
    "end": "525870"
  },
  {
    "text": "me go get all of that let me start my my sequel D and let me start my might not",
    "start": "525870",
    "end": "531300"
  },
  {
    "text": "mind the replication logs from the current master bring bring it up to a",
    "start": "531300",
    "end": "537930"
  },
  {
    "text": "certain threshold and start serving the travel all of that automation is built-in do witness using backups and",
    "start": "537930",
    "end": "546600"
  },
  {
    "text": "restores so bringing up new parts serving data is health checks and",
    "start": "546600",
    "end": "555029"
  },
  {
    "text": "observability is also built into attest and because of that something that is absolutely necessary",
    "start": "555029",
    "end": "561770"
  },
  {
    "text": "for running a distributed system in Google raise another another nice",
    "start": "561770",
    "end": "567290"
  },
  {
    "text": "property of witness is that all of the middleware that I've talked about so VT",
    "start": "567290",
    "end": "572420"
  },
  {
    "text": "tablets wiki gates they are all pretty much stateless and the state is either",
    "start": "572420",
    "end": "580160"
  },
  {
    "text": "in the client which know where which of course these to know about the key space that it is talking to or it's in the",
    "start": "580160",
    "end": "588560"
  },
  {
    "text": "databases where you are actually saving the state state full data right but all the middleware is completely stateless",
    "start": "588560",
    "end": "595990"
  },
  {
    "text": "so that's another good property next like this so this is the app this is",
    "start": "595990",
    "end": "604970"
  },
  {
    "start": "600000",
    "end": "690000"
  },
  {
    "text": "again I talked a little bit about ourselves you can support multiple cloud",
    "start": "604970",
    "end": "610790"
  },
  {
    "text": "regions using cells and the cell basically represents a unit of availability and also locality and you",
    "start": "610790",
    "end": "619040"
  },
  {
    "text": "can't run things under political communities clusters also because there",
    "start": "619040",
    "end": "624650"
  },
  {
    "text": "are multiple shards you can scale your rights as well as you can geo locate the",
    "start": "624650",
    "end": "631310"
  },
  {
    "text": "data what I mean by that is that if you have users in Japan hitting application",
    "start": "631310",
    "end": "638510"
  },
  {
    "text": "servers in Japan you can make sure that the their real queries are served by the",
    "start": "638510",
    "end": "645410"
  },
  {
    "text": "real applicants in Japan rather than from the Masters in Mountain View all of",
    "start": "645410",
    "end": "651020"
  },
  {
    "text": "this is again with us at YouTube the biggest key space that we have was 256 shards so 256",
    "start": "651020",
    "end": "659650"
  },
  {
    "text": "pastors each master had between 80 to 120 replicas distributed over 20 cells",
    "start": "659650",
    "end": "666340"
  },
  {
    "text": "globally so four to five replicas per cell in 20 cells globally and when",
    "start": "666340",
    "end": "672370"
  },
  {
    "text": "people connected to app servers running in those data centers the real traffic",
    "start": "672370",
    "end": "677650"
  },
  {
    "text": "was load balanced across replicas running locally it's only if somebody wrote something that went everywhere",
    "start": "677650",
    "end": "688589"
  },
  {
    "start": "690000",
    "end": "773000"
  },
  {
    "text": "okay this is I'm gonna hang on that mic",
    "start": "690720",
    "end": "697720"
  },
  {
    "text": "I think this one's wrong hi so gehenna has told you a little bit about what the global stateful kubernetes app looks",
    "start": "697720",
    "end": "704920"
  },
  {
    "text": "like but we wanted to show you what a global stateful kubernetes app looks like so what we've done is we've set up a little demo here I am gonna need your",
    "start": "704920",
    "end": "711220"
  },
  {
    "text": "help so just stand by what do you see here on the screen is the architecture that I have put together I would have",
    "start": "711220",
    "end": "716830"
  },
  {
    "text": "loved to show it to you live but it does take just a little bit too long so we have three different cells provisioned",
    "start": "716830",
    "end": "723610"
  },
  {
    "text": "two of them are here in Europe this is all in Google GCP so we have one in the",
    "start": "723610",
    "end": "730390"
  },
  {
    "text": "EU West one region one in the EU West four region and then just for kicks and",
    "start": "730390",
    "end": "735790"
  },
  {
    "text": "giggles and because I ran out of quota in Europe we have one in America so we have GU we have the United States East",
    "start": "735790",
    "end": "742720"
  },
  {
    "text": "one GU west one and E us for each of them are running a set of application servers and a set of attest shards they",
    "start": "742720",
    "end": "749860"
  },
  {
    "text": "are all fronted by load balancers and I'm actually not sure what's going on",
    "start": "749860",
    "end": "755170"
  },
  {
    "text": "with the ingress because I know Google has a global cloud router and it appears that everybody here at this conference",
    "start": "755170",
    "end": "761200"
  },
  {
    "text": "is in fact hitting the EU 4 region and that maybe Google shenanigans I'm not",
    "start": "761200",
    "end": "767500"
  },
  {
    "text": "sure but this is generally what it looks like so what I need all of you to do go",
    "start": "767500",
    "end": "773080"
  },
  {
    "text": "ahead and grab your phones and head out to dojo planet-scale labs comm and what",
    "start": "773080",
    "end": "780160"
  },
  {
    "text": "you're gonna see is the world's next big app it's a little bit self-explanatory but I'll give you a demo that you can help me what you're gonna",
    "start": "780160",
    "end": "786610"
  },
  {
    "text": "be doing is generating some load for this cluster so we can actually show what's going on I'm gonna swap screens",
    "start": "786610",
    "end": "792520"
  },
  {
    "text": "real quick nope I don't know what that",
    "start": "792520",
    "end": "799480"
  },
  {
    "text": "was okay so what I need to do is I need",
    "start": "799480",
    "end": "810339"
  },
  {
    "text": "you guys to start rating the dogs for me and yes alright let's try that and of",
    "start": "810339",
    "end": "827740"
  },
  {
    "text": "all the technical things that we're gonna break I did not think it would be this one all right we'll just alright",
    "start": "827740",
    "end": "844720"
  },
  {
    "text": "there we go cool nope not yet",
    "start": "844720",
    "end": "853380"
  },
  {
    "text": "okay all right so now we're back live all right so you all should see the",
    "start": "874820",
    "end": "880700"
  },
  {
    "text": "goodest aw go you should be able to rate the goodest dog oh okay so here is our nice little prometheus cluster let's see",
    "start": "880700",
    "end": "886730"
  },
  {
    "text": "how good you're all doing oh beautiful look at that alright so we've started generate some traffic on there what I",
    "start": "886730",
    "end": "893690"
  },
  {
    "text": "want to do is walk you through a little bit about the environment and then show you some common failures that we have so",
    "start": "893690",
    "end": "899720"
  },
  {
    "text": "here is the of the test dashboard this is again this is all part of open source",
    "start": "899720",
    "end": "904880"
  },
  {
    "start": "900000",
    "end": "945000"
  },
  {
    "text": "for tests here you see that I have two key spaces one called doggers one called lookup Dockers is split into four",
    "start": "904880",
    "end": "911780"
  },
  {
    "text": "different shards if I go into one of those shards you can see the layout of",
    "start": "911780",
    "end": "916940"
  },
  {
    "text": "the tablets I have a lot of read replicas here mainly because I'm lazy and I was really hoping this would take",
    "start": "916940",
    "end": "923300"
  },
  {
    "text": "off and be the next big thing so just optimistically provisioning here right",
    "start": "923300",
    "end": "932840"
  },
  {
    "text": "so right now the master is currently being served out of EU s4 which again that is the region I believe that is",
    "start": "932840",
    "end": "938900"
  },
  {
    "text": "closest to Spain here I may have been wrong about that but what I'm also gonna do is I also have for demonstration",
    "start": "938900",
    "end": "945920"
  },
  {
    "start": "945000",
    "end": "965000"
  },
  {
    "text": "purposes I have a little script here that is just writing some automatic",
    "start": "945920",
    "end": "950960"
  },
  {
    "text": "ratings into the application don't worry they are all very good ratings so we're not doing anything mean to these dogs",
    "start": "950960",
    "end": "957130"
  },
  {
    "text": "and then I have a few other tablets to go through what's going on other things that I want to introduce you to while",
    "start": "957130",
    "end": "963500"
  },
  {
    "text": "I'm here is this is called this is the VT gate interface so the VT gate",
    "start": "963500",
    "end": "968960"
  },
  {
    "start": "965000",
    "end": "1000000"
  },
  {
    "text": "interface shows us a little bit about query traffic that's going on again we have a number of these so the traffic",
    "start": "968960",
    "end": "976070"
  },
  {
    "text": "that you see here is not necessarily going to match up with the traffic across the whole cluster to do that we",
    "start": "976070",
    "end": "981860"
  },
  {
    "text": "rely on Prometheus which all of the metrics are exported by Prometheus so we have great visibility into not only the",
    "start": "981860",
    "end": "988580"
  },
  {
    "text": "queries coming in but how fast they're being serviced and I'll you know there was a little bit of enthusiasm there for",
    "start": "988580",
    "end": "994760"
  },
  {
    "text": "a bit but then I guess people got tired of reading dogs so we're still at a healthy 20 queries per second so the",
    "start": "994760",
    "end": "1002350"
  },
  {
    "start": "1000000",
    "end": "1025000"
  },
  {
    "text": "last thing I wanna introduce is this is a tool called Orchestrator this isn't technically part of it",
    "start": "1002350",
    "end": "1008020"
  },
  {
    "text": "but we integrate tightly with it I don't know how many of you are familiar with Orchestrator its project by github this",
    "start": "1008020",
    "end": "1013270"
  },
  {
    "text": "integrates natively with the test so we can also see here just like in the Vitesse dashboard I have all of my",
    "start": "1013270",
    "end": "1018730"
  },
  {
    "text": "shards if I go into one of my shards I can see all my replicas and I can see",
    "start": "1018730",
    "end": "1024610"
  },
  {
    "text": "how they are responding great great nice and easy okay so obviously there's a few",
    "start": "1024610",
    "end": "1031000"
  },
  {
    "start": "1025000",
    "end": "1040000"
  },
  {
    "text": "things that we may want to have to do with this cluster so let's say that we realize that we are getting a lot of",
    "start": "1031000",
    "end": "1037270"
  },
  {
    "text": "traffic in a different region right now all the Masters like I said I think all",
    "start": "1037270",
    "end": "1042819"
  },
  {
    "start": "1040000",
    "end": "1065000"
  },
  {
    "text": "of them are located in Europe or maybe not all of them I have 4 4 4 4 ok so all",
    "start": "1042819",
    "end": "1052990"
  },
  {
    "text": "of my masters are in Europe but maybe I'm getting a lot of traffic coming in through another region and I want to optimize a little bit but I can actually",
    "start": "1052990",
    "end": "1059260"
  },
  {
    "text": "do is I can do a planned reparent so I will pick one of my shards and I'm just",
    "start": "1059260",
    "end": "1066040"
  },
  {
    "start": "1065000",
    "end": "1090000"
  },
  {
    "text": "gonna click on the hamburger click on planned reparent and go ahead and I'm actually gonna change the master to one",
    "start": "1066040",
    "end": "1071650"
  },
  {
    "text": "that is in sell us West one looks like two thousand and one is in US West one",
    "start": "1071650",
    "end": "1077590"
  },
  {
    "text": "should go ahead and pick it and repacked it's gonna think about it for a little bit and it's gonna say good I've",
    "start": "1077590",
    "end": "1084460"
  },
  {
    "text": "successfully repented now if you are all rating dogs furiously you'll notice",
    "start": "1084460",
    "end": "1089770"
  },
  {
    "text": "there was no interruption traffic there and I can go back to my command line app and I'm just chug chug chugging along",
    "start": "1089770",
    "end": "1095220"
  },
  {
    "start": "1090000",
    "end": "1120000"
  },
  {
    "text": "the test can actually switch between who is the master completely transparently to the applications using it so we can",
    "start": "1095220",
    "end": "1101559"
  },
  {
    "text": "swap between different regions we can count away from nodes if we have to put a node into into maintenance or if we",
    "start": "1101559",
    "end": "1108880"
  },
  {
    "text": "want to cycle notes in and out so we can go ahead and do that so that's the first fun thing that",
    "start": "1108880",
    "end": "1114040"
  },
  {
    "text": "happens the second thing that happens is things fail right so obviously things",
    "start": "1114040",
    "end": "1119260"
  },
  {
    "text": "fail let's make things fail so keep CTO get all again this is running in three",
    "start": "1119260",
    "end": "1124780"
  },
  {
    "start": "1120000",
    "end": "1170000"
  },
  {
    "text": "kubernetes clusters across three regions there's no Federation going on here because I don't really trust Federation",
    "start": "1124780",
    "end": "1131050"
  },
  {
    "text": "and I think that's generally how we all feel at this point so I actually have",
    "start": "1131050",
    "end": "1136090"
  },
  {
    "text": "three different kubernetes clusters thankfully the test works really well in that situation we just have a global",
    "start": "1136090",
    "end": "1141370"
  },
  {
    "text": "server and a global Orchestrator and then Vitesse handles the coordination between the three of them so I am in",
    "start": "1141370",
    "end": "1147490"
  },
  {
    "text": "u.s. East one I'm gonna go ahead and make sure I don't kill a master but I'm",
    "start": "1147490",
    "end": "1152740"
  },
  {
    "text": "gonna go ahead and kill tablet 2003 so fine tablet 2003 oh that's us East one",
    "start": "1152740",
    "end": "1160840"
  },
  {
    "text": "nope so I don't want to kill anybody on us East one there you go",
    "start": "1160840",
    "end": "1167380"
  },
  {
    "text": "all right so actually I can kill anybody on us East one but I mean that's not",
    "start": "1167380",
    "end": "1175240"
  },
  {
    "start": "1170000",
    "end": "1235000"
  },
  {
    "text": "terribly exciting so we'll let that guy die again this could be right now I'm deleting it you",
    "start": "1175240",
    "end": "1181270"
  },
  {
    "text": "know I'm a silly sysadmin who's just randomly deleting stuff because I don't know what I'm doing but again if this",
    "start": "1181270",
    "end": "1186610"
  },
  {
    "text": "was actually when we were preparing for this talk something happened to my kubernetes cluster node went down tab we",
    "start": "1186610",
    "end": "1193030"
  },
  {
    "text": "went down it happened to be the master and I'll show that in a bit but um it",
    "start": "1193030",
    "end": "1198850"
  },
  {
    "text": "responds the same way whether it's a hard failure or me deleting it so we haven't done any tricks with like delete",
    "start": "1198850",
    "end": "1205210"
  },
  {
    "text": "hooks or anything like that there's no sleight of hand going on I am just deleting at a pod so that's gone",
    "start": "1205210",
    "end": "1211900"
  },
  {
    "text": "I'm gonna go ahead and switch contexts real quick and get to the European",
    "start": "1211900",
    "end": "1218560"
  },
  {
    "text": "context and go ahead and delete tablet 2002 okay",
    "start": "1218560",
    "end": "1228450"
  },
  {
    "text": "so what's going on let's make sure we can still rate some poppers oh you're a",
    "start": "1232890",
    "end": "1238570"
  },
  {
    "start": "1235000",
    "end": "1255000"
  },
  {
    "text": "good dog you are to 13 so I don't know",
    "start": "1238570",
    "end": "1244240"
  },
  {
    "text": "if you notice on your phones the scale actually goes to 13 because these are the Buddhist dollars they're not just",
    "start": "1244240",
    "end": "1250090"
  },
  {
    "text": "normal doggers okay so we're still able to rate we can see here in BTK I'll go",
    "start": "1250090",
    "end": "1255130"
  },
  {
    "start": "1255000",
    "end": "1285000"
  },
  {
    "text": "ahead and refresh it and it's telling us that some of the replicas are down the test doesn't really you know it lets you",
    "start": "1255130",
    "end": "1260260"
  },
  {
    "text": "know that it's now but because of those health checks that your 10 was talking about we're not gonna see queries fail we were actually gonna see the tests",
    "start": "1260260",
    "end": "1267730"
  },
  {
    "text": "knows immediately I mean within a fixed amount of time that a tablet has gone down and it will in fact reroute all",
    "start": "1267730",
    "end": "1273760"
  },
  {
    "text": "your queries seamlessly so you know traffic chugs along normally so there's",
    "start": "1273760",
    "end": "1281290"
  },
  {
    "text": "that the next thing that can obviously happen here is we could have a master",
    "start": "1281290",
    "end": "1287860"
  },
  {
    "start": "1285000",
    "end": "1305000"
  },
  {
    "text": "tablet fail so master tablets are used for all of our rights so let's go ahead",
    "start": "1287860",
    "end": "1292990"
  },
  {
    "text": "and find where one of the masters is it looks like the master for the c0 shard",
    "start": "1292990",
    "end": "1298780"
  },
  {
    "text": "is located at 2001 so let's go ahead and kill that one Orchestrator is still",
    "start": "1298780",
    "end": "1304990"
  },
  {
    "text": "pretty happy it hasn't detected any failures which is about right actually it has detected",
    "start": "1304990",
    "end": "1311170"
  },
  {
    "start": "1305000",
    "end": "1320000"
  },
  {
    "text": "failures now that it comes up you see those two black dots there was the tablet those are the tablets that it thought were there that are now gone",
    "start": "1311170",
    "end": "1316930"
  },
  {
    "text": "because I deleted them so let's go ahead and kill a master all right so I'm gonna kill 2001 Boop okay so I've now",
    "start": "1316930",
    "end": "1328890"
  },
  {
    "start": "1320000",
    "end": "1380000"
  },
  {
    "text": "unplanned killed the master it is now gone in a normal database world this is",
    "start": "1328890",
    "end": "1334330"
  },
  {
    "text": "kind of catastrophic right also all of these pods are completely ephemeral so there there's no persistent volumes",
    "start": "1334330",
    "end": "1340750"
  },
  {
    "text": "backing these up they are actually running on empty what is it empty Drive empty disk",
    "start": "1340750",
    "end": "1348660"
  },
  {
    "text": "storage so killing this master here I have actually killed everything we have two ways that we mitigate against this",
    "start": "1348660",
    "end": "1354820"
  },
  {
    "text": "one is we turn on something called semi sync so semi sync is a my sequel feature that before we actually accept a",
    "start": "1354820",
    "end": "1361510"
  },
  {
    "text": "transaction is committed my sequel will wait until at least one replica has written",
    "start": "1361510",
    "end": "1367910"
  },
  {
    "text": "it - it's been log now it hasn't written it - it hasn't applied it but it has put it in the bin log and my secret will",
    "start": "1367910",
    "end": "1374630"
  },
  {
    "text": "actually block on that so the fact that I have now killed this tablet well we should start seeing here is some of",
    "start": "1374630",
    "end": "1380930"
  },
  {
    "start": "1380000",
    "end": "1410000"
  },
  {
    "text": "these inserts are starting to fail and we can see if I were to just stop this",
    "start": "1380930",
    "end": "1387050"
  },
  {
    "text": "for a second you can see that the c0 shard that's the master I just killed some of those inserts are down",
    "start": "1387050",
    "end": "1393140"
  },
  {
    "text": "thankfully Orchestrator is there to save the day so Orchestrator has detected",
    "start": "1393140",
    "end": "1399800"
  },
  {
    "text": "that there is a failure and because it's",
    "start": "1399800",
    "end": "1405320"
  },
  {
    "text": "detected there is a failure it is gonna do something about it so Orchestrator has recovery the recovery is currently",
    "start": "1405320",
    "end": "1413179"
  },
  {
    "start": "1410000",
    "end": "1435000"
  },
  {
    "text": "waiting to find out to be the successor is what Orchestrator does is like I said with semi sink you know that that",
    "start": "1413179",
    "end": "1419990"
  },
  {
    "text": "transaction was written down to at least one place so what we need to do is we need to find where that transaction was",
    "start": "1419990",
    "end": "1425330"
  },
  {
    "text": "and that's the person that needs to be master so right now workers trader is actually evaluating the GT IDs of all",
    "start": "1425330",
    "end": "1430580"
  },
  {
    "text": "the replicas and it's gonna determine the one that is most suitable to be the new master so he does picked one the",
    "start": "1430580",
    "end": "1437780"
  },
  {
    "start": "1435000",
    "end": "1455000"
  },
  {
    "text": "successor is ten is this host doesn't mean a lot to us here but we can see it",
    "start": "1437780",
    "end": "1443179"
  },
  {
    "text": "back in the BT tablet part it is promoted the okay master and it is now",
    "start": "1443179",
    "end": "1449600"
  },
  {
    "text": "going through the rest its gonna apply some changes to it it does take a little bit of time if I go back here what we",
    "start": "1449600",
    "end": "1457160"
  },
  {
    "text": "should see in a few seconds is all these queries start working again and again",
    "start": "1457160",
    "end": "1462530"
  },
  {
    "text": "this only affected that one shard so if you are furiously rating dogs right now you would have noticed that some of",
    "start": "1462530",
    "end": "1468980"
  },
  {
    "text": "those would have returned errors but generally it would only need in a quarter of requests and we're back we're",
    "start": "1468980",
    "end": "1475070"
  },
  {
    "text": "live we have recovered from that failure so if and again this could be any number",
    "start": "1475070",
    "end": "1480710"
  },
  {
    "text": "of things that happened here I happen to have deleted that tablet but let's say you had the node go down that was on it",
    "start": "1480710",
    "end": "1486140"
  },
  {
    "text": "the back key node unexpectedly fail these things happen it would have recovered where if any other number of",
    "start": "1486140",
    "end": "1491390"
  },
  {
    "text": "unhandled events would have occurred so if your disk filled up and you can write to it anymore if a pod got evicted so when one",
    "start": "1491390",
    "end": "1498540"
  },
  {
    "text": "it's got evicted they just they sit around in the normal require human intervention we can Orchestrator can handle that automatically for you but",
    "start": "1498540",
    "end": "1505770"
  },
  {
    "text": "but wait there's more so that's the case of one nerd going down what happens if say like a whole",
    "start": "1505770",
    "end": "1511110"
  },
  {
    "start": "1510000",
    "end": "1520000"
  },
  {
    "text": "data center goes down so okay so let's",
    "start": "1511110",
    "end": "1518070"
  },
  {
    "text": "just say something happens here in Spain and use context I'm gonna go ahead to",
    "start": "1518070",
    "end": "1525420"
  },
  {
    "start": "1520000",
    "end": "1650000"
  },
  {
    "text": "Spain and I'm just gonna say cube CTL delete US Wes so I'm sorry",
    "start": "1525420",
    "end": "1533960"
  },
  {
    "text": "EU West for dot yeah mo just go ahead and do that and now I have actually done",
    "start": "1533960",
    "end": "1542460"
  },
  {
    "text": "something very bad I have deleted every single tablet and every single shard and all the VT gates that are powering that",
    "start": "1542460",
    "end": "1550110"
  },
  {
    "text": "if you were actually still furiously writing you may have noticed now that the app will no longer work so if you",
    "start": "1550110",
    "end": "1556110"
  },
  {
    "text": "keep trying to do it for a little bit you may see it hang that's because again I pretty sure Google is routing all of us to this specific data center and I",
    "start": "1556110",
    "end": "1563220"
  },
  {
    "text": "have just deleted not only the data base but I've also deleted it to be t-gates so the database connection for these app",
    "start": "1563220",
    "end": "1569820"
  },
  {
    "text": "servers are down so I haven't actually been able to figure out why this is I I",
    "start": "1569820",
    "end": "1574830"
  },
  {
    "text": "promise you and I did this in America I didn't get this problem so if you're trying to rate Gog's now you'll see that",
    "start": "1574830",
    "end": "1580710"
  },
  {
    "text": "that is in fact failing and we can see the test is very unhappy this is going",
    "start": "1580710",
    "end": "1586770"
  },
  {
    "text": "straight in through the other European Data Center and all of those rights are failing alright so if we go back to",
    "start": "1586770",
    "end": "1593640"
  },
  {
    "text": "Orchestrator however we see that Orchestrator has detected some more failures right so we now have failures",
    "start": "1593640",
    "end": "1601770"
  },
  {
    "text": "for all of our cards almost and I can go ahead and look at recovery so I see that",
    "start": "1601770",
    "end": "1607980"
  },
  {
    "text": "I have instantiated of recovery for c0 - 40 80 - 40 and ADC so this is all of my",
    "start": "1607980",
    "end": "1616770"
  },
  {
    "text": "shards have now been affected by this now you will not be able to commit anything until this is fixed so again",
    "start": "1616770",
    "end": "1624500"
  },
  {
    "text": "Orchestrator is doing in the same song and dance that it did last time it is going through all of your replicas it is",
    "start": "1624500",
    "end": "1630960"
  },
  {
    "text": "trying to which one is regrouping replicas by a GT ID so it's going through all your replicas figuring out which one has semi",
    "start": "1630960",
    "end": "1637940"
  },
  {
    "text": "synched all those transactions we don't want to lose a single rating these dago's are not are not something we want",
    "start": "1637940",
    "end": "1643610"
  },
  {
    "text": "to lightly brush aside once it figures out who they are it will go ahead and",
    "start": "1643610",
    "end": "1649520"
  },
  {
    "text": "recover it's actually possible to do this a little bit faster if you are not putting this in the most suboptimal",
    "start": "1649520",
    "end": "1656270"
  },
  {
    "start": "1650000",
    "end": "1690000"
  },
  {
    "text": "situation again I'm failing over between regions so Orchestrator actually has a",
    "start": "1656270",
    "end": "1662660"
  },
  {
    "text": "mode where it will explicitly disallow this if you remember back around six or seven months ago there was an outage at",
    "start": "1662660",
    "end": "1668930"
  },
  {
    "text": "github that outage was caused by Orchestrator failing over between data",
    "start": "1668930",
    "end": "1674360"
  },
  {
    "text": "centers which caused a little bit of data problems for them because it was tuned a little bit too aggressively so",
    "start": "1674360",
    "end": "1680620"
  },
  {
    "text": "right now I do have it turned on to failover across data centers because I wanted to show taking down data center",
    "start": "1680620",
    "end": "1686720"
  },
  {
    "text": "in reality this may not be how you decide you want to run it your needs may vary all that aside let's see how it's",
    "start": "1686720",
    "end": "1695030"
  },
  {
    "text": "doing okay so it has picked a master it has picked a successor and it is",
    "start": "1695030",
    "end": "1700970"
  },
  {
    "text": "starting to apply changes I'm gonna go back to VT VT it's gonna be very red right now ya see its unhappy it is it is",
    "start": "1700970",
    "end": "1707510"
  },
  {
    "text": "very unhappy and we can see that at least one of the one of the shards has a",
    "start": "1707510",
    "end": "1712700"
  },
  {
    "text": "master and let's wait to see the other ones ok worker Schrader is doing this",
    "start": "1712700",
    "end": "1720170"
  },
  {
    "text": "thing",
    "start": "1720170",
    "end": "1722620"
  },
  {
    "start": "1730000",
    "end": "1760000"
  },
  {
    "text": "okay so it's picked the master it's now going through the recovery procedure on that master also this is not a global",
    "start": "1731750",
    "end": "1738289"
  },
  {
    "text": "Orchestrator I kind of cheated here and I only put worker Schrader in one data center so all of this recovery is",
    "start": "1738289",
    "end": "1743750"
  },
  {
    "text": "actually happening across data centers as well Orchestrator has a mechanism that you can run as a a raft consensus",
    "start": "1743750",
    "end": "1750470"
  },
  {
    "text": "group which is how we would run this in production and how we set it up the planet scale when we run the test for",
    "start": "1750470",
    "end": "1756440"
  },
  {
    "text": "people okay so now it's done its thing it has successfully tablet externally",
    "start": "1756440",
    "end": "1763610"
  },
  {
    "start": "1760000",
    "end": "1775000"
  },
  {
    "text": "parented and we can sort of see here in VT gate that it has picked a new set of",
    "start": "1763610",
    "end": "1769909"
  },
  {
    "text": "masters for at least three of the four no ball four so all four of them now have one one of them ended up in America",
    "start": "1769909",
    "end": "1775490"
  },
  {
    "start": "1775000",
    "end": "1800000"
  },
  {
    "text": "go figure if I go back to my terminal we have resumed inserting data so just like that",
    "start": "1775490",
    "end": "1781940"
  },
  {
    "text": "you lose a data center we did have about two minutes of outage here in our experience we can get that down a little",
    "start": "1781940",
    "end": "1787340"
  },
  {
    "text": "lower I've been able to get fail overs in as little of nine seconds but again if you tune this too quickly you can get",
    "start": "1787340",
    "end": "1793909"
  },
  {
    "text": "into a situation where you failover into something you don't want so and then if",
    "start": "1793909",
    "end": "1801350"
  },
  {
    "start": "1800000",
    "end": "1820000"
  },
  {
    "text": "I go back to my good-ass dogger up up",
    "start": "1801350",
    "end": "1807470"
  },
  {
    "text": "the google shenanigans continue okay well I am NOT a very good web developer",
    "start": "1807470",
    "end": "1814010"
  },
  {
    "text": "I imagine this is my fault anyway the inserts have resumed again your",
    "start": "1814010",
    "end": "1821450"
  },
  {
    "start": "1820000",
    "end": "1860000"
  },
  {
    "text": "database has recovered again you can do this without any human intervention so",
    "start": "1821450",
    "end": "1827270"
  },
  {
    "text": "all the stuff that you saw me do here none of this was actually to recover the app all I did was delete lots of things",
    "start": "1827270",
    "end": "1833200"
  },
  {
    "text": "we've actually heard some people who use with tests have had similar situations happen where they have lost enough nodes",
    "start": "1833200",
    "end": "1840590"
  },
  {
    "text": "that they've lost the majority of the Masters on their database deployment Orchestrator kicks in rhe parents",
    "start": "1840590",
    "end": "1846919"
  },
  {
    "text": "everything that being said it's not perfect there's obviously situations that still require human intervention so",
    "start": "1846919",
    "end": "1853280"
  },
  {
    "text": "you can't throw away your pagers just yet but this is some of the stuff that you can do I think we're running out of",
    "start": "1853280",
    "end": "1858740"
  },
  {
    "text": "time here and I did want to leave some moments for questions so we're going to go ahead and present again I",
    "start": "1858740",
    "end": "1865440"
  },
  {
    "start": "1860000",
    "end": "1870000"
  },
  {
    "text": "gonna blow through the next couple slides like I said they're ephemeral so we are restoring from backups and then",
    "start": "1865440",
    "end": "1871889"
  },
  {
    "text": "catching up by a replication we are not using persistent disks anywhere in this there's a few places where you have less",
    "start": "1871889",
    "end": "1879690"
  },
  {
    "text": "common problems that do require you to actually delete things or interact with the cluster we have some catastrophic",
    "start": "1879690",
    "end": "1886320"
  },
  {
    "start": "1885000",
    "end": "1905000"
  },
  {
    "text": "failures like unplanned node failures that's when I believe the one master or a multi region data center outage so",
    "start": "1886320",
    "end": "1891840"
  },
  {
    "text": "right now like when I deleted that whole region that was three data centers it's really unlikely for Google Cloud to lose",
    "start": "1891840",
    "end": "1898230"
  },
  {
    "text": "an entire region like that but even if it does if you're using the tests your database will recover so the last thing",
    "start": "1898230",
    "end": "1905669"
  },
  {
    "text": "is the test is a CN CF project it is open source we would love for you to join our community to contribute to the",
    "start": "1905669",
    "end": "1912509"
  },
  {
    "text": "project if you like databases or are interested in computed systems please",
    "start": "1912509",
    "end": "1917820"
  },
  {
    "text": "stop by its of a test on Io if you want to join us on slack that's really where",
    "start": "1917820",
    "end": "1923070"
  },
  {
    "text": "we do most of our collaboration if you go to the test dot slack comm or if you go to fit test at i/o you can invite",
    "start": "1923070",
    "end": "1928289"
  },
  {
    "text": "yourself to our slack it is found on github if you really like the test and you want to work on it full-time",
    "start": "1928289",
    "end": "1933929"
  },
  {
    "text": "the team at planet-scale is hiring and we would love to talk to you so please",
    "start": "1933929",
    "end": "1939289"
  },
  {
    "text": "either email us there or come find one of us so we do have a few more minutes I",
    "start": "1939289",
    "end": "1944399"
  },
  {
    "text": "hope ya good nobody said no so we do so if anybody has any questions about the tests or",
    "start": "1944399",
    "end": "1950250"
  },
  {
    "text": "about what you saw or about where I got all those adorable pictures of dogs you just let me know let's go left to right",
    "start": "1950250",
    "end": "1962450"
  },
  {
    "text": "bureaus",
    "start": "1963010",
    "end": "1965550"
  },
  {
    "text": "okay so the question was the question was about semi sink replication how if",
    "start": "1979690",
    "end": "1984710"
  },
  {
    "text": "if I wait for an acknowledgment from one replica before I say it's okay what",
    "start": "1984710",
    "end": "1991700"
  },
  {
    "start": "1985000",
    "end": "2095000"
  },
  {
    "text": "happens if I lose the master and that replica that's a great question you have two options in that case one is",
    "start": "1991700",
    "end": "1997540"
  },
  {
    "text": "it doesn't have to be one you can set that number to be whatever you want so if you want it to be two replicas you",
    "start": "1997540",
    "end": "2003190"
  },
  {
    "text": "can the second thing you can do is obviously in this situation where I have",
    "start": "2003190",
    "end": "2008710"
  },
  {
    "text": "co-located replicas and masters right the place where that the place where",
    "start": "2008710",
    "end": "2014560"
  },
  {
    "text": "semi sink triggered the acceptance is going to be the local one right just the",
    "start": "2014560",
    "end": "2020500"
  },
  {
    "text": "laws of physics dictate that most of the time it's going to be the one that's closest when I lost that data center I",
    "start": "2020500",
    "end": "2025840"
  },
  {
    "text": "lost both of those nodes the way that we have around that is you don't have to",
    "start": "2025840",
    "end": "2031090"
  },
  {
    "text": "turn on semi sink for all your nodes the only nodes that participate in semi sink are the ones that you explicitly enable",
    "start": "2031090",
    "end": "2037660"
  },
  {
    "text": "it we let talk about slack yeah okay so one of the people who uses Vitesse's",
    "start": "2037660",
    "end": "2043300"
  },
  {
    "text": "slack they actually explicitly set semi sink so that it doesn't you will never",
    "start": "2043300",
    "end": "2048460"
  },
  {
    "text": "see me sink in the same availability zone we've talked about making a mechanism simply do that with their own",
    "start": "2048460",
    "end": "2054399"
  },
  {
    "text": "orchestration we've talked about making a mechanism like that available in the open source product so that you have the",
    "start": "2054400",
    "end": "2060820"
  },
  {
    "text": "same flexibility so instead of having to set the semi sink up to four or five to",
    "start": "2060820",
    "end": "2066070"
  },
  {
    "text": "make sure that it gets across data centers you can just say I only want to semi sync across availability zones or",
    "start": "2066070",
    "end": "2071500"
  },
  {
    "text": "across data centers and we had a gentleman back there yep we'll get back",
    "start": "2071500",
    "end": "2078070"
  },
  {
    "text": "to what do you got",
    "start": "2078070",
    "end": "2081000"
  },
  {
    "text": "so you are not fixed to a given version I think there are some okay the question",
    "start": "2087220",
    "end": "2093679"
  },
  {
    "text": "was are you fixed with a specific version or are you limited to only certain features no you're not so we",
    "start": "2093679",
    "end": "2100940"
  },
  {
    "start": "2095000",
    "end": "2195000"
  },
  {
    "text": "have compatibility from my sequel five six up to my sequel eight Oh that being said it is the test does have",
    "start": "2100940",
    "end": "2107420"
  },
  {
    "text": "its own sequel parser in it so there are some things especially in my sequel eight that we haven't implemented yet so",
    "start": "2107420",
    "end": "2113270"
  },
  {
    "text": "I know we have not implemented the JSON JSON portions of my sequel eight so that",
    "start": "2113270",
    "end": "2120440"
  },
  {
    "text": "if you were try to issue a query in a multi sharded environment with JSON we",
    "start": "2120440",
    "end": "2125540"
  },
  {
    "text": "wouldn't be able to do that for you if it was symbol sharted we would still be able to pass it right there and whatever you're backing version of my sequel was",
    "start": "2125540",
    "end": "2131330"
  },
  {
    "text": "in English art and environment we will handle all the features also in a sharded environment there are some",
    "start": "2131330",
    "end": "2137540"
  },
  {
    "text": "things that don't work so let's say you use a stored procedure and you count on your stored procedure to scan all of",
    "start": "2137540",
    "end": "2143210"
  },
  {
    "text": "your data and then return a result set that will not work but again I mean if",
    "start": "2143210",
    "end": "2150320"
  },
  {
    "text": "it's a single shot of a stored procedure that's where the acting will be on data in that shot that was just fine",
    "start": "2150320",
    "end": "2158140"
  },
  {
    "text": "[Music]",
    "start": "2168890",
    "end": "2178210"
  },
  {
    "text": "[Music]",
    "start": "2182910",
    "end": "2189788"
  },
  {
    "text": "yeah so the sharding is built into the test it is the one of the primary",
    "start": "2191859",
    "end": "2197869"
  },
  {
    "start": "2195000",
    "end": "2365000"
  },
  {
    "text": "features of a test so there is no other technology that does it to the outside application the test looks like a single",
    "start": "2197869",
    "end": "2205250"
  },
  {
    "text": "database in fact you can start to deploy the tests by deploying it if you have if you're using my sequel today and you're",
    "start": "2205250",
    "end": "2211280"
  },
  {
    "text": "running out of capacity on my sequel you can actually put the tests in front of it it can operate and then with with",
    "start": "2211280",
    "end": "2216410"
  },
  {
    "text": "what we call unmanaged my sequel mode and it'll just point to your existing my sequel server queries can come through",
    "start": "2216410",
    "end": "2222410"
  },
  {
    "text": "the tests and they can be serviced by your existing my sequel server and then returned in for reference the so there",
    "start": "2222410",
    "end": "2229820"
  },
  {
    "text": "is no knowledge of a test built into the application itself here is good as doggo",
    "start": "2229820",
    "end": "2235130"
  },
  {
    "text": "this is completely planet-scale proprietary none of you are allowed to mention that you saw this so here as",
    "start": "2235130",
    "end": "2241520"
  },
  {
    "text": "good as they'll go it's not a very complex app you can see that there is just three standard queries this would",
    "start": "2241520",
    "end": "2247700"
  },
  {
    "text": "work the same whether I had my sequel behind it orbit test behind it in fact this just uses the community my",
    "start": "2247700",
    "end": "2252890"
  },
  {
    "text": "sequel drivers for rust and it connected right to Vitesse so in terms of",
    "start": "2252890",
    "end": "2259760"
  },
  {
    "text": "performance which i think was your last part there is a performance overhead freezing to tests we have found the",
    "start": "2259760",
    "end": "2265280"
  },
  {
    "text": "performance overhead to be this there's two aspects to it one it's gonna be about 500 nanoseconds to all of your",
    "start": "2265280",
    "end": "2272180"
  },
  {
    "text": "queries so any query that you run on any size database you're gonna add another",
    "start": "2272180",
    "end": "2278089"
  },
  {
    "text": "500 nanoseconds to it best-case scenario and there's really that's just because",
    "start": "2278089",
    "end": "2284210"
  },
  {
    "text": "you are now going through two other processes to get to it we found if you're doing benchmarking against that and you have sub sub millisecond queries",
    "start": "2284210",
    "end": "2291020"
  },
  {
    "text": "then that doesn't reflect well on the tests under normal operations where you maybe have 10 or 15 millisecond queries",
    "start": "2291020",
    "end": "2297170"
  },
  {
    "text": "we don't really notice it that much the second thing that you'll see is that we use anywhere from 20 to 40 percent CPU 20 to",
    "start": "2297170",
    "end": "2305790"
  },
  {
    "text": "40% of the CPU that you're my sequel process will use and that really also depends on the query workload so in",
    "start": "2305790",
    "end": "2311520"
  },
  {
    "text": "general the tests to my sequel you'll probably see anywheres from 75 to 90%",
    "start": "2311520",
    "end": "2318480"
  },
  {
    "text": "performance as to what you would get over straight my sequel however as soon",
    "start": "2318480",
    "end": "2324090"
  },
  {
    "text": "as you shard that you're now way above what a single my sequel server could do for you anything you wanted add that and",
    "start": "2324090",
    "end": "2332370"
  },
  {
    "text": "then you wanted oh okay good that you",
    "start": "2332370",
    "end": "2362400"
  },
  {
    "text": "ask is we actually do that so under under a lot of circumstances we do pause",
    "start": "2362400",
    "end": "2368460"
  },
  {
    "start": "2365000",
    "end": "2430000"
  },
  {
    "text": "and I was actually I could have sworn to you before we did this that we actually paused on these failures as well the",
    "start": "2368460",
    "end": "2375360"
  },
  {
    "text": "test has a built-in buffer for situations like this so whenever we're doing things like restarting re parenting so you'll notice",
    "start": "2375360",
    "end": "2381810"
  },
  {
    "text": "money to the reparent we switched masters the test knows how to buffer queries so that we safely handle that",
    "start": "2381810",
    "end": "2388920"
  },
  {
    "text": "reparent teen will actually will buffer all begins will allow all commits to",
    "start": "2388920",
    "end": "2394200"
  },
  {
    "text": "finish and then we will swap the masters and then we will allow the begins to continue so it has that buffering",
    "start": "2394200",
    "end": "2399750"
  },
  {
    "text": "built-in that is a great question as to why we don't do it when the backing tablet goes down and it's definitely",
    "start": "2399750",
    "end": "2406050"
  },
  {
    "text": "something that we are now going to actively try to convince sugu is is absolutely necessary the second part of",
    "start": "2406050",
    "end": "2412470"
  },
  {
    "text": "that question I think was about GT IDs if I don't quite understand that part",
    "start": "2412470",
    "end": "2419090"
  },
  {
    "text": "so the GT IDs are gonna be through replication they're gonna be replicated",
    "start": "2425989",
    "end": "2431430"
  },
  {
    "start": "2430000",
    "end": "2485000"
  },
  {
    "text": "to all the replicas in that replica set that that cluster so and that's the",
    "start": "2431430",
    "end": "2437880"
  },
  {
    "text": "whole in the magic of semi sync is we're gonna make sure that a specific GT ID has made it to at least one other node",
    "start": "2437880",
    "end": "2443400"
  },
  {
    "text": "or a configure amount of nodes to make sure that it's not a single point of",
    "start": "2443400",
    "end": "2448890"
  },
  {
    "text": "failure so as soon as the transaction is ready to be committed before it actually returns back to the client that it has",
    "start": "2448890",
    "end": "2455039"
  },
  {
    "text": "been committed it's gonna replicate it to one other place and as the gentleman back there had said you know as long as",
    "start": "2455039",
    "end": "2460729"
  },
  {
    "text": "wherever it's been replicated to you stays up then you're guaranteed to still have it gentlemen the back",
    "start": "2460729",
    "end": "2472430"
  },
  {
    "text": "[Music]",
    "start": "2476040",
    "end": "2479210"
  },
  {
    "text": "yes so the question was will this work",
    "start": "2481279",
    "end": "2486690"
  },
  {
    "start": "2485000",
    "end": "2570000"
  },
  {
    "text": "with managed database systems such as AWS is RDS or Google's cloud sequel or",
    "start": "2486690",
    "end": "2491849"
  },
  {
    "text": "as yours thing I don't actually know",
    "start": "2491849",
    "end": "2496980"
  },
  {
    "text": "what it is yes it does in fact we we've worked extensively with Amazon to get",
    "start": "2496980",
    "end": "2502589"
  },
  {
    "text": "people running the test on top of RDS we've partnered with Amazon for their aurora product to show just how wide",
    "start": "2502589",
    "end": "2509609"
  },
  {
    "text": "there were just how performant their aurora product is we haven't published",
    "start": "2509609",
    "end": "2514680"
  },
  {
    "text": "that blog post yet but it's it's gonna be fantastic when we do so yes it works",
    "start": "2514680",
    "end": "2519690"
  },
  {
    "text": "perfectly there there are some things that you can't do or that you don't do",
    "start": "2519690",
    "end": "2525869"
  },
  {
    "text": "in Vitesse if you're using aurora so if you are using aurora most people would",
    "start": "2525869",
    "end": "2532770"
  },
  {
    "text": "want to have World War manage the master and replicas so in Vitesse you normally",
    "start": "2532770",
    "end": "2537960"
  },
  {
    "text": "would only when we've deployed into test we only put up two tablets no matter how many actual physical Aurora hosts or RDS",
    "start": "2537960",
    "end": "2545010"
  },
  {
    "text": "hosts you have we only put up two tablets and then we we do it that way and we allow order to scale the reads",
    "start": "2545010",
    "end": "2551460"
  },
  {
    "text": "instead of in this case I had a bunch of read replicas and they could each they managed by the test and it could choose",
    "start": "2551460",
    "end": "2557039"
  },
  {
    "text": "between them so but yes short answer is it absolutely works",
    "start": "2557039",
    "end": "2562818"
  },
  {
    "text": "so discover is the devil it does so we can't use discovery and so there is a",
    "start": "2568930",
    "end": "2576440"
  },
  {
    "start": "2570000",
    "end": "2671000"
  },
  {
    "text": "orchestrated configuration it does need a couple extra things besides the standard one there's a couple posts",
    "start": "2576440",
    "end": "2583190"
  },
  {
    "text": "failure hooks that we have to add specific for tests if you look on the Vitesse github page",
    "start": "2583190",
    "end": "2588320"
  },
  {
    "text": "the test has a helmet art that's developed by the community that has an Orchestrator config in it that",
    "start": "2588320",
    "end": "2594079"
  },
  {
    "text": "Orchestrator config is mostly correct it will generally work for you if you were on the Vitesse slack channel you'll",
    "start": "2594079",
    "end": "2599810"
  },
  {
    "text": "notice that recently some members of the community have had some hard times with that so both preparing for this and then",
    "start": "2599810",
    "end": "2605720"
  },
  {
    "text": "generally because we want to see the test succeed we have a modified version that you're gonna see committed probably",
    "start": "2605720",
    "end": "2612740"
  },
  {
    "text": "when I get back to the US that is very reliable and we did have that registers",
    "start": "2612740",
    "end": "2624890"
  },
  {
    "text": "the my security that it's managing with the orchestrators that's why orchestra",
    "start": "2624890",
    "end": "2630079"
  },
  {
    "text": "doesn't need to discover you just",
    "start": "2630079",
    "end": "2636050"
  },
  {
    "text": "specify the API endpoint for Orchestrator did you tab there alright",
    "start": "2636050",
    "end": "2643520"
  },
  {
    "text": "well we are I believe over our time but uh thanks everybody for coming and",
    "start": "2643520",
    "end": "2648980"
  },
  {
    "text": "watching if you have questions please let me know if you want to keep raiding dogs because I know it's super addictive",
    "start": "2648980",
    "end": "2655400"
  },
  {
    "text": "I'm gonna leave that app up a little bit and let's see if it came back online you know what I bet it's the whole thing",
    "start": "2655400",
    "end": "2661819"
  },
  {
    "text": "with Spain so [Applause]",
    "start": "2661819",
    "end": "2673459"
  }
]