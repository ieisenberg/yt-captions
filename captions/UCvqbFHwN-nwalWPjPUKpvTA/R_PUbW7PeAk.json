[
  {
    "text": "thank you for um coming here this afternoon instead of going back to see Hong Kong you know that's which I",
    "start": "40",
    "end": "5720"
  },
  {
    "text": "encourage you to do I'll try my best to keep you awake and uh I hope that's you",
    "start": "5720",
    "end": "11200"
  },
  {
    "text": "know you are learned something here as well so uh let's get started the title of the talk is right",
    "start": "11200",
    "end": "19119"
  },
  {
    "text": "ones wrong anywhere but for gpus you know so um I started my career in Java",
    "start": "19119",
    "end": "26000"
  },
  {
    "text": "you know when um at that time the the tagline was right right anywhere and uh",
    "start": "26000",
    "end": "31960"
  },
  {
    "text": "um I was really you know I was just out of school at the time you know I was really didn't really understand what's a big deal about that because you know",
    "start": "31960",
    "end": "38960"
  },
  {
    "text": "there's only two type of CPUs as far as there's only one type of CPUs as far as I'm concerned when Java first came out",
    "start": "38960",
    "end": "44559"
  },
  {
    "text": "right just the x8x the Intel CPU so at the time it's right once running Weare mostly refer to",
    "start": "44559",
    "end": "50920"
  },
  {
    "text": "the operating system so the developers they typically have windows so they",
    "start": "50920",
    "end": "56000"
  },
  {
    "text": "compile the application and develop the application but then they need to deploy um say a Linux server Apachi server or",
    "start": "56000",
    "end": "63359"
  },
  {
    "text": "something like that so Java Ser sort of serves as an intermediary and to go",
    "start": "63359",
    "end": "68360"
  },
  {
    "text": "across different operating systems and uh we can witness that it has tremendous",
    "start": "68360",
    "end": "73400"
  },
  {
    "text": "success coming from that I think fairly narrow use case um fast forward to7 2019",
    "start": "73400",
    "end": "80360"
  },
  {
    "text": "we started the project called was M A lot of people ask us how's your different from java you know because",
    "start": "80360",
    "end": "85600"
  },
  {
    "text": "that sounds like the same thing you have a virtual machine compile something to the bite code I always tell people you",
    "start": "85600",
    "end": "90680"
  },
  {
    "text": "know that's all the big difference is that you know we we we don't need Java the language we uh we don't need garbage",
    "start": "90680",
    "end": "97040"
  },
  {
    "text": "collection and you know things like that because we use say rust and the language itself take care of the man memory",
    "start": "97040",
    "end": "102840"
  },
  {
    "text": "management and you know things so we can have a very lightweight small um run",
    "start": "102840",
    "end": "108360"
  },
  {
    "text": "time that's that is suitable for a lot of you know lightweight use cases for instance uh sub functions you know",
    "start": "108360",
    "end": "114280"
  },
  {
    "text": "microservices or you or even blockchain smart contracts you know things like and Ed you know functions things like that",
    "start": "114280",
    "end": "121920"
  },
  {
    "text": "so it always lingers you know to say you know that's it sounds like you know",
    "start": "121920",
    "end": "127079"
  },
  {
    "text": "you're doing the same thing for the 20 years you know that's you first started in Java and then you started something",
    "start": "127079",
    "end": "132800"
  },
  {
    "text": "that's very similar to Java you know that's uh so it's only until I think um",
    "start": "132800",
    "end": "139920"
  },
  {
    "text": "uh two years ago maybe 18 months ago you know um we have uh at least for me and",
    "start": "139920",
    "end": "145239"
  },
  {
    "text": "uh for many people in our community that we have come to realize that wasm can actually be a lot more than say Java for",
    "start": "145239",
    "end": "153200"
  },
  {
    "text": "rust you know it actually takes the right one running wear idea to a whole new level that we want to do things that",
    "start": "153200",
    "end": "160480"
  },
  {
    "text": "are you know because we are in a world where there's a lot more Hardware you",
    "start": "160480",
    "end": "165680"
  },
  {
    "text": "know there's even in CPU there's three different CPUs now you know not just the Intel uh x86 you have the um the arm",
    "start": "165680",
    "end": "174360"
  },
  {
    "text": "CPUs and you have the risk five and even within each family you have different uh",
    "start": "174360",
    "end": "179480"
  },
  {
    "text": "Addam so for instance if you look at um you know the the the processing unit within the Intel CPU they have you have",
    "start": "179480",
    "end": "185280"
  },
  {
    "text": "simd you have AVX you know there's a large number of things you can turn on and turn off right so CPU compatibility",
    "start": "185280",
    "end": "191840"
  },
  {
    "text": "has become an issue especially if you have in the high performance Computing environment you want take take advantage of all the features the CPU has to offer",
    "start": "191840",
    "end": "199480"
  },
  {
    "text": "and then with the AI and large language Model come along there's a strong demand for gpus and uh as we can see uh even",
    "start": "199480",
    "end": "206640"
  },
  {
    "text": "with a media you have incompatible versions of Q you have Cuda 11 Cuda 12",
    "start": "206640",
    "end": "212159"
  },
  {
    "text": "and we we are all the way back to the same thing that we started developers they all have a Macbook you know",
    "start": "212159",
    "end": "218760"
  },
  {
    "text": "something like that or a Windows machine that has no gpus it's only CPU machine",
    "start": "218760",
    "end": "224200"
  },
  {
    "text": "so they develop applications they have no idea whether whether they compile the application whether the application",
    "start": "224200",
    "end": "229400"
  },
  {
    "text": "would work when they upload it into production environment or the cloud right so the the testing and development",
    "start": "229400",
    "end": "235879"
  },
  {
    "text": "again has been separated because there's no unified round time that allows developers and uh and the um you know um",
    "start": "235879",
    "end": "243959"
  },
  {
    "text": "and it managers to run the exact same binary application anymore and it's not just uh people need to recompile the",
    "start": "243959",
    "end": "250480"
  },
  {
    "text": "application like in old days when Java you know windows and Linux the People needs to write application in the way",
    "start": "250480",
    "end": "256160"
  },
  {
    "text": "that is toly different because the on the Mac the GPU API the SDK is very",
    "start": "256160",
    "end": "262040"
  },
  {
    "text": "different from the one that used by media in Cuda right so not only you need to um you basically need to rewrite your",
    "start": "262040",
    "end": "268120"
  },
  {
    "text": "application to use the new API in order to take advantage of the take advantage of the gpus",
    "start": "268120",
    "end": "275639"
  },
  {
    "text": "so that's what it brought us here you know so we thought you know there's a",
    "start": "275639",
    "end": "281120"
  },
  {
    "text": "there's a unique opportunity for a web assim orm as a as a lightweight virtual",
    "start": "281120",
    "end": "286400"
  },
  {
    "text": "machine format to um to address this issue the same way uh Java was able to address those issue 25 years ago so you",
    "start": "286400",
    "end": "293880"
  },
  {
    "text": "know that's that's where we are and uh so but before we go deep there you know",
    "start": "293880",
    "end": "300800"
  },
  {
    "text": "let's take a step back at the you know um the typical architecture of large language model application today you",
    "start": "300800",
    "end": "306880"
  },
  {
    "text": "know so people say why do I need cross GPU compatibility because I'm not using GPU at all I'm just using API the API",
    "start": "306880",
    "end": "314120"
  },
  {
    "text": "provided by chat gbt the open eye or entropic or you know U maybe my it",
    "start": "314120",
    "end": "320000"
  },
  {
    "text": "Department started something with AMA that's runs on GPU machine but from my application point of view I only need to",
    "start": "320000",
    "end": "326840"
  },
  {
    "text": "interact with the large language models through the Pi but my prediction is that",
    "start": "326840",
    "end": "331960"
  },
  {
    "text": "things like that would soon change because those setup is good for prototyping purposes you know the loose",
    "start": "331960",
    "end": "338360"
  },
  {
    "text": "coupling between what we call the loose coupling between the application and the large language model the truly useful",
    "start": "338360",
    "end": "344960"
  },
  {
    "text": "application has always being you are you need tight coupling you need to exactly match your application um the way you",
    "start": "344960",
    "end": "352199"
  },
  {
    "text": "write application has to be exactly matched to the large language model that you chose meaning if you chose a model",
    "start": "352199",
    "end": "359479"
  },
  {
    "text": "that that is um you know fine-tuned and better at say programming that you would feed it with very specific knowledge",
    "start": "359479",
    "end": "366039"
  },
  {
    "text": "base about programming and the with if you want to build a knowledge base about programming then you have a whole new",
    "start": "366039",
    "end": "373639"
  },
  {
    "text": "requirements for embedding for how to chunk the the the the knowledge text or youal text so that they don't break up",
    "start": "373639",
    "end": "379479"
  },
  {
    "text": "in the middle of a a code listing right to be able to understand what's the language the code is written and you",
    "start": "379479",
    "end": "385720"
  },
  {
    "text": "know things like that so there's a number of things there's a large variety of things that you have to um write in",
    "start": "385720",
    "end": "391520"
  },
  {
    "text": "your application to be very specifically tailored to the model so that it can take advantage of the model so that",
    "start": "391520",
    "end": "396880"
  },
  {
    "text": "means you should uh developers should bundle those application together and U",
    "start": "396880",
    "end": "402479"
  },
  {
    "text": "should not just treat the model as another API someone else problem to worry about you know that's it's open",
    "start": "402479",
    "end": "408120"
  },
  {
    "text": "eyes problem you know let them deal with the GPU stuff that means developers would need to think about how to take",
    "start": "408120",
    "end": "414800"
  },
  {
    "text": "advantage of gpus from their new generation of applications because I I",
    "start": "414800",
    "end": "419919"
  },
  {
    "text": "believe the new um you know the the AI application that actually works needs to",
    "start": "419919",
    "end": "425639"
  },
  {
    "text": "couple the the the application itself with the large language model that you choose meaning that puts the large",
    "start": "425639",
    "end": "431280"
  },
  {
    "text": "language model and the application into the same software package maybe in even in the same container right you know so",
    "start": "431280",
    "end": "436919"
  },
  {
    "text": "that gives the rise to the need of you know so I have a um you know I have",
    "start": "436919",
    "end": "442400"
  },
  {
    "text": "application that with large language model embedded in that I need a GPU and I need the wrong time to run the large",
    "start": "442400",
    "end": "448120"
  },
  {
    "text": "language model within my application taking advantage of the GPU so I need my application to be portable right to be",
    "start": "448120",
    "end": "454720"
  },
  {
    "text": "to be able to properly package it and Port it across different environment so the problem number one is you know U",
    "start": "454720",
    "end": "460840"
  },
  {
    "text": "when people face with this kind of type of problems you know the the uh the solution that Ty have is why not just",
    "start": "460840",
    "end": "467520"
  },
  {
    "text": "use Python because you know um all those models are training Python and py torch",
    "start": "467520",
    "end": "472560"
  },
  {
    "text": "and uh um you know so the the model file and the format and all the tools the",
    "start": "472560",
    "end": "477759"
  },
  {
    "text": "libraries and you know things like that in high likelihood they are available in Python right the Python's biggest",
    "start": "477759",
    "end": "484039"
  },
  {
    "text": "problem is that it's very complex and uh it's it has far more than you would need",
    "start": "484039",
    "end": "489319"
  },
  {
    "text": "for inference because it's designed for training and for scientific research and for that purposes right and uh even as a",
    "start": "489319",
    "end": "497199"
  },
  {
    "text": "scripting language python is not portable anyway you know so the way that you write python that's for the mac and for the nedia is different the code has",
    "start": "497199",
    "end": "503919"
  },
  {
    "text": "to be slightly different right so if you look at the dependencies that python P",
    "start": "503919",
    "end": "510199"
  },
  {
    "text": "has you know I I know I don't know if you can read the the the screenshot there it's a it's a screenshot from um",
    "start": "510199",
    "end": "516360"
  },
  {
    "text": "you know doer Hub it's a um it's a standard or the minimum image for uh for",
    "start": "516360",
    "end": "521479"
  },
  {
    "text": "py toch on on Linux um the image itself is like 3 GB okay and the other one you",
    "start": "521479",
    "end": "529680"
  },
  {
    "text": "know more developer friendly is 80 gb so the image itself without the large language model OKAY the large language",
    "start": "529680",
    "end": "535880"
  },
  {
    "text": "model adds another 5 to 50 gigabytes of space the the wrong time the python wrong time",
    "start": "535880",
    "end": "541800"
  },
  {
    "text": "itself has enough stuff that is 8 gab and if you add more stuff to it if you",
    "start": "541800",
    "end": "547240"
  },
  {
    "text": "embed this you know you know 8 GB plus 5 GB you know",
    "start": "547240",
    "end": "553079"
  },
  {
    "text": "it's like 13 GB you know you you you embed this whole thing into your application your application will soon",
    "start": "553079",
    "end": "558160"
  },
  {
    "text": "become very bloated and it doesn't address the portable problem anyway because when kues sees a uh a darker",
    "start": "558160",
    "end": "566320"
  },
  {
    "text": "image with python with um that is specifically tailored to uh uh say AMV",
    "start": "566320",
    "end": "571560"
  },
  {
    "text": "media it would have difficulty to just orchestrate it because it would need um",
    "start": "571560",
    "end": "576920"
  },
  {
    "text": "you know a software change in order for the python application to use a different GPU card right so you know so",
    "start": "576920",
    "end": "582519"
  },
  {
    "text": "that's problem number one you know is that can you do with python you can you can use Python to embed a large language",
    "start": "582519",
    "end": "587680"
  },
  {
    "text": "model into your own application you know just uh uh you know because there's different language bitings for python",
    "start": "587680",
    "end": "593440"
  },
  {
    "text": "you know that's uh so you could do that but it's uh it's very undesirable you know that's uh at least to me",
    "start": "593440",
    "end": "600279"
  },
  {
    "text": "right so the problem is such that you know uh you you often see twits like",
    "start": "600279",
    "end": "605399"
  },
  {
    "text": "this you know uh so uh Greg Bachman was the I think the um CTO or the president",
    "start": "605399",
    "end": "612640"
  },
  {
    "text": "of open AI right he said much of the modern machine learning engineering is making python not to be a bottleneck",
    "start": "612640",
    "end": "618720"
  },
  {
    "text": "right you know he that to figure out how to do this in Python right you know that's uh so he uh another day he you",
    "start": "618720",
    "end": "625399"
  },
  {
    "text": "know that's almost 6 months later he he he sent out another to he s out those tws those tws frequently right is",
    "start": "625399",
    "end": "631760"
  },
  {
    "text": "current status installing dependencies that's specifically referred to python depend complex dependencies there's",
    "start": "631760",
    "end": "637440"
  },
  {
    "text": "gigabytes of it right and uh then another guy said you know that's AGI would be built with python let that",
    "start": "637440",
    "end": "644040"
  },
  {
    "text": "thinking and uh um Yan said rust right you know so I think there's a a fairly",
    "start": "644040",
    "end": "650720"
  },
  {
    "text": "um strong consensus that's production application should not be built in Pyon uh in fact this is what we see in large",
    "start": "650720",
    "end": "657760"
  },
  {
    "text": "internet companies if you go to say by uh by Dan and you know um go to Alibaba",
    "start": "657760",
    "end": "663240"
  },
  {
    "text": "go to Google go to those places and to say ask to see their INF stack you know",
    "start": "663240",
    "end": "668399"
  },
  {
    "text": "I think it's mostly C++ there's very little python right you know there's uh because of those no well-known issues",
    "start": "668399",
    "end": "674760"
  },
  {
    "text": "with uh with the complexity of python and um with all this complexity 99% you",
    "start": "674760",
    "end": "680240"
  },
  {
    "text": "do not need and the 99% of the stuff is needed by professors and researchers who build the models if you just want to use",
    "start": "680240",
    "end": "687160"
  },
  {
    "text": "the model you don't need those but you know it's a pack in the way that you have to download it and install it so",
    "start": "687160",
    "end": "693959"
  },
  {
    "text": "then that leads to the problem scenario number two it's the developer experience",
    "start": "693959",
    "end": "700160"
  },
  {
    "text": "is terrible it's the same thing that's you know um when we started Java you know so all",
    "start": "700160",
    "end": "707240"
  },
  {
    "text": "those years ago I was a Java Champion you know so we had the first we started the first open source Java application server company and we were bought by red",
    "start": "707240",
    "end": "713600"
  },
  {
    "text": "hat so I stayed in red hat for a couple years until my West options all vested so what we did is that you know uh at",
    "start": "713600",
    "end": "720000"
  },
  {
    "text": "the time we tell people you know Java is right one dra anywhere you know so you can um you know you can move your",
    "start": "720000",
    "end": "725519"
  },
  {
    "text": "payloads around with regardless of your CPUs or you know or operating system and all that and most people thought we were",
    "start": "725519",
    "end": "732720"
  },
  {
    "text": "crazy you know because they thought we were doing that only because we were too young and too naive right you know that",
    "start": "732720",
    "end": "739279"
  },
  {
    "text": "we didn't see any any of the the production workload because to them the production workload is Linux on on on",
    "start": "739279",
    "end": "746120"
  },
  {
    "text": "Intel CPUs what's you why do you need right on anywhere you know that's you just compile for that platform you know",
    "start": "746120",
    "end": "751480"
  },
  {
    "text": "that's um so but then it turns out it solves an entirely different problem it solves the",
    "start": "751480",
    "end": "757160"
  },
  {
    "text": "problem I described earlier in my talk is the developers has Windows they don't have Linux on their desktop so they",
    "start": "757160",
    "end": "764160"
  },
  {
    "text": "compile it on Windows and then they Shi the binary to the to the to the back end and the back the back end Linux system",
    "start": "764160",
    "end": "769519"
  },
  {
    "text": "wouldn't be able to run it we have the exact same problem now is that developers are typically not using Linux",
    "start": "769519",
    "end": "776320"
  },
  {
    "text": "Nvidia on Linux platforms or AMD on Linux platforms they are using consumer grade laptops",
    "start": "776320",
    "end": "781720"
  },
  {
    "text": "like uh like this apple right and um you know so they you know you have the huge",
    "start": "781720",
    "end": "787480"
  },
  {
    "text": "overhead of have to rewrite recompile and retest for every platform and if you look at how the cloud native",
    "start": "787480",
    "end": "793399"
  },
  {
    "text": "infrastructure is set up kubernetes is not designed to recompile your code for each of the targeted deploys it's is",
    "start": "793399",
    "end": "799880"
  },
  {
    "text": "designed to just take the binary artifact and stick it to the new machine right you know so it's um it's so the",
    "start": "799880",
    "end": "808199"
  },
  {
    "text": "every every every Link in this Paradigm has been broken by the introduction of those large complex",
    "start": "808199",
    "end": "814320"
  },
  {
    "text": "gpus and there's lots of them you know there's not not just Nvidia gpus you",
    "start": "814320",
    "end": "819800"
  },
  {
    "text": "know every single um Cloud providers semiconductor providers now have their",
    "start": "819800",
    "end": "825440"
  },
  {
    "text": "own gpus or mpus or tpus you know that's whatever they call it right you know they all have their own Hardware you",
    "start": "825440",
    "end": "831360"
  },
  {
    "text": "have to adapt to all of them they all have a different SDK that you requires you to compile to specifically for their",
    "start": "831360",
    "end": "837279"
  },
  {
    "text": "Hardware right so I think this this problem is already getting out of hand this problem is already of a very urgent",
    "start": "837279",
    "end": "844680"
  },
  {
    "text": "pinpoints right so how to deal with this you know in the past we dealt with um",
    "start": "844680",
    "end": "850880"
  },
  {
    "text": "the um Linux to um Windows compatibilities by introducing the J the Java virtual machine so there's a very",
    "start": "850880",
    "end": "858160"
  },
  {
    "text": "um you know um I would say in computer science the all problem in computer science can be solved by another layer",
    "start": "858160",
    "end": "863199"
  },
  {
    "text": "of interaction right you know so we provide another abstraction that abstraction we call it where somebody",
    "start": "863199",
    "end": "868720"
  },
  {
    "text": "was right so let me stop here and give you a demo because I think I have talked enough you know I said all the things",
    "start": "868720",
    "end": "875800"
  },
  {
    "text": "about you know that's uh how how great this is right you know but you know let's see it for ourselves",
    "start": "875800",
    "end": "883880"
  },
  {
    "text": "so we have and so here I have application that I that I built with rust I I going to show you this",
    "start": "883880",
    "end": "889680"
  },
  {
    "text": "application in a minute but you can see you know um I'll show you the build file",
    "start": "889680",
    "end": "895720"
  },
  {
    "text": "cargo. Tomo so it it has some dependencies you know so what it",
    "start": "895720",
    "end": "902440"
  },
  {
    "text": "does is that it's a rust application that does AI inference give you a chatbot on the command line okay so it's",
    "start": "902440",
    "end": "909240"
  },
  {
    "text": "loads a uh it's loads of AI model ideally on the GPU but also works on CPU",
    "start": "909240",
    "end": "914480"
  },
  {
    "text": "and it give you the you the user interface to have allow you to interact with it so what I going to do is I going",
    "start": "914480",
    "end": "920959"
  },
  {
    "text": "to build it right cargo build and I going to build it web assembly so it going to take a",
    "start": "920959",
    "end": "927800"
  },
  {
    "text": "minutes to compile you know because um if you have used rust in the past you know that uh uh the compiler does uh ton",
    "start": "927800",
    "end": "935519"
  },
  {
    "text": "of work you know um you know the the the thing that I like most about rust is that if it compiles it most likely is",
    "start": "935519",
    "end": "941720"
  },
  {
    "text": "correct you know so it's uh it's rare to see something that is compiles but has WR time error you know unless logical",
    "start": "941720",
    "end": "948120"
  },
  {
    "text": "error of course but say things like memory error and you know things like that so if it compiles it's it does the",
    "start": "948120",
    "end": "953839"
  },
  {
    "text": "compiler does a lot of work and that make it you know um makes the job on the",
    "start": "953839",
    "end": "959360"
  },
  {
    "text": "WR time much easier so it's compiled okay so we have the application that compil into wasm so if you are familiar",
    "start": "959360",
    "end": "965519"
  },
  {
    "text": "with Java it's same like a Java application that compile into jvm bite code format so I going to copy",
    "start": "965519",
    "end": "973800"
  },
  {
    "text": "it I going to C it to the to my um home",
    "start": "973800",
    "end": "980160"
  },
  {
    "text": "directory right you know so I'm doing all this in the in the docker container inside my Mac okay so it's not it's",
    "start": "980160",
    "end": "987480"
  },
  {
    "text": "simulating so from inside the docker container it sees W to Linux which is",
    "start": "987480",
    "end": "992839"
  },
  {
    "text": "the operating system that runs on this container but also sees the arm CPU it",
    "start": "992839",
    "end": "998560"
  },
  {
    "text": "doesn't see the GPU on my Mac so in the compiler environment as far as the compiler environment is concerned I have",
    "start": "998560",
    "end": "1004959"
  },
  {
    "text": "no GPU okay so I go back I see this WM file is the one that",
    "start": "1004959",
    "end": "1011079"
  },
  {
    "text": "I just downloaded I I just compiled and we can see it's um you know",
    "start": "1011079",
    "end": "1019519"
  },
  {
    "text": "it's 5 megabytes right you know so it's um you know reasonable size okay so I",
    "start": "1019519",
    "end": "1025079"
  },
  {
    "text": "can run it inside this this um this um a doer container but I wouldn't right you",
    "start": "1025079",
    "end": "1030520"
  },
  {
    "text": "know because I want to show you the crossplatform capability of it so what I going to do is that I going to copy it",
    "start": "1030520",
    "end": "1035798"
  },
  {
    "text": "from the container out out copy it out of the container into my local machine so that I can run on the Mac and show",
    "start": "1035799",
    "end": "1041678"
  },
  {
    "text": "you that it actually runs on the Mac CP uh on the Mac",
    "start": "1041679",
    "end": "1046720"
  },
  {
    "text": "GPU so I already have the command here here uh",
    "start": "1046720",
    "end": "1051840"
  },
  {
    "text": "no sorry it's um",
    "start": "1051840",
    "end": "1058320"
  },
  {
    "text": "huh sorry let me do this again",
    "start": "1061039",
    "end": "1067400"
  },
  {
    "text": "um I think I have the script somewhere yeah okay so",
    "start": "1067400",
    "end": "1073919"
  },
  {
    "text": "I how already get that so I use a doer copy right from the",
    "start": "1073919",
    "end": "1079799"
  },
  {
    "text": "container to the local machine and uh I'm going to do that okay successfully copied and uh",
    "start": "1079799",
    "end": "1088720"
  },
  {
    "text": "what I going to show you is okay so you can see it's the same",
    "start": "1088720",
    "end": "1095280"
  },
  {
    "text": "same application it's the same size right you know it's the same size as this one and uh um what it does is that",
    "start": "1095280",
    "end": "1101640"
  },
  {
    "text": "I copyed a binary file I just copyed it there's no recompiling there's nothing you know it's just this binary file",
    "start": "1101640",
    "end": "1108280"
  },
  {
    "text": "being being being um you know um copied out of it and I already downloaded a large",
    "start": "1108280",
    "end": "1114679"
  },
  {
    "text": "language model so um why prepar this talk I thought which model I should use and I decided to use the model that",
    "start": "1114679",
    "end": "1121000"
  },
  {
    "text": "Microsoft just released yesterday you know it's called the 53.5 you know it's",
    "start": "1121000",
    "end": "1126480"
  },
  {
    "text": "a it's um uh a fairly small but also very highly scored large language model",
    "start": "1126480",
    "end": "1132840"
  },
  {
    "text": "so what I going to do is that because this is wasm file so I use wasm Edge to run this but I pass it a bunch of parameters I pass the model file name",
    "start": "1132840",
    "end": "1140679"
  },
  {
    "text": "the prom template you know like I said you know those um the the model need a lot of parameters needs the application",
    "start": "1140679",
    "end": "1146960"
  },
  {
    "text": "to fit it the model needs application around it to fit to to to fit to this model right so I started the application",
    "start": "1146960",
    "end": "1154840"
  },
  {
    "text": "ah why am I oh modifier is not fine so it's uh it",
    "start": "1154840",
    "end": "1159960"
  },
  {
    "text": "should be in this direction",
    "start": "1159960",
    "end": "1163320"
  },
  {
    "text": "sorry okay so the this starts almost instant instantly right you know that's",
    "start": "1165200",
    "end": "1171960"
  },
  {
    "text": "um so the wasm uh the wasm around time started wasm application now um I I've heard that the model is pretty good at",
    "start": "1171960",
    "end": "1179559"
  },
  {
    "text": "generating code so I going to ask a question say write me a rust",
    "start": "1179559",
    "end": "1187679"
  },
  {
    "text": "function that determines if and input number is prime okay so it's a",
    "start": "1187679",
    "end": "1197600"
  },
  {
    "text": "easy it's a easy thing so the Bob SS you know the the first question the first answer is always slow because the the",
    "start": "1197600",
    "end": "1203960"
  },
  {
    "text": "model is 5 gbes you needs to load the 5 GB of stuff into memory and then it starts but when it starts it goes really",
    "start": "1203960",
    "end": "1210600"
  },
  {
    "text": "fast it goes faster than I talk right you know if I talk like this it's about 3 to5 tokens per second but the the the",
    "start": "1210600",
    "end": "1217679"
  },
  {
    "text": "bot does it much faster the bot does at least 20 tokens per second the reason I'm showing this is because that shows",
    "start": "1217679",
    "end": "1225240"
  },
  {
    "text": "without doubt this is handled on the GPU because on the CPU going to be a lot smaller than I can speak it's going to",
    "start": "1225240",
    "end": "1231600"
  },
  {
    "text": "have one token or second something like that right so but you can see it's",
    "start": "1231600",
    "end": "1237000"
  },
  {
    "text": "actually pretty good you know that's uh so this is a rust code it gives you know it's um",
    "start": "1237000",
    "end": "1243640"
  },
  {
    "text": "um it determines you know it has it actually has some interesting optimization it says you know if it's",
    "start": "1243640",
    "end": "1249679"
  },
  {
    "text": "number one number two then it's Prime if it's above number two then I going to skip over all the even numbers okay",
    "start": "1249679",
    "end": "1255440"
  },
  {
    "text": "because even numbers are definitely not prime but then after that I going to do I going to get a square root of the",
    "start": "1255440",
    "end": "1260840"
  },
  {
    "text": "number that I I entered and then iterate all the way up to that number and check every one of them um every check OD",
    "start": "1260840",
    "end": "1267880"
  },
  {
    "text": "every odd number against whether they can be divided by some something else right so you know that's I think that's",
    "start": "1267880",
    "end": "1274360"
  },
  {
    "text": "the pretty classic rust implementation of this particular algorithm you know that's uh you know I just said this in",
    "start": "1274360",
    "end": "1280919"
  },
  {
    "text": "in complete uh natural language right so I think this model is pretty good so what I'm going to do is I'm going to say",
    "start": "1280919",
    "end": "1287360"
  },
  {
    "text": "translate um no no rewrite it in",
    "start": "1287360",
    "end": "1295120"
  },
  {
    "text": "Python please typically you don't need please right you know that's so the bot SS and",
    "start": "1295120",
    "end": "1301720"
  },
  {
    "text": "basically the bot takes the you see it goes faster this time because it doesn't need to load the 5 GB",
    "start": "1301720",
    "end": "1307600"
  },
  {
    "text": "of stuff into memory anymore but it does needs to know what I'm referring to when I say rewrite it what is it right now it",
    "start": "1307600",
    "end": "1314799"
  },
  {
    "text": "says it's a um it sees the rust application that is generated it above",
    "start": "1314799",
    "end": "1320039"
  },
  {
    "text": "so now it knows that my intention is to translate that into python so it does all this um so it's almost line by line",
    "start": "1320039",
    "end": "1327039"
  },
  {
    "text": "translation of the Rost application to python right you know so uh again this is um um you know I uh to recap what we",
    "start": "1327039",
    "end": "1334080"
  },
  {
    "text": "have done I rebuild this I build this application in the docker container on this Mac as far as the docker container",
    "start": "1334080",
    "end": "1341080"
  },
  {
    "text": "is concerned there's no GPU okay so it's generated a WM file a 5 megabytes WM",
    "start": "1341080",
    "end": "1346320"
  },
  {
    "text": "file I use stalker CP to copy that BM file into my into this Mac and then run",
    "start": "1346320",
    "end": "1353559"
  },
  {
    "text": "this application andc although the application was built without any GPU knowledge it now sees the machine it",
    "start": "1353559",
    "end": "1360240"
  },
  {
    "text": "runs on has a GPU that's has my Mac M2 GPU so it's used that GPU to run the",
    "start": "1360240",
    "end": "1365679"
  },
  {
    "text": "model and then generate all these answers so as far as I'm concerned I think this is pretty good so I want to",
    "start": "1365679",
    "end": "1371159"
  },
  {
    "text": "go one step further in the sto so I'm going to control C and and uh what I'm going to do is that I'm going to cop",
    "start": "1371159",
    "end": "1378240"
  },
  {
    "text": "here this whole thing into copy this wasm file into a remote machine that",
    "start": "1378240",
    "end": "1383400"
  },
  {
    "text": "runs onedia GPU okay so in in this entire building process um you know um",
    "start": "1383400",
    "end": "1391320"
  },
  {
    "text": "it's all down on the Mac while I tell while I'm saying you know this doesn't have any GPU knowledge you know that's",
    "start": "1391320",
    "end": "1397360"
  },
  {
    "text": "some people may be doubtful but it definitely doesn't know anything about n media so what I going to do is that let",
    "start": "1397360",
    "end": "1404320"
  },
  {
    "text": "me see if I can find the command maybe it's",
    "start": "1404320",
    "end": "1411480"
  },
  {
    "text": "here yeah so I have another machine that's running on Azure that has a um uh",
    "start": "1413240",
    "end": "1420200"
  },
  {
    "text": "ubo Linux with Nvidia GPU so what I going to do is I going to do the SCP command and copy this wasm file directly",
    "start": "1420200",
    "end": "1428120"
  },
  {
    "text": "into that new machine okay so I can do this into the home directory this is uh",
    "start": "1428120",
    "end": "1434720"
  },
  {
    "text": "5 megabytes I hope it's going to go fast yeah let go fast okay and now I can do I",
    "start": "1434720",
    "end": "1441480"
  },
  {
    "text": "SSH into this",
    "start": "1441480",
    "end": "1444520"
  },
  {
    "text": "machine",
    "start": "1447400",
    "end": "1450400"
  },
  {
    "text": "right come on okay so I assess into this machine and I can see this file",
    "start": "1455120",
    "end": "1460320"
  },
  {
    "text": "hopefully I see this file here again the same file I built from my Docker",
    "start": "1460320",
    "end": "1466080"
  },
  {
    "text": "container on my on my on my Mac right it's the same size it's the same file I copy to my Mac it's the same file that I",
    "start": "1466080",
    "end": "1472080"
  },
  {
    "text": "copy to this uh remote machine so I going to I'm going to stop whatever services that I have on this machine",
    "start": "1472080",
    "end": "1478000"
  },
  {
    "text": "okay and and now um instead of downloading um a large language model",
    "start": "1478000",
    "end": "1485120"
  },
  {
    "text": "what I going to do is that I going to um uh use the large language model",
    "start": "1485120",
    "end": "1490640"
  },
  {
    "text": "that's I already downloaded on this machine so let me copy and paste the command again",
    "start": "1490640",
    "end": "1497398"
  },
  {
    "text": "so instead of using the Microsoft um you know P 3.5 which was released yesterday",
    "start": "1499240",
    "end": "1504960"
  },
  {
    "text": "I'm going to use a more famous model that is metal Lama 3.1 which is uh Facebook released um you know a couple a",
    "start": "1504960",
    "end": "1512559"
  },
  {
    "text": "couple weeks ago which is uh uh highly regarded the model so as you can see it's uh I passed a bunch of um",
    "start": "1512559",
    "end": "1519279"
  },
  {
    "text": "parameters like the model file name the um the chat template how do you how do",
    "start": "1519279",
    "end": "1524399"
  },
  {
    "text": "you construct the chat in order to interact with the model so I do",
    "start": "1524399",
    "end": "1530080"
  },
  {
    "text": "um okay let me see it does have it oh it doesn't okay",
    "start": "1530080",
    "end": "1537840"
  },
  {
    "text": "we only have llama 3 it doesn't have llama 3.1 so let's do that I'm",
    "start": "1537840",
    "end": "1546120"
  },
  {
    "text": "sorry okay",
    "start": "1550960",
    "end": "1554960"
  },
  {
    "text": "all right so see this uh uh Tesla T T4 you know that's it's using the um the",
    "start": "1560200",
    "end": "1567640"
  },
  {
    "text": "the Cuda device now I'm going to ask it something else you know because this model is not particularly known for um",
    "start": "1567640",
    "end": "1573600"
  },
  {
    "text": "being good at say um um generating code right so I can ask an everyday question",
    "start": "1573600",
    "end": "1579039"
  },
  {
    "text": "it's so let's say plan me a",
    "start": "1579039",
    "end": "1584440"
  },
  {
    "text": "2day trip for site",
    "start": "1584440",
    "end": "1590640"
  },
  {
    "text": "scene in Hong Kong so you can do tell you something",
    "start": "1590640",
    "end": "1596000"
  },
  {
    "text": "that you can do with the weekend right it is you see the uh the Cuda GPU is a lot faster than",
    "start": "1596000",
    "end": "1603240"
  },
  {
    "text": "the Mac don't you see that you know that's uh you know it's uh it sprits out tax much faster and loads much faster",
    "start": "1603240",
    "end": "1608880"
  },
  {
    "text": "because I think you know um because the GPU itself has much um you know has much",
    "start": "1608880",
    "end": "1616120"
  },
  {
    "text": "faster memory you know so on the Mac the CPU U memory and GPU memory are shared so to load this 5 GB model into the uh",
    "start": "1616120",
    "end": "1623760"
  },
  {
    "text": "into the Mac M2 CPU takes multiple seconds as you can see there's a multiple second pause at the beginning of the conversation but as the GPU is",
    "start": "1623760",
    "end": "1630799"
  },
  {
    "text": "almost instantaneous because that's why the GPU was so expensive right you know that's why the the media stuff is more",
    "start": "1630799",
    "end": "1637039"
  },
  {
    "text": "expensive than the the similar stuff on Apple produces that's for that exact reason right you know so the the GPU is",
    "start": "1637039",
    "end": "1643880"
  },
  {
    "text": "much faster but you can see the does answer make sense I think it does you know let's go to the Victoria Peak you",
    "start": "1643880",
    "end": "1649679"
  },
  {
    "text": "know for the for the morning scene you know that's you see the view of the harbor you know afternoon you go to the",
    "start": "1649679",
    "end": "1654960"
  },
  {
    "text": "temple you know whatever and you can say I can even say please",
    "start": "1654960",
    "end": "1661640"
  },
  {
    "text": "translate the trip plan to",
    "start": "1661640",
    "end": "1667440"
  },
  {
    "text": "Chinese okay so you know my terminal is not I'm sorry you know that's oh oh oh I",
    "start": "1669840",
    "end": "1676519"
  },
  {
    "text": "know why you know that's uh um this one works much better on llama",
    "start": "1676519",
    "end": "1681600"
  },
  {
    "text": "3.1 you know so the major difference between llama 3 and llama 3.1 is llama 3 is only English so you know there was a",
    "start": "1681600",
    "end": "1688279"
  },
  {
    "text": "Chinese fine tune for llamas uh for the Lama 3 U for Lama 3 but llama 3.1 is the",
    "start": "1688279",
    "end": "1694440"
  },
  {
    "text": "one that they Incorporated all those different languages so I'm sorry you know that's um um well you even know to",
    "start": "1694440",
    "end": "1701279"
  },
  {
    "text": "traditional Chinese I didn't tell it's traditional Chinese okay anyway you know that's uh but then this be a good",
    "start": "1701279",
    "end": "1707440"
  },
  {
    "text": "exercise for you try on your machine right you know just to download the latest um you know um um uh llama model",
    "start": "1707440",
    "end": "1712880"
  },
  {
    "text": "and you will be able to you know so uh hopefully I have demonstrated that I build this thing on the do in in the",
    "start": "1712880",
    "end": "1719360"
  },
  {
    "text": "docker image with CPU only I move it move the file without recompiling without rewriting onto the mac and it",
    "start": "1719360",
    "end": "1727000"
  },
  {
    "text": "automatically understands that I need to use the Mac GPU the M2 GPU and generate",
    "start": "1727000",
    "end": "1732120"
  },
  {
    "text": "response for me and then I move it to uh a media machine Azure and then aut",
    "start": "1732120",
    "end": "1738080"
  },
  {
    "text": "automatically knows this machine has media GPU and it generate text for me right you know so so that's um so let's",
    "start": "1738080",
    "end": "1745679"
  },
  {
    "text": "go back to the presentation that's hopefully that's that's",
    "start": "1745679",
    "end": "1753760"
  },
  {
    "text": "um so yeah that's um you know so we have the entire script on on GitHub I I going",
    "start": "1754480",
    "end": "1759720"
  },
  {
    "text": "to upload this presentation to the um to the conference website so you can follow along as well and um um so here there",
    "start": "1759720",
    "end": "1768360"
  },
  {
    "text": "are some videos just in case it fail so the way that we did it really is that we",
    "start": "1768360",
    "end": "1776159"
  },
  {
    "text": "um there's a there's a new API in wasum called wnn wnn is abstraction that set",
    "start": "1776159",
    "end": "1783279"
  },
  {
    "text": "on top of uh Cuda or or met or or metal or all those GPU drivers so basically",
    "start": "1783279",
    "end": "1789679"
  },
  {
    "text": "you write applications to that API and it compiled to wasm stays in that API",
    "start": "1789679",
    "end": "1795000"
  },
  {
    "text": "but at the wrong time the wasm wrong time detects what's underneath it whether this has Cuda 11 available Cuda",
    "start": "1795000",
    "end": "1801279"
  },
  {
    "text": "12 available or it has metal available you know or it has some other you know uh int instruction set available and at",
    "start": "1801279",
    "end": "1808000"
  },
  {
    "text": "real time it's uh when it runs the application to translate that into GPU instructions and have it running on the",
    "start": "1808000",
    "end": "1813360"
  },
  {
    "text": "GPU right so it's a it's a fairly straightforward way to do it you know just provide a compatibility layer for",
    "start": "1813360",
    "end": "1819159"
  },
  {
    "text": "application developers so as application developers you no longer need to worry about what's underneath it the tradeoff",
    "start": "1819159",
    "end": "1825320"
  },
  {
    "text": "of course is that the the people becomes has to be more has to do more you know",
    "start": "1825320",
    "end": "1831600"
  },
  {
    "text": "it's a it's like you're installing operating system then you have to install the GPU drivers because you know",
    "start": "1831600",
    "end": "1837679"
  },
  {
    "text": "as far as I'm concerned you know that's the operating system is something that has to match the hardware if you have the hardware but you don't install the",
    "start": "1837679",
    "end": "1843480"
  },
  {
    "text": "drivers then you know you're not installing the operating system is that so you know so the Ops people it becomes",
    "start": "1843480",
    "end": "1849080"
  },
  {
    "text": "um um you know so for for a very long time we have Dev Ops as the same um you",
    "start": "1849080",
    "end": "1854360"
  },
  {
    "text": "know responsibility and uh uh you know we ask developer to the Ops but I think",
    "start": "1854360",
    "end": "1859399"
  },
  {
    "text": "in the AI era there are so many the the the underlying Hardware become so hetrogeneous that the Ops perhaps",
    "start": "1859399",
    "end": "1866320"
  },
  {
    "text": "becomes um you know a separate job all over again right you know so people some people would have to set up the Ops set",
    "start": "1866320",
    "end": "1872039"
  },
  {
    "text": "up the drivers and you know things like that and uh I want to go back to one slide to show you you know what this",
    "start": "1872039",
    "end": "1877760"
  },
  {
    "text": "wasi and soal wasi and API look like I don't know if you can you can see those but you know um the point I want to make",
    "start": "1877760",
    "end": "1884320"
  },
  {
    "text": "is that it's nothing like metal programming or Cuda programming I me whereas you have a lot of TDS very",
    "start": "1884320",
    "end": "1889600"
  },
  {
    "text": "lowlevel apis the whole application that I've have just shown you the chatbot application is can be fit into one PDF",
    "start": "1889600",
    "end": "1897760"
  },
  {
    "text": "uh PP slide and uh you can still read it it's not too small for you to read it so",
    "start": "1897760",
    "end": "1902960"
  },
  {
    "text": "it's other the right on this side is just you know you can see all those Cod are R by the way but you can do with",
    "start": "1902960",
    "end": "1909559"
  },
  {
    "text": "JavaScript as well you know that's uh so you can you have object for graph Builder and you you reading the file",
    "start": "1909559",
    "end": "1916240"
  },
  {
    "text": "name and you read the file into the model name and you read the model into the memory and then you build a context",
    "start": "1916240",
    "end": "1923000"
  },
  {
    "text": "you know load everything into the memory and then you have a loop the loop basically takes the takes the user input",
    "start": "1923000",
    "end": "1930559"
  },
  {
    "text": "and then you know uh ask the model to generate a response and once it generate",
    "start": "1930559",
    "end": "1936480"
  },
  {
    "text": "a response to convert that because the response is a array is a is a array of numbers you encode that into language",
    "start": "1936480",
    "end": "1942919"
  },
  {
    "text": "and then give it back to you and then ask you to to to ask another question so at the top it just says user right you",
    "start": "1942919",
    "end": "1948519"
  },
  {
    "text": "know and then you know then when the Mel is and then you know all",
    "start": "1948519",
    "end": "1953919"
  },
  {
    "text": "assisted all right sorry okay all right so this is um so so",
    "start": "1953919",
    "end": "1960360"
  },
  {
    "text": "what I'm I'm trying to say is a really simple code for you to um uh uh for the developers and then uh The Operators you",
    "start": "1960360",
    "end": "1968120"
  },
  {
    "text": "know the the people who do the it operation should be able to um you know install the right drivers and the right",
    "start": "1968120",
    "end": "1973399"
  },
  {
    "text": "WR times on their Hardware right so uh I think I have maybe 3 minutes left so um",
    "start": "1973399",
    "end": "1980120"
  },
  {
    "text": "I have shown you all the stuff that's you know um uh that runs wased that's",
    "start": "1980120",
    "end": "1985880"
  },
  {
    "text": "command line chatbot and all that um there's um um people keep asking me you know is there is there a crossplatform",
    "start": "1985880",
    "end": "1992320"
  },
  {
    "text": "GUI application that I can try you know that's I don't want to install uh those",
    "start": "1992320",
    "end": "1997679"
  },
  {
    "text": "sayings by hand like you just did with the command line and all that so um we are happy to to report there's a new",
    "start": "1997679",
    "end": "2003320"
  },
  {
    "text": "open source project and I think um it's um you know there's",
    "start": "2003320",
    "end": "2009159"
  },
  {
    "text": "um you know it's it's a combination of several large open source communities one is a crossplatform um Rost UI",
    "start": "2009159",
    "end": "2015519"
  },
  {
    "text": "Community is called rubius and one is wasm Edge community so they provide the",
    "start": "2015519",
    "end": "2020639"
  },
  {
    "text": "crossplatform user interface using rust we provide the cross GPU and inference engine that embedded into this",
    "start": "2020639",
    "end": "2026720"
  },
  {
    "text": "application so there's a there's so the result of this application is called Ming and U it's U um it has reached the",
    "start": "2026720",
    "end": "2035399"
  },
  {
    "text": "RPA releas it's all open source all on GitHub so you can install it for the Mac you can install it for Linux um actually",
    "start": "2035399",
    "end": "2042200"
  },
  {
    "text": "I didn't have the link but you can if you go to the repository you can also install it for Windows and uh um I don't",
    "start": "2042200",
    "end": "2048440"
  },
  {
    "text": "think I really have time to uh to to to demonstrate this but you know it's a um",
    "start": "2048440",
    "end": "2054158"
  },
  {
    "text": "the user interface looks like something like that so you go there and it give you a list of models that you can choose from and when you choose a model it um",
    "start": "2054159",
    "end": "2061919"
  },
  {
    "text": "give you the chat about interface you can you can interact with model you can actually switch to a different model in",
    "start": "2061919",
    "end": "2067440"
  },
  {
    "text": "the in in the middle of the chat you know so I started with so I wouldn't have the problem as I I just had right",
    "start": "2067440",
    "end": "2072560"
  },
  {
    "text": "you know I just uh um you know in fact the the the demo that didn't work really",
    "start": "2072560",
    "end": "2078158"
  },
  {
    "text": "made my point is that different models has very different capabilities so you really need to build your application",
    "start": "2078159",
    "end": "2083919"
  },
  {
    "text": "around the model not build the application side by side with the model because on the API provider on the side",
    "start": "2083919",
    "end": "2089960"
  },
  {
    "text": "car of the model provider they can upgrade the model at any time they would think llama 3.1 and Lama 3 are the same",
    "start": "2089960",
    "end": "2095878"
  },
  {
    "text": "you know all very similar but in fact for the use case of translation they they're drastically different right so",
    "start": "2095879",
    "end": "2101720"
  },
  {
    "text": "moing has um allows you to um switch between models and embed your model into your application and allow so you know",
    "start": "2101720",
    "end": "2109119"
  },
  {
    "text": "so there's um um many models you can choose from and then you can optimize for that I know it's a little too small",
    "start": "2109119",
    "end": "2114640"
  },
  {
    "text": "for you to read but you know uh if you are interested just uh you know go to um",
    "start": "2114640",
    "end": "2120440"
  },
  {
    "text": "go to this GitHub repository and KCK KCK on the releases and then you can download the the installer package for",
    "start": "2120440",
    "end": "2127200"
  },
  {
    "text": "Mac uh for Windows and for um you know for Linux and and and try this yourself",
    "start": "2127200",
    "end": "2133359"
  },
  {
    "text": "anyway so that's that's all I have and uh I think my time is almost up and",
    "start": "2133359",
    "end": "2138400"
  },
  {
    "text": "thank you [Applause]",
    "start": "2138400",
    "end": "2146109"
  },
  {
    "text": "yeah uh yes please so love this um what what we're",
    "start": "2146839",
    "end": "2153880"
  },
  {
    "text": "trying to do and what we're looking forward to doing in the future for our company is we write software for doctors",
    "start": "2153880",
    "end": "2160400"
  },
  {
    "text": "and we're going to be implementing llm but most likely we'll be implementing it on server side because they access the",
    "start": "2160400",
    "end": "2168160"
  },
  {
    "text": "application through a web browser or through an EHR or some application what are your thoughts on actually getting",
    "start": "2168160",
    "end": "2175000"
  },
  {
    "text": "edge llms or slms onto like a browser for instance or on their machines when",
    "start": "2175000",
    "end": "2182200"
  },
  {
    "text": "they log in the the the llm comes in through a CDN or something like that and",
    "start": "2182200",
    "end": "2187880"
  },
  {
    "text": "they can interact through uh the llm as fast as you're able to interact with the",
    "start": "2187880",
    "end": "2193359"
  },
  {
    "text": "GPU there what are your thoughts there yeah I think so I I think it's so in the device use cases you have to",
    "start": "2193359",
    "end": "2199079"
  },
  {
    "text": "pre-download the RMS I think you can't stream the RM um because I um we had another demo that use uh one of the",
    "start": "2199079",
    "end": "2205839"
  },
  {
    "text": "smallest LMS that we can find which is Alibaba Chen when it's only have half a billion parameters you know instead of 7",
    "start": "2205839",
    "end": "2212640"
  },
  {
    "text": "billion the you know so it's only have a half a billion but even that model uh qu",
    "start": "2212640",
    "end": "2218079"
  },
  {
    "text": "quantized you still have a half a giga gigabytes of space you know so you can't stream it on in the browser but Chrome",
    "start": "2218079",
    "end": "2225040"
  },
  {
    "text": "is is bundling a large language model in that in the browser itself right you know so you sort of you pre-downloaded",
    "start": "2225040",
    "end": "2231240"
  },
  {
    "text": "it that way yeah yeah that's that's that's a good point so you would need some form of operations or operational",
    "start": "2231240",
    "end": "2237200"
  },
  {
    "text": "team or have them download a llm in advance before yeah got it got it okay",
    "start": "2237200",
    "end": "2242920"
  },
  {
    "text": "thanks or or you distribute a client side application like this and ble your large language model interet to say",
    "start": "2242920",
    "end": "2248359"
  },
  {
    "text": "please install this right you know yeah yeah we could always package our our releases like that okay",
    "start": "2248359",
    "end": "2255560"
  },
  {
    "text": "thanks company that has already adopted this kind of like um runtime in their",
    "start": "2268760",
    "end": "2275520"
  },
  {
    "text": "production environment um yeah so you know it's um so we have uh we have a lot",
    "start": "2275520",
    "end": "2281160"
  },
  {
    "text": "of users for um um for the um production use case we have a we have a large",
    "start": "2281160",
    "end": "2287839"
  },
  {
    "text": "collaborator called uh called Gia ganet you know so what they do is they build personalized knowledge agent so",
    "start": "2287839",
    "end": "2293800"
  },
  {
    "text": "basically um every one of us is good at something right you know that's uh because maybe I'm good at physics you",
    "start": "2293800",
    "end": "2299880"
  },
  {
    "text": "know so I want to turn my physics knowledge into a knowledge base and then have the model to interact with other",
    "start": "2299880",
    "end": "2306400"
  },
  {
    "text": "people on my behalf right you know that's for instance my son who's uh who's in Middle School build a chemistry",
    "start": "2306400",
    "end": "2311800"
  },
  {
    "text": "part together with another author right the that other author wrote a a chemistry textbook so he turned that",
    "start": "2311800",
    "end": "2318880"
  },
  {
    "text": "into a knowledge base and run a large language model around that knowledge base build a whole application and make",
    "start": "2318880",
    "end": "2325119"
  },
  {
    "text": "it available in Discord that's uh you know so his his peer students would be able to ask questions about that you",
    "start": "2325119",
    "end": "2331359"
  },
  {
    "text": "know that so you know so there's um people who's working on those um those knowledge networks and you know and for",
    "start": "2331359",
    "end": "2339440"
  },
  {
    "text": "those use cases you need each use case you need a different model because sometime you need to find T the model",
    "start": "2339440",
    "end": "2344920"
  },
  {
    "text": "you need make the model to speak like a teacher for instance and you definitely need a custom knowledge that be injected",
    "start": "2344920",
    "end": "2350640"
  },
  {
    "text": "into the model so that the model speak reliably about CH chemistry element you know so to give you just to drag it a",
    "start": "2350640",
    "end": "2356240"
  },
  {
    "text": "little longer and the reason that we did it is because he asked the model question what is Mercury the model chat",
    "start": "2356240",
    "end": "2362440"
  },
  {
    "text": "gbt said Mercury is the innermost planet of a solar system blah blah blah blah but he's not asking for Mercury is the",
    "start": "2362440",
    "end": "2367760"
  },
  {
    "text": "planet he's asking for mercury the element right you know so he clarified his question what what about the element",
    "start": "2367760",
    "end": "2372880"
  },
  {
    "text": "then the chb answered it correctly for several rounds and then he asked what's the melting temperature of Mercury we",
    "start": "2372880",
    "end": "2378280"
  },
  {
    "text": "already knew the Mercury is liquid at room temperature so the melting temperature must be very low but chb answer is 400 Fahrenheit so at um you",
    "start": "2378280",
    "end": "2385839"
  },
  {
    "text": "know for a longest time I couldn't figure out why it had to be the answer to that then I realized the surface",
    "start": "2385839",
    "end": "2391440"
  },
  {
    "text": "temperature of Mercury the planet is 400 Fahrenheit okay so you can see with all the knowledge that in the in the large",
    "start": "2391440",
    "end": "2397839"
  },
  {
    "text": "language model it's it's hallucinates you know um by mixing those facts together right you know so to build a",
    "start": "2397839",
    "end": "2404560"
  },
  {
    "text": "custom build model with a custom build knowledge base package them together in an application I think has tremendous",
    "start": "2404560",
    "end": "2409599"
  },
  {
    "text": "value because that's really you know reduces the type of you is the the type of you know uncertain answers or the",
    "start": "2409599",
    "end": "2416000"
  },
  {
    "text": "hallucinations that the model provides to you thank you yeah all right thank you so much and um",
    "start": "2416000",
    "end": "2423359"
  },
  {
    "text": "you know um enjoy your enjoy your weekend yeah [Applause]",
    "start": "2423359",
    "end": "2430799"
  }
]