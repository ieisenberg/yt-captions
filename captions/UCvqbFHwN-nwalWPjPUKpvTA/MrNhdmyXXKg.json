[
  {
    "start": "0",
    "end": "35000"
  },
  {
    "text": "hello everyone i'm very happy to literally present today i'd rather be in",
    "start": "1520",
    "end": "6560"
  },
  {
    "text": "the same room as you are but we can do that hopefully next year",
    "start": "6560",
    "end": "11920"
  },
  {
    "text": "um so first of all let me introduce myself i'm lauren bernard i'm staff engineer at datadog and",
    "start": "11920",
    "end": "18480"
  },
  {
    "text": "i work on communities topics mostly and today i'm going to share a few",
    "start": "18480",
    "end": "23600"
  },
  {
    "text": "stories of incidents we had from which we learned a lot and i'm going to share what we",
    "start": "23600",
    "end": "30240"
  },
  {
    "text": "learned during this incident in case you don't know datadog we're a",
    "start": "30240",
    "end": "37920"
  },
  {
    "start": "35000",
    "end": "35000"
  },
  {
    "text": "sas based monitoring company i gave some figures on the left-hand side of the slides",
    "start": "37920",
    "end": "43520"
  },
  {
    "text": "but today we're not going to talk about datadog the product we're going to talk about",
    "start": "43520",
    "end": "48559"
  },
  {
    "text": "our infrastructure and to give you an idea because this is what will matter today",
    "start": "48559",
    "end": "54000"
  },
  {
    "text": "we're running dozens of kubernetes cluster some of them up to 4 000 nodes and we have tens of",
    "start": "54000",
    "end": "59840"
  },
  {
    "text": "thousands of nodes and we're growing pretty fast in terms of context uh we've shared a",
    "start": "59840",
    "end": "66799"
  },
  {
    "text": "few stories about our migration to kubernetes over the last two years",
    "start": "66799",
    "end": "72080"
  },
  {
    "text": "and i gave some examples there and what has changed in the meantime is our",
    "start": "72080",
    "end": "77759"
  },
  {
    "text": "infrastructure has kept growing and it's getting bigger and we found new scalability issues and",
    "start": "77759",
    "end": "83600"
  },
  {
    "text": "also much more complex problems and doing this um we've faced a lot of",
    "start": "83600",
    "end": "91439"
  },
  {
    "text": "issues and and today i've picked three different incidents that i personally like for",
    "start": "91439",
    "end": "98720"
  },
  {
    "text": "different reasons and the first one is actually the title of this talk and uh and we're going to talk about",
    "start": "98720",
    "end": "104560"
  },
  {
    "text": "this incident because it's interesting how interaction between different components",
    "start": "104560",
    "end": "109840"
  },
  {
    "text": "made this very surprising event happen so",
    "start": "109840",
    "end": "116159"
  },
  {
    "start": "115000",
    "end": "115000"
  },
  {
    "text": "this incident started when an engineer reached out to us and told us well my name space and everything in it is",
    "start": "116159",
    "end": "122479"
  },
  {
    "text": "gone um of course the first thing we did is help the team recover and redeploy everything",
    "start": "122479",
    "end": "129840"
  },
  {
    "text": "but then we wanted to understand what had happened right so we started the investigation and the",
    "start": "129840",
    "end": "136000"
  },
  {
    "start": "134000",
    "end": "134000"
  },
  {
    "text": "first thing we did is look at audit logs to see what happened and of course we",
    "start": "136000",
    "end": "141840"
  },
  {
    "text": "found a lot of deletion calls for every single thing in the namespace right from",
    "start": "141840",
    "end": "147040"
  },
  {
    "text": "replica sets to events and deployments and of course this was matching",
    "start": "147040",
    "end": "152080"
  },
  {
    "text": "perfectly the issue the engineer has seen however we didn't see the delete",
    "start": "152080",
    "end": "160080"
  },
  {
    "text": "namespace call so the initial call that should have triggered this event and of course the question became why",
    "start": "160080",
    "end": "167040"
  },
  {
    "text": "was the namespace deleted so we're looking at audit logs and we",
    "start": "167040",
    "end": "172160"
  },
  {
    "start": "171000",
    "end": "171000"
  },
  {
    "text": "couldn't find anything and we're zooming out zooming out looking at all the events for the specific namespace",
    "start": "172160",
    "end": "178080"
  },
  {
    "text": "and after zooming out enough we discover a delete call on the namespace four days",
    "start": "178080",
    "end": "184400"
  },
  {
    "text": "before the one highlighted there so this is when the namespace was successfully",
    "start": "184400",
    "end": "189440"
  },
  {
    "text": "deleted however this successful namespace deletion happened on the certains when the resource",
    "start": "189440",
    "end": "196400"
  },
  {
    "text": "edition happened four days later so of course we reached out to the",
    "start": "196400",
    "end": "203120"
  },
  {
    "text": "engineer that did the delete call and we were like why why did you delete the namespace",
    "start": "203120",
    "end": "208560"
  },
  {
    "text": "what happens and he told us well i didn't delete the namespace so it was all very weird and we started",
    "start": "208560",
    "end": "215680"
  },
  {
    "text": "discussing and he explained to her that he was migrating to a new deployment method so let's take a",
    "start": "215680",
    "end": "221360"
  },
  {
    "text": "look at how deployments work at databack v1 version of our deployment methods is",
    "start": "221360",
    "end": "228720"
  },
  {
    "start": "225000",
    "end": "225000"
  },
  {
    "text": "the one on this slide where engineers would modify the charts it would go through ci",
    "start": "228720",
    "end": "234080"
  },
  {
    "text": "we would create a rounded chart with everything in it and at a later time another engineer",
    "start": "234080",
    "end": "239920"
  },
  {
    "text": "would trigger a deployment using spinnaker that would retrieve the run the chart and use cube ctl applied to apply to the",
    "start": "239920",
    "end": "246720"
  },
  {
    "text": "cluster fairly straightforward this was working fine but we were trying",
    "start": "246720",
    "end": "253120"
  },
  {
    "text": "to make things better and we decided to move to a new version that is described here where all the first part is the",
    "start": "253120",
    "end": "260160"
  },
  {
    "start": "257000",
    "end": "257000"
  },
  {
    "text": "same but at the end to deploy to the cluster we are using home upgrade instead of",
    "start": "260160",
    "end": "266800"
  },
  {
    "text": "cuba city applied so it looks very similar right but there's a difference",
    "start": "266800",
    "end": "272800"
  },
  {
    "text": "if you use the first message and you have a chart with three resources a b and c and you create",
    "start": "272800",
    "end": "279520"
  },
  {
    "start": "273000",
    "end": "273000"
  },
  {
    "text": "a new version of this chart with new versions of b and c and",
    "start": "279520",
    "end": "284960"
  },
  {
    "text": "where you remove a when you do cubesat apply b and c will be updated of course but a",
    "start": "284960",
    "end": "291040"
  },
  {
    "text": "will remain because cubesat yellow won't delete it however when you use helm upgrade",
    "start": "291040",
    "end": "297919"
  },
  {
    "text": "helm is keeping track of the resources you deployed so it's going to see that a is not there",
    "start": "297919",
    "end": "303520"
  },
  {
    "text": "anymore and it will remove it so that's a big difference right if you remove something from your chart",
    "start": "303520",
    "end": "308880"
  },
  {
    "text": "it's not going to be in the cluster anymore and so what happened here is the chart",
    "start": "308880",
    "end": "315600"
  },
  {
    "start": "312000",
    "end": "312000"
  },
  {
    "text": "was refactored and the name space was moved out of the chart and so when the chart was reapplied",
    "start": "315600",
    "end": "321680"
  },
  {
    "text": "using helm the namespace was defeated so now we understand why the name says was deleted but what was very surprising was why did",
    "start": "321680",
    "end": "329840"
  },
  {
    "text": "it take four days so to understand what what had happened",
    "start": "329840",
    "end": "335280"
  },
  {
    "text": "uh we had to look at the namespace controller code and here on this slide you can see the",
    "start": "335280",
    "end": "341280"
  },
  {
    "text": "function responsible for handling the delete name space call and this function is is made of two",
    "start": "341280",
    "end": "349680"
  },
  {
    "text": "important parts the first part is getting all the resources in the namespace all the available",
    "start": "349680",
    "end": "355360"
  },
  {
    "text": "resources and the second part is iterating over all these api resources and deleting them",
    "start": "355360",
    "end": "361440"
  },
  {
    "text": "pretty simple right let's zoom on the discover all api resources parts",
    "start": "361440",
    "end": "368560"
  },
  {
    "text": "if you look at this this is how the namespace controller discover all the api resources that are",
    "start": "368560",
    "end": "374639"
  },
  {
    "text": "namespace and if you can see the path i highlight it there you can see that if the",
    "start": "374639",
    "end": "380240"
  },
  {
    "text": "discovery call fails then the function immediately returns which means if you can't discover all",
    "start": "380240",
    "end": "386639"
  },
  {
    "text": "resources in the cluster the content of the messages won't be deleted",
    "start": "386639",
    "end": "392240"
  },
  {
    "text": "note that this behavior has changed in kubernetes 116. um but this is what happened there",
    "start": "392240",
    "end": "399840"
  },
  {
    "text": "um when we saw that we looked at the code from the controller manager where the namespace controller is running and",
    "start": "400000",
    "end": "406560"
  },
  {
    "text": "in the logs we saw many of currencies of this line that says i'm unable to retrieve the complete list",
    "start": "406560",
    "end": "413360"
  },
  {
    "start": "407000",
    "end": "407000"
  },
  {
    "text": "of api server apis because this one is not answering and the metrics one so the problem with this",
    "start": "413360",
    "end": "421120"
  },
  {
    "text": "log that that make uh that makes it a bit hard to to pass is that there's no context you",
    "start": "421120",
    "end": "427039"
  },
  {
    "text": "don't know that this log is coming from the namespace controller so it's hard to link to the incident but now that we know that uh this is the",
    "start": "427039",
    "end": "434880"
  },
  {
    "text": "error from the discovery call and we have potential likely codes the metrics api",
    "start": "434880",
    "end": "442639"
  },
  {
    "text": "so let's talk about the metrics api so if you're familiar with kubernetes",
    "start": "442639",
    "end": "448160"
  },
  {
    "text": "you've probably heard about metric server metric server is an additional controller provide providing metrics",
    "start": "448160",
    "end": "454080"
  },
  {
    "text": "about nodes and pods and so you can do qtl tab for instance or use",
    "start": "454080",
    "end": "459919"
  },
  {
    "text": "horizontal port autoscaling and it registers an api service which is an additional",
    "start": "459919",
    "end": "466400"
  },
  {
    "text": "set of coins apis and in that case uh metrics api",
    "start": "466400",
    "end": "471520"
  },
  {
    "text": "and it's managed by an external controller and when this api service is registered two",
    "start": "471520",
    "end": "477199"
  },
  {
    "text": "pieces of information are provided the additional apis you're providing and where to forward this api call in",
    "start": "477199",
    "end": "484720"
  },
  {
    "text": "that case the metric server controller in the cube system namespace",
    "start": "484720",
    "end": "489759"
  },
  {
    "text": "so let's go back to the series of events so far on february 13th",
    "start": "489759",
    "end": "496879"
  },
  {
    "start": "490000",
    "end": "490000"
  },
  {
    "text": "the name space was deleted in the next four days the namespace controller failed to list",
    "start": "496879",
    "end": "502400"
  },
  {
    "text": "resources and and as a consequence dylan didn't delete the content of the",
    "start": "502400",
    "end": "508240"
  },
  {
    "text": "namespace and then something happened on the 17th that unblocked the namespace controller and",
    "start": "508240",
    "end": "514240"
  },
  {
    "text": "then everything was deleted so we looked at advanced on metric",
    "start": "514240",
    "end": "520240"
  },
  {
    "text": "server and we discovered that on this date the metric server was um killed and it restarted and",
    "start": "520240",
    "end": "526800"
  },
  {
    "text": "afterwards it was working fine and everything wasn't blocked and everything was defeated",
    "start": "526800",
    "end": "532000"
  },
  {
    "text": "so the next question was but why was metric 7 failing so to understand this we need to look at",
    "start": "532000",
    "end": "538560"
  },
  {
    "text": "how metric server is deployed in our setup so metric server is using a certificate",
    "start": "538560",
    "end": "544800"
  },
  {
    "start": "539000",
    "end": "539000"
  },
  {
    "text": "so to verify its identity and we have to rotate the certificate and we use a",
    "start": "544800",
    "end": "549920"
  },
  {
    "text": "sidecar to generate a certificate on a regular basis and signal metric server",
    "start": "549920",
    "end": "555200"
  },
  {
    "text": "to reload so that works completely fine but it requires a specific cointest",
    "start": "555200",
    "end": "560399"
  },
  {
    "text": "feature which is called shared namespace so the sidecar can actually find",
    "start": "560399",
    "end": "565920"
  },
  {
    "text": "metric server and for this feature to work you need two things you need to modify the spec of your pod but you also need",
    "start": "565920",
    "end": "573680"
  },
  {
    "text": "feature gates on the api server and most importantly for our example here on the cubelet in terms of how metric",
    "start": "573680",
    "end": "582399"
  },
  {
    "start": "581000",
    "end": "581000"
  },
  {
    "text": "server was deployed it was running on a set of nodes where we run all our controllers",
    "start": "582399",
    "end": "587440"
  },
  {
    "text": "and this was working completely fine except it wasn't exactly like this it was actually looking like this",
    "start": "587440",
    "end": "594640"
  },
  {
    "text": "so what i want to highlight in this new diagram is that nodes were a bit different we",
    "start": "594640",
    "end": "600080"
  },
  {
    "text": "had recent nodes where the feature gate was enabled and old nodes where the feature gate was",
    "start": "600080",
    "end": "605519"
  },
  {
    "text": "missing so when we had deployed metric server it had been deployed on a new node and everything was fine",
    "start": "605519",
    "end": "611120"
  },
  {
    "text": "and at one point it was rescheduled on an old mode and seeing something failing because the",
    "start": "611120",
    "end": "617279"
  },
  {
    "text": "site car wasn't able to signal metric server so matrix server didn't reload its certificate and then api server was failing to contact it",
    "start": "617279",
    "end": "627200"
  },
  {
    "start": "627000",
    "end": "627000"
  },
  {
    "text": "so here is the overall sequence of events sometime before the 13th network server",
    "start": "627279",
    "end": "634160"
  },
  {
    "text": "was rescheduled on a note note and discovery calls failed and after being um killed it picked up",
    "start": "634160",
    "end": "641839"
  },
  {
    "text": "the very recent certificate that the stat car i generated from disk and everything was working fine again",
    "start": "641839",
    "end": "647360"
  },
  {
    "text": "and in our case it was a bad thing because when everything started working fine again everything",
    "start": "647360",
    "end": "652399"
  },
  {
    "text": "was deleted so key takeaway api server extensions",
    "start": "652399",
    "end": "658480"
  },
  {
    "start": "655000",
    "end": "655000"
  },
  {
    "text": "are great because you can extend your api uh your kubernetes cluster to have new apis",
    "start": "658480",
    "end": "663920"
  },
  {
    "text": "and but they can be tricky if the api is not used very much",
    "start": "663920",
    "end": "669279"
  },
  {
    "text": "these extensions can be done for days without any any impact right however some controllers rely this api",
    "start": "669279",
    "end": "676720"
  },
  {
    "text": "to be available to work properly and can trigger issues in addition the communication between",
    "start": "676720",
    "end": "683839"
  },
  {
    "text": "the api server and api services is tricky to do right in terms of security",
    "start": "683839",
    "end": "689839"
  },
  {
    "text": "and if you're interested i really invite you to go and see this talk by tabisa a colleague of mine",
    "start": "689839",
    "end": "695600"
  },
  {
    "text": "where you learn a lot about tls and api service extensions",
    "start": "695600",
    "end": "702800"
  },
  {
    "text": "so we're now going to talk about a very different incident and i really like this one because um",
    "start": "702800",
    "end": "710320"
  },
  {
    "text": "it's a low-level bug that also involved networking which i tend to like quite a bit so",
    "start": "710320",
    "end": "719279"
  },
  {
    "text": "luckily this incident only happened in staging but it's still interesting so we're sharing what we",
    "start": "719279",
    "end": "724480"
  },
  {
    "start": "720000",
    "end": "720000"
  },
  {
    "text": "learned from it so we run our nodes in kubernetes and to",
    "start": "724480",
    "end": "729600"
  },
  {
    "text": "do that of course we have to build images and an engineer on the team pushed a small change and when we were",
    "start": "729600",
    "end": "736880"
  },
  {
    "text": "testing this new image we noted that nodes were becoming not ready after a few days",
    "start": "736880",
    "end": "742800"
  },
  {
    "text": "and what was weird is that change was in no way related to the cubelet or the runtime",
    "start": "742800",
    "end": "748959"
  },
  {
    "text": "but of course uh the nodes were not working so we couldn't promote to production",
    "start": "748959",
    "end": "754880"
  },
  {
    "text": "so we started investigation and the simplest thing to do was to describe the node and we can see",
    "start": "754880",
    "end": "760560"
  },
  {
    "text": "this message here where the cubelet was not ready because the container runtime",
    "start": "760560",
    "end": "766079"
  },
  {
    "text": "was down container runtime is down well we use continuity at the run time",
    "start": "766079",
    "end": "771920"
  },
  {
    "start": "769000",
    "end": "769000"
  },
  {
    "text": "and so the first thing we did is check if continuity was working properly and crstl seemed to say well yeah",
    "start": "771920",
    "end": "778560"
  },
  {
    "text": "content is working fine so we went back to the cube that logs",
    "start": "778560",
    "end": "784560"
  },
  {
    "text": "and we found this line that were interesting especially the one i highlighted at the in the middle",
    "start": "784560",
    "end": "790480"
  },
  {
    "text": "of the slide status from runtime service failed so we wondered maybe we can use your",
    "start": "790480",
    "end": "796399"
  },
  {
    "text": "ictl to do the status call it turned out there is a ci et cetera",
    "start": "796399",
    "end": "801839"
  },
  {
    "text": "method to do that and and so we we wanted to do it with sierra ctl",
    "start": "801839",
    "end": "807760"
  },
  {
    "text": "there's no cri cgl status sub command however there is an info sub command",
    "start": "807760",
    "end": "813200"
  },
  {
    "text": "that looks very close and so we checked in the code and if you call cricitl info",
    "start": "813200",
    "end": "819279"
  },
  {
    "text": "it actually invokes uh the status cri method and look we did cricgr info and",
    "start": "819279",
    "end": "827600"
  },
  {
    "text": "everything was completely blocked so now we know that there was definitely",
    "start": "827600",
    "end": "833600"
  },
  {
    "text": "an issue and that the qubit was right so we looked at the code of the status function in continuity",
    "start": "833600",
    "end": "839920"
  },
  {
    "text": "and it doesn't do much except invoking this function here netplugin.status so well the next step",
    "start": "839920",
    "end": "846880"
  },
  {
    "text": "was to look at this function this function is not really doing much",
    "start": "846880",
    "end": "853279"
  },
  {
    "start": "850000",
    "end": "850000"
  },
  {
    "text": "as you can see it's basically verifying that you've successfully passed",
    "start": "853279",
    "end": "858639"
  },
  {
    "text": "cni configuration and of course this error we already know",
    "start": "858639",
    "end": "864639"
  },
  {
    "text": "it's going to tell you that the network is not ready and it's very clear in the queue that",
    "start": "864639",
    "end": "870320"
  },
  {
    "text": "logs however it's not what we're seeing there we're seeing a timeout and",
    "start": "870320",
    "end": "875760"
  },
  {
    "text": "if you look at the code closely you will see that on line 126 we have a lock",
    "start": "875760",
    "end": "882000"
  },
  {
    "text": "attempt we have something that hangs we have a lock could this be related",
    "start": "882000",
    "end": "888639"
  },
  {
    "text": "so what we did next is take a dump of all the goroutines from containergy to",
    "start": "888639",
    "end": "894480"
  },
  {
    "start": "889000",
    "end": "889000"
  },
  {
    "text": "find blocked ones and we found quite a lot of them and as you can see here all these school",
    "start": "894480",
    "end": "900959"
  },
  {
    "text": "routines are blocked trying to acquire luck and there's a different one",
    "start": "900959",
    "end": "906000"
  },
  {
    "text": "every two minutes which is interesting because the errors we were seeing in the cube black were also happening every two minutes",
    "start": "906000",
    "end": "912880"
  },
  {
    "text": "and if we look at the stack trace from one of them we can see that the",
    "start": "912880",
    "end": "918160"
  },
  {
    "text": "lines match exactly what we're looking at before uh in in the code so it's exactly the problem right the lock is the problem",
    "start": "918160",
    "end": "926720"
  },
  {
    "text": "so it definitely seems sienna related now so let's bypass uh continuity and the",
    "start": "926720",
    "end": "932320"
  },
  {
    "start": "927000",
    "end": "927000"
  },
  {
    "text": "cubelet and see if we can reproduce more simply so what we did is we simply",
    "start": "932320",
    "end": "937839"
  },
  {
    "text": "created a network namespace and used this very convenient tool cni tool that will invoke cni without",
    "start": "937839",
    "end": "944560"
  },
  {
    "text": "going through the cuban container it worked fine and as you can see at the bottom of the slide the networking was",
    "start": "944560",
    "end": "950480"
  },
  {
    "text": "set up okay and were able to to being outside of the network namespace so everything was okay",
    "start": "950480",
    "end": "957360"
  },
  {
    "text": "the next step was we tried to delete uh the network namespace and this",
    "start": "957360",
    "end": "963600"
  },
  {
    "start": "958000",
    "end": "958000"
  },
  {
    "text": "completely blocked two so we actually had identified at that",
    "start": "963600",
    "end": "968800"
  },
  {
    "text": "point that the digital was a problem and after doing that we looked at the process tree",
    "start": "968800",
    "end": "974480"
  },
  {
    "text": "and we found this command here that had been blocked for quite some time",
    "start": "974480",
    "end": "982320"
  },
  {
    "text": "so let's talk about uh how we do cni in this cluster we use the lift cni",
    "start": "982320",
    "end": "987680"
  },
  {
    "start": "983000",
    "end": "983000"
  },
  {
    "text": "plugin which we've been extremely happy with and the delete calls fails for one of",
    "start": "987680",
    "end": "995839"
  },
  {
    "text": "the plugin a numbered ptp here and what we did is",
    "start": "995839",
    "end": "1000880"
  },
  {
    "text": "we modified the code of this plugin to add additional logs and after a few tests we tracked the",
    "start": "1000880",
    "end": "1006800"
  },
  {
    "text": "problem to this called this call here and what's weird",
    "start": "1006800",
    "end": "1011839"
  },
  {
    "text": "is this is a call from the net link library so a low level net link library it's not a code from the listing plugin so we",
    "start": "1011839",
    "end": "1019920"
  },
  {
    "text": "went to the netflix repo to look for clues",
    "start": "1019920",
    "end": "1025120"
  },
  {
    "start": "1025000",
    "end": "1025000"
  },
  {
    "text": "and we found this pull request where it actually says that this function",
    "start": "1025120",
    "end": "1030880"
  },
  {
    "text": "specifically doesn't work for knl 420 and needed to be fixed and i really want to say to thank daniel",
    "start": "1030880",
    "end": "1038240"
  },
  {
    "text": "baltman for finding the problem in fixing it because after updating the netlink library in",
    "start": "1038240",
    "end": "1043280"
  },
  {
    "text": "our cni plugin everything started working perfectly okay again",
    "start": "1043280",
    "end": "1048880"
  },
  {
    "text": "so as a summary uh what happened here is well um we configured packer to use the",
    "start": "1048880",
    "end": "1054960"
  },
  {
    "text": "latest boot 1904 and at one point the latest ubuntu changed from channel 415 to 5.4",
    "start": "1054960",
    "end": "1061919"
  },
  {
    "text": "and as we've just seen we had an issue with uh candle 420 and the reason the",
    "start": "1061919",
    "end": "1068559"
  },
  {
    "text": "behavior was so weird and it took a few days for things to break was it was a test cluster and this bug",
    "start": "1068559",
    "end": "1074400"
  },
  {
    "text": "only happened on pert deletion so you need the pod to be deleted to actually trigger it",
    "start": "1074400",
    "end": "1081840"
  },
  {
    "text": "so key takeaways from this incident is we tend to consider nodes as abstract compute resources but we need",
    "start": "1081919",
    "end": "1087840"
  },
  {
    "text": "to remember that nodes are actually instances where you have a kernel distribution hardware",
    "start": "1087840",
    "end": "1093440"
  },
  {
    "text": "and all these things can fail and also remember that error messages can be pretty misleading here we have",
    "start": "1093440",
    "end": "1101120"
  },
  {
    "text": "the logs in the qubit were saying that the runtime was down when actually only some parts of the runtime was was down",
    "start": "1101120",
    "end": "1107440"
  },
  {
    "text": "and it was hard to diagnose we're now getting to uh the last story i",
    "start": "1107440",
    "end": "1114480"
  },
  {
    "text": "want to share with you today and and this one you're gonna say it's pretty straightforward in terms of what happens",
    "start": "1114480",
    "end": "1119840"
  },
  {
    "text": "however we learned quite a lot from it and i want to share that with you",
    "start": "1119840",
    "end": "1126080"
  },
  {
    "text": "so the prompt started with user reporting connectivity issues to a cluster and when we checked api server they were",
    "start": "1126080",
    "end": "1132799"
  },
  {
    "text": "not doing very well so and they were not doing very well",
    "start": "1132799",
    "end": "1137840"
  },
  {
    "text": "because well they were restarting they couldn't reach hdd so the next step",
    "start": "1137840",
    "end": "1142960"
  },
  {
    "text": "was well let's look at lcd and well as you can see from this graph here memory",
    "start": "1142960",
    "end": "1148799"
  },
  {
    "text": "usage etsy wasn't doing very well either it was even getting",
    "start": "1148799",
    "end": "1154400"
  },
  {
    "text": "um killed which is not a great thing for et right",
    "start": "1154400",
    "end": "1159120"
  },
  {
    "start": "1159000",
    "end": "1159000"
  },
  {
    "text": "so what we know to start the investigation is well the cluster size is basically the same",
    "start": "1159520",
    "end": "1164880"
  },
  {
    "text": "as it was um a week before we haven't done any upgrades to the control plane",
    "start": "1164880",
    "end": "1170320"
  },
  {
    "text": "so it's very likely that something changed in the cluster and especially something that interacts with the api",
    "start": "1170320",
    "end": "1176840"
  },
  {
    "text": "server so let's look at the number of requests the api server are are serving and if we look at this graph",
    "start": "1176840",
    "end": "1183760"
  },
  {
    "start": "1178000",
    "end": "1178000"
  },
  {
    "text": "here we can see spikes of requests at the time of the incident and especially spike in list calls",
    "start": "1183760",
    "end": "1191840"
  },
  {
    "text": "we are we're now uh going to talk about list calls because list calls are very expensive so to",
    "start": "1191840",
    "end": "1198720"
  },
  {
    "text": "understand why they are very expensive we need to understand how api cyber caching works",
    "start": "1198720",
    "end": "1206880"
  },
  {
    "text": "so api server to make it very simple are big caches in front of that cd and when",
    "start": "1206880",
    "end": "1213120"
  },
  {
    "text": "they start they list resources and they start watching these resources to update the cache so",
    "start": "1213120",
    "end": "1219039"
  },
  {
    "text": "that's pretty simple right and then when client want to access api servers",
    "start": "1219039",
    "end": "1225200"
  },
  {
    "text": "they can do get all these calls and what is very important here as",
    "start": "1225200",
    "end": "1230880"
  },
  {
    "text": "you're going to see in the next few slides is when you get at least a resource",
    "start": "1230880",
    "end": "1236000"
  },
  {
    "text": "ideally you say which research version you want because every single",
    "start": "1236000",
    "end": "1241200"
  },
  {
    "text": "resource in kubernetes has a resource version and this allows you to get a consistent",
    "start": "1241200",
    "end": "1246640"
  },
  {
    "text": "version i mean whichever api server you connect to if you specify a resource version you're going to get the same answer",
    "start": "1246640",
    "end": "1253039"
  },
  {
    "text": "and also if you do an update you specify the resource version you want to update",
    "start": "1253039",
    "end": "1258320"
  },
  {
    "text": "which prevents conflicts when two processes want to update the same resource right because",
    "start": "1258320",
    "end": "1263919"
  },
  {
    "text": "if the resource has changed your update code will fail and you will get this error that you've probably already seen that says",
    "start": "1263919",
    "end": "1270320"
  },
  {
    "text": "resource version too old and then you have to retry and so if you do a get on list and set a",
    "start": "1270320",
    "end": "1277200"
  },
  {
    "text": "resource version to something you'll get this version from the cache of the api server there is a specific case where you set",
    "start": "1277200",
    "end": "1284000"
  },
  {
    "text": "resolution to zero which will give you the latest version from the cache",
    "start": "1284000",
    "end": "1289360"
  },
  {
    "text": "however what about when you're not setting a resource version",
    "start": "1289360",
    "end": "1295520"
  },
  {
    "text": "in that case api server completely bypass the cache and actually gets data directly from a",
    "start": "1295520",
    "end": "1302840"
  },
  {
    "text": "cd and the reason it does that is because since you're not specifying anything",
    "start": "1302840",
    "end": "1308159"
  },
  {
    "text": "you're getting the most consistent data you can get and what's important is this is the",
    "start": "1308159",
    "end": "1314480"
  },
  {
    "text": "behavior you you have when you use cube ctl get but it's also the default behavior you get when you",
    "start": "1314480",
    "end": "1321600"
  },
  {
    "text": "use client go the default library used to make use to make communities clients",
    "start": "1321600",
    "end": "1329200"
  },
  {
    "text": "it's going to be exactly the same so let's illustrate the difference",
    "start": "1329440",
    "end": "1336799"
  },
  {
    "start": "1333000",
    "end": "1333000"
  },
  {
    "text": "so as i was saying a cube ctl is only is never setting resource version",
    "start": "1336799",
    "end": "1342720"
  },
  {
    "text": "so we have to use curl and in this example i'm getting all the parts in the cluster",
    "start": "1342720",
    "end": "1348080"
  },
  {
    "text": "in the first example i'm not setting a resource version and it takes more than four seconds and in",
    "start": "1348080",
    "end": "1354000"
  },
  {
    "text": "the second case when i'm setting resource version equals zero it's much faster like all almost three times faster i did this",
    "start": "1354000",
    "end": "1361360"
  },
  {
    "text": "test on a pretty large cluster which explains why it's taking so long but also",
    "start": "1361360",
    "end": "1366640"
  },
  {
    "text": "i'm using table view i'm not getting the full json object just a summary version and i did that to",
    "start": "1366640",
    "end": "1372320"
  },
  {
    "text": "minimize transfer time because on this cluster the full json was about one gigabyte",
    "start": "1372320",
    "end": "1379280"
  },
  {
    "start": "1381000",
    "end": "1381000"
  },
  {
    "text": "something that i want to point out is sometimes you want to get a very specific resource and you use",
    "start": "1381120",
    "end": "1386320"
  },
  {
    "text": "label filters label selectors and in that case the the amount of",
    "start": "1386320",
    "end": "1391440"
  },
  {
    "text": "resources you're going to get from the api server is going to be very small a few parts and in this",
    "start": "1391440",
    "end": "1396640"
  },
  {
    "text": "example here i'm getting only pods from app a and in in that case it's uh",
    "start": "1396640",
    "end": "1404320"
  },
  {
    "text": "less than five pods and you can see something interesting here if i don't set resource version it's",
    "start": "1404320",
    "end": "1410240"
  },
  {
    "text": "still taking 3.6 seconds it's faster than before but not that much however",
    "start": "1410240",
    "end": "1416320"
  },
  {
    "text": "if i set resource version to zero it's extremely fast and the reason for this is when you're",
    "start": "1416320",
    "end": "1423120"
  },
  {
    "text": "not setting resource version all pods are still going to be retrieved from lcd",
    "start": "1423120",
    "end": "1428159"
  },
  {
    "text": "and filtering will happen on the api server whereas when you set a resource version the api",
    "start": "1428159",
    "end": "1434640"
  },
  {
    "text": "server with filter will filter locally from its cache and of course this is going to be extremely fast",
    "start": "1434640",
    "end": "1441039"
  },
  {
    "start": "1441000",
    "end": "1441000"
  },
  {
    "text": "if you if you've used a client go to build controllers um you've probably heard about informers",
    "start": "1442559",
    "end": "1450320"
  },
  {
    "text": "and informers are a way to have clients that are if much more efficient in the way they",
    "start": "1450320",
    "end": "1455760"
  },
  {
    "text": "talk to the api server because they maintain a cache of the resources they're interested in",
    "start": "1455760",
    "end": "1461760"
  },
  {
    "text": "and they receive events when they change and they're optimized to",
    "start": "1461760",
    "end": "1470400"
  },
  {
    "text": "to be very nice with the api server so what they do is when they start they do a list and",
    "start": "1470400",
    "end": "1475760"
  },
  {
    "text": "set resource version to zero so they get data from the cache and then they sell a watch based on the",
    "start": "1475760",
    "end": "1480880"
  },
  {
    "text": "resource version they get there are some edge cases and reconnections which can trigger",
    "start": "1480880",
    "end": "1486159"
  },
  {
    "text": "a list without resource version set which is bad as we said before because then the call will make it to a cd but note",
    "start": "1486159",
    "end": "1493200"
  },
  {
    "text": "that in kubernetes 120 um it should be much better because watch reconnection should avoid",
    "start": "1493200",
    "end": "1501200"
  },
  {
    "text": "these very expensive calls so as a summary remember that list calls go to lcd by",
    "start": "1501200",
    "end": "1508000"
  },
  {
    "text": "default and can have a huge impact on your cluster even if you use label filter and only",
    "start": "1508000",
    "end": "1513279"
  },
  {
    "text": "get a very small sets of of pods or notes for instance and the main message is avoid lists and",
    "start": "1513279",
    "end": "1520559"
  },
  {
    "text": "use informers something that i wanted to point out too is when you use cube ctl gets you",
    "start": "1520559",
    "end": "1528159"
  },
  {
    "text": "actually uses you actually use a list with resource version not set which is bad to it with that cd and i",
    "start": "1528159",
    "end": "1535200"
  },
  {
    "text": "believe that group scale could have an option that will that would allow you to set the result version to zero so you",
    "start": "1535200",
    "end": "1541919"
  },
  {
    "text": "get data from the cache so it would be much faster and make for a much better user experience",
    "start": "1541919",
    "end": "1547760"
  },
  {
    "text": "it would be much better for lcd and the control plane in general with a small trade-off because you will",
    "start": "1547760",
    "end": "1555120"
  },
  {
    "text": "you will have a small inconsistency window because uh the api server could be a bit behind",
    "start": "1555120",
    "end": "1560400"
  },
  {
    "text": "at city i really wanted to to thank uh voytek for",
    "start": "1560400",
    "end": "1565520"
  },
  {
    "text": "his help getting this right i'm sure a few precisions remained but it helps he helped a lot and thanks",
    "start": "1565520",
    "end": "1571279"
  },
  {
    "text": "again boydek let's go back to the incident so we know",
    "start": "1571279",
    "end": "1577200"
  },
  {
    "start": "1574000",
    "end": "1574000"
  },
  {
    "text": "the problem comes from lisco the next step is to identify which application is making these calls",
    "start": "1577200",
    "end": "1584159"
  },
  {
    "text": "so we're getting back to audit logs and as i was saying before audit logs are",
    "start": "1584159",
    "end": "1589279"
  },
  {
    "text": "very convenient because they can tell you what happened and when and who did it",
    "start": "1589279",
    "end": "1595440"
  },
  {
    "text": "but they also contain information about when the request started anyway it was finished so you can compute the query",
    "start": "1595440",
    "end": "1601440"
  },
  {
    "text": "time and in the graph here uh we're showing the accumulated query time",
    "start": "1601440",
    "end": "1607279"
  },
  {
    "text": "police called by user and as you can see here we have a single user",
    "start": "1607279",
    "end": "1612400"
  },
  {
    "text": "that is responsible for more than two days of query time over 20 minutes which if you do the math",
    "start": "1612400",
    "end": "1619440"
  },
  {
    "text": "is about 100 uh thread processing list calls at any given time during this window that's that's a lot",
    "start": "1619440",
    "end": "1627840"
  },
  {
    "text": "um this is the same view aggregated over a week in the table form and you can see here that this specific",
    "start": "1628480",
    "end": "1634799"
  },
  {
    "text": "user here is uh using about 30 of uh query time for fps ever that's",
    "start": "1634799",
    "end": "1641520"
  },
  {
    "text": "that's a lot so as you've made as you may have seen",
    "start": "1641520",
    "end": "1647760"
  },
  {
    "start": "1645000",
    "end": "1645000"
  },
  {
    "text": "on the previous slides uh the user responsible for this was called node group controller so let's",
    "start": "1647760",
    "end": "1653440"
  },
  {
    "text": "talk about nerd group controller it's an analysis controller we built to allow team to create pools of nodes",
    "start": "1653440",
    "end": "1661600"
  },
  {
    "text": "uh where they can specify the instance type they want to use for instance and we've used it extensively for two",
    "start": "1661600",
    "end": "1668159"
  },
  {
    "text": "years without any problem however we had just deployed a recent upgrade to provide deletion",
    "start": "1668159",
    "end": "1675440"
  },
  {
    "text": "protection and avoid deleting node groups that were running workloads",
    "start": "1675440",
    "end": "1680640"
  },
  {
    "text": "and so what uh this feature was doing is check each but were still running and refuse deletion if workloads were",
    "start": "1680640",
    "end": "1688799"
  },
  {
    "text": "still running in there technically this is implemented as an",
    "start": "1688799",
    "end": "1695440"
  },
  {
    "start": "1692000",
    "end": "1692000"
  },
  {
    "text": "admission controller uh and if you see another delete call",
    "start": "1695440",
    "end": "1700640"
  },
  {
    "text": "the controller would list all the nodes based on labels remember that even when when you do that",
    "start": "1700640",
    "end": "1707679"
  },
  {
    "text": "even if the number of nodes you get is small you're still going to hit lcd and get all the nodes",
    "start": "1707679",
    "end": "1714480"
  },
  {
    "text": "and what's important for the next uh step is that some groups are can can be a bit big",
    "start": "1714480",
    "end": "1719600"
  },
  {
    "text": "and then uh the next step is to list all the parts for these nodes but remember that getting all the parts",
    "start": "1719600",
    "end": "1726720"
  },
  {
    "text": "for this nodes also mean getting all the parts from lcd and filtering on the api server",
    "start": "1726720",
    "end": "1732799"
  },
  {
    "text": "to make things worse all these calls were made in parallel so that's a lot of calls making it",
    "start": "1732799",
    "end": "1739679"
  },
  {
    "text": "directly to lcd and of course this is what broke at cd and triggered the incident",
    "start": "1739679",
    "end": "1747279"
  },
  {
    "start": "1747000",
    "end": "1747000"
  },
  {
    "text": "so what we learned here is lease calls are very dangerous because the volume of",
    "start": "1747360",
    "end": "1752799"
  },
  {
    "text": "data can be extremely large and they will hit hcd most of the time",
    "start": "1752799",
    "end": "1758480"
  },
  {
    "text": "and so i said it before but i'm saying it again because it's an important message use informers whenever you can also",
    "start": "1758480",
    "end": "1766159"
  },
  {
    "text": "audit logs are extremely useful not only can they tell you who did what and when but they can also",
    "start": "1766159",
    "end": "1772480"
  },
  {
    "text": "tell you which user are responsible for query time which is very helpful when your api",
    "start": "1772480",
    "end": "1778240"
  },
  {
    "text": "server are overloaded because then you can know what is happening",
    "start": "1778240",
    "end": "1784080"
  },
  {
    "text": "so in conclusion i shared this incident because i found that they were a very good",
    "start": "1785919",
    "end": "1792640"
  },
  {
    "start": "1786000",
    "end": "1786000"
  },
  {
    "text": "occasion to learn a lot about the inner workings of kubernetes and that's",
    "start": "1792640",
    "end": "1797840"
  },
  {
    "text": "something i wanted to share as a very quick summary of what we learned today",
    "start": "1797840",
    "end": "1803840"
  },
  {
    "text": "api service extensions are extremely powerful but they can harm your cluster so be",
    "start": "1803840",
    "end": "1809600"
  },
  {
    "text": "very careful in how you monitor them and we tend to forget that nodes are not",
    "start": "1809600",
    "end": "1815600"
  },
  {
    "text": "abstract resources which is tempting especially when you run on cloud right you just create a vm",
    "start": "1815600",
    "end": "1821440"
  },
  {
    "text": "it works but yeah you will you can have kernel issues and level real issues",
    "start": "1821440",
    "end": "1827360"
  },
  {
    "text": "and especially for you running large clusters um you probably need to understand how",
    "start": "1827360",
    "end": "1833200"
  },
  {
    "text": "api server caching is working because it's very important because it controls the performance of",
    "start": "1833200",
    "end": "1839360"
  },
  {
    "text": "your cluster be extremely careful careful with how clients interact with the pi server",
    "start": "1839360",
    "end": "1845360"
  },
  {
    "text": "and avoid list calls i know i've said it a few times but it's important",
    "start": "1845360",
    "end": "1851840"
  },
  {
    "text": "where we're done for today if you're interested in in diving into kubernetes and and managing large clusters we're hiring",
    "start": "1851919",
    "end": "1859440"
  },
  {
    "text": "um don't they to to reach out to me either by email or twitter or",
    "start": "1859440",
    "end": "1864640"
  },
  {
    "text": "through the kundalini slack thank you very much",
    "start": "1864640",
    "end": "1871679"
  }
]