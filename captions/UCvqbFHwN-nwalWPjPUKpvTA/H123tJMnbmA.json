[
  {
    "text": "um my name is Marcel genba and I'm software engineer at Google and I'm also",
    "start": "2280",
    "end": "7440"
  },
  {
    "text": "one of the two chairs of sex scalability and today we'll be talking about sex",
    "start": "7440",
    "end": "12840"
  },
  {
    "text": "scalability introduction that also Deep dive so first question that we need to answer",
    "start": "12840",
    "end": "19260"
  },
  {
    "text": "is what we do as a sex scalability six scalability is a bit different from other six because we do not own",
    "start": "19260",
    "end": "26760"
  },
  {
    "text": "we do not own production code we only own test code that we use for testing",
    "start": "26760",
    "end": "33059"
  },
  {
    "text": "the scalability of kubernetes so basically we do have five areas",
    "start": "33059",
    "end": "38239"
  },
  {
    "text": "five areas that we are interested in in terms of",
    "start": "38239",
    "end": "43920"
  },
  {
    "text": "scalability so first of all the most important is actually defining what the scalability is and once we know what the",
    "start": "43920",
    "end": "51300"
  },
  {
    "text": "scalability means which will I which I will explain later and we also want to set up some goals so you can think of it",
    "start": "51300",
    "end": "58260"
  },
  {
    "text": "as you know we could start improving the kubernetes by making some improvements",
    "start": "58260",
    "end": "64440"
  },
  {
    "text": "but it doesn't matter if the end user is not able to see those",
    "start": "64440",
    "end": "70920"
  },
  {
    "text": "improvements and actually have some Better scalability Properties",
    "start": "70920",
    "end": "77360"
  },
  {
    "text": "except for defining and driving those goals we also",
    "start": "77360",
    "end": "82700"
  },
  {
    "text": "contribute to kubernetes in a way that we want to make kubernetes more scalable",
    "start": "82700",
    "end": "88740"
  },
  {
    "text": "so what we do is we usually coordinate between six or or ask other states to",
    "start": "88740",
    "end": "95700"
  },
  {
    "text": "actually make some improvements in order to make kubernetes more scalable",
    "start": "95700",
    "end": "102560"
  },
  {
    "text": "but okay let's say that we we do have those goals in mind for kubernetes that",
    "start": "106880",
    "end": "113159"
  },
  {
    "text": "we want to achieve but first of all we need to measure and monitor how the performance and scalability of",
    "start": "113159",
    "end": "119220"
  },
  {
    "text": "kubernetes actually changes over time um you probably know that every day",
    "start": "119220",
    "end": "124280"
  },
  {
    "text": "there are multiple PR's merged into kubernetes and each of these PRS",
    "start": "124280",
    "end": "130580"
  },
  {
    "text": "features these changes can potentially impact the performance of the whole kubernetes so that's why one of the most",
    "start": "130580",
    "end": "138180"
  },
  {
    "text": "important areas of six scalability is actually monitoring the performance of kubernetes",
    "start": "138180",
    "end": "144959"
  },
  {
    "text": "to make sure that we do not regress which is actually the next next Point",
    "start": "144959",
    "end": "150480"
  },
  {
    "text": "here so basically once we have this monitoring then we are able to track",
    "start": "150480",
    "end": "155700"
  },
  {
    "text": "different metrics and see if there is any regression and and last but not least we also",
    "start": "155700",
    "end": "163019"
  },
  {
    "text": "consult and Coach other six so probably most of you know that there are caps",
    "start": "163019",
    "end": "168780"
  },
  {
    "text": "which cap is kind of like design Dock and when you are when you want to implement new feature",
    "start": "168780",
    "end": "175560"
  },
  {
    "text": "there are some sections that are specific to six scalability and with",
    "start": "175560",
    "end": "181140"
  },
  {
    "text": "questions like how does your feature impact um control plane or what kind of calls",
    "start": "181140",
    "end": "187739"
  },
  {
    "text": "is making your uh making your feature that are additional to the previous state and so",
    "start": "187739",
    "end": "195780"
  },
  {
    "text": "one more important part is not to confuse wait a second oh not to confuse six",
    "start": "195780",
    "end": "203459"
  },
  {
    "text": "scalability with Sig Auto skating these are two different six",
    "start": "203459",
    "end": "209120"
  },
  {
    "text": "so what is the kubernetes scalability so sometimes we ask our users okay what do",
    "start": "210239",
    "end": "216959"
  },
  {
    "text": "they want and basically they say wait well they want scalable clusters but if",
    "start": "216959",
    "end": "222000"
  },
  {
    "text": "we ask them like okay but what does it mean well they usually don't really know",
    "start": "222000",
    "end": "228540"
  },
  {
    "text": "because scalability is not just a single number so I can give you one example",
    "start": "228540",
    "end": "234720"
  },
  {
    "text": "where in 2015 the kubernetes 1.0 was actually supporting 100 nodes and then",
    "start": "234720",
    "end": "242099"
  },
  {
    "text": "this number changed over a few releases to 1000 nodes two thousand five thousand",
    "start": "242099",
    "end": "248099"
  },
  {
    "text": "nodes and 5000 nodes was supported in 2017 in kubernetes 1.6",
    "start": "248099",
    "end": "256500"
  },
  {
    "text": "and this number did not change this number did not change since 2017.",
    "start": "256500",
    "end": "263820"
  },
  {
    "text": "so what I want to say is that the scalability of kubernetes is not just the number of nodes it's much more than",
    "start": "263820",
    "end": "270900"
  },
  {
    "text": "that so now I I just want to introduce this",
    "start": "270900",
    "end": "277979"
  },
  {
    "text": "concept of scalability envelope so if you think about kubernetes and the",
    "start": "277979",
    "end": "283320"
  },
  {
    "text": "scalability what you want to actually think of is multi-dimensional problem",
    "start": "283320",
    "end": "290400"
  },
  {
    "text": "so there are many many more Dimensions that you want to take into account when",
    "start": "290400",
    "end": "295919"
  },
  {
    "text": "thinking about the scalability of kubernetes so just to give you a few examples like number of nodes is just",
    "start": "295919",
    "end": "302400"
  },
  {
    "text": "one of the dimensions that we are interested in but then you'd have like number of Secrets or like how many pods",
    "start": "302400",
    "end": "309360"
  },
  {
    "text": "you can have per node Orchard for example",
    "start": "309360",
    "end": "314900"
  },
  {
    "text": "and now take into account all those Dimensions um we have scalability envelope so the",
    "start": "316320",
    "end": "323580"
  },
  {
    "text": "scalability envelope is safe zone so if you are within it your cluster is happy but we still don't know what does it",
    "start": "323580",
    "end": "330539"
  },
  {
    "text": "mean that the cluster is happy so you can think of it as like if you are",
    "start": "330539",
    "end": "336660"
  },
  {
    "text": "within those limits then something happens and now I will try to explain what",
    "start": "336660",
    "end": "342479"
  },
  {
    "text": "so what does the happiness of the cluster mean",
    "start": "342479",
    "end": "346880"
  },
  {
    "text": "so in order to think about cluster being happy we need to introduce two concepts",
    "start": "349979",
    "end": "356820"
  },
  {
    "text": "one is SLI which is service level indicator and SLO which is service level objective so",
    "start": "356820",
    "end": "364620"
  },
  {
    "text": "in simple words I think about SLI as a metric so let's say I have metric that",
    "start": "364620",
    "end": "371400"
  },
  {
    "text": "says pod startup latency is X second and",
    "start": "371400",
    "end": "377340"
  },
  {
    "text": "then on top of this metric what I can do is put some kind of threshold so for example I want to have put startup",
    "start": "377340",
    "end": "384960"
  },
  {
    "text": "latency like 99 percentile of put startup latency to be below specific",
    "start": "384960",
    "end": "390660"
  },
  {
    "text": "threshold so for example in kubernetes it would be five seconds",
    "start": "390660",
    "end": "395900"
  },
  {
    "text": "um so so basically the cluster is happy when",
    "start": "397740",
    "end": "402840"
  },
  {
    "text": "all the scalability slos are satisfied um so I I just want to give you a few",
    "start": "402840",
    "end": "410160"
  },
  {
    "text": "examples of scalability slos and this this list of scalability slos is",
    "start": "410160",
    "end": "416400"
  },
  {
    "text": "actually uh changing over time and we started with a very two simple slos a",
    "start": "416400",
    "end": "423780"
  },
  {
    "text": "few years ago which was API call latency and put startup latency you can find all",
    "start": "423780",
    "end": "429419"
  },
  {
    "text": "those slos and the description on our six scalability page but I wanted to",
    "start": "429419",
    "end": "436800"
  },
  {
    "text": "give you some kind of idea how well those slots are defined because",
    "start": "436800",
    "end": "443580"
  },
  {
    "text": "just saying that we care about the core latency is not enough and there is",
    "start": "443580",
    "end": "448919"
  },
  {
    "text": "actually way way more uh definition to that and",
    "start": "448919",
    "end": "456000"
  },
  {
    "text": "so let's start with the SLI SLI for the API call latency",
    "start": "456000",
    "end": "462000"
  },
  {
    "text": "so the slide for API call latency for mutating requests only is defined as you",
    "start": "462000",
    "end": "469199"
  },
  {
    "text": "take five minutes window and you choose one Resource One verb so let's say",
    "start": "469199",
    "end": "474979"
  },
  {
    "text": "deployments and patch and what you want to do is you measure 99th percentile",
    "start": "474979",
    "end": "480539"
  },
  {
    "text": "within this window um so you can think of it as like for each resource for each verb you will",
    "start": "480539",
    "end": "488280"
  },
  {
    "text": "have different metric and then what we do we say that the SLR is kind of",
    "start": "488280",
    "end": "495960"
  },
  {
    "text": "aggregated for each specific resource and verb for the whole day so in the",
    "start": "495960",
    "end": "502319"
  },
  {
    "text": "whole day you have 200 like 88 Windows of five minutes and then only",
    "start": "502319",
    "end": "509759"
  },
  {
    "text": "only two can like not met this this kind",
    "start": "509759",
    "end": "515099"
  },
  {
    "text": "of threshold which is one second so we want to have all the mutating API calls to be",
    "start": "515099",
    "end": "521039"
  },
  {
    "text": "within one second and if you have like 288 Windows of five minutes then basically basically you can only have",
    "start": "521039",
    "end": "528540"
  },
  {
    "text": "two windows that are not meeting this one second threshold so this is how the uh how the definition",
    "start": "528540",
    "end": "536100"
  },
  {
    "text": "of the API equal latency SLO looks like so",
    "start": "536100",
    "end": "543120"
  },
  {
    "text": "that's just one of the things that I showed before and once we have that and we have this",
    "start": "543120",
    "end": "549959"
  },
  {
    "text": "scalability envelope um what we can do is actually test",
    "start": "549959",
    "end": "556080"
  },
  {
    "text": "some limits because um Computing the whole scalability envelope",
    "start": "556080",
    "end": "562440"
  },
  {
    "text": "is impossible as you can see there are like multiple dimensions and you could",
    "start": "562440",
    "end": "568019"
  },
  {
    "text": "think of it as you know like I could have zero pods but like one million namespaces but is it useful it's it's",
    "start": "568019",
    "end": "574320"
  },
  {
    "text": "not really useful for for the user so what we want to do is kind of model it",
    "start": "574320",
    "end": "579660"
  },
  {
    "text": "how our users use kubernetes and let's say that okay we are interested in some",
    "start": "579660",
    "end": "586019"
  },
  {
    "text": "limits like number of nodes to be up to 5000 number of PODS like 30 pots per",
    "start": "586019",
    "end": "592620"
  },
  {
    "text": "node number of services to 10 000 and as a sixth probability what we do we",
    "start": "592620",
    "end": "599279"
  },
  {
    "text": "actually run those tests that maximize all those limits and we check for all the all the slos and make sure that they",
    "start": "599279",
    "end": "607440"
  },
  {
    "text": "are met and you can find all those thresholds that I mentioned like here we have three",
    "start": "607440",
    "end": "613920"
  },
  {
    "text": "examples but you can also find multiple other thresholds or limits like for pod",
    "start": "613920",
    "end": "620220"
  },
  {
    "text": "churn or number of Secrets and and so on",
    "start": "620220",
    "end": "625399"
  },
  {
    "text": "so now we will move toward towards um actually our infrastructure like what",
    "start": "635899",
    "end": "642839"
  },
  {
    "text": "kind of uh tools we use for making sure that the scalability envelope and all",
    "start": "642839",
    "end": "648720"
  },
  {
    "text": "those slos are are satisfied in kubernetes so the most important tool",
    "start": "648720",
    "end": "654660"
  },
  {
    "text": "that we use is cluster loader too so cluster Loader 2 2 is actually our our",
    "start": "654660",
    "end": "662279"
  },
  {
    "text": "internal um six collability tool used for for testing",
    "start": "662279",
    "end": "669240"
  },
  {
    "text": "testing scalability of kubernetes so what you can think of it is that you",
    "start": "669240",
    "end": "674339"
  },
  {
    "text": "provide some kind of states that you want your cluster to be in so let's say that you are starting with empty cluster",
    "start": "674339",
    "end": "680899"
  },
  {
    "text": "and then you want to make sure that the cluster is working when you have 1000",
    "start": "680899",
    "end": "686519"
  },
  {
    "text": "deployments and 100 000 parts for example so you are specifying this state",
    "start": "686519",
    "end": "692220"
  },
  {
    "text": "and then let's say you are also specifying how do you want to transition through from empty state of the cluster",
    "start": "692220",
    "end": "699899"
  },
  {
    "text": "to to this desired State and so you let's say you are saying that you want to create one deployment per one second",
    "start": "699899",
    "end": "707760"
  },
  {
    "text": "for example and cluster holder is actually doing that so cluster loader is",
    "start": "707760",
    "end": "713160"
  },
  {
    "text": "creating those those deployments as you as you wish and along the way the",
    "start": "713160",
    "end": "719160"
  },
  {
    "text": "cluster loader is also measuring all those slis and slos to make sure that",
    "start": "719160",
    "end": "724620"
  },
  {
    "text": "you know for example put startup latency did not exceed five seconds during the",
    "start": "724620",
    "end": "730079"
  },
  {
    "text": "whole test and there is also a bunch of a bunch of other",
    "start": "730079",
    "end": "735660"
  },
  {
    "text": "um other other features that you can use for for example for",
    "start": "735660",
    "end": "741420"
  },
  {
    "text": "debugging but I will mention a few a bit later",
    "start": "741420",
    "end": "747019"
  },
  {
    "text": "also you can imagine that if you want to test 5000 nodes",
    "start": "751140",
    "end": "756839"
  },
  {
    "text": "using real clusters is pretty expensive even if it's like you know one CPU per",
    "start": "756839",
    "end": "764459"
  },
  {
    "text": "per node then it's still 5000 CPUs so running those tests it's it's it's",
    "start": "764459",
    "end": "769620"
  },
  {
    "text": "pretty expensive and as a six capability we cannot run them like super often so",
    "start": "769620",
    "end": "777019"
  },
  {
    "text": "5000 nodes tests are on only one once a day but we also do have Cube Mark and",
    "start": "777019",
    "end": "783540"
  },
  {
    "text": "Cube Mark it's something that we call as a cluster simulation um",
    "start": "783540",
    "end": "789000"
  },
  {
    "text": "so basically the idea is that we have cubemark master and what we are",
    "start": "789000",
    "end": "796500"
  },
  {
    "text": "trying to do is actually test this this master and [Music]",
    "start": "796500",
    "end": "802079"
  },
  {
    "text": "we don't want to use 5000 nodes so what do we do is we actually create let's say 80 or 100 nodes and on those nodes what",
    "start": "802079",
    "end": "810540"
  },
  {
    "text": "we do is we actually run something that simulates Hollow nodes so what we do is",
    "start": "810540",
    "end": "817139"
  },
  {
    "text": "we Run Hollow nodes and those whole nodes simulate the traffic that the",
    "start": "817139",
    "end": "823079"
  },
  {
    "text": "normal control plane would would see and with that Improvement uh basically",
    "start": "823079",
    "end": "828779"
  },
  {
    "text": "instead of having 5000 nodes we can usually do with like 80 nodes and each",
    "start": "828779",
    "end": "835200"
  },
  {
    "text": "let's say four or eight CPUs which brings down the cost significantly",
    "start": "835200",
    "end": "840959"
  },
  {
    "text": "down but of course the issue is here like Okay so let's say that we want to have",
    "start": "840959",
    "end": "846480"
  },
  {
    "text": "this kind of setup where we have three actual nodes and here 15 whole notes so",
    "start": "846480",
    "end": "853440"
  },
  {
    "text": "how do you actually run those Hollow nodes so this whole notes this whole notes actually need to be",
    "start": "853440",
    "end": "860399"
  },
  {
    "text": "somehow scheduled onto the real nodes so this is actually pretty funny because we",
    "start": "860399",
    "end": "865800"
  },
  {
    "text": "use also kubernetes to to do that so there are actually two clusters one",
    "start": "865800",
    "end": "871800"
  },
  {
    "text": "cluster that is responsible for uh for running those Hollow nodes on actual",
    "start": "871800",
    "end": "877019"
  },
  {
    "text": "nodes and those whole nodes are connected to the separate master that we want to scale test and because of that",
    "start": "877019",
    "end": "883680"
  },
  {
    "text": "Improvement we are able to actually run scalability tests much more often because it does not consume as much",
    "start": "883680",
    "end": "890399"
  },
  {
    "text": "resources as like regular clusters",
    "start": "890399",
    "end": "896959"
  },
  {
    "text": "except for that we have a bunch of monitoring and observability tools and",
    "start": "899760",
    "end": "905600"
  },
  {
    "text": "perfdash is one of our favorite tools it looks super simple it's super simple but",
    "start": "905600",
    "end": "911579"
  },
  {
    "text": "it allows us to find multiple regressions um because you know like I mentioned",
    "start": "911579",
    "end": "917639"
  },
  {
    "text": "before that um you know we have slos we have some tests but there are sometimes",
    "start": "917639",
    "end": "924420"
  },
  {
    "text": "regressions that are not found by just as a loss let's say that",
    "start": "924420",
    "end": "930560"
  },
  {
    "text": "the pot startup latency actually this is the startup latency example and there",
    "start": "930560",
    "end": "938160"
  },
  {
    "text": "was some regression some time ago and we only saw that the number of that the post startup latency increased",
    "start": "938160",
    "end": "946440"
  },
  {
    "text": "by like 400 milliseconds and once we found it we fixed it and basically here",
    "start": "946440",
    "end": "951839"
  },
  {
    "text": "you can see that we bring brought down the latency of pod startup latency by 400 milliseconds but our tests were not",
    "start": "951839",
    "end": "958920"
  },
  {
    "text": "able to actually find it because the SLO says that you know the 99th percentile",
    "start": "958920",
    "end": "964500"
  },
  {
    "text": "needs to stay below five seconds which was still still true but we still saw",
    "start": "964500",
    "end": "970980"
  },
  {
    "text": "the regression due to the curve Dash and and all those cool charts here",
    "start": "970980",
    "end": "978320"
  },
  {
    "text": "except for that if you want to also see um one particular run of our",
    "start": "980160",
    "end": "989660"
  },
  {
    "text": "scalability test then you can use our grafana dashboards and they are super",
    "start": "989660",
    "end": "995100"
  },
  {
    "text": "useful and you can just use them",
    "start": "995100",
    "end": "999560"
  },
  {
    "text": "for for debugging scalability issues and also like when you're running",
    "start": "1000320",
    "end": "1006139"
  },
  {
    "text": "cluster loader cluster Loader 2 out of box you get Much",
    "start": "1006139",
    "end": "1011600"
  },
  {
    "text": "More Much More observability also in terms of profiling",
    "start": "1011600",
    "end": "1018079"
  },
  {
    "text": "so when running the test we are also Gathering memory and CPU profiling and",
    "start": "1018079",
    "end": "1024140"
  },
  {
    "text": "so for example then later you can see that okay during the load test there were some parts where",
    "start": "1024140",
    "end": "1031938"
  },
  {
    "text": "the control plane CPU was burning quite heavily and you can just grab the profiling and see what kind of parts of",
    "start": "1031939",
    "end": "1038720"
  },
  {
    "text": "the kubernetes control plane was actually consuming the CPU",
    "start": "1038720",
    "end": "1044558"
  },
  {
    "text": "so now we move to the scalability test so I showed you all those tools that we",
    "start": "1056120",
    "end": "1061520"
  },
  {
    "text": "use for monitoring and testing the scalability of kubernetes but now what kind of tests we are",
    "start": "1061520",
    "end": "1068299"
  },
  {
    "text": "actually running to to ensure that the kubernetes is scalable so there are two types of periodic tests",
    "start": "1068299",
    "end": "1077000"
  },
  {
    "text": "that we run one are release blocking tests and restocking tests that are",
    "start": "1077000",
    "end": "1084140"
  },
  {
    "text": "actually performance tests for 100 nodes 5000 nodes and these are actually run on",
    "start": "1084140",
    "end": "1090799"
  },
  {
    "text": "real clusters and and also correctness 5000 notes so the",
    "start": "1090799",
    "end": "1097039"
  },
  {
    "text": "performance is purely about the performance like how fast the pods startup latency is or like what kind of",
    "start": "1097039",
    "end": "1103760"
  },
  {
    "text": "latencies do we have on the control plane and correctness is more about that the features that kubernetes is",
    "start": "1103760",
    "end": "1110660"
  },
  {
    "text": "providing still work when you have like 5000 nodes in your cluster except for that we we do",
    "start": "1110660",
    "end": "1118280"
  },
  {
    "text": "have non-release blocking tests and they are super informative so one of the examples one of my favorite examples is",
    "start": "1118280",
    "end": "1126020"
  },
  {
    "text": "actually a benchmarking for going so in the past over last I think three or four",
    "start": "1126020",
    "end": "1132140"
  },
  {
    "text": "years we've noticed that there were multiple regressions coming from from",
    "start": "1132140",
    "end": "1137299"
  },
  {
    "text": "the golang and so what we did was actually we froze all the dependencies except for the Golan compiler",
    "start": "1137299",
    "end": "1144260"
  },
  {
    "text": "and what we do is we just have Frozen kubernetes version and we run our",
    "start": "1144260",
    "end": "1150260"
  },
  {
    "text": "cluster loader performance test and we just change the go compiler version and",
    "start": "1150260",
    "end": "1157820"
  },
  {
    "text": "based on that we noticed if there are some regressions in going compiler and",
    "start": "1157820",
    "end": "1163280"
  },
  {
    "text": "we just inform the golang team that okay we noticed that there is regression between like for example those two",
    "start": "1163280",
    "end": "1168799"
  },
  {
    "text": "comets and they are able to easily detect those regressions as well because kubernetes is actually one of the",
    "start": "1168799",
    "end": "1175039"
  },
  {
    "text": "actually the biggest project I think in in going so it's super useful for them",
    "start": "1175039",
    "end": "1180320"
  },
  {
    "text": "as well to validate if they are not introducing any regressions",
    "start": "1180320",
    "end": "1186080"
  },
  {
    "text": "but except for that we have also the the cube Mark some some bunch of storage tests and so on",
    "start": "1186080",
    "end": "1194200"
  },
  {
    "text": "there is one more thing I wanted to mention so if you are contributing to kubernetes then whenever you open PR",
    "start": "1198260",
    "end": "1204679"
  },
  {
    "text": "there is a bunch of pre-submits and one of the pre subnets is actually uh the",
    "start": "1204679",
    "end": "1211039"
  },
  {
    "text": "performance uh 100 nodes test test so whenever you open PR there's actually",
    "start": "1211039",
    "end": "1217220"
  },
  {
    "text": "job running that creates 100 nodes cluster and it's running the load test",
    "start": "1217220",
    "end": "1224179"
  },
  {
    "text": "to check if there are no any obvious regressions in performance",
    "start": "1224179",
    "end": "1230600"
  },
  {
    "text": "and that's basically it in terms of scalability tests how we protect the scalability of",
    "start": "1230600",
    "end": "1237679"
  },
  {
    "text": "kubernetes so I mentioned those CI tests and you",
    "start": "1237679",
    "end": "1243980"
  },
  {
    "text": "can see the our test grid what's happening basically is like we",
    "start": "1243980",
    "end": "1250340"
  },
  {
    "text": "have those tests running for for the master branch and also we do have for",
    "start": "1250340",
    "end": "1256820"
  },
  {
    "text": "all releases that will be happening for example 124 or 125 the releases are",
    "start": "1256820",
    "end": "1264140"
  },
  {
    "text": "still happening so we are still running those tests and and this test grid is actually available to everyone if you",
    "start": "1264140",
    "end": "1269720"
  },
  {
    "text": "want to see you can just go to to our test grid and see in what state the",
    "start": "1269720",
    "end": "1275900"
  },
  {
    "text": "current kubernetes scalability is",
    "start": "1275900",
    "end": "1281200"
  },
  {
    "text": "so I mentioned few few of the standard regressions that we already saw but",
    "start": "1283760",
    "end": "1291260"
  },
  {
    "text": "scalability is super sensitive One Small Change can break the whole scalability",
    "start": "1291260",
    "end": "1296659"
  },
  {
    "text": "of kubernetes and we've seen regressions pretty much from everywhere like going I",
    "start": "1296659",
    "end": "1304159"
  },
  {
    "text": "mentioned my favorite but then also operating system or controllers API Machinery scheduler at CD cublet you",
    "start": "1304159",
    "end": "1312020"
  },
  {
    "text": "name it it's it's basically all um so what what's happening is that usually once we observe that there is",
    "start": "1312020",
    "end": "1319400"
  },
  {
    "text": "some some regression we kind of try to narrow down where this regression",
    "start": "1319400",
    "end": "1324740"
  },
  {
    "text": "happened and usually contact out or or even revert the whole feature if if we",
    "start": "1324740",
    "end": "1330559"
  },
  {
    "text": "think that um the scalability is not as good as it was before",
    "start": "1330559",
    "end": "1337658"
  },
  {
    "text": "so there are a few actually um pretty interesting regressions that we solved quite recently",
    "start": "1341120",
    "end": "1349940"
  },
  {
    "text": "um to name it to name one example so for example there was",
    "start": "1349940",
    "end": "1357020"
  },
  {
    "text": "um recently introduced recently priority and fairness and probably you",
    "start": "1357020",
    "end": "1362720"
  },
  {
    "text": "heard about it and um some of the calls in priority and fairness were incorrectly estimated and",
    "start": "1362720",
    "end": "1369860"
  },
  {
    "text": "because they were incorrectly estimated they were consuming way more uh we call",
    "start": "1369860",
    "end": "1375860"
  },
  {
    "text": "it seats and what was happening was that in some of the setups that our customers",
    "start": "1375860",
    "end": "1381559"
  },
  {
    "text": "use the API call latency was significantly higher than it's supposed to be so",
    "start": "1381559",
    "end": "1389559"
  },
  {
    "text": "that's something that for example we recently detected and fixed",
    "start": "1389559",
    "end": "1394820"
  },
  {
    "text": "um last few versions",
    "start": "1394820",
    "end": "1399460"
  },
  {
    "text": "and there is the also interesting part where we actually",
    "start": "1405740",
    "end": "1410840"
  },
  {
    "text": "drive some improvements um so again like going back to going uh",
    "start": "1410840",
    "end": "1416780"
  },
  {
    "text": "it's not like going was only introducing uh regressions in kubernetes",
    "start": "1416780",
    "end": "1422200"
  },
  {
    "text": "migration to going one eight one eighteen uh helped help kubernetes",
    "start": "1422200",
    "end": "1428780"
  },
  {
    "text": "performance quite significantly but maybe you also heard that the memory footprint also increased significantly",
    "start": "1428780",
    "end": "1436400"
  },
  {
    "text": "so this is one of the examples where actually the Improvement was was quite",
    "start": "1436400",
    "end": "1442760"
  },
  {
    "text": "huge uh 99 percentile of API call latencies was basically like 10 time",
    "start": "1442760",
    "end": "1449500"
  },
  {
    "text": "smaller than it was before so it was quite huge Improvement but there are",
    "start": "1449500",
    "end": "1455419"
  },
  {
    "text": "also some other some other improvements that we made like page size progression",
    "start": "1455419",
    "end": "1461480"
  },
  {
    "text": "for selectors so what you can do is you can list some resources from the API",
    "start": "1461480",
    "end": "1467539"
  },
  {
    "text": "server and you can provide limit so let's say that you provide the limit of one but you also have some field",
    "start": "1467539",
    "end": "1474620"
  },
  {
    "text": "selector so what was happening was that whenever whenever you were making such",
    "start": "1474620",
    "end": "1480380"
  },
  {
    "text": "call there the elements from the hcd were fetched one by one so if you have like",
    "start": "1480380",
    "end": "1487280"
  },
  {
    "text": "very rare selector and let's say you have 1000 items and only the last one",
    "start": "1487280",
    "end": "1494000"
  },
  {
    "text": "matches the selector what was happening was that there were actually there were",
    "start": "1494000",
    "end": "1500000"
  },
  {
    "text": "actually 1 000 calls to to add CD made and but we fixed it with a progression",
    "start": "1500000",
    "end": "1506960"
  },
  {
    "text": "of the limit so the limit that you use for the API calls to API server is no longer bound to the",
    "start": "1506960",
    "end": "1514280"
  },
  {
    "text": "Limit that we used for fetching the results from edcd",
    "start": "1514280",
    "end": "1520179"
  },
  {
    "text": "so um now to sum up like if you want to get involved",
    "start": "1522940",
    "end": "1528380"
  },
  {
    "text": "um we have home page where you can find all the contact information if you are interested in scalability we do have a",
    "start": "1528380",
    "end": "1535279"
  },
  {
    "text": "bi-weekly public meetings uh we do have also a mailing list and if you want to",
    "start": "1535279",
    "end": "1541460"
  },
  {
    "text": "get involved in for example our testing infrastructure we do have some issues",
    "start": "1541460",
    "end": "1547760"
  },
  {
    "text": "that are marked as help wanted so you can just pick up one and and get",
    "start": "1547760",
    "end": "1554299"
  },
  {
    "text": "involved in scalability of kubernetes and now we have um I think a few minutes for Q a",
    "start": "1554299",
    "end": "1564940"
  },
  {
    "text": "do we have microphone baby okay",
    "start": "1576620",
    "end": "1582399"
  },
  {
    "text": "[Music]",
    "start": "1584750",
    "end": "1588009"
  },
  {
    "text": "thank you hi my question is basically is it the scale",
    "start": "1596659",
    "end": "1603080"
  },
  {
    "text": "we recommend to which kubernetes operators can go the one",
    "start": "1603080",
    "end": "1610520"
  },
  {
    "text": "which you are testing so if you they go beyond that it's their own risks of say",
    "start": "1610520",
    "end": "1618260"
  },
  {
    "text": "that's that's a great question so um actually um what's happening is that there are",
    "start": "1618260",
    "end": "1624679"
  },
  {
    "text": "limits as I mentioned that we are testing like for example 5000 nodes or",
    "start": "1624679",
    "end": "1629779"
  },
  {
    "text": "10 000 services and if you go beyond that what's usually happening the performance",
    "start": "1629779",
    "end": "1636799"
  },
  {
    "text": "is the grading kind of like gracefully and in a way that you know the latencies start increasing and everything starts",
    "start": "1636799",
    "end": "1644360"
  },
  {
    "text": "to slow down but it's usually not like it just breaks but and that's right that usually it's it's uh the cluster",
    "start": "1644360",
    "end": "1652120"
  },
  {
    "text": "operator who is responsible for that that needs to monitor it and make sure",
    "start": "1652120",
    "end": "1658100"
  },
  {
    "text": "that those limits um are like the cluster is within those limits but there is also one more thing",
    "start": "1658100",
    "end": "1664880"
  },
  {
    "text": "that I want to mention that um to make sure that all those slos are met",
    "start": "1664880",
    "end": "1670640"
  },
  {
    "text": "there are there is also a bunch of requirements that your cluster needs to uh to to be following for example one of",
    "start": "1670640",
    "end": "1678080"
  },
  {
    "text": "the requirements here we have that you know the control plane um you have to have 64 cars for example",
    "start": "1678080",
    "end": "1685640"
  },
  {
    "text": "or you have to have two instances of hcd and so on so basically there is the",
    "start": "1685640",
    "end": "1691700"
  },
  {
    "text": "bunch of list of requirements that you need to also have implemented in your cluster to to",
    "start": "1691700",
    "end": "1698720"
  },
  {
    "text": "make it scalable to those limits but it's all documented there basically",
    "start": "1698720",
    "end": "1705399"
  },
  {
    "text": "thank you so a follow-up to the question uh so is that documented what happens",
    "start": "1710720",
    "end": "1716240"
  },
  {
    "text": "when those latencies increase Beyond five thousand and and then it degrades",
    "start": "1716240",
    "end": "1721880"
  },
  {
    "text": "what happens and is it able to recover um when when you",
    "start": "1721880",
    "end": "1728120"
  },
  {
    "text": "reduce the load so um there is now document like that",
    "start": "1728120",
    "end": "1733880"
  },
  {
    "text": "and the reason is that it really depends like we are sure that you know if you have vanilla set up of kubernetes and",
    "start": "1733880",
    "end": "1740900"
  },
  {
    "text": "you don't have um super fancy controllers then it will work right but once you start using some",
    "start": "1740900",
    "end": "1748220"
  },
  {
    "text": "of the components I've seen in the past that there are components that actually do not gracefully kind of like stop",
    "start": "1748220",
    "end": "1756080"
  },
  {
    "text": "working but they just break the whole cluster at some point like if you if you reach the limit of whatever let's say",
    "start": "1756080",
    "end": "1764360"
  },
  {
    "text": "secrets and then if you have 20 000 Secrets it",
    "start": "1764360",
    "end": "1769760"
  },
  {
    "text": "will just stop working for example and there are components like that that cluster operators sometimes install so",
    "start": "1769760",
    "end": "1775520"
  },
  {
    "text": "there's no really simple answer here because there are so many components you can install in kubernetes that it always",
    "start": "1775520",
    "end": "1781340"
  },
  {
    "text": "depends on what kind of components do you have uh the second question I had was what is",
    "start": "1781340",
    "end": "1788480"
  },
  {
    "text": "the significance of that 100 node test that you talked about so this is kind of like",
    "start": "1788480",
    "end": "1794360"
  },
  {
    "text": "um our first first test that each change is tested on",
    "start": "1794360",
    "end": "1801140"
  },
  {
    "text": "um because you know in a perfect world what we would be doing is like whenever someone makes PR to kubernetes we would",
    "start": "1801140",
    "end": "1808640"
  },
  {
    "text": "run 5000 nodes test that tests everything but that's super expensive so",
    "start": "1808640",
    "end": "1813919"
  },
  {
    "text": "what we are doing is basically we are running this 100 nodes test instead which is much cheaper and we can do that",
    "start": "1813919",
    "end": "1821000"
  },
  {
    "text": "got it but why 100 I mean 100 is like can you just Mike yeah just last",
    "start": "1821000",
    "end": "1828320"
  },
  {
    "text": "question I mean why 100 like 100 is a two less of a number it's just because of cost you're saying or uh so it's",
    "start": "1828320",
    "end": "1834679"
  },
  {
    "text": "actually both cost but also it also detects um it also detects like very basic like",
    "start": "1834679",
    "end": "1842779"
  },
  {
    "text": "regressions like significant regressions so you know it's it's uh it actually",
    "start": "1842779",
    "end": "1849140"
  },
  {
    "text": "saves us quite a lot of time to to debug it because once it's past and merged to",
    "start": "1849140",
    "end": "1855799"
  },
  {
    "text": "the master branch of kubernetes then if we observe regression on larger scale",
    "start": "1855799",
    "end": "1861740"
  },
  {
    "text": "then it actually involves like human person to to to go through and see what",
    "start": "1861740",
    "end": "1867980"
  },
  {
    "text": "kind of changes were made and what was regressed so this is kind of like you know first",
    "start": "1867980",
    "end": "1873740"
  },
  {
    "text": "first test to reduce the number of noise",
    "start": "1873740",
    "end": "1878559"
  },
  {
    "text": "for uh for actually for what it's worth we did have a uh one of our clusters that went above like 5 000 nodes in the",
    "start": "1878779",
    "end": "1885919"
  },
  {
    "text": "control plane just broke it never recovered uh but that's actually a great",
    "start": "1885919",
    "end": "1891260"
  },
  {
    "text": "segment of my question you had a slide on profiling for a cube API server do you need to have access to the control",
    "start": "1891260",
    "end": "1897320"
  },
  {
    "text": "plane because there's a lot of cloud providers that don't give you that access that's that's true yeah so",
    "start": "1897320",
    "end": "1903860"
  },
  {
    "text": "um in open source we are actually testing you know um like fully on our VMS we are not",
    "start": "1903860",
    "end": "1911539"
  },
  {
    "text": "testing you know eks or gke uh we are only testing uh kubernetes",
    "start": "1911539",
    "end": "1918500"
  },
  {
    "text": "as you know the open source version basically yeah so there we have it",
    "start": "1918500",
    "end": "1926620"
  },
  {
    "text": "and just two questions the first one is that um for the we are developing some",
    "start": "1931299",
    "end": "1937340"
  },
  {
    "text": "platform and then part of the kubernetes ecosystem so what is is there any guideline to you know test those",
    "start": "1937340",
    "end": "1944299"
  },
  {
    "text": "operators like custom resource yes so that's a great question",
    "start": "1944299",
    "end": "1950360"
  },
  {
    "text": "um currently there is no guidance for it but um this is this this topic is actually",
    "start": "1950360",
    "end": "1958460"
  },
  {
    "text": "um coming up more and more often so probably we will need to invest some time into some guidance how to write",
    "start": "1958460",
    "end": "1965059"
  },
  {
    "text": "controllers and operators in a way that they do not put too much load on the",
    "start": "1965059",
    "end": "1970520"
  },
  {
    "text": "control plane um but from experience I would say that",
    "start": "1970520",
    "end": "1975740"
  },
  {
    "text": "usually the biggest issues are with the agents that are running on all the nodes",
    "start": "1975740",
    "end": "1981440"
  },
  {
    "text": "and then there is a chance that if you have some kind of specific bug then you put too much load on the control plane",
    "start": "1981440",
    "end": "1988159"
  },
  {
    "text": "and you can break basically control plane okay yeah",
    "start": "1988159",
    "end": "1994580"
  },
  {
    "text": "yeah that was your question or sort of you know we're trying to see if I",
    "start": "1994580",
    "end": "2000220"
  },
  {
    "text": "evaluate the performance and scalability of the controller in the kubernetes",
    "start": "2000220",
    "end": "2006519"
  },
  {
    "text": "ecosystem maybe not exactly the kubernetes cluster per se but controller",
    "start": "2006519",
    "end": "2012399"
  },
  {
    "text": "is part of the you know you talk the API server and they schedule an LCD all of them loads it there and then in this",
    "start": "2012399",
    "end": "2017559"
  },
  {
    "text": "kind of context um yeah I've been trying to still trying to figure out what's the best way to",
    "start": "2017559",
    "end": "2023260"
  },
  {
    "text": "evaluate this as controllers or the platform boys",
    "start": "2023260",
    "end": "2028299"
  },
  {
    "text": "but I think that's um yeah that's fine I think that we can skip the second question is that in the",
    "start": "2028299",
    "end": "2033880"
  },
  {
    "text": "cube Mark there's no part the Creator right it's like a hollow node as you mentioned right",
    "start": "2033880",
    "end": "2039340"
  },
  {
    "text": "and so in that sense um so this part of creation time or something if we want to evaluate that",
    "start": "2039340",
    "end": "2044860"
  },
  {
    "text": "then Cube Mark is there any other tools and tooling you know that can",
    "start": "2044860",
    "end": "2050080"
  },
  {
    "text": "um evaluate this um part of the actual part really yeah so",
    "start": "2050080",
    "end": "2055898"
  },
  {
    "text": "um not really so I think uh I think like yeah in Cube Mark basically we are not",
    "start": "2055899",
    "end": "2061599"
  },
  {
    "text": "running those spots so it's just simulated um but I don't think there exists",
    "start": "2061599",
    "end": "2067118"
  },
  {
    "text": "anything that you know like if you start running those spots and containers then basically you're you're close to having",
    "start": "2067119",
    "end": "2073960"
  },
  {
    "text": "like real clustering I think yeah okay",
    "start": "2073960",
    "end": "2078898"
  },
  {
    "text": "okay we have time for last question I think",
    "start": "2079300",
    "end": "2084299"
  },
  {
    "text": "foreign thank you I think on one of the your",
    "start": "2086020",
    "end": "2092740"
  },
  {
    "text": "slides you list like a three regression issues one of the three I think if I",
    "start": "2092740",
    "end": "2099940"
  },
  {
    "text": "remember right is about part startup latency so did did that",
    "start": "2099940",
    "end": "2106720"
  },
  {
    "text": "[Music] was that issue ever fixed oh yeah actually actually I can give a bit more",
    "start": "2106720",
    "end": "2113560"
  },
  {
    "text": "context there so so there was there was some feature",
    "start": "2113560",
    "end": "2118960"
  },
  {
    "text": "developed um in kubernetes and by accident what happened was that for each",
    "start": "2118960",
    "end": "2125740"
  },
  {
    "text": "request that was made to API server there were two Guardians created and you know by itself like",
    "start": "2125740",
    "end": "2132520"
  },
  {
    "text": "it shouldn't have huge impact but we observed because you know in our",
    "start": "2132520",
    "end": "2137619"
  },
  {
    "text": "scalability test we have hundreds of thousands of bird teams in the API server and because we doubled that what",
    "start": "2137619",
    "end": "2145060"
  },
  {
    "text": "was happening that the going scheduler was already kind of slowing down and",
    "start": "2145060",
    "end": "2150700"
  },
  {
    "text": "because of that we saw the regression input startup latency but yeah then basically we just reverted",
    "start": "2150700",
    "end": "2158079"
  },
  {
    "text": "this PR um because we decided that you know we need to fix it because the regression",
    "start": "2158079",
    "end": "2164619"
  },
  {
    "text": "was quite significant uh and then um outdoor had to actually implement it",
    "start": "2164619",
    "end": "2170500"
  },
  {
    "text": "in a way that we were not increasing the number of go routines in the API server",
    "start": "2170500",
    "end": "2177480"
  },
  {
    "text": "okay I don't know can we one more question",
    "start": "2181300",
    "end": "2188140"
  },
  {
    "text": "foreign just a quick one so basically all of the",
    "start": "2188140",
    "end": "2193260"
  },
  {
    "text": "examples and evil that you gave um were were mostly about uh reactive",
    "start": "2193260",
    "end": "2200920"
  },
  {
    "text": "um reactive action to to changes in kubernetes so basically we're doing a",
    "start": "2200920",
    "end": "2206320"
  },
  {
    "text": "lot of taxing and catching the scalability issues and fixing them when they get merged but how",
    "start": "2206320",
    "end": "2213400"
  },
  {
    "text": "about the trying to improve the actual scalability of the project with the",
    "start": "2213400",
    "end": "2218740"
  },
  {
    "text": "Baseline that we have so for example you had that five seconds deadline there is the sake working on trying to get those",
    "start": "2218740",
    "end": "2225520"
  },
  {
    "text": "thresholds lowered actually doing profiling of what's taking up those three seconds that you are considering",
    "start": "2225520",
    "end": "2231760"
  },
  {
    "text": "like the normal Baseline now or is that something that's uh completely out because you don't actually own the",
    "start": "2231760",
    "end": "2238119"
  },
  {
    "text": "production code um so I think so how we usually operate",
    "start": "2238119",
    "end": "2244240"
  },
  {
    "text": "is we we really want to set our goals based on our customers expectations so",
    "start": "2244240",
    "end": "2249460"
  },
  {
    "text": "it's not just you know in improving based on you know like just",
    "start": "2249460",
    "end": "2254680"
  },
  {
    "text": "make just like possibility of improving so uh there are some some metrics that",
    "start": "2254680",
    "end": "2260440"
  },
  {
    "text": "our users care about and there are some metrics that user don't they do not care about but we still measure them in order",
    "start": "2260440",
    "end": "2267040"
  },
  {
    "text": "to catch regressions um but there were some",
    "start": "2267040",
    "end": "2272740"
  },
  {
    "text": "some improvements that actually we but they still came from like our users so",
    "start": "2272740",
    "end": "2279160"
  },
  {
    "text": "what we were seeing for example is that people use quite a lot of Secrets and",
    "start": "2279160",
    "end": "2285099"
  },
  {
    "text": "what's happening is that when you use secret and you mount the secret to pod uh this pot or kublet actually opens",
    "start": "2285099",
    "end": "2292800"
  },
  {
    "text": "opens opens watch and we introduced immutable secrets to kind of resolve",
    "start": "2292800",
    "end": "2299859"
  },
  {
    "text": "this issue to reduce the number of watches and increase scalability but I think we need to actually finish right",
    "start": "2299859",
    "end": "2305500"
  },
  {
    "text": "now but it will be still still here so if you want to talk then",
    "start": "2305500",
    "end": "2311520"
  },
  {
    "text": "feel free thank you [Applause]",
    "start": "2311740",
    "end": "2317578"
  }
]