[
  {
    "start": "0",
    "end": "17000"
  },
  {
    "text": "so welcome to this session this is a deep dive session on running kubernetes",
    "start": "1689",
    "end": "7690"
  },
  {
    "text": "on the vSphere hypervisor my name is Steve Wong I'm an engineer with VMware",
    "start": "7690",
    "end": "13780"
  },
  {
    "text": "based in Los Angeles California I've been working on kubernetes since 2015 I got started working on storage",
    "start": "13780",
    "end": "21760"
  },
  {
    "start": "17000",
    "end": "87000"
  },
  {
    "text": "but recently I've been working on vSphere related things and I'll let my",
    "start": "21760",
    "end": "28539"
  },
  {
    "text": "partner introduce himself",
    "start": "28539",
    "end": "31770"
  },
  {
    "text": "oh I think it's not on no and also",
    "start": "47670",
    "end": "59739"
  },
  {
    "text": "contributor to Corrado is fear and crossed API vSphere our github IDs are",
    "start": "59739",
    "end": "68080"
  },
  {
    "text": "there on the slide if you want to get in touch with us by the way we have uploaded this presentation deck already",
    "start": "68080",
    "end": "75190"
  },
  {
    "text": "to the skid site so you should be able to get access to all of these slides and in many spots there are hyperlinks for",
    "start": "75190",
    "end": "82480"
  },
  {
    "text": "other information that are live in that uploaded presentation I'm gonna skip the",
    "start": "82480",
    "end": "88450"
  },
  {
    "start": "87000",
    "end": "203000"
  },
  {
    "text": "abstract because I presume you read it already when you came here but I put it in the deck just so that if you find it",
    "start": "88450",
    "end": "95260"
  },
  {
    "text": "later you'll know what this about this presentation is about without going through the whole deck this is what",
    "start": "95260",
    "end": "103060"
  },
  {
    "text": "we're going to cover today so we're going to cover kubernetes default scheduling kind of the generic mechanism",
    "start": "103060",
    "end": "109830"
  },
  {
    "text": "just so you know scheduling is the process where you take a pod that's ready to run or desire to run and choose",
    "start": "109830",
    "end": "117430"
  },
  {
    "text": "what worker node it runs on so that's scheduling and we're gonna cover what you get by default then with the vSphere",
    "start": "117430",
    "end": "124780"
  },
  {
    "text": "cloud provider it turns out that you have some additional options that there",
    "start": "124780",
    "end": "129819"
  },
  {
    "text": "are not necessarily they're on every cloud provider that can help you",
    "start": "129819",
    "end": "134980"
  },
  {
    "text": "scheduling and I'll go into why you might want to use those next I'm gonna",
    "start": "134980",
    "end": "141129"
  },
  {
    "text": "hand it back to my partner and he's going to cover NUMA non-uniform memory architecture",
    "start": "141129",
    "end": "148170"
  },
  {
    "text": "finally we'll move into the default resource management resource management",
    "start": "148170",
    "end": "154269"
  },
  {
    "text": "is the concept of these worker nodes have finite amounts of CPU cores in",
    "start": "154269",
    "end": "159730"
  },
  {
    "text": "memory and the pods have appetites for this and you nominally like to have",
    "start": "159730",
    "end": "167590"
  },
  {
    "text": "these things be scheduled efficiently yet not have things like noisy neighbor",
    "start": "167590",
    "end": "173920"
  },
  {
    "text": "problems or pods starved for resources or you know even being killed when they",
    "start": "173920",
    "end": "180220"
  },
  {
    "text": "go over limits I'm not sure we're going to have time for these because at 35",
    "start": "180220",
    "end": "186280"
  },
  {
    "text": "minutes I put a bunch of slides in this deck so we may or may not get to these",
    "start": "186280",
    "end": "191380"
  },
  {
    "text": "I'm gonna respect the time period but I did put them in the publish deck so if those are interesting if we don't get to",
    "start": "191380",
    "end": "198069"
  },
  {
    "text": "those you can look at the deck and get the information anyway so kubernetes",
    "start": "198069",
    "end": "204670"
  },
  {
    "start": "203000",
    "end": "429000"
  },
  {
    "text": "scheduler what does a scheduler do well as the pods are created they're actually put into a queue and this queue by",
    "start": "204670",
    "end": "213700"
  },
  {
    "text": "default they might be ordered first-come first-serve but there are mechanism with kubernetes where you can assign",
    "start": "213700",
    "end": "219700"
  },
  {
    "text": "priorities such that a pod gets in queued and jumps ahead in lye and they",
    "start": "219700",
    "end": "226060"
  },
  {
    "text": "get into that queue the scheduler continuously pulls pods out of that",
    "start": "226060",
    "end": "231340"
  },
  {
    "text": "queue looks at the requirements so you",
    "start": "231340",
    "end": "236470"
  },
  {
    "text": "really should have a statement on your pod gamal that declares what the",
    "start": "236470",
    "end": "241870"
  },
  {
    "text": "resource requirements of that pod are and this helps the kubernetes scheduler make a better decision on where to place",
    "start": "241870",
    "end": "248560"
  },
  {
    "text": "that depending on how you have kubernetes deployed you may be able to",
    "start": "248560",
    "end": "253840"
  },
  {
    "text": "leave that out but it's generally a bad idea and because you don't get predictable behavior on how those get",
    "start": "253840",
    "end": "262019"
  },
  {
    "text": "deployed so how does this placement decision work",
    "start": "262020",
    "end": "269480"
  },
  {
    "text": "when that control loop decides where which worker note it's going to be",
    "start": "269480",
    "end": "274910"
  },
  {
    "text": "assigned to first of all it filters out all the impossible worker notes you know",
    "start": "274910",
    "end": "280940"
  },
  {
    "text": "you can have things declarations that might have used labels on these pods",
    "start": "280940",
    "end": "286760"
  },
  {
    "text": "saying I can't run on this pod you might have a resource requirement statement that says I need 100 gigs of ram and any",
    "start": "286760",
    "end": "295670"
  },
  {
    "text": "of the any of the worker nodes that can't fulfill that requirement are impossible worker nodes so we throw",
    "start": "295670",
    "end": "302690"
  },
  {
    "text": "those out and it actually if you're if you have real specialized requirements",
    "start": "302690",
    "end": "308140"
  },
  {
    "text": "there are default these filters are called predicates and this is extensible",
    "start": "308140",
    "end": "314060"
  },
  {
    "text": "so in theory you can write your own to supplement what's in kubernetes or even",
    "start": "314060",
    "end": "319250"
  },
  {
    "text": "replace it finally once we've eliminated the impossible nodes we rank you know",
    "start": "319250",
    "end": "326090"
  },
  {
    "text": "let's just say that we have 10 nodes two of them are incapable of running this",
    "start": "326090",
    "end": "331370"
  },
  {
    "text": "workload but eight remain there's an attempt made to rank the remainder to",
    "start": "331370",
    "end": "336560"
  },
  {
    "text": "choose what might be preferred here and once again this ranking is delivered by",
    "start": "336560",
    "end": "342230"
  },
  {
    "text": "plug-in modules and there's a default collection of these built into kubernetes but it is possible to",
    "start": "342230",
    "end": "348710"
  },
  {
    "text": "supplement that with custom written code that you would do yourself I'll give you",
    "start": "348710",
    "end": "355850"
  },
  {
    "text": "one other clue that if you have a huge cluster I forget what the breakdown is",
    "start": "355850",
    "end": "361850"
  },
  {
    "text": "but I think it's at least hundreds of nodes kubernetes doesn't actually go",
    "start": "361850",
    "end": "367640"
  },
  {
    "text": "through the entire list it will just maybe take a subset of the non",
    "start": "367640",
    "end": "373730"
  },
  {
    "text": "impossible nodes filter those and in the interest of latency rather than going",
    "start": "373730",
    "end": "380270"
  },
  {
    "text": "through thousands of nodes for an evaluation it will look at a subset and if you get a high rank in this subset of",
    "start": "380270",
    "end": "388640"
  },
  {
    "text": "it it will go ahead and schedule it somewhere and I believe that's configurable so that if you wanted to",
    "start": "388640",
    "end": "394400"
  },
  {
    "text": "really force it to look at all of them you could but I'm telling you the default is that at a certain cluster size it",
    "start": "394400",
    "end": "402020"
  },
  {
    "text": "actually just grabs a portion of the eligible nodes and ranks those so the",
    "start": "402020",
    "end": "411590"
  },
  {
    "text": "steps are the pods created it's in queued evaluated the nodes are evaluated filtered ranked and then that pod is",
    "start": "411590",
    "end": "418550"
  },
  {
    "text": "actually the X's are the impossible ones this is the ranking and it would go to",
    "start": "418550",
    "end": "424670"
  },
  {
    "text": "the preferred node which had a one on it",
    "start": "424670",
    "end": "429520"
  },
  {
    "start": "429000",
    "end": "551000"
  },
  {
    "text": "so what can you do to modify the scheduler there are node selectors are based on labels",
    "start": "430750",
    "end": "436990"
  },
  {
    "text": "there are affinities so you this is used to say that I want my pod to be",
    "start": "436990",
    "end": "443440"
  },
  {
    "text": "scheduled on the same note as another one or never scheduled on the same this",
    "start": "443440",
    "end": "448970"
  },
  {
    "text": "list of elements that can influence this arc Jones taints toleration x' and",
    "start": "448970",
    "end": "454130"
  },
  {
    "text": "there's a number of these admission controllers that can affect that placement - so why you zone so well",
    "start": "454130",
    "end": "461600"
  },
  {
    "text": "kubernetes if you label your worker nodes with zone labels you would do this",
    "start": "461600",
    "end": "469310"
  },
  {
    "text": "to indicate failure domains within your within your data center so that you can",
    "start": "469310",
    "end": "477050"
  },
  {
    "text": "use this to help the kubernetes scheduler automatically spread out horizontally scalable services across",
    "start": "477050",
    "end": "485270"
  },
  {
    "text": "failure domains and that's why you would want to use these these own labels if",
    "start": "485270",
    "end": "491990"
  },
  {
    "text": "you're on vSphere this these own labels can be applied to ESX host those are",
    "start": "491990",
    "end": "498080"
  },
  {
    "text": "hypervisor hosts if you don't know that much about vSphere you can also do put",
    "start": "498080",
    "end": "503660"
  },
  {
    "text": "these labels on data centers and DRS",
    "start": "503660",
    "end": "510830"
  },
  {
    "text": "clusters so when you do this the",
    "start": "510830",
    "end": "516500"
  },
  {
    "text": "scheduler will use these to properly spread this out but I'll caution you that this falls into a category of",
    "start": "516500",
    "end": "525110"
  },
  {
    "text": "what's called a priority and they have another category called a predicate so that if it looks like you wanted three",
    "start": "525110",
    "end": "532070"
  },
  {
    "text": "no running but does there's insufficient zones you only have to it is it's rather",
    "start": "532070",
    "end": "538880"
  },
  {
    "text": "than not schedule your workload at all it's going to put two duplicates in the",
    "start": "538880",
    "end": "545420"
  },
  {
    "text": "same zone so it's a best-effort thing rather than a guarantee next we'll move on to zones",
    "start": "545420",
    "end": "556000"
  },
  {
    "start": "551000",
    "end": "745000"
  },
  {
    "text": "zones is used for spreading workloads Numa is an interesting concept and I'll let we cover it thank you",
    "start": "556000",
    "end": "564670"
  },
  {
    "text": "first I want you ask a question how many of you heard about the Numa before okay",
    "start": "564670",
    "end": "575440"
  },
  {
    "text": "that's good so Newmar stands for non unified memory",
    "start": "576370",
    "end": "581660"
  },
  {
    "text": "access the picture here showed these new",
    "start": "581660",
    "end": "587440"
  },
  {
    "text": "architecture we have to see few cars here and actually sockets right yeah but",
    "start": "587440",
    "end": "598760"
  },
  {
    "text": "we can also call it ok on the left side you have memory you have 12 memory dings",
    "start": "598760",
    "end": "607550"
  },
  {
    "text": "we six channels and on the right side of the same there the design in this way if",
    "start": "607550",
    "end": "618890"
  },
  {
    "text": "the CPU do want to access the memory on the left side it has latency here for",
    "start": "618890",
    "end": "626959"
  },
  {
    "text": "this one in the example is say 75 nanoseconds and since this is a new",
    "start": "626959",
    "end": "635690"
  },
  {
    "text": "market X CPU both CPU can access all the memory bus on the left side on the right side but there's a penalty if CPU zero",
    "start": "635690",
    "end": "644720"
  },
  {
    "text": "want to access the memory on the right side it's actually you have to go through the interconnect which is which",
    "start": "644720",
    "end": "653480"
  },
  {
    "text": "actually has overhead so the latency",
    "start": "653480",
    "end": "658700"
  },
  {
    "text": "will become 130 milliseconds and also their band wise is will be limiting",
    "start": "658700",
    "end": "666470"
  },
  {
    "text": "by this interconnect so the problem is if you are on application and if you",
    "start": "666470",
    "end": "675980"
  },
  {
    "text": "care about the performance then there's about can be can be 80% difference in",
    "start": "675980",
    "end": "683990"
  },
  {
    "text": "terms of latency and also the band wise so that's what we are trying to discuss",
    "start": "683990",
    "end": "692090"
  },
  {
    "text": "here if you are application performance",
    "start": "692090",
    "end": "698360"
  },
  {
    "text": "sensitive and you're running on the kubernetes and how can you make sure",
    "start": "698360",
    "end": "705250"
  },
  {
    "text": "first you want to improve the performance second actually you want to",
    "start": "705250",
    "end": "712820"
  },
  {
    "text": "make your application performance predictable means maybe it's not the",
    "start": "712820",
    "end": "719150"
  },
  {
    "text": "most optimal but you wanted ours enough",
    "start": "719150",
    "end": "724250"
  },
  {
    "text": "in a predictor way means it's flights now it's like a fluctuating and actually",
    "start": "724250",
    "end": "731750"
  },
  {
    "text": "consistency is the key and it's more important than you have optimal",
    "start": "731750",
    "end": "738470"
  },
  {
    "text": "performance in one second and you have bad performance later so I covered a",
    "start": "738470",
    "end": "748430"
  },
  {
    "start": "745000",
    "end": "889000"
  },
  {
    "text": "little bit why we we care about Numa if you care about performance then you have",
    "start": "748430",
    "end": "755600"
  },
  {
    "text": "to care about Numa especially in the",
    "start": "755600",
    "end": "760760"
  },
  {
    "text": "older days we have um a means unified memory access architecture now modern",
    "start": "760760",
    "end": "768230"
  },
  {
    "text": "data centers and modern servers most especially for the enterprise for the",
    "start": "768230",
    "end": "773780"
  },
  {
    "text": "server's they're mostly Numa means you have more than one sockets you have non",
    "start": "773780",
    "end": "781130"
  },
  {
    "text": "unified memory access they are I I consider their two type of",
    "start": "781130",
    "end": "789610"
  },
  {
    "text": "situation that you application are not accessing your local memory means you",
    "start": "789610",
    "end": "798590"
  },
  {
    "text": "don't have you don't have the most optimal performance one situation is you",
    "start": "798590",
    "end": "806870"
  },
  {
    "text": "are application running on there on the CPU zero here and the memory in this in",
    "start": "806870",
    "end": "816050"
  },
  {
    "text": "this CPU you already got allocated so when you start to request memory you are",
    "start": "816050",
    "end": "821930"
  },
  {
    "text": "getting memory found from the right side so in this in this situation you may get",
    "start": "821930",
    "end": "827450"
  },
  {
    "text": "some memory on there on your local and you also get some memory from your remote then you have a mix set of memory",
    "start": "827450",
    "end": "835400"
  },
  {
    "text": "and your performance is suffered a little bit another situation is for some",
    "start": "835400",
    "end": "845090"
  },
  {
    "text": "database servers it's try to allocate",
    "start": "845090",
    "end": "850900"
  },
  {
    "text": "all the memory in the in the server so if you are running under on a server",
    "start": "850900",
    "end": "859820"
  },
  {
    "text": "with multiple new multiple sockets then it's inevitably getting all the memory",
    "start": "859820",
    "end": "865670"
  },
  {
    "text": "means it has a lot of remote memory",
    "start": "865670",
    "end": "870890"
  },
  {
    "text": "allocated also if you want to avoid that situation if you want a better",
    "start": "870890",
    "end": "877730"
  },
  {
    "text": "performance you want to avoid this situation so now the question how can we",
    "start": "877730",
    "end": "889269"
  },
  {
    "text": "how can we avoid those issues so there",
    "start": "889540",
    "end": "895310"
  },
  {
    "text": "are two general ways or method one is I",
    "start": "895310",
    "end": "901640"
  },
  {
    "text": "took a little bit for performance the consistency is very clean so you can you",
    "start": "901640",
    "end": "908540"
  },
  {
    "text": "maybe you can tolerance its average performance but it's very consistent",
    "start": "908540",
    "end": "915020"
  },
  {
    "text": "it's new and it's flights one way to do it if you are running on",
    "start": "915020",
    "end": "922360"
  },
  {
    "text": "the linux wien you can use on linux machine you can use an uma ctrl command",
    "start": "922360",
    "end": "929400"
  },
  {
    "text": "to interleave the memory means you are getting memory from all your memory note",
    "start": "929400",
    "end": "937410"
  },
  {
    "text": "and they have you get a you get an",
    "start": "937410",
    "end": "944080"
  },
  {
    "text": "average performance but you get it's very consistent memory access because you are accessing all the memory note",
    "start": "944080",
    "end": "950230"
  },
  {
    "text": "yeah glory you'll take half of your memory from the good fast side and half from the slow",
    "start": "950230",
    "end": "955810"
  },
  {
    "text": "side always so this is just one way but",
    "start": "955810",
    "end": "961450"
  },
  {
    "text": "I'm going to show you if you're running on there if you're running on the",
    "start": "961450",
    "end": "967720"
  },
  {
    "text": "hypervisor like the vSphere environments we can do better because we can",
    "start": "967720",
    "end": "975900"
  },
  {
    "text": "vocabularies can probably properly sites their node sites reading the Neumann",
    "start": "975900",
    "end": "983830"
  },
  {
    "text": "boundary means I'll go back to there to",
    "start": "983830",
    "end": "989380"
  },
  {
    "start": "987000",
    "end": "1111000"
  },
  {
    "text": "the picture here so I'll create a",
    "start": "989380",
    "end": "995589"
  },
  {
    "text": "virtual machine that the number of virtual CPU is equal the number of",
    "start": "995589",
    "end": "1002520"
  },
  {
    "text": "physical CPU in one sockets for example if the socket on the left sided actually",
    "start": "1002520",
    "end": "1009560"
  },
  {
    "text": "eight physical CPU then I create virtual",
    "start": "1009560",
    "end": "1015630"
  },
  {
    "text": "CPU virtual machine and on a hypervisor we have something called their new",
    "start": "1015630",
    "end": "1023400"
  },
  {
    "text": "mascara so when the VN gets scheduled and you are requesting memory actually",
    "start": "1023400",
    "end": "1030688"
  },
  {
    "text": "it's going to get up get old memory from these on the left side means they're",
    "start": "1030689",
    "end": "1037260"
  },
  {
    "text": "mostly from the local memory and if your workload starts you accessing more",
    "start": "1037260",
    "end": "1044010"
  },
  {
    "text": "memory and you you actually get to get more remote memory over schedule",
    "start": "1044010",
    "end": "1050490"
  },
  {
    "text": "actually well well starting move migrates you means it's not affecting your application its",
    "start": "1050490",
    "end": "1059110"
  },
  {
    "text": "move you to a new Numa node so that you still get most of the memory from local",
    "start": "1059110",
    "end": "1066870"
  },
  {
    "text": "so the key here is the new mascara will do the intelligence scheduling and make",
    "start": "1066870",
    "end": "1074530"
  },
  {
    "text": "sure your performance is still consistent and also better so the",
    "start": "1074530",
    "end": "1083170"
  },
  {
    "text": "requirements folder for the developer is when you krill this note on hypervisor",
    "start": "1083170",
    "end": "1092800"
  },
  {
    "text": "environments you have to pay attention how to size your note how much memory",
    "start": "1092800",
    "end": "1098620"
  },
  {
    "text": "how much virtual CPU and other devices",
    "start": "1098620",
    "end": "1103870"
  },
  {
    "text": "and the guideline we can give is our vSphere environments the first you want",
    "start": "1103870",
    "end": "1118150"
  },
  {
    "start": "1111000",
    "end": "1297000"
  },
  {
    "text": "to I want to we wanna give is it possible just size your note reading",
    "start": "1118150",
    "end": "1126910"
  },
  {
    "text": "there Numa boundary means you know your server have in one Yamanote um how many",
    "start": "1126910",
    "end": "1134850"
  },
  {
    "text": "physical CPU costs and how much memory then you creates your",
    "start": "1134850",
    "end": "1141120"
  },
  {
    "text": "kubernetes note the virtual machine Academy but there are case that your",
    "start": "1141120",
    "end": "1148330"
  },
  {
    "text": "application do require big asides note",
    "start": "1148330",
    "end": "1154590"
  },
  {
    "text": "so our suggestion adjust limits how many how many number for Numa notes you may",
    "start": "1154590",
    "end": "1162670"
  },
  {
    "text": "require for example if you can fit into two then don't go up to three another",
    "start": "1162670",
    "end": "1171670"
  },
  {
    "text": "one is the number we CPU don't give all",
    "start": "1171670",
    "end": "1177340"
  },
  {
    "text": "the number of receive use this just because in the reality other Intel",
    "start": "1177340",
    "end": "1184190"
  },
  {
    "text": "AMD servers the number of course in physical CPUs in a1 new mobility usually",
    "start": "1184190",
    "end": "1191419"
  },
  {
    "text": "is an even number is 812 and it's never be seven or five so your your vsv CFU",
    "start": "1191419",
    "end": "1202700"
  },
  {
    "text": "number should be also be something like AIDS so that we can place this VN in the",
    "start": "1202700",
    "end": "1210409"
  },
  {
    "text": "Optima optimized fashion otherwise we may face some bin packing problem we",
    "start": "1210409",
    "end": "1217190"
  },
  {
    "text": "have an eight core CPU in physically and you create 7v CPU VM and we placed there",
    "start": "1217190",
    "end": "1226129"
  },
  {
    "text": "then we only have one one car left and it's hard to put another VM schedule",
    "start": "1226129",
    "end": "1232460"
  },
  {
    "text": "another we're into it and the last one",
    "start": "1232460",
    "end": "1238580"
  },
  {
    "text": "is never create a BN that you have a number of recipients a larger than the",
    "start": "1238580",
    "end": "1245090"
  },
  {
    "text": "physical CPU the host has that's actually obvious you actually don't want",
    "start": "1245090",
    "end": "1253070"
  },
  {
    "text": "to do that and the benefits the benefits",
    "start": "1253070",
    "end": "1261080"
  },
  {
    "text": "of doing this is actually to achieve",
    "start": "1261080",
    "end": "1266620"
  },
  {
    "text": "consistent predictable performance under",
    "start": "1266620",
    "end": "1271870"
  },
  {
    "text": "other kubernetes and this this word with",
    "start": "1271870",
    "end": "1279919"
  },
  {
    "text": "the hypervisor environments we our hypervisor schedule new mascara can help",
    "start": "1279919",
    "end": "1286580"
  },
  {
    "text": "you immediately now and in the near future or in the future the kubernetes",
    "start": "1286580",
    "end": "1293769"
  },
  {
    "text": "community has also started work on this",
    "start": "1293769",
    "end": "1299029"
  },
  {
    "text": "problem I will start to tackle this problem there's discussion about Numa",
    "start": "1299029",
    "end": "1304570"
  },
  {
    "text": "proposal and discussion around the Numa manager in the upstream community and",
    "start": "1304570",
    "end": "1312650"
  },
  {
    "text": "the community want to address this issue in all different kind of the environment",
    "start": "1312650",
    "end": "1319530"
  },
  {
    "text": "not only the hypervisor environments if you're interested please join the",
    "start": "1319530",
    "end": "1324860"
  },
  {
    "text": "discussion this link and with that I'll",
    "start": "1324860",
    "end": "1330900"
  },
  {
    "start": "1330000",
    "end": "1527000"
  },
  {
    "text": "hand over to Steve to talk about the resource management ok thanks we so on to the next topic",
    "start": "1330900",
    "end": "1337650"
  },
  {
    "text": "resource management let me start with explaining how it works so every time",
    "start": "1337650",
    "end": "1344370"
  },
  {
    "text": "you declare a pond and this gets dispatched these resources and resources",
    "start": "1344370",
    "end": "1350940"
  },
  {
    "text": "are things like CPU cores memory they're metered on a per container basis so",
    "start": "1350940",
    "end": "1356820"
  },
  {
    "text": "inside of a pod you could have multiple containers but you specify the resource requirements on a per container basis if",
    "start": "1356820",
    "end": "1364830"
  },
  {
    "text": "you happen to have say a three container pod these resource requirements",
    "start": "1364830",
    "end": "1370500"
  },
  {
    "text": "statements are broken up into two parts one is a specification of requests and",
    "start": "1370500",
    "end": "1379860"
  },
  {
    "text": "this is a declaration that look I need one gigabyte of ram minimum and this is",
    "start": "1379860",
    "end": "1388830"
  },
  {
    "text": "the minimum the containers guaranteed to get and if that just isn't available the",
    "start": "1388830",
    "end": "1396450"
  },
  {
    "text": "pods just not going to start you can in addition to the request you can ask for",
    "start": "1396450",
    "end": "1403710"
  },
  {
    "text": "a limit and this causes kubernetes to impose a limit to prevent noisy neighbor",
    "start": "1403710",
    "end": "1409980"
  },
  {
    "text": "problems for example let's say I had a memory leak in the application running",
    "start": "1409980",
    "end": "1415049"
  },
  {
    "text": "in that container and it proceeds over time to gobble up more and more memory",
    "start": "1415049",
    "end": "1421110"
  },
  {
    "text": "without limit the lip specifying a limit on that container will cause kubernetes",
    "start": "1421110",
    "end": "1426809"
  },
  {
    "text": "to monitor consumption and potentially even terminate that pod should the limit",
    "start": "1426809",
    "end": "1433620"
  },
  {
    "text": "get exceeded these requests and limits by default are",
    "start": "1433620",
    "end": "1440240"
  },
  {
    "text": "unmanaged but a mechanism exists in the cloud provider for the admin to supply",
    "start": "1440240",
    "end": "1448460"
  },
  {
    "text": "default to so that if somebody puts out a container a pod spec that doesn't",
    "start": "1448460",
    "end": "1456120"
  },
  {
    "text": "specify these it can automatically act as if these defaults were specified in",
    "start": "1456120",
    "end": "1461720"
  },
  {
    "text": "the ammo for each of the containers in this pod there's another mechanism for",
    "start": "1461720",
    "end": "1469049"
  },
  {
    "text": "keeping control of these resources at the namespace level this would typically",
    "start": "1469049",
    "end": "1475410"
  },
  {
    "text": "be used to to allow an administrator to",
    "start": "1475410",
    "end": "1480630"
  },
  {
    "text": "impose a quota per Department or per working group within the company so",
    "start": "1480630",
    "end": "1487710"
  },
  {
    "text": "maybe you've got an accounting department a dev department and a test Department all sharing the same",
    "start": "1487710",
    "end": "1495110"
  },
  {
    "text": "collection of these worker nodes an administrator can go assign each of these departments to a namespace and",
    "start": "1495110",
    "end": "1503480"
  },
  {
    "text": "impose some governance so that the developers can't hog everything leaving",
    "start": "1503480",
    "end": "1509309"
  },
  {
    "text": "nothing left for production for example so these resource quotas are a very",
    "start": "1509309",
    "end": "1515700"
  },
  {
    "text": "useful tool for specifying requests and",
    "start": "1515700",
    "end": "1520830"
  },
  {
    "text": "limits even there's even a mechanism to take certain types of resource objects",
    "start": "1520830",
    "end": "1528690"
  },
  {
    "start": "1527000",
    "end": "1623000"
  },
  {
    "text": "and impose these like limiting the number of config maps persistent volume claims secret services and it really is",
    "start": "1528690",
    "end": "1536250"
  },
  {
    "text": "a mechanism to prevent people from going out there and hogging everything and",
    "start": "1536250",
    "end": "1543020"
  },
  {
    "text": "preventing others from getting a fair access to these types of resources when",
    "start": "1543020",
    "end": "1549540"
  },
  {
    "text": "you specify these when you're specifying requests and limits for CPUs this",
    "start": "1549540",
    "end": "1555390"
  },
  {
    "text": "happens to be in units of Miller cores which are one one-thousandth IV a core memory is",
    "start": "1555390",
    "end": "1564129"
  },
  {
    "text": "specified in these BB bytes and you can see the formula there I will caution you",
    "start": "1564129",
    "end": "1570669"
  },
  {
    "text": "that on kubernetes these these cores",
    "start": "1570669",
    "end": "1575679"
  },
  {
    "text": "potentially if you have a non-uniform hardware don't take that into account",
    "start": "1575679",
    "end": "1581019"
  },
  {
    "text": "so if I have some five-year-old servers that have two gigahertz CPU cores and then I bought some new ones a month ago",
    "start": "1581019",
    "end": "1588129"
  },
  {
    "text": "that are twice the speed the kubernetes scheduling process and resource",
    "start": "1588129",
    "end": "1593470"
  },
  {
    "text": "management doesn't really take that into account now when you're on a hypervisor",
    "start": "1593470",
    "end": "1599529"
  },
  {
    "text": "like vSphere you can potentially adapt for some of those sorts of things in the",
    "start": "1599529",
    "end": "1604749"
  },
  {
    "text": "underlay and the hypervisor to get more predictable behavior but the default",
    "start": "1604749",
    "end": "1610059"
  },
  {
    "text": "kubernetes when you run in a bare-metal really it just blatantly assumes that every core in the entire cluster runs at",
    "start": "1610059",
    "end": "1619359"
  },
  {
    "text": "the same speed for its behavior so the",
    "start": "1619359",
    "end": "1626649"
  },
  {
    "start": "1623000",
    "end": "1651000"
  },
  {
    "text": "way these resources work is actually a nested series of goals isolating",
    "start": "1626649",
    "end": "1634570"
  },
  {
    "text": "workloads giving high priority workloads first claim on the share imposing",
    "start": "1634570",
    "end": "1640960"
  },
  {
    "text": "fairness and scheduling with efficiency to maximize the consumption of the",
    "start": "1640960",
    "end": "1646029"
  },
  {
    "text": "resources physically available in the underlying hardware the built-in",
    "start": "1646029",
    "end": "1653169"
  },
  {
    "start": "1651000",
    "end": "1749000"
  },
  {
    "text": "resource management and kubernetes enforces it enforces these limits at the",
    "start": "1653169",
    "end": "1661119"
  },
  {
    "text": "worker node level so there's code running at the worker nodes that are monitoring what the specified limits are",
    "start": "1661119",
    "end": "1668379"
  },
  {
    "text": "and what's what's allowed and not when",
    "start": "1668379",
    "end": "1673440"
  },
  {
    "text": "you should run short of CPU what happens is if you violate a limit you actually",
    "start": "1673440",
    "end": "1680859"
  },
  {
    "text": "just get throttling where the time slicing of the periods with which you",
    "start": "1680859",
    "end": "1688059"
  },
  {
    "text": "have access to the CPU core are cut back memory on the other hand is in",
    "start": "1688059",
    "end": "1693129"
  },
  {
    "text": "compressive so if you trigger a violation there of a limit you get the death penalty your",
    "start": "1693129",
    "end": "1699700"
  },
  {
    "text": "container can be can be terminated these",
    "start": "1699700",
    "end": "1704880"
  },
  {
    "text": "this enforcement is not instantaneous and these are configurable on kubernetes",
    "start": "1704880",
    "end": "1710770"
  },
  {
    "text": "so you can if you choose it as an administrator you can give them a grace",
    "start": "1710770",
    "end": "1716320"
  },
  {
    "text": "period where they might be able to go over the limit for a very short amount of time and escape the death penalty so",
    "start": "1716320",
    "end": "1724950"
  },
  {
    "text": "this is what goes on there but when you're initially scheduling that if a",
    "start": "1724950",
    "end": "1731980"
  },
  {
    "text": "node doesn't have enough to support you know a resource request it simply won't",
    "start": "1731980",
    "end": "1738039"
  },
  {
    "text": "will not be put on that node after scheduling pods aren't affected by this",
    "start": "1738039",
    "end": "1744820"
  },
  {
    "text": "quota so so resource management",
    "start": "1744820",
    "end": "1753070"
  },
  {
    "start": "1749000",
    "end": "1828000"
  },
  {
    "text": "enforcement is actually working in a",
    "start": "1753070",
    "end": "1758230"
  },
  {
    "text": "hierarchy where it operates on kubernetes some of this gets passed downs of the container runtime it might",
    "start": "1758230",
    "end": "1764590"
  },
  {
    "text": "happen to be you know your docker run time or some other container run time that in turn can pass through down to",
    "start": "1764590",
    "end": "1772539"
  },
  {
    "text": "the Linux operating system and kernel itself and if you're running on a hypervisor which is optional and you",
    "start": "1772539",
    "end": "1779650"
  },
  {
    "text": "could run it on bare metal but if you run on a hypervisor there's an opportunity there to have the hypervisor",
    "start": "1779650",
    "end": "1785679"
  },
  {
    "text": "itself engage in some resource governance for you",
    "start": "1785679",
    "end": "1790750"
  },
  {
    "text": "so the kubernetes control plane is where you declare a managed desired policy the",
    "start": "1790750",
    "end": "1799809"
  },
  {
    "text": "enforcement passes down through the pod to the container runtime to the OS down",
    "start": "1799809",
    "end": "1806049"
  },
  {
    "text": "at the OS it's C groups that are used to map pod CPU and memory resources to",
    "start": "1806049",
    "end": "1813640"
  },
  {
    "text": "whatever enforcement and monitoring takes place and those are the C group's",
    "start": "1813640",
    "end": "1820299"
  },
  {
    "text": "drivers that are involved okay I'm gonna have to speed this up so I'm not gonna",
    "start": "1820299",
    "end": "1826270"
  },
  {
    "text": "read this but look at the picture we're nearing the limit so you can it turns out that if",
    "start": "1826270",
    "end": "1832929"
  },
  {
    "start": "1828000",
    "end": "1883000"
  },
  {
    "text": "you're on a hypervisor and that hypervisor is vSphere there's an",
    "start": "1832929",
    "end": "1838900"
  },
  {
    "text": "existing mechanism called DRS distributed resource scheduler that can actually load balanced BMS on a",
    "start": "1838900",
    "end": "1846030"
  },
  {
    "text": "hypervisor cluster and you can put multiple tenants on these you know in",
    "start": "1846030",
    "end": "1854500"
  },
  {
    "text": "this case different tenants are in different colors we've got masternodes",
    "start": "1854500",
    "end": "1862169"
  },
  {
    "text": "production worker nodes over here test",
    "start": "1862169",
    "end": "1867250"
  },
  {
    "text": "worker nodes and you can actually use the vSphere DRS to prioritize resource",
    "start": "1867250",
    "end": "1876429"
  },
  {
    "text": "allocations based on these departments and have this enforce down at the underlay the hypervisor level so we've",
    "start": "1876429",
    "end": "1886240"
  },
  {
    "start": "1883000",
    "end": "1894000"
  },
  {
    "text": "got five minutes left I'll open it up to questions at this point I'll tell you",
    "start": "1886240",
    "end": "1891250"
  },
  {
    "text": "that when you go download the deck some of those other things that were in the agenda are in slides after that thank",
    "start": "1891250",
    "end": "1897700"
  },
  {
    "text": "you slide and it's explaining how you go configure vSphere you know in the either",
    "start": "1897700",
    "end": "1903429"
  },
  {
    "text": "the UI or you could use the COI to take advantage of these features but download",
    "start": "1903429",
    "end": "1908530"
  },
  {
    "start": "1908000",
    "end": "1934000"
  },
  {
    "text": "the deck later there's also a feature called high availability where you can",
    "start": "1908530",
    "end": "1914350"
  },
  {
    "text": "potentially use the underlay hypervisor to automatically migrate VMs from one",
    "start": "1914350",
    "end": "1922360"
  },
  {
    "text": "cluster node to the other and for some critical applications that might be of",
    "start": "1922360",
    "end": "1928270"
  },
  {
    "text": "great interest to you so at this point does anybody have any questions",
    "start": "1928270",
    "end": "1935158"
  },
  {
    "start": "1934000",
    "end": "2012000"
  },
  {
    "text": "[Music]",
    "start": "1940940",
    "end": "1946220"
  },
  {
    "text": "even if it isn't done what we talked about if it relates to kubernetes and vSphere or even kubernetes we'd be happy",
    "start": "1946220",
    "end": "1952909"
  },
  {
    "text": "to take them if there was something we rushed through 35 minutes is a little short for this topic unfortunately but",
    "start": "1952909",
    "end": "1961720"
  },
  {
    "text": "sure can you give them the mic vSphere",
    "start": "1961990",
    "end": "1973669"
  },
  {
    "text": "and physical volumes of physical volume claims aha persistent volume yes",
    "start": "1973669",
    "end": "1980419"
  },
  {
    "text": "probably right huh so what's the question or you want me to just summarize them here give them the mic",
    "start": "1980419",
    "end": "1994299"
  },
  {
    "text": "I'm doing this right now in AWS with EBS ok we're moving to a dedicated data",
    "start": "1994299",
    "end": "2002200"
  },
  {
    "text": "center uh-huh with the vSphere okay oh what's a good way to do it with vSphere",
    "start": "2002200",
    "end": "2007929"
  },
  {
    "text": "odd you definitely want to run the vSphere cloud provider which right now",
    "start": "2007929",
    "end": "2013570"
  },
  {
    "start": "2012000",
    "end": "2267000"
  },
  {
    "text": "these cloud providers that's an entry cloud provider meaning it's already built into kubernetes but you do have to",
    "start": "2013570",
    "end": "2021309"
  },
  {
    "text": "configure it to turn it on so it's a config parameter when you start up your",
    "start": "2021309",
    "end": "2026650"
  },
  {
    "text": "kubernetes cluster some distros if you're getting a distribution might do",
    "start": "2026650",
    "end": "2032080"
  },
  {
    "text": "this for you so that whether you know it or not the cloud provider is being",
    "start": "2032080",
    "end": "2037240"
  },
  {
    "text": "engaged but there is documentation explaining how to do it manually if you just take the open source kubernetes",
    "start": "2037240",
    "end": "2044049"
  },
  {
    "text": "don't take anybody's distro and run it and what will happen there is that for",
    "start": "2044049",
    "end": "2050800"
  },
  {
    "text": "persistent volumes you're just free to use whatever storage is if it works in",
    "start": "2050800",
    "end": "2055810"
  },
  {
    "text": "the vSphere hypervisor it will work for you so if this is a greenfield",
    "start": "2055810",
    "end": "2061148"
  },
  {
    "text": "application with a modern vSphere you probably you might well be using V San",
    "start": "2061149",
    "end": "2067510"
  },
  {
    "text": "if you're in a legacy data center you might have some external Santa rays from",
    "start": "2067510",
    "end": "2073148"
  },
  {
    "text": "any number of vendors you know there's essentially everybody in the IT industry I think has",
    "start": "2073149",
    "end": "2078590"
  },
  {
    "text": "to support storage through vSphere and if it works for VMs it should work for",
    "start": "2078590",
    "end": "2085550"
  },
  {
    "text": "your containers what will happen is if you only have one type of storage you",
    "start": "2085550",
    "end": "2091908"
  },
  {
    "text": "should go there and make that your default storage class just like on Amazon EBS volumes are your default",
    "start": "2091909",
    "end": "2098600"
  },
  {
    "text": "storage class and if you declare a volume that's going to be what you get",
    "start": "2098600",
    "end": "2104030"
  },
  {
    "text": "now with vSphere it's possible that you'd have multiple tiers of storage",
    "start": "2104030",
    "end": "2109700"
  },
  {
    "text": "maybe some super fast stuff that is very high-end SSDs and then maybe some bulk",
    "start": "2109700",
    "end": "2115850"
  },
  {
    "text": "storage that's cheap but you have it and you can define an administrator can",
    "start": "2115850",
    "end": "2121460"
  },
  {
    "text": "deploy multiple storage classes there still would be a default but you could",
    "start": "2121460",
    "end": "2126680"
  },
  {
    "text": "get potentially great advantage of having maybe the cheap storage used for log files and the high performance stuff",
    "start": "2126680",
    "end": "2133460"
  },
  {
    "text": "used for your database caches and what you would do there is define these",
    "start": "2133460",
    "end": "2138890"
  },
  {
    "text": "storage classes as an admin and let the developer specify in kubernetes the you",
    "start": "2138890",
    "end": "2146000"
  },
  {
    "text": "might know this already on Amazon it's the same way AWS would work where you have persistent volume claims that would",
    "start": "2146000",
    "end": "2153590"
  },
  {
    "text": "specify a storage class if they leave it out they get the default but if they specify it it gets mapped to these my",
    "start": "2153590",
    "end": "2160970"
  },
  {
    "text": "goal if you're moving for maybe AWS this implies to me maybe you're running multi",
    "start": "2160970",
    "end": "2166760"
  },
  {
    "text": "cloud where some on pram some in the cloud you should make those storage",
    "start": "2166760",
    "end": "2171890"
  },
  {
    "text": "class labels be labels affiliated with desired outcomes so never label them",
    "start": "2171890",
    "end": "2177350"
  },
  {
    "text": "with the name of the physical vendor you might want to label one fast expense /",
    "start": "2177350",
    "end": "2182750"
  },
  {
    "text": "expensive the other one cheap so that people okay and that way people labeling",
    "start": "2182750",
    "end": "2189470"
  },
  {
    "text": "by desired outcomes it just works wherever you're running in a cloud on Prem and the other advantage",
    "start": "2189470",
    "end": "2195950"
  },
  {
    "text": "of that is interestingly enough it works over time so you know if you go back in a time machine five or ten years ago",
    "start": "2195950",
    "end": "2202930"
  },
  {
    "text": "fast might have been 15,000 RPM SAS drives and slow would be I don't know",
    "start": "2202930",
    "end": "2209210"
  },
  {
    "text": "SATA drives but if your developers broke the database servers",
    "start": "2209210",
    "end": "2215160"
  },
  {
    "text": "into one log files get slow indexes get fast that still totally works and you",
    "start": "2215160",
    "end": "2220890"
  },
  {
    "text": "never had to go change anything and it just does the right thing so that's my takeaway for persistent volumes and you",
    "start": "2220890",
    "end": "2228630"
  },
  {
    "text": "can make vSphere arguably you can get more classes of storage potentially on",
    "start": "2228630",
    "end": "2234030"
  },
  {
    "text": "vSphere than you could in Amazon because they all they offer a finite number and",
    "start": "2234030",
    "end": "2240530"
  },
  {
    "text": "it is also when you get the storage you should probably also use that zones feature potentially that I'd clued you",
    "start": "2240530",
    "end": "2247350"
  },
  {
    "text": "in on and that can get complex because you might want to conceivably map",
    "start": "2247350",
    "end": "2253310"
  },
  {
    "text": "storage affiliated with what your failure domains are in the data center I won't get into a lot of detail here",
    "start": "2253310",
    "end": "2260130"
  },
  {
    "text": "because we've got a limited time but the options are there okay anything anybody",
    "start": "2260130",
    "end": "2267540"
  },
  {
    "text": "got any other questions on what we talked about I'll we'll take anything on vSphere we've even got another expert",
    "start": "2267540",
    "end": "2275550"
  },
  {
    "text": "standing by here no non-uniform memory",
    "start": "2275550",
    "end": "2290550"
  },
  {
    "text": "architecture is just I would call it it's not fair to say it's an Intel aspect but there's like the xeon",
    "start": "2290550",
    "end": "2296850"
  },
  {
    "start": "2293000",
    "end": "2693000"
  },
  {
    "text": "processors from Intel kind of were the first to materialize this go ahead he's",
    "start": "2296850",
    "end": "2302180"
  },
  {
    "text": "yeah I think that this is definitely see now the VMware technologies this is a hardware technology both Intel and AMD",
    "start": "2302180",
    "end": "2310010"
  },
  {
    "text": "yeah most of the server has Numa in the",
    "start": "2310010",
    "end": "2315750"
  },
  {
    "text": "data center and saying not the personal not a personal computer personal computer usually don't come with the new",
    "start": "2315750",
    "end": "2321590"
  },
  {
    "text": "other data center service mostly Numa servers yeah I think it was an artifact",
    "start": "2321590",
    "end": "2327060"
  },
  {
    "text": "once you went to multiple sockets just because of limits with speed of light being close to these CPU sockets and you",
    "start": "2327060",
    "end": "2335610"
  },
  {
    "text": "know finite bandwidth interconnects they just couldn't make they I think the",
    "start": "2335610",
    "end": "2342930"
  },
  {
    "text": "designers were ultimately faced with the choice of keeping uniform or but but limiting the number",
    "start": "2342930",
    "end": "2350119"
  },
  {
    "text": "of sockets or going up in socket count and the release hasunuma is is trying to",
    "start": "2350119",
    "end": "2357440"
  },
  {
    "text": "have has a architecture that can can extend so if you only have one socket",
    "start": "2357440",
    "end": "2364549"
  },
  {
    "text": "you actually has limits so you have you you limit your potential to to design a",
    "start": "2364549",
    "end": "2372650"
  },
  {
    "text": "computer that how much memory you can has because your one memory bus or the",
    "start": "2372650",
    "end": "2379039"
  },
  {
    "text": "number of memory bus can only suppose so many memories so if you wanted design",
    "start": "2379039",
    "end": "2384109"
  },
  {
    "text": "actually for example the HPC the high performance compute like we have the care who so",
    "start": "2384109",
    "end": "2391970"
  },
  {
    "text": "those all those high performance supercomputers actually they have to be",
    "start": "2391970",
    "end": "2397279"
  },
  {
    "text": "non-uniform memory access because this is how they have a really powerful",
    "start": "2397279",
    "end": "2405759"
  },
  {
    "text": "supercomputer that has giant memory and CPU and the GPU sand work has one",
    "start": "2405759",
    "end": "2412910"
  },
  {
    "text": "computer yeah those those cores actually have even like l2 and l3 caches",
    "start": "2412910",
    "end": "2418400"
  },
  {
    "text": "sometimes so and they're right up physically on the silicon chip moving",
    "start": "2418400",
    "end": "2423710"
  },
  {
    "text": "out to the DRAM strips and if you tried to get DRAM scripts there's something",
    "start": "2423710",
    "end": "2429559"
  },
  {
    "text": "called buffer electronics to be accessible to all those cores you have",
    "start": "2429559",
    "end": "2434630"
  },
  {
    "text": "to up the current flow on those the zoom more power and it just becomes really",
    "start": "2434630",
    "end": "2440029"
  },
  {
    "text": "difficult to do so you end up with this non-uniform architecture and kind of",
    "start": "2440029",
    "end": "2446569"
  },
  {
    "text": "this the takeaway here is by using a hypervisor you can carve up the",
    "start": "2446569",
    "end": "2452210"
  },
  {
    "text": "allocation of these physical limited boundaries intelligently so you're",
    "start": "2452210",
    "end": "2457579"
  },
  {
    "text": "minimizing the crossovers to these latency and bandwidth choke points and",
    "start": "2457579",
    "end": "2464170"
  },
  {
    "text": "fewer bad things can go on at random times",
    "start": "2464170",
    "end": "2469359"
  },
  {
    "text": "the beauty in the cabinet is monitoring",
    "start": "2471360",
    "end": "2478930"
  },
  {
    "text": "tours so the question is you know we're",
    "start": "2478930",
    "end": "2487210"
  },
  {
    "text": "building kubernetes in this fear of being word do we have any monitoring tours I think so for for",
    "start": "2487210",
    "end": "2498160"
  },
  {
    "text": "building solution we have one on vSphere we have two options why is purely open-source we showed here",
    "start": "2498160",
    "end": "2509710"
  },
  {
    "text": "is we have cloud provider and then you just basically download kubernetes",
    "start": "2509710",
    "end": "2514780"
  },
  {
    "text": "upstream kubernetes and you build a cluster by yourself then with this you then you have to",
    "start": "2514780",
    "end": "2523320"
  },
  {
    "text": "continue to the build solution by yourself or monitoring premises and",
    "start": "2523320",
    "end": "2528430"
  },
  {
    "text": "others but we do offering the enterprise",
    "start": "2528430",
    "end": "2533560"
  },
  {
    "text": "solution we we call PKS and pecans definitely has the tool to you and I",
    "start": "2533560",
    "end": "2540400"
  },
  {
    "text": "just it depends what you're monitoring so kubernetes has monitoring built in or optional some of this is optional but",
    "start": "2540400",
    "end": "2546730"
  },
  {
    "text": "there CN CF open source projects they're very popular like Prometheus and I would",
    "start": "2546730",
    "end": "2552760"
  },
  {
    "text": "contend for the most part they're geared towards monitoring the software",
    "start": "2552760",
    "end": "2557850"
  },
  {
    "text": "applications that are running and if you're on a hypervisor on Prem you might",
    "start": "2557850",
    "end": "2563350"
  },
  {
    "text": "want to get into if your definition of monitor includes that a fan failed in a physical server or something like that",
    "start": "2563350",
    "end": "2569890"
  },
  {
    "text": "vSphere itself has things there that out of the box you're not likely to find",
    "start": "2569890",
    "end": "2575560"
  },
  {
    "text": "built into kubernetes you might be able to write route those through to something like Prometheus but in vSphere",
    "start": "2575560",
    "end": "2583210"
  },
  {
    "text": "they're generally on a supported hardware platform going to be automatically done at the hypervisor",
    "start": "2583210",
    "end": "2588880"
  },
  {
    "text": "level some of these DRS functions the slides I couldn't get to with high availability will actually proactively",
    "start": "2588880",
    "end": "2595440"
  },
  {
    "text": "monitor that for you and engage in you",
    "start": "2595440",
    "end": "2600640"
  },
  {
    "text": "know let's say you have one power supply fail on a2 power supply server you might be able to have this",
    "start": "2600640",
    "end": "2608500"
  },
  {
    "text": "thing give you an early warning or potentially depending on your configuration even allow you to avoid",
    "start": "2608500",
    "end": "2614470"
  },
  {
    "text": "that or evacuate it so that you when somebody comes to take that server down",
    "start": "2614470",
    "end": "2620530"
  },
  {
    "text": "to service it there's not even the slightest disruption so I I think it's",
    "start": "2620530",
    "end": "2627220"
  },
  {
    "text": "fair to say there's plenty of monitoring built-in to kubernetes there's also external third-party monitoring",
    "start": "2627220",
    "end": "2634720"
  },
  {
    "text": "solutions that a lot of people use but in addition to that when you come down to the kind of what's say the hardware",
    "start": "2634720",
    "end": "2641350"
  },
  {
    "text": "level there's still a great value in what's available in vSphere and I think",
    "start": "2641350",
    "end": "2648970"
  },
  {
    "text": "there are drivers that might route that into kubernetes so that nobody wants to look at five panes of glass to keep an",
    "start": "2648970",
    "end": "2656140"
  },
  {
    "text": "eye on this so ultimately it's important more important that the collection is",
    "start": "2656140",
    "end": "2661630"
  },
  {
    "text": "there and all of these things have API so that if you wanted to you could cause",
    "start": "2661630",
    "end": "2668140"
  },
  {
    "text": "these to go to some central collection point for ease of consumption yeah okay",
    "start": "2668140",
    "end": "2677380"
  },
  {
    "text": "I think we're pretty much at our limit if not over but if we didn't get to your",
    "start": "2677380",
    "end": "2684250"
  },
  {
    "text": "question or you want to ask it privately catch us now before we leave the building thank you",
    "start": "2684250",
    "end": "2692010"
  }
]