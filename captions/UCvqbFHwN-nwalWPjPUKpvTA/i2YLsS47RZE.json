[
  {
    "start": "0",
    "end": "28000"
  },
  {
    "text": "and I just started it thank you all right now the floor deals",
    "start": "539",
    "end": "8480"
  },
  {
    "text": "okay so I'm see how this goes",
    "start": "9840",
    "end": "14940"
  },
  {
    "text": "so I am the cloud native architect for resource management not for all of Intel",
    "start": "14940",
    "end": "20100"
  },
  {
    "text": "um I'm not even sure what my org is currently because we just reworked but um Sasha is my counterpart and satg",
    "start": "20100",
    "end": "28680"
  },
  {
    "start": "28000",
    "end": "152000"
  },
  {
    "text": "um so just to make sure we're all talking about the same subject when you're talking about Resource Management",
    "start": "28680",
    "end": "34320"
  },
  {
    "text": "there's two parts and I I know most of you probably already know this but I run through it anyways just to make sure we're all on the same page as far as",
    "start": "34320",
    "end": "41040"
  },
  {
    "text": "what we're discussing so there's two parts of resource management and clusters they're scheduling on the Clusters where where",
    "start": "41040",
    "end": "48300"
  },
  {
    "text": "you schedule your workload matters and so if you don't want to be scheduling your simple workloads to your high",
    "start": "48300",
    "end": "53700"
  },
  {
    "text": "performance AI specific you know machines with gpus on them and you don't",
    "start": "53700",
    "end": "58860"
  },
  {
    "text": "want to be scheduling your high performance workloads to underpowered course right the other so here's the general picture",
    "start": "58860",
    "end": "66840"
  },
  {
    "text": "um which is basically this is just a spread of of a heterogeneous system and maybe",
    "start": "66840",
    "end": "73740"
  },
  {
    "text": "what you might want scheduled on there right So currently with",
    "start": "73740",
    "end": "79260"
  },
  {
    "text": "we'll go through those physical resource plugin but currently um",
    "start": "79260",
    "end": "84360"
  },
  {
    "text": "you ask for a certain number of cores and does it spells and scheduling so kubernetes is not great at handling",
    "start": "84360",
    "end": "89580"
  },
  {
    "text": "heterogeneous clusters at this time right um the second part is once you get to",
    "start": "89580",
    "end": "94920"
  },
  {
    "text": "the node what what do you do with your resources so how do you schedule your resources determines both the performance of your workload and how",
    "start": "94920",
    "end": "100439"
  },
  {
    "text": "long it runs and this is also getting added attention to the sustainability forums that are starting up and they're",
    "start": "100439",
    "end": "106619"
  },
  {
    "text": "all over the place there's one Within cncf ideally you want to power down all the",
    "start": "106619",
    "end": "111720"
  },
  {
    "text": "resources not currently being consumed from a sustainability standpoint and you at the same time you still want optimal",
    "start": "111720",
    "end": "117479"
  },
  {
    "text": "responsiveness and getting resources aligned when it's time to use them and this is just a generic example",
    "start": "117479",
    "end": "124920"
  },
  {
    "text": "um I know it's Numa specific but this you can do this with any sort of resource changing",
    "start": "124920",
    "end": "130380"
  },
  {
    "text": "um so if you have your memory your CPU and your xpu in different zones you have the UPI bust hole I call this the toll",
    "start": "130380",
    "end": "137400"
  },
  {
    "text": "you know there's a troll living in the under the bridge and the pi bus ceiling all your time",
    "start": "137400",
    "end": "143459"
  },
  {
    "text": "ideally you want these lined this gives you more responsiveness it's faster doesn't run",
    "start": "143459",
    "end": "149099"
  },
  {
    "text": "as long it doesn't take as much energy so my team has a few different projects",
    "start": "149099",
    "end": "155879"
  },
  {
    "start": "152000",
    "end": "575000"
  },
  {
    "text": "we have Telemetry where scheduling over in satg which I'll also run over as CPU",
    "start": "155879",
    "end": "161280"
  },
  {
    "text": "we're scheduling which is part of that there's power management and then there's the kublet piece that we would like to get done so that is a really big",
    "start": "161280",
    "end": "168780"
  },
  {
    "text": "piece that I would like to understand Community needs for um so Telemetry we're scheduling why we",
    "start": "168780",
    "end": "176400"
  },
  {
    "text": "want to avoid scheduling on unhealthy nodes we want to migrate pods away from unhealthy nodes when scheduling we want",
    "start": "176400",
    "end": "181620"
  },
  {
    "text": "to consider node property such as temperature current load power and other",
    "start": "181620",
    "end": "186840"
  },
  {
    "text": "and we want to give support to external components like the kubernetes desc scheduler and GPU or schedulers",
    "start": "186840",
    "end": "193819"
  },
  {
    "text": "um I think I did the site yeah so it's a Telemetry where scheduler is",
    "start": "194819",
    "end": "201720"
  },
  {
    "text": "currently an extender so we take it Telemetry data to eight scheduling and",
    "start": "201720",
    "end": "206940"
  },
  {
    "text": "descheduling decisions in kubernetes we use policies to enable rule-based decisions on pod placement and these are",
    "start": "206940",
    "end": "213300"
  },
  {
    "text": "powered by metrics collected from the nodes you can use Prometheus you can also use other uh metrics collectors",
    "start": "213300",
    "end": "220080"
  },
  {
    "text": "it knows how to interpret filtering and scoring and utilize node Affinity roles and the policy support multimetric rules",
    "start": "220080",
    "end": "227819"
  },
  {
    "text": "so this is this is new this year so we could take any of an olive so you can combine metrics",
    "start": "227819",
    "end": "234420"
  },
  {
    "text": "so this is the general layout of how this works whereas Prometheus there's Prometheus adapter there's customistics",
    "start": "234420",
    "end": "240360"
  },
  {
    "text": "API basically if it goes into the custom metrics API we can pull it then there's Telemetry where scheduling",
    "start": "240360",
    "end": "246480"
  },
  {
    "text": "scheduler working scheduling working with the scheduler um and then there's you can have Tas",
    "start": "246480",
    "end": "253080"
  },
  {
    "text": "policies so we're going to go through these one by one and there's don't schedule so a pod",
    "start": "253080",
    "end": "259739"
  },
  {
    "text": "with this strategy would will not be scheduled on a node breaking these metric rules right",
    "start": "259739",
    "end": "265380"
  },
  {
    "text": "so if it's metric name if it's equal to one",
    "start": "265380",
    "end": "270600"
  },
  {
    "text": "then don't schedule there and then this is if it is scheduled here",
    "start": "270600",
    "end": "275820"
  },
  {
    "text": "and it prioritizes nodes based on a comparator and an up-to-date metric value",
    "start": "275820",
    "end": "281400"
  },
  {
    "text": "so if the temperature is low in this case you schedule there",
    "start": "281400",
    "end": "286620"
  },
  {
    "text": "these schedule so if a pod with his policies running on a node and that it violates can be just scheduled with the",
    "start": "286620",
    "end": "292259"
  },
  {
    "text": "kubernetes scheduler so basically if your temperature is too high or your amount of RAM is less than some",
    "start": "292259",
    "end": "298560"
  },
  {
    "text": "amount then it will Des schedule and we also allow labeling this is less",
    "start": "298560",
    "end": "304620"
  },
  {
    "text": "than the scheduling but maybe based on your particular scheduler so there's",
    "start": "304620",
    "end": "310020"
  },
  {
    "text": "so in this case we basically make labels based on these rules so you have card zero equals true and card one equals",
    "start": "310020",
    "end": "315660"
  },
  {
    "text": "true and this is used partially with GPU we're scheduling",
    "start": "315660",
    "end": "320540"
  },
  {
    "text": "um so we can this is just info on this and we could you can submit PRS for changes this is open source and",
    "start": "321180",
    "end": "328919"
  },
  {
    "text": "we do have future work for Taz before I want to release this and try to put it into the community which is specifically",
    "start": "328919",
    "end": "335100"
  },
  {
    "text": "to move from an extender because we're currently currently a kubernetes extender to the kubernetes plugin which",
    "start": "335100",
    "end": "341100"
  },
  {
    "text": "plays a little bit better with the current scheduling uh decisions",
    "start": "341100",
    "end": "347000"
  },
  {
    "text": "so we're currently in that work but once that's done we do plan to try to push up straight into the community and these",
    "start": "348060",
    "end": "354360"
  },
  {
    "text": "are more links um I'll I can send this after but there's white papers on this",
    "start": "354360",
    "end": "359940"
  },
  {
    "text": "um we have a power specific example and then there is a recent kubecon talk",
    "start": "359940",
    "end": "366479"
  },
  {
    "text": "and demo done by denisio and madelina on my team",
    "start": "366479",
    "end": "371759"
  },
  {
    "text": "do you have any questions on this before I move to GPU or scheduling",
    "start": "371759",
    "end": "376699"
  },
  {
    "text": "uh not for me okay so caveat to this is their CPU we're",
    "start": "380880",
    "end": "388080"
  },
  {
    "text": "scheduling um so this is the case I'll go over the use case the node has two gpus and each",
    "start": "388080",
    "end": "395160"
  },
  {
    "text": "has you know a certain amount of memory and you want to make a replica set to",
    "start": "395160",
    "end": "400199"
  },
  {
    "text": "three and each of these needs five gigabytes of memory you end up with those nodes being split",
    "start": "400199",
    "end": "407220"
  },
  {
    "text": "or one one of those pods being split because you can put one in each but then you still have three and three",
    "start": "407220",
    "end": "412319"
  },
  {
    "text": "so this is basically keeping you from scheduling across uh across",
    "start": "412319",
    "end": "418319"
  },
  {
    "text": "gpus but there's other ways to do this there are other other pieces of this but",
    "start": "418319",
    "end": "424080"
  },
  {
    "text": "the but with this we we're using the Intel I 915 so that was that's a deep Intel",
    "start": "424080",
    "end": "431400"
  },
  {
    "text": "specific GPU and we can choose a number of millicores here and we can choose an amount of memory",
    "start": "431400",
    "end": "437880"
  },
  {
    "text": "per each and choose how many so this tells you how many uh gpus you want and then what",
    "start": "437880",
    "end": "445319"
  },
  {
    "text": "the spread of the memory is for the particular one so this is cases where you want to divide up a current",
    "start": "445319",
    "end": "452639"
  },
  {
    "text": "a current GPU into slices and then schedule it across many pods or",
    "start": "452639",
    "end": "457979"
  },
  {
    "text": "you know you want some sort of specific number of gpus on your on your pond",
    "start": "457979",
    "end": "466340"
  },
  {
    "text": "so this is that particular project um there's NFD with GPU plugin because you really do need NFD to do the node",
    "start": "467460",
    "end": "474360"
  },
  {
    "text": "feature Discovery so you know what's running are there any questions on that one",
    "start": "474360",
    "end": "481880"
  },
  {
    "text": "okay I think one question that I had was uh",
    "start": "485880",
    "end": "491099"
  },
  {
    "text": "on the GPU so this this is for any kind of GPU or",
    "start": "491099",
    "end": "497520"
  },
  {
    "text": "yeah as long as you can you can schedule multiple things to it so how you do the underlying piece is",
    "start": "497520",
    "end": "504539"
  },
  {
    "text": "the device plugins as your own piece but this will help you do the scheduling",
    "start": "504539",
    "end": "510300"
  },
  {
    "text": "okay so the basically the device plugin would take care of making sure the drivers are up in the node and then this",
    "start": "510300",
    "end": "516539"
  },
  {
    "text": "takes care of the plot scheduling and they can share it with you right this is the scheduling half",
    "start": "516539",
    "end": "522839"
  },
  {
    "text": "so like earlier we had the scheduling half and the resource management half this is the scheduling half",
    "start": "522839",
    "end": "529320"
  },
  {
    "text": "so to be to be a bit more specific where device plugin needs to be specifically crafted to utilize those functionality",
    "start": "529320",
    "end": "538019"
  },
  {
    "text": "so our team who who is uh doing the support for GPU is involved in this",
    "start": "538019",
    "end": "544440"
  },
  {
    "text": "scheduling part as well so it's two two sides of the same piece but I I guess the question I asked was",
    "start": "544440",
    "end": "551459"
  },
  {
    "text": "if I would be using an Nvidia GPU for example with a Nvidia operator",
    "start": "551459",
    "end": "556820"
  },
  {
    "text": "orchestrating the driver setup then they need a driver to handle it",
    "start": "556820",
    "end": "562320"
  },
  {
    "text": "okay and then Nvidia plugin needs to be adjusted to utilize the same Concepts all right okay thanks",
    "start": "562320",
    "end": "570180"
  },
  {
    "text": "yeah so yeah this is scheduling only um",
    "start": "570180",
    "end": "575220"
  },
  {
    "start": "575000",
    "end": "798000"
  },
  {
    "text": "and then we have the last project that's Intel specific well GPU aware scheduling and Telemetry where scheduling aren't",
    "start": "575220",
    "end": "581580"
  },
  {
    "text": "Intel specific but we are using them for Intel um is a power manager which it provides limited control over",
    "start": "581580",
    "end": "588480"
  },
  {
    "text": "the pot So kubernetes currently you don't have really any power over the",
    "start": "588480",
    "end": "593519"
  },
  {
    "text": "configuration of CPUs assigned to the pod uh so if you wanted to lower the seat",
    "start": "593519",
    "end": "599100"
  },
  {
    "text": "frequencies or raise the frequencies and you do want that all the time with performance or sustainability environments you can't do that",
    "start": "599100",
    "end": "606480"
  },
  {
    "text": "and so we've just the Intel kubernetes power manager is designed to expose and utilize the Intel specific power",
    "start": "606480",
    "end": "612480"
  },
  {
    "text": "management Technologies So currently we have granular control",
    "start": "612480",
    "end": "618000"
  },
  {
    "text": "over the configuration cores we can change the frequency of all the cores in a shared pool we can lower power consumption by",
    "start": "618000",
    "end": "624600"
  },
  {
    "text": "controlling the frequencies as a shared pull cores and then these are the particular features we have these are",
    "start": "624600",
    "end": "630360"
  },
  {
    "text": "sstbf sstcp and then frequency tuning and currently",
    "start": "630360",
    "end": "635580"
  },
  {
    "text": "so I'll advise everyone to wait about a month maybe less until we release a new",
    "start": "635580",
    "end": "641160"
  },
  {
    "text": "version of power manager but we're changing from using the library we were using which was a python Library",
    "start": "641160",
    "end": "646860"
  },
  {
    "text": "to a golang library that we've also built which is supporting it's easier to",
    "start": "646860",
    "end": "652440"
  },
  {
    "text": "deploy and also we have a better",
    "start": "652440",
    "end": "657480"
  },
  {
    "text": "is faster for us to get in functionality and it's open source so I those are also shared",
    "start": "657480",
    "end": "665220"
  },
  {
    "text": "uh here so I can share those links also and we",
    "start": "665220",
    "end": "672720"
  },
  {
    "text": "also have a white paper on how to use these it's I don't know we like it it's",
    "start": "672720",
    "end": "678180"
  },
  {
    "text": "currently um it'll remain an operator in the future I'd like us to add a grpc",
    "start": "678180",
    "end": "685140"
  },
  {
    "text": "interface to it so you can control cores from outside the power line the power manager",
    "start": "685140",
    "end": "691200"
  },
  {
    "text": "through the power manager basically so that it doesn't have to be through the pods but if you have something monitoring things on your system where",
    "start": "691200",
    "end": "697920"
  },
  {
    "text": "you want cores spun up or down you can do that and you don't have different Power managers and governors fighting",
    "start": "697920",
    "end": "703620"
  },
  {
    "text": "for control of the course does that make sense",
    "start": "703620",
    "end": "708800"
  },
  {
    "text": "okay and I'm happy to work with anyone who wants to work with us I I am requested",
    "start": "715260",
    "end": "721079"
  },
  {
    "text": "we got the police officer but were you sorry to go",
    "start": "721079",
    "end": "727700"
  },
  {
    "text": "and then this last piece which um extra this is a community project more than",
    "start": "727860",
    "end": "733620"
  },
  {
    "text": "Intel project um is that we have the currency of kublet",
    "start": "733620",
    "end": "739019"
  },
  {
    "text": "is we have some restrictions that we can't handle today so you can't mix pins with shared cores you can't choose which Newman Zone",
    "start": "739019",
    "end": "747240"
  },
  {
    "text": "tax spreads so topology manager does a complex uh packing",
    "start": "747240",
    "end": "753180"
  },
  {
    "text": "in Numa zones different yeah so if you have something",
    "start": "753180",
    "end": "759540"
  },
  {
    "text": "we already have different types of frequencies within a core you can't choose them according to frequency",
    "start": "759540",
    "end": "764700"
  },
  {
    "text": "the cores are allocated by container not by pod I can't assign them to a pod and then let the Pod move from",
    "start": "764700",
    "end": "770339"
  },
  {
    "text": "others I can't handle Affinity of anything below node level",
    "start": "770339",
    "end": "776160"
  },
  {
    "text": "for a cold so it doesn't support CPU or memory less nodes and there's still a Max 8 pneuma",
    "start": "776160",
    "end": "781920"
  },
  {
    "text": "Zone limitation which as you start looking at the Thailand without a lot of",
    "start": "781920",
    "end": "788579"
  },
  {
    "text": "cores are doing it these days plus the fact that there's multiple sockets that's a pretty big limitation at this",
    "start": "788579",
    "end": "795120"
  },
  {
    "text": "point and part of the challenge to this is",
    "start": "795120",
    "end": "800880"
  },
  {
    "start": "798000",
    "end": "1052000"
  },
  {
    "text": "Google it has a set of resource managers that have to be addressed every time we add a new feature to the couplet so the current Solutions including CRM",
    "start": "800880",
    "end": "808079"
  },
  {
    "text": "and CPU Pooler they work by turning off the couplet functionality entirely which can have unintended",
    "start": "808079",
    "end": "814440"
  },
  {
    "text": "assuming something's working thank you so and we still cannot schedule course by",
    "start": "814440",
    "end": "819779"
  },
  {
    "text": "pods or across specific Newman zones or affinities we want it out of this model",
    "start": "819779",
    "end": "826800"
  },
  {
    "text": "so I was speaking with Derek Carr and his suggestion was to split the kublet into a con data plane it makes the",
    "start": "826800",
    "end": "833639"
  },
  {
    "text": "control plane applicable so working from that",
    "start": "833639",
    "end": "841320"
  },
  {
    "text": "we would like to to do this work so we so we can release resource managers custom to our Hardware to handle our",
    "start": "841320",
    "end": "847440"
  },
  {
    "text": "specific use cases including plugging into Solutions we've developed such as the CPU manager control plane",
    "start": "847440",
    "end": "853860"
  },
  {
    "text": "uh that's wrong this is a CPU puller",
    "start": "853860",
    "end": "859940"
  },
  {
    "text": "or CRM while still being native to the ecosystem and if you want info on crrm",
    "start": "860700",
    "end": "866220"
  },
  {
    "text": "Sasha's here so he can give he probably has a presentation in his back pocket he can go through",
    "start": "866220",
    "end": "871680"
  },
  {
    "text": "and future is we want to finish getting this RFC through and developed create a",
    "start": "871680",
    "end": "876899"
  },
  {
    "text": "cap and start work and get the kublet remodeled following the specifications and which I have links to",
    "start": "876899",
    "end": "884160"
  },
  {
    "text": "we want to plug our resource managers into the new model which will be more native",
    "start": "884160",
    "end": "890000"
  },
  {
    "text": "you know historically kubernetes has kept things in plugable interfaces so",
    "start": "890040",
    "end": "895560"
  },
  {
    "text": "cni CSI device plugins but we haven't done that in with our CPUs and memory",
    "start": "895560",
    "end": "901740"
  },
  {
    "text": "and our for your for our devices",
    "start": "901740",
    "end": "906680"
  },
  {
    "text": "and if we're if we have time we're at the half hour we can go off over the RFC",
    "start": "907680",
    "end": "913980"
  },
  {
    "text": "for the CPU landscape exploration dock where we went through all of the different",
    "start": "913980",
    "end": "921360"
  },
  {
    "text": "uh issues and then customer customer requests Etc and we put them all in this document so that we knew what we were",
    "start": "921360",
    "end": "927480"
  },
  {
    "text": "missing so that list that we have up there mostly comes from customers or from those issues right added",
    "start": "927480",
    "end": "936240"
  },
  {
    "text": "do we want to go through that RFC do we want to talk about crrm I don't I'm putting on the spot Sasha sorry about",
    "start": "936240",
    "end": "942660"
  },
  {
    "text": "that no probably what",
    "start": "942660",
    "end": "948139"
  },
  {
    "text": "uh I'm from my point of view I'm happy to for you to continue a little if you'd like we've got time",
    "start": "963300",
    "end": "968940"
  },
  {
    "text": "okay we can go through the RSC Sasha at the g12 at the end go through CRM or do",
    "start": "968940",
    "end": "974519"
  },
  {
    "text": "you want to do that first up to you I'm finding you",
    "start": "974519",
    "end": "981660"
  },
  {
    "text": "okay let's go through the RFC it's fairly short let me pull that up",
    "start": "981660",
    "end": "987440"
  },
  {
    "text": "share that",
    "start": "989160",
    "end": "991759"
  },
  {
    "text": "okay you have to find which window I was on",
    "start": "1017240",
    "end": "1024398"
  },
  {
    "text": "okay let me share you have to stop sharing then re-share okay",
    "start": "1033079",
    "end": "1038799"
  },
  {
    "text": "okay so we'll go through this um",
    "start": "1047299",
    "end": "1052120"
  },
  {
    "text": "so at the beginning this basically goes through the fact that kubernetes was initially written with a simple model of",
    "start": "1053540",
    "end": "1059299"
  },
  {
    "text": "node resources and how they would be configured this has worked well for generic",
    "start": "1059299",
    "end": "1065679"
  },
  {
    "text": "um but now we have a wider range of use cases which is why a lot of you are in this group is you",
    "start": "1066020",
    "end": "1071360"
  },
  {
    "text": "have your own specialty use cases for HPC or AI or you know any other specialty case",
    "start": "1071360",
    "end": "1077860"
  },
  {
    "text": "Telco has worst cases as well oh so",
    "start": "1077860",
    "end": "1084320"
  },
  {
    "text": "we've we wish to move to a Kublai resource plug-in model similar to how specialty resources are handled as we",
    "start": "1084320",
    "end": "1090200"
  },
  {
    "text": "have within the device plugin model I'm currently here's what what the Kubla looks up like currently so you have the",
    "start": "1090200",
    "end": "1096500"
  },
  {
    "text": "kublet you have the topology manager and then you have the hinge providers the providers are your CPU manager your device manager your memory manager",
    "start": "1096500",
    "end": "1104059"
  },
  {
    "text": "and everything has to go through the topology manager to go through so anytime you make changes you have to",
    "start": "1104059",
    "end": "1110120"
  },
  {
    "text": "make sure all of these places work correctly",
    "start": "1110120",
    "end": "1114639"
  },
  {
    "text": "so this the exploration dock is is listed here",
    "start": "1115580",
    "end": "1120980"
  },
  {
    "text": "um there are some there is some commentary on here I would like more commentary just to make sure when we start the cup we know what we're doing",
    "start": "1120980",
    "end": "1126559"
  },
  {
    "text": "as far as who's working on what so we make sure we're handling everything the cases I have in there are",
    "start": "1126559",
    "end": "1133760"
  },
  {
    "text": "already in there so um you can't oh the other one is you can't take advantage of non-uniform L3",
    "start": "1133760",
    "end": "1139820"
  },
  {
    "text": "cache access configured for CPU course so we have Intel rtt but there are",
    "start": "1139820",
    "end": "1145160"
  },
  {
    "text": "others um so that was not listed another one probably add it to that",
    "start": "1145160",
    "end": "1151460"
  },
  {
    "text": "so solutions to any one of the of these challenges require related solution to optimized memory",
    "start": "1151460",
    "end": "1158780"
  },
  {
    "text": "so if we if we change where the cores are now we have to check the memory so if you touch the CPU manager you'll now",
    "start": "1158780",
    "end": "1164179"
  },
  {
    "text": "have to check the test touch the memory manager and you still have to touch the topology manager right",
    "start": "1164179",
    "end": "1170240"
  },
  {
    "text": "no matter what you do so our design proposal is to make a plugable resource hub",
    "start": "1170240",
    "end": "1176539"
  },
  {
    "text": "basically instead of retrofitting functionality to the existing",
    "start": "1176539",
    "end": "1182059"
  },
  {
    "text": "model continually right and basically to pull all the resource managers that are currently in their",
    "start": "1182059",
    "end": "1188000"
  },
  {
    "text": "topology manager CPU manager device manager and memory manager out into a plugin and then work backwards from there",
    "start": "1188000",
    "end": "1195020"
  },
  {
    "text": "we're still going to need to uh also handle the runtimes",
    "start": "1195020",
    "end": "1200720"
  },
  {
    "text": "so there needs whatever plug-in we do has to be both because because we do want to roll those",
    "start": "1200720",
    "end": "1207860"
  },
  {
    "text": "managers out probably into a grpc piece we still also want to make sure that rent you can also route them through the",
    "start": "1207860",
    "end": "1214640"
  },
  {
    "text": "runtimes because there's projects that route the including CRM that route the resources through the runtimes",
    "start": "1214640",
    "end": "1222520"
  },
  {
    "text": "so this particular one um will go through the goals we want to be able to plug resource",
    "start": "1224660",
    "end": "1231260"
  },
  {
    "text": "managers into couplet to allow customizer of resource requirements we want to be able to export resources to",
    "start": "1231260",
    "end": "1237320"
  },
  {
    "text": "expose them to the scheduler so that piece maybe more complicated because now we",
    "start": "1237320",
    "end": "1242960"
  },
  {
    "text": "have added annotations right we want to make it simple to expand resources to those currently not",
    "start": "1242960",
    "end": "1248780"
  },
  {
    "text": "envisioned so when you're talking about memory in the memoryless notes or CPU less nodes there's other components",
    "start": "1248780",
    "end": "1254360"
  },
  {
    "text": "there right you want to make it simple to expose attributes about resources non-goals we don't want to break any",
    "start": "1254360",
    "end": "1261140"
  },
  {
    "text": "existing use cases so whatever solution we add there should be full support of default Behavior",
    "start": "1261140",
    "end": "1267440"
  },
  {
    "text": "and we don't want to change default Behavior we don't want to create any more latency than there is today for",
    "start": "1267440",
    "end": "1272840"
  },
  {
    "text": "scheduling and that's I thought maybe something we do in the future or how to look at after we get",
    "start": "1272840",
    "end": "1279799"
  },
  {
    "text": "this done is there is still quite a bit of latency in scheduling we want to be able to support current",
    "start": "1279799",
    "end": "1285679"
  },
  {
    "text": "spot specs there may be additional expense sensibility but we still need to support the current pod specs and the",
    "start": "1285679",
    "end": "1292760"
  },
  {
    "text": "plugin should not be writing to the API themselves so there needs to be some sort of interface",
    "start": "1292760",
    "end": "1298840"
  },
  {
    "text": "so the next step is basically to start the cap but we're still looking for feedback here as to what goals and",
    "start": "1299780",
    "end": "1305299"
  },
  {
    "text": "non-goals are to make sure we have everything encapsulated",
    "start": "1305299",
    "end": "1310880"
  },
  {
    "text": "thoughts",
    "start": "1313760",
    "end": "1316480"
  },
  {
    "text": "no I mean it's very very good very detailed on the RFC side of things is this something you've had to do before",
    "start": "1323720",
    "end": "1328940"
  },
  {
    "text": "or is this the first time you've done this um this is the first time I've done this",
    "start": "1328940",
    "end": "1334100"
  },
  {
    "text": "has done this before I'm sure",
    "start": "1334100",
    "end": "1337480"
  },
  {
    "text": "um I had some questions actually almost right back at the beginning um when you're writing a custom schedule",
    "start": "1343520",
    "end": "1348980"
  },
  {
    "text": "like this how how do you go about testing it so that's part of the the questions so",
    "start": "1348980",
    "end": "1356419"
  },
  {
    "text": "um so to start we're going to be using the",
    "start": "1356419",
    "end": "1363200"
  },
  {
    "text": "default managers right so whatever functionality and testing that they use",
    "start": "1363200",
    "end": "1368720"
  },
  {
    "text": "when they put in those with the initial caps is a testing that we'll pull out to make sure it works now",
    "start": "1368720",
    "end": "1375340"
  },
  {
    "text": "so all of all of the the current ways that we already validated the topology",
    "start": "1375620",
    "end": "1381559"
  },
  {
    "text": "manager memory manager CPU manager all of that seems to work through the plugin model because if it doesn't work through the",
    "start": "1381559",
    "end": "1387380"
  },
  {
    "text": "plug plug-in model we're missing something right um we do have some resources to work on",
    "start": "1387380",
    "end": "1393260"
  },
  {
    "text": "this but we would like community health problem you know",
    "start": "1393260",
    "end": "1398259"
  },
  {
    "text": "does that answer your question as far as testing yeah basically we're we're looking currently at pulling out all of",
    "start": "1399020",
    "end": "1405679"
  },
  {
    "text": "the the current caps that were already put in for uh CPU manager and",
    "start": "1405679",
    "end": "1412220"
  },
  {
    "text": "device manager all of all of the managers and just pulling them out and then looking at those particular tests",
    "start": "1412220",
    "end": "1419020"
  },
  {
    "text": "and if someone wants to test a custom resource manager you're right we probably need to test harness so that's",
    "start": "1421220",
    "end": "1426440"
  },
  {
    "text": "a good good point to have there so should I add that to the goals yeah yeah",
    "start": "1426440",
    "end": "1433840"
  },
  {
    "text": "are we into general questions now or have you got anything else you want to cover I I think I'm done Sasha do you",
    "start": "1447140",
    "end": "1454460"
  },
  {
    "text": "want to go through crrm or do you want to just go into questions well I can say a few words because there",
    "start": "1454460",
    "end": "1462260"
  },
  {
    "text": "are a few additional things beside besides with CPU management uh activities uh if if you can stop shares",
    "start": "1462260",
    "end": "1470240"
  },
  {
    "text": "so I can reuse floor is your I think",
    "start": "1470240",
    "end": "1475840"
  },
  {
    "text": "leadership foreign",
    "start": "1489260",
    "end": "1494559"
  },
  {
    "text": "yeah that's good all right so I'm going just to reduce couple of slides uh from our",
    "start": "1508220",
    "end": "1513740"
  },
  {
    "start": "1509000",
    "end": "1722000"
  },
  {
    "text": "presentation with me and uh one of our team members from my team and auntie we",
    "start": "1513740",
    "end": "1520460"
  },
  {
    "text": "did on HPC and watch fork and they uh in in last pubecon so uh Marlo already",
    "start": "1520460",
    "end": "1530419"
  },
  {
    "text": "mentioned about our project here the history of what project is such what",
    "start": "1530419",
    "end": "1536440"
  },
  {
    "text": "we try to create a couplet uh resource plugins about three or four years ago",
    "start": "1536440",
    "end": "1544100"
  },
  {
    "text": "and at that time uh Community was not ready now it looks like the community a bit",
    "start": "1544100",
    "end": "1552020"
  },
  {
    "text": "more receptive but meanwhile to validate all the ideas and to to see what uh like",
    "start": "1552020",
    "end": "1560179"
  },
  {
    "text": "what we are proposing is actually working we needed to have some solution and we come up with some intermediate",
    "start": "1560179",
    "end": "1567140"
  },
  {
    "text": "step and this intermediate step is share a resource manager uh it works as a normal container",
    "start": "1567140",
    "end": "1573799"
  },
  {
    "text": "runtime so couplet sees it as a container the okra what whatever uh it's",
    "start": "1573799",
    "end": "1580220"
  },
  {
    "text": "absolutely transparent uh towards the couplet uh it doesn't reinvent review so",
    "start": "1580220",
    "end": "1588140"
  },
  {
    "text": "in my back end it still uses container view or cry or whatever you prefer to use",
    "start": "1588140",
    "end": "1593480"
  },
  {
    "text": "but what it does it allows you to have a dedicated set of policies on how you are",
    "start": "1593480",
    "end": "1600700"
  },
  {
    "text": "managing the resources so we have both uh policies related to",
    "start": "1600700",
    "end": "1608299"
  },
  {
    "text": "Hardware so like always scenarios what model was just mentioned like limit of Numa nodes or memory key in different",
    "start": "1608299",
    "end": "1615799"
  },
  {
    "text": "setups uh all of these things we tried we know how to work with it so like we have tests",
    "start": "1615799",
    "end": "1623360"
  },
  {
    "text": "with like huge machines like 32 circuits and so on um uh we had scenarios with different",
    "start": "1623360",
    "end": "1630860"
  },
  {
    "text": "memory tiers like how you've been to this memory uh PM cxl memory which is",
    "start": "1630860",
    "end": "1635960"
  },
  {
    "text": "upcoming the hardware and so on uh we also tried to walk uh from a",
    "start": "1635960",
    "end": "1642380"
  },
  {
    "text": "perspective of not only Hardware but also application so for example if you",
    "start": "1642380",
    "end": "1647419"
  },
  {
    "text": "have a set of containers which needs to work together let's say like your application plus service mesh container",
    "start": "1647419",
    "end": "1654320"
  },
  {
    "text": "you don't want to like when the data is password between those two containers you don't really want to run to cross uh",
    "start": "1654320",
    "end": "1661640"
  },
  {
    "text": "like L3 cache the domain zones or well even worse like memory domain zones and",
    "start": "1661640",
    "end": "1666799"
  },
  {
    "text": "so forth uh we support container affinity and anti-affinity so for example like your",
    "start": "1666799",
    "end": "1673820"
  },
  {
    "text": "database should not be affected by um like CPU consumption of backup",
    "start": "1673820",
    "end": "1680900"
  },
  {
    "text": "container or something similar so we provide to to the user who said uh of",
    "start": "1680900",
    "end": "1688580"
  },
  {
    "text": "knobs how to get out of open out and let's actually like the main",
    "start": "1688580",
    "end": "1695600"
  },
  {
    "text": "difference between like what uh Team what where Marvel is working and my",
    "start": "1695600",
    "end": "1701720"
  },
  {
    "text": "particular team my team we are focusing specifically what happening with zoom or",
    "start": "1701720",
    "end": "1707240"
  },
  {
    "text": "not like all the details always give knowledge of the hardware or all combinations of how it's ready it's",
    "start": "1707240",
    "end": "1714020"
  },
  {
    "text": "going to work so uh",
    "start": "1714020",
    "end": "1719860"
  },
  {
    "start": "1722000",
    "end": "1844000"
  },
  {
    "text": "so as I mentioned right now it's implemented uh other like kind of proxy",
    "start": "1723440",
    "end": "1729380"
  },
  {
    "text": "between where kublet and actual container runtime",
    "start": "1729380",
    "end": "1734419"
  },
  {
    "text": "but we are working together with um uh container game project and cryo",
    "start": "1734419",
    "end": "1741980"
  },
  {
    "text": "project to implement the thing which is called NRI an old resource of the phrase",
    "start": "1741980",
    "end": "1747020"
  },
  {
    "text": "it's also plug-in interface similar to what uh complete Community is now",
    "start": "1747020",
    "end": "1752539"
  },
  {
    "text": "thinking of implementing uh but it's a bit more detailed uh",
    "start": "1752539",
    "end": "1760340"
  },
  {
    "text": "the reason for it is what we need to understand the Border where",
    "start": "1760340",
    "end": "1767299"
  },
  {
    "text": "each layer of kubernetes Stack contains enough information of doing a proper",
    "start": "1767299",
    "end": "1774020"
  },
  {
    "text": "decision so right now we're communication between the couplet and runtime is kind of",
    "start": "1774020",
    "end": "1781760"
  },
  {
    "text": "imperative so cool blood dictates how certain things needs to be implemented",
    "start": "1781760",
    "end": "1786860"
  },
  {
    "text": "like the CPU said a bunch of other things or like",
    "start": "1786860",
    "end": "1791899"
  },
  {
    "text": "transforming a set of borders and so on the problem with that is what uh",
    "start": "1791899",
    "end": "1798919"
  },
  {
    "text": "it was okay five years ago when they had only runs the other runtime the current set of run times we have VM",
    "start": "1798919",
    "end": "1806899"
  },
  {
    "text": "based runtimes like Kata we have user leap based on times like guis and end",
    "start": "1806899",
    "end": "1812240"
  },
  {
    "text": "hours so all the assumptions what couplet has about how a continuous run",
    "start": "1812240",
    "end": "1817880"
  },
  {
    "text": "is not necessarily appropriate or not not necessarily with true so sum of information available only on",
    "start": "1817880",
    "end": "1825980"
  },
  {
    "text": "the runtime so with things what uh my team is trying",
    "start": "1825980",
    "end": "1831860"
  },
  {
    "text": "to do we are trying to make sure what certain information properly passed",
    "start": "1831860",
    "end": "1837320"
  },
  {
    "text": "between the couplet and runtimes and when utilizing uh how it's done and besides with CPU we have several other",
    "start": "1837320",
    "end": "1845600"
  },
  {
    "start": "1844000",
    "end": "2091000"
  },
  {
    "text": "activities like NRI I already mentioned uh but when we have a few things which",
    "start": "1845600",
    "end": "1852799"
  },
  {
    "text": "is related to class based resources or quality of service kind of resources so",
    "start": "1852799",
    "end": "1858860"
  },
  {
    "text": "it's cash it's memory bandages it's Google uh well to some degree memory",
    "start": "1858860",
    "end": "1865039"
  },
  {
    "text": "achieving can be represented as quality of service uh scenarios and so on and",
    "start": "1865039",
    "end": "1871340"
  },
  {
    "text": "another thing is device manager so like what uh Marlo just mentioned about the",
    "start": "1871340",
    "end": "1877820"
  },
  {
    "text": "GPU scheduling it's good it's it's a way how to utilize",
    "start": "1877820",
    "end": "1885860"
  },
  {
    "text": "uh GPU based on current device plugin API but the problem is what uh with",
    "start": "1885860",
    "end": "1893600"
  },
  {
    "text": "current device plugin apis contains a lot of uh",
    "start": "1893600",
    "end": "1899559"
  },
  {
    "text": "I wouldn't say give Zion mistakes but in inefficiencies uh if we are looking from",
    "start": "1899559",
    "end": "1906440"
  },
  {
    "text": "a current Hardware state-of-view so like it worked well if you have single",
    "start": "1906440",
    "end": "1912760"
  },
  {
    "text": "exclusive use of Hardware without any knowledge about internal resources no",
    "start": "1912760",
    "end": "1918140"
  },
  {
    "text": "shared usage and so on but as soon as we start to thinking like okay let's have one physical GPU or somehow accelerated",
    "start": "1918140",
    "end": "1925580"
  },
  {
    "text": "device to be shared let's think about what memory on it let's think about we internal topology of those accelerators",
    "start": "1925580",
    "end": "1932600"
  },
  {
    "text": "and so on and so forth Rose simply didn't work like what are different workarounds and what we",
    "start": "1932600",
    "end": "1939500"
  },
  {
    "text": "implemented with GPU device plugin for Intel accelerators it's also a set of workarounds uh and we have my own Google",
    "start": "1939500",
    "end": "1947539"
  },
  {
    "text": "for NVIDIA gpus we have very own but it's all not really extendable so what",
    "start": "1947539",
    "end": "1955520"
  },
  {
    "text": "we are working together with Nvidia and recently we also have uh people from our",
    "start": "1955520",
    "end": "1963140"
  },
  {
    "text": "projects joining so what one notable is akri uh like iot kind of devices Network",
    "start": "1963140",
    "end": "1970159"
  },
  {
    "text": "attaching devices uh we have those two initiatives like one is CGI container",
    "start": "1970159",
    "end": "1976580"
  },
  {
    "text": "device interface once again on the runtime level how we attach number container to the device yeah sorry how",
    "start": "1976580",
    "end": "1983659"
  },
  {
    "text": "how attaching device to a container all the Nifty details how how it should be done on a low level",
    "start": "1983659",
    "end": "1989840"
  },
  {
    "text": "but when the upper part like the couplet part is dynamic",
    "start": "1989840",
    "end": "1995899"
  },
  {
    "text": "resource allocation so this is revisiting how we how the user can uh",
    "start": "1995899",
    "end": "2002200"
  },
  {
    "text": "request the device so going from where previous model of",
    "start": "2002200",
    "end": "2007299"
  },
  {
    "text": "let's use extended resources and when try to have like all kind of combination",
    "start": "2007299",
    "end": "2012640"
  },
  {
    "text": "of those resources and when gpus scheduler extensions and so on uh we are",
    "start": "2012640",
    "end": "2018159"
  },
  {
    "text": "coming to interface similar to persistent volume claims so",
    "start": "2018159",
    "end": "2024519"
  },
  {
    "text": "your request with Device of particular class you specify a set of parameters to",
    "start": "2024519",
    "end": "2031539"
  },
  {
    "text": "to this device when device driver like the vendor specific code will be able to",
    "start": "2031539",
    "end": "2036700"
  },
  {
    "text": "understand what those parameters about how to get it properly allocated and",
    "start": "2036700",
    "end": "2042700"
  },
  {
    "text": "when hint to a scheduler where it will be allocated so like full flexibility for vendors to",
    "start": "2042700",
    "end": "2049540"
  },
  {
    "text": "provide like vendor logic specific to protocol device or actually even change",
    "start": "2049540",
    "end": "2055500"
  },
  {
    "text": "so this is like set of puzzles what Intel in overall working uh across",
    "start": "2056679",
    "end": "2063580"
  },
  {
    "text": "multiple teams in Work Source management from scheduler uh existing Cold Blood",
    "start": "2063580",
    "end": "2069460"
  },
  {
    "text": "things low level runtimes and well combinations of it",
    "start": "2069460",
    "end": "2076378"
  },
  {
    "text": "I think I will stop this web uh if there are additional questions I can pull out",
    "start": "2077080",
    "end": "2082960"
  },
  {
    "text": "some other slides or some other details",
    "start": "2082960",
    "end": "2087598"
  },
  {
    "text": "cool thank you um I don't know if anyone else has any questions",
    "start": "2090240",
    "end": "2096040"
  },
  {
    "start": "2091000",
    "end": "2553000"
  },
  {
    "text": "so I'm curious how the folks in the in the traditional Research Drive",
    "start": "2096040",
    "end": "2101619"
  },
  {
    "text": "management scheduler Community would interface with something like this so if you had somebody from the Altair or the",
    "start": "2101619",
    "end": "2108280"
  },
  {
    "text": "sketch MD community that wanted to um pass back to the scheduler to make",
    "start": "2108280",
    "end": "2114040"
  },
  {
    "text": "some to give it information about what components of a node what resources on",
    "start": "2114040",
    "end": "2119440"
  },
  {
    "text": "the Node it should have is there some is there something in what you're you're",
    "start": "2119440",
    "end": "2125020"
  },
  {
    "text": "proposing here that they would they would be using or that's a kind of",
    "start": "2125020",
    "end": "2130660"
  },
  {
    "text": "an entirely separate uh some respect so what are several things how we can",
    "start": "2130660",
    "end": "2138640"
  },
  {
    "text": "tackle it so right now recuplet is discovering uh our resources and when",
    "start": "2138640",
    "end": "2144400"
  },
  {
    "text": "doing my assumptions about how how those resources are present on the note it's",
    "start": "2144400",
    "end": "2150160"
  },
  {
    "text": "not necessarily much as what actually a runtime has and can runtime schedules",
    "start": "2150160",
    "end": "2156420"
  },
  {
    "text": "uh but what's one one side of the story I will come back to it the second part",
    "start": "2156420",
    "end": "2162040"
  },
  {
    "text": "of the resources is uh extendable resources and here we have two variants",
    "start": "2162040",
    "end": "2168339"
  },
  {
    "text": "how it can be uh announced to be uh like to a scheduler like one is uh device",
    "start": "2168339",
    "end": "2175180"
  },
  {
    "text": "plugins so device plugin says I have this amount of instances of particular",
    "start": "2175180",
    "end": "2180400"
  },
  {
    "text": "resource type uh second variant is what uh you can",
    "start": "2180400",
    "end": "2187540"
  },
  {
    "text": "patch of an old object and say what this node object has this amount of this resource allocatable",
    "start": "2187540",
    "end": "2194320"
  },
  {
    "text": "and when couplet will do with simple accounting how many ports are using this particular resource",
    "start": "2194320",
    "end": "2200740"
  },
  {
    "text": "to help with what we have NFD so like our GPU device plugin is working",
    "start": "2200740",
    "end": "2208780"
  },
  {
    "text": "together with NFD to actually uh automate with announcing of those",
    "start": "2208780",
    "end": "2214839"
  },
  {
    "text": "resources so like for example like this melee CPU or melee GPU parts of of a GPU",
    "start": "2214839",
    "end": "2223660"
  },
  {
    "text": "or like a GPU memory is handled by by NFD plugin",
    "start": "2223660",
    "end": "2230980"
  },
  {
    "text": "but as I mentioned it's most of it is like workarounds for for",
    "start": "2230980",
    "end": "2236260"
  },
  {
    "text": "current design or device buttons apis or current design of uh extendable resources",
    "start": "2236260",
    "end": "2242500"
  },
  {
    "text": "uh with things what I mentioned uh like this Dynamic resource allocation",
    "start": "2242500",
    "end": "2250420"
  },
  {
    "text": "uh it it has similar setup like uh storage drivers so you have a cluster",
    "start": "2250420",
    "end": "2256660"
  },
  {
    "text": "level components which works together with uh scheduler and you have a node component which is responsible to",
    "start": "2256660",
    "end": "2263320"
  },
  {
    "text": "actually attach a device to a container and work together with runtimes to do it",
    "start": "2263320",
    "end": "2269020"
  },
  {
    "text": "so it it will be the part of this cluster level component to to talk with the",
    "start": "2269020",
    "end": "2276040"
  },
  {
    "text": "scheduler to make sure what the resources are available and can be consumed for report and provide with",
    "start": "2276040",
    "end": "2282820"
  },
  {
    "text": "apology information on which nodes these resources will be available",
    "start": "2282820",
    "end": "2288660"
  },
  {
    "text": "um regarding the Google at runtime part um",
    "start": "2288820",
    "end": "2294640"
  },
  {
    "text": "it's long long way to actually get there but",
    "start": "2294640",
    "end": "2300700"
  },
  {
    "text": "uh what we eventually need to do is what we need to remove this discrepancy",
    "start": "2300700",
    "end": "2305859"
  },
  {
    "text": "between the couplet knowledge and runtime knowledge so it means what at some point we need",
    "start": "2305859",
    "end": "2311140"
  },
  {
    "text": "to release it to a protocol how a couplet and runtime are talking about the resources",
    "start": "2311140",
    "end": "2316960"
  },
  {
    "text": "so right now in this class based POS resources what we have uh",
    "start": "2316960",
    "end": "2324339"
  },
  {
    "text": "we have a CRI messages where runtime talk tells to the couplet",
    "start": "2324339",
    "end": "2330780"
  },
  {
    "text": "which quality of service classes are available and what like what types of",
    "start": "2330780",
    "end": "2337000"
  },
  {
    "text": "curious classes available and what is the potential values are available so we know the kublet Can report it into",
    "start": "2337000",
    "end": "2344740"
  },
  {
    "text": "an old status and then the scheduler can consume it to make a scheduling decision",
    "start": "2344740",
    "end": "2350400"
  },
  {
    "text": "so it's it's similar model what we have right now for the native resources one",
    "start": "2350400",
    "end": "2356020"
  },
  {
    "text": "of the difference is what Google had not discovering it googleb gets it from from varanta",
    "start": "2356020",
    "end": "2362140"
  },
  {
    "text": "foreign so if we are talking about Resource",
    "start": "2362140",
    "end": "2368140"
  },
  {
    "text": "Management plugins like regardless regardless on which level of couplet or on runtime so now later we will need to",
    "start": "2368140",
    "end": "2374800"
  },
  {
    "text": "have exactly the same interface so couplet or sorry plugin should be able",
    "start": "2374800",
    "end": "2379900"
  },
  {
    "text": "to tell to runtime or to couplet uh what kind of additional resources it might be available so it can be used in",
    "start": "2379900",
    "end": "2387180"
  },
  {
    "text": "scheduling decision and not obviously no status and that's part of the work is",
    "start": "2387180",
    "end": "2392740"
  },
  {
    "text": "figuring out how to have Dynamic resources as part of us right",
    "start": "2392740",
    "end": "2400619"
  },
  {
    "text": "so they do need to be advertised as a scheduler but some of that work is figuring out how",
    "start": "2401920",
    "end": "2408579"
  },
  {
    "text": "yeah my biggest problem with all of those how is what we have a lot of",
    "start": "2408579",
    "end": "2413619"
  },
  {
    "text": "Legacy code and we have a lot of users who are relying on this Legacy so",
    "start": "2413619",
    "end": "2419440"
  },
  {
    "text": "to change something where we need to be very careful how how to not to break",
    "start": "2419440",
    "end": "2425320"
  },
  {
    "text": "with things uh previously it was a huge roadblock in",
    "start": "2425320",
    "end": "2431200"
  },
  {
    "text": "in terms of dockersham because well",
    "start": "2431200",
    "end": "2437160"
  },
  {
    "text": "you you still needed to to take care of uh well the docker API which was quite",
    "start": "2437200",
    "end": "2443200"
  },
  {
    "text": "simple now when when dockershim is removed we have a bit more freedom like how we can",
    "start": "2443200",
    "end": "2449320"
  },
  {
    "text": "evolve a CRI protocol",
    "start": "2449320",
    "end": "2452579"
  },
  {
    "text": "awesome thank you um I don't understand time for one more quick question if there is one",
    "start": "2460300",
    "end": "2467579"
  },
  {
    "text": "of the wires I think that's probably that's a good time actually five two so thank you very much Marlo and and Sasha",
    "start": "2472900",
    "end": "2478960"
  },
  {
    "text": "for coming along I think a couple of people has dropped so sorry about that but we've got it all recorded and we'll share it so thank you very much for your",
    "start": "2478960",
    "end": "2484960"
  },
  {
    "text": "time it's really really good stuff yeah and I can send out the slides after Sasha if you want to add to mine before",
    "start": "2484960",
    "end": "2491020"
  },
  {
    "text": "I I send I'll send to you I just sent in my chat with a link to a",
    "start": "2491020",
    "end": "2496540"
  },
  {
    "text": "session or what we had on a batch on HBC a day so like for slides",
    "start": "2496540",
    "end": "2502359"
  },
  {
    "text": "what I showed it's attach it where and I think recordings also should be available",
    "start": "2502359",
    "end": "2508500"
  },
  {
    "text": "yeah that's great yeah and any slides just just Chuck them into the um into the slack Channel",
    "start": "2508500",
    "end": "2514060"
  },
  {
    "text": "all right that sounds great thank you brilliant okay thanks very much um so yeah that's probably it for today our",
    "start": "2514060",
    "end": "2520660"
  },
  {
    "text": "next session is going to be actually 6th of July I think it was in the agenda previously it was down the 29th of",
    "start": "2520660",
    "end": "2527800"
  },
  {
    "text": "uh June but it won't be then because we've already done two sessions in June so yeah 6th of July will be the next",
    "start": "2527800",
    "end": "2533980"
  },
  {
    "text": "time and we're gonna do uh have a session I think on psyllium and ebpf so yeah thank you thank you again uh Sasha",
    "start": "2533980",
    "end": "2541359"
  },
  {
    "text": "Marlo and uh see you all next time thank you thank you bye-bye",
    "start": "2541359",
    "end": "2548579"
  }
]