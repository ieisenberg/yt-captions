[
  {
    "text": "uh hello everyone my name is Marty Vegas and I'm representing seek Auto scaling",
    "start": "1280",
    "end": "6839"
  },
  {
    "text": "group here first of all Let Me Explain Who We Are",
    "start": "6839",
    "end": "12240"
  },
  {
    "text": "seek Auto scaling is a group of people within the kubernetes community focusing on reducing the cost of running your",
    "start": "12240",
    "end": "19440"
  },
  {
    "text": "clusters of course we cannot regurgitate your contract with your cloud provider",
    "start": "19440",
    "end": "24840"
  },
  {
    "text": "we cannot give you discounts codes to buy cheaper Hardware we don't optimize",
    "start": "24840",
    "end": "30060"
  },
  {
    "text": "your optimize applications either but we can make sure that you always",
    "start": "30060",
    "end": "35340"
  },
  {
    "text": "have just enough instances of your applications they are properly sized and",
    "start": "35340",
    "end": "40379"
  },
  {
    "text": "that the computing power provided to run those application tightly fits their needs in other ways that you are not",
    "start": "40379",
    "end": "46800"
  },
  {
    "text": "wasting resources and at the same time your applications are not overloaded",
    "start": "46800",
    "end": "52980"
  },
  {
    "text": "but having just enough resources doesn't only improve your efficiency it also",
    "start": "52980",
    "end": "58020"
  },
  {
    "text": "helps with the ease of the deployment development and reliability as your environment will adjust to the changes",
    "start": "58020",
    "end": "65400"
  },
  {
    "text": "made to your applications foreign owns a bunch of components that can help",
    "start": "65400",
    "end": "72659"
  },
  {
    "text": "you to execute and mention goals during this presentation I will very briefly explain what these components do so that",
    "start": "72659",
    "end": "80159"
  },
  {
    "text": "people so that the people who came here for some introductions get it and then I will describe what are the newest",
    "start": "80159",
    "end": "86280"
  },
  {
    "text": "developments there the first component I want to talk is horizontal Port Auto scalar",
    "start": "86280",
    "end": "93799"
  },
  {
    "text": "horizontal scalar is based on metrics that Express the load and that the application gets it can be either some",
    "start": "93799",
    "end": "101100"
  },
  {
    "text": "real CPU usage or something more custom like the number of queries per second on",
    "start": "101100",
    "end": "106439"
  },
  {
    "text": "or other codes like value provided by the user HPA takes this value take the",
    "start": "106439",
    "end": "112500"
  },
  {
    "text": "value of this metric Compares it with the user defined Target and depending on this comparison adds or removes replicas",
    "start": "112500",
    "end": "120119"
  },
  {
    "text": "hoping to move the metrics towards the desired value let's take a look on how does it work in",
    "start": "120119",
    "end": "126060"
  },
  {
    "text": "practice here the metric will be utilization understood as the ratio between the",
    "start": "126060",
    "end": "132540"
  },
  {
    "text": "actual CPU usage of a pod and the amount of CPU that the Pod requested",
    "start": "132540",
    "end": "138480"
  },
  {
    "text": "as you can see we have here four parts that are getting quite a bit of traffic",
    "start": "138480",
    "end": "144720"
  },
  {
    "text": "they are using around 90 of their requests and if the traffic increases",
    "start": "144720",
    "end": "149760"
  },
  {
    "text": "they may not have enough resources to handle the requests however if we add yet another",
    "start": "149760",
    "end": "155819"
  },
  {
    "text": "application replica then the load will spread across more instances and the",
    "start": "155819",
    "end": "160980"
  },
  {
    "text": "average airport utilization will get lower and will be closer to the desired Target",
    "start": "160980",
    "end": "167220"
  },
  {
    "text": "if the situation is opposite and we have many instances that are not that much utilized",
    "start": "167220",
    "end": "172920"
  },
  {
    "text": "removing one of them will move the average perpet utilization closer to the",
    "start": "172920",
    "end": "179040"
  },
  {
    "text": "desired Target and that is what horizontal pod Auto scalar does",
    "start": "179040",
    "end": "186440"
  },
  {
    "text": "HPA has been around for quite some time we reached a stable version V2 back in",
    "start": "186440",
    "end": "193319"
  },
  {
    "text": "late 2001. to 2021 it means that with the",
    "start": "193319",
    "end": "198900"
  },
  {
    "text": "accordance with the API deprecation policy the better versions are about to be removed V2 beta 1 was removed in 1.25",
    "start": "198900",
    "end": "208200"
  },
  {
    "text": "and Vito beta 2 will be removed in 1.26 if you are still using the old",
    "start": "208200",
    "end": "214920"
  },
  {
    "text": "definitions of HPA please update them as soon as possible because with your next",
    "start": "214920",
    "end": "220739"
  },
  {
    "text": "cluster update they may stop working in 1.26 we finally made the controller",
    "start": "220739",
    "end": "228659"
  },
  {
    "text": "multi-threaders it's quite embarrassing that it took us that long but anywhere here it is increasing concurrency will",
    "start": "228659",
    "end": "236280"
  },
  {
    "text": "help you with dealing with large number of HP objects especially if you are using custom metrics as obtaining those",
    "start": "236280",
    "end": "243540"
  },
  {
    "text": "is really time consuming some users are heavily over",
    "start": "243540",
    "end": "249680"
  },
  {
    "text": "subscribing their cluster and use targets above 100 of utilization up to 1",
    "start": "249680",
    "end": "256859"
  },
  {
    "text": "to 26 we were prone to some Corner case use issues but with 126 this problem",
    "start": "256859",
    "end": "264360"
  },
  {
    "text": "should not be fixed we still hope to finally land scale to zero support for custom metrics in the",
    "start": "264360",
    "end": "271740"
  },
  {
    "text": "API with 1 to 26 and post 126 we are",
    "start": "271740",
    "end": "277800"
  },
  {
    "text": "planning to have a dry run mode which will allow it to test you test your HPA with different metrics without actually",
    "start": "277800",
    "end": "285120"
  },
  {
    "text": "actuating the changes okay HP is about Auto scaling a single",
    "start": "285120",
    "end": "293639"
  },
  {
    "text": "deployment or to be precise an object that exposes a scale sub-resource the",
    "start": "293639",
    "end": "300120"
  },
  {
    "text": "apis allows only to provide a single Target what if you have a more complex",
    "start": "300120",
    "end": "305759"
  },
  {
    "text": "use case well and then you have a problem a problem that you share with other",
    "start": "305759",
    "end": "311580"
  },
  {
    "text": "kubernetes users like how to ensure more or less equal spreading of pods in free",
    "start": "311580",
    "end": "317759"
  },
  {
    "text": "zones of a region how to guarantee that in case of a zonal failure pods will",
    "start": "317759",
    "end": "323220"
  },
  {
    "text": "automatically go to the other zones and move back when the zone is back online",
    "start": "323220",
    "end": "329220"
  },
  {
    "text": "how to split pods let's say in 70 30 ratio between spot VMS and regular",
    "start": "329220",
    "end": "337020"
  },
  {
    "text": "on-demand VMS how to make sure that pods consume the nodes with negotiated rate",
    "start": "337020",
    "end": "342360"
  },
  {
    "text": "fares and then if they are not enough go to the others and how to make all of",
    "start": "342360",
    "end": "347940"
  },
  {
    "text": "these deployments horizontally and vertically Auto scaled and make sure that they work great with cluster Auto",
    "start": "347940",
    "end": "355259"
  },
  {
    "text": "scaler well so far there was no good answer to these questions so we decided",
    "start": "355259",
    "end": "361380"
  },
  {
    "text": "to provide some solution to the problem as I think we are bringing a new tool a",
    "start": "361380",
    "end": "369300"
  },
  {
    "text": "new controller and a new crd based API the central element of this API will be",
    "start": "369300",
    "end": "374699"
  },
  {
    "text": "called balancer the balancer object will have pointers to multiple deployments or",
    "start": "374699",
    "end": "380699"
  },
  {
    "text": "anything that exposes this mentioned skill sub resource each of these deployments may have a different node",
    "start": "380699",
    "end": "387780"
  },
  {
    "text": "selector different tolerances possible even different configurations but what they have in common is that the",
    "start": "387780",
    "end": "394919"
  },
  {
    "text": "pods from these deployments build kind of one service or one application balancer main task will be to properly",
    "start": "394919",
    "end": "402300"
  },
  {
    "text": "size these deployments each balancer will have a placement",
    "start": "402300",
    "end": "407940"
  },
  {
    "text": "policy According to which it will distribute replicas across its Target for example with the proportional policy",
    "start": "407940",
    "end": "415020"
  },
  {
    "text": "of 7030 it will distribute a 10 replicas like this 7 will go to the first one the",
    "start": "415020",
    "end": "423300"
  },
  {
    "text": "transom spot and free to the other one that runs on regular on-demand VMS",
    "start": "423300",
    "end": "429960"
  },
  {
    "text": "okay now what if the cloud provider Runs Out spot instances and start to preempt",
    "start": "429960",
    "end": "437180"
  },
  {
    "text": "virtual machines on which these seven parts are running as a result killing",
    "start": "437180",
    "end": "442259"
  },
  {
    "text": "the pods well after all configurable timeout balancer notices that the instances on",
    "start": "442259",
    "end": "449699"
  },
  {
    "text": "preemptable nodes are not going back and increases the size of the second deployment in order to account",
    "start": "449699",
    "end": "456740"
  },
  {
    "text": "for replicas that are failing on the first deployment and it will still keep",
    "start": "456740",
    "end": "462979"
  },
  {
    "text": "the first deployment at size 7 why chances are that are you are running a",
    "start": "462979",
    "end": "470280"
  },
  {
    "text": "cluster autoscaler in case of cluster Auto scalar it will keep on trying to bring nodes for those seven pods",
    "start": "470280",
    "end": "477180"
  },
  {
    "text": "targeted at spot instances eventually it will succeed and the nodes will be provided and the pods started there",
    "start": "477180",
    "end": "485099"
  },
  {
    "text": "and the balancer will decrease the number of replicas in the second deployment and the whole thing will be",
    "start": "485099",
    "end": "491099"
  },
  {
    "text": "in the desired configuration again if you want to Auto scale the",
    "start": "491099",
    "end": "496819"
  },
  {
    "text": "deployments you point your HPA at the balancer balancer exposes the very same",
    "start": "496819",
    "end": "502699"
  },
  {
    "text": "skill sub-resource as individual deployment so it will work out of the box with",
    "start": "502699",
    "end": "508620"
  },
  {
    "text": "horizontal potato scalar and vertical potato scaler so what is the status of balancer",
    "start": "508620",
    "end": "515339"
  },
  {
    "text": "project which reach an agreement about the API within six the code is almost",
    "start": "515339",
    "end": "521279"
  },
  {
    "text": "done in the Google internal repository yeah Google in actually wanted to start",
    "start": "521279",
    "end": "527279"
  },
  {
    "text": "it on gke first but change your mind in flight and it will be open source in",
    "start": "527279",
    "end": "532500"
  },
  {
    "text": "November assuming that all of the open source code reviews will go smoothly so",
    "start": "532500",
    "end": "538560"
  },
  {
    "text": "we expect the initial release sometime this year the next thing that seek Auto scaling",
    "start": "538560",
    "end": "545700"
  },
  {
    "text": "ohms is a vertical potato scaler vertical portal to scalar helps you to",
    "start": "545700",
    "end": "552600"
  },
  {
    "text": "get the pot size right it is based on actual historically resource usage of the pods it looks at CPU memory usage",
    "start": "552600",
    "end": "560580"
  },
  {
    "text": "and pays attention to out of memory events it recommends the pot or actually",
    "start": "560580",
    "end": "566580"
  },
  {
    "text": "its container sizes to keep the real usage within the requested capacity",
    "start": "566580",
    "end": "572519"
  },
  {
    "text": "so if a pod is using something like 95 percent of its current requests and",
    "start": "572519",
    "end": "578580"
  },
  {
    "text": "maybe even occasionally going above 100 vpa will increase the post slash",
    "start": "578580",
    "end": "584040"
  },
  {
    "text": "container size in this if this situation is opposite it will decrease the pot size",
    "start": "584040",
    "end": "591000"
  },
  {
    "text": "and vertical potato scalar has also been around for a while but despite that seek managed to make a",
    "start": "591000",
    "end": "598500"
  },
  {
    "text": "couple of important improvements the biggest one is probably the ability to have multiple recommenders running at",
    "start": "598500",
    "end": "605760"
  },
  {
    "text": "the same time each recommender which recommends what should be the pot size may have a different configuration or",
    "start": "605760",
    "end": "612540"
  },
  {
    "text": "even a completely different algorithm and you can decide which one to use by",
    "start": "612540",
    "end": "617580"
  },
  {
    "text": "providing its name in the vpa object specification to support this feature you can now",
    "start": "617580",
    "end": "623459"
  },
  {
    "text": "provide the percentiles used by the standard vpa recommender while the default 90 percentile plus a little bit",
    "start": "623459",
    "end": "630600"
  },
  {
    "text": "of buffer works for quite a lot of user you may want to increase this should",
    "start": "630600",
    "end": "635640"
  },
  {
    "text": "your workload be more spiky or and you care about latency or maybe decrease it",
    "start": "635640",
    "end": "642360"
  },
  {
    "text": "if you want to over subscribe to your nodes more soon we hope to have an ability to keep",
    "start": "642360",
    "end": "649140"
  },
  {
    "text": "a fixed ratio between a CPU and memory we want to limit the direction of the",
    "start": "649140",
    "end": "654360"
  },
  {
    "text": "updates so that for example containers and pod only scale up and we really",
    "start": "654360",
    "end": "659760"
  },
  {
    "text": "really hope to have this kubernetes in play Spot updates landed and then we",
    "start": "659760",
    "end": "666540"
  },
  {
    "text": "will use it in vpa so that it doesn't restart your pause while performing the updates",
    "start": "666540",
    "end": "673740"
  },
  {
    "text": "the last component I would like to talk is cluster rotoscaler",
    "start": "673740",
    "end": "679800"
  },
  {
    "text": "cluster Auto scaler ensures that your pod always have a place to run it",
    "start": "679800",
    "end": "685260"
  },
  {
    "text": "provides new nodes for the pods that could not be scheduled and removes nodes that are not that needed anymore it",
    "start": "685260",
    "end": "692459"
  },
  {
    "text": "doesn't use any metric it uses spot declared request and a lot of scheduling simulation to tell what would happen if",
    "start": "692459",
    "end": "701160"
  },
  {
    "text": "some actions were made let's take a look at this in more detail here we've have",
    "start": "701160",
    "end": "709320"
  },
  {
    "text": "four nodes with pots if a new Potter arrive it can be placed on the third node but if the situation",
    "start": "709320",
    "end": "717240"
  },
  {
    "text": "is different and all nodes are kind of busy then the green pot has no place to go the scheduler marches as",
    "start": "717240",
    "end": "724140"
  },
  {
    "text": "unschedulable cluster Auto scalar waits for this signal it makes some simulation and",
    "start": "724140",
    "end": "729600"
  },
  {
    "text": "notices that if one extra note was added then the Green Pod could go there so it",
    "start": "729600",
    "end": "735779"
  },
  {
    "text": "talks to your cloud provider and resizes the cluster accordingly the new note shows up and scheduler",
    "start": "735779",
    "end": "742620"
  },
  {
    "text": "places the port right there and let's take a look at the different",
    "start": "742620",
    "end": "748019"
  },
  {
    "text": "setup here we have two nodes that are not used to its full capabilities but if",
    "start": "748019",
    "end": "754380"
  },
  {
    "text": "we move the green part to the third node then the fifth note could be deleted",
    "start": "754380",
    "end": "760079"
  },
  {
    "text": "without any problem lowering your Cloud bill so what's new in class rotoscaler the",
    "start": "760079",
    "end": "768480"
  },
  {
    "text": "biggest changes is making the scale down process faster it includes better",
    "start": "768480",
    "end": "774000"
  },
  {
    "text": "handling of pending parts that don't block scale down anymore and most importantly ability for cluster",
    "start": "774000",
    "end": "781440"
  },
  {
    "text": "autoscaler to scare to handle multiple node drainings or migration at the same",
    "start": "781440",
    "end": "787800"
  },
  {
    "text": "times previously cluster autoscaler could delete only could delete empty",
    "start": "787800",
    "end": "793740"
  },
  {
    "text": "nodes in bulk but if notes were not that empty like on the examples that I showed",
    "start": "793740",
    "end": "798959"
  },
  {
    "text": "you before it had to migrate pods from them and Link only one node at a time",
    "start": "798959",
    "end": "806880"
  },
  {
    "text": "with the changes that we are making to the scale down algorithm we hope to get",
    "start": "806880",
    "end": "812339"
  },
  {
    "text": "significant scale down speed up and more savings for you as the unneeded nodes will go away faster",
    "start": "812339",
    "end": "818639"
  },
  {
    "text": "we are expanding the plugability of cluster Auto scalar by allowing you to have both grpc cloud provider and",
    "start": "818639",
    "end": "826500"
  },
  {
    "text": "expander we'll talk about geopysical provider in a moment and we are working on better clouds better",
    "start": "826500",
    "end": "835380"
  },
  {
    "text": "much support her use cases by integrating cluster",
    "start": "835380",
    "end": "841440"
  },
  {
    "text": "autoscaler better with job and queue that has been mentioned like a bit",
    "start": "841440",
    "end": "846480"
  },
  {
    "text": "before on six scheduling updates so as I said we wanted to tell",
    "start": "846480",
    "end": "852480"
  },
  {
    "text": "you more about the grpc cloud provider but unfortunately Diego the author of",
    "start": "852480",
    "end": "857639"
  },
  {
    "text": "the change could not be here in person so I have a video of him instead",
    "start": "857639",
    "end": "862680"
  },
  {
    "text": "so let me play it for you hello I am Diego bonfili SRI axis League",
    "start": "862680",
    "end": "869880"
  },
  {
    "text": "and with this presentation I will talk about the new erpc cloud provider so there's a plugin system to implement",
    "start": "869880",
    "end": "876480"
  },
  {
    "text": "Cloud providers as a separate process from the cluster of the scaler",
    "start": "876480",
    "end": "882480"
  },
  {
    "text": "please let me do us more Refresh on what a cloud provider is in the context of",
    "start": "882480",
    "end": "887519"
  },
  {
    "text": "the cluster of the scalar all CA from now on a cluster of the scalar adds nodes when",
    "start": "887519",
    "end": "894779"
  },
  {
    "text": "pods cannot fit on a current kubernetes nodes or remove nodes when resources are",
    "start": "894779",
    "end": "900540"
  },
  {
    "text": "underutilized the logic behind the scaling decisions are common for any environment but at",
    "start": "900540",
    "end": "907800"
  },
  {
    "text": "the end the ca needs to interact with the specific underlying infrastructure like create a new hosts remove a VM give",
    "start": "907800",
    "end": "916680"
  },
  {
    "text": "me the list of the current distances and so on and this is done by calling apis",
    "start": "916680",
    "end": "921839"
  },
  {
    "text": "for the specific cloud provider where the cluster is running the classroom skills supports many Club",
    "start": "921839",
    "end": "929160"
  },
  {
    "text": "providers I think at the moment almost 50 and a specific implementation for",
    "start": "929160",
    "end": "934560"
  },
  {
    "text": "each cloud provider is coded in the cluster to scale itself is abstracted by",
    "start": "934560",
    "end": "940380"
  },
  {
    "text": "a couple of golden interfaces and runs in the same process of the cluster Auto scale",
    "start": "940380",
    "end": "946560"
  },
  {
    "text": "so yeah in the context of a customer to",
    "start": "946560",
    "end": "951660"
  },
  {
    "text": "scare a cloud provider is the specific implementation that lets the cluster Auto scale talk to the cloud provider",
    "start": "951660",
    "end": "959040"
  },
  {
    "text": "apis what do you need to do at the moment if",
    "start": "959040",
    "end": "964620"
  },
  {
    "text": "you are a club provider and you want the cluster to scale to work with your services",
    "start": "964620",
    "end": "970980"
  },
  {
    "text": "you need to Fork the cluster to scare a code Implement a couple of gulang interfaces with your custom logic this",
    "start": "970980",
    "end": "978660"
  },
  {
    "text": "most probably makes use of specific couple provider apis and you have to",
    "start": "978660",
    "end": "984300"
  },
  {
    "text": "change some pieces of course to integrate your platform provider",
    "start": "984300",
    "end": "989880"
  },
  {
    "text": "if then you want to contribute back to the community and you want your fork to",
    "start": "989880",
    "end": "994980"
  },
  {
    "text": "be merged back to the official faster Auto scale you must respect some rules like you cannot add new dependencies at",
    "start": "994980",
    "end": "1002959"
  },
  {
    "text": "top level vendor I mean in the go.mod file of the project",
    "start": "1002959",
    "end": "1008120"
  },
  {
    "text": "and this is because of the problem with version conflicts in transitive dependencies",
    "start": "1008120",
    "end": "1013339"
  },
  {
    "text": "they are hard to understand and create problems with the version upgrades and then of course you have to wait for",
    "start": "1013339",
    "end": "1020420"
  },
  {
    "text": "a member of the kubernetes organization to review your code if you are not an",
    "start": "1020420",
    "end": "1025459"
  },
  {
    "text": "official maintainer you can understand that it could be",
    "start": "1025459",
    "end": "1031938"
  },
  {
    "text": "useful to have a pluggable cloud provider something external to the classroom to scalar core that implements",
    "start": "1031939",
    "end": "1039380"
  },
  {
    "text": "only the things specific to our cloud provider and leaves the core logic I",
    "start": "1039380",
    "end": "1045079"
  },
  {
    "text": "mean the scaling decisions on the cluster Autos here with the cloud provider Logics move it",
    "start": "1045079",
    "end": "1051380"
  },
  {
    "text": "to a different service then there is no need anymore for a fork of the ca",
    "start": "1051380",
    "end": "1057380"
  },
  {
    "text": "if of course you cannot Implement uh your top provider as an official couple",
    "start": "1057380",
    "end": "1063740"
  },
  {
    "text": "of other in the project itself and this is good because",
    "start": "1063740",
    "end": "1069020"
  },
  {
    "text": "uh it's simplified maintenance I mean the maintenance of forked projects is",
    "start": "1069020",
    "end": "1074720"
  },
  {
    "text": "not always straightforward for example you need to keep track of Upstream updates you need to integrate",
    "start": "1074720",
    "end": "1080960"
  },
  {
    "text": "them and then you need to carefully get them to understand if new dependencies back your uh",
    "start": "1080960",
    "end": "1088960"
  },
  {
    "text": "also now you can release new versions of your cloud provider whenever you want",
    "start": "1088960",
    "end": "1094160"
  },
  {
    "text": "without waiting for a new official CA release you can use libraries that you could not",
    "start": "1094160",
    "end": "1100940"
  },
  {
    "text": "use before for example because they are not under the Apache license and you can also use your own language",
    "start": "1100940",
    "end": "1107660"
  },
  {
    "text": "of choice if you want to avoid column for some reason this is no difference from many other",
    "start": "1107660",
    "end": "1114980"
  },
  {
    "text": "kubernetes components that are now structured as plugins think of the cni",
    "start": "1114980",
    "end": "1121160"
  },
  {
    "text": "the container storage interface the cluster API for provider implementers",
    "start": "1121160",
    "end": "1126919"
  },
  {
    "text": "and so on cluster of the scale now as a new plugin",
    "start": "1126919",
    "end": "1133520"
  },
  {
    "text": "system for cloud providers so now you can be out of three Cloud",
    "start": "1133520",
    "end": "1139280"
  },
  {
    "text": "providers if you use it the cluster to scale retains all the core Logic for scaling",
    "start": "1139280",
    "end": "1146419"
  },
  {
    "text": "of the specific piece of code for a cloud provider is served by occipate",
    "start": "1146419",
    "end": "1151940"
  },
  {
    "text": "service over the network the communication between the CIA and",
    "start": "1151940",
    "end": "1158240"
  },
  {
    "text": "external provider service is performed via the GRTC",
    "start": "1158240",
    "end": "1164000"
  },
  {
    "text": "technically the plugin system is yet another in three cloud provider",
    "start": "1164000",
    "end": "1169580"
  },
  {
    "text": "this nuclear provider called external grpc component implements",
    "start": "1169580",
    "end": "1175460"
  },
  {
    "text": "the golang interfaces like all the other in free Cloud providers but then it",
    "start": "1175460",
    "end": "1181520"
  },
  {
    "text": "actually wraps these function calls and send them to an external service via",
    "start": "1181520",
    "end": "1188179"
  },
  {
    "text": "grpc so the core CA takes the part of the",
    "start": "1188179",
    "end": "1193700"
  },
  {
    "text": "grpc client here an external provider service Acts as the service server sorry",
    "start": "1193700",
    "end": "1202539"
  },
  {
    "text": "okay we have a pluggable system and now we want to use it",
    "start": "1204039",
    "end": "1209960"
  },
  {
    "text": "let's talk about what you need to do to create a new cloud provider using this plugin system",
    "start": "1209960",
    "end": "1216500"
  },
  {
    "text": "let's digress for a moment on some general requirements that whole Cloud providers needs to meet both in three",
    "start": "1216500",
    "end": "1224179"
  },
  {
    "text": "Cloud providers and a new out of three was created with the plugin system",
    "start": "1224179",
    "end": "1230140"
  },
  {
    "text": "fears in the cluster of the scale there is a concept of node groups",
    "start": "1230140",
    "end": "1235580"
  },
  {
    "text": "to scale the CR CA works with groups no single nodes this means that when it",
    "start": "1235580",
    "end": "1243320"
  },
  {
    "text": "needs to add nodes the ca actually choose a group to scale out",
    "start": "1243320",
    "end": "1248480"
  },
  {
    "text": "so it's important that all nodes within a group have the same machine type same",
    "start": "1248480",
    "end": "1254419"
  },
  {
    "text": "labels same things and in this are in the same availability zone for the ca to properly decide which",
    "start": "1254419",
    "end": "1262640"
  },
  {
    "text": "group to be these does not mean that if a cloud provider does not provide not groups",
    "start": "1262640",
    "end": "1269660"
  },
  {
    "text": "apis then cannot be integrated in the CI for example your implementation can can",
    "start": "1269660",
    "end": "1276380"
  },
  {
    "text": "fake those groups but it helps when I was killing instead the ca",
    "start": "1276380",
    "end": "1282320"
  },
  {
    "text": "deletes specific nodes in another group so cloud provider must provide a way to",
    "start": "1282320",
    "end": "1288140"
  },
  {
    "text": "delete a single node and also resize the group at the same time",
    "start": "1288140",
    "end": "1293659"
  },
  {
    "text": "and also for this to work there must be a way to correlate our kubernetes node",
    "start": "1293659",
    "end": "1300260"
  },
  {
    "text": "to the actual host of the cloud provider and usually there is a node field for",
    "start": "1300260",
    "end": "1306620"
  },
  {
    "text": "this called provider the where Cloud providers add information to correlate",
    "start": "1306620",
    "end": "1313220"
  },
  {
    "text": "kubernetes nodes to cloud provider hosts",
    "start": "1313220",
    "end": "1318400"
  },
  {
    "text": "all these requirements I said are important because if you want to create a cloud provider you will need to",
    "start": "1318620",
    "end": "1325280"
  },
  {
    "text": "implement apis that assume these Concepts you can see a summary of rpcs",
    "start": "1325280",
    "end": "1330980"
  },
  {
    "text": "apis on the left here they protophile as dogs describing what",
    "start": "1330980",
    "end": "1336860"
  },
  {
    "text": "single rpcs and messages are used for so to write a couple provider as a",
    "start": "1336860",
    "end": "1343520"
  },
  {
    "text": "plugin pick your language create a grpc server that implements that for the file",
    "start": "1343520",
    "end": "1350179"
  },
  {
    "text": "write the logic for your car provider that most probably will in top perform cause the cloud provider itself and",
    "start": "1350179",
    "end": "1357620"
  },
  {
    "text": "expose the server this is very important to use mtls for this even if you can switch it off for",
    "start": "1357620",
    "end": "1364640"
  },
  {
    "text": "development purposes because if you look at the rpcs you are essentially giving",
    "start": "1364640",
    "end": "1370940"
  },
  {
    "text": "the permission to create and delete notes in your cluster to whoever is able to connect to this server so please use",
    "start": "1370940",
    "end": "1378820"
  },
  {
    "text": "mtls in production environments",
    "start": "1378820",
    "end": "1383380"
  },
  {
    "text": "so we now have a nice way to the couple Cloud providers from the core CA",
    "start": "1384020",
    "end": "1390500"
  },
  {
    "text": "here I report some things to know before using the plugin system one thing to take into consideration is",
    "start": "1390500",
    "end": "1397340"
  },
  {
    "text": "performances are of course local and so they are as",
    "start": "1397340",
    "end": "1404299"
  },
  {
    "text": "fast as they can get with external grpc cloud provider calls now go over the network",
    "start": "1404299",
    "end": "1410960"
  },
  {
    "text": "caching for rpcs has been implemented everywhere possible of course but still",
    "start": "1410960",
    "end": "1417080"
  },
  {
    "text": "at the moment the plugin system is not has not been tested",
    "start": "1417080",
    "end": "1423380"
  },
  {
    "text": "yet for very large clusters think class with thousands of nodes so",
    "start": "1423380",
    "end": "1428900"
  },
  {
    "text": "take these into account another thing to know is the external",
    "start": "1428900",
    "end": "1434600"
  },
  {
    "text": "glpc card provider has slight differences on some function with",
    "start": "1434600",
    "end": "1439700"
  },
  {
    "text": "respect to in three Cloud providers keep in mind that if you want to escape from zero in a group meaning groups with",
    "start": "1439700",
    "end": "1448880"
  },
  {
    "text": "zero nodes in some specific specific circumstances when you have mirror codes then",
    "start": "1448880",
    "end": "1455900"
  },
  {
    "text": "calculation performed by the ca to understand if a pending pod could fit a",
    "start": "1455900",
    "end": "1461539"
  },
  {
    "text": "new node could not be correct it could be slightly half",
    "start": "1461539",
    "end": "1466760"
  },
  {
    "text": "because the information about the full ports pods for another are not in the",
    "start": "1466760",
    "end": "1474260"
  },
  {
    "text": "grpc cloud provider another minor thing to know is that the",
    "start": "1474260",
    "end": "1479840"
  },
  {
    "text": "function gets resolution is not available but its use is really limited and almost no problem implements it",
    "start": "1479840",
    "end": "1486860"
  },
  {
    "text": "anyway and don't be confused by some missing functions in the product file with",
    "start": "1486860",
    "end": "1492860"
  },
  {
    "text": "respect to the golang interfaces because some functions are indeed deprecated in",
    "start": "1492860",
    "end": "1498679"
  },
  {
    "text": "the class in golang interfaces and so I have not been implemented as rpcc here",
    "start": "1498679",
    "end": "1506320"
  },
  {
    "text": "okay we are at the end of the presentation uh if you want to dig deeper here there are some links you",
    "start": "1506539",
    "end": "1514159"
  },
  {
    "text": "will be able to download these slides online so you will be able to look at",
    "start": "1514159",
    "end": "1519679"
  },
  {
    "text": "the links that's it I hope this presentation was useful and that you will use this new",
    "start": "1519679",
    "end": "1526520"
  },
  {
    "text": "plugin system thank you for your time okay so our presentation is slowly going",
    "start": "1526520",
    "end": "1534080"
  },
  {
    "text": "to an end so I would like to give you some more details about seek Auto",
    "start": "1534080",
    "end": "1539419"
  },
  {
    "text": "scaling if you have some ideas for improvements questions comments or you want to contribute",
    "start": "1539419",
    "end": "1545659"
  },
  {
    "text": "so we have meetings every Monday at 10 pm Eastern Detroit time",
    "start": "1545659",
    "end": "1552940"
  },
  {
    "text": "on Zoom we have a slack channel on standard kubernetes",
    "start": "1552940",
    "end": "1561340"
  },
  {
    "text": "site and most of our code sits in kubernetes slash Auto scale Repository",
    "start": "1561340",
    "end": "1569840"
  },
  {
    "text": "and I would like to thank you for coming this late in the presentation and showing up this was really really",
    "start": "1569840",
    "end": "1576860"
  },
  {
    "text": "amazing that's and now is the time for questions",
    "start": "1576860",
    "end": "1583059"
  },
  {
    "text": "regarding the vertical uh product Skillet so uh does what cloud operation supports",
    "start": "1590440",
    "end": "1597500"
  },
  {
    "text": "the custom metrics like example of data doc right so does it supports uh sorry",
    "start": "1597500",
    "end": "1602900"
  },
  {
    "text": "data dog Matrix on the vpa the external metrics for the VPI does it support",
    "start": "1602900",
    "end": "1609620"
  },
  {
    "text": "external metrics yes uh no it doesn't support but if you",
    "start": "1609620",
    "end": "1615340"
  },
  {
    "text": "modified recommender to pull this Matrix from your data site it the rest of the",
    "start": "1615340",
    "end": "1623120"
  },
  {
    "text": "environment will be okay with it so if you want to have external metrics you",
    "start": "1623120",
    "end": "1628640"
  },
  {
    "text": "you'll need to to contribute Samsung code oh okay so we have to use like just",
    "start": "1628640",
    "end": "1634820"
  },
  {
    "text": "regular metrics [Music] it gets metrics from a metric server and",
    "start": "1634820",
    "end": "1642400"
  },
  {
    "text": "storage them as a histogram hotel in tcd so we've got",
    "start": "1642400",
    "end": "1648200"
  },
  {
    "text": "a snapshot however if you wanted to use something some other source then you",
    "start": "1648200",
    "end": "1654980"
  },
  {
    "text": "need to do it by yourself there's there's some code around for getting the",
    "start": "1654980",
    "end": "1660380"
  },
  {
    "text": "metrics from Prime videos I can probably point you to uh to it you have another",
    "start": "1660380",
    "end": "1665419"
  },
  {
    "text": "question regarding cluster Auto scaler right for example in the cluster scalar for minimum we put like four nodes a",
    "start": "1665419",
    "end": "1672799"
  },
  {
    "text": "maximum equal to 10 nodes and four nodes are using around 80 percent but we put",
    "start": "1672799",
    "end": "1678200"
  },
  {
    "text": "the Threshold at 70 if it is 70 less than that remove the node right so four nodes are",
    "start": "1678200",
    "end": "1684799"
  },
  {
    "text": "using around like 80 percent and the fifth node is using around thirty percent so how does a cluster Auto",
    "start": "1684799",
    "end": "1689840"
  },
  {
    "text": "scalar like behaves is that it will delete the fifth node or it will rise to move those like so as",
    "start": "1689840",
    "end": "1698120"
  },
  {
    "text": "long as all your pots are uh scheduled it will do nothing so it will pack your",
    "start": "1698120",
    "end": "1707240"
  },
  {
    "text": "notes completely as long as the pots feeds there if the pot cannot be fitted",
    "start": "1707240",
    "end": "1713900"
  },
  {
    "text": "to your node and scheduler marks the pot as unschedulable then cluster autoscaler kicks in analyzes your cluster analysis",
    "start": "1713900",
    "end": "1721760"
  },
  {
    "text": "what as your configuration and checks whether adding new node will help yeah",
    "start": "1721760",
    "end": "1728240"
  },
  {
    "text": "it I think new note usually helps but if you a mistype for example pot size and",
    "start": "1728240",
    "end": "1734419"
  },
  {
    "text": "you put 400. instead of 400 millikors then obviously",
    "start": "1734419",
    "end": "1740539"
  },
  {
    "text": "your cloud provider will probably be unable to provide you a note with 400 CPUs and 12. cluster Auto scaler doesn't",
    "start": "1740539",
    "end": "1747500"
  },
  {
    "text": "even try to do anything",
    "start": "1747500",
    "end": "1750460"
  },
  {
    "text": "um did you consider a crd based solution instead of the grpc solution",
    "start": "1763279",
    "end": "1770299"
  },
  {
    "text": "therefore yeah for the cluster Auto scaler for the",
    "start": "1770299",
    "end": "1776600"
  },
  {
    "text": "um providing support for different clouds charity so a cluster Auto scaler does a",
    "start": "1776600",
    "end": "1784279"
  },
  {
    "text": "lot of simulation and it invokes a lot of calls to see what if what if we added",
    "start": "1784279",
    "end": "1792260"
  },
  {
    "text": "that node what if we added another node so it rescheduler a little bit of like a",
    "start": "1792260",
    "end": "1797960"
  },
  {
    "text": "black box so it doesn't look closely what is in",
    "start": "1797960",
    "end": "1803059"
  },
  {
    "text": "pod specification it just shows it new nodes and checks whether scheduler will",
    "start": "1803059",
    "end": "1808820"
  },
  {
    "text": "put it or not okay so doing it with crd is either",
    "start": "1808820",
    "end": "1816799"
  },
  {
    "text": "impossible or will take ages to do communication via API server",
    "start": "1816799",
    "end": "1825760"
  },
  {
    "text": "yeah so um regarding the the instance types available on the cloud providers right",
    "start": "1827080",
    "end": "1833419"
  },
  {
    "text": "now everything is hard coded into the into the cluster autoscaler code and it",
    "start": "1833419",
    "end": "1839000"
  },
  {
    "text": "has to be maintained so for example it's um using AWS and AWS releases a few easy",
    "start": "1839000",
    "end": "1845600"
  },
  {
    "text": "new instances that I want to use cluster Auto scalar is not going to have support until somebody actually patches that",
    "start": "1845600",
    "end": "1852500"
  },
  {
    "text": "into the code have you considered an automated ways to actually call the providers to get information about the",
    "start": "1852500",
    "end": "1860120"
  },
  {
    "text": "instance types that are available number of cores memory Etc so that you don't have to depend on this manually updated",
    "start": "1860120",
    "end": "1866360"
  },
  {
    "text": "list so it is up to cloud provider implementator so someone who coded it",
    "start": "1866360",
    "end": "1872419"
  },
  {
    "text": "how to handle it so on gke we have this process automated",
    "start": "1872419",
    "end": "1879159"
  },
  {
    "text": "but other Cloud providers are for some reasons not not adding this type of",
    "start": "1879159",
    "end": "1884539"
  },
  {
    "text": "support if you are interested please feel free to contribute to the cloud provider that you are running on and",
    "start": "1884539",
    "end": "1891080"
  },
  {
    "text": "will definitely accept those types of patches so they are not like against getting current configuration",
    "start": "1891080",
    "end": "1899360"
  },
  {
    "text": "current setting and so on is more than welcome but well sometimes people",
    "start": "1899360",
    "end": "1904520"
  },
  {
    "text": "implement the cloud providers in a simple way and we cannot force them to",
    "start": "1904520",
    "end": "1910159"
  },
  {
    "text": "have it working better I was aware that the gke was automatic",
    "start": "1910159",
    "end": "1915860"
  },
  {
    "text": "so it's yeah creates automatically and not positive",
    "start": "1915860",
    "end": "1921200"
  },
  {
    "text": "you configure not Auto provisioning",
    "start": "1921200",
    "end": "1926019"
  },
  {
    "text": "and for the cluster autoscaler um we've seen no scale down",
    "start": "1927200",
    "end": "1934039"
  },
  {
    "text": "um when there's one pending ports scaled up to a couple of hundred nodes now scaling down for one pending pod he said",
    "start": "1934039",
    "end": "1940220"
  },
  {
    "text": "that has changed in 124 yes um does that mean that you",
    "start": "1940220",
    "end": "1948380"
  },
  {
    "text": "just completely ignore pending pots or is the only pending pots that are marked as unschedulable or",
    "start": "1948380",
    "end": "1955340"
  },
  {
    "text": "[Music] so we analyze a cluster more carefully",
    "start": "1955340",
    "end": "1961279"
  },
  {
    "text": "and we are checking whether",
    "start": "1961279",
    "end": "1965440"
  },
  {
    "text": "removing nodes will not the make will not make the life of the spending Port",
    "start": "1966399",
    "end": "1972260"
  },
  {
    "text": "harder so if it's like completely independent and the spot has no chance",
    "start": "1972260",
    "end": "1978140"
  },
  {
    "text": "to run on this node that we are removing anyway then we are okay with removing it however we keep we don't scale down if",
    "start": "1978140",
    "end": "1987500"
  },
  {
    "text": "well scheduler has not yet managed to process the port and the Pod is likely",
    "start": "1987500",
    "end": "1993320"
  },
  {
    "text": "to benefit from those those being around but for instance the case that I've got",
    "start": "1993320",
    "end": "1999320"
  },
  {
    "text": "one pending pot left and I've got a hundred unknown unused notes then in the",
    "start": "1999320",
    "end": "2005620"
  },
  {
    "text": "current version you do not scale down any note you leave it at 100 notes has",
    "start": "2005620",
    "end": "2011679"
  },
  {
    "text": "that changed that so that is fixed that is fixed okay thank you",
    "start": "2011679",
    "end": "2017679"
  },
  {
    "text": "um dear if no other questions um note groups uh Carpenter is",
    "start": "2017679",
    "end": "2024640"
  },
  {
    "text": "a different kind of old scaler they've completely dropped the whole idea of node groups",
    "start": "2024640",
    "end": "2031240"
  },
  {
    "text": "um has that been given thought for the cluster autoscaler or so yeah it uh we",
    "start": "2031240",
    "end": "2037000"
  },
  {
    "text": "were thinking about adding it so why do we have node groups it is because",
    "start": "2037000",
    "end": "2042539"
  },
  {
    "text": "kubernetes in early version was running on node groups and we have we are having",
    "start": "2042539",
    "end": "2047620"
  },
  {
    "text": "cluster Auto scalar since 2016 or something like that and most of the",
    "start": "2047620",
    "end": "2053378"
  },
  {
    "text": "cloud providers operate with the concept of node group node post Auto scaling group or what whatever",
    "start": "2053379",
    "end": "2059560"
  },
  {
    "text": "uh the second thing is in order to scale up a cluster we need",
    "start": "2059560",
    "end": "2066339"
  },
  {
    "text": "to know what and the new node would look like you can either have this knowledge we",
    "start": "2066339",
    "end": "2074138"
  },
  {
    "text": "inferred from the existing nodes and or you need to hard code it",
    "start": "2074139",
    "end": "2081520"
  },
  {
    "text": "so you can get rid of this full concept and bring",
    "start": "2081520",
    "end": "2087158"
  },
  {
    "text": "your notes if you hard code a lot of code and that figures out what would be the good note for that particular pod",
    "start": "2087159",
    "end": "2095138"
  },
  {
    "text": "what is the cloud offering What notes are supported what are their prices so that you don't",
    "start": "2095139",
    "end": "2102760"
  },
  {
    "text": "get like the most expensive one and so on so a lot of very very Cloud specific",
    "start": "2102760",
    "end": "2108820"
  },
  {
    "text": "code that needs to get in and uh that may work if you have a group of",
    "start": "2108820",
    "end": "2117599"
  },
  {
    "text": "developers that are set to that particular project of bringing up this",
    "start": "2117599",
    "end": "2123640"
  },
  {
    "text": "article to Auto scaler and they are maintaining it keeping it up to date and so on however if we have 30 cloud",
    "start": "2123640",
    "end": "2131440"
  },
  {
    "text": "provider starting from AWS to some and that you barely have heard about that's",
    "start": "2131440",
    "end": "2138040"
  },
  {
    "text": "not a valid request to ask everyone crowd and that complex logic into their",
    "start": "2138040",
    "end": "2145300"
  },
  {
    "text": "cloud provider we wanted to have more or less uniform",
    "start": "2145300",
    "end": "2150720"
  },
  {
    "text": "experience across the cloud and cluster autoscaler mostly works the same on",
    "start": "2150720",
    "end": "2156040"
  },
  {
    "text": "Azure gke AWS and some other clouds you",
    "start": "2156040",
    "end": "2161260"
  },
  {
    "text": "haven't heard probably about and yes gke we had we made this effort",
    "start": "2161260",
    "end": "2169119"
  },
  {
    "text": "of coding a feature that we called node Auto",
    "start": "2169119",
    "end": "2174400"
  },
  {
    "text": "provisioning so it creates a new node pool for you should this notebooks that you have in the cluster were not enough",
    "start": "2174400",
    "end": "2181599"
  },
  {
    "text": "like were too small or didn't have the requested GPU but there's a lot a lot",
    "start": "2181599",
    "end": "2187599"
  },
  {
    "text": "really a lot of Cloud specific code that even if we open source it would be hard for the other",
    "start": "2187599",
    "end": "2194980"
  },
  {
    "text": "cloud provider authors to benefit from because they will need to do the very",
    "start": "2194980",
    "end": "2200260"
  },
  {
    "text": "same coding but specific to their Cloud providers",
    "start": "2200260",
    "end": "2205660"
  },
  {
    "text": "so for that reason cluster autoscaler is like it is if you want Carpenter",
    "start": "2205660",
    "end": "2210880"
  },
  {
    "text": "experience then please come to gke and if someone wants to devote their time to",
    "start": "2210880",
    "end": "2216960"
  },
  {
    "text": "coding providing this not as a provisioning experience to other Cloud",
    "start": "2216960",
    "end": "2222099"
  },
  {
    "text": "providers we are very happy to give you support and tell you how to do it in cluster of the scalar but the warning is",
    "start": "2222099",
    "end": "2227680"
  },
  {
    "text": "that well it requires quite a bit of of work to get it done properly yeah so AWS",
    "start": "2227680",
    "end": "2233380"
  },
  {
    "text": "is chosen to just do it in Carpenter so that's sorry AWS has chosen to just do it in Carpenter so that's uh yes so they",
    "start": "2233380",
    "end": "2242200"
  },
  {
    "text": "chosen they put Engineers on it and well inventory Carpenter is open source but it will take a lot of effort in order to",
    "start": "2242200",
    "end": "2251500"
  },
  {
    "text": "make it work on other cloud provider because they need to implement this whole logic that is very very specific",
    "start": "2251500",
    "end": "2257619"
  },
  {
    "text": "to cloud provider and also they have where you create the cluster how you expand and how how is it built what's",
    "start": "2257619",
    "end": "2266260"
  },
  {
    "text": "the offering what uh how the how to create new notes and what they would do it like like a lot of code I will keep",
    "start": "2266260",
    "end": "2273760"
  },
  {
    "text": "single cross for Carpenter but it will be hard for them to replicate it on other Cloud providers",
    "start": "2273760",
    "end": "2280260"
  },
  {
    "text": "oh so the cluster Auto scaler is aware of",
    "start": "2281320",
    "end": "2287500"
  },
  {
    "text": "node groups and has some awareness of cloud providers for the HPA and the vpa is there any like talk in the community",
    "start": "2287500",
    "end": "2295000"
  },
  {
    "text": "of sake Auto scaling about adding some support and the reason I ask is because",
    "start": "2295000",
    "end": "2300040"
  },
  {
    "text": "you may have a heterogeneous set of nodes that your application is running",
    "start": "2300040",
    "end": "2306040"
  },
  {
    "text": "on and because HP or sorry because vpa provides a single recommendation for CPU",
    "start": "2306040",
    "end": "2313240"
  },
  {
    "text": "and memory it may not fit all of your nodes that your application is running",
    "start": "2313240",
    "end": "2319420"
  },
  {
    "text": "on um with the same request so I'm just I'm just wondering if there is any discussion there but fixing that",
    "start": "2319420",
    "end": "2327280"
  },
  {
    "text": "or so the question is about integration of workload controllers with",
    "start": "2327280",
    "end": "2333820"
  },
  {
    "text": "cluster auto scanner or how to run your workload in heterogeneous environment",
    "start": "2333820",
    "end": "2339220"
  },
  {
    "text": "yeah the latter how to run your workload so I guess I guess either you know you could you could run your workload in a",
    "start": "2339220",
    "end": "2344920"
  },
  {
    "text": "different way such that you're only running on a certain node group or vpa could have for no group recommendations",
    "start": "2344920",
    "end": "2352060"
  },
  {
    "text": "I guess that's kind of my question okay so running your workload on",
    "start": "2352060",
    "end": "2357820"
  },
  {
    "text": "significantly different nodes will cause your problems because the application on",
    "start": "2357820",
    "end": "2362859"
  },
  {
    "text": "one note may run orders on magnitude faster than the other and that will",
    "start": "2362859",
    "end": "2368619"
  },
  {
    "text": "confuse horizontal but autoscaler what is actually going on so you can run your application will",
    "start": "2368619",
    "end": "2377500"
  },
  {
    "text": "know that don't differ that much probably we can handle like 10 percent of performance differences without any",
    "start": "2377500",
    "end": "2382540"
  },
  {
    "text": "problems but if you the other note is like three times better than the other",
    "start": "2382540",
    "end": "2388480"
  },
  {
    "text": "than historical data gathered by vertical pot autoscaler will not be valid",
    "start": "2388480",
    "end": "2394740"
  },
  {
    "text": "HPA will be confused whether you are above or below the Target and whether",
    "start": "2394740",
    "end": "2400359"
  },
  {
    "text": "you should give you more replicas uh the rest replicas it may actually shrink",
    "start": "2400359",
    "end": "2406180"
  },
  {
    "text": "your replicas and leaving one that is on this spoon node supervised training and",
    "start": "2406180",
    "end": "2412119"
  },
  {
    "text": "causing some type of Errors to the customers so at this moment running here workloads",
    "start": "2412119",
    "end": "2420400"
  },
  {
    "text": "on very heterogeneous nodes is rather not recommended with auto scaling and",
    "start": "2420400",
    "end": "2425560"
  },
  {
    "text": "probably even with auto scaling too because what you need to size the pots somehow and exercise this will be",
    "start": "2425560",
    "end": "2431680"
  },
  {
    "text": "different for different types of nodes got it",
    "start": "2431680",
    "end": "2437040"
  },
  {
    "text": "one last question here",
    "start": "2439359",
    "end": "2442799"
  },
  {
    "text": "sorry this might be kind of a newbish question in terms of HPA we're currently utilizing it pretty well but vpa is",
    "start": "2445359",
    "end": "2451240"
  },
  {
    "text": "fairly new to us from a usability perspective when it comes to HPA a lot of git Ops pipelines such as flux in",
    "start": "2451240",
    "end": "2458260"
  },
  {
    "text": "terms of replication come into a problem with like three-way merges where like flux will give a static replica count",
    "start": "2458260",
    "end": "2465520"
  },
  {
    "text": "and HPA will give a dynamic or different replica count and you'll get like a change has already been applied please",
    "start": "2465520",
    "end": "2471579"
  },
  {
    "text": "try again later error on some signs do you is that same issue ever occur on a vpa perspective in terms of requests and",
    "start": "2471579",
    "end": "2478359"
  },
  {
    "text": "limits that you know of or has it ever been reported at all so I'm afraid I didn't understand your question oh my",
    "start": "2478359",
    "end": "2485440"
  },
  {
    "text": "fault so so for things like flux when applying a specific workloader",
    "start": "2485440",
    "end": "2490480"
  },
  {
    "text": "configurations typically what ends up happening is the actual get Ops approach",
    "start": "2490480",
    "end": "2497200"
  },
  {
    "text": "of applying a static replica count in terms of like async from a git Ops repository we'll apply a static replica",
    "start": "2497200",
    "end": "2504220"
  },
  {
    "text": "say one or two HPA will scale that number dynamically via the deployments",
    "start": "2504220",
    "end": "2509800"
  },
  {
    "text": "API to let's say three or four depending on capacity or usage but",
    "start": "2509800",
    "end": "2516099"
  },
  {
    "text": "um what ends up happening sometimes is a three-way merge problem where uh flux will keep attempting to reapply to or",
    "start": "2516099",
    "end": "2523660"
  },
  {
    "text": "reapply a lower number than what HPA represents I'm wondering if the same thing happens with vpa at all with",
    "start": "2523660",
    "end": "2530320"
  },
  {
    "text": "vertical part of the scalar with um requests and limits specifically because I know what flux specifically ended up",
    "start": "2530320",
    "end": "2537040"
  },
  {
    "text": "doing to mitigate that was to ignore the replica count so I'm wondering if that's",
    "start": "2537040",
    "end": "2542440"
  },
  {
    "text": "something that other providers so yeah regarding updates in vertical potential",
    "start": "2542440",
    "end": "2548099"
  },
  {
    "text": "vertical potato scalar doesn't of your deployment it de updates spots in the",
    "start": "2548099",
    "end": "2556000"
  },
  {
    "text": "admission phase oh yes we didn't do it because the updating uh deployment will",
    "start": "2556000",
    "end": "2563920"
  },
  {
    "text": "cause Pottery creation uh and it's not necessarily the thing that you would",
    "start": "2563920",
    "end": "2569859"
  },
  {
    "text": "like to have at the moment vpa has couple of operating modes so you can have it have only recommendations",
    "start": "2569859",
    "end": "2577300"
  },
  {
    "text": "without any actuation that's for your information you can see what would a vpa",
    "start": "2577300",
    "end": "2582640"
  },
  {
    "text": "give to your call center apply it manually or completely ignore I see",
    "start": "2582640",
    "end": "2588880"
  },
  {
    "text": "other option is to have a onion creation time so",
    "start": "2588880",
    "end": "2594839"
  },
  {
    "text": "vpa gives you recommended size when the Pod is created however doesn't touch",
    "start": "2594839",
    "end": "2600700"
  },
  {
    "text": "existing pods so if something is running it's running fine we don't touch it even if we wanted to give a little bit more",
    "start": "2600700",
    "end": "2606540"
  },
  {
    "text": "uh CPU and the third mode is automatic when we",
    "start": "2606540",
    "end": "2614619"
  },
  {
    "text": "updated pause if they are outside of the reasonable values for pod sizes so not",
    "start": "2614619",
    "end": "2622300"
  },
  {
    "text": "only if they are a little bit different from the recommended value but different so that we think it's worth to update",
    "start": "2622300",
    "end": "2628839"
  },
  {
    "text": "them and even then we respect things like pod disruption budget and we try to",
    "start": "2628839",
    "end": "2635260"
  },
  {
    "text": "do it slowly on GK we also have an integration with cluster Auto scale",
    "start": "2635260",
    "end": "2640319"
  },
  {
    "text": "updating pods in updating sizes in deployment holding",
    "start": "2640319",
    "end": "2646119"
  },
  {
    "text": "our disruption budget and codes like immediate rollout of the new version which will be quite disruptive and",
    "start": "2646119",
    "end": "2653140"
  },
  {
    "text": "definitely more disruptive than our current process so yeah if you are running vpa from time",
    "start": "2653140",
    "end": "2662200"
  },
  {
    "text": "to time you should update your deployment so that if vpa is idle for",
    "start": "2662200",
    "end": "2667540"
  },
  {
    "text": "some reason not to are running or you have want to have some idea what would be the size of the deployment if created",
    "start": "2667540",
    "end": "2673780"
  },
  {
    "text": "and you should put it take the values from VIP object and put your put those in the deployment gotcha so to reiterate",
    "start": "2673780",
    "end": "2681339"
  },
  {
    "text": "it only messes with the pods which is the post living deployments uh itself",
    "start": "2681339",
    "end": "2687579"
  },
  {
    "text": "alone I see that provides a lot of context thank you",
    "start": "2687579",
    "end": "2691859"
  }
]