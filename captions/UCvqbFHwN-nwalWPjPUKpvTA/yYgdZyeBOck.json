[
  {
    "start": "0",
    "end": "82000"
  },
  {
    "text": "good oh hello everybody my name is Tom sorry I'm starting a bit late I was",
    "start": "30",
    "end": "5069"
  },
  {
    "text": "having a good chat at the front right then so today we're going to talk about blazing-fast prom ql this is a project",
    "start": "5069",
    "end": "12780"
  },
  {
    "text": "that I've been working on now for about a year or so and yeah hopefully take how",
    "start": "12780",
    "end": "18390"
  },
  {
    "text": "long do we have do you know I've now 35 okay good with questions nice thank you so yeah",
    "start": "18390",
    "end": "25529"
  },
  {
    "text": "take off now to explain what we've done here so a bit of an introduction as I",
    "start": "25529",
    "end": "30599"
  },
  {
    "text": "said my name's Tom I'm really honored to be basically giving one of the first track talks a cubic on this means that",
    "start": "30599",
    "end": "37440"
  },
  {
    "text": "all of my anchorman memes are going to be nice and fresh I think in three days time you're gonna be really sick of",
    "start": "37440",
    "end": "42510"
  },
  {
    "text": "anchorman quotes yeah I work for a company called Cortana labs who here uses kevanna Wow excellent yeah I I'm",
    "start": "42510",
    "end": "51360"
  },
  {
    "text": "technically the VP product but I'm actually just a software engineer by trade at least I don't do many PRS",
    "start": "51360",
    "end": "58829"
  },
  {
    "text": "anymore I started the cortex project cortex is one of these CNCs projects to",
    "start": "58829",
    "end": "63870"
  },
  {
    "text": "do a horizontally scalable from easiest I also started something called Loki about a year ago we announced a cube",
    "start": "63870",
    "end": "69180"
  },
  {
    "text": "comp and I'm yeah I was invited to be a prometheus maintainer about two and a",
    "start": "69180",
    "end": "74250"
  },
  {
    "text": "half three years ago as well which is pretty cool if you want me on Twitter on github there you go and that",
    "start": "74250",
    "end": "79710"
  },
  {
    "text": "is not my photo so this talk is not about cortex but I just want to give a",
    "start": "79710",
    "end": "85259"
  },
  {
    "start": "82000",
    "end": "238000"
  },
  {
    "text": "brief intro because it will be relevant to the rest of the discussion so cortex is this CN CF project we started it",
    "start": "85259",
    "end": "91049"
  },
  {
    "text": "three years ago and it's to build a horizontally scalable kind of clustered Prometheus so we do a high availability",
    "start": "91049",
    "end": "97320"
  },
  {
    "text": "we do replication we do all of these kind of things and really cortex is",
    "start": "97320",
    "end": "102479"
  },
  {
    "text": "about about giving you like solving some like key problems with previous so if",
    "start": "102479",
    "end": "107970"
  },
  {
    "text": "you have multiple independent kubernetes clusters and you run independent Prometheus in them you want to have like",
    "start": "107970",
    "end": "114000"
  },
  {
    "text": "a global view of all your metrics like that's when you might consider using I think the audio is cutting out okay yeah",
    "start": "114000",
    "end": "122880"
  },
  {
    "text": "okay we're Karam yeah you could also use other projects like Thanos exists souls very similar problems there's an",
    "start": "122880",
    "end": "128160"
  },
  {
    "text": "excellent talk by Bartok in the audience here and my set off from pom con two weeks ago where we kind of compare",
    "start": "128160",
    "end": "133920"
  },
  {
    "text": "the two systems but they're yeah they're set out to solve the same problem we also like in Prometheus if ever you have",
    "start": "133920",
    "end": "140790"
  },
  {
    "text": "if you guys use Prometheus who use it for me theater hopefully with everyone right otherwise why are you in this talk but if you've if you've experienced like",
    "start": "140790",
    "end": "148260"
  },
  {
    "text": "sometimes you know you might need to upgrade your Prometheus server you're gonna end up with gaps in your graphs from that restart you know maybe you",
    "start": "148260",
    "end": "154709"
  },
  {
    "text": "need to upgrade the hostess running on a sonnet like that so this is another thing we try and solve with cortex we can do like hae-joo ping again look at",
    "start": "154709",
    "end": "162209"
  },
  {
    "text": "this prom con talk but the reason I want to talk about cortex and I want to put this talk in in context get those two",
    "start": "162209",
    "end": "168989"
  },
  {
    "text": "words correct is that you know in a cortex cluster or sorry in a Prometheus cluster we have this kind of rule you",
    "start": "168989",
    "end": "174750"
  },
  {
    "text": "should never really query more than about a hundred thousand time series in a single query and this is you might hear people talk about cardinality and",
    "start": "174750",
    "end": "181650"
  },
  {
    "text": "this is kind of what they mean like don't query too many time series otherwise your queries will be slow but",
    "start": "181650",
    "end": "186959"
  },
  {
    "text": "when you've got a cortex cluster and you've got the maybe fifteen kubernetes clusters all sending data into one cortex instance you're gonna have",
    "start": "186959",
    "end": "193470"
  },
  {
    "text": "fifteen times as many time series per query as you will impress yes it is very easy to get yourself into a very high",
    "start": "193470",
    "end": "199560"
  },
  {
    "text": "cardinality situation with cortex and this is just something you know something that happens naturally when you start using when you start",
    "start": "199560",
    "end": "206070"
  },
  {
    "text": "aggregating together multiple Prometheus worth of metrics and so it's not enough for us just us to say don't do that",
    "start": "206070",
    "end": "212489"
  },
  {
    "text": "like we have to actually come up with solutions for people who do that because they do it all the time and that's really where this talk comes from like",
    "start": "212489",
    "end": "218340"
  },
  {
    "text": "when running cortex you end up querying millions of metrics normally and and you",
    "start": "218340",
    "end": "225030"
  },
  {
    "text": "have to therefore do something about that so yeah the rest of the talk isn't really about cortex I'm sorry about that",
    "start": "225030",
    "end": "230400"
  },
  {
    "text": "and if you're here to hear about cortex I understand the next talk in room 15 is about cortex but yeah I'm gonna move on",
    "start": "230400",
    "end": "237120"
  },
  {
    "text": "now so what we do to execute these",
    "start": "237120",
    "end": "242910"
  },
  {
    "start": "238000",
    "end": "493000"
  },
  {
    "text": "queries we actually just use the upstream Prometheus query engine the prom ql query engine we actually in",
    "start": "242910",
    "end": "248280"
  },
  {
    "text": "cortex use the same storage format and this is like I think if you're familiar with Thanos if you're familiar with",
    "start": "248280",
    "end": "253440"
  },
  {
    "text": "other Prometheus compatible systems this is really common like they it's really easy to vendor and really easy to reuse",
    "start": "253440",
    "end": "260519"
  },
  {
    "text": "this Prometheus query engine so we just do that but one thing you should know about the Prometheus query engine is it",
    "start": "260519",
    "end": "267330"
  },
  {
    "text": "it's effectively single-threaded so for a single request it's going to execute on a single core it's gonna a great",
    "start": "267330",
    "end": "273509"
  },
  {
    "text": "together all that data decode it aggregate it do whatever maths you're doing all on a single core and for those",
    "start": "273509",
    "end": "280080"
  },
  {
    "text": "of you have been following along cause are actually not getting much faster in fact like we're getting into a world of",
    "start": "280080",
    "end": "285479"
  },
  {
    "text": "multi-core you know already in the world of multi-core really and so in fact cores are getting a bit slower and so if",
    "start": "285479",
    "end": "291240"
  },
  {
    "text": "you've got a query engine the only ones on a single core and you want to make it faster you've really got to look at ways of paralyzing those queries and that's",
    "start": "291240",
    "end": "297780"
  },
  {
    "text": "what the rest of this talk is really about the other thing we did with the the query engine inside cortex is we",
    "start": "297780",
    "end": "305460"
  },
  {
    "text": "made it kind of stateless and we made it horizontally scalable so if your queries are slow just run more of them is the",
    "start": "305460",
    "end": "312330"
  },
  {
    "text": "idea accessory doesn't really work right so if you send us lots of very small queries then we compare allies them",
    "start": "312330",
    "end": "318840"
  },
  {
    "text": "between different query engines you know we can scale up if you have a high concurrent query load but everyone",
    "start": "318840",
    "end": "324690"
  },
  {
    "text": "sitting here using graph Arnall right you you load up and you maybe show five or 10 graphs on your dashboard and if you're really aggressive you",
    "start": "324690",
    "end": "330750"
  },
  {
    "text": "might you know refresh them every few seconds but that's still only gonna be like single digits maybe not even single",
    "start": "330750",
    "end": "336300"
  },
  {
    "text": "digits QPS and so parallelizing them is you know running multiple queries and",
    "start": "336300",
    "end": "341490"
  },
  {
    "text": "then load balancing across those queries isn't really going to make it much faster we've actually got to find a way of paralyzing individual queries you",
    "start": "341490",
    "end": "349379"
  },
  {
    "text": "can't really just keep scaling up the query and putting on bigger bigger boxes with faster processors does it really",
    "start": "349379",
    "end": "354509"
  },
  {
    "text": "exist anymore yes so this talk is really about how we handle big queries like if you come along and you ask you ask to do",
    "start": "354509",
    "end": "361500"
  },
  {
    "text": "a query over a few weeks worth of data over a few million time series how are we going to handle that but before we",
    "start": "361500",
    "end": "369539"
  },
  {
    "text": "get into that first thing we did like every good software engineer we just went and put as much caching as possible into the system so we went in we we have",
    "start": "369539",
    "end": "376139"
  },
  {
    "text": "we have a separate indexing cortex and a separate chunk store of storing the compressed time series data and we just",
    "start": "376139",
    "end": "383849"
  },
  {
    "text": "went and stuck em caches in front of them so the chunks was easy because chunks are immutable they don't change once they're written",
    "start": "383849",
    "end": "389520"
  },
  {
    "text": "to disk so you can just stick a memcache in and and when you write them we do a write through cache so when you write",
    "start": "389520",
    "end": "394529"
  },
  {
    "text": "them they get written to memcache which means you pretty much have like 90 something percent hit right on your chunk of memcache",
    "start": "394529",
    "end": "400699"
  },
  {
    "text": "the index is a lot harder to cache because the index is always changing and you're always adding new stuff to the index and if you if you get the cache",
    "start": "400699",
    "end": "407149"
  },
  {
    "text": "wrong and we did have a bug that took us a long time to figure out where we turned out we've got the caching wrong you end up seeing gaps or missing data",
    "start": "407149",
    "end": "414019"
  },
  {
    "text": "in your queries so you've got to be really careful with with caching kind of accesses to indexes like this but anyway",
    "start": "414019",
    "end": "420739"
  },
  {
    "text": "it works now and this this this made a big difference this like now 80 90 % of queries will just hit the index we still",
    "start": "420739",
    "end": "426949"
  },
  {
    "text": "have to process them on a single thread but at least we're not putting a load on the database and we're not putting load on the chunk store and we're effectively",
    "start": "426949",
    "end": "434059"
  },
  {
    "text": "just saving that capacity for when someone does come along and run one of these really big queries anyway now we",
    "start": "434059",
    "end": "440149"
  },
  {
    "text": "actually get to the kind of exciting bit who here has heard of a project called trickster by Comcast oh not so many okay",
    "start": "440149",
    "end": "446629"
  },
  {
    "text": "well tricks there's this really cool thing that that caches the results of the query after you've executed the query they basically take the result",
    "start": "446629",
    "end": "453219"
  },
  {
    "text": "they stick it in a cache and then if you ask the same query twice they return that result so this is an awesome idea",
    "start": "453219",
    "end": "459559"
  },
  {
    "text": "right this relieves even more pressure from the database and from the query execution and we wanted to do this now",
    "start": "459559",
    "end": "465169"
  },
  {
    "text": "unfortunately when we did this over a year ago now the trickster code wasn't a reusable and there was a few things we",
    "start": "465169",
    "end": "471529"
  },
  {
    "text": "didn't particularly like about it and you know I didn't invent it so I kind of wanted to write my own I wanted to do it",
    "start": "471529",
    "end": "477919"
  },
  {
    "text": "in a different style I wanted to make it more reusable and I wanted to do some more interesting things with it so I built my own thing we called it the",
    "start": "477919",
    "end": "484969"
  },
  {
    "text": "query front-end and if the query front ends job was to cache query results we",
    "start": "484969",
    "end": "490459"
  },
  {
    "text": "also use it as an opportunity to do a few more cool things and I'm going to just like deep dive into the query",
    "start": "490459",
    "end": "496969"
  },
  {
    "text": "front-end for five minutes and talk through what it does to process a request so imagine you have a request",
    "start": "496969",
    "end": "503360"
  },
  {
    "text": "HTTP durations seconds count right so we're rating it will probably summit this what this is basically telling me",
    "start": "503360",
    "end": "509899"
  },
  {
    "text": "the QPS of this shipping system the first thing we can do we're going to",
    "start": "509899",
    "end": "515120"
  },
  {
    "text": "take the query into the crew front end and we're going to align the steps right what this means is in in the prometheus",
    "start": "515120",
    "end": "521029"
  },
  {
    "text": "world you ask a query say you ask it from the Prometheus UI it's just going to say from now from right now this",
    "start": "521029",
    "end": "527089"
  },
  {
    "text": "second this millisecond back to let's say an hour ago every 15 seconds give me",
    "start": "527089",
    "end": "532730"
  },
  {
    "text": "the value of this query that's how we plot the graph and then you run it again and it'll say from right now every 15 seconds give me the",
    "start": "532730",
    "end": "539450"
  },
  {
    "text": "value of this graph and so those those of you who have kind of seen this you'll see like if your graphs are a refreshing",
    "start": "539450",
    "end": "546590"
  },
  {
    "text": "constantly they're kind of they're jumping around a little bit they're not they're not constant because if they're",
    "start": "546590",
    "end": "551870"
  },
  {
    "text": "refreshing a slightly different interval to the step you're actually going to be getting back different data what we do",
    "start": "551870",
    "end": "558050"
  },
  {
    "text": "here is we actually just align it so that the start of the graph the start and end of the query are multiples of",
    "start": "558050",
    "end": "564140"
  },
  {
    "text": "the step this stops the graph dancing around also makes the results much more predictable and therefore much more",
    "start": "564140",
    "end": "569480"
  },
  {
    "text": "cacheable this is a bit contentious because like when you ask cortex our",
    "start": "569480",
    "end": "575060"
  },
  {
    "text": "query we're not actually returning you the answer to the query you asked we're returning you the answer to a subtly different query and there was a big bit",
    "start": "575060",
    "end": "582680"
  },
  {
    "text": "of back and forth when tricks the first came out tricks also does this when trickster first came out there was a bit of back and forth over whether this was",
    "start": "582680",
    "end": "587990"
  },
  {
    "text": "even a good thing but what we did because I work for co-founder labs we just changed Ravana to always issue step",
    "start": "587990",
    "end": "593540"
  },
  {
    "text": "aligned queries as well so actually if you're using graphing on top of Prometheus and on top of cortex you won't notice any difference it'll behave",
    "start": "593540",
    "end": "599510"
  },
  {
    "text": "exactly the same and so if you're using graph honor to send queries to cortex this step alignment will be a no op",
    "start": "599510",
    "end": "606380"
  },
  {
    "text": "right it won't do anything if you're doing it from your own user interface or you're doing it from a different user",
    "start": "606380",
    "end": "611780"
  },
  {
    "text": "interface that doesn't do this step alignment then cortex will subtly adjust the query by a few seconds to make the",
    "start": "611780",
    "end": "617450"
  },
  {
    "text": "result more cashable and this is where we start to diverge from what what trickster does we start to do our own",
    "start": "617450",
    "end": "623630"
  },
  {
    "text": "thing we actually take that resulting query and we split it up we split up into different sub why can't you use the",
    "start": "623630",
    "end": "630260"
  },
  {
    "text": "term sub queries because they exist in Prometheus let's call them partial queries we split them up into different",
    "start": "630260",
    "end": "635930"
  },
  {
    "text": "partial queries we chose in cortex to split them up by day this is a flag you",
    "start": "635930",
    "end": "641660"
  },
  {
    "text": "can configure this however you want we're reusing this exact code base for a different system where we've chosen to",
    "start": "641660",
    "end": "646970"
  },
  {
    "text": "split it up by our instead of by day but we chose to split it up by day and this",
    "start": "646970",
    "end": "652520"
  },
  {
    "text": "is because the index we use in cortex is day aligned every day you get a fresh index so by splitting up by day we don't",
    "start": "652520",
    "end": "658340"
  },
  {
    "text": "get any kind of read amplification on to the index basically the same load and now we've got for in this example for",
    "start": "658340",
    "end": "665930"
  },
  {
    "text": "independent queries we can actually go and look them up in the cache sometimes you'll get a hit you'll go",
    "start": "665930",
    "end": "670970"
  },
  {
    "text": "like oh I ask you today Squier a few days ago and that day's results already in the cache done don't need to do",
    "start": "670970",
    "end": "677210"
  },
  {
    "text": "anything more but actually what's more interesting is the I don't know whether you can see my cursor you can't the one",
    "start": "677210",
    "end": "682310"
  },
  {
    "text": "kind of towards the bottom on the right there's kind of a partial hit like we looked in the cache for a day we found",
    "start": "682310",
    "end": "688640"
  },
  {
    "text": "it but it didn't contain a whole 24 hours worth of data and maybe only contained 20 hours or 18 hours worth of",
    "start": "688640",
    "end": "694280"
  },
  {
    "text": "data so in that case we actually have to like construct a kind of a new partial query for the data that's missing then",
    "start": "694280",
    "end": "702650"
  },
  {
    "text": "the final thing is not made obvious here is we never cache we never returned",
    "start": "702650",
    "end": "707840"
  },
  {
    "text": "recent cache data so it's configurable again we set it to 10 minutes but it will never use the cache for the last 10",
    "start": "707840",
    "end": "714410"
  },
  {
    "text": "minutes of data you'll always go to the storage for 10 minutes of data this is because cortex is a push based system",
    "start": "714410",
    "end": "721100"
  },
  {
    "text": "from easiest pushes metrics the cortex it might get slightly behind you know might get behind by a minute or two and so that that last 10 minutes of data is",
    "start": "721100",
    "end": "728510"
  },
  {
    "text": "still changing it's still being updated it's still not not you know not not constant yet anyway",
    "start": "728510",
    "end": "734150"
  },
  {
    "text": "once we've constructed this list of queries we've filtered out the ones that already cached we found the kind of partial cache hits in the partial cache",
    "start": "734150",
    "end": "741350"
  },
  {
    "text": "cache hit logic is actually kind of fun because like you can have like an hour",
    "start": "741350",
    "end": "746360"
  },
  {
    "text": "in the middle of a 24 hour block and then another like 30 minutes in the middle of that block and you have to kind of decompose them into almost an",
    "start": "746360",
    "end": "752060"
  },
  {
    "text": "arbitrary number of sub queries which is kind of cool anyway then we actually dispatch them in parallel off till",
    "start": "752060",
    "end": "759560"
  },
  {
    "text": "you're off to our back-end right and and this is how we get the speed up because we're going to execute this one for day",
    "start": "759560",
    "end": "765500"
  },
  {
    "text": "query we're gonna actually execute it as three separate queries some with cached results this is how we can start to",
    "start": "765500",
    "end": "771290"
  },
  {
    "text": "paralyze start to use that scale out query back a query service and actually",
    "start": "771290",
    "end": "776480"
  },
  {
    "text": "process bits of this query in parallel there's a bit more to it in cortex",
    "start": "776480",
    "end": "781580"
  },
  {
    "text": "there's a bit more we do we actually have a queue that they go into first this is because cortex is a multi-tenant",
    "start": "781580",
    "end": "788180"
  },
  {
    "text": "system we have a few hundred clients use sharing a cortex cluster with isolated",
    "start": "788180",
    "end": "793190"
  },
  {
    "text": "data we really don't want one client to come along and mass refresh on their browser and affect other clients group",
    "start": "793190",
    "end": "799670"
  },
  {
    "text": "performance so we actually per quarter per client and we do come a really naive really basic quality of",
    "start": "799670",
    "end": "805260"
  },
  {
    "text": "service between clients anyway so that's that's how the query front-end works I",
    "start": "805260",
    "end": "810570"
  },
  {
    "text": "wanted to I wanted to talk about something I thought was pretty cool this is that this idea came up about two months ago a month ago nothing we've",
    "start": "810570",
    "end": "818490"
  },
  {
    "text": "done here is actually cortex specific but what happens if I wanted to use this system use the query front-end with a",
    "start": "818490",
    "end": "825690"
  },
  {
    "text": "system that isn't cortex could I use it with Thanos that said could I do just use it with Prometheus it turns out you",
    "start": "825690",
    "end": "833190"
  },
  {
    "text": "can we did this PR the date on here third of June ok so it's like four or",
    "start": "833190",
    "end": "838500"
  },
  {
    "text": "five months ago we did this PR to make it so that we can avoid that queueing logic and just dispatch the queries",
    "start": "838500",
    "end": "843600"
  },
  {
    "text": "directly to any Prometheus compatible API so be it panels be a Prometheus even",
    "start": "843600",
    "end": "850050"
  },
  {
    "text": "m3 even like in flux anything that has a Prometheus compatible API you can now put the query",
    "start": "850050",
    "end": "856200"
  },
  {
    "text": "front end in front of and it will cache the results so let's do a demo this is",
    "start": "856200",
    "end": "863130"
  },
  {
    "start": "861000",
    "end": "1101000"
  },
  {
    "text": "about the only meme that actually works by the way the rest of them are just there for the lulz so okay I'm just",
    "start": "863130",
    "end": "870030"
  },
  {
    "text": "going to have to fiddle with my display a second yeah Oh window right oh there's",
    "start": "870030",
    "end": "890430"
  },
  {
    "text": "nothing nothing on my sleeves I do have a make it bit bigger I do have a graph",
    "start": "890430",
    "end": "897150"
  },
  {
    "text": "on or running in the background that I kind of set up beforehand but there's nothing special there what I'm gonna do I'm gonna run the cortex query front-end",
    "start": "897150",
    "end": "903780"
  },
  {
    "text": "and it's just this this one command here",
    "start": "903780",
    "end": "908540"
  },
  {
    "text": "they go so there's prebuilt config file that sets this up for you so you don't have to worry about",
    "start": "910220",
    "end": "915500"
  },
  {
    "text": "it and and how many of you know about that demo dot robust perception die oh this is like one of the most useful",
    "start": "915500",
    "end": "921050"
  },
  {
    "text": "websites if you ever want to demo Prometheus to anyone it's run by a a small Irish startup run by one of the",
    "start": "921050",
    "end": "927920"
  },
  {
    "text": "Prometheus maintainer and he runs this and maintains this Prometheus and it's got loads of data in it and you can send it queries and he doesn't mind me doing",
    "start": "927920",
    "end": "933740"
  },
  {
    "text": "this so I'm just gonna use that as a demo so I'm gonna point the court with a query front-end app his Prometheus over",
    "start": "933740",
    "end": "939529"
  },
  {
    "text": "the Internet there we go now I'm gonna switch to the graph honor",
    "start": "939529",
    "end": "945910"
  },
  {
    "text": "make that a bit bigger now I've got two data sources in here I've got Prometheus",
    "start": "946300",
    "end": "952970"
  },
  {
    "text": "pointed directly at robust perception and I've got a cached version of Prometheus pointed at my local machine",
    "start": "952970",
    "end": "958459"
  },
  {
    "text": "which is then pointed at robust perception but just to show this working we'll go to the non cached one first and",
    "start": "958459",
    "end": "966339"
  },
  {
    "text": "because I can never do histogram functions live I'm just gonna copy out",
    "start": "966339",
    "end": "972500"
  },
  {
    "text": "the slides paste that in now this is only running over the last hour of data",
    "start": "972500",
    "end": "977870"
  },
  {
    "text": "so take 600 milliseconds even though the servers running somewhere else in the world so this is really quick but we we",
    "start": "977870",
    "end": "984110"
  },
  {
    "text": "need to come up with something that's a bit more aggressive so you can really see the caching effect and the paralyzing effect a parallelism effect",
    "start": "984110",
    "end": "990610"
  },
  {
    "text": "let's let's do a bit longer if we do three hours now it's still pretty quick this is all now in the page cache in",
    "start": "990610",
    "end": "997339"
  },
  {
    "text": "that Linux VM still pretty quick at 6 hours this is actually the problem when you're doing like query optimization work on Prometheus resis is pretty quick",
    "start": "997339",
    "end": "1004420"
  },
  {
    "text": "it's quite hard to actually notice any difference so let's just let's do a day nope it's got faster let's do seven days",
    "start": "1004420",
    "end": "1012420"
  },
  {
    "text": "here we go now it's actually kind of hard to do some work two and a half seconds that is quite quick and the",
    "start": "1012420",
    "end": "1019029"
  },
  {
    "text": "Wi-Fi here is probably faster than my hotel anyway so doing this request directly against Prometheus took two and",
    "start": "1019029",
    "end": "1024400"
  },
  {
    "text": "a half seconds now what I'm gonna do is do this request against the local query front-end on my laptop this is gonna",
    "start": "1024400",
    "end": "1030699"
  },
  {
    "text": "there's no cache because I've just started it so it's gone in a completely empty cache this is going to split the requests up into seven requests and",
    "start": "1030699",
    "end": "1037178"
  },
  {
    "text": "dispatch them in parallel or to a single premiere Prometheus instance if I do that you'll see it takes 1.4 seconds",
    "start": "1037179",
    "end": "1046050"
  },
  {
    "text": "this is already faster this is no caching this is twice as fast even though there's no cash just through the",
    "start": "1046050",
    "end": "1051270"
  },
  {
    "text": "parallelism effect interesting you got on one of the prometheus maintained that's had a bet with me that this wouldn't be any faster because it's",
    "start": "1051270",
    "end": "1057600"
  },
  {
    "text": "still going to a single machine but what this is really showing you is even just by dispatching multiple independent",
    "start": "1057600",
    "end": "1063570"
  },
  {
    "text": "queries to the same Prometheus note it can start to use more of its cause each of those queries being processed on a",
    "start": "1063570",
    "end": "1068880"
  },
  {
    "text": "single core it can start to use the multi cores on the box and it gets faster but then the really cool stuff",
    "start": "1068880",
    "end": "1074940"
  },
  {
    "text": "happens now that I've run that query one once the results of this query are sitting in the cache on my local machine and so if i refresh this takes 200",
    "start": "1074940",
    "end": "1083220"
  },
  {
    "text": "milliseconds this is now 10 times faster than it was directly hitting the Prometheus so you know I think that's",
    "start": "1083220",
    "end": "1090480"
  },
  {
    "text": "pretty cool now let's continue yes great Odin's",
    "start": "1090480",
    "end": "1098340"
  },
  {
    "text": "Raven but but that's not the end that's this is where I know I finished the talk",
    "start": "1098340",
    "end": "1103830"
  },
  {
    "start": "1101000",
    "end": "1185000"
  },
  {
    "text": "when I last gave it I kind of stopped there told people you know go and deploy this it's one command you go and deploy",
    "start": "1103830",
    "end": "1109950"
  },
  {
    "text": "entire in you know inside your company and stick it between the Gravano and the prometheus you'll just relieve a load a",
    "start": "1109950",
    "end": "1115740"
  },
  {
    "text": "load off of the Prometheus all your users will be happy because they've refiner dashboards will be much faster and we're done right wrong there's one",
    "start": "1115740",
    "end": "1122910"
  },
  {
    "text": "more thing this still means if you're executing a query for the last sub 24",
    "start": "1122910",
    "end": "1129810"
  },
  {
    "text": "hours worth of data if you're executing a query for the last few hours worth of data and you're hitting millions tens of",
    "start": "1129810",
    "end": "1136740"
  },
  {
    "text": "millions of time-series it's still not going to be fast enough you know internally we're like we really want",
    "start": "1136740",
    "end": "1141900"
  },
  {
    "text": "queries to be instant like anything around 100 millisecond mark is considered to be basically instant and",
    "start": "1141900",
    "end": "1147270"
  },
  {
    "text": "so our aim is to be able to process kind of millions tens of millions of series in the hundreds of milliseconds this is",
    "start": "1147270",
    "end": "1155190"
  },
  {
    "text": "crazy right because if you do the math I don't know you're going to be pulling in gigabytes if not tens of gigabytes of",
    "start": "1155190",
    "end": "1161550"
  },
  {
    "text": "data in sub-second and aggregating all together like this is really hard and this approach I've got which I kind of",
    "start": "1161550",
    "end": "1168030"
  },
  {
    "text": "call kind of tight and a called time-based parallelism this isn't going",
    "start": "1168030",
    "end": "1173520"
  },
  {
    "text": "to help in that approach so we need to need a different form of parallelism right we need a different way of",
    "start": "1173520",
    "end": "1178820"
  },
  {
    "text": "guarding the query executing across multiple course in combining the results so but the sake of the rest of this talk",
    "start": "1178820",
    "end": "1186889"
  },
  {
    "start": "1185000",
    "end": "1226000"
  },
  {
    "text": "there's only five more minutes to go for the sake of the rest of this talk imagine all Prometheus queries look like",
    "start": "1186889",
    "end": "1191960"
  },
  {
    "text": "this and this actually isn't too much of a stretch like this pattern probably",
    "start": "1191960",
    "end": "1197149"
  },
  {
    "text": "covers 80% of queries to your Prometheus server and the intuition there is that if I've got 10 million series in my",
    "start": "1197149",
    "end": "1203179"
  },
  {
    "text": "query I can't display 10 million series in a graph on graph on ax like there's just not that many pixels on my screen right",
    "start": "1203179",
    "end": "1209600"
  },
  {
    "text": "so I'm having to do some form of aggregation you know there's a bunch of different forms of aggregation but let's",
    "start": "1209600",
    "end": "1215360"
  },
  {
    "text": "imagine some is the way I'm reducing the cardinality of the result set so that I",
    "start": "1215360",
    "end": "1220460"
  },
  {
    "text": "can display it to my users and if my queries look like this and you probably already know where I'm going I can",
    "start": "1220460",
    "end": "1226730"
  },
  {
    "start": "1226000",
    "end": "1325000"
  },
  {
    "text": "actually turn them into kind of almost an arbitrary number of partial queries over the different independent subsets",
    "start": "1226730",
    "end": "1234620"
  },
  {
    "text": "of the data execute these partial queries in parallel and then we a",
    "start": "1234620",
    "end": "1241190"
  },
  {
    "text": "granade the result to get the final result and this is like if anyone's you know paying attention to this point this",
    "start": "1241190",
    "end": "1246799"
  },
  {
    "text": "is just MapReduce right this is just the same theory behind MapReduce these sums are independent they can be executed",
    "start": "1246799",
    "end": "1251809"
  },
  {
    "text": "independently as long as each of these partial queries runs over independent",
    "start": "1251809",
    "end": "1257509"
  },
  {
    "text": "data then this is kind of accurate you know I quoted the plus here because you",
    "start": "1257509",
    "end": "1262789"
  },
  {
    "text": "know you're not actually adding these together you need some kind of merge reduce operation and of course like this",
    "start": "1262789",
    "end": "1268669"
  },
  {
    "text": "shard equals 1 won't work either like Prometheus doesn't support this out the box so this has to rely on some specific",
    "start": "1268669",
    "end": "1275570"
  },
  {
    "text": "ways in which we lay the index out in cortex in cortex we hash every single time series and and then mod 16 it and",
    "start": "1275570",
    "end": "1283100"
  },
  {
    "text": "that tells us the shard of the index we put it in and we chose 16 just because",
    "start": "1283100",
    "end": "1288289"
  },
  {
    "text": "that actually we did this for a completely unrelated reason we just wanted to shorten the row lengths in the index because we're overloading our big",
    "start": "1288289",
    "end": "1294409"
  },
  {
    "text": "tables but it enabled this extra thing and again by picking and aligning the",
    "start": "1294409",
    "end": "1300019"
  },
  {
    "text": "sharding that you do at the very front end with the way we've laid out our index we managed to do this without any amplification of the read load on the",
    "start": "1300019",
    "end": "1306590"
  },
  {
    "text": "index anyway we started doing this actually I didn't start doing this Oh who is not in the audience no Owen and",
    "start": "1306590",
    "end": "1313840"
  },
  {
    "text": "Cyril have kind of been working on this and so I can't take the credit but just to give you an example this works there's code there's a PR that exists",
    "start": "1313840",
    "end": "1320679"
  },
  {
    "text": "and we're still like we haven't got it into production yet but but it works and to give you an example here's a query",
    "start": "1320679",
    "end": "1326620"
  },
  {
    "text": "over quite a few time series this is running one of our larger clusters and there's you know thousands of containers",
    "start": "1326620",
    "end": "1332499"
  },
  {
    "text": "and and I think each machine has like 16 cores so this is kind of this is not",
    "start": "1332499",
    "end": "1338049"
  },
  {
    "text": "millions but this is hundreds of thousands of of time-series and because it's kind of we turn the caching off we",
    "start": "1338049",
    "end": "1344499"
  },
  {
    "text": "just want to highlight the effect of the parallelization this query is artificially slow but but it really",
    "start": "1344499",
    "end": "1351929"
  },
  {
    "text": "demonstrates the effect so without the without the caching and without the parallelization this query was taking",
    "start": "1351929",
    "end": "1357549"
  },
  {
    "text": "about ten seconds in our dev environment and once we turn the caching and parallelization on the exact same query",
    "start": "1357549",
    "end": "1363309"
  },
  {
    "text": "on the exact same data was about three times faster but we were hoping it would",
    "start": "1363309",
    "end": "1368350"
  },
  {
    "text": "be 16 times faster but we'll take three combined with everything else I think that's a pretty big pretty big",
    "start": "1368350",
    "end": "1373389"
  },
  {
    "text": "improvement and once you turn caching on once you deploy this in environment with enough resources and and hundreds of",
    "start": "1373389",
    "end": "1379450"
  },
  {
    "text": "replicas of the query service this actually gets significantly faster yeah so I think that's amazing",
    "start": "1379450",
    "end": "1387450"
  },
  {
    "text": "and just to prove it's really doing that this is the this is a Yaeger trace of the query and you can see that it's",
    "start": "1387450",
    "end": "1394480"
  },
  {
    "start": "1388000",
    "end": "1436000"
  },
  {
    "text": "actually processing each each partial query in parallel which is super cool we",
    "start": "1394480",
    "end": "1401350"
  },
  {
    "text": "haven't done this yet but we want to take this technology and apply it to Thanos right in a Thanos environment you",
    "start": "1401350",
    "end": "1408279"
  },
  {
    "text": "can actually guarantee certain sets of data are isolated because they're stored at the edges in the individual Promethea",
    "start": "1408279",
    "end": "1415470"
  },
  {
    "text": "so you know once we start you know once we start merging this with some of the Thanos work we can actually hopefully",
    "start": "1415470",
    "end": "1421869"
  },
  {
    "text": "push down these partial aggregations out to the edges and only ship back you know",
    "start": "1421869",
    "end": "1427480"
  },
  {
    "text": "some pre aggregated data and do the reallocation centrally and hopefully I think with something like Santos I think",
    "start": "1427480",
    "end": "1432820"
  },
  {
    "text": "it's gonna make even bigger difference than it makes with cortex so what are we gonna do in the future well we've kind",
    "start": "1432820",
    "end": "1439299"
  },
  {
    "start": "1436000",
    "end": "1525000"
  },
  {
    "text": "of you know I don't know whether you've realized what we kind of built is a query planner for prom QL like if",
    "start": "1439299",
    "end": "1444669"
  },
  {
    "text": "anyone's hears from with SQL databases we're really kind of looking at the query we're picking it apart we're figuring out which bits can",
    "start": "1444669",
    "end": "1450010"
  },
  {
    "text": "be executed in parallel you know so we really want to kind of tease this apart a bit more and separate kind of the",
    "start": "1450010",
    "end": "1455890"
  },
  {
    "text": "planning from the execution of the queries so that we can do even more interesting optimizations one of the",
    "start": "1455890",
    "end": "1461530"
  },
  {
    "text": "things I really want to do is like averages can't be can't be reallocated",
    "start": "1461530",
    "end": "1466600"
  },
  {
    "text": "right you get a different answer if you do that but if you decompose an average into a sum divided by account you can",
    "start": "1466600",
    "end": "1472090"
  },
  {
    "text": "parallelize the individual sum and count do the binary operation centrally and and you know different tricks like this",
    "start": "1472090",
    "end": "1477940"
  },
  {
    "text": "I think will make even more different Prometheus queries parallelizable and faster and then this is one we've been",
    "start": "1477940",
    "end": "1484990"
  },
  {
    "text": "talking about I don't quite know whether this is actually going to work yet we haven't done the work but one of the",
    "start": "1484990",
    "end": "1490030"
  },
  {
    "text": "weak points with Prometheus is if you do like you want a year's worth of data and you want to do like rate 30 days you",
    "start": "1490030",
    "end": "1496900"
  },
  {
    "text": "know over you know get a rolling one month window over a year's worth of data every single point is going to have to",
    "start": "1496900",
    "end": "1503590"
  },
  {
    "text": "consider a whole months worth of data and this gets really expensive so we",
    "start": "1503590",
    "end": "1508810"
  },
  {
    "text": "want to kind of decompose a 30 day rate into like a series of one-day rates and then combining this with like maybe",
    "start": "1508810",
    "end": "1514990"
  },
  {
    "text": "memorizing the cache or something hopefully reuse overlapping one-day rates I think this is going to be really",
    "start": "1514990",
    "end": "1521230"
  },
  {
    "text": "cool and this is this is where we're going to take this I hope anyway the",
    "start": "1521230",
    "end": "1526840"
  },
  {
    "start": "1525000",
    "end": "1913000"
  },
  {
    "text": "last and the most obvious this is probably not the first time you're going to see this meme today yeah anyone have",
    "start": "1526840",
    "end": "1533260"
  },
  {
    "text": "any questions thank you very much",
    "start": "1533260",
    "end": "1536250"
  },
  {
    "text": "yeah I'm not I'm not gonna do any product pictures but if you do want them we have a booth in the other room and they'll give you them there go on how do",
    "start": "1541070",
    "end": "1553800"
  },
  {
    "text": "we even start to identify which queries are running slow it's a good question in production we log every single query out",
    "start": "1553800",
    "end": "1560400"
  },
  {
    "text": "to out two loci actually funnily enough and then we've built some scripts that process those logs link them to traces",
    "start": "1560400",
    "end": "1567630"
  },
  {
    "text": "and and we can just kind of literally look over the last few weeks worth of queries yet the ones that were in that",
    "start": "1567630",
    "end": "1573630"
  },
  {
    "text": "1% little slowest and then go and view the traces for them but this is kind of custom tooling we've built internally I",
    "start": "1573630",
    "end": "1579320"
  },
  {
    "text": "said I wasn't gonna do any pot of piss but if you go to the booth we're actually showing how we're trying to",
    "start": "1579320",
    "end": "1584910"
  },
  {
    "text": "make a lot of that kind of those tools we've developed like more generic and more reusable for other people so you can actually engrave on them now links",
    "start": "1584910",
    "end": "1591720"
  },
  {
    "text": "directly from a log to a trace but we do it just with logs there's also an option",
    "start": "1591720",
    "end": "1597330"
  },
  {
    "text": "on the query front-end now too we log every query because we've got a logging infrastructure but if you don't have",
    "start": "1597330",
    "end": "1602670"
  },
  {
    "text": "that or if you don't want to log that much volume you can set tell accrue front-end only to log queries over a",
    "start": "1602670",
    "end": "1608130"
  },
  {
    "text": "certain amount of time or log slow queries and yeah that's basically we do log analysis to identify slow queries",
    "start": "1608130",
    "end": "1614640"
  },
  {
    "text": "over there yeah",
    "start": "1614640",
    "end": "1624650"
  },
  {
    "text": "so the question was you know the thesis is we're repeating queries that overlap over existing data but in a production",
    "start": "1634029",
    "end": "1640749"
  },
  {
    "text": "environment those windows are moving so is there really that that benefit I mean it depends what you do right I think",
    "start": "1640749",
    "end": "1647499"
  },
  {
    "text": "it's pretty typical to set your graph on to look over the last hour or maybe the last three hours worth of data and if",
    "start": "1647499",
    "end": "1654460"
  },
  {
    "text": "you're going back to that repeatedly then yes you're going to have quite a strong cache hit race cache hit rate",
    "start": "1654460",
    "end": "1659469"
  },
  {
    "text": "because yeah you're gonna like your you might say we have our graph on a set for a 10-second refresh period so even if",
    "start": "1659469",
    "end": "1666190"
  },
  {
    "text": "you've got the window open for a few minutes you probably issued you know 20 queries for each panel and you're gonna",
    "start": "1666190",
    "end": "1672009"
  },
  {
    "text": "have a huge hit rate just in the moment but between different sessions maybe not you know between different sessions it",
    "start": "1672009",
    "end": "1678369"
  },
  {
    "text": "depends how big your team is it depends who's looking at these queries you know one of the things the father has its own",
    "start": "1678369",
    "end": "1684249"
  },
  {
    "text": "inbuilt alerting system I'm not you know I don't use it myself but we use it internally or grow fana labs and that is just",
    "start": "1684249",
    "end": "1690849"
  },
  {
    "text": "constantly doing the same query over and over again like every few and no tens of seconds or something and when we",
    "start": "1690849",
    "end": "1697629"
  },
  {
    "text": "introduce the caching system that saw a huge increase in performance that saw a",
    "start": "1697629",
    "end": "1702909"
  },
  {
    "text": "huge cache hit rate but it does it is work like dependent like I think if you only ever go to a graph are no graphs",
    "start": "1702909",
    "end": "1708070"
  },
  {
    "text": "like once a month then no there's not gonna be any cache it right like it's really for people who are using it more",
    "start": "1708070",
    "end": "1713559"
  },
  {
    "text": "actively than that does that answer question oh yeah how much time we got go",
    "start": "1713559",
    "end": "1720580"
  },
  {
    "text": "on quick follow-up",
    "start": "1720580",
    "end": "1723118"
  },
  {
    "text": "I think it works the intuition is it works I just haven't like sat down done the math yeah cool definitely I know one more question",
    "start": "1728320",
    "end": "1735510"
  },
  {
    "text": "yep no I said yeah it's an interesting",
    "start": "1741540",
    "end": "1747649"
  },
  {
    "text": "question yeah so the question was we we",
    "start": "1747649",
    "end": "1756529"
  },
  {
    "text": "we use Prometheus X or chunks which is like I forgotten the name of the paper",
    "start": "1756529",
    "end": "1762169"
  },
  {
    "text": "really famous there gorilla paper yeah we basically use the gorilla encoding in",
    "start": "1762169",
    "end": "1767209"
  },
  {
    "text": "chunks for each individual time series in Prometheus and in cortex and in Thanos it's quite a popular system we we don't",
    "start": "1767209",
    "end": "1775369"
  },
  {
    "text": "use the same format for the query results we actually like use protobufs it's interesting that you mention I",
    "start": "1775369",
    "end": "1781609"
  },
  {
    "text": "haven't thought of it like because some of these query results are very large like we've had to configure our memcache",
    "start": "1781609",
    "end": "1786919"
  },
  {
    "text": "is to like store values of hundreds of megabytes of protobufs and yeah I see no reason why we couldn't encode them into",
    "start": "1786919",
    "end": "1794119"
  },
  {
    "text": "the same X or chunk format yeah that makes a lot of sense even better right the X or chunk format when you have very",
    "start": "1794119",
    "end": "1799999"
  },
  {
    "text": "very repeatable timestamps like with very even differences between them gets really efficient and in query results",
    "start": "1799999",
    "end": "1806419"
  },
  {
    "text": "you definitely have very even gaps between your values I might I might actually go and look at look at doing that I think it's a really good idea",
    "start": "1806419",
    "end": "1811879"
  },
  {
    "text": "thank you gone then we get this every time every",
    "start": "1811879",
    "end": "1823489"
  },
  {
    "text": "time so that one of the panels maintainer is sitting just here Bartek sorry so the the question was I've",
    "start": "1823489",
    "end": "1829939"
  },
  {
    "text": "mentioned cortex and Thanos multiple times and there's there any plans to merge these projects and categorically I",
    "start": "1829939",
    "end": "1835339"
  },
  {
    "text": "can say here yes no I don't know like",
    "start": "1835339",
    "end": "1841269"
  },
  {
    "text": "that's like the Thanos like what I you",
    "start": "1841359",
    "end": "1846529"
  },
  {
    "text": "know Bartek and I both live in London we both enjoy going for a beer together I think I would love to work more with him",
    "start": "1846529",
    "end": "1852799"
  },
  {
    "text": "and more closely with him making things like the query front-end work with honest we've actually recently embedded",
    "start": "1852799",
    "end": "1859339"
  },
  {
    "text": "Thanos into cortex so cortex can now store blocks in object stores in exactly",
    "start": "1859339",
    "end": "1864439"
  },
  {
    "text": "the same way that us can and this opens up more opportunities to collaborate on like extending Thanos performance for",
    "start": "1864439",
    "end": "1871069"
  },
  {
    "text": "like the store gateway and stuff I like the fan esteem you know it's a large overlap with the Prometheus team",
    "start": "1871069",
    "end": "1876739"
  },
  {
    "text": "and with the cortex team and I look forward to collaborating more if that means the projects merge great if it doesn't",
    "start": "1876739",
    "end": "1882470"
  },
  {
    "text": "that's probably also fine yeah yeah then they differ they're very different",
    "start": "1882470",
    "end": "1888049"
  },
  {
    "text": "deployment models and and that's not going to change right so but no good question we get it every time one more",
    "start": "1888049",
    "end": "1894350"
  },
  {
    "text": "question I think I've got time I'm going to keep going until someone stops me actually at the back over there we do as well so but",
    "start": "1894350",
    "end": "1905000"
  },
  {
    "text": "we don't we don't expose metrics for every individual query because the cardinality of that metric would be too",
    "start": "1905000",
    "end": "1910429"
  },
  {
    "text": "high but actually if you you look here we've probably whilst running this yeah",
    "start": "1910429",
    "end": "1915590"
  },
  {
    "start": "1913000",
    "end": "2061000"
  },
  {
    "text": "whilst running this you see we've logged a slow query that took 1.4 seconds so",
    "start": "1915590",
    "end": "1921350"
  },
  {
    "text": "that's that's really the key for like things that can be used with like events and logs and things that can use new",
    "start": "1921350",
    "end": "1926419"
  },
  {
    "text": "metrics like if we put that query into a metric label the cardinality of that",
    "start": "1926419",
    "end": "1931429"
  },
  {
    "text": "metric would blow up really quickly and then you need something like this to query it yeah oh the question was why",
    "start": "1931429",
    "end": "1938900"
  },
  {
    "text": "don't I put the we log slow crews why don't we put it in a metric and it's all about cardinality yeah any more",
    "start": "1938900",
    "end": "1944870"
  },
  {
    "text": "questions no more how am I missing anyone Chris go on you had your hand moving no",
    "start": "1944870",
    "end": "1950559"
  },
  {
    "text": "you were saying two minutes okay cool nope no more quick okay one more last one",
    "start": "1950559",
    "end": "1957070"
  },
  {
    "text": "yeah",
    "start": "1962750",
    "end": "1965140"
  },
  {
    "text": "but yeah the question was do we have like an explained on queries like we",
    "start": "1969620",
    "end": "1974990"
  },
  {
    "text": "don't we don't have at least a user accessible one I don't think there's anything can Prometheus that does this",
    "start": "1974990",
    "end": "1980029"
  },
  {
    "text": "either I think it's a great idea like as you said we have Yaeger so internally we can see why queries are",
    "start": "1980029",
    "end": "1985880"
  },
  {
    "text": "slow because we just go and look at Yaeger and I don't know whether we could expose that to customers because I don't",
    "start": "1985880",
    "end": "1991909"
  },
  {
    "text": "know that there's no like well there shouldn't be any interplay between between different customers and there shouldn't be any any data in there that",
    "start": "1991909",
    "end": "1998779"
  },
  {
    "text": "wouldn't be safe but maybe maybe that's the solution is would be to expose the the trace the the Yaeger trace to the",
    "start": "1998779",
    "end": "2006159"
  },
  {
    "text": "customers so if they want to they can go through and you can actually export Yaeger traces as big blobs of JSON and then you could just you know give an API",
    "start": "2006159",
    "end": "2013539"
  },
  {
    "text": "for a customer to go and get that and then they could load it into their own Yaeger instance and visualize it ya know",
    "start": "2013539",
    "end": "2019000"
  },
  {
    "text": "we don't we don't have it but I think it's a good idea and especially when the query planner starts getting more and",
    "start": "2019000",
    "end": "2024220"
  },
  {
    "text": "more complicated and you know eventually we're gonna have to deploy various different heuristics we're already doing this for like deciding what to query in",
    "start": "2024220",
    "end": "2031779"
  },
  {
    "text": "the index you know there's basically no point in querying very very large dimensions in",
    "start": "2031779",
    "end": "2036789"
  },
  {
    "text": "the index so we kind of already use some basic heuristics when we're hitting our inverted index to kind of figure out",
    "start": "2036789",
    "end": "2042429"
  },
  {
    "text": "what to query and yeah like there's definitely feedback where you could give to the user saying like just don't",
    "start": "2042429",
    "end": "2048669"
  },
  {
    "text": "bother touching this label it's got you know it's got 30 million entries in it doing reg X's over that I'm never going",
    "start": "2048669",
    "end": "2054368"
  },
  {
    "text": "to work but know it's a good idea we don't do it but I think we should I'm going to finish there thank you very",
    "start": "2054369",
    "end": "2059378"
  },
  {
    "text": "much enjoy the rest of Cuba com [Applause]",
    "start": "2059379",
    "end": "2063260"
  }
]