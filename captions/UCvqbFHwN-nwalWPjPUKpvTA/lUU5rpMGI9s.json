[
  {
    "text": "hi welcome to our talk on enabling autonomous teams through policy enforcement before we get started a quick",
    "start": "0",
    "end": "5759"
  },
  {
    "text": "introduction about ourselves my name is james alsoth and i'm a security engineer at ubico currently",
    "start": "5759",
    "end": "11040"
  },
  {
    "text": "focused on cloud infrastructure security presenting with me is john reese who is a software engineer at yubico with a lot",
    "start": "11040",
    "end": "17600"
  },
  {
    "text": "of experience in go and kubernetes before we get started let's go over what we're going to be discussing today",
    "start": "17600",
    "end": "23760"
  },
  {
    "text": "first we're going to start with a brief history of kubernetes at yubico how we got started on our kubernetes journey and some of the previous gaps in",
    "start": "23760",
    "end": "30560"
  },
  {
    "text": "our kubernetes tech stack we're then going to discuss how policy helps to address those gaps and enable",
    "start": "30560",
    "end": "36800"
  },
  {
    "text": "more autonomous teams we're definitely going to talk about the awesome open source tooling that enables us to enforce these policies",
    "start": "36800",
    "end": "44320"
  },
  {
    "text": "and we're going to wrap up with discussing our journey thus far sort of where we are now and where we",
    "start": "44320",
    "end": "49440"
  },
  {
    "text": "see ourselves in the future and of course there will be some time with them for questions",
    "start": "49440",
    "end": "55520"
  },
  {
    "text": "so let's chat about how ubico got started on its kubernetes adoption journey for us that started about two years ago",
    "start": "55520",
    "end": "61840"
  },
  {
    "text": "with an initiative led by our infrastructure team to standardize the platform that our services run on",
    "start": "61840",
    "end": "67040"
  },
  {
    "text": "uh previous to this we were mostly running on virtual machines and we didn't have too many",
    "start": "67040",
    "end": "72080"
  },
  {
    "text": "containerized workloads yet like many organizations we used to manage kubernetes service to get up and",
    "start": "72080",
    "end": "78479"
  },
  {
    "text": "running as fast as possible and to help avoid some of the pain points of cluster setup and cluster",
    "start": "78479",
    "end": "84840"
  },
  {
    "text": "management for us though probably the first question that pops into our head is how can we ensure that the kubernetes",
    "start": "84840",
    "end": "90400"
  },
  {
    "text": "workloads are configured securely and what that's really asking is how do we control changes that are made to the",
    "start": "90400",
    "end": "96320"
  },
  {
    "text": "cluster well we started where i think most organizations do leaning on three things",
    "start": "96320",
    "end": "102560"
  },
  {
    "text": "authentication authorization as well as consistent peer review",
    "start": "102560",
    "end": "108320"
  },
  {
    "text": "diving into the first of those we were able to take advantage of the managed kubernetes offering",
    "start": "108640",
    "end": "114079"
  },
  {
    "text": "than that it allowed us to tie into our existing identity provider very easily so we got up and running with authentication pretty quick",
    "start": "114079",
    "end": "121200"
  },
  {
    "text": "additionally it probably comes to no surprise to those of you who are familiar with yubico but we also require strong multi-factor",
    "start": "121200",
    "end": "127680"
  },
  {
    "text": "authentication with web on web authent and yubikeys additionally we",
    "start": "127680",
    "end": "132720"
  },
  {
    "text": "regularly expire the sessions for those with access to infrastructure requiring re-authentication with",
    "start": "132720",
    "end": "138080"
  },
  {
    "text": "multi-factor authentication frequently moving on to the role-based authorization thankfully",
    "start": "138080",
    "end": "144720"
  },
  {
    "text": "kubernetes has role-based access control built in and it has for quite a few versions now this allows us to tie",
    "start": "144720",
    "end": "150879"
  },
  {
    "text": "users groups and service accounts to any any role these roles can either be",
    "start": "150879",
    "end": "156400"
  },
  {
    "text": "scoped to a namespace or they can be applied cluster-wide these roles allow for extremely",
    "start": "156400",
    "end": "162879"
  },
  {
    "text": "fine-grained permissions allowing you to specify the exact verbs an actor can use such as create update or delete what",
    "start": "162879",
    "end": "170080"
  },
  {
    "text": "types of resources they can act on such as a deployment resource and it can even go as far as",
    "start": "170080",
    "end": "175440"
  },
  {
    "text": "allow restricting it only to specifically named resources",
    "start": "175440",
    "end": "180720"
  },
  {
    "text": "again our managed kubernetes offering made this easier by tying groups into from our idp into this system",
    "start": "180720",
    "end": "189280"
  },
  {
    "text": "and the final tool in our toolbelt was peer review for us we enforce this using github branch protection rules ensuring that",
    "start": "189760",
    "end": "196239"
  },
  {
    "text": "all changes happened through pull requests and on each of these pull requests we required at least one other person to",
    "start": "196239",
    "end": "202319"
  },
  {
    "text": "review for us most of this peer review work landed on our infrastructure team because they",
    "start": "202319",
    "end": "209440"
  },
  {
    "text": "had the most experience of kubernetes and had spent the most time to learn about all of the best practices",
    "start": "209440",
    "end": "214640"
  },
  {
    "text": "security or otherwise about peer review though it definitely",
    "start": "214640",
    "end": "220560"
  },
  {
    "text": "has some drawbacks when it's the only way that you're restricting changes to your clusters for one it needs to be consistent in",
    "start": "220560",
    "end": "227040"
  },
  {
    "text": "order to be effective and when you have consistent review that's a significant time investment for the reviewers",
    "start": "227040",
    "end": "233680"
  },
  {
    "text": "because of this it often bottlenecks on the team or individual that has the most experience with the technology",
    "start": "233680",
    "end": "239200"
  },
  {
    "text": "which of course in this case is kubernetes and all of this adds up to slowing down the release cycle",
    "start": "239200",
    "end": "244959"
  },
  {
    "text": "and this is important because when he teams hit too much friction they often start to work around your processes whether you know it or not",
    "start": "244959",
    "end": "252640"
  },
  {
    "text": "this is of course bad for security because you no longer have control over these configurations",
    "start": "252640",
    "end": "257919"
  },
  {
    "text": "but they're also bad for just general cluster consistency and maintainability",
    "start": "257919",
    "end": "264079"
  },
  {
    "text": "none of this was really a surprise to us we kind of saw this coming from a mile away but that didn't make the problem any less",
    "start": "264320",
    "end": "270639"
  },
  {
    "text": "real when we had to deal with it so what we did is we spent some time researching what other",
    "start": "270639",
    "end": "276479"
  },
  {
    "text": "organizations were doing and what the kubernetes community was doing and for us the answer became abundantly",
    "start": "276479",
    "end": "281600"
  },
  {
    "text": "clear policy was the way forward when people think of policy it's usually a negative reaction as they",
    "start": "281600",
    "end": "287840"
  },
  {
    "text": "imagine having more hoops to jump through in order to get their work done however in our case since all of these",
    "start": "287840",
    "end": "294880"
  },
  {
    "text": "changes to kubernetes happen through the api server working with structured json data we can automate the enforcement of these",
    "start": "294880",
    "end": "301280"
  },
  {
    "text": "policies entirely but what do we mean when we say that in this context well",
    "start": "301280",
    "end": "307120"
  },
  {
    "text": "it allows us to enforce what we actually care about for example we don't really care that a services team is deploying a",
    "start": "307120",
    "end": "313600"
  },
  {
    "text": "new version of their service that's a part of their core job function however we do care that when they do",
    "start": "313600",
    "end": "318880"
  },
  {
    "text": "that the resources are configured securely for example we probably want to ensure",
    "start": "318880",
    "end": "324800"
  },
  {
    "text": "that the workloads aren't running as root and that they don't have any extra linux capabilities attached to them",
    "start": "324800",
    "end": "331600"
  },
  {
    "text": "these policies can also easily extend past security related settings though for example we can require each resource",
    "start": "331600",
    "end": "338479"
  },
  {
    "text": "in a namespace to have a certain label set that identifies the owner of that resource that makes it easy so when you're",
    "start": "338479",
    "end": "345199"
  },
  {
    "text": "working on troubleshooting an issue or you just need to know who's who owns that resource it's right there in the",
    "start": "345199",
    "end": "350400"
  },
  {
    "text": "metadata of the resource with that i'd like to turn it over to john to discuss the tooling we've",
    "start": "350400",
    "end": "356560"
  },
  {
    "text": "selected to enforce these policies thanks james so as james mentioned we",
    "start": "356560",
    "end": "362000"
  },
  {
    "text": "knew we wanted to use policy to solve a lot of the problems we were having at yubico we looked at a lot of the tools out",
    "start": "362000",
    "end": "368479"
  },
  {
    "text": "there that solved this problem but opa was the clear winner in this space we saw a lot of adoption with",
    "start": "368479",
    "end": "375440"
  },
  {
    "text": "other tools that we knew we wanted to leverage and it came with its own policy language but before getting into",
    "start": "375440",
    "end": "382400"
  },
  {
    "text": "opa itself it's really important to understand what a policy is what it looks like and rego itself so",
    "start": "382400",
    "end": "389600"
  },
  {
    "text": "rego is the policy language that opa knows that understands and on my screen here",
    "start": "389600",
    "end": "396720"
  },
  {
    "text": "you can see a policy for a kubernetes manifest that says that it must have",
    "start": "396720",
    "end": "402880"
  },
  {
    "text": "a owner label on it specifically namespaces must have an owner label so when a when",
    "start": "402880",
    "end": "409120"
  },
  {
    "text": "a request comes in in the cluster for a namespace creation this policy will first check to see",
    "start": "409120",
    "end": "415199"
  },
  {
    "text": "this input that's coming in this input document is it of type namespace and if it doesn't have an owner's label",
    "start": "415199",
    "end": "421919"
  },
  {
    "text": "on it return a message that says namespaces must have an owner so the user the uh the individual trying to",
    "start": "421919",
    "end": "428319"
  },
  {
    "text": "deploy this namespace knows how to fix it and so the important takeaway here",
    "start": "428319",
    "end": "434000"
  },
  {
    "text": "is the input keyword the input keyword denotes a input document for rego and",
    "start": "434000",
    "end": "440080"
  },
  {
    "text": "the input document is just the structured data in our case it's a yaml file",
    "start": "440080",
    "end": "445280"
  },
  {
    "text": "so anything beyond the input dot should look really familiar we see kind we see metadata but the the input dot",
    "start": "445280",
    "end": "453440"
  },
  {
    "text": "and anything after that is just dependent upon the data that you give it it could be terraform",
    "start": "453440",
    "end": "459360"
  },
  {
    "text": "it could be a docker file any sort of of structured data so again very generic language um",
    "start": "459360",
    "end": "467120"
  },
  {
    "text": "immensely immensely powerful and so now that we have this this rego file we need a way to actually",
    "start": "467120",
    "end": "474000"
  },
  {
    "text": "determine if the document that we give it would be would be in violation and there's a few",
    "start": "474000",
    "end": "479199"
  },
  {
    "text": "ways to do this right we could we could be the input document the the rego to james and he can verify on a",
    "start": "479199",
    "end": "486639"
  },
  {
    "text": "case-by-case basis whether or not the document is uh is violated or not but we're we're all about automation so",
    "start": "486639",
    "end": "492800"
  },
  {
    "text": "as briefly talked about before we decided to go with uh with the open policy agent and just like",
    "start": "492800",
    "end": "498000"
  },
  {
    "text": "just look at the logo of of course we did um there's there's just there's no reason not to",
    "start": "498000",
    "end": "503039"
  },
  {
    "text": "choose opa it's a work of art but no really we um",
    "start": "503039",
    "end": "508720"
  },
  {
    "text": "the community is great again there's there's so much adoption around opa it was um it was it's been it's been",
    "start": "508720",
    "end": "514560"
  },
  {
    "text": "a real joy to leverage opa um working in their their slack channel",
    "start": "514560",
    "end": "520320"
  },
  {
    "text": "everyone's super friendly there's always someone there to help you out it was it was a really good a really",
    "start": "520320",
    "end": "525839"
  },
  {
    "text": "really good choice for us and so how this how this works how opa",
    "start": "525839",
    "end": "531200"
  },
  {
    "text": "works as a service you uh you deploy it somewhere be it kubernetes be it a web server wherever you want to",
    "start": "531200",
    "end": "538080"
  },
  {
    "text": "put it as long as you can get a web request to it um and then you also include your your",
    "start": "538080",
    "end": "543680"
  },
  {
    "text": "regular files with that with that deployment so opa can know which policies you want it to enforce and so you you",
    "start": "543680",
    "end": "550880"
  },
  {
    "text": "submit an input document again be it a kubernetes manifest be it a docker file anything you want",
    "start": "550880",
    "end": "556880"
  },
  {
    "text": "give it to the opa service it will validate the the document run through run it through the policies and",
    "start": "556880",
    "end": "562560"
  },
  {
    "text": "tell you does this document violate any of the policies you have loaded into it and there's also the the",
    "start": "562560",
    "end": "569279"
  },
  {
    "text": "really nice piece of the fact that it can also take external data so you can see here in the the",
    "start": "569279",
    "end": "575120"
  },
  {
    "text": "bottom right the the data document that's external data and that can come from",
    "start": "575120",
    "end": "580480"
  },
  {
    "text": "any number of sources so if we build on the previous example of the policy where that teams must have a",
    "start": "580480",
    "end": "587440"
  },
  {
    "text": "namespace label all name spaces must have a label of owner on it we could also add a policy that says",
    "start": "587440",
    "end": "593120"
  },
  {
    "text": "that owners teams can only own a single namespace and so in order to do that we would need some",
    "start": "593120",
    "end": "600560"
  },
  {
    "text": "form of external data in this case a count of how many namespaces that they've already",
    "start": "600560",
    "end": "606160"
  },
  {
    "text": "created and so when we create namespaces when we delete namespaces we can keep",
    "start": "606160",
    "end": "612079"
  },
  {
    "text": "track of that number give it to opa and then opa can use that when it's when it's evaluating all of its policies",
    "start": "612079",
    "end": "618880"
  },
  {
    "text": "and so while that was an example of using op as a service you can actually use opa as a library",
    "start": "618880",
    "end": "624720"
  },
  {
    "text": "which makes it so much easier to to enforce policy there's a lot of tools out there",
    "start": "624720",
    "end": "631040"
  },
  {
    "text": "that will actually take the opa engine um import it as a dependency and then run",
    "start": "631040",
    "end": "637040"
  },
  {
    "text": "the the same checks that uh that opa itself would and so we really wanted to leverage this",
    "start": "637040",
    "end": "644640"
  },
  {
    "text": "um this type of functionality in order to shift our policy enforcement to the left because we quickly realized",
    "start": "644640",
    "end": "651200"
  },
  {
    "text": "that when we deployed opa while we were able to have this policy enforcement this policy",
    "start": "651200",
    "end": "657600"
  },
  {
    "text": "validation our engineers didn't really know whether or not the code they were writing the policies and",
    "start": "657600",
    "end": "665120"
  },
  {
    "text": "uh manifest they were writing would be in violation until they actually deployed it",
    "start": "665120",
    "end": "670640"
  },
  {
    "text": "so we really wanted a solution that we could give them put on their local machine and they",
    "start": "670640",
    "end": "676240"
  },
  {
    "text": "could just run it without having to worry about any of that and that silly networking stuff",
    "start": "676240",
    "end": "681519"
  },
  {
    "text": "and so we found a tool called called comptest and comtess more or less lets you",
    "start": "681519",
    "end": "689360"
  },
  {
    "text": "run opa on on on your local machine like opa you can but contest just makes the",
    "start": "689360",
    "end": "695279"
  },
  {
    "text": "experience so much better so like when i said before you can give any input document to opa",
    "start": "695279",
    "end": "702800"
  },
  {
    "text": "it doesn't really care it's like it's half true it still doesn't care but as long as it's yaml or json so",
    "start": "702800",
    "end": "709920"
  },
  {
    "text": "com tests can actually take a lot of different file formats you can see here ini tomal hcl and it can convert those",
    "start": "709920",
    "end": "716560"
  },
  {
    "text": "file formats into into json and then shepard into opa so you can actually use",
    "start": "716560",
    "end": "722639"
  },
  {
    "text": "any format with with opa there's also some nicety about being able to take in files from from",
    "start": "722639",
    "end": "729360"
  },
  {
    "text": "different folders anywhere on your on your machine and then print the result again in a user",
    "start": "729360",
    "end": "735040"
  },
  {
    "text": "friendly way so com test is for local policy validation it's for",
    "start": "735040",
    "end": "741920"
  },
  {
    "text": "pipeline validation in all of your your manifest and then anything beyond that would be",
    "start": "741920",
    "end": "748720"
  },
  {
    "text": "like opa deployed as a service for continue enforcement and so you can see here this is a",
    "start": "748720",
    "end": "754480"
  },
  {
    "text": "another example of a policy file on the left this time we're looking at deployments to make sure they're not",
    "start": "754480",
    "end": "760560"
  },
  {
    "text": "running as root and that the container has app label for pawn selectors",
    "start": "760560",
    "end": "766000"
  },
  {
    "text": "and on the right we have our input document our deploy.yaml itself and so if we actually run this",
    "start": "766000",
    "end": "772720"
  },
  {
    "text": "through com test using the test command all you have to do is pass in the policy.rego as well as the deploy.yaml",
    "start": "772720",
    "end": "779920"
  },
  {
    "text": "and then it will take that deploy.yml and see does that input document violate any of",
    "start": "779920",
    "end": "786160"
  },
  {
    "text": "the policies that we have set forth in this case there are there are two policies that were that were violated",
    "start": "786160",
    "end": "791920"
  },
  {
    "text": "and we can take this policy and we can apply it to the op as a service and get the exact",
    "start": "791920",
    "end": "797279"
  },
  {
    "text": "same behavior so we're checking both the the local environment as well as the",
    "start": "797279",
    "end": "802320"
  },
  {
    "text": "um as well as the production environment and then again this it can be run local machines pipelines and you can get",
    "start": "802320",
    "end": "808639"
  },
  {
    "text": "that immediate feedback there's also the benefit with comtess to be able to",
    "start": "808639",
    "end": "814639"
  },
  {
    "text": "share policies across the organization because it's a fair use case a valid use case",
    "start": "814639",
    "end": "820240"
  },
  {
    "text": "that your policies are managed and written by a completely separate team a security team much like",
    "start": "820240",
    "end": "825600"
  },
  {
    "text": "they are done at yubico we'll have a lot of different repositories we want to make sure that",
    "start": "825600",
    "end": "831839"
  },
  {
    "text": "the our policies are being enforced and there's really no way to get those policies without",
    "start": "831839",
    "end": "838480"
  },
  {
    "text": "trying them out in the cluster but with comptest you can actually push policies policy bundles to a oci",
    "start": "838480",
    "end": "845279"
  },
  {
    "text": "compliant registry and then pull them down for for later use so you can see here in the example",
    "start": "845279",
    "end": "850639"
  },
  {
    "text": "where we're pulling down a bundle of cluster policies and then we're actually running the",
    "start": "850639",
    "end": "857360"
  },
  {
    "text": "comptest test command locally on that bundle that we just pulled on that same deployed.yaml and then the",
    "start": "857360",
    "end": "863680"
  },
  {
    "text": "result is the same so this is a way to be able to push policies out there and then pull them",
    "start": "863680",
    "end": "869279"
  },
  {
    "text": "down for for cross-team usage this is really huge as well in um in pipelines or other kind of approaches",
    "start": "869279",
    "end": "875519"
  },
  {
    "text": "where you need to bundle together a lot of different policies and so artifact hub is",
    "start": "875519",
    "end": "883680"
  },
  {
    "text": "an attempt to be able to expose a lot of different policy bundles",
    "start": "883680",
    "end": "889120"
  },
  {
    "text": "because more or less like the policies that we really care about things like",
    "start": "889120",
    "end": "894320"
  },
  {
    "text": "containers having resource constraints containers not running as root those types of policies",
    "start": "894320",
    "end": "900560"
  },
  {
    "text": "should more or less be the same but we're kind of in a state now where teams companies organizations",
    "start": "900560",
    "end": "906240"
  },
  {
    "text": "are all writing the same policies we don't have a good distribution mechanism and artifact hub is again the the the",
    "start": "906240",
    "end": "913440"
  },
  {
    "text": "solution for that um it's currently it's a sandbox project there are a whole lot of policy bundles out there i think there's",
    "start": "913440",
    "end": "919920"
  },
  {
    "text": "like one or two but i would definitely keep an eye on this contribute where you can",
    "start": "919920",
    "end": "925519"
  },
  {
    "text": "if you do have a public bundle that you want to to contribute i recommend pushing it up there and let's",
    "start": "925519",
    "end": "931279"
  },
  {
    "text": "try to make this successful though so we can actually start sharing policies with one another not having to uh",
    "start": "931279",
    "end": "937040"
  },
  {
    "text": "to rewrite them over and over and over again for every team who wants to be able to use policy",
    "start": "937040",
    "end": "942079"
  },
  {
    "text": "and so a lot of the talks of we've a lot of the conversation we've had is all about policy enforcement on the",
    "start": "942079",
    "end": "947920"
  },
  {
    "text": "local environments but we also need to make sure that the policies are enforced in",
    "start": "947920",
    "end": "953360"
  },
  {
    "text": "in production and so once you get beyond the local environment we decided to go",
    "start": "953360",
    "end": "959040"
  },
  {
    "text": "with a tool called called gatekeeper and gatekeeper lets us enforce these",
    "start": "959040",
    "end": "965519"
  },
  {
    "text": "policies continuously inside inside of a kubernetes cluster it's a mission controller",
    "start": "965519",
    "end": "973519"
  },
  {
    "text": "so when you deploy a resource to kubernetes it'll go through the admission controller look at all the policy that",
    "start": "973519",
    "end": "979839"
  },
  {
    "text": "you have loaded and if any of the the resources that you are attempting to add to the cluster it",
    "start": "979839",
    "end": "986240"
  },
  {
    "text": "will reject or or let the policy go through it also has an and some audit functionality",
    "start": "986240",
    "end": "992480"
  },
  {
    "text": "saying it'll continually audit your cluster are there any resources that that are",
    "start": "992480",
    "end": "997680"
  },
  {
    "text": "violating policy if in the case that you're just starting to adopt gatekeeper",
    "start": "997680",
    "end": "1003040"
  },
  {
    "text": "and you want to see if i were to actually enforce this policy how many resources are kind of out of band or just in the",
    "start": "1003040",
    "end": "1009680"
  },
  {
    "text": "case that gatekeeper went down for a little bit you know want to make sure that there wasn't a a resource",
    "start": "1009680",
    "end": "1015600"
  },
  {
    "text": "that got in like during that that small window and again the huge one here is it's using the same policies so",
    "start": "1015600",
    "end": "1021759"
  },
  {
    "text": "we're if you're checking on your local versus in production we can always continuously use the same",
    "start": "1021759",
    "end": "1027360"
  },
  {
    "text": "policy that doesn't change from environment to environment but with that said um it's",
    "start": "1027360",
    "end": "1034160"
  },
  {
    "text": "almost the same policies so uh we go back to the input document um when you're",
    "start": "1034160",
    "end": "1041038"
  },
  {
    "text": "working with yaml the input document is going to be a yaml file so we would expect input.kind",
    "start": "1041039",
    "end": "1047240"
  },
  {
    "text": "input.metadata just like we saw before but when we're in the context of",
    "start": "1047240",
    "end": "1052400"
  },
  {
    "text": "admission control it's going to be a little different i bolded here input.review.object because this is what",
    "start": "1052400",
    "end": "1059039"
  },
  {
    "text": "a admission review is going to look like to to gatekeeper this is the the document that is going",
    "start": "1059039",
    "end": "1065600"
  },
  {
    "text": "to be received from opa so again it's not quite the same you would actually have",
    "start": "1065600",
    "end": "1071679"
  },
  {
    "text": "to write a policy that had input.review.object in it but that would only work for gatekeeper",
    "start": "1071679",
    "end": "1077760"
  },
  {
    "text": "and if you did just input.kind that wouldn't work for gatekeeper so what we ended up doing was really",
    "start": "1077760",
    "end": "1084559"
  },
  {
    "text": "adopting this idea of rego being the source of truth your policies because it really it really is rego is",
    "start": "1084559",
    "end": "1091440"
  },
  {
    "text": "generic it's it doesn't matter the uh the context that you're in it's really all about does this input",
    "start": "1091440",
    "end": "1099200"
  },
  {
    "text": "document that you're giving me violate this policy that you have defined and so in every",
    "start": "1099200",
    "end": "1104960"
  },
  {
    "text": "everything that's that you do that's based off of your rego should should adjust the changes of your",
    "start": "1104960",
    "end": "1110960"
  },
  {
    "text": "regula and not the other way around and so to solve this problem we actually wrote a tool called constraint",
    "start": "1110960",
    "end": "1117919"
  },
  {
    "text": "and constraint brings to the table three really important factors first and foremost it it",
    "start": "1117919",
    "end": "1123919"
  },
  {
    "text": "actually does provide a library where you can write policies that work with both comptest and gatekeeper",
    "start": "1123919",
    "end": "1129520"
  },
  {
    "text": "it's really just a a wrapper that normalizes a a polyfill that that will say",
    "start": "1129520",
    "end": "1136799"
  },
  {
    "text": "if you're in the context of gatekeeper then spit out your policies for input.review.object if you're in the",
    "start": "1136799",
    "end": "1143039"
  },
  {
    "text": "context of conf test it's just input dot and that handles all of that for you so your policies can be completely",
    "start": "1143039",
    "end": "1150799"
  },
  {
    "text": "unchanged no matter what environment that you're running in the other benefit is the template and constraint creation and",
    "start": "1150799",
    "end": "1157919"
  },
  {
    "text": "management so when you saw on the previous constraint uh previous screen the the rego there was actually embedded",
    "start": "1157919",
    "end": "1165280"
  },
  {
    "text": "into the constraint template the yaml and that's because that's how gatekeeper is able to um",
    "start": "1165280",
    "end": "1172160"
  },
  {
    "text": "to uh to load in your policies it's just done through a yaml file and so you're gonna have a rego file on",
    "start": "1172160",
    "end": "1178799"
  },
  {
    "text": "disk and if you were to change that rego file you would also have to",
    "start": "1178799",
    "end": "1184080"
  },
  {
    "text": "copy and paste your changes into that yaml which isn't the most ideal situation and so",
    "start": "1184080",
    "end": "1190240"
  },
  {
    "text": "what constraint will actually do is it will look at all of your radio files and then it will actually",
    "start": "1190240",
    "end": "1196480"
  },
  {
    "text": "generate the template and constraint for you so you never have to touch yaml you're only",
    "start": "1196480",
    "end": "1202400"
  },
  {
    "text": "focused purely on on your rego file and then lastly it actually will generate documentation",
    "start": "1202400",
    "end": "1208240"
  },
  {
    "text": "for your policies we really wanted to give our engineers the ability to see what policies were being enforced as",
    "start": "1208240",
    "end": "1214720"
  },
  {
    "text": "well as how they can how they can resolve them if they ever run into a policy violation",
    "start": "1214720",
    "end": "1220720"
  },
  {
    "text": "and so when it comes to the policies themselves it'll look relatively familiar that the",
    "start": "1220720",
    "end": "1226080"
  },
  {
    "text": "biggest difference here is definitely the the comment header we added some some metadata to to the",
    "start": "1226080",
    "end": "1232400"
  },
  {
    "text": "policy in form of a header comment where we say title is the the title of of the the policy",
    "start": "1232400",
    "end": "1241039"
  },
  {
    "text": "here we're saying images must not use the latest tag and then why this policy exists or",
    "start": "1241039",
    "end": "1246880"
  },
  {
    "text": "really any other flavor that you want to give it to give this policy be the description or anything",
    "start": "1246880",
    "end": "1252400"
  },
  {
    "text": "else the the enforcement type here is saying deny the alternative is dry run in case you",
    "start": "1252400",
    "end": "1259120"
  },
  {
    "text": "want to test out this policy in your cluster and not actually do any sort of enforcement and then which kinds which",
    "start": "1259120",
    "end": "1266080"
  },
  {
    "text": "kubernetes resources that this policy will be enforced on be it just ingresses",
    "start": "1266080",
    "end": "1271919"
  },
  {
    "text": "be it just namespaces workloads etc so we define here a list of resources that this policy",
    "start": "1271919",
    "end": "1277760"
  },
  {
    "text": "isn't forced on and then we use this metadata to generate all the other yamls that we were previously talking about and in the violation",
    "start": "1277760",
    "end": "1285039"
  },
  {
    "text": "itself you see here we're importing two libraries which is the constraint library",
    "start": "1285039",
    "end": "1290159"
  },
  {
    "text": "the the important note here is that pods.container our pods library will actually look at",
    "start": "1290159",
    "end": "1297120"
  },
  {
    "text": "containers from any possible source because pods can come from cron jobs they can come from",
    "start": "1297120",
    "end": "1302880"
  },
  {
    "text": "deployment staple sets daemon sets there's a large list of those and they're all embedded in kubernetes",
    "start": "1302880",
    "end": "1308799"
  },
  {
    "text": "resources differently so pods handles that for you and then we can take",
    "start": "1308799",
    "end": "1314559"
  },
  {
    "text": "the the resulting container that comes out of that does that container have an image of latest and if it does",
    "start": "1314559",
    "end": "1321120"
  },
  {
    "text": "give a note to the the user saying that images must not use the the latest tag and again we have two",
    "start": "1321120",
    "end": "1327039"
  },
  {
    "text": "commands for that one to generate the the templates and the constraints and then one for for the markdown",
    "start": "1327039",
    "end": "1332880"
  },
  {
    "text": "describing describing our our policies and then this is an example of what the",
    "start": "1332880",
    "end": "1339919"
  },
  {
    "text": "documentation looks like you can see here the the id p1001 we also embedded something into",
    "start": "1339919",
    "end": "1348080"
  },
  {
    "text": "into the tool that you can actually assign an id to a policy so you can refer back to it",
    "start": "1348080",
    "end": "1353440"
  },
  {
    "text": "instead of just using a title the the severity is coming from the the severity of the rule because in rego you",
    "start": "1353440",
    "end": "1359840"
  },
  {
    "text": "can have warning you can have deny you can have violation and based on that rule",
    "start": "1359840",
    "end": "1365200"
  },
  {
    "text": "different things will happen so we pull the severity out of the rego and put that in the document for you",
    "start": "1365200",
    "end": "1370400"
  },
  {
    "text": "same with the resources that it impacts the the description that dictates what this",
    "start": "1370400",
    "end": "1376799"
  },
  {
    "text": "policy does or why we have it and then this is the the rego itself if you want that information",
    "start": "1376799",
    "end": "1382960"
  },
  {
    "text": "again completely completely configurable if you change your rego again all the documentation is",
    "start": "1382960",
    "end": "1388559"
  },
  {
    "text": "regenerated none of this is hand typed and then you also are given a link to to the source and",
    "start": "1388559",
    "end": "1394880"
  },
  {
    "text": "that source can either be a relative url if the policy lives in the same location of the source the same repository",
    "start": "1394880",
    "end": "1401600"
  },
  {
    "text": "or if it's a remote repository the source can also be a uh be a url so we um",
    "start": "1401600",
    "end": "1408960"
  },
  {
    "text": "we found a lot of usage out of this tool we use it for um like almost every day",
    "start": "1408960",
    "end": "1415120"
  },
  {
    "text": "it's really ingrained into into our workflows when it comes to gatekeeper and policy so",
    "start": "1415120",
    "end": "1421120"
  },
  {
    "text": "if you're interested if it sounds something like you'd want to use always open to uh to talk about it",
    "start": "1421120",
    "end": "1428799"
  },
  {
    "text": "contributions always welcome and uh that's really all i have for for the tooling piece of it",
    "start": "1428799",
    "end": "1435039"
  },
  {
    "text": "uh james is going to kind of go into into deeper detail of how we actually have leveraged these",
    "start": "1435039",
    "end": "1441120"
  },
  {
    "text": "tools in our policies and in our pipelines and processes",
    "start": "1441120",
    "end": "1446240"
  },
  {
    "text": "thanks john now that we have an understanding of how the tooling works together let's dive into ubiko's policy journey",
    "start": "1447440",
    "end": "1454159"
  },
  {
    "text": "in the beginning since we didn't have too many workloads on kubernetes yet and we were sure that we had consistent",
    "start": "1454159",
    "end": "1459520"
  },
  {
    "text": "peer review happening we started with a simple plan because we didn't anticipate too many resources",
    "start": "1459520",
    "end": "1464720"
  },
  {
    "text": "would violate these policies we would start with writing the policies and their tests after which we would",
    "start": "1464720",
    "end": "1470960"
  },
  {
    "text": "engage with our services teams to add these checks to their ci flows using conf test",
    "start": "1470960",
    "end": "1476000"
  },
  {
    "text": "simultaneous to that we would be working on deploying gatekeeper to our clusters in audit only mode",
    "start": "1476000",
    "end": "1481679"
  },
  {
    "text": "and then finally after we had worked with the teams to remediate all of the identified issues we would flip the switch and move",
    "start": "1481679",
    "end": "1488000"
  },
  {
    "text": "gatekeeper into enforcement mode as you might have expected from how i framed the previous slide this plan ran",
    "start": "1488000",
    "end": "1495120"
  },
  {
    "text": "into some issues we made it as far as writing and testing the policies but when we moved to engage",
    "start": "1495120",
    "end": "1500159"
  },
  {
    "text": "with some select teams to add conf tests to their ci flows the issues became pretty readily",
    "start": "1500159",
    "end": "1505679"
  },
  {
    "text": "apparent well for one there were more violations than anticipated",
    "start": "1505679",
    "end": "1511679"
  },
  {
    "text": "which meant that there was a potentially a very long window between when we identified these resources and when we",
    "start": "1511679",
    "end": "1517919"
  },
  {
    "text": "would actually be able to apply remediation across all of our clusters",
    "start": "1517919",
    "end": "1523360"
  },
  {
    "text": "this was compounded by the fact that we were all not only migrating existing workloads to",
    "start": "1523360",
    "end": "1529200"
  },
  {
    "text": "kubernetes but we were in a growth period hiring and starting up some new services on",
    "start": "1529200",
    "end": "1534240"
  },
  {
    "text": "kubernetes as well additionally since the policies were",
    "start": "1534240",
    "end": "1539440"
  },
  {
    "text": "always essentially production there was no way to safely test new policies or changes to existing policies",
    "start": "1539440",
    "end": "1546320"
  },
  {
    "text": "this meant that in a ci flow with conf test the ci flow would fail immediately if a violation was found and similarly",
    "start": "1546320",
    "end": "1554960"
  },
  {
    "text": "gatekeeper would just reject changes to the cluster if it didn't meet policy",
    "start": "1554960",
    "end": "1560960"
  },
  {
    "text": "additionally adding conf test to rci flows wasn't as easy as it could have been for one it kind of required our services",
    "start": "1561440",
    "end": "1568080"
  },
  {
    "text": "teams to know the comp test flags and sort of how it worked and additionally these it took multiple",
    "start": "1568080",
    "end": "1574320"
  },
  {
    "text": "steps to actually get policies from a remote source into the repo and then run the test",
    "start": "1574320",
    "end": "1580880"
  },
  {
    "text": "but most importantly the contest results weren't surface to the teams working on the resources",
    "start": "1580880",
    "end": "1586240"
  },
  {
    "text": "this meant that unless there was an actual violation no one would even know what tests were",
    "start": "1586240",
    "end": "1591279"
  },
  {
    "text": "run or anything like that this was especially important to us because we do have some policies that we",
    "start": "1591279",
    "end": "1596320"
  },
  {
    "text": "have labeled as just warnings which aren't blockers for deployment but they are a way that we want to",
    "start": "1596320",
    "end": "1602000"
  },
  {
    "text": "communicate to our teams that the way that they have the resources configured might not be best",
    "start": "1602000",
    "end": "1607360"
  },
  {
    "text": "practices practice finally policy admins such as myself had no",
    "start": "1607360",
    "end": "1613840"
  },
  {
    "text": "visibility into the test results so we didn't really have any way to track which",
    "start": "1613840",
    "end": "1618880"
  },
  {
    "text": "repositories or which teams had started using the policies or any of the results uh for which policies were making",
    "start": "1618880",
    "end": "1625279"
  },
  {
    "text": "the test runs fail or which ones were just emitting warnings",
    "start": "1625279",
    "end": "1630640"
  },
  {
    "text": "with these pain points identified we determined that there was essentially two things that we needed to do",
    "start": "1630640",
    "end": "1636400"
  },
  {
    "text": "we needed to build a policy pipeline and that pipeline would ensure that the policies are safe to enforce throughout our clusters",
    "start": "1636400",
    "end": "1642480"
  },
  {
    "text": "and we had to make the policy adoption as easy as possible if either of these weren't true it was a",
    "start": "1642480",
    "end": "1648399"
  },
  {
    "text": "pretty good chance that they our policies wouldn't be adopted or it would just be a long uphill battle",
    "start": "1648399",
    "end": "1653679"
  },
  {
    "text": "trying to get our teams to adopt them so first let's focus on what we did to",
    "start": "1653679",
    "end": "1658960"
  },
  {
    "text": "make that policy adoption as easy as possible and to do that we created two github",
    "start": "1658960",
    "end": "1664159"
  },
  {
    "text": "actions the first is a wrapper around conf test itself and it addresses some of the pain",
    "start": "1664159",
    "end": "1669200"
  },
  {
    "text": "points from the previous slides it automatically pulls the latest policies from a remote source",
    "start": "1669200",
    "end": "1675360"
  },
  {
    "text": "it surfaces the policy violation warnings and violations and warnings into the pull request comments so that the teams",
    "start": "1675360",
    "end": "1681600"
  },
  {
    "text": "working on the resources can see the test results",
    "start": "1681600",
    "end": "1686640"
  },
  {
    "text": "and it submits the results to a remote server so policy admins can monitor the deployments",
    "start": "1686640",
    "end": "1694159"
  },
  {
    "text": "the second addresses an issue that we learned about later was that some of our teams had started",
    "start": "1695039",
    "end": "1701120"
  },
  {
    "text": "to adopt flux cd for continuous delivery and we're using its helm operator and",
    "start": "1701120",
    "end": "1706640"
  },
  {
    "text": "this home operator has a custom resource that lets you specify the helm chart",
    "start": "1706640",
    "end": "1711840"
  },
  {
    "text": "source and then what values you want to apply to that chart and then it will go and fetch everything needed and just",
    "start": "1711840",
    "end": "1718240"
  },
  {
    "text": "make those changes in your cluster for you however since helm templates are just",
    "start": "1718240",
    "end": "1724159"
  },
  {
    "text": "that they're templates they aren't kubernetes resources we couldn't use config on them directly",
    "start": "1724159",
    "end": "1729360"
  },
  {
    "text": "and this is because the data structure in the yaml was different and even if it was the same with",
    "start": "1729360",
    "end": "1734960"
  },
  {
    "text": "templates you wouldn't actually have everything you needed until after execution so what this action does is it parses",
    "start": "1734960",
    "end": "1742399"
  },
  {
    "text": "the the helm release resource it pulls the chart info the version all",
    "start": "1742399",
    "end": "1748320"
  },
  {
    "text": "of those things it automatically sets up a helm repository so that it can pull those",
    "start": "1748320",
    "end": "1754080"
  },
  {
    "text": "templates from the remote repository and then executes the templates so with this we can easily",
    "start": "1754080",
    "end": "1760320"
  },
  {
    "text": "template out these resources before we run conf test so that it's nice and easy and it's a",
    "start": "1760320",
    "end": "1765360"
  },
  {
    "text": "solid flow this makes it easy for our developers",
    "start": "1765360",
    "end": "1770399"
  },
  {
    "text": "because they don't have to remember all of the flags for the helm template command or with the argument order or anything like that one thing to note",
    "start": "1770399",
    "end": "1778240"
  },
  {
    "text": "here is that this currently only supports public helm repositories but we're working on adding support for private repositories too",
    "start": "1778240",
    "end": "1785600"
  },
  {
    "text": "so with the ease of adoption addressed let's move on to the policy pipeline itself",
    "start": "1785600",
    "end": "1791600"
  },
  {
    "text": "early on in the design of the policy pipeline we have this one rule is that we must not ever break",
    "start": "1791600",
    "end": "1797039"
  },
  {
    "text": "production and for us that also includes any of the pipelines leading up to production",
    "start": "1797039",
    "end": "1802240"
  },
  {
    "text": "this meant that for our teams that have an automated deployment from development to staging to production",
    "start": "1802240",
    "end": "1808080"
  },
  {
    "text": "using custom metrics that breaking their access to the development cluster is the same as breaking their production",
    "start": "1808080",
    "end": "1813919"
  },
  {
    "text": "pipeline in order to accomplish this we used two main methodologies",
    "start": "1813919",
    "end": "1819760"
  },
  {
    "text": "the first was data-driven policy promotion and the second was a git ops deployment flow",
    "start": "1819760",
    "end": "1826320"
  },
  {
    "text": "so diving into our policy promotion strategy we wanted to tackle one policy at a time",
    "start": "1826320",
    "end": "1832399"
  },
  {
    "text": "which again is just reducing the window that new resources are introduced that violate the policy while you're",
    "start": "1832399",
    "end": "1838399"
  },
  {
    "text": "working about on remediating the ones that you already know about another key component is we use",
    "start": "1838399",
    "end": "1845039"
  },
  {
    "text": "gatekeepers enforcement action property to enforce to introduce the policies in dry run mode",
    "start": "1845039",
    "end": "1850559"
  },
  {
    "text": "where they are not enforced however when they are in this mode we can still use gatekeepers audit functionality",
    "start": "1850559",
    "end": "1857039"
  },
  {
    "text": "to audit our resources in our clusters and see which of them violate the policies",
    "start": "1857039",
    "end": "1864000"
  },
  {
    "text": "we've made the decision to only switch to enforcement mode after we've identified that all of the",
    "start": "1864000",
    "end": "1869440"
  },
  {
    "text": "offending resources have been remediated and as a side effect of this we have",
    "start": "1869440",
    "end": "1874720"
  },
  {
    "text": "avoided setting hard deadlines for our teams to update their resources and make sure",
    "start": "1874720",
    "end": "1880000"
  },
  {
    "text": "that they're remediated so one thing that we've done to work around that is in the case where a team",
    "start": "1880000",
    "end": "1886480"
  },
  {
    "text": "says they just really can't get this fixed in the next couple weeks we can add temporary",
    "start": "1886480",
    "end": "1892000"
  },
  {
    "text": "exceptions to the policy for those specific resources in specific name spaces",
    "start": "1892000",
    "end": "1897120"
  },
  {
    "text": "but only when necessary that way we still have good coverage of you know 99 of the resources in our clusters that",
    "start": "1897120",
    "end": "1903600"
  },
  {
    "text": "are here adhering to the policy and finally we only promote policies to",
    "start": "1903600",
    "end": "1910720"
  },
  {
    "text": "production when they're linked to a change management ticket which allows them to be scheduled",
    "start": "1910720",
    "end": "1916240"
  },
  {
    "text": "this of course ensures that all the potentially impacted parties are aware of this upcoming change and it lets us schedule around times",
    "start": "1916240",
    "end": "1923039"
  },
  {
    "text": "that we may be adding a feature release a product launch or something like that moving on to our get ops deployment flow",
    "start": "1923039",
    "end": "1929760"
  },
  {
    "text": "approach it's a pretty standard approach where we use pull requests to move policies throughout the pipeline",
    "start": "1929760",
    "end": "1935600"
  },
  {
    "text": "we use branch protection rules to ensure that peer review occurs and that all of our unit tests pass",
    "start": "1935600",
    "end": "1941360"
  },
  {
    "text": "a couple of things that we're doing there is we require each policy have a unique policy identifier and we",
    "start": "1941360",
    "end": "1947360"
  },
  {
    "text": "require that each policy have at least two unit tests one is for the positive path where",
    "start": "1947360",
    "end": "1953519"
  },
  {
    "text": "gatekeeper blocks something that we expect it to and the second is the negative path where we",
    "start": "1953519",
    "end": "1959679"
  },
  {
    "text": "a gatekeeper allow something through that we expect it to not lock",
    "start": "1959679",
    "end": "1964960"
  },
  {
    "text": "we also take advantage of the github code owner's features which ensures that the peer reviews come",
    "start": "1965519",
    "end": "1971120"
  },
  {
    "text": "from policy admins and that's because the policy admins are the ones who are the most familiar with the regular language",
    "start": "1971120",
    "end": "1976399"
  },
  {
    "text": "and also some of the more intricate details of kubernetes",
    "start": "1976399",
    "end": "1982158"
  },
  {
    "text": "and what's probably the most important point is that we use automation to create the gatekeeper and conf test resources",
    "start": "1982640",
    "end": "1989840"
  },
  {
    "text": "this means that reviewers can focus on the policy and not the gatekeeper resources you",
    "start": "1989840",
    "end": "1995039"
  },
  {
    "text": "know those giant emo files and it also makes it really obvious if someone is attempting to go around the",
    "start": "1995039",
    "end": "2000399"
  },
  {
    "text": "tooling in place because no human should ever be trying to modify the gatekeeper or conf test",
    "start": "2000399",
    "end": "2005600"
  },
  {
    "text": "resources directly with that let's go through the life of a policy in this policy pipeline",
    "start": "2005600",
    "end": "2012000"
  },
  {
    "text": "so here we have a higher level view of how the policy flows we start by introducing a policy into",
    "start": "2012000",
    "end": "2018399"
  },
  {
    "text": "the dev branch through a pull request and we want to make sure that the policy always has the enforcement action set to dry run",
    "start": "2018399",
    "end": "2025360"
  },
  {
    "text": "after it's merged flux will automatically pick it up and start syncing it to the development clusters",
    "start": "2025360",
    "end": "2031200"
  },
  {
    "text": "next we then promote this to staging and production branches still in dry run mode and this is to",
    "start": "2031200",
    "end": "2036720"
  },
  {
    "text": "ensure that we have full visibility across all of our clusters because try as we might to keep all of these clusters configured identically",
    "start": "2036720",
    "end": "2043919"
  },
  {
    "text": "through development staging and prod there's almost inevitably some variation in how they're configured",
    "start": "2043919",
    "end": "2051280"
  },
  {
    "text": "so one thing to note is when we merge into the production branch is when we actually generate the conf test",
    "start": "2051280",
    "end": "2056398"
  },
  {
    "text": "resources and one thing that we do there since conf test doesn't have the concept of",
    "start": "2056399",
    "end": "2062079"
  },
  {
    "text": "an enforcement action is that we parse that enforcement action from the comment header and we",
    "start": "2062079",
    "end": "2068638"
  },
  {
    "text": "in line rewrite any violation or deny rules to warnings so that when they're used in the ci",
    "start": "2068639",
    "end": "2075280"
  },
  {
    "text": "flows the warnings are still surfaced to the teams working on those resources but it doesn't fail the ci flows yet",
    "start": "2075280",
    "end": "2084079"
  },
  {
    "text": "so once we have all of these set up and running we then use the gatekeeper audit data as the source of truth",
    "start": "2084079",
    "end": "2089679"
  },
  {
    "text": "to identify our existing resources that violate the policy and then we open tickets to work with",
    "start": "2089679",
    "end": "2096240"
  },
  {
    "text": "our teams to remediate the issues that we've identified or to add exceptions where necessary",
    "start": "2096240",
    "end": "2103440"
  },
  {
    "text": "after we've ensured that those are all remediated and the audit data shows this we switch the development clusters and",
    "start": "2103440",
    "end": "2110160"
  },
  {
    "text": "the development branch to actually enforcement this is done by changing the enforcement action in the",
    "start": "2110160",
    "end": "2115839"
  },
  {
    "text": "header from dry run to deny and once again after we make this change and merge to",
    "start": "2115839",
    "end": "2121680"
  },
  {
    "text": "dev we are continuously monitoring the gatekeeper audit data to make sure that there isn't anything that we didn't",
    "start": "2121680",
    "end": "2128839"
  },
  {
    "text": "expect finally once we do that we follow the same flow to promote to staging and then production",
    "start": "2128839",
    "end": "2136079"
  },
  {
    "text": "it's worth noting again that when we make this change to production is when the conf test policies will",
    "start": "2136079",
    "end": "2142480"
  },
  {
    "text": "actually be changed back to either a violation or a deny",
    "start": "2142480",
    "end": "2147680"
  },
  {
    "text": "so at this point is when the ci flows will start failing if they have resources that violate the",
    "start": "2147680",
    "end": "2153440"
  },
  {
    "text": "policy the comptest metrics here is useful for",
    "start": "2153440",
    "end": "2159119"
  },
  {
    "text": "us to see what's going on across our organization to have a more full picture especially of newer repositories and",
    "start": "2159119",
    "end": "2165839"
  },
  {
    "text": "newer projects that haven't actually made it into a cluster yet but they are not the source of truth for when we promote",
    "start": "2165839",
    "end": "2172480"
  },
  {
    "text": "policies so where is ubico now on its policy journey",
    "start": "2172480",
    "end": "2178000"
  },
  {
    "text": "well we have gatekeeper deployed to all of our clusters and we're tracking the resources that violate our policies",
    "start": "2178000",
    "end": "2184079"
  },
  {
    "text": "additionally we're making our way through each policy and moving each to enforcement as we go we've",
    "start": "2184079",
    "end": "2189920"
  },
  {
    "text": "noticed that in a lot of scenarios these policy violations are actually introduced upstream",
    "start": "2189920",
    "end": "2195440"
  },
  {
    "text": "whether that be internal or external project and whenever we run into that we try and make our",
    "start": "2195440",
    "end": "2200480"
  },
  {
    "text": "upstream our changes as well looking into the future though there's a few ways that we can work on making this",
    "start": "2200480",
    "end": "2206560"
  },
  {
    "text": "better the first is to enable our teams to write their own policies for enforcing",
    "start": "2206560",
    "end": "2211599"
  },
  {
    "text": "their own specific best practices this can be anything from a custom label to requiring that every deployment have",
    "start": "2211599",
    "end": "2218320"
  },
  {
    "text": "a horizontal pod auto scaler attached additionally we're looking into syncing",
    "start": "2218320",
    "end": "2223440"
  },
  {
    "text": "data from outside the cluster into gatekeeper to be used for more informed policy decisions for",
    "start": "2223440",
    "end": "2229040"
  },
  {
    "text": "example we might want to sync our on-call rotation schedule into gatekeeper so that only the person who's",
    "start": "2229040",
    "end": "2235040"
  },
  {
    "text": "on call for a given team can make changes to production in the event that a production issue or outage",
    "start": "2235040",
    "end": "2240320"
  },
  {
    "text": "is occurring looking even further we are considering adding mutation controllers as well",
    "start": "2240320",
    "end": "2246240"
  },
  {
    "text": "and those are a different type of controller that the api server can work with which rather than just rejecting a",
    "start": "2246240",
    "end": "2252079"
  },
  {
    "text": "resource if it doesn't meet policy it can inline change the defaults as",
    "start": "2252079",
    "end": "2257119"
  },
  {
    "text": "needed one thing to note there is gatekeeper currently has an open design for their implementation of that",
    "start": "2257119",
    "end": "2264480"
  },
  {
    "text": "they haven't started on it yet so if you want to add your thoughts about how that should be shaped or anything like that",
    "start": "2264480",
    "end": "2270960"
  },
  {
    "text": "go ahead and just search on the opa slack for mutation design and it'll come right up",
    "start": "2270960",
    "end": "2278160"
  },
  {
    "text": "and that about wraps it up thank you everyone for attending and we are going to open it for questions",
    "start": "2278800",
    "end": "2285838"
  }
]