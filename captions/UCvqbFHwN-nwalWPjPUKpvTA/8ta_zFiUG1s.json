[
  {
    "text": "hi everyone and welcome to our talk My name is Max Kerbesha and my name is Alexa Griffith I'm a senior software",
    "start": "240",
    "end": "7120"
  },
  {
    "text": "engineer at Bloomberg working on their AI platforms and uh today we will have a",
    "start": "7120",
    "end": "12400"
  },
  {
    "text": "little exploratory journey throughout the platform engineering genai universe",
    "start": "12400",
    "end": "18160"
  },
  {
    "text": "Our idea behind is like either you pick up little items out of this whole",
    "start": "18160",
    "end": "23439"
  },
  {
    "text": "universe or you see a full-blown large platform but you never see the journey in between and how you maybe come from",
    "start": "23439",
    "end": "29840"
  },
  {
    "text": "one thing to the other thing and that's how our journey will start for also today We would like to go step by step",
    "start": "29840",
    "end": "37040"
  },
  {
    "text": "through some little ideas We'll get you set up and then we'll dive into one to",
    "start": "37040",
    "end": "42239"
  },
  {
    "text": "other demos show you a few little things and uh yeah we'll round it up in the end",
    "start": "42239",
    "end": "47920"
  },
  {
    "text": "Sounds good So this map you maybe know and usually",
    "start": "47920",
    "end": "54719"
  },
  {
    "text": "you know this way more bigger landscape from the cloudnative computing foundation but this is a specialized map",
    "start": "54719",
    "end": "60399"
  },
  {
    "text": "just showing the AI landscape from Kubernetes and the CNCF universe The",
    "start": "60399",
    "end": "66000"
  },
  {
    "text": "very interesting thing is it's actually kind of small So AI happens with cloud",
    "start": "66000",
    "end": "72479"
  },
  {
    "text": "native but it also happens alongside of the cloud native universe",
    "start": "72479",
    "end": "78000"
  },
  {
    "text": "What I mean with this is like this beautiful map and that's actually insane and it's old",
    "start": "78000",
    "end": "84600"
  },
  {
    "text": "2023 Um but it's also great because you can see different blocks in it You see",
    "start": "84600",
    "end": "90000"
  },
  {
    "text": "infrastructure you see data analytics you see also the open source layer which is like on the bottom this uh kind of",
    "start": "90000",
    "end": "96560"
  },
  {
    "text": "greenish um area Now keep your eyes on it Don't blink because there will be a",
    "start": "96560",
    "end": "102159"
  },
  {
    "text": "little change between 2023 and 2024 It's almost feels like it's",
    "start": "102159",
    "end": "109200"
  },
  {
    "text": "exploded right I'm still waiting for the 2025 version I would love to bring it here",
    "start": "109200",
    "end": "115119"
  },
  {
    "text": "but I believe there's so much changes on the market that the guys have built this map need a little bit more time But why",
    "start": "115119",
    "end": "123200"
  },
  {
    "text": "this map is now relevant to us is that we need to understand we live in a cloudnative ecos and everything is",
    "start": "123200",
    "end": "130399"
  },
  {
    "text": "moving fast We have a lot of open source projects a lot of things going on but we very often talk about the same things",
    "start": "130399",
    "end": "137640"
  },
  {
    "text": "where yes you find also a lot of commercial solutions for sure but it",
    "start": "137640",
    "end": "142879"
  },
  {
    "text": "also means there's a total other entire echosace that we maybe haven't thought",
    "start": "142879",
    "end": "148000"
  },
  {
    "text": "of yet or maybe have already provided solutions right So Kubernetes platform engineering needs",
    "start": "148000",
    "end": "156080"
  },
  {
    "text": "to bring in some kind of flexibility of allow us to adopt to all of this very",
    "start": "156080",
    "end": "162800"
  },
  {
    "text": "fast changes with this very fast explosions on the market and maybe also roll out some of the uh market player",
    "start": "162800",
    "end": "169440"
  },
  {
    "text": "which we can see and Kubernetes is therefore the perfect platform because it knows already from 2020 some of the",
    "start": "169440",
    "end": "175840"
  },
  {
    "text": "automation hype then we go to edge IoT telco last year and always about",
    "start": "175840",
    "end": "182319"
  },
  {
    "text": "internal development platforms and stuff like that And this year if you look into the schedule similar to our talk at",
    "start": "182319",
    "end": "189040"
  },
  {
    "text": "least every second talk has something with AI written in it",
    "start": "189040",
    "end": "194400"
  },
  {
    "text": "But the reason for it is not just because it's a hype but we provide with Kubernetes and platform engineering",
    "start": "194400",
    "end": "200319"
  },
  {
    "text": "actually a very good foundation to drive this kind of innovation and provide also",
    "start": "200319",
    "end": "205519"
  },
  {
    "text": "an environment to frequently change some of the tools which maybe got outdated",
    "start": "205519",
    "end": "211040"
  },
  {
    "text": "because something new or fancier popped up into the market Not saying that this is a good thing but it's something we",
    "start": "211040",
    "end": "217440"
  },
  {
    "text": "should consider On the other hand side and that's why platform engineering becomes",
    "start": "217440",
    "end": "222959"
  },
  {
    "text": "irrelevant in this area is that we do not just have this infinite devops cycle",
    "start": "222959",
    "end": "228959"
  },
  {
    "text": "If you look closely to the image you can see that there's at least I would say three to four infinite devops cycle",
    "start": "228959",
    "end": "235280"
  },
  {
    "text": "which also all are interconnected and that complicated the way how we are working how we are",
    "start": "235280",
    "end": "241360"
  },
  {
    "text": "integrating tools dramatically It comes from just you build it to run it to you",
    "start": "241360",
    "end": "247760"
  },
  {
    "text": "build it you run it you find and to maintain your data and then you need to help others to use it to integrate it",
    "start": "247760",
    "end": "254239"
  },
  {
    "text": "with your AI applications to provide LLMs and so on and so forth",
    "start": "254239",
    "end": "261560"
  },
  {
    "text": "And this brings us to the big question what are my options to run genai workload and to enable others to use",
    "start": "263360",
    "end": "271840"
  },
  {
    "text": "them too Because what we also can see is that very clearly not every role in the",
    "start": "271840",
    "end": "278000"
  },
  {
    "text": "market is specialized in utilizing open source and going through the whole stack",
    "start": "278000",
    "end": "283360"
  },
  {
    "text": "If you're a data scientist you build it you run it doesn't work anymore You're a specialist in something",
    "start": "283360",
    "end": "289040"
  },
  {
    "text": "completely different And that's where we come in to bring an environment where we can help offloading some of the complex",
    "start": "289040",
    "end": "296160"
  },
  {
    "text": "environments of the complex integrations and make the life of someone else um easy And that's where we start also our",
    "start": "296160",
    "end": "302840"
  },
  {
    "text": "journey So for this one we talk about the TVP the thinnest viable platform And",
    "start": "302840",
    "end": "309520"
  },
  {
    "text": "as you can see we're swimming here in a little rudder boat I don't know if this is the right translation from my German",
    "start": "309520",
    "end": "315840"
  },
  {
    "text": "uh brain but it helps us to get around It does its things We do not think That's a",
    "start": "315840",
    "end": "322160"
  },
  {
    "text": "good start How does it looks like in a platform engineering perspective This little thingy for",
    "start": "322160",
    "end": "329039"
  },
  {
    "text": "getting around is actually quite big just to provide a platform and integrate",
    "start": "329039",
    "end": "334080"
  },
  {
    "text": "well with some cloud provider like AWS If I talk with my team about it we dream",
    "start": "334080",
    "end": "340960"
  },
  {
    "text": "about something like that one The cool thing is with open source projects like",
    "start": "340960",
    "end": "346600"
  },
  {
    "text": "Canoe we get the things more or less easy fast up and running For sure we",
    "start": "346600",
    "end": "352800"
  },
  {
    "text": "need to tweak for example the key to be integrated with our corporate environment but everything else is",
    "start": "352800",
    "end": "360080"
  },
  {
    "text": "coming more or less out of the hand And also in this rather complicated setup we",
    "start": "360080",
    "end": "366000"
  },
  {
    "text": "can also throw out for example um crossplane or ago workflows to simplify it in some way We",
    "start": "366000",
    "end": "373280"
  },
  {
    "text": "also maybe don't need backstage but we maybe also would like to provide backstage to make the entry point very",
    "start": "373280",
    "end": "379199"
  },
  {
    "text": "very easy for spinning up an inference service And we will look in this tool later a little bit more However with",
    "start": "379199",
    "end": "387199"
  },
  {
    "text": "this tool setup we can start in a very very fast way to provide a first",
    "start": "387199",
    "end": "392240"
  },
  {
    "text": "environment to work",
    "start": "392240",
    "end": "394960"
  },
  {
    "text": "within Not sure if the video plays now So normally I would like to play you",
    "start": "397639",
    "end": "403039"
  },
  {
    "text": "a video um which would show the setup So actually to prevent that the demon gods",
    "start": "403039",
    "end": "409199"
  },
  {
    "text": "uh screw up this talk we thought like we are smart and pre-record something Looks like the pre-recording demons also set",
    "start": "409199",
    "end": "415680"
  },
  {
    "text": "up this thing and uh make it a little bit bad for us Actually what I going to show you in the short demo you can just",
    "start": "415680",
    "end": "422720"
  },
  {
    "text": "imagine that we run a Cenoa deployment and you can see that um after some while we have the setup of all the different",
    "start": "422720",
    "end": "429840"
  },
  {
    "text": "applications that are needed for running our platform We see our git u Argo CD",
    "start": "429840",
    "end": "435360"
  },
  {
    "text": "being integrated We see the deployment of the [Music]",
    "start": "435360",
    "end": "441000"
  },
  {
    "text": "um maybe we'll get this done Um you see that we after a couple of minutes can go",
    "start": "441000",
    "end": "448319"
  },
  {
    "text": "into the Argo CD and see also that everything else getting deployed We will have the backstage We have the search",
    "start": "448319",
    "end": "453759"
  },
  {
    "text": "manager We have the external DNS who automatically starts integrating with the load balancer and setting up also",
    "start": "453759",
    "end": "460000"
  },
  {
    "text": "the routes for the machine so that we can after 15 minutes roundabout reach our",
    "start": "460000",
    "end": "466960"
  },
  {
    "text": "first services and that makes it for us very simple in the beginning to get the",
    "start": "466960",
    "end": "472000"
  },
  {
    "text": "things running and so from here we move on to our MVP",
    "start": "472000",
    "end": "477840"
  },
  {
    "text": "Thank you Okay So for our minimal viable platform we'll start with talking about",
    "start": "477840",
    "end": "484960"
  },
  {
    "text": "Kserve for serving inference in our DNA platform So Kserve is an open- source",
    "start": "484960",
    "end": "490160"
  },
  {
    "text": "tool that simplifies AI model development on Kubernetes by extracting away the complexity of Kubernetes Um it",
    "start": "490160",
    "end": "497440"
  },
  {
    "text": "allows developers to focus on models rather than worrying about all the configurations you need to run your AI",
    "start": "497440",
    "end": "503360"
  },
  {
    "text": "models So it supports multiple geni genai and ML frameworks making it",
    "start": "503360",
    "end": "508960"
  },
  {
    "text": "versatile and framework agnostic Um and the past uh we have we currently have a",
    "start": "508960",
    "end": "515440"
  },
  {
    "text": "unified API that also supports open AI So also what's great about Kserve is",
    "start": "515440",
    "end": "521200"
  },
  {
    "text": "that it integrates with other open source tools easily and provides a lot of out ofthe-box features",
    "start": "521200",
    "end": "528880"
  },
  {
    "text": "So KServe has been uh serving predictive inference for a while now and these are",
    "start": "528880",
    "end": "534959"
  },
  {
    "text": "all of the features that have come out of the box with that Um with Genai will add some more but these are the basic",
    "start": "534959",
    "end": "540800"
  },
  {
    "text": "features that come even with predictive inference So the ability to scale up and down from zero request batching um",
    "start": "540800",
    "end": "547800"
  },
  {
    "text": "security distributed tracing autoscaling on GPU and CPU uh logging different",
    "start": "547800",
    "end": "553519"
  },
  {
    "text": "observability and traffic traffic management So with genai has come a host",
    "start": "553519",
    "end": "559200"
  },
  {
    "text": "of new challenges and in order to tackle those and accommodate these we have",
    "start": "559200",
    "end": "564720"
  },
  {
    "text": "added a few new and upgraded features So adaptive scaling now we can do uh",
    "start": "564720",
    "end": "570880"
  },
  {
    "text": "there's built-in tokenbased autoscaling for our genai models also performance um",
    "start": "570880",
    "end": "576240"
  },
  {
    "text": "boost with model cache and prompt cache So this minimizes our latency and our ability to quickly um to quickly",
    "start": "576240",
    "end": "583519"
  },
  {
    "text": "download to quickly use a model that's already been downloaded rather than have to download it again Um also again like",
    "start": "583519",
    "end": "589760"
  },
  {
    "text": "I mentioned we have open AAI protocol support and we also have scalable inference So as models grow larger we",
    "start": "589760",
    "end": "596959"
  },
  {
    "text": "need to sometimes serve them on multiple nodes um with VLM and this gives us high performance workloads Additionally and",
    "start": "596959",
    "end": "604160"
  },
  {
    "text": "we'll talk about this in a bit too is our integration with the AI gateway project Um so we connect seamlessly with",
    "start": "604160",
    "end": "610640"
  },
  {
    "text": "our managed AI services giving us a better platform and Queser for example is an",
    "start": "610640",
    "end": "616880"
  },
  {
    "text": "perfect example where we can say like hey look we can provide you for whatever",
    "start": "616880",
    "end": "622160"
  },
  {
    "text": "position you are a template within backstage um where you can see the inference service on the bottom Um so",
    "start": "622160",
    "end": "629040"
  },
  {
    "text": "this will pop up in your front end and if you're let's say team A you would like to have the first inference service",
    "start": "629040",
    "end": "635120"
  },
  {
    "text": "just for yourself deployed you will go into backstage just fill out the basic information who's the owner which name",
    "start": "635120",
    "end": "641440"
  },
  {
    "text": "space for example it should be deployed can be also cluster whatsoever and you can also fill in like um some more",
    "start": "641440",
    "end": "647680"
  },
  {
    "text": "specifications about it like um different models uh whatever you would like to have and um through backstage",
    "start": "647680",
    "end": "654720"
  },
  {
    "text": "and the automation process you're going to go and deploy it for example in the inference cluster The cool thing is it",
    "start": "654720",
    "end": "660000"
  },
  {
    "text": "obviously will also work with other services So doesn't matter if you use KSERF VLM or if you even would like to",
    "start": "660000",
    "end": "666880"
  },
  {
    "text": "implement some other use cases For example creating a new tenant in Milvos's vector database So what it",
    "start": "666880",
    "end": "673279"
  },
  {
    "text": "allows you is that you can bring really the self-service for a team and being",
    "start": "673279",
    "end": "678480"
  },
  {
    "text": "autonomous integrating their learnings their models and giving them for example",
    "start": "678480",
    "end": "684320"
  },
  {
    "text": "the space in in a database or space in S3 bucket um just out of a backstage portal Yeah And I also like how this",
    "start": "684320",
    "end": "692079"
  },
  {
    "text": "slide shows how easy it is to uh set up a a service using Kserve you have a very",
    "start": "692079",
    "end": "697760"
  },
  {
    "text": "small um a very small YAML and you can get something up and running and have all the features that we just discussed",
    "start": "697760",
    "end": "705120"
  },
  {
    "text": "So I want to talk about another component and I'll show how this all fits together next Um Envoy AI gateway",
    "start": "705120",
    "end": "711279"
  },
  {
    "text": "It's a new open-source tool um built from the Envoy Gateway project that we've been working on and it adds",
    "start": "711279",
    "end": "717360"
  },
  {
    "text": "features to better serve AI workloads especially as a platform team So it's",
    "start": "717360",
    "end": "722399"
  },
  {
    "text": "built by the collaboration of Tetrate and Bloomberg engineers and it offers uh in the MVP centralized access So a",
    "start": "722399",
    "end": "729680"
  },
  {
    "text": "standard controlled and audible way to access both self-trained um and open-",
    "start": "729680",
    "end": "736320"
  },
  {
    "text": "source or commercial models It also uh provides a way to manage credentials",
    "start": "736320",
    "end": "741839"
  },
  {
    "text": "between different LM providers and on-prem And it offers out of the box cost monitoring tuning specific for your",
    "start": "741839",
    "end": "748320"
  },
  {
    "text": "geni workloads um like tokenbased limits and tokenbased cost",
    "start": "748320",
    "end": "754440"
  },
  {
    "text": "optimizations So this is a diagram of how the platform would fit together with the components that we just mentioned So",
    "start": "754440",
    "end": "760959"
  },
  {
    "text": "let's say you hit a load balancer and then you'll hit the envoy gateway So you'll get a these are just a few of the",
    "start": "760959",
    "end": "766560"
  },
  {
    "text": "features that I mentioned Um so as we know different L and providers might have different access patterns What's",
    "start": "766560",
    "end": "773040"
  },
  {
    "text": "super nice about this is from the client's perspective they just need to know one access pattern to hit envoy",
    "start": "773040",
    "end": "778560"
  },
  {
    "text": "gateway one centralized authentication method and under the hood the envoy AI AI gateway will route you to the correct",
    "start": "778560",
    "end": "785760"
  },
  {
    "text": "provider Um so this is an example of what your uh platform could look like in",
    "start": "785760",
    "end": "792000"
  },
  {
    "text": "a hybrid environment with a manage inference cluster and using an LLM provider",
    "start": "792000",
    "end": "798560"
  },
  {
    "text": "So we have a demo I hope it works Uh we'll see But anyways what it will show is um using an AI agent to call a tool",
    "start": "798560",
    "end": "807600"
  },
  {
    "text": "to get the a weather This is what a platform could look like uh architecture",
    "start": "807600",
    "end": "813440"
  },
  {
    "text": "when you do that So you can easily use gateway to uh have your agent route to",
    "start": "813440",
    "end": "819360"
  },
  {
    "text": "either a managed inference uh Kubernetes cluster or something like AWS Bedrock",
    "start": "819360",
    "end": "826839"
  },
  {
    "text": "Oh no Well there is a video here",
    "start": "828560",
    "end": "834880"
  },
  {
    "text": "but it's not showing up Maybe while we are fighting with the demon gods de demo gods not demon gods",
    "start": "834880",
    "end": "841760"
  },
  {
    "text": "whatever Same thing Are there already some first question about like the platform engineering part How does it",
    "start": "841760",
    "end": "848959"
  },
  {
    "text": "enable the AI or geni workflows anything that any questions there maybe on Canoe",
    "start": "848959",
    "end": "857199"
  },
  {
    "text": "or uh backstate Argo how the deployment",
    "start": "857199",
    "end": "862720"
  },
  {
    "text": "works If you're shy it's not a problem You can run to the microphone It's hidden in the shadows No one will see",
    "start": "863160",
    "end": "869880"
  },
  {
    "text": "you Give you the chance to still ask awesome questions",
    "start": "869880",
    "end": "875639"
  },
  {
    "text": "Are you aware of any example like can we ker",
    "start": "878959",
    "end": "885160"
  },
  {
    "text": "yeah so the question is if there's uh any example with sen with ksurf um I",
    "start": "885279",
    "end": "891279"
  },
  {
    "text": "haven't seen one uh I would like to work on one or would like to provide it to this one um actually the interesting",
    "start": "891279",
    "end": "897680"
  },
  {
    "text": "part is when you deploy ksurf it most of the time to run very very good requires",
    "start": "897680",
    "end": "903199"
  },
  {
    "text": "some part of so you have a very strict dependencies coming together right Um",
    "start": "903199",
    "end": "908880"
  },
  {
    "text": "and that's the the one downside of it So if you want to run the kerf inferencing model you have to go this way So maybe",
    "start": "908880",
    "end": "915519"
  },
  {
    "text": "you would prefer VLM or some other setup Um but if you're fine with that",
    "start": "915519",
    "end": "922079"
  },
  {
    "text": "dependency is obviously a very good choice The thing is why we use it here is like the setup itself is kind of easy",
    "start": "922079",
    "end": "929120"
  },
  {
    "text": "You just deploy it It's cluster wide available and it reacts very simply on inference service that you drop in for",
    "start": "929120",
    "end": "935600"
  },
  {
    "text": "backstage for example So um it makes a life very easy to get the first services up and",
    "start": "935600",
    "end": "942399"
  },
  {
    "text": "running Any other question",
    "start": "945160",
    "end": "949720"
  },
  {
    "text": "All right During the mech is starting I I would like to dance or something I like",
    "start": "955040",
    "end": "960320"
  },
  {
    "text": "tell [Laughter]",
    "start": "960320",
    "end": "966420"
  },
  {
    "text": "joke you guys run Well that depends on um so the question",
    "start": "972199",
    "end": "978320"
  },
  {
    "text": "is which LMS are used most frequently I first answer shortly this question Yeah",
    "start": "978320",
    "end": "984759"
  },
  {
    "text": "Um so it depends on I cannot answer this question for for Alexa's organization",
    "start": "984759",
    "end": "991440"
  },
  {
    "text": "but what we see is like um in the integration and the demo which we have running we use for example AWS bedrock",
    "start": "991440",
    "end": "997839"
  },
  {
    "text": "and just enable clot for example but doesn't matter you can also have uh JPD",
    "start": "997839",
    "end": "1003279"
  },
  {
    "text": "running or um set up your own ones like um um the the llama from from Facebook",
    "start": "1003279",
    "end": "1010160"
  },
  {
    "text": "right so it's very flexible but this is also the power of the API gateway um",
    "start": "1010160",
    "end": "1016160"
  },
  {
    "text": "which Alexa shows that you can just play around with the different LLMs and just",
    "start": "1016160",
    "end": "1021360"
  },
  {
    "text": "exchange them as you need it because you have a gateway in between who orchestrates for you all the other complexity away and that makes the",
    "start": "1021360",
    "end": "1028079"
  },
  {
    "text": "integration very easy please",
    "start": "1028079",
    "end": "1034120"
  },
  {
    "text": "the gateway is ready like for production ready is it we can just use it I think",
    "start": "1034400",
    "end": "1039839"
  },
  {
    "text": "that's more question for you it's it's getting there You can check out the Envoy Gateway project It's in the it's",
    "start": "1039839",
    "end": "1047360"
  },
  {
    "text": "under envoy gateway and uh yeah we uh we just announced it at",
    "start": "1047360",
    "end": "1053039"
  },
  {
    "text": "KubeCon NA and uh I think we had a big release that came out about a month ago",
    "start": "1053039",
    "end": "1058320"
  },
  {
    "text": "but we're definitely uh expecting people to come into the community and help us uh run it as well Thank you Yeah",
    "start": "1058320",
    "end": "1067679"
  },
  {
    "text": "In short there's always room for more contribution to it",
    "start": "1067679",
    "end": "1073200"
  },
  {
    "text": "But in a demo I hopefully will show you how how it is to use it though",
    "start": "1073200",
    "end": "1082039"
  },
  {
    "text": "Yeah we will put up the slides later on to the sket and then you can find the slides and the links to the videos um so",
    "start": "1094080",
    "end": "1101360"
  },
  {
    "text": "that you can also rewatch the demos Um some of the demos are quite easy and simple right It's just like a wrap-up of",
    "start": "1101360",
    "end": "1107919"
  },
  {
    "text": "like how does it looks like if you deploy an infrastructure with sceny Um Alexa's demo is a little bit more",
    "start": "1107919",
    "end": "1113360"
  },
  {
    "text": "complicated than that Um and then we would have also a third one looking into like how we enable through open",
    "start": "1113360",
    "end": "1120480"
  },
  {
    "text": "telemetry and open LLM uh the observability for large language",
    "start": "1120480",
    "end": "1126080"
  },
  {
    "text": "models and how does it forwarded to any kind of your preferred",
    "start": "1126080",
    "end": "1131679"
  },
  {
    "text": "um open observability tool Nice",
    "start": "1131679",
    "end": "1137360"
  },
  {
    "text": "Okay awesome Thank you guys So here we have a pretty basic agent Uh I'm going",
    "start": "1137360",
    "end": "1142480"
  },
  {
    "text": "to run it locally It's going to ask what's the weather like in New York City It's going to call to the LM running in",
    "start": "1142480",
    "end": "1149760"
  },
  {
    "text": "Bedrock It's going to get a response and that's going to say \"Hey use this tool.\"",
    "start": "1149760",
    "end": "1154799"
  },
  {
    "text": "Um and it's going to go use that tool and give us a response Um so yeah let's look at how that would happen So first I",
    "start": "1154799",
    "end": "1161919"
  },
  {
    "text": "just want to and I'll actually pause it here I just want to show so uh this is using the envoy gateway I just inspected",
    "start": "1161919",
    "end": "1168160"
  },
  {
    "text": "one of the resources and you can see here that we have um a backend set up and it shows we have a case of lm",
    "start": "1168160",
    "end": "1174720"
  },
  {
    "text": "backend which the value is dsplama and then we have an envoy basic AWS backend",
    "start": "1174720",
    "end": "1180240"
  },
  {
    "text": "and it's eu.enthropic So that value is what we're going to use in our request makes it",
    "start": "1180240",
    "end": "1185840"
  },
  {
    "text": "super easy U I'm going to port forward to the envoy service and I also just want to show you the name spaces we",
    "start": "1185840",
    "end": "1191280"
  },
  {
    "text": "create I have a kind cluster running Um this is what the AI gateway creates So",
    "start": "1191280",
    "end": "1196320"
  },
  {
    "text": "report forwarded Um now we are going to show",
    "start": "1196320",
    "end": "1202960"
  },
  {
    "text": "how we can uh run our agent and get a request back So here we're hitting DSP",
    "start": "1202960",
    "end": "1209039"
  },
  {
    "text": "llama blah blah blah That's our on-prem Kubernetes self-hosted service And I'm",
    "start": "1209039",
    "end": "1215360"
  },
  {
    "text": "also going to hit AWS bedrock um and also get an answer Now I'm using claude",
    "start": "1215360",
    "end": "1220960"
  },
  {
    "text": "uh anthropic and I'm also using llama So we get slightly different results but same same idea Also I want to show with",
    "start": "1220960",
    "end": "1226880"
  },
  {
    "text": "just a very small config that was super easy to do um I get out of this this out of the box uh cost monitoring as well So",
    "start": "1226880",
    "end": "1234960"
  },
  {
    "text": "that shows just how easy it is with the the same endpoint just the given the setup the value is um the name and we",
    "start": "1234960",
    "end": "1242799"
  },
  {
    "text": "could easily hit uh easily manage routing to both on prim and uh using an",
    "start": "1242799",
    "end": "1249039"
  },
  {
    "text": "LM provider like AWS But we'll have to do that one one more",
    "start": "1249039",
    "end": "1255440"
  },
  {
    "text": "time Yeah there's one more late Not right now but",
    "start": "1255440",
    "end": "1261640"
  },
  {
    "text": "later We'll see Um yeah thanks to the tech guys That was",
    "start": "1261640",
    "end": "1268799"
  },
  {
    "text": "really great",
    "start": "1268799",
    "end": "1272120"
  },
  {
    "text": "Okay So um with predictive inference it's common for users to you know manage",
    "start": "1279280",
    "end": "1285280"
  },
  {
    "text": "their own models train them everything but now it's also popular to use these out of the box you know open source or",
    "start": "1285280",
    "end": "1291360"
  },
  {
    "text": "vendor LLMs So this has changed our perspective of an inference platform a little So now we look towards the",
    "start": "1291360",
    "end": "1297679"
  },
  {
    "text": "solution as a platform team more focused on managing LLM and providing them as a service to our users So what we want to",
    "start": "1297679",
    "end": "1304720"
  },
  {
    "text": "do is provide AI without the overhead Uh give users access to state-of-the-art",
    "start": "1304720",
    "end": "1310240"
  },
  {
    "text": "updated LMS like Deepseek Llama Anthropic etc without deploying or",
    "start": "1310240",
    "end": "1316159"
  },
  {
    "text": "maintaining them Um we want to provide centralized access So we want an AI",
    "start": "1316159",
    "end": "1321760"
  },
  {
    "text": "gateway that abstracts all of these model management complexities making AI as simple as a consistent API call Um we",
    "start": "1321760",
    "end": "1330159"
  },
  {
    "text": "also want to give optimiz optimized performance with curated and up-to-date models Uh we want to be able to manage",
    "start": "1330159",
    "end": "1336960"
  },
  {
    "text": "these models well and focus on so our users can focus on innovation and not the infrastructure underlying it Uh",
    "start": "1336960",
    "end": "1343919"
  },
  {
    "text": "additionally we want to build it with you know Kubernetes native cloudnative principles to ensure that we're",
    "start": "1343919",
    "end": "1349520"
  },
  {
    "text": "efficient cost-effective um and flexible So let's dive into stage three",
    "start": "1349520",
    "end": "1357280"
  },
  {
    "text": "Now we've upgraded from a nice sailboat to a spaceship and we are going to talk",
    "start": "1357280",
    "end": "1362880"
  },
  {
    "text": "about our advanced features like intelligent load balancing observability patterns and different techniques for",
    "start": "1362880",
    "end": "1369280"
  },
  {
    "text": "optimizing our LLMs So expedition mode uh as we're thinking of LMS as a service",
    "start": "1369280",
    "end": "1376159"
  },
  {
    "text": "right we were just I just showed you the architecture um well where we could a user could hit you know an inference a",
    "start": "1376159",
    "end": "1383039"
  },
  {
    "text": "self-hosted model or something like on AWS well additionally uh we want to make",
    "start": "1383039",
    "end": "1388320"
  },
  {
    "text": "our platform as easy to use we want to make it optimize for performance as well and as a platform team we want to be",
    "start": "1388320",
    "end": "1395039"
  },
  {
    "text": "able to um give users different options so some workloads can be done in batch",
    "start": "1395039",
    "end": "1400559"
  },
  {
    "text": "uh they don't need real-time processing Um it's not critical that they have that Therefore having something like a batch",
    "start": "1400559",
    "end": "1406559"
  },
  {
    "text": "inference platform with these open source tools the same open source tools that we use to create the other platform",
    "start": "1406559",
    "end": "1412400"
  },
  {
    "text": "can greatly help with our uh resource utilization and cost And this is an example of an architecture of how you as",
    "start": "1412400",
    "end": "1419520"
  },
  {
    "text": "a platform team can provide that to your users So as already teasered um there's also",
    "start": "1419520",
    "end": "1427600"
  },
  {
    "text": "obviously we we need to have deeper insights about the um the models how they're doing um getting feedback from",
    "start": "1427600",
    "end": "1434720"
  },
  {
    "text": "the system maybe even see the traces because you sometimes see how the um",
    "start": "1434720",
    "end": "1440240"
  },
  {
    "text": "tools are communicating with each other especially if you chain a few of those models together you're very very",
    "start": "1440240",
    "end": "1445360"
  },
  {
    "text": "interested in what's going on and where I'm actually losing a lot of time maybe because I need to optimize the requests",
    "start": "1445360",
    "end": "1451440"
  },
  {
    "text": "maybe I need to choose a different model giving me a shorter answer whatsoever How do I get that Well on the one hand",
    "start": "1451440",
    "end": "1457440"
  },
  {
    "text": "side I can use open telemetry This gives me the the broad big picture but open telemetry doesn't not yet completely",
    "start": "1457440",
    "end": "1464400"
  },
  {
    "text": "understand like what's going on within large language model doesn't understand the the specifics the details of it And",
    "start": "1464400",
    "end": "1471200"
  },
  {
    "text": "there's a nice development around that called the open meta mitri Hope I spell it right Um that integrates with",
    "start": "1471200",
    "end": "1477840"
  },
  {
    "text": "different models but also with different platforms So we do not just talk about like using for example cloud.jbt JBT but",
    "start": "1477840",
    "end": "1484000"
  },
  {
    "text": "also utilizing something like a bedrock or pine cone or whatsoever But from",
    "start": "1484000",
    "end": "1489200"
  },
  {
    "text": "there it just grabs the information and put it over to the good old open telemetry and just forward it to our",
    "start": "1489200",
    "end": "1495679"
  },
  {
    "text": "favorite endpoint whatever it is might be a commercial solution might be open source solution might be something which",
    "start": "1495679",
    "end": "1500960"
  },
  {
    "text": "I have developed by myself and uh",
    "start": "1500960",
    "end": "1506760"
  },
  {
    "text": "there's another video perfect wow um so this is for example uh something with bedrock integration um in this case we",
    "start": "1506799",
    "end": "1515039"
  },
  {
    "text": "have uh JPT in the background and you can see different stuff like costs You see the amount of requests which we do",
    "start": "1515039",
    "end": "1521919"
  },
  {
    "text": "Um how long is the average duration but you can also see that we have a lot of",
    "start": "1521919",
    "end": "1527200"
  },
  {
    "text": "service quality and guardrails implemented and this is just on the basic information which we get back from",
    "start": "1527200",
    "end": "1534240"
  },
  {
    "text": "um the the observability integration and we put some guardrails around it Um and",
    "start": "1534240",
    "end": "1539760"
  },
  {
    "text": "last but not least you also can see how prompt caching can help us to optimize for example the performance and how much",
    "start": "1539760",
    "end": "1545200"
  },
  {
    "text": "time we can save And yeah I mean $9 saved or not that's not the big difference but it depend on how much we",
    "start": "1545200",
    "end": "1551679"
  },
  {
    "text": "have in the end um running here So this is a very simple demo The guardrails are",
    "start": "1551679",
    "end": "1558400"
  },
  {
    "text": "very easy It's just like if we get a a negative feedback we just say like okay maybe either it's toxic or it leaks",
    "start": "1558400",
    "end": "1565279"
  },
  {
    "text": "something Um but from the from the information which we receive already",
    "start": "1565279",
    "end": "1570480"
  },
  {
    "text": "from open telemetry we can use that to directly instrumentalize it and give a",
    "start": "1570480",
    "end": "1576080"
  },
  {
    "text": "back to the developers to the engineers who would like to maybe see that right especially if we are working with the",
    "start": "1576080",
    "end": "1582080"
  },
  {
    "text": "models and try to integrate and set up something that's um in the end um yeah",
    "start": "1582080",
    "end": "1588400"
  },
  {
    "text": "useful and should go into production so we have a good cycle um overall",
    "start": "1588400",
    "end": "1594760"
  },
  {
    "text": "Nice So this is uh an advanced view of running KSERve uh enterprise scale the",
    "start": "1595240",
    "end": "1601600"
  },
  {
    "text": "type of components that you may have and I'll try to go through this quickly but um one is model caching So a deepseek",
    "start": "1601600",
    "end": "1608559"
  },
  {
    "text": "model is around a terabyte and if we have an H100 node we have about 640 gigabytes of storage So um we're running",
    "start": "1608559",
    "end": "1616080"
  },
  {
    "text": "into situations where it might not fit on a single node Um so model sizes are",
    "start": "1616080",
    "end": "1621760"
  },
  {
    "text": "growing and that's for multiode inference Sorry So that's why we're also offering multi-node inference uh serving",
    "start": "1621760",
    "end": "1628159"
  },
  {
    "text": "for model caching Again models are also growing bigger So the download time takes a while as well",
    "start": "1628159",
    "end": "1634520"
  },
  {
    "text": "Um so model caching helps us to reduce that cost of downloading the model every",
    "start": "1634520",
    "end": "1640799"
  },
  {
    "text": "time a pod starts up Additionally oops Additionally we have prompt caching",
    "start": "1640799",
    "end": "1648799"
  },
  {
    "text": "So uh KV cache is something that comes out of the box with VLM and it helps us to optimize our prompt caching when um",
    "start": "1648799",
    "end": "1657120"
  },
  {
    "text": "on the the pot as well Okay And that helps us also to all these",
    "start": "1657120",
    "end": "1663679"
  },
  {
    "text": "caching especially model caching helps us to autoscale quicker",
    "start": "1663679",
    "end": "1668960"
  },
  {
    "text": "also Okay So model caching This visualization is an example of the time",
    "start": "1672520",
    "end": "1678799"
  },
  {
    "text": "that uh caching a model will save So let's just say for example you have uh an init container that comes up You want",
    "start": "1678799",
    "end": "1685360"
  },
  {
    "text": "to autoscale You need to uh you need to serve more requests You're getting more requests and you want to scale your",
    "start": "1685360",
    "end": "1691440"
  },
  {
    "text": "service uh and your pod comes up and it needs to download the model That takes",
    "start": "1691440",
    "end": "1696480"
  },
  {
    "text": "12 minutes Um the rest of the time let's see that takes like 3 minutes But if you",
    "start": "1696480",
    "end": "1702159"
  },
  {
    "text": "have a local uh a local cache of your model saved what's nice is that we can",
    "start": "1702159",
    "end": "1707600"
  },
  {
    "text": "just get rid of this bottleneck that takes so long and we can now autoscale a lot quicker and serve our requests a lot",
    "start": "1707600",
    "end": "1715559"
  },
  {
    "text": "better Also uh there's a KV cache growth challenge So KV cache expands",
    "start": "1715559",
    "end": "1722799"
  },
  {
    "text": "exponentially with sequence link as we can see So another solution that we are",
    "start": "1722799",
    "end": "1727840"
  },
  {
    "text": "implementing is the LM cache So it's also an open- source system designed to manage your KV caches efficiently It",
    "start": "1727840",
    "end": "1734640"
  },
  {
    "text": "makes KV caching more scalable Um it does this by caching common input prefixes to reduce your redundant",
    "start": "1734640",
    "end": "1741520"
  },
  {
    "text": "computation speed up repeated queries So it also makes access and storage of caches faster especially for long",
    "start": "1741520",
    "end": "1748399"
  },
  {
    "text": "documents and multi-turning conversations It's nice because it can share and store across multiple VLM",
    "start": "1748399",
    "end": "1754320"
  },
  {
    "text": "instances and route queries to the instances that already hold the relevant context for your KV cache So this has",
    "start": "1754320",
    "end": "1760480"
  },
  {
    "text": "been shown to be able to reduce the time to first token by around 3 to 10% and",
    "start": "1760480",
    "end": "1767279"
  },
  {
    "text": "save on the GPU cycle Um one more uh advanced feature",
    "start": "1767279",
    "end": "1772559"
  },
  {
    "text": "that we are looking into is the genai uh is to helping with disagregated",
    "start": "1772559",
    "end": "1778240"
  },
  {
    "text": "serving So jai inference latency isn't always predictable because we have these two steps uh the prefill and decoding",
    "start": "1778240",
    "end": "1785120"
  },
  {
    "text": "one's computebound and one's memory bound So what we can do to optimize this is to se separate the two um and execute",
    "start": "1785120",
    "end": "1792480"
  },
  {
    "text": "them independently and have two different sets of GPUs that can handle them therefore increasing our throughput",
    "start": "1792480",
    "end": "1799520"
  },
  {
    "text": "and optimizing our hardware performance So we want to input we want to optimize both performance and latency here",
    "start": "1799520",
    "end": "1808480"
  },
  {
    "text": "So with that we are at the end of our journey Uh like a good hike sometimes it",
    "start": "1808480",
    "end": "1813760"
  },
  {
    "text": "gets a little bit windy stormy and a little bit uh challenging So maybe it's not really the end also for for this one",
    "start": "1813760",
    "end": "1820880"
  },
  {
    "text": "What we wanted to show is like in the end like on the way you will find all the time something that maybe can",
    "start": "1820880",
    "end": "1827120"
  },
  {
    "text": "improved that we can for go up for the next challenge um that maybe needs to be",
    "start": "1827120",
    "end": "1832320"
  },
  {
    "text": "replaced Right Sometimes replacements are not worse They are part of improving things if you keep with the things which",
    "start": "1832320",
    "end": "1838399"
  },
  {
    "text": "you have implemented in the past Well it's not always something that sustains",
    "start": "1838399",
    "end": "1843720"
  },
  {
    "text": "forever And that's what we want to demonstrate with having platform engineering as an enablement platform",
    "start": "1843720",
    "end": "1851039"
  },
  {
    "text": "also to bring up more complex topics around genai um easily implementing um",
    "start": "1851039",
    "end": "1856960"
  },
  {
    "text": "an AI gateway and so on Yeah totally agree Thank you",
    "start": "1856960",
    "end": "1862399"
  },
  {
    "text": "Thank you very much and enjoy the rest of your cube",
    "start": "1862399",
    "end": "1867639"
  }
]