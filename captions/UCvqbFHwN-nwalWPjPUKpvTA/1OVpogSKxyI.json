[
  {
    "start": "0",
    "end": "55000"
  },
  {
    "text": "hello everyone",
    "start": "2240",
    "end": "3679"
  },
  {
    "text": "welcome to better scalability and more",
    "start": "3679",
    "end": "6160"
  },
  {
    "text": "isolation the cortex",
    "start": "6160",
    "end": "7919"
  },
  {
    "text": "shuffle sharding story hope everyone's",
    "start": "7919",
    "end": "9840"
  },
  {
    "text": "having a good kubecon so far",
    "start": "9840",
    "end": "12719"
  },
  {
    "text": "so my name's tom i am the vp product at",
    "start": "12719",
    "end": "15920"
  },
  {
    "text": "grafana labs um",
    "start": "15920",
    "end": "18400"
  },
  {
    "text": "in my what little spare time i have i",
    "start": "18400",
    "end": "20400"
  },
  {
    "text": "guess i'm one of the prometheus",
    "start": "20400",
    "end": "22240"
  },
  {
    "text": "team um my main contribution being the",
    "start": "22240",
    "end": "24880"
  },
  {
    "text": "the remote write code there",
    "start": "24880",
    "end": "26720"
  },
  {
    "text": "uh i also started the cortex project",
    "start": "26720",
    "end": "29439"
  },
  {
    "text": "which is the",
    "start": "29439",
    "end": "30000"
  },
  {
    "text": "horizontally scalable version of",
    "start": "30000",
    "end": "32160"
  },
  {
    "text": "prometheus that's part of the cncf",
    "start": "32160",
    "end": "34079"
  },
  {
    "text": "sandbox",
    "start": "34079",
    "end": "35520"
  },
  {
    "text": "more recently i started loki which is a",
    "start": "35520",
    "end": "37760"
  },
  {
    "text": "grafana labs",
    "start": "37760",
    "end": "38719"
  },
  {
    "text": "log aggregation system in",
    "start": "38719",
    "end": "42079"
  },
  {
    "text": "what you know in when i'm not looking",
    "start": "42079",
    "end": "44079"
  },
  {
    "text": "after these projects",
    "start": "44079",
    "end": "45680"
  },
  {
    "text": "i have a bunch of 3d printers sitting on",
    "start": "45680",
    "end": "47440"
  },
  {
    "text": "my desk and i used to make beer as well",
    "start": "47440",
    "end": "49840"
  },
  {
    "text": "but i haven't actually had a chance to",
    "start": "49840",
    "end": "52000"
  },
  {
    "text": "to brew for a while",
    "start": "52000",
    "end": "55039"
  },
  {
    "start": "55000",
    "end": "87000"
  },
  {
    "text": "so today we're going to talk about",
    "start": "55120",
    "end": "56879"
  },
  {
    "text": "shuffle sharding",
    "start": "56879",
    "end": "58399"
  },
  {
    "text": "and how it allows us to build a more",
    "start": "58399",
    "end": "60239"
  },
  {
    "text": "scalable",
    "start": "60239",
    "end": "61440"
  },
  {
    "text": "version of cortex with with better",
    "start": "61440",
    "end": "63280"
  },
  {
    "text": "isolation but before we do that",
    "start": "63280",
    "end": "65439"
  },
  {
    "text": "i'd like to spend some time just",
    "start": "65439",
    "end": "67439"
  },
  {
    "text": "introducing you to cortex what it does",
    "start": "67439",
    "end": "69119"
  },
  {
    "text": "what it is",
    "start": "69119",
    "end": "69760"
  },
  {
    "text": "why it's important before we go on and",
    "start": "69760",
    "end": "72400"
  },
  {
    "text": "talk about",
    "start": "72400",
    "end": "73360"
  },
  {
    "text": "you know how we solved this problem",
    "start": "73360",
    "end": "75200"
  },
  {
    "text": "before we added shuffle sharding",
    "start": "75200",
    "end": "76880"
  },
  {
    "text": "we'll then talk about shuffle sharding",
    "start": "76880",
    "end": "78159"
  },
  {
    "text": "what it does and and finally how good it",
    "start": "78159",
    "end": "80400"
  },
  {
    "text": "is",
    "start": "80400",
    "end": "81520"
  },
  {
    "text": "you know if it really delivers on um on",
    "start": "81520",
    "end": "84000"
  },
  {
    "text": "what we said it would",
    "start": "84000",
    "end": "85520"
  },
  {
    "text": "so without further ado",
    "start": "85520",
    "end": "89118"
  },
  {
    "start": "87000",
    "end": "363000"
  },
  {
    "text": "cortex horizontally scalable prometheus",
    "start": "89840",
    "end": "93360"
  },
  {
    "text": "so prometheus is an awesome monitoring",
    "start": "93360",
    "end": "96079"
  },
  {
    "text": "system it's incredibly easy to use",
    "start": "96079",
    "end": "98320"
  },
  {
    "text": "and we see a lot of people get started",
    "start": "98320",
    "end": "100799"
  },
  {
    "text": "with prometheus",
    "start": "100799",
    "end": "101759"
  },
  {
    "text": "very very easily you know you you deploy",
    "start": "101759",
    "end": "104560"
  },
  {
    "text": "it alongside your applications you maybe",
    "start": "104560",
    "end": "106799"
  },
  {
    "text": "instrument your applications or",
    "start": "106799",
    "end": "108320"
  },
  {
    "text": "or add a few exporters to to adapt them",
    "start": "108320",
    "end": "110560"
  },
  {
    "text": "to prometheus",
    "start": "110560",
    "end": "112079"
  },
  {
    "text": "and very quickly you know you attach",
    "start": "112079",
    "end": "113920"
  },
  {
    "text": "your grafana very quickly you can build",
    "start": "113920",
    "end": "115520"
  },
  {
    "text": "some really awesome",
    "start": "115520",
    "end": "116719"
  },
  {
    "text": "dashboards right and you can really get",
    "start": "116719",
    "end": "118479"
  },
  {
    "text": "some great insight into",
    "start": "118479",
    "end": "120159"
  },
  {
    "text": "your application's behavior and and",
    "start": "120159",
    "end": "122399"
  },
  {
    "text": "start debugging it and start responding",
    "start": "122399",
    "end": "124079"
  },
  {
    "text": "to any problems it might has",
    "start": "124079",
    "end": "125439"
  },
  {
    "text": "you know it is a really powerful and",
    "start": "125439",
    "end": "127360"
  },
  {
    "text": "flexible system",
    "start": "127360",
    "end": "129360"
  },
  {
    "text": "the challenge we see uh in prometheus is",
    "start": "129360",
    "end": "132160"
  },
  {
    "text": "really when you start to",
    "start": "132160",
    "end": "134160"
  },
  {
    "text": "grow beyond the confines of a single",
    "start": "134160",
    "end": "136560"
  },
  {
    "text": "location beyond the confines of a single",
    "start": "136560",
    "end": "138239"
  },
  {
    "text": "data center",
    "start": "138239",
    "end": "139280"
  },
  {
    "text": "a single region you know maybe you've",
    "start": "139280",
    "end": "141360"
  },
  {
    "text": "got your application deployed",
    "start": "141360",
    "end": "143440"
  },
  {
    "text": "in three different locations you know",
    "start": "143440",
    "end": "144800"
  },
  {
    "text": "grafana labs we run 15 plus",
    "start": "144800",
    "end": "147280"
  },
  {
    "text": "kubernetes clusters in",
    "start": "147280",
    "end": "150800"
  },
  {
    "text": "in the first instance what we see is is",
    "start": "150800",
    "end": "153200"
  },
  {
    "text": "users add data sources to grafana for",
    "start": "153200",
    "end": "155760"
  },
  {
    "text": "each",
    "start": "155760",
    "end": "156000"
  },
  {
    "text": "one of these instances um you know this",
    "start": "156000",
    "end": "159200"
  },
  {
    "text": "allows you to build dashboards with with",
    "start": "159200",
    "end": "161519"
  },
  {
    "text": "little dropdowns that you can select",
    "start": "161519",
    "end": "163280"
  },
  {
    "text": "what region you're interested in",
    "start": "163280",
    "end": "166000"
  },
  {
    "text": "i guess the reason prometheus has to be",
    "start": "166000",
    "end": "167760"
  },
  {
    "text": "deployed like this is because prometheus",
    "start": "167760",
    "end": "169200"
  },
  {
    "text": "has to be next to your application right",
    "start": "169200",
    "end": "171280"
  },
  {
    "text": "it wants to talk to the local cluster to",
    "start": "171280",
    "end": "173200"
  },
  {
    "text": "do service discovery",
    "start": "173200",
    "end": "174720"
  },
  {
    "text": "and it wants to connect directly to the",
    "start": "174720",
    "end": "176400"
  },
  {
    "text": "application to collect metrics from it",
    "start": "176400",
    "end": "178959"
  },
  {
    "text": "um in the prometheus world there is a",
    "start": "178959",
    "end": "181760"
  },
  {
    "text": "solution to",
    "start": "181760",
    "end": "182800"
  },
  {
    "text": "to kind of bringing this all together",
    "start": "182800",
    "end": "184560"
  },
  {
    "text": "into a central global view",
    "start": "184560",
    "end": "187440"
  },
  {
    "text": "and i guess you know to be clear the",
    "start": "187440",
    "end": "188959"
  },
  {
    "text": "problem is",
    "start": "188959",
    "end": "190480"
  },
  {
    "text": "whilst it's fine you know this approach",
    "start": "190480",
    "end": "192319"
  },
  {
    "text": "is fine for getting information about an",
    "start": "192319",
    "end": "193920"
  },
  {
    "text": "individual cluster",
    "start": "193920",
    "end": "194959"
  },
  {
    "text": "there's no way in this approach of",
    "start": "194959",
    "end": "196480"
  },
  {
    "text": "really getting a kind of",
    "start": "196480",
    "end": "198159"
  },
  {
    "text": "global latency number or or finding out",
    "start": "198159",
    "end": "201040"
  },
  {
    "text": "what the global error rate is you know",
    "start": "201040",
    "end": "202959"
  },
  {
    "text": "they just can't do it because each",
    "start": "202959",
    "end": "204319"
  },
  {
    "text": "cluster is being monitored by an",
    "start": "204319",
    "end": "206400"
  },
  {
    "text": "independent prometheus",
    "start": "206400",
    "end": "208879"
  },
  {
    "text": "so in prometheus world we recommend",
    "start": "208879",
    "end": "210720"
  },
  {
    "text": "people deploy a global federation server",
    "start": "210720",
    "end": "213680"
  },
  {
    "text": "and this federation server can scrape",
    "start": "213680",
    "end": "215840"
  },
  {
    "text": "the federation endpoint",
    "start": "215840",
    "end": "217680"
  },
  {
    "text": "on each of your prometheuses and bring",
    "start": "217680",
    "end": "219840"
  },
  {
    "text": "that data into a single place",
    "start": "219840",
    "end": "221840"
  },
  {
    "text": "where you can run these these central",
    "start": "221840",
    "end": "224080"
  },
  {
    "text": "queries where you can",
    "start": "224080",
    "end": "225599"
  },
  {
    "text": "ask things like you know what's my",
    "start": "225599",
    "end": "227519"
  },
  {
    "text": "global latency",
    "start": "227519",
    "end": "228799"
  },
  {
    "text": "what's my slowest region and so on",
    "start": "228799",
    "end": "232480"
  },
  {
    "text": "you know this this isn't this isn't that",
    "start": "232480",
    "end": "234159"
  },
  {
    "text": "tricky to set up um it generally works",
    "start": "234159",
    "end": "236239"
  },
  {
    "text": "reasonably well you've got to you know",
    "start": "236239",
    "end": "237360"
  },
  {
    "text": "you got to get the authentication and",
    "start": "237360",
    "end": "238840"
  },
  {
    "text": "firewalls",
    "start": "238840",
    "end": "240080"
  },
  {
    "text": "all kind of all kind of working",
    "start": "240080",
    "end": "241519"
  },
  {
    "text": "correctly and and got to secure the",
    "start": "241519",
    "end": "243360"
  },
  {
    "text": "network and so on but in general this is",
    "start": "243360",
    "end": "245040"
  },
  {
    "text": "this is feasible this is possible",
    "start": "245040",
    "end": "247439"
  },
  {
    "text": "where this starts to break down is when",
    "start": "247439",
    "end": "249680"
  },
  {
    "text": "people",
    "start": "249680",
    "end": "250480"
  },
  {
    "text": "you know scale very large and start",
    "start": "250480",
    "end": "252480"
  },
  {
    "text": "storing all the raw data in a single",
    "start": "252480",
    "end": "254159"
  },
  {
    "text": "prometheus server",
    "start": "254159",
    "end": "255840"
  },
  {
    "text": "it's very easy to overwhelm that central",
    "start": "255840",
    "end": "258079"
  },
  {
    "text": "global federation server",
    "start": "258079",
    "end": "260160"
  },
  {
    "text": "so we recommend as best practices that",
    "start": "260160",
    "end": "263199"
  },
  {
    "text": "people only federate",
    "start": "263199",
    "end": "264639"
  },
  {
    "text": "pre-aggregated data you know and",
    "start": "264639",
    "end": "266880"
  },
  {
    "text": "commonly this might mean recording rules",
    "start": "266880",
    "end": "268880"
  },
  {
    "text": "that have",
    "start": "268880",
    "end": "269520"
  },
  {
    "text": "erased away cane let's say the instance",
    "start": "269520",
    "end": "271759"
  },
  {
    "text": "label",
    "start": "271759",
    "end": "273040"
  },
  {
    "text": "um so these these these are useful for",
    "start": "273040",
    "end": "275280"
  },
  {
    "text": "building those dashboards",
    "start": "275280",
    "end": "276720"
  },
  {
    "text": "but it really prevents you doing kind of",
    "start": "276720",
    "end": "278880"
  },
  {
    "text": "drill down and add hot queries against",
    "start": "278880",
    "end": "281040"
  },
  {
    "text": "this global federation",
    "start": "281040",
    "end": "282080"
  },
  {
    "text": "server you know if this federation",
    "start": "282080",
    "end": "284080"
  },
  {
    "text": "server points to a problem in a region",
    "start": "284080",
    "end": "286080"
  },
  {
    "text": "it won't be able to point to a problem",
    "start": "286080",
    "end": "287360"
  },
  {
    "text": "with a particular instance of a service",
    "start": "287360",
    "end": "289040"
  },
  {
    "text": "because you've erased that label away",
    "start": "289040",
    "end": "292000"
  },
  {
    "text": "um so we were looking you know five",
    "start": "292000",
    "end": "295520"
  },
  {
    "text": "years ago now we were looking for",
    "start": "295520",
    "end": "297199"
  },
  {
    "text": "a different way of doing this maybe a",
    "start": "297199",
    "end": "298960"
  },
  {
    "text": "better way of doing this",
    "start": "298960",
    "end": "300479"
  },
  {
    "text": "and this is where we built cortex so",
    "start": "300479",
    "end": "302800"
  },
  {
    "text": "cortex replaces the need for that global",
    "start": "302800",
    "end": "305039"
  },
  {
    "text": "federation server",
    "start": "305039",
    "end": "306720"
  },
  {
    "text": "and you can push you can have all the",
    "start": "306720",
    "end": "308160"
  },
  {
    "text": "edge locations push",
    "start": "308160",
    "end": "309840"
  },
  {
    "text": "all their raw samples directly to that",
    "start": "309840",
    "end": "311759"
  },
  {
    "text": "cortex cluster",
    "start": "311759",
    "end": "313440"
  },
  {
    "text": "and this is this is good for two reasons",
    "start": "313440",
    "end": "315039"
  },
  {
    "text": "right one um",
    "start": "315039",
    "end": "316479"
  },
  {
    "text": "you know this is a push not a pull now",
    "start": "316479",
    "end": "318080"
  },
  {
    "text": "so so in some ways this is uh kind of",
    "start": "318080",
    "end": "320880"
  },
  {
    "text": "more sympathetic towards how",
    "start": "320880",
    "end": "322800"
  },
  {
    "text": "a lot of organizations have their",
    "start": "322800",
    "end": "324160"
  },
  {
    "text": "networks organized but also",
    "start": "324160",
    "end": "326240"
  },
  {
    "text": "the cortex cluster is is scalable and so",
    "start": "326240",
    "end": "329120"
  },
  {
    "text": "it can as you add more clusters",
    "start": "329120",
    "end": "331280"
  },
  {
    "text": "as you add more metrics in individual",
    "start": "331280",
    "end": "332880"
  },
  {
    "text": "locations",
    "start": "332880",
    "end": "334320"
  },
  {
    "text": "you can scale up that cortex cluster to",
    "start": "334320",
    "end": "336320"
  },
  {
    "text": "take all the load all the raw data",
    "start": "336320",
    "end": "338320"
  },
  {
    "text": "and this means it makes it really easy",
    "start": "338320",
    "end": "339759"
  },
  {
    "text": "to do these kind of ad hoc queries",
    "start": "339759",
    "end": "342080"
  },
  {
    "text": "it's got you've got all the data that",
    "start": "342080",
    "end": "343199"
  },
  {
    "text": "you can drill down just within the",
    "start": "343199",
    "end": "344400"
  },
  {
    "text": "central cortex cluster",
    "start": "344400",
    "end": "346080"
  },
  {
    "text": "and because you've centralized all of",
    "start": "346080",
    "end": "347600"
  },
  {
    "text": "this there's also like one natural place",
    "start": "347600",
    "end": "350080"
  },
  {
    "text": "to add things like long-term storage to",
    "start": "350080",
    "end": "352720"
  },
  {
    "text": "invest in query performance and and and",
    "start": "352720",
    "end": "355680"
  },
  {
    "text": "really make sure your users know there's",
    "start": "355680",
    "end": "357120"
  },
  {
    "text": "one place to go to get all their",
    "start": "357120",
    "end": "358639"
  },
  {
    "text": "all their answers",
    "start": "358639",
    "end": "361360"
  },
  {
    "start": "363000",
    "end": "452000"
  },
  {
    "text": "so that's cortex in a nutshell really",
    "start": "363199",
    "end": "365600"
  },
  {
    "text": "it's a",
    "start": "365600",
    "end": "366560"
  },
  {
    "text": "time series database um you know it uses",
    "start": "366560",
    "end": "369840"
  },
  {
    "text": "the same storage engine",
    "start": "369840",
    "end": "371120"
  },
  {
    "text": "and query engine as prometheus and what",
    "start": "371120",
    "end": "373600"
  },
  {
    "text": "we've done in cortex is really",
    "start": "373600",
    "end": "375360"
  },
  {
    "text": "add the the distributed systems glue to",
    "start": "375360",
    "end": "378400"
  },
  {
    "text": "turn those from a kind of single node",
    "start": "378400",
    "end": "380400"
  },
  {
    "text": "solution into something that works uh",
    "start": "380400",
    "end": "382479"
  },
  {
    "text": "in a horizontally scalable kind of",
    "start": "382479",
    "end": "383919"
  },
  {
    "text": "clustered fashion",
    "start": "383919",
    "end": "385919"
  },
  {
    "text": "so cortex is horizontally scalable it's",
    "start": "385919",
    "end": "388319"
  },
  {
    "text": "highly available we replicate data in",
    "start": "388319",
    "end": "390319"
  },
  {
    "text": "cortex between nodes",
    "start": "390319",
    "end": "392000"
  },
  {
    "text": "this means when when a node fails you're",
    "start": "392000",
    "end": "394319"
  },
  {
    "text": "not going to see gaps in your in your",
    "start": "394319",
    "end": "395759"
  },
  {
    "text": "graphs",
    "start": "395759",
    "end": "396720"
  },
  {
    "text": "um and we add a more durable long-term",
    "start": "396720",
    "end": "400160"
  },
  {
    "text": "storage so",
    "start": "400160",
    "end": "400880"
  },
  {
    "text": "in cortex you can store data in an",
    "start": "400880",
    "end": "402960"
  },
  {
    "text": "object store",
    "start": "402960",
    "end": "404080"
  },
  {
    "text": "um and effectively store data for as",
    "start": "404080",
    "end": "406319"
  },
  {
    "text": "long as you like",
    "start": "406319",
    "end": "408080"
  },
  {
    "text": "and finally one of the things i think",
    "start": "408080",
    "end": "409360"
  },
  {
    "text": "that makes cortex quite different to a",
    "start": "409360",
    "end": "411759"
  },
  {
    "text": "lot of systems is",
    "start": "411759",
    "end": "412880"
  },
  {
    "text": "it natively from day zero kind of built",
    "start": "412880",
    "end": "415599"
  },
  {
    "text": "to be multi-tenant",
    "start": "415599",
    "end": "416720"
  },
  {
    "text": "to support different isolated tenants",
    "start": "416720",
    "end": "420080"
  },
  {
    "text": "um on the same cluster this means if",
    "start": "420080",
    "end": "422880"
  },
  {
    "text": "you're",
    "start": "422880",
    "end": "423520"
  },
  {
    "text": "an internal kind of observability team",
    "start": "423520",
    "end": "426479"
  },
  {
    "text": "providing a service to the rest of your",
    "start": "426479",
    "end": "428080"
  },
  {
    "text": "organization",
    "start": "428080",
    "end": "429199"
  },
  {
    "text": "uh cortex is really easy to kind of",
    "start": "429199",
    "end": "431280"
  },
  {
    "text": "deploy and add",
    "start": "431280",
    "end": "432800"
  },
  {
    "text": "lots and lots of different isolated",
    "start": "432800",
    "end": "434479"
  },
  {
    "text": "teams within your organization",
    "start": "434479",
    "end": "435919"
  },
  {
    "text": "too without having to spin up a separate",
    "start": "435919",
    "end": "438080"
  },
  {
    "text": "cluster per team",
    "start": "438080",
    "end": "441120"
  },
  {
    "text": "um we've you know we we joined the cncf",
    "start": "441199",
    "end": "445120"
  },
  {
    "text": "a few years ago we're part of the",
    "start": "445120",
    "end": "446960"
  },
  {
    "text": "incubating phase now",
    "start": "446960",
    "end": "449039"
  },
  {
    "text": "and it's apache license it's available",
    "start": "449039",
    "end": "450639"
  },
  {
    "text": "on github a bit of a bit of a timeline",
    "start": "450639",
    "end": "453199"
  },
  {
    "start": "452000",
    "end": "582000"
  },
  {
    "text": "you know as i've mentioned we kind of",
    "start": "453199",
    "end": "454639"
  },
  {
    "text": "started the project uh julius and i",
    "start": "454639",
    "end": "456160"
  },
  {
    "text": "started the project",
    "start": "456160",
    "end": "457520"
  },
  {
    "text": "almost five years ago now um we",
    "start": "457520",
    "end": "459680"
  },
  {
    "text": "originally stored all the data in",
    "start": "459680",
    "end": "461440"
  },
  {
    "text": "dynamodb",
    "start": "461440",
    "end": "462240"
  },
  {
    "text": "in amazon dynamodb um and then over the",
    "start": "462240",
    "end": "465039"
  },
  {
    "text": "next year or two i added support for",
    "start": "465039",
    "end": "466720"
  },
  {
    "text": "bigtable and for cassandra",
    "start": "466720",
    "end": "469039"
  },
  {
    "text": "um one thing i'm particularly proud of",
    "start": "469039",
    "end": "471120"
  },
  {
    "text": "with cortex is",
    "start": "471120",
    "end": "472479"
  },
  {
    "text": "uh in the early days i think we kind of",
    "start": "472479",
    "end": "474160"
  },
  {
    "text": "got the the right path",
    "start": "474160",
    "end": "475520"
  },
  {
    "text": "right you know we um you know it's very",
    "start": "475520",
    "end": "478319"
  },
  {
    "text": "scalable and very performed from the",
    "start": "478319",
    "end": "479919"
  },
  {
    "text": "get-go and it didn't take us a long time",
    "start": "479919",
    "end": "481360"
  },
  {
    "text": "to kind of you know make that",
    "start": "481360",
    "end": "482879"
  },
  {
    "text": "kind of effectively done um and so early",
    "start": "482879",
    "end": "485919"
  },
  {
    "text": "on in the life of the project we started",
    "start": "485919",
    "end": "487840"
  },
  {
    "text": "focusing on query performance on",
    "start": "487840",
    "end": "489759"
  },
  {
    "text": "on accelerating and distributing and",
    "start": "489759",
    "end": "491759"
  },
  {
    "text": "parallelizing",
    "start": "491759",
    "end": "493120"
  },
  {
    "text": "massive queries against prometheus data",
    "start": "493120",
    "end": "495599"
  },
  {
    "text": "and i feel like we made uh made some",
    "start": "495599",
    "end": "496960"
  },
  {
    "text": "really good strides there with with",
    "start": "496960",
    "end": "498400"
  },
  {
    "text": "query caching",
    "start": "498400",
    "end": "499440"
  },
  {
    "text": "with parallelization and sharding and",
    "start": "499440",
    "end": "501919"
  },
  {
    "text": "i'm very proud of what we achieved",
    "start": "501919",
    "end": "503840"
  },
  {
    "text": "we joined the the cnc cncf sandbox uh",
    "start": "503840",
    "end": "507039"
  },
  {
    "text": "just",
    "start": "507039",
    "end": "507360"
  },
  {
    "text": "about two and a half years ago now and",
    "start": "507360",
    "end": "509199"
  },
  {
    "text": "then really was the focus",
    "start": "509199",
    "end": "510800"
  },
  {
    "text": "on uh ease of use and and on the",
    "start": "510800",
    "end": "512880"
  },
  {
    "text": "community we launched a website",
    "start": "512880",
    "end": "514719"
  },
  {
    "text": "we uh did a 1-0 release we wrote a load",
    "start": "514719",
    "end": "517279"
  },
  {
    "text": "of docs",
    "start": "517279",
    "end": "518240"
  },
  {
    "text": "generally we really kind of put a lot of",
    "start": "518240",
    "end": "519680"
  },
  {
    "text": "effort into making cortex easier to use",
    "start": "519680",
    "end": "522880"
  },
  {
    "text": "and now now we're up to date now more",
    "start": "522880",
    "end": "524959"
  },
  {
    "text": "recently in the past year or so",
    "start": "524959",
    "end": "526800"
  },
  {
    "text": "we've been focused on new and exciting",
    "start": "526800",
    "end": "529040"
  },
  {
    "text": "features in cortex so",
    "start": "529040",
    "end": "530880"
  },
  {
    "text": "we added a system called block storage",
    "start": "530880",
    "end": "532640"
  },
  {
    "text": "this is uh basically the same thing",
    "start": "532640",
    "end": "534160"
  },
  {
    "text": "thanos does",
    "start": "534160",
    "end": "536000"
  },
  {
    "text": "where we we've reduced the only",
    "start": "536000",
    "end": "537760"
  },
  {
    "text": "dependency that cortex has now is on an",
    "start": "537760",
    "end": "540080"
  },
  {
    "text": "object store",
    "start": "540080",
    "end": "541600"
  },
  {
    "text": "making it a lot easier to deploy and",
    "start": "541600",
    "end": "543360"
  },
  {
    "text": "manage",
    "start": "543360",
    "end": "545440"
  },
  {
    "text": "also block storage is fantastically",
    "start": "545440",
    "end": "547600"
  },
  {
    "text": "cheaper to operate than",
    "start": "547600",
    "end": "549120"
  },
  {
    "text": "the previous kind of dynamodb chunk",
    "start": "549120",
    "end": "550959"
  },
  {
    "text": "storage",
    "start": "550959",
    "end": "552320"
  },
  {
    "text": "um we added shuffle sharding towards the",
    "start": "552320",
    "end": "554240"
  },
  {
    "text": "end of last year and this is what i'm",
    "start": "554240",
    "end": "555839"
  },
  {
    "text": "going to talk about for the rest of the",
    "start": "555839",
    "end": "556880"
  },
  {
    "text": "talk",
    "start": "556880",
    "end": "557279"
  },
  {
    "text": "so i won't go into any more detail now",
    "start": "557279",
    "end": "559440"
  },
  {
    "text": "and then more recently we've added",
    "start": "559440",
    "end": "560720"
  },
  {
    "text": "things like query federation",
    "start": "560720",
    "end": "562160"
  },
  {
    "text": "relaxing some of those multi-tenancy",
    "start": "562160",
    "end": "564800"
  },
  {
    "text": "isolation features so you can",
    "start": "564800",
    "end": "566720"
  },
  {
    "text": "query data in multiple different tenants",
    "start": "566720",
    "end": "569120"
  },
  {
    "text": "and per tenant retention",
    "start": "569120",
    "end": "570959"
  },
  {
    "text": "so different tenants can have different",
    "start": "570959",
    "end": "572720"
  },
  {
    "text": "amounts of data stored for different",
    "start": "572720",
    "end": "574080"
  },
  {
    "text": "lengths of time so yeah really exciting",
    "start": "574080",
    "end": "577120"
  },
  {
    "text": "uh really exciting progress on cortex um",
    "start": "577120",
    "end": "579120"
  },
  {
    "text": "but today",
    "start": "579120",
    "end": "579920"
  },
  {
    "text": "we're going to talk about shuffle",
    "start": "579920",
    "end": "581040"
  },
  {
    "text": "sharding but",
    "start": "581040",
    "end": "582959"
  },
  {
    "start": "582000",
    "end": "807000"
  },
  {
    "text": "to tease you a little bit more first i",
    "start": "582959",
    "end": "584800"
  },
  {
    "text": "really have to describe",
    "start": "584800",
    "end": "586240"
  },
  {
    "text": "um how how we you know how things work",
    "start": "586240",
    "end": "588720"
  },
  {
    "text": "before shuffle sharding",
    "start": "588720",
    "end": "590560"
  },
  {
    "text": "so in a cortex system you know one of",
    "start": "590560",
    "end": "592720"
  },
  {
    "text": "the main goals of cortex is to be this",
    "start": "592720",
    "end": "594320"
  },
  {
    "text": "horizontally scalable",
    "start": "594320",
    "end": "595920"
  },
  {
    "text": "what this means is we need to be able to",
    "start": "595920",
    "end": "597839"
  },
  {
    "text": "take in data and and shard it and spread",
    "start": "597839",
    "end": "600240"
  },
  {
    "text": "it",
    "start": "600240",
    "end": "601120"
  },
  {
    "text": "amongst the nodes in a cluster so we do",
    "start": "601120",
    "end": "604560"
  },
  {
    "text": "this",
    "start": "604560",
    "end": "605519"
  },
  {
    "text": "by hashing the labels within uh within",
    "start": "605519",
    "end": "608720"
  },
  {
    "text": "the samples that get written and this is",
    "start": "608720",
    "end": "611440"
  },
  {
    "text": "really how we make cortex scalable",
    "start": "611440",
    "end": "613440"
  },
  {
    "text": "right how we make a cluster in aggregate",
    "start": "613440",
    "end": "616800"
  },
  {
    "text": "able to cope with more writes and more",
    "start": "616800",
    "end": "618959"
  },
  {
    "text": "reads than any single",
    "start": "618959",
    "end": "620320"
  },
  {
    "text": "node in that cluster can this is all",
    "start": "620320",
    "end": "623120"
  },
  {
    "text": "automatic the user doesn't really have",
    "start": "623120",
    "end": "624320"
  },
  {
    "text": "to configure anything",
    "start": "624320",
    "end": "625600"
  },
  {
    "text": "you know and as you add new nodes we can",
    "start": "625600",
    "end": "627600"
  },
  {
    "text": "scale up and scale down as you remove",
    "start": "627600",
    "end": "629519"
  },
  {
    "text": "nodes",
    "start": "629519",
    "end": "630000"
  },
  {
    "text": "it's really quite cool",
    "start": "630000",
    "end": "633200"
  },
  {
    "text": "the challenge with this is um a a single",
    "start": "633200",
    "end": "635760"
  },
  {
    "text": "node outage can potentially impact",
    "start": "635760",
    "end": "637760"
  },
  {
    "text": "all of the all of the tenants on the",
    "start": "637760",
    "end": "639920"
  },
  {
    "text": "cluster you know the tenants of the cat",
    "start": "639920",
    "end": "641360"
  },
  {
    "text": "and the dolphin and the fox",
    "start": "641360",
    "end": "643920"
  },
  {
    "text": "so to prevent this kind of single node",
    "start": "643920",
    "end": "645680"
  },
  {
    "text": "outage and you know it's worth noting as",
    "start": "645680",
    "end": "647440"
  },
  {
    "text": "you add more nodes the chance of any one",
    "start": "647440",
    "end": "649120"
  },
  {
    "text": "of them failing just randomly",
    "start": "649120",
    "end": "650959"
  },
  {
    "text": "is is higher right to avoid this uh",
    "start": "650959",
    "end": "656480"
  },
  {
    "text": "outage from a node failure we replicate",
    "start": "656480",
    "end": "658240"
  },
  {
    "text": "the data between nodes",
    "start": "658240",
    "end": "660000"
  },
  {
    "text": "so we use a replication factor of three",
    "start": "660000",
    "end": "662079"
  },
  {
    "text": "and quarant reads and writes what this",
    "start": "662079",
    "end": "664000"
  },
  {
    "text": "means is",
    "start": "664000",
    "end": "665200"
  },
  {
    "text": "when you write data we write to three",
    "start": "665200",
    "end": "667279"
  },
  {
    "text": "nodes but we only wait for a positive",
    "start": "667279",
    "end": "669040"
  },
  {
    "text": "response from two of them from a quorum",
    "start": "669040",
    "end": "671040"
  },
  {
    "text": "of them",
    "start": "671040",
    "end": "672000"
  },
  {
    "text": "then this means when there's a node",
    "start": "672000",
    "end": "673519"
  },
  {
    "text": "outage you know you can continue to",
    "start": "673519",
    "end": "675440"
  },
  {
    "text": "write uninterrupted to the cluster",
    "start": "675440",
    "end": "677040"
  },
  {
    "text": "because we'll still be getting that",
    "start": "677040",
    "end": "678399"
  },
  {
    "text": "positive response from two of the notes",
    "start": "678399",
    "end": "681519"
  },
  {
    "text": "what you'll see though is if a second",
    "start": "681519",
    "end": "683360"
  },
  {
    "text": "node fails",
    "start": "683360",
    "end": "684880"
  },
  {
    "text": "even with replication factor three we're",
    "start": "684880",
    "end": "687600"
  },
  {
    "text": "going to have an outage right because",
    "start": "687600",
    "end": "688640"
  },
  {
    "text": "we're not getting that positive response",
    "start": "688640",
    "end": "690240"
  },
  {
    "text": "on rights the",
    "start": "690240",
    "end": "691279"
  },
  {
    "text": "the we don't know that they've succeeded",
    "start": "691279",
    "end": "693920"
  },
  {
    "text": "and therefore",
    "start": "693920",
    "end": "694720"
  },
  {
    "text": "that you basically have an outage for",
    "start": "694720",
    "end": "696640"
  },
  {
    "text": "all of your tenants",
    "start": "696640",
    "end": "698720"
  },
  {
    "text": "now what's what's potentially uh more",
    "start": "698720",
    "end": "701040"
  },
  {
    "text": "worrying is",
    "start": "701040",
    "end": "702240"
  },
  {
    "text": "as cortex clusters get bigger and bigger",
    "start": "702240",
    "end": "704079"
  },
  {
    "text": "you know five years ago we were running",
    "start": "704079",
    "end": "705600"
  },
  {
    "text": "kind of four five nine clusters and",
    "start": "705600",
    "end": "707440"
  },
  {
    "text": "and ten and twenty no clusters now we're",
    "start": "707440",
    "end": "709120"
  },
  {
    "text": "running multi hundred node clusters",
    "start": "709120",
    "end": "711600"
  },
  {
    "text": "and that chance of two node failures",
    "start": "711600",
    "end": "713760"
  },
  {
    "text": "just randomly or through user error",
    "start": "713760",
    "end": "716399"
  },
  {
    "text": "is getting higher right so the chance of",
    "start": "716399",
    "end": "718320"
  },
  {
    "text": "there being a total outage on the",
    "start": "718320",
    "end": "719680"
  },
  {
    "text": "cluster is getting higher",
    "start": "719680",
    "end": "722160"
  },
  {
    "text": "and it's worse than that the uh you know",
    "start": "722160",
    "end": "725760"
  },
  {
    "text": "because every tenant in effect is",
    "start": "725760",
    "end": "727440"
  },
  {
    "text": "writing to every node in the cluster",
    "start": "727440",
    "end": "729440"
  },
  {
    "text": "if there's a bug in cortex if there's a",
    "start": "729440",
    "end": "731279"
  },
  {
    "text": "misconfiguration and the tenant",
    "start": "731279",
    "end": "733440"
  },
  {
    "text": "finds a way to exploit that you know a",
    "start": "733440",
    "end": "735680"
  },
  {
    "text": "poison request or a bad query",
    "start": "735680",
    "end": "737839"
  },
  {
    "text": "could take out an entire cluster for all",
    "start": "737839",
    "end": "739760"
  },
  {
    "text": "tenants",
    "start": "739760",
    "end": "741920"
  },
  {
    "text": "so so these really are the problems",
    "start": "741920",
    "end": "744079"
  },
  {
    "text": "we're trying to solve",
    "start": "744079",
    "end": "745360"
  },
  {
    "text": "in uh with shuffle sharding and",
    "start": "745360",
    "end": "747120"
  },
  {
    "text": "shuffling to be clear is not the only",
    "start": "747120",
    "end": "748480"
  },
  {
    "text": "way of solving it we could we could",
    "start": "748480",
    "end": "750079"
  },
  {
    "text": "we could do something simple right we",
    "start": "750079",
    "end": "751360"
  },
  {
    "text": "could do uh something you know we call",
    "start": "751360",
    "end": "753120"
  },
  {
    "text": "bulkheads right this is where we",
    "start": "753120",
    "end": "754560"
  },
  {
    "text": "effectively",
    "start": "754560",
    "end": "755920"
  },
  {
    "text": "turn you know you can you can think of",
    "start": "755920",
    "end": "757519"
  },
  {
    "text": "this as instead of having one big nine",
    "start": "757519",
    "end": "759120"
  },
  {
    "text": "node cluster you just have three",
    "start": "759120",
    "end": "761040"
  },
  {
    "text": "smaller three node clusters um and you",
    "start": "761040",
    "end": "763920"
  },
  {
    "text": "would just",
    "start": "763920",
    "end": "764399"
  },
  {
    "text": "map tenants to clusters and this way an",
    "start": "764399",
    "end": "767279"
  },
  {
    "text": "outage",
    "start": "767279",
    "end": "767920"
  },
  {
    "text": "in uh you know a poison request by cap",
    "start": "767920",
    "end": "770000"
  },
  {
    "text": "would not affect dolphin or or fox",
    "start": "770000",
    "end": "772800"
  },
  {
    "text": "a sentence i i never thought i'd say you",
    "start": "772800",
    "end": "775600"
  },
  {
    "text": "know we also see",
    "start": "775600",
    "end": "776560"
  },
  {
    "text": "you know two node outage would have to",
    "start": "776560",
    "end": "778320"
  },
  {
    "text": "be in the same shard",
    "start": "778320",
    "end": "780000"
  },
  {
    "text": "to impact any tenant you know challenge",
    "start": "780000",
    "end": "782720"
  },
  {
    "text": "here",
    "start": "782720",
    "end": "783600"
  },
  {
    "text": "is that this mapping is relatively rigid",
    "start": "783600",
    "end": "786800"
  },
  {
    "text": "you know it's very hard in this world to",
    "start": "786800",
    "end": "789040"
  },
  {
    "text": "to have a tenant that needs",
    "start": "789040",
    "end": "790480"
  },
  {
    "text": "all nine nodes worth of throughput um",
    "start": "790480",
    "end": "793200"
  },
  {
    "text": "it's also hard you know if i want to",
    "start": "793200",
    "end": "794480"
  },
  {
    "text": "scale up",
    "start": "794480",
    "end": "795279"
  },
  {
    "text": "you know i do i scale all of the shards",
    "start": "795279",
    "end": "797120"
  },
  {
    "text": "up do i scale one of the shots up what",
    "start": "797120",
    "end": "798480"
  },
  {
    "text": "do i do right",
    "start": "798480",
    "end": "799360"
  },
  {
    "text": "and generally you can see how this kind",
    "start": "799360",
    "end": "800880"
  },
  {
    "text": "of cellular approach",
    "start": "800880",
    "end": "802480"
  },
  {
    "text": "is is it can be a bit of a management",
    "start": "802480",
    "end": "804480"
  },
  {
    "text": "burden",
    "start": "804480",
    "end": "806560"
  },
  {
    "text": "so this is where shuffle sharding comes",
    "start": "806560",
    "end": "809519"
  },
  {
    "start": "807000",
    "end": "958000"
  },
  {
    "text": "in and now i'm going to try and explain",
    "start": "809519",
    "end": "810959"
  },
  {
    "text": "to you",
    "start": "810959",
    "end": "811839"
  },
  {
    "text": "how shuffle sharding works and and then",
    "start": "811839",
    "end": "814000"
  },
  {
    "text": "we'll go on and analyze kind of how we",
    "start": "814000",
    "end": "815760"
  },
  {
    "text": "tune it and what its properties are",
    "start": "815760",
    "end": "818720"
  },
  {
    "text": "so first things worth saying is we",
    "start": "818720",
    "end": "820720"
  },
  {
    "text": "didn't invent shuffle sharding",
    "start": "820720",
    "end": "822560"
  },
  {
    "text": "um the first time i uh became aware of",
    "start": "822560",
    "end": "825040"
  },
  {
    "text": "it was based on this",
    "start": "825040",
    "end": "826000"
  },
  {
    "text": "amazon article in its uh in its builders",
    "start": "826000",
    "end": "828880"
  },
  {
    "text": "library",
    "start": "828880",
    "end": "829600"
  },
  {
    "text": "about how they improved the isolation in",
    "start": "829600",
    "end": "831920"
  },
  {
    "text": "route 53 that dns service",
    "start": "831920",
    "end": "834079"
  },
  {
    "text": "using this technique they called shuffle",
    "start": "834079",
    "end": "835760"
  },
  {
    "text": "sharding um",
    "start": "835760",
    "end": "838399"
  },
  {
    "text": "we read this when when this was",
    "start": "838399",
    "end": "839920"
  },
  {
    "text": "published you know it got passed around",
    "start": "839920",
    "end": "841199"
  },
  {
    "text": "internally at grafana labs and we're",
    "start": "841199",
    "end": "842560"
  },
  {
    "text": "like yeah this would be a really kind of",
    "start": "842560",
    "end": "844399"
  },
  {
    "text": "interesting piece of work to do",
    "start": "844399",
    "end": "846560"
  },
  {
    "text": "on cortex you know we could see its",
    "start": "846560",
    "end": "848320"
  },
  {
    "text": "direct benefits",
    "start": "848320",
    "end": "849920"
  },
  {
    "text": "so what shuffle shining does is it",
    "start": "849920",
    "end": "852399"
  },
  {
    "text": "effectively picks a random",
    "start": "852399",
    "end": "855519"
  },
  {
    "text": "sub cluster of the cluster for each",
    "start": "855519",
    "end": "858240"
  },
  {
    "text": "tenant",
    "start": "858240",
    "end": "859760"
  },
  {
    "text": "you know we we pick this subset in",
    "start": "859760",
    "end": "862480"
  },
  {
    "text": "random but we but it is a",
    "start": "862480",
    "end": "863760"
  },
  {
    "text": "deterministically random so",
    "start": "863760",
    "end": "865680"
  },
  {
    "text": "we we use the tenant id we actually hash",
    "start": "865680",
    "end": "867760"
  },
  {
    "text": "the tenant id to select",
    "start": "867760",
    "end": "869839"
  },
  {
    "text": "the the nodes in the cluster and then",
    "start": "869839",
    "end": "872000"
  },
  {
    "text": "with those nodes in the cluster that",
    "start": "872000",
    "end": "873440"
  },
  {
    "text": "that tenant is using",
    "start": "873440",
    "end": "874880"
  },
  {
    "text": "we use the normal cortex replication",
    "start": "874880",
    "end": "877040"
  },
  {
    "text": "scheme",
    "start": "877040",
    "end": "878399"
  },
  {
    "text": "to to distribute rights among those",
    "start": "878399",
    "end": "880240"
  },
  {
    "text": "notes",
    "start": "880240",
    "end": "882560"
  },
  {
    "text": "this means that you know this this gives",
    "start": "882639",
    "end": "884399"
  },
  {
    "text": "you a nice property where",
    "start": "884399",
    "end": "886079"
  },
  {
    "text": "you can have tenants of different sizes",
    "start": "886079",
    "end": "887839"
  },
  {
    "text": "you know using the same cluster",
    "start": "887839",
    "end": "889760"
  },
  {
    "text": "and you can control depending on like",
    "start": "889760",
    "end": "891680"
  },
  {
    "text": "how many nodes you give each tenant",
    "start": "891680",
    "end": "894160"
  },
  {
    "text": "the the isolation between tenants",
    "start": "894160",
    "end": "898720"
  },
  {
    "text": "you know to give you an example you know",
    "start": "898720",
    "end": "899920"
  },
  {
    "text": "if we if if we have a",
    "start": "899920",
    "end": "901920"
  },
  {
    "text": "three node outage in this situation we",
    "start": "901920",
    "end": "904160"
  },
  {
    "text": "can see this only affected one tenant",
    "start": "904160",
    "end": "906000"
  },
  {
    "text": "because",
    "start": "906000",
    "end": "906800"
  },
  {
    "text": "both dolphin and fox only had one node",
    "start": "906800",
    "end": "909519"
  },
  {
    "text": "impacted by that outage",
    "start": "909519",
    "end": "911360"
  },
  {
    "text": "you know so this is kind of the basic",
    "start": "911360",
    "end": "912800"
  },
  {
    "text": "idea right it gives you",
    "start": "912800",
    "end": "914880"
  },
  {
    "text": "much better tolerance to failure with",
    "start": "914880",
    "end": "916720"
  },
  {
    "text": "with kind of a partially degraded state",
    "start": "916720",
    "end": "919760"
  },
  {
    "text": "you know another example and you know i",
    "start": "919760",
    "end": "921120"
  },
  {
    "text": "talked about poison request earlier",
    "start": "921120",
    "end": "922880"
  },
  {
    "text": "if cap were to do a poison request you",
    "start": "922880",
    "end": "926000"
  },
  {
    "text": "can see how dolphin and fox are not",
    "start": "926000",
    "end": "928720"
  },
  {
    "text": "affected because",
    "start": "928720",
    "end": "929839"
  },
  {
    "text": "they've again only got one node that's",
    "start": "929839",
    "end": "932000"
  },
  {
    "text": "been been poisoned",
    "start": "932000",
    "end": "934399"
  },
  {
    "text": "so this is the basic idea you know we",
    "start": "934399",
    "end": "936079"
  },
  {
    "text": "randomly select",
    "start": "936079",
    "end": "937440"
  },
  {
    "text": "subsets of the node for each tenant as",
    "start": "937440",
    "end": "939839"
  },
  {
    "text": "subsets of the cluster for each tenant",
    "start": "939839",
    "end": "942240"
  },
  {
    "text": "we then randomly distribute using the",
    "start": "942240",
    "end": "944320"
  },
  {
    "text": "normal scheme um",
    "start": "944320",
    "end": "945519"
  },
  {
    "text": "samples from these tenants within that",
    "start": "945519",
    "end": "947120"
  },
  {
    "text": "subcluster and then we make sure you",
    "start": "947120",
    "end": "949600"
  },
  {
    "text": "know we want to tune the",
    "start": "949600",
    "end": "951199"
  },
  {
    "text": "the number of nodes that we give to each",
    "start": "951199",
    "end": "953360"
  },
  {
    "text": "tenant to optimize between you know",
    "start": "953360",
    "end": "955759"
  },
  {
    "text": "optimize for",
    "start": "955759",
    "end": "956560"
  },
  {
    "text": "for isolation so this is where we kind",
    "start": "956560",
    "end": "960240"
  },
  {
    "text": "of",
    "start": "960240",
    "end": "960880"
  },
  {
    "text": "have to start thinking about well how",
    "start": "960880",
    "end": "962480"
  },
  {
    "text": "many nodes do we want to give",
    "start": "962480",
    "end": "964399"
  },
  {
    "text": "each tenant and how do we optimize",
    "start": "964399",
    "end": "966399"
  },
  {
    "text": "isolation and what are the trade-offs",
    "start": "966399",
    "end": "969920"
  },
  {
    "text": "i'm going to play cards so imagine we",
    "start": "969920",
    "end": "972639"
  },
  {
    "text": "had a 52-node cluster represented by a",
    "start": "972639",
    "end": "974720"
  },
  {
    "text": "deck of cards you know we're going to",
    "start": "974720",
    "end": "976000"
  },
  {
    "text": "shuffle that deck",
    "start": "976000",
    "end": "977199"
  },
  {
    "text": "and we're going to deal out four cards",
    "start": "977199",
    "end": "978880"
  },
  {
    "text": "right how many different hands",
    "start": "978880",
    "end": "981519"
  },
  {
    "text": "do you think how many different",
    "start": "981519",
    "end": "982399"
  },
  {
    "text": "combinations of four cards are there",
    "start": "982399",
    "end": "984240"
  },
  {
    "text": "well it turns out",
    "start": "984240",
    "end": "985199"
  },
  {
    "text": "there's some maths that can work this",
    "start": "985199",
    "end": "986800"
  },
  {
    "text": "out it's called the m",
    "start": "986800",
    "end": "988320"
  },
  {
    "text": "choose k problem and and 52",
    "start": "988320",
    "end": "991839"
  },
  {
    "text": "52 choose four is about 270 000 right so",
    "start": "991839",
    "end": "995440"
  },
  {
    "text": "if i were to pick sets of four nodes",
    "start": "995440",
    "end": "997759"
  },
  {
    "text": "from my 52 node cluster",
    "start": "997759",
    "end": "999600"
  },
  {
    "text": "there's 270 000 different combinations",
    "start": "999600",
    "end": "1002480"
  },
  {
    "text": "of four nodes",
    "start": "1002480",
    "end": "1004000"
  },
  {
    "text": "it's a huge number but that's actually",
    "start": "1004000",
    "end": "1006560"
  },
  {
    "text": "in and of itself not super useful",
    "start": "1006560",
    "end": "1008480"
  },
  {
    "text": "what i really want to know is of those",
    "start": "1008480",
    "end": "1011320"
  },
  {
    "text": "270",
    "start": "1011320",
    "end": "1012639"
  },
  {
    "text": "000 combinations how many of them",
    "start": "1012639",
    "end": "1015920"
  },
  {
    "text": "share one node how many of them share",
    "start": "1015920",
    "end": "1019360"
  },
  {
    "text": "two nodes in common you know and it",
    "start": "1019360",
    "end": "1021680"
  },
  {
    "text": "turns out that's not difficult to work",
    "start": "1021680",
    "end": "1023199"
  },
  {
    "text": "out either um there's a link in the top",
    "start": "1023199",
    "end": "1024798"
  },
  {
    "text": "to",
    "start": "1024799",
    "end": "1025520"
  },
  {
    "text": "a stack overflow article about how to",
    "start": "1025520",
    "end": "1026959"
  },
  {
    "text": "work it out my math is not good enough",
    "start": "1026959",
    "end": "1028400"
  },
  {
    "text": "to",
    "start": "1028400",
    "end": "1028880"
  },
  {
    "text": "to derive this but suffice to say you",
    "start": "1028880",
    "end": "1031120"
  },
  {
    "text": "know almost three quarters of these",
    "start": "1031120",
    "end": "1033438"
  },
  {
    "text": "of these selections don't share any",
    "start": "1033439",
    "end": "1035678"
  },
  {
    "text": "nodes in common",
    "start": "1035679",
    "end": "1036640"
  },
  {
    "text": "you know a quarter of them share one",
    "start": "1036640",
    "end": "1039360"
  },
  {
    "text": "node in common",
    "start": "1039360",
    "end": "1040720"
  },
  {
    "text": "and only about two and a half percent",
    "start": "1040720",
    "end": "1042240"
  },
  {
    "text": "share two notes in common right so this",
    "start": "1042240",
    "end": "1044000"
  },
  {
    "text": "is an incredibly kind of strong result",
    "start": "1044000",
    "end": "1046720"
  },
  {
    "text": "that shows that you know for argument's",
    "start": "1046720",
    "end": "1049440"
  },
  {
    "text": "sake a 52-note cluster where all the",
    "start": "1049440",
    "end": "1051120"
  },
  {
    "text": "shuffle shards were",
    "start": "1051120",
    "end": "1052400"
  },
  {
    "text": "of size four a two note outage would",
    "start": "1052400",
    "end": "1055120"
  },
  {
    "text": "only impact",
    "start": "1055120",
    "end": "1056000"
  },
  {
    "text": "two and a half well less than two and a",
    "start": "1056000",
    "end": "1057600"
  },
  {
    "text": "half percent of the tenants",
    "start": "1057600",
    "end": "1059840"
  },
  {
    "text": "worst case two and a half percent of the",
    "start": "1059840",
    "end": "1061200"
  },
  {
    "text": "tenants",
    "start": "1061200",
    "end": "1063360"
  },
  {
    "start": "1062000",
    "end": "1355000"
  },
  {
    "text": "but there's more to it than that right",
    "start": "1063360",
    "end": "1065039"
  },
  {
    "text": "when we're picking",
    "start": "1065039",
    "end": "1066400"
  },
  {
    "text": "how many nodes to give each tenant",
    "start": "1066400",
    "end": "1070160"
  },
  {
    "text": "we we need to trade off uh you know",
    "start": "1070160",
    "end": "1073120"
  },
  {
    "text": "fewer nodes means we're going to have",
    "start": "1073120",
    "end": "1074559"
  },
  {
    "text": "better isolation if i give",
    "start": "1074559",
    "end": "1076080"
  },
  {
    "text": "each tenant one node you know the the",
    "start": "1076080",
    "end": "1079039"
  },
  {
    "text": "num the",
    "start": "1079039",
    "end": "1079679"
  },
  {
    "text": "you know this the the isolation between",
    "start": "1079679",
    "end": "1083200"
  },
  {
    "text": "each tenant is going to be as good as it",
    "start": "1083200",
    "end": "1084400"
  },
  {
    "text": "possibly can be right because",
    "start": "1084400",
    "end": "1086240"
  },
  {
    "text": "you know the chance of two tenants",
    "start": "1086240",
    "end": "1087840"
  },
  {
    "text": "basically hitting the same node is just",
    "start": "1087840",
    "end": "1090400"
  },
  {
    "text": "going to be like 1 in 52.",
    "start": "1090400",
    "end": "1093120"
  },
  {
    "text": "if i uh give tenants more nodes though",
    "start": "1093120",
    "end": "1095679"
  },
  {
    "text": "i'm going to be able to spread that load",
    "start": "1095679",
    "end": "1097360"
  },
  {
    "text": "more evenly",
    "start": "1097360",
    "end": "1098320"
  },
  {
    "text": "and in a cortex cluster the",
    "start": "1098320",
    "end": "1101440"
  },
  {
    "text": "you know the tenants aren't all the same",
    "start": "1101440",
    "end": "1102960"
  },
  {
    "text": "size right we have some very large",
    "start": "1102960",
    "end": "1104480"
  },
  {
    "text": "tenants we have some very small tanks we",
    "start": "1104480",
    "end": "1105919"
  },
  {
    "text": "have everything in between",
    "start": "1105919",
    "end": "1108240"
  },
  {
    "text": "so we need an algorithm really for",
    "start": "1108240",
    "end": "1109840"
  },
  {
    "text": "picking how many nodes",
    "start": "1109840",
    "end": "1111280"
  },
  {
    "text": "how many shuffle shards to give each",
    "start": "1111280",
    "end": "1113600"
  },
  {
    "text": "tenant",
    "start": "1113600",
    "end": "1114960"
  },
  {
    "text": "you know one thing i would say is that",
    "start": "1114960",
    "end": "1117280"
  },
  {
    "text": "you know better load balancing isn't",
    "start": "1117280",
    "end": "1118720"
  },
  {
    "text": "just a nice to have",
    "start": "1118720",
    "end": "1120000"
  },
  {
    "text": "right better load balancing can lead to",
    "start": "1120000",
    "end": "1122720"
  },
  {
    "text": "higher utilization of resources",
    "start": "1122720",
    "end": "1125120"
  },
  {
    "text": "can lead to a lower cost of running the",
    "start": "1125120",
    "end": "1126960"
  },
  {
    "text": "cluster and if you run",
    "start": "1126960",
    "end": "1128320"
  },
  {
    "text": "cortex as uh as an offering you know as",
    "start": "1128320",
    "end": "1130720"
  },
  {
    "text": "your sas platform like we do in grafana",
    "start": "1130720",
    "end": "1132480"
  },
  {
    "text": "cloud",
    "start": "1132480",
    "end": "1133039"
  },
  {
    "text": "you know this is super important to us",
    "start": "1133039",
    "end": "1136080"
  },
  {
    "text": "so we proposed a simple algorithm right",
    "start": "1136080",
    "end": "1138960"
  },
  {
    "text": "this is to give tenants the number of",
    "start": "1138960",
    "end": "1140720"
  },
  {
    "text": "shuffle shards",
    "start": "1140720",
    "end": "1141840"
  },
  {
    "text": "uh proportional to the number of series",
    "start": "1141840",
    "end": "1144000"
  },
  {
    "text": "so let's say you know if you've got",
    "start": "1144000",
    "end": "1145440"
  },
  {
    "text": "a million series and we decide that",
    "start": "1145440",
    "end": "1147360"
  },
  {
    "text": "we're gonna give you one shuffle shard",
    "start": "1147360",
    "end": "1149039"
  },
  {
    "text": "per hundred thousand",
    "start": "1149039",
    "end": "1150240"
  },
  {
    "text": "we'd give you ten shuffle shards and",
    "start": "1150240",
    "end": "1152720"
  },
  {
    "text": "really what we want to do is find out",
    "start": "1152720",
    "end": "1154480"
  },
  {
    "text": "what that",
    "start": "1154480",
    "end": "1155360"
  },
  {
    "text": "that hundred thousand number is",
    "start": "1155360",
    "end": "1158400"
  },
  {
    "text": "you know what is the right value for",
    "start": "1158400",
    "end": "1159760"
  },
  {
    "text": "that number what is that constant",
    "start": "1159760",
    "end": "1161919"
  },
  {
    "text": "so again as i said earlier my math is",
    "start": "1161919",
    "end": "1164880"
  },
  {
    "text": "not good enough to derive this from kind",
    "start": "1164880",
    "end": "1166480"
  },
  {
    "text": "of first principles",
    "start": "1166480",
    "end": "1167679"
  },
  {
    "text": "i if anyone in the audience knows how to",
    "start": "1167679",
    "end": "1170160"
  },
  {
    "text": "do this kind of um",
    "start": "1170160",
    "end": "1171600"
  },
  {
    "text": "mathematically i'd be really interested",
    "start": "1171600",
    "end": "1173120"
  },
  {
    "text": "in chatting to you but i'm a software",
    "start": "1173120",
    "end": "1175280"
  },
  {
    "text": "engineer so we built a simulator",
    "start": "1175280",
    "end": "1177520"
  },
  {
    "text": "um you know the simulator kind of",
    "start": "1177520",
    "end": "1179200"
  },
  {
    "text": "simulated a",
    "start": "1179200",
    "end": "1180559"
  },
  {
    "text": "a cortex cluster of a certain size i",
    "start": "1180559",
    "end": "1182480"
  },
  {
    "text": "think we simulated kind of 60 70",
    "start": "1182480",
    "end": "1184559"
  },
  {
    "text": "nodes um simulated a a set of tenants of",
    "start": "1184559",
    "end": "1188400"
  },
  {
    "text": "roughly",
    "start": "1188400",
    "end": "1189360"
  },
  {
    "text": "you know a distribution of sizes that we",
    "start": "1189360",
    "end": "1191520"
  },
  {
    "text": "observe in our production clusters",
    "start": "1191520",
    "end": "1193679"
  },
  {
    "text": "and simulated kind of you know picking",
    "start": "1193679",
    "end": "1196480"
  },
  {
    "text": "shuffle shard sizes",
    "start": "1196480",
    "end": "1198240"
  },
  {
    "text": "distributing the samples to each of the",
    "start": "1198240",
    "end": "1200400"
  },
  {
    "text": "virtual each of the nodes",
    "start": "1200400",
    "end": "1202799"
  },
  {
    "text": "and measuring kind of the um",
    "start": "1202799",
    "end": "1206400"
  },
  {
    "text": "the variance in node load just just",
    "start": "1206400",
    "end": "1208799"
  },
  {
    "text": "based on the number of uh",
    "start": "1208799",
    "end": "1210080"
  },
  {
    "text": "of series that they they have and the",
    "start": "1210080",
    "end": "1213280"
  },
  {
    "text": "number of tenants that were impacted by",
    "start": "1213280",
    "end": "1217120"
  },
  {
    "text": "by two nodes going away what proportion",
    "start": "1217120",
    "end": "1219120"
  },
  {
    "text": "of tenants would be impacted by two",
    "start": "1219120",
    "end": "1220640"
  },
  {
    "text": "nodes we actually measured any two nodes",
    "start": "1220640",
    "end": "1222400"
  },
  {
    "text": "going away",
    "start": "1222400",
    "end": "1224159"
  },
  {
    "text": "i think the uh the simulator is open",
    "start": "1224159",
    "end": "1225760"
  },
  {
    "text": "source um so so do",
    "start": "1225760",
    "end": "1227280"
  },
  {
    "text": "ask me uh ask me afterwards if you want",
    "start": "1227280",
    "end": "1229039"
  },
  {
    "text": "to link to the source code",
    "start": "1229039",
    "end": "1230799"
  },
  {
    "text": "so suffice to say we got a couple of",
    "start": "1230799",
    "end": "1232159"
  },
  {
    "text": "graphs from the simulator this one",
    "start": "1232159",
    "end": "1234240"
  },
  {
    "text": "shows the um the load balancing how how",
    "start": "1234240",
    "end": "1238400"
  },
  {
    "text": "well load is distributed within the",
    "start": "1238400",
    "end": "1240080"
  },
  {
    "text": "cluster",
    "start": "1240080",
    "end": "1240799"
  },
  {
    "text": "versus the size of each shuffle shard so",
    "start": "1240799",
    "end": "1243200"
  },
  {
    "text": "shuffle shard along the",
    "start": "1243200",
    "end": "1244240"
  },
  {
    "text": "x-axis load load distribution along the",
    "start": "1244240",
    "end": "1247039"
  },
  {
    "text": "y",
    "start": "1247039",
    "end": "1247440"
  },
  {
    "text": "and what you can see here is as you",
    "start": "1247440",
    "end": "1251520"
  },
  {
    "text": "increase the size of the shuffle shards",
    "start": "1251520",
    "end": "1253520"
  },
  {
    "text": "the distribution of load gets worse",
    "start": "1253520",
    "end": "1255280"
  },
  {
    "text": "as we predicted um you can see kind of",
    "start": "1255280",
    "end": "1258559"
  },
  {
    "text": "interestingly kind of the distribution",
    "start": "1258559",
    "end": "1260080"
  },
  {
    "text": "of load starts to tail off i believe",
    "start": "1260080",
    "end": "1261919"
  },
  {
    "text": "this happens as kind of just",
    "start": "1261919",
    "end": "1263760"
  },
  {
    "text": "small tenants start to hit the minimum",
    "start": "1263760",
    "end": "1266400"
  },
  {
    "text": "number of shuffle shots which is three",
    "start": "1266400",
    "end": "1267840"
  },
  {
    "text": "for replication",
    "start": "1267840",
    "end": "1270159"
  },
  {
    "text": "we we also see here that you know",
    "start": "1270159",
    "end": "1273440"
  },
  {
    "text": "at kind of let's just pick a number then",
    "start": "1273440",
    "end": "1275039"
  },
  {
    "text": "the numbers aren't super relevant in",
    "start": "1275039",
    "end": "1276480"
  },
  {
    "text": "this this is just a general rule of",
    "start": "1276480",
    "end": "1278000"
  },
  {
    "text": "thumb but",
    "start": "1278000",
    "end": "1278880"
  },
  {
    "text": "let's say a shuffle shard size of 40 000",
    "start": "1278880",
    "end": "1281679"
  },
  {
    "text": "um",
    "start": "1281679",
    "end": "1283200"
  },
  {
    "text": "series we can see that the",
    "start": "1283200",
    "end": "1287280"
  },
  {
    "text": "maximum size a node gets to right the",
    "start": "1287280",
    "end": "1290080"
  },
  {
    "text": "maximum number of series on a single",
    "start": "1290080",
    "end": "1291520"
  },
  {
    "text": "node is about one and a half million",
    "start": "1291520",
    "end": "1293039"
  },
  {
    "text": "and the minimum is about 75 uh 750 000",
    "start": "1293039",
    "end": "1297039"
  },
  {
    "text": "right so there's a factor of two",
    "start": "1297039",
    "end": "1298240"
  },
  {
    "text": "difference here",
    "start": "1298240",
    "end": "1299520"
  },
  {
    "text": "right that gives us some kind of you",
    "start": "1299520",
    "end": "1301919"
  },
  {
    "text": "know idea you know we probably",
    "start": "1301919",
    "end": "1303360"
  },
  {
    "text": "don't want a factor of two difference in",
    "start": "1303360",
    "end": "1305120"
  },
  {
    "text": "the in the size",
    "start": "1305120",
    "end": "1306559"
  },
  {
    "text": "of our modes right this is going to make",
    "start": "1306559",
    "end": "1308000"
  },
  {
    "text": "it very hard to optimize um",
    "start": "1308000",
    "end": "1310000"
  },
  {
    "text": "utilization we also see",
    "start": "1310000",
    "end": "1313360"
  },
  {
    "text": "as you increase the size of the shard",
    "start": "1313360",
    "end": "1315520"
  },
  {
    "text": "the isolation",
    "start": "1315520",
    "end": "1316960"
  },
  {
    "text": "measured as the percentage of tenants",
    "start": "1316960",
    "end": "1318640"
  },
  {
    "text": "affected by a two node outage",
    "start": "1318640",
    "end": "1320559"
  },
  {
    "text": "the isolation starts to fall and",
    "start": "1320559",
    "end": "1322640"
  },
  {
    "text": "eventually again plateaus",
    "start": "1322640",
    "end": "1324400"
  },
  {
    "text": "so we can see that let's say thirty",
    "start": "1324400",
    "end": "1326400"
  },
  {
    "text": "thousand again thirty forty thousand",
    "start": "1326400",
    "end": "1328559"
  },
  {
    "text": "you know way less than one percent of",
    "start": "1328559",
    "end": "1330159"
  },
  {
    "text": "tenants in your cluster",
    "start": "1330159",
    "end": "1331600"
  },
  {
    "text": "are affected by by a two node outage you",
    "start": "1331600",
    "end": "1334640"
  },
  {
    "text": "know this was modeled with",
    "start": "1334640",
    "end": "1335840"
  },
  {
    "text": "a thousand tenants um we were averaging",
    "start": "1335840",
    "end": "1338000"
  },
  {
    "text": "i think a hundred thousand series per",
    "start": "1338000",
    "end": "1339679"
  },
  {
    "text": "tenants",
    "start": "1339679",
    "end": "1340400"
  },
  {
    "text": "one of the key things this um simulation",
    "start": "1340400",
    "end": "1342880"
  },
  {
    "text": "took into account",
    "start": "1342880",
    "end": "1344080"
  },
  {
    "text": "was it also measured you know also",
    "start": "1344080",
    "end": "1346080"
  },
  {
    "text": "simulated replication factor",
    "start": "1346080",
    "end": "1349519"
  },
  {
    "text": "so whilst working on this we kind of",
    "start": "1349600",
    "end": "1351840"
  },
  {
    "text": "picked some numbers we debated",
    "start": "1351840",
    "end": "1353200"
  },
  {
    "text": "internally we kind of find where the two",
    "start": "1353200",
    "end": "1354799"
  },
  {
    "text": "graphs cross",
    "start": "1354799",
    "end": "1355840"
  },
  {
    "start": "1355000",
    "end": "1427000"
  },
  {
    "text": "and we came up with this kind of good",
    "start": "1355840",
    "end": "1357520"
  },
  {
    "text": "rule of thumb you know at around 20",
    "start": "1357520",
    "end": "1359280"
  },
  {
    "text": "000 series per shard we have a roughly",
    "start": "1359280",
    "end": "1362159"
  },
  {
    "text": "20",
    "start": "1362159",
    "end": "1362799"
  },
  {
    "text": "variance in the uh series per node and",
    "start": "1362799",
    "end": "1365600"
  },
  {
    "text": "roughly two percent of tenants affected",
    "start": "1365600",
    "end": "1367360"
  },
  {
    "text": "by a two node outage",
    "start": "1367360",
    "end": "1368720"
  },
  {
    "text": "and i believe our production config that",
    "start": "1368720",
    "end": "1371679"
  },
  {
    "text": "we run",
    "start": "1371679",
    "end": "1372320"
  },
  {
    "text": "on our large cortex clusters matches",
    "start": "1372320",
    "end": "1375440"
  },
  {
    "text": "this roughly i think 20 30 000",
    "start": "1375440",
    "end": "1377440"
  },
  {
    "text": "series per shard is what we run",
    "start": "1377440",
    "end": "1379039"
  },
  {
    "text": "internally and this is really good",
    "start": "1379039",
    "end": "1381039"
  },
  {
    "text": "because what this means is",
    "start": "1381039",
    "end": "1383520"
  },
  {
    "text": "by by reducing the chance of an outage",
    "start": "1383520",
    "end": "1386720"
  },
  {
    "text": "for",
    "start": "1386720",
    "end": "1387039"
  },
  {
    "text": "most tenants uh when there's two nodes",
    "start": "1387039",
    "end": "1389919"
  },
  {
    "text": "two nodes",
    "start": "1389919",
    "end": "1390880"
  },
  {
    "text": "that are suffering problems we've been",
    "start": "1390880",
    "end": "1392400"
  },
  {
    "text": "able to scale up to even larger cortex",
    "start": "1392400",
    "end": "1394400"
  },
  {
    "text": "clusters",
    "start": "1394400",
    "end": "1395120"
  },
  {
    "text": "you know to hundreds of nodes as opposed",
    "start": "1395120",
    "end": "1396799"
  },
  {
    "text": "to tens right",
    "start": "1396799",
    "end": "1398480"
  },
  {
    "text": "we've also managed to better isolate",
    "start": "1398480",
    "end": "1401039"
  },
  {
    "text": "tenants from each other so there'll be",
    "start": "1401039",
    "end": "1402320"
  },
  {
    "text": "less noisy neighbor",
    "start": "1402320",
    "end": "1403679"
  },
  {
    "text": "there'll be less chance of a poison pill",
    "start": "1403679",
    "end": "1405760"
  },
  {
    "text": "affecting other tenants",
    "start": "1405760",
    "end": "1407919"
  },
  {
    "text": "we managed to do all of this whilst",
    "start": "1407919",
    "end": "1410159"
  },
  {
    "text": "keeping the variance in load",
    "start": "1410159",
    "end": "1412240"
  },
  {
    "text": "amongst these nodes relatively bound and",
    "start": "1412240",
    "end": "1415679"
  },
  {
    "text": "and therefore kind of not reducing you",
    "start": "1415679",
    "end": "1417840"
  },
  {
    "text": "know not increasing rather the",
    "start": "1417840",
    "end": "1419600"
  },
  {
    "text": "the cost of running this cluster and not",
    "start": "1419600",
    "end": "1421279"
  },
  {
    "text": "passing on any kind of cost to the",
    "start": "1421279",
    "end": "1423279"
  },
  {
    "text": "customer for this",
    "start": "1423279",
    "end": "1425840"
  },
  {
    "text": "so i think this is a really positive",
    "start": "1425840",
    "end": "1427039"
  },
  {
    "start": "1427000",
    "end": "1539000"
  },
  {
    "text": "result um i'm i'm really kind of pleased",
    "start": "1427039",
    "end": "1430080"
  },
  {
    "text": "with the work",
    "start": "1430080",
    "end": "1430720"
  },
  {
    "text": "and and surprised at how effective",
    "start": "1430720",
    "end": "1432480"
  },
  {
    "text": "shuffle sharding is",
    "start": "1432480",
    "end": "1434000"
  },
  {
    "text": "we talked today about you know what",
    "start": "1434000",
    "end": "1435520"
  },
  {
    "text": "cortex is the horizontally scalable",
    "start": "1435520",
    "end": "1437440"
  },
  {
    "text": "version of prometheus that which kind of",
    "start": "1437440",
    "end": "1439679"
  },
  {
    "text": "allows you to centralize your",
    "start": "1439679",
    "end": "1441279"
  },
  {
    "text": "observability",
    "start": "1441279",
    "end": "1442400"
  },
  {
    "text": "into a single single cluster and and act",
    "start": "1442400",
    "end": "1445520"
  },
  {
    "text": "as kind of your own service provider",
    "start": "1445520",
    "end": "1446960"
  },
  {
    "text": "within within your organization",
    "start": "1446960",
    "end": "1448880"
  },
  {
    "text": "we've talked about how we distributed",
    "start": "1448880",
    "end": "1450400"
  },
  {
    "text": "load before we implemented shuffle",
    "start": "1450400",
    "end": "1451840"
  },
  {
    "text": "sharding",
    "start": "1451840",
    "end": "1452400"
  },
  {
    "text": "and how we just distributed all tenants",
    "start": "1452400",
    "end": "1454720"
  },
  {
    "text": "to all nodes",
    "start": "1454720",
    "end": "1455520"
  },
  {
    "text": "and how we use the hashing algorithm and",
    "start": "1455520",
    "end": "1457120"
  },
  {
    "text": "a kind of a dht to",
    "start": "1457120",
    "end": "1458640"
  },
  {
    "text": "to do that then we've talked about",
    "start": "1458640",
    "end": "1460159"
  },
  {
    "text": "shuffle sharding how shuffle sharding",
    "start": "1460159",
    "end": "1463039"
  },
  {
    "text": "effectively builds small virtual",
    "start": "1463039",
    "end": "1464960"
  },
  {
    "text": "clusters inside a much larger",
    "start": "1464960",
    "end": "1467120"
  },
  {
    "text": "real cluster and how these virtual",
    "start": "1467120",
    "end": "1470000"
  },
  {
    "text": "clusters",
    "start": "1470000",
    "end": "1471520"
  },
  {
    "text": "improve the isolation between tenants",
    "start": "1471520",
    "end": "1475200"
  },
  {
    "text": "at not a huge expense in in terms of",
    "start": "1475200",
    "end": "1477600"
  },
  {
    "text": "utilization",
    "start": "1477600",
    "end": "1479039"
  },
  {
    "text": "and that's really the talk i wanted to",
    "start": "1479039",
    "end": "1481120"
  },
  {
    "text": "say thank you to a few people",
    "start": "1481120",
    "end": "1482799"
  },
  {
    "text": "i wanted to say thank you to marco marco",
    "start": "1482799",
    "end": "1486080"
  },
  {
    "text": "and and thank you to peter who really",
    "start": "1486080",
    "end": "1488400"
  },
  {
    "text": "did all the work here",
    "start": "1488400",
    "end": "1489520"
  },
  {
    "text": "and they should be the ones giving this",
    "start": "1489520",
    "end": "1490799"
  },
  {
    "text": "talk what's more kind of the",
    "start": "1490799",
    "end": "1493600"
  },
  {
    "text": "the slides i'm giving here are an",
    "start": "1493600",
    "end": "1495840"
  },
  {
    "text": "evolution of marco's internal slides",
    "start": "1495840",
    "end": "1497679"
  },
  {
    "text": "that he gave",
    "start": "1497679",
    "end": "1498400"
  },
  {
    "text": "at a talk inside grafana labs i also",
    "start": "1498400",
    "end": "1501520"
  },
  {
    "text": "wanted to say thank you to amazon",
    "start": "1501520",
    "end": "1503840"
  },
  {
    "text": "they sponsored uh grafana labs to make",
    "start": "1503840",
    "end": "1506000"
  },
  {
    "text": "these changes to cortex",
    "start": "1506000",
    "end": "1508000"
  },
  {
    "text": "really worked closely with us on the",
    "start": "1508000",
    "end": "1509679"
  },
  {
    "text": "design and on reviewing it and really",
    "start": "1509679",
    "end": "1512400"
  },
  {
    "text": "kind of giving them some great feedback",
    "start": "1512400",
    "end": "1514640"
  },
  {
    "text": "we uh if you want to hear more about how",
    "start": "1514640",
    "end": "1516480"
  },
  {
    "text": "grafana labs and amazon have worked",
    "start": "1516480",
    "end": "1518240"
  },
  {
    "text": "together to",
    "start": "1518240",
    "end": "1519279"
  },
  {
    "text": "to help uh amazon launch their",
    "start": "1519279",
    "end": "1521039"
  },
  {
    "text": "prometheus service there's a",
    "start": "1521039",
    "end": "1522559"
  },
  {
    "text": "there's a blog post on amazon's blog and",
    "start": "1522559",
    "end": "1524400"
  },
  {
    "text": "a blog on uh grafana's blog",
    "start": "1524400",
    "end": "1526320"
  },
  {
    "text": "that really goes into a little detail",
    "start": "1526320",
    "end": "1527600"
  },
  {
    "text": "about how how the relationships worked",
    "start": "1527600",
    "end": "1529279"
  },
  {
    "text": "and",
    "start": "1529279",
    "end": "1529600"
  },
  {
    "text": "what kind of things we've built for",
    "start": "1529600",
    "end": "1531600"
  },
  {
    "text": "amazon",
    "start": "1531600",
    "end": "1533200"
  },
  {
    "text": "and with that i'd like to say thank you",
    "start": "1533200",
    "end": "1535279"
  },
  {
    "text": "and open up the floor to uh to questions",
    "start": "1535279",
    "end": "1538480"
  },
  {
    "text": "thank you",
    "start": "1538480",
    "end": "1541440"
  }
]