[
  {
    "start": "0",
    "end": "156000"
  },
  {
    "text": "okay let's get started welcome we are",
    "start": "170",
    "end": "7319"
  },
  {
    "text": "talking about cardinality and disasters today so who doesn't love a disaster",
    "start": "7319",
    "end": "13679"
  },
  {
    "text": "movie my name is Brian Boram I work for a company called weave works I worked",
    "start": "13679",
    "end": "23279"
  },
  {
    "text": "for a company called spunk and yeah so yeah so so we're making a disaster movie",
    "start": "23279",
    "end": "31550"
  },
  {
    "text": "and the first disastrous the keyboard doesn't work great yeah so colorful",
    "start": "31550",
    "end": "42210"
  },
  {
    "text": "pictures are essential to any movie and what happens if you if you do a google",
    "start": "42210",
    "end": "49469"
  },
  {
    "text": "image search for cardinality you've got a bunch of really really dull like",
    "start": "49469",
    "end": "55800"
  },
  {
    "text": "mathematical diagrams and so that wasn't any good",
    "start": "55800",
    "end": "61230"
  },
  {
    "text": "and this is this is my contribution it's",
    "start": "61230",
    "end": "66689"
  },
  {
    "text": "a cardinal bird so go go Cardinals okay",
    "start": "66689",
    "end": "74520"
  },
  {
    "text": "so who are we and why are we talking about cardinality I'll go first",
    "start": "74520",
    "end": "80000"
  },
  {
    "text": "so as I mentioned I work at we've works and one of the things we have a product",
    "start": "80159",
    "end": "89130"
  },
  {
    "text": "offering is a Prometheus as a service you know it's part of our weave cloud",
    "start": "89130",
    "end": "95549"
  },
  {
    "text": "offering and what we let people do is",
    "start": "95549",
    "end": "102200"
  },
  {
    "text": "fire up agents on their cluster which are uploading metrics time series data",
    "start": "102200",
    "end": "108360"
  },
  {
    "text": "to our back-end and anyone can do this you can you can you know take out a free",
    "start": "108360",
    "end": "114810"
  },
  {
    "text": "trial so basically we we see we've seen everything we've seen all manner of",
    "start": "114810",
    "end": "122310"
  },
  {
    "text": "random data and misconfigured Prometheus and kubernetes and everything everything",
    "start": "122310",
    "end": "129450"
  },
  {
    "text": "comes in if you open your service to anyone who comes up shows up on the Internet I said I'm Chris we're gonna",
    "start": "129450",
    "end": "137940"
  },
  {
    "text": "Splunk cloud so we use Prometheus internally for our Spallone cloud and just as we've spun up new services ran",
    "start": "137940",
    "end": "145500"
  },
  {
    "text": "into different carnality things stress the system in interesting ways and really just contribute back a fair",
    "start": "145500",
    "end": "151770"
  },
  {
    "text": "amount of things and it's a good time so now yeah so not too many thank you",
    "start": "151770",
    "end": "158959"
  },
  {
    "start": "156000",
    "end": "156000"
  },
  {
    "text": "yeah why we why are we talking about cardinality well first of all we could",
    "start": "158959",
    "end": "165989"
  },
  {
    "text": "have a show of hands who who thinks they might have had a cardinality disaster and oh okay",
    "start": "165989",
    "end": "172650"
  },
  {
    "text": "okay so you just came for like sympathy problem shared yeah so you may have seen",
    "start": "172650",
    "end": "181530"
  },
  {
    "text": "this message in the docs either before or after you had the disaster and this",
    "start": "181530",
    "end": "188610"
  },
  {
    "text": "is the Prometheus documentation and it",
    "start": "188610",
    "end": "194519"
  },
  {
    "text": "says it says don't do it right and so",
    "start": "194519",
    "end": "200910"
  },
  {
    "text": "that's that's Prometheus I mentioned on the slide maintainer of cortex which is",
    "start": "200910",
    "end": "206820"
  },
  {
    "text": "a scalable variant of Prometheus that's how we run our back-end the you know",
    "start": "206820",
    "end": "213450"
  },
  {
    "text": "when we talk to people at m3 and other other metric systems they're basically",
    "start": "213450",
    "end": "219420"
  },
  {
    "text": "everyone runs into this problem so it's it's pretty generic you do have to worry",
    "start": "219420",
    "end": "225810"
  },
  {
    "text": "about cardinality how much is too much in terms of Prometheus land there's a",
    "start": "225810",
    "end": "233760"
  },
  {
    "start": "229000",
    "end": "229000"
  },
  {
    "text": "couple of rules of thumb that we've developed one is try not to query more than a hundred thousand series at any",
    "start": "233760",
    "end": "240060"
  },
  {
    "text": "time you can write to more than that but trying to query it gets really slow by 200,000 series a single query is taking",
    "start": "240060",
    "end": "246600"
  },
  {
    "text": "five seconds eventually they just start timing out or get rejected by Prometheus",
    "start": "246600",
    "end": "251700"
  },
  {
    "text": "and UX kind of sucks there and then a single Prometheus it can handle about",
    "start": "251700",
    "end": "259150"
  },
  {
    "text": "10 million unique series before really the operational load becomes too much",
    "start": "259150",
    "end": "264370"
  },
  {
    "text": "that's a fair amount of serious oh that's a lot of data so you've got some",
    "start": "264370",
    "end": "270129"
  },
  {
    "text": "leeway most of the time but once that carnality starts kicking in like you'll",
    "start": "270129",
    "end": "276699"
  },
  {
    "text": "start seeing startup times taking 15 minutes 20 minutes if you lose an instance it's a long time I'll typically",
    "start": "276699",
    "end": "285159"
  },
  {
    "text": "see heap on the order of hundreds of gigabytes beefy boxes like it and then",
    "start": "285159",
    "end": "291719"
  },
  {
    "text": "you know a 2x increase anymore that's really bad when you're at a 200 gigabyte",
    "start": "291719",
    "end": "296889"
  },
  {
    "text": "heap or something like that and then where you normally see carnality issues",
    "start": "296889",
    "end": "304419"
  },
  {
    "text": "being manifested in prometheus is commonly its memory usage you'll see big spikes in memory usage or just more than",
    "start": "304419",
    "end": "312310"
  },
  {
    "text": "you would expect out of the box so that's sort of one of those things I'm sure you'll see it a couple times in our",
    "start": "312310",
    "end": "317680"
  },
  {
    "text": "slides but first off good pass it over to Brian for the first carnelli disaster",
    "start": "317680",
    "end": "323259"
  },
  {
    "text": "of our talk yeah so seen act one of the movie and this is a story about alerts",
    "start": "323259",
    "end": "331289"
  },
  {
    "start": "324000",
    "end": "324000"
  },
  {
    "text": "you know in in Prometheus you can set up alerting rules that I you know I want",
    "start": "331289",
    "end": "337089"
  },
  {
    "text": "you to send something to pager duty when this goes over this number or something",
    "start": "337089",
    "end": "343150"
  },
  {
    "text": "like that so we're monitoring our our service that",
    "start": "343150",
    "end": "351849"
  },
  {
    "text": "we run and this is kind of the the sign that we're in some kind of trouble",
    "start": "351849",
    "end": "358120"
  },
  {
    "text": "the so the bit of the bit of cortex that executes the rules it's same same code",
    "start": "358120",
    "end": "365440"
  },
  {
    "text": "as Prometheus but it's it's in a standalone subpart of the system called the ruler and this thing was growing and",
    "start": "365440",
    "end": "373000"
  },
  {
    "text": "growing and growing over hours and it doesn't it doesn't look good right the",
    "start": "373000",
    "end": "379589"
  },
  {
    "text": "the prognosis for this process is is terminal so",
    "start": "379589",
    "end": "388870"
  },
  {
    "text": "so we tried to dive in and you know what do you do if you if you've got access",
    "start": "388870",
    "end": "394889"
  },
  {
    "text": "memory utilization you maybe take a profile heap profile and try and figure",
    "start": "394889",
    "end": "401320"
  },
  {
    "text": "out where all the memory is going and that wasn't entirely useful it basically",
    "start": "401320",
    "end": "406990"
  },
  {
    "text": "said it was doing a lot of queries and couldn't really figure it out from there",
    "start": "406990",
    "end": "412330"
  },
  {
    "text": "but you know given a few hints we we did eventually narrow it down and figure out",
    "start": "412330",
    "end": "419430"
  },
  {
    "text": "what was happening so so one of our customers had I've changed specifics of",
    "start": "419430",
    "end": "427900"
  },
  {
    "text": "the naming to protect the the details but but let's say they they had a they",
    "start": "427900",
    "end": "433900"
  },
  {
    "text": "had a particular interest in a concept of an invalid tag and they set up a alerting rule if the if the if there",
    "start": "433900",
    "end": "441729"
  },
  {
    "text": "were any you know greater than zero invalid tag trouble was they they had",
    "start": "441729",
    "end": "449460"
  },
  {
    "text": "40,000 of those time series and one",
    "start": "449460",
    "end": "455050"
  },
  {
    "text": "alert like that will generate one",
    "start": "455050",
    "end": "460419"
  },
  {
    "text": "alerting rule like that will generate one alert for every time series that matches you'll get an entry in the",
    "start": "460419",
    "end": "466240"
  },
  {
    "text": "alerts table with all the same labels as as what you've queried to generate the alert and in fact not just one you get",
    "start": "466240",
    "end": "473560"
  },
  {
    "text": "to prometheus generates one time series which is the alert and another one which",
    "start": "473560",
    "end": "479950"
  },
  {
    "text": "is a kind of a state storage thing to do with the internals of Prometheus so so",
    "start": "479950",
    "end": "488410"
  },
  {
    "text": "this ended up the cause of our memory bloat that it was trying to write 80,000 samples to the store every 15 seconds",
    "start": "488410",
    "end": "497340"
  },
  {
    "text": "the underlying data was was kind of turning over and don't just they weren't",
    "start": "497340",
    "end": "502450"
  },
  {
    "text": "just stable series so these people generated 300,000 alerts in in a few",
    "start": "502450",
    "end": "508840"
  },
  {
    "text": "hours so that was that was kind of painful the good news is Wow",
    "start": "508840",
    "end": "515770"
  },
  {
    "text": "if you like Prometheus has a fixed sized Channel down which it deliver",
    "start": "515770",
    "end": "521789"
  },
  {
    "text": "the alerts on their way to pager duty that channel overflowed and in fact",
    "start": "521790",
    "end": "527220"
  },
  {
    "text": "dropped almost all of the 300,000 alerts so I would not have been you know",
    "start": "527220",
    "end": "532770"
  },
  {
    "text": "wanting to be the owner of that that phone that was receiving 300,000",
    "start": "532770",
    "end": "538680"
  },
  {
    "text": "notifications so so what can you do about it",
    "start": "538680",
    "end": "544610"
  },
  {
    "text": "yeah do be careful with the with the query in your alerts so one one way to",
    "start": "544610",
    "end": "551820"
  },
  {
    "text": "alert might might be just to to count how many of these things there are and",
    "start": "551820",
    "end": "557070"
  },
  {
    "text": "an alert if there's more than zero of them so then the the person who gets the",
    "start": "557070",
    "end": "562290"
  },
  {
    "text": "the page can go figure out what the detail is if you want some detail in",
    "start": "562290",
    "end": "567330"
  },
  {
    "text": "your alerts you might want to consider a call like top K which which will give you the the top in this case at the top",
    "start": "567330",
    "end": "572580"
  },
  {
    "text": "10 so you could generate 10 alerts if you know if if some of the detail is useful top a thousand if 4,000 of them",
    "start": "572580",
    "end": "581070"
  },
  {
    "text": "are useful but but please don't don't do 300,000 so that's real one let's move on",
    "start": "581070",
    "end": "590490"
  },
  {
    "text": "yeah so act 2 it's this time using",
    "start": "590490",
    "end": "595890"
  },
  {
    "text": "Prometheus monitoring a bunch of stuff it's all going well and then suddenly we",
    "start": "595890",
    "end": "601530"
  },
  {
    "text": "find there are entire error messages being put in as labeled values and",
    "start": "601530",
    "end": "607770"
  },
  {
    "text": "entrees error messages hundred kilobyte long error messages this is an example",
    "start": "607770",
    "end": "615840"
  },
  {
    "text": "not particularly useful it goes on for many more lines I just truncated it because that was all the tiny tiny text",
    "start": "615840",
    "end": "621870"
  },
  {
    "text": "I want it on this slide anyone have an idea of what happens to Prometheus when",
    "start": "621870",
    "end": "629010"
  },
  {
    "text": "you give it many unique hundred kilobyte live a lose yeah it uh all of it goes",
    "start": "629010",
    "end": "638010"
  },
  {
    "start": "636000",
    "end": "636000"
  },
  {
    "text": "down so we saw looking back on it a nice",
    "start": "638010",
    "end": "644820"
  },
  {
    "text": "climb in memory usage it was somewhat slow we over-provision these this particular instance has 200 gigabytes of",
    "start": "644820",
    "end": "652050"
  },
  {
    "text": "heap and start around a hundred and for the course of a couple days went to 200 gigabytes and maybe we should have",
    "start": "652050",
    "end": "658050"
  },
  {
    "text": "had an alert on that but you know oops and then everything um and memory wise",
    "start": "658050",
    "end": "666320"
  },
  {
    "text": "because it's memory you run H a pairs of Prometheus in this case we had three of them but it's memory they're all",
    "start": "666320",
    "end": "673710"
  },
  {
    "text": "scraping the exact same things which mean they have the same data memory usage is the same and they all at the",
    "start": "673710",
    "end": "680640"
  },
  {
    "text": "same time luckily we have a Deadman switch alert we stopped getting our alerts we're gonna what got our other",
    "start": "680640",
    "end": "687480"
  },
  {
    "text": "alert this is bad started debugging into it and this was hard to solve it was like this was our",
    "start": "687480",
    "end": "693480"
  },
  {
    "text": "only clue was like what what the heck there's a time memory usage and we found like okay going around the thing we",
    "start": "693480",
    "end": "700620"
  },
  {
    "text": "found was symbol table size it grew from 5 gigabytes to something on the order of",
    "start": "700620",
    "end": "706650"
  },
  {
    "text": "60 gigabytes before everything crashed what does that mean what is a symbol",
    "start": "706650",
    "end": "713190"
  },
  {
    "text": "table in Prometheus it's kind of weird",
    "start": "713190",
    "end": "718230"
  },
  {
    "text": "like you're like what is this metric I don't know so the symbol table is actually a very important thing when",
    "start": "718230",
    "end": "723900"
  },
  {
    "text": "dealing with carnality issues it is a table that in every single Prometheus block there is basically there's a list",
    "start": "723900",
    "end": "731040"
  },
  {
    "text": "of all the unique strings seen in all of the data in that block and the symbol",
    "start": "731040",
    "end": "736770"
  },
  {
    "text": "table size is the sum of all those unique strings over all of the blocks in your Prometheus system so effectively",
    "start": "736770",
    "end": "743520"
  },
  {
    "text": "its how many unique strings you have in your system very relevant for basically",
    "start": "743520",
    "end": "748560"
  },
  {
    "text": "for almost all carnality issues where there's lots of unique strings being generated so super useful learned about",
    "start": "748560",
    "end": "756390"
  },
  {
    "text": "it as part of this debugging did not know about it beforehand but great like",
    "start": "756390",
    "end": "763860"
  },
  {
    "text": "what do you do about that how do you find that that's a bit bit trickier and I've a reasonable example later on for",
    "start": "763860",
    "end": "770910"
  },
  {
    "start": "766000",
    "end": "766000"
  },
  {
    "text": "some tools we actually had to come in it some tools back into Prometheus just to figure out where that error label is",
    "start": "770910",
    "end": "777180"
  },
  {
    "text": "actually coming from but demo that at the end and but what to do about this one obvious don't put raw messages in",
    "start": "777180",
    "end": "783990"
  },
  {
    "text": "your label values it's a bad idea use an use some types and then if you need the",
    "start": "783990",
    "end": "790650"
  },
  {
    "text": "raw label message log it I don't know what you're doing with honor kilobyte error messages that I don't want that in",
    "start": "790650",
    "end": "796080"
  },
  {
    "text": "my logs either like that that just hurts a lot second thing you can do is it",
    "start": "796080",
    "end": "803610"
  },
  {
    "text": "might take a little bit you know change that so we had a I have to do in the",
    "start": "803610",
    "end": "809100"
  },
  {
    "text": "meantime is put in a real abling rule to just drop the metric once you can actually identify the metric put in a",
    "start": "809100",
    "end": "815970"
  },
  {
    "text": "relay Bolling rule drop the metric you can't just isolate out and drop the specific label because then you might",
    "start": "815970",
    "end": "822810"
  },
  {
    "text": "have multiple metrics collapse into one which is our multiple series collapse into one not a great situation so you",
    "start": "822810",
    "end": "829500"
  },
  {
    "text": "have to drop the whole the whole thing and it's our key thing here like those",
    "start": "829500",
    "end": "836310"
  },
  {
    "text": "messages were coming from sto sto is a very well used well tested program has bugs in it this can happen to any",
    "start": "836310",
    "end": "842790"
  },
  {
    "text": "program out there I would like to thank this do team we reported this issue and they fixed it within a week and a new",
    "start": "842790",
    "end": "849510"
  },
  {
    "text": "version up and running us was good so very responsive from them and finally",
    "start": "849510",
    "end": "855209"
  },
  {
    "text": "one other key thing about the symbol table is it doesn't go away until that",
    "start": "855209",
    "end": "862620"
  },
  {
    "text": "block is deleted or the data is deleted so that's 60 gigabyte that's sitting",
    "start": "862620",
    "end": "868320"
  },
  {
    "text": "there you either have to delete all that data or you have to wait for it to aged out and that's very painful so I",
    "start": "868320",
    "end": "875790"
  },
  {
    "text": "definitely recommend keeping an eye on that and not letting it to grow too big especially if you run a system like fan",
    "start": "875790",
    "end": "882570"
  },
  {
    "text": "O's where you have to lower load all historical data it gets really really",
    "start": "882570",
    "end": "887580"
  },
  {
    "text": "big yeah as ACTU yeah quite interesting",
    "start": "887580",
    "end": "892760"
  },
  {
    "text": "back to Brian my next next next we turn to buckets I'm I'm singer well a lot of",
    "start": "892760",
    "end": "903330"
  },
  {
    "start": "893000",
    "end": "893000"
  },
  {
    "text": "blank stares maybe a couple of nods so let me just explain",
    "start": "903330",
    "end": "908540"
  },
  {
    "text": "Prometheus histogram how that works histograms are really useful for seeing",
    "start": "908640",
    "end": "916339"
  },
  {
    "text": "something like the the 95th percentile of a trick you know compared to the the",
    "start": "916339",
    "end": "922420"
  },
  {
    "text": "median compared to the mean really powerful feature of Prometheus and and",
    "start": "922420",
    "end": "928899"
  },
  {
    "text": "the way it works is you set up when you define the metric in your code you set",
    "start": "928899",
    "end": "934420"
  },
  {
    "text": "up buckets and each one of them is a counter so Prometheus starts counting",
    "start": "934420",
    "end": "942029"
  },
  {
    "text": "the samples that fit in each one of the buckets and and from that from those",
    "start": "942029",
    "end": "947260"
  },
  {
    "text": "counters it can work out or at least give you an estimate for the the percentiles so in my first picture here",
    "start": "947260",
    "end": "956260"
  },
  {
    "text": "I have a bucket at everything up to 20",
    "start": "956260",
    "end": "961360"
  },
  {
    "text": "milliseconds well there actually weren't any of those in this example everything up to 40 milliseconds there was one of",
    "start": "961360",
    "end": "967449"
  },
  {
    "text": "those everything up to 80 milliseconds a bunch of dots in that bucket everything up to 160 milliseconds and everything",
    "start": "967449",
    "end": "974920"
  },
  {
    "text": "above that so that's five buckets so this is in my histogram definition that",
    "start": "974920",
    "end": "982779"
  },
  {
    "text": "I'm using for this example so if you work with a histogram like this what you",
    "start": "982779",
    "end": "990850"
  },
  {
    "text": "might find is is is when you do the queries the value the value get out of",
    "start": "990850",
    "end": "997480"
  },
  {
    "text": "it we'll just kind of jump around between the 40 millisecond and the 80",
    "start": "997480",
    "end": "1002940"
  },
  {
    "text": "millisecond point because almost all the data is in that bucket so you might you",
    "start": "1002940",
    "end": "1008339"
  },
  {
    "text": "might find there's no definition in that range so no problem add a bucket in this",
    "start": "1008339",
    "end": "1017040"
  },
  {
    "text": "case I've I've added one at 60 milliseconds now we get that extra extra definition in that range where there was",
    "start": "1017040",
    "end": "1024270"
  },
  {
    "text": "a lot of density now them there might be another reason to use a the this kind of",
    "start": "1024270",
    "end": "1031290"
  },
  {
    "text": "data collection which is to check your service level agreements or the the",
    "start": "1031290",
    "end": "1037918"
  },
  {
    "text": "thing you would check is the s Li and you would compare it against your internal SLO and you might have to pay",
    "start": "1037919",
    "end": "1044220"
  },
  {
    "text": "out on your SLA everyone's read the book right so so let's imagine our SL",
    "start": "1044220",
    "end": "1051590"
  },
  {
    "text": "Oh is 100 milliseconds well we can't answer the question with those buckets",
    "start": "1051590",
    "end": "1058220"
  },
  {
    "text": "so so better put one in put it in a bucket at 100 milliseconds maybe you",
    "start": "1058220",
    "end": "1066200"
  },
  {
    "text": "find you don't have much definition at the top end maybe you know you want to kind of know whether whether those ones",
    "start": "1066200",
    "end": "1071780"
  },
  {
    "text": "that are above 160 milliseconds are like a little bit above or a lot above and so you add more buckets there and you",
    "start": "1071780",
    "end": "1077960"
  },
  {
    "text": "buckets bucketz buckets and before before long you're you're pretty much",
    "start": "1077960",
    "end": "1086120"
  },
  {
    "text": "swimming in buckets so the the other",
    "start": "1086120",
    "end": "1092810"
  },
  {
    "text": "thing that you need to watch out for is the multiplicative effect so you might",
    "start": "1092810",
    "end": "1099230"
  },
  {
    "text": "have let's say 10 buckets you know that's a very modest number but then you",
    "start": "1099230",
    "end": "1106310"
  },
  {
    "text": "you put some labels on it you you measuring all of those buckets per entry",
    "start": "1106310",
    "end": "1111620"
  },
  {
    "text": "point into your system and you're actually measuring them on all the different nodes or maybe all the",
    "start": "1111620",
    "end": "1117410"
  },
  {
    "text": "different pods in the system and so let's say you had 10 buckets and 20",
    "start": "1117410",
    "end": "1124730"
  },
  {
    "text": "entry points and 100 pods and maybe now we want to separate out the error codes",
    "start": "1124730",
    "end": "1131000"
  },
  {
    "text": "that can come back and there's like seven different error codes they're all those numbers multiply up and and before",
    "start": "1131000",
    "end": "1138590"
  },
  {
    "text": "long Prometheus explodes because you know you've given it 10 million series",
    "start": "1138590",
    "end": "1147670"
  },
  {
    "text": "so the disaster the real-life disaster I just want to highlight is this little",
    "start": "1148420",
    "end": "1154330"
  },
  {
    "start": "1153000",
    "end": "1153000"
  },
  {
    "text": "snippet here this is real code from",
    "start": "1154330",
    "end": "1159650"
  },
  {
    "text": "cubed DNS where they define linear buckets is a is a Prometheus helper",
    "start": "1159650",
    "end": "1166070"
  },
  {
    "text": "function and it has a start and an increment and account and so they",
    "start": "1166070",
    "end": "1173380"
  },
  {
    "text": "excellent resolution the you know they had a bucket at 10 milliseconds 20",
    "start": "1173380",
    "end": "1179810"
  },
  {
    "text": "milliseconds 30 milliseconds 40 milliseconds five hundred buckets they gave you never",
    "start": "1179810",
    "end": "1190169"
  },
  {
    "text": "mind somebody fixed it I happened to be able",
    "start": "1190169",
    "end": "1196440"
  },
  {
    "start": "1191000",
    "end": "1191000"
  },
  {
    "text": "to find that PR and I switched it out to",
    "start": "1196440",
    "end": "1203629"
  },
  {
    "text": "exponential buckets which you know goes up by multiple which probably makes more",
    "start": "1203629",
    "end": "1209789"
  },
  {
    "text": "sense a lot of the time you probably you probably want the fine detail at the low end and then you know if your DNS",
    "start": "1209789",
    "end": "1217529"
  },
  {
    "text": "request takes 10 seconds and you probably don't care whether it was 10 or 12 or you know it you've lost hope at",
    "start": "1217529",
    "end": "1226529"
  },
  {
    "text": "that point okay so there you go that's",
    "start": "1226529",
    "end": "1233249"
  },
  {
    "text": "like three back to Chris yeah so Act four this is the final disaster of our",
    "start": "1233249",
    "end": "1240719"
  },
  {
    "start": "1236000",
    "end": "1236000"
  },
  {
    "text": "presentation so imagine your team our account service instrumenting everything",
    "start": "1240719",
    "end": "1245879"
  },
  {
    "text": "in prometheus it's going great so if there's a service it's like libraries they're using they sort of support",
    "start": "1245879",
    "end": "1252809"
  },
  {
    "text": "Prometheus but not really they're more designed for this old-style dot notation",
    "start": "1252809",
    "end": "1259199"
  },
  {
    "text": "well okay but I need some tags in there so put them in the names sure like",
    "start": "1259199",
    "end": "1264899"
  },
  {
    "text": "they're there but at least you can do stuff so you get metrics that look like my service blah some routes some tenant",
    "start": "1264899",
    "end": "1273199"
  },
  {
    "text": "status codes etc and then you end up doing queries like oh I need to query on",
    "start": "1273199",
    "end": "1279419"
  },
  {
    "text": "across these dimensions oh no now I'm doing regex matches on the metric names",
    "start": "1279419",
    "end": "1286589"
  },
  {
    "text": "and it's very slow please not actually do this so what happens when you have",
    "start": "1286589",
    "end": "1293959"
  },
  {
    "text": "hundreds of thousands of series with different metric names each individual",
    "start": "1293959",
    "end": "1302519"
  },
  {
    "text": "metric name doesn't have too much carnality but overall cardinality of your system is still the same like all",
    "start": "1302519",
    "end": "1307859"
  },
  {
    "text": "those unique global values are still there and what now happens is",
    "start": "1307859",
    "end": "1313679"
  },
  {
    "text": "the Prometheus UI breaks completely there's been some improvements it might",
    "start": "1313679",
    "end": "1320440"
  },
  {
    "start": "1314000",
    "end": "1314000"
  },
  {
    "text": "not break completely anymore but for a while it like my oldest browser crashed on me every time I tried to load from",
    "start": "1320440",
    "end": "1327010"
  },
  {
    "text": "atheists and the reason for this is prometheus and the ger fauna you is if you're trying to like put in a new query",
    "start": "1327010",
    "end": "1333480"
  },
  {
    "text": "into the Griffons UI they pull back every single metric name for the type of",
    "start": "1333480",
    "end": "1340059"
  },
  {
    "text": "head completion it's a nice tool I don't know all the names of every metric in my system like them that's way too much",
    "start": "1340059",
    "end": "1345520"
  },
  {
    "text": "cognitive load type it in start getting there great like that's a really nice feature but some systems have a two",
    "start": "1345520",
    "end": "1354040"
  },
  {
    "start": "1353000",
    "end": "1353000"
  },
  {
    "text": "million metric names and what's the number on here I think it is one meg",
    "start": "1354040",
    "end": "1361900"
  },
  {
    "text": "some 121 megabytes of data that you have to pull back to just to get your metric",
    "start": "1361900",
    "end": "1368110"
  },
  {
    "text": "names back from the server there's a lot of it's a lot of data to pull back to load a simple little UI it's a lot of",
    "start": "1368110",
    "end": "1374860"
  },
  {
    "text": "JSON yes a lot of JSON so clearly in good news we're not the only ones with",
    "start": "1374860",
    "end": "1380950"
  },
  {
    "text": "who have come across this issue and there's actually I think I've seen three",
    "start": "1380950",
    "end": "1387850"
  },
  {
    "text": "currently open prometheus issues around this behavior and what to do about it hopefully some of those will get worked",
    "start": "1387850",
    "end": "1394990"
  },
  {
    "text": "on before too long yeah so do not put label values into your metric names not",
    "start": "1394990",
    "end": "1402490"
  },
  {
    "text": "a good idea lots of different in efficiencies um this can also happen from systems that",
    "start": "1402490",
    "end": "1409120"
  },
  {
    "text": "are misconfigure you might have a stats d exporter or a jmx exporter you just grab a regex and put it into the name",
    "start": "1409120",
    "end": "1415270"
  },
  {
    "text": "and oops now i have weird data in prometheus this is another case of label",
    "start": "1415270",
    "end": "1423370"
  },
  {
    "text": "names are forever that you pulled it back from all the blocks in your database so if you do this once you can",
    "start": "1423370",
    "end": "1429250"
  },
  {
    "text": "have to wait for your attention to age out for them to disappear from type-ahead or you have to delete the",
    "start": "1429250",
    "end": "1434650"
  },
  {
    "text": "entire series with the admin API but there are ways around this hopefully you",
    "start": "1434650",
    "end": "1440230"
  },
  {
    "text": "can fix that the source but if not you can write some great relabeling rules",
    "start": "1440230",
    "end": "1445630"
  },
  {
    "text": "too parse out all the label values and then put them into labels themselves and then",
    "start": "1445630",
    "end": "1451420"
  },
  {
    "text": "change the name to something more sane I think we have probably 400 lines of",
    "start": "1451420",
    "end": "1458230"
  },
  {
    "text": "relabeling configs to do this at one point it's a good time so yeah stop back",
    "start": "1458230",
    "end": "1467320"
  },
  {
    "start": "1466000",
    "end": "1466000"
  },
  {
    "text": "to Brian for yeah how to travel a bit of fun talking about the the good times in",
    "start": "1467320",
    "end": "1475870"
  },
  {
    "text": "the in the trenches and the sre trenches the so yeah how what we wanted to do is",
    "start": "1475870",
    "end": "1485950"
  },
  {
    "text": "leave you with some tips and ideas for what to practicality practically do in",
    "start": "1485950",
    "end": "1495070"
  },
  {
    "text": "the in the system oh yeah so that's that's that's that's my message it's",
    "start": "1495070",
    "end": "1501160"
  },
  {
    "text": "unpleasant and what are the techniques I",
    "start": "1501160",
    "end": "1507400"
  },
  {
    "text": "have to say oh I came into this with one of my favorite techniques is this query",
    "start": "1507400",
    "end": "1514330"
  },
  {
    "text": "which I I can I can type by heart I don't know nobody else recognizes this",
    "start": "1514330",
    "end": "1521350"
  },
  {
    "text": "okay this this is so so working from the inside out we we have a query with no",
    "start": "1521350",
    "end": "1528310"
  },
  {
    "text": "metric name so it's just curly brackets nothing nothing before that and then",
    "start": "1528310",
    "end": "1533410"
  },
  {
    "text": "what's inside the curly brackets is we're going to match on name and we're gonna match it with a regex which is dot",
    "start": "1533410",
    "end": "1541570"
  },
  {
    "text": "plus for reasons and then and then we count those by name and that way we",
    "start": "1541570",
    "end": "1549220"
  },
  {
    "text": "could we could find out what the highest cardinality is but in big clusters that",
    "start": "1549220",
    "end": "1557710"
  },
  {
    "text": "stops working if I try to do that query on our Prometheus instances it just sits",
    "start": "1557710",
    "end": "1563170"
  },
  {
    "text": "there and spins and spins and if I'm lucky it returns in 58 seconds when our",
    "start": "1563170",
    "end": "1568300"
  },
  {
    "text": "timeout is one minute and if I'm not lucky you're on this not my smaller",
    "start": "1568300",
    "end": "1574090"
  },
  {
    "text": "instances that sit never gonna happen so what do you do instead on these bigger instances well",
    "start": "1574090",
    "end": "1580640"
  },
  {
    "text": "a few questions I like asking myself first how many series are in memory gives me a good snapshot there's a",
    "start": "1580640",
    "end": "1588440"
  },
  {
    "text": "metric Prometheus exports for that Prometheus T used to be head series nice",
    "start": "1588440",
    "end": "1593930"
  },
  {
    "text": "and helpful typical that's where I'll typically make sure I try and keep under that 10",
    "start": "1593930",
    "end": "1599270"
  },
  {
    "text": "million mark that goes much above 10 million probably in for a bad time before too long and ii talked about a",
    "start": "1599270",
    "end": "1607190"
  },
  {
    "text": "bit earlier how much space do my unique strings take as I said TS DB symbol",
    "start": "1607190",
    "end": "1613010"
  },
  {
    "text": "table size very helpful bit mysteriously named but now you know what it does",
    "start": "1613010",
    "end": "1619390"
  },
  {
    "text": "third I really like looking at how many samples are scraped by job there's a",
    "start": "1619390",
    "end": "1626150"
  },
  {
    "text": "internal metric called scrape sample scraped at every target tells you how",
    "start": "1626150",
    "end": "1631400"
  },
  {
    "text": "many and then you can summit by job you can also do there's also a scraped set",
    "start": "1631400",
    "end": "1638320"
  },
  {
    "text": "scrape series added that you can do a sum over time to see churn and these",
    "start": "1638320",
    "end": "1643700"
  },
  {
    "text": "really instead of doing full names and the name matches this really looks into well now I actually have a name so it's",
    "start": "1643700",
    "end": "1650240"
  },
  {
    "text": "a much smaller data set and this will return Italy just it points you towards the job and now you could plug in if you",
    "start": "1650240",
    "end": "1657650"
  },
  {
    "text": "have a couple jobs you're curious about you can plug those into Brian's query and that'll probably return and finally",
    "start": "1657650",
    "end": "1664640"
  },
  {
    "text": "as a version two point fourteen there's just R which is just out very new we can",
    "start": "1664640",
    "end": "1670970"
  },
  {
    "text": "have a good week ago is when I released it as a non release candidate there's a",
    "start": "1670970",
    "end": "1677630"
  },
  {
    "text": "TS DB status page built into the UI that will quickly tell you many many different metrics so now let's take an",
    "start": "1677630",
    "end": "1685310"
  },
  {
    "text": "example of well you don't always have to have carnality disasters to learn more",
    "start": "1685310",
    "end": "1690950"
  },
  {
    "text": "about the carnality of your system so last week you know I'm prepping for a cube top con talk it's like well I gotta",
    "start": "1690950",
    "end": "1697940"
  },
  {
    "text": "have something so like looked at what the carnality of my system is and was like whoa that simple table side is",
    "start": "1697940",
    "end": "1703730"
  },
  {
    "text": "still like gig in this environment it goes for gigs in a different environment where the hex",
    "start": "1703730",
    "end": "1709070"
  },
  {
    "text": "coming from well now prometheus UI comes with this nice page use click status",
    "start": "1709070",
    "end": "1715399"
  },
  {
    "start": "1711000",
    "end": "1711000"
  },
  {
    "text": "runtime information and there's a bunch of sections one of those sections is label names with highest cumulative",
    "start": "1715399",
    "end": "1721970"
  },
  {
    "text": "label value length so this catchy name catchin a new what's telling you is what",
    "start": "1721970",
    "end": "1728690"
  },
  {
    "text": "is the label that has the most values this is what we had to add to at that time as a TST be analyzed command line",
    "start": "1728690",
    "end": "1735559"
  },
  {
    "text": "tool to figure out where those error messages were coming from like what what label has all of this symbol sighs well",
    "start": "1735559",
    "end": "1743440"
  },
  {
    "text": "last week we found there is one metric or one label log file with two orders of",
    "start": "1743440",
    "end": "1750230"
  },
  {
    "text": "magnitude more data than anything else in our cluster as you can see one of those other things is the container ID",
    "start": "1750230",
    "end": "1756470"
  },
  {
    "text": "just literally just a sha so that's a",
    "start": "1756470",
    "end": "1762200"
  },
  {
    "text": "lot of data two orders of magnitude more so great okay that gives me a great place to start now instead of an empty",
    "start": "1762200",
    "end": "1770080"
  },
  {
    "start": "1768000",
    "end": "1768000"
  },
  {
    "text": "name regex match I can do a count of where log file does isn't empty so tell",
    "start": "1770080",
    "end": "1777320"
  },
  {
    "text": "me what all series come back that have this log file there are two of them they're both from M tail for those of",
    "start": "1777320",
    "end": "1784730"
  },
  {
    "text": "you who don't know M tail is a program that will monitor log files and you can",
    "start": "1784730",
    "end": "1790909"
  },
  {
    "text": "do some regex matching on the log entries and they'll convert them into metrics that you can then scrape so for",
    "start": "1790909",
    "end": "1796549"
  },
  {
    "text": "things that maybe don't expose error message errors as Prometheus metrics you can use em tail to grep for these error",
    "start": "1796549",
    "end": "1805509"
  },
  {
    "text": "messages whatever you really want just just regex matching and export them as metrics super useful for some cases that",
    "start": "1805509",
    "end": "1814730"
  },
  {
    "text": "said it's looking at fifty thousand different log files seems like a lot",
    "start": "1814730",
    "end": "1823389"
  },
  {
    "text": "entail in our system works on three workloads we're scraping 50 or we're",
    "start": "1823389",
    "end": "1830029"
  },
  {
    "text": "following fifty thousand files for three workloads of container logs I don't know",
    "start": "1830029",
    "end": "1836299"
  },
  {
    "text": "if you've seen container log pads and kubernetes goes something like var log containers",
    "start": "1836299",
    "end": "1841620"
  },
  {
    "text": "pod name container name no name space container names container you--it dot",
    "start": "1841620",
    "end": "1848460"
  },
  {
    "text": "log it's pretty pretty long that that could definitely explain why log file is",
    "start": "1848460",
    "end": "1854280"
  },
  {
    "text": "taking up so much space so made em tail all makes great blog files that be",
    "start": "1854280",
    "end": "1860100"
  },
  {
    "text": "actually cared about reduce this to seventy nine instead of fifty thousand",
    "start": "1860100",
    "end": "1865220"
  },
  {
    "text": "pretty good and this is the result you can see on this graph reduce the number",
    "start": "1865220",
    "end": "1873300"
  },
  {
    "text": "of containers we are climbing climbing climbing and then symbol table size starts decreasing as I said you have to",
    "start": "1873300",
    "end": "1881070"
  },
  {
    "text": "wait and wait until blocks actually age out so it takes a few days but like now",
    "start": "1881070",
    "end": "1886440"
  },
  {
    "text": "we are we have in this cluster I believe is fifty percent or so or I guess it's a",
    "start": "1886440",
    "end": "1892170"
  },
  {
    "text": "30% decrease so far and one of our other clusters the symbol table just over the",
    "start": "1892170",
    "end": "1897300"
  },
  {
    "text": "last four days has decreased by almost 50 percent that's a big difference just for something that was misconfigured and",
    "start": "1897300",
    "end": "1905040"
  },
  {
    "text": "these carnality metrics pointed towards this misconfigured thing so what I'd",
    "start": "1905040",
    "end": "1910650"
  },
  {
    "start": "1910000",
    "end": "1910000"
  },
  {
    "text": "like to leave you with is what carnality can you find in your systems try out",
    "start": "1910650",
    "end": "1917370"
  },
  {
    "text": "these tools see what see what they look like see what you can find and when a Karen ality is disaster strikes you",
    "start": "1917370",
    "end": "1923250"
  },
  {
    "text": "you'll be better prepared to find the culprit that's all thank you very very",
    "start": "1923250",
    "end": "1929820"
  },
  {
    "text": "much what questions you have",
    "start": "1929820",
    "end": "1933769"
  },
  {
    "text": "we raise your hand and we can get the microphone to this one over there on by",
    "start": "1936960",
    "end": "1943210"
  },
  {
    "text": "were waiting for the microphone to arrive let me remind you to fill in the feedback on this sched website and go so",
    "start": "1943210",
    "end": "1952030"
  },
  {
    "text": "how do you deal with one of those cardinality disasters that bleeds into something like a long-term storage like",
    "start": "1952030",
    "end": "1957760"
  },
  {
    "text": "cortex or that house well yeah so one of",
    "start": "1957760",
    "end": "1966040"
  },
  {
    "text": "the things about cortex is because we have all this history of random people sending us random data on the Internet",
    "start": "1966040",
    "end": "1972250"
  },
  {
    "text": "we actually have a whole suite of soft limits which are actually configurable",
    "start": "1972250",
    "end": "1978640"
  },
  {
    "text": "power tenant so you know what you'll find if you're if probably someone the",
    "start": "1978640",
    "end": "1984970"
  },
  {
    "text": "rooms tried this already if you know if you sign up for a free trial and start sending us a million series you will you just will send you errors so that's the",
    "start": "1984970",
    "end": "1993430"
  },
  {
    "text": "cortex answer we have like limits everywhere the Sun oh sensor delay",
    "start": "1993430",
    "end": "2001620"
  },
  {
    "text": "delete your data yeah that's the only that's all they think I know how to do sad face sad look the deep dive of",
    "start": "2001620",
    "end": "2008790"
  },
  {
    "text": "tano's is going on right now so I don't think there's gonna be any super famous experts in here but okay yeah good",
    "start": "2008790",
    "end": "2015810"
  },
  {
    "text": "question do we have another yeah yeah great you talked about th DB command line to them what tool is that so",
    "start": "2015810",
    "end": "2021900"
  },
  {
    "text": "there's a TST be analyzed tool that is it is included with the in the",
    "start": "2021900",
    "end": "2027750"
  },
  {
    "text": "Prometheus repo I don't think it is currently in the Prometheus docker file though but if you pull down the tar file",
    "start": "2027750",
    "end": "2035400"
  },
  {
    "text": "it will be in there it is soon to be integrated into prom tool as well so",
    "start": "2035400",
    "end": "2040620"
  },
  {
    "text": "within the next couple release cycles we'll just be in prom tool and then you'll actually have it in your container that you're running as well",
    "start": "2040620",
    "end": "2047150"
  },
  {
    "text": "yep beyond code reviews is there any way to like enforce or test or policies to",
    "start": "2047150",
    "end": "2053159"
  },
  {
    "text": "write to catch this before it makes it in that's a great question yeah I mean you know ideally in the",
    "start": "2053160",
    "end": "2060830"
  },
  {
    "text": "integration test of your CI you would spin up a realistic instance",
    "start": "2060830",
    "end": "2069450"
  },
  {
    "text": "and then hit the slash metrics endpoint and and do some analysis on it and sup",
    "start": "2069450",
    "end": "2075629"
  },
  {
    "text": "who does that but yeah I could do that",
    "start": "2075630",
    "end": "2083360"
  },
  {
    "text": "there is discussion of adding some more limits into Prometheus itself such as",
    "start": "2083360",
    "end": "2088850"
  },
  {
    "text": "don't allow hundred thousand kilobyte error message it label values so that is",
    "start": "2088850",
    "end": "2096110"
  },
  {
    "text": "something we are discussing at least and hopefully we'll make it into core Prometheus at some point just some",
    "start": "2096110",
    "end": "2101340"
  },
  {
    "text": "protections around this there are also some things around there's a scrape",
    "start": "2101340",
    "end": "2106440"
  },
  {
    "text": "limit you can put on to scrape jobs in Prometheus to limit no target can have",
    "start": "2106440",
    "end": "2112860"
  },
  {
    "text": "in this job can have more than 500 series or whatever your limit and that",
    "start": "2112860",
    "end": "2117990"
  },
  {
    "text": "can help out with this a little bit but there's still like if you're running 500 instances that's still a ton not like so",
    "start": "2117990",
    "end": "2125040"
  },
  {
    "text": "it doesn't fix everything any quick question about the the name name query",
    "start": "2125040",
    "end": "2130110"
  },
  {
    "text": "with the dot plus I mean there are blank named metrics that we shouldn't care",
    "start": "2130110",
    "end": "2137280"
  },
  {
    "text": "about so there are what I need answered first of all I said it's dot plus for a",
    "start": "2137280",
    "end": "2144210"
  },
  {
    "text": "reason and the reason is if if that reg reg X matches the empty string then it just",
    "start": "2144210",
    "end": "2150690"
  },
  {
    "text": "doesn't work at all and it sounds like",
    "start": "2150690",
    "end": "2156120"
  },
  {
    "text": "maybe it wouldn't work for you exactly the way I wrote it and sorry yeah you",
    "start": "2156120",
    "end": "2164160"
  },
  {
    "text": "can manage to get emptied name the metrics into Prometheus okay I might be",
    "start": "2164160",
    "end": "2177210"
  },
  {
    "text": "very impressive you could get an empty metric name in to prevent label names",
    "start": "2177210",
    "end": "2183030"
  },
  {
    "text": "have been rejected by Prometheus since 2.0 yeah we used to get them prior to",
    "start": "2183030",
    "end": "2189480"
  },
  {
    "text": "2.0 yeah so is it like the TST be like",
    "start": "2189480",
    "end": "2195960"
  },
  {
    "text": "series count metric that you can use to track like you how many series you have previous put put an alert on that",
    "start": "2195960",
    "end": "2202520"
  },
  {
    "text": "protect you from some of these like I know it probably protect you from like 100 kilobyte label size yeah",
    "start": "2202520",
    "end": "2210330"
  },
  {
    "text": "but or would it I don't yeah that counts a separate series when you have yes you",
    "start": "2210330",
    "end": "2216900"
  },
  {
    "text": "can set alerts on these the ones that are Prometheus metrics yeah by all means experiment with what's a good alert",
    "start": "2216900",
    "end": "2223770"
  },
  {
    "text": "level for you okay yep you can definitely do that any more one more",
    "start": "2223770",
    "end": "2234330"
  },
  {
    "text": "what are we doing for time we have over time so we asked question we don't have",
    "start": "2234330",
    "end": "2240780"
  },
  {
    "text": "time ago on one last one how are you clean the symbol table sorry so again",
    "start": "2240780",
    "end": "2247290"
  },
  {
    "text": "okay how do you clean the symbol table clean it yeah so the so there's two",
    "start": "2247290",
    "end": "2253320"
  },
  {
    "text": "options for cleaning the symbol table one is delete a block not great the",
    "start": "2253320",
    "end": "2259980"
  },
  {
    "text": "second is you can figure out which series are causing the large symbol table and there's a TS DB admin API that",
    "start": "2259980",
    "end": "2266640"
  },
  {
    "text": "allows you to delete based on mashers so you will have to enable that API",
    "start": "2266640",
    "end": "2272370"
  },
  {
    "text": "go in very carefully only delete the certain ones from the specific time",
    "start": "2272370",
    "end": "2278130"
  },
  {
    "text": "range that you care about and then you have to do a second API call to say do a",
    "start": "2278130",
    "end": "2284310"
  },
  {
    "text": "compaction now and thick and then the compaction process will work on that",
    "start": "2284310",
    "end": "2289650"
  },
  {
    "text": "data and remove it permanently from your database at which point this it the symbol table",
    "start": "2289650",
    "end": "2295190"
  },
  {
    "text": "is for those strings is in trees in trees for those strings are no longer necessary and you will be in a better",
    "start": "2295190",
    "end": "2302180"
  },
  {
    "text": "spot that's the safer way to do it be careful it's an admin API for a reason all right",
    "start": "2302180",
    "end": "2309800"
  },
  {
    "text": "well thank you all for coming have a good evening [Applause]",
    "start": "2309800",
    "end": "2314559"
  }
]