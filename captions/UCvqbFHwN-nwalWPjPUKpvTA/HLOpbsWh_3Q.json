[
  {
    "text": "okay so hello everyone uh welcome to another session of our cncf uh research",
    "start": "2520",
    "end": "9440"
  },
  {
    "text": "and user group um today we have um exciting topic with a running uh very",
    "start": "9440",
    "end": "17880"
  },
  {
    "text": "popular uh tool for HBC and B scheduling slurm on in cornetes the project Slinky",
    "start": "17880",
    "end": "26160"
  },
  {
    "text": "so thanks a lot for to Marlo Skyler Allan to prepare this presentation uh there's not really any",
    "start": "26160",
    "end": "34079"
  },
  {
    "text": "other uh housekeeping to do so I I would suggest we just jump into it so Mario",
    "start": "34079",
    "end": "39800"
  },
  {
    "text": "thanks again for for presenting and go",
    "start": "39800",
    "end": "45200"
  },
  {
    "text": "ahead thank you um and thanks for inviting us to speak Ricardo we really",
    "start": "47640",
    "end": "53440"
  },
  {
    "text": "appreciate it um so to start we're talking about uh slman kubernetes so",
    "start": "53440",
    "end": "58960"
  },
  {
    "text": "we'll explain what that means um",
    "start": "58960",
    "end": "63920"
  },
  {
    "text": "Maro I think we lost your audio a couple of seconds ago I don't know if you muted that",
    "start": "93119",
    "end": "100439"
  },
  {
    "text": "accidentally yeah apparently a button did that um so we we lost the last 20",
    "start": "100439",
    "end": "106719"
  },
  {
    "text": "seconds or 30 seconds yeah okay so we're from a company called skmd",
    "start": "106719",
    "end": "114880"
  },
  {
    "text": "um and we are the maintainers and supporters of slurm we're the only",
    "start": "114880",
    "end": "120079"
  },
  {
    "text": "organization providing level three support we also have training consultation custom development but",
    "start": "120079",
    "end": "125159"
  },
  {
    "text": "everything we're doing is open source um most people know slurm maybe",
    "start": "125159",
    "end": "133400"
  },
  {
    "text": "not here but in HPC they do um it's the it's basically the big scheduler for",
    "start": "133400",
    "end": "139440"
  },
  {
    "text": "your HPC workloads it allocates access resources to users for some duration of time it's a good framework for starting",
    "start": "139440",
    "end": "146640"
  },
  {
    "text": "executing and monitoring the work on the allocated nodes and it arbitrates contention so it helps",
    "start": "146640",
    "end": "153480"
  },
  {
    "text": "with fair use type cases um where policy driven open source fall tolerant you can",
    "start": "153480",
    "end": "159720"
  },
  {
    "text": "read this slide um scalable um and by scalable I'm talking tens of thousands of nodes some people",
    "start": "159720",
    "end": "166519"
  },
  {
    "text": "have pushed it further uh I I know the details of those",
    "start": "166519",
    "end": "171920"
  },
  {
    "text": "particular cases so what is Slinky we are a collection of projects initiatives to",
    "start": "171920",
    "end": "178159"
  },
  {
    "text": "enable slur on kubernetes um the one we're demoing today is very specifically just slurm in kubernetes but we're still",
    "start": "178159",
    "end": "184720"
  },
  {
    "text": "working on the scheduling component so we have the slurm operator we have a kubernetes deployment which has Helm",
    "start": "184720",
    "end": "190760"
  },
  {
    "text": "chart and container images um we will have underlying resource drivers that",
    "start": "190760",
    "end": "196519"
  },
  {
    "text": "we'll have to add in um because kubernetes native doesn't have all the capabilities we would like to",
    "start": "196519",
    "end": "202879"
  },
  {
    "text": "have and then we're working currently on a resource scheduler plugin as well",
    "start": "202879",
    "end": "211400"
  },
  {
    "text": "so I have this slide in here because there's a lot of discussions every time I give presentations like this um",
    "start": "211720",
    "end": "218720"
  },
  {
    "text": "because the assumptions in the HPC world are very different than those in the cloud native world and these aren't hard",
    "start": "218720",
    "end": "226000"
  },
  {
    "text": "and fast rules but these are generalized uh generalized notes about these sorts",
    "start": "226000",
    "end": "233599"
  },
  {
    "text": "of systems and more importantly who your users are so when you're talking with HPC see",
    "start": "233599",
    "end": "240200"
  },
  {
    "text": "your underlying software is mutable the the users are assuming find green control of those nodes they often syst",
    "start": "240200",
    "end": "247799"
  },
  {
    "text": "some experts that understand infrastructure and have a tolerance for complexity",
    "start": "247799",
    "end": "252840"
  },
  {
    "text": "um they have the access to compute is handled by resource manager a scheduling system not anything else um there's",
    "start": "252840",
    "end": "260880"
  },
  {
    "text": "usually a login node users own the node entirely during that computation there's",
    "start": "260880",
    "end": "266199"
  },
  {
    "text": "not as much multi-tenancy that's not entirely true with slurm but it is often",
    "start": "266199",
    "end": "271360"
  },
  {
    "text": "an assumption for your compute and there's an assumption of node homogeneity um which is very very",
    "start": "271360",
    "end": "278639"
  },
  {
    "text": "different from the cloud environments in general where your underlying software is immutable you can't change the",
    "start": "278639",
    "end": "284560"
  },
  {
    "text": "drivers on your note um the users are not system experts they don't think in terms of parallel they have limited",
    "start": "284560",
    "end": "290600"
  },
  {
    "text": "tolerance to complexity um the summary that I can often use is they just want",
    "start": "290600",
    "end": "296759"
  },
  {
    "text": "to run their Jupiter notebooks um the users sh their nodes which introduces Jitter can flow through your bandwidth",
    "start": "296759",
    "end": "303600"
  },
  {
    "text": "and there's an assumption of heterogeneous nodes you don't know what you're running on underneath necessarily there's also not a ton of attention",
    "start": "303600",
    "end": "310280"
  },
  {
    "text": "given to network topology which um matters a lot with your training",
    "start": "310280",
    "end": "315880"
  },
  {
    "text": "jobs any questions on that",
    "start": "315880",
    "end": "319800"
  },
  {
    "text": "slide okay I'll go ahead forward um PL schedule features that are",
    "start": "324479",
    "end": "331319"
  },
  {
    "text": "not necessarily native in kubernetes you advanced in timed reservations of resources for particular job or group",
    "start": "331319",
    "end": "339080"
  },
  {
    "text": "there's priority scheduling as well which determines job execution order based on priorities and weights a ages in the only one and fair",
    "start": "339080",
    "end": "347880"
  },
  {
    "text": "share where the resources are distributed equally about amount users based on your usage historical there's",
    "start": "347880",
    "end": "355600"
  },
  {
    "text": "also quality of service which is different from the kubernetes definition which which is set of policies such as",
    "start": "355600",
    "end": "360960"
  },
  {
    "text": "limits of resources priorities preemption backfilling the other set of things that",
    "start": "360960",
    "end": "367680"
  },
  {
    "text": "that we are trying to account transfer is job accounting so information for job and job step executed that is critical",
    "start": "367680",
    "end": "375440"
  },
  {
    "text": "the job step job dependencies which allows users to specify relationships between jobs from start succeed fail or",
    "start": "375440",
    "end": "382560"
  },
  {
    "text": "a particular state so you're not just waiting for worked or didn't work you can look at a",
    "start": "382560",
    "end": "388599"
  },
  {
    "text": "particular state of job there's also more granular job allocation CPU memory GPU I know we have",
    "start": "388599",
    "end": "396400"
  },
  {
    "text": "some in kubernetes but it is very core scened um Network topology we have",
    "start": "396400",
    "end": "402680"
  },
  {
    "text": "properties taken into account such as node proximity bandwidth and latency these are things that we're still trying",
    "start": "402680",
    "end": "407800"
  },
  {
    "text": "to get a little more native in kubernetes and then workflows with partitioning so divide that cluster",
    "start": "407800",
    "end": "412919"
  },
  {
    "text": "resource into sections for job management any questions on those two slides",
    "start": "412919",
    "end": "420319"
  },
  {
    "text": "okay I'm going to hand this over to Skyler who's going to go through the architecture again this is just the",
    "start": "424599",
    "end": "429800"
  },
  {
    "text": "slurm operator um this is the first piece and the piece we'll be demonstrating",
    "start": "429800",
    "end": "435000"
  },
  {
    "text": "today Sky thanks Marlo can you hear me yes all right great yep so uh starting",
    "start": "435000",
    "end": "443759"
  },
  {
    "text": "off before we talk about the slurm operator as in the kubernetes piece let's talk a little bit about slurm uh",
    "start": "443759",
    "end": "451400"
  },
  {
    "text": "so you can better understand how slurm is architecturally organized and then",
    "start": "451400",
    "end": "457000"
  },
  {
    "text": "later you can understand how contextually the slurm operator interacts so uh generally speaking this",
    "start": "457000",
    "end": "464720"
  },
  {
    "text": "is the rough diagramming of a slurm cluster you have the slurm control plane",
    "start": "464720",
    "end": "469960"
  },
  {
    "text": "slurm ctld uh as as you might know it uh which kind of sits in the middle between the",
    "start": "469960",
    "end": "477159"
  },
  {
    "text": "slurm accounting that manages a database typically Maria or",
    "start": "477159",
    "end": "483479"
  },
  {
    "text": "MySQL uh it also communicates directly with the slurm uh node agents the slurm",
    "start": "483479",
    "end": "489919"
  },
  {
    "text": "D's which is where uh work is actually dispatched to uh and then the slurm rust",
    "start": "489919",
    "end": "497240"
  },
  {
    "text": "API uh becomes a well an API for which",
    "start": "497240",
    "end": "502599"
  },
  {
    "text": "we can communicate with the cluster over a network uh new in slurm is the",
    "start": "502599",
    "end": "510120"
  },
  {
    "text": "uh sakd this is the slurm OED agent uh for those who are familiar with mung uh",
    "start": "510120",
    "end": "517800"
  },
  {
    "text": "slurm or sacd is a drop in replacement for mung uh and it gives us flexibility to",
    "start": "517800",
    "end": "525000"
  },
  {
    "text": "add features in the future okay uh next",
    "start": "525000",
    "end": "531760"
  },
  {
    "text": "slide so typically this is the general uh job submission pipelines for slurm",
    "start": "532560",
    "end": "540079"
  },
  {
    "text": "you can either use an authenticated login host um or you can submit jobs",
    "start": "540079",
    "end": "546600"
  },
  {
    "text": "through the rest API uh authenticating through a JWT token that was generated",
    "start": "546600",
    "end": "553920"
  },
  {
    "text": "by the slurm control plane um all right next",
    "start": "553920",
    "end": "561600"
  },
  {
    "text": "slide all right so uh this is how the slurm",
    "start": "561600",
    "end": "566680"
  },
  {
    "text": "operator uh fits into the big picture under kubernetes you can see that there is a",
    "start": "566680",
    "end": "573200"
  },
  {
    "text": "set of crds and CRS respectively uh created for",
    "start": "573200",
    "end": "578279"
  },
  {
    "text": "your slurm cluster right uh the main communication channels between the slurm",
    "start": "578279",
    "end": "584720"
  },
  {
    "text": "operator between kubernetes world and the slurm world is through their",
    "start": "584720",
    "end": "589800"
  },
  {
    "text": "respective apis the cube API and the slurm rust",
    "start": "589800",
    "end": "594880"
  },
  {
    "text": "API okay next slide so this is the internal",
    "start": "595839",
    "end": "601200"
  },
  {
    "text": "architecture of how the slurm operator is uh set up generally",
    "start": "601200",
    "end": "607600"
  },
  {
    "text": "speaking um when a cluster CR is generated it uh",
    "start": "607600",
    "end": "615360"
  },
  {
    "text": "generally points to where and how the slurm operator can communicate back to a",
    "start": "615360",
    "end": "621560"
  },
  {
    "text": "slurm cluster so you have the uh Network URL for the slurm reste and and",
    "start": "621560",
    "end": "629880"
  },
  {
    "text": "accompanying uh secret that contains the off",
    "start": "629880",
    "end": "635320"
  },
  {
    "text": "token which then the uh cluster controller will create a slurm client uh",
    "start": "635320",
    "end": "641040"
  },
  {
    "text": "which similar to the cube client is a library method to communicate directly",
    "start": "641040",
    "end": "648480"
  },
  {
    "text": "with some sort of remote API in this case the uh slur",
    "start": "648480",
    "end": "654000"
  },
  {
    "text": "Rd then uh you can create a set of",
    "start": "654000",
    "end": "659800"
  },
  {
    "text": "nodes which we call a node set therefore a crd right and a respective CR for your",
    "start": "659800",
    "end": "667360"
  },
  {
    "text": "slurm cluster uh to create pools of resources and the node set controller is",
    "start": "667360",
    "end": "674639"
  },
  {
    "text": "responsible for creating those slurm D pods uh as well as",
    "start": "674639",
    "end": "682560"
  },
  {
    "text": "orchestrating um intelligent uh creation and deletion of",
    "start": "682560",
    "end": "689079"
  },
  {
    "text": "the resources um for one example if you have a running workload on your compute nodes",
    "start": "689079",
    "end": "696040"
  },
  {
    "text": "and you tell tell um kubernetes to scale back your slurm compute pods then you",
    "start": "696040",
    "end": "703680"
  },
  {
    "text": "want to be careful about how you uh remove those slurm DS because there may",
    "start": "703680",
    "end": "709079"
  },
  {
    "text": "be running work right in the world the slurm typically you uh drain the node uh",
    "start": "709079",
    "end": "715000"
  },
  {
    "text": "before you can safely uh terminate the well in this case the Pod containing the",
    "start": "715000",
    "end": "723079"
  },
  {
    "text": "uh slur node uh and then the life cycle I was",
    "start": "723079",
    "end": "728360"
  },
  {
    "text": "going to say one more thing yeah and then the uh slurm D like the uh the the nod set pods",
    "start": "728360",
    "end": "734880"
  },
  {
    "text": "self manage um their um membership into the cluster so",
    "start": "734880",
    "end": "742639"
  },
  {
    "text": "we are leveraging the uh slurm Dynamic uh node features or sorry",
    "start": "742639",
    "end": "749399"
  },
  {
    "text": "Dynamic nodes right which is a relatively new feature in slurm which allows you to create uh arbitrary",
    "start": "749399",
    "end": "756959"
  },
  {
    "text": "effectively uh compute nodes then have it join a cluster provided that it has",
    "start": "756959",
    "end": "762360"
  },
  {
    "text": "the proper authentication okay next",
    "start": "762360",
    "end": "767839"
  },
  {
    "text": "slide so because um this is mainly Network driven you can actually have a",
    "start": "767839",
    "end": "777199"
  },
  {
    "text": "heterogeneous architecture from the standpoint of Hosting slurm resources",
    "start": "777320",
    "end": "782839"
  },
  {
    "text": "within and uh outside of kubernetes um so the only things I need",
    "start": "782839",
    "end": "789360"
  },
  {
    "text": "to live within kubernetes is the slurm operator and uh a particular metrics pod",
    "start": "789360",
    "end": "797000"
  },
  {
    "text": "that we have created that uh talks to your slurm cluster um but feasibly everything else",
    "start": "797000",
    "end": "804279"
  },
  {
    "text": "in slurm could be within or outside of kubernetes so this just shows an",
    "start": "804279",
    "end": "810399"
  },
  {
    "text": "arbitrary diagram of how you could say have pretty much your entire slurm",
    "start": "810399",
    "end": "817079"
  },
  {
    "text": "cluster outside of kubernetes if you so chose but still leverage kubernetes say",
    "start": "817079",
    "end": "822720"
  },
  {
    "text": "for elastic Computing purposes if that was your particular use",
    "start": "822720",
    "end": "827839"
  },
  {
    "text": "case uh although you could also host everything in kubernetes or slice and dice as you see fit we have a question",
    "start": "827839",
    "end": "836120"
  },
  {
    "text": "from uh yaen did I say that correctly",
    "start": "836120",
    "end": "841040"
  },
  {
    "text": "yes thank you do you hear me yes yes okay yeah sorry for the delay uh thank",
    "start": "843920",
    "end": "849800"
  },
  {
    "text": "you Ricardo and thank you uh Skyler for sharing your thoughts I'm just having so be with me I'm just having some",
    "start": "849800",
    "end": "856199"
  },
  {
    "text": "difficulty understand the approach you you're driving here the operator is supposed to to merge both world the HPC",
    "start": "856199",
    "end": "864240"
  },
  {
    "text": "Lear classic world and the kubernetes world so it's like using the same infastructure we're talking when I say",
    "start": "864240",
    "end": "871320"
  },
  {
    "text": "infrastructure I'm talking in term of Hardware can I like use the same hardware and have kubernetes deployed",
    "start": "871320",
    "end": "878880"
  },
  {
    "text": "and spin up and down slum nodes within my infrastructure using the classical kues",
    "start": "878880",
    "end": "886600"
  },
  {
    "text": "API or I'm still supposed to to have like two separated",
    "start": "886600",
    "end": "891800"
  },
  {
    "text": "work so in in this particular model you have both a slurm is taking a subset of your your",
    "start": "891800",
    "end": "899240"
  },
  {
    "text": "kubernetes resources so you can still actually have kubernetes workload uh running in your cluster as",
    "start": "899240",
    "end": "906680"
  },
  {
    "text": "well as a essentially a virtual slurm cluster that can then be running slurm",
    "start": "906680",
    "end": "912360"
  },
  {
    "text": "workload on its allocated uh kubernetes node resources right because they the",
    "start": "912360",
    "end": "919920"
  },
  {
    "text": "actual kuber or the actual slurm nodes are represented as pods under",
    "start": "919920",
    "end": "927240"
  },
  {
    "text": "kubernetes so it can be either is is the summary okay so you can run yeah yeah so",
    "start": "927240",
    "end": "934920"
  },
  {
    "text": "this this piece we have running entirely in kubernetes um but this is also that",
    "start": "934920",
    "end": "940920"
  },
  {
    "text": "you can do multiple multicluster as well okay thank you from from your experience",
    "start": "940920",
    "end": "947560"
  },
  {
    "text": "so far what would you like uh decide to to choose to stick to Pure kubernetes or",
    "start": "947560",
    "end": "955199"
  },
  {
    "text": "have like uh everything else outside of kubernetes",
    "start": "955199",
    "end": "960519"
  },
  {
    "text": "there's pros and cons to yeah this is a long journey um so",
    "start": "960519",
    "end": "968399"
  },
  {
    "text": "this is the first step of just get the accounting Etc working within kubernetes",
    "start": "968399",
    "end": "973639"
  },
  {
    "text": "um we have another project that we're we have a PO for and now we need to write it a little more solidly which will",
    "start": "973639",
    "end": "980639"
  },
  {
    "text": "allow this slurm scheduling and then we have to connect the components kubernetes still has some restrictions",
    "start": "980639",
    "end": "987440"
  },
  {
    "text": "for instance with CPU scheduling because you still are either static or uh best",
    "start": "987440",
    "end": "994480"
  },
  {
    "text": "effort and there's I had worked at a project um at Intel designed it where we",
    "start": "994480",
    "end": "1002480"
  },
  {
    "text": "had were using Dr to get that those resources exposed so we could mix we're going to have to do some similar tricks",
    "start": "1002480",
    "end": "1008920"
  },
  {
    "text": "so round one is get the um basically get",
    "start": "1008920",
    "end": "1014319"
  },
  {
    "text": "that accounting capabilities and the scheduling components in there and then after that we'll start working on",
    "start": "1014319",
    "end": "1020959"
  },
  {
    "text": "getting this more native does that help yes thank you moo thank you Skyler",
    "start": "1020959",
    "end": "1027839"
  },
  {
    "text": "it's great seeing this progress progress so far yeah we're we're well aware of",
    "start": "1027839",
    "end": "1033319"
  },
  {
    "text": "the gaps um but step one is to start doing",
    "start": "1033319",
    "end": "1040199"
  },
  {
    "text": "that Skyler do you have anything else you want to talk about on this slide uh nope next",
    "start": "1040199",
    "end": "1046520"
  },
  {
    "text": "slide yep so uh I'll be turning this back over to Marlo for the demo portion",
    "start": "1047079",
    "end": "1053360"
  },
  {
    "text": "U maybe I do have a question then quickly uh so I I understood this is",
    "start": "1053360",
    "end": "1058880"
  },
  {
    "text": "deploying like a virtual um cluster solarm cluster inside kubernetes or",
    "start": "1058880",
    "end": "1065039"
  },
  {
    "text": "adding some nodes on kubernetes to an existing slur cluster uh what what are",
    "start": "1065039",
    "end": "1072559"
  },
  {
    "text": "there any challenges or limitations on on the sharing of the kubernetes nodes",
    "start": "1072559",
    "end": "1078720"
  },
  {
    "text": "uh between slurm and non slurm uh workloads and the other question I had",
    "start": "1078720",
    "end": "1084679"
  },
  {
    "text": "is a lot of the slurm deployments have pretty Advanced topology um definitions for like Network",
    "start": "1084679",
    "end": "1092000"
  },
  {
    "text": "topology Infinity band things like this uh is is is there anything else needed",
    "start": "1092000",
    "end": "1099640"
  },
  {
    "text": "to provide this on the kubernetes world or is are the learn demons um enough to to",
    "start": "1099640",
    "end": "1108840"
  },
  {
    "text": "build this topology okay uh so",
    "start": "1108840",
    "end": "1114200"
  },
  {
    "text": "the first question was uh I guess difficulties in running uh slurm D's",
    "start": "1114200",
    "end": "1120440"
  },
  {
    "text": "under kubernetes uh yes there is limitations within kubernetes one such limitation is",
    "start": "1120440",
    "end": "1128000"
  },
  {
    "text": "uh cgroup management or uh and to what granularity pods have control over their",
    "start": "1128000",
    "end": "1134600"
  },
  {
    "text": "c groups so for instance um the slurm is within a c group dictated by the",
    "start": "1134600",
    "end": "1143120"
  },
  {
    "text": "kubet uh but the slurm D is unable to sub manage its uh sub Tree in c groups",
    "start": "1143120",
    "end": "1150640"
  },
  {
    "text": "among the slurm stees so in practice what that means is",
    "start": "1150640",
    "end": "1155679"
  },
  {
    "text": "you'll have uh users will have to be careful when submitting jobs on these nodes uh",
    "start": "1155679",
    "end": "1163320"
  },
  {
    "text": "because if uh c groups is violated your slurm deep pod will get killed um which",
    "start": "1163320",
    "end": "1171520"
  },
  {
    "text": "will impact all workloads under that node so there is uh some limitations therein so for",
    "start": "1171520",
    "end": "1178280"
  },
  {
    "text": "instance uh doing exclusive ownership over these type of nodes would be one",
    "start": "1178280",
    "end": "1184400"
  },
  {
    "text": "way of preventing unwanted uh in this case oom killing from",
    "start": "1184400",
    "end": "1191400"
  },
  {
    "text": "uh c groups um uh the other question Kevin hold on",
    "start": "1191400",
    "end": "1198760"
  },
  {
    "text": "Kevin has a thing to comment on that and he also has one in the chat go ahead Kevin kind of related to that is is",
    "start": "1198760",
    "end": "1207559"
  },
  {
    "text": "the the networking for that pod host Network or is it going through the",
    "start": "1207559",
    "end": "1213039"
  },
  {
    "text": "regular kubernetes stack currently it's going through the kubernetes stack um we're going we're going to have to",
    "start": "1213039",
    "end": "1219400"
  },
  {
    "text": "change that we're aware of that over time yeah um but yeah that that is by",
    "start": "1219400",
    "end": "1226159"
  },
  {
    "text": "itself a headache I'm familiar with the problem or or maybe something with",
    "start": "1226159",
    "end": "1231640"
  },
  {
    "text": "mulus yeah you have mulus or network service mesh um there there's a couple of them that",
    "start": "1231640",
    "end": "1238799"
  },
  {
    "text": "we can look into yeah okay yeah right and then the uh follow-up question about",
    "start": "1238799",
    "end": "1246120"
  },
  {
    "text": "topology so uh we are looking at how to",
    "start": "1246120",
    "end": "1252720"
  },
  {
    "text": "automagically have slurm manage topology within your kubernetes by using say um",
    "start": "1252720",
    "end": "1260080"
  },
  {
    "text": "uh what is it node feature Discovery if I recall the project correctly yeah",
    "start": "1260080",
    "end": "1265400"
  },
  {
    "text": "basically stand standardize labeling and then be able to generate a slurm",
    "start": "1265400",
    "end": "1271559"
  },
  {
    "text": "topology configuration uh based upon uh well this type of",
    "start": "1271559",
    "end": "1277679"
  },
  {
    "text": "standard so the NFD is still kind of a hack so ultimately we're still going to have to have a driver on the Node that's",
    "start": "1277679",
    "end": "1284360"
  },
  {
    "text": "pulling in those resource understanding and exposing it to our schedule",
    "start": "1284360",
    "end": "1290519"
  },
  {
    "text": "awesome that's great thanks s yeah where there are a lot of limitations that",
    "start": "1293919",
    "end": "1299159"
  },
  {
    "text": "we're going to have to work around with um within kubernetes so again this is just the first",
    "start": "1299159",
    "end": "1306278"
  },
  {
    "text": "step go ahead H yeah thank you uh it's a great step uh it's the first brid I",
    "start": "1306520",
    "end": "1313640"
  },
  {
    "text": "definitely the right direction uh my question is related to",
    "start": "1313640",
    "end": "1318720"
  },
  {
    "text": "based on a question before me basically the SLB part uh uses certain images and",
    "start": "1318720",
    "end": "1327159"
  },
  {
    "text": "the idea is how am I able to to access my specific Hardware using slurm D do I",
    "start": "1327159",
    "end": "1335400"
  },
  {
    "text": "use the existing uh images that that comes like uh loaded with every Hardware",
    "start": "1335400",
    "end": "1341919"
  },
  {
    "text": "supported or uh is it like it should be kind of a base image that I would use",
    "start": "1341919",
    "end": "1348840"
  },
  {
    "text": "and then like bake in my hardware dependencies and then have slmd",
    "start": "1348840",
    "end": "1355039"
  },
  {
    "text": "running so there there's multiple ways of dealing with this problem right um",
    "start": "1355039",
    "end": "1361679"
  },
  {
    "text": "one way is you can bake stuff into the image right we provide a pathway that",
    "start": "1361679",
    "end": "1367480"
  },
  {
    "text": "you can uh build a derived image uh based on our image which will just add",
    "start": "1367480",
    "end": "1374559"
  },
  {
    "text": "all The slurm Damons and you're free to add your B Aries for your particular",
    "start": "1374559",
    "end": "1379919"
  },
  {
    "text": "workloads and your you know specifics uh another way is using oci containers so",
    "start": "1379919",
    "end": "1386760"
  },
  {
    "text": "if you can containerize your workloads then you don't care about what you're underlying images right um or if you",
    "start": "1386760",
    "end": "1395840"
  },
  {
    "text": "prefer as some sites do to use like a network file share to host binaries then",
    "start": "1395840",
    "end": "1401600"
  },
  {
    "text": "your image matters less as long as you can set up a mutual or shared file",
    "start": "1401600",
    "end": "1407440"
  },
  {
    "text": "system which could post your binaries uh and then also uh along the",
    "start": "1407440",
    "end": "1415039"
  },
  {
    "text": "same lines you could also instead of using a shared file system you could use like an object storage and have your",
    "start": "1415039",
    "end": "1421480"
  },
  {
    "text": "jobs pull binaries over the network if that's how you wanted to you know manage",
    "start": "1421480",
    "end": "1429039"
  },
  {
    "text": "um and then images become less of a problem yeah yeah but just back to the oci approach if I'm assuming I'm",
    "start": "1429039",
    "end": "1437039"
  },
  {
    "text": "deploying my jobs within oci images and uh are there any standard that slurm",
    "start": "1437039",
    "end": "1442640"
  },
  {
    "text": "expect them to have so that like when when scheduling it it knows exactly how",
    "start": "1442640",
    "end": "1448200"
  },
  {
    "text": "to trigger my my my training job or my",
    "start": "1448200",
    "end": "1453440"
  },
  {
    "text": "inference as long as the container that's being run is an oci compliant container slurm can launch and dispatch",
    "start": "1453440",
    "end": "1460880"
  },
  {
    "text": "within slurm so it actually Sid steps kubernetes it will be running in the slurm context okay okay thank you",
    "start": "1460880",
    "end": "1470120"
  },
  {
    "text": "Kevin has some commentary and Mohamad has a couple of questions there specific to slur Skyler in the chat oh yeah let's",
    "start": "1470120",
    "end": "1477000"
  },
  {
    "text": "see uh the SL ones you'll take I'll take Heaven's okay let's see uh question does",
    "start": "1477000",
    "end": "1483720"
  },
  {
    "text": "task plugin T cgroup work under this model uh so unfortunately we there's no",
    "start": "1483720",
    "end": "1490880"
  },
  {
    "text": "cgroup support currently in this model of slurm under kubernetes we will be looking at uh",
    "start": "1490880",
    "end": "1498360"
  },
  {
    "text": "figuring out how to rectify that um probably with Dr and some drpc",
    "start": "1498360",
    "end": "1503720"
  },
  {
    "text": "or some sort of way to interface with slurm it's going to take some changes to",
    "start": "1503720",
    "end": "1509000"
  },
  {
    "text": "slurm as well right there there there's architectural improvements on both ends",
    "start": "1509000",
    "end": "1514360"
  },
  {
    "text": "to support uh this type of thing so in general c groups slurm under kubernetes",
    "start": "1514360",
    "end": "1520080"
  },
  {
    "text": "does not work or rather the sub management of cgroups by slurm",
    "start": "1520080",
    "end": "1525159"
  },
  {
    "text": "specifically right let's see depending upon the definition HBC HPC at a certain",
    "start": "1525159",
    "end": "1531200"
  },
  {
    "text": "uh scale relies really cares about performance that implies compiling code for the hardware and questions soes stop",
    "start": "1531200",
    "end": "1538039"
  },
  {
    "text": "being portable so this commentary Kevin goes back to the init the earlier slide I had on HBC versus Cloud native",
    "start": "1538039",
    "end": "1546200"
  },
  {
    "text": "environments um one of the things that cloud native does get you is it gets you",
    "start": "1546200",
    "end": "1551880"
  },
  {
    "text": "a place where the user doesn't necessarily have to care about the hardware and if you're trying to get top",
    "start": "1551880",
    "end": "1558880"
  },
  {
    "text": "performance really you just need to be running bare metal if you want a cloud it like there's a cost right I I I would",
    "start": "1558880",
    "end": "1564520"
  },
  {
    "text": "argue that you can use images that are not portable to gain that same level of",
    "start": "1564520",
    "end": "1570480"
  },
  {
    "text": "performance and use the container as a way to separate from the the host OS",
    "start": "1570480",
    "end": "1576120"
  },
  {
    "text": "enough that you don't have giant flag days when you have to change uh underlying operating",
    "start": "1576120",
    "end": "1584320"
  },
  {
    "text": "system yeah absolutely but you're going to have to make sure your schedule to",
    "start": "1584320",
    "end": "1589679"
  },
  {
    "text": "nodes because you you if you're in a heterogeneous cluster you still have to make sure you're scheduled to nodes that",
    "start": "1589679",
    "end": "1595000"
  },
  {
    "text": "have the hardware You're Expecting yes but generally the the kernel matters",
    "start": "1595000",
    "end": "1602640"
  },
  {
    "text": "a lot less than uh all of the support libraries yeah",
    "start": "1602640",
    "end": "1609278"
  },
  {
    "text": "yeah yeah but you're you're still going to have the hardware the specific Hardware piece which again we're still",
    "start": "1610200",
    "end": "1615320"
  },
  {
    "text": "going to have to find a way to thread that through yeah yeah but but generally like the cluster stays homogeneous",
    "start": "1615320",
    "end": "1622960"
  },
  {
    "text": "during one of these like OS upgrades that the hardware doesn't change you're just updating a kernel and",
    "start": "1622960",
    "end": "1631000"
  },
  {
    "text": "kind of the the container orchestration system or whatever out from under it but",
    "start": "1631000",
    "end": "1636279"
  },
  {
    "text": "that could be completely decoupled from the applications",
    "start": "1636279",
    "end": "1641679"
  },
  {
    "text": "yeah do you have any more discussion before we want to go run to the",
    "start": "1645840",
    "end": "1652000"
  },
  {
    "text": "demo I don't see any I think we can move on okay",
    "start": "1652120",
    "end": "1657960"
  },
  {
    "text": "um pull this over so this is basically nothing is",
    "start": "1657960",
    "end": "1663840"
  },
  {
    "text": "running piece and",
    "start": "1663840",
    "end": "1670600"
  },
  {
    "text": "um so I have an AI Benchmark a very simple one set up um",
    "start": "1672559",
    "end": "1680120"
  },
  {
    "text": "and it took me a while I mean I'm running in kind because I wanted to not be reliant",
    "start": "1684240",
    "end": "1689320"
  },
  {
    "text": "on other resources so I could run this demo um which means that I had to find a",
    "start": "1689320",
    "end": "1695399"
  },
  {
    "text": "benchmark that actually ran on a single machine so I'm using this AI Benchmark that used to be used for uh phones",
    "start": "1695399",
    "end": "1702080"
  },
  {
    "text": "believe it or not um",
    "start": "1702080",
    "end": "1706640"
  },
  {
    "text": "so I'm sending that off you see that the job is started and",
    "start": "1712120",
    "end": "1718279"
  },
  {
    "text": "just so we know what's running here uh",
    "start": "1718279",
    "end": "1723519"
  },
  {
    "text": "can you guys see this and trying to make it",
    "start": "1743279",
    "end": "1747080"
  },
  {
    "text": "bigger we go so these are the two compute nodes",
    "start": "1750519",
    "end": "1755960"
  },
  {
    "text": "that we're running and they're shown up here um we're currently partway through",
    "start": "1755960",
    "end": "1761039"
  },
  {
    "text": "this run here and then uh the partition is named purple so so",
    "start": "1761039",
    "end": "1768799"
  },
  {
    "text": "here we have our slimm controller the metrics which is pulling it through and then this rest API which you saw in the",
    "start": "1768799",
    "end": "1774880"
  },
  {
    "text": "earlier forms um so you see 50% is using is being used",
    "start": "1774880",
    "end": "1782679"
  },
  {
    "text": "because currently we're running that work load on one of the two nodes and",
    "start": "1782679",
    "end": "1789000"
  },
  {
    "text": "then we're going to queue up a whole bunch of sleep jobs um mostly for the purposes of this",
    "start": "1793679",
    "end": "1801440"
  },
  {
    "text": "demo so you can see them all up here some are waiting for resources um one of them is already",
    "start": "1801679",
    "end": "1808159"
  },
  {
    "text": "running on the second node so what we'll see on the grafana is that now we have",
    "start": "1808159",
    "end": "1815200"
  },
  {
    "text": "100% because both nodes are getting used here's the jobs that are cued which are going up exponentially now because we",
    "start": "1815200",
    "end": "1821600"
  },
  {
    "text": "have a whole bunch of sleeps that we just added um and yes moabit we will um",
    "start": "1821600",
    "end": "1829960"
  },
  {
    "text": "so this is where all of the these are the jobs coming up and then after they complete they'll start",
    "start": "1829960",
    "end": "1836240"
  },
  {
    "text": "to come down the workload that we ran takes",
    "start": "1836240",
    "end": "1844320"
  },
  {
    "text": "about two minutes so still going so give another like 30",
    "start": "1844320",
    "end": "1852360"
  },
  {
    "text": "seconds I can so I can prove that worked",
    "start": "1852360",
    "end": "1857360"
  },
  {
    "text": "okay it finished and then the output I I'm",
    "start": "1871600",
    "end": "1878639"
  },
  {
    "text": "piping into here so we can prove that the job actually ran um we have all these sleeps going into different nodes",
    "start": "1878639",
    "end": "1886919"
  },
  {
    "text": "concurrently so we'll see this go up and down according to what gets",
    "start": "1886919",
    "end": "1892399"
  },
  {
    "text": "scheduled um so you'll see this go up and down and up and down and meanwhile you'll have",
    "start": "1896679",
    "end": "1903200"
  },
  {
    "text": "crisscrosses here on um when it's idle versus when they're being allocated and",
    "start": "1903200",
    "end": "1910080"
  },
  {
    "text": "run any questions",
    "start": "1911279",
    "end": "1915559"
  },
  {
    "text": "it's always nice when the demo Works live demos are",
    "start": "1927360",
    "end": "1932440"
  },
  {
    "text": "dangerous yeah looking",
    "start": "1932440",
    "end": "1936039"
  },
  {
    "text": "good um Ricardo for your question I think that's where we're Landing",
    "start": "1938840",
    "end": "1944679"
  },
  {
    "text": "um Tim can probably answer that more than I can he's still",
    "start": "1944679",
    "end": "1950159"
  },
  {
    "text": "here uh which is the git the the thing in the chat Tim is the gitlab skmd",
    "start": "1950159",
    "end": "1955880"
  },
  {
    "text": "slinky",
    "start": "1955880",
    "end": "1958559"
  },
  {
    "text": "repo yeah I was asking the question because I did find the project uh but it",
    "start": "1962880",
    "end": "1968840"
  },
  {
    "text": "had the schedu I think it was the sket plugin yeah that's the scheduler plugin",
    "start": "1968840",
    "end": "1974279"
  },
  {
    "text": "and that's the proof of concept for the scheduler part this is more the operator and running inside kubernetes",
    "start": "1974279",
    "end": "1982240"
  },
  {
    "text": "yeah uh we have not running open shift yet but it should run because we're using",
    "start": "1983919",
    "end": "1990360"
  },
  {
    "text": "uh we're using bare metal kubernetes but it should it should work just fine I",
    "start": "1990360",
    "end": "1995960"
  },
  {
    "text": "don't see anything that would be an issue um Kevin the initial release will",
    "start": "1995960",
    "end": "2003320"
  },
  {
    "text": "be ccon na that's our current Target",
    "start": "2003320",
    "end": "2008200"
  },
  {
    "text": "cool yes and maybe my last question uh do do",
    "start": "2010639",
    "end": "2016559"
  },
  {
    "text": "you have some kind of guide how to set up kind of Dev environment just like",
    "start": "2016559",
    "end": "2022880"
  },
  {
    "text": "yours I'll be glad to give one um so I'm I'm using kind finding I will tell you",
    "start": "2022880",
    "end": "2028720"
  },
  {
    "text": "finding a workload that would run on a machine and not out of memory kill everything was very",
    "start": "2028720",
    "end": "2035639"
  },
  {
    "text": "difficult yeah cool if can share something I'll be really Greatful thank yeah I'll be happy to I'll happy to send",
    "start": "2035799",
    "end": "2042200"
  },
  {
    "text": "you that okay cool maybe maybe well while we're here I",
    "start": "2042200",
    "end": "2050520"
  },
  {
    "text": "had another question which is uh what are the connectivity requirements between the multiple",
    "start": "2050520",
    "end": "2057158"
  },
  {
    "text": "nodes uh in this learn cluster I guess there has to be like IP layer three",
    "start": "2057159",
    "end": "2064358"
  },
  {
    "text": "connectivity between all nodes in the cluster or currently we're using yeah currently",
    "start": "2064359",
    "end": "2070000"
  },
  {
    "text": "we're using the cni we have not solved the the MPI piece we'll probably try to use the MPI operator in CP flow to start",
    "start": "2070000",
    "end": "2077560"
  },
  {
    "text": "um and then we'll have to get more clever so we can use the the networking",
    "start": "2077560",
    "end": "2082599"
  },
  {
    "text": "underneath that's faster I guess my question was more because um I think Skyler mentioned",
    "start": "2082599",
    "end": "2089398"
  },
  {
    "text": "before one of the use cases or you that one of these cases was using kubernetes",
    "start": "2089399",
    "end": "2094960"
  },
  {
    "text": "to kind of burst capacity on existing Le clusters uh but imagine this would be",
    "start": "2094960",
    "end": "2101560"
  },
  {
    "text": "like on the public Cloud does it is there some magic on in slurm to do the",
    "start": "2101560",
    "end": "2106640"
  },
  {
    "text": "networking between like some tunnel between the nodes or or does it require",
    "start": "2106640",
    "end": "2112760"
  },
  {
    "text": "like connectivity between the the nodes and the remote uh cluster and like in a",
    "start": "2112760",
    "end": "2119119"
  },
  {
    "text": "public cloud with a slurm controller or or with all the nodes in",
    "start": "2119119",
    "end": "2124280"
  },
  {
    "text": "the cluster Skyler feel free to answer this but I believe is there is no magic yeah",
    "start": "2124280",
    "end": "2132280"
  },
  {
    "text": "there there is no magic you're dealing with your standard uh networking wos of",
    "start": "2132280",
    "end": "2137400"
  },
  {
    "text": "uh resolving let's say in DNS right or DHCP uh your nodes having nodes that",
    "start": "2137400",
    "end": "2144800"
  },
  {
    "text": "live say on Prem and in a cloud you'll have to typically work with your cloud",
    "start": "2144800",
    "end": "2149880"
  },
  {
    "text": "provider to set up a secure tunnel uh between your on Prem site and the cloud",
    "start": "2149880",
    "end": "2156200"
  },
  {
    "text": "uh or you can try to segment your workloads to prevent any cross",
    "start": "2156200",
    "end": "2162240"
  },
  {
    "text": "chatter uh between like on Prem and in the cloud if you don't have that pipe",
    "start": "2162240",
    "end": "2167599"
  },
  {
    "text": "set up for instance okay yeah that was my question thank",
    "start": "2167599",
    "end": "2173920"
  },
  {
    "text": "you any more questions",
    "start": "2185960",
    "end": "2189560"
  },
  {
    "text": "yeah I don't see any",
    "start": "2193480",
    "end": "2197480"
  },
  {
    "text": "y you mentioned there would be a like a second part in the presentation is it",
    "start": "2205480",
    "end": "2210680"
  },
  {
    "text": "like a yeah we can there's a future work section um",
    "start": "2210680",
    "end": "2219520"
  },
  {
    "text": "so for the future work we're going to we're working on the slurm scheduler",
    "start": "2219560",
    "end": "2224640"
  },
  {
    "text": "component which you've seen the pock we're going to try to do something a little more cohesive before ccon and no",
    "start": "2224640",
    "end": "2232839"
  },
  {
    "text": "guarantees but we're working on it um we need finer grain management of kuet resource",
    "start": "2232839",
    "end": "2238440"
  },
  {
    "text": "allocations um and that was the problem I alluded to earlier with the you can't",
    "start": "2238440",
    "end": "2243800"
  },
  {
    "text": "mix pinned and unpinned while on anything more complicated set needs to to be added in um and also increase the",
    "start": "2243800",
    "end": "2251920"
  },
  {
    "text": "pluggable infrastructure of kubernetes the current CPU and memory manager leaves a lot to be desired um we could",
    "start": "2251920",
    "end": "2258280"
  },
  {
    "text": "go through Dr um which maybe with their new numerical",
    "start": "2258280",
    "end": "2264119"
  },
  {
    "text": "model I think it's possible um we're also going to start using NFD combined",
    "start": "2264119",
    "end": "2269319"
  },
  {
    "text": "with slurm internals as far as a more native scheduling",
    "start": "2269319",
    "end": "2274720"
  },
  {
    "text": "um as in pod placement and then add in the slurm scheduling extension to hand resource scheduling for the cluster so",
    "start": "2274720",
    "end": "2282119"
  },
  {
    "text": "basically map the scheduling Concepts not in slurm to slurm like Affinity anti-affinity",
    "start": "2282119",
    "end": "2289160"
  },
  {
    "text": "work uh this all looks really good it it's focused primarily on like the",
    "start": "2291440",
    "end": "2298800"
  },
  {
    "text": "backend side of things it seems like um what are the plans so far around",
    "start": "2298800",
    "end": "2306480"
  },
  {
    "text": "having a cuber native API for job",
    "start": "2306480",
    "end": "2311880"
  },
  {
    "text": "submission so the goal is to be able to just have this working in the back end",
    "start": "2314640",
    "end": "2320240"
  },
  {
    "text": "and then do the scheduling like you do today with um but through the slurm",
    "start": "2320240",
    "end": "2325319"
  },
  {
    "text": "scheduler so so still sbatch yeah submit jobs um yeah and you can go either",
    "start": "2325319",
    "end": "2333040"
  },
  {
    "text": "through the slurm uh API or you can go through the kubernetes you'll have the",
    "start": "2333040",
    "end": "2339119"
  },
  {
    "text": "two models but we're not looking to make yet another model on top that you now",
    "start": "2339119",
    "end": "2345079"
  },
  {
    "text": "have to figure out how to submit to what do you mean the the kubernetes",
    "start": "2345079",
    "end": "2351119"
  },
  {
    "text": "model like kubernetes job objects or yeah the job objects yeah",
    "start": "2351119",
    "end": "2357920"
  },
  {
    "text": "okay that makes",
    "start": "2357920",
    "end": "2361318"
  },
  {
    "text": "sense um is there any of that stuff in",
    "start": "2363119",
    "end": "2368319"
  },
  {
    "text": "the code basee already or is that like future planned work no that's future planed work okay so yeah Step One is get",
    "start": "2368319",
    "end": "2376240"
  },
  {
    "text": "this running and then start working on",
    "start": "2376240",
    "end": "2380720"
  },
  {
    "text": "yeah thanks for the presentation I could continue a little bit of uh Kevin's",
    "start": "2385160",
    "end": "2390960"
  },
  {
    "text": "comments there so when you mentioned kubernetes objects is there a possibility to essentially submit an",
    "start": "2390960",
    "end": "2396880"
  },
  {
    "text": "spatch script or something similar as a cetes crd or other object is that what",
    "start": "2396880",
    "end": "2402200"
  },
  {
    "text": "you intend with cetes objects I think we should be supporting",
    "start": "2402200",
    "end": "2408440"
  },
  {
    "text": "anything that's currently the kubernetes uh",
    "start": "2408440",
    "end": "2414040"
  },
  {
    "text": "Paradigm right so does mean does that mean that you are also",
    "start": "2414040",
    "end": "2419280"
  },
  {
    "text": "considering so there's um jobs there's pods just plain pod specs are you",
    "start": "2419280",
    "end": "2426280"
  },
  {
    "text": "planning on mapping those to es batch resources essentially running the images",
    "start": "2426280",
    "end": "2432119"
  },
  {
    "text": "through yes something like aeral Singularity or or similar on top of SL",
    "start": "2432119",
    "end": "2437920"
  },
  {
    "text": "probably not with Singularity um so it'll be still kubernetes um okay but we'll be using",
    "start": "2437920",
    "end": "2443480"
  },
  {
    "text": "the serm scheduling uh Singularity I think they dropped support for kubernetes a while",
    "start": "2443480",
    "end": "2450839"
  },
  {
    "text": "ago no I think the idea would be like sbatch to submit the job and then",
    "start": "2450839",
    "end": "2457960"
  },
  {
    "text": "Singularity run the image that the user specified for the Pod or something like",
    "start": "2457960",
    "end": "2463079"
  },
  {
    "text": "that yes that was what I was intending yeah I don't know",
    "start": "2463079",
    "end": "2468400"
  },
  {
    "text": "especially especially if you also aim to support external clusters so it wasn't kind of fully clear from the",
    "start": "2468400",
    "end": "2473920"
  },
  {
    "text": "presentation but as far as I could gleam you're essentially aiming to support",
    "start": "2473920",
    "end": "2479079"
  },
  {
    "text": "both um running a virtual so cluster on top of the competive noes as well as",
    "start": "2479079",
    "end": "2484520"
  },
  {
    "text": "targeting a completely or connecting to a completely separate external cluster yes where you don't have the context of",
    "start": "2484520",
    "end": "2491599"
  },
  {
    "text": "the like kubernetes runtime so you would need something like aain Singularity to",
    "start": "2491599",
    "end": "2496800"
  },
  {
    "text": "run a containerized workload right it would be regular slurm so if you're",
    "start": "2496800",
    "end": "2502280"
  },
  {
    "text": "going through the oci interface I think you're still okay yeah Ricardo oh the",
    "start": "2502280",
    "end": "2510880"
  },
  {
    "text": "okay right and then that can plug into like pod man or Docker or whatever okay",
    "start": "2510880",
    "end": "2516760"
  },
  {
    "text": "yeah that makes sense we we have done this in the past to run Singularity as run time and still use KU as",
    "start": "2516760",
    "end": "2525480"
  },
  {
    "text": "scheduler all right so something similar could be done I guess just thinking they",
    "start": "2525480",
    "end": "2531680"
  },
  {
    "text": "they're not talking about launching Singularity from the badge file but use",
    "start": "2531680",
    "end": "2537760"
  },
  {
    "text": "the built-in oci support that storm has now yeah and then whatever runtime you plug into that",
    "start": "2537760",
    "end": "2544960"
  },
  {
    "text": "works right yeah similar way I guess yeah and since you're planning to also",
    "start": "2544960",
    "end": "2552440"
  },
  {
    "text": "support declaring your workloads through for example the ppec how are you or how",
    "start": "2552440",
    "end": "2558520"
  },
  {
    "text": "far are you planning on integrating with well for example kubernetes networking",
    "start": "2558520",
    "end": "2563640"
  },
  {
    "text": "config Max map support CSI Etc and also especially with an external slum cluster",
    "start": "2563640",
    "end": "2570880"
  },
  {
    "text": "so assuming I have this uh secure tunnel between my slum environment and my",
    "start": "2570880",
    "end": "2576200"
  },
  {
    "text": "Commodus environment are there is there any support plan for that yeah so we are still planning to",
    "start": "2576200",
    "end": "2583280"
  },
  {
    "text": "report regular mounts so that includes all your CSI support right um and the",
    "start": "2583280",
    "end": "2590880"
  },
  {
    "text": "networking I think what kubernetes currently has available regarding networking isn't",
    "start": "2590880",
    "end": "2596680"
  },
  {
    "text": "sufficient so we're going to have to work on that area either us or people",
    "start": "2596680",
    "end": "2602119"
  },
  {
    "text": "with us okay there there may be some overlap with the cube ver",
    "start": "2602119",
    "end": "2607720"
  },
  {
    "text": "team because they can run into similar issues yeah I agree they might be good",
    "start": "2607720",
    "end": "2613480"
  },
  {
    "text": "collaborators yeah thank you for the",
    "start": "2613480",
    "end": "2619480"
  },
  {
    "text": "pointer and maybe one final question that I had written down here is uh when",
    "start": "2622200",
    "end": "2627800"
  },
  {
    "text": "you mentioned the heterogeneous uh clusters or heterogeneous nodes so",
    "start": "2627800",
    "end": "2632920"
  },
  {
    "text": "commus has some native uh ways of ofar targeting workloads or mapping workloads",
    "start": "2632920",
    "end": "2638800"
  },
  {
    "text": "to specific nodes specifically labels and tains Etc so have you considered",
    "start": "2638800",
    "end": "2644839"
  },
  {
    "text": "that interface yeah so we are planning so NFD basically is all of that so when we talk",
    "start": "2644839",
    "end": "2651640"
  },
  {
    "text": "about using NFD for that Network topology we'll also use that for",
    "start": "2651640",
    "end": "2657119"
  },
  {
    "text": "the where you're placing if needed okay yeah yeah all right thank you yeah I I I",
    "start": "2657119",
    "end": "2665240"
  },
  {
    "text": "still think of NFD a little bit as a hack because it's not automatic you still have to find a way to label",
    "start": "2665240",
    "end": "2670800"
  },
  {
    "text": "Everything E",
    "start": "2670800",
    "end": "2674960"
  },
  {
    "text": "yeah um related to this and it's probably purely an implementation detail",
    "start": "2676079",
    "end": "2682839"
  },
  {
    "text": "is has there been any effort to replace munge or supplement it or",
    "start": "2682839",
    "end": "2689359"
  },
  {
    "text": "something with something other than just like pre-shared",
    "start": "2689359",
    "end": "2694359"
  },
  {
    "text": "key that may be a sky question uh I can't say anything it's",
    "start": "2695240",
    "end": "2702079"
  },
  {
    "text": "more of a Tim question at this point yeah I I've been helping out the Spy",
    "start": "2702079",
    "end": "2708559"
  },
  {
    "text": "project for a while and it there might be some interesting collaboration that could be done there okay but what I can",
    "start": "2708559",
    "end": "2716559"
  },
  {
    "text": "say is at least within the this uh Slinky slurm operator uh piece we are",
    "start": "2716559",
    "end": "2722640"
  },
  {
    "text": "using uh o slurm instead of off munge",
    "start": "2722640",
    "end": "2727880"
  },
  {
    "text": "H okay is there a reason for a mun",
    "start": "2727880",
    "end": "2733480"
  },
  {
    "text": "or o mun historically was the only HPC ready authentication and credential",
    "start": "2733480",
    "end": "2739680"
  },
  {
    "text": "system yeah became the standard and then off slurm came about because there was",
    "start": "2739680",
    "end": "2745960"
  },
  {
    "text": "need to change part of the API to support other features and functionality",
    "start": "2745960",
    "end": "2751720"
  },
  {
    "text": "but U mun being a separate project controlled externally there was limits on traction",
    "start": "2751720",
    "end": "2758800"
  },
  {
    "text": "therein uh so a slurm was created which gives us uh the control over its future",
    "start": "2758800",
    "end": "2765760"
  },
  {
    "text": "uh and because of uh a lot of the work being done under slurm we can support",
    "start": "2765760",
    "end": "2772000"
  },
  {
    "text": "a expandable uh credential uh without issues that we run",
    "start": "2772000",
    "end": "2777240"
  },
  {
    "text": "into with say Munch so dides it start off like life as a fork of mune or is it",
    "start": "2777240",
    "end": "2784040"
  },
  {
    "text": "was it Rewritten from scratch it's uh written from scratch uh Tim could talk",
    "start": "2784040",
    "end": "2789359"
  },
  {
    "text": "at at length since yeah he has some notes in the chat as well okay I'll I'll",
    "start": "2789359",
    "end": "2794880"
  },
  {
    "text": "dig into it more thanks",
    "start": "2794880",
    "end": "2798359"
  },
  {
    "text": "yeah okay I'll add one more then uh you the question before regarding",
    "start": "2802160",
    "end": "2808440"
  },
  {
    "text": "supporting um jobs and and other kubernetes resources are you following",
    "start": "2808440",
    "end": "2814839"
  },
  {
    "text": "the job Set uh work as well um I I know this goes in the batch working group of",
    "start": "2814839",
    "end": "2821240"
  },
  {
    "text": "kubernetes uh but but I think the what they are trying to come up with there is",
    "start": "2821240",
    "end": "2827599"
  },
  {
    "text": "very very similar to a lot of the um Concepts that SL already",
    "start": "2827599",
    "end": "2833240"
  },
  {
    "text": "has yeah I think we do need to be following more closely um to be honest I",
    "start": "2833240",
    "end": "2839200"
  },
  {
    "text": "have to go look yeah I pasted the the documentation",
    "start": "2839200",
    "end": "2845440"
  },
  {
    "text": "they have uh to day but I think this is really a work in progress and they're",
    "start": "2845440",
    "end": "2850880"
  },
  {
    "text": "changing things quite fast so it's probably the moment to if you have some time to to go influence yeah to go and",
    "start": "2850880",
    "end": "2857000"
  },
  {
    "text": "listen and and and make sure that the things are included that you need especially like when you start talking",
    "start": "2857000",
    "end": "2863319"
  },
  {
    "text": "about networking uh and low latency networks I think this quite",
    "start": "2863319",
    "end": "2868960"
  },
  {
    "text": "important all right yeah I'll go I'll go look and then go talk with them as well",
    "start": "2868960",
    "end": "2876480"
  },
  {
    "text": "all right I have one more then uh the first line that says slurm scheduler",
    "start": "2881319",
    "end": "2888119"
  },
  {
    "text": "component this is not the operator no this is not the operator so the operator isn't doing scheduling when we talk",
    "start": "2888119",
    "end": "2894920"
  },
  {
    "text": "about a slurm scheduler component it's basically to be able to run uh with slurm on your cluster and",
    "start": "2894920",
    "end": "2901720"
  },
  {
    "text": "to be able to use the slurm scheduler and but using the cores API or",
    "start": "2901720",
    "end": "2909640"
  },
  {
    "text": "yeah yeah okay okay so that's what it is okay awesome yeah I think we can already",
    "start": "2909640",
    "end": "2916040"
  },
  {
    "text": "predu prch schedu a session about that on yeah the rest the Restriction with it",
    "start": "2916040",
    "end": "2922880"
  },
  {
    "text": "will probably be that we'll have to have both the kuet and the slmd installed on all the nodes because of the cgroups",
    "start": "2922880",
    "end": "2929400"
  },
  {
    "text": "piece so again we we still have to do work to get a more native",
    "start": "2929400",
    "end": "2934640"
  },
  {
    "text": "solution okay and the and the goal would be to translate whatever kubernetes resource to to the slur uh equivalents",
    "start": "2934640",
    "end": "2943359"
  },
  {
    "text": "right and be able to use the cluster exactly like a SL cluster or and still be able to do your scheduling for your",
    "start": "2943359",
    "end": "2949400"
  },
  {
    "text": "kubernetes jobs as well okay very nice so on a single node you won't be able to",
    "start": "2949400",
    "end": "2955839"
  },
  {
    "text": "and and Kevin actually referenced this up above you you still won't be able to do both kubernetes and a slurm job on a",
    "start": "2955839",
    "end": "2962720"
  },
  {
    "text": "Noe at the same time but you would be able to have them both in your cluster",
    "start": "2962720",
    "end": "2968440"
  },
  {
    "text": "yeah because either the kuet or or slurm D has to own the cgroups the problem is the c",
    "start": "2968440",
    "end": "2974119"
  },
  {
    "text": "groups yeah sounds good right it should be noted that using this slur scheduler",
    "start": "2974119",
    "end": "2979960"
  },
  {
    "text": "component is a different operational model not to be confused with what the slurm operator aims to do like they're",
    "start": "2979960",
    "end": "2988160"
  },
  {
    "text": "they're two different models currently like not not not as cohesive",
    "start": "2988160",
    "end": "2993480"
  },
  {
    "text": "as what we want in the future yeah yeah I I understood that was why I was asking",
    "start": "2993480",
    "end": "2999040"
  },
  {
    "text": "because the operator is is managing the resources but still using slurm as slurm",
    "start": "2999040",
    "end": "3004520"
  },
  {
    "text": "while the other one kind of goes more deep into the coriz native uh definitions",
    "start": "3004520",
    "end": "3012160"
  },
  {
    "text": "yeah don't see another",
    "start": "3017000",
    "end": "3021119"
  },
  {
    "text": "question all right uh I I think well we're getting to the end as well but",
    "start": "3024319",
    "end": "3029720"
  },
  {
    "text": "maybe just to wrap up I know there was a question about timelines you mentioned",
    "start": "3029720",
    "end": "3035359"
  },
  {
    "text": "North America coupon to is this for the for the repo and the code to be",
    "start": "3035359",
    "end": "3041079"
  },
  {
    "text": "available yes okay okay very good for and and for both",
    "start": "3041079",
    "end": "3046960"
  },
  {
    "text": "the the operator and the scheduler uh definitely the operator we we'll try to",
    "start": "3046960",
    "end": "3052440"
  },
  {
    "text": "complete the Schuler before then as well we have committed to the operator",
    "start": "3052440",
    "end": "3057920"
  },
  {
    "text": "and the second thing I was going to say is that you know where this group hangs out in slack so I guess uh from from the",
    "start": "3057920",
    "end": "3065000"
  },
  {
    "text": "comments we saw and the questions I think a lot of people would be willing to try it out as soon as you have you",
    "start": "3065000",
    "end": "3070960"
  },
  {
    "text": "have it so if you need yeah I'm on your",
    "start": "3070960",
    "end": "3076440"
  },
  {
    "text": "slack so I'll be sure to send a link when we are available yeah I think if you need users there will be plenty in",
    "start": "3076440",
    "end": "3083240"
  },
  {
    "text": "this group yeah thank you very much",
    "start": "3083240",
    "end": "3088559"
  },
  {
    "text": "all right uh we do have time for one or two more questions if anyone",
    "start": "3089640",
    "end": "3096119"
  },
  {
    "text": "has or if not uh thank you very much again U Marlon Skyler for for presenting",
    "start": "3097720",
    "end": "3104160"
  },
  {
    "text": "and this was very well attended and very interesting lots of questions and um I think we can stay in",
    "start": "3104160",
    "end": "3111760"
  },
  {
    "text": "touch and maybe schedule something um later maybe after con um with with an",
    "start": "3111760",
    "end": "3118160"
  },
  {
    "text": "update it seems that this will move pretty fast so yeah happy to do that y",
    "start": "3118160",
    "end": "3125480"
  },
  {
    "text": "yeah thank you all right yeah thank you very much and uh for everyone else uh",
    "start": "3125480",
    "end": "3131839"
  },
  {
    "text": "thanks a lot for attending um we will have our next uh meeting in two weeks",
    "start": "3131839",
    "end": "3138839"
  },
  {
    "text": "and very much on topic because we will get an update on uh on the job set and",
    "start": "3138839",
    "end": "3144599"
  },
  {
    "text": "the news in that area um um so yeah I think I think it's a perfect followup",
    "start": "3144599",
    "end": "3151200"
  },
  {
    "text": "for this and uh looking forward to see you all again thank you very much",
    "start": "3151200",
    "end": "3157760"
  },
  {
    "text": "byebye bye thank you thank you everyone byebye",
    "start": "3157760",
    "end": "3163720"
  }
]