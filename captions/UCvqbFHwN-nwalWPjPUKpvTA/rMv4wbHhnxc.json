[
  {
    "text": "so welcome to our presentation on how to scale job triggering with a distributed",
    "start": "640",
    "end": "6240"
  },
  {
    "text": "schuer and we're going to present it in the context of the daer project and how that help us solve some scaling problems",
    "start": "6240",
    "end": "12880"
  },
  {
    "text": "that we had in the project uh my name is Arthur Susan and",
    "start": "12880",
    "end": "19320"
  },
  {
    "text": "I'm Cassie Coyle a software engineer at diagrid where I am an approver for the",
    "start": "19320",
    "end": "24640"
  },
  {
    "text": "Dappa runtime project along with the Java yeah and uh hi of a diagrid and",
    "start": "24640",
    "end": "32320"
  },
  {
    "text": "maintainer of dapper for The Last 5 Years um and so going to reach out to us",
    "start": "32320",
    "end": "38040"
  },
  {
    "text": "here's our contact information so here's some agenda of",
    "start": "38040",
    "end": "43520"
  },
  {
    "text": "what we're going to be talking about today um I'm going to start by in",
    "start": "43520",
    "end": "48800"
  },
  {
    "text": "showing a talk AIT Baer the architecture that we have go deeper into actors actor",
    "start": "48800",
    "end": "55879"
  },
  {
    "text": "reminders and daer workflow and that will be a s way for you to understand",
    "start": "55879",
    "end": "61640"
  },
  {
    "text": "why we had this needed AER project um and it might also be helpful for the",
    "start": "61640",
    "end": "67119"
  },
  {
    "text": "project you have if you're having similar",
    "start": "67119",
    "end": "71759"
  },
  {
    "text": "problems okay so Inu to Dapper um is anyone here familiar with the Dapper",
    "start": "72720",
    "end": "79520"
  },
  {
    "text": "project okay quite a few is anyone running Dapper in production a few some of half production",
    "start": "79520",
    "end": "87240"
  },
  {
    "text": "what is what is half production products not ready yet okay so in development okay I get that so it's St",
    "start": "87240",
    "end": "94320"
  },
  {
    "text": "by Booth if you want to talk about that too okay uh so for those who don't know",
    "start": "94320",
    "end": "100759"
  },
  {
    "text": "I'm going to give a quick overview so imagine that you have an application with M micro services and you want to do",
    "start": "100759",
    "end": "107600"
  },
  {
    "text": "service inv location uh you usually have to configure your application to know the endpoint of each service um or us a",
    "start": "107600",
    "end": "114880"
  },
  {
    "text": "service MH in case it's Dapper you make a API call to the Dapper API say calls",
    "start": "114880",
    "end": "120640"
  },
  {
    "text": "call service b or call service C or a and Dapper will handle the invocation for you we do mtls we also do retries",
    "start": "120640",
    "end": "129080"
  },
  {
    "text": "all that configurable including tracing as well and metrics the same thing happens for Pub sub when you do a",
    "start": "129080",
    "end": "135040"
  },
  {
    "text": "publisher message you can publish to Kafka or publish to a service bus um or publish to sqs all that distracted from",
    "start": "135040",
    "end": "143480"
  },
  {
    "text": "the application because daa offers the API for you um on top of those",
    "start": "143480",
    "end": "148680"
  },
  {
    "text": "Primitives we also offer for actors with virtual actors and also workflow which",
    "start": "148680",
    "end": "154080"
  },
  {
    "text": "was built on top of um actors uh so this like a high level overview of what daer can do for you and",
    "start": "154080",
    "end": "160920"
  },
  {
    "text": "then you can check our website for more information here's some some of the apis we have jobs zi we're going to talk",
    "start": "160920",
    "end": "167760"
  },
  {
    "text": "about today workflow disb law configuration Secrets observability",
    "start": "167760",
    "end": "174280"
  },
  {
    "text": "actors uh what is that uh binding Pub sub State Management and service",
    "start": "174280",
    "end": "180760"
  },
  {
    "text": "invocation and all that runs on kubernetes so daer runs as a separate process as a side car um you can also",
    "start": "180760",
    "end": "187640"
  },
  {
    "text": "run without kuties if you want but it's we really buildt for kues as well and I'm going to show you why um we also",
    "start": "187640",
    "end": "194040"
  },
  {
    "text": "offer sdks on different programming languages and you can talk via HTP or JPC if you want to skip SDK you also can",
    "start": "194040",
    "end": "201920"
  },
  {
    "text": "do that just talk directly with apis uh so check out more information",
    "start": "201920",
    "end": "207879"
  },
  {
    "text": "about dapper and Dapper recently graduated",
    "start": "207879",
    "end": "213280"
  },
  {
    "text": "yay so these are some of stats of our project uh we are now",
    "start": "213280",
    "end": "218680"
  },
  {
    "text": "14th project out of all the s projects we have quite a few contributors quite a few Stars uh been going on for five",
    "start": "218680",
    "end": "225920"
  },
  {
    "text": "years like I mentioned uh the ders architecture so",
    "start": "225920",
    "end": "231599"
  },
  {
    "text": "this present will be most Focus focused on running dper on kubernetes okay",
    "start": "231599",
    "end": "239879"
  },
  {
    "text": "imag you have a commun cluster and and in this case it install D install in the",
    "start": "239879",
    "end": "245360"
  },
  {
    "text": "Dapper system name space there are a few Contra plane Services you're going to notice one of",
    "start": "245360",
    "end": "251680"
  },
  {
    "text": "them is a side car injector which is a mutating web hook that will modify the part template to",
    "start": "251680",
    "end": "258560"
  },
  {
    "text": "inject the diaper side car with the correct configurations based on your annotations on your",
    "start": "258560",
    "end": "264479"
  },
  {
    "text": "deployment or also it can be a job or state full set",
    "start": "264479",
    "end": "270320"
  },
  {
    "text": "um so that will be automatically mutated and injected for you we have Sentry we",
    "start": "270320",
    "end": "275600"
  },
  {
    "text": "handles mtls certificates uh implement this PV uh protocol we have the operator",
    "start": "275600",
    "end": "282360"
  },
  {
    "text": "which is the interface between the S car and the kuber apis the sard does not",
    "start": "282360",
    "end": "288120"
  },
  {
    "text": "have a direct dependency on the kuber API it's agnostic but because of",
    "start": "288120",
    "end": "294520"
  },
  {
    "text": "deploying crimin is the operator is what allows the sard to see which components you have configured and other",
    "start": "294520",
    "end": "301400"
  },
  {
    "text": "settings we have placement which we're going to talk more about today which keeps um uh almost like a we call it",
    "start": "301400",
    "end": "309919"
  },
  {
    "text": "placement table which maps for each actual type which sidecar instances HS",
    "start": "309919",
    "end": "316000"
  },
  {
    "text": "that particular OT type and and that's going to be important for today's talk and brand new uh we have the schul which",
    "start": "316000",
    "end": "324080"
  },
  {
    "text": "handles the storage and triggering of crown jobs like type of uh uh uh",
    "start": "324080",
    "end": "332120"
  },
  {
    "text": "triggers and for the first time it has a persistent storage so placement although",
    "start": "332120",
    "end": "337440"
  },
  {
    "text": "is a stateful set and has uh stor storage as well the the table it can be",
    "start": "337440",
    "end": "342680"
  },
  {
    "text": "recreated easily via just starting over versus now in schul the jobs are",
    "start": "342680",
    "end": "348840"
  },
  {
    "text": "persisted there and it has to be durable so it's the first time that we have actually a contour plane that requires",
    "start": "348840",
    "end": "356280"
  },
  {
    "text": "durability and and again in the Pod of your application you're going to have have the S car and we call dard D",
    "start": "356280",
    "end": "362800"
  },
  {
    "text": "um okay let's go to the next step so actors how actors work in Dapper who who",
    "start": "362800",
    "end": "370440"
  },
  {
    "text": "here is familiar with the concept of virtual actors okay one person okay great",
    "start": "370440",
    "end": "376720"
  },
  {
    "text": "um that's basically implementing virtual actors which means that you don't need",
    "start": "376720",
    "end": "382080"
  },
  {
    "text": "to instantiate is the kind of virtually always present you can invoke any actor",
    "start": "382080",
    "end": "388160"
  },
  {
    "text": "at any time and we figure out which instance is supposed to receive the invocation and there is a single",
    "start": "388160",
    "end": "394280"
  },
  {
    "text": "threaded concurrency guarantee for each invocation with turn-based concurrency so if you two calls to the",
    "start": "394280",
    "end": "400599"
  },
  {
    "text": "same act ID takes place one of them will win and get executed before the next one and that also allow some kind of state",
    "start": "400599",
    "end": "408080"
  },
  {
    "text": "caching in this example if the application is making vocation to an addor on on the the number one here",
    "start": "408080",
    "end": "416639"
  },
  {
    "text": "number one Arrow it will figure out that it's not hosted here actor number three",
    "start": "416639",
    "end": "423680"
  },
  {
    "text": "order actor as our actor type and three is the actor ID is not hosted here and",
    "start": "423680",
    "end": "428960"
  },
  {
    "text": "it will redirect the call to the next side car that actually hosted as you can see and this one we actually the one",
    "start": "428960",
    "end": "435319"
  },
  {
    "text": "going to be process request all that is done behind the scenes for you uh is just looks like an invocation to method",
    "start": "435319",
    "end": "442039"
  },
  {
    "text": "in a class and it's highly recommend that you do that using one of the sdks that implements accur",
    "start": "442039",
    "end": "449800"
  },
  {
    "text": "and here you're going to see the placement Service uh being used so every daper side car even if it does not host",
    "start": "449800",
    "end": "458440"
  },
  {
    "text": "actors has a copy of the placement table uh the ones that host actors you",
    "start": "458440",
    "end": "466240"
  },
  {
    "text": "it kind of analysis should placement service I am um side car I don't know",
    "start": "466240",
    "end": "472240"
  },
  {
    "text": "one two three with this IP address and I'm hosting act type uh in this example",
    "start": "472240",
    "end": "479159"
  },
  {
    "text": "uh actor type just actor types this act type is an example but could say order type or any other actor type you have",
    "start": "479159",
    "end": "487000"
  },
  {
    "text": "and placement service will basically keep a map the exacto type has this this",
    "start": "487000",
    "end": "492599"
  },
  {
    "text": "and that instance serving it and that will be used for consistent",
    "start": "492599",
    "end": "498479"
  },
  {
    "text": "hashing uh and since because that is copied or disseminated to every single",
    "start": "498560",
    "end": "504440"
  },
  {
    "text": "side car when you invoke an actor in Dapper it's a local decision the side",
    "start": "504440",
    "end": "509919"
  },
  {
    "text": "car you don't have to do a lookup on the placement service it's a local decision and then you redirect to whatever side car you need to and that Al of course is",
    "start": "509919",
    "end": "517120"
  },
  {
    "text": "also done with mtls now we go to actual reminders what",
    "start": "517120",
    "end": "522959"
  },
  {
    "text": "is actual reminders act reminders imagine like a crown job for actors you can say that they have a order to be",
    "start": "522959",
    "end": "528760"
  },
  {
    "text": "processed and you want to have a reminder to try to process the payment every hour",
    "start": "528760",
    "end": "534800"
  },
  {
    "text": "for example or try to uh do the accounting reconciliation for the order",
    "start": "534800",
    "end": "541200"
  },
  {
    "text": "and that has to be done on a schedule for whatever reason and it could could be also be like a single trigger let's",
    "start": "541200",
    "end": "547040"
  },
  {
    "text": "say call me within an hour but call it only",
    "start": "547040",
    "end": "552200"
  },
  {
    "text": "once uh we do support that kind of calls in the actor reminders but basically the",
    "start": "552200",
    "end": "557519"
  },
  {
    "text": "way you do it is you make an invocation to the side car say register this",
    "start": "557519",
    "end": "562880"
  },
  {
    "text": "reminder and if you're using the D SDK that is done behind the scenes for you don't need to to worry too much about",
    "start": "562880",
    "end": "568079"
  },
  {
    "text": "the API and the side car we basically read and write reminders from the Stage",
    "start": "568079",
    "end": "574760"
  },
  {
    "text": "store and that stage store is used to also load the reminders when the side",
    "start": "574760",
    "end": "581480"
  },
  {
    "text": "car starts up and because the reminders can be for multiple",
    "start": "581480",
    "end": "587240"
  },
  {
    "text": "actors that is partition across all the side cars and only the reminders that",
    "start": "587240",
    "end": "594600"
  },
  {
    "text": "that belong to the to the acces that are on by this side car are they going to be",
    "start": "594600",
    "end": "601120"
  },
  {
    "text": "loaded and the other ones are going be loaded by the other one and the other one and if there's any kind of change of membership there's a rebalancing which",
    "start": "601120",
    "end": "608200"
  },
  {
    "text": "means some reminders might move from one host to another and uh that's a high level of",
    "start": "608200",
    "end": "615360"
  },
  {
    "text": "how actor reminders work today before the introduction of the schedule",
    "start": "615360",
    "end": "620880"
  },
  {
    "text": "service but there's a problem the actual Stage store API does not offer a list",
    "start": "620880",
    "end": "627880"
  },
  {
    "text": "API and even with the list is offer consistency but the way it was really",
    "start": "627880",
    "end": "633040"
  },
  {
    "text": "designed is that there will be a record with like actors the Act and the the",
    "start": "633040",
    "end": "639320"
  },
  {
    "text": "actor type and the Act and the sorry actors and the actor type right after in this case customers actor type and then",
    "start": "639320",
    "end": "646560"
  },
  {
    "text": "would list every single reminder as you can see that would quickly fall short because of",
    "start": "646560",
    "end": "653680"
  },
  {
    "text": "limitations of databases of record size if you use C DB or D DB on many other",
    "start": "653680",
    "end": "660200"
  },
  {
    "text": "diabases there's a limit of how big this can be so users were hitting this",
    "start": "660200",
    "end": "665680"
  },
  {
    "text": "limitation really quickly and could not Rish many reminders so we had like a a",
    "start": "665680",
    "end": "671560"
  },
  {
    "text": "new design or I would say almost like a workaround where we would partition the data into multiple records and you could",
    "start": "671560",
    "end": "678600"
  },
  {
    "text": "decide how many partitions you have up front like let's say seven 10 and that would basically spread the list of",
    "start": "678600",
    "end": "684959"
  },
  {
    "text": "remainers across mulle records so that alleviate the the the problem and here's",
    "start": "684959",
    "end": "690880"
  },
  {
    "text": "you can see an example how there a random ID and how this random ID is used to separate this metadata instance from",
    "start": "690880",
    "end": "697200"
  },
  {
    "text": "others and there's some reason why this Mead dat is here um it's not really the",
    "start": "697200",
    "end": "702399"
  },
  {
    "text": "reason for this uh the topic for this presentation but that allows us to that",
    "start": "702399",
    "end": "707680"
  },
  {
    "text": "when you change the partition number it will make a new ID for this and avoid C of race conditions um um in the",
    "start": "707680",
    "end": "715519"
  },
  {
    "text": "database um but that is just an implementation detail so we had some still some really big limitations the",
    "start": "715519",
    "end": "721279"
  },
  {
    "text": "active reminders even after this a really low throughput so you can only R your reminders at the rate of",
    "start": "721279",
    "end": "728279"
  },
  {
    "text": "45 give your take TPS so ring 45 MERS per second is really really low uh from",
    "start": "728279",
    "end": "735199"
  },
  {
    "text": "most production workloads and it canot scale horizontally or vertically there's nothing you can do to scale the TPS um",
    "start": "735199",
    "end": "743560"
  },
  {
    "text": "adding more side cars does not help uh adding more partitions does not help too",
    "start": "743560",
    "end": "749560"
  },
  {
    "text": "much either with the TPS it can only help with the um size with how many you",
    "start": "749560",
    "end": "755120"
  },
  {
    "text": "can store but not with the TPS so we kind of end up hitting like a",
    "start": "755120",
    "end": "760160"
  },
  {
    "text": "practical limit of like a thousand reminders that's how much you could register and again and rebalancing was",
    "start": "760160",
    "end": "766600"
  },
  {
    "text": "required every time an application part goes up and down um on a rolling deployment that you would do in your",
    "start": "766600",
    "end": "772279"
  },
  {
    "text": "cluster that would cause a rebalance and the the reminder might be down for for a few seconds",
    "start": "772279",
    "end": "780399"
  },
  {
    "text": "daer workflow now let's go really quick on daer workflow because daer workflow uses accors behind the scenes is called",
    "start": "780600",
    "end": "787600"
  },
  {
    "text": "internal actors what does it mean it means that it has the runtime uh uh implemented but the actor function",
    "start": "787600",
    "end": "795440"
  },
  {
    "text": "implementations the math implementation of the actor are no longer in the application they are in the daper side",
    "start": "795440",
    "end": "801920"
  },
  {
    "text": "car itself it's still used on the application for you to implement the the",
    "start": "801920",
    "end": "807720"
  },
  {
    "text": "activities so workflow flow activities or the authoring of the workflow they are all in the SDK in the specific SDK",
    "start": "807720",
    "end": "815160"
  },
  {
    "text": "for Apper workflow that you can see in the most programming languages that we listed early",
    "start": "815160",
    "end": "820600"
  },
  {
    "text": "on and actor and the Apper workflow user reminders there's a reminder called",
    "start": "820600",
    "end": "826320"
  },
  {
    "text": "start there's a reminder for new event there's a reminder for timer and there's a reminder for run activity and all that",
    "start": "826320",
    "end": "833079"
  },
  {
    "text": "is should guarantee the the the long running uh capabilities of of workflows",
    "start": "833079",
    "end": "840040"
  },
  {
    "text": "and and the side car and the and the application they they talk to each other",
    "start": "840040",
    "end": "845440"
  },
  {
    "text": "through a Jey streaming where you can get the work item stream you you you get",
    "start": "845440",
    "end": "851320"
  },
  {
    "text": "work items and gets executed on the application and then the results are send back to the Dapper card this a high",
    "start": "851320",
    "end": "859279"
  },
  {
    "text": "level of how Dapper workflow work um it's not a pres about Dr workflow either",
    "start": "859279",
    "end": "864560"
  },
  {
    "text": "there there are some nice talks about that uh if you can go online you can see on the daer YouTube channel there's one",
    "start": "864560",
    "end": "870959"
  },
  {
    "text": "really good from Chris Gillum from Microsoft and still have limitations we",
    "start": "870959",
    "end": "877680"
  },
  {
    "text": "found like a practical limit of like about 100 activities and it could only run with with two replicas so it was",
    "start": "877680",
    "end": "884240"
  },
  {
    "text": "really Alpha stage uh at that point and U Cassie and the open source team of",
    "start": "884240",
    "end": "890880"
  },
  {
    "text": "diag has been working really hard to uh redesign um uh from a lot of the",
    "start": "890880",
    "end": "896800"
  },
  {
    "text": "decision that were made early on to make this scale Cassie all righty let's get into",
    "start": "896800",
    "end": "905440"
  },
  {
    "text": "theuer work okay so the scheduler is a brand new control plane service deployed",
    "start": "905440",
    "end": "911720"
  },
  {
    "text": "by default with a Dapper knit it runs in kubernetes mode and Standalone mode as a",
    "start": "911720",
    "end": "917279"
  },
  {
    "text": "single instance or in ha mode so one or three instances respectfully the",
    "start": "917279",
    "end": "922920"
  },
  {
    "text": "capabilities of the scheduler are to store jobs to be triggered at some point in the future while guaranteeing that",
    "start": "922920",
    "end": "929920"
  },
  {
    "text": "only one scheduler is going to send back that job so you don't have to worry about you know multiple schedulers",
    "start": "929920",
    "end": "936440"
  },
  {
    "text": "sending back the same job and I'll explain how we do that here in a bit",
    "start": "936440",
    "end": "942040"
  },
  {
    "text": "implementation wise the scheduler includes an embedded ETD database and an",
    "start": "942040",
    "end": "947639"
  },
  {
    "text": "internal cron scheduling Library which is the glue that makes it all",
    "start": "947639",
    "end": "953199"
  },
  {
    "text": "work design decisions for the scheduler are as follows it is to be an orchest",
    "start": "953199",
    "end": "959120"
  },
  {
    "text": "Ator not executor so the scheduler um from the Schuler's perspective it simply",
    "start": "959120",
    "end": "964880"
  },
  {
    "text": "takes in a job and then it sends it back at Trigger Time and then like you can",
    "start": "964880",
    "end": "970120"
  },
  {
    "text": "you know register a Handler Funk um using an SDK if you want to go execute you know the specific business logic but",
    "start": "970120",
    "end": "977360"
  },
  {
    "text": "from the scheduler it just sends it back to the side car and then we do guarantee at least one um job execution this was",
    "start": "977360",
    "end": "984600"
  },
  {
    "text": "for workflow as Archer had mentioned for durability and we did have a bias",
    "start": "984600",
    "end": "990360"
  },
  {
    "text": "towards durability and scaling over the clock time Precision so we guarantee",
    "start": "990360",
    "end": "996000"
  },
  {
    "text": "that a job is never going to be invoked before it schedule is due but we don't necessarily guarantee you know a cealing",
    "start": "996000",
    "end": "1002639"
  },
  {
    "text": "time for when it's invoked after that um due time is reached and this is because",
    "start": "1002639",
    "end": "1007720"
  },
  {
    "text": "you know it's possible to get behind on triggers for whatever reason but we do",
    "start": "1007720",
    "end": "1012880"
  },
  {
    "text": "guarantee that we will never lose a trigger and we do catch up but that clock time Precision you know might get",
    "start": "1012880",
    "end": "1019199"
  },
  {
    "text": "a tiny bit behind and then theuer was written to be",
    "start": "1019199",
    "end": "1024558"
  },
  {
    "text": "generic for multi- job um like purpose so it's extensible we can create various",
    "start": "1024559",
    "end": "1031038"
  },
  {
    "text": "types of jobs we'll go into that here in a bit too and scale and performance were",
    "start": "1031039",
    "end": "1037520"
  },
  {
    "text": "really key for us when we were designing and implementing the scheduler to you know scale active reminders and",
    "start": "1037520",
    "end": "1045280"
  },
  {
    "text": "workflows now let's get into how does it work so the scheduler is you know a single",
    "start": "1045280",
    "end": "1051799"
  },
  {
    "text": "binary that includes an embedded ETD database so you won't see you know multiple binaries it's just one",
    "start": "1051799",
    "end": "1059559"
  },
  {
    "text": "scheduler and then that does include the cron scheduling Library as I mentioned it is the glue to kind of make it all",
    "start": "1059559",
    "end": "1068480"
  },
  {
    "text": "work and here is actually the go code for how we are spinning up our schedu",
    "start": "1068679",
    "end": "1074520"
  },
  {
    "text": "service the concurrency new Runner manager simply spins up go routines and",
    "start": "1074520",
    "end": "1079720"
  },
  {
    "text": "kind of manages the context for us and so we have server. new where we take in those server options like Port the",
    "start": "1079720",
    "end": "1086679"
  },
  {
    "text": "listen address all the way down to the ETD config where of course we do provide same defaults so you really don't have",
    "start": "1086679",
    "end": "1093640"
  },
  {
    "text": "to do anything for it all to work out of the box but you can like update the data",
    "start": "1093640",
    "end": "1098720"
  },
  {
    "text": "directory and whatnot if you choose and then of course we run server. run and",
    "start": "1098720",
    "end": "1103919"
  },
  {
    "text": "that runs our scheduler the ellipses is just you know spinning up our health server metrics exporter security",
    "start": "1103919",
    "end": "1110679"
  },
  {
    "text": "provider those types of things the embedded ETD um of course is",
    "start": "1110679",
    "end": "1116960"
  },
  {
    "text": "in our scheduler we all know that etcd is a distributed key value store and",
    "start": "1116960",
    "end": "1123280"
  },
  {
    "text": "that is where our jobs live so when you schedule a job it lives in ETD and then",
    "start": "1123280",
    "end": "1129159"
  },
  {
    "text": "you know at trigger time we have the cron Library helping to facilitate things with ETD um we do get that data",
    "start": "1129159",
    "end": "1136320"
  },
  {
    "text": "consistency and replication out of the box so you see the HDs are you know communicating with each other running",
    "start": "1136320",
    "end": "1142919"
  },
  {
    "text": "raft under the hood and maintaining the Quorum for us of the data and then with",
    "start": "1142919",
    "end": "1149080"
  },
  {
    "text": "that every single scheduler has the complete set of all jobs because of you",
    "start": "1149080",
    "end": "1155039"
  },
  {
    "text": "know how FTV works and then we are persisting data by default so if all the schedulers go down you know our data is",
    "start": "1155039",
    "end": "1162320"
  },
  {
    "text": "persisted um all righty this is the go code for how we are creating that",
    "start": "1162320",
    "end": "1168080"
  },
  {
    "text": "embedded CD of course we have our import and then we're simply calling start at CD giving it our um config and of course",
    "start": "1168080",
    "end": "1176799"
  },
  {
    "text": "watching for that context and shutting down appropriately these are what the records",
    "start": "1176799",
    "end": "1183640"
  },
  {
    "text": "look like inside our etcd um the Dapper is the name space here leadership is for",
    "start": "1183640",
    "end": "1189960"
  },
  {
    "text": "our leadership key it is how we are dividing up jobs among schedulers I will",
    "start": "1189960",
    "end": "1195559"
  },
  {
    "text": "explain more on that here in a bit just know it exists and then we have 012 which is the like the replica ID of the",
    "start": "1195559",
    "end": "1202039"
  },
  {
    "text": "scheduler and then three would be you know the total so in this case I have three schedulers running in ha",
    "start": "1202039",
    "end": "1209400"
  },
  {
    "text": "mode when you schedule a job via the jobs API this is what that record looks like you'll see Dapper jobs app and then",
    "start": "1209400",
    "end": "1217480"
  },
  {
    "text": "you know so forth this is what an actor reminder type of job looks like um as arur had",
    "start": "1217480",
    "end": "1225120"
  },
  {
    "text": "mentioned you know now our reminders are going to live in here so we can scale and then these are the actor reminders",
    "start": "1225120",
    "end": "1231000"
  },
  {
    "text": "for our workflows with a like a zero um second due time you can see we have a",
    "start": "1231000",
    "end": "1237200"
  },
  {
    "text": "start event like a run activity and a new event here that's a complete set not um shown",
    "start": "1237200",
    "end": "1244799"
  },
  {
    "text": "is a job counter which is how we are tracking the state of triggered jobs over",
    "start": "1244799",
    "end": "1251280"
  },
  {
    "text": "time now let's talk about the internal cron scheduling Library this was a repo",
    "start": "1251280",
    "end": "1257600"
  },
  {
    "text": "design specific for a highly distributed highly concurrent environment to enable",
    "start": "1257600",
    "end": "1263360"
  },
  {
    "text": "the scalable distributed job management um and this gives us Dynamic",
    "start": "1263360",
    "end": "1268880"
  },
  {
    "text": "job partition leadership coordination which is a lot of words to say job ownership and how this works is the jobs",
    "start": "1268880",
    "end": "1277520"
  },
  {
    "text": "are split up such that each scheduler owns a subset of all the jobs to trigger",
    "start": "1277520",
    "end": "1283760"
  },
  {
    "text": "back and then this enables us to have low distribution because you know as you",
    "start": "1283760",
    "end": "1289159"
  },
  {
    "text": "scale out and have more schedulers each Schuler owns like a smaller subset of",
    "start": "1289159",
    "end": "1294559"
  },
  {
    "text": "all the jobs to trigger so we're actually becoming like more performant as you scale as",
    "start": "1294559",
    "end": "1299600"
  },
  {
    "text": "[Music] well the Chom Library includes various packages to make it all work there's a",
    "start": "1299600",
    "end": "1306799"
  },
  {
    "text": "nice illustration I made to kind of show what they all look like starting with a job CU which is an inmemory cue of our",
    "start": "1306799",
    "end": "1314200"
  },
  {
    "text": "jobs it's used to manage the scheduling and triggering of them we have a counter",
    "start": "1314200",
    "end": "1320000"
  },
  {
    "text": "it's that counter key that is used to track the state over time so if you have you know 10 repeats two repeats we are",
    "start": "1320000",
    "end": "1326080"
  },
  {
    "text": "keeping track with a counter we have a leadership package",
    "start": "1326080",
    "end": "1331600"
  },
  {
    "text": "which is essentially job ownership as I mentioned with a caveat that every",
    "start": "1331600",
    "end": "1336760"
  },
  {
    "text": "scheduler is a peer there is no leader there is no follower essentially they are all equal and each scheduler is a",
    "start": "1336760",
    "end": "1345840"
  },
  {
    "text": "leader for its own partition of the jobs which is the subset of the jobs and it's",
    "start": "1345840",
    "end": "1351200"
  },
  {
    "text": "responsible to trigger those back and then this is dynamically recalculated",
    "start": "1351200",
    "end": "1356640"
  },
  {
    "text": "such that will never have a loss of jobs you know if a Schuler goes down we're going to reshuffle the jobs amongst the",
    "start": "1356640",
    "end": "1361840"
  },
  {
    "text": "available schedulers then we have an Informer it",
    "start": "1361840",
    "end": "1367279"
  },
  {
    "text": "is watching our etcd job key space for those creates and deletes we have a graveyard package",
    "start": "1367279",
    "end": "1374679"
  },
  {
    "text": "which is really an optimization for the expired jobs so jobs have exceeded the time to live we don't have to do a queue",
    "start": "1374679",
    "end": "1382440"
  },
  {
    "text": "lookup which can be expensive based on you know how big our queue is and then it really prevents reprocessing of those",
    "start": "1382440",
    "end": "1389520"
  },
  {
    "text": "recently deleted keys and of course we have a garbage collector because when you create things",
    "start": "1389520",
    "end": "1396520"
  },
  {
    "text": "we have to delete them so this will bulk delete our keys to maintain that cleaned state so every 180 seconds we clean up",
    "start": "1396520",
    "end": "1404960"
  },
  {
    "text": "or um we have a sooner channel that will be triggered if we hit 500,000 garbage",
    "start": "1404960",
    "end": "1410840"
  },
  {
    "text": "keys to be collected and this is what it looks like to run our Kon Library it's pretty clean",
    "start": "1410840",
    "end": "1418799"
  },
  {
    "text": "we have crown. new where we take in those cron options like the client the Nam space which is Dapper and R case",
    "start": "1418799",
    "end": "1425039"
  },
  {
    "text": "which is the prefix of the keys we saw and then the partition ID and total that's like the replica ID and then of",
    "start": "1425039",
    "end": "1432360"
  },
  {
    "text": "course the total amount of the schedulers the trigger function is the function that gets executed of course at",
    "start": "1432360",
    "end": "1439200"
  },
  {
    "text": "at trigger time it's where we send the job back to it's exposed on the scheduler and then the replica data is",
    "start": "1439200",
    "end": "1445200"
  },
  {
    "text": "theuer host and Port this is a new thing that's going in and actively being worked on right now such that the",
    "start": "1445200",
    "end": "1451559"
  },
  {
    "text": "sidecar can get the complete set of all active schedulers um so it's not just a",
    "start": "1451559",
    "end": "1457679"
  },
  {
    "text": "static set of you know scheduler addresses and then we call cr. run and I",
    "start": "1457679",
    "end": "1463240"
  },
  {
    "text": "also like to add like some of you might be familiar with the go at CD Chrome Library it was created by rub fig and",
    "start": "1463240",
    "end": "1470080"
  },
  {
    "text": "then forked by calingo like a comp another company uh initially we had a fork of it but it changed so much that",
    "start": "1470080",
    "end": "1478159"
  },
  {
    "text": "basically just the name is is there it's not the same Library anymore okay let's talk about the job ownership",
    "start": "1478159",
    "end": "1485559"
  },
  {
    "text": "model which is that Dynamic leadership partitioning so in this case we have a",
    "start": "1485559",
    "end": "1490720"
  },
  {
    "text": "single scheduler and that scheduler is responsible for triggering every single job from Z to n when you have three",
    "start": "1490720",
    "end": "1497760"
  },
  {
    "text": "schedulers we're running in ha mode this is the exact line from our partitioner package we do the partition ID mod the",
    "start": "1497760",
    "end": "1505679"
  },
  {
    "text": "total partitions that is just like the scheduler ID mod the total um so roughly",
    "start": "1505679",
    "end": "1512760"
  },
  {
    "text": "we get an equivalent split of the jobs to you know the schedulers such that",
    "start": "1512760",
    "end": "1518000"
  },
  {
    "text": "schedulers own a subset of the jobs to trigger back sharing the",
    "start": "1518000",
    "end": "1523039"
  },
  {
    "text": "load and then because of this we actually don't know which scheduler owns which job",
    "start": "1523039",
    "end": "1528840"
  },
  {
    "text": "so all schedu or all schedulers all sidecars have to connect to all",
    "start": "1528840",
    "end": "1534480"
  },
  {
    "text": "schedulers to watch for the jobs and then that is actually on a streaming",
    "start": "1534480",
    "end": "1539640"
  },
  {
    "text": "connection um so the the scheduler has a connection pool of the available side",
    "start": "1539640",
    "end": "1545159"
  },
  {
    "text": "cars to send the job back to and we will round robin between the um the available",
    "start": "1545159",
    "end": "1553320"
  },
  {
    "text": "side cars for the same app ID so if you have more than one we're going to round robin between them and send the job back",
    "start": "1553320",
    "end": "1559760"
  },
  {
    "text": "on a streaming connection schedu resiliency you know we",
    "start": "1559760",
    "end": "1565159"
  },
  {
    "text": "have a lot baked in here we have that data persistence and replication which is a given from etcd so every scheduler",
    "start": "1565159",
    "end": "1572039"
  },
  {
    "text": "has a complete set of all the data we're persisting by default um so we won't have that downtime then we have that",
    "start": "1572039",
    "end": "1578840"
  },
  {
    "text": "Dynamic job partition leadership or just like the job ownership and distribution",
    "start": "1578840",
    "end": "1583960"
  },
  {
    "text": "where we are reshuffling we have our jobs being like dynamic reshuffled upon",
    "start": "1583960",
    "end": "1589360"
  },
  {
    "text": "them coming up and down and then sharing the load of course and then jobs are",
    "start": "1589360",
    "end": "1594919"
  },
  {
    "text": "always triggered because we have a persisted counter we are keeping track of as they're being",
    "start": "1594919",
    "end": "1600080"
  },
  {
    "text": "triggered and then we do have of course a failure policy so if a job is not",
    "start": "1600080",
    "end": "1605159"
  },
  {
    "text": "successfully sent back you can elect to drop it that's completely fine or we do by default have a consistent retry for",
    "start": "1605159",
    "end": "1612520"
  },
  {
    "text": "those client side errors and then for non-client side errors so say a side car",
    "start": "1612520",
    "end": "1618320"
  },
  {
    "text": "goes down and never comes back up but we still have jobs for that side car it's just going to live in a staging queue to",
    "start": "1618320",
    "end": "1624760"
  },
  {
    "text": "be held until there is a side car available to send the job back to the impact on you know the active",
    "start": "1624760",
    "end": "1633000"
  },
  {
    "text": "reminders and workflows was pretty substantial going back from version 113 to 114 which included the",
    "start": "1633000",
    "end": "1640640"
  },
  {
    "text": "scheduler we saw performance gains for after reminders um with schedulers in ha",
    "start": "1640640",
    "end": "1646720"
  },
  {
    "text": "mode so three of them we were able to schedule 50,000 active reminders with an average trigger QPS of roughly",
    "start": "1646720",
    "end": "1654480"
  },
  {
    "text": "4500 which was at least a 10x Improvement while keeping roughly the same QPS and then invoking the scheduler",
    "start": "1654480",
    "end": "1662640"
  },
  {
    "text": "jobs API directly we observed a QPS of up to 35,000 for triggering where QPS is",
    "start": "1662640",
    "end": "1669960"
  },
  {
    "text": "queries per second and going back so you see um for version 113 we had a QPS of",
    "start": "1669960",
    "end": "1676720"
  },
  {
    "text": "creating actual reminders of 50 now with Schuler that is 4,000 so an 80x",
    "start": "1676720",
    "end": "1683000"
  },
  {
    "text": "improvement with using the Schuler to store those reminders workflow performance numbers",
    "start": "1683000",
    "end": "1690399"
  },
  {
    "text": "um this is from running our performance tests pretty regularly so daily um we",
    "start": "1690399",
    "end": "1696120"
  },
  {
    "text": "have a test to run parallel workflow tests with a Max concurrent count of 60",
    "start": "1696120",
    "end": "1701559"
  },
  {
    "text": "to 90 workf flows we saw performance improvements that were 71% higher versus",
    "start": "1701559",
    "end": "1706760"
  },
  {
    "text": "the old reminder system would dropped by 44% it just couldn't keep up it couldn't handle the load and then a highs scale",
    "start": "1706760",
    "end": "1713799"
  },
  {
    "text": "test where we had a Max concurrent workflow count of 350 and we are running 1,400 iterations",
    "start": "1713799",
    "end": "1721000"
  },
  {
    "text": "we saw performance improvements that were 50% higher than the old reminder system where we can scale to millions of",
    "start": "1721000",
    "end": "1728720"
  },
  {
    "text": "reminders and jobs in the scheduler and we are at most limited by the storage",
    "start": "1728720",
    "end": "1733880"
  },
  {
    "text": "capabilities in etcd but not the throughput of those jobs and REM",
    "start": "1733880",
    "end": "1739360"
  },
  {
    "text": "ERS the Dapper jobs API is a brand new Alpha API you can schedule a job get a",
    "start": "1739360",
    "end": "1746960"
  },
  {
    "text": "job and delete a job and those jobs live in theuer so here's a curl for what it",
    "start": "1746960",
    "end": "1753360"
  },
  {
    "text": "would look like to schedule a job named text or test and um the data can be",
    "start": "1753360",
    "end": "1758559"
  },
  {
    "text": "literally anything I put my name there and the job is due in 3 seconds this is what is a job this is",
    "start": "1758559",
    "end": "1766840"
  },
  {
    "text": "the definition of our job it's the Proto message from our runtime code where a job will have data and then it can",
    "start": "1766840",
    "end": "1774080"
  },
  {
    "text": "optionally have you know a schedule repeat due time time to live and a failure policy so say you'll have a",
    "start": "1774080",
    "end": "1781760"
  },
  {
    "text": "production database backup job that you want to run monthly you could set you know Chron expression or you could set",
    "start": "1781760",
    "end": "1788480"
  },
  {
    "text": "the schedule to be at monthly or weekly or daily and then you know maybe you",
    "start": "1788480",
    "end": "1794120"
  },
  {
    "text": "just want it to run for a year if it's monthly then you could just set set 12 repeats I don't know that i' recommend",
    "start": "1794120",
    "end": "1800919"
  },
  {
    "text": "that but you could do it and then you know you could set a due time so a delayed start essentially maybe you want",
    "start": "1800919",
    "end": "1806720"
  },
  {
    "text": "it to start the first of next month that's how you could use a due time and then time to live is pretty",
    "start": "1806720",
    "end": "1813039"
  },
  {
    "text": "self-explanatory and then as is failure policy the jobs API is the section",
    "start": "1813039",
    "end": "1819080"
  },
  {
    "text": "between your app and the sidecar so via grpc and HTTP and then the side card",
    "start": "1819080",
    "end": "1825039"
  },
  {
    "text": "will send the job back at Trigger Time via this SL job event or SL job for HTTP",
    "start": "1825039",
    "end": "1830960"
  },
  {
    "text": "and the on job event for grpc and then with the SDK I mentioned this earlier but you can register a Handler Funk that",
    "start": "1830960",
    "end": "1838200"
  },
  {
    "text": "will just get called at Trigger Time once your app receives the job",
    "start": "1838200",
    "end": "1843360"
  },
  {
    "text": "back putting the whole thing together if you were to schedule a job called my job",
    "start": "1843360",
    "end": "1849039"
  },
  {
    "text": "it'll go to the sidecar then the sidecar will pick up the app ID in the namespace and then send it along to the scheduler",
    "start": "1849039",
    "end": "1855639"
  },
  {
    "text": "where it will live inside that etcd data Bas from there you know we have some cron Library magic that is doing its",
    "start": "1855639",
    "end": "1862120"
  },
  {
    "text": "thing and at trigger time we'll send it back to any of the same app ID like",
    "start": "1862120",
    "end": "1868360"
  },
  {
    "text": "Dapper instances on a streaming connection called watch jobs and then",
    "start": "1868360",
    "end": "1874120"
  },
  {
    "text": "from there Dapper will just send that back to your app in the future we will be allowing",
    "start": "1874120",
    "end": "1880279"
  },
  {
    "text": "even more types of jobs um we have a pub sub API so we will be enabling delayed",
    "start": "1880279",
    "end": "1885679"
  },
  {
    "text": "pubsub calls so you know publishing a message at a specific point in time in",
    "start": "1885679",
    "end": "1891760"
  },
  {
    "text": "the future that kind of thing and then scheduled service inv vacation another one of our apis enabling the schedule of",
    "start": "1891760",
    "end": "1899919"
  },
  {
    "text": "method calls you know between your applications and then of course autoscaling the scheduler which is just",
    "start": "1899919",
    "end": "1905600"
  },
  {
    "text": "a little bit more complicated because we have the ETD database inside um but",
    "start": "1905600",
    "end": "1910639"
  },
  {
    "text": "beyond three we'll you know take out the etcd and kind of scale as is and use the existing etcd instances that will come",
    "start": "1910639",
    "end": "1918320"
  },
  {
    "text": "and then optionally storing the job data separately if you don't want the data to live in ETD the cron stuff will but the",
    "start": "1918320",
    "end": "1925120"
  },
  {
    "text": "data can live you know in postgress or another state store if you're choosing and then of course for feature parody",
    "start": "1925120",
    "end": "1931399"
  },
  {
    "text": "with our Crown binding we will be allowing ACD job creation such that you can run",
    "start": "1931399",
    "end": "1937519"
  },
  {
    "text": "Cube apply DF to create a job I want to thank everyone in this",
    "start": "1937519",
    "end": "1944320"
  },
  {
    "text": "slide um the scheduler was a massive project was very very complicated and",
    "start": "1944320",
    "end": "1949600"
  },
  {
    "text": "took a lot of work so thank you to all the contributors and then thank you all for being here with us um we appreciate",
    "start": "1949600",
    "end": "1957639"
  },
  {
    "text": "your time and then our turn n will be available in the front if youall have any",
    "start": "1957639",
    "end": "1963919"
  },
  {
    "text": "questions thank you [Applause]",
    "start": "1965320",
    "end": "1971539"
  }
]