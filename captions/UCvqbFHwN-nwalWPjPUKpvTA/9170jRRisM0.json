[
  {
    "text": "okay uh good afternoon everyone uh my",
    "start": "5240",
    "end": "9840"
  },
  {
    "text": "name is Delia this is my partner we come",
    "start": "9840",
    "end": "13080"
  },
  {
    "text": "from bad thoughts today we are very",
    "start": "13080",
    "end": "15059"
  },
  {
    "text": "delighted to have this opportunity to",
    "start": "15059",
    "end": "17100"
  },
  {
    "text": "share our batch processing to you and",
    "start": "17100",
    "end": "19800"
  },
  {
    "text": "since batch process is a huge topic and",
    "start": "19800",
    "end": "22380"
  },
  {
    "text": "we will focus on the scheduling",
    "start": "22380",
    "end": "24300"
  },
  {
    "text": "especially for the performance and the",
    "start": "24300",
    "end": "26580"
  },
  {
    "text": "capabilities",
    "start": "26580",
    "end": "29180"
  },
  {
    "text": "I will show the agent that first I will",
    "start": "29779",
    "end": "32279"
  },
  {
    "text": "have a YouTube brief introduction about",
    "start": "32279",
    "end": "34380"
  },
  {
    "text": "our batch processing uh just about our",
    "start": "34380",
    "end": "36960"
  },
  {
    "text": "view overview and we will focus on the",
    "start": "36960",
    "end": "38760"
  },
  {
    "text": "problem with the Native Native schedule",
    "start": "38760",
    "end": "41340"
  },
  {
    "text": "and where do we develop a patterns",
    "start": "41340",
    "end": "44340"
  },
  {
    "text": "unified scheduling to support all of our",
    "start": "44340",
    "end": "46559"
  },
  {
    "text": "workload we go to the schedule a good",
    "start": "46559",
    "end": "49260"
  },
  {
    "text": "schedule we will give that to the",
    "start": "49260",
    "end": "50940"
  },
  {
    "text": "performance and capabilities and",
    "start": "50940",
    "end": "52860"
  },
  {
    "text": "including the evaluation at last we will",
    "start": "52860",
    "end": "56219"
  },
  {
    "text": "have a brief introduction about the",
    "start": "56219",
    "end": "58800"
  },
  {
    "text": "future work and open source",
    "start": "58800",
    "end": "62100"
  },
  {
    "text": "okay first of all I'd like to have uh",
    "start": "62100",
    "end": "65760"
  },
  {
    "text": "show you the ecosystem",
    "start": "65760",
    "end": "68760"
  },
  {
    "text": "any problem",
    "start": "68760",
    "end": "71479"
  },
  {
    "text": "sorry for the technical difficulties",
    "start": "73979",
    "end": "78020"
  },
  {
    "text": "I think you have to go to this place",
    "start": "94400",
    "end": "96659"
  },
  {
    "text": "settings again all right",
    "start": "96659",
    "end": "99960"
  },
  {
    "text": "sorry",
    "start": "99960",
    "end": "102438"
  },
  {
    "text": "okay uh this is our patch scheduling",
    "start": "116340",
    "end": "119700"
  },
  {
    "text": "ecosystem and I would like to divide",
    "start": "119700",
    "end": "122340"
  },
  {
    "text": "into four layers the first layer is the",
    "start": "122340",
    "end": "124380"
  },
  {
    "text": "hardware Hardware layer we manage a lot",
    "start": "124380",
    "end": "127320"
  },
  {
    "text": "of the",
    "start": "127320",
    "end": "128759"
  },
  {
    "text": "what happened",
    "start": "128759",
    "end": "131539"
  },
  {
    "text": "I think not this proper option just go",
    "start": "142980",
    "end": "146040"
  },
  {
    "text": "go slow assist",
    "start": "146040",
    "end": "149299"
  },
  {
    "text": "okay",
    "start": "151560",
    "end": "154160"
  },
  {
    "text": "uh okay for the hardware layout we use",
    "start": "154940",
    "end": "158040"
  },
  {
    "text": "connect is to manage a lot different",
    "start": "158040",
    "end": "159900"
  },
  {
    "text": "type of resource including many tabs GPU",
    "start": "159900",
    "end": "163319"
  },
  {
    "text": "and we also manage a lot of the CPUs and",
    "start": "163319",
    "end": "166680"
  },
  {
    "text": "we use hum to reduce our costs and we",
    "start": "166680",
    "end": "169500"
  },
  {
    "text": "also focus on the network bandwidth",
    "start": "169500",
    "end": "171900"
  },
  {
    "text": "scheduling since our training job and",
    "start": "171900",
    "end": "174060"
  },
  {
    "text": "cost consumer loss of the network",
    "start": "174060",
    "end": "175980"
  },
  {
    "text": "bandwidth and currently we already use",
    "start": "175980",
    "end": "179040"
  },
  {
    "text": "the kubernetes to manage all of our",
    "start": "179040",
    "end": "180780"
  },
  {
    "text": "results and provide different",
    "start": "180780",
    "end": "182160"
  },
  {
    "text": "obstruction to the workloads we will",
    "start": "182160",
    "end": "184800"
  },
  {
    "text": "place the group schedule which are a",
    "start": "184800",
    "end": "186900"
  },
  {
    "text": "unified schedule we call it good",
    "start": "186900",
    "end": "188819"
  },
  {
    "text": "schedule good is famous is famous for",
    "start": "188819",
    "end": "192060"
  },
  {
    "text": "mathematics and hoopla poster or",
    "start": "192060",
    "end": "194519"
  },
  {
    "text": "incomplete new series which means that",
    "start": "194519",
    "end": "197159"
  },
  {
    "text": "if we want to want to develop a really",
    "start": "197159",
    "end": "200819"
  },
  {
    "text": "really good schedules while we have a",
    "start": "200819",
    "end": "203819"
  },
  {
    "text": "lot of change and the photo framework",
    "start": "203819",
    "end": "206239"
  },
  {
    "text": "will not have a lot of different",
    "start": "206239",
    "end": "208379"
  },
  {
    "text": "framework running on our kubernetes or",
    "start": "208379",
    "end": "211560"
  },
  {
    "text": "which are spread better operator we have",
    "start": "211560",
    "end": "214560"
  },
  {
    "text": "the spark for big data processing and we",
    "start": "214560",
    "end": "217800"
  },
  {
    "text": "use Link to support our machine learning",
    "start": "217800",
    "end": "220560"
  },
  {
    "text": "and Big Data stream processing and we",
    "start": "220560",
    "end": "222959"
  },
  {
    "text": "also launch a lot of tensorflow and",
    "start": "222959",
    "end": "224940"
  },
  {
    "text": "pathology Frameworks and we also",
    "start": "224940",
    "end": "226739"
  },
  {
    "text": "developed many spectrums machine",
    "start": "226739",
    "end": "228840"
  },
  {
    "text": "learning framework to achieve better",
    "start": "228840",
    "end": "230580"
  },
  {
    "text": "accuracy we we support a lot of business",
    "start": "230580",
    "end": "234120"
  },
  {
    "text": "including the recommendations search",
    "start": "234120",
    "end": "236099"
  },
  {
    "text": "advertisement and neutral workflow",
    "start": "236099",
    "end": "238560"
  },
  {
    "text": "process and compute vision",
    "start": "238560",
    "end": "242459"
  },
  {
    "text": "this slide I want to adjust all all a",
    "start": "242459",
    "end": "245640"
  },
  {
    "text": "large scale today we have more than one",
    "start": "245640",
    "end": "247920"
  },
  {
    "text": "billion job learning every day this is",
    "start": "247920",
    "end": "250500"
  },
  {
    "text": "just for the best jobs and this is the",
    "start": "250500",
    "end": "253739"
  },
  {
    "text": "best job especially for the spark Circle",
    "start": "253739",
    "end": "255599"
  },
  {
    "text": "which creates a lot of ports so we have",
    "start": "255599",
    "end": "259019"
  },
  {
    "text": "more than 130 million ports created and",
    "start": "259019",
    "end": "263100"
  },
  {
    "text": "schedule and deleted every day which",
    "start": "263100",
    "end": "265199"
  },
  {
    "text": "requires a lot of a big challenge for",
    "start": "265199",
    "end": "267900"
  },
  {
    "text": "the scheduling performance and currently",
    "start": "267900",
    "end": "270180"
  },
  {
    "text": "our batch job consumes up to 16 million",
    "start": "270180",
    "end": "273840"
  },
  {
    "text": "CPU calls and our biggest cluster manage",
    "start": "273840",
    "end": "277680"
  },
  {
    "text": "more than 20 uh 20 thousands notes since",
    "start": "277680",
    "end": "280979"
  },
  {
    "text": "we do not to manage a small cluster to",
    "start": "280979",
    "end": "283919"
  },
  {
    "text": "increase the resource fragmentations and",
    "start": "283919",
    "end": "286080"
  },
  {
    "text": "most of our clusters are more than five",
    "start": "286080",
    "end": "288000"
  },
  {
    "text": "thousands notes",
    "start": "288000",
    "end": "290400"
  },
  {
    "text": "and this picture will have a brief",
    "start": "290400",
    "end": "292860"
  },
  {
    "text": "evolution of our battery processing I",
    "start": "292860",
    "end": "295860"
  },
  {
    "text": "would like to divide it to three stage",
    "start": "295860",
    "end": "298259"
  },
  {
    "text": "the fourth stage begins at about 2016 uh",
    "start": "298259",
    "end": "303120"
  },
  {
    "text": "in the stage at States you are nothing",
    "start": "303120",
    "end": "305300"
  },
  {
    "text": "we built up online platform with",
    "start": "305300",
    "end": "308100"
  },
  {
    "text": "kubernetes and we build our offline",
    "start": "308100",
    "end": "310020"
  },
  {
    "text": "platform with hot yeah uh both the both",
    "start": "310020",
    "end": "313680"
  },
  {
    "text": "to Resource and manage systems where you",
    "start": "313680",
    "end": "315840"
  },
  {
    "text": "are no sure no resources and it has a",
    "start": "315840",
    "end": "319320"
  },
  {
    "text": "dedicated control plan and dedicated",
    "start": "319320",
    "end": "321060"
  },
  {
    "text": "cluster and and it has some problem",
    "start": "321060",
    "end": "324660"
  },
  {
    "text": "first is the resource fragmentations and",
    "start": "324660",
    "end": "327479"
  },
  {
    "text": "also the online service uh applied more",
    "start": "327479",
    "end": "330560"
  },
  {
    "text": "resources than uh sensor workload needs",
    "start": "330560",
    "end": "334320"
  },
  {
    "text": "and we also have a lot of resource with",
    "start": "334320",
    "end": "336960"
  },
  {
    "text": "during the off-peak traffic odds",
    "start": "336960",
    "end": "339000"
  },
  {
    "text": "especially for students Nets so we come",
    "start": "339000",
    "end": "341699"
  },
  {
    "text": "to stage two as about three years later",
    "start": "341699",
    "end": "343979"
  },
  {
    "text": "we uh we we try to shoot a note between",
    "start": "343979",
    "end": "346740"
  },
  {
    "text": "the Hadoop yeah and the kubernetes uh in",
    "start": "346740",
    "end": "349860"
  },
  {
    "text": "this uh how to how to solve the",
    "start": "349860",
    "end": "352259"
  },
  {
    "text": "complications we develop a coordinator",
    "start": "352259",
    "end": "354800"
  },
  {
    "text": "to solve the conflicts between the",
    "start": "354800",
    "end": "357840"
  },
  {
    "text": "kubernetes and the hadoopia we will",
    "start": "357840",
    "end": "360360"
  },
  {
    "text": "deployed the agent of magnetics and",
    "start": "360360",
    "end": "362160"
  },
  {
    "text": "Hardware in the same node and this",
    "start": "362160",
    "end": "364860"
  },
  {
    "text": "coordinator will allocate the results",
    "start": "364860",
    "end": "367440"
  },
  {
    "text": "took Nexus agent and also allocation",
    "start": "367440",
    "end": "369780"
  },
  {
    "text": "resource to the hardware during the test",
    "start": "369780",
    "end": "372060"
  },
  {
    "text": "the online workload consumes a lot of",
    "start": "372060",
    "end": "374220"
  },
  {
    "text": "CPU so the coordinator will allocate",
    "start": "374220",
    "end": "377280"
  },
  {
    "text": "most of you to the connectives but",
    "start": "377280",
    "end": "379440"
  },
  {
    "text": "during the Nets the cognitive will claim",
    "start": "379440",
    "end": "382979"
  },
  {
    "text": "most of CPU and tell the node manager or",
    "start": "382979",
    "end": "386759"
  },
  {
    "text": "I have resourced a lot of resource here",
    "start": "386759",
    "end": "388979"
  },
  {
    "text": "at least distribute more workload here",
    "start": "388979",
    "end": "391259"
  },
  {
    "text": "yeah at this stage we increased our CPU",
    "start": "391259",
    "end": "394319"
  },
  {
    "text": "utilization a lot and but it still has",
    "start": "394319",
    "end": "397620"
  },
  {
    "text": "some problem first ones the high",
    "start": "397620",
    "end": "399660"
  },
  {
    "text": "operation costs assist when you start to",
    "start": "399660",
    "end": "402539"
  },
  {
    "text": "need to manage to a scheduling and the",
    "start": "402539",
    "end": "405419"
  },
  {
    "text": "resource management system and the",
    "start": "405419",
    "end": "407639"
  },
  {
    "text": "resource elasticities they are not on",
    "start": "407639",
    "end": "409860"
  },
  {
    "text": "developed since the control plane is",
    "start": "409860",
    "end": "411900"
  },
  {
    "text": "still we still have two different",
    "start": "411900",
    "end": "414000"
  },
  {
    "text": "country plans so uh so the year before",
    "start": "414000",
    "end": "416340"
  },
  {
    "text": "last year we've come to stage three we",
    "start": "416340",
    "end": "419940"
  },
  {
    "text": "use the communities to manage all of our",
    "start": "419940",
    "end": "421919"
  },
  {
    "text": "resources and the normal hadoopia and",
    "start": "421919",
    "end": "425160"
  },
  {
    "text": "since the default schedule does not met",
    "start": "425160",
    "end": "428400"
  },
  {
    "text": "all requirements we develop a bad dance",
    "start": "428400",
    "end": "430380"
  },
  {
    "text": "University schedule and we also uh we",
    "start": "430380",
    "end": "434039"
  },
  {
    "text": "and we and we also keep the coordinators",
    "start": "434039",
    "end": "436919"
  },
  {
    "text": "this is some co-located agent and I will",
    "start": "436919",
    "end": "441180"
  },
  {
    "text": "have brief introduction at the end of US",
    "start": "441180",
    "end": "443400"
  },
  {
    "text": "Open Source",
    "start": "443400",
    "end": "444660"
  },
  {
    "text": "and in this stage we achieve health and",
    "start": "444660",
    "end": "447419"
  },
  {
    "text": "resource elasticity and utilization and",
    "start": "447419",
    "end": "450060"
  },
  {
    "text": "we also include our population",
    "start": "450060",
    "end": "452099"
  },
  {
    "text": "efficiency",
    "start": "452099",
    "end": "454620"
  },
  {
    "text": "and this this section I want to adjust",
    "start": "454620",
    "end": "457979"
  },
  {
    "text": "some problems with the native schedule",
    "start": "457979",
    "end": "459979"
  },
  {
    "text": "uh the first one is the stability since",
    "start": "459979",
    "end": "463020"
  },
  {
    "text": "we have more than",
    "start": "463020",
    "end": "465440"
  },
  {
    "text": "130 million posts start and scheduling",
    "start": "465440",
    "end": "469080"
  },
  {
    "text": "and delete every day so since the",
    "start": "469080",
    "end": "472259"
  },
  {
    "text": "bachelor so when care about the",
    "start": "472259",
    "end": "475860"
  },
  {
    "text": "stability a lot but but schedule that",
    "start": "475860",
    "end": "480479"
  },
  {
    "text": "does the serious processing so as The",
    "start": "480479",
    "end": "482880"
  },
  {
    "text": "Benchmark shoes in most cases the",
    "start": "482880",
    "end": "485639"
  },
  {
    "text": "support is no more than 200 seconds",
    "start": "485639",
    "end": "489620"
  },
  {
    "text": "the second thing I want to address is",
    "start": "489620",
    "end": "492840"
  },
  {
    "text": "about the capabilities since the",
    "start": "492840",
    "end": "494880"
  },
  {
    "text": "kinetics is designed was designed for",
    "start": "494880",
    "end": "497220"
  },
  {
    "text": "the microservice first it lacks some",
    "start": "497220",
    "end": "499560"
  },
  {
    "text": "important capabilities for the batch",
    "start": "499560",
    "end": "501240"
  },
  {
    "text": "processing as we all know are imported",
    "start": "501240",
    "end": "503639"
  },
  {
    "text": "the high performance gun scheduling and",
    "start": "503639",
    "end": "505620"
  },
  {
    "text": "we also care about many many special",
    "start": "505620",
    "end": "508680"
  },
  {
    "text": "features such as job lab Affinity which",
    "start": "508680",
    "end": "510960"
  },
  {
    "text": "means that I want to put all of the",
    "start": "510960",
    "end": "513000"
  },
  {
    "text": "ports in the same chain job to the same",
    "start": "513000",
    "end": "515820"
  },
  {
    "text": "network switch to achieve bet to achieve",
    "start": "515820",
    "end": "518640"
  },
  {
    "text": "better Network performance and we also",
    "start": "518640",
    "end": "521520"
  },
  {
    "text": "care about Metal technology scheduling",
    "start": "521520",
    "end": "523680"
  },
  {
    "text": "to improve the scheduling accuracy and",
    "start": "523680",
    "end": "526440"
  },
  {
    "text": "the better way and to connect also lack",
    "start": "526440",
    "end": "528959"
  },
  {
    "text": "some capabilities for located workloads",
    "start": "528959",
    "end": "531480"
  },
  {
    "text": "such as some some certain domain",
    "start": "531480",
    "end": "534000"
  },
  {
    "text": "resource fairness and practice based and",
    "start": "534000",
    "end": "536100"
  },
  {
    "text": "we also have some sorting lecture fascia",
    "start": "536100",
    "end": "539240"
  },
  {
    "text": "the last thing I want to address is",
    "start": "539240",
    "end": "541440"
  },
  {
    "text": "about the preemption especially we are",
    "start": "541440",
    "end": "544200"
  },
  {
    "text": "located",
    "start": "544200",
    "end": "546320"
  },
  {
    "text": "our workload together the cloud",
    "start": "546320",
    "end": "549180"
  },
  {
    "text": "complicated",
    "start": "549180",
    "end": "552180"
  },
  {
    "text": "for the next section please imagine to",
    "start": "553880",
    "end": "557519"
  },
  {
    "text": "share links thank you",
    "start": "557519",
    "end": "559740"
  },
  {
    "text": "okay uh thanks darling so now I'll be",
    "start": "559740",
    "end": "563339"
  },
  {
    "text": "talking a bit more about our scheduler",
    "start": "563339",
    "end": "565200"
  },
  {
    "text": "and uh yeah so we are we are set to open",
    "start": "565200",
    "end": "568560"
  },
  {
    "text": "source this scheduler soon uh and this",
    "start": "568560",
    "end": "571620"
  },
  {
    "text": "is sort of like a preview of some of the",
    "start": "571620",
    "end": "573360"
  },
  {
    "text": "features and capabilities and hopefully",
    "start": "573360",
    "end": "575700"
  },
  {
    "text": "also sharing some of the considerations",
    "start": "575700",
    "end": "577740"
  },
  {
    "text": "that we had or trying to build a",
    "start": "577740",
    "end": "579360"
  },
  {
    "text": "scheduler for scale can provide some",
    "start": "579360",
    "end": "581399"
  },
  {
    "text": "useful insights for everyone yep so",
    "start": "581399",
    "end": "584760"
  },
  {
    "text": "before I dive into the details uh maybe",
    "start": "584760",
    "end": "586860"
  },
  {
    "text": "you can talk about some of the goals",
    "start": "586860",
    "end": "588120"
  },
  {
    "text": "that we had so we wanted to create a",
    "start": "588120",
    "end": "590339"
  },
  {
    "text": "high performance scheduler with both",
    "start": "590339",
    "end": "592680"
  },
  {
    "text": "support for online offline scheduling",
    "start": "592680",
    "end": "594420"
  },
  {
    "text": "capabilities so this is to address the",
    "start": "594420",
    "end": "597120"
  },
  {
    "text": "scale that I think learned shared about",
    "start": "597120",
    "end": "598920"
  },
  {
    "text": "earlier yep so we also hope to use the",
    "start": "598920",
    "end": "601860"
  },
  {
    "text": "scheduler to increase the resource",
    "start": "601860",
    "end": "603779"
  },
  {
    "text": "utilization across our clusters and also",
    "start": "603779",
    "end": "606240"
  },
  {
    "text": "unify both our online offline resource",
    "start": "606240",
    "end": "608640"
  },
  {
    "text": "pools to achieve complete resource",
    "start": "608640",
    "end": "610500"
  },
  {
    "text": "elasticity yep so we believe the last",
    "start": "610500",
    "end": "612660"
  },
  {
    "text": "two goals are the key to reducing costs",
    "start": "612660",
    "end": "614640"
  },
  {
    "text": "within our company and achieving higher",
    "start": "614640",
    "end": "616920"
  },
  {
    "text": "efficiency",
    "start": "616920",
    "end": "619399"
  },
  {
    "text": "okay so now share some interesting",
    "start": "620760",
    "end": "623360"
  },
  {
    "text": "highlights of our architecture so",
    "start": "623360",
    "end": "626519"
  },
  {
    "text": "actually the girls scheduler is designed",
    "start": "626519",
    "end": "628380"
  },
  {
    "text": "as a distributed scheduling system so uh",
    "start": "628380",
    "end": "631500"
  },
  {
    "text": "the queue scheduler is actually a",
    "start": "631500",
    "end": "633180"
  },
  {
    "text": "non-distributed one and it runs with a",
    "start": "633180",
    "end": "636240"
  },
  {
    "text": "single component yeah but Godot actually",
    "start": "636240",
    "end": "638519"
  },
  {
    "text": "consists of three separate components so",
    "start": "638519",
    "end": "641040"
  },
  {
    "text": "we have the dispatcher the scheduler",
    "start": "641040",
    "end": "642779"
  },
  {
    "text": "instances and the binder",
    "start": "642779",
    "end": "645060"
  },
  {
    "text": "so this design actually allows us to",
    "start": "645060",
    "end": "647100"
  },
  {
    "text": "scale the number of scheduler instances",
    "start": "647100",
    "end": "648600"
  },
  {
    "text": "horizontally to achieve higher",
    "start": "648600",
    "end": "650220"
  },
  {
    "text": "throughputs yep but naturally having",
    "start": "650220",
    "end": "653279"
  },
  {
    "text": "multiple scheduling instances running at",
    "start": "653279",
    "end": "655200"
  },
  {
    "text": "the same time will lead to conflicts",
    "start": "655200",
    "end": "657360"
  },
  {
    "text": "right so for example you might have two",
    "start": "657360",
    "end": "659940"
  },
  {
    "text": "scheduled instances",
    "start": "659940",
    "end": "661380"
  },
  {
    "text": "no without any shared State they might",
    "start": "661380",
    "end": "663060"
  },
  {
    "text": "choose to assign the last CPU on the",
    "start": "663060",
    "end": "664920"
  },
  {
    "text": "Node to two different parts so this is",
    "start": "664920",
    "end": "667260"
  },
  {
    "text": "where the binder comes in the binder is",
    "start": "667260",
    "end": "669000"
  },
  {
    "text": "in charge of conflict resolution before",
    "start": "669000",
    "end": "671459"
  },
  {
    "text": "actually writing the results back to the",
    "start": "671459",
    "end": "673260"
  },
  {
    "text": "API server",
    "start": "673260",
    "end": "676040"
  },
  {
    "text": "yep so now let me talk about our",
    "start": "676260",
    "end": "678360"
  },
  {
    "text": "scheduling framework so we try to follow",
    "start": "678360",
    "end": "680519"
  },
  {
    "text": "kubernetes scheduling framework as",
    "start": "680519",
    "end": "682560"
  },
  {
    "text": "closely as possible the difference is",
    "start": "682560",
    "end": "684959"
  },
  {
    "text": "that we added a few stages and at the",
    "start": "684959",
    "end": "687959"
  },
  {
    "text": "same time different stages are actually",
    "start": "687959",
    "end": "689579"
  },
  {
    "text": "carried out in different components",
    "start": "689579",
    "end": "692579"
  },
  {
    "text": "so for Godot the scheduling actually",
    "start": "692579",
    "end": "694800"
  },
  {
    "text": "starts at the dispatcher and the",
    "start": "694800",
    "end": "696839"
  },
  {
    "text": "dispatcher is in charge of watching",
    "start": "696839",
    "end": "698279"
  },
  {
    "text": "ports on the API server and grouping",
    "start": "698279",
    "end": "700019"
  },
  {
    "text": "them into groups which we call",
    "start": "700019",
    "end": "701760"
  },
  {
    "text": "Scheduling units yeah so the dispatcher",
    "start": "701760",
    "end": "705060"
  },
  {
    "text": "then sorts these requests sorry these",
    "start": "705060",
    "end": "706980"
  },
  {
    "text": "units based on priority and sense that",
    "start": "706980",
    "end": "709980"
  },
  {
    "text": "assigns them to Dedicated scheduler",
    "start": "709980",
    "end": "711660"
  },
  {
    "text": "instances",
    "start": "711660",
    "end": "713040"
  },
  {
    "text": "so the scheduler is in charge of",
    "start": "713040",
    "end": "715680"
  },
  {
    "text": "assigning the best node to each Port",
    "start": "715680",
    "end": "717300"
  },
  {
    "text": "within the scheduling unit and it does",
    "start": "717300",
    "end": "719640"
  },
  {
    "text": "this by executing a filter and score",
    "start": "719640",
    "end": "721380"
  },
  {
    "text": "stage so this is similar or the same as",
    "start": "721380",
    "end": "723959"
  },
  {
    "text": "what is happening in the native Cube",
    "start": "723959",
    "end": "725519"
  },
  {
    "text": "scheduler",
    "start": "725519",
    "end": "726600"
  },
  {
    "text": "so additionally uh it also has a",
    "start": "726600",
    "end": "729060"
  },
  {
    "text": "dedicated preemption stage where it",
    "start": "729060",
    "end": "730500"
  },
  {
    "text": "suggests candidates for preemption yep",
    "start": "730500",
    "end": "733260"
  },
  {
    "text": "last but not least we have the binder",
    "start": "733260",
    "end": "735360"
  },
  {
    "text": "and as mentioned previously it's uh the",
    "start": "735360",
    "end": "737339"
  },
  {
    "text": "key to conflict resolution giving us a",
    "start": "737339",
    "end": "739500"
  },
  {
    "text": "form of optimistic concurrency control",
    "start": "739500",
    "end": "741899"
  },
  {
    "text": "yep so for scheduling decisions that",
    "start": "741899",
    "end": "744540"
  },
  {
    "text": "pass the conflict resolution it will",
    "start": "744540",
    "end": "746579"
  },
  {
    "text": "actually execute the reserve and buying",
    "start": "746579",
    "end": "748980"
  },
  {
    "text": "stages and finally write the results",
    "start": "748980",
    "end": "750720"
  },
  {
    "text": "back to the API server it's also in",
    "start": "750720",
    "end": "753240"
  },
  {
    "text": "charge of actually executing any",
    "start": "753240",
    "end": "754740"
  },
  {
    "text": "preemptions if needed",
    "start": "754740",
    "end": "758120"
  },
  {
    "text": "yep so uh yeah now I'll highlight some",
    "start": "758279",
    "end": "761760"
  },
  {
    "text": "of the performance features that we have",
    "start": "761760",
    "end": "764040"
  },
  {
    "text": "to increase the scheduling throughput so",
    "start": "764040",
    "end": "766440"
  },
  {
    "text": "as mentioned one of the goals that we",
    "start": "766440",
    "end": "768300"
  },
  {
    "text": "had for Godot was to achieve higher",
    "start": "768300",
    "end": "770220"
  },
  {
    "text": "throughput to meet our requirements",
    "start": "770220",
    "end": "773720"
  },
  {
    "text": "so Gala supports concurrent scheduling",
    "start": "774180",
    "end": "776040"
  },
  {
    "text": "so I think this is pretty obvious by now",
    "start": "776040",
    "end": "778019"
  },
  {
    "text": "we can have multiple scheduler instances",
    "start": "778019",
    "end": "780420"
  },
  {
    "text": "running at the same time and the binder",
    "start": "780420",
    "end": "782399"
  },
  {
    "text": "is in charge of resolving the conflicts",
    "start": "782399",
    "end": "784339"
  },
  {
    "text": "for the scheduling decisions yeah",
    "start": "784339",
    "end": "787260"
  },
  {
    "text": "however having more scheduled instances",
    "start": "787260",
    "end": "789480"
  },
  {
    "text": "also results in higher chance for",
    "start": "789480",
    "end": "791579"
  },
  {
    "text": "conflict so actually a balance is needed",
    "start": "791579",
    "end": "794160"
  },
  {
    "text": "when scaling the number of scheduler",
    "start": "794160",
    "end": "795660"
  },
  {
    "text": "instances",
    "start": "795660",
    "end": "796560"
  },
  {
    "text": "and in the addition we also support node",
    "start": "796560",
    "end": "798660"
  },
  {
    "text": "partitioning to reduce the chance of",
    "start": "798660",
    "end": "800040"
  },
  {
    "text": "conflicts so no partitioning is where",
    "start": "800040",
    "end": "802380"
  },
  {
    "text": "each scheduler instance uh is given a",
    "start": "802380",
    "end": "805260"
  },
  {
    "text": "dedicated set of nodes to schedule two",
    "start": "805260",
    "end": "807420"
  },
  {
    "text": "and but while this reduces the conflict",
    "start": "807420",
    "end": "810000"
  },
  {
    "text": "uh there's also a trade-off because uh",
    "start": "810000",
    "end": "812399"
  },
  {
    "text": "you know the best Note might not be",
    "start": "812399",
    "end": "814079"
  },
  {
    "text": "found within a single no partition yep",
    "start": "814079",
    "end": "818279"
  },
  {
    "text": "so we have a few other optimizations uh",
    "start": "818279",
    "end": "820920"
  },
  {
    "text": "one of them is also result caching so",
    "start": "820920",
    "end": "823260"
  },
  {
    "text": "actually we this is based on the fact",
    "start": "823260",
    "end": "825360"
  },
  {
    "text": "that we found that most ports within the",
    "start": "825360",
    "end": "827579"
  },
  {
    "text": "workload actually share the same",
    "start": "827579",
    "end": "829079"
  },
  {
    "text": "template and same requirements so a",
    "start": "829079",
    "end": "831180"
  },
  {
    "text": "feasible node that fits one part within",
    "start": "831180",
    "end": "833399"
  },
  {
    "text": "a workload is likely to be suitable for",
    "start": "833399",
    "end": "835440"
  },
  {
    "text": "others within the same workload so",
    "start": "835440",
    "end": "837600"
  },
  {
    "text": "during a filter stage we actually cache",
    "start": "837600",
    "end": "839579"
  },
  {
    "text": "uh feasible nodes and this helps to",
    "start": "839579",
    "end": "841860"
  },
  {
    "text": "speed up the filter stage as a whole",
    "start": "841860",
    "end": "844260"
  },
  {
    "text": "Yeah by repeating by avoiding repeated",
    "start": "844260",
    "end": "846540"
  },
  {
    "text": "computations",
    "start": "846540",
    "end": "849079"
  },
  {
    "text": "yep so now move on to share some of the",
    "start": "849860",
    "end": "853079"
  },
  {
    "text": "scheduling capabilities uh will not go",
    "start": "853079",
    "end": "855420"
  },
  {
    "text": "through all of them for the interest of",
    "start": "855420",
    "end": "857160"
  },
  {
    "text": "time so these scheduling capabilities",
    "start": "857160",
    "end": "859019"
  },
  {
    "text": "are designed to help us to run our",
    "start": "859019",
    "end": "860700"
  },
  {
    "text": "offline workloads on kubernetes",
    "start": "860700",
    "end": "864139"
  },
  {
    "text": "so the first feature and I think a very",
    "start": "864240",
    "end": "866639"
  },
  {
    "text": "common requirement for batch processing",
    "start": "866639",
    "end": "868740"
  },
  {
    "text": "schedulers it's a gang scheduling so in",
    "start": "868740",
    "end": "871740"
  },
  {
    "text": "gang scheduling uh in the context of",
    "start": "871740",
    "end": "874200"
  },
  {
    "text": "girdle or ports within a single gang uh",
    "start": "874200",
    "end": "877440"
  },
  {
    "text": "my schedule they are scheduled together",
    "start": "877440",
    "end": "878760"
  },
  {
    "text": "with all other things semantics so this",
    "start": "878760",
    "end": "880680"
  },
  {
    "text": "is important for scenarios like",
    "start": "880680",
    "end": "882000"
  },
  {
    "text": "distributed learning where maybe the",
    "start": "882000",
    "end": "884040"
  },
  {
    "text": "absence of a port running the parameter",
    "start": "884040",
    "end": "886079"
  },
  {
    "text": "server would block the progress of the",
    "start": "886079",
    "end": "887760"
  },
  {
    "text": "whole job as a whole",
    "start": "887760",
    "end": "890100"
  },
  {
    "text": "yep so God supports this by not",
    "start": "890100",
    "end": "893100"
  },
  {
    "text": "scheduling pods individually but in",
    "start": "893100",
    "end": "894899"
  },
  {
    "text": "scheduling units like I mentioned",
    "start": "894899",
    "end": "896760"
  },
  {
    "text": "previously uh yeah so this actually",
    "start": "896760",
    "end": "899040"
  },
  {
    "text": "fills the semantic Gap needed to support",
    "start": "899040",
    "end": "901199"
  },
  {
    "text": "gang scheduling in communities and the",
    "start": "901199",
    "end": "903480"
  },
  {
    "text": "binder will actually take the necessary",
    "start": "903480",
    "end": "904800"
  },
  {
    "text": "actions to ensure that all ports get",
    "start": "904800",
    "end": "907079"
  },
  {
    "text": "binded together yep",
    "start": "907079",
    "end": "910740"
  },
  {
    "text": "so next you also support some more",
    "start": "910740",
    "end": "912720"
  },
  {
    "text": "complex queuing semantics so uh when you",
    "start": "912720",
    "end": "916019"
  },
  {
    "text": "have a lot of offline jobs in a queue",
    "start": "916019",
    "end": "918240"
  },
  {
    "text": "waiting to be run I think it's important",
    "start": "918240",
    "end": "920639"
  },
  {
    "text": "to ensure that there's no starvation in",
    "start": "920639",
    "end": "922199"
  },
  {
    "text": "this fairness so uh while the native",
    "start": "922199",
    "end": "925320"
  },
  {
    "text": "Cube scheduler I think it supports some",
    "start": "925320",
    "end": "927360"
  },
  {
    "text": "basic priority based sorting we actually",
    "start": "927360",
    "end": "929820"
  },
  {
    "text": "implemented more sophisticated policies",
    "start": "929820",
    "end": "932339"
  },
  {
    "text": "in our dispatcher such as domain",
    "start": "932339",
    "end": "935220"
  },
  {
    "text": "resource fairness and fair share yeah so",
    "start": "935220",
    "end": "938519"
  },
  {
    "text": "yeah uh this is you know to help us to",
    "start": "938519",
    "end": "941220"
  },
  {
    "text": "run our support the running of our",
    "start": "941220",
    "end": "943260"
  },
  {
    "text": "offline workloads and kubernetes",
    "start": "943260",
    "end": "946760"
  },
  {
    "text": "so next we also have Micro topology",
    "start": "947040",
    "end": "948899"
  },
  {
    "text": "scheduling so uh batch and ml jobs are",
    "start": "948899",
    "end": "952560"
  },
  {
    "text": "characterized by high IO and frequent",
    "start": "952560",
    "end": "954420"
  },
  {
    "text": "memory access so uh micro topology",
    "start": "954420",
    "end": "957180"
  },
  {
    "text": "scheduling is where we schedule a CPU",
    "start": "957180",
    "end": "960180"
  },
  {
    "text": "and memory from the same pneuma node to",
    "start": "960180",
    "end": "962339"
  },
  {
    "text": "a single port and this can actually",
    "start": "962339",
    "end": "963779"
  },
  {
    "text": "improve the performance of our offline",
    "start": "963779",
    "end": "965279"
  },
  {
    "text": "jobs yeah uh so this actually requires",
    "start": "965279",
    "end": "968699"
  },
  {
    "text": "cooperation between the scheduler and",
    "start": "968699",
    "end": "971519"
  },
  {
    "text": "also the cubelet owner agent yeah so",
    "start": "971519",
    "end": "973920"
  },
  {
    "text": "while the cubelet now supports pneuma",
    "start": "973920",
    "end": "976199"
  },
  {
    "text": "binding using topology manager the",
    "start": "976199",
    "end": "978240"
  },
  {
    "text": "native Cube scheduler still lacks",
    "start": "978240",
    "end": "979980"
  },
  {
    "text": "support for microt topology scheduling",
    "start": "979980",
    "end": "982139"
  },
  {
    "text": "so we use a customer agent that writes",
    "start": "982139",
    "end": "985199"
  },
  {
    "text": "topology information has no annotations",
    "start": "985199",
    "end": "987360"
  },
  {
    "text": "and the girls schedule is able to",
    "start": "987360",
    "end": "989639"
  },
  {
    "text": "process this information to make its",
    "start": "989639",
    "end": "991920"
  },
  {
    "text": "scheduling decisions yep",
    "start": "991920",
    "end": "995220"
  },
  {
    "text": "so we implemented a few other",
    "start": "995220",
    "end": "996839"
  },
  {
    "text": "capabilities which I will not go through",
    "start": "996839",
    "end": "998639"
  },
  {
    "text": "in the interest of time and I think this",
    "start": "998639",
    "end": "1000800"
  },
  {
    "text": "has also been mentioned by Italian",
    "start": "1000800",
    "end": "1002180"
  },
  {
    "text": "earlier",
    "start": "1002180",
    "end": "1004100"
  },
  {
    "text": "yeah so now share a bit about the",
    "start": "1004100",
    "end": "1006560"
  },
  {
    "text": "results that we achieve with the girl",
    "start": "1006560",
    "end": "1008180"
  },
  {
    "text": "scheduler so actually",
    "start": "1008180",
    "end": "1010220"
  },
  {
    "text": "um we ran some benchmarks comparing uh",
    "start": "1010220",
    "end": "1012560"
  },
  {
    "text": "communities with a girl scheduler",
    "start": "1012560",
    "end": "1014899"
  },
  {
    "text": "running a single scheduling instance and",
    "start": "1014899",
    "end": "1017120"
  },
  {
    "text": "with some of the optimizations that we",
    "start": "1017120",
    "end": "1018740"
  },
  {
    "text": "have uh we actually managed to achieve a",
    "start": "1018740",
    "end": "1021500"
  },
  {
    "text": "much higher scheduling throughput and we",
    "start": "1021500",
    "end": "1023720"
  },
  {
    "text": "can also take this a step further by",
    "start": "1023720",
    "end": "1025760"
  },
  {
    "text": "scaling the number of scheduler",
    "start": "1025760",
    "end": "1027860"
  },
  {
    "text": "instances",
    "start": "1027860",
    "end": "1028900"
  },
  {
    "text": "and it allows us to hit almost uh 5000",
    "start": "1028900",
    "end": "1033079"
  },
  {
    "text": "parts per second as you can see here",
    "start": "1033079",
    "end": "1034938"
  },
  {
    "text": "when we run run benchmarks on a 10 000",
    "start": "1034939",
    "end": "1037760"
  },
  {
    "text": "note cluster",
    "start": "1037760",
    "end": "1040100"
  },
  {
    "text": "yep so we've been running it in",
    "start": "1040100",
    "end": "1042319"
  },
  {
    "text": "production for some time now and we've",
    "start": "1042319",
    "end": "1044900"
  },
  {
    "text": "managed to increase the resource",
    "start": "1044900",
    "end": "1046400"
  },
  {
    "text": "utilization in our resources in our",
    "start": "1046400",
    "end": "1048860"
  },
  {
    "text": "clusters to up to 60 we've managed to",
    "start": "1048860",
    "end": "1051679"
  },
  {
    "text": "sustain a peak scheduling throughput of",
    "start": "1051679",
    "end": "1054080"
  },
  {
    "text": "5000 parts per second and we've also",
    "start": "1054080",
    "end": "1056240"
  },
  {
    "text": "managed to unify our resource pools",
    "start": "1056240",
    "end": "1058460"
  },
  {
    "text": "consisting of millions of course",
    "start": "1058460",
    "end": "1060740"
  },
  {
    "text": "yeah",
    "start": "1060740",
    "end": "1063080"
  },
  {
    "text": "so now we're coming to the end of the",
    "start": "1063080",
    "end": "1065059"
  },
  {
    "text": "presentation now share a bit about some",
    "start": "1065059",
    "end": "1066919"
  },
  {
    "text": "of the future work that we have in store",
    "start": "1066919",
    "end": "1068240"
  },
  {
    "text": "for the girl scheduler So currently",
    "start": "1068240",
    "end": "1070820"
  },
  {
    "text": "transitory stages used by the different",
    "start": "1070820",
    "end": "1072620"
  },
  {
    "text": "components are persisted in the API",
    "start": "1072620",
    "end": "1074840"
  },
  {
    "text": "server in the form of Port annotations",
    "start": "1074840",
    "end": "1077480"
  },
  {
    "text": "Etc so we are planning on using an",
    "start": "1077480",
    "end": "1079220"
  },
  {
    "text": "in-memory cache or in-memory database to",
    "start": "1079220",
    "end": "1081440"
  },
  {
    "text": "store these stages and we expect this to",
    "start": "1081440",
    "end": "1084620"
  },
  {
    "text": "increase the performance of the Google",
    "start": "1084620",
    "end": "1086480"
  },
  {
    "text": "scheduler",
    "start": "1086480",
    "end": "1088220"
  },
  {
    "text": "we're also working on a girl rescheduler",
    "start": "1088220",
    "end": "1090799"
  },
  {
    "text": "and his job is to take preemptive",
    "start": "1090799",
    "end": "1092600"
  },
  {
    "text": "measures uh by washing running pots and",
    "start": "1092600",
    "end": "1095840"
  },
  {
    "text": "uh",
    "start": "1095840",
    "end": "1096980"
  },
  {
    "text": "carrying out actions to reduce resource",
    "start": "1096980",
    "end": "1098900"
  },
  {
    "text": "fragmentation and contention achieving",
    "start": "1098900",
    "end": "1101179"
  },
  {
    "text": "higher quality of service for critical",
    "start": "1101179",
    "end": "1102919"
  },
  {
    "text": "workloads",
    "start": "1102919",
    "end": "1104480"
  },
  {
    "text": "you're also developing an explainer and",
    "start": "1104480",
    "end": "1106280"
  },
  {
    "text": "simulator to improve the explainability",
    "start": "1106280",
    "end": "1109160"
  },
  {
    "text": "and visibility of our scheduling",
    "start": "1109160",
    "end": "1111080"
  },
  {
    "text": "decisions",
    "start": "1111080",
    "end": "1112100"
  },
  {
    "text": "yep",
    "start": "1112100",
    "end": "1114200"
  },
  {
    "text": "so yeah we're planning to open source",
    "start": "1114200",
    "end": "1116419"
  },
  {
    "text": "this soon and actually in baitans we",
    "start": "1116419",
    "end": "1119120"
  },
  {
    "text": "have been working with kubernetes for",
    "start": "1119120",
    "end": "1120380"
  },
  {
    "text": "quite some time now and trying to scale",
    "start": "1120380",
    "end": "1122600"
  },
  {
    "text": "it and if at least begin to open source",
    "start": "1122600",
    "end": "1125539"
  },
  {
    "text": "a lot of our projects and solutions",
    "start": "1125539",
    "end": "1127700"
  },
  {
    "text": "under this organization called Cube wolf",
    "start": "1127700",
    "end": "1130100"
  },
  {
    "text": "so for the girls scheduler we're also",
    "start": "1130100",
    "end": "1132620"
  },
  {
    "text": "planning to open source this in the",
    "start": "1132620",
    "end": "1134299"
  },
  {
    "text": "fourth quarter of this year under this",
    "start": "1134299",
    "end": "1135919"
  },
  {
    "text": "organization yep so do stay tuned for",
    "start": "1135919",
    "end": "1138500"
  },
  {
    "text": "any updates",
    "start": "1138500",
    "end": "1140900"
  },
  {
    "text": "yep so we've come to the end of the",
    "start": "1140900",
    "end": "1143120"
  },
  {
    "text": "sharing and you have to summarize we",
    "start": "1143120",
    "end": "1145940"
  },
  {
    "text": "shared about batch processing invite",
    "start": "1145940",
    "end": "1147679"
  },
  {
    "text": "dance why we decided to move them to",
    "start": "1147679",
    "end": "1149539"
  },
  {
    "text": "kubernetes some of the problems that we",
    "start": "1149539",
    "end": "1151880"
  },
  {
    "text": "faced and why we wrote Our Own scheduler",
    "start": "1151880",
    "end": "1154039"
  },
  {
    "text": "yeah so thank you so much",
    "start": "1154039",
    "end": "1156720"
  },
  {
    "text": "[Applause]",
    "start": "1156720",
    "end": "1164539"
  },
  {
    "text": "what were you using to gauge the",
    "start": "1166580",
    "end": "1168440"
  },
  {
    "text": "performance",
    "start": "1168440",
    "end": "1169820"
  },
  {
    "text": "between the two",
    "start": "1169820",
    "end": "1172779"
  },
  {
    "text": "oh okay yeah so actually we we ran the",
    "start": "1175340",
    "end": "1178580"
  },
  {
    "text": "benchmarks in uh test clusters where we",
    "start": "1178580",
    "end": "1181700"
  },
  {
    "text": "actually tried to saturate the number of",
    "start": "1181700",
    "end": "1184520"
  },
  {
    "text": "scheduling requests by creating pots",
    "start": "1184520",
    "end": "1187039"
  },
  {
    "text": "within the cluster and yeah once we",
    "start": "1187039",
    "end": "1189140"
  },
  {
    "text": "reach that saturation point then we got",
    "start": "1189140",
    "end": "1191179"
  },
  {
    "text": "the scheduling throughput yeah I mean",
    "start": "1191179",
    "end": "1193280"
  },
  {
    "text": "compared the two yeah",
    "start": "1193280",
    "end": "1196240"
  },
  {
    "text": "yeah so yeah",
    "start": "1197120",
    "end": "1198580"
  },
  {
    "text": "hope that answers your question",
    "start": "1198580",
    "end": "1202600"
  },
  {
    "text": "so I have a question for the",
    "start": "1207919",
    "end": "1209600"
  },
  {
    "text": "architectural part so I saw the",
    "start": "1209600",
    "end": "1212120"
  },
  {
    "text": "scheduler is paralyzed as in they are",
    "start": "1212120",
    "end": "1214460"
  },
  {
    "text": "using a shared plan Bender so this one",
    "start": "1214460",
    "end": "1217700"
  },
  {
    "text": "is working I mean the scheduler zero and",
    "start": "1217700",
    "end": "1220340"
  },
  {
    "text": "is working in different gold routine or",
    "start": "1220340",
    "end": "1222620"
  },
  {
    "text": "say there are different components as in",
    "start": "1222620",
    "end": "1224660"
  },
  {
    "text": "communicate with each other use some RPC",
    "start": "1224660",
    "end": "1226940"
  },
  {
    "text": "or whatever oh no actually they they",
    "start": "1226940",
    "end": "1229280"
  },
  {
    "text": "they write the the transitory stages to",
    "start": "1229280",
    "end": "1232280"
  },
  {
    "text": "the API server so in the form of like",
    "start": "1232280",
    "end": "1234500"
  },
  {
    "text": "annotations on ports and such yeah",
    "start": "1234500",
    "end": "1237140"
  },
  {
    "text": "so yeah there's no like direct",
    "start": "1237140",
    "end": "1239240"
  },
  {
    "text": "communication between the components",
    "start": "1239240",
    "end": "1241400"
  },
  {
    "text": "actually",
    "start": "1241400",
    "end": "1243580"
  },
  {
    "text": "yep",
    "start": "1247280",
    "end": "1249940"
  },
  {
    "text": "okay uh yeah",
    "start": "1253039",
    "end": "1256779"
  },
  {
    "text": "hi uh how do you compare your scheduler",
    "start": "1271480",
    "end": "1274580"
  },
  {
    "text": "with volcano scheduler it seems like",
    "start": "1274580",
    "end": "1276740"
  },
  {
    "text": "they are too similar",
    "start": "1276740",
    "end": "1279080"
  },
  {
    "text": "goals",
    "start": "1279080",
    "end": "1280460"
  },
  {
    "text": "okay uh in uh honest speaking uh before",
    "start": "1280460",
    "end": "1286340"
  },
  {
    "text": "we developed a good schedule and it's",
    "start": "1286340",
    "end": "1289520"
  },
  {
    "text": "about two years years ago and we test",
    "start": "1289520",
    "end": "1292760"
  },
  {
    "text": "the group schedule including the good",
    "start": "1292760",
    "end": "1294620"
  },
  {
    "text": "schedule framework we too and we also",
    "start": "1294620",
    "end": "1296900"
  },
  {
    "text": "have a test about the volcano but the",
    "start": "1296900",
    "end": "1300740"
  },
  {
    "text": "most important is about the performance",
    "start": "1300740",
    "end": "1302539"
  },
  {
    "text": "yeah I think if if you manage your",
    "start": "1302539",
    "end": "1305240"
  },
  {
    "text": "customers from 5000 to 20 thousands you",
    "start": "1305240",
    "end": "1310039"
  },
  {
    "text": "can have a benchmark about the volcano",
    "start": "1310039",
    "end": "1314200"
  },
  {
    "text": "so sorry I could I could not hear",
    "start": "1318860",
    "end": "1324740"
  },
  {
    "text": "okay",
    "start": "1325580",
    "end": "1328059"
  },
  {
    "text": "so is your scheduler also a kubernetes",
    "start": "1336460",
    "end": "1339679"
  },
  {
    "text": "or scheduled Plugin or is like a a",
    "start": "1339679",
    "end": "1343580"
  },
  {
    "text": "separate scheduler running on",
    "start": "1343580",
    "end": "1346600"
  },
  {
    "text": "kubernetes okay a good question uh it's",
    "start": "1346600",
    "end": "1349940"
  },
  {
    "text": "uh it's a surprise it's a security",
    "start": "1349940",
    "end": "1352640"
  },
  {
    "text": "schedule which will replace the group",
    "start": "1352640",
    "end": "1354860"
  },
  {
    "text": "schedule yeah we we use the schedule to",
    "start": "1354860",
    "end": "1358820"
  },
  {
    "text": "schedule all of our workloads",
    "start": "1358820",
    "end": "1362380"
  },
  {
    "text": "we'll we will release your like code in",
    "start": "1365480",
    "end": "1368000"
  },
  {
    "text": "the git repo we can we can chat so okay",
    "start": "1368000",
    "end": "1373000"
  },
  {
    "text": "okay this is our this is our open source",
    "start": "1375200",
    "end": "1378740"
  },
  {
    "text": "uh organization and you can find some",
    "start": "1378740",
    "end": "1381140"
  },
  {
    "text": "interesting open source uh in the",
    "start": "1381140",
    "end": "1384140"
  },
  {
    "text": "organization and uh please allow me to",
    "start": "1384140",
    "end": "1388280"
  },
  {
    "text": "have uh want me to have a brief",
    "start": "1388280",
    "end": "1389840"
  },
  {
    "text": "introduction you say uh we uh we extend",
    "start": "1389840",
    "end": "1393620"
  },
  {
    "text": "the kubernetes from five thousands to 20",
    "start": "1393620",
    "end": "1397100"
  },
  {
    "text": "thousands the first thing we do is to",
    "start": "1397100",
    "end": "1399559"
  },
  {
    "text": "replace the ETC CD which is the first",
    "start": "1399559",
    "end": "1402020"
  },
  {
    "text": "project called plan we use a wireless",
    "start": "1402020",
    "end": "1405260"
  },
  {
    "text": "key value consistent stock to replace",
    "start": "1405260",
    "end": "1407480"
  },
  {
    "text": "our ecity to achieve the to solve the",
    "start": "1407480",
    "end": "1410960"
  },
  {
    "text": "the first book of the storage system",
    "start": "1410960",
    "end": "1413840"
  },
  {
    "text": "yeah and uh and you can see a catalyst",
    "start": "1413840",
    "end": "1417860"
  },
  {
    "text": "call this is a place also very important",
    "start": "1417860",
    "end": "1420799"
  },
  {
    "text": "law in the node agent because this is",
    "start": "1420799",
    "end": "1423860"
  },
  {
    "text": "our most important located open source",
    "start": "1423860",
    "end": "1427520"
  },
  {
    "text": "project which will solve the conflicts",
    "start": "1427520",
    "end": "1430220"
  },
  {
    "text": "between the online workload and the",
    "start": "1430220",
    "end": "1432380"
  },
  {
    "text": "offline workload data will also report a",
    "start": "1432380",
    "end": "1434539"
  },
  {
    "text": "lot of the different",
    "start": "1434539",
    "end": "1435880"
  },
  {
    "text": "resources and different quality of",
    "start": "1435880",
    "end": "1438140"
  },
  {
    "text": "service to the to our to our kubernetes",
    "start": "1438140",
    "end": "1441260"
  },
  {
    "text": "and the scheduler will collaborate which",
    "start": "1441260",
    "end": "1443419"
  },
  {
    "text": "is to achieve a better scheduling",
    "start": "1443419",
    "end": "1445760"
  },
  {
    "text": "qualities",
    "start": "1445760",
    "end": "1448159"
  },
  {
    "text": "uh we'll close here for the next session",
    "start": "1448159",
    "end": "1450559"
  },
  {
    "text": "thank you",
    "start": "1450559",
    "end": "1452360"
  },
  {
    "text": "thank you for your presentation",
    "start": "1452360",
    "end": "1456640"
  }
]