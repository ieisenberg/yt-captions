[
  {
    "text": "okay let's start um thank you all for joining our session um that's going to",
    "start": "599",
    "end": "5799"
  },
  {
    "text": "be about detecting and overcoming GPU failures during ml training before we",
    "start": "5799",
    "end": "11200"
  },
  {
    "text": "start the presentation we're going to quickly introduce ourselves so I'm Sarah I'm a ml platform",
    "start": "11200",
    "end": "16920"
  },
  {
    "text": "engineer at wave so wave is a is we're UK based and our mission is to uh solve",
    "start": "16920",
    "end": "23680"
  },
  {
    "text": "cell driving um using embodied AI so as you can imagine um training AI models is",
    "start": "23680",
    "end": "29679"
  },
  {
    "text": "at the core of what we do um so it's super important to have efficient training um and detect and handle um GPU",
    "start": "29679",
    "end": "36840"
  },
  {
    "text": "failures when they occur hi I'm Ganesh and I'm a software",
    "start": "36840",
    "end": "42559"
  },
  {
    "text": "engineer in the AA kubernetes service team at Microsoft AKs is a managed kubernetes service platform and we help",
    "start": "42559",
    "end": "49879"
  },
  {
    "text": "make it easy to run a variety of computer workloads including ml workloads like ml training workloads I",
    "start": "49879",
    "end": "57440"
  },
  {
    "text": "particularly work on GPU management Within the AKs team and I've seen a bunch of GPU errors before and I'm happy",
    "start": "57440",
    "end": "63800"
  },
  {
    "text": "to share about some of the insights from that experience so here's our agenda we're",
    "start": "63800",
    "end": "71680"
  },
  {
    "text": "going to start with a quick introduction then we'll present some U we present GPU",
    "start": "71680",
    "end": "76759"
  },
  {
    "text": "failures and the impact that they can have on M workloads um then we'll move on to uh detection and resolution um",
    "start": "76759",
    "end": "84479"
  },
  {
    "text": "with the demo and conclusion about the detection and resolution um Ganesh and I",
    "start": "84479",
    "end": "89640"
  },
  {
    "text": "will present two different perspectives so I'll have the GPU uh the uh industry",
    "start": "89640",
    "end": "95720"
  },
  {
    "text": "uh point of view and then Ganesh will present the cloud provider or info provider side of things so what can be",
    "start": "95720",
    "end": "101799"
  },
  {
    "text": "done on that side to lighten the burden on the customer um so why kubernetes for ML",
    "start": "101799",
    "end": "108560"
  },
  {
    "text": "training I think we've seen um a lot of these throughout the uh conference uh",
    "start": "108560",
    "end": "114159"
  },
  {
    "text": "but just some uh three key points uh so kubernetes is uh designed for distributed systems which is really",
    "start": "114159",
    "end": "120640"
  },
  {
    "text": "helpful when um when it comes to running uh multi note training uh workloads it",
    "start": "120640",
    "end": "126039"
  },
  {
    "text": "also has a good uh ecosystem of jab scheduling tools like volcano C flow uh",
    "start": "126039",
    "end": "132000"
  },
  {
    "text": "Q um they support gang scheduling uh also helpful for multi note training so",
    "start": "132000",
    "end": "137599"
  },
  {
    "text": "the pods get scheduled at the same time and they also have plugins for AI Frameworks so that's configuration that",
    "start": "137599",
    "end": "143920"
  },
  {
    "text": "gets done behind the scen for you uh in order to use those AI Frameworks",
    "start": "143920",
    "end": "150200"
  },
  {
    "text": "uh the last point is the GPU device plugging operator I don't know if you joined a session about the uh GPU",
    "start": "150200",
    "end": "156080"
  },
  {
    "text": "operator um but it's um makes it really easy for you to manage all the software",
    "start": "156080",
    "end": "161840"
  },
  {
    "text": "layer that gives you access to the different Hardware components that required for your AI",
    "start": "161840",
    "end": "168720"
  },
  {
    "text": "training so a little bit about J GPU failures uh so gpus are very complex",
    "start": "168720",
    "end": "174640"
  },
  {
    "text": "pieces of hardware and they're um much more likely to um have failures than CPUs",
    "start": "174640",
    "end": "181000"
  },
  {
    "text": "um so this is a table from the um Lama 3 paper published by meta last month um",
    "start": "181000",
    "end": "188440"
  },
  {
    "text": "and they listed all the unexpected interruptions that happened during the 54 day period of the Lama 3 model",
    "start": "188440",
    "end": "196080"
  },
  {
    "text": "pre-training and what they uh explain in this table is that uh out of all the",
    "start": "196080",
    "end": "201200"
  },
  {
    "text": "unexpected interruptions that happened 58.7% were due to GPU issues so it's a",
    "start": "201200",
    "end": "207360"
  },
  {
    "text": "really um important problem that's uh critical to um tackle",
    "start": "207360",
    "end": "213200"
  },
  {
    "text": "well uh in terms of GPU failures the the issues the errors can happen at",
    "start": "213200",
    "end": "218599"
  },
  {
    "text": "different parts of the different components uh it can happen at the GPU itself um we all familiar with the GPU",
    "start": "218599",
    "end": "226519"
  },
  {
    "text": "has fallen off the bus um error uh it can happen uh at the GPU memory level as",
    "start": "226519",
    "end": "232360"
  },
  {
    "text": "well you can have networking issues uh between the gpus um and then at the",
    "start": "232360",
    "end": "238560"
  },
  {
    "text": "software level you can also have GPU driver errors so what impacts do uh these",
    "start": "238560",
    "end": "246920"
  },
  {
    "text": "issues have on the M workare CLS so the obvious one is failures so um failures",
    "start": "246920",
    "end": "252959"
  },
  {
    "text": "means uh um the the job actually fails the P terminates but you can also have",
    "start": "252959",
    "end": "259120"
  },
  {
    "text": "um other impacts uh such as hangs so in the case of hangs there's no more um",
    "start": "259120",
    "end": "265080"
  },
  {
    "text": "useful computation that is done and the job just hangs indefinitely uh with without",
    "start": "265080",
    "end": "270560"
  },
  {
    "text": "terminating um and you can have slow downs as well so slowdowns they're still useful computations that are being done",
    "start": "270560",
    "end": "276680"
  },
  {
    "text": "but they're much slower uh in our case we've we've seen examples of um R run drums running six times slower than",
    "start": "276680",
    "end": "284120"
  },
  {
    "text": "normal um one thing to note is because um in the case of distributed training",
    "start": "284120",
    "end": "289280"
  },
  {
    "text": "uh even with a single uh 4y GPU Um this",
    "start": "289280",
    "end": "294479"
  },
  {
    "text": "can impact an entire job running across uh tens thousands hundreds and thousands of of gpus um because of the gradient",
    "start": "294479",
    "end": "303360"
  },
  {
    "text": "synchronization so the different ranks will share the gradients and to average",
    "start": "303360",
    "end": "308720"
  },
  {
    "text": "it um and they they will wait on each other uh so that's uh how it can impact",
    "start": "308720",
    "end": "314280"
  },
  {
    "text": "an entire job um this um issues can be very",
    "start": "314280",
    "end": "321280"
  },
  {
    "text": "expensive uh so as a an example I put an example of a h100 node for example that",
    "start": "321280",
    "end": "326919"
  },
  {
    "text": "has 8 gpus public prices 2000 US dollar per day um for a job that um",
    "start": "326919",
    "end": "333680"
  },
  {
    "text": "usually runs uh for three days on such noes imagine is six times slower um so",
    "start": "333680",
    "end": "340800"
  },
  {
    "text": "it will run on uh for 18 days instead that's initial",
    "start": "340800",
    "end": "346039"
  },
  {
    "text": "30,000 uh US dollar and that's only for one note so you imagine the the impact",
    "start": "346039",
    "end": "352479"
  },
  {
    "text": "that Financial impact they has um on your well when um across your",
    "start": "352479",
    "end": "359160"
  },
  {
    "text": "infrastructure structure and that's only financial but it's also like um longer development times as",
    "start": "359160",
    "end": "366280"
  },
  {
    "text": "well so um the resolution starts with detection so the different goals of",
    "start": "366280",
    "end": "372720"
  },
  {
    "text": "detections are uh a avoid workloads from being uh scheduled on faulty notes B uh",
    "start": "372720",
    "end": "378960"
  },
  {
    "text": "remove jobs from um faulty notes during training and reschedule them on healthy notes and then finally repair or replace",
    "start": "378960",
    "end": "386720"
  },
  {
    "text": "the 4y notes so there are different um",
    "start": "386720",
    "end": "391840"
  },
  {
    "text": "solutions for detection so the first one is uh node Readiness so that's detecting",
    "start": "391840",
    "end": "397560"
  },
  {
    "text": "failures before the job starts so the idea is using an init container that runs um health checks um and such as GPU",
    "start": "397560",
    "end": "406039"
  },
  {
    "text": "bandwidth but um nickel test and GPU errors uh it's very useful to run uh",
    "start": "406039",
    "end": "412479"
  },
  {
    "text": "tests that actually use the resources because it it it runs ahead of the training job and um in the case where",
    "start": "412479",
    "end": "419840"
  },
  {
    "text": "the health checks succeed the you can move on to the training uh job container",
    "start": "419840",
    "end": "425120"
  },
  {
    "text": "and start your training and if the health check fail um the the node is Tainted um and the p terminates and gets",
    "start": "425120",
    "end": "433400"
  },
  {
    "text": "um rescheduled but we'll see that later um important to taint the node or um any",
    "start": "433400",
    "end": "440240"
  },
  {
    "text": "strategy to disable uh further workloads or the workload from being scheduled on that note um the benefit of this",
    "start": "440240",
    "end": "448400"
  },
  {
    "text": "approach is that work clo don't get scheduled uh on nodes that have um such",
    "start": "448400",
    "end": "454319"
  },
  {
    "text": "errors uh second approach um Solution that's complimentary it's monitoring",
    "start": "454319",
    "end": "459840"
  },
  {
    "text": "while the jobs are running um so you have health checks um and monitoring at",
    "start": "459840",
    "end": "465120"
  },
  {
    "text": "the node level um so you run um health checks dpu health checks in the",
    "start": "465120",
    "end": "470280"
  },
  {
    "text": "background um and you can collect metrics so um these",
    "start": "470280",
    "end": "475400"
  },
  {
    "text": "are easy to find um ganes will touch on uh the GPU health checks uh later on uh",
    "start": "475400",
    "end": "482440"
  },
  {
    "text": "but in terms of metrics you can use the for example the dcgi the dcgm exporter",
    "start": "482440",
    "end": "488039"
  },
  {
    "text": "metrics uh the pros um is that they're very easy to install with the GPU",
    "start": "488039",
    "end": "493280"
  },
  {
    "text": "operator and they can detect issues um at a veryify level at the GPU level the",
    "start": "493280",
    "end": "498440"
  },
  {
    "text": "cons is that they're very they very difficult with those um metrics and health checks to uh detect the the",
    "start": "498440",
    "end": "507039"
  },
  {
    "text": "hangs and then you can um do monitoring at the workload level uh so that's uh",
    "start": "507039",
    "end": "513360"
  },
  {
    "text": "for example combined metrics on GPU utilization and versus memory access so",
    "start": "513360",
    "end": "518479"
  },
  {
    "text": "we noticed that um in some of our um workloads when they were hanging we we noticed the 100% of GPZ and 0% of memory",
    "start": "518479",
    "end": "527399"
  },
  {
    "text": "access um or you can also uh collect metrics around Collective uh operations",
    "start": "527399",
    "end": "533040"
  },
  {
    "text": "so how long it takes for uh Collective operations the pro is that um they can actually detect HKS um um sometimes it",
    "start": "533040",
    "end": "540560"
  },
  {
    "text": "can help you detect those issues cons is that is more complex to implement so it's still custom and so you have to",
    "start": "540560",
    "end": "547519"
  },
  {
    "text": "inst start yourself um and um it's more difficult as well to identify the um",
    "start": "547519",
    "end": "553399"
  },
  {
    "text": "which GPU is um responsible for the Hang so uh it's more at the the job",
    "start": "553399",
    "end": "561720"
  },
  {
    "text": "level um so a bit on the resolution so at the job level um the main approach is",
    "start": "561720",
    "end": "567279"
  },
  {
    "text": "obviously migrating the the job so job preion and rescheduling so you you save",
    "start": "567279",
    "end": "572480"
  },
  {
    "text": "the checkpoint so that can be done um automatically um with Frameworks such as pytorch so you can use a call back with",
    "start": "572480",
    "end": "579399"
  },
  {
    "text": "an on exception function and what it does is um on an exception so for example when you receive a syum it will",
    "start": "579399",
    "end": "585720"
  },
  {
    "text": "automatically save a checkpoint and then the job will terminate um and um then you reschedule",
    "start": "585720",
    "end": "593839"
  },
  {
    "text": "a new job on your new note that's healthy and then you restore from the safe checkpoint",
    "start": "593839",
    "end": "600920"
  },
  {
    "text": "um resolution at the node level so that's a repair or replace uh the node so you got all the metrics and health",
    "start": "600959",
    "end": "608040"
  },
  {
    "text": "checks that you that you run um whether in it was at the back in the background or at um at the beginning of",
    "start": "608040",
    "end": "615160"
  },
  {
    "text": "a job and then once there's an error you you taint the node so that's for to prevent um further scheduling on that",
    "start": "615160",
    "end": "621959"
  },
  {
    "text": "node and then you can either depending on the nature of the the error you can either take some corrective actions",
    "start": "621959",
    "end": "628120"
  },
  {
    "text": "which such as like reset the device restart the driver or reboot the node um",
    "start": "628120",
    "end": "634120"
  },
  {
    "text": "run uh health checks again um and then untaint if they pass or you can uh drain",
    "start": "634120",
    "end": "640600"
  },
  {
    "text": "and remove the node so they will remove the node from from your note pool um and",
    "start": "640600",
    "end": "646600"
  },
  {
    "text": "then some Cloud providers uh Pro uh provide a functionality to add um Mark",
    "start": "646600",
    "end": "652800"
  },
  {
    "text": "the node as in healthy so it doesn't get um re re addded later on to your um to",
    "start": "652800",
    "end": "658839"
  },
  {
    "text": "your note that's important that um to know",
    "start": "658839",
    "end": "664000"
  },
  {
    "text": "that and that's me I'll hand it over to ganes for the infr provider",
    "start": "664000",
    "end": "671040"
  },
  {
    "text": "perspective thank you Sarah from the info provider perspective we want to minimize the number of steps",
    "start": "674480",
    "end": "681639"
  },
  {
    "text": "and checks that an application layer workload needs to do we want to take over most of the health checks that can",
    "start": "681639",
    "end": "688440"
  },
  {
    "text": "be done at the level or across the fleet so that customers can focus on their",
    "start": "688440",
    "end": "694120"
  },
  {
    "text": "machine learning workloads and figure out how to maybe optimize the their platform better rather than worrying",
    "start": "694120",
    "end": "699200"
  },
  {
    "text": "about a lot of the lower level infra details so there are many different",
    "start": "699200",
    "end": "706000"
  },
  {
    "text": "layers in which you can do failure detection Sarah focused on the first layer in terms of detecting at the",
    "start": "706000",
    "end": "712639"
  },
  {
    "text": "workload layer with multiple hell checks but at the orchestrator layer which is the area",
    "start": "712639",
    "end": "719079"
  },
  {
    "text": "that that I'll focus on in this section we can run a variety of node health checks throughout the life cycle of the",
    "start": "719079",
    "end": "725440"
  },
  {
    "text": "node and that can be combined with other tools like the K device plug-in for gpus",
    "start": "725440",
    "end": "731519"
  },
  {
    "text": "for checking GPU Readiness then there's layers before",
    "start": "731519",
    "end": "736560"
  },
  {
    "text": "that that are also relevant like the virtual machine deployment layer so in this phase you can try to predict",
    "start": "736560",
    "end": "743399"
  },
  {
    "text": "whether the gpus will fail based on previous workloads for instance and if you have other signal related to latent",
    "start": "743399",
    "end": "750760"
  },
  {
    "text": "health issues you could also leverage that to decide whether or not the nodes can even be surfaced to the",
    "start": "750760",
    "end": "756560"
  },
  {
    "text": "orchestrator then at the very initial phase you also have testing during the",
    "start": "756560",
    "end": "762240"
  },
  {
    "text": "onboarding of large GPU clusters where you uh do some performance testing uh to",
    "start": "762240",
    "end": "768199"
  },
  {
    "text": "make sure that the set of gpus that you've purchased are appropriate and for most infr providers",
    "start": "768199",
    "end": "774279"
  },
  {
    "text": "whether or not it's cloud cloud or on Prem they would go through these various layers of pH",
    "start": "774279",
    "end": "781120"
  },
  {
    "text": "detection for the kubernetes case node problem detector is a standardized way to test and monitor node Health this is",
    "start": "781120",
    "end": "789720"
  },
  {
    "text": "the repo which is quite stable and at a high level node problem detector can be",
    "start": "789720",
    "end": "796320"
  },
  {
    "text": "run either as a demon set or a systemd unit on every node and it's got checks",
    "start": "796320",
    "end": "801920"
  },
  {
    "text": "to look at uh kernel monitoring or look at the system generated logs and you can",
    "start": "801920",
    "end": "807279"
  },
  {
    "text": "also have additional Pro problem demons for the test that you want to run then you also have another component",
    "start": "807279",
    "end": "814839"
  },
  {
    "text": "called remedy controller which is paired with the node problem detector typically to be able to take action on the",
    "start": "814839",
    "end": "821480"
  },
  {
    "text": "detections themselves and in today's talk I'm going to mainly focus on the open-source",
    "start": "821480",
    "end": "827839"
  },
  {
    "text": "components that you can use for almost any kubernetes environments this is uh an example of a",
    "start": "827839",
    "end": "835720"
  },
  {
    "text": "healthy node on an azary service cluster where we are running node problem Detector by default you will see that",
    "start": "835720",
    "end": "842720"
  },
  {
    "text": "there are a variety of node conditions that are populated on the Node based on",
    "start": "842720",
    "end": "848399"
  },
  {
    "text": "the Node problem detector's test so this is looking at things like the container run time whether that's up or not so if",
    "start": "848399",
    "end": "854720"
  },
  {
    "text": "container d goes down then this node condition would be set to true there's",
    "start": "854720",
    "end": "859800"
  },
  {
    "text": "also other conditions for memory pressure and network availability for instance but the problem or the Gap",
    "start": "859800",
    "end": "866800"
  },
  {
    "text": "right now with node problem detector is that of the tests are GPU specific and",
    "start": "866800",
    "end": "873000"
  },
  {
    "text": "we want to figure out how we can extend it to run GPU specific",
    "start": "873000",
    "end": "878240"
  },
  {
    "text": "tests taking a step back there's many ways to run GPU health checks uh some",
    "start": "878240",
    "end": "884320"
  },
  {
    "text": "could be like custom heal checks based on the specific gpus that you have and the specific configuration in your",
    "start": "884320",
    "end": "889920"
  },
  {
    "text": "cluster uh this has been done in the past uh where many INF providers would",
    "start": "889920",
    "end": "895399"
  },
  {
    "text": "might have like customized jobs for their HPC clusters to run uh GPU health",
    "start": "895399",
    "end": "901000"
  },
  {
    "text": "checks then there's another project from Lawrence Berkeley National Lab called",
    "start": "901000",
    "end": "906199"
  },
  {
    "text": "node health checks it's an open source project which has been there for a while and it has Integrations with multiple",
    "start": "906199",
    "end": "913480"
  },
  {
    "text": "HPC workload managers like slur and torque and one of the main advantages is",
    "start": "913480",
    "end": "919759"
  },
  {
    "text": "that it's reliable it's got many timers to prevent hangs for the health checks themselves the other advantages are that",
    "start": "919759",
    "end": "926839"
  },
  {
    "text": "it's extensible you can add more GPU related health checks and you could also reuse it across multiple workload",
    "start": "926839",
    "end": "934279"
  },
  {
    "text": "managers the Azure HPC health checks which are also an open source uh an",
    "start": "934279",
    "end": "940000"
  },
  {
    "text": "experimental approach extend the lbnl node health checks to test it with a",
    "start": "940000",
    "end": "947079"
  },
  {
    "text": "bunch of VMS which are supported in Azure so these include some of the multi-instance gpus from Nvidia and also",
    "start": "947079",
    "end": "953600"
  },
  {
    "text": "from AMD this is an example of of heal checks",
    "start": "953600",
    "end": "960519"
  },
  {
    "text": "that are run as part of these uh HPC heal checks you can find the repo here",
    "start": "960519",
    "end": "967000"
  },
  {
    "text": "too and one thing to note is that the K device plugin from Nvidia for instance",
    "start": "967000",
    "end": "972360"
  },
  {
    "text": "already has some health checks built in but we want to go beyond that to test things like bandwidth or you want to",
    "start": "972360",
    "end": "978880"
  },
  {
    "text": "test things like the uh memory errors or application related xid errors you you",
    "start": "978880",
    "end": "984560"
  },
  {
    "text": "you can actually extend it with these H checks uh so here you see a table where",
    "start": "984560",
    "end": "989839"
  },
  {
    "text": "there's expected GPU counts for particular uh multi-instance gpus and",
    "start": "989839",
    "end": "995360"
  },
  {
    "text": "you also have things like bandwidth expectations for infin band devices which also vary based on whether it's an",
    "start": "995360",
    "end": "1002360"
  },
  {
    "text": "a100 or an h100 or the GPU that you want to use it",
    "start": "1002360",
    "end": "1008160"
  },
  {
    "text": "with so this is a custom plugin for node problem detector which just leverages",
    "start": "1008160",
    "end": "1013880"
  },
  {
    "text": "those health checks to be able to run regularly during the node life cycles so",
    "start": "1013880",
    "end": "1019000"
  },
  {
    "text": "node problem detector will has some configuration to say okay this is the the script that I want to run right here",
    "start": "1019000",
    "end": "1025839"
  },
  {
    "text": "it's like checking GPU counts and you also have multiple fields which you can",
    "start": "1025839",
    "end": "1031600"
  },
  {
    "text": "use to configure it so things like timeouts for the heal checks or how frequently you want to run those health",
    "start": "1031600",
    "end": "1037240"
  },
  {
    "text": "checks can be configured then here's another uh",
    "start": "1037240",
    "end": "1043038"
  },
  {
    "text": "example of the script for testing your uh GPU health so here it's looking at a",
    "start": "1043039",
    "end": "1048480"
  },
  {
    "text": "variety of xid errors that are just defined here and you can also modify it if you need to and based on this it's",
    "start": "1048480",
    "end": "1055320"
  },
  {
    "text": "going to create node conditions so NPD as the output will be",
    "start": "1055320",
    "end": "1060679"
  },
  {
    "text": "creating node conditions of the node and also generating events based on the H checks and these custom plugins are",
    "start": "1060679",
    "end": "1066400"
  },
  {
    "text": "going to allow you to do it for GPU specific",
    "start": "1066400",
    "end": "1071120"
  },
  {
    "text": "checks so NPD is doing detection and now what do you do with those detections you",
    "start": "1073200",
    "end": "1078480"
  },
  {
    "text": "look at another component called remedy controller which is just a general term for a variety of projects which are",
    "start": "1078480",
    "end": "1085000"
  },
  {
    "text": "taking actions based on those detections Dro medicates are some of the",
    "start": "1085000",
    "end": "1090400"
  },
  {
    "text": "common open source solutions for that but then uh you have these two",
    "start": "1090400",
    "end": "1095840"
  },
  {
    "text": "components that both need to be deployed to make sure uh it's working as expected and sometimes Cloud providers also have",
    "start": "1095840",
    "end": "1103120"
  },
  {
    "text": "their internal implementations of these remedy controllers which allow you to take specific actions you could restart",
    "start": "1103120",
    "end": "1108880"
  },
  {
    "text": "start the node restart some of the GPU driver related components uh and take",
    "start": "1108880",
    "end": "1114600"
  },
  {
    "text": "actions based on the error that you see and this remedy controller typically can be configured to take uh specific",
    "start": "1114600",
    "end": "1121400"
  },
  {
    "text": "actions based on the errors and if you still have the error you can uh remove the node or and before that you know",
    "start": "1121400",
    "end": "1127200"
  },
  {
    "text": "drain and remove uh the the pods as well so one thing to know is how do you",
    "start": "1127200",
    "end": "1136240"
  },
  {
    "text": "actually make sure it's working I think that's an important question especially from the infr provider perspective you",
    "start": "1136240",
    "end": "1141559"
  },
  {
    "text": "can do simulation through different ways one is actually doing dropping the gpus",
    "start": "1141559",
    "end": "1147919"
  },
  {
    "text": "in your testing so you could run like Nvidia SMI drop commands to uh drain one",
    "start": "1147919",
    "end": "1154200"
  },
  {
    "text": "GPU if you have like multiple gpus in on your node and see if that's uh being",
    "start": "1154200",
    "end": "1159440"
  },
  {
    "text": "detected properly or you could also do Network Rel networking related load testing so you run a workload which",
    "start": "1159440",
    "end": "1166240"
  },
  {
    "text": "takes up the bandwidth and then in your testing make sure that that uh your node conditions are correctly populated when",
    "start": "1166240",
    "end": "1173520"
  },
  {
    "text": "you expect higher bandwidth this is an interesting project called quar uh which was talked about in",
    "start": "1173520",
    "end": "1179720"
  },
  {
    "text": "the keynote yesterday where you can use Virtual nodes to simulate uh your gpus",
    "start": "1179720",
    "end": "1186799"
  },
  {
    "text": "and also simulate nodes in general you can inject errors to your nodes and pod logs and see if that's being picked up",
    "start": "1186799",
    "end": "1193360"
  },
  {
    "text": "by your components I this is very promising at for the future particularly for GPU testing because it's expensive",
    "start": "1193360",
    "end": "1199520"
  },
  {
    "text": "to provision gpus now I'm going to show a demo of",
    "start": "1199520",
    "end": "1204600"
  },
  {
    "text": "these components coming together and keep in mind these are all open source and you can leverage it from multiple uh",
    "start": "1204600",
    "end": "1213520"
  },
  {
    "text": "infrastructures before I go into the demo I want to summarize what we're going to see we're going to have the custom GPU node problem detector Dro",
    "start": "1215360",
    "end": "1222799"
  },
  {
    "text": "which is the remedy controller and I'm going to do a simple test where I just drop the GPU uh when I have eight gpus",
    "start": "1222799",
    "end": "1230039"
  },
  {
    "text": "in that uh node and I want to see the node condition show",
    "start": "1230039",
    "end": "1235559"
  },
  {
    "text": "up so I have an a kuet service cluster I'm adding an a100 node right there",
    "start": "1237240",
    "end": "1243360"
  },
  {
    "text": "which has eight gpus I'm also skipping the GPU driver installation because I want that to be handled by the GPU",
    "start": "1243360",
    "end": "1250360"
  },
  {
    "text": "operator you'll see after fast weding you've got the the the a100 node and",
    "start": "1250360",
    "end": "1256240"
  },
  {
    "text": "then when you look at the operator Rel ated pods you see that the driver installation has been completed as well",
    "start": "1256240",
    "end": "1263520"
  },
  {
    "text": "and when you describe the node you'll see a variety of information including the node conditions there which right",
    "start": "1263520",
    "end": "1270840"
  },
  {
    "text": "now it's completely healthy node uh so you know you don't see any errors the node conditions are false now I'm",
    "start": "1270840",
    "end": "1278039"
  },
  {
    "text": "deploying the custom GPU node problem detector the Pod has been",
    "start": "1278039",
    "end": "1284120"
  },
  {
    "text": "created and it's going to be running GP related health checks you will see that",
    "start": "1284120",
    "end": "1289760"
  },
  {
    "text": "there are node conditions related to gpus um and some events as well that that were generated which are GPU",
    "start": "1289760",
    "end": "1297400"
  },
  {
    "text": "specific so here things like the GPU count is correct is set to false which is uh a note condition which checks",
    "start": "1297400",
    "end": "1304440"
  },
  {
    "text": "whether the expected number of gpus which is eight matches what it actually",
    "start": "1304440",
    "end": "1309559"
  },
  {
    "text": "sees and the check is running Nvidia SMI commands in the",
    "start": "1309559",
    "end": "1314760"
  },
  {
    "text": "background then you also see other events related to GPU H",
    "start": "1314760",
    "end": "1320600"
  },
  {
    "text": "checks now this is a Dro configuration here I have GPU related uh events and",
    "start": "1320600",
    "end": "1328640"
  },
  {
    "text": "note conditions that I want to look at so these like things like GPU cons are what I'm going to look at in this uh",
    "start": "1328640",
    "end": "1334559"
  },
  {
    "text": "deployment of dro and that's going to be used to drain the node when those conditions are set I've deployed Dro as",
    "start": "1334559",
    "end": "1342600"
  },
  {
    "text": "well and then we see the G the the kubernetes node for a100 still it's",
    "start": "1343880",
    "end": "1350080"
  },
  {
    "text": "marked as ready now I'm going to go inside the driver pod and then I'm going",
    "start": "1350080",
    "end": "1355159"
  },
  {
    "text": "to go uh drop the GPU so now you see eight gpus zero indexed and then I'm",
    "start": "1355159",
    "end": "1362440"
  },
  {
    "text": "going to drop uh with Nvidia s SMI commands one of those gpus and I'm also disabling persistence D",
    "start": "1362440",
    "end": "1370080"
  },
  {
    "text": "there so once I drop it I you'll see that there are seven gpus so 0 to 7 0 to",
    "start": "1370080",
    "end": "1377720"
  },
  {
    "text": "six there and then you'll see how uh this is going to be picked up by",
    "start": "1377720",
    "end": "1384720"
  },
  {
    "text": "the GP node problem protector you'll see the events are related to GPU count being bad as well",
    "start": "1384720",
    "end": "1392520"
  },
  {
    "text": "expected to see seven but you see expect to see eight but you see seven and then",
    "start": "1392520",
    "end": "1397559"
  },
  {
    "text": "the node conditions are set as well uh to indicate that you one of the gpus is",
    "start": "1397559",
    "end": "1405000"
  },
  {
    "text": "missing this is useful because some of this functionality is not Prov provided by the device plug-in because it just",
    "start": "1405000",
    "end": "1410320"
  },
  {
    "text": "changes the allocatable but you don't actually take actions based on these",
    "start": "1410320",
    "end": "1416480"
  },
  {
    "text": "outputs and then you see that you have a status that say scheduling disabled too so from a user perspective it's much",
    "start": "1416840",
    "end": "1424720"
  },
  {
    "text": "easier now because you directly see just the nodes where there are issues and the",
    "start": "1424720",
    "end": "1430120"
  },
  {
    "text": "infr provider can take care of all of all of these",
    "start": "1430120",
    "end": "1434360"
  },
  {
    "text": "components and",
    "start": "1436679",
    "end": "1440440"
  },
  {
    "text": "okay the challenge with this approach is that some of these checks depend on GPU",
    "start": "1451880",
    "end": "1458320"
  },
  {
    "text": "and network related config they're very specific to particular gpus so doing it across the fleet is hard because you",
    "start": "1458320",
    "end": "1464559"
  },
  {
    "text": "need to know the expected values for uh these gpus and the configuration and",
    "start": "1464559",
    "end": "1470240"
  },
  {
    "text": "that's possible to get especially if you have apis to the infr provider to get that information then you also need to",
    "start": "1470240",
    "end": "1477480"
  },
  {
    "text": "make sure that the health checks that you're running consume minimal resources and don't affect the actual workloads",
    "start": "1477480",
    "end": "1483559"
  },
  {
    "text": "because you don't want to have a check which is disrupting the workload itself so things like Nvidia SMI commands",
    "start": "1483559",
    "end": "1489000"
  },
  {
    "text": "typically are not that resource in intensive so it's fine to run them regularly but some of the infinite band",
    "start": "1489000",
    "end": "1495600"
  },
  {
    "text": "checks for instance might end up affecting the workload so you have to be careful about when you run it how",
    "start": "1495600",
    "end": "1500720"
  },
  {
    "text": "frequently you run it then you also need to distinguish between actionable",
    "start": "1500720",
    "end": "1506440"
  },
  {
    "text": "Hardware issue actionable software issues and actual hardware issues so the hardware issues typically you know you",
    "start": "1506440",
    "end": "1512360"
  },
  {
    "text": "cannot fix them on the fly from your node but there are certain components that can be fixed through restarts for",
    "start": "1512360",
    "end": "1519440"
  },
  {
    "text": "instance or taking other actions on your node so you need to differentiate between that and that's complex",
    "start": "1519440",
    "end": "1525240"
  },
  {
    "text": "especially when you have many components uh at the GPU layer",
    "start": "1525240",
    "end": "1531000"
  },
  {
    "text": "involved going further I want to share about some of the advanced developments in this field and what the potential or",
    "start": "1531000",
    "end": "1537600"
  },
  {
    "text": "the main gaps are right now this is a very nent space there's a lot of things",
    "start": "1537600",
    "end": "1542919"
  },
  {
    "text": "going on uh to make it easier to monitor gpus because right now customers often",
    "start": "1542919",
    "end": "1549159"
  },
  {
    "text": "and like end users typically have to think about failures but you don't want them to think about it um this is a",
    "start": "1549159",
    "end": "1556919"
  },
  {
    "text": "great project called checkpoint ReStore in user space for gpus checkpoint ReStore in user space has been there for",
    "start": "1556919",
    "end": "1563559"
  },
  {
    "text": "a while and that's a way to migrate your live processes and this has been",
    "start": "1563559",
    "end": "1570000"
  },
  {
    "text": "extended to checkpoint containers and there's a kubernetes feature as well called forensic checkpointing which was",
    "start": "1570000",
    "end": "1576840"
  },
  {
    "text": "motivated for security reasons but that can also be used for checkpointing your",
    "start": "1576840",
    "end": "1581960"
  },
  {
    "text": "uh container State and then you can leverage features like that to be able",
    "start": "1581960",
    "end": "1587840"
  },
  {
    "text": "to migrate your workloads easily and just to be clear this",
    "start": "1587840",
    "end": "1594960"
  },
  {
    "text": "checkpointing is different from the model level checkpointing because model checkpointing is done by the application",
    "start": "1594960",
    "end": "1600520"
  },
  {
    "text": "layer where you checkpoint the full model but here it's checkpointing the entire container so all of the memory",
    "start": "1600520",
    "end": "1607120"
  },
  {
    "text": "for instance should be captured and restored but it's criu has been well",
    "start": "1607120",
    "end": "1612880"
  },
  {
    "text": "tested for CPUs and it's being used in many places as well for that but for gpus this still more work to make it",
    "start": "1612880",
    "end": "1619440"
  },
  {
    "text": "natively integrated with kubernetes there's open source projects from Nvidia for instance where they are capturing",
    "start": "1619440",
    "end": "1627000"
  },
  {
    "text": "the GPU state from uh from the node as well like the Cuda State it's called cuda checkpoint is the name of the open",
    "start": "1627000",
    "end": "1633159"
  },
  {
    "text": "source project and there's similar projects from AMD too and the challenge",
    "start": "1633159",
    "end": "1638799"
  },
  {
    "text": "with gpus is that you need to capture the GPU State properly along with the",
    "start": "1638799",
    "end": "1643960"
  },
  {
    "text": "rest of the CPU State and the memory and other parts of the process",
    "start": "1643960",
    "end": "1649840"
  },
  {
    "text": "uh then the one of the root causes for the complexity also is that gpus are not",
    "start": "1649840",
    "end": "1657559"
  },
  {
    "text": "treated as a native resource in kubernetes uh compared to CPUs for instance and that leads to the need for",
    "start": "1657559",
    "end": "1665320"
  },
  {
    "text": "more and more uh custom Integrations and checks and needing to integrate it with",
    "start": "1665320",
    "end": "1671159"
  },
  {
    "text": "multiple components and kubernetes and there's a lot of work in the open source Community that's trying to make it",
    "start": "1671159",
    "end": "1677200"
  },
  {
    "text": "easier to uh manage",
    "start": "1677200",
    "end": "1681320"
  },
  {
    "text": "gpus I want to summarize the key takeaways from this presentation first",
    "start": "1682559",
    "end": "1687840"
  },
  {
    "text": "Sarah shared about why GPU failures are disruptive and expensive and how for ML training in particular that gets",
    "start": "1687840",
    "end": "1694840"
  },
  {
    "text": "compounded especially in terms of cost then we talked about multiple layers of",
    "start": "1694840",
    "end": "1699919"
  },
  {
    "text": "health checks we start off by application layer health checks that can be run before running the workload this",
    "start": "1699919",
    "end": "1705760"
  },
  {
    "text": "could be through init containers or you could have more advanced workload specific checks which are actually",
    "start": "1705760",
    "end": "1711080"
  },
  {
    "text": "looking at workload patterns throughout time then you could have orchestrator",
    "start": "1711080",
    "end": "1716600"
  },
  {
    "text": "level checks where you're running at the kubernetes layer or the HBC workload manager layer some node health checks",
    "start": "1716600",
    "end": "1723399"
  },
  {
    "text": "regularly in the background and that can be done for faster detection and",
    "start": "1723399",
    "end": "1729360"
  },
  {
    "text": "migrating workloads then in terms of resolution there's you start off by checkpointing",
    "start": "1729360",
    "end": "1735720"
  },
  {
    "text": "it's a great practice to make sure your model weights are captured regularly and can be restored later uh and then you",
    "start": "1735720",
    "end": "1743600"
  },
  {
    "text": "also have remedy Control Systems along with the node problem detector that I mentioned earlier where you move",
    "start": "1743600",
    "end": "1749640"
  },
  {
    "text": "workloads and take actions based on what you've detected uh so that you can either fix the node or move your",
    "start": "1749640",
    "end": "1756159"
  },
  {
    "text": "workload seamlessly to a different workload worker node and then uh you",
    "start": "1756159",
    "end": "1761360"
  },
  {
    "text": "want to also make sure you prevent the node from being reused again for other deployments",
    "start": "1761360",
    "end": "1768159"
  },
  {
    "text": "we briefly touched upon ongoing work in the field in terms of criu support for gpus in kubernetes and making GPU Health",
    "start": "1768159",
    "end": "1776960"
  },
  {
    "text": "more K its native so that it's much more seamless to manage uh GPU workloads on",
    "start": "1776960",
    "end": "1783440"
  },
  {
    "text": "kubernetes thank you everyone uh and we hope you have a great rest of the time in Hong Kong we've also added more links",
    "start": "1783440",
    "end": "1790600"
  },
  {
    "text": "to projects we've referenced there's a lot of interesting projects in the space and we'd recommend checking it out and",
    "start": "1790600",
    "end": "1797600"
  },
  {
    "text": "now we're open to questions thank you [Applause]",
    "start": "1797600",
    "end": "1807768"
  },
  {
    "text": "one of the uh one of the several gpus or noes",
    "start": "1830440",
    "end": "1835880"
  },
  {
    "text": "are just done because uh driver issues or runtime issues or other compil issues",
    "start": "1835880",
    "end": "1841360"
  },
  {
    "text": "uh so I want to know more about your uh strategies you will fall back to the",
    "start": "1841360",
    "end": "1847440"
  },
  {
    "text": "previous checkpoint or you will continue uh training process uh what is I want to",
    "start": "1847440",
    "end": "1854559"
  },
  {
    "text": "know more about your uh maybe your best practice thank",
    "start": "1854559",
    "end": "1859799"
  },
  {
    "text": "you this one um yeah I think um it depends on the um the",
    "start": "1859799",
    "end": "1866519"
  },
  {
    "text": "error um so if your if you your workloads can't run anymore um I think",
    "start": "1866519",
    "end": "1873039"
  },
  {
    "text": "it should be it should be migrated so you should uh deschedule it so that's the the actions that we mentioned and",
    "start": "1873039",
    "end": "1879360"
  },
  {
    "text": "then um and then the node is um like replaced or uh yeah the node is replaced",
    "start": "1879360",
    "end": "1885919"
  },
  {
    "text": "or repaired so yeah there are some actions that you can take to uh actually like um you mentioned like um driver",
    "start": "1885919",
    "end": "1892399"
  },
  {
    "text": "issues so you can restart your driver for example um to to save that um so",
    "start": "1892399",
    "end": "1898000"
  },
  {
    "text": "yeah it depends on like the impact it has on your",
    "start": "1898000",
    "end": "1901960"
  },
  {
    "text": "workload uh if if uh assuming that it's uh some uh Hardware GPU issues you have",
    "start": "1904679",
    "end": "1912279"
  },
  {
    "text": "to replace it with a new GPU so once you uh replace the",
    "start": "1912279",
    "end": "1918399"
  },
  {
    "text": "uh broken GPU with crack GPU uh you have to start for the very beginning or you",
    "start": "1918399",
    "end": "1924840"
  },
  {
    "text": "have other strategies yeah no so um I think to to maximize the efficiency of",
    "start": "1924840",
    "end": "1931120"
  },
  {
    "text": "your training and your training platform um you don't want to lose the time that you spent uh training um before the the",
    "start": "1931120",
    "end": "1937399"
  },
  {
    "text": "issue happened so um that's why we mentioned checkpointing and it's also like uh important um for Recovery to",
    "start": "1937399",
    "end": "1944840"
  },
  {
    "text": "have a efficient checkpointing um because that's going to decides how much time you lose basically",
    "start": "1944840",
    "end": "1951320"
  },
  {
    "text": "for Recovery uh so you um so as I as I mentioned before you've got like um",
    "start": "1951320",
    "end": "1958120"
  },
  {
    "text": "automate automated uh checkpointing that can be done uh on failure on exception",
    "start": "1958120",
    "end": "1963399"
  },
  {
    "text": "uh so it saves the checkpoint and then when you rem migrate the the the node you will restore from that checkpoint so",
    "start": "1963399",
    "end": "1970320"
  },
  {
    "text": "you don't need to start over again U you can just um pick up where you left off basically okay uh but once you",
    "start": "1970320",
    "end": "1978440"
  },
  {
    "text": "uh P back to the previous checkpoint uh I think for example you have to reset",
    "start": "1978440",
    "end": "1985360"
  },
  {
    "text": "the par training parameters like TP PP size again it will cost some extra time",
    "start": "1985360",
    "end": "1991799"
  },
  {
    "text": "or you have some uh you have some better solutions to uh just start the training",
    "start": "1991799",
    "end": "1999039"
  },
  {
    "text": "uh once you have uh replaced the broken GPU you you have anything uh special to",
    "start": "1999039",
    "end": "2006679"
  },
  {
    "text": "uh reduce the uh I mean the restoring",
    "start": "2006679",
    "end": "2011919"
  },
  {
    "text": "time yeah so I think your question is focused",
    "start": "2013240",
    "end": "2020159"
  },
  {
    "text": "a lot like it's very application specific right because you have the at the application layers where you specify how to restore uh checkpoints right and",
    "start": "2020159",
    "end": "2027679"
  },
  {
    "text": "how you also um make sure that you efficiently push the check points I think there is work that's done in terms",
    "start": "2027679",
    "end": "2034880"
  },
  {
    "text": "of um like even within the broad of Microsoft to make sure that you can checkpoint and upload uh your",
    "start": "2034880",
    "end": "2040760"
  },
  {
    "text": "checkpoints faster so that's one one level of efficiency in terms of being able to efficiently checkpoint and",
    "start": "2040760",
    "end": "2047880"
  },
  {
    "text": "restore from those model checkpoints so this would be the application year but it's there's also ways to uh speed that",
    "start": "2047880",
    "end": "2055158"
  },
  {
    "text": "part of the process okay uh but but even if you can fall back to and reload the",
    "start": "2055159",
    "end": "2061240"
  },
  {
    "text": "previous check phones uh but it should cost a lot of time especially for the large models like uh models larger than",
    "start": "2061240",
    "end": "2068720"
  },
  {
    "text": "gpts 3 right loading downloading has CA some time do you have any better",
    "start": "2068720",
    "end": "2074638"
  },
  {
    "text": "solutions to yeah actually I love that that part because I worked on a project related to that before it's called",
    "start": "2074639",
    "end": "2080919"
  },
  {
    "text": "artifact streaming and it leverages this open source project called overlay BD and that's mainly to do lazy image",
    "start": "2080919",
    "end": "2087440"
  },
  {
    "text": "pulling so you can speed up pod start times uh and that that's one way to just",
    "start": "2087440",
    "end": "2093118"
  },
  {
    "text": "get what you need when you need it so let's say you are trying to create uh you know some workload where you don't",
    "start": "2093119",
    "end": "2100119"
  },
  {
    "text": "need most of the contents in your container image assuming you're using a container image for your workload then",
    "start": "2100119",
    "end": "2105599"
  },
  {
    "text": "you don't need to download the entire container image before you start to run it so that's one way in which we speed",
    "start": "2105599",
    "end": "2112200"
  },
  {
    "text": "up your uh process to start running your workload again and then there's also",
    "start": "2112200",
    "end": "2117960"
  },
  {
    "text": "additional layers there so you could do P2P uh to speed it up where you pick up",
    "start": "2117960",
    "end": "2124000"
  },
  {
    "text": "your other image content from other nodes in your cluster okay so that you don't need to",
    "start": "2124000",
    "end": "2130440"
  },
  {
    "text": "download it from a remote storage which is much further away that's the the next layer where you optimize it further then",
    "start": "2130440",
    "end": "2136960"
  },
  {
    "text": "there's some features which are a little specific to um your actual cluster configuration",
    "start": "2136960",
    "end": "2143320"
  },
  {
    "text": "and your cloud provider where you end up um having nodes with the images sort of",
    "start": "2143320",
    "end": "2150480"
  },
  {
    "text": "pre-warmed as well where you already have sort of some backup nodes where you have the container image and there's",
    "start": "2150480",
    "end": "2156040"
  },
  {
    "text": "some features which you can use used to make that easy to use so that the next note that you pick up has those images",
    "start": "2156040",
    "end": "2162680"
  },
  {
    "text": "cached already um so that also speeds up your your workload and there's a few",
    "start": "2162680",
    "end": "2167800"
  },
  {
    "text": "more things that you can do to to speed up those starts okay I see but there should be some uh uh difficulties",
    "start": "2167800",
    "end": "2175560"
  },
  {
    "text": "because for example uh even if you can replace a new GPU un but the device ID",
    "start": "2175560",
    "end": "2181720"
  },
  {
    "text": "ID is rank ID is different right in the uh high toch level or deep speed level",
    "start": "2181720",
    "end": "2188680"
  },
  {
    "text": "you you have to uh I think it's harder for you to just inser this new ideas into the H",
    "start": "2188680",
    "end": "2196440"
  },
  {
    "text": "pipeline right you you have to restart your your work from the previous checkpoint actually right yeah we'll get",
    "start": "2196440",
    "end": "2203680"
  },
  {
    "text": "to that but just one uh survey before are there more questions just to be respectful of the time but we'll",
    "start": "2203680",
    "end": "2209040"
  },
  {
    "text": "definitely are interested in Sharing okay uh we can continue the conversation",
    "start": "2209040",
    "end": "2214119"
  },
  {
    "text": "later but quick answer there is that there are ALS Al operators that you can leverage to make that process seamless",
    "start": "2214119",
    "end": "2221720"
  },
  {
    "text": "so that you have these so if you have these configurations as environment variables it can be picked up later so",
    "start": "2221720",
    "end": "2228680"
  },
  {
    "text": "you may be familiar with things like Cub operator and pych related operators those can typically be leveraged but I",
    "start": "2228680",
    "end": "2235440"
  },
  {
    "text": "think for your specific case uh we can also talk more to see if you have some best practices to to share yeah thank",
    "start": "2235440",
    "end": "2242720"
  },
  {
    "text": "you okay yes one more question thank you",
    "start": "2242720",
    "end": "2247960"
  },
  {
    "text": "um thank you very much for the awesome presentations so um one questions I have is um what are the patterns of slowdowns",
    "start": "2247960",
    "end": "2256599"
  },
  {
    "text": "that you have um experienced like um GPU bang withd slowdown of course but um",
    "start": "2256599",
    "end": "2262319"
  },
  {
    "text": "maybe temperature changes cost the GPU to F go itself um is there any advice",
    "start": "2262319",
    "end": "2267720"
  },
  {
    "text": "around like how do we detect if we are doing training or if we are doing",
    "start": "2267720",
    "end": "2272839"
  },
  {
    "text": "inference um the slow down of the GPU is like observable and how do we what kind",
    "start": "2272839",
    "end": "2279280"
  },
  {
    "text": "of metric or kpi we should observe in in this case so thank",
    "start": "2279280",
    "end": "2285000"
  },
  {
    "text": "you yeah so that's one area that's that's um we haven't like fully covered",
    "start": "2285000",
    "end": "2290640"
  },
  {
    "text": "yet in terms of Automation and Remediation um but it's something that we observed in the logging so um in the",
    "start": "2290640",
    "end": "2297200"
  },
  {
    "text": "training logging we have the number of iterations per second um and that gives",
    "start": "2297200",
    "end": "2302280"
  },
  {
    "text": "you um for like a specific size of model and batch size and everything it it tells you exactly like um if they if",
    "start": "2302280",
    "end": "2309920"
  },
  {
    "text": "they're the same uh during two training runs um the iteration should be the same",
    "start": "2309920",
    "end": "2315000"
  },
  {
    "text": "for um when they're run a similar um platform um but we noticed that they um",
    "start": "2315000",
    "end": "2321480"
  },
  {
    "text": "you do like six times less iterations per second basically so that's how we noticed it currently we don't really",
    "start": "2321480",
    "end": "2327400"
  },
  {
    "text": "have anything to um detect that and other than the",
    "start": "2327400",
    "end": "2334440"
  },
  {
    "text": "logs one thing to add there as well is that from an infra perspective we have",
    "start": "2336440",
    "end": "2343680"
  },
  {
    "text": "some signals from the gpus but then there definitely should be a better way",
    "start": "2343680",
    "end": "2348760"
  },
  {
    "text": "in terms of knowing the source of truth of like what exactly is the Slowdown measure like how is it being I guess",
    "start": "2348760",
    "end": "2355800"
  },
  {
    "text": "propagated upwards right we need to know the underlying cause for that and ra you know what Sarah mentioned is at the very",
    "start": "2355800",
    "end": "2362200"
  },
  {
    "text": "at the application layer where you see the actual impact but ideally we want it to be at the GPU level itself where we",
    "start": "2362200",
    "end": "2369920"
  },
  {
    "text": "need to have a signal which just tells us hey this is an indicator of slowdown",
    "start": "2369920",
    "end": "2375599"
  },
  {
    "text": "and you want to report it up I think there's gaps and it's also very GPU specific based on which wend you use for",
    "start": "2375599",
    "end": "2382160"
  },
  {
    "text": "instance and that's something that we want to work with the vendors to figure out okay which which signals we can use",
    "start": "2382160",
    "end": "2388680"
  },
  {
    "text": "to be able to automatically detect it uh and then do those detection of the cloud layer so that the application layer does",
    "start": "2388680",
    "end": "2395119"
  },
  {
    "text": "not need to U worry about impact there yeah yeah great",
    "start": "2395119",
    "end": "2402960"
  },
  {
    "text": "question yeah thank you all oh there's one more one more question yeah I think",
    "start": "2402960",
    "end": "2408599"
  },
  {
    "text": "this should be the last one because we are past time uh so uh if one GPU uh devices uh",
    "start": "2408599",
    "end": "2416440"
  },
  {
    "text": "fail then maybe we have to uh reschedule the at least one port or we have to",
    "start": "2416440",
    "end": "2421720"
  },
  {
    "text": "restart all the job uh maybe um so uh if the port is res in another uh host then",
    "start": "2421720",
    "end": "2429839"
  },
  {
    "text": "on another host they have to download the checkpoint and recovery and I think this time cost right so maybe uh can can",
    "start": "2429839",
    "end": "2436920"
  },
  {
    "text": "we is make some U uh cascading recovery mechanism that maybe for some errors",
    "start": "2436920",
    "end": "2442680"
  },
  {
    "text": "that uh the the training framework make can can ignore the errors then they just we can uh make some notice to the we",
    "start": "2442680",
    "end": "2451000"
  },
  {
    "text": "have some mechanism to notice the the the training framework to know that there is an error but maybe they can",
    "start": "2451000",
    "end": "2456720"
  },
  {
    "text": "ignore it and then uh so for some errors maybe we can recover the the GPU by",
    "start": "2456720",
    "end": "2461839"
  },
  {
    "text": "restart and then we we start the put on the same node then they don't have to uh",
    "start": "2461839",
    "end": "2467240"
  },
  {
    "text": "download the checkpoint file maybe the recovery uh procedure will be faster",
    "start": "2467240",
    "end": "2473920"
  },
  {
    "text": "right yeah uh great question I think there's definitely uh errors which are",
    "start": "2473920",
    "end": "2479640"
  },
  {
    "text": "ignored and which are not so from the device plugin itself there's a set of Errors the application layer which are",
    "start": "2479640",
    "end": "2485040"
  },
  {
    "text": "ignored right and then when you run your G puu node Health uh checks as well you can decide which errors to ignore or not",
    "start": "2485040",
    "end": "2491960"
  },
  {
    "text": "right at the infra layer and at the application layer it's or the framework cayer it's always decided by the",
    "start": "2491960",
    "end": "2497760"
  },
  {
    "text": "framework itself in terms of choosing which uh errors to ignore and which ones should not be ignored uh but then I",
    "start": "2497760",
    "end": "2505079"
  },
  {
    "text": "guess in terms of I going back to the infr layer where you have this remedy controllers depending on the type of",
    "start": "2505079",
    "end": "2511880"
  },
  {
    "text": "error you can also choose to uh make take minimal actions and if it's satisfied then your workload can just",
    "start": "2511880",
    "end": "2518359"
  },
  {
    "text": "continue running uh in the same node so you don't have to pull the the part again right so it's all but currently we",
    "start": "2518359",
    "end": "2524560"
  },
  {
    "text": "don't have that mechanism right so we have to train the node or or uh",
    "start": "2524560",
    "end": "2530520"
  },
  {
    "text": "reschedule the Pole right yeah so it it depends on the remedy controller implementation right so there's another",
    "start": "2530520",
    "end": "2537680"
  },
  {
    "text": "question is that how do the remedy controller to uh restart the port on the",
    "start": "2537680",
    "end": "2542720"
  },
  {
    "text": "or restart the node on because we must have something running on the Node",
    "start": "2542720",
    "end": "2548520"
  },
  {
    "text": "rest must that that already exists in many Cloud providers typically where you have",
    "start": "2548520",
    "end": "2555160"
  },
  {
    "text": "like in aob service for instance there is a component called the remediator which uh takes actions for things like",
    "start": "2555160",
    "end": "2562000"
  },
  {
    "text": "you know container D being down for instance it has privileged permissions to to do those changes",
    "start": "2562000",
    "end": "2570359"
  },
  {
    "text": "yeah great thank you all so much and we appreciate how passionate you are at at",
    "start": "2571599",
    "end": "2577319"
  },
  {
    "text": "this time too and we can see that you'll love uh this topic so thanks everyone",
    "start": "2577319",
    "end": "2584550"
  },
  {
    "text": "[Applause]",
    "start": "2584550",
    "end": "2587459"
  }
]