[
  {
    "text": "all right hi so I'm Andrew Kim I'm a software engineer based in Toronto Canada where I work remotely forwarded",
    "start": "0",
    "end": "6299"
  },
  {
    "text": "ocean so my talk is going to be about our globally distributed container networks and how we integrated",
    "start": "6299",
    "end": "12480"
  },
  {
    "text": "kubernetes directly into our data center networking to offer what we think is a very effective networking model for our",
    "start": "12480",
    "end": "18840"
  },
  {
    "text": "engineers so my talk is going to be a technical deep dive and my hope is that",
    "start": "18840",
    "end": "24480"
  },
  {
    "text": "this talk gives you enough context that you can actually build a similar container networking for your own clusters and if if I didn't give enough",
    "start": "24480",
    "end": "31679"
  },
  {
    "text": "context I'd love to chat with you about how to do that for you for your clusters so before going there though I do want",
    "start": "31679",
    "end": "37530"
  },
  {
    "text": "to provide some background on kubernetes at digitalocean so our first production",
    "start": "37530",
    "end": "42629"
  },
  {
    "text": "deployment of kubernetes was way back when kubernetes version 1.1 was released so that was roughly two years ago early",
    "start": "42629",
    "end": "49379"
  },
  {
    "text": "2016 and at that time we were experimenting with kubernetes because we wanted to",
    "start": "49379",
    "end": "54809"
  },
  {
    "text": "reduce the operational overhead that a lot of engineers were struggling with so",
    "start": "54809",
    "end": "60090"
  },
  {
    "text": "we want to create a consistent platform that made it really easy for developers to deploy applications into production which I'm sure everyone here kind of had",
    "start": "60090",
    "end": "66960"
  },
  {
    "text": "that similar story so as we're looking into adopting kubernetes we faced a few",
    "start": "66960",
    "end": "72330"
  },
  {
    "text": "challenges one of them being that kubernetes wasn't moving target and it still is today so best practices and",
    "start": "72330",
    "end": "79530"
  },
  {
    "text": "features are constantly changing or sometimes just being deprecated and we also found that because kubernetes",
    "start": "79530",
    "end": "85439"
  },
  {
    "text": "offered so many features and knobs that you can tune a lot of develop a lot of",
    "start": "85439",
    "end": "90509"
  },
  {
    "text": "developers just didn't know where to start so on top of that the general impression of the time was that",
    "start": "90509",
    "end": "96180"
  },
  {
    "text": "kubernetes had too much ml and and no-one really wanted to learn what the metadata the api groups and all that",
    "start": "96180",
    "end": "101700"
  },
  {
    "text": "stuff was so out of that we built in an internal platform called GOCC which",
    "start": "101700",
    "end": "107189"
  },
  {
    "text": "stands for digital ocean control center so d LCC is a multi tenant internal path",
    "start": "107189",
    "end": "113040"
  },
  {
    "text": "built on top of kubernetes it's very simple API layer that translates a specification that we maintain into",
    "start": "113040",
    "end": "119579"
  },
  {
    "text": "kubernetes resources what's really powerful about this is that it's built with simplicity in mind while also",
    "start": "119579",
    "end": "125790"
  },
  {
    "text": "promoting and sometimes even enforcing some of the best practices that that we think that you should follow when you",
    "start": "125790",
    "end": "132150"
  },
  {
    "text": "deploy applications containers and with kubernetes so some examples are if a engineer deploy",
    "start": "132150",
    "end": "138950"
  },
  {
    "text": "something with the latest tab they just drop will just block it and be like no no latest tack and if you deploy an",
    "start": "138950",
    "end": "144290"
  },
  {
    "text": "application and into production and does have an in metric standpoint we'll just drop that thun it also meant that there",
    "start": "144290",
    "end": "150770"
  },
  {
    "text": "were no huge files of yeah more for developers to onboard them on our platform today it's a little bit",
    "start": "150770",
    "end": "156260"
  },
  {
    "text": "different where we have some engineers who really want that cubes ETL access and and is okay with managing ya know",
    "start": "156260",
    "end": "161480"
  },
  {
    "text": "and there are some engineers that don't but that's kind of a different story for another talk and as much as I do want to",
    "start": "161480",
    "end": "168320"
  },
  {
    "text": "talk about the internals of DCC more and kind of the motivation behind why we did this I don't have time but we did have",
    "start": "168320",
    "end": "173989"
  },
  {
    "text": "previous talks that previous cube cons that you can look at so two years later and we're managing DL CC at a pretty",
    "start": "173989",
    "end": "181790"
  },
  {
    "text": "large scale and do CC manages a lot of applications across all our data centers and across all our environments what we",
    "start": "181790",
    "end": "188810"
  },
  {
    "text": "found was really good at this large scale was that kubernetes provided an excellent multi-tenant environment where",
    "start": "188810",
    "end": "194330"
  },
  {
    "text": "resources were utilized more efficiently where we didn't have like a single machine running you know this really",
    "start": "194330",
    "end": "199340"
  },
  {
    "text": "small micro service and it did an excellent job of making sure containers were running where they should be",
    "start": "199340",
    "end": "204890"
  },
  {
    "text": "and we find that kubernetes provided all the great tools that may building a",
    "start": "204890",
    "end": "212000"
  },
  {
    "text": "platform really easy so it was great platform to build a platform on the flip",
    "start": "212000",
    "end": "217670"
  },
  {
    "text": "side of things once we're running at a larger scale we faced into a few challenges and it was",
    "start": "217670",
    "end": "223130"
  },
  {
    "text": "mostly around our networking so we found that our initial design of our pod networking was fast enough for a",
    "start": "223130",
    "end": "230360"
  },
  {
    "text": "majority of applications that are on GCC but it wasn't fast enough for the applications that were kind of latency",
    "start": "230360",
    "end": "235579"
  },
  {
    "text": "sensitive and had really high throughput we also found that our container network wasn't intuitive so we had source and",
    "start": "235579",
    "end": "243350"
  },
  {
    "text": "destination adding and developers has never really knew like which IPS were private and which IPS were like the",
    "start": "243350",
    "end": "249650"
  },
  {
    "text": "external ingress facing or public facing IPs and so we have to explain to developers a lot what those different",
    "start": "249650",
    "end": "255799"
  },
  {
    "text": "components were so I do want to dig a little deeper into why this was the case",
    "start": "255799",
    "end": "261250"
  },
  {
    "text": "so since we were pretty early adopters of kubernetes when we built our first production cluster we",
    "start": "261250",
    "end": "266960"
  },
  {
    "text": "use what was kind of well known at the time which was an overlaid pod network with sanam so this diagram kind of shows",
    "start": "266960",
    "end": "272930"
  },
  {
    "text": "the path of a packet between two pods using this overlay and I'm not going to",
    "start": "272930",
    "end": "278270"
  },
  {
    "text": "go into too much detail into how this works but the idea was that the packets went through a lot of different abstractions and and different layers on",
    "start": "278270",
    "end": "285050"
  },
  {
    "text": "the kernel for that pod - pod communication to work so some challenges",
    "start": "285050",
    "end": "290180"
  },
  {
    "text": "with this top this pod network topology was that the packet overhead became more",
    "start": "290180",
    "end": "295400"
  },
  {
    "text": "and more impactful to performance with higher throughput we also didn't like how every node in the cluster had to",
    "start": "295400",
    "end": "302030"
  },
  {
    "text": "have a route for every other node in order for the cluster networking to work because that made operating the individual nodes extremely difficult we",
    "start": "302030",
    "end": "309680"
  },
  {
    "text": "also didn't like how I FEMA's grading was essentially mandatory for any cluster network to work so on top of",
    "start": "309680",
    "end": "316190"
  },
  {
    "text": "this because we had this overlay and is confined to the cluster only if we wanted to ingress traffic into the",
    "start": "316190",
    "end": "322099"
  },
  {
    "text": "cluster we have to set up as I'm sure many of you know ingress controllers and if you've ever administered ingress",
    "start": "322099",
    "end": "327680"
  },
  {
    "text": "controllers you know the network ends up looking something like this where the ingress controller pods are configured",
    "start": "327680",
    "end": "333800"
  },
  {
    "text": "usually with a host port binding or there they run on host network so that they can actually receive external",
    "start": "333800",
    "end": "339139"
  },
  {
    "text": "traffic for the cluster so I just want to a quick poll raise your hand if you've used ingress controllers in",
    "start": "339139",
    "end": "344840"
  },
  {
    "text": "production okay a lot of you okay raise your hand if you have a love/hate",
    "start": "344840",
    "end": "349940"
  },
  {
    "text": "relationship with ingress controllers okay it's like half of you and I'll tell you why okay well I I definitely have a",
    "start": "349940",
    "end": "357199"
  },
  {
    "text": "love-hate relationship and I'll tell you why so what we really liked about ingress controllers was that we were able to standardize how traffic came",
    "start": "357199",
    "end": "362449"
  },
  {
    "text": "into the cluster so we can set connection timeouts that are global to the cluster and all those things and we",
    "start": "362449",
    "end": "368599"
  },
  {
    "text": "were able to use a lot of the kubernetes primitives to build mechanisms to kind of automatically failover in case we",
    "start": "368599",
    "end": "373880"
  },
  {
    "text": "lost a node that's running ingress controller so how do we do this so we did this by running our English",
    "start": "373880",
    "end": "379159"
  },
  {
    "text": "controllers as daemon sets with the node selector and then there would be this another service called label controller",
    "start": "379159",
    "end": "385490"
  },
  {
    "text": "and what it did was it always ensured that there were enough healthy nodes with this label and then it would see",
    "start": "385490",
    "end": "391820"
  },
  {
    "text": "kind of the state of the the nodes with this label and then it would think this wall cárdenas entry that we use for the",
    "start": "391820",
    "end": "397159"
  },
  {
    "text": "cluster so that would ensure that there was always enough nodes the English controller and if we move",
    "start": "397159",
    "end": "402830"
  },
  {
    "text": "labels on the notes for whatever reason the DNS synced automatically by a label",
    "start": "402830",
    "end": "408050"
  },
  {
    "text": "controller and then we can actually do this kind of stuff a lot that we even coined the term label development so as",
    "start": "408050",
    "end": "415460"
  },
  {
    "text": "you can tell by now this was really complicated and difficult to operate it worked but nonetheless very complicated",
    "start": "415460",
    "end": "422720"
  },
  {
    "text": "and no one really wanted to kind of explain how this works anyone so on top of that we had IPS being added both for",
    "start": "422720",
    "end": "430640"
  },
  {
    "text": "incoming and outgoing traffic so when an outside client needs to request and the ingress controller proxies the request",
    "start": "430640",
    "end": "436490"
  },
  {
    "text": "to a pod the pod will see the source IP as the ingress controller instead of the actual client IP",
    "start": "436490",
    "end": "442300"
  },
  {
    "text": "likewise if pods talk to services that are outside of a cluster the source IP would be n-- added to whatever the node",
    "start": "442300",
    "end": "449120"
  },
  {
    "text": "IP was which is kind of a requirement because if the source IP was the pot IP",
    "start": "449120",
    "end": "454130"
  },
  {
    "text": "and the outside application doesn't know how to route to that pod the networking would work so I had to be the node IP",
    "start": "454130",
    "end": "460550"
  },
  {
    "text": "for this set up to work so with all this in mind what was lacking was that there",
    "start": "460550",
    "end": "466280"
  },
  {
    "text": "was too much overhead from packet encapsulation it was extremely operationally complex to manage this",
    "start": "466280",
    "end": "472100"
  },
  {
    "text": "this hacky you node labels and DNS thinking it wasn't reliable reliable",
    "start": "472100",
    "end": "477410"
  },
  {
    "text": "enough at a large scale so once we had hundreds of nodes in the cluster and we lost in ingress it actually affected a",
    "start": "477410",
    "end": "483410"
  },
  {
    "text": "lot of applications in that cluster and there was lack of visibility for source and destination IP s so what we wanted",
    "start": "483410",
    "end": "491300"
  },
  {
    "text": "in this new network was something that was simple to operate had better visibility for source and destination IP",
    "start": "491300",
    "end": "497450"
  },
  {
    "text": "s something that skills with the cluster so as you add capacity into the cluster you're automatically scaling that",
    "start": "497450",
    "end": "503180"
  },
  {
    "text": "ingress capability and we wanted something that was just generally faster and then it had so much overhead on the",
    "start": "503180",
    "end": "508370"
  },
  {
    "text": "actual node so we started doing some discovery around what this what this",
    "start": "508370",
    "end": "513650"
  },
  {
    "text": "networking might look like and it was important that what we designed initially and put into production was",
    "start": "513650",
    "end": "520070"
  },
  {
    "text": "done right the first time because we know from experience that is extremely difficult to actually change the",
    "start": "520070",
    "end": "525770"
  },
  {
    "text": "container networking on a cluster that is actually already serving production traffic and if you did build it",
    "start": "525770",
    "end": "531020"
  },
  {
    "text": "incorrectly you always had to build a new cluster and then set the networking crack there and then",
    "start": "531020",
    "end": "536240"
  },
  {
    "text": "you have to move all your pods over and then you have to deprecate that cluster and I've done it once and it just it's a",
    "start": "536240",
    "end": "541910"
  },
  {
    "text": "big hassle no one wants to so during this discovery Mack browning my manager had a really interesting idea and asked",
    "start": "541910",
    "end": "549170"
  },
  {
    "text": "what if we can directly connect to a pod or service from anywhere on our network",
    "start": "549170",
    "end": "554300"
  },
  {
    "text": "which would essentially eliminate this barrier this imaginary barrier we had between what we called this regular",
    "start": "554300",
    "end": "561320"
  },
  {
    "text": "Network and what we called this container network which was always known to have this this complex overlay an",
    "start": "561320",
    "end": "567380"
  },
  {
    "text": "abstraction that developers had a hard time understanding and it was hard to",
    "start": "567380",
    "end": "572540"
  },
  {
    "text": "figure out what this might look like without grasping a better understanding of what our data center networking looked like and understanding the",
    "start": "572540",
    "end": "578990"
  },
  {
    "text": "internal internals of our data center network is also important to understand how we efficiently built this globally",
    "start": "578990",
    "end": "584210"
  },
  {
    "text": "distributed network so I want to be starting out with what we call a pod and it's not a kubernetes pod but it's a pod",
    "start": "584210",
    "end": "591080"
  },
  {
    "text": "kind of in the networking world which in this context represents a single component of compute and network in our",
    "start": "591080",
    "end": "596720"
  },
  {
    "text": "data centers you can think of a pod as a building block for our data centers where we host physical machines on racks",
    "start": "596720",
    "end": "603950"
  },
  {
    "text": "that are connected over an aggregation of switches so you can see in this diagram at the top of each rack you have",
    "start": "603950",
    "end": "610370"
  },
  {
    "text": "a pair of switches which are the Leafs in this diagram which are also often called the top of rack switches so we",
    "start": "610370",
    "end": "617330"
  },
  {
    "text": "run to top of rack switches per rack for redundancy and then each server in the rack is connected to each type of rack",
    "start": "617330",
    "end": "624290"
  },
  {
    "text": "with the 40g connection and then link to each top of rack is another tier of",
    "start": "624290",
    "end": "629360"
  },
  {
    "text": "switches called the spines and each spice which would then actually be connected to all the top of racks in the",
    "start": "629360",
    "end": "634700"
  },
  {
    "text": "same pod providing kind of that extra layer of redundancy but also allows a",
    "start": "634700",
    "end": "640010"
  },
  {
    "text": "lot more bandwidth into that comes from the other parts of the data center and",
    "start": "640010",
    "end": "645860"
  },
  {
    "text": "to connect pods together you have another aggregation there called the core switches and then you if you want",
    "start": "645860",
    "end": "651110"
  },
  {
    "text": "to kind of scale up the pot or Santilli you would then connect them through the core switches but even then you may want",
    "start": "651110",
    "end": "659420"
  },
  {
    "text": "to scale out even further but you can do by adding new pods connected to this new layer of course witches which we call",
    "start": "659420",
    "end": "665030"
  },
  {
    "text": "zones and then the core switches of each zone would be connected so we and still have networking across all the",
    "start": "665030",
    "end": "670100"
  },
  {
    "text": "different pods and zones and so I'm sure some of you are asking why do we need this because it seems really complicated",
    "start": "670100",
    "end": "676389"
  },
  {
    "text": "and the reason why so many data centers are designed this way is because there",
    "start": "676389",
    "end": "682310"
  },
  {
    "text": "are limitations with how many physical connections you could have for per switch and even in theory if you can",
    "start": "682310",
    "end": "688490"
  },
  {
    "text": "create a magical switch that had thousands of physical ports that you can actually connect every single machine in the data center to those switches would",
    "start": "688490",
    "end": "695389"
  },
  {
    "text": "actually just end up being single points of failure for your data center network so with data center is hosting thousands",
    "start": "695389",
    "end": "701449"
  },
  {
    "text": "of machines these sorts of networking topologies were created so we can actually connect our network in a way",
    "start": "701449",
    "end": "707240"
  },
  {
    "text": "that provides enough bandwidth and enough redundancy for our use cases so",
    "start": "707240",
    "end": "713839"
  },
  {
    "text": "really what we wanted was some way to integrate closely with this data center network because we know that data center",
    "start": "713839",
    "end": "719630"
  },
  {
    "text": "switches are fast and reliable so the switches used in data centers are programmed at the at the hardware level",
    "start": "719630",
    "end": "725750"
  },
  {
    "text": "making it significantly faster when making routing decisions and there's also an extremely high level of",
    "start": "725750",
    "end": "732139"
  },
  {
    "text": "redundancy that are that's kind of provided by the topology that I just showed so how do we do this how do we",
    "start": "732139",
    "end": "738560"
  },
  {
    "text": "integrate our systems with this data center network so it turns out there's actually protocol that's older than",
    "start": "738560",
    "end": "744110"
  },
  {
    "text": "probably most of us in this room called BGP and it's a protocol that lets us",
    "start": "744110",
    "end": "749390"
  },
  {
    "text": "actually communicate with with the switches to actually configure routes in our data center so in short BGP stands",
    "start": "749390",
    "end": "757070"
  },
  {
    "text": "for the border border gateway protocol it's in a inter autonomous system routing protocol so and really BGP is",
    "start": "757070",
    "end": "764810"
  },
  {
    "text": "kind of the the protocol that's that's kind of keeping the public Internet together and the way it works is that",
    "start": "764810",
    "end": "770480"
  },
  {
    "text": "the internet really is this interconnection of autonomous systems that are built upon a system of trust so",
    "start": "770480",
    "end": "776569"
  },
  {
    "text": "these autonomous systems can be digital O'Shane Google cloud or you're probably at your internet service provider and",
    "start": "776569",
    "end": "782810"
  },
  {
    "text": "these autonomous systems are uniquely identified using autonomous system number or an ASN and they appear amongst",
    "start": "782810",
    "end": "789649"
  },
  {
    "text": "each other to kind of share routing information either about itself or about another system that it peers with so I",
    "start": "789649",
    "end": "796250"
  },
  {
    "text": "am skipping a lot of important details on how this really works but this is kind of the gist you need to know for this talk and just like how there are public and",
    "start": "796250",
    "end": "803610"
  },
  {
    "text": "private IPs there are also public and private autonomous systems so within a",
    "start": "803610",
    "end": "808680"
  },
  {
    "text": "single autonomous system you could have many smaller autonomous systems that are appearing with each other sharing",
    "start": "808680",
    "end": "813960"
  },
  {
    "text": "routing information that is only contained within that larger autonomous system so going back to that network",
    "start": "813960",
    "end": "821160"
  },
  {
    "text": "topology in our data centers you can think of each tier in this topology to be an autonomous system and each of them",
    "start": "821160",
    "end": "828420"
  },
  {
    "text": "are peering with the switches that it's directly connected to and each data",
    "start": "828420",
    "end": "833850"
  },
  {
    "text": "center you can imagine is a public-facing autonomous system that's kind of advertising routes to the other",
    "start": "833850",
    "end": "839760"
  },
  {
    "text": "public katanas autonomous systems in the area but there's also bgp peering that happens between our own data centers",
    "start": "839760",
    "end": "845970"
  },
  {
    "text": "which are usually connected by dedicated links though that we manage to give us more predictable and reliable transport",
    "start": "845970",
    "end": "852690"
  },
  {
    "text": "between our data centers so this is what we call our backbone Network so with this in mind we start to think about how",
    "start": "852690",
    "end": "859830"
  },
  {
    "text": "we can fit kubernetes clusters as these bgp autonomous systems if we can do that",
    "start": "859830",
    "end": "865530"
  },
  {
    "text": "it meant that we can use BGP to dynamically advertise routes in our data centers and propagate IPs and our",
    "start": "865530",
    "end": "871110"
  },
  {
    "text": "clusters into our entire backbone network and luckily due to the the",
    "start": "871110",
    "end": "877140"
  },
  {
    "text": "nature of this Kloss topology we don't necessarily need to peer directly to all the different tiers in earth you know",
    "start": "877140",
    "end": "883230"
  },
  {
    "text": "and different tiers of switches in our data centers we can just peer with the the top of rack above each server and",
    "start": "883230",
    "end": "888810"
  },
  {
    "text": "the top of racks would then peer to all its known peers and then so it'd be it would peer with spines and then it",
    "start": "888810",
    "end": "894600"
  },
  {
    "text": "appeared to the core of the network so that meant from from a node per node",
    "start": "894600",
    "end": "899820"
  },
  {
    "text": "configuration standpoint and kind of all we have to do from there was to know",
    "start": "899820",
    "end": "905220"
  },
  {
    "text": "what is the internal autonomous system number for the cluster and how do we actually peer with this top-of-rack",
    "start": "905220",
    "end": "911070"
  },
  {
    "text": "switch so what is the the IP of the switch and what is the autonomous system number that the switches the top of rack",
    "start": "911070",
    "end": "916560"
  },
  {
    "text": "switches so to accomplish this we used a really cool open source project called cube router so it supports in cluster",
    "start": "916560",
    "end": "924620"
  },
  {
    "text": "internal BGP and external BGP peirong which is what we use to peer with our top of rack switches and you can also",
    "start": "924620",
    "end": "931200"
  },
  {
    "text": "configure it to advertise the pod and the service IPS to all of its beech Pearse and lastly you came with a lot of",
    "start": "931200",
    "end": "937980"
  },
  {
    "text": "other features that we didn't anticipate of meeting but it ended up being pretty helpful for some of our current and",
    "start": "937980",
    "end": "944040"
  },
  {
    "text": "future use cases which I'm going to talk about more year at the end of the slide at the top so to go back to the top of",
    "start": "944040",
    "end": "950880"
  },
  {
    "text": "our configuration really all we had to do on cube router let's tell it what is",
    "start": "950880",
    "end": "955920"
  },
  {
    "text": "the cluster autonomous system number which again you can think of as this unique identifier for a clusters but",
    "start": "955920",
    "end": "961680"
  },
  {
    "text": "then this this larger bottom system that is our data center and then how do you peer with this top-of-rack switch which",
    "start": "961680",
    "end": "967769"
  },
  {
    "text": "I mentioned earlier which is how what is the top of racks IP and what is the autonomous system that it's part of and",
    "start": "967769",
    "end": "974040"
  },
  {
    "text": "from there we can fully rely on cube router to advertise the IPS we need to",
    "start": "974040",
    "end": "979290"
  },
  {
    "text": "accomplish what we wanted which was to advertise the pod and the service IPS in all our clusters so by doing this what",
    "start": "979290",
    "end": "989670"
  },
  {
    "text": "we actually accomplished was that our service IPS were not only these virtual IPS that are only routable to",
    "start": "989670",
    "end": "996300"
  },
  {
    "text": "applications inside the same cluster but they're now these anycast IPS that are available in on our entire backbone",
    "start": "996300",
    "end": "1002959"
  },
  {
    "text": "network and I say any cast IPS because on the balance system we now have this",
    "start": "1002959",
    "end": "1008149"
  },
  {
    "text": "one too many Association where we route packets to one receiver out of many where each kubernetes node that's",
    "start": "1008149",
    "end": "1014449"
  },
  {
    "text": "running Kubb router becomes a potential receiver for that IP and we can easily achieve this by just running cube router",
    "start": "1014449",
    "end": "1020690"
  },
  {
    "text": "as a daemon set and we found that this was extremely powerful for engineers",
    "start": "1020690",
    "end": "1026319"
  },
  {
    "text": "because it meant that just by creating a community service which usually it was",
    "start": "1026319",
    "end": "1031640"
  },
  {
    "text": "just done by GOCC they're given this static IP that is globally routable and load balance",
    "start": "1031640",
    "end": "1037880"
  },
  {
    "text": "within our entire backbone network and with it also came you know automatic",
    "start": "1037880",
    "end": "1043159"
  },
  {
    "text": "health checking graceful termination and all the kind of great features that we know kubernetes is great at and lastly",
    "start": "1043159",
    "end": "1049669"
  },
  {
    "text": "that IP was completely owned by service owners and my team only had to worry about how are the IP is actually being",
    "start": "1049669",
    "end": "1056059"
  },
  {
    "text": "advertised within our datacenter and how many IP is do I want to allocate per cluster which is you know just setting",
    "start": "1056059",
    "end": "1061760"
  },
  {
    "text": "some flags on the control team so we did work to provide these globally routable",
    "start": "1061760",
    "end": "1068510"
  },
  {
    "text": "keys to engineers and though what we were aiming for was just a faster and intuitive and simpler to operate",
    "start": "1068510",
    "end": "1074960"
  },
  {
    "text": "container Network we actually found a lot of interesting improvements and news cases that came out of having this",
    "start": "1074960",
    "end": "1080510"
  },
  {
    "text": "topology for the first one which I've kind of talked about already was that it",
    "start": "1080510",
    "end": "1085550"
  },
  {
    "text": "became extremely easy for developers to start distributing their applications because they can kind of deploy the same",
    "start": "1085550",
    "end": "1092540"
  },
  {
    "text": "docc specification across all the regions and then from there it was kind of up to developers to either configure",
    "start": "1092540",
    "end": "1098420"
  },
  {
    "text": "a per region DNS with that IP or update our service discovery service with that",
    "start": "1098420",
    "end": "1103940"
  },
  {
    "text": "IP and engineers were really comfortable doing this because they knew that the service IPS that we give them are static",
    "start": "1103940",
    "end": "1111940"
  },
  {
    "text": "but to make it even easier we realize that we can actually configure a shared some net across all our clusters using",
    "start": "1111940",
    "end": "1119630"
  },
  {
    "text": "external IDs meaning that if more than one cluster is advertised in the same ip",
    "start": "1119630",
    "end": "1124640"
  },
  {
    "text": "we can rely on be GPS shortest path algorithm to find the closest data",
    "start": "1124640",
    "end": "1129920"
  },
  {
    "text": "center running a service and of course this made it even easier for service",
    "start": "1129920",
    "end": "1135170"
  },
  {
    "text": "owners because they're required to only manage this one single IP for a globally distributed application and you can",
    "start": "1135170",
    "end": "1141980"
  },
  {
    "text": "imagine that this makes a huge difference for some applications because they know they no longer have to cross",
    "start": "1141980",
    "end": "1147080"
  },
  {
    "text": "our Oceanic Lynn's to talk to services anymore also since the PATA IPS were also now",
    "start": "1147080",
    "end": "1155350"
  },
  {
    "text": "routable within our entire of data centers we can actually disable masquerading for egress traffic since we",
    "start": "1155350",
    "end": "1162230"
  },
  {
    "text": "now assume that the application that outside the cluster can respond back to that party so this made a huge",
    "start": "1162230",
    "end": "1169910"
  },
  {
    "text": "difference when it came to monitoring our district our distributed systems because we can correlate force IPS",
    "start": "1169910",
    "end": "1175700"
  },
  {
    "text": "directly back to services whereas before we have to correlate source IPS back to",
    "start": "1175700",
    "end": "1181070"
  },
  {
    "text": "note IPs and then we'd have to check all the pods running on that know what IP to figure out what service could possibly",
    "start": "1181070",
    "end": "1186440"
  },
  {
    "text": "be talking to another service so this was really helpful when it came to kind of monitoring things that pods talk to",
    "start": "1186440",
    "end": "1192620"
  },
  {
    "text": "outside the cluster this model also made configuring and operating the nodes in",
    "start": "1192620",
    "end": "1198710"
  },
  {
    "text": "the cluster significantly easier so we no longer had issue in production because this one node for",
    "start": "1198710",
    "end": "1204500"
  },
  {
    "text": "whatever reason couldn't converge a route to another node and these are the kind of the worst problems to have",
    "start": "1204500",
    "end": "1209600"
  },
  {
    "text": "because like you'd have connectivity from one pod to another but not other pods but here we can kind of just rely",
    "start": "1209600",
    "end": "1217039"
  },
  {
    "text": "on the top of rack switch to route traffic for the pod Network which meant that on up on the nodes we just needed",
    "start": "1217039",
    "end": "1222650"
  },
  {
    "text": "the route that goes to the containers local to the node and then we just need everything else we can just you know",
    "start": "1222650",
    "end": "1228350"
  },
  {
    "text": "send it to the top of rack and we can rely on the top of rack to manage that pond networking for us so this was this",
    "start": "1228350",
    "end": "1235789"
  },
  {
    "text": "is an interesting one so DNS was something that also kind of improved a lot with this topology so one day we",
    "start": "1235789",
    "end": "1242360"
  },
  {
    "text": "remembered that cube DNS has a service IP which is the service IP that's injected into the resolver file for pods",
    "start": "1242360",
    "end": "1249650"
  },
  {
    "text": "which is kind of how the in cluster DNS resolution works which meant that this IP that we use is also routable in our",
    "start": "1249650",
    "end": "1256880"
  },
  {
    "text": "data centers which meant that developers could easily use cube DNS as a utility",
    "start": "1256880",
    "end": "1262280"
  },
  {
    "text": "for applications that are not even in this in the kubernetes cluster but",
    "start": "1262280",
    "end": "1267409"
  },
  {
    "text": "instead what we did was we actually set an NS record in all our regional DNS",
    "start": "1267409",
    "end": "1273470"
  },
  {
    "text": "servers and we would configure them so that if a query matches a cluster domain for any of our clusters we could then",
    "start": "1273470",
    "end": "1280159"
  },
  {
    "text": "have the the regional DNS server query the regional cube DNS service and then cube dns would return either a service",
    "start": "1280159",
    "end": "1286909"
  },
  {
    "text": "IP or party if you're using a hello service back to the regional DNS server and then the regional DNS server was",
    "start": "1286909",
    "end": "1292669"
  },
  {
    "text": "would kind of give back to whatever queried it so this feature is actually",
    "start": "1292669",
    "end": "1297740"
  },
  {
    "text": "really useful because from a service discovery standpoint engineers didn't actually have to care if a service was",
    "start": "1297740",
    "end": "1303650"
  },
  {
    "text": "deployed to kubernetes cluster or not because the DNS domains can just work anywhere as long as you have",
    "start": "1303650",
    "end": "1309440"
  },
  {
    "text": "connectivity to that regional DNS server which almost always you did and we found",
    "start": "1309440",
    "end": "1314990"
  },
  {
    "text": "that this made migrating applications into and out of our communities cluster significantly easier because again you",
    "start": "1314990",
    "end": "1321530"
  },
  {
    "text": "wouldn't have to worry about what domains to use if you're in cluster or out of cluster you just use you just use the same domain everywhere and lastly we",
    "start": "1321530",
    "end": "1330409"
  },
  {
    "text": "no longer had you know all our needed ingress controllers and kind of all the operation operational",
    "start": "1330409",
    "end": "1336180"
  },
  {
    "text": "overhead that came with it so we still kept them around because many applications depended on it from our",
    "start": "1336180",
    "end": "1341880"
  },
  {
    "text": "previous generation of clusters but we were able to make significant architectural changes that made it so",
    "start": "1341880",
    "end": "1347550"
  },
  {
    "text": "much easier to operate ingress controllers so instead of binding pods",
    "start": "1347550",
    "end": "1353490"
  },
  {
    "text": "to the host network using daemon sets we can just create a service for the English controller and then use the",
    "start": "1353490",
    "end": "1359370"
  },
  {
    "text": "corresponding anycast IP for the DNS and this meant that we don't have to do this magic with daemon sets and node labels",
    "start": "1359370",
    "end": "1366270"
  },
  {
    "text": "we can just run the ingress controller as a deployment and just use the service IP so like any big change in our",
    "start": "1366270",
    "end": "1375390"
  },
  {
    "text": "networking or kind of any architectural change we definitely faced some challenges and we there was definitely",
    "start": "1375390",
    "end": "1381660"
  },
  {
    "text": "some trade-offs along the way so for one we were seeing this this is what I call",
    "start": "1381660",
    "end": "1387990"
  },
  {
    "text": "the proxy mesh it's where you route traffic for a service to node that isn't actually running any of the pods for the",
    "start": "1387990",
    "end": "1394440"
  },
  {
    "text": "service so then it reroutes traffic back out to another node which causes like which causes unnecessary bandwidth",
    "start": "1394440",
    "end": "1400800"
  },
  {
    "text": "between your cluster so luckily cube router does offer a solution to this",
    "start": "1400800",
    "end": "1406800"
  },
  {
    "text": "problem which I'm going to talk more about shortly next thing we noticed was that we now had this uniqueness",
    "start": "1406800",
    "end": "1413960"
  },
  {
    "text": "requirement for the pod and service IPS we were using across our clusters so in our previous overlay networking this was",
    "start": "1413960",
    "end": "1420810"
  },
  {
    "text": "never a problem because we can easily just use the same subnet for all our clusters and since those IPS were contained in the same pod network we",
    "start": "1420810",
    "end": "1428010"
  },
  {
    "text": "didn't care if they overlapped so when creating clusters we now had to be more",
    "start": "1428010",
    "end": "1433710"
  },
  {
    "text": "resourceful with how large the IP prefixes were and this became very important in capacity planning because",
    "start": "1433710",
    "end": "1439800"
  },
  {
    "text": "there's only so many private ipv4 IPS that you can actually use in your in",
    "start": "1439800",
    "end": "1445110"
  },
  {
    "text": "your entire network globally and lastly BGP route conversions and rebalancing is",
    "start": "1445110",
    "end": "1452040"
  },
  {
    "text": "was difficult to monitor it's it still is difficult to Margaret so as routes are added and removed Inter",
    "start": "1452040",
    "end": "1457920"
  },
  {
    "text": "clusters it was difficult for us to gain visibility into how quickly those routes",
    "start": "1457920",
    "end": "1463050"
  },
  {
    "text": "were converging over our backbone Network so in general we found it fast enough to handle",
    "start": "1463050",
    "end": "1468960"
  },
  {
    "text": "pretty much most of our failure scenarios but having more visibility to the path of a packet outside of our",
    "start": "1468960",
    "end": "1474899"
  },
  {
    "text": "clusters is something we definitely want to work on so luckily we do have very",
    "start": "1474899",
    "end": "1479970"
  },
  {
    "text": "experienced network engineers that we can page during incidents who kind of do have better visibility there so with",
    "start": "1479970",
    "end": "1487409"
  },
  {
    "text": "these challenges in mind there are definitely future improvements that that we want to make as well so going back to",
    "start": "1487409",
    "end": "1495690"
  },
  {
    "text": "the proxy mesh the reason we have this proxy mesh is because all the nodes in",
    "start": "1495690",
    "end": "1501000"
  },
  {
    "text": "your clusters are advertising all the service IDs and this is okay since we know that any node can receive traffic",
    "start": "1501000",
    "end": "1507899"
  },
  {
    "text": "for a service IP since its proxy can route it to the right node eventually",
    "start": "1507899",
    "end": "1513559"
  },
  {
    "text": "but there are cases where you really want to reduce that proxy mesh especially for applications that ingress",
    "start": "1513559",
    "end": "1519630"
  },
  {
    "text": "a lot of bandwidth and by following this model you're actually doubling the amount of bandwidth you actually need so",
    "start": "1519630",
    "end": "1526020"
  },
  {
    "text": "a cube router does provide a simple solution to this problem by specifying the services external external traffic",
    "start": "1526020",
    "end": "1531659"
  },
  {
    "text": "policy to local when you do this you configure the proxy to only route to",
    "start": "1531659",
    "end": "1536880"
  },
  {
    "text": "pods local to the node and then cube router would only advertise routes if the pot is at the pod for that service",
    "start": "1536880",
    "end": "1543750"
  },
  {
    "text": "actually exists on the node so doing this would actually eliminate the proxy meshes since the top of racks only route",
    "start": "1543750",
    "end": "1550230"
  },
  {
    "text": "to nodes advertising that IP and it's worth noting that by doing this we would",
    "start": "1550230",
    "end": "1555480"
  },
  {
    "text": "also preserve the source IP for ingress traffic whereas with our current model we still have matting for the for the",
    "start": "1555480",
    "end": "1561510"
  },
  {
    "text": "ingress causing the because we have the source netting that happens from queue proxy so something else we were",
    "start": "1561510",
    "end": "1568799"
  },
  {
    "text": "experimenting with to reduce the effects of that proxy mesh with something called DSR which stands for direct server",
    "start": "1568799",
    "end": "1575220"
  },
  {
    "text": "return so this is a feature supported also supported in cube router so in",
    "start": "1575220",
    "end": "1581039"
  },
  {
    "text": "short the way it works is you you end up modifying the packets return address as it goes through the proxy so that the",
    "start": "1581039",
    "end": "1587850"
  },
  {
    "text": "receiving pot could actually so that the receiving pot ends up returning packets directly back to the client instead of",
    "start": "1587850",
    "end": "1594210"
  },
  {
    "text": "going back out through the proxy so this is a really neat feature and you'll find it used in a lot of large-scale little",
    "start": "1594210",
    "end": "1600419"
  },
  {
    "text": "bouncers but we found kind of less of a need for this since we can just rely on that external traffic policy I'll talk about earlier",
    "start": "1600419",
    "end": "1608450"
  },
  {
    "text": "we also want to see what it looks like to expose kubernetes services directly to the public Internet",
    "start": "1608480",
    "end": "1614310"
  },
  {
    "text": "by advertising the public IP straight from cue batter so there are there definitely a lot of considerations here",
    "start": "1614310",
    "end": "1620540"
  },
  {
    "text": "such as what does BGP rep convergence look like when you're appearing with all these publicly available autonomous",
    "start": "1620540",
    "end": "1626070"
  },
  {
    "text": "systems and and of course digitalocean since we are a public cloud we already",
    "start": "1626070",
    "end": "1632550"
  },
  {
    "text": "do that at a pretty large scale but if we wanted to know how we could actually but but we want to know how it could",
    "start": "1632550",
    "end": "1638040"
  },
  {
    "text": "actually change the way developers actually manage their applications when they have kind of that control also for",
    "start": "1638040",
    "end": "1644700"
  },
  {
    "text": "a kubernetes cluster that's receiving public traffic what sort of pod security best practices should be in place that",
    "start": "1644700",
    "end": "1650700"
  },
  {
    "text": "so that this is actually doable and what does it look like to use cube routers network networking policy feature to",
    "start": "1650700",
    "end": "1657060"
  },
  {
    "text": "enforce stronger ingress and egress rules for our services and lastly we",
    "start": "1657060",
    "end": "1663270"
  },
  {
    "text": "want to use ipv6 with our containers we're not there yet but we'll get there one day okay so to recap we found that",
    "start": "1663270",
    "end": "1673820"
  },
  {
    "text": "there are a lot of interesting ways to design networks especially when you have",
    "start": "1673820",
    "end": "1679170"
  },
  {
    "text": "that context like when you're in the context of a data center so we found that the combination of using protocol",
    "start": "1679170",
    "end": "1685620"
  },
  {
    "text": "like BGP that is widely used in data centers and and new technologies like",
    "start": "1685620",
    "end": "1690720"
  },
  {
    "text": "kubernetes can be extremely powerful the combination of relying on kubernetes to",
    "start": "1690720",
    "end": "1696090"
  },
  {
    "text": "manage services and endpoints and then being able to sync that state into the context of our data center networking",
    "start": "1696090",
    "end": "1701940"
  },
  {
    "text": "gives us a lot of interesting ways to tackle common problems in distributed systems more importantly though we found",
    "start": "1701940",
    "end": "1710430"
  },
  {
    "text": "that designing a container network that is that is simple and intuitive to understand goes a really long way",
    "start": "1710430",
    "end": "1717530"
  },
  {
    "text": "there was definitely a journey for my team trying to understand the data center networking and adopting new",
    "start": "1717530",
    "end": "1723450"
  },
  {
    "text": "technologies to make this work but what we actually ended up offering to our engineers which was this extremely",
    "start": "1723450",
    "end": "1729300"
  },
  {
    "text": "simple to understand that working model made a big difference when it came to adopting containers and",
    "start": "1729300",
    "end": "1734900"
  },
  {
    "text": "making it easy for engineers to actually build reliable applications so there are",
    "start": "1734900",
    "end": "1741500"
  },
  {
    "text": "quite a few topics I didn't get to cover we definitely learned a lot kind of running this in production one of the",
    "start": "1741500",
    "end": "1747680"
  },
  {
    "text": "things I really want to talk about was can you actually do this on the cloud because in my context we actually have access to the data center networking and",
    "start": "1747680",
    "end": "1754490"
  },
  {
    "text": "you can do this on the cloud and there is definitely some more architectural and security considerations that that I",
    "start": "1754490",
    "end": "1761300"
  },
  {
    "text": "didn't get to cover but I don't think I have time but if any of these topics do interest you please approach me I'd love",
    "start": "1761300",
    "end": "1767900"
  },
  {
    "text": "to chat all these things kind of really trust me so if you have any opinions or you want to chat more about this please",
    "start": "1767900",
    "end": "1773300"
  },
  {
    "text": "approach me at any point during the conference and with that thank you for listening to my talk so before I do QA",
    "start": "1773300",
    "end": "1785990"
  },
  {
    "text": "QA I did want to take a second to announce that with the many years of",
    "start": "1785990",
    "end": "1791300"
  },
  {
    "text": "experience we have running kubernetes in production we will be providing a managed kubernetes offering soon so",
    "start": "1791300",
    "end": "1798500"
  },
  {
    "text": "questions know so well the the",
    "start": "1798500",
    "end": "1813230"
  },
  {
    "text": "networking model here was for the kubernetes cluster to be used to actually manage the internal services we have but the drop the droplet networking",
    "start": "1813230",
    "end": "1820370"
  },
  {
    "text": "would be different yeah but obviously we definitely use BGP in like the public Internet but yeah so I'm actually not on",
    "start": "1820370",
    "end": "1833930"
  },
  {
    "text": "the team that manages the the droplet networking so I I can't tell you sorry",
    "start": "1833930",
    "end": "1839050"
  },
  {
    "text": "yeah sorry sorry I can't hear you",
    "start": "1839050",
    "end": "1848830"
  },
  {
    "text": "okay yeah yes so the question was how we",
    "start": "1851050",
    "end": "1865130"
  },
  {
    "text": "considered using route reflectors essentially route reflectors that you if",
    "start": "1865130",
    "end": "1870800"
  },
  {
    "text": "you have a if you have a node with if you have a cluster with 100 nodes maybe only five of them are actually appearing",
    "start": "1870800",
    "end": "1876740"
  },
  {
    "text": "with the top of racks and then the other ninety five would peer with the the nodes that's kind of what reflectors do",
    "start": "1876740",
    "end": "1881960"
  },
  {
    "text": "so we actually did not have to use route reflectors yet and that's mostly because",
    "start": "1881960",
    "end": "1888710"
  },
  {
    "text": "kind of the switches we use are actually really beefy and powerful so we didn't have to do that yet",
    "start": "1888710",
    "end": "1893990"
  },
  {
    "text": "but it's definitely on our if we have scalability issues the first thing we want to try is use rob reflectors yeah",
    "start": "1893990",
    "end": "1904810"
  },
  {
    "text": "yeah sorry can you repeat that with with",
    "start": "1908050",
    "end": "1914450"
  },
  {
    "text": "external IP oh so cube that's part of cube routers features it also checks the external IDs that are attached to",
    "start": "1914450",
    "end": "1920570"
  },
  {
    "text": "services all right so so we don't",
    "start": "1920570",
    "end": "1926930"
  },
  {
    "text": "actually do public eyepiece yet for the cluster but we would probably get a pull from our networking team if we wanted to",
    "start": "1926930",
    "end": "1932840"
  },
  {
    "text": "that in future yes",
    "start": "1932840",
    "end": "1941110"
  },
  {
    "text": "yeah so um we don't do network policy enforcement's yet but cube router does",
    "start": "1946840",
    "end": "1953510"
  },
  {
    "text": "support it and the way it works I'm sorry I'll repeat the question so how are we enforcing networking policies between our pods and between our",
    "start": "1953510",
    "end": "1959059"
  },
  {
    "text": "clusters so cube routers so there's an actual API in kubernetes that actually",
    "start": "1959059",
    "end": "1964760"
  },
  {
    "text": "has a spec for what a network network and policy looks like and cube router is just a consumer of that network policy",
    "start": "1964760",
    "end": "1970490"
  },
  {
    "text": "type and then it will create kind of IP table rules on the actual nodes to enforce that policy so there's like an",
    "start": "1970490",
    "end": "1977299"
  },
  {
    "text": "ingress and egress spec that you can specify so that out be up to the",
    "start": "1977299",
    "end": "1989299"
  },
  {
    "text": "developers because of the developer knows like what something that's the so if a service a and cluster a oxes",
    "start": "1989299",
    "end": "1995809"
  },
  {
    "text": "service B in cluster B the developer should know what the source and destination IP is between the clusters should be so they can set their network",
    "start": "1995809",
    "end": "2002200"
  },
  {
    "text": "policies themselves we haven't hashed that out though because if you haven't used network policies yet because this",
    "start": "2002200",
    "end": "2007570"
  },
  {
    "text": "is all in kind of our internal network so we don't have to worry too much about that were policies but we definitely want to dig into that",
    "start": "2007570",
    "end": "2014850"
  },
  {
    "text": "sorry can you repeat the question I couldn't hear you okay so the question",
    "start": "2030169",
    "end": "2036679"
  },
  {
    "text": "was do we plan to use cow yes so the",
    "start": "2036679",
    "end": "2042080"
  },
  {
    "text": "question was do we plan to use calicoes BGP feature so the when we were",
    "start": "2042080",
    "end": "2048460"
  },
  {
    "text": "initially kind of looking at technologies to use for this we were debating between calico key router or to",
    "start": "2048460",
    "end": "2055520"
  },
  {
    "text": "just build our own the reason why we didn't stick with calcoast because it didn't actually support advertising the",
    "start": "2055520",
    "end": "2062148"
  },
  {
    "text": "service at Pease and we'd have to build that in and so we and cue powder already at the pod and the service and external",
    "start": "2062149",
    "end": "2068839"
  },
  {
    "text": "IPS and it also supported Network policy so we can't just run with that yeah so",
    "start": "2068839",
    "end": "2101540"
  },
  {
    "text": "the question was around um why we didn't use metal lp's out yeah essentially we",
    "start": "2101540",
    "end": "2107660"
  },
  {
    "text": "didn't use a metal lb because we really want that the pod IPS to be advertised and metal LP does not support the",
    "start": "2107660",
    "end": "2114980"
  },
  {
    "text": "advertising of parties and the reason why we actually want to use parties is this really interesting use case where",
    "start": "2114980",
    "end": "2120710"
  },
  {
    "text": "we actually want to run from ETS outside of the actual cluster because we don't want this dependency loop between your clusters and Prometheus and with all the",
    "start": "2120710",
    "end": "2127460"
  },
  {
    "text": "parties kind of being where you can actually scrape the parties outside the cluster you can actually have managed to",
    "start": "2127460",
    "end": "2133130"
  },
  {
    "text": "burn a managed prometheus outside of your cluster which is why we wanted the valuable polities",
    "start": "2133130",
    "end": "2139990"
  },
  {
    "text": "yeah so the question was do we allow automatic like dynamically assigned",
    "start": "2147400",
    "end": "2153470"
  },
  {
    "text": "service ip's and for most services we actually let the control plane the kubernetes control plane assign the",
    "start": "2153470",
    "end": "2159560"
  },
  {
    "text": "dynamic IP out of the subnet we tell it it can use and then from there as long as you don't delete your service then",
    "start": "2159560",
    "end": "2165740"
  },
  {
    "text": "that's pretty much a static IP for you and even if you deleted it if you knew what the IP was you can recreate the",
    "start": "2165740",
    "end": "2171800"
  },
  {
    "text": "service and specify exactly which cluster IP that you want it assuming someone else didn't create a service in",
    "start": "2171800",
    "end": "2177800"
  },
  {
    "text": "the meantime that use that I peek yes I",
    "start": "2177800",
    "end": "2191540"
  },
  {
    "text": "didn't cover that so I can try with you after yeah yeah no worries okay I don't",
    "start": "2191540",
    "end": "2197240"
  },
  {
    "text": "know how much time we have yeah so I think we're out of time so yeah I'll add all answer more questions",
    "start": "2197240",
    "end": "2203690"
  },
  {
    "text": "come to the digital booth I'll be there most of today and tomorrow all right thanks [Applause]",
    "start": "2203690",
    "end": "2211389"
  }
]