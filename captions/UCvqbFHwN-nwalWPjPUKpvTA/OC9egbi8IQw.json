[
  {
    "text": "uh welcome this time live from Amsterdam kubecon I'm so happy to be with you all",
    "start": "840",
    "end": "8460"
  },
  {
    "text": "um if you've ever heard me saying those same words that means you've been either on sick apps uh weekly meetings well",
    "start": "8460",
    "end": "15719"
  },
  {
    "text": "bi-weekly meetings or eventually on six July which I'm also helping to run",
    "start": "15719",
    "end": "22920"
  },
  {
    "text": "so on to things like I said my name is mache and I'm",
    "start": "22920",
    "end": "28680"
  },
  {
    "text": "also one of the chairs for Sega apps the other two chairs and Technical leads are",
    "start": "28680",
    "end": "36660"
  },
  {
    "text": "Janet Kan who unfortunately were not able to join us today in this beautiful",
    "start": "36660",
    "end": "43739"
  },
  {
    "text": "Amsterdam so if you have any issues about how the cigaps operates",
    "start": "43739",
    "end": "50100"
  },
  {
    "text": "if you have any particular questions about technical stuff related with what",
    "start": "50100",
    "end": "55800"
  },
  {
    "text": "sigups does and what you want to see being worked on those are the three",
    "start": "55800",
    "end": "61020"
  },
  {
    "text": "people that you want to start talking with so how to best reach us aside from today",
    "start": "61020",
    "end": "68280"
  },
  {
    "text": "and we are trying to be present on all of the kubecons and give a little bit of",
    "start": "68280",
    "end": "74100"
  },
  {
    "text": "a give a little bit of about ourselves give some updates on what we're working",
    "start": "74100",
    "end": "79380"
  },
  {
    "text": "on you can also find us on the bi-weekly meetings every Monday the next one is",
    "start": "79380",
    "end": "84900"
  },
  {
    "text": "planned on May 1st there are times depending on which times are you're on if the meeting just does",
    "start": "84900",
    "end": "92759"
  },
  {
    "text": "not work for you there there's always the opportunity to jump on kubernetes slack Channel their Sig apps and either",
    "start": "92759",
    "end": "100619"
  },
  {
    "text": "directly one of us or just leave a question we're trying to go through all",
    "start": "100619",
    "end": "106140"
  },
  {
    "text": "the questions all the suggestions and respond to them",
    "start": "106140",
    "end": "111600"
  },
  {
    "text": "lastly but not least there's also an email group if that's your preference you can write an email ask questions",
    "start": "111600",
    "end": "119280"
  },
  {
    "text": "propose suggestions and we'll be talking about suggestions in a in a moment",
    "start": "119280",
    "end": "125520"
  },
  {
    "text": "so what does sigups is responsible for in short we are responsible for",
    "start": "125520",
    "end": "131039"
  },
  {
    "text": "deploying running and operating applications on kubernetes",
    "start": "131039",
    "end": "136440"
  },
  {
    "text": "there is a link that you can read the entire Charter which describes our",
    "start": "136440",
    "end": "141660"
  },
  {
    "text": "mission but that's roughly a short description if you're interested in presenting to",
    "start": "141660",
    "end": "148080"
  },
  {
    "text": "Cigars whether your solution it doesn't have to be specifically uh working or",
    "start": "148080",
    "end": "155760"
  },
  {
    "text": "changing how kubernetes work but if you are deploying an application you are struggling with",
    "start": "155760",
    "end": "161700"
  },
  {
    "text": "um with the implementation or with running your application on kubernetes this is the place that you can always",
    "start": "161700",
    "end": "168000"
  },
  {
    "text": "pump uh ping us and we can either help you with your application or if you have",
    "start": "168000",
    "end": "174360"
  },
  {
    "text": "any suggestions or you would like to present your ideas um we are more than happy to see you",
    "start": "174360",
    "end": "181920"
  },
  {
    "text": "join one of our sessions or bring this topic to our attention",
    "start": "181920",
    "end": "187019"
  },
  {
    "text": "I've also linked the annual report if by any chance the annual report should be",
    "start": "187019",
    "end": "192480"
  },
  {
    "text": "merged within a day or two if you by any chance to check the link before that",
    "start": "192480",
    "end": "198180"
  },
  {
    "text": "there is a PR still open with the annual report for 2022. so don't freak out the",
    "start": "198180",
    "end": "204540"
  },
  {
    "text": "the annual report is empty only for one or two more days it's already approved",
    "start": "204540",
    "end": "209760"
  },
  {
    "text": "by the steering committee it just didn't merge on time so",
    "start": "209760",
    "end": "216060"
  },
  {
    "text": "I did mention that we will be we are doing those presentation every roughly",
    "start": "216060",
    "end": "221400"
  },
  {
    "text": "six months so every kubecon so since last cubecon in Valencia we basically",
    "start": "221400",
    "end": "228540"
  },
  {
    "text": "released three uh major versions of kubernetes 125.26 and just recently last week 127.",
    "start": "228540",
    "end": "236700"
  },
  {
    "text": "so I would like to cover the stuff the features that we promoted over the alpha",
    "start": "236700",
    "end": "242099"
  },
  {
    "text": "beta and finally to uh to stable so let's start with the stable features",
    "start": "242099",
    "end": "247140"
  },
  {
    "text": "that landed in the past three releases the first one is we finally stabilized",
    "start": "247140",
    "end": "254159"
  },
  {
    "text": "the max search for Dame instead the main reason to be able uh to uh or",
    "start": "254159",
    "end": "261359"
  },
  {
    "text": "let me go back uh the feature basically allows you to set",
    "start": "261359",
    "end": "267300"
  },
  {
    "text": "um an additional number of of nodes that can run more than a single part of a",
    "start": "267300",
    "end": "273180"
  },
  {
    "text": "Daemon set normally uh we will be only running one node on one part on one note",
    "start": "273180",
    "end": "279720"
  },
  {
    "text": "but during rolling upgrade if you want to maintain a or minimize the downtime",
    "start": "279720",
    "end": "285180"
  },
  {
    "text": "of your Daemon set you would like to be able to progress in a little bit faster",
    "start": "285180",
    "end": "290460"
  },
  {
    "text": "pace and allow during the upgrade to run more parts of",
    "start": "290460",
    "end": "295560"
  },
  {
    "text": "your Daemon set of course it has some downsides and you have to be aware of if you're a Daemon set pot are",
    "start": "295560",
    "end": "303419"
  },
  {
    "text": "um heavy consumers of resources you need to be aware that at some point in time",
    "start": "303419",
    "end": "309780"
  },
  {
    "text": "there will be two parts consuming double the usual resources but you can steer",
    "start": "309780",
    "end": "315060"
  },
  {
    "text": "that number either through a direct number of parts or by a percentage of",
    "start": "315060",
    "end": "320759"
  },
  {
    "text": "number of parts that your demon set is running the other the other feature that we promoted to",
    "start": "320759",
    "end": "327600"
  },
  {
    "text": "stable is mineral Min ready seconds for stateful sets so previously",
    "start": "327600",
    "end": "334880"
  },
  {
    "text": "the stateful set controller had a very fixed num had a fixed timeout for how",
    "start": "334880",
    "end": "341280"
  },
  {
    "text": "long it waited for an application to be fully ready but with stateful sets it",
    "start": "341280",
    "end": "347580"
  },
  {
    "text": "might take a little bit longer for the application to to become fully available the most frequently used example was",
    "start": "347580",
    "end": "356520"
  },
  {
    "text": "we want to ensure that the caches for my applications are warm enough so that I",
    "start": "356520",
    "end": "362400"
  },
  {
    "text": "can serve the traffic already at my full potential and not",
    "start": "362400",
    "end": "368460"
  },
  {
    "text": "wait additional more time out there can be probably additional more use cases",
    "start": "368460",
    "end": "373620"
  },
  {
    "text": "but that's where setting that time out and extending the um the timeout for how long it takes for",
    "start": "373620",
    "end": "381539"
  },
  {
    "text": "the Pod to be considered fully ready it is now possible for a user",
    "start": "381539",
    "end": "387000"
  },
  {
    "text": "to Define that value and let's switch track a little bit so",
    "start": "387000",
    "end": "394740"
  },
  {
    "text": "the two features that I was talking about were covering the apps area so",
    "start": "394740",
    "end": "400259"
  },
  {
    "text": "basically the long running workloads the other side of the sigops per view is",
    "start": "400259",
    "end": "408440"
  },
  {
    "text": "running workload to a completion so basically everything related with batch jobs work cron jobs and so forth the two",
    "start": "408440",
    "end": "418139"
  },
  {
    "text": "major advancement that we did over the past literally a couple of releases is one is",
    "start": "418139",
    "end": "427740"
  },
  {
    "text": "we change how we are tracking the pods for a job so when Eric and myself wrote",
    "start": "427740",
    "end": "434520"
  },
  {
    "text": "the original job controller and the very early days of kubernetes we assume that",
    "start": "434520",
    "end": "440880"
  },
  {
    "text": "keeping the PODS of a completed job such that we can",
    "start": "440880",
    "end": "446580"
  },
  {
    "text": "calculate its status was reasonable and it worked fine to a certain degree",
    "start": "446580",
    "end": "453360"
  },
  {
    "text": "but if you start scaling the scaling your job to hundreds or",
    "start": "453360",
    "end": "459300"
  },
  {
    "text": "thousands of pots keeping those parts around is literally wasting resources",
    "start": "459300",
    "end": "465000"
  },
  {
    "text": "unfortunately so we had to change how this is working not to mention the fact that this also uh conflicts with pot",
    "start": "465000",
    "end": "473580"
  },
  {
    "text": "garbage collection which is responsible for removing the pots that have completed so suddenly you were in a",
    "start": "473580",
    "end": "481740"
  },
  {
    "text": "situation where a garbage pod garbage collector is removing the pot that it",
    "start": "481740",
    "end": "486780"
  },
  {
    "text": "finds completed and job controller in the Sim at the same time is requiring",
    "start": "486780",
    "end": "492900"
  },
  {
    "text": "the pods to be still around so that it can calculate that a particular job has",
    "start": "492900",
    "end": "499020"
  },
  {
    "text": "completed over the past I I would say like three or four releases even because we've",
    "start": "499020",
    "end": "505020"
  },
  {
    "text": "we've run into multiple issues we change how we are calculating the uh the",
    "start": "505020",
    "end": "510419"
  },
  {
    "text": "completions such that we are setting a finalizer on every pod that is Created",
    "start": "510419",
    "end": "515880"
  },
  {
    "text": "from a job and then upon completion of this particular pod we are removing the",
    "start": "515880",
    "end": "521580"
  },
  {
    "text": "the finalizer only after we included the information that oh yes this part has",
    "start": "521580",
    "end": "527760"
  },
  {
    "text": "been has been already included in the status of the job",
    "start": "527760",
    "end": "533279"
  },
  {
    "text": "so that opens a lot of fields for uh heavy",
    "start": "533279",
    "end": "539660"
  },
  {
    "text": "uh heavy jobs basically that could run even to multiple days or weeks",
    "start": "539660",
    "end": "545880"
  },
  {
    "text": "the other one that is also literally fresh from the oven uh because it was",
    "start": "545880",
    "end": "552440"
  },
  {
    "text": "stabilized in 127 and it's actually the work itself wasn't that hard is adding",
    "start": "552440",
    "end": "559140"
  },
  {
    "text": "the time zone to Cron job so it just so happens that a couple releases back",
    "start": "559140",
    "end": "564660"
  },
  {
    "text": "we've upgraded the library to responsible for parsing the Cron job",
    "start": "564660",
    "end": "570839"
  },
  {
    "text": "schedule and one of the users discovered that",
    "start": "570839",
    "end": "576060"
  },
  {
    "text": "within this Library there is an ability to set time zone through a TZ variable",
    "start": "576060",
    "end": "582540"
  },
  {
    "text": "that you can prepend to the schedule this is not supported and you should not",
    "start": "582540",
    "end": "590279"
  },
  {
    "text": "be doing that if you are and I'll I'll tell you why in a moment so that was",
    "start": "590279",
    "end": "596700"
  },
  {
    "text": "additional step that we just realized that oh yeah we have to finally add the",
    "start": "596700",
    "end": "601800"
  },
  {
    "text": "time zone because time zone was requested to be added very early on in",
    "start": "601800",
    "end": "607200"
  },
  {
    "text": "the kubernetes life cycle when we shortly after we created back then it was even called scheduler",
    "start": "607200",
    "end": "613860"
  },
  {
    "text": "jobs if you haven't heard only a couple releases afterwards we renamed it uh",
    "start": "613860",
    "end": "619800"
  },
  {
    "text": "crunch ups but that was way before 110 if I remember correctly so yes we've added the time zone to Cron",
    "start": "619800",
    "end": "626820"
  },
  {
    "text": "job but we still had that TZ issue that a lot of people suddenly started relying",
    "start": "626820",
    "end": "632760"
  },
  {
    "text": "on we've put warnings so whenever you try to create a Cron job or update a",
    "start": "632760",
    "end": "639060"
  },
  {
    "text": "Cron job with that TZ variable or cron the underscore TZ in both cases it will it will warn you",
    "start": "639060",
    "end": "646980"
  },
  {
    "text": "through HTTP warnings that API server has exposed",
    "start": "646980",
    "end": "652079"
  },
  {
    "text": "um it will warn you that you are using an unsupported version now in 127 we went",
    "start": "652079",
    "end": "658260"
  },
  {
    "text": "even a step further because the time zone because the time",
    "start": "658260",
    "end": "663779"
  },
  {
    "text": "zone feature is fully supported we will prevent you from creating a Cron job",
    "start": "663779",
    "end": "670200"
  },
  {
    "text": "with the TZ variable in it you will be after you upgrade to 127 you",
    "start": "670200",
    "end": "676920"
  },
  {
    "text": "still will be able to update your cron jobs your existing Crown jobs but you will not be able to create new ones",
    "start": "676920",
    "end": "684360"
  },
  {
    "text": "in the next release in 128 we will even prevent you from upgrading",
    "start": "684360",
    "end": "690959"
  },
  {
    "text": "sorry we will even prevent you from upgrading",
    "start": "690959",
    "end": "696959"
  },
  {
    "text": "did it pull the other one oh yeah okay cool sorry I have a timer uh",
    "start": "696959",
    "end": "704040"
  },
  {
    "text": "so it will even in 128 prevent you from updating con jobs with otz",
    "start": "704040",
    "end": "710880"
  },
  {
    "text": "so if you are relying on that TZ variable and schedule please switch over",
    "start": "710880",
    "end": "715980"
  },
  {
    "text": "to the supported time zone variable oh the one thing that I did not mention uh",
    "start": "715980",
    "end": "722160"
  },
  {
    "text": "if you were wondering what was the time zone that we previously used it was always the time zone that has been at",
    "start": "722160",
    "end": "729300"
  },
  {
    "text": "the time zone of the cube controller manager process running on the host so",
    "start": "729300",
    "end": "735120"
  },
  {
    "text": "the host where the cube controller manager was running was the time zone that was always being",
    "start": "735120",
    "end": "741120"
  },
  {
    "text": "used for scheduling new new jobs okay so moving back to moving further to",
    "start": "741120",
    "end": "750000"
  },
  {
    "text": "Beta features and we will stay in the batch area and I'll cover the apps area",
    "start": "750000",
    "end": "756360"
  },
  {
    "text": "in a little bit so we are working very closely with a batch",
    "start": "756360",
    "end": "762899"
  },
  {
    "text": "work group and I'll talk about the batch work group in them in a moment",
    "start": "762899",
    "end": "768120"
  },
  {
    "text": "but the two very important items that came from that collaboration is",
    "start": "768120",
    "end": "776220"
  },
  {
    "text": "retryable and non-retrievable failures for a job so currently if you're looking at a job or you've worked with a job you",
    "start": "776220",
    "end": "783300"
  },
  {
    "text": "are probably aware that the job has very limited ability to specify when it will retry when we",
    "start": "783300",
    "end": "791519"
  },
  {
    "text": "originally designed the jobs based on our experience the goal was to",
    "start": "791519",
    "end": "798180"
  },
  {
    "text": "make sure it always reaches a specific completions or alternatively there was another",
    "start": "798180",
    "end": "804000"
  },
  {
    "text": "option where we figure out that we want to have a a guard pod which when it",
    "start": "804000",
    "end": "810300"
  },
  {
    "text": "completes basically the entire job is finished so we implemented only those",
    "start": "810300",
    "end": "817260"
  },
  {
    "text": "and we always retry the only thing that you could configure was the number of backoffs",
    "start": "817260",
    "end": "824300"
  },
  {
    "text": "that it would retry a couple of times and afterwards um so when we started talking within the",
    "start": "824300",
    "end": "832920"
  },
  {
    "text": "batch working group it turned out that if you start building on top of the kubernetes bad uh",
    "start": "832920",
    "end": "839100"
  },
  {
    "text": "jobs and you start building AIML workloads or",
    "start": "839100",
    "end": "844740"
  },
  {
    "text": "any kind of um HPC related uh workloads you run into",
    "start": "844740",
    "end": "851760"
  },
  {
    "text": "problems because there are cases where you are certain that the job should not",
    "start": "851760",
    "end": "857040"
  },
  {
    "text": "be retried and there are the other cases where yes I'm perfectly aware that it",
    "start": "857040",
    "end": "862079"
  },
  {
    "text": "failed but it's a something that we can safely retry so the batch the folks from from batch",
    "start": "862079",
    "end": "870000"
  },
  {
    "text": "work group put together a proposal to extend the job API with a retryable",
    "start": "870000",
    "end": "876899"
  },
  {
    "text": "policy pot failure policy sorry that's the field name in it you can specify either Exit codes",
    "start": "876899",
    "end": "884940"
  },
  {
    "text": "based on which we will retry or pot conditions",
    "start": "884940",
    "end": "890339"
  },
  {
    "text": "and they are still working hard to expand the surface of that API so if you",
    "start": "890339",
    "end": "895380"
  },
  {
    "text": "have your own ideas about what should be added which case it should be covered over there",
    "start": "895380",
    "end": "901440"
  },
  {
    "text": "I'm pretty sure that even pop either popping up to Sig apps or the batch work",
    "start": "901440",
    "end": "906480"
  },
  {
    "text": "group and sharing your experiences your requirements with with us would be very",
    "start": "906480",
    "end": "914160"
  },
  {
    "text": "um would be very helpful and the other topic that I wanted to",
    "start": "914160",
    "end": "919199"
  },
  {
    "text": "cover also coming from um from The Bachelor group was elastic",
    "start": "919199",
    "end": "924300"
  },
  {
    "text": "index job so I did mention that um one of the use cases for a job was that",
    "start": "924300",
    "end": "932160"
  },
  {
    "text": "you were able to specify a guard pod which when completed would mean that the",
    "start": "932160",
    "end": "939660"
  },
  {
    "text": "job is finalized you do that by not specifying the completion number so",
    "start": "939660",
    "end": "944940"
  },
  {
    "text": "either it will run to a specific number of completions or if you don't specify",
    "start": "944940",
    "end": "950399"
  },
  {
    "text": "the first part that completes the job is is marked as a completed when",
    "start": "950399",
    "end": "956820"
  },
  {
    "text": "we initially started working on the index job we reuse the same patterns for",
    "start": "956820",
    "end": "962459"
  },
  {
    "text": "the index job but it turned out that for the index job which basically every",
    "start": "962459",
    "end": "967560"
  },
  {
    "text": "single part has an assigned index and we will ensure that if this particular pod",
    "start": "967560",
    "end": "973079"
  },
  {
    "text": "failed we will retry this and we will hard assign it a specific Index this",
    "start": "973079",
    "end": "979320"
  },
  {
    "text": "allows you to um to work with work work use or eventually",
    "start": "979320",
    "end": "984560"
  },
  {
    "text": "divide your work into specific parts so we use the same patterns for the uh",
    "start": "984560",
    "end": "990720"
  },
  {
    "text": "for the index job but it turned out that in an index job that's not quite necessarily useful",
    "start": "990720",
    "end": "997820"
  },
  {
    "text": "and we were discussing this a couple of times in how we should be implementing",
    "start": "997820",
    "end": "1005420"
  },
  {
    "text": "this because we would like to be able to to change the number of completions for",
    "start": "1005420",
    "end": "1010759"
  },
  {
    "text": "index job because the other thing that I did not mention is you are not able to",
    "start": "1010759",
    "end": "1016040"
  },
  {
    "text": "change the number of completions it was set up front upon job creation and the",
    "start": "1016040",
    "end": "1021680"
  },
  {
    "text": "only parameter that you were allowed to change was parallelism you could modify depending on the traffic depending on",
    "start": "1021680",
    "end": "1028280"
  },
  {
    "text": "your time whatever the workload of of your cluster is how many concurrent",
    "start": "1028280",
    "end": "1034760"
  },
  {
    "text": "parts are running your job but you were not able ever to modify the completions",
    "start": "1034760",
    "end": "1040339"
  },
  {
    "text": "so with the index job what we found out it would be reasonable to be able to modify the completions sorry again",
    "start": "1040339",
    "end": "1049040"
  },
  {
    "text": "um so we discussed this a couple of times and we figure out that",
    "start": "1049040",
    "end": "1054799"
  },
  {
    "text": "we cannot break the index job because that was already at the point where index up was",
    "start": "1054799",
    "end": "1060620"
  },
  {
    "text": "released and it was fully GA and we could just not allow ourselves to break the API promises",
    "start": "1060620",
    "end": "1067880"
  },
  {
    "text": "um that both the project and our sigis so we figure out we will name it differently and we will ensure that we",
    "start": "1067880",
    "end": "1075320"
  },
  {
    "text": "will not break users by allowing modifying both completions and parallelism and as a requirement",
    "start": "1075320",
    "end": "1083120"
  },
  {
    "text": "we would allow modifying but only if both of them are modified at the same",
    "start": "1083120",
    "end": "1089120"
  },
  {
    "text": "time this allowed us to ensure that we do not break users because",
    "start": "1089120",
    "end": "1095360"
  },
  {
    "text": "breaking users is the the worst thing that can happen to all of you and us as",
    "start": "1095360",
    "end": "1101179"
  },
  {
    "text": "well as the maintainers okay so that covers the batch side of",
    "start": "1101179",
    "end": "1108620"
  },
  {
    "text": "things going back to the app side of things again um so",
    "start": "1108620",
    "end": "1113720"
  },
  {
    "text": "the first topic that I want to talk a little bit is pot healthy policies for pod disruption budgets if you use pod",
    "start": "1113720",
    "end": "1121940"
  },
  {
    "text": "disruption budget before you know that it allows you basically to set a",
    "start": "1121940",
    "end": "1127460"
  },
  {
    "text": "specific numbers um of PODS that always have to run your application this is especially important",
    "start": "1127460",
    "end": "1134539"
  },
  {
    "text": "during upgrades when you are rolling your application when you're rolling your cluster",
    "start": "1134539",
    "end": "1140299"
  },
  {
    "text": "upgrade and you want to ensure that your application will always serve the traffic for your users",
    "start": "1140299",
    "end": "1146000"
  },
  {
    "text": "uh this way you that allows you to create a a pdb wrap in your application",
    "start": "1146000",
    "end": "1151220"
  },
  {
    "text": "the problem with that we figure out at some point in time unfortunately it was",
    "start": "1151220",
    "end": "1156260"
  },
  {
    "text": "a little bit too late was that um all the parts",
    "start": "1156260",
    "end": "1161380"
  },
  {
    "text": "running but not necessarily healthy are also accounted for",
    "start": "1161380",
    "end": "1167620"
  },
  {
    "text": "uh in the pdb budget which in some edge cases will brought a will",
    "start": "1167620",
    "end": "1175340"
  },
  {
    "text": "block the upgrade because the application cannot progress because something has been broken and up in an",
    "start": "1175340",
    "end": "1181280"
  },
  {
    "text": "app uh but it it cannot remove the broken pod",
    "start": "1181280",
    "end": "1186559"
  },
  {
    "text": "because it is already running so we doubled back and forth several",
    "start": "1186559",
    "end": "1192140"
  },
  {
    "text": "times because it's a bug on one hand but on the other hand there",
    "start": "1192140",
    "end": "1197720"
  },
  {
    "text": "will be users who strictly rely on a particular Behavior",
    "start": "1197720",
    "end": "1203360"
  },
  {
    "text": "um that it guarantees just that behavior so at the end of the day we've decided",
    "start": "1203360",
    "end": "1208880"
  },
  {
    "text": "to add another field which allows you to define the pot",
    "start": "1208880",
    "end": "1214280"
  },
  {
    "text": "healthy policy the Newport healthy policy so the current behavior is as it was up until",
    "start": "1214280",
    "end": "1220820"
  },
  {
    "text": "this point the new policy currently allows you to specify that",
    "start": "1220820",
    "end": "1226580"
  },
  {
    "text": "um unhealthy Parts the ones that are running but not healthy are not accounted",
    "start": "1226580",
    "end": "1232179"
  },
  {
    "text": "uh during the pdb calculation and they can be evicted safely",
    "start": "1232179",
    "end": "1238100"
  },
  {
    "text": "and this way this allows you to unstuck your upgrades and at least in our case and and",
    "start": "1238100",
    "end": "1245860"
  },
  {
    "text": "openshift that I've been working very heavily with we have multiple options",
    "start": "1245860",
    "end": "1251000"
  },
  {
    "text": "where we where we got bitten by this case the other two topics are covering the",
    "start": "1251000",
    "end": "1258679"
  },
  {
    "text": "stateful sets um one of the main objectives when we design stateful sets was to ensure the",
    "start": "1258679",
    "end": "1265940"
  },
  {
    "text": "stability of the data that is supporting the stateful set to ensure that we've decided never to",
    "start": "1265940",
    "end": "1273860"
  },
  {
    "text": "remove the PVC backing a particular State full set and that work for pretty extended uh",
    "start": "1273860",
    "end": "1281360"
  },
  {
    "text": "time but it turned out eventually that there are some cases where you are safe it is",
    "start": "1281360",
    "end": "1287240"
  },
  {
    "text": "safe to remove the data behind a particular stateful set so",
    "start": "1287240",
    "end": "1292700"
  },
  {
    "text": "someone opened a a proposal that they would like to be able to remove the PVC",
    "start": "1292700",
    "end": "1297860"
  },
  {
    "text": "along with the state full set it's a very simple opt-in not much code was",
    "start": "1297860",
    "end": "1304640"
  },
  {
    "text": "actually required to be done but obviously we didn't do it as a as a",
    "start": "1304640",
    "end": "1311419"
  },
  {
    "text": "default option because that would break too many users but we did open an option so you can",
    "start": "1311419",
    "end": "1317120"
  },
  {
    "text": "specify something like that and we're currently at beta on a happy path",
    "start": "1317120",
    "end": "1322760"
  },
  {
    "text": "towards stabilizing this feature around 129.",
    "start": "1322760",
    "end": "1328940"
  },
  {
    "text": "uh the last topic for stateful sets was",
    "start": "1328940",
    "end": "1334640"
  },
  {
    "text": "ordinal lumbering when you're running a stateful set on a single cluster everything is is just",
    "start": "1334640",
    "end": "1342320"
  },
  {
    "text": "fine but if you start discussing the topics of migrating the state full sets between",
    "start": "1342320",
    "end": "1348140"
  },
  {
    "text": "different clusters that's when you start running into issues because during the migration you",
    "start": "1348140",
    "end": "1354500"
  },
  {
    "text": "still want to maintain the ability to run your particular stateful set and the lack of control how the state",
    "start": "1354500",
    "end": "1363140"
  },
  {
    "text": "flow set controller num numbers the parts in your stateful set",
    "start": "1363140",
    "end": "1369020"
  },
  {
    "text": "did not allow you up until this time um",
    "start": "1369020",
    "end": "1374539"
  },
  {
    "text": "to spread your your stateful set of between two cluster basically that was",
    "start": "1374539",
    "end": "1380960"
  },
  {
    "text": "the goal so we've introduced something that is called an ordinal number which basically says",
    "start": "1380960",
    "end": "1386780"
  },
  {
    "text": "this is the starting number of my state full set and everything from so ins normally",
    "start": "1386780",
    "end": "1393620"
  },
  {
    "text": "stateful set would be numbered from zero it will number its parts from 0 to n",
    "start": "1393620",
    "end": "1398960"
  },
  {
    "text": "minus 1. with ordinal you can affect and shift the number",
    "start": "1398960",
    "end": "1404659"
  },
  {
    "text": "to start from your given number to n minus 1. in the initial part you will be",
    "start": "1404659",
    "end": "1410600"
  },
  {
    "text": "basically covering by creating a regular stateful set with the with your new",
    "start": "1410600",
    "end": "1417559"
  },
  {
    "text": "ordinal which is set in the auto part sorry again uh in the yatapod in the other stateful",
    "start": "1417559",
    "end": "1424820"
  },
  {
    "text": "set as a replicas number in the lower one",
    "start": "1424820",
    "end": "1431200"
  },
  {
    "text": "okay so that covers the topics that we've been working on for the past three",
    "start": "1431360",
    "end": "1436640"
  },
  {
    "text": "releases 120 128 hasn't started yet I literally was checking the dates just to",
    "start": "1436640",
    "end": "1443600"
  },
  {
    "text": "make sure that I didn't miss something but there are no official dates for 128 yet although the brand the master",
    "start": "1443600",
    "end": "1449960"
  },
  {
    "text": "branches has opened already and there's a bunch of PRS Landing in kubernetes if you have any",
    "start": "1449960",
    "end": "1456200"
  },
  {
    "text": "topics and I know that there's at least three or four that has that are that are slowly starting and will be",
    "start": "1456200",
    "end": "1463520"
  },
  {
    "text": "getting to Alpha such as the uh all of the workload controller",
    "start": "1463520",
    "end": "1470480"
  },
  {
    "text": "status is consolidation we are discussing the topic for I would say a",
    "start": "1470480",
    "end": "1475820"
  },
  {
    "text": "good couple years by now we're still trying to figure out how to best design the API to cover",
    "start": "1475820",
    "end": "1482860"
  },
  {
    "text": "this topic there are a couple topics which will be like the ones that are",
    "start": "1482860",
    "end": "1488299"
  },
  {
    "text": "beta which we will be promoting to stable in the next releases if there is a topic",
    "start": "1488299",
    "end": "1495500"
  },
  {
    "text": "that you are interested in that you want to help with speak up if there is something new like I said May 1st will",
    "start": "1495500",
    "end": "1503360"
  },
  {
    "text": "be the next cigaps meeting or send an email to mailing list or reach out to us",
    "start": "1503360",
    "end": "1510200"
  },
  {
    "text": "on Slack let us know what is something that you would like to see being added in the",
    "start": "1510200",
    "end": "1516020"
  },
  {
    "text": "core core controllers I did mention the bachelor group I want to thank everyone in the in the batch",
    "start": "1516020",
    "end": "1522679"
  },
  {
    "text": "work group they are doing amazing job and progressing where our bat API so",
    "start": "1522679",
    "end": "1529460"
  },
  {
    "text": "both jobs and chrome jobs the work has been invaluable they are even helping us",
    "start": "1529460",
    "end": "1536720"
  },
  {
    "text": "with improving some of the workloads so the apps controllers that I've been discussing earlier today",
    "start": "1536720",
    "end": "1543740"
  },
  {
    "text": "they also have bi-weekly meetings they are meeting actually on Thursday there's",
    "start": "1543740",
    "end": "1549860"
  },
  {
    "text": "also an email group and a slack channel that you can that you can join if you",
    "start": "1549860",
    "end": "1555020"
  },
  {
    "text": "have any ideas about workflows or any kind of AIML batch workloads that you are",
    "start": "1555020",
    "end": "1562880"
  },
  {
    "text": "running in your environment or you can learn or you want to learn about how to run these top uh how to run",
    "start": "1562880",
    "end": "1569419"
  },
  {
    "text": "these workloads on kubernetes clusters this is the place that you want to be listen the stories that many people",
    "start": "1569419",
    "end": "1576799"
  },
  {
    "text": "are what what kind of struggles people have in the ecosystem or what are the",
    "start": "1576799",
    "end": "1583460"
  },
  {
    "text": "solutions that we are slowly building lastly",
    "start": "1583460",
    "end": "1589220"
  },
  {
    "text": "um over the past second half of 2022 Iran",
    "start": "1589220",
    "end": "1594679"
  },
  {
    "text": "mentoring session for say gaps to become reviewers and approvers we have a small",
    "start": "1594679",
    "end": "1601760"
  },
  {
    "text": "number of people who are actually working on the core controllers I'm working uh effortlessly to",
    "start": "1601760",
    "end": "1610340"
  },
  {
    "text": "to raise additional uh group of people who will be able to help with us",
    "start": "1610340",
    "end": "1616279"
  },
  {
    "text": "it's uh it's a very challenging word there's a lot of uh initial knowledge that is required",
    "start": "1616279",
    "end": "1623299"
  },
  {
    "text": "for the controllers that we own it's not an easy task not as easy like",
    "start": "1623299",
    "end": "1628580"
  },
  {
    "text": "for example for six July and I have a comparison because I also help Mentor",
    "start": "1628580",
    "end": "1633679"
  },
  {
    "text": "folks for the 6cli so for cube CTL for cube CTO it's very simple and the risk",
    "start": "1633679",
    "end": "1640039"
  },
  {
    "text": "of breaking something is slightly slow slightly smaller because in the worst",
    "start": "1640039",
    "end": "1645919"
  },
  {
    "text": "case you'll break single command which is not that bad if you break a",
    "start": "1645919",
    "end": "1650960"
  },
  {
    "text": "controller that's a little bit more scary um especially if the controller has some data behind it that's that's super scary",
    "start": "1650960",
    "end": "1658640"
  },
  {
    "text": "so if you're interested in something um reach out let us know we're more than",
    "start": "1658640",
    "end": "1666080"
  },
  {
    "text": "happy to help you grow in the community and",
    "start": "1666080",
    "end": "1671360"
  },
  {
    "text": "um and help us all so I think that's basically all this",
    "start": "1671360",
    "end": "1676940"
  },
  {
    "text": "stuff that I wanted to share with you thank you very much for today if you have any particular questions I can take",
    "start": "1676940",
    "end": "1683539"
  },
  {
    "text": "them either on the recording if you don't feel comfortable doing them on recording I'll be standing here",
    "start": "1683539",
    "end": "1689860"
  },
  {
    "text": "for a while and you can just come say hi or there are some questions",
    "start": "1689860",
    "end": "1695720"
  },
  {
    "text": "or tough issues at me thank you [Applause]",
    "start": "1695720",
    "end": "1703739"
  },
  {
    "text": "there's a question from Buckle",
    "start": "1715220",
    "end": "1718658"
  },
  {
    "text": "um",
    "start": "1725240",
    "end": "1727240"
  },
  {
    "text": "yeah I I know what you're asking about so uh",
    "start": "1733460",
    "end": "1738500"
  },
  {
    "text": "the the batch work group put together a proposal for a job set",
    "start": "1738500",
    "end": "1745220"
  },
  {
    "text": "and they are uh we've just last week or two weeks ago we've opened",
    "start": "1745220",
    "end": "1751880"
  },
  {
    "text": "uh we've opened requests to create a kubernetes sponsored kubernetes",
    "start": "1751880",
    "end": "1759080"
  },
  {
    "text": "in the kubernetes sick org we have a jobs at repo so we are not starting in",
    "start": "1760399",
    "end": "1767179"
  },
  {
    "text": "the core we will be starting as an additional add-on as an additional controller we would like to be able this",
    "start": "1767179",
    "end": "1774020"
  },
  {
    "text": "way this will this will allow us to progress and experiment uh and",
    "start": "1774020",
    "end": "1779299"
  },
  {
    "text": "experiment a little bit faster gather a lot of feedback within the batch workloop because like I mentioned",
    "start": "1779299",
    "end": "1785899"
  },
  {
    "text": "the batch group gives us a lot of feedback about how they want to run stuff uh around the topic and there was",
    "start": "1785899",
    "end": "1794480"
  },
  {
    "text": "a proposal sent to both Sega apps and in The Bachelor group if you haven't seen",
    "start": "1794480",
    "end": "1800539"
  },
  {
    "text": "it just check the archives for either of the group or ping me I'll be happy to",
    "start": "1800539",
    "end": "1805760"
  },
  {
    "text": "share with you and I'm pretty sure that there will be further discussions and further",
    "start": "1805760",
    "end": "1811880"
  },
  {
    "text": "development if it's something that you are interested in uh make sure to to sync with the bachelor group and provide",
    "start": "1811880",
    "end": "1818240"
  },
  {
    "text": "the feedback so that you know it goes in the direction that matches your expectations basically",
    "start": "1818240",
    "end": "1825580"
  },
  {
    "text": "thank you very much all [Applause]",
    "start": "1831140",
    "end": "1837049"
  }
]