[
  {
    "text": "good afternoon Wilfred spiegelenburg and Peter bashco here",
    "start": "299",
    "end": "5779"
  },
  {
    "text": "we're part of the PMC unique arpeggio unicorn group designed the Apache",
    "start": "5779",
    "end": "12120"
  },
  {
    "text": "unicorn from scratch I've been doing Apache unicorn since Inception about",
    "start": "12120",
    "end": "18060"
  },
  {
    "text": "three years ago and Peter hi my name is Peter rochko I",
    "start": "18060",
    "end": "24660"
  },
  {
    "text": "joined Club there in 2016. we worked on Uzi Hadoop my produced yarn and then",
    "start": "24660",
    "end": "30539"
  },
  {
    "text": "joined the nuclear team in 2021. so what we're going to talk about today",
    "start": "30539",
    "end": "36780"
  },
  {
    "text": "is how we are using the schedule framework to give you some batch",
    "start": "36780",
    "end": "42239"
  },
  {
    "text": "extensions in the kubernetes environment so like we said we're coming out of the",
    "start": "42239",
    "end": "49340"
  },
  {
    "text": "Hep Hadoop yarn area so batch processing",
    "start": "49340",
    "end": "55680"
  },
  {
    "text": "large data processing and we're trying to give you the similar kinds of things",
    "start": "55680",
    "end": "60780"
  },
  {
    "text": "within the setup around kubernetes so what are",
    "start": "60780",
    "end": "68340"
  },
  {
    "text": "the things that we're looking at when when we look at scheduling on on kubernetes",
    "start": "68340",
    "end": "74400"
  },
  {
    "text": "we want to do workload queuing when when batches gets deployed when batches get generated we generate often or one in",
    "start": "74400",
    "end": "82080"
  },
  {
    "text": "one go we want to keep them around we want to start them whenever things will",
    "start": "82080",
    "end": "88080"
  },
  {
    "text": "um or need to be run we want to do an all-in-one kind of scheduling setup gang scheduling we",
    "start": "88080",
    "end": "96060"
  },
  {
    "text": "don't spawn up one pot we want to do a set of",
    "start": "96060",
    "end": "101340"
  },
  {
    "text": "pods which one is a driver other side of workers spark is a is a good example for",
    "start": "101340",
    "end": "107759"
  },
  {
    "text": "that but there's other pie torch uh imp there's a number of things that will do",
    "start": "107759",
    "end": "112979"
  },
  {
    "text": "that for you things that are not available in instantly the other",
    "start": "112979",
    "end": "119820"
  },
  {
    "text": "um extra bit that we want to give you is application sorting so instead of",
    "start": "119820",
    "end": "125219"
  },
  {
    "text": "looking at just one pot or a job or a demon set with another there's a mixture",
    "start": "125219",
    "end": "130440"
  },
  {
    "text": "of bots a group of bots that together for the application we want to schedule based on the",
    "start": "130440",
    "end": "138180"
  },
  {
    "text": "requests that come from from that thing it could be one pot to begin with scale",
    "start": "138180",
    "end": "144180"
  },
  {
    "text": "up to a thousand parts and then drop down again to a couple of pots like what we do with data processing at the point",
    "start": "144180",
    "end": "151140"
  },
  {
    "text": "in time that we need it we scale up the pods we do what we need to do and the pots go away so we've got a real bursty",
    "start": "151140",
    "end": "159900"
  },
  {
    "text": "kind of a deployment but we we still want to see all these",
    "start": "159900",
    "end": "166980"
  },
  {
    "text": "Bots being scheduled as part of one thing not every single port separately so",
    "start": "166980",
    "end": "173580"
  },
  {
    "text": "these kinds of schedulers these kinds of facilities are there when you look at high performance",
    "start": "173580",
    "end": "180480"
  },
  {
    "text": "product HPC pros and the batch processing when you come from a slurm or a yarn kind of setup",
    "start": "180480",
    "end": "187080"
  },
  {
    "text": "yeah these same kind of things we're going to do or we want to do from",
    "start": "187080",
    "end": "195420"
  },
  {
    "text": "Apache unicorn within kubernetes but kubernetes from its Origins was always a Services",
    "start": "195420",
    "end": "202800"
  },
  {
    "text": "based setup we want to do both at the same time",
    "start": "202800",
    "end": "209159"
  },
  {
    "text": "there's a number of schedulers that will give you some of the batch things that you want to do but then they don't do",
    "start": "209159",
    "end": "214920"
  },
  {
    "text": "the services or the other way around where you use the default scheduler you get the services but you don't get the",
    "start": "214920",
    "end": "221280"
  },
  {
    "text": "batch things that you want to do so Apache unicorn we schedule whatever you",
    "start": "221280",
    "end": "227280"
  },
  {
    "text": "give us so we put a an application on top of the existing",
    "start": "227280",
    "end": "233940"
  },
  {
    "text": "pod objects the applications the the demon sets whatever you want to run on",
    "start": "233940",
    "end": "240120"
  },
  {
    "text": "on a kubernetes layer we we run on that and then we also allow you to easily run",
    "start": "240120",
    "end": "246540"
  },
  {
    "text": "spark jobs tensorflow jobs MPI jobs whatever you want to do without needing",
    "start": "246540",
    "end": "252720"
  },
  {
    "text": "to change the underlying framework to submit your jobs",
    "start": "252720",
    "end": "258660"
  },
  {
    "text": "so we we don't want to go in and need to run a specific tensorflow",
    "start": "258660",
    "end": "264780"
  },
  {
    "text": "set up or compiled from scratch because you want to do batch processing or the",
    "start": "264780",
    "end": "271080"
  },
  {
    "text": "same with spark so we we're going to give you a simple integration",
    "start": "271080",
    "end": "276479"
  },
  {
    "text": "based on the minimum amount of code changes preferably no code changes on the way",
    "start": "276479",
    "end": "284040"
  },
  {
    "text": "that you submit an application the way you submit a job and purely use",
    "start": "284040",
    "end": "289560"
  },
  {
    "text": "annotations and labels on on pots or demon set or whatever you submit to",
    "start": "289560",
    "end": "296280"
  },
  {
    "text": "schedule and give you the batch scheduling things on top of that when you start looking at",
    "start": "296280",
    "end": "303540"
  },
  {
    "text": "data processing there's always the question where do I run it how do I",
    "start": "303540",
    "end": "310080"
  },
  {
    "text": "share my resources that I've got in my cluster when you look at the default scheduler",
    "start": "310080",
    "end": "316199"
  },
  {
    "text": "you've got one scheduling queue so a pot submitted by the first user or by the",
    "start": "316199",
    "end": "323520"
  },
  {
    "text": "10th user it doesn't matter they're all coming into one queue and it's getting scheduled based on a priority kind of",
    "start": "323520",
    "end": "330300"
  },
  {
    "text": "setup most data processing that you want to do is not looking at one use of five uses",
    "start": "330300",
    "end": "337620"
  },
  {
    "text": "but you're looking at hundreds of users you want to share the quotas you want to share the system nicely so",
    "start": "337620",
    "end": "346500"
  },
  {
    "text": "unicorn provides a hierarchical queue structure to place",
    "start": "346500",
    "end": "352259"
  },
  {
    "text": "these applications in and do you make your scheduling decisions so instead of having one single queue that will",
    "start": "352259",
    "end": "358620"
  },
  {
    "text": "contain all the pots for the whole cluster you now can subdivide and then schedule subsets of the pods based on",
    "start": "358620",
    "end": "366600"
  },
  {
    "text": "their own rules and their own priorities first names whatever you want",
    "start": "366600",
    "end": "371880"
  },
  {
    "text": "to do within the hierarchical view system we give you the possible we give you",
    "start": "371880",
    "end": "378060"
  },
  {
    "text": "handles to say these these queues can only run an X",
    "start": "378060",
    "end": "385380"
  },
  {
    "text": "number of parts or the next number of resources but we also want you to be able to say with guaranteed this queue",
    "start": "385380",
    "end": "392520"
  },
  {
    "text": "gets half of the cluster or it needs 10 of the nodes always",
    "start": "392520",
    "end": "398100"
  },
  {
    "text": "so every single point in that hierarchical queue we can give you",
    "start": "398100",
    "end": "403380"
  },
  {
    "text": "guaranteed resources quotas or different scheduling policies so going from one",
    "start": "403380",
    "end": "410940"
  },
  {
    "text": "possible policy throughout the whole cluster we now give you a",
    "start": "410940",
    "end": "416220"
  },
  {
    "text": "set that you can self-define and self-configure over the whole cluster",
    "start": "416220",
    "end": "422460"
  },
  {
    "text": "when we started off working on Apache unicorn there was no plugin architecture",
    "start": "422460",
    "end": "428400"
  },
  {
    "text": "in kubernetes so before we when we started the there was the only way that you could",
    "start": "428400",
    "end": "435000"
  },
  {
    "text": "really change things on the schedule was completely replacing the scheduler so we",
    "start": "435000",
    "end": "441240"
  },
  {
    "text": "wrote that on scheduler we implemented all the functionality for binding ports",
    "start": "441240",
    "end": "446580"
  },
  {
    "text": "doing all that kind of stuff because that was the only way that we could do things The Next Step was the extensions that",
    "start": "446580",
    "end": "453660"
  },
  {
    "text": "were there for doing HTTP call outs and then you could customize some of the behavior that",
    "start": "453660",
    "end": "462060"
  },
  {
    "text": "didn't perform well so that was pulled out of the the kubernetes code and the next step that they came up with was the",
    "start": "462060",
    "end": "468180"
  },
  {
    "text": "plugin architecture within the plugin architecture they they",
    "start": "468180",
    "end": "474300"
  },
  {
    "text": "gave you a number of extra things that you could play around with so",
    "start": "474300",
    "end": "479400"
  },
  {
    "text": "unicorn went from the standard deployment like what we're running now which is a complete custom scheduler",
    "start": "479400",
    "end": "487860"
  },
  {
    "text": "to a plug-in architecture so we started off with",
    "start": "487860",
    "end": "493199"
  },
  {
    "text": "simple core that does all the unical code that does all the scheduling",
    "start": "493199",
    "end": "498360"
  },
  {
    "text": "decision makes all the scheduling decisions and then we've got a shim that integrates with",
    "start": "498360",
    "end": "504000"
  },
  {
    "text": "the API server pulls the information in does all the things with the pods and what we need to do and we've got an",
    "start": "504000",
    "end": "510479"
  },
  {
    "text": "admission controller sitting on top of that to do some of the more advanced stuff making sure that I think that it's",
    "start": "510479",
    "end": "517979"
  },
  {
    "text": "as easy as possible for the end user moving to the plugin version of that we",
    "start": "517979",
    "end": "526380"
  },
  {
    "text": "changed over from doing everything within the core to having the shim that now completely",
    "start": "526380",
    "end": "534060"
  },
  {
    "text": "includes the default scheduler so instead of writing all the code to bind pots to",
    "start": "534060",
    "end": "542160"
  },
  {
    "text": "node and and do the volume binding and everything ourselves we now rely on the default scheduler to do all these things",
    "start": "542160",
    "end": "548339"
  },
  {
    "text": "forwarders so we hook into the scheduling framework we Implement",
    "start": "548339",
    "end": "553620"
  },
  {
    "text": "certain points in there and we augment or we replace whatever we don't want or",
    "start": "553620",
    "end": "559320"
  },
  {
    "text": "what we want to change from the default schedule so",
    "start": "559320",
    "end": "565019"
  },
  {
    "text": "this is the picture that's probably how for people that have been looking at the",
    "start": "565019",
    "end": "570180"
  },
  {
    "text": "scheduling and scheduler in kubernetes that have shown up before",
    "start": "570180",
    "end": "575399"
  },
  {
    "text": "um the right hand side the binding cycle is last point in the whole the whole cycle",
    "start": "575399",
    "end": "583200"
  },
  {
    "text": "that's where we leave things the scheduling cycle that's already been there that's been there for for a long",
    "start": "583200",
    "end": "589500"
  },
  {
    "text": "time that's what does the real checking which node",
    "start": "589500",
    "end": "596459"
  },
  {
    "text": "do we want how do we place it on there do all that kind of stuff and then part of what is just come out in 126 127",
    "start": "596459",
    "end": "605459"
  },
  {
    "text": "is the pre and Q plugin side of things so we worked with",
    "start": "605459",
    "end": "612000"
  },
  {
    "text": "the scheduling seek and we said look we want we want to be able to do a little",
    "start": "612000",
    "end": "617640"
  },
  {
    "text": "bit more instead of just having all the pots flow into the scheduling cycle immediately we want to be able to gate",
    "start": "617640",
    "end": "625320"
  },
  {
    "text": "the pots in a pre and Q cycle so that has been delivered as part of of",
    "start": "625320",
    "end": "632100"
  },
  {
    "text": "126. so how does unicorn use these",
    "start": "632100",
    "end": "638779"
  },
  {
    "text": "plugins and where where do we sit we've kept the Unicorn core so our core",
    "start": "638820",
    "end": "645360"
  },
  {
    "text": "scheduling code has not changed so whatever we deploy",
    "start": "645360",
    "end": "651180"
  },
  {
    "text": "default mode the standard mode or the plug-in framework the Unicorn core still",
    "start": "651180",
    "end": "656399"
  },
  {
    "text": "makes all the scheduling decisions the only way that has changed between what we do with the plugin and the Divo mode",
    "start": "656399",
    "end": "663120"
  },
  {
    "text": "is the way that we interact with kubernetes and what we need to do",
    "start": "663120",
    "end": "668640"
  },
  {
    "text": "ourselves and what the default scheduler does for us yeah",
    "start": "668640",
    "end": "674040"
  },
  {
    "text": "the first part that we Implement is the the pre and Q plugin because we want to",
    "start": "674040",
    "end": "680220"
  },
  {
    "text": "be able to decide which parts scheduler looks at and which ports that",
    "start": "680220",
    "end": "687360"
  },
  {
    "text": "get put onto nodes and and be processed so without the p and Q",
    "start": "687360",
    "end": "694200"
  },
  {
    "text": "there's a lot of overhead that flows through the whole system because",
    "start": "694200",
    "end": "699480"
  },
  {
    "text": "we can't stop the default scheduler from from looking at a pod that's just not built into the",
    "start": "699480",
    "end": "706079"
  },
  {
    "text": "scheduling framework and that meant that without the pre and Q hook",
    "start": "706079",
    "end": "711959"
  },
  {
    "text": "but that unicorn thought could not be scheduled yet the default scheduler had",
    "start": "711959",
    "end": "718019"
  },
  {
    "text": "already looked at and marked as unscheduled and then the auto scaler kicks in and says hey I'm I need a new",
    "start": "718019",
    "end": "725339"
  },
  {
    "text": "note because this is remarked as unschedule so that's why the the pre and Q hook came from",
    "start": "725339",
    "end": "732000"
  },
  {
    "text": "then the second point that we Implement is the pre-filter the pre-filter runs over all the nodes",
    "start": "732000",
    "end": "738959"
  },
  {
    "text": "and decides which notes to to use and which you're not not used so that in",
    "start": "738959",
    "end": "744660"
  },
  {
    "text": "combination with the filter hook allows us to select the nodes and make",
    "start": "744660",
    "end": "751019"
  },
  {
    "text": "sure that the node is select the used that we from the Unicorn core have",
    "start": "751019",
    "end": "756899"
  },
  {
    "text": "decided on the last bit that we Implement is the post bind",
    "start": "756899",
    "end": "763860"
  },
  {
    "text": "that is the last point in the cycle and when the post bind comes back we know",
    "start": "763860",
    "end": "769740"
  },
  {
    "text": "that the port is scheduled completely accepted by the",
    "start": "769740",
    "end": "776100"
  },
  {
    "text": "cubelet and is getting started by the cubelet so the post bond is a",
    "start": "776100",
    "end": "782100"
  },
  {
    "text": "housekeeping point for us because we know that everything has gone through and that we are at the final stage",
    "start": "782100",
    "end": "789240"
  },
  {
    "text": "go the input from the core scheduling cycle goes into the first three plugins so we",
    "start": "789240",
    "end": "797700"
  },
  {
    "text": "decide what goes through decide which node we put on and for that we interact",
    "start": "797700",
    "end": "802980"
  },
  {
    "text": "with the three plugins the pre and Q the pre-filter and the filter",
    "start": "802980",
    "end": "810200"
  },
  {
    "text": "okay with all of this we want to do quotas",
    "start": "810779",
    "end": "815880"
  },
  {
    "text": "and all the other things so Peter will go into further all the quota tricking",
    "start": "815880",
    "end": "823040"
  },
  {
    "text": "yeah so let's talk about quotas so as we've had mentioned we have a hierarchical",
    "start": "823200",
    "end": "829940"
  },
  {
    "text": "model it's a hierarchy of so-called resource queues that's what we use to",
    "start": "829940",
    "end": "836760"
  },
  {
    "text": "calculate the available resources to the running applications these queues",
    "start": "836760",
    "end": "844560"
  },
  {
    "text": "can be created automatically but also in in config in configuration",
    "start": "844560",
    "end": "849839"
  },
  {
    "text": "files which is right now I am we have to submit applications or pods",
    "start": "849839",
    "end": "856560"
  },
  {
    "text": "to delete queues you cannot you cannot run stuff in the parent queue and when whenever there's an allocation",
    "start": "856560",
    "end": "863100"
  },
  {
    "text": "or there's a resource request we do the accounting in the leave queue",
    "start": "863100",
    "end": "868980"
  },
  {
    "text": "increase counters and that proper gets up all the way to the root queue so this",
    "start": "868980",
    "end": "876360"
  },
  {
    "text": "essentially means that at any point in the hierarchy we always know always",
    "start": "876360",
    "end": "881760"
  },
  {
    "text": "know the the resource usage we know the resource usage at for every for every",
    "start": "881760",
    "end": "887820"
  },
  {
    "text": "subtree uh you can put quotas on leave queues and",
    "start": "887820",
    "end": "895260"
  },
  {
    "text": "and parent cues and since we know the usage all the time",
    "start": "895260",
    "end": "901100"
  },
  {
    "text": "it's very easy to enforce it and we enforce it in every scheduling cycle",
    "start": "901100",
    "end": "908720"
  },
  {
    "text": "so here is a very very simple example uh so there's a cluster for developers and",
    "start": "909740",
    "end": "915839"
  },
  {
    "text": "testers leave username after the users",
    "start": "915839",
    "end": "921019"
  },
  {
    "text": "there are some running pods Alice is running two pods and Mallory is running one pod",
    "start": "921740",
    "end": "929779"
  },
  {
    "text": "this is the current resource usage that can be observed so obviously root sees",
    "start": "929880",
    "end": "935760"
  },
  {
    "text": "the the total which is 15 gigs of memory 5 CPU",
    "start": "935760",
    "end": "941459"
  },
  {
    "text": "Dev 20 gigs of memory to CPU and QA 30X",
    "start": "941459",
    "end": "946740"
  },
  {
    "text": "and 3 CPU and now we want to put some resource",
    "start": "946740",
    "end": "953760"
  },
  {
    "text": "limit or quota on the on the dev queue uh we want to limit resource usage for",
    "start": "953760",
    "end": "960779"
  },
  {
    "text": "the developers UH 60 gigabytes of memory and for CPU and also it's worth",
    "start": "960779",
    "end": "966120"
  },
  {
    "text": "mentioning that for the rooku this is a calculated automatically when the Unicorn starts",
    "start": "966120",
    "end": "972600"
  },
  {
    "text": "the nodes are registered and we just received the",
    "start": "972600",
    "end": "978480"
  },
  {
    "text": "capacity and later on WE update it when a node join node joins the cluster or or",
    "start": "978480",
    "end": "985320"
  },
  {
    "text": "leaves the cluster okay and then",
    "start": "985320",
    "end": "990600"
  },
  {
    "text": "Bob Bob wants to run a pod but unfortunately for him this pod cannot",
    "start": "990600",
    "end": "997380"
  },
  {
    "text": "run at least not at the moment this is pending why because of the of the limit so this spot",
    "start": "997380",
    "end": "1004639"
  },
  {
    "text": "is asking for four CPUs there's nothing wrong with the memory but the current usage at the dev queue",
    "start": "1004639",
    "end": "1011779"
  },
  {
    "text": "is 2 CPU and there's a there's a limit for for CPU and there's the and that's not enough so",
    "start": "1011779",
    "end": "1019940"
  },
  {
    "text": "sobop has to wait uh until uh the pods that were started by Alice terminate",
    "start": "1019940",
    "end": "1027319"
  },
  {
    "text": "and it's important to not know that the Bob spot is pending it's not rejected",
    "start": "1027319",
    "end": "1034400"
  },
  {
    "text": "and you will see why this is important",
    "start": "1034400",
    "end": "1038559"
  },
  {
    "text": "okay so namespace quotas versus unicorn quotas we can also use unicorn to manage",
    "start": "1040939",
    "end": "1047178"
  },
  {
    "text": "namespace quotas why why would we do that",
    "start": "1047179",
    "end": "1051580"
  },
  {
    "text": "is exceeded the Pod is ejected end of story",
    "start": "1054940",
    "end": "1061100"
  },
  {
    "text": "and if you want to run your workload and probably you you want to run it sometime",
    "start": "1061100",
    "end": "1066919"
  },
  {
    "text": "later you need you need some retry logic it's a script or you try manually",
    "start": "1066919",
    "end": "1073059"
  },
  {
    "text": "and also other users might be competing for for resources too and there is no no ordering between the",
    "start": "1073059",
    "end": "1080299"
  },
  {
    "text": "users so you might lose this race against others so this is um",
    "start": "1080299",
    "end": "1086900"
  },
  {
    "text": "this is not an ideal scenario in unicorn you can have a setup",
    "start": "1086900",
    "end": "1092740"
  },
  {
    "text": "where the queues are Auto created based on existing namespaces so for here here",
    "start": "1092740",
    "end": "1099580"
  },
  {
    "text": "in this example sales Finance Dev and test these are existing name spaces",
    "start": "1099580",
    "end": "1105919"
  },
  {
    "text": "and unicore created them when oppod was submitted",
    "start": "1105919",
    "end": "1111799"
  },
  {
    "text": "and in this case the pods are not rejected if there is is there's no more",
    "start": "1111799",
    "end": "1117500"
  },
  {
    "text": "room the quotas exceeded them the users just had to wait and",
    "start": "1117500",
    "end": "1123799"
  },
  {
    "text": "when there is enough resource then they will be picked automatically based on based on the ordering that is set",
    "start": "1123799",
    "end": "1131299"
  },
  {
    "text": "uh you have to set so-called placement rules in order to have this queue",
    "start": "1131299",
    "end": "1137780"
  },
  {
    "text": "created and also we have to update the namespace object themselves there are two",
    "start": "1137780",
    "end": "1143720"
  },
  {
    "text": "annotations that you can use the the quota itself and the pattern",
    "start": "1143720",
    "end": "1149600"
  },
  {
    "text": "queue this this Arrow says that the development and production cues are",
    "start": "1149600",
    "end": "1155780"
  },
  {
    "text": "optional grouping so that's that's not as that's not necessary uh all uh the",
    "start": "1155780",
    "end": "1162679"
  },
  {
    "text": "the leaves can appear on the root directly so that's that's not an issue but sometimes this is this is desirable",
    "start": "1162679",
    "end": "1172419"
  },
  {
    "text": "so this is um so this is how it looks like when you want to configure this this is by the way from the Upstream",
    "start": "1173240",
    "end": "1180380"
  },
  {
    "text": "documentation when it's explained really well so on the left side there's the",
    "start": "1180380",
    "end": "1185780"
  },
  {
    "text": "Unicode config diam in green you can see this the static use",
    "start": "1185780",
    "end": "1191660"
  },
  {
    "text": "the the production and the development this is what uh this is what I call the",
    "start": "1191660",
    "end": "1196940"
  },
  {
    "text": "optional uh we call them sometimes configured cues or manage queues",
    "start": "1196940",
    "end": "1203020"
  },
  {
    "text": "this queues always exist they didn't they don't disappear until you remove them uh in",
    "start": "1203020",
    "end": "1210200"
  },
  {
    "text": "in the yellow there's the placement rule section and if you have used Apache Hadoop before",
    "start": "1210200",
    "end": "1217220"
  },
  {
    "text": "Hadoop yarn with fast scheduler or also capacity schedule it's the very same",
    "start": "1217220",
    "end": "1222980"
  },
  {
    "text": "idea it's also called placement rules there um Utah unicorn how to name the cues that",
    "start": "1222980",
    "end": "1229580"
  },
  {
    "text": "don't exist uh and and that's it then there are all kinds of rules like name after the user",
    "start": "1229580",
    "end": "1235940"
  },
  {
    "text": "name after namespace uh some maybe some labeling so there are different kinds of",
    "start": "1235940",
    "end": "1241520"
  },
  {
    "text": "rules and these are the uh the orange the uh",
    "start": "1241520",
    "end": "1247700"
  },
  {
    "text": "this is these are the two intentions that you have to put um on the on the namespace object",
    "start": "1247700",
    "end": "1254620"
  },
  {
    "text": "the first is is the quota is this is a tiny Json",
    "start": "1254620",
    "end": "1259640"
  },
  {
    "text": "and the second one is is the pattern queue that uh that does where the namespace queue",
    "start": "1259640",
    "end": "1267500"
  },
  {
    "text": "should be created and finally",
    "start": "1267500",
    "end": "1272679"
  },
  {
    "text": "we also want to put quotas on users and groups and um",
    "start": "1272679",
    "end": "1278440"
  },
  {
    "text": "this this feature consists of three parts",
    "start": "1278440",
    "end": "1283940"
  },
  {
    "text": "the first we have to determine who submit submitted the actual workload and",
    "start": "1283940",
    "end": "1289700"
  },
  {
    "text": "it turns out that this info is only available inside the emission controller it's not present on the Pod or",
    "start": "1289700",
    "end": "1296900"
  },
  {
    "text": "deployment or any kind of other objects at least that's the only way that we found you need you need a muted web hook",
    "start": "1296900",
    "end": "1305679"
  },
  {
    "text": "we have a muted web hook we extract this info and we we modify the we modify the path",
    "start": "1305679",
    "end": "1314000"
  },
  {
    "text": "spec of the workload not just reports also for a deployment job ground job except",
    "start": "1314000",
    "end": "1322159"
  },
  {
    "text": "application controller so we had an extra annotation this is also a tiny tiny Json",
    "start": "1322159",
    "end": "1329360"
  },
  {
    "text": "and later we deny changes to this annotation you can you can fine tune this Behavior change it but that's",
    "start": "1329360",
    "end": "1335840"
  },
  {
    "text": "that's the basic idea and then there's the tracking itself",
    "start": "1335840",
    "end": "1341919"
  },
  {
    "text": "it's it's the accounting increasing decrease encounters when there's an allocation or a report termination",
    "start": "1342260",
    "end": "1349820"
  },
  {
    "text": "for the different groups and the users and we have a nice rest API where this",
    "start": "1349820",
    "end": "1355460"
  },
  {
    "text": "is visible so we have separate endpoints for users and groups and if you open the",
    "start": "1355460",
    "end": "1361460"
  },
  {
    "text": "user endpoint you can see this user total and",
    "start": "1361460",
    "end": "1369020"
  },
  {
    "text": "then you can see the pair queue statistics where that user is running applications and the usage",
    "start": "1369020",
    "end": "1377679"
  },
  {
    "text": "demo about that and that is the third one which is not",
    "start": "1377980",
    "end": "1383179"
  },
  {
    "text": "ready the actual this is the actual enforcement so this is um this is in progress",
    "start": "1383179",
    "end": "1391159"
  },
  {
    "text": "and we targeted this for one three oh the latest unicorn version is one two oh",
    "start": "1391159",
    "end": "1397880"
  },
  {
    "text": "um there's the general link you can check it out it would be great",
    "start": "1397880",
    "end": "1404059"
  },
  {
    "text": "yeah so that's it",
    "start": "1404059",
    "end": "1407320"
  },
  {
    "text": "uh oh I'll skip over uh preemptions so one of the main things that we were",
    "start": "1412580",
    "end": "1417679"
  },
  {
    "text": "missing in um our current setups is how do we do preemption in in this kind of setup uh",
    "start": "1417679",
    "end": "1425179"
  },
  {
    "text": "if you do want to run one single port you can preempt but in this case we've",
    "start": "1425179",
    "end": "1430640"
  },
  {
    "text": "got a full Q hierarchy so how do we decide what we do with preemption now",
    "start": "1430640",
    "end": "1436039"
  },
  {
    "text": "like the default scheduling Cycles we do preemption based on priority classes the",
    "start": "1436039",
    "end": "1442100"
  },
  {
    "text": "only thing is that we've added an opt-out from a unicorn perspective so instead of saying that something is",
    "start": "1442100",
    "end": "1450440"
  },
  {
    "text": "allowed to preempt during the scheduling cycle we now say don't preempt me when",
    "start": "1450440",
    "end": "1456200"
  },
  {
    "text": "I'm already running if you look at jobs that are running for instance if you've got a spark job",
    "start": "1456200",
    "end": "1462799"
  },
  {
    "text": "running or some other job running for a couple of days and you kill the pot that",
    "start": "1462799",
    "end": "1468200"
  },
  {
    "text": "belongs to that that could be really costly so you want to be able to opt out that we allow that",
    "start": "1468200",
    "end": "1475120"
  },
  {
    "text": "we've got the queue configuration that we definitely needed we Max is the the",
    "start": "1475159",
    "end": "1480620"
  },
  {
    "text": "quota and we guarantee a certain amount of resources which is the basis for",
    "start": "1480620",
    "end": "1485840"
  },
  {
    "text": "preemption um we've got some other nice really complex things that we can do",
    "start": "1485840",
    "end": "1493520"
  },
  {
    "text": "with fencing so that you can say certain parts of the hierarchies can't preempt",
    "start": "1493520",
    "end": "1498799"
  },
  {
    "text": "other workloads um things you don't want to have production preempted because somebody in",
    "start": "1498799",
    "end": "1504320"
  },
  {
    "text": "starts something up in a def or in a test environment I'll skip over all of that there's a",
    "start": "1504320",
    "end": "1510559"
  },
  {
    "text": "presentation that was done during the HPC and batch day or from Tuesday that",
    "start": "1510559",
    "end": "1515840"
  },
  {
    "text": "goes into that really elaborately and then shows you with a full demo demo on",
    "start": "1515840",
    "end": "1523820"
  },
  {
    "text": "the bottom of the slide is the link to that presentation so preemption within the other system we",
    "start": "1523820",
    "end": "1532580"
  },
  {
    "text": "do that again as part of the normal scheduling cycle and we",
    "start": "1532580",
    "end": "1538100"
  },
  {
    "text": "use the guaranteed resources for that so again within the the setup of the queue",
    "start": "1538100",
    "end": "1545419"
  },
  {
    "text": "system we allow you to specify certain points in the in the hierarchy and we",
    "start": "1545419",
    "end": "1552380"
  },
  {
    "text": "give you a guaranteed amount of resources in the setup",
    "start": "1552380",
    "end": "1557720"
  },
  {
    "text": "so like a multi multi-tenancy fencing with priority offsets or nice fancy",
    "start": "1557720",
    "end": "1563960"
  },
  {
    "text": "things there's a lot of documentation on the side um goes a bit too far into them",
    "start": "1563960",
    "end": "1569779"
  },
  {
    "text": "what we can show you and we're going to try a live demo here uh based on a kind cluster we'll show you how we",
    "start": "1569779",
    "end": "1577720"
  },
  {
    "text": "redistribute some of the um",
    "start": "1577720",
    "end": "1583059"
  },
  {
    "text": "some of the workloads between the different cues and what we what we do I've set up",
    "start": "1583279",
    "end": "1591260"
  },
  {
    "text": "a small kind cluster to to do that for a three node cluster and I'm going to",
    "start": "1591260",
    "end": "1598700"
  },
  {
    "text": "submit a simple application the application is just a normal",
    "start": "1598700",
    "end": "1606799"
  },
  {
    "text": "job with a couple of extra annotations on there from",
    "start": "1606799",
    "end": "1611840"
  },
  {
    "text": "users to look to show what we want to do and where we want to run it",
    "start": "1611840",
    "end": "1619240"
  },
  {
    "text": "now let me create the application so we create the application we start up 10",
    "start": "1619580",
    "end": "1625400"
  },
  {
    "text": "ports because of the quotas that we've set up we can't run all of them we can run eight out of the the ten the other",
    "start": "1625400",
    "end": "1632419"
  },
  {
    "text": "two pots will just stay there and stay as pending in",
    "start": "1632419",
    "end": "1638360"
  },
  {
    "text": "the Unicorn web UI and now I've got",
    "start": "1638360",
    "end": "1644720"
  },
  {
    "text": "what's that one we can see we can see that kind of information too",
    "start": "1644720",
    "end": "1651220"
  },
  {
    "text": "and the web UI is gone okay that's what you could do a live demo thanks man",
    "start": "1653179",
    "end": "1660380"
  },
  {
    "text": "we've set up the the queues we've got this all running it runs in a low priority",
    "start": "1660380",
    "end": "1666200"
  },
  {
    "text": "setup and now what we're going to do we're going to submit a high priority job against it",
    "start": "1666200",
    "end": "1674539"
  },
  {
    "text": "on the same system it runs in a different queue",
    "start": "1674539",
    "end": "1679580"
  },
  {
    "text": "and it creates a number of high priority pods these spots at first stay in",
    "start": "1679580",
    "end": "1685279"
  },
  {
    "text": "pending because we don't we're not going to directly preempt things we wait a little bit to see what what's going on",
    "start": "1685279",
    "end": "1690860"
  },
  {
    "text": "and then after the wait time is done we are going to look for enough resources",
    "start": "1690860",
    "end": "1698179"
  },
  {
    "text": "that will allow us to do what we want to do so 30 seconds is is the the setup",
    "start": "1698179",
    "end": "1706460"
  },
  {
    "text": "and no come back",
    "start": "1706460",
    "end": "1712900"
  },
  {
    "text": "and in the meantime we'll see that we have redistributed some of the the data",
    "start": "1714860",
    "end": "1720320"
  },
  {
    "text": "so here's the Unicorn web UI we have created the the cues",
    "start": "1720320",
    "end": "1727279"
  },
  {
    "text": "we've got a root with low priority route with a high priority and within the",
    "start": "1727279",
    "end": "1732440"
  },
  {
    "text": "queues we have got the the setup that we had first and we have redistributed some of the load from root load to root High",
    "start": "1732440",
    "end": "1740840"
  },
  {
    "text": "um and when we reload we should get the the final state",
    "start": "1740840",
    "end": "1747679"
  },
  {
    "text": "and over time we have moved we've killed some of the root low we moved that into",
    "start": "1747679",
    "end": "1754159"
  },
  {
    "text": "root High we've got a guaranteed quota",
    "start": "1754159",
    "end": "1761419"
  },
  {
    "text": "sitting here that means that we are going to preempt whatever we want and we need but we are",
    "start": "1761419",
    "end": "1768620"
  },
  {
    "text": "never going to preempt more than that we allow to so we all we've",
    "start": "1768620",
    "end": "1774440"
  },
  {
    "text": "guaranteed that wood law will always have this this these spots running so we will give that",
    "start": "1774440",
    "end": "1780260"
  },
  {
    "text": "we don't go below that and it also means that rude High which had a higher guaranteed",
    "start": "1780260",
    "end": "1788000"
  },
  {
    "text": "but we've only got eight CPUs can't get to allocated to the the maximum of the",
    "start": "1788000",
    "end": "1794659"
  },
  {
    "text": "guaranteed that is there so we still have got pending pots",
    "start": "1794659",
    "end": "1801320"
  },
  {
    "text": "but we've redistributed some of the high priority are running some of the high priority are still",
    "start": "1801320",
    "end": "1806419"
  },
  {
    "text": "pending we've got low priority pending because they're still not being",
    "start": "1806419",
    "end": "1811760"
  },
  {
    "text": "scheduled that's just sitting there waiting for for things to to happen to hook that back in again to",
    "start": "1811760",
    "end": "1819860"
  },
  {
    "text": "what Peter mentioned around the quotas so we've got the quota",
    "start": "1819860",
    "end": "1826580"
  },
  {
    "text": "tracking going through the load for Peter runs in the root low",
    "start": "1826580",
    "end": "1832279"
  },
  {
    "text": "we see exactly what's Happening Here the second user is also tracked route High",
    "start": "1832279",
    "end": "1839539"
  },
  {
    "text": "we get the memory we get everything and then from the group we were assigned we both had assigned the same group",
    "start": "1839539",
    "end": "1847159"
  },
  {
    "text": "we see that both applications attract under that same group with rude law and root high",
    "start": "1847159",
    "end": "1854539"
  },
  {
    "text": "so the total usage is there to for everything everybody to check",
    "start": "1854539",
    "end": "1861200"
  },
  {
    "text": "now if I now delete",
    "start": "1861200",
    "end": "1866320"
  },
  {
    "text": "the first application we will see that the",
    "start": "1869120",
    "end": "1874220"
  },
  {
    "text": "leftover pots that were pending for the scheduler will get scheduled and",
    "start": "1874220",
    "end": "1880640"
  },
  {
    "text": "will become running after a short amount of time so we pick up where we were and",
    "start": "1880640",
    "end": "1886880"
  },
  {
    "text": "we we schedule whatever we have got as a leftover amount",
    "start": "1886880",
    "end": "1893059"
  },
  {
    "text": "that shows what we do with within the scheduling framework we've got full control over what we do with pots what",
    "start": "1899059",
    "end": "1906380"
  },
  {
    "text": "we assign which gets scheduled where we place them because in a kind",
    "start": "1906380",
    "end": "1911960"
  },
  {
    "text": "cluster that's a little bit more difficult to see but we have got full control of what what",
    "start": "1911960",
    "end": "1918200"
  },
  {
    "text": "gets placed where and we also are able to do preemption",
    "start": "1918200",
    "end": "1924320"
  },
  {
    "text": "based on to quote us what that you set up that you want so from a batch",
    "start": "1924320",
    "end": "1930399"
  },
  {
    "text": "processing perspective that gives you a full control over your environment you",
    "start": "1930399",
    "end": "1937340"
  },
  {
    "text": "can say I want certain users to have priority over other users you can even",
    "start": "1937340",
    "end": "1943460"
  },
  {
    "text": "say um Peter is not allowed anything in in one queue or the other queue so you can do a",
    "start": "1943460",
    "end": "1952520"
  },
  {
    "text": "full blown multi-tenant environment within the",
    "start": "1952520",
    "end": "1958279"
  },
  {
    "text": "system that we're going to set up so this was the the cluster that we used",
    "start": "1958279",
    "end": "1963440"
  },
  {
    "text": "we had a prepared cluster quotas and pmshin the",
    "start": "1963440",
    "end": "1969260"
  },
  {
    "text": "applications that got submitted were one for the use period one for the other",
    "start": "1969260",
    "end": "1974299"
  },
  {
    "text": "user and we saw the redistribution of",
    "start": "1974299",
    "end": "1979340"
  },
  {
    "text": "the quota to the other user",
    "start": "1979340",
    "end": "1984278"
  },
  {
    "text": "and we hopefully have some time left over for Q a if there's any further questions",
    "start": "1987140",
    "end": "1995980"
  },
  {
    "text": "yeah there's a microphone on the side for this for the people that are streaming because I was told that there",
    "start": "1996679",
    "end": "2002200"
  },
  {
    "text": "were a couple of people that were streaming so if you could go to the microphone please",
    "start": "2002200",
    "end": "2009120"
  },
  {
    "text": "hello yeah how do you um say like with driver and executors in",
    "start": "2013600",
    "end": "2020380"
  },
  {
    "text": "spark how do you stop the driver being prohibited so the the main thing is the",
    "start": "2020380",
    "end": "2026200"
  },
  {
    "text": "priority class that you set on on the driver and within the priority class you opt",
    "start": "2026200",
    "end": "2033460"
  },
  {
    "text": "out from the",
    "start": "2033460",
    "end": "2038340"
  },
  {
    "text": "preemption so that's that's the first thing the other thing that we do is when you submit",
    "start": "2039279",
    "end": "2046179"
  },
  {
    "text": "a spark driver and other things we create an application object within the",
    "start": "2046179",
    "end": "2051940"
  },
  {
    "text": "application object we Mark the first Port as the originator Port that's the",
    "start": "2051940",
    "end": "2057040"
  },
  {
    "text": "one that gets created first the also the originated Bots even if it doesn't have the allow preemption tag set always gets",
    "start": "2057040",
    "end": "2065200"
  },
  {
    "text": "put put in the back of the queue for preemption so we try our best not to",
    "start": "2065200",
    "end": "2072580"
  },
  {
    "text": "preempt to drive apart it's not a guarantee it's",
    "start": "2072580",
    "end": "2078040"
  },
  {
    "text": "a bit of like yes we put it always in the back of the queue if we can't do anything else then we will still",
    "start": "2078040",
    "end": "2085480"
  },
  {
    "text": "kill the driver pod yeah thank you and um the demo you showed",
    "start": "2085480",
    "end": "2091300"
  },
  {
    "text": "yeah were those two applications running with the same priority class no no so they were running with a different",
    "start": "2091300",
    "end": "2096460"
  },
  {
    "text": "priority class one was running with low priority that",
    "start": "2096460",
    "end": "2102880"
  },
  {
    "text": "was the one from in the in the the first queue that we set up and the second one was with high priority if I would have",
    "start": "2102880",
    "end": "2110260"
  },
  {
    "text": "submitted a third application while the first low priority was running and I",
    "start": "2110260",
    "end": "2116200"
  },
  {
    "text": "would put in a third application also with low priority the second application would have been scheduled first not only",
    "start": "2116200",
    "end": "2123820"
  },
  {
    "text": "because it's a higher priority but we also put fifo on the system so first in",
    "start": "2123820",
    "end": "2129880"
  },
  {
    "text": "first out but you can decide that anywhere in the queue hierarchy so if",
    "start": "2129880",
    "end": "2135760"
  },
  {
    "text": "you say I want to schedule a certain part of the queue first in first out but",
    "start": "2135760",
    "end": "2142660"
  },
  {
    "text": "the other part of the queue I want to schedule fairly or purely based on",
    "start": "2142660",
    "end": "2148420"
  },
  {
    "text": "priority you can do that that's that's all set up and all available within the",
    "start": "2148420",
    "end": "2153700"
  },
  {
    "text": "queue hierarchy so it's not just one policy it's policy per part of the queue that you set up okay",
    "start": "2153700",
    "end": "2161800"
  },
  {
    "text": "so and one more question so if I had two applications of the same priority class running yeah well I ran one and it had a",
    "start": "2161800",
    "end": "2168460"
  },
  {
    "text": "guaranteed resource of X but it filled the cluster and I ran my second one I had the set guarantee resource but the",
    "start": "2168460",
    "end": "2175599"
  },
  {
    "text": "same app the priority class would resources preempted from application one and given to application two yeah such",
    "start": "2175599",
    "end": "2181720"
  },
  {
    "text": "they both had at least yes guarantee yeah all right thank you if they are exactly the same priority yes yeah we",
    "start": "2181720",
    "end": "2188560"
  },
  {
    "text": "will still do that we distribute again over over the two applications",
    "start": "2188560",
    "end": "2195599"
  },
  {
    "text": "yeah I have two questions one is how does it is compare with Q the Q with K",
    "start": "2203440",
    "end": "2209320"
  },
  {
    "text": "that was also being shown like if you can broadly see the advantages and disadvantages of of both and the second",
    "start": "2209320",
    "end": "2216579"
  },
  {
    "text": "one is actually to do with this part which I it was not clear to me if you wanted if you have some kind of",
    "start": "2216579",
    "end": "2221740"
  },
  {
    "text": "implementation for fair usage and if it has like something fancy like the CL the",
    "start": "2221740",
    "end": "2227079"
  },
  {
    "text": "classical job schedules do which can kind of having how much have you used in the past with a kind of uh uh a lifetime",
    "start": "2227079",
    "end": "2236800"
  },
  {
    "text": "kind of um we we don't look at lifetime so to say so",
    "start": "2236800",
    "end": "2243040"
  },
  {
    "text": "um we because we don't know when you run for instance this park job or whatever we've got no idea what what comes in",
    "start": "2243040",
    "end": "2250540"
  },
  {
    "text": "um you can ask for gang scheduling but you ask for a certain amount of",
    "start": "2250540",
    "end": "2256180"
  },
  {
    "text": "resources we try to do our best and give you all the resources that you ask for if we can't",
    "start": "2256180",
    "end": "2262900"
  },
  {
    "text": "USD application submitter can say do I want to proceed with whatever I",
    "start": "2262900",
    "end": "2270099"
  },
  {
    "text": "can get or do I just want to fail so soft or hard game scheduling so that's",
    "start": "2270099",
    "end": "2275320"
  },
  {
    "text": "the only thing that you've got when when you look at that side of things",
    "start": "2275320",
    "end": "2280500"
  },
  {
    "text": "compared to Q I think Q is not hierarchical so it has a set of cues but",
    "start": "2280500",
    "end": "2289359"
  },
  {
    "text": "the hierarchy and and the the way that we distribute with guaranteed",
    "start": "2289359",
    "end": "2295359"
  },
  {
    "text": "um the other thing Q when it does preemption it PM's the whole job it",
    "start": "2295359",
    "end": "2301119"
  },
  {
    "text": "can't do one pot at a time it can't do distributions like that it it",
    "start": "2301119",
    "end": "2307420"
  },
  {
    "text": "does the whole job or nothing so that's not as flexible",
    "start": "2307420",
    "end": "2313359"
  },
  {
    "text": "um sharing quotas in the when you look at what we do in a",
    "start": "2313359",
    "end": "2321339"
  },
  {
    "text": "setup like you see here we can share quota from QA",
    "start": "2321339",
    "end": "2328859"
  },
  {
    "text": "all the way to the other side that is not possible with q",
    "start": "2328960",
    "end": "2336460"
  },
  {
    "text": "you've got a group of group of cues that you've set up beforehand so the",
    "start": "2336460",
    "end": "2342880"
  },
  {
    "text": "the flexibility is not there as much within the the queue setup",
    "start": "2342880",
    "end": "2349000"
  },
  {
    "text": "thanks a lot",
    "start": "2349000",
    "end": "2351780"
  },
  {
    "text": "any further questions",
    "start": "2355060",
    "end": "2358020"
  },
  {
    "text": "hello so when you leave the part in pending is the application able to determine",
    "start": "2363400",
    "end": "2370480"
  },
  {
    "text": "whether the pot is impending because there's not enough resource available and it's effectively queued compared to",
    "start": "2370480",
    "end": "2377020"
  },
  {
    "text": "it's impending because some secret it needs isn't there or some other reason why it might not be impending yeah so we",
    "start": "2377020",
    "end": "2383140"
  },
  {
    "text": "we use the event system and we put events on the pots to show you that the",
    "start": "2383140",
    "end": "2388380"
  },
  {
    "text": "Pod is waiting for resource to become available yeah we do that",
    "start": "2388380",
    "end": "2393700"
  },
  {
    "text": "yeah for sure thank you that was it then",
    "start": "2393700",
    "end": "2398860"
  },
  {
    "text": "[Applause]",
    "start": "2398860",
    "end": "2401369"
  }
]