[
  {
    "text": "happy to see all of you here today but not quite happy with the technical issues that we got okay so now me Gio",
    "start": "359",
    "end": "9960"
  },
  {
    "text": "and my friend here giri will walk you through to our journey to support 10,000 AGD",
    "start": "9960",
    "end": "17160"
  },
  {
    "text": "apps first uh you can find us on our social media and a bit brief introduction to",
    "start": "17160",
    "end": "24039"
  },
  {
    "text": "our company go to financial this is the financial group of of go to the leading digital ecosystem in",
    "start": "24039",
    "end": "32160"
  },
  {
    "text": "Indonesia and then uh we want you to understand where where we are coming",
    "start": "32160",
    "end": "37360"
  },
  {
    "text": "from so an overview about our C clusters and our",
    "start": "37360",
    "end": "43399"
  },
  {
    "text": "setup we have we manage 50 greatest clusters across different Cloud providers in Singapore indona region and",
    "start": "43399",
    "end": "51640"
  },
  {
    "text": "we have even more than 30,000 pots running and here is an AG of",
    "start": "51640",
    "end": "61000"
  },
  {
    "text": "our uh OB monitoring tools you can see that we already reached 11,000 ald apps",
    "start": "61000",
    "end": "68119"
  },
  {
    "text": "earlier this year the numbers is only at 7,000 only 7,000 and then as we onboard",
    "start": "68119",
    "end": "75520"
  },
  {
    "text": "more teams to our platform and consequently arusd as well we grew and",
    "start": "75520",
    "end": "82040"
  },
  {
    "text": "un unfortunately arusd can get up those 11,000 repositories consisted",
    "start": "82040",
    "end": "89640"
  },
  {
    "text": "of 60,000 repositories yeah 60,000 repositories 60 projects and itust up",
    "start": "89640",
    "end": "96640"
  },
  {
    "text": "more than 38 3800k total objects and on our largest",
    "start": "96640",
    "end": "102799"
  },
  {
    "text": "cluster alone we have 2,000 applications and watches 40,000",
    "start": "102799",
    "end": "108799"
  },
  {
    "text": "objects uh what we currently use is a centralized AG model where we have a",
    "start": "108799",
    "end": "114000"
  },
  {
    "text": "single management cluster where we install our own arusd and it connects to",
    "start": "114000",
    "end": "119520"
  },
  {
    "text": "a sing single git provider which is our gitlab",
    "start": "119520",
    "end": "124240"
  },
  {
    "text": "instance there are several approach using this simple model which is easy to maintain you the the maintenance is only",
    "start": "124799",
    "end": "132200"
  },
  {
    "text": "we upgrade the client library and the image version on one",
    "start": "132200",
    "end": "137319"
  },
  {
    "text": "night it's easy to integrate with our automation tool we only need a single",
    "start": "137319",
    "end": "143920"
  },
  {
    "text": "credential and single client Library fion it's easy to manage the centralized ARB on the policy",
    "start": "143920",
    "end": "150440"
  },
  {
    "text": "CSV and it's nice to have a single dashboard to view all of the resource across 50",
    "start": "150440",
    "end": "157640"
  },
  {
    "text": "clusters here here this is how we use Aro CD we have our own developer",
    "start": "157640",
    "end": "163239"
  },
  {
    "text": "platform called go page and we expose",
    "start": "163239",
    "end": "169360"
  },
  {
    "text": "deployment API to developer deployment functionality so they can so our tool",
    "start": "169360",
    "end": "175440"
  },
  {
    "text": "generate manifest and commit it to G repository and some applications can",
    "start": "175440",
    "end": "181200"
  },
  {
    "text": "share the same repos story where this this AR application will be pushed to",
    "start": "181200",
    "end": "186319"
  },
  {
    "text": "each of their own cluster and here is how we use the AG",
    "start": "186319",
    "end": "193799"
  },
  {
    "text": "different AGD apps we separate our service into two arusd apps into and stable app where",
    "start": "193799",
    "end": "202480"
  },
  {
    "text": "this is very easy for the to perform a more complicated deployment using Canary",
    "start": "202480",
    "end": "208080"
  },
  {
    "text": "strategies perform promotion to from the canary version to stable and also",
    "start": "208080",
    "end": "214680"
  },
  {
    "text": "perform roll back next we also have separate applications foro side car Ando",
    "start": "214680",
    "end": "222239"
  },
  {
    "text": "Gateway and this is where we roll we split we perform traffic split and also",
    "start": "222239",
    "end": "228680"
  },
  {
    "text": "we can decouple the fun the sto Gateway and resources to another app and it",
    "start": "228680",
    "end": "235200"
  },
  {
    "text": "makes it very easy for our platform to perform the update separately different",
    "start": "235200",
    "end": "240400"
  },
  {
    "text": "from the way developer from deployment next um oh for our runtime",
    "start": "240400",
    "end": "249959"
  },
  {
    "text": "components we also leverage argd we have a monor repo in this monor repo we have",
    "start": "249959",
    "end": "256040"
  },
  {
    "text": "a root app set which generates app the parent app for each clusters on each",
    "start": "256040",
    "end": "261519"
  },
  {
    "text": "cluster they can have the base applications they can have another apps",
    "start": "261519",
    "end": "266919"
  },
  {
    "text": "set to installs different components using Helm and in case there is a",
    "start": "266919",
    "end": "273080"
  },
  {
    "text": "difference between the base and the final manifest we want we have a an",
    "start": "273080",
    "end": "278759"
  },
  {
    "text": "application using Ser side apply Pates and this model also is used for all 50",
    "start": "278759",
    "end": "285000"
  },
  {
    "text": "different clusters and by the way we also install argd using",
    "start": "285000",
    "end": "290919"
  },
  {
    "text": "argd there are challenges to this Simplicity the first one is that we need",
    "start": "290919",
    "end": "297080"
  },
  {
    "text": "to maintain connectivity to all 50 clusters that we we manage and sometimes using tunnel oping doesn't work because",
    "start": "297080",
    "end": "303479"
  },
  {
    "text": "of compliance or other reasons we use public MTS conect",
    "start": "303479",
    "end": "308960"
  },
  {
    "text": "connectivity there are also functionalities that that's challenging we must okay this must maintain unique",
    "start": "308960",
    "end": "315199"
  },
  {
    "text": "application name globally 63 charts but this is actually updated because because",
    "start": "315199",
    "end": "321000"
  },
  {
    "text": "we move from label identity to annotation identity and 63 chge is",
    "start": "321000",
    "end": "326560"
  },
  {
    "text": "limitation of surfaces object and helm so I think this is no longer problem but",
    "start": "326560",
    "end": "332080"
  },
  {
    "text": "we still pay the technical depths to have prefixes and suffixes in our aid",
    "start": "332080",
    "end": "337440"
  },
  {
    "text": "app name and more this is also a single point of",
    "start": "337440",
    "end": "342880"
  },
  {
    "text": "failure and then we also face performance issue where slow recation",
    "start": "342880",
    "end": "348400"
  },
  {
    "text": "and S happens this is painful for our developer because they are the one who performs the deployment and syn this is",
    "start": "348400",
    "end": "356280"
  },
  {
    "text": "also reflected in the work dep and AGD Aon buet metrix okay some of you already maybe",
    "start": "356280",
    "end": "364120"
  },
  {
    "text": "felt it that slow UI loading time and we have also frequent suffer",
    "start": "364120",
    "end": "370560"
  },
  {
    "text": "rep suffer om kills another issue is the high rate of git API calls this is refed in some of",
    "start": "370560",
    "end": "378080"
  },
  {
    "text": "the locks disconnection issue with our git lab and also feasible through AR git request total",
    "start": "378080",
    "end": "384319"
  },
  {
    "text": "Matrix there are also issue of high repo cashm and imbalance charts and I",
    "start": "384319",
    "end": "391199"
  },
  {
    "text": "clusters next my friend giri will show you our journey to to tune the",
    "start": "391199",
    "end": "396319"
  },
  {
    "text": "performance of our argd instense all right let's talk about our journey in",
    "start": "396319",
    "end": "401400"
  },
  {
    "text": "tuning the parameters and configurations of our Argo City instance to scale and",
    "start": "401400",
    "end": "406440"
  },
  {
    "text": "support uh more than 10,000 apps that we have as a refresher uh here's the aric",
    "start": "406440",
    "end": "412360"
  },
  {
    "text": "components it's Argo City architecture basically Argo City consists of four different layers first layer is is the",
    "start": "412360",
    "end": "419720"
  },
  {
    "text": "UI layer which is the primary uh primary layer that the users are interact with",
    "start": "419720",
    "end": "425440"
  },
  {
    "text": "the Argos systems the layer contains a web app and the CLI second layer is the",
    "start": "425440",
    "end": "431400"
  },
  {
    "text": "application layer that contains the API server or the Argo City server which serves requests coming from the web app",
    "start": "431400",
    "end": "438400"
  },
  {
    "text": "and the CLI the third layer is the core layer which is the primary layer that serves the main responsibilities the",
    "start": "438400",
    "end": "445080"
  },
  {
    "text": "main functionalities of Aro CD systems it contains the app controll the abset",
    "start": "445080",
    "end": "450199"
  },
  {
    "text": "controller and the report server the app controller watches application objects and reconciles the abset controller",
    "start": "450199",
    "end": "458120"
  },
  {
    "text": "generates application objects based on certain templates and the repo server uh",
    "start": "458120",
    "end": "463319"
  },
  {
    "text": "serves manifest generation requests coming from the app controller and the argu CD server the last layer the infr",
    "start": "463319",
    "end": "470560"
  },
  {
    "text": "layer is uh contains the external uh components that argd depends on there's",
    "start": "470560",
    "end": "477199"
  },
  {
    "text": "redis for catching purpose there's Cube API that arus used to watch kubernetes",
    "start": "477199",
    "end": "482240"
  },
  {
    "text": "objects and apply uh changes there's a git that contains the desired manifest",
    "start": "482240",
    "end": "487759"
  },
  {
    "text": "that Aro CD refers to and there's Dex which contains uh",
    "start": "487759",
    "end": "493879"
  },
  {
    "text": "authentication let's first talk about arity server component uh as we grow we",
    "start": "495360",
    "end": "501280"
  },
  {
    "text": "started receiving complaints from our arusd users that the dashboard is very",
    "start": "501280",
    "end": "506840"
  },
  {
    "text": "slow it it it has slow UI uh for us uh I I I believe if you",
    "start": "506840",
    "end": "513440"
  },
  {
    "text": "started having more than thousand applications you would start uh facing the same problem for us uh it took us up",
    "start": "513440",
    "end": "520560"
  },
  {
    "text": "to 2 minutes in order to load the entire ago CD UI dashboard which is really a pain to solve this we enabled gzip",
    "start": "520560",
    "end": "527200"
  },
  {
    "text": "compression in the Argo CD server and the impact was uh really great it uh",
    "start": "527200",
    "end": "532480"
  },
  {
    "text": "fastened up the loadup time by 5x and it reduced the data transfer from the",
    "start": "532480",
    "end": "538440"
  },
  {
    "text": "server to browser by 7x smaller next is more of a tip on using",
    "start": "538440",
    "end": "544519"
  },
  {
    "text": "the argui dashboard it's not really a tuning NE it's not necessarily tuning uh",
    "start": "544519",
    "end": "550200"
  },
  {
    "text": "the Argo CD provides a feature called selectors the selectors user can use",
    "start": "550200",
    "end": "555640"
  },
  {
    "text": "selectors uh to filter out applications based on labels projects or name spaces",
    "start": "555640",
    "end": "561640"
  },
  {
    "text": "uh as can be seen by this screenshot as an example we selected uh one project",
    "start": "561640",
    "end": "566720"
  },
  {
    "text": "and one name space instead of loading up the entire 10,000 application objects in the homepage uh this was just a subset of",
    "start": "566720",
    "end": "574680"
  },
  {
    "text": "those applications and it loads up quite instantly under one second and what's nice is that the selectors are safe the",
    "start": "574680",
    "end": "581560"
  },
  {
    "text": "next time we load up load up the Argo CD UI which is quite handy for the",
    "start": "581560",
    "end": "587160"
  },
  {
    "text": "users next is about kubernetes CPU limits this is more on uh tuning on the",
    "start": "587160",
    "end": "592880"
  },
  {
    "text": "kubernetes site so we can also apply this outside of ourd use case uh as we",
    "start": "592880",
    "end": "598079"
  },
  {
    "text": "grow we started see see a lot of our agic components got CPU throtle as can",
    "start": "598079",
    "end": "603320"
  },
  {
    "text": "be seen by our monitoring dashboard we started seeing CPU saturation as can see be seen by the spikes especially during",
    "start": "603320",
    "end": "610000"
  },
  {
    "text": "the uh the peak hours and when the number of synchronizations get uh high",
    "start": "610000",
    "end": "615760"
  },
  {
    "text": "so uh kubernetes implements the CPU request and limit using cgroup mechanism",
    "start": "615760",
    "end": "622399"
  },
  {
    "text": "and the cgroup mechanism is based on the CFS or completely Fair scheduler by the",
    "start": "622399",
    "end": "627640"
  },
  {
    "text": "Linux kernel the CFS guarantees or throttle CPU",
    "start": "627640",
    "end": "633680"
  },
  {
    "text": "utilization uh based on the proportions of the container shares or in the cgroup uh terminology it's called CPU shares",
    "start": "633680",
    "end": "640000"
  },
  {
    "text": "and CPU quota on that particular note we don't have much time to talk more about this but it's a pretty deep and",
    "start": "640000",
    "end": "646240"
  },
  {
    "text": "interesting topic if you are interested you can see the attached references uh with this we decided to live off the CPU",
    "start": "646240",
    "end": "652680"
  },
  {
    "text": "limit so in especially in our app controller we don't use CPU limit at all we only use the uh the CPU uh the CPU",
    "start": "652680",
    "end": "660440"
  },
  {
    "text": "request in kubernetes the result we no longer uh see CPU saturation and the",
    "start": "660440",
    "end": "667000"
  },
  {
    "text": "components can use up the remaining uh available CPUs in particular node let's move on to the report server",
    "start": "667000",
    "end": "674720"
  },
  {
    "text": "as go mentioned uh as our number of repositories grow we started seeing the",
    "start": "674720",
    "end": "681480"
  },
  {
    "text": "repo server got om killed quite frequently at that time so the repo server fork and EXA the git processes so",
    "start": "681480",
    "end": "689839"
  },
  {
    "text": "if you enter EXA uh into the repo server you would see a bunch of Kit processes",
    "start": "689839",
    "end": "695320"
  },
  {
    "text": "there as well as the template generators like Helm and customized and this consumes uh memory as we scale uh the",
    "start": "695320",
    "end": "704240"
  },
  {
    "text": "repo server consumes a lot of memory and get om kill to scale this it's it's a",
    "start": "704240",
    "end": "709399"
  },
  {
    "text": "classic solution we increase the replicas of the repo server and enable",
    "start": "709399",
    "end": "714440"
  },
  {
    "text": "HPA or horizontal Port outo scaling and po uh and and put the memory utilization",
    "start": "714440",
    "end": "719680"
  },
  {
    "text": "as a target with this we distribute more memory uh requests to uh across uh",
    "start": "719680",
    "end": "726440"
  },
  {
    "text": "multiple replicas of repo server as an alternative there's actually a feature in repo server there's a flag called",
    "start": "726440",
    "end": "733880"
  },
  {
    "text": "parallelism limit which will uh control how many manifest generation requests",
    "start": "733880",
    "end": "739199"
  },
  {
    "text": "can be performed in parallel per pod per repos pot and this avoids om kills",
    "start": "739199",
    "end": "744880"
  },
  {
    "text": "however there's a tradeoff that uh we would be having a lower Troop of manifest generation as we set the",
    "start": "744880",
    "end": "750800"
  },
  {
    "text": "parallelism limit uh as I mentioned Also earlier uh",
    "start": "750800",
    "end": "757519"
  },
  {
    "text": "the Aro the the Argo CD server and the controller talks to the repo server for manifest generation requests right um as",
    "start": "757519",
    "end": "765680"
  },
  {
    "text": "we grow we started seeing uh a bunch of timeout errors from our logs to to to uh",
    "start": "765680",
    "end": "772120"
  },
  {
    "text": "to solve this we have to tune the client timeout uh configurations in both the",
    "start": "772120",
    "end": "778560"
  },
  {
    "text": "app controller and the server components we really need to set it up in both because we we thought by setting up only",
    "start": "778560",
    "end": "786320"
  },
  {
    "text": "in one component to solve the problem but we we really need to set it up in both",
    "start": "786320",
    "end": "793079"
  },
  {
    "text": "components and then we started uh we we encountered a problem with a persistent",
    "start": "793079",
    "end": "799000"
  },
  {
    "text": "High git fetch request putting pressure to our our git infrastructure uh and it's coming from",
    "start": "799000",
    "end": "805760"
  },
  {
    "text": "the repo server so Argo CD catches the generated manifest into the radius and",
    "start": "805760",
    "end": "812279"
  },
  {
    "text": "by default it has a default expired time of 24 hours when the Manifest remote",
    "start": "812279",
    "end": "818240"
  },
  {
    "text": "file changes in the repository even though the repository tag hasn't changed",
    "start": "818240",
    "end": "823760"
  },
  {
    "text": "so for instance if our users uh apply changes and it doesn't change the tag in",
    "start": "823760",
    "end": "829000"
  },
  {
    "text": "the repository or in the case of uh hell manifest it's it's still behind the same",
    "start": "829000",
    "end": "834360"
  },
  {
    "text": "helmt version or Helm Tech version uh the a shorter expired time is is",
    "start": "834360",
    "end": "840120"
  },
  {
    "text": "preferable so the repo server can pick up update quicker than 24 hours however",
    "start": "840120",
    "end": "845440"
  },
  {
    "text": "for our case the remote files references are already hermetic which means we",
    "start": "845440",
    "end": "851399"
  },
  {
    "text": "always do forward fixes and forward patches and and increase the versions of the the repository in this case for our",
    "start": "851399",
    "end": "858079"
  },
  {
    "text": "case we can uh use a higher expiry time so uh after applying this we see",
    "start": "858079",
    "end": "865240"
  },
  {
    "text": "dramatic decrease in our git request total uh metrics quite",
    "start": "865240",
    "end": "871720"
  },
  {
    "text": "significantly next is about monor repo our mono as zo mentioned uh uses app of",
    "start": "871800",
    "end": "878680"
  },
  {
    "text": "apps pattern and the apps app Feature of Argo CD additionally we also use the",
    "start": "878680",
    "end": "884079"
  },
  {
    "text": "multisources app Feature by Argo C which would allow one application to refer to multiple directories in in the same",
    "start": "884079",
    "end": "891839"
  },
  {
    "text": "repository with different generation strategies so One Directory can be plain kubernetes manifest the other one can be",
    "start": "891839",
    "end": "898079"
  },
  {
    "text": "helm and and and things like that uh this feature puts up very high git Fetch",
    "start": "898079",
    "end": "904560"
  },
  {
    "text": "and git LS remote request again putting pressure to our our gate infrastructure we've investigated it and we found out",
    "start": "904560",
    "end": "911320"
  },
  {
    "text": "that this was potentially a bu in Argo CD so we discussed with the Aro CD",
    "start": "911320",
    "end": "916600"
  },
  {
    "text": "there's an Argo CD uh six scalability uh group also in in the community we",
    "start": "916600",
    "end": "922000"
  },
  {
    "text": "implemented an undocumented workr so if you're interested uh please look at the giab issue after applying this",
    "start": "922000",
    "end": "928920"
  },
  {
    "text": "undocumented work around actually yeah that one the the red highlighted lines",
    "start": "928920",
    "end": "933959"
  },
  {
    "text": "are the undocumented work around right now it's a open issue but in in in in",
    "start": "933959",
    "end": "939600"
  },
  {
    "text": "the in coupon us we brainstorm a a a solution with the maintainers we found a",
    "start": "939600",
    "end": "944639"
  },
  {
    "text": "solution I hope the fix I think the fix is going uh soon after applying the workaround the",
    "start": "944639",
    "end": "951720"
  },
  {
    "text": "git Fest requests drop very dramatically on the on the right screenshot however we are still seeing a very high LS",
    "start": "951720",
    "end": "958839"
  },
  {
    "text": "remote request and the bug issue is still is still open the monor repo usually if you have",
    "start": "958839",
    "end": "965639"
  },
  {
    "text": "monor repo in Argo CD uh you would want to use the web hook integration as well",
    "start": "965639",
    "end": "971639"
  },
  {
    "text": "the monor because it contains multiple uh application objects if there's a change in one of the directories it will",
    "start": "971639",
    "end": "979199"
  },
  {
    "text": "trigger Aro City to refresh all the applications that refer to the same repository even though those changes are",
    "start": "979199",
    "end": "986079"
  },
  {
    "text": "not related at all so in the refresh process Argo CD",
    "start": "986079",
    "end": "991120"
  },
  {
    "text": "invalidates uh the catch for all applications and call kubernetes Api to",
    "start": "991120",
    "end": "996720"
  },
  {
    "text": "annotate every single application with some special annotations and this",
    "start": "996720",
    "end": "1002079"
  },
  {
    "text": "process is a network bound process which slows down the entire ago CD update process especially it's impacting",
    "start": "1002079",
    "end": "1009560"
  },
  {
    "text": "reconciliation performance when you have more than 1,000 applications there's a feature in Argo CD uh Through The",
    "start": "1009560",
    "end": "1016480"
  },
  {
    "text": "annotation called manifest generate path we can filter out the directories or the Manifest that are not related uh at all",
    "start": "1016480",
    "end": "1024360"
  },
  {
    "text": "to the applications they speed up uh the update process of our AR",
    "start": "1024360",
    "end": "1029678"
  },
  {
    "text": "CD let's move to the app controller component so the app controller",
    "start": "1029679",
    "end": "1034760"
  },
  {
    "text": "component uh uses two kind of cues for reconciliation and synchronization purposes as we grow we started uh seeing",
    "start": "1034760",
    "end": "1042798"
  },
  {
    "text": "our work depth piling up but it never went out at all this means there are more Works uh push to the cubat the",
    "start": "1042799",
    "end": "1049760"
  },
  {
    "text": "number of consumers or processors are less uh those workers or processors are called operation processors and Status",
    "start": "1049760",
    "end": "1056720"
  },
  {
    "text": "processors in Argo CD we need to tune or increase the number of processors so there are more consumers of the task uh",
    "start": "1056720",
    "end": "1063919"
  },
  {
    "text": "as a rule of Thum uh for every 1,000 application we set 50 status processors",
    "start": "1063919",
    "end": "1069440"
  },
  {
    "text": "and 25 operation processors and then the app",
    "start": "1069440",
    "end": "1075159"
  },
  {
    "text": "controller uh can be scaled the this app controller is horizontally shable shable",
    "start": "1075159",
    "end": "1082080"
  },
  {
    "text": "we can uh we can Scale based on chards and the sharding algorithm in Argo CD is",
    "start": "1082080",
    "end": "1087200"
  },
  {
    "text": "on the cluster level which means uh different clusters can get assigned to different uh shs app controller shs uh",
    "start": "1087200",
    "end": "1095120"
  },
  {
    "text": "to set it up it's pretty straightforward we increase the number of replicas of uh app controller and set the environment",
    "start": "1095120",
    "end": "1101880"
  },
  {
    "text": "variable with this we distribute uh the workloads the reconcile and syncing workloads to uh to to different pods",
    "start": "1101880",
    "end": "1109400"
  },
  {
    "text": "different sharts after implementing sharts we started seeing the sharts are consuming",
    "start": "1109400",
    "end": "1116440"
  },
  {
    "text": "CPU utilization uh unevenly so there's an un imbalance uh CPU consumption",
    "start": "1116440",
    "end": "1122720"
  },
  {
    "text": "across the sharts some sharts consume higher CPU than the other shars so Argo",
    "start": "1122720",
    "end": "1127880"
  },
  {
    "text": "CD because Aro CD sharts per cluster not per app technically um the largest",
    "start": "1127880",
    "end": "1134360"
  },
  {
    "text": "clusters can be assigned to the same Shar and likewise smaller clusters can",
    "start": "1134360",
    "end": "1139760"
  },
  {
    "text": "get assigned to the same Shard as well creating imbalance uh workload and",
    "start": "1139760",
    "end": "1144880"
  },
  {
    "text": "creates cluster noisy problem and the Argo CD released new sharding algorithm",
    "start": "1144880",
    "end": "1150280"
  },
  {
    "text": "called round robin uh sharding algorithm but this might not help much either because there's still a chance that the",
    "start": "1150280",
    "end": "1157120"
  },
  {
    "text": "largest cruster can get assigned to the same shart so to solve this issue we did",
    "start": "1157120",
    "end": "1162400"
  },
  {
    "text": "Manual shart allocation uh to fun the shark resources so we ensure through this manual allocation we ensure our",
    "start": "1162400",
    "end": "1169280"
  },
  {
    "text": "larest clusters are spread across the sharts and to do this we simply uh apply",
    "start": "1169280",
    "end": "1174360"
  },
  {
    "text": "secret object in argu CD there's a Shar number that we can apply from there after applying this from the screenshot",
    "start": "1174360",
    "end": "1180440"
  },
  {
    "text": "we can start seeing uh the the the CPU utilizations are more even evenly spread",
    "start": "1180440",
    "end": "1186480"
  },
  {
    "text": "across the charts um there's an open discussion GitHub uh",
    "start": "1186480",
    "end": "1192720"
  },
  {
    "text": "discussion that we think argd on the app level will be better than the cluster",
    "start": "1192720",
    "end": "1198000"
  },
  {
    "text": "level after all this shuning we still see the",
    "start": "1198000",
    "end": "1203799"
  },
  {
    "text": "app controller still consumes very high uh CPU utilization and we think uh this",
    "start": "1203799",
    "end": "1209760"
  },
  {
    "text": "also impacts uh some of the reconciliation still can be uh improved so Argo CD watches uh all the field",
    "start": "1209760",
    "end": "1217840"
  },
  {
    "text": "changes of all the projects all the objects that that it tracks so some kubernetes Fields uh could be very",
    "start": "1217840",
    "end": "1224880"
  },
  {
    "text": "concise or frequently updated dynamically updated by this kubernetes so things like uh kubernetes status",
    "start": "1224880",
    "end": "1232080"
  },
  {
    "text": "resource version those are the fields that kubernetes dynamically generates and updates but we don't really Define",
    "start": "1232080",
    "end": "1239679"
  },
  {
    "text": "them in the desired manifest those dynamic or frequently updated Fields uh",
    "start": "1239679",
    "end": "1246039"
  },
  {
    "text": "we call it high turn objects which trigger uh unnecessary reconciliations",
    "start": "1246039",
    "end": "1251480"
  },
  {
    "text": "and and and consumption of the app controller in the recent 2.8 version",
    "start": "1251480",
    "end": "1257320"
  },
  {
    "text": "there's a feature called ignore resource updates and ignore differences we can use this feature to filter out all the",
    "start": "1257320",
    "end": "1264000"
  },
  {
    "text": "fields that are not related we we don't really care so from the left hand side",
    "start": "1264000",
    "end": "1269480"
  },
  {
    "text": "the example we uh decided to ignore the HPA annotations the replica set",
    "start": "1269480",
    "end": "1276279"
  },
  {
    "text": "annotations the endpoint slice fields and annotations which are pretty noisy",
    "start": "1276279",
    "end": "1281320"
  },
  {
    "text": "and uh yeah which are pretty noisy and we also ignored the status uh field note",
    "start": "1281320",
    "end": "1287000"
  },
  {
    "text": "that if you implement customer controllers you might rely on the status uh field uh please be careful also but",
    "start": "1287000",
    "end": "1294080"
  },
  {
    "text": "for our use case uh we we don't really need the status fields in all the cumulated objects as can be seen the the result",
    "start": "1294080",
    "end": "1303520"
  },
  {
    "text": "was quite amazing it reduced the CP utilization by almost half almost half",
    "start": "1303520",
    "end": "1309039"
  },
  {
    "text": "year for a relatively simple feature",
    "start": "1309039",
    "end": "1314159"
  },
  {
    "text": "with this amazing impact uh it's it's really great shout out to the the Argo City maintainers for making this",
    "start": "1314159",
    "end": "1321559"
  },
  {
    "text": "happen our last problem that we encountered was related to Aro City API",
    "start": "1321559",
    "end": "1327320"
  },
  {
    "text": "client Library so as uh Gio mentioned and the keynote this morning we have internal",
    "start": "1327320",
    "end": "1335120"
  },
  {
    "text": "developer platform and our internal platform uses the aroid API client library to interact with the Argos",
    "start": "1335120",
    "end": "1342279"
  },
  {
    "text": "servers as we scale we started seeing HTTP go away errors from our platform we",
    "start": "1342279",
    "end": "1348240"
  },
  {
    "text": "investigated it and we found out there's a bug in this Upstream uh API client",
    "start": "1348240",
    "end": "1353919"
  },
  {
    "text": "Library when using GPC web mode through uh we fixed this in upstream and uh yeah",
    "start": "1353919",
    "end": "1360799"
  },
  {
    "text": "right now it's it's already fixed uh I think it's going to be in 2.10 or above",
    "start": "1360799",
    "end": "1366919"
  },
  {
    "text": "that um also one one note um if our users are using argd CLI even though the",
    "start": "1366919",
    "end": "1374520"
  },
  {
    "text": "grpc web mode is not set explicitly it may fall back GPC web mode true uh if",
    "start": "1374520",
    "end": "1381480"
  },
  {
    "text": "you if you are using it please check your Aro CD config file in your local machine as an alternative we can use the",
    "start": "1381480",
    "end": "1388799"
  },
  {
    "text": "native grpc to talk to AR CD server as opposed to the grpc web mode uh to avoid",
    "start": "1388799",
    "end": "1394400"
  },
  {
    "text": "this issue next uh gu is going to talk how we can further improve our existing Argo CD",
    "start": "1394400",
    "end": "1403279"
  },
  {
    "text": "setup now that we have taken a good look to how we manage AR CD and identifies",
    "start": "1404559",
    "end": "1409840"
  },
  {
    "text": "those problem there are several approaches that you want to explore the first one is dat",
    "start": "1409840",
    "end": "1416159"
  },
  {
    "text": "decentralized model this is completely opposite of what we already have using",
    "start": "1416159",
    "end": "1422000"
  },
  {
    "text": "Okay each argd instance in each clusters it offers a couple adventages",
    "start": "1422000",
    "end": "1428240"
  },
  {
    "text": "the first one is the application control workload now is distributed across",
    "start": "1428240",
    "end": "1434559"
  },
  {
    "text": "clusters this makes it very easy to scale more easy to scale and then the",
    "start": "1434559",
    "end": "1441600"
  },
  {
    "text": "access to KU API now is limited to local only no longer maintaining connectivity from the management clusters this makes",
    "start": "1441600",
    "end": "1448440"
  },
  {
    "text": "it better security practice and narrow down our attack surface but yeah there are also thread",
    "start": "1448440",
    "end": "1456279"
  },
  {
    "text": "offs using this using this approach one is we lose our easier maintenance and",
    "start": "1456279",
    "end": "1462640"
  },
  {
    "text": "upgrade headache we we are consistent of small platform engineering team so we",
    "start": "1462640",
    "end": "1469320"
  },
  {
    "text": "already have a lot to upgrade greatest clusters and the still M inside so now",
    "start": "1469320",
    "end": "1475399"
  },
  {
    "text": "we want to avoid this kind of main burden another one is to avoid",
    "start": "1475399",
    "end": "1481120"
  },
  {
    "text": "automation ha where we maintain several different versions of AGD client",
    "start": "1481120",
    "end": "1486919"
  },
  {
    "text": "versions and then we will lose the centralized dashboard because not that we have each AR versions each AR",
    "start": "1486919",
    "end": "1494240"
  },
  {
    "text": "instance in each clusters we will have 50 different AR CD dashboards and as you know that we have",
    "start": "1494240",
    "end": "1502200"
  },
  {
    "text": "a single largest cluster then we still need tuning regardless on that clusters",
    "start": "1502200",
    "end": "1507960"
  },
  {
    "text": "because the size is different besides decentralized model we",
    "start": "1507960",
    "end": "1513880"
  },
  {
    "text": "also look up into aent based AR or the hybrid model this is PED by aqu and as",
    "start": "1513880",
    "end": "1520720"
  },
  {
    "text": "you can see it has a distributed controller where the workload is spread",
    "start": "1520720",
    "end": "1526679"
  },
  {
    "text": "evenly for distributedly and S a single control panel C contr panel to view all",
    "start": "1526679",
    "end": "1532799"
  },
  {
    "text": "resources throughout the workload clusters we hope similar model is also",
    "start": "1532799",
    "end": "1540080"
  },
  {
    "text": "supported in the community versions and in fact progress has been made so there is a the first is the",
    "start": "1540080",
    "end": "1548120"
  },
  {
    "text": "optional Feature Feature optional pool mechanism for application set so that the application set in the",
    "start": "1548120",
    "end": "1556000"
  },
  {
    "text": "remote clusters can generate applications in that own clusters and the ar ar instance in that cluster can",
    "start": "1556000",
    "end": "1564480"
  },
  {
    "text": "reconcile on its own the second one is the issue of issue for centralized UI for multiple arity",
    "start": "1564480",
    "end": "1571039"
  },
  {
    "text": "instances yeah we can track this issues numbers to see where the future for this",
    "start": "1571039",
    "end": "1578000"
  },
  {
    "text": "model in community verions another nice thing to add is the suide pagination this is already there",
    "start": "1578000",
    "end": "1584799"
  },
  {
    "text": "on aqu version and I believe there are planning to bring it upstreams and they are targeting version 2.10 for the",
    "start": "1584799",
    "end": "1593760"
  },
  {
    "text": "release and we want to say thank you for and since we are heavily inspired from",
    "start": "1593760",
    "end": "1600120"
  },
  {
    "text": "the work being done in Tik Tok adubi from Alex and the amazing AGD",
    "start": "1600120",
    "end": "1606440"
  },
  {
    "text": "documentations and that's it um we still have time for",
    "start": "1606440",
    "end": "1613759"
  },
  {
    "text": "questions thank you",
    "start": "1615320",
    "end": "1619600"
  },
  {
    "text": "okay now the questions",
    "start": "1620600",
    "end": "1624039"
  },
  {
    "text": "time okay any questions okay in front can anyone please give the mic",
    "start": "1625679",
    "end": "1632480"
  },
  {
    "text": "think you can go to the mic in the center I think there is a mic in the Isel questions please go to the aisle",
    "start": "1632480",
    "end": "1637960"
  },
  {
    "text": "mic and use the microphone go to the Isel",
    "start": "1637960",
    "end": "1642240"
  },
  {
    "text": "yeah um may ask a question about the um you",
    "start": "1646520",
    "end": "1653760"
  },
  {
    "text": "mentioned you optimize the UI low right the what sorry um the Aro City UI low",
    "start": "1653760",
    "end": "1660399"
  },
  {
    "text": "cuz Okay UI as we have like uh more than 10,000 applications the U load is quite",
    "start": "1660399",
    "end": "1666840"
  },
  {
    "text": "slow yes I'd like to know have you done any um like customer patch on Argo City",
    "start": "1666840",
    "end": "1673559"
  },
  {
    "text": "to solve the questions or just use some like gz okay the answer is uh have we",
    "start": "1673559",
    "end": "1679519"
  },
  {
    "text": "done anything custom to optimize the UI load of ago that's I think we are we",
    "start": "1679519",
    "end": "1685039"
  },
  {
    "text": "have um we encounter same problem but um yeah our situation may worsen than you",
    "start": "1685039",
    "end": "1692919"
  },
  {
    "text": "guys um I'm I'm from Tik Tok and yeah the the the presentation is",
    "start": "1692919",
    "end": "1700159"
  },
  {
    "text": "shared by my previous leader and uh Equity uh c c um yeah our our question",
    "start": "1700159",
    "end": "1707679"
  },
  {
    "text": "are more tough um for example our UI loading time uh is like 3 minutes 3",
    "start": "1707679",
    "end": "1714960"
  },
  {
    "text": "minutes for the first um for for first when we the the initial list for example",
    "start": "1714960",
    "end": "1722720"
  },
  {
    "text": "we when we need to Le uh load the UI the initial list is cost like maybe 20",
    "start": "1722720",
    "end": "1729159"
  },
  {
    "text": "seconds and the UI load is almost about 3 minutes and after the caching you know",
    "start": "1729159",
    "end": "1735799"
  },
  {
    "text": "they will cash all the rback result results after caching the load is like",
    "start": "1735799",
    "end": "1741200"
  },
  {
    "text": "maybe 20 seconds and the UI load is still uh more than 1 minute um I like",
    "start": "1741200",
    "end": "1746640"
  },
  {
    "text": "I'd like to know have you guys done any improvements on that oh so the answer is we haven't done anything custom we",
    "start": "1746640",
    "end": "1752919"
  },
  {
    "text": "relied on the gzip compression and then uh because we so for us we don't",
    "start": "1752919",
    "end": "1760440"
  },
  {
    "text": "necessarily allow the users to open the homepage entirely because we uh we we",
    "start": "1760440",
    "end": "1766320"
  },
  {
    "text": "have a UI portal Dev portal in our platform that links to uh the Argo CD",
    "start": "1766320",
    "end": "1772039"
  },
  {
    "text": "applications that the each team owns So with with this uh linking right we don't",
    "start": "1772039",
    "end": "1779840"
  },
  {
    "text": "necessarily link give users redirection to the homepage to the SL homepage so",
    "start": "1779840",
    "end": "1785080"
  },
  {
    "text": "it's like we can control the filters oh I from our platform and let's say the filters we filtered based on teams or B",
    "start": "1785080",
    "end": "1792120"
  },
  {
    "text": "on certain app group so we we don't allow like 10,000 objects in that hom",
    "start": "1792120",
    "end": "1799440"
  },
  {
    "text": "page that that's how we we do it but we don't do any any in our cases our users just bookmark our links ah true yeah we",
    "start": "1799440",
    "end": "1807039"
  },
  {
    "text": "we' seen that also that behavior also um for we're over on time already so are we",
    "start": "1807039",
    "end": "1813519"
  },
  {
    "text": "continue the questions offline okay have to move to the next session thank you so",
    "start": "1813519",
    "end": "1819039"
  },
  {
    "text": "much",
    "start": "1819279",
    "end": "1822279"
  }
]