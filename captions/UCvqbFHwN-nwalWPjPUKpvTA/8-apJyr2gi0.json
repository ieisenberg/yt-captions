[
  {
    "start": "0",
    "end": "26000"
  },
  {
    "text": "hello everyone thank you for coming to the session especially on a Friday afternoon after there's amazing coop con",
    "start": "30",
    "end": "6390"
  },
  {
    "text": "I hope you had a good time I certainly did I'm Michael working for VMware and today's session is about",
    "start": "6390",
    "end": "13230"
  },
  {
    "text": "kubernetes resource management mainly from like the mechanics behind kubernetes resource management as well",
    "start": "13230",
    "end": "19800"
  },
  {
    "text": "as some of the lessons learned and best practices that I learned from working with customers in the field before we",
    "start": "19800",
    "end": "27930"
  },
  {
    "start": "26000",
    "end": "78000"
  },
  {
    "text": "kick off I just wanted to get a feeling of the audience developers in the room please raise your hands what it's that's",
    "start": "27930",
    "end": "35760"
  },
  {
    "text": "a--that's almost 50% good operators Zuri's architects this group of people it's a",
    "start": "35760",
    "end": "44370"
  },
  {
    "text": "little bit more ok nice good so who who has heard about kubernetes resource",
    "start": "44370",
    "end": "50309"
  },
  {
    "text": "management the primitives just whose family I was this topic that's that's",
    "start": "50309",
    "end": "56579"
  },
  {
    "text": "less people okay I was using it in production it's more people ok",
    "start": "56579",
    "end": "65269"
  },
  {
    "text": "and who thinks who thinks it's easy to use and easy to get right some hints",
    "start": "66380",
    "end": "72110"
  },
  {
    "text": "there okay III don't think it's easy to use so glad you re in this session two",
    "start": "72110",
    "end": "79250"
  },
  {
    "text": "years ago Tim Harkin gave a great presentation on resource management the kind of the reason why you would want to",
    "start": "79250",
    "end": "85970"
  },
  {
    "text": "do resource management in your cluster but also the complexities associated with was this topic mainly from his",
    "start": "85970",
    "end": "93530"
  },
  {
    "text": "experience of running a bore work or or working with work but also the",
    "start": "93530",
    "end": "99170"
  },
  {
    "text": "primitives that kubernetes give us when it comes to resource management and also the gaps between kubernetes and Borg",
    "start": "99170",
    "end": "105619"
  },
  {
    "text": "where kubernetes was falling short by that time and I thought I'd stand on the",
    "start": "105619",
    "end": "111200"
  },
  {
    "text": "shoulders of this giant or giants and build on his presentation and see how far we've come since then and what we've",
    "start": "111200",
    "end": "118490"
  },
  {
    "text": "learned as well as the best practices that I typically use in the field with customers but also keep in mind that",
    "start": "118490",
    "end": "125450"
  },
  {
    "text": "this topic and what we're gonna cover here in 30 minutes is just a small thing",
    "start": "125450",
    "end": "132709"
  },
  {
    "text": "in a bit much bigger picture of resource management mainly what we can cover here is capacity planning tooling processes",
    "start": "132709",
    "end": "140450"
  },
  {
    "text": "change management which all play into this huge topic of resource management",
    "start": "140450",
    "end": "145569"
  },
  {
    "start": "145000",
    "end": "159000"
  },
  {
    "text": "the agenda is or will be like a short introduction then we'll cover kubernetes",
    "start": "145569",
    "end": "150769"
  },
  {
    "text": "primitives as well as how they are implemented and then at the end of the presentation we'll have best practices",
    "start": "150769",
    "end": "156859"
  },
  {
    "text": "and hopefully time for Q&A so this is",
    "start": "156859",
    "end": "162109"
  },
  {
    "start": "159000",
    "end": "250000"
  },
  {
    "text": "the and I'm sorry if it's not readable in the back but I just wanted to show you this is the kubernetes engine X",
    "start": "162109",
    "end": "168109"
  },
  {
    "text": "ingress spec deployments back and if since this is a talk on quality of",
    "start": "168109",
    "end": "173630"
  },
  {
    "text": "service and resource management can someone spot what's missing in this",
    "start": "173630",
    "end": "178720"
  },
  {
    "text": "deployment file just shout it out loud if you want great thank you very much",
    "start": "178720",
    "end": "184609"
  },
  {
    "text": "there's something missing in here so what if you deploy the engineer's",
    "start": "184609",
    "end": "189829"
  },
  {
    "text": "controller with this kind of manifest what what could happen if you send it to the API server during admission this",
    "start": "189829",
    "end": "196579"
  },
  {
    "text": "deployment or part at the end of the day could be rejected because that might be the resource for the",
    "start": "196579",
    "end": "201620"
  },
  {
    "text": "admission controller be activated it could be modified by the limit Ranger another admission plug-in and the API",
    "start": "201620",
    "end": "208400"
  },
  {
    "text": "server both of them we'll cover later if it made it through and it's running this",
    "start": "208400",
    "end": "214609"
  },
  {
    "text": "part might not get enough resources because it didn't ask for any resources so it might be starved",
    "start": "214609",
    "end": "221000"
  },
  {
    "text": "it could affect other parts running on the same cubelet so it become a noisy neighbor for example of its bursty it",
    "start": "221000",
    "end": "227870"
  },
  {
    "text": "could be affected by the cube evicted by the cube let when the cubelet comes under pressure or it could be killed by",
    "start": "227870",
    "end": "234829"
  },
  {
    "text": "the Linux kernel as a last line of defense so basically this pod won't give you any kind of predictable runtime",
    "start": "234829",
    "end": "242030"
  },
  {
    "text": "behavior but depending on the workload that you want to run so not necessarily the nginx but maybe batch jobs that",
    "start": "242030",
    "end": "248780"
  },
  {
    "text": "might be fine so you might ask like well I'm running kubernetes in production",
    "start": "248780",
    "end": "254810"
  },
  {
    "start": "250000",
    "end": "257000"
  },
  {
    "text": "everything is fine so what who cares so I found these user stories pretty",
    "start": "254810",
    "end": "260269"
  },
  {
    "start": "257000",
    "end": "303000"
  },
  {
    "text": "interesting and if the guys from munzo and the lando are in there here in the",
    "start": "260269",
    "end": "265550"
  },
  {
    "text": "audience big shout out to these guys because I learned a lot from those issues that they posted and they're some",
    "start": "265550",
    "end": "272090"
  },
  {
    "text": "of the pros modems or outages that they reported and they were pretty interesting and lessons learned from",
    "start": "272090",
    "end": "277699"
  },
  {
    "text": "they're not necessary by not applying kubernetes resource management perimeters but by for example by some",
    "start": "277699",
    "end": "284180"
  },
  {
    "text": "defaults that are in the like implementation detail which just didn't work for for these for these people all",
    "start": "284180",
    "end": "291949"
  },
  {
    "text": "the links by the way are in the paintings of this presentation it's already online so just have a look at",
    "start": "291949",
    "end": "297949"
  },
  {
    "text": "the appendix so some operating system basics first before we deep dig into kubernetes if",
    "start": "297949",
    "end": "304130"
  },
  {
    "start": "303000",
    "end": "330000"
  },
  {
    "text": "you run a container say you talk around hello declined the docker client sends a",
    "start": "304130",
    "end": "310159"
  },
  {
    "text": "create command to the docker engine and this happens in userspace and after some",
    "start": "310159",
    "end": "315650"
  },
  {
    "text": "initialization in the docker engine eventually a Cisco X act will be called for the entry point or command specify",
    "start": "315650",
    "end": "322280"
  },
  {
    "text": "on it in the in the docker file what happens then is we switch in the kernel",
    "start": "322280",
    "end": "327440"
  },
  {
    "text": "mode and I left out some some of the details here but we switch in the kernel mode and now the kernel takes over and",
    "start": "327440",
    "end": "332960"
  },
  {
    "start": "330000",
    "end": "443000"
  },
  {
    "text": "creates toss struct for this command and by narrated that we want to run so",
    "start": "332960",
    "end": "338419"
  },
  {
    "text": "basically the colonel doesn't really know or has this notion of thread and processes what he builds in memory is at",
    "start": "338419",
    "end": "344960"
  },
  {
    "text": "a struct which has some properties and I just sketched out summary which is C groups in namespaces some of you you",
    "start": "344960",
    "end": "351710"
  },
  {
    "text": "have might have heard already and then it needs to find a resource to run our",
    "start": "351710",
    "end": "357249"
  },
  {
    "text": "container or our binary so it kicks in the scheduler the scheduler kicks in of the Linux kernel it finds the CPU",
    "start": "357249",
    "end": "364969"
  },
  {
    "text": "hopefully and if it found the CPU its kind of schedules the process or the the",
    "start": "364969",
    "end": "370129"
  },
  {
    "text": "task struct on the CPU and eventually we'll go running and print hello world so Linux kernel is just concerned mainly",
    "start": "370129",
    "end": "378050"
  },
  {
    "text": "around a task rack here is because I'll get a lot of question of ok this is the process and this is this right so what",
    "start": "378050",
    "end": "384619"
  },
  {
    "text": "are we dealing was basically we're dealing with tossed trucks and also we are not dealing with containers in the sense of the Linux kernel the Linux",
    "start": "384619",
    "end": "390949"
  },
  {
    "text": "kernel as of now doesn't really have a notion of container it's just a set of properties in the task struct so that",
    "start": "390949",
    "end": "397219"
  },
  {
    "text": "might sound easy right now as a concrete example imagine we have one CPU and we want to run two",
    "start": "397219",
    "end": "404089"
  },
  {
    "text": "containers which are pretty CPU bound suppose want to keep keep two CPU busy what the Linux kernel in the default",
    "start": "404089",
    "end": "412149"
  },
  {
    "text": "scheduling algorithm which is a completely fair scheduler tries to do is",
    "start": "412149",
    "end": "417409"
  },
  {
    "text": "it just wants to be fair to all the task drugs it it's asked to run so it puts in",
    "start": "417409",
    "end": "424039"
  },
  {
    "text": "a weight share so to say which you can find here and by default that's 1024 if",
    "start": "424039",
    "end": "429589"
  },
  {
    "text": "the shares are equal that's basically assign for the color to give both the same amount of CPU time now that's easy",
    "start": "429589",
    "end": "436999"
  },
  {
    "text": "if just our containers want to run each a toss track or just consider a construct of this one task right but",
    "start": "436999",
    "end": "444019"
  },
  {
    "start": "443000",
    "end": "507000"
  },
  {
    "text": "what if our container array is a little bit smaller and spin small threats and user lingo basically meaning that you it",
    "start": "444019",
    "end": "450889"
  },
  {
    "text": "creates more task trucks now the Linux kernel if we don't have a groovy magnet mechanism it tries to be fair to all the",
    "start": "450889",
    "end": "457909"
  },
  {
    "text": "task tracks and now certainly we don't we're not very fair between the two processes because container II was the",
    "start": "457909",
    "end": "464479"
  },
  {
    "text": "more task drugs gets more CPU time the end of the day so 10 years ago",
    "start": "464479",
    "end": "469569"
  },
  {
    "text": "Google contributed cgroups to the Linux kernel and basically what that means it",
    "start": "469569",
    "end": "474729"
  },
  {
    "text": "kind of implemented the grouping mechanism around these da structs that now at a group level you can define your",
    "start": "474729",
    "end": "481029"
  },
  {
    "text": "weights or shares that specify the amount of CPU time those groups get and",
    "start": "481029",
    "end": "489009"
  },
  {
    "text": "we kind of back into fairness see groups also can be used for accounting and other types of resource management so",
    "start": "489009",
    "end": "495879"
  },
  {
    "text": "it's not always a mechanism to or needed for resource management you can also",
    "start": "495879",
    "end": "501069"
  },
  {
    "text": "just use it for some statistics and apply some statistics coverage is there",
    "start": "501069",
    "end": "507580"
  },
  {
    "start": "507000",
    "end": "553000"
  },
  {
    "text": "but mainly we use C groups for a CPU and memory and IO control where you can",
    "start": "507580",
    "end": "513909"
  },
  {
    "text": "specify priorities or limits on certain resources for example also on P IDs and",
    "start": "513909",
    "end": "519789"
  },
  {
    "text": "you can do accounting so it's a pretty sophisticated thing that we have in a",
    "start": "519789",
    "end": "525970"
  },
  {
    "text": "Linux kernel scheduler to work with resources usually it's mounted in",
    "start": "525970",
    "end": "531579"
  },
  {
    "text": "disease FS so the hierarchy and the interface that we can interact from userland is mounted in disease FS and",
    "start": "531579",
    "end": "537339"
  },
  {
    "text": "there's two versions that exist in Linux kernel version 2 has just been finalized and one of the reasons Linux kernel",
    "start": "537339",
    "end": "544240"
  },
  {
    "text": "versions as far as I know most of the container runtimes still use version 1",
    "start": "544240",
    "end": "550570"
  },
  {
    "text": "of this secret interface now if you want",
    "start": "550570",
    "end": "556029"
  },
  {
    "start": "553000",
    "end": "597000"
  },
  {
    "text": "to run a container just to switch more on - kano world now you got those flags like - - CPU and - memory for example",
    "start": "556029",
    "end": "563560"
  },
  {
    "text": "here we want to run busybox and execute top and busybox and we say just give me one CPU and turnip",
    "start": "563560",
    "end": "570370"
  },
  {
    "text": "megabytes of memory what ends up happening is that top in the busybox",
    "start": "570370",
    "end": "576160"
  },
  {
    "text": "container still sees all of our CPU which are in anode and it sees one point",
    "start": "576160",
    "end": "582130"
  },
  {
    "text": "ago you could get a gigabyte of memory instead of the memory that we specified so first thing to note here is that C",
    "start": "582130",
    "end": "588760"
  },
  {
    "text": "groups are not named spaced so the containers is kind of - all the resources on the host and not from a",
    "start": "588760",
    "end": "594490"
  },
  {
    "text": "from its container isolation perspective and second thing to note here is and I'm",
    "start": "594490",
    "end": "599949"
  },
  {
    "start": "597000",
    "end": "611000"
  },
  {
    "text": "going to read it for those people in the back us since the container is kind of grouped in this",
    "start": "599949",
    "end": "605850"
  },
  {
    "text": "secret hierarchy which is sis of SC group CPU docker in this case and to get",
    "start": "605850",
    "end": "612130"
  },
  {
    "text": "some properties that tell the Linux kernel how to regulate resources on this",
    "start": "612130",
    "end": "617350"
  },
  {
    "text": "container in this case it gets a CPU share of 1024 that's the default value but it gets some limits applied mainly a",
    "start": "617350",
    "end": "624670"
  },
  {
    "text": "CFS period and quota which is some kernel internals that mean that everyone",
    "start": "624670",
    "end": "629830"
  },
  {
    "text": "had millisecond evaluate the quota of this container and if it's beyond this",
    "start": "629830",
    "end": "636279"
  },
  {
    "text": "quota just take it off and and don't give it more time now coming from a VM",
    "start": "636279",
    "end": "641950"
  },
  {
    "text": "centric world where we run one app VM we had this reservations typically that we",
    "start": "641950",
    "end": "647140"
  },
  {
    "text": "have in the VN like give me four CPUs and forget about the memory here in this case it's kind of the inverse we specify",
    "start": "647140",
    "end": "653080"
  },
  {
    "text": "some hard limits some hard caps on the on our process on our container which",
    "start": "653080",
    "end": "658150"
  },
  {
    "text": "you might get but imagine you have just one CPU and you're on ten of these containers the - - one CPU doesn't give",
    "start": "658150",
    "end": "664960"
  },
  {
    "text": "you one CPU it means basically you at best time your maximum get one CPU but",
    "start": "664960",
    "end": "670300"
  },
  {
    "text": "you could get less because if you have more processes running everyone gets a fair share also from memory here it will",
    "start": "670300",
    "end": "677350"
  },
  {
    "text": "specify a limit so it's not a guarantee it's actually a limit that's been applied was the - flag and so this is",
    "start": "677350",
    "end": "682750"
  },
  {
    "text": "some of the misunderstandings that are typically seen in the field when people start with containers that they come",
    "start": "682750",
    "end": "687970"
  },
  {
    "text": "from this VM reservation model now they are into the kernel model use those flags but actually the flags are kind of",
    "start": "687970",
    "end": "694300"
  },
  {
    "text": "the inverse what they expect also keep in mind that we don't specify gigahertz",
    "start": "694300",
    "end": "700060"
  },
  {
    "text": "or anything else here we do specify as a kind of a time a quote our period which",
    "start": "700060",
    "end": "706810"
  },
  {
    "text": "could vary between your infrastructure say for example you run this container on your local machine was this flag that",
    "start": "706810",
    "end": "712750"
  },
  {
    "text": "could be 1.8 gigahertz machine or on the cloud could be one point whatever machine so it's not really a megahertz",
    "start": "712750",
    "end": "720339"
  },
  {
    "text": "or gigahertz that you specify here and the hard limit that's what we already covered just to recap this introduction",
    "start": "720339",
    "end": "728230"
  },
  {
    "text": "here from the Linux kernel point of view containers are just normal processes or task tracks to be precise here",
    "start": "728230",
    "end": "734710"
  },
  {
    "text": "the Linux kernel default algorithm is completely fair so the motels tracks are running at recipe fab beyond them or",
    "start": "734710",
    "end": "740380"
  },
  {
    "text": "amongst them and you can influence this behavior by tuning those shares continues you see groups and namespaces",
    "start": "740380",
    "end": "747610"
  },
  {
    "start": "745000",
    "end": "761000"
  },
  {
    "text": "for prioritization and isolation on the host but for example for C groups there's not always isolation as we've seen with the",
    "start": "747610",
    "end": "754540"
  },
  {
    "text": "busybox another biggest question is okay I got this for containers on my machine but how do we do that at cluster scale and",
    "start": "754540",
    "end": "761800"
  },
  {
    "start": "761000",
    "end": "772000"
  },
  {
    "text": "this is where kubernetes quality of service comes comes into game I made this slide for 1.10 I just changed them",
    "start": "761800",
    "end": "768370"
  },
  {
    "text": "for one to ten and I hope I have covered everything and the reason changes there so in kubernetes an equivalence class",
    "start": "768370",
    "end": "774339"
  },
  {
    "start": "772000",
    "end": "830000"
  },
  {
    "text": "that typically what we want to achieve is third level of isolation between ten ends or different namespaces or teams we",
    "start": "774339",
    "end": "781029"
  },
  {
    "text": "want to have prioritization we want to apply some quotas so that some team cannot use more than for example eight",
    "start": "781029",
    "end": "786220"
  },
  {
    "text": "CPUs we want to be fair between them but also we want to increase efficiency in the cluster that example of one team is",
    "start": "786220",
    "end": "792130"
  },
  {
    "text": "not using its resources the other one can make use of the idle resources so there's a lot of things that we want to",
    "start": "792130",
    "end": "797950"
  },
  {
    "text": "achieve in a kubernetes cluster and those are things that might not always work together nicely also keep in mind",
    "start": "797950",
    "end": "804339"
  },
  {
    "text": "all the premise that primitives that we're going to cover it next are just at the cluster boundary so if you have",
    "start": "804339",
    "end": "810490"
  },
  {
    "text": "multiple clusters you have to think about how to make this kind of resource management work across cluster",
    "start": "810490",
    "end": "816070"
  },
  {
    "text": "boundaries and if you run in a cloud or in a hypervisor typically that's what the underlying infrastructure does for",
    "start": "816070",
    "end": "821500"
  },
  {
    "text": "you if you run on bare metal you have to be aware of kind of not creating silos by running multiple clusters because",
    "start": "821500",
    "end": "827050"
  },
  {
    "text": "there's not yet there's kind of global resource sharing between clusters so the next slides will cover kind of quality",
    "start": "827050",
    "end": "834220"
  },
  {
    "start": "830000",
    "end": "863000"
  },
  {
    "text": "of service lifecycle the different stages that your application travels in",
    "start": "834220",
    "end": "839920"
  },
  {
    "text": "during its lifecycle first of course the pod specification where we can specify our requests our resource requests or",
    "start": "839920",
    "end": "847209"
  },
  {
    "text": "guarantees that we expect from from kubernetes then we go into the Cuban Ares master",
    "start": "847209",
    "end": "852610"
  },
  {
    "text": "which does admission and then scheduling will cover that and in the end we're going to look at the node where actually",
    "start": "852610",
    "end": "858459"
  },
  {
    "text": "the enforcement happens when your container or process runs on the Q plot",
    "start": "858459",
    "end": "864180"
  },
  {
    "start": "863000",
    "end": "876000"
  },
  {
    "text": "on the right hand side you can obviously kind of in which phase view are currently in community stays stable",
    "start": "864180",
    "end": "871300"
  },
  {
    "text": "resources for specifying resource requirements and those are CPU and memory we've also got some beta",
    "start": "871300",
    "end": "877720"
  },
  {
    "start": "876000",
    "end": "912000"
  },
  {
    "text": "resources which is used huge pages ephemeral storage and device plugins and yesterday there was a great talk by",
    "start": "877720",
    "end": "883029"
  },
  {
    "text": "Louise from Intel and she covered a lot of the very details and performance tuning aspects of some advanced",
    "start": "883029",
    "end": "888699"
  },
  {
    "text": "techniques like new mask railing NUMA tuning huge pitches etc was a great",
    "start": "888699",
    "end": "893740"
  },
  {
    "text": "talk I also have some custom resources for example you want to have some some",
    "start": "893740",
    "end": "898749"
  },
  {
    "text": "very specific companies specific resources that only you you have those are extended resources that you can use",
    "start": "898749",
    "end": "904569"
  },
  {
    "text": "in a cluster for example for licenses or o'donnell's so you can can extend on the",
    "start": "904569",
    "end": "910119"
  },
  {
    "text": "resource properties for each resource in kubernetes in the spec we have requests",
    "start": "910119",
    "end": "916600"
  },
  {
    "start": "912000",
    "end": "1022000"
  },
  {
    "text": "and limits I usually see a requests are kind of guarantees I'm gonna expect from from kubernetes and the limits are the",
    "start": "916600",
    "end": "921910"
  },
  {
    "text": "hot cap that my process should not get over or cross over there was a specified",
    "start": "921910",
    "end": "928360"
  },
  {
    "text": "at the container level so they are not specified at a pod level but at a container level and currently only CPU memoranda ephemeral storage allow for",
    "start": "928360",
    "end": "935829"
  },
  {
    "text": "over-commitment meaning that you can specify different requests and limits you can send certain the same which you",
    "start": "935829",
    "end": "942189"
  },
  {
    "text": "have to for example for GPUs which doesn't allow for overcommit but you also can say I just want one CPU as a",
    "start": "942189",
    "end": "948459"
  },
  {
    "text": "request for my pod and maybe four CPUs as a limit so you cannot converse in between them those fields are read-only",
    "start": "948459",
    "end": "955059"
  },
  {
    "text": "so if you want to change them you have to recreate the part of the deployment so it's controls for example all the",
    "start": "955059",
    "end": "961540"
  },
  {
    "text": "things that you just didn't found here in the spec CPU memory things that you didn't found can be may be set by sis",
    "start": "961540",
    "end": "967959"
  },
  {
    "text": "control so this link has a link a list of all the things that you can set from the SIS control perspective either per",
    "start": "967959",
    "end": "974559"
  },
  {
    "text": "container or on the host and then there's always the question like how can I set bandwidth guarantees or thresholds",
    "start": "974559",
    "end": "981009"
  },
  {
    "text": "or limits for example for disk and network and that turned out to be not so easy and there's also discussion on",
    "start": "981009",
    "end": "987399"
  },
  {
    "text": "whether it will eventually this will ever make it into kubernetes so this link has a design guide a discussion on",
    "start": "987399",
    "end": "993879"
  },
  {
    "text": "why this is currently not implemented as a resource here just as an example we",
    "start": "993879",
    "end": "999129"
  },
  {
    "text": "have an engine expert that does specify limits in this case keep in mind if you",
    "start": "999129",
    "end": "1004800"
  },
  {
    "text": "only specify limits kubernetes sets request equals limit so you that's just a shortcut and not so you don't have to",
    "start": "1004800",
    "end": "1011490"
  },
  {
    "text": "specify a request in this case our container will get we ask for two CPUs and two megabytes of memory meaning as a",
    "start": "1011490",
    "end": "1018720"
  },
  {
    "text": "request but also as a as a limit in this in this specification okay we got our",
    "start": "1018720",
    "end": "1024168"
  },
  {
    "start": "1022000",
    "end": "1077000"
  },
  {
    "text": "specification we have our pot yellow file now we're going to submit it to the API server the API server has different",
    "start": "1024169",
    "end": "1033390"
  },
  {
    "text": "admission controls that you might be aware of and to add mission controls that we are concerned here are the",
    "start": "1033390",
    "end": "1038579"
  },
  {
    "text": "resource quota at mission controller and the limit arrangement which we cover later our source quota admission",
    "start": "1038579",
    "end": "1044819"
  },
  {
    "text": "controller allows you as a cluster operator to set a certain quota on a namespace which names based resource",
    "start": "1044819",
    "end": "1050850"
  },
  {
    "text": "right now that you can say this team this namespace should never ever get more than 8 GPUs or 20 pods or whatever",
    "start": "1050850",
    "end": "1057929"
  },
  {
    "text": "but you can also set some requests as a upper bound of boundary then if you if",
    "start": "1057929",
    "end": "1064980"
  },
  {
    "text": "you don't want to enforce your users to specify requests and limits or if you don't trust them you as a cluster",
    "start": "1064980",
    "end": "1070110"
  },
  {
    "text": "operator can set some defaults or and for some some limits and requests with",
    "start": "1070110",
    "end": "1075570"
  },
  {
    "text": "the limit ranger and the API server keep in mind when you do those sizing and",
    "start": "1075570",
    "end": "1080640"
  },
  {
    "text": "calculations or specifications as a cluster operator keep in mind that there's currently no namespace resource",
    "start": "1080640",
    "end": "1086370"
  },
  {
    "text": "sharing that for example if you set a namespace on eight CPU and the other one on four CPU but the for CPU namespace is",
    "start": "1086370",
    "end": "1093390"
  },
  {
    "text": "kind of on its limit and the other one is idle it cannot currently make use of the idle resources in another namespace",
    "start": "1093390",
    "end": "1099990"
  },
  {
    "text": "and there's a community project I'm going to mention later which tries to resolve this so be it be aware of how to",
    "start": "1099990",
    "end": "1105929"
  },
  {
    "text": "size the quota on the namespaces now we go into the scheduling phase the",
    "start": "1105929",
    "end": "1111960"
  },
  {
    "start": "1108000",
    "end": "1140000"
  },
  {
    "text": "scheduler looks at all the nodes in your cluster and kind of builds and note info cache and memory so it has it knows",
    "start": "1111960",
    "end": "1118679"
  },
  {
    "text": "about each node and in its memory but it looks only at the capacity like what",
    "start": "1118679",
    "end": "1125309"
  },
  {
    "text": "does this node have for example if you have a VM versus 4 CPUs it looks at those properties and not actual usage",
    "start": "1125309",
    "end": "1131690"
  },
  {
    "text": "but also keep in mind that the scheduler does not use the know capacity those four CPUs for example but",
    "start": "1131690",
    "end": "1138210"
  },
  {
    "text": "it uses something called allocatable what is allocatable allocatable is the",
    "start": "1138210",
    "end": "1143390"
  },
  {
    "start": "1140000",
    "end": "1169000"
  },
  {
    "text": "at the end of the date is sommore the rest of everything that the note has minus some reservations that you can",
    "start": "1143390",
    "end": "1149760"
  },
  {
    "text": "apply minus some eviction threshold which we'll cover later so allocated will typically is lower",
    "start": "1149760",
    "end": "1155160"
  },
  {
    "text": "than the whole note capacity and this caused some confusion for some customers like I do have capacity in my cluster",
    "start": "1155160",
    "end": "1161520"
  },
  {
    "text": "but schedulers nots gathering it it's just because the scheduler uses allocatable and not default note",
    "start": "1161520",
    "end": "1167010"
  },
  {
    "text": "capacity during scheduling the scheduler has some credit cards and priority",
    "start": "1167010",
    "end": "1173430"
  },
  {
    "start": "1169000",
    "end": "1229000"
  },
  {
    "text": "ranking we don't have time to look into the scheduling algorithm there was a talk yesterday in six scheduling which covered this in detail but what a",
    "start": "1173430",
    "end": "1179940"
  },
  {
    "text": "scheduler does it looks at the resource requests of the incoming part doesn't come is not concerned about limits right",
    "start": "1179940",
    "end": "1185820"
  },
  {
    "text": "now it just looks at the requests coming in and it looks at the allocatable resources that it tracks in memory for",
    "start": "1185820",
    "end": "1192060"
  },
  {
    "text": "each node and it starts while scheduling it would never over commit a node based on the request so if you have a pot with",
    "start": "1192060",
    "end": "1198480"
  },
  {
    "text": "four CPUs coming in but there's no allocatable left for four CPUs it will not schedule it on there that's",
    "start": "1198480",
    "end": "1203820"
  },
  {
    "text": "admission admission check there's something that you have to keep in mind",
    "start": "1203820",
    "end": "1209520"
  },
  {
    "text": "daemon sets currently do not use the Linux and the kubernetes schedule the schedule so they have to kind of their",
    "start": "1209520",
    "end": "1215460"
  },
  {
    "text": "own scheduling mechanism which could lead to race conditions or some unintended behavior there's a feature",
    "start": "1215460",
    "end": "1222210"
  },
  {
    "text": "Alphin 1.10 which tries to fix it and the goal is to make the daemon sets also being scheduled by the learner by the",
    "start": "1222210",
    "end": "1227790"
  },
  {
    "text": "kubernetes scheduler so now on the node which is where the magic actually",
    "start": "1227790",
    "end": "1234180"
  },
  {
    "start": "1229000",
    "end": "1243000"
  },
  {
    "text": "happens we are on the couplet now see groups are used on the cubelet to enforce most of what we've covered so",
    "start": "1234180",
    "end": "1240360"
  },
  {
    "text": "far around CPU and memory I created this diagram because I found",
    "start": "1240360",
    "end": "1246390"
  },
  {
    "start": "1243000",
    "end": "1327000"
  },
  {
    "text": "it very hard to figure out what CPU requests or memory requests and limits",
    "start": "1246390",
    "end": "1251580"
  },
  {
    "text": "map to actually on the node I think this is there some room in the documentation to improve this so on the kind of the",
    "start": "1251580",
    "end": "1259110"
  },
  {
    "text": "upper layer is your pot manifest which specifies CPU requests and limits also for memory and internally they map on",
    "start": "1259110",
    "end": "1265350"
  },
  {
    "text": "the nodes on the cubelet the couplet maps those two CPU shares for West and CPU quota and period for the",
    "start": "1265350",
    "end": "1271980"
  },
  {
    "text": "limits for the memory it's a little bit different because the Linux kernel has no memory kind of request or a guarantee",
    "start": "1271980",
    "end": "1278399"
  },
  {
    "text": "in there because it's also a shared resource so requests map to something called out of memories go adjust or",
    "start": "1278399",
    "end": "1284820"
  },
  {
    "text": "cover that later and limits map to memory limits actually now one thing to point out here is that if you run on a",
    "start": "1284820",
    "end": "1292799"
  },
  {
    "text": "cloud provider or on a on a hypervisor platform also be in mind or taken taken",
    "start": "1292799",
    "end": "1298110"
  },
  {
    "text": "to account that your underlying hypervisor has to be aligned or has to actually provide those guarantees for",
    "start": "1298110",
    "end": "1303570"
  },
  {
    "text": "example if you have a Linux OS with four CPUs you also want to have the hypervisor provide the resources to this",
    "start": "1303570",
    "end": "1309029"
  },
  {
    "text": "VM and not being over committed or contended or at least be aware of this for example if you run multiple cloud",
    "start": "1309029",
    "end": "1315659"
  },
  {
    "text": "instances that at some point of time your CPU will just be kept and this lelouch kernel is just not aware of this",
    "start": "1315659",
    "end": "1322470"
  },
  {
    "text": "because it happens under on the right and so you could run into performance performance issues so here we run an",
    "start": "1322470",
    "end": "1329009"
  },
  {
    "start": "1327000",
    "end": "1414000"
  },
  {
    "text": "engine X container with limits CPU 1 and memory 200 megabytes and we just want to",
    "start": "1329009",
    "end": "1334529"
  },
  {
    "text": "dive into how this looks how the couplet translates this into the actual actual secrets the cubelet does adjust CPU",
    "start": "1334529",
    "end": "1344340"
  },
  {
    "text": "shares in this case actually it's 1024 but if you would have specified something else here the couplet does",
    "start": "1344340",
    "end": "1349889"
  },
  {
    "text": "some adjustment on the CPU shares and for the CFS period and quoted also",
    "start": "1349889",
    "end": "1355200"
  },
  {
    "text": "applies the limits in this case so again here we get we are hot kept at one CPU this part won't ever get more than one",
    "start": "1355200",
    "end": "1362309"
  },
  {
    "text": "CPU for memory these are also applied as a upper binary in this case to have",
    "start": "1362309",
    "end": "1368009"
  },
  {
    "text": "megabytes so again here we are applying capacity limits to our port and as a",
    "start": "1368009",
    "end": "1374759"
  },
  {
    "text": "guarantees of guarantee we just have to use shares in this case now it can be a",
    "start": "1374759",
    "end": "1381059"
  },
  {
    "text": "little bit complicated to work with this requests and limits and figure out which kind of pot has which resource guarantees or which one has a higher",
    "start": "1381059",
    "end": "1388230"
  },
  {
    "text": "priority or lower priority so very early queue quality of service classes were incorporated into kubernetes that make",
    "start": "1388230",
    "end": "1396149"
  },
  {
    "text": "it a little bit easier or from an end user perspective to see which kind of resource guarantees or",
    "start": "1396149",
    "end": "1401940"
  },
  {
    "text": "classes I I get but also for the couplet to decide which part has a higher",
    "start": "1401940",
    "end": "1407340"
  },
  {
    "text": "priority or not those classes are derived from CPU and memory resource pacification in the in the pods back and",
    "start": "1407340",
    "end": "1415130"
  },
  {
    "start": "1414000",
    "end": "1423000"
  },
  {
    "text": "just to give you some examples if you don't specify anything for because limits for a container you are a",
    "start": "1415130",
    "end": "1421320"
  },
  {
    "text": "best-effort pot classified best effort if you specified some you are a burst of",
    "start": "1421320",
    "end": "1426570"
  },
  {
    "start": "1423000",
    "end": "1435000"
  },
  {
    "text": "all pot and if you specify all request limits for CPU and memory for all containers in this pot then you are",
    "start": "1426570",
    "end": "1433170"
  },
  {
    "text": "considered to be a guaranteed pot now there's some tricky magic happening",
    "start": "1433170",
    "end": "1439500"
  },
  {
    "start": "1435000",
    "end": "1476000"
  },
  {
    "text": "inside which we'll cover in a second first of all the couplet builds a hierarchy on locally on the node which",
    "start": "1439500",
    "end": "1447150"
  },
  {
    "text": "you can see here on the right meaning that for example our guaranteed pot will be living directly in the root hierarchy",
    "start": "1447150",
    "end": "1453450"
  },
  {
    "text": "of cube pots so the higher you derive hierarchy you typically have a higher resource guarantee or resource",
    "start": "1453450",
    "end": "1460680"
  },
  {
    "text": "yeah guarantees from the cubelet and then it builds a sub hierarchy for best effort and worst of all which it",
    "start": "1460680",
    "end": "1465990"
  },
  {
    "text": "continuously adjusts values in there so the couplet does a lot of more magic than I'm showing here and I think on the",
    "start": "1465990",
    "end": "1472200"
  },
  {
    "text": "next slide there's also a link of how the cubelet does this magic under the cover so now for since we talked about",
    "start": "1472200",
    "end": "1480140"
  },
  {
    "start": "1476000",
    "end": "1553000"
  },
  {
    "text": "resources of a pot with requests will never be over committed what if they actually do over commit you specify",
    "start": "1480140",
    "end": "1486420"
  },
  {
    "text": "requests and limits and you actually roll out pots on on a node that has not so many resources that you specified in",
    "start": "1486420",
    "end": "1493440"
  },
  {
    "text": "the limits you have to be aware of that there's kind of two types of resources compressible resources and",
    "start": "1493440",
    "end": "1498540"
  },
  {
    "text": "uncompressible resources CPU as a compressible resource meaning we can throttle it in case of there's",
    "start": "1498540",
    "end": "1503820"
  },
  {
    "text": "contention memory and storage we cannot shrink it or compact it so that's",
    "start": "1503820",
    "end": "1508920"
  },
  {
    "text": "uncompressible and we have to have to find another way to make space in case of there's contention on the couplet so",
    "start": "1508920",
    "end": "1515880"
  },
  {
    "text": "for CPU throttling the the kind of the value that's taking encountered CPU shares the more shares you have and the",
    "start": "1515880",
    "end": "1522690"
  },
  {
    "text": "CPU has to be throttled the more time you had at the end getting through in this even during the earth rattling but",
    "start": "1522690",
    "end": "1529710"
  },
  {
    "text": "for memory and storage the cubelet has to evict start evicting make space on the on the",
    "start": "1529710",
    "end": "1535110"
  },
  {
    "text": "and as eviction logic applied here if the cupid cannot evict fast enough because there's kind of I think 10 sec",
    "start": "1535110",
    "end": "1540539"
  },
  {
    "text": "10 second polling evolved in here currently the Linux kernel will just kick in and out of memory kill as a last",
    "start": "1540539",
    "end": "1546269"
  },
  {
    "text": "line of defense which is kind of not kind of synced this the cubelet so the Linux kernel could just kick in and do",
    "start": "1546269",
    "end": "1554070"
  },
  {
    "start": "1553000",
    "end": "1576000"
  },
  {
    "text": "its job now the eviction threshold can be specified the cubelet has some flex that you can use for eviction threshold so",
    "start": "1554070",
    "end": "1560070"
  },
  {
    "text": "you actually have some control for the eviction eviction behavior and also the",
    "start": "1560070",
    "end": "1565320"
  },
  {
    "text": "queue blood when under pressure will signal to the API server that it's under pressure the scheduler will take this into account and not schedule hopefully",
    "start": "1565320",
    "end": "1571649"
  },
  {
    "text": "not schedule pots on the dis note because that would make the situation even worse now before kubernetes 1.9",
    "start": "1571649",
    "end": "1579299"
  },
  {
    "start": "1576000",
    "end": "1663000"
  },
  {
    "text": "those QoS classes that we just learned post a bill best effort and guarantee they had an effect on the eviction order",
    "start": "1579299",
    "end": "1585299"
  },
  {
    "text": "when the cubelet had to make decisions so it was that the quality of service",
    "start": "1585299",
    "end": "1591029"
  },
  {
    "text": "best effort was likely to be evicted first and then most able and guaranteed as the last line of defense that was",
    "start": "1591029",
    "end": "1597149"
  },
  {
    "text": "pretty pretty easy to understand now that changed in 1.9 where actually",
    "start": "1597149",
    "end": "1602539"
  },
  {
    "text": "there's the question the cupid asks which part has the highest usage above its request typically that would be",
    "start": "1602539",
    "end": "1610009"
  },
  {
    "text": "possibly at the staff report but it could also be a burstable part so the QoS classes here kind of are not really",
    "start": "1610009",
    "end": "1616230"
  },
  {
    "text": "involved anymore then it looks for pot priority if you don't have pot priority enabled in your",
    "start": "1616230",
    "end": "1621419"
  },
  {
    "text": "cluster and you it will just skip it and then as a last line it just you a usage",
    "start": "1621419",
    "end": "1626850"
  },
  {
    "text": "miners request and the pots that remain are then ranked so the eviction logic has changed slightly and it could also",
    "start": "1626850",
    "end": "1632850"
  },
  {
    "text": "mean that guaranteed pot will be evicted because for example you have a system demon or system process on the node which is kind of struggling and then the",
    "start": "1632850",
    "end": "1640139"
  },
  {
    "text": "queue blood will make space for Mike by killing even guaranteed pots now keep in",
    "start": "1640139",
    "end": "1645419"
  },
  {
    "text": "mind daemon sets are also pots the Kuebler doesn't understand that this is a demon set for example it just sees a",
    "start": "1645419",
    "end": "1650580"
  },
  {
    "text": "pot make sure that for demon sets you also apply at least some sunburst uber or guaranteed resources because",
    "start": "1650580",
    "end": "1657149"
  },
  {
    "text": "otherwise you could end up in killing B being evicted your demon set which could be your network plug-in for example",
    "start": "1657149",
    "end": "1664100"
  },
  {
    "text": "there's much more going on that we can cover here on the covers during this 30min I mean so I made this slide just as a",
    "start": "1664100",
    "end": "1671629"
  },
  {
    "text": "reference it's all it's all in the deck so that could be helpful for troubleshooting if there's something",
    "start": "1671629",
    "end": "1677539"
  },
  {
    "text": "that you have to log in to also the community has seen that the some of the parameters that there are in kubernetes",
    "start": "1677539",
    "end": "1683899"
  },
  {
    "start": "1678000",
    "end": "1741000"
  },
  {
    "text": "for resource management might not be sufficient so the created project or extensions like for example the",
    "start": "1683899",
    "end": "1689899"
  },
  {
    "text": "kubernetes arbitrator which allows for namespace sharing that's pretty interesting the CPU manager which kind",
    "start": "1689899",
    "end": "1695809"
  },
  {
    "text": "of flips from those shares which are just weights which are relative weights into CPU pinning so some of you might be",
    "start": "1695809",
    "end": "1701869"
  },
  {
    "text": "familiar with CP opening the soup you CPU manager will allow for CPU pinning then you actually get a CPU if you ask",
    "start": "1701869",
    "end": "1707869"
  },
  {
    "text": "for a CPU for CPU dedicated what priority is something that I was really happy to see coming into the cube",
    "start": "1707869",
    "end": "1714109"
  },
  {
    "text": "scheduler which allows for adding priority feels to your workloads to your",
    "start": "1714109",
    "end": "1719179"
  },
  {
    "text": "pots saying there's a system critical and so you have an easy way to specify your kind of priority so that the that",
    "start": "1719179",
    "end": "1725690"
  },
  {
    "text": "will also be taking into account for example during eviction and the vertical pot autoscaler is actually starting to",
    "start": "1725690",
    "end": "1731029"
  },
  {
    "text": "look at the metrics to use resource utilization in your cluster and trying to find the right requests and limits so",
    "start": "1731029",
    "end": "1737239"
  },
  {
    "text": "you don't have to always watch and specify them manually exciting projects so some best practices from the field",
    "start": "1737239",
    "end": "1744109"
  },
  {
    "start": "1741000",
    "end": "1767000"
  },
  {
    "text": "before we wrap up if in doubt if you don't use quality of servers or there was the primitives in",
    "start": "1744109",
    "end": "1751279"
  },
  {
    "text": "kubernetes yet just start with with guaranteed resource classes guaranteed",
    "start": "1751279",
    "end": "1756350"
  },
  {
    "text": "won't allow for over-commitment so you might have end up in having a pet cluster utilization if you don't get the",
    "start": "1756350",
    "end": "1762379"
  },
  {
    "text": "values right but it gives you a safety net to learn and and and tune the system also enable cloak quotas and enforce",
    "start": "1762379",
    "end": "1770600"
  },
  {
    "start": "1767000",
    "end": "1782000"
  },
  {
    "text": "defaults in a cluster this sounds easy but I've seen so many clusters not really enforcing quarters or at least",
    "start": "1770600",
    "end": "1777289"
  },
  {
    "text": "not enforcing write defaults which ends up happening a bad experience for free workloads also protect a critical system",
    "start": "1777289",
    "end": "1785029"
  },
  {
    "start": "1782000",
    "end": "1810000"
  },
  {
    "text": "parts we covered us for daemon sets also keep in mind that as long as the cube letters on the control of your workload",
    "start": "1785029",
    "end": "1791059"
  },
  {
    "text": "that's fine but you also have may have other jobs running on your on your nodes which are outside of control of the",
    "start": "1791059",
    "end": "1796970"
  },
  {
    "text": "cubelet for example a backup agent a plugin that's not managed by cubelet also make sure that photo",
    "start": "1796970",
    "end": "1802790"
  },
  {
    "text": "processes in the linux kernel you have resources apply to which can be done",
    "start": "1802790",
    "end": "1808580"
  },
  {
    "text": "with C groups and then if you specify quality of service guarantees or",
    "start": "1808580",
    "end": "1816410"
  },
  {
    "start": "1810000",
    "end": "1853000"
  },
  {
    "text": "requests in your and your pod manifest make sure you also kind of have them early in the CCD pipeline not just in",
    "start": "1816410",
    "end": "1823160"
  },
  {
    "text": "production when you roll out because this would help catching some wrong fields or entries that you have very",
    "start": "1823160",
    "end": "1829370"
  },
  {
    "text": "early during this stage also do some stress testing benchmarking to find the right values because if it ends up",
    "start": "1829370",
    "end": "1835580"
  },
  {
    "text": "happening in production that's actually bad Tim in this presentation spoke about",
    "start": "1835580",
    "end": "1840800"
  },
  {
    "text": "the bork auto pilot which is a very sophisticated and maybe one of the best engineering things I've ever heard of",
    "start": "1840800",
    "end": "1846860"
  },
  {
    "text": "which does this kind of adjustment and learning and pecking the cluster increasing high increasing utilization",
    "start": "1846860",
    "end": "1854140"
  },
  {
    "text": "also we cover this when we talked about the underlying infrastructure hypervisors brewster blue cloud",
    "start": "1854140",
    "end": "1860060"
  },
  {
    "text": "instances that make sure the whole stack is aligned not just the pot and the kubernetes but also the Linux operating",
    "start": "1860060",
    "end": "1865550"
  },
  {
    "text": "system or Windows operating system and underlying infrastructure if you run on a hypervisor under cloud monitoring is a",
    "start": "1865550",
    "end": "1872090"
  },
  {
    "start": "1870000",
    "end": "1896000"
  },
  {
    "text": "very important aspect and not just for CPU memory but also the Linux kernel is a shared resource some of the primitives",
    "start": "1872090",
    "end": "1878720"
  },
  {
    "text": "we do have in pot and the pots back can be applied but there are some things like open file descriptors to TCP",
    "start": "1878720",
    "end": "1884390"
  },
  {
    "text": "connections pits other other stuff that at least you should monitor because you cannot",
    "start": "1884390",
    "end": "1889520"
  },
  {
    "text": "express them in the pod specification so you at least have to monitor them in order to avoid that you run into issues",
    "start": "1889520",
    "end": "1895070"
  },
  {
    "text": "there now remember the busy box top example that we had where busy box was",
    "start": "1895070",
    "end": "1901400"
  },
  {
    "start": "1896000",
    "end": "1917000"
  },
  {
    "text": "limited to one CPU but it's all all the CPUs right now this is a big issue for most of the programming languages",
    "start": "1901400",
    "end": "1907520"
  },
  {
    "text": "laughing somebody Java Runtime the go runtime dotnet Java used to get better if York shot from metal spheres in the",
    "start": "1907520",
    "end": "1914420"
  },
  {
    "text": "room he wrote a great blog post on this I think it's even linked in there so for example for Java you would seen detune",
    "start": "1914420",
    "end": "1920690"
  },
  {
    "text": "the heap size garbage collection threats forego you would even go max procs based on the limits that you specify I've seen",
    "start": "1920690",
    "end": "1927610"
  },
  {
    "text": "environments with go max procs not being aligned to the limits in the pot spec",
    "start": "1927610",
    "end": "1932990"
  },
  {
    "text": "which could end up having bad performance or like monster long GC pauses also for dotnet there's",
    "start": "1932990",
    "end": "1940240"
  },
  {
    "text": "the GC tuning and other stuff that you applied all the links in there have more details that I can cover here and at the",
    "start": "1940240",
    "end": "1946929"
  },
  {
    "text": "end of the not aligning those values could end up at best and just low performance but at the adverse crashes",
    "start": "1946929",
    "end": "1953410"
  },
  {
    "text": "of view of your system you can use the download API in kubernetes and other",
    "start": "1953410",
    "end": "1959170"
  },
  {
    "start": "1954000",
    "end": "1966000"
  },
  {
    "text": "tools to make this more automated so you don't have to do it manually there's different ways of doing it download API",
    "start": "1959170",
    "end": "1965260"
  },
  {
    "text": "is something that I typically use some advanced tuning for the cubelet you can you know you should you tune at least",
    "start": "1965260",
    "end": "1971950"
  },
  {
    "start": "1966000",
    "end": "2006000"
  },
  {
    "text": "the eviction thresholds hard soft eviction thresholds swept typically is enforced to be not enabled in your and",
    "start": "1971950",
    "end": "1977950"
  },
  {
    "text": "your on your note because the cubelet has it just has a hard time and in identifying what's fab is or giving you",
    "start": "1977950",
    "end": "1985540"
  },
  {
    "text": "a good resource guarantees because web is the disk resource and on the memories results that's why it's web is enforced to be off on the node and also apply",
    "start": "1985540",
    "end": "1992410"
  },
  {
    "text": "Cupra soft and cube or system reserved that's just reducing the amount of allocatable resources on node and making",
    "start": "1992410",
    "end": "1998650"
  },
  {
    "text": "some buffer for system critical demons like for example the couplet and the container runtime and other not",
    "start": "1998650",
    "end": "2003750"
  },
  {
    "text": "parts on our port workloads for a system or latency critical workloads use the",
    "start": "2003750",
    "end": "2009330"
  },
  {
    "start": "2006000",
    "end": "2038000"
  },
  {
    "text": "cpu manager or as we covered yesterday in your talk during resource management",
    "start": "2009330",
    "end": "2014880"
  },
  {
    "text": "there's much more like Numa optimization that you can apply to for latency critical workloads or telco networking",
    "start": "2014880",
    "end": "2020910"
  },
  {
    "text": "and use the best of all quality of service clouds if you are more advanced without CPU limits because then you can",
    "start": "2020910",
    "end": "2027480"
  },
  {
    "text": "make use of spare idle CPU resources on the host and there's some interesting discussions going on with the solando",
    "start": "2027480",
    "end": "2033390"
  },
  {
    "text": "guys on how to even tune this unity behavior that we currently have and",
    "start": "2033390",
    "end": "2038880"
  },
  {
    "start": "2038000",
    "end": "2058000"
  },
  {
    "text": "lastly before we close the kernel has a shared resource make sure you understand that Windows is not Linux there's",
    "start": "2038880",
    "end": "2045450"
  },
  {
    "text": "differences the link has all that kind of the way the internals of how Windows implements resource management from a",
    "start": "2045450",
    "end": "2051750"
  },
  {
    "text": "cubelet perspective and yeah those are just some lessons learned here I want to",
    "start": "2051750",
    "end": "2056850"
  },
  {
    "text": "leave some time for questions now which may be one one a minute thank you very much sorry for rushing through slides",
    "start": "2056850",
    "end": "2062908"
  },
  {
    "start": "2058000",
    "end": "2276000"
  },
  {
    "text": "are online",
    "start": "2062909",
    "end": "2065179"
  },
  {
    "text": "I think we have four questions yes please yeah go ahead",
    "start": "2068760",
    "end": "2073860"
  },
  {
    "text": "I repeat just shot yeah so the question",
    "start": "2073860",
    "end": "2089490"
  },
  {
    "text": "was this customers running kubernetes on vm / / / provisioning on the vmware side and now over provisioning in kubernetes",
    "start": "2089490",
    "end": "2095310"
  },
  {
    "text": "this is a good thing or not yes and no please come see me afterwards we have done some best practices in lime",
    "start": "2095310",
    "end": "2101640"
  },
  {
    "text": "and just see me afterwards there was another question there yes please",
    "start": "2101640",
    "end": "2106160"
  },
  {
    "text": "well prometheus for example you could use this CEO dick sorry the question was some of the workloads that you run could",
    "start": "2114900",
    "end": "2121920"
  },
  {
    "text": "be starved because you don't specify requests or just minimal requests in different environments how to how to",
    "start": "2121920",
    "end": "2127050"
  },
  {
    "text": "monitor this I talked to Timo maybe he's in the room at the same issue you could",
    "start": "2127050",
    "end": "2132390"
  },
  {
    "text": "use SI group statistics that to another beat pretty interesting you sinner okay come come see me",
    "start": "2132390",
    "end": "2140130"
  },
  {
    "text": "afterwards team FG moves in the room we figure it out yes please",
    "start": "2140130",
    "end": "2145609"
  },
  {
    "text": "yeah okay so the question was since you run into this fixed default quota",
    "start": "2183130",
    "end": "2188810"
  },
  {
    "text": "enforcement of 100 millisecond which for burstable workloads could have neck or negative affect your workload is there",
    "start": "2188810",
    "end": "2195380"
  },
  {
    "text": "something to mitigators so mon-sol created like their own implementation or they kind of tuned kubernetes in there",
    "start": "2195380",
    "end": "2203000"
  },
  {
    "text": "that they allow for changing of this value chip that have reduced it right now it's not like in core upstream so",
    "start": "2203000",
    "end": "2209270"
  },
  {
    "text": "you cannot change this and an alternative would be the cpu manager unfortunately dead static so it's kind",
    "start": "2209270",
    "end": "2215480"
  },
  {
    "text": "of isolating your CPUs so the other parts won't be able to to use them I think if Louise from Intel is in the",
    "start": "2215480",
    "end": "2221900"
  },
  {
    "text": "room their CPU manager implementation was aa solando guys nice you're right",
    "start": "2221900",
    "end": "2234619"
  },
  {
    "text": "yeah there was which recommended yesterday that totally disabling quota so even if you specify limits the",
    "start": "2234619",
    "end": "2241190"
  },
  {
    "text": "cubelet will just not translate that and you will end up not having limits applied on the system now that could be",
    "start": "2241190",
    "end": "2247850"
  },
  {
    "text": "that could work but it could also mean that if you don't have enough resource requests underneath that there's too",
    "start": "2247850",
    "end": "2252950"
  },
  {
    "text": "much pressure on the node and you end up not not having good resources so it's it's currently unsolved and there's a",
    "start": "2252950",
    "end": "2259790"
  },
  {
    "text": "lot of discussion going on there's in this presentation there was the link to the solando and the discussion with",
    "start": "2259790",
    "end": "2265880"
  },
  {
    "text": "bridge and the other guys she's champion make your thoughts and comments please thank you just see me",
    "start": "2265880",
    "end": "2273680"
  },
  {
    "text": "afterwards if there's more questions please thank you",
    "start": "2273680",
    "end": "2277720"
  }
]