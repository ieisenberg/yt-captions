[
  {
    "text": "okay cool so this talk will be about",
    "start": "480",
    "end": "3880"
  },
  {
    "text": "optimizing model serving inference on a",
    "start": "3880",
    "end": "8080"
  },
  {
    "text": "Kubernetes",
    "start": "8080",
    "end": "9840"
  },
  {
    "text": "um with model weight",
    "start": "9840",
    "end": "13719"
  },
  {
    "text": "streaming um so",
    "start": "13719",
    "end": "16920"
  },
  {
    "text": "I'm happy to be here um I was I'm from",
    "start": "16920",
    "end": "21840"
  },
  {
    "text": "the Ranai team i was I was the",
    "start": "21840",
    "end": "24480"
  },
  {
    "text": "co-founder and CTO of Ranai we started",
    "start": "24480",
    "end": "26720"
  },
  {
    "text": "in 2018 um RAI we're doing what we call",
    "start": "26720",
    "end": "31279"
  },
  {
    "text": "AI infrastructure",
    "start": "31279",
    "end": "33160"
  },
  {
    "text": "orchestration runai was acquired by",
    "start": "33160",
    "end": "36079"
  },
  {
    "text": "Nvidia last year and now we're part of",
    "start": "36079",
    "end": "38960"
  },
  {
    "text": "the Nvidia team before that I did my PhD",
    "start": "38960",
    "end": "42079"
  },
  {
    "text": "and and postto in information theory and",
    "start": "42079",
    "end": "44960"
  },
  {
    "text": "Aken is here with me yes so I am the",
    "start": "44960",
    "end": "48160"
  },
  {
    "text": "developer advocate from the runai team",
    "start": "48160",
    "end": "50239"
  },
  {
    "text": "now in media uh I also did my masters in",
    "start": "50239",
    "end": "54480"
  },
  {
    "text": "robotics cognition intelligence uh in",
    "start": "54480",
    "end": "57399"
  },
  {
    "text": "Munich uh so today we are going to talk",
    "start": "57399",
    "end": "60079"
  },
  {
    "text": "about uh model inference yes so model",
    "start": "60079",
    "end": "63520"
  },
  {
    "text": "inference and and we got this uh um rani",
    "start": "63520",
    "end": "67360"
  },
  {
    "text": "model weight streamer it's an open",
    "start": "67360",
    "end": "69680"
  },
  {
    "text": "source project that we published last",
    "start": "69680",
    "end": "72320"
  },
  {
    "text": "year and this is the work of Noah and",
    "start": "72320",
    "end": "74880"
  },
  {
    "text": "Omar who couldn't be here but we're",
    "start": "74880",
    "end": "77119"
  },
  {
    "text": "presenting their work so great work by",
    "start": "77119",
    "end": "79080"
  },
  {
    "text": "them um so let's start so inference in",
    "start": "79080",
    "end": "82080"
  },
  {
    "text": "theory right there is a training data",
    "start": "82080",
    "end": "84000"
  },
  {
    "text": "set and a model is being trained on that",
    "start": "84000",
    "end": "86479"
  },
  {
    "text": "data set it's being evaluated after the",
    "start": "86479",
    "end": "89520"
  },
  {
    "text": "training and if results are good then it",
    "start": "89520",
    "end": "91439"
  },
  {
    "text": "it moves to inference right and",
    "start": "91439",
    "end": "94320"
  },
  {
    "text": "inference new data or users queries are",
    "start": "94320",
    "end": "98079"
  },
  {
    "text": "getting into the model being processed",
    "start": "98079",
    "end": "100479"
  },
  {
    "text": "and new predictions or",
    "start": "100479",
    "end": "103560"
  },
  {
    "text": "generated right and this is what's the",
    "start": "103560",
    "end": "106479"
  },
  {
    "text": "the the topic of this talk right about",
    "start": "106479",
    "end": "109200"
  },
  {
    "text": "inference how how models can serve new",
    "start": "109200",
    "end": "113280"
  },
  {
    "text": "requests in a",
    "start": "113280",
    "end": "115560"
  },
  {
    "text": "cloudnative space on on GPUs",
    "start": "115560",
    "end": "119280"
  },
  {
    "text": "so in traditional applications with web",
    "start": "119280",
    "end": "121439"
  },
  {
    "text": "web applications common practice is to",
    "start": "121439",
    "end": "124159"
  },
  {
    "text": "use autoscalers right and there are",
    "start": "124159",
    "end": "126240"
  },
  {
    "text": "users queries getting in and there are",
    "start": "126240",
    "end": "128479"
  },
  {
    "text": "instances replicas of of of applications",
    "start": "128479",
    "end": "132319"
  },
  {
    "text": "running on maybe on in a container on a",
    "start": "132319",
    "end": "134640"
  },
  {
    "text": "CPU and when traffic goes up new uh",
    "start": "134640",
    "end": "138800"
  },
  {
    "text": "replicas are spawn up right um to serve",
    "start": "138800",
    "end": "142720"
  },
  {
    "text": "those requests and when traffic goes",
    "start": "142720",
    "end": "144800"
  },
  {
    "text": "down those replicas are being spawned",
    "start": "144800",
    "end": "147040"
  },
  {
    "text": "down right to save costs",
    "start": "147040",
    "end": "149920"
  },
  {
    "text": "So that's with traditional applications",
    "start": "149920",
    "end": "151920"
  },
  {
    "text": "but with AI it's much much more",
    "start": "151920",
    "end": "153800"
  },
  {
    "text": "difficult uh because of few reasons but",
    "start": "153800",
    "end": "156959"
  },
  {
    "text": "one of the biggest reasons is the cold",
    "start": "156959",
    "end": "158640"
  },
  {
    "text": "start problem what we call the cold",
    "start": "158640",
    "end": "160239"
  },
  {
    "text": "start problem it takes a long time to",
    "start": "160239",
    "end": "162400"
  },
  {
    "text": "spin up a new replica right so first of",
    "start": "162400",
    "end": "165840"
  },
  {
    "text": "all the process of spinning up a new",
    "start": "165840",
    "end": "168000"
  },
  {
    "text": "replica it involves provisioning a new",
    "start": "168000",
    "end": "170160"
  },
  {
    "text": "machine right but with GPU machines it",
    "start": "170160",
    "end": "172800"
  },
  {
    "text": "takes longer than provisioning a CPU",
    "start": "172800",
    "end": "175200"
  },
  {
    "text": "machine usually because there are less",
    "start": "175200",
    "end": "177599"
  },
  {
    "text": "GPUs out there in the cloud and because",
    "start": "177599",
    "end": "180080"
  },
  {
    "text": "there are software libraries like CUDA",
    "start": "180080",
    "end": "182159"
  },
  {
    "text": "drivers and and and and CUDA kernels and",
    "start": "182159",
    "end": "185599"
  },
  {
    "text": "libraries that that wait a lot and it",
    "start": "185599",
    "end": "187440"
  },
  {
    "text": "takes time to install them second thing",
    "start": "187440",
    "end": "189200"
  },
  {
    "text": "is the container image just loading the",
    "start": "189200",
    "end": "191280"
  },
  {
    "text": "container image usually within in AI",
    "start": "191280",
    "end": "193519"
  },
  {
    "text": "those images are big it involves a lot",
    "start": "193519",
    "end": "195760"
  },
  {
    "text": "of Python",
    "start": "195760",
    "end": "197159"
  },
  {
    "text": "libraries booting up the inference",
    "start": "197159",
    "end": "199280"
  },
  {
    "text": "engine usually also takes not a",
    "start": "199280",
    "end": "201760"
  },
  {
    "text": "considerable time but this talk is going",
    "start": "201760",
    "end": "203840"
  },
  {
    "text": "to be about the last part about loading",
    "start": "203840",
    "end": "206159"
  },
  {
    "text": "the model weights right just downloading",
    "start": "206159",
    "end": "209440"
  },
  {
    "text": "the model weights from a storage",
    "start": "209440",
    "end": "210879"
  },
  {
    "text": "location into the GPU and it can take a",
    "start": "210879",
    "end": "214879"
  },
  {
    "text": "long time right it can be really",
    "start": "214879",
    "end": "217879"
  },
  {
    "text": "significant for example with llama 3 8",
    "start": "217879",
    "end": "220879"
  },
  {
    "text": "billion parameters usually it's being",
    "start": "220879",
    "end": "223360"
  },
  {
    "text": "it's being deployed with 16 bits per",
    "start": "223360",
    "end": "227040"
  },
  {
    "text": "weight and then the weights are around",
    "start": "227040",
    "end": "230239"
  },
  {
    "text": "15 gigabytes so that's a lot right uh",
    "start": "230239",
    "end": "233760"
  },
  {
    "text": "llama 3 with 70 billion parameters it",
    "start": "233760",
    "end": "236560"
  },
  {
    "text": "will be more than 100 gigabyte just the",
    "start": "236560",
    "end": "239040"
  },
  {
    "text": "weights themselves to download from a",
    "start": "239040",
    "end": "240640"
  },
  {
    "text": "storage and and load into a GPU deepseek",
    "start": "240640",
    "end": "244080"
  },
  {
    "text": "R1 right more than one terabyte so",
    "start": "244080",
    "end": "246640"
  },
  {
    "text": "that's huge that's huge and it can take",
    "start": "246640",
    "end": "249120"
  },
  {
    "text": "many minutes can take sometimes more",
    "start": "249120",
    "end": "251519"
  },
  {
    "text": "than 10 minutes just to download the the",
    "start": "251519",
    "end": "253599"
  },
  {
    "text": "weights and load into the GPU so the our",
    "start": "253599",
    "end": "256639"
  },
  {
    "text": "goal is to right to accelerate that",
    "start": "256639",
    "end": "258239"
  },
  {
    "text": "process and why is it that important",
    "start": "258239",
    "end": "260160"
  },
  {
    "text": "just a few slides about that so a few",
    "start": "260160",
    "end": "262400"
  },
  {
    "text": "use cases is for inference one one is",
    "start": "262400",
    "end": "264800"
  },
  {
    "text": "real time inference where there is one",
    "start": "264800",
    "end": "267360"
  },
  {
    "text": "model and a high load right many users",
    "start": "267360",
    "end": "271120"
  },
  {
    "text": "quering just one model and maybe there",
    "start": "271120",
    "end": "273680"
  },
  {
    "text": "are multiple replicas running on a GPU",
    "start": "273680",
    "end": "277080"
  },
  {
    "text": "and as we said right we want to scale up",
    "start": "277080",
    "end": "280400"
  },
  {
    "text": "replicas when traffic goes up and scale",
    "start": "280400",
    "end": "282400"
  },
  {
    "text": "them down right usually those GPUs will",
    "start": "282400",
    "end": "285120"
  },
  {
    "text": "be available it's like a whole story if",
    "start": "285120",
    "end": "287520"
  },
  {
    "text": "like provisioning new GPUs or not",
    "start": "287520",
    "end": "290000"
  },
  {
    "text": "sometimes it's not it's not possible to",
    "start": "290000",
    "end": "291840"
  },
  {
    "text": "spin down GPU GPUs but let's assume that",
    "start": "291840",
    "end": "293840"
  },
  {
    "text": "those GPUs are out there and and if they",
    "start": "293840",
    "end": "297680"
  },
  {
    "text": "are not used to those to to run those",
    "start": "297680",
    "end": "299840"
  },
  {
    "text": "replicas they are used for some other",
    "start": "299840",
    "end": "301840"
  },
  {
    "text": "workloads but spinning up and down those",
    "start": "301840",
    "end": "304479"
  },
  {
    "text": "replicas can take a long time and users",
    "start": "304479",
    "end": "308080"
  },
  {
    "text": "won't wait for a few minutes until their",
    "start": "308080",
    "end": "309919"
  },
  {
    "text": "queries are being answered so what we",
    "start": "309919",
    "end": "312080"
  },
  {
    "text": "see usually happening is",
    "start": "312080",
    "end": "314120"
  },
  {
    "text": "overprovisioning those GPUs are being",
    "start": "314120",
    "end": "316720"
  },
  {
    "text": "more more GPUs are being provisioned to",
    "start": "316720",
    "end": "319360"
  },
  {
    "text": "run those servers and it means high cost",
    "start": "319360",
    "end": "322080"
  },
  {
    "text": "and low GPU",
    "start": "322080",
    "end": "324520"
  },
  {
    "text": "utilization another use case it's cold",
    "start": "324520",
    "end": "328080"
  },
  {
    "text": "models or like multiple",
    "start": "328080",
    "end": "330280"
  },
  {
    "text": "models that needs to be access needs to",
    "start": "330280",
    "end": "333360"
  },
  {
    "text": "be served to different users but",
    "start": "333360",
    "end": "335199"
  },
  {
    "text": "infrequent load right just infrequently",
    "start": "335199",
    "end": "338000"
  },
  {
    "text": "users are accessing one of those models",
    "start": "338000",
    "end": "340960"
  },
  {
    "text": "so what you will want to do right to",
    "start": "340960",
    "end": "342960"
  },
  {
    "text": "scale to zero use scale to zero and put",
    "start": "342960",
    "end": "345840"
  },
  {
    "text": "all of those models in some storage and",
    "start": "345840",
    "end": "348160"
  },
  {
    "text": "spin a replica up when users are quering",
    "start": "348160",
    "end": "351039"
  },
  {
    "text": "that",
    "start": "351039",
    "end": "351880"
  },
  {
    "text": "model but if that spinning up that",
    "start": "351880",
    "end": "354960"
  },
  {
    "text": "replica takes many minutes then you",
    "start": "354960",
    "end": "356800"
  },
  {
    "text": "won't be able to do that if your",
    "start": "356800",
    "end": "358479"
  },
  {
    "text": "application needs like low latency so",
    "start": "358479",
    "end": "360880"
  },
  {
    "text": "usually what we see is cold models just",
    "start": "360880",
    "end": "363440"
  },
  {
    "text": "being provisioned on many GPUs just to",
    "start": "363440",
    "end": "366319"
  },
  {
    "text": "keep them warm so latencies will be",
    "start": "366319",
    "end": "368720"
  },
  {
    "text": "latency will be low and that's again",
    "start": "368720",
    "end": "372080"
  },
  {
    "text": "high cost low GPU",
    "start": "372080",
    "end": "374759"
  },
  {
    "text": "utilization um last use case for",
    "start": "374759",
    "end": "377639"
  },
  {
    "text": "inference offline inference right",
    "start": "377639",
    "end": "380319"
  },
  {
    "text": "running a batch job that",
    "start": "380319",
    "end": "384199"
  },
  {
    "text": "process a lot of data in a in a batch",
    "start": "384199",
    "end": "387680"
  },
  {
    "text": "fashion right so a job starts it starts",
    "start": "387680",
    "end": "390160"
  },
  {
    "text": "to to process the data outputs outputs",
    "start": "390160",
    "end": "392960"
  },
  {
    "text": "right outputs out there and and when the",
    "start": "392960",
    "end": "395600"
  },
  {
    "text": "job finishes those replicas spawn down",
    "start": "395600",
    "end": "398960"
  },
  {
    "text": "but you're paying for the time that it",
    "start": "398960",
    "end": "401199"
  },
  {
    "text": "takes to spin up those replicas right",
    "start": "401199",
    "end": "403759"
  },
  {
    "text": "you're paying for those GPUs And it can",
    "start": "403759",
    "end": "406080"
  },
  {
    "text": "if it's lasts for several minutes then",
    "start": "406080",
    "end": "409120"
  },
  {
    "text": "it can be significant so reducing that",
    "start": "409120",
    "end": "412400"
  },
  {
    "text": "cold start problem reducing the time",
    "start": "412400",
    "end": "414319"
  },
  {
    "text": "that it takes to load models into your",
    "start": "414319",
    "end": "416840"
  },
  {
    "text": "GPU can be significant and that's the",
    "start": "416840",
    "end": "419440"
  },
  {
    "text": "goal of of this project and AK will",
    "start": "419440",
    "end": "422080"
  },
  {
    "text": "explain more about lo model loading and",
    "start": "422080",
    "end": "424400"
  },
  {
    "text": "what we did with the runi streamer yes",
    "start": "424400",
    "end": "426960"
  },
  {
    "text": "let's let's zoom into those containers",
    "start": "426960",
    "end": "429120"
  },
  {
    "text": "and the model loading process a bit so",
    "start": "429120",
    "end": "432880"
  },
  {
    "text": "we have a storage that can be a local",
    "start": "432880",
    "end": "435440"
  },
  {
    "text": "storage that can be an object uh storage",
    "start": "435440",
    "end": "439120"
  },
  {
    "text": "doesn't matter you have your model there",
    "start": "439120",
    "end": "441680"
  },
  {
    "text": "and first of all you need to move your",
    "start": "441680",
    "end": "444319"
  },
  {
    "text": "model to your uh CPU memory um and in",
    "start": "444319",
    "end": "448240"
  },
  {
    "text": "the CPU memory there are also some",
    "start": "448240",
    "end": "450319"
  },
  {
    "text": "different things that might be happening",
    "start": "450319",
    "end": "451919"
  },
  {
    "text": "like sharding the model quantization etc",
    "start": "451919",
    "end": "454479"
  },
  {
    "text": "that will add up on time as well and",
    "start": "454479",
    "end": "457520"
  },
  {
    "text": "then after you load the um model to the",
    "start": "457520",
    "end": "460880"
  },
  {
    "text": "CPU memory you need to transfer to the",
    "start": "460880",
    "end": "463680"
  },
  {
    "text": "uh to those weights to the GPU memory",
    "start": "463680",
    "end": "466160"
  },
  {
    "text": "which is the second step so these these",
    "start": "466160",
    "end": "469360"
  },
  {
    "text": "steps are happening sequentially there",
    "start": "469360",
    "end": "471440"
  },
  {
    "text": "is no paralleliz parallelization",
    "start": "471440",
    "end": "474639"
  },
  {
    "text": "uh and it takes a lot of time so when we",
    "start": "474639",
    "end": "478080"
  },
  {
    "text": "started checking out the problem uh we",
    "start": "478080",
    "end": "480479"
  },
  {
    "text": "realized that we needed some specific",
    "start": "480479",
    "end": "483039"
  },
  {
    "text": "things uh before creating the the",
    "start": "483039",
    "end": "485520"
  },
  {
    "text": "project so first of all the se",
    "start": "485520",
    "end": "488000"
  },
  {
    "text": "sequential loading will not work for us",
    "start": "488000",
    "end": "490160"
  },
  {
    "text": "so we need something that's working uh",
    "start": "490160",
    "end": "492560"
  },
  {
    "text": "in a parallel manner um second of all we",
    "start": "492560",
    "end": "496879"
  },
  {
    "text": "need a library that supports multiple",
    "start": "496879",
    "end": "500080"
  },
  {
    "text": "storage types uh for example for safe",
    "start": "500080",
    "end": "503599"
  },
  {
    "text": "tanzers live loader um S3 is not uh",
    "start": "503599",
    "end": "507039"
  },
  {
    "text": "supported and you might need to change",
    "start": "507039",
    "end": "508960"
  },
  {
    "text": "your either storage or your uh code base",
    "start": "508960",
    "end": "511599"
  },
  {
    "text": "in order to make your uh storage type",
    "start": "511599",
    "end": "514479"
  },
  {
    "text": "work um so that's something that we",
    "start": "514479",
    "end": "516719"
  },
  {
    "text": "wanted to avoid um third we want to uh",
    "start": "516719",
    "end": "521680"
  },
  {
    "text": "be able to be compatible with safe",
    "start": "521680",
    "end": "524159"
  },
  {
    "text": "tensorers uh safe tensor format for",
    "start": "524159",
    "end": "526560"
  },
  {
    "text": "model weights is becoming the",
    "start": "526560",
    "end": "528600"
  },
  {
    "text": "state-of-the-art and it's very safe um",
    "start": "528600",
    "end": "531279"
  },
  {
    "text": "so we wanted to work with safe tensor",
    "start": "531279",
    "end": "533560"
  },
  {
    "text": "specifically and we want um this project",
    "start": "533560",
    "end": "536800"
  },
  {
    "text": "to be um integrated easily with",
    "start": "536800",
    "end": "539600"
  },
  {
    "text": "different inference engines we don't",
    "start": "539600",
    "end": "541519"
  },
  {
    "text": "want to uh keep pushing a single",
    "start": "541519",
    "end": "544000"
  },
  {
    "text": "inference engine uh that you should use",
    "start": "544000",
    "end": "546560"
  },
  {
    "text": "in order to leverage the project itself",
    "start": "546560",
    "end": "549760"
  },
  {
    "text": "so this brought us to runai model",
    "start": "549760",
    "end": "552600"
  },
  {
    "text": "streamer so uh we created this Python",
    "start": "552600",
    "end": "556640"
  },
  {
    "text": "SDK with a C++ implementation and we",
    "start": "556640",
    "end": "560560"
  },
  {
    "text": "designed it in a way that it accelerates",
    "start": "560560",
    "end": "562959"
  },
  {
    "text": "the uh model loading times onto GPUs",
    "start": "562959",
    "end": "565760"
  },
  {
    "text": "from various types of storage network",
    "start": "565760",
    "end": "568399"
  },
  {
    "text": "file systems S3 disk doesn't matter so",
    "start": "568399",
    "end": "571920"
  },
  {
    "text": "the the two key things that we are doing",
    "start": "571920",
    "end": "574399"
  },
  {
    "text": "here is reading tensors concurrently",
    "start": "574399",
    "end": "576640"
  },
  {
    "text": "from the storage while transferring them",
    "start": "576640",
    "end": "579360"
  },
  {
    "text": "to GPU and we also divide the tensor",
    "start": "579360",
    "end": "582640"
  },
  {
    "text": "into equal parts uh for saturating the",
    "start": "582640",
    "end": "585600"
  },
  {
    "text": "bandwidth while reading because we want",
    "start": "585600",
    "end": "587600"
  },
  {
    "text": "to saturate uh storage bandwidth uh uh",
    "start": "587600",
    "end": "591279"
  },
  {
    "text": "with our",
    "start": "591279",
    "end": "593720"
  },
  {
    "text": "loader um a bit more information about",
    "start": "593720",
    "end": "597519"
  },
  {
    "text": "what is so special about runi model",
    "start": "597519",
    "end": "599600"
  },
  {
    "text": "streamer so as I said we have",
    "start": "599600",
    "end": "601200"
  },
  {
    "text": "concurrency so we use multiple reading",
    "start": "601200",
    "end": "603680"
  },
  {
    "text": "uh requests to read model tensorers",
    "start": "603680",
    "end": "606240"
  },
  {
    "text": "concurrently from storage and we stream",
    "start": "606240",
    "end": "608640"
  },
  {
    "text": "those to the GPU at the same time um we",
    "start": "608640",
    "end": "612160"
  },
  {
    "text": "have some adjustable parameters um so",
    "start": "612160",
    "end": "614880"
  },
  {
    "text": "you can adjust the level of concurrency",
    "start": "614880",
    "end": "617360"
  },
  {
    "text": "depending on your storage type you can",
    "start": "617360",
    "end": "619839"
  },
  {
    "text": "choose um how you want to divide the",
    "start": "619839",
    "end": "622399"
  },
  {
    "text": "safe tensor like the the data chunk size",
    "start": "622399",
    "end": "625279"
  },
  {
    "text": "for each thread um and you can also",
    "start": "625279",
    "end": "628720"
  },
  {
    "text": "define CPU memory usage for example you",
    "start": "628720",
    "end": "631440"
  },
  {
    "text": "might have a limited CPU memory um or",
    "start": "631440",
    "end": "634320"
  },
  {
    "text": "you might have a gigantic CPU memory",
    "start": "634320",
    "end": "636720"
  },
  {
    "text": "that you want to leverage you can",
    "start": "636720",
    "end": "638320"
  },
  {
    "text": "basically adjust those parameters with",
    "start": "638320",
    "end": "640160"
  },
  {
    "text": "runi model streamer and we will use that",
    "start": "640160",
    "end": "643200"
  },
  {
    "text": "um balance workload for reading that's",
    "start": "643200",
    "end": "645600"
  },
  {
    "text": "super important um tensor uh in AI come",
    "start": "645600",
    "end": "649200"
  },
  {
    "text": "in different sizes so we divide those",
    "start": "649200",
    "end": "652000"
  },
  {
    "text": "tensors into equal parts uh for reading",
    "start": "652000",
    "end": "655519"
  },
  {
    "text": "so that we can separate the the storage",
    "start": "655519",
    "end": "658200"
  },
  {
    "text": "bandwidth we support multiple storage",
    "start": "658200",
    "end": "660720"
  },
  {
    "text": "types local file systems cloud-based",
    "start": "660720",
    "end": "663120"
  },
  {
    "text": "object storage uh we support both um you",
    "start": "663120",
    "end": "666800"
  },
  {
    "text": "don't need to uh convert any uh tensor",
    "start": "666800",
    "end": "670079"
  },
  {
    "text": "um uh from safe tensor on and then uh",
    "start": "670079",
    "end": "673920"
  },
  {
    "text": "other way around so we support uh widely",
    "start": "673920",
    "end": "676560"
  },
  {
    "text": "adapted safe tensorers format no need to",
    "start": "676560",
    "end": "679200"
  },
  {
    "text": "convert them and uh um um store them",
    "start": "679200",
    "end": "683160"
  },
  {
    "text": "separately and um we created the runi",
    "start": "683160",
    "end": "687120"
  },
  {
    "text": "model streamer in a way that it is",
    "start": "687120",
    "end": "689839"
  },
  {
    "text": "actually a safe tensor iterator so it's",
    "start": "689839",
    "end": "692560"
  },
  {
    "text": "very similar how safe tensor iterator",
    "start": "692560",
    "end": "695200"
  },
  {
    "text": "the traditional uh version of uh loading",
    "start": "695200",
    "end": "698399"
  },
  {
    "text": "models works um so you can easily",
    "start": "698399",
    "end": "701839"
  },
  {
    "text": "integrate it with uh inference engines",
    "start": "701839",
    "end": "704560"
  },
  {
    "text": "such as VLM TGI etc whatever you are",
    "start": "704560",
    "end": "707680"
  },
  {
    "text": "using and speaking of VLM uh if you are",
    "start": "707680",
    "end": "712000"
  },
  {
    "text": "using a version uh um that's higher than",
    "start": "712000",
    "end": "716200"
  },
  {
    "text": "0.66 we are also coming out of the box",
    "start": "716200",
    "end": "719120"
  },
  {
    "text": "now uh uh with like in the in the VLM",
    "start": "719120",
    "end": "722399"
  },
  {
    "text": "containers and VLM uh versions uh so you",
    "start": "722399",
    "end": "726079"
  },
  {
    "text": "can uh try it out uh with VLM as well um",
    "start": "726079",
    "end": "730959"
  },
  {
    "text": "and now I want to talk a bit about our",
    "start": "730959",
    "end": "733279"
  },
  {
    "text": "benchmarkings like how does it actually",
    "start": "733279",
    "end": "735680"
  },
  {
    "text": "perform in in uh uh in practice right um",
    "start": "735680",
    "end": "740560"
  },
  {
    "text": "so I will not go through the whole",
    "start": "740560",
    "end": "742240"
  },
  {
    "text": "software stack I will leave the QR code",
    "start": "742240",
    "end": "744639"
  },
  {
    "text": "here later on so that you can uh read",
    "start": "744639",
    "end": "746880"
  },
  {
    "text": "the benchmarking white paper uh but we",
    "start": "746880",
    "end": "750560"
  },
  {
    "text": "essentially used a meta model metal lama",
    "start": "750560",
    "end": "753920"
  },
  {
    "text": "uh 8 billion uh it was f 15 gigs and we",
    "start": "753920",
    "end": "757680"
  },
  {
    "text": "stored it in a single safe tanzers file",
    "start": "757680",
    "end": "760079"
  },
  {
    "text": "As hardware we used a single GPU A10G um",
    "start": "760079",
    "end": "763920"
  },
  {
    "text": "on AWS um and uh we chose three",
    "start": "763920",
    "end": "768079"
  },
  {
    "text": "different storage types here um first of",
    "start": "768079",
    "end": "770720"
  },
  {
    "text": "all local SSDs one is GP3 SSD and the",
    "start": "770720",
    "end": "774079"
  },
  {
    "text": "other one is uh IO2 SSD the important",
    "start": "774079",
    "end": "777279"
  },
  {
    "text": "thing that you should know about these",
    "start": "777279",
    "end": "778959"
  },
  {
    "text": "two is IoT SSD has a higher throughput",
    "start": "778959",
    "end": "782160"
  },
  {
    "text": "um and uh we use Amazon S3 uh which is",
    "start": "782160",
    "end": "786480"
  },
  {
    "text": "located in the same region with our um",
    "start": "786480",
    "end": "789519"
  },
  {
    "text": "with our",
    "start": "789519",
    "end": "791160"
  },
  {
    "text": "instance a little sneak peek to the",
    "start": "791160",
    "end": "793600"
  },
  {
    "text": "whole benchmarking experiment uh you can",
    "start": "793600",
    "end": "796000"
  },
  {
    "text": "see here and QR code is here uh feel",
    "start": "796000",
    "end": "798560"
  },
  {
    "text": "free to scan it uh to uh check out the",
    "start": "798560",
    "end": "801959"
  },
  {
    "text": "whole benchmarking study so the two main",
    "start": "801959",
    "end": "806880"
  },
  {
    "text": "things that we checked is standalone",
    "start": "806880",
    "end": "808880"
  },
  {
    "text": "loaders safe tensor loader run model",
    "start": "808880",
    "end": "811760"
  },
  {
    "text": "streamer and tensorizer they how long it",
    "start": "811760",
    "end": "814639"
  },
  {
    "text": "takes for them to load a model from",
    "start": "814639",
    "end": "817200"
  },
  {
    "text": "storage to uh GPU and then we also",
    "start": "817200",
    "end": "820959"
  },
  {
    "text": "checked how much time it takes to boot",
    "start": "820959",
    "end": "823839"
  },
  {
    "text": "the engine and load the model with VLM",
    "start": "823839",
    "end": "826720"
  },
  {
    "text": "so these are the two things that we uh",
    "start": "826720",
    "end": "828560"
  },
  {
    "text": "checked out and as you can see we see",
    "start": "828560",
    "end": "831440"
  },
  {
    "text": "quite some amount uh improvements uh",
    "start": "831440",
    "end": "835040"
  },
  {
    "text": "with runi model streamer um and I want",
    "start": "835040",
    "end": "838880"
  },
  {
    "text": "to share the key takeaways right now",
    "start": "838880",
    "end": "841279"
  },
  {
    "text": "with you that we learned throughout this",
    "start": "841279",
    "end": "843680"
  },
  {
    "text": "uh these experiments so first of all um",
    "start": "843680",
    "end": "848079"
  },
  {
    "text": "conc concurrency drives speed uh up to a",
    "start": "848079",
    "end": "851600"
  },
  {
    "text": "point so concurrency is amazing we",
    "start": "851600",
    "end": "854160"
  },
  {
    "text": "checked out also different levels of",
    "start": "854160",
    "end": "855920"
  },
  {
    "text": "concurrency uh from different storage",
    "start": "855920",
    "end": "858320"
  },
  {
    "text": "types um and we see a lot of improvement",
    "start": "858320",
    "end": "861839"
  },
  {
    "text": "when we increase the concurrency um but",
    "start": "861839",
    "end": "865360"
  },
  {
    "text": "when you saturate your storage bandwidth",
    "start": "865360",
    "end": "868079"
  },
  {
    "text": "you are not going to uh get more uh",
    "start": "868079",
    "end": "871120"
  },
  {
    "text": "improvement because laws of physics you",
    "start": "871120",
    "end": "873760"
  },
  {
    "text": "can't do more um so we we uh realize",
    "start": "873760",
    "end": "877040"
  },
  {
    "text": "that we are hitting the limit after uh",
    "start": "877040",
    "end": "879199"
  },
  {
    "text": "increasing concurrency uh uh",
    "start": "879199",
    "end": "883240"
  },
  {
    "text": "enough um the second key takeaway is as",
    "start": "883240",
    "end": "887279"
  },
  {
    "text": "I said balanced workload distribution is",
    "start": "887279",
    "end": "889360"
  },
  {
    "text": "crucial tensors vary in size um",
    "start": "889360",
    "end": "892480"
  },
  {
    "text": "partitioning work into equal chunks uh",
    "start": "892480",
    "end": "896000"
  },
  {
    "text": "among those threads help um optimizing",
    "start": "896000",
    "end": "898800"
  },
  {
    "text": "bandwidth saturation um and uh making",
    "start": "898800",
    "end": "901920"
  },
  {
    "text": "the streamer process faster because",
    "start": "901920",
    "end": "904000"
  },
  {
    "text": "imagine like one one tensor being",
    "start": "904000",
    "end": "906720"
  },
  {
    "text": "gigabytes and one is megabytes so then",
    "start": "906720",
    "end": "910800"
  },
  {
    "text": "we need to wait for the for the uh",
    "start": "910800",
    "end": "913600"
  },
  {
    "text": "reading process that's happening with",
    "start": "913600",
    "end": "915279"
  },
  {
    "text": "the gigabytes of uh of tensors first um",
    "start": "915279",
    "end": "919600"
  },
  {
    "text": "and the third one is storage bandwidth",
    "start": "919600",
    "end": "922639"
  },
  {
    "text": "it matters a lot um so for deployments",
    "start": "922639",
    "end": "926399"
  },
  {
    "text": "that you are going to uh go for uh if",
    "start": "926399",
    "end": "930000"
  },
  {
    "text": "they demand fast model access think",
    "start": "930000",
    "end": "933360"
  },
  {
    "text": "about investing in high performance",
    "start": "933360",
    "end": "935519"
  },
  {
    "text": "storage um because we realize that it it",
    "start": "935519",
    "end": "938480"
  },
  {
    "text": "matters a lot the bandwidth itself uh if",
    "start": "938480",
    "end": "940959"
  },
  {
    "text": "you are dealing with cold start problem",
    "start": "940959",
    "end": "943199"
  },
  {
    "text": "um it reduces the load times a lot uh",
    "start": "943199",
    "end": "945839"
  },
  {
    "text": "especially in on-prem and hybrid",
    "start": "945839",
    "end": "947600"
  },
  {
    "text": "environments",
    "start": "947600",
    "end": "949600"
  },
  {
    "text": "um and as we uh talked before streamer",
    "start": "949600",
    "end": "953680"
  },
  {
    "text": "has some parameters so you should",
    "start": "953680",
    "end": "956240"
  },
  {
    "text": "actually check out uh the concurrency",
    "start": "956240",
    "end": "959120"
  },
  {
    "text": "like the optimal uh optimal amount of",
    "start": "959120",
    "end": "962240"
  },
  {
    "text": "concurrency that you need specifically",
    "start": "962240",
    "end": "964240"
  },
  {
    "text": "for your storage type and uh also um if",
    "start": "964240",
    "end": "967440"
  },
  {
    "text": "you have uh any specific requirements",
    "start": "967440",
    "end": "969680"
  },
  {
    "text": "for CPU memory etc you should tune the",
    "start": "969680",
    "end": "972320"
  },
  {
    "text": "the uh streamers parameters uh because",
    "start": "972320",
    "end": "975440"
  },
  {
    "text": "it's it affects the um cold start times",
    "start": "975440",
    "end": "978560"
  },
  {
    "text": "quite a lot um and with S3 we we got",
    "start": "978560",
    "end": "983839"
  },
  {
    "text": "very good results um this is because uh",
    "start": "983839",
    "end": "987680"
  },
  {
    "text": "our approach with S uh S3 so creating an",
    "start": "987680",
    "end": "991920"
  },
  {
    "text": "AWS S3 client per thread with each",
    "start": "991920",
    "end": "994959"
  },
  {
    "text": "thread sending uh multiple asynchronous",
    "start": "994959",
    "end": "997759"
  },
  {
    "text": "requests uh to the back end um affects",
    "start": "997759",
    "end": "1001279"
  },
  {
    "text": "the the um performance quite a lot it's",
    "start": "1001279",
    "end": "1004720"
  },
  {
    "text": "it's super powerful we see results uh",
    "start": "1004720",
    "end": "1007920"
  },
  {
    "text": "that's under five seconds um I actually",
    "start": "1007920",
    "end": "1011040"
  },
  {
    "text": "had uh difficulties to put them in the",
    "start": "1011040",
    "end": "1013759"
  },
  {
    "text": "in the uh chart uh in the earlier days",
    "start": "1013759",
    "end": "1017360"
  },
  {
    "text": "so this was quite amazing if you are",
    "start": "1017360",
    "end": "1019839"
  },
  {
    "text": "using S3 give it a",
    "start": "1019839",
    "end": "1021800"
  },
  {
    "text": "go and some things that we noticed in",
    "start": "1021800",
    "end": "1025199"
  },
  {
    "text": "the cloud um which is uh a bit",
    "start": "1025199",
    "end": "1028319"
  },
  {
    "text": "independent of uh RNAi streamer um so we",
    "start": "1028319",
    "end": "1032880"
  },
  {
    "text": "were checking out the um theoretical",
    "start": "1032880",
    "end": "1036400"
  },
  {
    "text": "throughput that we are going to get uh",
    "start": "1036400",
    "end": "1038720"
  },
  {
    "text": "storage bandwidth",
    "start": "1038720",
    "end": "1039839"
  },
  {
    "text": "we are going to get with the um SSDs um",
    "start": "1039839",
    "end": "1043839"
  },
  {
    "text": "even if it's written four gigs per",
    "start": "1043839",
    "end": "1045918"
  },
  {
    "text": "second in the documentation on the uh",
    "start": "1045919",
    "end": "1048558"
  },
  {
    "text": "websites we could only see up to two",
    "start": "1048559",
    "end": "1051440"
  },
  {
    "text": "gigs per second so keep in mind that",
    "start": "1051440",
    "end": "1054640"
  },
  {
    "text": "there might be some practical limits uh",
    "start": "1054640",
    "end": "1057840"
  },
  {
    "text": "that needs a bit",
    "start": "1057840",
    "end": "1059320"
  },
  {
    "text": "planning um and also if you want to run",
    "start": "1059320",
    "end": "1062160"
  },
  {
    "text": "your own benchmarking uh with the",
    "start": "1062160",
    "end": "1064240"
  },
  {
    "text": "streamer or with some something else um",
    "start": "1064240",
    "end": "1067360"
  },
  {
    "text": "make sure that you are accounting for S3",
    "start": "1067360",
    "end": "1070400"
  },
  {
    "text": "caching effects that's something that we",
    "start": "1070400",
    "end": "1072160"
  },
  {
    "text": "realize um probably in the S3 back end",
    "start": "1072160",
    "end": "1075360"
  },
  {
    "text": "there is some caching happening um so",
    "start": "1075360",
    "end": "1078160"
  },
  {
    "text": "after running some experiments one after",
    "start": "1078160",
    "end": "1080160"
  },
  {
    "text": "each other uh in the S3 setup uh we saw",
    "start": "1080160",
    "end": "1083679"
  },
  {
    "text": "um quite some acceleration uh which is",
    "start": "1083679",
    "end": "1086240"
  },
  {
    "text": "not good if you're trying to calculate a",
    "start": "1086240",
    "end": "1088960"
  },
  {
    "text": "cold start performance uh so if you want",
    "start": "1088960",
    "end": "1091760"
  },
  {
    "text": "to give it a go and check out some uh",
    "start": "1091760",
    "end": "1094080"
  },
  {
    "text": "some benchmarkings uh make sure that you",
    "start": "1094080",
    "end": "1097600"
  },
  {
    "text": "um add a cooldown period between cloud",
    "start": "1097600",
    "end": "1100799"
  },
  {
    "text": "tests",
    "start": "1100799",
    "end": "1102559"
  },
  {
    "text": "um yeah and some some exciting news",
    "start": "1102559",
    "end": "1105440"
  },
  {
    "text": "that's that's coming up and that's",
    "start": "1105440",
    "end": "1107360"
  },
  {
    "text": "already here so we have a new version uh",
    "start": "1107360",
    "end": "1111000"
  },
  {
    "text": "0.13 uh it has full AWS S3 native",
    "start": "1111000",
    "end": "1114520"
  },
  {
    "text": "authentication uh now we also support GS",
    "start": "1114520",
    "end": "1118080"
  },
  {
    "text": "uh GCS um and we have some usability",
    "start": "1118080",
    "end": "1121360"
  },
  {
    "text": "improvements and in the road map uh we",
    "start": "1121360",
    "end": "1123919"
  },
  {
    "text": "have some very cool stuff coming up we",
    "start": "1123919",
    "end": "1126160"
  },
  {
    "text": "are going to support sharded models we",
    "start": "1126160",
    "end": "1128160"
  },
  {
    "text": "are going to optimize uh multiGPU model",
    "start": "1128160",
    "end": "1130320"
  },
  {
    "text": "loading uh we are going to have parallel",
    "start": "1130320",
    "end": "1132799"
  },
  {
    "text": "multi file loading and we are also",
    "start": "1132799",
    "end": "1134960"
  },
  {
    "text": "planning to support GPU direct storage",
    "start": "1134960",
    "end": "1138160"
  },
  {
    "text": "and uh we are especially excited about",
    "start": "1138160",
    "end": "1140480"
  },
  {
    "text": "the G uh GCS uh support uh we are",
    "start": "1140480",
    "end": "1144000"
  },
  {
    "text": "working with the G GKE team uh and uh",
    "start": "1144000",
    "end": "1147360"
  },
  {
    "text": "the first impressions from the G GKE",
    "start": "1147360",
    "end": "1149840"
  },
  {
    "text": "team is amazing they see uh",
    "start": "1149840",
    "end": "1153000"
  },
  {
    "text": "96% model load time reduction with the",
    "start": "1153000",
    "end": "1156320"
  },
  {
    "text": "model streamer when used with VLM uh in",
    "start": "1156320",
    "end": "1159840"
  },
  {
    "text": "compare in compared compared to uh",
    "start": "1159840",
    "end": "1161919"
  },
  {
    "text": "downloading the model from the cloud",
    "start": "1161919",
    "end": "1163840"
  },
  {
    "text": "object storage directly which makes us",
    "start": "1163840",
    "end": "1166919"
  },
  {
    "text": "happy um yeah uh so here are some QR",
    "start": "1166919",
    "end": "1171600"
  },
  {
    "text": "codes for you the first one is uh to the",
    "start": "1171600",
    "end": "1173919"
  },
  {
    "text": "GitHub repo and this is a QR code for",
    "start": "1173919",
    "end": "1177600"
  },
  {
    "text": "the benchmarking white paper we have a",
    "start": "1177600",
    "end": "1179760"
  },
  {
    "text": "lot of results if you want to have a",
    "start": "1179760",
    "end": "1181280"
  },
  {
    "text": "look and if you have any questions we",
    "start": "1181280",
    "end": "1183760"
  },
  {
    "text": "are going to be at the booth uh after",
    "start": "1183760",
    "end": "1185520"
  },
  {
    "text": "this talk we are very happy to um answer",
    "start": "1185520",
    "end": "1188799"
  },
  {
    "text": "any questions chat about uh how you",
    "start": "1188799",
    "end": "1191120"
  },
  {
    "text": "experience this uh uh this challenge and",
    "start": "1191120",
    "end": "1195360"
  },
  {
    "text": "uh one more thing we are very excited",
    "start": "1195360",
    "end": "1198320"
  },
  {
    "text": "about this specific cubecon uh because",
    "start": "1198320",
    "end": "1202000"
  },
  {
    "text": "um the the core of the runai platform uh",
    "start": "1202000",
    "end": "1205039"
  },
  {
    "text": "our runaiuler is now available uh under",
    "start": "1205039",
    "end": "1208160"
  },
  {
    "text": "Apache uh license um so if you are",
    "start": "1208160",
    "end": "1211919"
  },
  {
    "text": "scheduling AI workloads uh we are very",
    "start": "1211919",
    "end": "1214480"
  },
  {
    "text": "happy to hear your feedback uh any any",
    "start": "1214480",
    "end": "1217679"
  },
  {
    "text": "feedback any contribution any um early",
    "start": "1217679",
    "end": "1221200"
  },
  {
    "text": "adopter uh adopters are very much",
    "start": "1221200",
    "end": "1224240"
  },
  {
    "text": "welcome and you can also come to booth",
    "start": "1224240",
    "end": "1227200"
  },
  {
    "text": "if you want to talk about theuler we are",
    "start": "1227200",
    "end": "1229360"
  },
  {
    "text": "very happy to hear your um uh hear your",
    "start": "1229360",
    "end": "1233280"
  },
  {
    "text": "ideas and opinions um and we call it kai",
    "start": "1233280",
    "end": "1236159"
  },
  {
    "text": "scheduleuler okay",
    "start": "1236159",
    "end": "1239120"
  },
  {
    "text": "yes whatever you call it kai Ki Ki um",
    "start": "1239120",
    "end": "1244000"
  },
  {
    "text": "yes that's that's it for for today from",
    "start": "1244000",
    "end": "1247919"
  },
  {
    "text": "us",
    "start": "1247919",
    "end": "1250919"
  }
]