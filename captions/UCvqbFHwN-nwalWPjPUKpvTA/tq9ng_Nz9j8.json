[
  {
    "text": "hello everyone my name is tim hawkin i work at google and i've been working on the kubernetes project uh since before it was open",
    "start": "80",
    "end": "6799"
  },
  {
    "text": "sourced hi i'm bowie i'm also work at google and i work on",
    "start": "6799",
    "end": "13519"
  },
  {
    "text": "networking in particular on kubernetes",
    "start": "13519",
    "end": "17920"
  },
  {
    "text": "and thanks to valerie for some of the slide material today in our agenda",
    "start": "22960",
    "end": "31840"
  },
  {
    "text": "we will be going over two things really this session is two presentations first part will be",
    "start": "34000",
    "end": "39440"
  },
  {
    "text": "an intro this will be an overview of what the sig does and some of the basics",
    "start": "39440",
    "end": "45039"
  },
  {
    "text": "the intro provides an overview of how networking works in kubernetes and we'll be going over infrastructure apis and",
    "start": "45039",
    "end": "50399"
  },
  {
    "text": "concepts basically if you're new to kubernetes or not familiar with the things that our sigs deal with",
    "start": "50399",
    "end": "56480"
  },
  {
    "text": "this is for you the second half of this presentation will be a deep dive",
    "start": "56480",
    "end": "61680"
  },
  {
    "text": "and really we're going to look at sort of the new developments that are happening in the sig so this will be a deeper look",
    "start": "61680",
    "end": "67439"
  },
  {
    "text": "at some of the newest work that the sig has been doing if you're comfortable with kubernetes networking concepts and",
    "start": "67439",
    "end": "72799"
  },
  {
    "text": "want to see what's next this is for you and especially if you want to influence the direction of the",
    "start": "72799",
    "end": "77840"
  },
  {
    "text": "project we highly recommend you listen to this presentation now",
    "start": "77840",
    "end": "87840"
  },
  {
    "text": "first let's go over an intro to kubernetes networking",
    "start": "89920",
    "end": "95280"
  },
  {
    "text": "so one of the uh natural questions for users of",
    "start": "95280",
    "end": "101119"
  },
  {
    "text": "kubernetes is how the project is organized and how it operates so kubernetes project",
    "start": "101119",
    "end": "106880"
  },
  {
    "text": "is split into multiple special interest groups which focus on various aspects of the",
    "start": "106880",
    "end": "112079"
  },
  {
    "text": "system and we break the project problem into aspects that are covered by",
    "start": "112079",
    "end": "117119"
  },
  {
    "text": "these special interest groups and sig network covers basically networking for the cooper's netize project",
    "start": "117119",
    "end": "123280"
  },
  {
    "text": "this is our charter we're responsible for kubernetes networking between",
    "start": "123280",
    "end": "128560"
  },
  {
    "text": "various components including pod networking within and between nodes service",
    "start": "128560",
    "end": "134080"
  },
  {
    "text": "abstractions both for ingress egress and within the cluster and then also network policies",
    "start": "134080",
    "end": "140800"
  },
  {
    "text": "or security policies and access control below is a link to our zoom meeting and our slack",
    "start": "140800",
    "end": "148720"
  },
  {
    "text": "and don't worry we will show this again at the end",
    "start": "148720",
    "end": "153599"
  },
  {
    "text": "now of the kubernetes apis that comprise the system the networking",
    "start": "155360",
    "end": "162720"
  },
  {
    "text": "apis are as follows we have services endpoints and endpoint slice and these",
    "start": "162720",
    "end": "168879"
  },
  {
    "text": "apis cover service registration and discovery we have an api called",
    "start": "168879",
    "end": "174160"
  },
  {
    "text": "ingress which covers l7 http routing in sort of upcoming work we have gateway",
    "start": "174160",
    "end": "180400"
  },
  {
    "text": "which is a next generation http routing and service ingress api and finally for security we have network",
    "start": "180400",
    "end": "186640"
  },
  {
    "text": "policy which is the application firewall and sig network owns all of these",
    "start": "186640",
    "end": "192159"
  },
  {
    "text": "resources now to start off with uh",
    "start": "192159",
    "end": "200640"
  },
  {
    "text": "there's also the infrastructure of how these apis are implemented",
    "start": "200640",
    "end": "205680"
  },
  {
    "text": "so at the lowest level the low level network drivers and how pod networking and connectivity",
    "start": "205680",
    "end": "213120"
  },
  {
    "text": "is wired up is covered by what is known as the container network interface which is the cubelet implementation",
    "start": "213120",
    "end": "220959"
  },
  {
    "text": "also included in kubernetes networking is an implementation of the service api and this is known as cube proxy",
    "start": "220959",
    "end": "228400"
  },
  {
    "text": "also there are many controllers that act on the api for example they implement endpoints and",
    "start": "228400",
    "end": "235280"
  },
  {
    "text": "endpoint slice the various service load balancer integrations for example the things that create load balancers or",
    "start": "235280",
    "end": "242080"
  },
  {
    "text": "configure load balancers in response to services and also aspects of network management",
    "start": "242080",
    "end": "247760"
  },
  {
    "text": "such as ip address allocation or ipam for pods finally kubernetes networking comes by",
    "start": "247760",
    "end": "254959"
  },
  {
    "text": "default with a dns system so it is there is the cube dns implementation",
    "start": "254959",
    "end": "261359"
  },
  {
    "text": "that handles name based service discovery uh using standard dns mechanisms as",
    "start": "261359",
    "end": "267120"
  },
  {
    "text": "opposed to sort of via the kubernetes api",
    "start": "267120",
    "end": "272320"
  },
  {
    "text": "so let's begin at the beginning what is the kubernetes networking model so before we get too far into detail",
    "start": "274639",
    "end": "280639"
  },
  {
    "text": "it's worth revisiting briefly the basics and the kubernetes api networking model is that all pods",
    "start": "280639",
    "end": "287680"
  },
  {
    "text": "can reach all other pods now this might sound simple and the key here is that pods are not",
    "start": "287680",
    "end": "294000"
  },
  {
    "text": "special from a networking standpoint in many implementations they will appear as yet another network endpoints think",
    "start": "294000",
    "end": "299840"
  },
  {
    "text": "about the way your vms and other network devices work now while the concept is simple there",
    "start": "299840",
    "end": "305520"
  },
  {
    "text": "are actually many implementations for example there are flat where the paws",
    "start": "305520",
    "end": "310720"
  },
  {
    "text": "exist on the network directly there are overlays such as vxlan and their routing-based configs such as",
    "start": "310720",
    "end": "316720"
  },
  {
    "text": "bgp we won't be getting into the details of how to configure this aspect",
    "start": "316720",
    "end": "321759"
  },
  {
    "text": "of networking today it's more like an hour of its own but we will provide references after the",
    "start": "321759",
    "end": "327759"
  },
  {
    "text": "presentation",
    "start": "327759",
    "end": "330400"
  },
  {
    "text": "now tim will talk about how services work in kubernetes",
    "start": "332880",
    "end": "340880"
  },
  {
    "text": "so i think it's worth digging into one of the more fundamental abstractions in kubernetes services",
    "start": "340960",
    "end": "348160"
  },
  {
    "text": "we have a fairly common situation i have a client and the client wants to talk to one of my servers i have a bunch of server",
    "start": "348160",
    "end": "355199"
  },
  {
    "text": "replicas because availability scalability how does my client find and reach those",
    "start": "355199",
    "end": "362240"
  },
  {
    "text": "servers we've already established that pods can reach each other on ip addresses but how does the client",
    "start": "362240",
    "end": "369520"
  },
  {
    "text": "know how to find it and which one to use let's assume for a moment that it does some sort of name lookup",
    "start": "369520",
    "end": "376319"
  },
  {
    "text": "and it picks one of the pods",
    "start": "376319",
    "end": "381840"
  },
  {
    "text": "but bad things happen if i choose a server at random what happens when that server goes down",
    "start": "382000",
    "end": "390000"
  },
  {
    "text": "the client has to reconnect to the server instance",
    "start": "390000",
    "end": "395840"
  },
  {
    "text": "sorry and it has to pick a new server now this sounds easy but the truth is",
    "start": "397919",
    "end": "404240"
  },
  {
    "text": "that most software out there does not handle this well especially if the server's ip address is going to change",
    "start": "404240",
    "end": "410240"
  },
  {
    "text": "pod ips are ephemeral ips and clients can't rely on them one of our main goals in kubernetes was",
    "start": "410240",
    "end": "416479"
  },
  {
    "text": "to work with open source software as it exists",
    "start": "416479",
    "end": "422560"
  },
  {
    "text": "so logically we need something in between the client and the server that logical",
    "start": "422560",
    "end": "428080"
  },
  {
    "text": "something is a kubernetes service so to recap pod ips are ephemeral",
    "start": "428080",
    "end": "438000"
  },
  {
    "text": "the statement i have a group of servers and i want my clients to find them is a pretty typical thing",
    "start": "438000",
    "end": "443039"
  },
  {
    "text": "services are how you expose a group of pods to your clients mostly that involves",
    "start": "443039",
    "end": "449919"
  },
  {
    "text": "having a durable virtual ip but you don't have to that includes a virtual port",
    "start": "449919",
    "end": "455599"
  },
  {
    "text": "and a tcp protocol or ip protocol this is how we build service discovery it has built-in load",
    "start": "455599",
    "end": "462400"
  },
  {
    "text": "balancing but it doesn't have to you can opt out of that thing we're not going to go into all the details of all the facets of service today we just",
    "start": "462400",
    "end": "468879"
  },
  {
    "text": "simply don't have time so coming back to the problem statement",
    "start": "468879",
    "end": "474240"
  },
  {
    "text": "let's look at what really happens the node runs a proxy which intercepts",
    "start": "474240",
    "end": "479840"
  },
  {
    "text": "the traffic now this api is pretty abstract and the default implementation is in cube proxy",
    "start": "479840",
    "end": "486160"
  },
  {
    "text": "which isn't actually a proxy usually but it does turn your node into a proxy for example we use iptables or ipvs to",
    "start": "486160",
    "end": "494319"
  },
  {
    "text": "make the node become your sidecar proxy we'll get into that in a little bit more detail",
    "start": "494319",
    "end": "501440"
  },
  {
    "text": "so the client starts by doing a dns query and we'll talk more about dns later too",
    "start": "501919",
    "end": "508319"
  },
  {
    "text": "uh dns itself is a service in kubernetes",
    "start": "508319",
    "end": "513599"
  },
  {
    "text": "that dns service returns the virtual ip of the service that you're trying to",
    "start": "514719",
    "end": "519839"
  },
  {
    "text": "resolve the client now can connect to that",
    "start": "519839",
    "end": "525120"
  },
  {
    "text": "virtual ip the proxy will intercept the traffic and redirect it to the backend pods ip",
    "start": "525120",
    "end": "532959"
  },
  {
    "text": "address async to all of this there are the",
    "start": "532959",
    "end": "538640"
  },
  {
    "text": "controllers that are out there reading the services and the end points compiling them down into usable data",
    "start": "538640",
    "end": "544800"
  },
  {
    "text": "structures and publishing them for use by things like the proxy",
    "start": "544800",
    "end": "550319"
  },
  {
    "text": "so now what happens when a server goes down the service here is hiding a lot of the",
    "start": "550720",
    "end": "556000"
  },
  {
    "text": "details of what happens with a service so when that server goes",
    "start": "556000",
    "end": "561600"
  },
  {
    "text": "down clients just need to reconnect to the",
    "start": "561600",
    "end": "567200"
  },
  {
    "text": "same ip address which honestly most clients can handle",
    "start": "567200",
    "end": "571920"
  },
  {
    "text": "digging into services a little bit more i'd like to show you yaml because everybody loves yammer what",
    "start": "573600",
    "end": "579519"
  },
  {
    "text": "we have here is a service definition this is what you would specify when you're writing a service you can",
    "start": "579519",
    "end": "584560"
  },
  {
    "text": "see that we have uh metadata the name is my service and the namespace is default we're going to",
    "start": "584560",
    "end": "590399"
  },
  {
    "text": "use that as part of our service discovery system the specification says which pods",
    "start": "590399",
    "end": "596399"
  },
  {
    "text": "are mapped to this service in this case i'm taking all pods running that are labeled app my app",
    "start": "596399",
    "end": "604000"
  },
  {
    "text": "and lastly we define the ports these are logical ports for your clients to use that's the port and then the target port",
    "start": "604000",
    "end": "610160"
  },
  {
    "text": "is the actual port on the backend pod the service allows you to remap these things which makes a bunch of things easier",
    "start": "610160",
    "end": "618720"
  },
  {
    "text": "once you submit that to the api server it's going to fill in a bunch of other fields for you it's going to default the type to a cluster ip",
    "start": "618720",
    "end": "626079"
  },
  {
    "text": "and it's going to allocate you a virtual ip address it's also going to define the protocol by default but you can specify",
    "start": "626079",
    "end": "631120"
  },
  {
    "text": "that manually if you really wanted to now i'll hand it to bowie to talk about",
    "start": "631120",
    "end": "636160"
  },
  {
    "text": "the endpoints part of this a service is virtual so how does the",
    "start": "636160",
    "end": "641279"
  },
  {
    "text": "proxy know what pods to send traffic to like most everything in kubernetes it's",
    "start": "641279",
    "end": "646399"
  },
  {
    "text": "yet another controller basically the controller will convert the end pods using the",
    "start": "646399",
    "end": "651680"
  },
  {
    "text": "label selector to a smaller set of endpoints so how endpoints are represented is a",
    "start": "651680",
    "end": "657760"
  },
  {
    "text": "list of ips behind a service now usually these are pods but actually endpoints api also gives you",
    "start": "657760",
    "end": "663519"
  },
  {
    "text": "a way to put your own ips and have custom controllers that do this",
    "start": "663519",
    "end": "668560"
  },
  {
    "text": "now remember that ports services had a port and a target port field so in fact the kubernetes cube proxy",
    "start": "668560",
    "end": "676240"
  },
  {
    "text": "mechanism can actually remap these ports uh given your configuration and generally",
    "start": "676240",
    "end": "682160"
  },
  {
    "text": "these endpoints most people use them are managed by the system so there's an endpoints controller that comes out of",
    "start": "682160",
    "end": "687600"
  },
  {
    "text": "the box with kubernetes but as i said before you can use them to generate custom endpoints as",
    "start": "687600",
    "end": "693440"
  },
  {
    "text": "well now let's look in detail about how this works",
    "start": "693440",
    "end": "698160"
  },
  {
    "text": "so here we have a pod a service sorry and it's the name is foo and it has that",
    "start": "699600",
    "end": "705760"
  },
  {
    "text": "app for selector as we said before and along with the",
    "start": "705760",
    "end": "711040"
  },
  {
    "text": "service we have a bunch of pods who have app selectors now not all of",
    "start": "711040",
    "end": "716320"
  },
  {
    "text": "them are foo only some of them are who in this diagram and the ones that are foo sort of comprise the service foo",
    "start": "716320",
    "end": "725360"
  },
  {
    "text": "now what happens with the endpoints controller is it will look at the selector in service look at",
    "start": "725360",
    "end": "731279"
  },
  {
    "text": "all the pods that have the appropriate labels and create an endpoints object",
    "start": "731279",
    "end": "737519"
  },
  {
    "text": "that comprises the endpoints of the pods that match the service and this",
    "start": "737519",
    "end": "744160"
  },
  {
    "text": "endpoints object is then something that will be fed into the rest of the system for example load",
    "start": "744160",
    "end": "749600"
  },
  {
    "text": "balancers cube proxy dns to tell the rest of the system that this service",
    "start": "749600",
    "end": "756480"
  },
  {
    "text": "has these endpoints as part of the service",
    "start": "756480",
    "end": "762639"
  },
  {
    "text": "now as i mentioned before there is dns so dns is an integral part of service",
    "start": "763920",
    "end": "769279"
  },
  {
    "text": "discovery if you think of service discovery as being layered you have the kubernetes api which is",
    "start": "769279",
    "end": "774720"
  },
  {
    "text": "endpoints then you'll have traditional mechanisms to access the service such as dns and then of course q proxy",
    "start": "774720",
    "end": "782880"
  },
  {
    "text": "so dns starts with a specification there's a specification online of sort of the set of records",
    "start": "782880",
    "end": "788639"
  },
  {
    "text": "that are generated in response to various kubernetes apis and generally",
    "start": "788639",
    "end": "794800"
  },
  {
    "text": "there's a default implementation that runs as a pod in the cluster but of course this is not necessary any",
    "start": "794800",
    "end": "800880"
  },
  {
    "text": "implementation that meets the specification will do just fine and the dns is exposed",
    "start": "800880",
    "end": "808079"
  },
  {
    "text": "itself as a service vip but again this is not required containers are configured by",
    "start": "808079",
    "end": "815120"
  },
  {
    "text": "cubelet to use cube dns and in fact kubernetes has an interesting concept that's sort of",
    "start": "815120",
    "end": "821360"
  },
  {
    "text": "an extension on standard dns behavior in that we use mechanisms to create aliases to make it",
    "start": "821360",
    "end": "828480"
  },
  {
    "text": "easier for people to consume a service today the default implementation in",
    "start": "828480",
    "end": "834720"
  },
  {
    "text": "kubernetes is core dns now let's look at",
    "start": "834720",
    "end": "839839"
  },
  {
    "text": "an example canonical name so here we have my service which is the name of your",
    "start": "839839",
    "end": "845920"
  },
  {
    "text": "service dot default which is the name space your service lives in and then there's this sort of sigil i guess a token",
    "start": "845920",
    "end": "854800"
  },
  {
    "text": "svc that's just a constant that denotes as a service and then cluster.local is the cluster's dns node zone",
    "start": "854800",
    "end": "861600"
  },
  {
    "text": "now we note that because of the alias mechanism you can actually say",
    "start": "861600",
    "end": "866880"
  },
  {
    "text": "my service and expect to be scoped within your namespace or myservice.default",
    "start": "866880",
    "end": "872399"
  },
  {
    "text": "to be scoped within a kubernetes cluster and these things are automatically handled for you",
    "start": "872399",
    "end": "878320"
  },
  {
    "text": "through the cubelet dns configuration injection",
    "start": "878320",
    "end": "885839"
  },
  {
    "text": "now i'll hand it over to tim to talk about how q proxy works",
    "start": "886320",
    "end": "891440"
  },
  {
    "text": "so we promised that we would tell you more about how the proxy system works at first it's worth bearing in mind that",
    "start": "892160",
    "end": "899360"
  },
  {
    "text": "cube proxy is the default implementation but it is not the only implementation uh it can be replaced and in fact we",
    "start": "899360",
    "end": "906480"
  },
  {
    "text": "encourage people if you have special use cases to go ahead and replace it it is an implementation of the apis",
    "start": "906480",
    "end": "912240"
  },
  {
    "text": "so a cube proxy by default runs on every node in your cluster it uses that node as a",
    "start": "912240",
    "end": "919360"
  },
  {
    "text": "proxy for traffic from pods on that node the mechanisms that we have built into it right now are",
    "start": "919360",
    "end": "926000"
  },
  {
    "text": "iptables ipvs user space windows has also a user space and an",
    "start": "926000",
    "end": "931519"
  },
  {
    "text": "internal options in linux iptables and ipvs are the more modern versions the user space",
    "start": "931519",
    "end": "937839"
  },
  {
    "text": "is less scalable and less behaviorally correct in general",
    "start": "937839",
    "end": "944720"
  },
  {
    "text": "cube proxy is completely transparent to consumers all those pods that are running on the same node don't know",
    "start": "944720",
    "end": "951440"
  },
  {
    "text": "that cube proxy is there intercepting traffic that's part of the api",
    "start": "951440",
    "end": "956480"
  },
  {
    "text": "so the way cubeproxy works on the control path is it's going to watch all of the services and endpoints in the",
    "start": "958480",
    "end": "964720"
  },
  {
    "text": "system which we just looked at it's going to apply some filters for example it's going to ignore some",
    "start": "964720",
    "end": "969839"
  },
  {
    "text": "services that don't have virtual ip addresses we call those headless it's going to link those endpoints the",
    "start": "969839",
    "end": "976240"
  },
  {
    "text": "back ends with the services which are the front ends and it's going to accumulate the changes to all of those",
    "start": "976240",
    "end": "982160"
  },
  {
    "text": "and it's going to then update rules on the node to represent those changes for example in iptables it will capture",
    "start": "982160",
    "end": "989120"
  },
  {
    "text": "traffic destined for the virtual ip and port and it will redirect it into the data",
    "start": "989120",
    "end": "995040"
  },
  {
    "text": "path to handle the data path on the data path it's going to like i",
    "start": "995040",
    "end": "1001440"
  },
  {
    "text": "said recognize the destination ip and port it's going to choose a back end pod",
    "start": "1001440",
    "end": "1006880"
  },
  {
    "text": "generally it's going to consider things like client affinity have there been other sessions from this client",
    "start": "1006880",
    "end": "1012639"
  },
  {
    "text": "to this virtual ip if the service has requested those and then it's going to rewrite those",
    "start": "1012639",
    "end": "1017920"
  },
  {
    "text": "packets to a new destination that's destination nat on response it's going to apply the",
    "start": "1017920",
    "end": "1023440"
  },
  {
    "text": "opposite of that and undeniab it and return it back to the original client again the clients are unaware of what's",
    "start": "1023440",
    "end": "1029360"
  },
  {
    "text": "happening here this is all happening on the node as a service to the",
    "start": "1029360",
    "end": "1034480"
  },
  {
    "text": "pods on the node so we hear this a lot um why don't we",
    "start": "1034480",
    "end": "1040720"
  },
  {
    "text": "just use dns round-robining for load balancing why is this so complicated",
    "start": "1040720",
    "end": "1045839"
  },
  {
    "text": "one of the fundamental things we learned about kubernetes very early on is that most dns clients are broken and",
    "start": "1045839",
    "end": "1052240"
  },
  {
    "text": "they don't handle things like dns records changing very well many applications never resolve re-resolve the names they've",
    "start": "1052240",
    "end": "1058799"
  },
  {
    "text": "looked up this abstraction provides a stable ip address so that clients can work",
    "start": "1058799",
    "end": "1063919"
  },
  {
    "text": "as they existed before kubernetes in a world where durable ip addresses were more prevalent",
    "start": "1063919",
    "end": "1069760"
  },
  {
    "text": "for those clients that are enlightened and can actually handle this properly can you opt out",
    "start": "1069760",
    "end": "1074960"
  },
  {
    "text": "of this stuff of course you can this is kubernetes you have a million options there are things like headless services",
    "start": "1074960",
    "end": "1080480"
  },
  {
    "text": "which allow you to do dns around robbing without a virtual ip address and without any nat translations",
    "start": "1080480",
    "end": "1088320"
  },
  {
    "text": "alongside services is the load balancer system services are how you expose load",
    "start": "1089760",
    "end": "1096000"
  },
  {
    "text": "balancers in kubernetes we have different load balancer implementations and they work in very different ways across cloud providers",
    "start": "1096000",
    "end": "1102240"
  },
  {
    "text": "and across mechanisms with your networking it's a little too broad for us to go into and into this talk",
    "start": "1102240",
    "end": "1109120"
  },
  {
    "text": "but there are other talks online that you can watch that are we'll go into this in great detail most",
    "start": "1109120",
    "end": "1115039"
  },
  {
    "text": "cloud providers have implementations of load balancers for services in",
    "start": "1115039",
    "end": "1120840"
  },
  {
    "text": "kubernetes now i'll hand it to bowie to talk up one more level",
    "start": "1120840",
    "end": "1127760"
  },
  {
    "text": "thanks tim so we saw that services describe l4 protocol based",
    "start": "1127919",
    "end": "1135039"
  },
  {
    "text": "services but what about l7 and this is where the ingress resource comes in so what an",
    "start": "1135039",
    "end": "1140320"
  },
  {
    "text": "ingress resource does is it's an api to describe an http proxy and routing rules",
    "start": "1140320",
    "end": "1146160"
  },
  {
    "text": "so this is a very simple api to match hostnames and url paths it might be too simple so we'll see more",
    "start": "1146160",
    "end": "1151840"
  },
  {
    "text": "on this later in the deep dive and what it does is it targets a service for each http",
    "start": "1151840",
    "end": "1157280"
  },
  {
    "text": "url path rule now kubernetes defines this api but the implementations are all third-party and integrates",
    "start": "1157280",
    "end": "1165679"
  },
  {
    "text": "with most clouds and popular software lbs in fact i think there are quite a few",
    "start": "1165679",
    "end": "1172080"
  },
  {
    "text": "ingress implementations so let's go over sort of what the shape of this api looks",
    "start": "1172080",
    "end": "1177200"
  },
  {
    "text": "like so here we have a ingress configuration",
    "start": "1177200",
    "end": "1183039"
  },
  {
    "text": "along with the services and pods and you'll see at the top there's an ingress with a hostname and a bunch of paths and",
    "start": "1183039",
    "end": "1189919"
  },
  {
    "text": "each path corresponds to a rule that then maps to a specific service so you can imagine",
    "start": "1189919",
    "end": "1195840"
  },
  {
    "text": "there's a path for slash store and that will be your store service and there's a path for slash search and",
    "start": "1195840",
    "end": "1202400"
  },
  {
    "text": "that will go to your search service and then the services are used",
    "start": "1202400",
    "end": "1207440"
  },
  {
    "text": "as a way to select which server pods are you know implementing that service",
    "start": "1207440",
    "end": "1214480"
  },
  {
    "text": "for example in this case the foo service will be implemented by back ends with the label appfu and the same thing",
    "start": "1214480",
    "end": "1222080"
  },
  {
    "text": "with the bar service",
    "start": "1222080",
    "end": "1225120"
  },
  {
    "text": "now a couple of questions come up which is one how is this different from service type load balancer",
    "start": "1227200",
    "end": "1232640"
  },
  {
    "text": "so i think the difference is that service type load balancer is constrained at the l4 it doesn't talk",
    "start": "1232640",
    "end": "1238720"
  },
  {
    "text": "about anything higher in terms of load balancing and ingress provides a way to describe",
    "start": "1238720",
    "end": "1245360"
  },
  {
    "text": "these things and the other thing to note is that when you go to l7 it is very common that you will be composing services",
    "start": "1245360",
    "end": "1252400"
  },
  {
    "text": "multiple l4 services together to create a single l7 service",
    "start": "1252400",
    "end": "1257440"
  },
  {
    "text": "second question is why isn't there a controller in the box so there were actually um many",
    "start": "1257440",
    "end": "1263840"
  },
  {
    "text": "if you think about the overall l7 proxy landscape uh it is a pretty mature landscape there",
    "start": "1263840",
    "end": "1271760"
  },
  {
    "text": "are many implementations many different proxies and we didn't want to pick a winner among the software lbs however you know",
    "start": "1271760",
    "end": "1279840"
  },
  {
    "text": "it is an interesting point to have a system that has batteries included",
    "start": "1279840",
    "end": "1285600"
  },
  {
    "text": "next tim we'll talk about network policy",
    "start": "1286960",
    "end": "1291840"
  },
  {
    "text": "so you can't really have a network without some way of describing what's allowed and disallowed on the network",
    "start": "1292080",
    "end": "1298720"
  },
  {
    "text": "network policy is the kubernetes api that describes the graph of your application it is",
    "start": "1298720",
    "end": "1305360"
  },
  {
    "text": "allows your app developers to express what do you expect to be happening here and disallow everything that isn't",
    "start": "1305360",
    "end": "1311760"
  },
  {
    "text": "expected so for example you could have all your front ends which talk to your back ends and your back ends which talk to your",
    "start": "1311760",
    "end": "1317120"
  },
  {
    "text": "databases but you'd never want your front end to talk directly to your database that would either be a bug or something even worse",
    "start": "1317120",
    "end": "1324880"
  },
  {
    "text": "like ingress network policy implementations are all third-party it's very likely that the",
    "start": "1324880",
    "end": "1330320"
  },
  {
    "text": "implementations of network policy are very coupled to the way your network is configured",
    "start": "1330320",
    "end": "1335360"
  },
  {
    "text": "and so it didn't really make sense to have an in the box version of this when we don't have an in the box version of pod networking",
    "start": "1335360",
    "end": "1343520"
  },
  {
    "text": "network policy is a very simple api it's very focused on application owners rather than network administrators",
    "start": "1343520",
    "end": "1350080"
  },
  {
    "text": "and you know we may yet need another api at a higher level for network administrators",
    "start": "1350080",
    "end": "1355679"
  },
  {
    "text": "but network policy is really supposed to be part of your application's description so you can see here",
    "start": "1355679",
    "end": "1363919"
  },
  {
    "text": "the example that we mentioned all the frontends can talk to all the backends and all the backends can talk to the databases",
    "start": "1363919",
    "end": "1370000"
  },
  {
    "text": "but you would never want your frontends to talk to your databases or for your front ends to talk to other",
    "start": "1370000",
    "end": "1375280"
  },
  {
    "text": "front-ends that might represents that you've been compromised and someone was trying to do a lateral attack",
    "start": "1375280",
    "end": "1380880"
  },
  {
    "text": "network policy lets you describe the green lines and everything that's not green is",
    "start": "1380880",
    "end": "1386159"
  },
  {
    "text": "automatically blocked so now i'm going to hand it to bowie to",
    "start": "1386159",
    "end": "1393039"
  },
  {
    "text": "dig into the details of some of the ongoing work in the sig",
    "start": "1393039",
    "end": "1398799"
  },
  {
    "text": "so hopefully that gives you sort of a coverage of at least the table of contents for",
    "start": "1399679",
    "end": "1406559"
  },
  {
    "text": "kubernetes networking we know that was a lot of deep material and we'll be happy to answer lots of questions at the end of this",
    "start": "1406559",
    "end": "1416799"
  },
  {
    "text": "so as part of the deep dive we're going to look at a closer look at the work that's happening in sig",
    "start": "1416799",
    "end": "1422880"
  },
  {
    "text": "network so some of this is a ga and others are in alpha and beta stages",
    "start": "1422880",
    "end": "1427919"
  },
  {
    "text": "and i'd like to repeat that if you're looking to influence the direction of kubernetes networking we really value",
    "start": "1427919",
    "end": "1433679"
  },
  {
    "text": "feedback and participation from the community here and the ongoing work in the sig that",
    "start": "1433679",
    "end": "1440400"
  },
  {
    "text": "we're going to highlight for the deep dive are endpoint slice which is work in",
    "start": "1440400",
    "end": "1445520"
  },
  {
    "text": "scalability and extensibility no local dns which makes dns more scalable",
    "start": "1445520",
    "end": "1451679"
  },
  {
    "text": "all the factoring around services sort of thinking about where to go next with services and",
    "start": "1451679",
    "end": "1457120"
  },
  {
    "text": "finally support for ipv4 and v6 dual stack",
    "start": "1457120",
    "end": "1462400"
  },
  {
    "text": "so no local dns was a project that was initiated because we knew from",
    "start": "1465520",
    "end": "1471520"
  },
  {
    "text": "copious feedback and issues on github with many many comments on them that the kubernetes dns",
    "start": "1471520",
    "end": "1477440"
  },
  {
    "text": "resource cost is quite high right so this is partially due to the expansion due to",
    "start": "1477440",
    "end": "1483520"
  },
  {
    "text": "name aliases so it turns out that using an alias mechanism results in potentially with",
    "start": "1483520",
    "end": "1489279"
  },
  {
    "text": "ipv6 enabled a 10x query explosion but it also has to do with the fact that",
    "start": "1489279",
    "end": "1495120"
  },
  {
    "text": "as kubernetes users we have higher application density so we have lots of microservices so we see",
    "start": "1495120",
    "end": "1500880"
  },
  {
    "text": "lots more dns clients we see that lots of modern application",
    "start": "1500880",
    "end": "1506480"
  },
  {
    "text": "libraries and infrastructure such as node.js are quite dns heavy in their default",
    "start": "1506480",
    "end": "1512840"
  },
  {
    "text": "configuration and finally that the cube proxy at least with iptables",
    "start": "1512840",
    "end": "1519360"
  },
  {
    "text": "implementation the use of contract and various connection tracking",
    "start": "1519360",
    "end": "1525039"
  },
  {
    "text": "mechanisms in the kernel also have a bad interaction with udp which is the",
    "start": "1525039",
    "end": "1530400"
  },
  {
    "text": "transport used by a dns protocol so as dns",
    "start": "1530400",
    "end": "1535440"
  },
  {
    "text": "is the canonical example of achieving global scalability through caching the",
    "start": "1535440",
    "end": "1541200"
  },
  {
    "text": "answer is simple run a cache on every node however we have to be very careful",
    "start": "1541200",
    "end": "1546640"
  },
  {
    "text": "because as it turns out if you run something per node actually the overhead can dominate in",
    "start": "1546640",
    "end": "1552640"
  },
  {
    "text": "large clusters so we have to be very careful and in fact the node local dns",
    "start": "1552640",
    "end": "1557919"
  },
  {
    "text": "is an extremely slimmed down very small local cache that's based on core dns but",
    "start": "1557919",
    "end": "1564720"
  },
  {
    "text": "basically without many of the plugins compiled in as a system critical component which is",
    "start": "1564720",
    "end": "1571440"
  },
  {
    "text": "in a daemon set we also have to be quite careful about high ability during upgrades and failures and this was",
    "start": "1571440",
    "end": "1577039"
  },
  {
    "text": "one of the gating criteria for the ga of no local dns so let's go into how no",
    "start": "1577039",
    "end": "1583679"
  },
  {
    "text": "local dns looks on your system so here we have sort of how dns works",
    "start": "1583679",
    "end": "1591200"
  },
  {
    "text": "without no local dns so you see in green uh you have a set of pods and cubelet has configured for your",
    "start": "1591200",
    "end": "1597600"
  },
  {
    "text": "pod the upstream dns resolver which is uh in this case 10010 and cube dns will use",
    "start": "1597600",
    "end": "1606159"
  },
  {
    "text": "the standard q proxy service mechanism that we described before to reach the back ends",
    "start": "1606159",
    "end": "1612080"
  },
  {
    "text": "of the dns system now when you enable node local dns",
    "start": "1612080",
    "end": "1617279"
  },
  {
    "text": "what you'll see is that it inserts itself as a daemon set on your node as part of this",
    "start": "1617279",
    "end": "1624480"
  },
  {
    "text": "it creates a dummy interface this is an orange here with binding to",
    "start": "1624480",
    "end": "1631919"
  },
  {
    "text": "two ip addresses and along with it it inserts a bunch of no track entries",
    "start": "1631919",
    "end": "1637840"
  },
  {
    "text": "for those service vips on the local node what this does is with no track you",
    "start": "1637840",
    "end": "1644960"
  },
  {
    "text": "avoid the usage of contract entries for the udp packets so it will skip q proxy the secondary effect of the",
    "start": "1644960",
    "end": "1652840"
  },
  {
    "text": "interface and the binding of the vip is that actually this allows node local dns to capture the vip",
    "start": "1652840",
    "end": "1660080"
  },
  {
    "text": "traffic and sort of live alongside cube proxy you'll notice that we still have to go",
    "start": "1660080",
    "end": "1667039"
  },
  {
    "text": "upstream to cube dns so how do we do that we actually create a another service that tells",
    "start": "1667039",
    "end": "1674880"
  },
  {
    "text": "the no local dns about the upstream which is this cube dns upstream here this is the 10",
    "start": "1674880",
    "end": "1680399"
  },
  {
    "text": "0 and then the cluster vip will be allocated from your cluster vip range now you'll",
    "start": "1680399",
    "end": "1686720"
  },
  {
    "text": "notice what a very interesting thing here is that these two mechanisms live side by side and in fact for ha",
    "start": "1686720",
    "end": "1694320"
  },
  {
    "text": "and upgrades for example if the no local dns goes down it has a",
    "start": "1694320",
    "end": "1700799"
  },
  {
    "text": "watcher process that will remove the node track and once the node track entries are removed actually the",
    "start": "1700799",
    "end": "1707440"
  },
  {
    "text": "standard q proxy service mechanism will happen your pod will talk to cube dns and",
    "start": "1707440",
    "end": "1714159"
  },
  {
    "text": "seamlessly fail over to the old mechanism so this allows us to not only manage upgrades but also",
    "start": "1714159",
    "end": "1721039"
  },
  {
    "text": "situations where no local dns has failed for some reason and we are",
    "start": "1721039",
    "end": "1726240"
  },
  {
    "text": "able to at least default you back to the cube dns infrastructure so one final note",
    "start": "1726240",
    "end": "1734640"
  },
  {
    "text": "on dns is we can do better so even though no local dns handles some",
    "start": "1734640",
    "end": "1740960"
  },
  {
    "text": "of the load issues it is still yet another agent doing dns in your cluster",
    "start": "1740960",
    "end": "1747440"
  },
  {
    "text": "to handle some of the query expansion for the aliases so there is a proposal",
    "start": "1747440",
    "end": "1752799"
  },
  {
    "text": "as a cap to push alias expansion into the server the dns server as an api for kubernetes",
    "start": "1752799",
    "end": "1760240"
  },
  {
    "text": "and what this will do is to make some of the mechanisms and plug-ins that lots of people use such as the",
    "start": "1760240",
    "end": "1766000"
  },
  {
    "text": "core dns auto path plugin actually pulled that into a standard kubernetes api",
    "start": "1766000",
    "end": "1771679"
  },
  {
    "text": "finally one other topic that sort of is interesting is how do we refactor the",
    "start": "1771679",
    "end": "1777679"
  },
  {
    "text": "dns naming scheme altogether so some of the search expansions and aliases may not be",
    "start": "1777679",
    "end": "1783679"
  },
  {
    "text": "necessary but also what kind of improvements can we make on the schema",
    "start": "1783679",
    "end": "1790158"
  },
  {
    "text": "the next topic for our deep dive is also partially scalability and partially",
    "start": "1791279",
    "end": "1797360"
  },
  {
    "text": "extensibility so as we talked about before there is an endpoints api",
    "start": "1797360",
    "end": "1802960"
  },
  {
    "text": "that serves as a service discovery mechanism for kubernetes at the kubernetes api level",
    "start": "1802960",
    "end": "1810320"
  },
  {
    "text": "now in larger clusters think 15k nodes for instance and very large services will lead to api",
    "start": "1810320",
    "end": "1816720"
  },
  {
    "text": "scalability issue so if you think about it an endpoints object is a single object and",
    "start": "1816720",
    "end": "1822000"
  },
  {
    "text": "let's say you have a thousand endpoints all thousand endpoints will have to go into that single object",
    "start": "1822000",
    "end": "1827600"
  },
  {
    "text": "if you keep expanding services and the number of backends in a service eventually this will exceed the size of a single",
    "start": "1827600",
    "end": "1834399"
  },
  {
    "text": "object in fcd now of course you can reconfigure fcd to support larger",
    "start": "1834399",
    "end": "1839679"
  },
  {
    "text": "objects but there is a limit to that you can conceivably double the size of the object maybe",
    "start": "1839679",
    "end": "1845279"
  },
  {
    "text": "triple it but at some point you're going to have to live with the limits of sed also",
    "start": "1845279",
    "end": "1851760"
  },
  {
    "text": "a huge issue is the number of watchers for endpoint objects so as you",
    "start": "1851760",
    "end": "1858000"
  },
  {
    "text": "recall cube proxy watches the endpoints objects in the cluster and there is a copy of q",
    "start": "1858000",
    "end": "1864320"
  },
  {
    "text": "proxy running on every single node in addition uh other controllers may be watching",
    "start": "1864320",
    "end": "1869919"
  },
  {
    "text": "endpoints as well and what this does is it creates a large amount of data that needs to be sent to",
    "start": "1869919",
    "end": "1876559"
  },
  {
    "text": "the watchers so in this example below and we have a",
    "start": "1876559",
    "end": "1881760"
  },
  {
    "text": "5 000 nodes let's say the size of each endpoint object is one megabyte and if you have an update to the",
    "start": "1881760",
    "end": "1888000"
  },
  {
    "text": "endpoints object it will actually send five gigabytes of data now this",
    "start": "1888000",
    "end": "1894000"
  },
  {
    "text": "is basically a dvd and then if you do a rolling update where you are slowly changing the endpoints one by one",
    "start": "1894000",
    "end": "1900799"
  },
  {
    "text": "and you imagine these updates propagating through the system by the end of your rolling updates this will be 25 terabytes which is a lot of data",
    "start": "1900799",
    "end": "1910720"
  },
  {
    "text": "so the idea of endpoint slice is that instead of a single endpoints object we in fact have slices of endpoints and",
    "start": "1911760",
    "end": "1918960"
  },
  {
    "text": "then sort of create a relationship between the endpoint slices and the services",
    "start": "1918960",
    "end": "1924000"
  },
  {
    "text": "in a different mechanism here we have a diagram of what the endpoints objects",
    "start": "1924000",
    "end": "1930880"
  },
  {
    "text": "update looks like for an endpoints object so here we have at the top the",
    "start": "1930880",
    "end": "1937600"
  },
  {
    "text": "resource living in the api server so there's like a single resource and we perform a single update which is",
    "start": "1937600",
    "end": "1944080"
  },
  {
    "text": "the yellow highlighted endpoint here but for each cube proxy in the cluster",
    "start": "1944080",
    "end": "1949120"
  },
  {
    "text": "we actually end up having to send the entire endpoints object even though that little piece",
    "start": "1949120",
    "end": "1954159"
  },
  {
    "text": "changed we have to send the entire thing with endpoint slices what happens is",
    "start": "1954159",
    "end": "1959919"
  },
  {
    "text": "that that service is in fact instead represented by a set",
    "start": "1959919",
    "end": "1965760"
  },
  {
    "text": "of endpoint slices so there are actually many endpoint slices for a given",
    "start": "1965760",
    "end": "1973360"
  },
  {
    "text": "service and if you see and they are each uh of a maximum size by default they will",
    "start": "1973360",
    "end": "1979679"
  },
  {
    "text": "have a hundred endpoints each and a single update to an endpoint actually only",
    "start": "1979679",
    "end": "1984799"
  },
  {
    "text": "needs to send the slice so you'll see here that highlighted in red that endpoint slice object abbreviated",
    "start": "1984799",
    "end": "1990559"
  },
  {
    "text": "eps will be sent to these cube proxies but the ones that did not change",
    "start": "1990559",
    "end": "1997200"
  },
  {
    "text": "will not be sent so to implement endpoint slices",
    "start": "1997200",
    "end": "2004240"
  },
  {
    "text": "there are a couple of controllers that are needed there's the endpoint slices controller which basically creates slices from the",
    "start": "2004240",
    "end": "2009919"
  },
  {
    "text": "service selector and you'll notice that these are linked to the service via labeling and",
    "start": "2009919",
    "end": "2015120"
  },
  {
    "text": "selection it's a standard kubernetes mechanisms also we notice that there is another",
    "start": "2015120",
    "end": "2022720"
  },
  {
    "text": "interface that people use for the endpoints api which is to custom populate endpoints apis",
    "start": "2022720",
    "end": "2028320"
  },
  {
    "text": "outside of the standard endpoints controller and this is handled by the endpoint slice mirroring controller",
    "start": "2028320",
    "end": "2034000"
  },
  {
    "text": "which mirrors endpoints from uh endpoint resources that are sort of populated",
    "start": "2034000",
    "end": "2040159"
  },
  {
    "text": "outside of the controller into slices as well finally the",
    "start": "2040159",
    "end": "2045840"
  },
  {
    "text": "endpoints slice api we know that lots of people will want to extend service discovery",
    "start": "2045840",
    "end": "2053520"
  },
  {
    "text": "so it is in fact designed to be a generic service discovery mechanism and other users can set this annotation",
    "start": "2053520",
    "end": "2061520"
  },
  {
    "text": "to specify uh who is managing the endpoint slices",
    "start": "2061520",
    "end": "2069279"
  },
  {
    "text": "so sort of more detail on how endpoint slices work so it turns out the update",
    "start": "2070480",
    "end": "2076158"
  },
  {
    "text": "algorithm is actually an interesting optimization problem so at the same time you want to keep the",
    "start": "2076159",
    "end": "2082638"
  },
  {
    "text": "number of slices low because you don't want to have create too many resources and start running into fcd",
    "start": "2082639",
    "end": "2090560"
  },
  {
    "text": "right issues in terms of number of resources you need to write to the database while minimizing the number of changes",
    "start": "2090560",
    "end": "2097359"
  },
  {
    "text": "to the slices per update because we don't remember every single change in every single resource needs to",
    "start": "2097359",
    "end": "2102800"
  },
  {
    "text": "be sent out to a watcher and of course keep the amount of data sent low",
    "start": "2102800",
    "end": "2107920"
  },
  {
    "text": "so these are kind of somewhat intersecting and you can imagine that to keep the",
    "start": "2107920",
    "end": "2114000"
  },
  {
    "text": "number slices low you might need to make more changes to slices and so forth the current algorithm that we have",
    "start": "2114000",
    "end": "2119920"
  },
  {
    "text": "settled on is the following the endpoint slice controller will",
    "start": "2119920",
    "end": "2125280"
  },
  {
    "text": "go over all of the endpoints first remove stale endpoints in all the existing slices",
    "start": "2125280",
    "end": "2131200"
  },
  {
    "text": "and then fill in new endpoints into the free space and then finally if you have no space left over to create",
    "start": "2131200",
    "end": "2137440"
  },
  {
    "text": "new slices now you'll notice that this has no active rebalancing and the claim is that we can do",
    "start": "2137440",
    "end": "2145200"
  },
  {
    "text": "active rebalancing but it will be too much churn of course this is an open area and since endpoint",
    "start": "2145200",
    "end": "2150640"
  },
  {
    "text": "slice is going uh is in beta and going ga soon we would love to see how people's actual",
    "start": "2150640",
    "end": "2157200"
  },
  {
    "text": "experiences with this algorithm shake out and just to go into kind of",
    "start": "2157200",
    "end": "2163119"
  },
  {
    "text": "the timelines because this is a core api its rollout is going to take a couple of releases so",
    "start": "2163119",
    "end": "2170240"
  },
  {
    "text": "in 117 the endpoint slice controller and api was available in 118 we made the controller enabled",
    "start": "2170240",
    "end": "2178079"
  },
  {
    "text": "so that the resources will be created but q proxy will not be defaulted in and then 119 we expect endpoint slice",
    "start": "2178079",
    "end": "2184720"
  },
  {
    "text": "controller endpoint slice mirror windows and q proxy enabled and finally in 120 it will be ga",
    "start": "2184720",
    "end": "2190720"
  },
  {
    "text": "the reason that this has to be staged is to basically take care of the fact that",
    "start": "2190720",
    "end": "2196000"
  },
  {
    "text": "you don't want a race condition when you're upgrading between cue proxy consuming endpoint slices and",
    "start": "2196000",
    "end": "2201599"
  },
  {
    "text": "the endpoint slices actually existing in the cluster",
    "start": "2201599",
    "end": "2206400"
  },
  {
    "text": "next i'll hand it to tim to talk about services across clusters",
    "start": "2206720",
    "end": "2212880"
  },
  {
    "text": "thanks bowie as kubernetes users become more prevalent in more market",
    "start": "2213280",
    "end": "2219440"
  },
  {
    "text": "segments and their installations get bigger their needs get bigger it's becoming very common for",
    "start": "2219440",
    "end": "2225760"
  },
  {
    "text": "users to have more than one cluster and that we have a lot of reasons why users have more than one clusters",
    "start": "2225760",
    "end": "2231520"
  },
  {
    "text": "some are ha some are blast radius geography legal restrictions uh dev test prod",
    "start": "2231520",
    "end": "2238560"
  },
  {
    "text": "environments i can probably give you 25 reasons why people have multiple clusters",
    "start": "2238560",
    "end": "2245839"
  },
  {
    "text": "the problem here is that services have always been a cluster-centric abstraction we're now starting to work",
    "start": "2245839",
    "end": "2252640"
  },
  {
    "text": "through how do we export and extend services across clusters this is a collaboration",
    "start": "2252640",
    "end": "2257839"
  },
  {
    "text": "with the sig multi-cluster this comes up as one of the most common pain points that we hear from users",
    "start": "2257839",
    "end": "2265440"
  },
  {
    "text": "for example let's look at this situation i have two clusters a and b and i have",
    "start": "2265440",
    "end": "2271920"
  },
  {
    "text": "my front-end service in cluster a and my back-end service in cluster b how do i get my front-ends to be able to",
    "start": "2271920",
    "end": "2279680"
  },
  {
    "text": "reach my back ends this seems like it should be easy there are ways to do this generally",
    "start": "2279680",
    "end": "2285680"
  },
  {
    "text": "involving load balancers and translating from one ip space to another",
    "start": "2285680",
    "end": "2291200"
  },
  {
    "text": "or or rewriting packets but i think we can do better than that",
    "start": "2291200",
    "end": "2296400"
  },
  {
    "text": "so one of the things that we're starting to look at is what if we could expose services between these clusters",
    "start": "2296400",
    "end": "2303920"
  },
  {
    "text": "so for example given the service on the left the backend service what if i had",
    "start": "2303920",
    "end": "2309839"
  },
  {
    "text": "something like the right that simply declares that this service is exported",
    "start": "2309839",
    "end": "2315440"
  },
  {
    "text": "right but what does it mean exported to what",
    "start": "2315440",
    "end": "2320800"
  },
  {
    "text": "so first let's assert that a group exists i'm not defining the group i'm just",
    "start": "2322079",
    "end": "2327680"
  },
  {
    "text": "saying that it exists this leaves a lot of room for interesting implementations for vendors and for people to try",
    "start": "2327680",
    "end": "2334240"
  },
  {
    "text": "different things with their grouping mechanisms so let's just assume that it exists and within",
    "start": "2334240",
    "end": "2339760"
  },
  {
    "text": "that group these clusters are related within the group we're going to assert",
    "start": "2339760",
    "end": "2346560"
  },
  {
    "text": "that namespaces mean the same thing and that's a pretty vague term what it means is",
    "start": "2346560",
    "end": "2351920"
  },
  {
    "text": "there's a single owner for a given namespace regardless of which cluster they're talking about so what that",
    "start": "2351920",
    "end": "2358640"
  },
  {
    "text": "really means is this that service logically exists across",
    "start": "2358640",
    "end": "2363920"
  },
  {
    "text": "both namespaces sorry the namespace exists across both clusters",
    "start": "2363920",
    "end": "2369200"
  },
  {
    "text": "and even if it doesn't physically exist in the other cluster",
    "start": "2369200",
    "end": "2374640"
  },
  {
    "text": "that means that we can start to build controllers that operate across those clusters in this case",
    "start": "2375200",
    "end": "2381440"
  },
  {
    "text": "we wrote the service export controller which triggers that we go out to each other cluster and",
    "start": "2381440",
    "end": "2387839"
  },
  {
    "text": "republish that service to find that for them to find it across the different clusters so in this",
    "start": "2387839",
    "end": "2395200"
  },
  {
    "text": "case it will create a service import which is the opposite of a service export and we'll get to more of that in",
    "start": "2395200",
    "end": "2402079"
  },
  {
    "text": "a second and it will also create endpoint slices in your local cluster",
    "start": "2402079",
    "end": "2407680"
  },
  {
    "text": "which represent all of the back ends for this service across all of the clusters we'll get to that in a second too",
    "start": "2407680",
    "end": "2414880"
  },
  {
    "text": "in this case we just have the one cluster that's exporting the back-end service now clients in my front-end service can",
    "start": "2414880",
    "end": "2422720"
  },
  {
    "text": "access my backend service the same way that they would access any other service that was in their cluster",
    "start": "2422720",
    "end": "2429359"
  },
  {
    "text": "except it's not just in their cluster it's in a different cluster",
    "start": "2429359",
    "end": "2434160"
  },
  {
    "text": "so we looked at dns before so we'll look at dns again here similar to the main dns we have backend",
    "start": "2435280",
    "end": "2441520"
  },
  {
    "text": "service which is the name of my service we have backend which is the name of my namespace and we have a new suffix",
    "start": "2441520",
    "end": "2448400"
  },
  {
    "text": "this suffix here is to be decided on the name but right now we're running with super cluster uh we'll give you a different",
    "start": "2448400",
    "end": "2455359"
  },
  {
    "text": "zone and by simply changing the zone that you use from cluster.local to",
    "start": "2455359",
    "end": "2461079"
  },
  {
    "text": "supercluster.local you have now enabled multi-cluster services assuming that you have this controller",
    "start": "2461079",
    "end": "2467119"
  },
  {
    "text": "that is operating across your group",
    "start": "2467119",
    "end": "2470960"
  },
  {
    "text": "to make it even more fun that back-end service might exist in both clusters",
    "start": "2472560",
    "end": "2477599"
  },
  {
    "text": "in this case accessing the super cluster service will use back ends in either cluster",
    "start": "2477599",
    "end": "2483119"
  },
  {
    "text": "you can see there's a lot of lines in this drawing here but both clusters will have endpoints and endpoint slices that",
    "start": "2483119",
    "end": "2489359"
  },
  {
    "text": "represent the union of all of the back ends across both clusters",
    "start": "2489359",
    "end": "2495838"
  },
  {
    "text": "now to be fair this is mostly kepware right now there's some proof of concepts that exist we're still hammering out a lot of",
    "start": "2496800",
    "end": "2503839"
  },
  {
    "text": "details things like api and names we have a poll running on",
    "start": "2503839",
    "end": "2509200"
  },
  {
    "text": "what the proper suffix should be we're still working out some of the semantics for example conflict",
    "start": "2509200",
    "end": "2514880"
  },
  {
    "text": "resolution uh and propagation time and scalability uh we're still looking at things like cross-cluster",
    "start": "2514880",
    "end": "2522480"
  },
  {
    "text": "network policy which doesn't quite exist yet but we're excited about this work that's coming",
    "start": "2522480",
    "end": "2528079"
  },
  {
    "text": "again this is a collaboration with sig multi-cluster",
    "start": "2528079",
    "end": "2532400"
  },
  {
    "text": "so i also want to talk about some of the exciting work that's being worked on right now uh ipv4 and",
    "start": "2534079",
    "end": "2540240"
  },
  {
    "text": "ipv6 dual stack uh the reality of the world is that some",
    "start": "2540240",
    "end": "2545359"
  },
  {
    "text": "users need both ipv4 and ipv6 at the same time kubernetes has gotten",
    "start": "2545359",
    "end": "2551359"
  },
  {
    "text": "this far by assuming that there's a single ip address for pods we've also gotten this far by",
    "start": "2551359",
    "end": "2558800"
  },
  {
    "text": "assuming there's a single ip address for a service if we're going to exist in a dual",
    "start": "2558800",
    "end": "2564000"
  },
  {
    "text": "stack world we have to fix this this is a small change to the api uh but",
    "start": "2564000",
    "end": "2570720"
  },
  {
    "text": "it's actually very profound and the impact of it is pretty large now some of you may be asking wait",
    "start": "2570720",
    "end": "2576240"
  },
  {
    "text": "wasn't this done already didn't she talk about this before yes we did we got this to alpha and we",
    "start": "2576240",
    "end": "2581760"
  },
  {
    "text": "found a bunch of significant problems with it so we're doing a major reboot of the api so",
    "start": "2581760",
    "end": "2587119"
  },
  {
    "text": "i thought it was worthwhile to throw out the new api here so",
    "start": "2587119",
    "end": "2593440"
  },
  {
    "text": "starting with a plain old pod we had this now we're going to have this",
    "start": "2593440",
    "end": "2600000"
  },
  {
    "text": "so we're going to take the pod ip field and we're going to pluralize it and we've worked out a protocol for what",
    "start": "2600000",
    "end": "2605839"
  },
  {
    "text": "pluralization means within the api and it involves synchronizing the legacy",
    "start": "2605839",
    "end": "2612240"
  },
  {
    "text": "singular variable into the plural field it's always going to be the zeroth",
    "start": "2612240",
    "end": "2617760"
  },
  {
    "text": "element of the plural field so you can see here one two three four is the same in both cases and now we have a place where we can add",
    "start": "2617760",
    "end": "2624880"
  },
  {
    "text": "our secondary ip address similar to that you can see we have",
    "start": "2624880",
    "end": "2630960"
  },
  {
    "text": "nodes node has a single pod cider for a single ip family",
    "start": "2630960",
    "end": "2636640"
  },
  {
    "text": "that now becomes a plural in the same way that pod ips became a plural",
    "start": "2636640",
    "end": "2642800"
  },
  {
    "text": "again you see that the first the zeroth element maps to each other and now we can add the ipv6 address",
    "start": "2642800",
    "end": "2650319"
  },
  {
    "text": "alongside it now it gets a little bit more fun i've",
    "start": "2650319",
    "end": "2655760"
  },
  {
    "text": "got a service and type cluster ip and i've got a single cluster ip",
    "start": "2655760",
    "end": "2661040"
  },
  {
    "text": "this is going to become this which is a little bit more involved than what we saw before",
    "start": "2661040",
    "end": "2666640"
  },
  {
    "text": "you can see the same pattern of synchronization between cluster ip and cluster ips and you can see the",
    "start": "2666640",
    "end": "2673599"
  },
  {
    "text": "new field the ipv6 field being added but there's also a couple of extra fields",
    "start": "2673599",
    "end": "2678720"
  },
  {
    "text": "these fields are here to tell the service subsystem what did you want",
    "start": "2678720",
    "end": "2685599"
  },
  {
    "text": "you can see that there's a policy which lets uh the user express various things",
    "start": "2686800",
    "end": "2692319"
  },
  {
    "text": "i'll get to that in a second and an ip families list this is what has been",
    "start": "2692319",
    "end": "2697359"
  },
  {
    "text": "allocated to this service and what consumers who are using this service such as cubeproxy can use to route",
    "start": "2697359",
    "end": "2704319"
  },
  {
    "text": "traffic into this service there was a bunch of different requirements here",
    "start": "2704319",
    "end": "2710160"
  },
  {
    "text": "we needed users to be able to express things like i need single stack and not just i need single stack but i",
    "start": "2710160",
    "end": "2716000"
  },
  {
    "text": "need single stack and i need it to be specifically ipv6 it was also important that people be",
    "start": "2716000",
    "end": "2722079"
  },
  {
    "text": "able to express i need dual stack and if i don't have it i want you to fail",
    "start": "2722079",
    "end": "2728000"
  },
  {
    "text": "and in between those two people who would say i'd like dual stack if it's available but if it's not available that's okay",
    "start": "2728000",
    "end": "2735760"
  },
  {
    "text": "and so this api allows users to express that but if user doesn't express anything if",
    "start": "2735760",
    "end": "2742240"
  },
  {
    "text": "you take a service that exists in kubernetes today and bring it into this dual stack enabled cluster",
    "start": "2742240",
    "end": "2748079"
  },
  {
    "text": "you will get a single stack service this works for for vip services it works",
    "start": "2748079",
    "end": "2755599"
  },
  {
    "text": "for headless services it works for node ports and it works for load balancers assuming that your cloud",
    "start": "2755599",
    "end": "2760960"
  },
  {
    "text": "provider supports node load balancers on different protocols that's a cloud provider decision that",
    "start": "2760960",
    "end": "2767359"
  },
  {
    "text": "they're going to have to make so this work was alpha and it is alpha in kubernetes",
    "start": "2767359",
    "end": "2773200"
  },
  {
    "text": "1.18 and 1.19 we're shooting for a second alpha and 1.20 this is a breaking",
    "start": "2773200",
    "end": "2778720"
  },
  {
    "text": "change from the api point of view because we're still in alpha we can technically do that it's",
    "start": "2778720",
    "end": "2784800"
  },
  {
    "text": "unfortunate for people who have started using it and to all those people i apologize",
    "start": "2784800",
    "end": "2790160"
  },
  {
    "text": "hopefully we get it right this time i'd like to hand it back to bowie to",
    "start": "2790160",
    "end": "2795599"
  },
  {
    "text": "talk about what else is upcoming thanks tim so as part of uh thinking",
    "start": "2795599",
    "end": "2802319"
  },
  {
    "text": "about services we titled this services v plus one is we are looking",
    "start": "2802319",
    "end": "2807440"
  },
  {
    "text": "at sort of what the evolution of services is and if you think about it currently",
    "start": "2807440",
    "end": "2812720"
  },
  {
    "text": "kubernetes service is a number of things one is it's a method of exposure so",
    "start": "2812720",
    "end": "2818079"
  },
  {
    "text": "on the side you have here the hierarchy of different service types such as headless cluster ip node port",
    "start": "2818079",
    "end": "2825280"
  },
  {
    "text": "and load balancer in fact they imply for example a node load bouncer implies a node port",
    "start": "2825280",
    "end": "2831599"
  },
  {
    "text": "implies a cluster ip and so forth the second thing that a service does is sort of talk about groupings of pods",
    "start": "2831599",
    "end": "2838240"
  },
  {
    "text": "into a workload which is a service eg the selector and finally the service",
    "start": "2838240",
    "end": "2843599"
  },
  {
    "text": "uh resource contains things which is attributes about the service about various policies such as external",
    "start": "2843599",
    "end": "2849599"
  },
  {
    "text": "traffic policy session affinity and so forth now what we're finding is that evolving",
    "start": "2849599",
    "end": "2855040"
  },
  {
    "text": "and extending this resource has become harder and harder because all of these fields they sort of serve",
    "start": "2855040",
    "end": "2860240"
  },
  {
    "text": "different purposes but they as part of the same api they will interact in strange ways at the same",
    "start": "2860240",
    "end": "2867119"
  },
  {
    "text": "time there's been an evolution a desire to evolve the l7 ingress api as i said before",
    "start": "2867119",
    "end": "2873359"
  },
  {
    "text": "it was fairly bare bones and simplistic and a lot of the proxy vendors today",
    "start": "2873359",
    "end": "2880400"
  },
  {
    "text": "can support you know much more expressibility and we also want to incorporate as part of this",
    "start": "2880400",
    "end": "2887599"
  },
  {
    "text": "division of responsibility and resources that allows for modeling of different roles so it's",
    "start": "2887599",
    "end": "2893040"
  },
  {
    "text": "both what do we do about extending service further and also how do we describe some of",
    "start": "2893040",
    "end": "2899680"
  },
  {
    "text": "these things that we know exist and can be done",
    "start": "2899680",
    "end": "2904240"
  },
  {
    "text": "so the idea of services v plus one which is also known as the gateway api",
    "start": "2905599",
    "end": "2910640"
  },
  {
    "text": "is to sort of decouple these things along a role in concept access so uh we have a bunch of roles which is",
    "start": "2910640",
    "end": "2918960"
  },
  {
    "text": "and the apparently the uh emojis did not translate well into powerpoint",
    "start": "2918960",
    "end": "2924559"
  },
  {
    "text": "so there is supposed to be some you see the female sign is actually supposed to be an emoji so",
    "start": "2924559",
    "end": "2930720"
  },
  {
    "text": "there are a couple of roles that we're looking at in terms of modeling uh these resources there's an infrastructure provider which is",
    "start": "2930720",
    "end": "2937520"
  },
  {
    "text": "sort of the set of people that have set up your cloud environment your cloud",
    "start": "2937520",
    "end": "2942960"
  },
  {
    "text": "platform have created your cluster there's a cluster operator who are sort of the super user of an individual",
    "start": "2942960",
    "end": "2948720"
  },
  {
    "text": "cluster and or network operator who control say who can have access to internet access",
    "start": "2948720",
    "end": "2955119"
  },
  {
    "text": "and egress proxies and so forth and then finally there's the application developer",
    "start": "2955119",
    "end": "2960640"
  },
  {
    "text": "so what we want to do with the services api is to basically allow a user to express these roles and",
    "start": "2960640",
    "end": "2967760"
  },
  {
    "text": "sort of grant different permissions based on these different roles the second thing that we wanted to do in sort of service",
    "start": "2967760",
    "end": "2975440"
  },
  {
    "text": "refactoring is to separate these concepts into different resources",
    "start": "2975440",
    "end": "2980800"
  },
  {
    "text": "so that the evolution of each of the resources can be uh much easier to deal with so the three",
    "start": "2980800",
    "end": "2987680"
  },
  {
    "text": "concepts i said before are grouping and selection so this is sort of what workloads comprise a",
    "start": "2987680",
    "end": "2992880"
  },
  {
    "text": "service the second piece is how routing and protocol specific attributes will work so for l7 http this",
    "start": "2992880",
    "end": "2999599"
  },
  {
    "text": "will be for example that a given hostname and a given path will go to a particular",
    "start": "2999599",
    "end": "3004640"
  },
  {
    "text": "service and then finally the last part is how the service will be exposed and how it",
    "start": "3004640",
    "end": "3010160"
  },
  {
    "text": "will be accessed so i'll kind of go over a high level of what",
    "start": "3010160",
    "end": "3015920"
  },
  {
    "text": "this looks like in the api proposal so at the top we have and you'll see on",
    "start": "3015920",
    "end": "3021920"
  },
  {
    "text": "the right we have the various resources that are being developed",
    "start": "3021920",
    "end": "3027119"
  },
  {
    "text": "so at the top we have gateway class so gateway class is a resource intended for the",
    "start": "3027119",
    "end": "3032720"
  },
  {
    "text": "infrastructure provider and what a gateway class describes is it defines a kind of service access to a",
    "start": "3032720",
    "end": "3038079"
  },
  {
    "text": "cluster for example uh example gateway classes would be for example an internal proxy like a",
    "start": "3038079",
    "end": "3043920"
  },
  {
    "text": "proxy let's say nginx ingress that runs on the cluster or an internet lb for example a kind of",
    "start": "3043920",
    "end": "3051200"
  },
  {
    "text": "proxy that runs in the cloud provider infrastructure so similar to storage",
    "start": "3051200",
    "end": "3057200"
  },
  {
    "text": "class this abstracts the implementation of the mechanism for example an internal proxy may use a",
    "start": "3057200",
    "end": "3062480"
  },
  {
    "text": "specific version or deployment of al7 proxy",
    "start": "3062480",
    "end": "3068079"
  },
  {
    "text": "from the consumer so from the consumer standpoint they would just say i want to have an internal proxy or i",
    "start": "3068079",
    "end": "3074640"
  },
  {
    "text": "want to have an internet lb and then the cluster operator the infrastructure provider will create this",
    "start": "3074640",
    "end": "3080800"
  },
  {
    "text": "class sort of say hey you wanted an internet lb this is how we're going to actually",
    "start": "3080800",
    "end": "3086160"
  },
  {
    "text": "do it and below you can see an example of the configuration for a gateway class",
    "start": "3086160",
    "end": "3092480"
  },
  {
    "text": "you'll see that it has a controller so for example if acme io was",
    "start": "3092480",
    "end": "3098079"
  },
  {
    "text": "a proxy implementation and this would be the controller for",
    "start": "3098079",
    "end": "3103359"
  },
  {
    "text": "acme io and a way to pass controller specific parameters to it",
    "start": "3103359",
    "end": "3109040"
  },
  {
    "text": "but from the user standpoint for example people who create gateways and routes and so forth they will only know that this gateway",
    "start": "3109040",
    "end": "3115680"
  },
  {
    "text": "class corresponds to a in cluster gateway which is sort of the name here they wouldn't have to know",
    "start": "3115680",
    "end": "3122160"
  },
  {
    "text": "for example that in actuality that in-cluster gateway is implemented using an acme i o gateway controller",
    "start": "3122160",
    "end": "3131119"
  },
  {
    "text": "oh next we have the gateway resource so the gateway resource is what we would",
    "start": "3136839",
    "end": "3143680"
  },
  {
    "text": "typically think of controlled by the role of a cluster operator or network operator",
    "start": "3143680",
    "end": "3148880"
  },
  {
    "text": "and this governs how the service is accessed by the user eg what port it's on what protocol it's",
    "start": "3148880",
    "end": "3154319"
  },
  {
    "text": "on what addresses it uses this is a keystone resource what i mean by that is this will be one to one with",
    "start": "3154319",
    "end": "3160640"
  },
  {
    "text": "the lifecycle of some infrastructure action so the configuration of instruction will be triggered by the association of",
    "start": "3160640",
    "end": "3168480"
  },
  {
    "text": "a valid gateway with a class and a route and this could have many",
    "start": "3168480",
    "end": "3173599"
  },
  {
    "text": "uh implementations in terms of deployment it could spawn a software lb it could add a configuration stanza to",
    "start": "3173599",
    "end": "3179119"
  },
  {
    "text": "an lb it could program the sdn and so forth but basically the gateway sort of hooks",
    "start": "3179119",
    "end": "3184319"
  },
  {
    "text": "all of the resources together and provides that life cycle relationship now one thing to note is",
    "start": "3184319",
    "end": "3191599"
  },
  {
    "text": "that a gateway may be underspecified quote unquote for example there may be many defaults",
    "start": "3191599",
    "end": "3197119"
  },
  {
    "text": "that are sort of provided by a gateway class so the user can only needs to concern themselves with aspects of the gateway",
    "start": "3197119",
    "end": "3203440"
  },
  {
    "text": "that they care about and the gateway class will provide most of the other things so if we take a",
    "start": "3203440",
    "end": "3209040"
  },
  {
    "text": "look at an example gateway we have a gateway that has a class cluster gateway which we mentioned before",
    "start": "3209040",
    "end": "3215200"
  },
  {
    "text": "and we say that the user of this gateway says i need a gateway that has port 80 and then i'm going to associate a couple",
    "start": "3215200",
    "end": "3222079"
  },
  {
    "text": "of uh l7 routes with this but no more and the gateway cluster gateway",
    "start": "3222079",
    "end": "3227440"
  },
  {
    "text": "if it is able to do so will be able to populate the rest of the gateway uh such as the address",
    "start": "3227440",
    "end": "3234079"
  },
  {
    "text": "and where it's located finally we get to the route resources so",
    "start": "3234079",
    "end": "3240720"
  },
  {
    "text": "route resources are a protocol specific description of how to compose services together into a",
    "start": "3240720",
    "end": "3248319"
  },
  {
    "text": "overall uh l7 description now we",
    "start": "3248319",
    "end": "3254960"
  },
  {
    "text": "would typically expect the application to developer to be writing routes and this will be uh pretty standard in",
    "start": "3254960",
    "end": "3262079"
  },
  {
    "text": "terms of they will be for example talking about compositions such as search going to a search service in a store",
    "start": "3262079",
    "end": "3268079"
  },
  {
    "text": "going to sewer service and here you see a little asterisk there is that actually this is a family of",
    "start": "3268079",
    "end": "3273359"
  },
  {
    "text": "resources so it's a family resources group by protocol and use case to solve the issue of for",
    "start": "3273359",
    "end": "3280960"
  },
  {
    "text": "example in the api defining a single closed union type basically a single type that has all the",
    "start": "3280960",
    "end": "3287680"
  },
  {
    "text": "possible types enclosed in it that probably will make it very hard",
    "start": "3287680",
    "end": "3293280"
  },
  {
    "text": "to evolve it quickly for example the single api type needs to",
    "start": "3293280",
    "end": "3298400"
  },
  {
    "text": "talk about tcp needs to be talked about sni needs to talk about http and so forth it also precludes",
    "start": "3298400",
    "end": "3304960"
  },
  {
    "text": "extensibility by for example an infrastructure provider providing their own routing types",
    "start": "3304960",
    "end": "3311440"
  },
  {
    "text": "for safe special protocol that you know wouldn't make sense to put into the standard common core",
    "start": "3311440",
    "end": "3320319"
  },
  {
    "text": "now what about service so service is at the bottom so we see that service probably should",
    "start": "3320319",
    "end": "3326720"
  },
  {
    "text": "retain its place as a grouping and selection mechanism and of course since the v1 functionality",
    "start": "3326720",
    "end": "3333119"
  },
  {
    "text": "is ga that will still work but hopefully with this new decomposition of the resources we won't have to add",
    "start": "3333119",
    "end": "3340000"
  },
  {
    "text": "significantly to the existing surface area and that'll really help us evolve things",
    "start": "3340000",
    "end": "3345839"
  },
  {
    "text": "as we go forward so here we have an example of sort of how all these things are hooked",
    "start": "3345839",
    "end": "3352079"
  },
  {
    "text": "up we have a gateway class that is referenced by a gateway and the gateway",
    "start": "3352079",
    "end": "3358079"
  },
  {
    "text": "references other routes in green and then finally the routes route each of the interesting paths and protocol specific",
    "start": "3358079",
    "end": "3366160"
  },
  {
    "text": "matches to their uh respective services",
    "start": "3366160",
    "end": "3371760"
  },
  {
    "text": "so we're still working on the v1 alpha one but this is a cut list of the things that we really",
    "start": "3372559",
    "end": "3378400"
  },
  {
    "text": "want to get done we want to show demonstrate basic applications and the data types which is",
    "start": "3378400",
    "end": "3383760"
  },
  {
    "text": "the resources we want to have show that gateway class can be used for interoperation between",
    "start": "3383760",
    "end": "3389280"
  },
  {
    "text": "controllers so we want to have two controllers or end controllers and then sort of switch the classes between them and see that",
    "start": "3389280",
    "end": "3395119"
  },
  {
    "text": "working support basic support for http and tcp has two examples one at l4",
    "start": "3395119",
    "end": "3401760"
  },
  {
    "text": "one at l7 of course https and then finally implementability we",
    "start": "3401760",
    "end": "3407040"
  },
  {
    "text": "want to have implementations demonstrate a wide range of how it's being done",
    "start": "3407040",
    "end": "3412720"
  },
  {
    "text": "so there's the merging style which is multiple gateways hosted in a single proxy infrastructure",
    "start": "3412720",
    "end": "3417760"
  },
  {
    "text": "and then there's also the cloud provisioning style in which gateways are mapped to the externally managed resources",
    "start": "3417760",
    "end": "3425119"
  },
  {
    "text": "finally i'll hand it over to tim to wrap it all up thanks bowie uh so",
    "start": "3426000",
    "end": "3434000"
  },
  {
    "text": "we're getting close to an hour of talking and i know that there's a ton of details that we've thrown at everybody",
    "start": "3434000",
    "end": "3440559"
  },
  {
    "text": "uh i want to say this is just some of what we're up to uh the sig at large is dozens of people",
    "start": "3440559",
    "end": "3447040"
  },
  {
    "text": "who are working and sending ideas and new features and writing caps it's amazing and",
    "start": "3447040",
    "end": "3453920"
  },
  {
    "text": "overwhelming how much has been happening in sig network this year so i want to send a special thanks",
    "start": "3453920",
    "end": "3459440"
  },
  {
    "text": "to everybody who's part of c network for doing all of this amazing work",
    "start": "3459440",
    "end": "3465280"
  },
  {
    "text": "we have an issue tracker of course we do you can go to issues.kates.io and you",
    "start": "3466240",
    "end": "3472720"
  },
  {
    "text": "can file bugs if you find them you can go look for cleanup ideas if you want to contribute",
    "start": "3472720",
    "end": "3478160"
  },
  {
    "text": "um or you can file feature requests or come and help us with things like triage this is",
    "start": "3478160",
    "end": "3484720"
  },
  {
    "text": "something we do on a bi-weekly basis with our sig calls which again we'll post the zoom link",
    "start": "3484720",
    "end": "3490079"
  },
  {
    "text": "in just a minute come and help the issues are a great place for people",
    "start": "3490079",
    "end": "3496000"
  },
  {
    "text": "to get started and get involved come and look for the problems you've had or help other users with the problems",
    "start": "3496000",
    "end": "3503119"
  },
  {
    "text": "that they're having we also have a repository for our",
    "start": "3503119",
    "end": "3508319"
  },
  {
    "text": "enhancements our keps our kubernetes enhancement proposals these are user-visible changes usually",
    "start": "3508319",
    "end": "3514400"
  },
  {
    "text": "features but sometimes they can also be non-functional changes like scalability",
    "start": "3514400",
    "end": "3519599"
  },
  {
    "text": "but mostly these are usable user visible things we welcome anybody who's watching to",
    "start": "3519599",
    "end": "3526079"
  },
  {
    "text": "participate in our enhancement conversations and the planning we always welcome more eyeballs help us find",
    "start": "3526079",
    "end": "3533200"
  },
  {
    "text": "new problems ask great questions or come and submit an enhancement proposal of your own",
    "start": "3533200",
    "end": "3538400"
  },
  {
    "text": "if you've got a problem with kubernetes we want to hear about it",
    "start": "3538400",
    "end": "3542480"
  },
  {
    "text": "lastly to get involved with our sig you can find all of our information at our sig",
    "start": "3543599",
    "end": "3548960"
  },
  {
    "text": "network community repo we do meet every other thursday at 2100",
    "start": "3548960",
    "end": "3554319"
  },
  {
    "text": "utc we have a slack channel sig network on the slack.kates.ioslac server",
    "start": "3554319",
    "end": "3559839"
  },
  {
    "text": "and we have a mailing list so there's got to be something in here that works for you to get a hold of us and come have a",
    "start": "3559839",
    "end": "3566000"
  },
  {
    "text": "conversation with that i want to say thank you to",
    "start": "3566000",
    "end": "3571040"
  },
  {
    "text": "everybody who came today and thanks to bowie for all of your hard work on this",
    "start": "3571040",
    "end": "3587440"
  },
  {
    "text": "hi everybody i guess we're live now",
    "start": "3587440",
    "end": "3591440"
  },
  {
    "text": "so we have a bunch of questions boy i don't know if you're a mute i think um",
    "start": "3593359",
    "end": "3600319"
  },
  {
    "text": "there we go uh they we've done some rough categorization of them while people uh were asking them so i'll just go",
    "start": "3600319",
    "end": "3606960"
  },
  {
    "text": "through and i'll read the questions out and we'll answer them and feel free to submit new",
    "start": "3606960",
    "end": "3612319"
  },
  {
    "text": "questions and there are people prioritizing those two we'll try to get to as many as we can and afterwards we'll jump to the slack channel",
    "start": "3612319",
    "end": "3620480"
  },
  {
    "text": "question our team is using gke and we had multiple problems running out of contract we had to deploy a demon set to",
    "start": "3620480",
    "end": "3626799"
  },
  {
    "text": "customize it for our needs and there was some fallout questions that uh the usual answer of node local dns",
    "start": "3626799",
    "end": "3632400"
  },
  {
    "text": "uh isn't solving the problem um so we've seen it with a small number of",
    "start": "3632400",
    "end": "3638559"
  },
  {
    "text": "users um not a large number of people actually run out of the contract uh records we set up a lot of records um",
    "start": "3638559",
    "end": "3647680"
  },
  {
    "text": "there isn't a general answer what you're doing with the demon set is is a good way of customizing nodes um",
    "start": "3647680",
    "end": "3654240"
  },
  {
    "text": "i'd love to talk to you and understand exactly what sort of records are uh being stuck around in your system",
    "start": "3654240",
    "end": "3660799"
  },
  {
    "text": "udp is generally a culprit here but if it's not udp i'd love to",
    "start": "3660799",
    "end": "3665920"
  },
  {
    "text": "understand what exactly uh is happening in your system so we can see if there's a more general way of solving",
    "start": "3665920",
    "end": "3672400"
  },
  {
    "text": "the problem yeah i think we've found that mostly",
    "start": "3672400",
    "end": "3678240"
  },
  {
    "text": "the contract is actually pretty high so there must be something special that your workload is hitting yeah",
    "start": "3678240",
    "end": "3686079"
  },
  {
    "text": "unfortunately it's hard to do contract very dynamically the kernel does a lot of work when you change the size of the contract tables",
    "start": "3686079",
    "end": "3694079"
  },
  {
    "text": "next question when i deploy a load balancer ip if i don't specify an external ip will i still be able to",
    "start": "3694079",
    "end": "3700559"
  },
  {
    "text": "access it outside of the cluster load balancer services are",
    "start": "3700559",
    "end": "3706640"
  },
  {
    "text": "quote external to the cluster so the idea is yes you'd be able to access that load",
    "start": "3706640",
    "end": "3712720"
  },
  {
    "text": "balancer ip from outside the cluster what exactly that means really depends on the implementation",
    "start": "3712720",
    "end": "3718000"
  },
  {
    "text": "so for example on google cloud there are two different kinds of load balancers that you can choose",
    "start": "3718000",
    "end": "3723119"
  },
  {
    "text": "one which is external to the cluster but internal to your private network and the",
    "start": "3723119",
    "end": "3729280"
  },
  {
    "text": "other which is on the internet",
    "start": "3729280",
    "end": "3732799"
  },
  {
    "text": "you want me to read the next question tim i'm just seeing if you have anything to add",
    "start": "3735359",
    "end": "3741760"
  },
  {
    "text": "next question if my service has three pods and q proxy runs in ip tables mode i.e 33",
    "start": "3741760",
    "end": "3747200"
  },
  {
    "text": "randomness how does it cope if one pod crashes how long is the delay before it",
    "start": "3747200",
    "end": "3752559"
  },
  {
    "text": "figures out one service pod is down um so the delay is generally as long as",
    "start": "3752559",
    "end": "3760240"
  },
  {
    "text": "you've set your readiness probe on your pod uh so if your pot is down",
    "start": "3760240",
    "end": "3766240"
  },
  {
    "text": "but alive like it's running but it's not serving for some reason uh your readiness probe will trigger the",
    "start": "3766240",
    "end": "3772559"
  },
  {
    "text": "update to the pod uh generally readiness probes are on the order of single digit to two digit",
    "start": "3772559",
    "end": "3777599"
  },
  {
    "text": "seconds uh most people can figure them that way um so it will be a few seconds before it's removed from",
    "start": "3777599",
    "end": "3784559"
  },
  {
    "text": "the service if you don't have a readiness probe a you should have one and b uh",
    "start": "3784559",
    "end": "3792240"
  },
  {
    "text": "well we won't know if your pod actually crashes uh we'll know pretty quickly because it",
    "start": "3792240",
    "end": "3798160"
  },
  {
    "text": "will no longer be something that the service controller is selecting so that that is the quickest way of getting it removed from the set",
    "start": "3798160",
    "end": "3805039"
  },
  {
    "text": "yeah and then i think uh there's also a work on getting sli slos for how long it",
    "start": "3805039",
    "end": "3811359"
  },
  {
    "text": "takes to propagate and if you're curious about details you can ping me on slack",
    "start": "3811359",
    "end": "3818558"
  },
  {
    "text": "yeah so this is in addition to your readiness probe right next question uh was regarding",
    "start": "3818799",
    "end": "3825680"
  },
  {
    "text": "ingress why not include a controller now that we know better oh you want to handle that why not put a",
    "start": "3825680",
    "end": "3833039"
  },
  {
    "text": "controller in the box yeah it's it's an interesting question i think it still returns back to the fact",
    "start": "3833039",
    "end": "3839200"
  },
  {
    "text": "that there are many controllers and there are many that are quite popular it's sort of tough to sort of pick a",
    "start": "3839200",
    "end": "3846160"
  },
  {
    "text": "single one i know that i think the ingress engine x is quite popular but there's",
    "start": "3846160",
    "end": "3851680"
  },
  {
    "text": "there's quite a few and you know including those by vendors and those in oss",
    "start": "3851680",
    "end": "3857760"
  },
  {
    "text": "yeah it's going to be very hard to choose my fear with putting one in the box now is most people have already compensated",
    "start": "3857760",
    "end": "3864319"
  },
  {
    "text": "for this either in the distributions or in their own clusters putting one in the box seems like a",
    "start": "3864319",
    "end": "3869440"
  },
  {
    "text": "recipe for breaking people um and you know it may be that it's just too far gone to to try to fix that",
    "start": "3869440",
    "end": "3877760"
  },
  {
    "text": "and i think oh yeah i just moved that one out um",
    "start": "3877760",
    "end": "3884400"
  },
  {
    "text": "for years people talked about ipv6 as a replacement for ipv4 yes literally decades uh why kubernetes pods are still",
    "start": "3884400",
    "end": "3891920"
  },
  {
    "text": "using v4 by default and not v6 v4 by default because it's ubiquitous",
    "start": "3891920",
    "end": "3897680"
  },
  {
    "text": "we know it works everywhere for example all the cloud providers um but the dual stack support is coming",
    "start": "3897680",
    "end": "3903520"
  },
  {
    "text": "along and i suspect that that will get a lot more traction once it's available",
    "start": "3903520",
    "end": "3909838"
  },
  {
    "text": "will service meshes be integrated into kubernetes networking sig in the future so uh i'm on record",
    "start": "3911200",
    "end": "3918960"
  },
  {
    "text": "saying i think kubernetes is already a service mesh is becoming a service mesh is taking on more service mesh-like",
    "start": "3918960",
    "end": "3924559"
  },
  {
    "text": "properties um if you look at the gateway work which i'll let bowie speak to you start to see a lot more affordances",
    "start": "3924559",
    "end": "3931280"
  },
  {
    "text": "for things like service meshes i don't think we're going to like absorb istio",
    "start": "3931280",
    "end": "3936880"
  },
  {
    "text": "or linker d or or any of the service meshes um i do think kubernetes apis will start",
    "start": "3936880",
    "end": "3943200"
  },
  {
    "text": "to cover more service mesh like functionality right yeah i think that's right like it's hard",
    "start": "3943200",
    "end": "3950079"
  },
  {
    "text": "to say if a service mesh is an api or an implementation i'd say as an api",
    "start": "3950079",
    "end": "3955599"
  },
  {
    "text": "gateway may grow up to cover some of the same api space which is great because if it's",
    "start": "3955599",
    "end": "3963680"
  },
  {
    "text": "portable as part of kubernetes it's going to be more you know universally accessible uh specific service mesh implementations",
    "start": "3963680",
    "end": "3970720"
  },
  {
    "text": "on the implementation side right then hopefully the service meshes will be able to support these apis along with their own",
    "start": "3970720",
    "end": "3977280"
  },
  {
    "text": "and then the idea is that gateway is extensible so you might be able to get away with using the",
    "start": "3977280",
    "end": "3983839"
  },
  {
    "text": "portable api rather than having to have a custom one",
    "start": "3983839",
    "end": "3988480"
  },
  {
    "text": "next question have you considered evpf instead of iptables for the default um we've thought about it evpf",
    "start": "3990000",
    "end": "3997760"
  },
  {
    "text": "is uh more complicated and honestly my personal preference is to",
    "start": "3997760",
    "end": "4003599"
  },
  {
    "text": "decouple more uh cube proxy as a monolith has sort of become a problem from a",
    "start": "4003599",
    "end": "4009119"
  },
  {
    "text": "software management point of view um so i'm actually a fan of the fact that projects like psyllium are",
    "start": "4009119",
    "end": "4015359"
  },
  {
    "text": "decoupled from the core project i think it forces us to be a little bit more um",
    "start": "4015359",
    "end": "4020720"
  },
  {
    "text": "honest about compatibility so thought about it i don't think we make ebpf the default in the project",
    "start": "4020720",
    "end": "4027760"
  },
  {
    "text": "we might end of life cube proxy eventually and tell people that uh something like",
    "start": "4027760",
    "end": "4033200"
  },
  {
    "text": "psyllium is a better answer right",
    "start": "4033200",
    "end": "4037359"
  },
  {
    "text": "is node local dns enabled by default from 1.18 or do you need to enable it boeing it",
    "start": "4038319",
    "end": "4044319"
  },
  {
    "text": "depends on your platform so uh i think someone just checked it looks like it's enabled on",
    "start": "4044319",
    "end": "4050400"
  },
  {
    "text": "some platforms like eks i think but it is not it really it does take up some resources so for",
    "start": "4050400",
    "end": "4056960"
  },
  {
    "text": "very small clusters it probably doesn't make sense they won't hit that scale anyways but the way it's implemented it is a",
    "start": "4056960",
    "end": "4062319"
  },
  {
    "text": "good especially with the aha and transparency is a good easy drop in if you hit",
    "start": "4062319",
    "end": "4067920"
  },
  {
    "text": "dns issues next question some network control plane",
    "start": "4067920",
    "end": "4075599"
  },
  {
    "text": "implementations allow is based enforcement of network flows like nsxt",
    "start": "4075599",
    "end": "4081599"
  },
  {
    "text": "how do you feel about that way of doing traffic security versus internal policy systems um i don't know what internal here means",
    "start": "4081599",
    "end": "4088720"
  },
  {
    "text": "what i care about is consistent apis and for example network policy",
    "start": "4088720",
    "end": "4094160"
  },
  {
    "text": "uh if you can implement network policy in terms of some other ias system",
    "start": "4094160",
    "end": "4099278"
  },
  {
    "text": "fantastic but i would for most users who are trying to get portability with",
    "start": "4099279",
    "end": "4104880"
  },
  {
    "text": "kubernetes i would say um relying on is systems like that for security",
    "start": "4104880",
    "end": "4110960"
  },
  {
    "text": "directly uh is going to break your portability goals so apis apis apis",
    "start": "4110960",
    "end": "4119838"
  },
  {
    "text": "uh what happens if a cached vip is no longer valid if the associated pod",
    "start": "4120560",
    "end": "4126000"
  },
  {
    "text": "is crashed i assume that's in context of the node local dns uh we set the ttls",
    "start": "4126000",
    "end": "4134000"
  },
  {
    "text": "pretty low so caches will turn over reasonably quickly generally on the order of tens of",
    "start": "4134000",
    "end": "4140880"
  },
  {
    "text": "seconds i think right yeah and generally dns had does",
    "start": "4140880",
    "end": "4146000"
  },
  {
    "text": "have that caching issue so if you're that's probably why q proxy exists in the first place",
    "start": "4146000",
    "end": "4152880"
  },
  {
    "text": "it is exactly why cube proxy exists uh what will be the most recommended way to",
    "start": "4152880",
    "end": "4159359"
  },
  {
    "text": "load balance grpc services in kubernetes oh you want to talk about this one it's",
    "start": "4159359",
    "end": "4164560"
  },
  {
    "text": "an l7 uh protocol and it's something that we expect to be",
    "start": "4164560",
    "end": "4170238"
  },
  {
    "text": "the sort of thing we expect to be covered by gateway so i know that it load bounces on hostname",
    "start": "4170239",
    "end": "4175278"
  },
  {
    "text": "as i understand it low bounces on hostname and path sort of like htp and that's sort of where the future will",
    "start": "4175279",
    "end": "4182400"
  },
  {
    "text": "go right now you can use some ingress implementations i think may support it",
    "start": "4182400",
    "end": "4187679"
  },
  {
    "text": "but it will be an extension and there's some work now happening for proxy list grpc where you can use",
    "start": "4187679",
    "end": "4195360"
  },
  {
    "text": "a more mesh like control signal to run your grpc services without",
    "start": "4195360",
    "end": "4201440"
  },
  {
    "text": "proxies next question how will services v plus",
    "start": "4201440",
    "end": "4206480"
  },
  {
    "text": "one interact with cloud lbs or customer lbs like metal lb will there be improvements to",
    "start": "4206480",
    "end": "4212159"
  },
  {
    "text": "them if they integrate with the new api boy that's you yeah at least for the",
    "start": "4212159",
    "end": "4217840"
  },
  {
    "text": "cloud lbs uh we definitely it's in this target of what will be supporting the",
    "start": "4217840",
    "end": "4225040"
  },
  {
    "text": "apis i'm less familiar with metal lb and how it fits in the picture mostly because uh it will be relevant",
    "start": "4225040",
    "end": "4232480"
  },
  {
    "text": "from the l4 i just don't know what the metal lb l7 story is so it to",
    "start": "4232480",
    "end": "4239760"
  },
  {
    "text": "it would be great if you ping me on slack we can follow up there i'm just less familiar with metal lb",
    "start": "4239760",
    "end": "4247360"
  },
  {
    "text": "services across clusters can it be via wan is there any latency limitation there's no intrinsic limitation right",
    "start": "4248400",
    "end": "4255920"
  },
  {
    "text": "now kubernetes does not really respect topology so if you have two clusters",
    "start": "4255920",
    "end": "4261280"
  },
  {
    "text": "in two different cloud regions for example or if you have them linked across a lan it will happily spread traffic across",
    "start": "4261280",
    "end": "4268400"
  },
  {
    "text": "that link which is probably not what you really wanted so right now the the implementations",
    "start": "4268400",
    "end": "4273440"
  },
  {
    "text": "that we're looking at are assuming same uh latency zone um kubernetes itself will be growing",
    "start": "4273440",
    "end": "4281440"
  },
  {
    "text": "better regionality awareness and so uh the vast majority of your traffic",
    "start": "4281440",
    "end": "4286480"
  },
  {
    "text": "should stay local to your own zone and which case linking up the awan is a",
    "start": "4286480",
    "end": "4291520"
  },
  {
    "text": "completely appropriate thing to do",
    "start": "4291520",
    "end": "4295120"
  },
  {
    "text": "some projects have a custom ingress service like k native will they be able to leverage services v plus one yeah actually",
    "start": "4296960",
    "end": "4304239"
  },
  {
    "text": "that's we are working with k native to make sure that they're compatible so we had some people in the",
    "start": "4304239",
    "end": "4311360"
  },
  {
    "text": "the working group on gateway basically talk to the k native folks and then we had some back and forth so",
    "start": "4311360",
    "end": "4317440"
  },
  {
    "text": "that's definitely something we're looking at",
    "start": "4317440",
    "end": "4321678"
  },
  {
    "text": "next question do you know how the network plug-ins calico psyllium stand with dual stack uh calico i",
    "start": "4322480",
    "end": "4329840"
  },
  {
    "text": "believe already supports dual stack it just can't expose it through kubernetes because we lack the api",
    "start": "4329840",
    "end": "4335760"
  },
  {
    "text": "um but you can create pods with two ip addresses uh psyllium i don't know for sure but",
    "start": "4335760",
    "end": "4341600"
  },
  {
    "text": "i'd be surprised if it doesn't have some form of that also um and i imagine most the others will be",
    "start": "4341600",
    "end": "4348560"
  },
  {
    "text": "relatively straightforward to integrate favorite question what's the time frame",
    "start": "4348560",
    "end": "4354480"
  },
  {
    "text": "for the new service api when can i actually",
    "start": "4354480",
    "end": "4359678"
  },
  {
    "text": "actually we have a goal of v1 alpha one and if you attend the meetings or take a look at the notes we're basically burning down",
    "start": "4360080",
    "end": "4367040"
  },
  {
    "text": "the features the idea is to sort of get the out something out uh in the user's hands to get feedback i",
    "start": "4367040",
    "end": "4374159"
  },
  {
    "text": "think that's the most valuable at this point um some things you'll like some things you'll hate and we would love to hear",
    "start": "4374159",
    "end": "4380400"
  },
  {
    "text": "all that feedback how will observability be addressed in",
    "start": "4380400",
    "end": "4386239"
  },
  {
    "text": "service view plus one yeah i had a answer to that historically",
    "start": "4386239",
    "end": "4392800"
  },
  {
    "text": "observability has been orthogonal so um ping me on slack because i kind of",
    "start": "4392800",
    "end": "4398800"
  },
  {
    "text": "understand understand which part of observatory you're talking about there are some aspects where we",
    "start": "4398800",
    "end": "4403920"
  },
  {
    "text": "are adding sort of extra hooks to put to describe observability such as",
    "start": "4403920",
    "end": "4410320"
  },
  {
    "text": "request mirroring but the other aspect which is like what metrics would be published that's a separate",
    "start": "4410320",
    "end": "4416080"
  },
  {
    "text": "thing that i think we haven't looked deeply into but be interesting to sort of get some ideas there",
    "start": "4416080",
    "end": "4426400"
  },
  {
    "text": "so if i already have an ingress controller in place will i be able to replace it with services v plus one",
    "start": "4426400",
    "end": "4434239"
  },
  {
    "text": "well we certainly hope so and we're working with all the uh controller authors so we're trying to",
    "start": "4434239",
    "end": "4440960"
  },
  {
    "text": "we did a bunch of reach out to uh the major controller authors to get them to participate",
    "start": "4440960",
    "end": "4446800"
  },
  {
    "text": "in the proposal and the hope is that yes eventually you will be able to use the",
    "start": "4446800",
    "end": "4452560"
  },
  {
    "text": "api will gateway support mtls rate limits allow listing etc more",
    "start": "4452840",
    "end": "4458800"
  },
  {
    "text": "l7 features yeah that is the idea is we are both making it more extensible",
    "start": "4458800",
    "end": "4464960"
  },
  {
    "text": "and also some of the what is now table stakes in terms of",
    "start": "4464960",
    "end": "4471120"
  },
  {
    "text": "l7 stuff will hopefully be able to be incorporated as a core part of the api",
    "start": "4471120",
    "end": "4478559"
  },
  {
    "text": "with the current ingress resource i would register the ingress controllers load balancer ip with a wild card domain",
    "start": "4479920",
    "end": "4486960"
  },
  {
    "text": "how would that translate to future gateway class uh there's two one of them is that if",
    "start": "4486960",
    "end": "4493679"
  },
  {
    "text": "you are i think i might misread the question when i answer this when you're if you're using a feature",
    "start": "4493679",
    "end": "4499040"
  },
  {
    "text": "like a wildcard domain it should be uh the gateway the new api should be",
    "start": "4499040",
    "end": "4505280"
  },
  {
    "text": "a superset so those features should continue to be like you could be able to express them",
    "start": "4505280",
    "end": "4510719"
  },
  {
    "text": "the other question is how will it interact potentially with something like external",
    "start": "4510719",
    "end": "4516320"
  },
  {
    "text": "dns or actually search management as well and that is an area that we're",
    "start": "4516320",
    "end": "4523199"
  },
  {
    "text": "explicitly looking at is that gateway and service api is not just as a way to express things but as also a",
    "start": "4523199",
    "end": "4530640"
  },
  {
    "text": "layer that people build on top i know that for ingress api uh for example cert manager will build",
    "start": "4530640",
    "end": "4537120"
  },
  {
    "text": "will look at your ingress resource and then do cert management stuff and that's that's the kind of use case that we are also",
    "start": "4537120",
    "end": "4543360"
  },
  {
    "text": "thinking of uh as part of its design",
    "start": "4543360",
    "end": "4547679"
  },
  {
    "text": "so if gateway will support mtls etc will it compete with or replace service",
    "start": "4549040",
    "end": "4554719"
  },
  {
    "text": "meshes so this is the the service mesh topic um again apis and implementation should",
    "start": "4554719",
    "end": "4563600"
  },
  {
    "text": "be thought of differently so if kubernetes absorbs these sort of richer apis that doesn't mean we ship an",
    "start": "4563600",
    "end": "4570159"
  },
  {
    "text": "implementation so if you want to implement your rate limiting api with istio or with some other service",
    "start": "4570159",
    "end": "4577760"
  },
  {
    "text": "mesh that's fine right it's a pluginable system you want to add to that bowie yeah i do",
    "start": "4577760",
    "end": "4584320"
  },
  {
    "text": "have to just reiterate that right there's the implementation there's the apis and it seems like",
    "start": "4584320",
    "end": "4590080"
  },
  {
    "text": "many of the service mesh concepts it's great so then they're sort of being absorbed into sort of the base layer",
    "start": "4590080",
    "end": "4596320"
  },
  {
    "text": "that's a natural progression of the system in terms of implementation then you know it will be like your favorite",
    "start": "4596320",
    "end": "4604080"
  },
  {
    "text": "thing will remain relevant can you give some more pointers on no",
    "start": "4604080",
    "end": "4611040"
  },
  {
    "text": "track with regard to node local dns uh yes although i",
    "start": "4611040",
    "end": "4617600"
  },
  {
    "text": "this probably will be worth just discussing one-on-one so please ping me on slack i don't know which aspect of no track",
    "start": "4617600",
    "end": "4624320"
  },
  {
    "text": "you might be curious about so the short answer is you can tell the",
    "start": "4624320",
    "end": "4631600"
  },
  {
    "text": "kernel at the beginning of a packet's lifetime don't track this packet any further and it won't go through the rest of",
    "start": "4631600",
    "end": "4637840"
  },
  {
    "text": "the connection tracking subsystem which means it isn't subject to the normal",
    "start": "4637840",
    "end": "4643440"
  },
  {
    "text": "kubernetes load balancing the service load balancing",
    "start": "4643440",
    "end": "4649199"
  },
  {
    "text": "which means you don't create contract records for it um which is fine if you know that the back end is always a stable ip address like",
    "start": "4649199",
    "end": "4655440"
  },
  {
    "text": "the same host yeah it's like a two for one it uh it gives us the aha properties and",
    "start": "4655440",
    "end": "4662400"
  },
  {
    "text": "also it skips contract and contract for dns is one of the prime",
    "start": "4662400",
    "end": "4668159"
  },
  {
    "text": "culprits that we've seen for people who are having contract troubles uh because dns is udp",
    "start": "4668159",
    "end": "4673760"
  },
  {
    "text": "so there's no connection to close so you just have to time out those connections and because they tend to be one request",
    "start": "4673760",
    "end": "4681280"
  },
  {
    "text": "one response and then the client opens a new socket uh we're getting we have just one minute",
    "start": "4681280",
    "end": "4687040"
  },
  {
    "text": "left so uh we're at almost at the end of our questions",
    "start": "4687040",
    "end": "4692080"
  },
  {
    "text": "what is the status of nf tables as a back end for cube proxy uh there is an nf tables cube proxy uh",
    "start": "4692080",
    "end": "4699280"
  },
  {
    "text": "project that is happening out of the core tree i said earlier i don't like the um",
    "start": "4699280",
    "end": "4705920"
  },
  {
    "text": "monolithic cube proxy from a software engineering point of view at this point so they're doing it out of tree",
    "start": "4705920",
    "end": "4711679"
  },
  {
    "text": "if your system is using ip tables on top of nf tables then cube proxy will just use that",
    "start": "4711679",
    "end": "4720080"
  },
  {
    "text": "we have more i think there's actually there's actually two implementations i think uh so this",
    "start": "4720080",
    "end": "4725840"
  },
  {
    "text": "is an interesting space to watch uh do we have more documentation on",
    "start": "4725840",
    "end": "4731120"
  },
  {
    "text": "gateway boeing uh i just sent a link it's a kubernetes sigs",
    "start": "4731120",
    "end": "4736239"
  },
  {
    "text": "service apis and as part of the v1 alpha one we will be sprucing up the docs i think we were",
    "start": "4736239",
    "end": "4742080"
  },
  {
    "text": "trying to get everything in so the docs might be a little stale but they should be updated in the coming",
    "start": "4742080",
    "end": "4748480"
  },
  {
    "text": "weeks oops let me make this and the very last",
    "start": "4748480",
    "end": "4753600"
  },
  {
    "text": "question that came in was kubernetes native support for multiple network interfaces um it's still something we're looking at",
    "start": "4753600",
    "end": "4760159"
  },
  {
    "text": "it's not high on the agenda right this very minute but it is something i see as",
    "start": "4760159",
    "end": "4766239"
  },
  {
    "text": "eventually inevitable",
    "start": "4766239",
    "end": "4769360"
  },
  {
    "text": "keep the conversation going visit number two cubecon maintainer on our slack workspace",
    "start": "4773600",
    "end": "4779520"
  },
  {
    "text": "please be sure to rate the session five stars thanks everyone we're gonna move to",
    "start": "4779520",
    "end": "4786719"
  },
  {
    "text": "slack thanks we'll see you all there",
    "start": "4786719",
    "end": "4791760"
  }
]