[
  {
    "text": "e",
    "start": "28960",
    "end": "31960"
  },
  {
    "text": "read",
    "start": "42280",
    "end": "44840"
  },
  {
    "text": "sh",
    "start": "58920",
    "end": "61920"
  },
  {
    "text": "that",
    "start": "88840",
    "end": "91840"
  },
  {
    "text": "hey everyone uh so we'll just start the session uh so first let me introduce",
    "start": "98799",
    "end": "105240"
  },
  {
    "text": "myself I'm Rohit uh I'm co-founder and VP engineering at fex Cloud uh fex cloud",
    "start": "105240",
    "end": "112000"
  },
  {
    "text": "is a platform engineering product basically what we offer is a product where you can you get a canvas you can",
    "start": "112000",
    "end": "119880"
  },
  {
    "text": "drag drop in components and uh build a blueprint for your software architecture",
    "start": "119880",
    "end": "125240"
  },
  {
    "text": "and using that architecture you can uh you know create and manage environment so that's a short introduction to our",
    "start": "125240",
    "end": "131800"
  },
  {
    "text": "product you can check out our website and everything uh but today why we are here is to discuss specifically about uh",
    "start": "131800",
    "end": "139280"
  },
  {
    "text": "Loki uh so uh to introduce the other talkers",
    "start": "139280",
    "end": "145120"
  },
  {
    "text": "here so we have srijit so he is a devops lead at capillary techn te Oles uh he he",
    "start": "145120",
    "end": "152480"
  },
  {
    "text": "has been at cap Technologies for like nearly a decade and he has seen through uh all sort of modernization efforts and",
    "start": "152480",
    "end": "160120"
  },
  {
    "text": "you know Loki adoption is just one amongst them and fortunately like we could work with Capal Technologies as",
    "start": "160120",
    "end": "165800"
  },
  {
    "text": "one of the early adopters of uh the facets implementation of uh Loki uh and",
    "start": "165800",
    "end": "172200"
  },
  {
    "text": "promode who is a lead at uh facets uh he uh he joined uh so he uh was responsible",
    "start": "172200",
    "end": "180480"
  },
  {
    "text": "for developing the uh Loki offering in facets and uh we co-built it with uh the",
    "start": "180480",
    "end": "186120"
  },
  {
    "text": "capillary folks and sjit was very heavily involved in uh in that and a lot of his feedback and learnings of uh",
    "start": "186120",
    "end": "192480"
  },
  {
    "text": "taking it to production at a large scale uh helped us mature are offering um like",
    "start": "192480",
    "end": "199480"
  },
  {
    "text": "greatly right um let me just share my",
    "start": "199480",
    "end": "206400"
  },
  {
    "text": "screen",
    "start": "208680",
    "end": "211680"
  },
  {
    "text": "yeah so as I said so sorry yeah I already introduced uh the speakers here",
    "start": "213760",
    "end": "221239"
  },
  {
    "text": "and so I'll also briefly introduce capillary Technologies so capary Technologies is a customer engagement uh",
    "start": "221239",
    "end": "227720"
  },
  {
    "text": "leader so they are into the CRM business and they serve a massive user base so if",
    "start": "227720",
    "end": "233560"
  },
  {
    "text": "you have ever been to a retail store and you have been uh issued a coupon or or",
    "start": "233560",
    "end": "239280"
  },
  {
    "text": "you have gotten some points from a transaction uh it's a fairly good uh",
    "start": "239280",
    "end": "244680"
  },
  {
    "text": "there's a fairly good chance that you have been served by uh capit Technologies right uh and they Empower a",
    "start": "244680",
    "end": "251400"
  },
  {
    "text": "lot of major Global Brands so uh and of course with the gdpr restrictions so",
    "start": "251400",
    "end": "256799"
  },
  {
    "text": "they have uh deployments in deployments across the globe like uh Europe us uh",
    "start": "256799",
    "end": "263680"
  },
  {
    "text": "southeast Asia India specifically right and U they have moreover they have a",
    "start": "263680",
    "end": "270320"
  },
  {
    "text": "large engineering operation of around 200 developers and uh which is where",
    "start": "270320",
    "end": "275720"
  },
  {
    "text": "which is why I think today's talk would be uh more interesting in the sense that it's not just a technical problem it's",
    "start": "275720",
    "end": "282280"
  },
  {
    "text": "also a cultural switch when you are uh you know switching to a new logging solution Al together uh so yeah so as I",
    "start": "282280",
    "end": "289199"
  },
  {
    "text": "said uh we were co-building Loki with um with s from Capital Technologies and uh",
    "start": "289199",
    "end": "296479"
  },
  {
    "text": "what we'll do today in today's session is to share some of our learnings basically we'll just reiterate uh what",
    "start": "296479",
    "end": "302840"
  },
  {
    "text": "we went through we will not go into the technical depths right now because uh we might have people with varied",
    "start": "302840",
    "end": "309280"
  },
  {
    "text": "experiences and with Loki or VAR varied exposure to Loki in the audience but we",
    "start": "309280",
    "end": "314759"
  },
  {
    "text": "are happy to take any questions so do put your questions into the chat at the end of the session we'll have a Q&A",
    "start": "314759",
    "end": "320600"
  },
  {
    "text": "session where we'll try to answer as many questions as possible uh yeah so",
    "start": "320600",
    "end": "326360"
  },
  {
    "text": "that said I'll hand it over to sjit uh so yeah sjit you can take over thank you",
    "start": "326360",
    "end": "333520"
  },
  {
    "text": "RIT thank you for having me let me share the screen",
    "start": "333520",
    "end": "340198"
  },
  {
    "text": "here I hope it's uh",
    "start": "345560",
    "end": "349520"
  },
  {
    "text": "visible yeah yeah",
    "start": "352000",
    "end": "357160"
  },
  {
    "text": "um hello everyone uh thanks for joining um so my name is sjit U I'm working as a",
    "start": "357160",
    "end": "363880"
  },
  {
    "text": "devops engineer in capillary Technologies um I have been with the company for uh like around nine years",
    "start": "363880",
    "end": "370240"
  },
  {
    "text": "now and all these years I have been working with the infrastructures and",
    "start": "370240",
    "end": "375479"
  },
  {
    "text": "Cloud resources across Technologies and um recently um upgrading our Observatory",
    "start": "375479",
    "end": "383319"
  },
  {
    "text": "platform we decide to move our logging um Tech strategy to Loki",
    "start": "383319",
    "end": "389919"
  },
  {
    "text": "and U in this webinar I will be explaining our journey experience that we had with",
    "start": "389919",
    "end": "397720"
  },
  {
    "text": "Loi um so when it comes to the logging in capillary um we have customers across",
    "start": "397720",
    "end": "404280"
  },
  {
    "text": "the globe we have infrastructure across the globe and uh we have around 1.5 TP",
    "start": "404280",
    "end": "409880"
  },
  {
    "text": "of uh logs that is being u in average we are uh turning out where uh which is",
    "start": "409880",
    "end": "417759"
  },
  {
    "text": "required to be uh retained for uh logs for a year also due to compliance",
    "start": "417759",
    "end": "423879"
  },
  {
    "text": "requirements um logs often refer for um backtracking uh real time investigating",
    "start": "423879",
    "end": "431080"
  },
  {
    "text": "real time issues and going back to um older older date when a customer reports",
    "start": "431080",
    "end": "437280"
  },
  {
    "text": "a problem so uh to certain extent these logs are used for investigation",
    "start": "437280",
    "end": "442800"
  },
  {
    "text": "purposes now the Legacy mechanism that we hold here to handle the logs and",
    "start": "442800",
    "end": "448919"
  },
  {
    "text": "store where we use a fluentd uh collector which sends the data to all our log data",
    "start": "448919",
    "end": "455000"
  },
  {
    "text": "to uh EFS where we store it as a temporary storage and from there uh we",
    "start": "455000",
    "end": "461000"
  },
  {
    "text": "attach a VY SSS terminal um which is a web terminal V It's actually an open",
    "start": "461000",
    "end": "468039"
  },
  {
    "text": "source tool with which developers or anybody can run Linux commands on those",
    "start": "468039",
    "end": "473680"
  },
  {
    "text": "log files and grab the strings that they want during the investigating time and U",
    "start": "473680",
    "end": "481199"
  },
  {
    "text": "later on after 3 days we will be pushing those logs to S3 bucket as an archival strategy and we'll be keeping it there",
    "start": "481199",
    "end": "488039"
  },
  {
    "text": "now um as time flies by uh number of applications increase number of logs that are written by the applications",
    "start": "488039",
    "end": "494319"
  },
  {
    "text": "increase due to Due feature addition um EFS started getting um bottleneck in the",
    "start": "494319",
    "end": "501080"
  },
  {
    "text": "iops part and we had to increase the iops in time to time and later on we had",
    "start": "501080",
    "end": "508159"
  },
  {
    "text": "to um create a replica of EFS to have the reads and the rights separated out",
    "start": "508159",
    "end": "514800"
  },
  {
    "text": "in order to handle the slers or the io but to certain extent we were not able",
    "start": "514800",
    "end": "520959"
  },
  {
    "text": "to scale up more than this and uh as for the log volume increased",
    "start": "520959",
    "end": "526160"
  },
  {
    "text": "the whole system was in scaling up we wanted uh we had to add additional maners in managing different different",
    "start": "526160",
    "end": "532800"
  },
  {
    "text": "moving components here creating alerts monitoring them uh that was a tedious job at some part at some point",
    "start": "532800",
    "end": "539720"
  },
  {
    "text": "now what is the Roa on this um a fraction of these logs are used for",
    "start": "539720",
    "end": "545440"
  },
  {
    "text": "troubleshooting there are billions of log lines being written and uh terabytes of data being stored it's a large chunk",
    "start": "545440",
    "end": "553200"
  },
  {
    "text": "of data where we can have uh we can identify an application performance how",
    "start": "553200",
    "end": "559560"
  },
  {
    "text": "the uh API calls are happening um a lot and lot more data will be present in",
    "start": "559560",
    "end": "564880"
  },
  {
    "text": "those log files which we can Leverage",
    "start": "564880",
    "end": "570680"
  },
  {
    "text": "this uh is an architecture diagram that we used to follow in our Legacy",
    "start": "570680",
    "end": "576279"
  },
  {
    "text": "mechanism where we have a fluent pod fetching the log files from different applications and sending that to EFS on",
    "start": "576279",
    "end": "583959"
  },
  {
    "text": "which every application part is running which on uh through which the developers",
    "start": "583959",
    "end": "589160"
  },
  {
    "text": "can run their Linux commands to grab the",
    "start": "589160",
    "end": "594680"
  },
  {
    "text": "data now considering all these issues at hand we were looking for for a better solution that can solve us that can",
    "start": "595160",
    "end": "602480"
  },
  {
    "text": "improve our Dev efficiency as well as solve our problems and get more insights from the log data that we have so these",
    "start": "602480",
    "end": "610560"
  },
  {
    "text": "were the four candidates that we picked during our evaluation where elk Parable",
    "start": "610560",
    "end": "615680"
  },
  {
    "text": "low key New Relic were the major ones now considering each one of them elk is",
    "start": "615680",
    "end": "621640"
  },
  {
    "text": "scalable we used to run that in our smaller cluster subsidiary application",
    "start": "621640",
    "end": "627040"
  },
  {
    "text": "clusters but to certain what we understand is that is bit expensive to operate you need more",
    "start": "627040",
    "end": "635000"
  },
  {
    "text": "number of nodes as the log line log volume goes High um availability dispas issues and archived log retrieval was",
    "start": "635000",
    "end": "643120"
  },
  {
    "text": "quite a problem for us then comes parsible which was the simplest",
    "start": "643120",
    "end": "648959"
  },
  {
    "text": "application among them uh which didn't have an ha but was very um very",
    "start": "648959",
    "end": "655639"
  },
  {
    "text": "nonexpensive and and it was relatively in a n stage where some of the use cases",
    "start": "655639",
    "end": "661680"
  },
  {
    "text": "that we required was not bet by the application then uh New Relic uh is",
    "start": "661680",
    "end": "667680"
  },
  {
    "text": "something that is common across the community and it's popular as well but it is expensive and we have a huge",
    "start": "667680",
    "end": "674880"
  },
  {
    "text": "amount of data that's going in there which we need to pay for and it is a",
    "start": "674880",
    "end": "680079"
  },
  {
    "text": "completely managed solution but again the cost perspective is something which we need to consider whether we should go",
    "start": "680079",
    "end": "686399"
  },
  {
    "text": "for or not now then comes low key which is again scalable open source widely",
    "start": "686399",
    "end": "693120"
  },
  {
    "text": "accepted in the community and has NH and Native integration of grafana apps",
    "start": "693120",
    "end": "699959"
  },
  {
    "text": "whatever whatever application that grafana Labs have developed is easily can be integrated with Loki and not much",
    "start": "699959",
    "end": "706720"
  },
  {
    "text": "expensive compared to Elk and new considering all these factors we decided",
    "start": "706720",
    "end": "713600"
  },
  {
    "text": "we'll go ahead with Loki since it is scalable we can have alerts and dashboarding for for our insights cost",
    "start": "713600",
    "end": "720160"
  },
  {
    "text": "effective completely managed in our infrastructure itself and popular among",
    "start": "720160",
    "end": "726079"
  },
  {
    "text": "the whole community and we were used to Prometheus and Prometheus grafana and we",
    "start": "726079",
    "end": "732320"
  },
  {
    "text": "have metrics there then why can't we have the logs also and same grafana which will make it easy for us to handle",
    "start": "732320",
    "end": "739600"
  },
  {
    "text": "both together so this these points helped us to finalize that low Kei is",
    "start": "739600",
    "end": "746639"
  },
  {
    "text": "something that we need to go for at this point I will take a pause and I",
    "start": "746639",
    "end": "753560"
  },
  {
    "text": "would like to invite promod iapen from facets Cloud to explain about Loki",
    "start": "753560",
    "end": "759519"
  },
  {
    "text": "architecture and welcome rot hey thanks RIT yeah I'm sharing my",
    "start": "759519",
    "end": "767760"
  },
  {
    "text": "screen yeah so hope you can see my screen so hi everyone uh this is promode",
    "start": "773880",
    "end": "780320"
  },
  {
    "text": "um so I'm part of faers for more than a year now as a technical lead and today",
    "start": "780320",
    "end": "785600"
  },
  {
    "text": "I'll be going over the overview of grafana Loki right so what is Loki Loki",
    "start": "785600",
    "end": "792440"
  },
  {
    "text": "is horizontally scalable highly available and then multi-tenant log aggregation system that is inspired by",
    "start": "792440",
    "end": "798720"
  },
  {
    "text": "promethus so all these Bus words here you are seeing right this is what makes Loki stand out from other logging",
    "start": "798720",
    "end": "804800"
  },
  {
    "text": "Solutions and it is developed by graphon Labs so as you can see it is having really really lot popularity and widely",
    "start": "804800",
    "end": "812680"
  },
  {
    "text": "accepted by the community and like Prometheus it is for logs in promises we",
    "start": "812680",
    "end": "819440"
  },
  {
    "text": "have metrics but this is enly for logs and it is very cost effective uh it is",
    "start": "819440",
    "end": "825000"
  },
  {
    "text": "open source and it is still actively maintained uh next is minimal index",
    "start": "825000",
    "end": "830839"
  },
  {
    "text": "logging so unlike other logging Solutions Loki actually does not index",
    "start": "830839",
    "end": "836000"
  },
  {
    "text": "the entire log content instead what it does is is the only the entries are grouped into streams and it will index",
    "start": "836000",
    "end": "843079"
  },
  {
    "text": "only the labels and that to the labels are Prometheus style labels I will talk about this promeal sty style labels in",
    "start": "843079",
    "end": "850120"
  },
  {
    "text": "the next slide and here there is a small diagrammatic representation where you",
    "start": "850120",
    "end": "855800"
  },
  {
    "text": "can see log data is about 10 TB and only 200 MB of index data is there right so",
    "start": "855800",
    "end": "863279"
  },
  {
    "text": "what this will improve the performance as well as quing time will be really faster because only 200",
    "start": "863279",
    "end": "869279"
  },
  {
    "text": "200 MB of data we need to query right and this is not the",
    "start": "869279",
    "end": "874440"
  },
  {
    "text": "exact like exact representation like if this will change environment to environment as well as configuration to",
    "start": "874440",
    "end": "881160"
  },
  {
    "text": "configuration it is all based on the logs that you are pushing into the system so that that you need to consider",
    "start": "881160",
    "end": "888839"
  },
  {
    "text": "and then next is indexing logs so here you can see how the label how it is",
    "start": "888839",
    "end": "895199"
  },
  {
    "text": "indexing and then what is not indexed here so only the time stamp and then the Promethea style labels are getting",
    "start": "895199",
    "end": "901800"
  },
  {
    "text": "indexed and here why promia style labels is because we are here in Loki it is",
    "start": "901800",
    "end": "907480"
  },
  {
    "text": "exactly using same as Prometheus right that is why it is called Prometheus style labels and only the uh contents is",
    "start": "907480",
    "end": "914920"
  },
  {
    "text": "not indexed here and worth noting here is when you use fewer labels you will",
    "start": "914920",
    "end": "920240"
  },
  {
    "text": "get a better performance out of Loki and then um we'll I'll will be",
    "start": "920240",
    "end": "927360"
  },
  {
    "text": "talking about log string what is log stream as well as high C nty problem so what is log stream right so log stream",
    "start": "927360",
    "end": "934319"
  },
  {
    "text": "is a stream of entries with same exact label set so here we have three different lines but if you see if you",
    "start": "934319",
    "end": "940920"
  },
  {
    "text": "look at the labels here they are all same labels right so what what happens",
    "start": "940920",
    "end": "946000"
  },
  {
    "text": "is that this is considered as a single stream and similarly in the other two lines you will have a same set of labels",
    "start": "946000",
    "end": "953240"
  },
  {
    "text": "so this is also considered as single stream so totally we have three uh totally we have two streams here and now",
    "start": "953240",
    "end": "960120"
  },
  {
    "text": "if we talk about cality problem let me introduce one one one more label called",
    "start": "960120",
    "end": "965279"
  },
  {
    "text": "node right uh for let's take the first stream so we have three different lines",
    "start": "965279",
    "end": "971160"
  },
  {
    "text": "and in that we are going to add a new label called node and each will have node one node two and node three now",
    "start": "971160",
    "end": "977120"
  },
  {
    "text": "what happens is that this will become U so first first line will become uh",
    "start": "977120",
    "end": "982759"
  },
  {
    "text": "stream and then the next line will become its own stream and then third will become a different Stream So if",
    "start": "982759",
    "end": "988160"
  },
  {
    "text": "this is happening in the log system let's say if we introduce a a label called IP and whenever there is a user",
    "start": "988160",
    "end": "995759"
  },
  {
    "text": "there are multiple users who this URL right and those will also get processed",
    "start": "995759",
    "end": "1002079"
  },
  {
    "text": "right so that's what ional problem will like that that will lead to high cality",
    "start": "1002079",
    "end": "1007759"
  },
  {
    "text": "problem when there are multiple unique strings then that will be the problem for ional there is an example here as",
    "start": "1007759",
    "end": "1015759"
  },
  {
    "text": "you can see from a log line we have levels log level and then statuses and then we also have",
    "start": "1015759",
    "end": "1021720"
  },
  {
    "text": "parts right for each log level we will have each different statuses as from",
    "start": "1021720",
    "end": "1027280"
  },
  {
    "text": "that same log line and then we will have for each status we will have three different parts so ultimately if you",
    "start": "1027280",
    "end": "1034720"
  },
  {
    "text": "like if you do a quick math here like four into three into three that will be lead leading to uh potentially 36",
    "start": "1034720",
    "end": "1042438"
  },
  {
    "text": "streams here right so that is what causing the high cality problem so we have actually",
    "start": "1042439",
    "end": "1049000"
  },
  {
    "text": "handled this in our product like removing ialy labels and then we are packing it into that I will talk in the",
    "start": "1049000",
    "end": "1056160"
  },
  {
    "text": "different slide but yeah and then next is uh deployment modes so we have three",
    "start": "1056160",
    "end": "1062960"
  },
  {
    "text": "uh deployment modes here monolithic mode simple scalable and then microservices mode so what mon monolithic mode is uh",
    "start": "1062960",
    "end": "1071240"
  },
  {
    "text": "basically as you can see all the Loki components are packed into a single process as a single binary right and",
    "start": "1071240",
    "end": "1078440"
  },
  {
    "text": "this is only useful when for getting started and to do some experimentation",
    "start": "1078440",
    "end": "1083720"
  },
  {
    "text": "on the Loki side and this monol monolithic mode only supports average of",
    "start": "1083720",
    "end": "1090120"
  },
  {
    "text": "20 GB per day of logs and which is not really adequate for production grade systems right and in simple scalable all",
    "start": "1090120",
    "end": "1099000"
  },
  {
    "text": "as you can see each execution path is uh like splitted into uh three Target",
    "start": "1099000",
    "end": "1105080"
  },
  {
    "text": "groups basically write read and then backend and and each have its own use",
    "start": "1105080",
    "end": "1110960"
  },
  {
    "text": "cases like you can um like scale each like targets individually and you",
    "start": "1110960",
    "end": "1117640"
  },
  {
    "text": "can increase the performance and here it will support few under few TBS of logs",
    "start": "1117640",
    "end": "1123679"
  },
  {
    "text": "and however if you go beyond that few TBS right then only approaches microservices mode here as you can see",
    "start": "1123679",
    "end": "1130080"
  },
  {
    "text": "all the components in the Loki are run separately and you can do customization",
    "start": "1130080",
    "end": "1136080"
  },
  {
    "text": "and you have the flexibility to configure each each component separately and based on the load you can increase",
    "start": "1136080",
    "end": "1141799"
  },
  {
    "text": "one of the component or the other so that are the advantages of microservices",
    "start": "1141799",
    "end": "1147720"
  },
  {
    "text": "mode now how it works right so unlike Prometheus Loki doesn't do a pull",
    "start": "1147720",
    "end": "1153760"
  },
  {
    "text": "instead it only pushes right so for that we need to use a agent that should",
    "start": "1153760",
    "end": "1160320"
  },
  {
    "text": "collect logs from node and then uh which will be sent to the Loki so that is what",
    "start": "1160320",
    "end": "1166080"
  },
  {
    "text": "so that's that's how Loki works here here in this case we are using promp tail and promail uses same Library as",
    "start": "1166080",
    "end": "1174840"
  },
  {
    "text": "Prometheus for service Discovery as well as promethus styling style labels so",
    "start": "1174840",
    "end": "1180760"
  },
  {
    "text": "those are all created by promil itself and it is a product from graphon Labs again um so this will push the logs to",
    "start": "1180760",
    "end": "1188240"
  },
  {
    "text": "Loki and to visualize the logs you can um use graphon uh where you can",
    "start": "1188240",
    "end": "1194000"
  },
  {
    "text": "visualize through Lal queries or also you can use a command line tool called log CLI again there also you can use a",
    "start": "1194000",
    "end": "1201960"
  },
  {
    "text": "logq query and then uh with log with Loi you can also set up alerts uh using",
    "start": "1201960",
    "end": "1208480"
  },
  {
    "text": "alert manager and uh also you can do recording hules so uh it's all possible",
    "start": "1208480",
    "end": "1214240"
  },
  {
    "text": "from Loki said and here is a sample um query Lo query as you can see here I",
    "start": "1214240",
    "end": "1221720"
  },
  {
    "text": "have filtered with query Fred container and then name space loky Dev and I'm gripping for error log right so what it",
    "start": "1221720",
    "end": "1229039"
  },
  {
    "text": "does is it will return with all the error logs from this container so next is the architecture so",
    "start": "1229039",
    "end": "1237520"
  },
  {
    "text": "here we have right path and then the read path right in the right path we have distributor ingestor and in the",
    "start": "1237520",
    "end": "1243200"
  },
  {
    "text": "read path we have query front end querier and then the ruler right so",
    "start": "1243200",
    "end": "1248360"
  },
  {
    "text": "whenever log enters into low Kei first distributor will be the one that will receive the log and it will do some",
    "start": "1248360",
    "end": "1255480"
  },
  {
    "text": "validation checks as well as it will this is this is the service that is responsible to determine which ingestor",
    "start": "1255480",
    "end": "1261720"
  },
  {
    "text": "that log needs to be pushed to and once it reaches ingestor ingestor will be uh sending the logs to long-term storage",
    "start": "1261720",
    "end": "1268960"
  },
  {
    "text": "like Object Store like S3 gcp blob or whatever the cloud provided storage",
    "start": "1268960",
    "end": "1275600"
  },
  {
    "text": "right and then once ingestor comes here a querer will be responsible for",
    "start": "1275600",
    "end": "1280720"
  },
  {
    "text": "handling all the queries from the API or from the grafana right and first corer",
    "start": "1280720",
    "end": "1286279"
  },
  {
    "text": "will hit the ingestor uh and it will look for in memory data first if the",
    "start": "1286279",
    "end": "1291720"
  },
  {
    "text": "timeline doesn't suffice then what it will do is it will fall back to the object store and it will get the data",
    "start": "1291720",
    "end": "1298080"
  },
  {
    "text": "and here query front end is just an optimization of querer where uh uh what",
    "start": "1298080",
    "end": "1303640"
  },
  {
    "text": "it will do is it will split large number large very big query into smaller chunks",
    "start": "1303640",
    "end": "1309039"
  },
  {
    "text": "and then it will concurrently execute them and then stitches back the result and give it to the customer user whoever",
    "start": "1309039",
    "end": "1316960"
  },
  {
    "text": "using the graphon or the API calls and then here on the left hand side you can see we have a ruler so ruler is the",
    "start": "1316960",
    "end": "1323640"
  },
  {
    "text": "component that is responsible for creating rule alerts as well as recording rules so whenever we create a",
    "start": "1323640",
    "end": "1331120"
  },
  {
    "text": "rule that we there we can specify which uh Target like in alert manager if we want to get we can uh Target that so",
    "start": "1331120",
    "end": "1339080"
  },
  {
    "text": "that all taken care by ruler so this is about the",
    "start": "1339080",
    "end": "1344679"
  },
  {
    "text": "architecture uh now we will talk about the hashing uh so basically um",
    "start": "1344679",
    "end": "1350520"
  },
  {
    "text": "distributor is the primary service that is responsible for um uh determining the inor right so how it determines is that",
    "start": "1350520",
    "end": "1357840"
  },
  {
    "text": "it uses consistent ashing plus configurable replication Factor right",
    "start": "1357840",
    "end": "1363159"
  },
  {
    "text": "and the stream is ashed into ashed using both tenant ID as well as label it and",
    "start": "1363159",
    "end": "1369760"
  },
  {
    "text": "also low Kei maintains the ring for each service and here in this case like inor",
    "start": "1369760",
    "end": "1376240"
  },
  {
    "text": "register thems into hash string with a set of tokens let me quickly Show an example of this so here in this diagram",
    "start": "1376240",
    "end": "1384880"
  },
  {
    "text": "you can see a circle here right that is called a ring and each ingestor on the right hand side will register uh with",
    "start": "1384880",
    "end": "1392640"
  },
  {
    "text": "couple like with with set of tokens that is 0 to 16348 uh 16384 right so that",
    "start": "1392640",
    "end": "1400799"
  },
  {
    "text": "that range will be registered by the inor and that goes some till inor 4 so whenever a logs comes in from promp to",
    "start": "1400799",
    "end": "1409120"
  },
  {
    "text": "distributor what happens is that distributor does some ashing so at the",
    "start": "1409120",
    "end": "1414520"
  },
  {
    "text": "bottom you can see right first log comes in and it will hash the label and it will get the hash number and based on",
    "start": "1414520",
    "end": "1421720"
  },
  {
    "text": "that hash it will select the ingestor within the range so it will go clockwise so if you see it will uh the number",
    "start": "1421720",
    "end": "1428919"
  },
  {
    "text": "corresponds to the range uh that is in the in to and similarly the other other",
    "start": "1428919",
    "end": "1435320"
  },
  {
    "text": "all others as you can see from the image with uh colors it has like placed into",
    "start": "1435320",
    "end": "1441360"
  },
  {
    "text": "two three different inors and let's say if we have a replication",
    "start": "1441360",
    "end": "1446440"
  },
  {
    "text": "Factor larger than one currently this is for replication one right what if it is",
    "start": "1446440",
    "end": "1452200"
  },
  {
    "text": "two replication two right in that case what happens is that first it will place",
    "start": "1452200",
    "end": "1457880"
  },
  {
    "text": "the first line into ingestor two and then it will in the clockwise it will select the next subsequent ingestor in",
    "start": "1457880",
    "end": "1465120"
  },
  {
    "text": "this case ingestor three so two copies of first stream will be stored in ingestor 2 and ingestor 3 so this is how",
    "start": "1465120",
    "end": "1472399"
  },
  {
    "text": "the ashing works um yeah and then next move on",
    "start": "1472399",
    "end": "1477760"
  },
  {
    "text": "to integrate yeah integration of Loki into faces platform so with these",
    "start": "1477760",
    "end": "1482919"
  },
  {
    "text": "learnings as well as from capar insides we are able to integrate Loki into faets",
    "start": "1482919",
    "end": "1489240"
  },
  {
    "text": "uh with the right production grade configuration and what we did actually is we have done load testing using Loki",
    "start": "1489240",
    "end": "1496000"
  },
  {
    "text": "Canary um and also we we have uh tested with actual data from various deployed",
    "start": "1496000",
    "end": "1501840"
  },
  {
    "text": "applications and with the uh with from within faet we are able to we support",
    "start": "1501840",
    "end": "1508360"
  },
  {
    "text": "storage pen uh including credential handling uh Loki needs to have a",
    "start": "1508360",
    "end": "1513480"
  },
  {
    "text": "credential right so for like storage back end that is in the cloud so that is",
    "start": "1513480",
    "end": "1518720"
  },
  {
    "text": "automatically handled and also we provide Gades native Mino storage solution as",
    "start": "1518720",
    "end": "1524440"
  },
  {
    "text": "well and then along with this we also automat Ally create loky data source and",
    "start": "1524440",
    "end": "1529880"
  },
  {
    "text": "integrate into grafana and also we deploy quick Log search for easily accessing logs from filters and then um",
    "start": "1529880",
    "end": "1538200"
  },
  {
    "text": "we have also dropped High cality label so to make low Kei very efficient and",
    "start": "1538200",
    "end": "1543840"
  },
  {
    "text": "performant and then what we did is we actually like packed all these drop lab",
    "start": "1543840",
    "end": "1550440"
  },
  {
    "text": "labels uh into the log line itself so that at the query time what we do is after the uh result comes in we we",
    "start": "1550440",
    "end": "1558000"
  },
  {
    "text": "perform the transformation and get the filters within the grafana itself so that's how we are doing internally and",
    "start": "1558000",
    "end": "1565320"
  },
  {
    "text": "then there is one other option called Auto forget unhealthy instances that we have enabled because uh Whenever there",
    "start": "1565320",
    "end": "1573320"
  },
  {
    "text": "is a iner that gets down abruptly without any doing any cleanup activity",
    "start": "1573320",
    "end": "1579360"
  },
  {
    "text": "right at that time what happens is that it will still be in the ring so it is not it will be uh with the state",
    "start": "1579360",
    "end": "1586320"
  },
  {
    "text": "unhealthy so because of that lowy won't accept any more logs that needs to be",
    "start": "1586320",
    "end": "1592480"
  },
  {
    "text": "manually removed from the ring so we have to access the API and we have to",
    "start": "1592480",
    "end": "1597559"
  },
  {
    "text": "remove it from the ring so that's why we gone with this approach and we have added Auto forget anal and it will",
    "start": "1597559",
    "end": "1604919"
  },
  {
    "text": "automatically remove all the unhealthy instances and then there are few more optimizations um like two and four from",
    "start": "1604919",
    "end": "1612320"
  },
  {
    "text": "capies inside and we have made all those optimizations and now we have have very",
    "start": "1612320",
    "end": "1618720"
  },
  {
    "text": "good Loki integration in faet platform and here you can see on the right hand side it is an example from faets",
    "start": "1618720",
    "end": "1625840"
  },
  {
    "text": "resource so we have deployed an application and as you can see from that application we are able to get the logs",
    "start": "1625840",
    "end": "1632120"
  },
  {
    "text": "and uh from this tab we will be able to list all the logs for that particular",
    "start": "1632120",
    "end": "1638760"
  },
  {
    "text": "res yeah so that's it from me and I'll be uh giving it to sjit sjit you can",
    "start": "1638760",
    "end": "1646480"
  },
  {
    "text": "take over thank you thank PR for a detailed insights on the",
    "start": "1646480",
    "end": "1653480"
  },
  {
    "text": "overview of Loki",
    "start": "1653480",
    "end": "1659279"
  },
  {
    "text": "yeah yeah um so uh the audience I have an update here uh you can uh post your",
    "start": "1660640",
    "end": "1667720"
  },
  {
    "text": "questions in the chat box provided uh we'll be taking those questions at the end of the session and we will provide",
    "start": "1667720",
    "end": "1673519"
  },
  {
    "text": "the answer thank you and uh yeah where we we we have selected Loki to be",
    "start": "1673519",
    "end": "1681000"
  },
  {
    "text": "implemented in our capillary clusters so we had to decide how to implement this",
    "start": "1681000",
    "end": "1687919"
  },
  {
    "text": "how to a deployment strategy needs to be uh defined uh to take it forward so we",
    "start": "1687919",
    "end": "1693880"
  },
  {
    "text": "came up with this plan where first we will have it deployed in non- production clusters then once it is stabilized and",
    "start": "1693880",
    "end": "1700320"
  },
  {
    "text": "signed off we will move to the production clusters and then uh see the stability and other things and then roll",
    "start": "1700320",
    "end": "1706760"
  },
  {
    "text": "out to the restaurant of the environment now slowly after that we'll face out the Legacy",
    "start": "1706760",
    "end": "1714200"
  },
  {
    "text": "setup now this is the event line that we had while deploying the Loki in the",
    "start": "1714640",
    "end": "1721720"
  },
  {
    "text": "non-production Clusters so as facets to cloud has integrated Loki in its module",
    "start": "1721720",
    "end": "1728360"
  },
  {
    "text": "um for us it was just a matter of changing the configuration in the blueprint and deploying uh Loki in our",
    "start": "1728360",
    "end": "1735760"
  },
  {
    "text": "clusters so uh the release or the installation was seamless later we",
    "start": "1735760",
    "end": "1741640"
  },
  {
    "text": "conducted low test for Loki in our non- production infrastructure with whatever",
    "start": "1741640",
    "end": "1747720"
  },
  {
    "text": "limit it had and uh to understand the behavior and scaling patterns of Loki",
    "start": "1747720",
    "end": "1753159"
  },
  {
    "text": "now um on along with the load test we wanted to to identify the behavior of",
    "start": "1753159",
    "end": "1758880"
  },
  {
    "text": "Loki we need to see how um how the other metrices behave along with the variation",
    "start": "1758880",
    "end": "1765840"
  },
  {
    "text": "in load so we def find some metrices which Loi already provides into Pras",
    "start": "1765840",
    "end": "1771720"
  },
  {
    "text": "like log drop resource usages scaling patterns Chun size push to S3 bucket Etc",
    "start": "1771720",
    "end": "1777760"
  },
  {
    "text": "were created and alerts were created on top of it to see how the behavior or how",
    "start": "1777760",
    "end": "1783120"
  },
  {
    "text": "the log pattern variation happens and how luki behaves now when it comes into the",
    "start": "1783120",
    "end": "1789320"
  },
  {
    "text": "developer perspective U we were completely planning to uh remove the old",
    "start": "1789320",
    "end": "1794480"
  },
  {
    "text": "Legacy setup and bring in a new uh completely new uh what do you call",
    "start": "1794480",
    "end": "1800720"
  },
  {
    "text": "completely new setup of low key logging strategy now this transition we wanted",
    "start": "1800720",
    "end": "1808080"
  },
  {
    "text": "to have it seamless for the developers to handle and towards to the newer technology adoption so on that basis",
    "start": "1808080",
    "end": "1814640"
  },
  {
    "text": "what we uh tried is to create uh documents for references create KT and",
    "start": "1814640",
    "end": "1821000"
  },
  {
    "text": "workshops for uh how Loki works and all then um as for the feedback of the",
    "start": "1821000",
    "end": "1828039"
  },
  {
    "text": "developers we asked them to create J tickets to let us know how Loki is behaving how what sort of issues they",
    "start": "1828039",
    "end": "1834279"
  },
  {
    "text": "are facing which gave us the idea how uh effective Loki is how it is improving",
    "start": "1834279",
    "end": "1840720"
  },
  {
    "text": "the efficiency of developers day-to-day work slowly once we decided the uh",
    "start": "1840720",
    "end": "1846919"
  },
  {
    "text": "timeline of removing uh the Legacy setup in nonpr cluster after getting a sign",
    "start": "1846919",
    "end": "1852279"
  },
  {
    "text": "off we slowly removed the whole uh uninstall the whole old setup and",
    "start": "1852279",
    "end": "1857360"
  },
  {
    "text": "completely we were running on low Kei in non- production clusters then later on",
    "start": "1857360",
    "end": "1863200"
  },
  {
    "text": "uh we decided to go for the production uh release we wanted to do a capacity",
    "start": "1863200",
    "end": "1868600"
  },
  {
    "text": "planning and this capacity planning was something which we uh decided depending on the log volume the log the low test",
    "start": "1868600",
    "end": "1876399"
  },
  {
    "text": "that we conducted and later on that linear projection that we created we",
    "start": "1876399",
    "end": "1881600"
  },
  {
    "text": "came up with the configuration which we decided we go ahead into uh the uh",
    "start": "1881600",
    "end": "1886679"
  },
  {
    "text": "production cluster now um as I said we have",
    "start": "1886679",
    "end": "1892159"
  },
  {
    "text": "clusters across the globe some are generating heavy uh amount of log some are not generating that much but there",
    "start": "1892159",
    "end": "1898000"
  },
  {
    "text": "are clusters sitting in the middle having a medium amount of log so considering that scenario we thought of",
    "start": "1898000",
    "end": "1904639"
  },
  {
    "text": "going ahead with the medium amount of log which would give us a balance between the two and see how effective is",
    "start": "1904639",
    "end": "1911320"
  },
  {
    "text": "Lo in those kind of clusters thus deciding the cluster ones we started um",
    "start": "1911320",
    "end": "1918080"
  },
  {
    "text": "uh we planned for a release and we went ahead with the release of the Loki in that cluster and parall we wanted the",
    "start": "1918080",
    "end": "1926919"
  },
  {
    "text": "Legacy setup also running in the same infrastructure considering we may need",
    "start": "1926919",
    "end": "1931960"
  },
  {
    "text": "some time to um stabilize Loki in our production cluster now this is a time",
    "start": "1931960",
    "end": "1939000"
  },
  {
    "text": "for monitoring yes um as soon as we deployed",
    "start": "1939000",
    "end": "1946480"
  },
  {
    "text": "Loki in to the production cluster we came across uh different roadblocks on a",
    "start": "1946480",
    "end": "1953120"
  },
  {
    "text": "uh on a categorizing scale we uh categorize those um issues into four",
    "start": "1953120",
    "end": "1959440"
  },
  {
    "text": "points and they are like rate limits where the injection rate that we",
    "start": "1959440",
    "end": "1965159"
  },
  {
    "text": "calculated and in real time was very different uh ingestor issues where the",
    "start": "1965159",
    "end": "1970480"
  },
  {
    "text": "ingestor Cs went into kill and restart issues were there S3 rate limit where",
    "start": "1970480",
    "end": "1976279"
  },
  {
    "text": "from S3 from ingester to um S3 push of chunks we having throttling issues and",
    "start": "1976279",
    "end": "1984000"
  },
  {
    "text": "at the end querying problem where we started getting timeouts whenever we run a query and that was another kind of a",
    "start": "1984000",
    "end": "1991440"
  },
  {
    "text": "problem in the read path now let's go into each and every",
    "start": "1991440",
    "end": "1997039"
  },
  {
    "text": "I'll serve through each and every issues uh and just to understand how uh how",
    "start": "1997039",
    "end": "2002880"
  },
  {
    "text": "each and each problems were handled differently now right limiting as I said",
    "start": "2002880",
    "end": "2008240"
  },
  {
    "text": "the inje rate from promail to distributor to ingestor that is",
    "start": "2008240",
    "end": "2013399"
  },
  {
    "text": "something which was completely um identified in the non-production",
    "start": "2013399",
    "end": "2018919"
  },
  {
    "text": "Clusters now when it comes to the production cluster what we could see is that the configured value was never",
    "start": "2018919",
    "end": "2026000"
  },
  {
    "text": "enough and Distributors started throwing error that there are too many uh log volume happening we need to control it",
    "start": "2026000",
    "end": "2032600"
  },
  {
    "text": "now sooner we identified this error and we started increasing the size now",
    "start": "2032600",
    "end": "2038840"
  },
  {
    "text": "looking having a closer look at the ingestor rate uh Loki is designed for a",
    "start": "2038840",
    "end": "2045159"
  },
  {
    "text": "multitenant environment where you can send logs from different clusters to a single Loki infrastructure in such a",
    "start": "2045159",
    "end": "2052358"
  },
  {
    "text": "scenario the ingestion rate of one tenant shouldn't affect the LW collection of other tenants in and bring",
    "start": "2052359",
    "end": "2060079"
  },
  {
    "text": "down the Loki environment for other other um and other tenants also because",
    "start": "2060079",
    "end": "2065480"
  },
  {
    "text": "there is one Loki environment to which all the tenants are sending in that scenario the injection rate is something",
    "start": "2065480",
    "end": "2072720"
  },
  {
    "text": "that is needed to keep the volume in in a constant or optimal",
    "start": "2072720",
    "end": "2079200"
  },
  {
    "text": "rate but what about in capillary we have one lowy cluster in per production",
    "start": "2079200",
    "end": "2086118"
  },
  {
    "text": "cluster in production three infra and um",
    "start": "2086119",
    "end": "2091320"
  },
  {
    "text": "in our case we don't have a multi-tenant environment in this case we have only a single tenant whose",
    "start": "2091320",
    "end": "2097440"
  },
  {
    "text": "rate can be adjusted but it's not like you can give the range more than Beyond",
    "start": "2097440",
    "end": "2103079"
  },
  {
    "text": "a limit so that the ingestor will get loaded and have to manage a lot of logs and you keep on adding more resources to",
    "start": "2103079",
    "end": "2110400"
  },
  {
    "text": "it so it is always good to have a sweet spot to have keep the logs in limit but",
    "start": "2110400",
    "end": "2116720"
  },
  {
    "text": "making the other components not overloaded so we give an uh safe upper",
    "start": "2116720",
    "end": "2122200"
  },
  {
    "text": "bound and uh in in combination with the ingestor rate limit as well as the ingestor burst Limit we have around 100",
    "start": "2122200",
    "end": "2129560"
  },
  {
    "text": "MVPs in majority of the Clusters now in large scale most of the Clusters have",
    "start": "2129560",
    "end": "2135320"
  },
  {
    "text": "similar limit that we have configured but actual rate is much lesser than that keeping the keeping the injection rate",
    "start": "2135320",
    "end": "2143320"
  },
  {
    "text": "in control so that um ingestor won't get loaded and they can easily handle which",
    "start": "2143320",
    "end": "2149720"
  },
  {
    "text": "in effect will help us to send um the chunks properly to S3",
    "start": "2149720",
    "end": "2155800"
  },
  {
    "text": "paret yes we had uh we received lot of out of memory issues after increasing",
    "start": "2155800",
    "end": "2162040"
  },
  {
    "text": "the inje rate now a lot of streams were getting created uh in each and every",
    "start": "2162040",
    "end": "2168680"
  },
  {
    "text": "inest on investigating more what we could see is uh the node number number",
    "start": "2168680",
    "end": "2174839"
  },
  {
    "text": "of nodes in production and number of nodes in non- production clusters were different we have more numbers in",
    "start": "2174839",
    "end": "2181480"
  },
  {
    "text": "production cluster obviously now the node label is one of the label that we are sending to Lo and that is being",
    "start": "2181480",
    "end": "2188480"
  },
  {
    "text": "indexed automatically stream numbers also increased because of the same so we",
    "start": "2188480",
    "end": "2195160"
  },
  {
    "text": "uh decided to bring down node label after considering that in our usual or",
    "start": "2195160",
    "end": "2200960"
  },
  {
    "text": "day-to-day troubleshooting um proed process we don't see that node label is much of an importance so in that",
    "start": "2200960",
    "end": "2208480"
  },
  {
    "text": "scenario what we decided we will drop the node label directly from tromp tail itself which in effect helped us to",
    "start": "2208480",
    "end": "2215000"
  },
  {
    "text": "reduce the number of streams and reduce the memory usage of each and every",
    "start": "2215000",
    "end": "2220359"
  },
  {
    "text": "ingestor but again out of a surprise we got out of memory again for ingestor and",
    "start": "2220359",
    "end": "2228200"
  },
  {
    "text": "in uh in this scenario we didn't have all the inors going into W kill we had",
    "start": "2228200",
    "end": "2233240"
  },
  {
    "text": "one or two among them going into Hill and this specifically is because the",
    "start": "2233240",
    "end": "2238760"
  },
  {
    "text": "streams are allocated per inest in that scenario the um streams the since the",
    "start": "2238760",
    "end": "2246400"
  },
  {
    "text": "Stream are going to a single ingestor the application which is generating that stream if it is having higher volume and",
    "start": "2246400",
    "end": "2254160"
  },
  {
    "text": "at some point if it is generating a sudden burst of log volumes then",
    "start": "2254160",
    "end": "2259440"
  },
  {
    "text": "automatically the memory usage of that ingestor will go high and it goes to om",
    "start": "2259440",
    "end": "2264839"
  },
  {
    "text": "in such a scenario we wanted something to handle or control the size of streams",
    "start": "2264839",
    "end": "2271160"
  },
  {
    "text": "and that's where Loki is providing an automatic sharding option which we enabled and we gave a limit of around 3",
    "start": "2271160",
    "end": "2278599"
  },
  {
    "text": "MV per per Stream So that anything above 3 MV will get sharded and a sharding key",
    "start": "2278599",
    "end": "2284800"
  },
  {
    "text": "will be added to that specific stream which in effect distributed these streams among different inors rather",
    "start": "2284800",
    "end": "2291920"
  },
  {
    "text": "than one ingestor and after that what we could see that all these inors were starting to use similar amount of memory",
    "start": "2291920",
    "end": "2302160"
  },
  {
    "text": "consistently now out of dis issues uh this this is something that we noticed",
    "start": "2302160",
    "end": "2307920"
  },
  {
    "text": "in different instances where um the inors get into a restart and the Val",
    "start": "2307920",
    "end": "2314640"
  },
  {
    "text": "replay kicks in and once Val replay starts the disk ingested dis starts",
    "start": "2314640",
    "end": "2320440"
  },
  {
    "text": "filing up with Val file and I reported this issue in the Loki Community but I",
    "start": "2320440",
    "end": "2326640"
  },
  {
    "text": "haven't got an update on the same till now um so it doesn't have a pattern it",
    "start": "2326640",
    "end": "2331960"
  },
  {
    "text": "will get get cleared off after some time it doesn't have a fixed size or it doesn't have fixed number or a time at",
    "start": "2331960",
    "end": "2339359"
  },
  {
    "text": "which it will get cleared so that was bit causing a problem so what as a workr",
    "start": "2339359",
    "end": "2344920"
  },
  {
    "text": "what I had to do was to increase the dis size a little bit over protion dis and",
    "start": "2344920",
    "end": "2350520"
  },
  {
    "text": "keep an alerting on those diss so once such an issue happens we'll get an alert",
    "start": "2350520",
    "end": "2355560"
  },
  {
    "text": "and we will be in a position to decide whether we should expand the disk online so that is the only work that that I",
    "start": "2355560",
    "end": "2362480"
  },
  {
    "text": "could Implement now after completely releasing everything in our production clusters till now I haven't faced this",
    "start": "2362480",
    "end": "2369760"
  },
  {
    "text": "issue and this was something that we noticed during our testing and stabilizing",
    "start": "2369760",
    "end": "2376680"
  },
  {
    "text": "phase unhealthy inors yes as promote said Auto forget is something that was",
    "start": "2376680",
    "end": "2382720"
  },
  {
    "text": "not by default enabled which in effect uh when a inor restarts or it was not",
    "start": "2382720",
    "end": "2388119"
  },
  {
    "text": "completely uh it was an incomplete restart or a node goes into H State and injest has got a heartbeat request which",
    "start": "2388119",
    "end": "2395599"
  },
  {
    "text": "if it doesn't reply to that it is considered unhealthy automatically the ring will get considered unhealthy and",
    "start": "2395599",
    "end": "2402880"
  },
  {
    "text": "distributor will not be able to commit the logs in so automat to remove these unhealthy inors an auto forget flag was",
    "start": "2402880",
    "end": "2410640"
  },
  {
    "text": "enabled so as to handle that scenario now S3 rate limits so ingest",
    "start": "2410640",
    "end": "2419319"
  },
  {
    "text": "push the chunks log chunks into S3 bucket now uh the since the number of",
    "start": "2419319",
    "end": "2425160"
  },
  {
    "text": "API calls started increasing when the more log volume increases S3 started",
    "start": "2425160",
    "end": "2430200"
  },
  {
    "text": "throttling all our request and it started throwing error as well in this scenario we had to limit the number of",
    "start": "2430200",
    "end": "2436079"
  },
  {
    "text": "API calls that is going to S3 pet so where we had to adjust the chunk size uh",
    "start": "2436079",
    "end": "2442440"
  },
  {
    "text": "Max chunk age Etc to minimize the API",
    "start": "2442440",
    "end": "2447560"
  },
  {
    "text": "calls and then comes the querying issue so all these issues that I had discussed",
    "start": "2447560",
    "end": "2452599"
  },
  {
    "text": "now till now is regarding the right path of loop now when it comes to the read path where",
    "start": "2452599",
    "end": "2459079"
  },
  {
    "text": "the querer comes in we have lot of components that are there in front of querer we have querer fren we have Loki",
    "start": "2459079",
    "end": "2467040"
  },
  {
    "text": "Gateway we have an grafana we have inine X Ingress Etc which through which the",
    "start": "2467040",
    "end": "2474000"
  },
  {
    "text": "query is passed by the time querer respond back with the uh query result we",
    "start": "2474000",
    "end": "2480040"
  },
  {
    "text": "used to get a get a timeout and some of these gateways had very like 30 seconds or 60 seconds uh Gateway time out set",
    "start": "2480040",
    "end": "2487319"
  },
  {
    "text": "which in effect to solve we had to go for go go into each and every component",
    "start": "2487319",
    "end": "2492960"
  },
  {
    "text": "and then re reconfigure the Gateway and proxy",
    "start": "2492960",
    "end": "2497880"
  },
  {
    "text": "timeouts thus uh after as I said we were we had launched this Loi in a medium",
    "start": "2498240",
    "end": "2504440"
  },
  {
    "text": "cluster and we came across a bunch of issues and learning uh learnings were",
    "start": "2504440",
    "end": "2509599"
  },
  {
    "text": "there from these issues so cating all of them we reconfigured our blueprint of",
    "start": "2509599",
    "end": "2515040"
  },
  {
    "text": "Loki according to to the issues and how it was solved then and making the same",
    "start": "2515040",
    "end": "2520280"
  },
  {
    "text": "into the facet Cloud blueprint we were confident enough that this after releasing uh this configuration in",
    "start": "2520280",
    "end": "2526800"
  },
  {
    "text": "another cluster we won't be facing the same issues with that confidence we decided to go ahead with other clusters",
    "start": "2526800",
    "end": "2532920"
  },
  {
    "text": "as well and uh to say maximum we had to",
    "start": "2532920",
    "end": "2537960"
  },
  {
    "text": "uh reconfigure some of the CPU and memory uh configuration other than that these same issues we couldn't find in",
    "start": "2537960",
    "end": "2545119"
  },
  {
    "text": "other clusters and they are running fine now uh once it was stabilized across",
    "start": "2545119",
    "end": "2551240"
  },
  {
    "text": "clusters and our death started using Loki across the production clusters we slowly started thinking of uh removing",
    "start": "2551240",
    "end": "2558200"
  },
  {
    "text": "the Legacy setup now uh immediately what we did is we made the logs unavailable",
    "start": "2558200",
    "end": "2564359"
  },
  {
    "text": "in vti and but still fluentd was pushing the logs to EFS as well as S3 bucket and",
    "start": "2564359",
    "end": "2569920"
  },
  {
    "text": "slowly once we were completely confirmed and confident and it was in a green zone",
    "start": "2569920",
    "end": "2575079"
  },
  {
    "text": "we decided to uh face out the whole Legacy system and thus Loki was",
    "start": "2575079",
    "end": "2581599"
  },
  {
    "text": "successfully rolled out in our production clusters now what is the business uh",
    "start": "2581599",
    "end": "2589119"
  },
  {
    "text": "impact we could leverage from the logs now logs are available in grafana uh the querying uh the promql query sorry the",
    "start": "2589119",
    "end": "2596200"
  },
  {
    "text": "lql query uh pattern can be used or the method can be used to querry all the",
    "start": "2596200",
    "end": "2601920"
  },
  {
    "text": "logs from Loki now the business metric that we can generate from the bunch of",
    "start": "2601920",
    "end": "2607680"
  },
  {
    "text": "logs like how many requests are coming in what are the API calls what are the",
    "start": "2607680",
    "end": "2612760"
  },
  {
    "text": "um latencies that we are having Etc we started recording using the Loki",
    "start": "2612760",
    "end": "2619760"
  },
  {
    "text": "recording rules and uh they were pre-computed and sent to Prometheus and",
    "start": "2619760",
    "end": "2625079"
  },
  {
    "text": "that's where we look into the metrics that are generated from the log",
    "start": "2625079",
    "end": "2630760"
  },
  {
    "text": "lines now uh alerts also could be be configured uh for Loki as well as other",
    "start": "2630760",
    "end": "2637319"
  },
  {
    "text": "applications and even we were monitoring Loki performance using Loki alerting",
    "start": "2637319",
    "end": "2642559"
  },
  {
    "text": "rules and different applications which is throwing exceptions errors Etc were",
    "start": "2642559",
    "end": "2649559"
  },
  {
    "text": "alerted using the alerting rules of loing now um if you look back uh if I",
    "start": "2649559",
    "end": "2657359"
  },
  {
    "text": "look back and understand how the whole journey was um Loki is an excellent",
    "start": "2657359",
    "end": "2662920"
  },
  {
    "text": "product and uh it does its job very well but not like uh with the whole default",
    "start": "2662920",
    "end": "2670000"
  },
  {
    "text": "setup uh you won't be able to implement it uh you have to make your adjustments",
    "start": "2670000",
    "end": "2675040"
  },
  {
    "text": "as per the L as per the volume or as per the cluster that you are running in yes",
    "start": "2675040",
    "end": "2681119"
  },
  {
    "text": "there are tons and tons of configurations in Loki that needs to be handled and understood which requires an",
    "start": "2681119",
    "end": "2686760"
  },
  {
    "text": "engineering investment and extensive domain knowledge is something which require because there are a lot of",
    "start": "2686760",
    "end": "2691880"
  },
  {
    "text": "components that are running uh across the uh board which needs to be handled and",
    "start": "2691880",
    "end": "2698680"
  },
  {
    "text": "understood with that uh we'll conclude the session here and thanks thank you",
    "start": "2699359",
    "end": "2706559"
  },
  {
    "text": "again for everyone to join and uh we I would be happy to take the",
    "start": "2706559",
    "end": "2713079"
  },
  {
    "text": "questions yes s so I'll probably quickly uh send over the questions to you so",
    "start": "2713079",
    "end": "2719480"
  },
  {
    "text": "maybe first we'll address the questions that uh that you have to answer sjit",
    "start": "2719480",
    "end": "2726440"
  },
  {
    "text": "um so as ashutosh your question is more directed at promode we'll take that at the end uh Vishnu so Vishnu Raj has",
    "start": "2726440",
    "end": "2735359"
  },
  {
    "text": "asked what are the lead read latencies that you are seeing uh srijit read",
    "start": "2735359",
    "end": "2742240"
  },
  {
    "text": "latencies so I think P95 I just looked up I think in in your largest cluster I",
    "start": "2742240",
    "end": "2747680"
  },
  {
    "text": "do see a few hundred seconds there sounds okay I don't remember the exact",
    "start": "2747680",
    "end": "2754160"
  },
  {
    "text": "numbers V but yeah so Vishnu there I do see um uh like P95 to be like a few",
    "start": "2754160",
    "end": "2763119"
  },
  {
    "text": "hundred seconds uh so did you do anything to address the latency itself did you try any optimization to reduce a",
    "start": "2763119",
    "end": "2769880"
  },
  {
    "text": "quiring latency sjit or are you living with it yeah no no no we had changed the",
    "start": "2769880",
    "end": "2775599"
  },
  {
    "text": "compression ratio compression where was Snappy we changed it to G6 and um we had",
    "start": "2775599",
    "end": "2782079"
  },
  {
    "text": "even change the CH size and query timeouts as well as the querier size and",
    "start": "2782079",
    "end": "2787520"
  },
  {
    "text": "scaling Etc to handle the querying",
    "start": "2787520",
    "end": "2793040"
  },
  {
    "text": "performance okay but uh in in general yeah so from my point of view Vishnu so",
    "start": "2793119",
    "end": "2800040"
  },
  {
    "text": "uh for a for a specific time window when when you're within a localized time window the query performance is not an",
    "start": "2800040",
    "end": "2806920"
  },
  {
    "text": "issue but when you're looking for like events over a day or things like that that's when Loki starts acting up and",
    "start": "2806920",
    "end": "2814119"
  },
  {
    "text": "you you probably need to live with some amount of log log Laten because of course there is this uh turn around time",
    "start": "2814119",
    "end": "2819960"
  },
  {
    "text": "with S3 and things like that um yeah but I don't think so so and also as sjit",
    "start": "2819960",
    "end": "2826640"
  },
  {
    "text": "mentioned initially like uh there are only a few hundred queries happening so more important was to collect all the",
    "start": "2826640",
    "end": "2832680"
  },
  {
    "text": "data and be able to create recording rules and uh you know create dashboards so that people do not need to do uh you",
    "start": "2832680",
    "end": "2839960"
  },
  {
    "text": "know large analytics type queries because that are getting converted into metrics in Prometheus and address from",
    "start": "2839960",
    "end": "2845599"
  },
  {
    "text": "there um yeah so I think Vish had a couple of more questions uh what is a caching",
    "start": "2845599",
    "end": "2851839"
  },
  {
    "text": "system used in the system if possible share the specifications of the cash",
    "start": "2851839",
    "end": "2857559"
  },
  {
    "text": "size uh sjit um I think caching system I think m cach is there sizing I don't",
    "start": "2859280",
    "end": "2866359"
  },
  {
    "text": "have it with me but it's there okay what is the Quick lck Search",
    "start": "2866359",
    "end": "2872839"
  },
  {
    "text": "in grafana yeah Vishnu so that is a custom dashboard that we ended up creating so that people need not write",
    "start": "2872839",
    "end": "2880319"
  },
  {
    "text": "the common log quills at least searching by an application searching by a Time range and searching searching for",
    "start": "2880319",
    "end": "2886160"
  },
  {
    "text": "certain patterns within the selected applications so for those uh we created a custom dashboard in grafana so so it's",
    "start": "2886160",
    "end": "2892119"
  },
  {
    "text": "it's just the name of the dashboard uh it's not something that is packaged with grafana and coming to yeah what is a uh",
    "start": "2892119",
    "end": "2901079"
  },
  {
    "text": "daily compressed log volume in capillary that is running with I think over a TB",
    "start": "2901079",
    "end": "2906800"
  },
  {
    "text": "right that is a compress in S3 per day that the day-to-day increment in S3 uh",
    "start": "2906800",
    "end": "2913000"
  },
  {
    "text": "is is around the TV uh that's that's in on an average",
    "start": "2913000",
    "end": "2919240"
  },
  {
    "text": "right um yeah and Vishnu yeah so S3 throttling uh was removed by increasing",
    "start": "2919240",
    "end": "2925480"
  },
  {
    "text": "the chunk size yeah so that I think is right so basically idea is to reduce the number of API calls um recording rules",
    "start": "2925480",
    "end": "2932640"
  },
  {
    "text": "can create metrics from logs and send to Prometheus yeah so this is something yeah so can you explain this",
    "start": "2932640",
    "end": "2939319"
  },
  {
    "text": "so how do you configure a recording rule is what vishno is asking so basically by",
    "start": "2939319",
    "end": "2945119"
  },
  {
    "text": "specific patterns you can uh extract out metrics and and send it to Prometheus",
    "start": "2945119",
    "end": "2951079"
  },
  {
    "text": "right s maybe you can elaborate a Yeah so basically Vishnu it's you you just",
    "start": "2951079",
    "end": "2956520"
  },
  {
    "text": "configuring the same lql query in the recording rules so the ruler is",
    "start": "2956520",
    "end": "2962400"
  },
  {
    "text": "something that this is used for in the ruler config map you can mention the your rules rules is basically the logl",
    "start": "2962400",
    "end": "2971119"
  },
  {
    "text": "query that you want to generate metrics on which you will be running your aggregation like sum count Etc which",
    "start": "2971119",
    "end": "2977160"
  },
  {
    "text": "will generate the metric metrics with labels and then in the ruler config you can mention which Prometheus you need to",
    "start": "2977160",
    "end": "2984319"
  },
  {
    "text": "send this data to and rest ruler will take care of yeah so maybe uh if we can find out",
    "start": "2984319",
    "end": "2992559"
  },
  {
    "text": "the and put in the link maybe promote if you can do that that will be helpful um",
    "start": "2992559",
    "end": "2999000"
  },
  {
    "text": "yeah so and Vishnu has asked what are the concurrent users that are actively using uh Lok Kei and also about what is",
    "start": "2999000",
    "end": "3005720"
  },
  {
    "text": "the chunk size that you are uh you have configured uh so that you can work around that AP rate limit do you know",
    "start": "3005720",
    "end": "3012319"
  },
  {
    "text": "the exact numbers numbers I haven't I don't have those numbers with me as of",
    "start": "3012319",
    "end": "3018359"
  },
  {
    "text": "now okay uh yeah we we can get back so",
    "start": "3018359",
    "end": "3023839"
  },
  {
    "text": "know so I think we'll at the end plug in our slack channel so maybe we can uh you",
    "start": "3023839",
    "end": "3029000"
  },
  {
    "text": "know connect there and uh discuss a bit more those questions I can maybe answer in the slack Channel where I can get the",
    "start": "3029000",
    "end": "3035599"
  },
  {
    "text": "correct numbers and share with you yeah so now let me go back",
    "start": "3035599",
    "end": "3042280"
  },
  {
    "text": "uh so sh Jan has asked",
    "start": "3042280",
    "end": "3047200"
  },
  {
    "text": "um yeah I was going through the Loki and its analysis done by different people one of the issues with Loki is that",
    "start": "3047440",
    "end": "3053640"
  },
  {
    "text": "since it does not index all text but rather performs a distributed GP so the searching time is a bit long so do you",
    "start": "3053640",
    "end": "3060599"
  },
  {
    "text": "have information on the best ways to deal with it and how well you are able to improve on",
    "start": "3060599",
    "end": "3066359"
  },
  {
    "text": "it so basically the I think the querer side is something that we need to tune",
    "start": "3066359",
    "end": "3071520"
  },
  {
    "text": "for and the number of um so basic the As for the time window um quer will start",
    "start": "3071520",
    "end": "3078160"
  },
  {
    "text": "downloading the logs uh downloading the chunks from S3 bucket and then load that into the memory and then that's where",
    "start": "3078160",
    "end": "3085119"
  },
  {
    "text": "this GP and the searching mechanism happens so most probably what you need to try to look at is how effective the",
    "start": "3085119",
    "end": "3091200"
  },
  {
    "text": "Chun size is how how large is your querer size memory size as well and how",
    "start": "3091200",
    "end": "3097440"
  },
  {
    "text": "the scaling of querer you can uh make it fast those things I would I think that",
    "start": "3097440",
    "end": "3103200"
  },
  {
    "text": "should help more in the making querying faster I think there is Autos scaling",
    "start": "3103200",
    "end": "3108359"
  },
  {
    "text": "set there right sjit it is having HPA yes we have career",
    "start": "3108359",
    "end": "3114200"
  },
  {
    "text": "HPA yeah and yeah so uh jiri has asked regarding",
    "start": "3114200",
    "end": "3121520"
  },
  {
    "text": "the autof forget healthy member uh when running ingestor at stateful sets this flag helps in",
    "start": "3121520",
    "end": "3128559"
  },
  {
    "text": "removing bad ingestor from the ring but the PVC okay he's continued it in another yeah attach seems to have",
    "start": "3128559",
    "end": "3135559"
  },
  {
    "text": "corrupted well when the node terminates abruptly and the new ingester attach attached itself to the PVC could not",
    "start": "3135559",
    "end": "3142839"
  },
  {
    "text": "read the corrupted Val files as a result of a failure is this some is this something that you have experienced is",
    "start": "3142839",
    "end": "3149359"
  },
  {
    "text": "corrupted Val is not something that you have experienced right corrupted Val is not something I have experienced in my",
    "start": "3149359",
    "end": "3156000"
  },
  {
    "text": "in my testing um until now so the Val explosion is something which I have",
    "start": "3156000",
    "end": "3161400"
  },
  {
    "text": "faced let me see if something comes comes around yeah so uh what we",
    "start": "3161400",
    "end": "3167000"
  },
  {
    "text": "experience is that the Val files which should be cleaned up periodically uh does not happen in in case of such",
    "start": "3167000",
    "end": "3173240"
  },
  {
    "text": "unclean ter uh un terminations and when it comes back up we have seen this",
    "start": "3173240",
    "end": "3179760"
  },
  {
    "text": "pattern that it tends to uh you know explode all of a sudden and fill up the disc and which is what sjit mentioned",
    "start": "3179760",
    "end": "3186720"
  },
  {
    "text": "that he had to over protion the dis for the time being and he's raised an issue with uh uh with Loki Community for that",
    "start": "3186720",
    "end": "3194559"
  },
  {
    "text": "um yeah so yeah how did you achieve no log drops",
    "start": "3194559",
    "end": "3201319"
  },
  {
    "text": "did you use any queuing service I guess there's no queuing service assist it's mostly by tuning uh",
    "start": "3201319",
    "end": "3209960"
  },
  {
    "text": "the distributor and just making sure that they are always available to accept the request from prompt and also uh and",
    "start": "3209960",
    "end": "3217599"
  },
  {
    "text": "also you have WR at lock so that nothing gets I mean so in does not you you're not relying on injested availability uh",
    "start": "3217599",
    "end": "3225200"
  },
  {
    "text": "throughout um yeah so and anything else to add this SJ um you can tune the promp",
    "start": "3225200",
    "end": "3230640"
  },
  {
    "text": "tail retrial mechanism which you can reconfigure how much time it should keep",
    "start": "3230640",
    "end": "3236280"
  },
  {
    "text": "trying and those things also can be tuned so that and along with that",
    "start": "3236280",
    "end": "3242000"
  },
  {
    "text": "there's a Max chunk age that Max age I think it's max age I think uh suppose",
    "start": "3242000",
    "end": "3248240"
  },
  {
    "text": "inest is down for last 15 minutes and any lock Beyond 15 minutes it might not",
    "start": "3248240",
    "end": "3253359"
  },
  {
    "text": "accept if that configuration is there so those things you can increase considering there are no log drops or",
    "start": "3253359",
    "end": "3259319"
  },
  {
    "text": "else once inest comes up the and the log log the time stamp of the log is very",
    "start": "3259319",
    "end": "3265799"
  },
  {
    "text": "old automatically it just um promil will have to drop those logs because",
    "start": "3265799",
    "end": "3272680"
  },
  {
    "text": "distributor will not accept the log due to the setting yeah and and uh I think also to",
    "start": "3272680",
    "end": "3279960"
  },
  {
    "text": "monitor this was an important so important aspect so we did have uh so promil of course gives a metric has a",
    "start": "3279960",
    "end": "3287119"
  },
  {
    "text": "drop metrics yes and also there is this Loki Canary test testing that you can do",
    "start": "3287119",
    "end": "3293480"
  },
  {
    "text": "where uh there's a small application that will generate a log and then make sure that the log is searchable through",
    "start": "3293480",
    "end": "3298960"
  },
  {
    "text": "Loki after certain period right uh so this is something that we actively made sure is there in all systems that is one",
    "start": "3298960",
    "end": "3306079"
  },
  {
    "text": "of the big I mean red flags if that is not working and then of course you have component level alerts which will help",
    "start": "3306079",
    "end": "3312640"
  },
  {
    "text": "you isolate where the issue lies so over a period of a few weeks I think we could",
    "start": "3312640",
    "end": "3317799"
  },
  {
    "text": "uh you know optimize on uh all the scaling parameters and uh let's say Chun sizes Q sizes all of those parameters",
    "start": "3317799",
    "end": "3324839"
  },
  {
    "text": "right yeah uh can Loki handle mon a monolith",
    "start": "3324839",
    "end": "3330760"
  },
  {
    "text": "application that generate a few TBS of unstructured log unstructured log not not Jason log uh where we do not have an",
    "start": "3330760",
    "end": "3337680"
  },
  {
    "text": "intelligent way of creating labels uh yeah so this is technically possible so you can do it two ways one",
    "start": "3337680",
    "end": "3344520"
  },
  {
    "text": "is at a prom tail level you could go ahead and uh configure some sort of a I",
    "start": "3344520",
    "end": "3349640"
  },
  {
    "text": "mean you can conf configure a log pattern and extract labels from it this will be a little bit more intensive at",
    "start": "3349640",
    "end": "3355720"
  },
  {
    "text": "the collection end or like one of the ways that I would suggest is like do not",
    "start": "3355720",
    "end": "3362039"
  },
  {
    "text": "index anything in the labels make sure that you're when when you're searching you are searching a reasonable uh time",
    "start": "3362039",
    "end": "3367960"
  },
  {
    "text": "window and what you can do is uh low key lock ql itself has a support where you can specify the lock pattern and it can",
    "start": "3367960",
    "end": "3375000"
  },
  {
    "text": "extract out Fields so you don't really need index labels for this uh you can you can extract out the fields and you",
    "start": "3375000",
    "end": "3381079"
  },
  {
    "text": "can even filter by them so what happens is that the essentially the log lines that are being queried from the index",
    "start": "3381079",
    "end": "3387200"
  },
  {
    "text": "and filtered as based on the times time time window and then what you do is you you use this uh log qls pattern option",
    "start": "3387200",
    "end": "3395119"
  },
  {
    "text": "to uh go and extract out the fields and then filter by it so that would happen at the query in memory in the querer and",
    "start": "3395119",
    "end": "3402559"
  },
  {
    "text": "uh not it will not have to hit uh S3 so that's a more reasonable balancing act",
    "start": "3402559",
    "end": "3407720"
  },
  {
    "text": "that uh you can probably take up unless you of course need to dashboard based on a label or something like",
    "start": "3407720",
    "end": "3413520"
  },
  {
    "text": "that um yeah I think I have gone through most",
    "start": "3413520",
    "end": "3419960"
  },
  {
    "text": "question okay does it support GCS uh also apart from S3 for storage yeah yes",
    "start": "3419960",
    "end": "3425440"
  },
  {
    "text": "and so it supports GCS as well um and um",
    "start": "3425440",
    "end": "3430520"
  },
  {
    "text": "just one more question how many nodes have you deployed to handle 1.5 TB of logs every day yeah so that's an",
    "start": "3430520",
    "end": "3436440"
  },
  {
    "text": "interest so how how much like can you give a ballpark reg of how much for you're spending for inest and or the",
    "start": "3436440",
    "end": "3442359"
  },
  {
    "text": "whole setup maybe I think think I'm running around 7 to eight nodes average what what",
    "start": "3442359",
    "end": "3449200"
  },
  {
    "text": "configuration like uh um 8 CPU 32 GB R 8 core 32 and around 7",
    "start": "3449200",
    "end": "3457359"
  },
  {
    "text": "to eight no what you saying and uh do you retain the log somewhere in the local storage before you flush it out",
    "start": "3457359",
    "end": "3463799"
  },
  {
    "text": "yeah so how much do you retain in the investor how many days 24 hours is it no no no I think like maybe 2 hours or less",
    "start": "3463799",
    "end": "3471440"
  },
  {
    "text": "30 minutes or less that's all oh 30 minutes sorry because Max Maes it's keep on pushing to S3 yeah so 30 minutes",
    "start": "3471440",
    "end": "3479160"
  },
  {
    "text": "stays in uh in inor and everything 30 minutes or the size that I have given",
    "start": "3479160",
    "end": "3486319"
  },
  {
    "text": "which one is first it will push to uh if we are not storing the logs",
    "start": "3486319",
    "end": "3493039"
  },
  {
    "text": "locally then how does a 24hour squarey behave since it has to fetch everything from the uh Object Store",
    "start": "3493039",
    "end": "3501000"
  },
  {
    "text": "uh yeah so yeah so this is something that you have to I mean so it it does to",
    "start": "3501000",
    "end": "3506920"
  },
  {
    "text": "search actually you don't need to fetch all the data you only need to fetch the indexes back from the uh from S3 so that",
    "start": "3506920",
    "end": "3514559"
  },
  {
    "text": "is usually a smaller file to fetch and then you get uh the exact log lines to fetch but yeah for a query range of 24",
    "start": "3514559",
    "end": "3522119"
  },
  {
    "text": "hours uh definitely you're looking at like probably a few seconds of wait time",
    "start": "3522119",
    "end": "3527319"
  },
  {
    "text": "to get the uh results back it's not going to be instant with Loki at least",
    "start": "3527319",
    "end": "3533520"
  },
  {
    "text": "um do we need comput in intensive instance for deploying our Loki instead",
    "start": "3533520",
    "end": "3538680"
  },
  {
    "text": "of memory optimized no we don't I am we are not running on a compute intensive instance",
    "start": "3538680",
    "end": "3546000"
  },
  {
    "text": "we are using M type instances everywhere general purpose and I don't see the CPU stage of L is not that",
    "start": "3546000",
    "end": "3553880"
  },
  {
    "text": "intensive I haven't seen that going uh more than two CPUs but uh memory is",
    "start": "3553880",
    "end": "3559480"
  },
  {
    "text": "something that does more of demand here yeah and he has also asked what",
    "start": "3559480",
    "end": "3566119"
  },
  {
    "text": "components in which ingestor or query which one needs to be more memory intensive uh and which instances need to",
    "start": "3566119",
    "end": "3572880"
  },
  {
    "text": "be computed inors and quer are both memory considering the number of queries",
    "start": "3572880",
    "end": "3578000"
  },
  {
    "text": "and the number of volume that they are handling yeah cool so I think uh we have",
    "start": "3578000",
    "end": "3585359"
  },
  {
    "text": "covered most questions now I think there is one question from ashutosh he wanted a little bit more clarity on the uh",
    "start": "3585359",
    "end": "3591839"
  },
  {
    "text": "consistent hashing ring so promode if you can share the ring ring uh slide",
    "start": "3591839",
    "end": "3597440"
  },
  {
    "text": "maybe I can take a dig at",
    "start": "3597440",
    "end": "3600880"
  },
  {
    "text": "it meanwhile keep the questions coming guys",
    "start": "3606839",
    "end": "3611880"
  },
  {
    "text": "uh uh you can see my screen right yeah yeah so uh so as maybe I'll try to",
    "start": "3611880",
    "end": "3619039"
  },
  {
    "text": "explain this so this is a general this not just a Loki thing uh it's something that you would see across this",
    "start": "3619039",
    "end": "3624559"
  },
  {
    "text": "distributed data stores where uh if you have multiple instances that where the",
    "start": "3624559",
    "end": "3629920"
  },
  {
    "text": "data is spread across uh the strategy of how where which data goes to which",
    "start": "3629920",
    "end": "3635680"
  },
  {
    "text": "instance is determined by this uh method called consistent hashing right so what you do is you you take a range an",
    "start": "3635680",
    "end": "3643200"
  },
  {
    "text": "integer range let's say and put it on the circle like this so for in this particular case it is zero to whatever",
    "start": "3643200",
    "end": "3648680"
  },
  {
    "text": "is that uh the max of unsigned in I think and U whatever and you place the",
    "start": "3648680",
    "end": "3655440"
  },
  {
    "text": "ingestor equid distance uh amongst them so you say 0 to X would be handled by",
    "start": "3655440",
    "end": "3661599"
  },
  {
    "text": "inor 1 x to Y by the other one and Y to Z by the other one that sort of a thing",
    "start": "3661599",
    "end": "3666720"
  },
  {
    "text": "and this data is being stored in console in the case of Loki I think and what happens is this distributor who gets the",
    "start": "3666720",
    "end": "3673079"
  },
  {
    "text": "log line uh determines the stream and based on the stream and the Tenant it would uh like uh it it would uh compute",
    "start": "3673079",
    "end": "3680200"
  },
  {
    "text": "a hash so hash function spits out a number in this particular range and then it will figure out which range",
    "start": "3680200",
    "end": "3687480"
  },
  {
    "text": "the number belongs to and uh select the ingestor that is adjacent to it so",
    "start": "3687480",
    "end": "3693440"
  },
  {
    "text": "that's how you assign the uh ingestor to streams and if you have a replication factor of more than one uh you assign it",
    "start": "3693440",
    "end": "3701440"
  },
  {
    "text": "to one more neighbor uh of that investor so the idea is that even if one investor goes uh goes away uh you have a simple",
    "start": "3701440",
    "end": "3710000"
  },
  {
    "text": "strategy on who needs to uh take care of what data and you have uh people with",
    "start": "3710000",
    "end": "3715680"
  },
  {
    "text": "redundancy so if a data needs to be copied over from one instance to the other uh the data is still available uh",
    "start": "3715680",
    "end": "3721720"
  },
  {
    "text": "in the uh neighboring instance so uh that that's about the hashing ring so",
    "start": "3721720",
    "end": "3727440"
  },
  {
    "text": "which is where the interesting thing happened for uh s right so he had the app label so app app label I think in",
    "start": "3727440",
    "end": "3734400"
  },
  {
    "text": "touch API started generating a lot of logs and everything was going to a particular investor and that caused it",
    "start": "3734400",
    "end": "3740839"
  },
  {
    "text": "to go out of memory but once he enabled sharding automated sharding so the number of streams got split out",
    "start": "3740839",
    "end": "3747079"
  },
  {
    "text": "and so now it is more evenly distributed across all the uh all the inest instances uh so yeah so I think this",
    "start": "3747079",
    "end": "3754200"
  },
  {
    "text": "hashing ring concept is something that that everyone needs to be aware of uh while using Loki because uh you can",
    "start": "3754200",
    "end": "3761319"
  },
  {
    "text": "narrow down many issues uh by understanding the architecture of this",
    "start": "3761319",
    "end": "3766799"
  },
  {
    "text": "one um I if that is clear um if you have any",
    "start": "3766799",
    "end": "3772520"
  },
  {
    "text": "specific questions on that let me know okay also ADI has linked our uh",
    "start": "3772520",
    "end": "3780079"
  },
  {
    "text": "slack Community uh uh Link in the chat uh so if if you want like I think people",
    "start": "3780079",
    "end": "3787000"
  },
  {
    "text": "who needed exact numbers and sizings and and things like that we can uh connect",
    "start": "3787000",
    "end": "3792640"
  },
  {
    "text": "over slack and discuss um and of course there will be more issues that will be coming up in Loki and and whatnot uh so",
    "start": "3792640",
    "end": "3801000"
  },
  {
    "text": "it it'll be a good place to share the knowledge and our experience as well uh",
    "start": "3801000",
    "end": "3806079"
  },
  {
    "text": "there's also an AI bot that answers some questions about facets so so those who use facets do try it out uh if if it is",
    "start": "3806079",
    "end": "3813839"
  },
  {
    "text": "able to give you uh good responses yeah so I think we can wind up the",
    "start": "3813839",
    "end": "3821160"
  },
  {
    "text": "session um thanks everyone for uh joining uh and also see you around in",
    "start": "3821160",
    "end": "3827000"
  },
  {
    "text": "the community and we'll have more of these sessions thanks",
    "start": "3827000",
    "end": "3832599"
  },
  {
    "text": "everyone thanks everyone",
    "start": "3832599",
    "end": "3836279"
  }
]