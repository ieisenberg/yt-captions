[
  {
    "text": "okay hello everyone welcome to our talk cube guns us killing AI inference with",
    "start": "0",
    "end": "7049"
  },
  {
    "text": "kubernetes GPUs so let's present ourselves first my name is Renault Gerber I've been working at a",
    "start": "7049",
    "end": "13200"
  },
  {
    "text": "video for past two years on containers kubernetes and open source and I'm Ryan Olson I'm a solution architect for deep",
    "start": "13200",
    "end": "19770"
  },
  {
    "text": "learning HPC and I guess cloud we call renault mr. kubernetes because he's kind",
    "start": "19770",
    "end": "25859"
  },
  {
    "text": "of our main guy so I'm I'm kind of a GPU user and a person who's developing",
    "start": "25859",
    "end": "31199"
  },
  {
    "text": "applications for GPUs and then using kubernetes to scale those out well Renault is actually working on kubernetes directly yeah and we've",
    "start": "31199",
    "end": "40590"
  },
  {
    "text": "actually started interacting with the community two years ago when I joined the Nvidia with the container team and a",
    "start": "40590",
    "end": "47010"
  },
  {
    "text": "lot of the work that we've been doing in the in the kubernetes ecosystem is making sure that GPUs are acknowledged",
    "start": "47010",
    "end": "55020"
  },
  {
    "text": "as a first-class citizen and not something that you hack your way inside",
    "start": "55020",
    "end": "60570"
  },
  {
    "text": "a lot of effort has been and has gone in the community a lot of things and",
    "start": "60570",
    "end": "66810"
  },
  {
    "text": "interactions have been involved and for example you can see that and the last",
    "start": "66810",
    "end": "73080"
  },
  {
    "text": "face to face that was in March was actually actually happened at Nvidia so",
    "start": "73080",
    "end": "80150"
  },
  {
    "text": "today we're gonna talk about scaling AI interest with kubernetes and GPUs sure and so to do this I'll suppose with you",
    "start": "80150",
    "end": "87720"
  },
  {
    "text": "yeah give us some like obligatory like why do we care and then I want to break",
    "start": "87720",
    "end": "93780"
  },
  {
    "text": "down the title of the talk scaling ai inference with kubernetes and GPUs and really talk about what that",
    "start": "93780",
    "end": "99000"
  },
  {
    "text": "overloaded scaling term means scaling the GPUs means something different than scaling with kubernetes normally with",
    "start": "99000",
    "end": "105299"
  },
  {
    "text": "kubernetes we talk about scale out the scaling with GPUs means scaling up or adding some new capability to your nodes",
    "start": "105299",
    "end": "111270"
  },
  {
    "text": "that weren't there before we'll talk about that then scaling the AI inference pipeline this is a very different",
    "start": "111270",
    "end": "116759"
  },
  {
    "text": "pipeline than your traditional microservices so what are the new parameters of that how to look for",
    "start": "116759",
    "end": "123899"
  },
  {
    "text": "potential bottlenecks within that pipeline and how do they fit into your infrastructure and then of course we're",
    "start": "123899",
    "end": "129479"
  },
  {
    "text": "gonna go into actually scaling out kubernetes so why do we care it's an AI",
    "start": "129479",
    "end": "135760"
  },
  {
    "text": "they eye inference defund a machine learning it's everywhere in the news it's kind of a big deal it's touching",
    "start": "135760",
    "end": "141550"
  },
  {
    "text": "every vertical and here's the examples of video speech and recommenders they're",
    "start": "141550",
    "end": "146920"
  },
  {
    "text": "used that it's a hyper scale level everyone kind of knows about this kind of stuff but it's also being used in",
    "start": "146920",
    "end": "152560"
  },
  {
    "text": "other verticals and so I kind of like to like switch gears a little bit and talk about some examples that you might not have heard of but these are some",
    "start": "152560",
    "end": "158290"
  },
  {
    "text": "examples of how these new ways these new state-of-the-art methods things that",
    "start": "158290",
    "end": "163360"
  },
  {
    "text": "have been traditionally extremely difficult for traditional machine learning to do now with deep neural",
    "start": "163360",
    "end": "168820"
  },
  {
    "text": "networks you know the ability to perceive the world and to make",
    "start": "168820",
    "end": "174030"
  },
  {
    "text": "predictions has just has fundamentally changed how we've approached some of our",
    "start": "174030",
    "end": "179200"
  },
  {
    "text": "business aspects so first example here is medical imaging this one hits pretty close to home this is doing a stroke",
    "start": "179200",
    "end": "185530"
  },
  {
    "text": "analysis so 40% of radiology images that come in are classified as high priority",
    "start": "185530",
    "end": "191110"
  },
  {
    "text": "like if you're having a stroke and you need you need a quick evaluation you're",
    "start": "191110",
    "end": "198400"
  },
  {
    "text": "still in a queue of 40% of high priority emergency cases and the sooner that you",
    "start": "198400",
    "end": "203470"
  },
  {
    "text": "can get evaluated the sooner that they can actually deliver potential medicines",
    "start": "203470",
    "end": "208600"
  },
  {
    "text": "that could be good or bad for your stroke condition so having an AI that can do this could potentially be",
    "start": "208600",
    "end": "214570"
  },
  {
    "text": "life-saving in some of those scenarios the next two examples an infrastructure in this",
    "start": "214570",
    "end": "220329"
  },
  {
    "text": "industry these are examples of predictive maintenance so in the example of emergency pothole repair is it turns",
    "start": "220329",
    "end": "227920"
  },
  {
    "text": "out like if you pretty with all predictive maintenance if you let the problem go too long it costs way more to",
    "start": "227920",
    "end": "233769"
  },
  {
    "text": "fix it then if you can fix it early so this is an example of using data and using some deep neural networks",
    "start": "233769",
    "end": "240340"
  },
  {
    "text": "to apply that to say where we have a limited amount of budget for our road",
    "start": "240340",
    "end": "245350"
  },
  {
    "text": "repairs where do we specify where do we actually go do that before those potholes become a problem and so try to",
    "start": "245350",
    "end": "252060"
  },
  {
    "text": "predict where those emergency cases will be and correct them before they become an emergency and the same is true in the",
    "start": "252060",
    "end": "259390"
  },
  {
    "text": "industry this is with GE with these big gas turbines when you go and take those big gas turbines down",
    "start": "259390",
    "end": "265270"
  },
  {
    "text": "you actually want to like go in and do your repair specifically where you need to if you know about bearing is loose if",
    "start": "265270",
    "end": "271509"
  },
  {
    "text": "you know like a fan blade needs to replace you want to have all these sensors attached that until you use these deep neural networks to then",
    "start": "271509",
    "end": "277770"
  },
  {
    "text": "predict when you take it down and then when you do take it down where you focus your efforts doing that maintenance so",
    "start": "277770",
    "end": "284889"
  },
  {
    "text": "it's really pretty much touching every industry so it's really exciting and so",
    "start": "284889",
    "end": "291159"
  },
  {
    "text": "now we're gonna switch gears and talk about scaling with GPUs so these networks over the last you know eight",
    "start": "291159",
    "end": "297250"
  },
  {
    "text": "six to eight years are just getting more and more complex we throw more data at them we throw more compute at them and",
    "start": "297250",
    "end": "302949"
  },
  {
    "text": "they get better and better and better so this is just an example that the big dot",
    "start": "302949",
    "end": "309030"
  },
  {
    "text": "is basically a the multiplicative effect of the compute times the memory",
    "start": "309030",
    "end": "314800"
  },
  {
    "text": "bandwidth and so you can see that the complexity of the compute complexities gain greater over time so how do we",
    "start": "314800",
    "end": "321009"
  },
  {
    "text": "combat that we combat that with our tensor core GPUs so if you want tenser course we have these Jenga sets in our",
    "start": "321009",
    "end": "327610"
  },
  {
    "text": "booth our tensor cords and our GPUs are specific Asics designed for the key",
    "start": "327610",
    "end": "334389"
  },
  {
    "text": "compute in deep neural networks specifically matrix-matrix multiply as it accumulates so these are a sixes are",
    "start": "334389",
    "end": "340030"
  },
  {
    "text": "added into the sm units so our traditional FP 32 into 8 FP 64 in some",
    "start": "340030",
    "end": "345610"
  },
  {
    "text": "cases and now add on tensor cores for deep learning and in our training",
    "start": "345610",
    "end": "350710"
  },
  {
    "text": "products RTX for real-time rate tracing these tensor cores provide an incredible",
    "start": "350710",
    "end": "359110"
  },
  {
    "text": "amount of extra performance and and that helps combat that growing curve of",
    "start": "359110",
    "end": "365259"
  },
  {
    "text": "compute needed to evaluate these models so it was pretty exciting is this small",
    "start": "365259",
    "end": "370870"
  },
  {
    "text": "little GPU this t4 GPU is 70 watts you get 75 watts of power on your PC IU",
    "start": "370870",
    "end": "378430"
  },
  {
    "text": "bus this is 70 watts so it fits in that profile you can drop a t4 into any server and get incredible amount of",
    "start": "378430",
    "end": "385000"
  },
  {
    "text": "extra compute capability on your device this is what we call scaling up I'm gonna show you an example of that right",
    "start": "385000",
    "end": "391659"
  },
  {
    "text": "now this is resident 152 running on traditional CPU every time",
    "start": "391659",
    "end": "399270"
  },
  {
    "text": "you see a flash that's an inference request comes going out and coming back to the server so you",
    "start": "399270",
    "end": "405810"
  },
  {
    "text": "can see our this is a 24v CPU skylake",
    "start": "405810",
    "end": "411210"
  },
  {
    "text": "server is getting about three to four images per second and actually this demo",
    "start": "411210",
    "end": "417240"
  },
  {
    "text": "might not work so well if you guys are all on the network because we're on the Wi-Fi and you can actually see this as running live so if you see a pause that",
    "start": "417240",
    "end": "424530"
  },
  {
    "text": "means we're actually getting some Network interference this is when we",
    "start": "424530",
    "end": "430650"
  },
  {
    "text": "switch over to the T poor this is the same exact model running on a part that",
    "start": "430650",
    "end": "436590"
  },
  {
    "text": "has the same wattage and you can now get 1,500 images per second this is why GPUs",
    "start": "436590",
    "end": "442170"
  },
  {
    "text": "are so valuable for evaluating deep neural networks",
    "start": "442170",
    "end": "446930"
  },
  {
    "text": "next the inference pipeline says the inference pipeline is very different",
    "start": "453420",
    "end": "459960"
  },
  {
    "text": "than well not very different but different enough than your traditional micro-service pipeline these are I'm",
    "start": "459960",
    "end": "466740"
  },
  {
    "text": "going to talk about some tools so upfront I'm gonna give you some acronyms if you're in the industry you understand",
    "start": "466740",
    "end": "472080"
  },
  {
    "text": "these if not maybe it gives you some places to go to understand so the tools that I'm gonna talk about is NGC nvidia",
    "start": "472080",
    "end": "479190"
  },
  {
    "text": "GPU cloud this is a registry of docker images these work really well on the kubernetes land everything that's below",
    "start": "479190",
    "end": "485910"
  },
  {
    "text": "here are basically packaged within those containers so we have envy RPC which is",
    "start": "485910",
    "end": "491550"
  },
  {
    "text": "really just a simple wraparound gr PC this is to help build micro services basically billion compute bound micro",
    "start": "491550",
    "end": "498330"
  },
  {
    "text": "services and compute bound async micro",
    "start": "498330",
    "end": "503490"
  },
  {
    "text": "service simplifying all the simplifying all the extra overhead and extra",
    "start": "503490",
    "end": "510000"
  },
  {
    "text": "boilerplate code that goes into writing an e-signature RPC the tensor T product",
    "start": "510000",
    "end": "515039"
  },
  {
    "text": "is a library for optimizing it's basically an inference optimising compiler it's one of the reasons why we",
    "start": "515040",
    "end": "520800"
  },
  {
    "text": "can run so many images per second on such a little GPU the tensor T inference server actually wraps not the",
    "start": "520800",
    "end": "528510"
  },
  {
    "text": "capabilities of it's it's the actual service it's a rat's tensor rt+ tensor",
    "start": "528510",
    "end": "535680"
  },
  {
    "text": "flow plus Chi f8 and then deliver it delivers and so this is one of our",
    "start": "535680",
    "end": "541890"
  },
  {
    "text": "containers and then kubernetes for GPU orchestration the cloud days it projects",
    "start": "541890",
    "end": "548400"
  },
  {
    "text": "that we're going to use we're going to talk about G RPC envoy for load balancing SEO for service matched and",
    "start": "548400",
    "end": "553950"
  },
  {
    "text": "also for load balancing Prometheus for metrics and then rook for model store and intermediate results so what these",
    "start": "553950",
    "end": "562500"
  },
  {
    "text": "are compute Python would like so in the example I showed you of images flashing that's our input data is the image",
    "start": "562500",
    "end": "570510"
  },
  {
    "text": "itself so this could be speech that would just be a sentence and it could go through a network you can convert it to",
    "start": "570510",
    "end": "576270"
  },
  {
    "text": "friend which so we've ever known I can have a good conversation",
    "start": "576270",
    "end": "581839"
  },
  {
    "text": "but before goes into the network that input data has to get converted into some raw tensors and so there's this",
    "start": "582700",
    "end": "588670"
  },
  {
    "text": "transformation stuff that has to happen and that chance transformation step either happens on the CPU or on the GPU",
    "start": "588670",
    "end": "594430"
  },
  {
    "text": "and we call that pre pre processing once it's converted to the input tensors of the network it has to move into GPU",
    "start": "594430",
    "end": "601180"
  },
  {
    "text": "memory so that's a transaction over the PCI bus that you have to make into account more then the compute itself which is the",
    "start": "601180",
    "end": "607810"
  },
  {
    "text": "evaluation of the deep neural network and that's using usually using tensor RT or potentially a back-end framework",
    "start": "607810",
    "end": "614640"
  },
  {
    "text": "inside of our your inference server and then you have the same kind of reverse onion appeal going back outside to",
    "start": "614640",
    "end": "621850"
  },
  {
    "text": "output tensors going back to hosts memory some post processing to convert those output tensors into some",
    "start": "621850",
    "end": "627790"
  },
  {
    "text": "consumable output so we might output a whole bunch of bounding boxes but then we need to actually convert them to the",
    "start": "627790",
    "end": "634030"
  },
  {
    "text": "dimensions like the XYZ coordinates of that bounding box within an image so if",
    "start": "634030",
    "end": "640390"
  },
  {
    "text": "we want to get the best use out of our GPUs and performance we want to make sure that we keep this pipeline full and",
    "start": "640390",
    "end": "646660"
  },
  {
    "text": "that there's no particularly long pole in the tent so we want to make sure that this path this pipeline is well balanced",
    "start": "646660",
    "end": "654030"
  },
  {
    "text": "both with data movement as well as compute and so what that means is we're integrating a lot of HPC best practices",
    "start": "654030",
    "end": "660970"
  },
  {
    "text": "into the data center workloads as you saw these networks as they perform on",
    "start": "660970",
    "end": "666130"
  },
  {
    "text": "CPU versus GPU they have an incredibly different compute balance and what typical data center workloads have had",
    "start": "666130",
    "end": "673000"
  },
  {
    "text": "in the past and so because of that they also have an incredible amount of data movement that can happen as well it's a",
    "start": "673000",
    "end": "680170"
  },
  {
    "text": "balancing those two things out of this critical foot performance so what are the bottlenecks moving data is always a",
    "start": "680170",
    "end": "686680"
  },
  {
    "text": "bottleneck so somehow we have to get it from our input to our compute and there's a lot of different places where",
    "start": "686680",
    "end": "692890"
  },
  {
    "text": "data can be moved and a lot of different conversions of data from one format to",
    "start": "692890",
    "end": "698800"
  },
  {
    "text": "another that we have to account for that big one is that input to input tensors or output tensors back to output",
    "start": "698800",
    "end": "705780"
  },
  {
    "text": "depending on your problem it's extremely problem-specific so an image image is almost your",
    "start": "705780",
    "end": "712630"
  },
  {
    "text": "best-case scenario whereas potentially for language models the the input might be incredibly small",
    "start": "712630",
    "end": "719650"
  },
  {
    "text": "but the output tensor could be megabytes or gigabytes so you have to think to yourself if I'm building completely",
    "start": "719650",
    "end": "725470"
  },
  {
    "text": "discrete micro-services and I converts a small sentence into this hundreds of",
    "start": "725470",
    "end": "731230"
  },
  {
    "text": "megabyte tensors I don't want to move that over the network again do I because all of a sudden I'm gonna become Network",
    "start": "731230",
    "end": "737890"
  },
  {
    "text": "balanced so thinking about how big these memory objects are and where do I move",
    "start": "737890",
    "end": "744190"
  },
  {
    "text": "them is going to be critical for success and so success here is really defined by",
    "start": "744190",
    "end": "749310"
  },
  {
    "text": "making sure that you understand your pipeline that you choose the right hardware and the right set of software",
    "start": "749310",
    "end": "755500"
  },
  {
    "text": "to be successful so we're gonna kind of start at compute and work our way back",
    "start": "755500",
    "end": "762450"
  },
  {
    "text": "for computes the actual valuation of the deep neural network this these are our",
    "start": "762450",
    "end": "769360"
  },
  {
    "text": "options ideally we use the tensor RT product this is a library that is",
    "start": "769360",
    "end": "774940"
  },
  {
    "text": "designed to both optimize and compute a neural network it gives you the best possible performance the best possible",
    "start": "774940",
    "end": "780940"
  },
  {
    "text": "memory footprint you have really precise control over all of the the memory buffers as well as the precision that's",
    "start": "780940",
    "end": "787390"
  },
  {
    "text": "being used and it's an incredibly deployable package you just packages up as a library and you can ship it off as",
    "start": "787390",
    "end": "793900"
  },
  {
    "text": "a C++ application or Python but it also has the lowest DNN compatibility so to",
    "start": "793900",
    "end": "801340"
  },
  {
    "text": "come back combat that we integrate tensor RT into the some of the frameworks so tensorflow has it",
    "start": "801340",
    "end": "806650"
  },
  {
    "text": "integrated and just recently PI torches integrated tensor RT operations as well which covers about 95 98 percent of the",
    "start": "806650",
    "end": "814360"
  },
  {
    "text": "deep learning community the the pitfalls of integrating the framework is now you",
    "start": "814360",
    "end": "819370"
  },
  {
    "text": "have the extra framework overheads frameworks are generally designed for the training process not necessary the inference process and who owns the",
    "start": "819370",
    "end": "826540"
  },
  {
    "text": "memory is kind of a slight battle so it's a little bit less efficient and",
    "start": "826540",
    "end": "832110"
  },
  {
    "text": "generally it requires you don't get quite as much throughput as long",
    "start": "832110",
    "end": "838240"
  },
  {
    "text": "pressing and finally the worst case scenario is if the model doesn't convert to tensor RT at all",
    "start": "838240",
    "end": "843550"
  },
  {
    "text": "you're just in in the realm of using the framework to evaluate the inference model so preferred is",
    "start": "843550",
    "end": "849890"
  },
  {
    "text": "- right green to red so what test our tea does as a product is it evaluates a",
    "start": "849890",
    "end": "856070"
  },
  {
    "text": "neural network and it does a whole bunch of symbolic optimizations on the graph as well as runtime the valuation but",
    "start": "856070",
    "end": "863360"
  },
  {
    "text": "what's really really neat about it is the same network evaluated on a small",
    "start": "863360",
    "end": "868370"
  },
  {
    "text": "GPU like a t4 or a big GPU like a v100 will have based on the different compute",
    "start": "868370",
    "end": "874310"
  },
  {
    "text": "characteristics of the device what tensor cores are available how much memory bandwidth is available that the",
    "start": "874310",
    "end": "879860"
  },
  {
    "text": "temps Archie auto-tuning will choose the proper kernels for the device and make sure that you have the",
    "start": "879860",
    "end": "886730"
  },
  {
    "text": "most optimal run time so it's really a pretty amazing product and the result of",
    "start": "886730",
    "end": "893540"
  },
  {
    "text": "that is these incredible gains in performance so we see 21 X speed ups in deep speech - 27 X in resin at 50",
    "start": "893540",
    "end": "901269"
  },
  {
    "text": "actually sometimes even better and then the 36 X for nmt next is if we step back",
    "start": "901269",
    "end": "910070"
  },
  {
    "text": "from the compute we have the pre and post processing and this is actually where you have to really start thinking",
    "start": "910070",
    "end": "915110"
  },
  {
    "text": "about it from a deployment perspective as as I kind of alluded to before you",
    "start": "915110",
    "end": "920449"
  },
  {
    "text": "can do that transformation of the input to input tensors on CPU or GPU an example here would be video decode is a",
    "start": "920449",
    "end": "927440"
  },
  {
    "text": "great example of just doing everything on device h.264 video coming in can get decoded on device go directly into GPU",
    "start": "927440",
    "end": "934040"
  },
  {
    "text": "memory and being be inferred directly meaning that the CPU component of it is very small which changes your deployment",
    "start": "934040",
    "end": "941089"
  },
  {
    "text": "if you have a huge amount of CPU compute that is required to do that transformation or that the sizes are",
    "start": "941089",
    "end": "946610"
  },
  {
    "text": "really big then the location of it is very important so there's four primary",
    "start": "946610",
    "end": "952699"
  },
  {
    "text": "locations that you can do it you can do it in process so we open source a10",
    "start": "952699",
    "end": "957800"
  },
  {
    "text": "start to the infant server you can build your entire pre and post processing pipeline directly into the temp",
    "start": "957800",
    "end": "962899"
  },
  {
    "text": "directory in print server you just download the code add your stuff in recompile it now you're in process",
    "start": "962899",
    "end": "968630"
  },
  {
    "text": "that's the fastest possible way to go from pre and post processing to compute",
    "start": "968630",
    "end": "973790"
  },
  {
    "text": "problem is you're coupled in your scaling right so that kind of breaks traditional micro services now to get",
    "start": "973790",
    "end": "981500"
  },
  {
    "text": "more pre and post processing you have to gaile up your compute as well a similar",
    "start": "981500",
    "end": "986749"
  },
  {
    "text": "way if you want to keep your logic decoupled and separate you can do in pod so you have a container in pod that does",
    "start": "986749",
    "end": "993529"
  },
  {
    "text": "pre and post-processing and the tensor RT inference server that way you don't have to touch the tensor T inference server you can build your your logic",
    "start": "993529",
    "end": "1000639"
  },
  {
    "text": "external from it but you can still use things like system v IPC and it's",
    "start": "1000639",
    "end": "1006850"
  },
  {
    "text": "basically the shared namespaces of the pod to get better performance on moving the data between the two and you might",
    "start": "1006850",
    "end": "1012370"
  },
  {
    "text": "still have to make some modifications to the inference server in this example you use shared memory but we're looking on adding that next though your back would",
    "start": "1012370",
    "end": "1020110"
  },
  {
    "text": "be like in node and this actually represents a few problems but you so you can use positive finding these to",
    "start": "1020110",
    "end": "1025870"
  },
  {
    "text": "co-locate your pre and post-processing containers on the same note as the thing that's actually doing the compute but",
    "start": "1025870",
    "end": "1033130"
  },
  {
    "text": "you might need to have some hacks to break down the name space barrier so using system v shared memory is an",
    "start": "1033130",
    "end": "1038798"
  },
  {
    "text": "example there is no way to say I would like to use the same IP the same IPC",
    "start": "1038799",
    "end": "1044558"
  },
  {
    "text": "namespace as this pod in kubernetes the only way to do it is to go down to host which really kind of breaks this a whole",
    "start": "1044559",
    "end": "1051730"
  },
  {
    "text": "containerization strategy so I think working with the community they're on better like namespace affinities would",
    "start": "1051730",
    "end": "1059649"
  },
  {
    "text": "be pretty great and finally like fully independent like traditional micro services pre post processing puts all of",
    "start": "1059649",
    "end": "1065919"
  },
  {
    "text": "its data on a network stack and sends it and that can work in some scenarios but in other scenarios this would be",
    "start": "1065919",
    "end": "1073059"
  },
  {
    "text": "prohibitively expensive and at the end of the day is you really have to think about data movement data movement is is",
    "start": "1073059",
    "end": "1080260"
  },
  {
    "text": "absolutely the critical bottlenecking components serving so those are the",
    "start": "1080260",
    "end": "1088029"
  },
  {
    "text": "compute and pre and post-processing step those steps that you have to kind of go through and benchmark and think about what it is that you're trying to deploy",
    "start": "1088029",
    "end": "1094570"
  },
  {
    "text": "this is how you actually serve it so we have this tensor RT inference server we",
    "start": "1094570",
    "end": "1101679"
  },
  {
    "text": "package all of this up into not just an application but we also it also lives in a container that you can download from",
    "start": "1101679",
    "end": "1107980"
  },
  {
    "text": "our ng C so ng Co and video comm the advantages of this is we do all the",
    "start": "1107980",
    "end": "1114220"
  },
  {
    "text": "logic for you so we offer tunable concurrency you can choose to optimize for latency or a throughput or",
    "start": "1114220",
    "end": "1120769"
  },
  {
    "text": "some slider in between and usually you can find the sweet spot in between that allows you to get kind of the best",
    "start": "1120769",
    "end": "1126889"
  },
  {
    "text": "throughput with only the minimal increase in latency because of some of",
    "start": "1126889",
    "end": "1133370"
  },
  {
    "text": "the challenges that were kind of alluded to in previous talks about how to oversubscribed GPUs we support multiple",
    "start": "1133370",
    "end": "1139700"
  },
  {
    "text": "models within the same process and this is a big advantage because it allows us to simplify the memory management on the",
    "start": "1139700",
    "end": "1146659"
  },
  {
    "text": "GPU and to get to interleave and get good to get good overlap between model",
    "start": "1146659",
    "end": "1156679"
  },
  {
    "text": "different models under different loads and you can actually see this example down in our booth we're running four different models being perfectly",
    "start": "1156679",
    "end": "1163940"
  },
  {
    "text": "interleaved on the same device we essentially support all AI frameworks either through conversion for tonics or",
    "start": "1163940",
    "end": "1171529"
  },
  {
    "text": "with tensor flow directly we support tensor flow and cafe - and $0.10 RTS",
    "start": "1171529",
    "end": "1177230"
  },
  {
    "text": "backends but Onix helps gives us to the rest of the frameworks now if you so if",
    "start": "1177230",
    "end": "1184940"
  },
  {
    "text": "you don't integrate your pre and post-processing steps into your into the",
    "start": "1184940",
    "end": "1190129"
  },
  {
    "text": "inference server itself you need to do have some sort of service that does that so you can build that with your favorite",
    "start": "1190129",
    "end": "1195409"
  },
  {
    "text": "micro service library we have some examples here actually those examples will be",
    "start": "1195409",
    "end": "1201980"
  },
  {
    "text": "published later but we call them the middleman service in the bathroom service we use this and the RPC wrapper",
    "start": "1201980",
    "end": "1208279"
  },
  {
    "text": "around G RPC you can find that into the inference server itself and we'll publish some more examples on it in the",
    "start": "1208279",
    "end": "1213919"
  },
  {
    "text": "future we have some examples on this pre pre post-processing where the data comes",
    "start": "1213919",
    "end": "1220730"
  },
  {
    "text": "in and basically acts as a middleman and then the priam the middleman service talks to the tense RT inference service",
    "start": "1220730",
    "end": "1226759"
  },
  {
    "text": "on your behalf or the batching service which collects low batch requests before",
    "start": "1226759",
    "end": "1232399"
  },
  {
    "text": "the load balancer types them through a load balancer to a back-end so there's some examples metrics",
    "start": "1232399",
    "end": "1240960"
  },
  {
    "text": "so metrics are really important you can always get note level metrics but no",
    "start": "1240960",
    "end": "1247330"
  },
  {
    "text": "level metrics only give you half the story if you really want to like make",
    "start": "1247330",
    "end": "1253210"
  },
  {
    "text": "smart intelligent decisions on when to scale your deployment you want to actually have application level metrics",
    "start": "1253210",
    "end": "1259420"
  },
  {
    "text": "and so our ten start to the inference server these are the metrics that we provide so we have different ways to",
    "start": "1259420",
    "end": "1266290"
  },
  {
    "text": "look at GPU utilization on the inference load as well as the latency and so all",
    "start": "1266290",
    "end": "1272680"
  },
  {
    "text": "of these are exposed as Prometheus metrics and can just be absorbed and viewed in group on a-- and again that's",
    "start": "1272680",
    "end": "1279250"
  },
  {
    "text": "an example that's being run in the booth right now so with that you get some",
    "start": "1279250",
    "end": "1287410"
  },
  {
    "text": "examples here with helm charts and you installed the Prometheus software Thank You Prometheus as usual and then you get",
    "start": "1287410",
    "end": "1294310"
  },
  {
    "text": "these dashboards here's one example and then here's another example and this is",
    "start": "1294310",
    "end": "1299890"
  },
  {
    "text": "what's actually running down in our booth and yep so the question that you",
    "start": "1299890",
    "end": "1310090"
  },
  {
    "text": "might ask when you actually and when we actually run through this whole pipeline is how does this intervention Bernays",
    "start": "1310090",
    "end": "1316180"
  },
  {
    "text": "and kubernetes is actually a really interesting area of focus right now because it's synergize really well all",
    "start": "1316180",
    "end": "1323650"
  },
  {
    "text": "containers kubernetes and GPUs synergize really well the way that we see things",
    "start": "1323650",
    "end": "1329140"
  },
  {
    "text": "is that before kubernetes and containers people were actually building their",
    "start": "1329140",
    "end": "1335080"
  },
  {
    "text": "model on their own machines and actually going through the process of training on these machines and once they actually",
    "start": "1335080",
    "end": "1341710"
  },
  {
    "text": "got their applications they would actually give it to their IT ops or sysadmin these people would then start a",
    "start": "1341710",
    "end": "1351010"
  },
  {
    "text": "VM or if we run them on their cluster and so the that old way of doing things",
    "start": "1351010",
    "end": "1358980"
  },
  {
    "text": "actually is starting to change and this is where people are who are actually",
    "start": "1358980",
    "end": "1365650"
  },
  {
    "text": "building clusters and their own clouds are thinking about how this integrates",
    "start": "1365650",
    "end": "1372280"
  },
  {
    "text": "with kubernetes and how this interface with GPU and HPC and so the rise of actually",
    "start": "1372280",
    "end": "1378370"
  },
  {
    "text": "compute has allowed this new way of seeing the cluster as the thing that",
    "start": "1378370",
    "end": "1384100"
  },
  {
    "text": "you're gonna let your user actually run their GP workloads and their HPC",
    "start": "1384100",
    "end": "1390190"
  },
  {
    "text": "workloads and so people start building their data center not just around one",
    "start": "1390190",
    "end": "1395530"
  },
  {
    "text": "single place where you saw or where you saw store all your machines but also as",
    "start": "1395530",
    "end": "1401560"
  },
  {
    "text": "a place where you want to be able to have HPC and you want to be able to have data flow and you want to be able to",
    "start": "1401560",
    "end": "1407770"
  },
  {
    "text": "have your user run one model on one machine one model on multiple machines multiple models on multiple machines and",
    "start": "1407770",
    "end": "1414700"
  },
  {
    "text": "so this new way of seeing your cluster has been enabled not only by kubernetes",
    "start": "1414700",
    "end": "1420280"
  },
  {
    "text": "containers but also by the rise of hyper so high performance computing and the",
    "start": "1420280",
    "end": "1425500"
  },
  {
    "text": "Asics irrelevance and so a lot of that has been or at least NVIDIA has been in",
    "start": "1425500",
    "end": "1433450"
  },
  {
    "text": "this space for the past two or three years around the Nvidia can with the Nvidia container runtime also known as",
    "start": "1433450",
    "end": "1438880"
  },
  {
    "text": "in video docker this interprets pretty well was like containers it actually",
    "start": "1438880",
    "end": "1444400"
  },
  {
    "text": "integrates with a lot of runtimes out there docker CIO singularity Alec C and we",
    "start": "1444400",
    "end": "1452260"
  },
  {
    "text": "integrate at the runtime level which allows us to actually be able to do to",
    "start": "1452260",
    "end": "1457390"
  },
  {
    "text": "have one software that you can use for the diff all the different runtimes and also have a behavior that is exactly the",
    "start": "1457390",
    "end": "1465190"
  },
  {
    "text": "same on all the different runtimes how this ties Apache in communities is",
    "start": "1465190",
    "end": "1471310"
  },
  {
    "text": "allows you to realize allows three major use cases the first one and this is",
    "start": "1471310",
    "end": "1477250"
  },
  {
    "text": "something that is enabled through communities with namespaces with quotas with priority and preemption and a lot",
    "start": "1477250",
    "end": "1484180"
  },
  {
    "text": "of tools that kubernetes is going to allow you to use which was around resource attribution how do I actually",
    "start": "1484180",
    "end": "1490620"
  },
  {
    "text": "say this user is going to be able to have this machine or how many GPUs or",
    "start": "1490620",
    "end": "1497190"
  },
  {
    "text": "how many machines in general do I basically chord on a few nodes and then",
    "start": "1497190",
    "end": "1503650"
  },
  {
    "text": "give him SSH access or her X SSH access or do I allow him or her to wear a pot",
    "start": "1503650",
    "end": "1510519"
  },
  {
    "text": "speck that described what his or her job is going to be and then try and run that on the available nodes second one and",
    "start": "1510519",
    "end": "1518320"
  },
  {
    "text": "this is the use case that communities has been mostly built around is around running production workloads how do I do",
    "start": "1518320",
    "end": "1524950"
  },
  {
    "text": "and for instruction and as we've seen was Ryan running production ai is a lot",
    "start": "1524950",
    "end": "1531039"
  },
  {
    "text": "more complicated than your traditional micro service because it is a pipeline and the the slowest element in your",
    "start": "1531039",
    "end": "1538779"
  },
  {
    "text": "pipeline is going to define how fast you're going to go how fast you're going to serve your requests and the last one",
    "start": "1538779",
    "end": "1545679"
  },
  {
    "text": "then the last few skis that kubernetes allows you to to do is cloud bursting",
    "start": "1545679",
    "end": "1551499"
  },
  {
    "text": "how do I take my on-premise cluster when I don't have enough resource what do I",
    "start": "1551499",
    "end": "1556929"
  },
  {
    "text": "do and cloud bursting is a really interesting way of seeing things instead",
    "start": "1556929",
    "end": "1562659"
  },
  {
    "text": "of trying to plan for capacity I can now instantly burst in the cloud and this is",
    "start": "1562659",
    "end": "1569169"
  },
  {
    "text": "a really interesting use case that kubernetes allowed so we're going to see how we actually took the different CNCs",
    "start": "1569169",
    "end": "1575979"
  },
  {
    "text": "projects and use them with NVIDIA GPUs with Nvidia products to build that AI",
    "start": "1575979",
    "end": "1582580"
  },
  {
    "text": "production pipeline here you can see one of the example production deployment",
    "start": "1582580",
    "end": "1589419"
  },
  {
    "text": "that we have if we run through the user paths you can see that you the user is",
    "start": "1589419",
    "end": "1594849"
  },
  {
    "text": "going to communicate to your API endpoint using G RPC and that API",
    "start": "1594849",
    "end": "1601359"
  },
  {
    "text": "endpoint is going to go through some post-processing steps and a pause and",
    "start": "1601359",
    "end": "1607710"
  },
  {
    "text": "then send it to your sensor RT inference server GRP CN and voi are actually used",
    "start": "1607710",
    "end": "1613629"
  },
  {
    "text": "all along that process so that you can actually have these services communicates your 10 Sorority server or",
    "start": "1613629",
    "end": "1620229"
  },
  {
    "text": "you're generally your inference server is gonna start doing the inference work and then send it back to your service to",
    "start": "1620229",
    "end": "1627429"
  },
  {
    "text": "do some post-processing to send it back to the user finally the models that you will see in",
    "start": "1627429",
    "end": "1633820"
  },
  {
    "text": "the tensor are used in for a server are you going to be served by a model repository which might or might not be",
    "start": "1633820",
    "end": "1640899"
  },
  {
    "text": "backed by things like Luke and on wall",
    "start": "1640899",
    "end": "1646660"
  },
  {
    "text": "everything is running on this you can actually see that prometheus is going to gather there metric and serve them to",
    "start": "1646660",
    "end": "1653500"
  },
  {
    "text": "the consumers and consumers might be for example your autoscaler which is going to be able to scale your different",
    "start": "1653500",
    "end": "1660570"
  },
  {
    "text": "inference server pots based on load",
    "start": "1660570",
    "end": "1667259"
  },
  {
    "text": "right so we've actually seen so when we showed the flowers demo running in the",
    "start": "1668610",
    "end": "1673690"
  },
  {
    "text": "past you saw exactly this pipeline we're using this client here is an OpenGL application that is sending G RPC",
    "start": "1673690",
    "end": "1680049"
  },
  {
    "text": "requests to a middleman service that middleman service is in V RPC that",
    "start": "1680049",
    "end": "1685270"
  },
  {
    "text": "basically well we kind of for demo purposes we do a little trick the data set because we",
    "start": "1685270",
    "end": "1691450"
  },
  {
    "text": "can't guarantee the Wi-Fi is gonna be good the data set is actually a service in that middleman service so we're",
    "start": "1691450",
    "end": "1697030"
  },
  {
    "text": "basically the laptop is sending a request to say please in per image mm that's handled by the middleman service",
    "start": "1697030",
    "end": "1704110"
  },
  {
    "text": "which put packages that data into a request another the G RPC request that",
    "start": "1704110",
    "end": "1711190"
  },
  {
    "text": "goes into the ten sorry to you inference server it comes back we pre-pro we post process that to like provide the label",
    "start": "1711190",
    "end": "1718240"
  },
  {
    "text": "and we send it back so let's go back and now look at the demo again and we're",
    "start": "1718240",
    "end": "1724870"
  },
  {
    "text": "gonna add one more component so you can see that network is a little bit jittery",
    "start": "1724870",
    "end": "1729910"
  },
  {
    "text": "here but one of the things that we did was we built a control around convoys so",
    "start": "1729910",
    "end": "1736120"
  },
  {
    "text": "we have our own basically envoy discovery service that we can control the endpoints and if we all go on",
    "start": "1736120",
    "end": "1742480"
  },
  {
    "text": "airplane mode maybe this gets better because I'm on like two-point 4G Wi-Fi here but if we jump back into our",
    "start": "1742480",
    "end": "1749559"
  },
  {
    "text": "communities container urn to our granny's deployment I'm just gonna scale this up I'm gonna scale my deployment by",
    "start": "1749559",
    "end": "1754750"
  },
  {
    "text": "16 so oh boy I'm not attached live demos gotta",
    "start": "1754750",
    "end": "1763720"
  },
  {
    "text": "love it okay it's scaled so when we go back here we should start to see them start showing up in our",
    "start": "1763720",
    "end": "1772750"
  },
  {
    "text": "in our web UI our web UI is basically just monitoring the kubernetes api and",
    "start": "1772750",
    "end": "1778169"
  },
  {
    "text": "we're controlling this directly so normally that would just go into your load balancer but because we're telling",
    "start": "1778169",
    "end": "1783820"
  },
  {
    "text": "the story didactically and we have really bad Wi-Fi we're not gonna try to",
    "start": "1783820",
    "end": "1789070"
  },
  {
    "text": "actually scale this up to 16 man it'd be great if we could just get it smooth again but we so we can go and add in",
    "start": "1789070",
    "end": "1797010"
  },
  {
    "text": "come on make the request there we go and so now that that basically modified the",
    "start": "1797010",
    "end": "1806080"
  },
  {
    "text": "Envoy discovery service and had the second GPU into the mix and you can see now we can jump up to about 3,000 images",
    "start": "1806080",
    "end": "1812559"
  },
  {
    "text": "a second if all goes well on the network and so we can crank this up and and play",
    "start": "1812559",
    "end": "1817870"
  },
  {
    "text": "with it on demand and so it was kind of a fun the fun demo except for the Wi-Fi",
    "start": "1817870",
    "end": "1824080"
  },
  {
    "text": "is miserable no",
    "start": "1824080",
    "end": "1828059"
  },
  {
    "text": "and so actually running this on kubernetes and running HPC pipelines on kubernetes has some of its pitfalls the",
    "start": "1832170",
    "end": "1839670"
  },
  {
    "text": "first one that you're gonna hit is around resource management so this is a",
    "start": "1839670",
    "end": "1845190"
  },
  {
    "text": "slide that I actually pulled from the queue Connie you reach inside kubernetes resource management but one of the thing",
    "start": "1845190",
    "end": "1851160"
  },
  {
    "text": "that you're gonna find out is that you want to be able to set the quality class",
    "start": "1851160",
    "end": "1857190"
  },
  {
    "text": "of your pods and the quality class of your pods is defined by how you set your CPU request and limits and memory",
    "start": "1857190",
    "end": "1863850"
  },
  {
    "text": "requests and limits this defines this basically allows you to tell kubernetes",
    "start": "1863850",
    "end": "1869600"
  },
  {
    "text": "my pod is important or not and based on that kubernetes is going to evict your",
    "start": "1869600",
    "end": "1877940"
  },
  {
    "text": "the different pods based on this priority scale and this also allows you",
    "start": "1877940",
    "end": "1885840"
  },
  {
    "text": "to tell kubernetes whether you're using static CPU pinning or not so one of the issues that you currently have with",
    "start": "1885840",
    "end": "1892020"
  },
  {
    "text": "communities right now is when you're using HPC services so services were pods",
    "start": "1892020",
    "end": "1898590"
  },
  {
    "text": "that are bound by a compute most of the time is like kubernetes runs CFS quotas",
    "start": "1898590",
    "end": "1903840"
  },
  {
    "text": "and right now there's a known bug affecting well-behaved applications in the linux kernel where where",
    "start": "1903840",
    "end": "1912500"
  },
  {
    "text": "applications that behave correctly will actually get cpu star old and so that's",
    "start": "1912500",
    "end": "1917790"
  },
  {
    "text": "a big issue when you're when you're running CPU bound applications and to",
    "start": "1917790",
    "end": "1924060"
  },
  {
    "text": "take that a step further on top of just the Linux scheduler one of the really",
    "start": "1924060",
    "end": "1929070"
  },
  {
    "text": "important things to do when you're thinking about doing your deployment is to align your CPUs and GPUs together so",
    "start": "1929070",
    "end": "1934110"
  },
  {
    "text": "that you're not having CPUs on one socket trying to communicate with the GPU that exists on the other socket so",
    "start": "1934110",
    "end": "1940770"
  },
  {
    "text": "part of topology aware scheduling for these resources is really important and",
    "start": "1940770",
    "end": "1946200"
  },
  {
    "text": "something to keep in mind as well and unfortunately unfortunately with kubernetes static CPU pinning is",
    "start": "1946200",
    "end": "1952320"
  },
  {
    "text": "possible but you don't control on which Numa socket you on their own so that means that you might actually get static",
    "start": "1952320",
    "end": "1959070"
  },
  {
    "text": "leap in and get the best CPU possible but your transfer transferring your data from your CPU to G",
    "start": "1959070",
    "end": "1965639"
  },
  {
    "text": "pew is going to be really slow because you're not on the right sockets so Cuban",
    "start": "1965639",
    "end": "1970799"
  },
  {
    "text": "Andes has a lot of pitfalls around running HPC applications but this is an ongoing conversation in the community",
    "start": "1970799",
    "end": "1977999"
  },
  {
    "text": "and expect to see some solutions to this as time goes on yep with that we have",
    "start": "1977999",
    "end": "1987779"
  },
  {
    "text": "questions time for questions and I we",
    "start": "1987779",
    "end": "1993209"
  },
  {
    "text": "don't have a mic so you have to present your question we'll repeat it and then",
    "start": "1993209",
    "end": "1998778"
  },
  {
    "text": "yeah the tents RT imprint server has a model ingest that looks very similar",
    "start": "2011360",
    "end": "2016500"
  },
  {
    "text": "because it is the tents it's based on the tensor it's based on tensorflow serving model stores yeah so the",
    "start": "2016500",
    "end": "2027390"
  },
  {
    "text": "question was above saved model formats from tensorflow and yes a the tents RT",
    "start": "2027390",
    "end": "2034140"
  },
  {
    "text": "inference server uses consumes save models as a preferred way the new preferred way as well as dot PP or graph",
    "start": "2034140",
    "end": "2041940"
  },
  {
    "text": "deaths from cafe - I think there was one over here so the question was relevant",
    "start": "2041940",
    "end": "2055889"
  },
  {
    "text": "I'd tenants and can you explain - just what you meant about a little bit further so - different - different",
    "start": "2055890",
    "end": "2069030"
  },
  {
    "text": "inference small to performing inference on two different models on the same device that's actually what M sorry in",
    "start": "2069030",
    "end": "2074550"
  },
  {
    "text": "print server does you have male model B they'll have their own set of buffers in some cases they can share buffers but",
    "start": "2074550",
    "end": "2081050"
  },
  {
    "text": "essentially transactionally they're completely separate but there are also interleaved if you need discrete then",
    "start": "2081050",
    "end": "2088860"
  },
  {
    "text": "you just display them as separate services but if you don't mind that they're in the same service you can you just get better ROI and better",
    "start": "2088860",
    "end": "2096090"
  },
  {
    "text": "efficiency that way better throughput one more question it's something that",
    "start": "2096090",
    "end": "2106110"
  },
  {
    "text": "we're working on I would say at this point no but we could easily add it no",
    "start": "2106110",
    "end": "2113490"
  },
  {
    "text": "so each each molecule has its own has its own thread that's driving it so",
    "start": "2113490",
    "end": "2119760"
  },
  {
    "text": "essentially it's almost basically at the contention of the lock on some of those",
    "start": "2119760",
    "end": "2127380"
  },
  {
    "text": "sources it's not it's not that much but if you wanted to define it you could go in and and make some explicit priorities",
    "start": "2127380",
    "end": "2133890"
  },
  {
    "text": "and something we're looking at",
    "start": "2133890",
    "end": "2136880"
  },
  {
    "text": "yes yes that's definitely an effort in the community that we're for sharing",
    "start": "2142380",
    "end": "2147670"
  },
  {
    "text": "right now one of the documents that would try that we're trying to push right now with the community is called",
    "start": "2147670",
    "end": "2154330"
  },
  {
    "text": "the new map manager one more question",
    "start": "2154330",
    "end": "2160740"
  },
  {
    "text": "so fractional gpus is is an interesting concept but the way that you want to",
    "start": "2174180",
    "end": "2182260"
  },
  {
    "text": "talk you want to see it right now is can I share my models can I can I have",
    "start": "2182260",
    "end": "2187510"
  },
  {
    "text": "multiple models running on my GPU fractional GPU is not something you cannot see the GPU as the same way as a",
    "start": "2187510",
    "end": "2193840"
  },
  {
    "text": "CPU so if we know and I were both two different processes on the same GPU",
    "start": "2193840",
    "end": "2199290"
  },
  {
    "text": "without something like MPs I get exclusive access to the device or he gets exclusive access to the device so",
    "start": "2199290",
    "end": "2205720"
  },
  {
    "text": "if I'm performing a copy and then doing like a pipeline my pipeline gets interrupted well he takes out well his",
    "start": "2205720",
    "end": "2212530"
  },
  {
    "text": "contacts takes over so we don't get that efficiency where we get that good interleaving between our two processes",
    "start": "2212530",
    "end": "2219070"
  },
  {
    "text": "on the device and that's why we have the ten story inference server to combat that for inference right now is that by",
    "start": "2219070",
    "end": "2224410"
  },
  {
    "text": "co-locating the models in the same process we get that interleaving and we get that performance but if it's an",
    "start": "2224410",
    "end": "2230230"
  },
  {
    "text": "action that can be really it can be pretty big in some cases so if we even take a step back the way to see GPU",
    "start": "2230230",
    "end": "2236110"
  },
  {
    "text": "sharing is that it's not only a problem about how do I run two processes on the",
    "start": "2236110",
    "end": "2241420"
  },
  {
    "text": "same GPU as we've highlighted during the talk like running GP application is is",
    "start": "2241420",
    "end": "2247030"
  },
  {
    "text": "really a pipeline and every single step of that pipeline might break and this is",
    "start": "2247030",
    "end": "2253180"
  },
  {
    "text": "exactly the same problem where is it going to break on your PCI throughput is it going to break on your Linux",
    "start": "2253180",
    "end": "2260860"
  },
  {
    "text": "threading isn't gonna break on getting the data from you know the right CPU from the right CPU to the GPU",
    "start": "2260860",
    "end": "2267370"
  },
  {
    "text": "we're so multi-tenancy and like sharing pyou is a really complex problem that",
    "start": "2267370",
    "end": "2272980"
  },
  {
    "text": "does not involve only GPUs and that's that's the problem that we have yes yep",
    "start": "2272980",
    "end": "2283830"
  },
  {
    "text": "yes exactly I think we have time for one more yes oh",
    "start": "2283830",
    "end": "2298290"
  },
  {
    "text": "why envoy instead of you know I think when I first wrote built the demo envoy",
    "start": "2302070",
    "end": "2309190"
  },
  {
    "text": "was one of the few uh l7 load balancers out there that handled HTTP 2 with G RPC",
    "start": "2309190",
    "end": "2314800"
  },
  {
    "text": "you know now nginx does it as well and you know other things so you can use like ambassador which is just a wrapper",
    "start": "2314800",
    "end": "2320800"
  },
  {
    "text": "around envoy or you can use misty Oh which is another wrapped around on voyage yeah certainly although it was a",
    "start": "2320800",
    "end": "2347800"
  },
  {
    "text": "great choice because of III I thought that the the separation of the control plane and the data plane was really nice",
    "start": "2347800",
    "end": "2353820"
  },
  {
    "text": "that allowed us to actually build that discovery service to like control it explicitly which made it kind of fun and",
    "start": "2353820",
    "end": "2360310"
  },
  {
    "text": "like a cool project to use but the examples that we use you know with the students you have projects you know you",
    "start": "2360310",
    "end": "2366430"
  },
  {
    "text": "can pick you can choose to replace those with what's ever in your infrastructure those are just the ones that we chose to like build out this demo and kind of",
    "start": "2366430",
    "end": "2372970"
  },
  {
    "text": "like evaluate this inference pipeline how a customer might build it but of",
    "start": "2372970",
    "end": "2379000"
  },
  {
    "text": "course if you have your favorites or you know your infrastructure mandates they use certain tools you can just swap swap",
    "start": "2379000",
    "end": "2385420"
  },
  {
    "text": "out any of these tools for those soon yeah yeah of course yeah Indian these are",
    "start": "2385420",
    "end": "2392350"
  },
  {
    "text": "blocks that you can take and I choose to arrange however you think is better fits",
    "start": "2392350",
    "end": "2398260"
  },
  {
    "text": "your architecture we think here in this case there these are decision that we",
    "start": "2398260",
    "end": "2403390"
  },
  {
    "text": "can make in terms of performance and adaptability yeah yep yeah it's pretty",
    "start": "2403390",
    "end": "2412570"
  },
  {
    "text": "fast thank you okay thank you very much",
    "start": "2412570",
    "end": "2417910"
  },
  {
    "text": "[Applause]",
    "start": "2417910",
    "end": "2421059"
  }
]