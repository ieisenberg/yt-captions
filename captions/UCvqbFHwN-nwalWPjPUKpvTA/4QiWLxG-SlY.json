[
  {
    "start": "0",
    "end": "115000"
  },
  {
    "text": "welcome everyone to the last session of cube con and we want to thank you for well staying till the very end of the",
    "start": "60",
    "end": "7020"
  },
  {
    "text": "conference we are monographs key and Tina Zang we are Rica e a series from",
    "start": "7020",
    "end": "12990"
  },
  {
    "text": "Google London and we're here to tell you a bit about what kubernetes does when",
    "start": "12990",
    "end": "20460"
  },
  {
    "text": "something goes wrong what happens when there are some failures in the system how system tries to fix itself or",
    "start": "20460",
    "end": "27750"
  },
  {
    "text": "mitigate the problems what what kind of",
    "start": "27750",
    "end": "33090"
  },
  {
    "text": "problems can be fixed this way and what are the limits of this automation we would like to ask you to keep your",
    "start": "33090",
    "end": "38670"
  },
  {
    "text": "questions to the end unless we will tell something that's completely unclear and with just clarifying so before we start",
    "start": "38670",
    "end": "48480"
  },
  {
    "text": "I want to tell you a bit more who we are some of you may be already familiar with the site reliability engineering",
    "start": "48480",
    "end": "55739"
  },
  {
    "text": "engineering role but for ones who are not this is how our VP described he",
    "start": "55739",
    "end": "64500"
  },
  {
    "text": "described it he said that a Sri is fundamentally what happens when you ask",
    "start": "64500",
    "end": "69540"
  },
  {
    "text": "a software engineer to design an Operations function the something here",
    "start": "69540",
    "end": "75479"
  },
  {
    "text": "is that software engineers are inherently lazy and do not like doing the same thing multiple multiple times",
    "start": "75479",
    "end": "82610"
  },
  {
    "text": "the result is supposed to be that the s-series are there to resolve the problem when",
    "start": "82610",
    "end": "88290"
  },
  {
    "text": "they occur but then make sure that they never occur again or if they do doesn't",
    "start": "88290",
    "end": "95759"
  },
  {
    "text": "do not require human action this may take a form of like fixing the bug that",
    "start": "95759",
    "end": "104640"
  },
  {
    "text": "caused an outage or building AD automation that will well fix the",
    "start": "104640",
    "end": "111090"
  },
  {
    "text": "problem itself without like notifying human this probably explains what we are",
    "start": "111090",
    "end": "119490"
  },
  {
    "start": "115000",
    "end": "115000"
  },
  {
    "text": "going to tell you about and basically the automation one of the promises of the kubernetes is that it will make the",
    "start": "119490",
    "end": "127680"
  },
  {
    "text": "what you reduce the ops work required to run workloads like your workloads",
    "start": "127680",
    "end": "133500"
  },
  {
    "text": "this means that the kubernetes needs to have the automation built into it that",
    "start": "133500",
    "end": "140220"
  },
  {
    "text": "will resolve like the most common failures that you can see in the clusters that that you run what happened",
    "start": "140220",
    "end": "148800"
  },
  {
    "text": "is that during the development each team and like every different areas that and",
    "start": "148800",
    "end": "154650"
  },
  {
    "text": "they and then identified those like most common failures and try to build the system in in a way that it will try to",
    "start": "154650",
    "end": "162000"
  },
  {
    "text": "well fix those fix those problems if they ever occur the small note here is",
    "start": "162000",
    "end": "169500"
  },
  {
    "text": "that I it wasn't such a centralized blood process so every single team build their own automation to fix the staff so",
    "start": "169500",
    "end": "176190"
  },
  {
    "text": "it is not really very every consistent across the board so let's start with the",
    "start": "176190",
    "end": "184950"
  },
  {
    "start": "181000",
    "end": "181000"
  },
  {
    "text": "basics one of the first automated healing mechanisms that new users of kubernetes learn about is creating pods",
    "start": "184950",
    "end": "192900"
  },
  {
    "text": "as part of a replica set typically in a deployment so the controller manager",
    "start": "192900",
    "end": "199080"
  },
  {
    "text": "here will be helping you automate the process of keeping the desired number of replicas alive so that even when your",
    "start": "199080",
    "end": "206519"
  },
  {
    "text": "pod containers keep crashing due to a seg fault memory leak or some other",
    "start": "206519",
    "end": "212280"
  },
  {
    "text": "applications specific bug they can recover automatically so the process",
    "start": "212280",
    "end": "219420"
  },
  {
    "start": "218000",
    "end": "218000"
  },
  {
    "text": "that is restarting any crashed containers is of course cubelet it will",
    "start": "219420",
    "end": "225329"
  },
  {
    "text": "restart a crash container according to the pods restart policy which is always",
    "start": "225329",
    "end": "230549"
  },
  {
    "text": "defaulted to always we start cubelet also contains logic to restart pods on",
    "start": "230549",
    "end": "236610"
  },
  {
    "text": "an exponential back-off delay hence when you describe pods using cube control the",
    "start": "236610",
    "end": "242280"
  },
  {
    "text": "name of the pod status is called crash loop back off so how is it doing this",
    "start": "242280",
    "end": "247350"
  },
  {
    "text": "cubelets is doing this by interacting with docker in order to fulfill its responsibilities of actuated the desired",
    "start": "247350",
    "end": "255570"
  },
  {
    "text": "pod spec on the node in the interests of fairness other container runtimes are",
    "start": "255570",
    "end": "260820"
  },
  {
    "text": "available and the introduction of the container runs interface as an abstraction layer a few",
    "start": "260820",
    "end": "266780"
  },
  {
    "text": "years ago made people Nettie's really extensible in terms of how it starts and",
    "start": "266780",
    "end": "272000"
  },
  {
    "text": "stops containers but because kubernetes is not tied to docker it's not actually",
    "start": "272000",
    "end": "277940"
  },
  {
    "text": "using Dockers native health checking features such as the auto restart policy",
    "start": "277940",
    "end": "282970"
  },
  {
    "text": "instead cubelet periodically calls docker PS to list all containers",
    "start": "282970",
    "end": "288250"
  },
  {
    "text": "compares it with the previous output and along with docker inspect Gabon's the",
    "start": "288250",
    "end": "295250"
  },
  {
    "text": "state of containers so if there is a change Keibler triggers a synchronization event",
    "start": "295250",
    "end": "300800"
  },
  {
    "text": "with respect to the pod spec some operating systems such as the container",
    "start": "300800",
    "end": "306260"
  },
  {
    "text": "optimized operating system on gke also use system D to run health checks on",
    "start": "306260",
    "end": "312440"
  },
  {
    "text": "both cubelet and docker every minute by using the cubelets help the endpoint and calling docker PS",
    "start": "312440",
    "end": "319400"
  },
  {
    "text": "if the response time times out the relevant process is killed and a",
    "start": "319400",
    "end": "325100"
  },
  {
    "text": "separate system D process restarts the relevant process unfortunately this",
    "start": "325100",
    "end": "331220"
  },
  {
    "text": "means that the docker daemon lifecycle is tied to the container lifecycle so if",
    "start": "331220",
    "end": "336800"
  },
  {
    "text": "this health check fails all your containers get killed there is a docker live restore feature",
    "start": "336800",
    "end": "342440"
  },
  {
    "text": "that allows containers to keep running even when the docker demon has died and we are currently working on enabling",
    "start": "342440",
    "end": "348950"
  },
  {
    "text": "this in a forthcoming release of the container optimized OS so given that",
    "start": "348950",
    "end": "355370"
  },
  {
    "text": "cubelet has the ability to restart containers and we can ask it to do so in a much smarter way than simply using",
    "start": "355370",
    "end": "362780"
  },
  {
    "text": "docker to see if the containers are alive or not so users can configure cubelet to perform smarter health",
    "start": "362780",
    "end": "369260"
  },
  {
    "text": "checking using something called liveness probes this is especially important if",
    "start": "369260",
    "end": "375020"
  },
  {
    "text": "you have a workload or an application that can get into a broken state without crashing the container so maybe just as",
    "start": "375020",
    "end": "382639"
  },
  {
    "text": "a show of hands how many people have heard of liveness probes okay great so",
    "start": "382639",
    "end": "388280"
  },
  {
    "text": "I'll go through the slide really quickly and there are three types of liveness probes actually how many people are",
    "start": "388280",
    "end": "395120"
  },
  {
    "text": "using live probes in prod okay that's good to see well it could be it could be higher I",
    "start": "395120",
    "end": "400710"
  },
  {
    "text": "think how many people are using liveness commands okay so it's a few I'd be",
    "start": "400710",
    "end": "407940"
  },
  {
    "text": "really really interested to hear kind of all of the use cases afterwards if you can if we can stick around and how about HTTP requests and TCP socket probes okay",
    "start": "407940",
    "end": "420270"
  },
  {
    "text": "so I think the HTTP requests are slightly more popular I guess as",
    "start": "420270",
    "end": "425699"
  },
  {
    "text": "computer programmers it's a paradigm that we're more used to as with any code",
    "start": "425699",
    "end": "433199"
  },
  {
    "start": "426000",
    "end": "426000"
  },
  {
    "text": "bug in the liveness configuration can be extremely problematic as it can lead to",
    "start": "433199",
    "end": "439400"
  },
  {
    "text": "crash looping pods but hopefully this is something that's easy to spot and ideally your liveness config should be",
    "start": "439400",
    "end": "446490"
  },
  {
    "text": "as simple as possible and I guess with HTTP and TCP probes if something does go wrong you also have to debug at the",
    "start": "446490",
    "end": "454289"
  },
  {
    "text": "networking layer too so just briefly here are some example configurations",
    "start": "454289",
    "end": "460229"
  },
  {
    "start": "456000",
    "end": "456000"
  },
  {
    "text": "from the official documentation and this is what you would insert into the pod spec so for people perhaps less familiar",
    "start": "460229",
    "end": "466800"
  },
  {
    "text": "with HTTP probes you can set custom",
    "start": "466800",
    "end": "471990"
  },
  {
    "text": "headers you can also tweak the timing of how long cubelet should wait before",
    "start": "471990",
    "end": "477060"
  },
  {
    "text": "performing the first probe and how frequently probes should occur you can also customize the number of failures",
    "start": "477060",
    "end": "483750"
  },
  {
    "text": "that would count the cubelet should count before seeing a pod restart and",
    "start": "483750",
    "end": "489150"
  },
  {
    "text": "there's also such a thing as readiness probes which is very similar and they define health checks that must be passed",
    "start": "489150",
    "end": "494969"
  },
  {
    "text": "before traffic is sent to pods and just quickly an example of when liveness",
    "start": "494969",
    "end": "500070"
  },
  {
    "text": "probes can go wrong it can go wrong when you have a container that takes a long",
    "start": "500070",
    "end": "505500"
  },
  {
    "text": "time to start up and your initial delay seconds is set to shorts this is",
    "start": "505500",
    "end": "511229"
  },
  {
    "text": "something we actually experienced in production on gke hosted masters and we",
    "start": "511229",
    "end": "516719"
  },
  {
    "text": "found that for some very large clusters or running very large workloads we'd find that the HDD container on the",
    "start": "516719",
    "end": "523050"
  },
  {
    "text": "master with crash loop and after a restart what's happening there is that Etsy D is replaying it's right ahead log",
    "start": "523050",
    "end": "530400"
  },
  {
    "text": "after restart but for a cluster with a large workload and long log entries this",
    "start": "530400",
    "end": "535440"
  },
  {
    "text": "was taking much longer than the default 15 initial delay seconds and when we get",
    "start": "535440",
    "end": "543030"
  },
  {
    "text": "paged for it the simple solution was simply allowing @cd more time to initialize by increasing the initial",
    "start": "543030",
    "end": "550560"
  },
  {
    "text": "delay seconds in the config thank you so",
    "start": "550560",
    "end": "556620"
  },
  {
    "start": "553000",
    "end": "553000"
  },
  {
    "text": "Tina told us what happened what how kubernetes try to solve the issues when",
    "start": "556620",
    "end": "562200"
  },
  {
    "text": "there's some problem in the binary itself I basically tries to restart attack if the liveness proper training",
    "start": "562200",
    "end": "568500"
  },
  {
    "text": "is probe like blindness process is failing but that's not the only reason why this service might be broken or if I",
    "start": "568500",
    "end": "575730"
  },
  {
    "text": "kid the binder itself might be perfectly fine and correctly written but the",
    "start": "575730",
    "end": "580800"
  },
  {
    "text": "service still might not be responsive the reason might be that the pod the binder itself is lives in a unhealthy",
    "start": "580800",
    "end": "588270"
  },
  {
    "text": "environment they it might have some noisy neighbor that just consumes all the resources of the machine and does",
    "start": "588270",
    "end": "593880"
  },
  {
    "text": "not leave enough for for the part that's supposed to be serving well in principle",
    "start": "593880",
    "end": "601320"
  },
  {
    "text": "containers should be good like actually do offer rather good isolation on many",
    "start": "601320",
    "end": "607170"
  },
  {
    "text": "resource and dimensions but we are usually usually not using it like very",
    "start": "607170",
    "end": "613200"
  },
  {
    "text": "strictly we try to like relax this isolation a bit the reason is if we",
    "start": "613200",
    "end": "621180"
  },
  {
    "text": "allow some some we allow container to",
    "start": "621180",
    "end": "626820"
  },
  {
    "text": "share some full of resources we can achieve much higher utilization in the cluster right like if the few not fuel",
    "start": "626820",
    "end": "635370"
  },
  {
    "text": "pots they can just like burst if they need we can pack more of them in the",
    "start": "635370",
    "end": "640710"
  },
  {
    "text": "under single now than if we just like limit all of them strictly to like the highest possible utilization hence we",
    "start": "640710",
    "end": "648480"
  },
  {
    "text": "have the quality of service concept in the kubernetes my back to the noisy neighbor the way the kubernetes shows",
    "start": "648480",
    "end": "658200"
  },
  {
    "text": "like tells the user that there is something wrong with the node it's through node conditions",
    "start": "658200",
    "end": "665600"
  },
  {
    "text": "the two of them that are connected to",
    "start": "665600",
    "end": "670890"
  },
  {
    "start": "667000",
    "end": "667000"
  },
  {
    "text": "the noise never problem our disk pressure and memory pressure they well they of course mean that the note is",
    "start": "670890",
    "end": "677610"
  },
  {
    "text": "running low on disk or memory some of you may ask why not CPU well the CPU",
    "start": "677610",
    "end": "683670"
  },
  {
    "text": "like running out of CPU is bad but it's not terrible I think we'll run slower",
    "start": "683670",
    "end": "688710"
  },
  {
    "text": "but they will not be killed nothing will be kill today no this",
    "start": "688710",
    "end": "694860"
  },
  {
    "text": "running out of CPU unless like it's very like really start and CPU and and watchdogs all start killing things so",
    "start": "694860",
    "end": "703700"
  },
  {
    "text": "luckily this this is a problem that like computer science like a solar dissolve",
    "start": "703700",
    "end": "710820"
  },
  {
    "start": "705000",
    "end": "705000"
  },
  {
    "text": "right in the or even on a single master single single single machines like the",
    "start": "710820",
    "end": "716490"
  },
  {
    "text": "even single machines can run out of memory and kernel knows how to handle that it basically it kills stuff so",
    "start": "716490",
    "end": "723450"
  },
  {
    "text": "maybe we would be it would be okay to just leave it to kernel to just kill",
    "start": "723450",
    "end": "728790"
  },
  {
    "text": "something on the machine and make sure that it's stable sadly that's not a right choice because kernel doesn't have",
    "start": "728790",
    "end": "736890"
  },
  {
    "text": "any idea about like which processes are important for kubernetes stability and which are not so for example if the um",
    "start": "736890",
    "end": "746400"
  },
  {
    "text": "killer will do the killing like it can kill like cubelet or daughter or sshd and or anything which would basically",
    "start": "746400",
    "end": "754830"
  },
  {
    "text": "destabilize the whole cluster or at least a node the better way to do it is",
    "start": "754830",
    "end": "760200"
  },
  {
    "text": "to allow cubelet to do the killing because like it's know it knows what can",
    "start": "760200",
    "end": "765390"
  },
  {
    "text": "be killed safely hence it can be like more logical not completely random when when killing",
    "start": "765390",
    "end": "772080"
  },
  {
    "text": "things so it will be killing pots because that's the thing it oversees but",
    "start": "772080",
    "end": "779100"
  },
  {
    "text": "not all the pots can be killed safely right like you probably don't want to",
    "start": "779100",
    "end": "784140"
  },
  {
    "text": "kill Q proxy because if you do you your services will stop working right like",
    "start": "784140",
    "end": "790710"
  },
  {
    "text": "nothing will update the iptables which may because of that we made the Q proxy",
    "start": "790710",
    "end": "797820"
  },
  {
    "text": "special in in the default configuration by special I mean it is it possesses it",
    "start": "797820",
    "end": "804750"
  },
  {
    "text": "has two properties in the same time it has a critical pata notation this is a",
    "start": "804750",
    "end": "810330"
  },
  {
    "text": "notation you may know of it basically says the system that this is very",
    "start": "810330",
    "end": "815580"
  },
  {
    "text": "critical thing that means to run and it is allowed to kill other things to make",
    "start": "815580",
    "end": "821550"
  },
  {
    "text": "a space for itself and it's also a static part which means it runs from the",
    "start": "821550",
    "end": "826800"
  },
  {
    "text": "manifest start on the machine on the note itself instead of reading it from the API server so if any part has those",
    "start": "826800",
    "end": "833550"
  },
  {
    "text": "two properties it is safe from evictions than done by cubelets all other parts",
    "start": "833550",
    "end": "841980"
  },
  {
    "text": "can be killed and by all other I really mean all other like stateful sets it's",
    "start": "841980",
    "end": "848430"
  },
  {
    "text": "fine cubed victorious a full set doesn't matter too much if it does or it will",
    "start": "848430",
    "end": "855240"
  },
  {
    "text": "also ignore poor destruction budget if you have I if we run out of put a",
    "start": "855240",
    "end": "860700"
  },
  {
    "text": "certain budget the cubit will ignore this and kill the pot despite not but",
    "start": "860700",
    "end": "868530"
  },
  {
    "text": "it's not that it will kill anything it tries to respect quality of service",
    "start": "868530",
    "end": "873840"
  },
  {
    "text": "classes so it will start by killing the best effort pot first if it ran out of",
    "start": "873840",
    "end": "879420"
  },
  {
    "text": "them it would and it should kill worst of all thoughts and only after they're gone killed the guaranteed ones so now",
    "start": "879420",
    "end": "887640"
  },
  {
    "text": "you may understand why we need to ignore put a certain budget if we didn't it was",
    "start": "887640",
    "end": "893850"
  },
  {
    "text": "put it would be possible to gain the system by saying like really low for the certain budget for the pot in a",
    "start": "893850",
    "end": "899250"
  },
  {
    "text": "best-effort class and this way forced the cubelet to kill the post from the higher hierarchy oasis which we want to",
    "start": "899250",
    "end": "906990"
  },
  {
    "text": "want to avoid of course like killing as",
    "start": "906990",
    "end": "912030"
  },
  {
    "text": "in as in the case of the um killer it's a slider random so it pick you but my",
    "start": "912030",
    "end": "917160"
  },
  {
    "text": "failed to kill the noisy neighbor it may kill some pod which was perfectly calm",
    "start": "917160",
    "end": "923820"
  },
  {
    "text": "and and then normal pod instead of the one that causes harm but then the issue",
    "start": "923820",
    "end": "929280"
  },
  {
    "text": "will reoccur and Cubitt will try again and eventually hopefully it will kill the one that causes problem okay but",
    "start": "929280",
    "end": "938699"
  },
  {
    "text": "even if all of the neighbors of your pod are nice and friendly and don't try to use up extra resources your pod might",
    "start": "938699",
    "end": "945269"
  },
  {
    "text": "still be experiencing some problems and even if you have liveness probes configured correctly it doesn't matter",
    "start": "945269",
    "end": "951149"
  },
  {
    "text": "if there's something wrong at the machine level so to make problems at the level of the node more visible we have a",
    "start": "951149",
    "end": "958170"
  },
  {
    "start": "953000",
    "end": "953000"
  },
  {
    "text": "kubernetes add-on called node problem detector this is a demon that can make",
    "start": "958170",
    "end": "963209"
  },
  {
    "text": "automatic diagnosis of node problems and report them to the API server using",
    "start": "963209",
    "end": "968819"
  },
  {
    "text": "event objects for temporary problems or node condition objects for more permanent ones this is enabled by",
    "start": "968819",
    "end": "975720"
  },
  {
    "text": "default on GCE but you can also run it as a daemon set or on a standalone basis",
    "start": "975720",
    "end": "981119"
  },
  {
    "text": "and because no problem detector is a daemon albeit a very small one it does",
    "start": "981119",
    "end": "986639"
  },
  {
    "text": "use resources but benchmark testing was done to make sure that the footprint was",
    "start": "986639",
    "end": "992100"
  },
  {
    "text": "relatively minimal and in any case you can limit its impact in config so this",
    "start": "992100",
    "end": "998129"
  },
  {
    "start": "997000",
    "end": "997000"
  },
  {
    "text": "slide shows the architecture of the node problem detector which as with many kubernetes components was designed with",
    "start": "998129",
    "end": "1005119"
  },
  {
    "text": "the principle of extensibility there are sub daemons of the node problem detector",
    "start": "1005119",
    "end": "1010429"
  },
  {
    "text": "called problem daemons that report on a very specific class of problem we",
    "start": "1010429",
    "end": "1015799"
  },
  {
    "text": "currently only have the kernel monitor which is monitoring kernel logs to",
    "start": "1015799",
    "end": "1022100"
  },
  {
    "text": "detect known issues in particular kernel deadlock by matching against predefined",
    "start": "1022100",
    "end": "1027798"
  },
  {
    "text": "rules and these rules are customizable and can be extended to include new sets",
    "start": "1027799",
    "end": "1032839"
  },
  {
    "text": "of problems however you can add custom plug-in monitors that can deal with infrastructure hardware or container",
    "start": "1032839",
    "end": "1040308"
  },
  {
    "text": "runtime issues and in the future the idea is that any node problems reported to the API server can be self-healed",
    "start": "1040309",
    "end": "1048500"
  },
  {
    "text": "with a remedy controller that could perhaps drain or reboot the node as Jessie from github mentioned in his",
    "start": "1048500",
    "end": "1055970"
  },
  {
    "text": "keynote this morning they actually have an internal only tool that is doing this called node problem healer and this",
    "start": "1055970",
    "end": "1063470"
  },
  {
    "text": "would really close the on having diagnosis and treatment fully automated we just need to make it open",
    "start": "1063470",
    "end": "1070400"
  },
  {
    "text": "source in other words this would really become an SRE streamed system where",
    "start": "1070400",
    "end": "1075500"
  },
  {
    "text": "there would be no need for sorries however if your node is not accessible",
    "start": "1075500",
    "end": "1081590"
  },
  {
    "start": "1078000",
    "end": "1078000"
  },
  {
    "text": "due to networking issues or there is a more severe serious hardware malfunction",
    "start": "1081590",
    "end": "1087320"
  },
  {
    "text": "such that the node becomes partitioned from the rest of the cluster the node problem detector can't help you as its",
    "start": "1087320",
    "end": "1095150"
  },
  {
    "text": "diagnosis will not even reach master and unfortunately we would have no way of",
    "start": "1095150",
    "end": "1100310"
  },
  {
    "text": "knowing the difference between a networking failure or a node failure we simply know that the node is unreachable",
    "start": "1100310",
    "end": "1107050"
  },
  {
    "start": "1107000",
    "end": "1107000"
  },
  {
    "text": "so in order to account for this scenario kubernetes has the node controller so",
    "start": "1107050",
    "end": "1114020"
  },
  {
    "text": "what happens here is that nodes send heartbeat updates to the node controller every 10 seconds and the node controller",
    "start": "1114020",
    "end": "1121040"
  },
  {
    "text": "is a component on the master and when this component does not receive heartbeats for 40 seconds from any node",
    "start": "1121040",
    "end": "1128930"
  },
  {
    "text": "it marks its node ready condition as unknown so also what it does when you're",
    "start": "1128930",
    "end": "1136490"
  },
  {
    "text": "running a cluster in the cloud is that node controller makes a comparison of the internal list of nodes versus the",
    "start": "1136490",
    "end": "1143420"
  },
  {
    "text": "cloud providers list of available VMs and deletes any unhealthy nodes whose",
    "start": "1143420",
    "end": "1150140"
  },
  {
    "text": "vm's no longer exists according to the cloud provider so for nodes that do",
    "start": "1150140",
    "end": "1155420"
  },
  {
    "text": "reach an undesirable state for a long period of time and the default being 5 minutes and the node controller will",
    "start": "1155420",
    "end": "1161840"
  },
  {
    "text": "eventually begin evicting that nodes pods and which Marik will now expand",
    "start": "1161840",
    "end": "1166880"
  },
  {
    "text": "them thank you okay so as Tina said like",
    "start": "1166880",
    "end": "1172370"
  },
  {
    "start": "1168000",
    "end": "1168000"
  },
  {
    "text": "if machine is unreachable we need to do something having in mind that the most",
    "start": "1172370",
    "end": "1178580"
  },
  {
    "text": "important thing for the whole system is to keep user workers running right this is the goal of the whole this whole",
    "start": "1178580",
    "end": "1184180"
  },
  {
    "text": "experiment right we want to make sure that your pod runs as such well as as",
    "start": "1184180",
    "end": "1191210"
  },
  {
    "text": "far as possible so like if the machine is unresponsive or we are",
    "start": "1191210",
    "end": "1198230"
  },
  {
    "text": "actually reports itself as unhealthy we need to move parts from this machine",
    "start": "1198230",
    "end": "1205010"
  },
  {
    "text": "somewhere else well in companies were world we basically just kill the parts",
    "start": "1205010",
    "end": "1210230"
  },
  {
    "text": "from this machine and allow the replica set controller or any other controller to does create replacement parts with",
    "start": "1210230",
    "end": "1217280"
  },
  {
    "text": "the end again this will be scheduled by the scheduler on some healthy ones there's one detail here demons are not",
    "start": "1217280",
    "end": "1226040"
  },
  {
    "text": "evicted which might get be confusing from time to time this is because like",
    "start": "1226040",
    "end": "1231410"
  },
  {
    "text": "they like even if we have agreements they won't go anywhere they are like assigned to lick this note all right like so this is how demons",
    "start": "1231410",
    "end": "1238520"
  },
  {
    "text": "work they are attached a note and can't be scheduled anywhere else so that's the",
    "start": "1238520",
    "end": "1245570"
  },
  {
    "text": "simple behavior in case of like a small scale problems but what happens if a",
    "start": "1245570",
    "end": "1252950"
  },
  {
    "start": "1251000",
    "end": "1251000"
  },
  {
    "text": "large number of nodes is unhealthy can we do exactly the same thing well kind",
    "start": "1252950",
    "end": "1259190"
  },
  {
    "text": "of can write like we can evacuate all the unhealthy nodes and try to put the",
    "start": "1259190",
    "end": "1264260"
  },
  {
    "text": "pods that well are pending for the for the time being on the healthy ones but",
    "start": "1264260",
    "end": "1270230"
  },
  {
    "text": "it may turn out that there's not enough space for everyone all right",
    "start": "1270230",
    "end": "1276559"
  },
  {
    "text": "this of course might be the desired behavior like if the pots are really",
    "start": "1276559",
    "end": "1282290"
  },
  {
    "text": "unhealthy right like we want to put this workload somewhere but if it wasn't",
    "start": "1282290",
    "end": "1290240"
  },
  {
    "text": "really the problem with nodes but for some reason the nodes were like",
    "start": "1290240",
    "end": "1296169"
  },
  {
    "text": "disconnected from the master but like all other communication was perfectly fine the pots are running there right",
    "start": "1296169",
    "end": "1302230"
  },
  {
    "text": "they're still running this is not a huge deal because until the nodes will hear",
    "start": "1302230",
    "end": "1311210"
  },
  {
    "text": "from the master they won't kill the pots cause like they don't know they should but after they reconnect to the to the",
    "start": "1311210",
    "end": "1319549"
  },
  {
    "text": "cluster they will all in the same time notice that they should kill the pots",
    "start": "1319549",
    "end": "1324919"
  },
  {
    "text": "and do it in the same time which will cause the egg the massive pod eviction",
    "start": "1324919",
    "end": "1330530"
  },
  {
    "text": "which is usually undesirable because it takes some time to start them up again",
    "start": "1330530",
    "end": "1336480"
  },
  {
    "text": "lack of luckily the experience shows that if the large number of nodes is",
    "start": "1337650",
    "end": "1345340"
  },
  {
    "text": "unhealthy it is quite unlikely that this is a problem with nodes actually luckily",
    "start": "1345340",
    "end": "1353490"
  },
  {
    "text": "exam of you were the talk 101 ways of",
    "start": "1353490",
    "end": "1360159"
  },
  {
    "text": "breaking covers with companies clusters they were showing exactly this kind of failures which were caused by some",
    "start": "1360159",
    "end": "1366429"
  },
  {
    "text": "networking issues all right so this is exactly what I'm talking about this",
    "start": "1366429",
    "end": "1373029"
  },
  {
    "text": "means that if this happens you probably want a person to take a look",
    "start": "1373029",
    "end": "1378159"
  },
  {
    "text": "sadly like it's not the perfect world we would like to be optimized automatized but it's best for to make have someone",
    "start": "1378159",
    "end": "1385720"
  },
  {
    "text": "that will take a look the bad thing here is that machines are much faster than",
    "start": "1385720",
    "end": "1392080"
  },
  {
    "text": "people right like node controller can go and kill the pause before anyone notices",
    "start": "1392080",
    "end": "1397679"
  },
  {
    "text": "because of that we change the behavior of node controller and start throttling",
    "start": "1397679",
    "end": "1403600"
  },
  {
    "text": "the killing if we observe like the large number of parts of nodes being unhealthy",
    "start": "1403600",
    "end": "1409179"
  },
  {
    "text": "I believe default is like a third it like more than a third of the cluster is unhealthy it will start slowing down it",
    "start": "1409179",
    "end": "1416710"
  },
  {
    "text": "won't be killing anything very quickly like pause very quickly I just configure with a flag the default normal eviction",
    "start": "1416710",
    "end": "1425830"
  },
  {
    "text": "rate is like one note per 10 seconds so like a hundred node cluster will be evicted in like 20 minutes give or take",
    "start": "1425830",
    "end": "1434220"
  },
  {
    "text": "and the slower one is one note per hundred seconds which will bump it to",
    "start": "1434220",
    "end": "1439720"
  },
  {
    "text": "like over three hours which is probably enough time for human to go and see",
    "start": "1439720",
    "end": "1445960"
  },
  {
    "text": "what's going on and decide of what's the correct course of action is of course if",
    "start": "1445960",
    "end": "1451090"
  },
  {
    "text": "the human wander is like eventually all the pods will get evicted and created",
    "start": "1451090",
    "end": "1456640"
  },
  {
    "text": "somewhere else okay there's one more case here that's probably worth",
    "start": "1456640",
    "end": "1461770"
  },
  {
    "text": "mentioning like what would happen if all of the notes are healthy this is like it is almost certain that",
    "start": "1461770",
    "end": "1467380"
  },
  {
    "text": "is a issue with master like I haven't seen idly all of the nodes like failed in the same time which means we",
    "start": "1467380",
    "end": "1473830"
  },
  {
    "text": "shouldn't do anything because it's probably like most likely master and secondly even if he killed this post",
    "start": "1473830",
    "end": "1479020"
  },
  {
    "text": "like what can we do with them like there's no place to schedule them right so like they will just keep pending okay",
    "start": "1479020",
    "end": "1486340"
  },
  {
    "start": "1486000",
    "end": "1486000"
  },
  {
    "text": "so now I will tell you how to make this work for you even better the issue with",
    "start": "1486340",
    "end": "1492970"
  },
  {
    "text": "the thing that I just said is that it is not really configurable there are four",
    "start": "1492970",
    "end": "1498300"
  },
  {
    "text": "knobs that are exposed you can tell the system how quickly it should start to",
    "start": "1498300",
    "end": "1504760"
  },
  {
    "text": "mark notice unreachable so stop scheduling new post there how quick",
    "start": "1504760",
    "end": "1510730"
  },
  {
    "text": "quickly it should start evicting pots from the front from the given notes how",
    "start": "1510730",
    "end": "1515740"
  },
  {
    "text": "at what rate it should evict if there is a small number on not ready note or unreachable and it was the the slower",
    "start": "1515740",
    "end": "1523480"
  },
  {
    "text": "rate of addiction that's the only four knobs that are exposed this main might",
    "start": "1523480",
    "end": "1530650"
  },
  {
    "text": "be fine if you run the same prods like the same class of pots all that I like",
    "start": "1530650",
    "end": "1535960"
  },
  {
    "text": "in the cluster but generally the waters are heterogeneous so some of them may",
    "start": "1535960",
    "end": "1542410"
  },
  {
    "text": "have a special needs right like for example you may have a pot that really depends on the local disk and there's",
    "start": "1542410",
    "end": "1548290"
  },
  {
    "text": "make it make no sense whatsoever to evicted right like the disk will be gone or you may have a pot that takes a",
    "start": "1548290",
    "end": "1555970"
  },
  {
    "text": "really long time to start and then you probably will like to wait a bit more",
    "start": "1555970",
    "end": "1561580"
  },
  {
    "text": "with the hope that it will come back before killing it from the nodes or on",
    "start": "1561580",
    "end": "1567400"
  },
  {
    "text": "the contrary you may have a very very critical part that you really need to like really want to be running and you",
    "start": "1567400",
    "end": "1574960"
  },
  {
    "text": "wanted to remove from the node as quickly as possible after it is treated",
    "start": "1574960",
    "end": "1580030"
  },
  {
    "text": "i considered unhealthy like and with the single timeout you can't possibly make",
    "start": "1580030",
    "end": "1588430"
  },
  {
    "text": "all of those like make the system work correctly for all of those cases right so there's an alpha feature called paint",
    "start": "1588430",
    "end": "1595960"
  },
  {
    "start": "1594000",
    "end": "1594000"
  },
  {
    "text": "based addiction which changes the behavior of no which instead of directly killing parts",
    "start": "1595960",
    "end": "1606039"
  },
  {
    "text": "from the nodes it just paints notes that applies thanks to the nose like the not control paints the note that has no",
    "start": "1606039",
    "end": "1612190"
  },
  {
    "text": "component is dot io / initial bottom not ready depends account of what's the condition of the note and leaves the",
    "start": "1612190",
    "end": "1620159"
  },
  {
    "text": "killing to the change controller exactly the same taint controller that does it",
    "start": "1620159",
    "end": "1625870"
  },
  {
    "text": "when you apply when you do keep CTL paint for those who are not familiar",
    "start": "1625870",
    "end": "1633759"
  },
  {
    "text": "tense this is something that you can apply to notes that will generally",
    "start": "1633759",
    "end": "1639159"
  },
  {
    "text": "prevent thoughts from being scheduled and those no on this note or like a big stuff that's on that ransom then on them",
    "start": "1639159",
    "end": "1646230"
  },
  {
    "text": "and the second part of this picture is toleration switch you can apply to pots and tell them that they can ignore some",
    "start": "1646230",
    "end": "1654519"
  },
  {
    "text": "of those things so even this feature is",
    "start": "1654519",
    "end": "1659619"
  },
  {
    "text": "enabled in both addictions it means that you can actually configure the behavior",
    "start": "1659619",
    "end": "1668320"
  },
  {
    "text": "of your pots of every single pot on your cluster when there are so problems that",
    "start": "1668320",
    "end": "1677499"
  },
  {
    "text": "the note has some problems like you can like specify that this pot needs to live there forever I don't want to affect it",
    "start": "1677499",
    "end": "1683769"
  },
  {
    "text": "ever or this part needs to be evicted as quickly as possible just like I will set depolarization",
    "start": "1683769",
    "end": "1690369"
  },
  {
    "text": "seconds to 1 and it will be evicted one second after the note will turn will",
    "start": "1690369",
    "end": "1696669"
  },
  {
    "text": "become unreachable the important detail here is that because of backward",
    "start": "1696669",
    "end": "1703149"
  },
  {
    "text": "compatibility the kubernetes will by default applied the Toleration 's equal",
    "start": "1703149",
    "end": "1709230"
  },
  {
    "text": "to' like did the water the behavior that you can't observe so like the like 4",
    "start": "1709230",
    "end": "1714789"
  },
  {
    "text": "minutes 20 seconds of toleration for the unreachable or whatever to to talk well",
    "start": "1714789",
    "end": "1721539"
  },
  {
    "text": "to keep the current behavior which means if you want to evict quickly you can't",
    "start": "1721539",
    "end": "1727269"
  },
  {
    "text": "just don't set it you need to set it to 1 because like if it will be unset",
    "start": "1727269",
    "end": "1732730"
  },
  {
    "text": "therefore default will kick in and and just like your default behavior okay so",
    "start": "1732730",
    "end": "1739900"
  },
  {
    "start": "1738000",
    "end": "1738000"
  },
  {
    "text": "that's it if when it comes to the workloads making sure that workers",
    "start": "1739900",
    "end": "1745600"
  },
  {
    "text": "running how kubernetes tries to make sure that that there's no problems for clouds now a short story about how",
    "start": "1745600",
    "end": "1752500"
  },
  {
    "text": "kubernetes tries to heal itself well of",
    "start": "1752500",
    "end": "1758260"
  },
  {
    "text": "course all the controllers and like that normal binaries which can die be killed and we also want to upgrade them which",
    "start": "1758260",
    "end": "1765220"
  },
  {
    "text": "means the every single controller needs to be able to start from any States no",
    "start": "1765220",
    "end": "1772030"
  },
  {
    "text": "matter how broken the state of the cluster is it needs to be able to like after being started let's make it better",
    "start": "1772030",
    "end": "1781260"
  },
  {
    "text": "this is like the standard like micro services and stateless architecture but",
    "start": "1781470",
    "end": "1787750"
  },
  {
    "text": "because of that we can use the exactly the same logic the same behavior",
    "start": "1787750",
    "end": "1795150"
  },
  {
    "text": "periodically to just like pretend that the controller is being restarted so it",
    "start": "1795150",
    "end": "1800950"
  },
  {
    "text": "instead of but instead to make the a bit lighter on the API server side so do not",
    "start": "1800950",
    "end": "1807430"
  },
  {
    "text": "just kill it within a lot of lists it will just like IT right through internal States and pretend it sees the every",
    "start": "1807430",
    "end": "1815140"
  },
  {
    "text": "single object for the first time and like what it will do with that and if it's wrong it will fix it if it's right",
    "start": "1815140",
    "end": "1821740"
  },
  {
    "text": "it will just like as its general in the idempotent like it won't do anything this might sound like not very powerful",
    "start": "1821740",
    "end": "1829540"
  },
  {
    "text": "but it's actually really strong saying it is so strong that it masks a very",
    "start": "1829540",
    "end": "1837550"
  },
  {
    "text": "serious back in the service controller for like a number of releases the buck",
    "start": "1837550",
    "end": "1843310"
  },
  {
    "text": "was that when we had we had this like node port service type service which",
    "start": "1843310",
    "end": "1848370"
  },
  {
    "text": "assigns a port number to a service and then whatever traffic comes to like for",
    "start": "1848370",
    "end": "1856510"
  },
  {
    "text": "any connection that every connection that gets to any poor node in the cluster on this given part it gets",
    "start": "1856510",
    "end": "1862960"
  },
  {
    "text": "forwarded to the service so of course it's bad it's like the same port numbers assigned",
    "start": "1862960",
    "end": "1868559"
  },
  {
    "text": "to multiple services but component is doing what doing that for like number of releases really the reason why no one",
    "start": "1868559",
    "end": "1876450"
  },
  {
    "text": "noticed is because like after pretty soon after it was it happened like this reconciliation look pretend that you see",
    "start": "1876450",
    "end": "1882600"
  },
  {
    "text": "this for the first time kicked in and what it was noticing that something's wrong because this this particular jerk",
    "start": "1882600",
    "end": "1890039"
  },
  {
    "text": "was correct like we have I have two services assigned the same port and just change one of them and system load",
    "start": "1890039",
    "end": "1895740"
  },
  {
    "text": "itself summarize the self-healing",
    "start": "1895740",
    "end": "1903299"
  },
  {
    "text": "features that we have covered in this talk we have cubelets on each nodes and",
    "start": "1903299",
    "end": "1908909"
  },
  {
    "text": "monitoring your pods waiting to restart any crashing containers or when liveness",
    "start": "1908909",
    "end": "1914010"
  },
  {
    "text": "probe health checks are failing the node problem detector helps to diagnose kernel issues and report them to the api",
    "start": "1914010",
    "end": "1920610"
  },
  {
    "text": "server node controller is performing pod evictions when nodes fail and with taints and",
    "start": "1920610",
    "end": "1927059"
  },
  {
    "text": "toleration available for you to provide more subtle customized ways to",
    "start": "1927059",
    "end": "1932370"
  },
  {
    "text": "redistribute pods when this happens and as Ken mentioned in her keynote yesterday these automated healing",
    "start": "1932370",
    "end": "1938250"
  },
  {
    "text": "mechanisms are really part of the superpowers of kubernetes but even with",
    "start": "1938250",
    "end": "1943500"
  },
  {
    "start": "1942000",
    "end": "1942000"
  },
  {
    "text": "these surprisingly powerful mechanisms that can handle a great number of problems automatically it's not perfect",
    "start": "1943500",
    "end": "1950159"
  },
  {
    "text": "there are bugs there are maybe health checking heuristics that don't cover all",
    "start": "1950159",
    "end": "1956039"
  },
  {
    "text": "cases and we really hope that this talk is inspired you to raise it with the",
    "start": "1956039",
    "end": "1961860"
  },
  {
    "text": "community if you think that there is a feature that should be handled automatically or a bug that needs fixing",
    "start": "1961860",
    "end": "1967320"
  },
  {
    "text": "and finally just to wrap up going one level further up from kubernetes your",
    "start": "1967320",
    "end": "1973169"
  },
  {
    "start": "1969000",
    "end": "1969000"
  },
  {
    "text": "favorite cloud provider is also having some automated features when you run a",
    "start": "1973169",
    "end": "1978720"
  },
  {
    "text": "managed to kubernetes cluster as a user of gke so in the context of this talk gke",
    "start": "1978720",
    "end": "1984990"
  },
  {
    "text": "provides automated repairs of masters whenever component statuses become unhealthy the repair potion is very",
    "start": "1984990",
    "end": "1992220"
  },
  {
    "text": "simple it's very straightforward but it's actually very effective we simply recreate the master v",
    "start": "1992220",
    "end": "1997669"
  },
  {
    "text": "and you also have a team of friendly sres that will help debug issues on your",
    "start": "1997669",
    "end": "2003159"
  },
  {
    "text": "master when this automated process does not work and we also have a similar feature for node workers we also provide",
    "start": "2003159",
    "end": "2011049"
  },
  {
    "text": "automatic master and node upgrades making sure that they're operating on the latest stable minor version",
    "start": "2011049",
    "end": "2018719"
  },
  {
    "text": "up-to-date with security and bug fixes and we've even been known to upgrade",
    "start": "2018719",
    "end": "2024279"
  },
  {
    "text": "clusters in between open source kubernetes releases to cover important security vulnerabilities such as the DNS",
    "start": "2024279",
    "end": "2031389"
  },
  {
    "text": "mask vulnerability that was identified by the Google security team in October",
    "start": "2031389",
    "end": "2036669"
  },
  {
    "text": "but we're developing tools all the time to make problem diagnosis and automated",
    "start": "2036669",
    "end": "2043389"
  },
  {
    "text": "healing processes more intelligent so we would love to hear your feedback and with that I think we're out of time but",
    "start": "2043389",
    "end": "2049599"
  },
  {
    "start": "2048000",
    "end": "2048000"
  },
  {
    "text": "thanks for listening thanks for sticking til the end and see you guys in Copenhagen",
    "start": "2049599",
    "end": "2055649"
  },
  {
    "text": "[Applause]",
    "start": "2055650",
    "end": "2060359"
  }
]