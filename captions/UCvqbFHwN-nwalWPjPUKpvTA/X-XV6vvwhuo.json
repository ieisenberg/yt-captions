[
  {
    "text": "welcome to our talk about cgi-bin which is definitely a really new technology it",
    "start": "179",
    "end": "8610"
  },
  {
    "text": "makes developing web applications really simple all you have to do is drop a really simple pearls clip script into a",
    "start": "8610",
    "end": "15330"
  },
  {
    "text": "certain directory called CGI bin on your web server and and that's it it's",
    "start": "15330",
    "end": "20490"
  },
  {
    "text": "deployed and you can just run it from whatever path it's at so unfortunately",
    "start": "20490",
    "end": "25650"
  },
  {
    "text": "our talk is not exactly about CGI bin it's about functions as a service which is definitely a new technology for",
    "start": "25650",
    "end": "32910"
  },
  {
    "text": "kubernetes this talk is about phishing so so on a more serious note it's been",
    "start": "32910",
    "end": "39989"
  },
  {
    "text": "more than two decades in cgi-bin and we lost that simplicity that it had so",
    "start": "39989",
    "end": "47579"
  },
  {
    "text": "today you have this long series of steps build a container push it to a registry figure out your kubernetes configuration",
    "start": "47579",
    "end": "54780"
  },
  {
    "text": "make sure that luster is set up correctly and then repeat some of these steps every time your your app changes rebuild the container push it figure out",
    "start": "54780",
    "end": "61170"
  },
  {
    "text": "how your versions run so this is not to say that this new world is terrible we've gained a lot we've gained the",
    "start": "61170",
    "end": "67830"
  },
  {
    "text": "ability to have homogenous deployment seas in containers we've gained really powerful orthogonal distributed systems",
    "start": "67830",
    "end": "74010"
  },
  {
    "text": "primitives with kubernetes but we made it really hard to get started in this world so so what we're really trying to",
    "start": "74010",
    "end": "81810"
  },
  {
    "text": "do is get to a point where we can have the power of containers and kubernetes but make it much simpler also so hold",
    "start": "81810",
    "end": "88560"
  },
  {
    "text": "that thought I'm going to switch topics a little bit to resource utilization",
    "start": "88560",
    "end": "93710"
  },
  {
    "text": "specifically CPU and memory so we're we're going towards this world where we have divided our solutions or",
    "start": "93710",
    "end": "100619"
  },
  {
    "text": "applications into a lot of tiny pieces and many of those pieces are very rarely used especially if they are driven by",
    "start": "100619",
    "end": "106590"
  },
  {
    "text": "events that occur fairly rarely so so once you have enough of those services",
    "start": "106590",
    "end": "111780"
  },
  {
    "text": "your cluster capacity has to account for all of the services that you've deployed and our idle and also for the services",
    "start": "111780",
    "end": "118799"
  },
  {
    "text": "that you that are actually loaded which are the ones that you which are the only ones that you really would like to pay",
    "start": "118799",
    "end": "124320"
  },
  {
    "text": "for so ideally the services that are idle should be free so what if we could",
    "start": "124320",
    "end": "130379"
  },
  {
    "text": "solve both these problems what if we could have dinner's and the power of containers in kubernetes but really simple their",
    "start": "130379",
    "end": "136640"
  },
  {
    "text": "workflows and what if we could have our cluster capacities be more directly proportional to our to our actual usage",
    "start": "136640",
    "end": "143709"
  },
  {
    "text": "and the answer to that the answer to both of those things is functions as a service it's one of the really good",
    "start": "143709",
    "end": "151280"
  },
  {
    "text": "answers to both of these questions and to focus on one of the points in",
    "start": "151280",
    "end": "156829"
  },
  {
    "text": "functions as a service is that if you want services to be free when they're idle and if you also want them to have",
    "start": "156829",
    "end": "163370"
  },
  {
    "text": "good performance especially latency when when they do get a bunch of requests then you need to make sure that you go",
    "start": "163370",
    "end": "169700"
  },
  {
    "text": "from no instances to enough instances really quickly when your traffic does come in so this brings us to fishing",
    "start": "169700",
    "end": "176299"
  },
  {
    "text": "fishing is a functions as a service for kubernetes the user you write short lists short-lived",
    "start": "176299",
    "end": "182329"
  },
  {
    "text": "stateless functions you define them declaratively and at the source level we'll talk much more about that and they",
    "start": "182329",
    "end": "188780"
  },
  {
    "text": "are free when idle you only pay for the storage of those functions they consume CPU and memory only when they're running",
    "start": "188780",
    "end": "194750"
  },
  {
    "text": "and they started quickly on demand so as a user you have only three concepts to",
    "start": "194750",
    "end": "200780"
  },
  {
    "text": "learn inflation functions environments and triggers functions more properly",
    "start": "200780",
    "end": "205819"
  },
  {
    "text": "their modules that the entry point is a function the function runs inside an",
    "start": "205819",
    "end": "211579"
  },
  {
    "text": "environment so an environment efficient is all of the language specific stuff that that's in a all of the language",
    "start": "211579",
    "end": "219709"
  },
  {
    "text": "specific stuff efficient is in environment so you have no Jersey and an environment is a container which",
    "start": "219709",
    "end": "228409"
  },
  {
    "text": "loads a function dynamically and a trigger is a mapping of an event to a",
    "start": "228409",
    "end": "233540"
  },
  {
    "text": "function call so HTTP triggers map HTTP requests to function calls their message",
    "start": "233540",
    "end": "238879"
  },
  {
    "text": "q triggers and so on so there's a whole bunch of triggers and environments supported node.js Python go the",
    "start": "238879",
    "end": "246530"
  },
  {
    "text": "synchronous HTTP 2 or 3 message queues and kubernetes watches timers and so on",
    "start": "246530",
    "end": "253359"
  },
  {
    "text": "more complete list on the website so let's get a little bit into how fission",
    "start": "253359",
    "end": "259789"
  },
  {
    "text": "executes functions and this is one of the ways fission execute functions but it's the more interesting way so we'll",
    "start": "259789",
    "end": "265400"
  },
  {
    "text": "talk a little more about this fundamentally the way to get fast cold starts is by",
    "start": "265400",
    "end": "271600"
  },
  {
    "text": "having a pre warmed pool of something and since efficient runs on kubernetes",
    "start": "271600",
    "end": "277060"
  },
  {
    "text": "it has a pre one pool of containers running in parts and so there's a",
    "start": "277060",
    "end": "282910"
  },
  {
    "text": "fishing client and fish and resources are stored as kubernetes custom resources so the client let's lesson you",
    "start": "282910",
    "end": "290590"
  },
  {
    "text": "know it functions as these colorful circles and the client uploads these functions into the kubernetes api and",
    "start": "290590",
    "end": "296310"
  },
  {
    "text": "oversimplifying a little bit and the patient pool manager notices that these",
    "start": "296310",
    "end": "302050"
  },
  {
    "text": "functions have been created on the kubernetes api it figures out what environments these functions are running",
    "start": "302050",
    "end": "308050"
  },
  {
    "text": "so these functions need so if there is a node.js and a Python function it creates pools of node and Python containers and",
    "start": "308050",
    "end": "317220"
  },
  {
    "text": "let's focus on HTTP requests so if there's an HTTP request it comes into the router and let's say it's a blue",
    "start": "317220",
    "end": "324880"
  },
  {
    "text": "request for that blue function then the let's say it's the first time this function has drawn so now that request",
    "start": "324880",
    "end": "332080"
  },
  {
    "text": "is waiting and we need to create an instance a running instance of this function while that request is waiting so this is a cold start so what we need",
    "start": "332080",
    "end": "340780"
  },
  {
    "text": "to do is the router makes a request to the pool manager which draws a already running part from that pool it loads",
    "start": "340780",
    "end": "349240"
  },
  {
    "text": "that function into that pod and it hands over the address of that part to the router which then proxies that request",
    "start": "349240",
    "end": "356650"
  },
  {
    "text": "into the pot similarly as more requests come in the same process repeats this",
    "start": "356650",
    "end": "364180"
  },
  {
    "text": "cold start process takes on the order of 100 or so milliseconds give or take",
    "start": "364180",
    "end": "370320"
  },
  {
    "text": "those pods are cached as for a while even if there are no more requests for a",
    "start": "370320",
    "end": "376000"
  },
  {
    "text": "few minutes so that subsequent requests can hold on subsequent requests can reuse those pods and if they're no more",
    "start": "376000",
    "end": "383110"
  },
  {
    "text": "requests if a function is idle for several minutes those parts are killed and you regain that CPU and memory that",
    "start": "383110",
    "end": "390250"
  },
  {
    "text": "they were using so the function is free again so let's look at how this actually works I'm gonna switch my",
    "start": "390250",
    "end": "398350"
  },
  {
    "text": "this way so I can actually see that screen okay that's way too small people",
    "start": "398350",
    "end": "413260"
  },
  {
    "text": "in the back can you read yes know anyone yes all right thank you okay so screen all",
    "start": "413260",
    "end": "424690"
  },
  {
    "text": "right okay so this is a sorry other demo",
    "start": "424690",
    "end": "436050"
  },
  {
    "text": "so this is a kubernetes cluster deployed on gke with fission installed",
    "start": "436470",
    "end": "442860"
  },
  {
    "text": "okay we're connected to the cluster there are those nodes you can just see the fish in deployment it's not super",
    "start": "444660",
    "end": "451270"
  },
  {
    "text": "important what those pods are will run a really simple hello world function and",
    "start": "451270",
    "end": "457750"
  },
  {
    "text": "will create an environment for that function in on this time across red was already created but all you have to do",
    "start": "457750",
    "end": "464260"
  },
  {
    "text": "is specify that node.js environment that fish and ships with and here we're just",
    "start": "464260",
    "end": "471250"
  },
  {
    "text": "looking at the pool that got created so all of those containers all of those pods are idle",
    "start": "471250",
    "end": "477520"
  },
  {
    "text": "you can tune how many there are but in this case there are three and we create",
    "start": "477520",
    "end": "482920"
  },
  {
    "text": "the fission function this this function is is now uploaded to fission it's stored as a kubernetes custom resource",
    "start": "482920",
    "end": "489340"
  },
  {
    "text": "but it's not executing yet there's no runtime resources allocated to it just yet then we set up an HTTP trigger for",
    "start": "489340",
    "end": "497590"
  },
  {
    "text": "this function also called a route and we actually hit that route with curl and",
    "start": "497590",
    "end": "505740"
  },
  {
    "text": "let's see so here you can see it took so",
    "start": "505740",
    "end": "511900"
  },
  {
    "text": "we ran called three times on that hello world function it first of all it actually worked and we got hello world",
    "start": "511900",
    "end": "517919"
  },
  {
    "text": "secondly you can see that the first one was a little bit slower than the subsequent invocations the first one to",
    "start": "517919",
    "end": "525070"
  },
  {
    "text": "290 milliseconds that's 90 milliseconds and that's about 100 so there's something between 100 and 200",
    "start": "525070",
    "end": "531190"
  },
  {
    "text": "milliseconds of overhead in the cold start and that part is cached and you can find it with your usual kubernetes",
    "start": "531190",
    "end": "538020"
  },
  {
    "text": "commands the part is labeled with the function so if you have any monitoring tools that that use labels you can keep",
    "start": "538020",
    "end": "544720"
  },
  {
    "text": "using them okay so that's so that's hello world I'm gonna switch back a",
    "start": "544720",
    "end": "552700"
  },
  {
    "text": "little bit to our slides and talk about",
    "start": "552700",
    "end": "559959"
  },
  {
    "text": "specifying an application so so we saw that this this demo creates create your function using the command line which is",
    "start": "559959",
    "end": "567100"
  },
  {
    "text": "great for starting out and experimenting but how do you deal with how do you do that in production where do you save",
    "start": "567100",
    "end": "573550"
  },
  {
    "text": "that command line do you write a script and check it in that sounds kind of horrible so ideally you'd have some sort",
    "start": "573550",
    "end": "581110"
  },
  {
    "text": "of declarative specification and you check it into source control and that's great for doing for doing updates for",
    "start": "581110",
    "end": "588670"
  },
  {
    "text": "having idempotent behavior and i what we really want is both we need we want command lines to get started and",
    "start": "588670",
    "end": "594580"
  },
  {
    "text": "declarative specifications so that we can check them in and have ongoing maintenance and we can do both of these",
    "start": "594580",
    "end": "600760"
  },
  {
    "text": "things we're doing something that we call config by example so what what we",
    "start": "600760",
    "end": "607120"
  },
  {
    "text": "want to do is say create this function but save what you're doing in some sort",
    "start": "607120",
    "end": "612279"
  },
  {
    "text": "of like relative spec so that you can then apply that spec to another cluster",
    "start": "612279",
    "end": "617850"
  },
  {
    "text": "okay so let's switch back to switch back",
    "start": "617850",
    "end": "628380"
  },
  {
    "text": "all right let's try this",
    "start": "631010",
    "end": "635000"
  },
  {
    "text": "okay [Music]",
    "start": "641940",
    "end": "646089"
  },
  {
    "text": "hopefully people in the back can read this okay",
    "start": "648010",
    "end": "654220"
  },
  {
    "text": "so let's take a simple Python function and it's over there I'm actually gonna",
    "start": "654220",
    "end": "666040"
  },
  {
    "text": "fullscreen that and now what we've done is we've done the same function create command that you saw earlier except that",
    "start": "666040",
    "end": "672700"
  },
  {
    "text": "we've saved what it did in a specification right and that",
    "start": "672700",
    "end": "679690"
  },
  {
    "text": "specification is saved in a specs directory at the root of your application and there's a lot more in",
    "start": "679690",
    "end": "685330"
  },
  {
    "text": "that llaman but this is basically a Cuban Aires custom resource you don't ever have to write the sea animal from",
    "start": "685330",
    "end": "691450"
  },
  {
    "text": "scratch you can just create them using these command lines okay and then you can just apply this spec you can check",
    "start": "691450",
    "end": "698290"
  },
  {
    "text": "that ml in and you can apply it to the same placer to any other cluster and so that function is now created and we can",
    "start": "698290",
    "end": "705430"
  },
  {
    "text": "run that test okay so that's actually",
    "start": "705430",
    "end": "714970"
  },
  {
    "text": "run now what we'd also like to do is have a really nice and fast code edit",
    "start": "714970",
    "end": "723490"
  },
  {
    "text": "test debug change the code cycle right here especially in a developer workflow",
    "start": "723490",
    "end": "735930"
  },
  {
    "text": "so what we can do is since we have a declarative specification we can get fish in the fish inclined to watch your",
    "start": "736290",
    "end": "742150"
  },
  {
    "text": "file system and every time you change the file to rebuild the function upload",
    "start": "742150",
    "end": "747580"
  },
  {
    "text": "it and deploy it and this is basically to make your dev workflows really easy",
    "start": "747580",
    "end": "753730"
  },
  {
    "text": "and fast right so let's save that and the efficiency Ally says okay I noticed",
    "start": "753730",
    "end": "761950"
  },
  {
    "text": "a file change I'm gonna apply the spec again and now if we just reload this we",
    "start": "761950",
    "end": "767110"
  },
  {
    "text": "see it's deployed ok so you have a feedback loop that works on the order of milliseconds instead of a really long",
    "start": "767110",
    "end": "773980"
  },
  {
    "text": "time we didn't rebuild any containers here your code was just deployed in the cluster now this is great for a dev",
    "start": "773980",
    "end": "781720"
  },
  {
    "text": "workflow but what you can do is you can pull out the the file you can once you",
    "start": "781720",
    "end": "788829"
  },
  {
    "text": "qualify your file and you're satisfied with the testing you can then save that file and check in your specifications",
    "start": "788829",
    "end": "797170"
  },
  {
    "text": "and use them in production and everyone on your team can have an easy",
    "start": "797170",
    "end": "802300"
  },
  {
    "text": "development workflow using this this kind of thing so so this is Python which",
    "start": "802300",
    "end": "811410"
  },
  {
    "text": "in which is fairly easy to do these things because it's an interpreted language so so we don't have to build it",
    "start": "811410",
    "end": "817199"
  },
  {
    "text": "but what about things like go alright so we go we need we need to build the",
    "start": "817199",
    "end": "825940"
  },
  {
    "text": "function and we can do that too in fission and we can also do it declaratively so here I have a go",
    "start": "825940",
    "end": "835269"
  },
  {
    "text": "HelloWorld application the specs have already been created using the same way and I'm going to do the same apply watch",
    "start": "835269",
    "end": "840850"
  },
  {
    "text": "thing and oops",
    "start": "840850",
    "end": "845220"
  },
  {
    "text": "so that one was already there but let's",
    "start": "848519",
    "end": "854649"
  },
  {
    "text": "do the same thing and ok so we save it",
    "start": "854649",
    "end": "862529"
  },
  {
    "text": "and now fission needs to build that so it just builds it the the the pattern is",
    "start": "862529",
    "end": "871300"
  },
  {
    "text": "again decorated the declarative so the source code is uploaded and a build controller watches that package notices",
    "start": "871300",
    "end": "878260"
  },
  {
    "text": "that their source code runs the go compiler nothing needs to be installed",
    "start": "878260",
    "end": "883480"
  },
  {
    "text": "on your on your local machine all of the actual compile happens on the cluster itself and you can reload that and that",
    "start": "883480",
    "end": "893920"
  },
  {
    "text": "works too alright so you can keep watching this you can keep making changes it takes a few seconds for the",
    "start": "893920",
    "end": "904360"
  },
  {
    "text": "go builder to run you can do that alright ok so",
    "start": "904360",
    "end": "910660"
  },
  {
    "text": "so this is great for four so we talked a",
    "start": "910660",
    "end": "920290"
  },
  {
    "text": "bit about development workflows I'm gonna switch to things that you care about at runtime okay so things that you",
    "start": "920290",
    "end": "934450"
  },
  {
    "text": "care about at runtime two of the things that you care about are observability and and we'll talk",
    "start": "934450",
    "end": "940480"
  },
  {
    "text": "about auto scaling as well if you are at at ben securement keynote yesterday you saw how modern outages are kind of like",
    "start": "940480",
    "end": "947800"
  },
  {
    "text": "murder mysteries because there's so many interacting components and so and if you",
    "start": "947800",
    "end": "953560"
  },
  {
    "text": "think about it functions as a service makes those murder mysteries even worse because now you have 10 times as many",
    "start": "953560",
    "end": "959830"
  },
  {
    "text": "interacting components so observability becomes really important when you have a lot of interacting functions so like",
    "start": "959830",
    "end": "967810"
  },
  {
    "text": "everyone else we're really excited about sto and we've integrated fishin with sto so that your function pods have the sto",
    "start": "967810",
    "end": "974950"
  },
  {
    "text": "sidecar have envoi and they report into",
    "start": "974950",
    "end": "980850"
  },
  {
    "text": "it reports data into Prometheus graph fauna and so on let me find that",
    "start": "980850",
    "end": "988589"
  },
  {
    "text": "seem to have lost mine is to that long well that's just running to me",
    "start": "996730",
    "end": "1002570"
  },
  {
    "text": "so we have just a HelloWorld function in no just on this cluster and we have",
    "start": "1008300",
    "end": "1014810"
  },
  {
    "text": "history or deployed as well so you can see it's to assist so it's deployed here",
    "start": "1014810",
    "end": "1025939"
  },
  {
    "text": "and when we actually test the function",
    "start": "1025940",
    "end": "1030669"
  },
  {
    "text": "it runs okay pretend that didn't happen",
    "start": "1034030",
    "end": "1039940"
  },
  {
    "text": "all right so this is a HelloWorld function for those who can't see it at",
    "start": "1040030",
    "end": "1045589"
  },
  {
    "text": "the back here it is again and that shows up on both Prometheus and open tracing",
    "start": "1045589",
    "end": "1055360"
  },
  {
    "text": "the Jaeger dashboards that sto creates for us okay so now let's actually",
    "start": "1055360",
    "end": "1064100"
  },
  {
    "text": "generate a little bit of load and see that see how that shows up in our in our",
    "start": "1064100",
    "end": "1069740"
  },
  {
    "text": "dashboards this is the function being loaded hey is a simple load generator what",
    "start": "1069740",
    "end": "1076940"
  },
  {
    "text": "we're doing here is sending it two thousand requests with it send it 200",
    "start": "1076940",
    "end": "1083230"
  },
  {
    "text": "with a concurrency of 20 so 20 requests simultaneously going on and that was the",
    "start": "1083230",
    "end": "1092480"
  },
  {
    "text": "little test request we did earlier so we should ideally see this this graph go up",
    "start": "1092480",
    "end": "1097490"
  },
  {
    "text": "in a in a moment and this is the usual",
    "start": "1097490",
    "end": "1103550"
  },
  {
    "text": "Prometheus dashboard at sto creates meanwhile okay here it is it's doing",
    "start": "1103550",
    "end": "1110650"
  },
  {
    "text": "it'll eventually go to 20 I think I've actually finished I can do some more",
    "start": "1110650",
    "end": "1119570"
  },
  {
    "text": "all right and similarly we get tracing so let's pick an arbitrary trace and",
    "start": "1119570",
    "end": "1129090"
  },
  {
    "text": "look at it and we can see here that this trace is pretty simple there's just two",
    "start": "1129090",
    "end": "1135179"
  },
  {
    "text": "spans this is the ficient the first one up there you probably can't read it is",
    "start": "1135179",
    "end": "1140490"
  },
  {
    "text": "is efficient router and the lower one is the actual fission function which just does hello world in this case the router",
    "start": "1140490",
    "end": "1148799"
  },
  {
    "text": "took already something milliseconds and actual function took 28 milliseconds so",
    "start": "1148799",
    "end": "1156360"
  },
  {
    "text": "this might have been the cold start this",
    "start": "1156360",
    "end": "1161658"
  },
  {
    "text": "is a slightly more representative trace not really not sure why anyway this",
    "start": "1162530",
    "end": "1170159"
  },
  {
    "text": "trace actually shows you the overhead of the router which in this case is of single-digit milliseconds in this case",
    "start": "1170159",
    "end": "1178950"
  },
  {
    "text": "it's 2 and 1/2 milliseconds which is on the high side but at least you have some",
    "start": "1178950",
    "end": "1184950"
  },
  {
    "text": "visibility into your system and we can talk a little bit later about what we're trying to do about overheads of our",
    "start": "1184950",
    "end": "1190110"
  },
  {
    "text": "router all right so that's that's observability using st or service mesh",
    "start": "1190110",
    "end": "1197690"
  },
  {
    "text": "and I want to talk a little bit about having more traffic on functions so the",
    "start": "1197690",
    "end": "1203669"
  },
  {
    "text": "execution method that I showed you earlier is great for having really low latency for loading a function into a",
    "start": "1203669",
    "end": "1210600"
  },
  {
    "text": "part and being able to send it a request but it's not great for having for systems that have high throughput but",
    "start": "1210600",
    "end": "1218610"
  },
  {
    "text": "may or may not care about latency for those systems you need something like the kubernetes horizontal pod or a",
    "start": "1218610",
    "end": "1223950"
  },
  {
    "text": "scaler and I'm going to demo that in a in a video because the actual order",
    "start": "1223950",
    "end": "1230070"
  },
  {
    "text": "scalar takes a few minutes to actually to actually notice things so this is a",
    "start": "1230070",
    "end": "1235710"
  },
  {
    "text": "speeded up video of the autoscaler running so sufficient function the",
    "start": "1235710",
    "end": "1242309"
  },
  {
    "text": "efficient environment is created and we allow you to specify a min and Max CPU",
    "start": "1242309",
    "end": "1247980"
  },
  {
    "text": "and memory these are defaults for functions created in that environment the environment the function can override it so we're",
    "start": "1247980",
    "end": "1256150"
  },
  {
    "text": "creating a function and we define a min and Max scale we pointed at that",
    "start": "1256150",
    "end": "1263770"
  },
  {
    "text": "environment we define a min and Max scale of 1 & 6 and we specify that we",
    "start": "1263770",
    "end": "1269950"
  },
  {
    "text": "want to use a new separate deployment back end which will also create a",
    "start": "1269950",
    "end": "1275169"
  },
  {
    "text": "horizontal part or a scalar next we create an HTP trigger for that function",
    "start": "1275169",
    "end": "1280900"
  },
  {
    "text": "so we can generate some load and point at it we do a simple request on it to",
    "start": "1280900",
    "end": "1286419"
  },
  {
    "text": "make sure it works that's again a simple hello world function we've artificially",
    "start": "1286419",
    "end": "1292230"
  },
  {
    "text": "kept that we've artificially kept the the maximum CPU really low so that hello",
    "start": "1292230",
    "end": "1298539"
  },
  {
    "text": "world will actually order scale otherwise you would you would be fine with a very small number of instances",
    "start": "1298539",
    "end": "1305440"
  },
  {
    "text": "you can see that fission created a horizontal power order scalar for that function automatically and again we set",
    "start": "1305440",
    "end": "1312940"
  },
  {
    "text": "the target a bit low for this demo but you can set the target to something more realistic like 80 90 percent and it set",
    "start": "1312940",
    "end": "1320260"
  },
  {
    "text": "a min and Max scale that you had provided at the at the function level and currently there's just one replica",
    "start": "1320260",
    "end": "1325899"
  },
  {
    "text": "so again we start our load generator with a concurrency of 250 and we send it",
    "start": "1325899",
    "end": "1332890"
  },
  {
    "text": "to that same URL and what we're looking",
    "start": "1332890",
    "end": "1338470"
  },
  {
    "text": "for is that CPU to go up and the part or a scalar to catch up and create more replicas so it does so pretty quickly",
    "start": "1338470",
    "end": "1344710"
  },
  {
    "text": "again this video is sped up about 2x yeah 2x I think so",
    "start": "1344710",
    "end": "1350230"
  },
  {
    "text": "so you quickly reach three replicas the CPU usage is still pretty high once the",
    "start": "1350230",
    "end": "1356529"
  },
  {
    "text": "actual load generator finishes the CPU usage comes way back down and the part autoscaler notices that and it actually",
    "start": "1356529",
    "end": "1364510"
  },
  {
    "text": "has a in a scaled down time of several minutes which is why this is a video but",
    "start": "1364510",
    "end": "1371559"
  },
  {
    "text": "five or six minutes after your load stops the autoscaler brings your instances back down to to the minimum",
    "start": "1371559",
    "end": "1377799"
  },
  {
    "text": "scale that you had set up with the function so this is function auto scaling and there's work going on to",
    "start": "1377799",
    "end": "1383500"
  },
  {
    "text": "combine both these back ends so that you have low latency for the startup as well",
    "start": "1383500",
    "end": "1389110"
  },
  {
    "text": "as high throughput when you have a lot of load all right so that's auto-scaling",
    "start": "1389110",
    "end": "1399180"
  },
  {
    "text": "go back to our slides for a bit and I'll",
    "start": "1399180",
    "end": "1408400"
  },
  {
    "text": "talk a little bit about larger applications of that contain interacting",
    "start": "1408400",
    "end": "1413890"
  },
  {
    "text": "functions so there's a lot of different ways that you can have your functions interact you can just do plain old HTTP requests",
    "start": "1413890",
    "end": "1420610"
  },
  {
    "text": "from between functions and because of this do integration you'll have some measure of observability into those",
    "start": "1420610",
    "end": "1426700"
  },
  {
    "text": "you'll have some insight into into how those requests are going but it would be",
    "start": "1426700",
    "end": "1432160"
  },
  {
    "text": "pretty cool if that entire interaction between functions were abstracted away",
    "start": "1432160",
    "end": "1437320"
  },
  {
    "text": "in some way so we've created a system called ficient workflows where you can create some sort of flowchart in some",
    "start": "1437320",
    "end": "1444010"
  },
  {
    "text": "sense of functions and the workflow engine will will coordinate those",
    "start": "1444010",
    "end": "1449530"
  },
  {
    "text": "functions it will manage both data and controls flow and and you'll actually be able to to have functions talk to each",
    "start": "1449530",
    "end": "1456910"
  },
  {
    "text": "other without calling each other explicitly using a using a separate workflow that that operates on them so",
    "start": "1456910",
    "end": "1464440"
  },
  {
    "text": "I'm going to switch back to a demo talk",
    "start": "1464440",
    "end": "1469660"
  },
  {
    "text": "mostly demos see if I can find my okay",
    "start": "1469660",
    "end": "1476730"
  },
  {
    "text": "so again I'm going to show a really trivial workflow here we're going to run",
    "start": "1479890",
    "end": "1488220"
  },
  {
    "text": "the the UNIX fortune command as a function and it I don't know it it",
    "start": "1488220",
    "end": "1497679"
  },
  {
    "text": "outputs some sort of random quote and we have this function which outputs an",
    "start": "1497679",
    "end": "1505059"
  },
  {
    "text": "ASCII art cartoon whale that that contains a speech bubble containing whatever it whatever it's sent okay so",
    "start": "1505059",
    "end": "1513070"
  },
  {
    "text": "now we're going to create a workflow that combines these two functions without actually having the first",
    "start": "1513070",
    "end": "1519070"
  },
  {
    "text": "function call the other and we do that by defining a a workflow in gamal and",
    "start": "1519070",
    "end": "1526140"
  },
  {
    "text": "it's got tasks and each task is efficient function call and the first",
    "start": "1526140",
    "end": "1534790"
  },
  {
    "text": "one is the generate fortune task which cause fortune and the second one is the",
    "start": "1534790",
    "end": "1539799"
  },
  {
    "text": "whale with fortune so it calls whale say with the input of that first task so the",
    "start": "1539799",
    "end": "1546070"
  },
  {
    "text": "input here that's the data flow and there's a requires which makes sure that the generate runs before this task and",
    "start": "1546070",
    "end": "1553210"
  },
  {
    "text": "that's the control flow so you can define dependencies this way this is a really simple workflow with two tasks",
    "start": "1553210",
    "end": "1559330"
  },
  {
    "text": "but if you had more tasks you could you have implicit parallelism parallelism",
    "start": "1559330",
    "end": "1567090"
  },
  {
    "text": "and hopefully we should see a whale saying something silly eventually all",
    "start": "1567780",
    "end": "1577929"
  },
  {
    "text": "right the code is entirely run okay so the first function was run the",
    "start": "1577929",
    "end": "1584840"
  },
  {
    "text": "workflow engine interpreted the Yama and and allowed you to allow those functions",
    "start": "1584840",
    "end": "1591200"
  },
  {
    "text": "to just have the data sent from one function to another we don't have a whole lot of time to dive into how the",
    "start": "1591200",
    "end": "1597200"
  },
  {
    "text": "workflow engine works but essentially it uses a message queue to be to have",
    "start": "1597200",
    "end": "1602650"
  },
  {
    "text": "persistent events tracked as the workflow executes so as each task",
    "start": "1602650",
    "end": "1609200"
  },
  {
    "text": "finishes it's tracked in the message queue and that triggers the workflow engine again - to invoke the next",
    "start": "1609200",
    "end": "1616190"
  },
  {
    "text": "function and that's also how concurrency work so if you have a task that depends",
    "start": "1616190",
    "end": "1621980"
  },
  {
    "text": "on a bunch of different tasks and they don't have any dependencies between them then all those tasks could run in",
    "start": "1621980",
    "end": "1627890"
  },
  {
    "text": "parallel and again you would do this without explicitly having any kind of",
    "start": "1627890",
    "end": "1633610"
  },
  {
    "text": "explicit parallelism in the workflow okay so I've run through my demos and so",
    "start": "1633610",
    "end": "1646910"
  },
  {
    "text": "a little bit about the status of the project fushion core actually open sourced exactly a year ago at cube con it's",
    "start": "1646910",
    "end": "1655010"
  },
  {
    "text": "close to beta real soon now we should be releasing a fairly stable beta we're",
    "start": "1655010",
    "end": "1660290"
  },
  {
    "text": "going to focus on performance security scalability and so on and have a 1.0 around the middle of next year workflows",
    "start": "1660290",
    "end": "1667730"
  },
  {
    "text": "is a relatively early project it should have a beta mid to late next year",
    "start": "1667730",
    "end": "1672910"
  },
  {
    "text": "yeah and security scalability and performance are the focuses of the",
    "start": "1672910",
    "end": "1679070"
  },
  {
    "text": "project for the next few months at least for more on the roadmap and everything",
    "start": "1679070",
    "end": "1684380"
  },
  {
    "text": "else check out fishin Dario and github and talk to us on slack or Twitter and I",
    "start": "1684380",
    "end": "1690650"
  },
  {
    "text": "think we have a few minutes for questions anyone hey thanks",
    "start": "1690650",
    "end": "1697810"
  },
  {
    "text": "somebody to switch the song yeah it's all hi hi can you explain how imports",
    "start": "1697810",
    "end": "1709090"
  },
  {
    "text": "like requirements are baked into a function yes so I can open up an example",
    "start": "1709090",
    "end": "1718380"
  },
  {
    "text": "essentially the the declarative bail system that I showed you that the Builder is the one that can do imports",
    "start": "1718830",
    "end": "1725470"
  },
  {
    "text": "and requirements gathering as well so you can provide a build script with a function or you can use the built-in one",
    "start": "1725470",
    "end": "1731860"
  },
  {
    "text": "in the environment and when whenever the function is built it uses C let me just",
    "start": "1731860",
    "end": "1739120"
  },
  {
    "text": "use my editor it uses that sorry so the",
    "start": "1739120",
    "end": "1752110"
  },
  {
    "text": "stuff I demoed is it's actually in a pull request but you can have",
    "start": "1752110",
    "end": "1760049"
  },
  {
    "text": "I'm having a hard time finding it but let me tweet out that link later to",
    "start": "1763470",
    "end": "1768810"
  },
  {
    "text": "actually show you how that works but essentially you can have a spec of requirements using whatever is idiomatic",
    "start": "1768810",
    "end": "1774570"
  },
  {
    "text": "in the language of its Python there's a requirement start txt if it's good there's a either a glide file or the",
    "start": "1774570",
    "end": "1780540"
  },
  {
    "text": "various other dependency tools and you can write a script that that runs those and fish and then packages it and is in",
    "start": "1780540",
    "end": "1786930"
  },
  {
    "text": "charge of transporting that package to the function when it's supposed to Thanks what kind of message queue",
    "start": "1786930",
    "end": "1797760"
  },
  {
    "text": "because of we do not instead we do not deeply integrate with any message queue",
    "start": "1797760",
    "end": "1804980"
  },
  {
    "text": "the the picture I showed you earlier of triggers that's how we integrate with any message queue so there's a Nats",
    "start": "1804980",
    "end": "1812010"
  },
  {
    "text": "message queue trigger there's a Kafka trigger that's the super early right now",
    "start": "1812010",
    "end": "1817950"
  },
  {
    "text": "and folks from Microsoft have contributed and as your storage queue trigger as well so that's it for message",
    "start": "1817950",
    "end": "1824790"
  },
  {
    "text": "queues the workflow engine uses not streaming which is a durable version of Nats yes",
    "start": "1824790",
    "end": "1831920"
  },
  {
    "text": "where are you storing the build output yes so so kubernetes custom resources",
    "start": "1831920",
    "end": "1840660"
  },
  {
    "text": "have a size limit obviously so efficient actually installs is what we call a storage service it's a really thin",
    "start": "1840660",
    "end": "1847080"
  },
  {
    "text": "wrapper on top of either persistent volumes or it can also be configured to use things like s3 and you fetch that",
    "start": "1847080",
    "end": "1855050"
  },
  {
    "text": "when you run specialized but yes exactly right so the custom resource points do",
    "start": "1855050",
    "end": "1861810"
  },
  {
    "text": "that points to that object and verifies the integrity and so on so if I were to",
    "start": "1861810",
    "end": "1868940"
  },
  {
    "text": "if I were to run something like NPM dependencies through the Builder yes you",
    "start": "1868940",
    "end": "1874260"
  },
  {
    "text": "would fetch the entire node modules down when you were doing specialization no no so the build process is separate from",
    "start": "1874260",
    "end": "1882480"
  },
  {
    "text": "the running process right so it but if we were storing that in s3 yes it was a",
    "start": "1882480",
    "end": "1889770"
  },
  {
    "text": "keener you'd have yeah that's the whole moon box right there's some work going on on prefetching that that would effect",
    "start": "1889770",
    "end": "1896300"
  },
  {
    "text": "the cold start times especially these dependencies get really large yeah yeah",
    "start": "1896300",
    "end": "1901510"
  },
  {
    "text": "could I write a pot initialize room and efficient and if so how would I do that",
    "start": "1901510",
    "end": "1906650"
  },
  {
    "text": "yeah you're done what the new kubernetes initializes feature right um today right",
    "start": "1906650",
    "end": "1916370"
  },
  {
    "text": "now you'll have to create a the the best way to do it would be to create a trigger for four part for kubernetes",
    "start": "1916370",
    "end": "1923240"
  },
  {
    "text": "initializers and and then declare it you'd need a new type that says this is",
    "start": "1923240",
    "end": "1928610"
  },
  {
    "text": "an initializer and the implemented the fission implementation of that would watch the initial would be a custom",
    "start": "1928610",
    "end": "1935120"
  },
  {
    "text": "controller that watches that and executes the certain function when that",
    "start": "1935120",
    "end": "1941950"
  },
  {
    "text": "whenever the the API call specified in the initializer actually occurs so this",
    "start": "1942220",
    "end": "1947720"
  },
  {
    "text": "isn't implemented yet in fission but we want to have a way for you to do this without having to do all the work of",
    "start": "1947720",
    "end": "1953450"
  },
  {
    "text": "writing the controller we do have kubernetes watches so you can watch an arbitrary set of resources after they're",
    "start": "1953450",
    "end": "1959720"
  },
  {
    "text": "created so it's quite different from the initializer but if you're trying to do custom behavior on top of kubernetes",
    "start": "1959720",
    "end": "1964730"
  },
  {
    "text": "that fits some of the use cases so you could you could use fission to create",
    "start": "1964730",
    "end": "1970280"
  },
  {
    "text": "like a controller that washes for CRTs for example yes okay thank you",
    "start": "1970280",
    "end": "1977559"
  },
  {
    "text": "all right thank you very much [Applause]",
    "start": "1980660",
    "end": "1988079"
  }
]