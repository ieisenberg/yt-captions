[
  {
    "text": "okay I guess let's get started thanks everybody for coming to the second cementation deep dive in case you missed",
    "start": "0",
    "end": "7859"
  },
  {
    "text": "the intro it's recorded but just a very general overview of sig instrumentation",
    "start": "7859",
    "end": "13620"
  },
  {
    "text": "we're basically responsible or India in charge of the instrumentation pieces in",
    "start": "13620",
    "end": "20609"
  },
  {
    "text": "kubernetes that doesn't necessarily mean we own all the lines of code that do",
    "start": "20609",
    "end": "26119"
  },
  {
    "text": "metric or log or tracing instrumentation but it means that we provide the best",
    "start": "26119",
    "end": "31170"
  },
  {
    "text": "best practices and guidelines for the project for other SIG's to hopefully",
    "start": "31170",
    "end": "37290"
  },
  {
    "text": "comply to and frameworks for these things and yeah if you want to know more",
    "start": "37290",
    "end": "43110"
  },
  {
    "text": "of the basics of that check out the second fermentation intro we are at the deep dive here and actually max is going",
    "start": "43110",
    "end": "50219"
  },
  {
    "text": "to be talking about one of our sub projects so yeah I'm not actually part",
    "start": "50219",
    "end": "56309"
  },
  {
    "text": "of this session even though I'm kind of in the schedule but take it away max thank you all right cool",
    "start": "56309",
    "end": "69860"
  },
  {
    "text": "so after this introduction thank you I want to talk about optimizing metric",
    "start": "69860",
    "end": "76830"
  },
  {
    "text": "rendering in keep statement ryx which is one of the components of sec instrumentation and now we'll go a",
    "start": "76830",
    "end": "82049"
  },
  {
    "text": "little bit into what that is and what that means and so on mostly metrics rendering from the perspective about",
    "start": "82049",
    "end": "88560"
  },
  {
    "text": "like the Prometheus metric format and how we can do that in a very efficient way so how can we performance optimize",
    "start": "88560",
    "end": "95430"
  },
  {
    "text": "metric rendering so in the end string interpolation in a very efficient way",
    "start": "95430",
    "end": "101930"
  },
  {
    "text": "for myself and max and part of RedHat and part of the Prometheus upstream team",
    "start": "101930",
    "end": "109079"
  },
  {
    "text": "as well some one of the maintainer of a load manager for example but also I'm very connected to the Camaro's world so",
    "start": "109079",
    "end": "116250"
  },
  {
    "text": "whatever connects the two worlds and I'm working on the bunch yeah cool all right so we already saw",
    "start": "116250",
    "end": "124530"
  },
  {
    "text": "this light optimizing metric renewing of CubeSat metrics on that of that very",
    "start": "124530",
    "end": "129780"
  },
  {
    "text": "specific component in second cementation but you could also take a different perspective at this talk or look at it",
    "start": "129780",
    "end": "136980"
  },
  {
    "text": "differently maybe you're not very interested in cube state metrics for example or that particular component or",
    "start": "136980",
    "end": "143250"
  },
  {
    "text": "you're saying okay it performs optimized now like cool I don't really need to know about the internals but the",
    "start": "143250",
    "end": "149520"
  },
  {
    "text": "alternative perspective you can really have here is how can I performs optimize",
    "start": "149520",
    "end": "156000"
  },
  {
    "text": "an application with the help of my monitoring system on top of kubernetes so we'll do that with the example",
    "start": "156000",
    "end": "163800"
  },
  {
    "text": "Prometheus but that of course could always just be any other monitoring",
    "start": "163800",
    "end": "169260"
  },
  {
    "text": "system and what kind of metrics do we need to look out for and how can we do that in a very efficient and iterative",
    "start": "169260",
    "end": "175290"
  },
  {
    "text": "process right that is the other perspective but still I look at it at",
    "start": "175290",
    "end": "182190"
  },
  {
    "text": "this perspective of cube state metrics so whoever heard of CubeSat metrics",
    "start": "182190",
    "end": "189950"
  },
  {
    "text": "right we are very used to keep state metrics that is cool okay all right so",
    "start": "189950",
    "end": "197600"
  },
  {
    "text": "we have a problem that we want to solve with keep state metrics we have the API server on the one hand the API server or",
    "start": "197600",
    "end": "205019"
  },
  {
    "text": "kubernetes in general has a bunch of state that is mostly saved inside of EDD and we want introspection in that into",
    "start": "205019",
    "end": "213000"
  },
  {
    "text": "that state we want observability into that state and we don't only want snapshots of that state but we also want",
    "start": "213000",
    "end": "219810"
  },
  {
    "text": "that in history like in a time serious manner kubernetes very nice it gives us",
    "start": "219810",
    "end": "225690"
  },
  {
    "text": "great API on top but it can really only give us the current view and it also only gives that to us in in in the end",
    "start": "225690",
    "end": "234630"
  },
  {
    "text": "in a restful manner as kubernetes objects on the other hand we want to funnel all this into prometheus",
    "start": "234630",
    "end": "241010"
  },
  {
    "text": "Prometheus there's a time series database or any really any of your monitoring systems this expects a",
    "start": "241010",
    "end": "248370"
  },
  {
    "text": "certain format so in object-oriented programming this would really like just be the adapter pattern",
    "start": "248370",
    "end": "253850"
  },
  {
    "text": "and permit us this is called the exporter pattern so we need something in between the two to translate the tree",
    "start": "253850",
    "end": "259940"
  },
  {
    "text": "and that would be cube state metrics here's like a small scenario you have",
    "start": "259940",
    "end": "266510"
  },
  {
    "text": "the API server we use the watch API notify us that there's new pod we take",
    "start": "266510",
    "end": "272510"
  },
  {
    "text": "that part we cache it inside of memory on cube state metrics and then a",
    "start": "272510",
    "end": "279140"
  },
  {
    "text": "synchronously or not in line with that Prometheus comes along at some point tries to get all the metrics it's on",
    "start": "279140",
    "end": "286130"
  },
  {
    "text": "pull model so it does request cube state metrics iterates over its entire cache",
    "start": "286130",
    "end": "291470"
  },
  {
    "text": "takes every part looks at it generates the metrics out of those parts or contract maps or whatnot and then",
    "start": "291470",
    "end": "299150"
  },
  {
    "text": "returns to the metrics as and in the end it's a byte slice on golang but yeah",
    "start": "299150",
    "end": "305390"
  },
  {
    "text": "anything really cool this is the architecture of the old version the",
    "start": "305390",
    "end": "312560"
  },
  {
    "text": "overall idea stayed the same it's still the adapter between the two but the",
    "start": "312560",
    "end": "318560"
  },
  {
    "text": "internals changed quite a bit so I refer to the old version as the not so much it",
    "start": "318560",
    "end": "324740"
  },
  {
    "text": "performs optimized versions and with the recent performance optimizations that's the new version that we're going to talk",
    "start": "324740",
    "end": "330530"
  },
  {
    "text": "about cool a little bit more like what do we actually cache or what am I",
    "start": "330530",
    "end": "336830"
  },
  {
    "text": "talking about when I talk about commit Nets objects you all know the Yammer representation here for example you have",
    "start": "336830",
    "end": "343070"
  },
  {
    "text": "a pod all that info you would get from the rest of all API and on the bottom",
    "start": "343070",
    "end": "348290"
  },
  {
    "text": "you see a snippet of the metrics that we would generate out of this nag for example we have a coupon container info",
    "start": "348290",
    "end": "354140"
  },
  {
    "text": "metric which tells you the container name namespace and the pod and you",
    "start": "354140",
    "end": "359330"
  },
  {
    "text": "really really see the data matching like it's really like traversing one data into some other in general cube state",
    "start": "359330",
    "end": "366050"
  },
  {
    "text": "metrics doesn't do any heuristics on the data but tries to give you the raw data so that you can do all the calculations",
    "start": "366050",
    "end": "371900"
  },
  {
    "text": "and allows us inside your monitoring system that makes you most powerful on that and all right so we cache the",
    "start": "371900",
    "end": "380570"
  },
  {
    "text": "kubernetes object and we've returned prometheus metrics to Prometheus in the end",
    "start": "380570",
    "end": "386510"
  },
  {
    "text": "now all of this worked quite well actually I can go love it back one more time all the communication between API",
    "start": "386510",
    "end": "393830"
  },
  {
    "text": "server and chip state matrix was entirely happening via kubernetes client go and then all the communication",
    "start": "393830",
    "end": "399770"
  },
  {
    "text": "between cube state matrix and Prometheus was entirely happening with Prometheus client go so we promptly just had to",
    "start": "399770",
    "end": "405920"
  },
  {
    "text": "change the prefix and Oliver was just figured out for us now we had some",
    "start": "405920",
    "end": "412760"
  },
  {
    "text": "problems with this design it worked quite well over time but long term problems were on bigger clusters we're",
    "start": "412760",
    "end": "419510"
  },
  {
    "text": "talking about something like 50 megabytes of the metric output or higher so that's I don't know our benchmarking",
    "start": "419510",
    "end": "426260"
  },
  {
    "text": "clusters were at least 250 nodes constantly scaling workloads so that it",
    "start": "426260",
    "end": "432260"
  },
  {
    "text": "was a small cluster but we had customers on a lot bigger clusters so problems we",
    "start": "432260",
    "end": "437540"
  },
  {
    "text": "had were high response rates we're talking about 10 to 20 seconds about simply like Prometheus during the",
    "start": "437540",
    "end": "443870"
  },
  {
    "text": "request and then the metrics coming back that whole time span was 20 seconds on the one hand that sounds like something",
    "start": "443870",
    "end": "451010"
  },
  {
    "text": "that is not very nice performance optimized like we can probably get more out of this in addition like this in terms of",
    "start": "451010",
    "end": "457880"
  },
  {
    "text": "business in terms of monitoring data like having all your monitoring data 20 seconds late is a problem because you",
    "start": "457880",
    "end": "465260"
  },
  {
    "text": "might want to correlate that data with other systems and if some are 20 seconds off and some are not or some are 30",
    "start": "465260",
    "end": "472400"
  },
  {
    "text": "seconds off correlating the two is very difficult and that would be the power of",
    "start": "472400",
    "end": "477860"
  },
  {
    "text": "Prometheus or any monitoring system to correlate different data streams which we really want to do with keep state",
    "start": "477860",
    "end": "483530"
  },
  {
    "text": "metrics so we want to reduce that number by a lot in addition running this on",
    "start": "483530",
    "end": "489740"
  },
  {
    "text": "bigger clusters we had quite a lot of issues opened about like how much memory",
    "start": "489740",
    "end": "495650"
  },
  {
    "text": "does this actually use how much CPU should I give it so all of this planning is quite difficult in general we could",
    "start": "495650",
    "end": "501680"
  },
  {
    "text": "optimize it but also one goal was to stabilize for example CPU usage by a lot",
    "start": "501680",
    "end": "507140"
  },
  {
    "text": "so not have it be a very spiky workload but instead being quite flat out right",
    "start": "507140",
    "end": "514340"
  },
  {
    "text": "so that's the problem statement now this was all inside of the instrumentation so",
    "start": "514340",
    "end": "520219"
  },
  {
    "text": "we do this well of course we use our monitoring system to optimize our monitoring system so let's look at a",
    "start": "520220",
    "end": "526940"
  },
  {
    "text": "couple of metrics that we want to use to optimize all of this or the metrics that we optimize one optimize for so the",
    "start": "526940",
    "end": "533150"
  },
  {
    "text": "first thing is really squared duration seconds this is just a snippet least I",
    "start": "533150",
    "end": "538310"
  },
  {
    "text": "did later on after the performance tested things I didn't have the bigger cluster available anymore there's",
    "start": "538310",
    "end": "543800"
  },
  {
    "text": "actually a super small cluster was a huge amount of config maps not very optimized but the the graphs are pretty",
    "start": "543800",
    "end": "550490"
  },
  {
    "text": "much very similar you see very spiky return rates like how long does it take",
    "start": "550490",
    "end": "557900"
  },
  {
    "text": "for Prometheus to scrape and that is difficult to correlate when sometimes your workloads like your metrics coming",
    "start": "557900",
    "end": "565970"
  },
  {
    "text": "back at like 6 seconds and sometimes they come back at 3 seconds how do you correlate that with other metrics in",
    "start": "565970",
    "end": "573710"
  },
  {
    "text": "addition while we added why not also optimize a couple of other things to",
    "start": "573710",
    "end": "579110"
  },
  {
    "text": "make this easier to to run to operate so for example CPU usage was quite spiky",
    "start": "579110",
    "end": "585020"
  },
  {
    "text": "we'll go into why that is mostly because like we have to integrate our cache every time and generate all the metrics",
    "start": "585020",
    "end": "591730"
  },
  {
    "text": "so CPU load is very spiky maybe we can optimize for that and also in general",
    "start": "591730",
    "end": "598400"
  },
  {
    "text": "memory would be nice to optimize in the long run so these are the three metrics",
    "start": "598400",
    "end": "603650"
  },
  {
    "text": "pretty much that we want to look at along the way all right so we can dive",
    "start": "603650",
    "end": "609710"
  },
  {
    "text": "right into it my graphs are a bit off but anyways caching the first thing and",
    "start": "609710",
    "end": "615920"
  },
  {
    "text": "probably the most helpful thing that we were able to do is optimize the caching",
    "start": "615920",
    "end": "621650"
  },
  {
    "text": "labor layer on keep state metrics in general Cavanaugh's very nice as it gives us that client library and watch",
    "start": "621650",
    "end": "628040"
  },
  {
    "text": "API and we pretty much don't have to take care of cache invalidation which makes this project a lot easier in",
    "start": "628040",
    "end": "634820"
  },
  {
    "text": "general so this is the old overview you",
    "start": "634820",
    "end": "640430"
  },
  {
    "text": "remember this the API server notifies us of a new pod we cache that part",
    "start": "640430",
    "end": "645530"
  },
  {
    "text": "and whenever prometheus comes in and wants to get the metrics we iterate over our entire and then return the metrics so instead",
    "start": "645530",
    "end": "655900"
  },
  {
    "text": "how about we get a new pod we generate the metrics for that pod and we will",
    "start": "655900",
    "end": "662120"
  },
  {
    "text": "catch the metrics and then whenever Prometheus comes along and once all the metrics well were to return the metrics",
    "start": "662120",
    "end": "668240"
  },
  {
    "text": "that we really have in cash here that is a lot more optimized that is in the best",
    "start": "668240",
    "end": "673790"
  },
  {
    "text": "case like taking this memory chunk and simply shoving it into the network card or the network interface that's the best",
    "start": "673790",
    "end": "681230"
  },
  {
    "text": "case of course that's not what's in the end happening but very close to all",
    "start": "681230",
    "end": "687770"
  },
  {
    "text": "right so this is all under the premise that probably more our kubernetes",
    "start": "687770",
    "end": "695240"
  },
  {
    "text": "objects are going to change then we get scrapes in so that kubernetes objects",
    "start": "695240",
    "end": "700400"
  },
  {
    "text": "actually changed more often and but a lot of them stays the same while prometheus regularly scrapes us",
    "start": "700400",
    "end": "709210"
  },
  {
    "text": "cool um make this more specific previously we were keeping the",
    "start": "709210",
    "end": "715010"
  },
  {
    "text": "kubernetes object in cache now we keep the Prometheus object in cache now how",
    "start": "715010",
    "end": "720800"
  },
  {
    "text": "much did this actually help this is on two hundred fifty notes cluster right now a couple I think we even in the end",
    "start": "720800",
    "end": "729110"
  },
  {
    "text": "like it only matters how many objects you have on cube state metrics doesn't really matter like how many workloads or",
    "start": "729110",
    "end": "735140"
  },
  {
    "text": "how powerful those workloads are has it simply like for enduring text so this is I think I think it and parts per node so",
    "start": "735140",
    "end": "743900"
  },
  {
    "text": "quite a lot of traffic for cube state measures itself and on the top you see the the red one is the old version you",
    "start": "743900",
    "end": "750080"
  },
  {
    "text": "see it quite spiky it alternates between two x's and one a half and then on the",
    "start": "750080",
    "end": "755150"
  },
  {
    "text": "bottom you see the new version super stable yeah keeping the same scrape",
    "start": "755150",
    "end": "761210"
  },
  {
    "text": "durations pretty much all the long all",
    "start": "761210",
    "end": "767240"
  },
  {
    "text": "of this cash logic I already said like kubernetes clang go is super helpful on",
    "start": "767240",
    "end": "772580"
  },
  {
    "text": "that it gives the idea or the high-level idea of Informer's underneath that you",
    "start": "772580",
    "end": "778280"
  },
  {
    "text": "have reflectors reflectors in the end like let you watch a resource",
    "start": "778280",
    "end": "783640"
  },
  {
    "text": "and update a store that you can define and you can implement this interface that you see on the red side and instead",
    "start": "783640",
    "end": "791650"
  },
  {
    "text": "of using the the normal store that would simply like cash given as objects we implemented our own store and that uses",
    "start": "791650",
    "end": "798310"
  },
  {
    "text": "simply like metrics as in the cache and we're being notified of new objects we",
    "start": "798310",
    "end": "804820"
  },
  {
    "text": "transfer them into metrics and then stage those in our own cache all the",
    "start": "804820",
    "end": "810370"
  },
  {
    "text": "like cash and validation notifications are coming from the reflectors so not a lot of work on our side here cool from",
    "start": "810370",
    "end": "821050"
  },
  {
    "text": "cashing on we can dive into compression",
    "start": "821050",
    "end": "826080"
  },
  {
    "text": "on the top here you have a snippet of the Prometheus metric exposition format and this is simply one one single metric",
    "start": "827040",
    "end": "834760"
  },
  {
    "text": "and you see here keeps each secret type like that is super repetitive and as an",
    "start": "834760",
    "end": "841420"
  },
  {
    "text": "engineer you would say like okay let's take compression and we save more than half of our network bandwidth in total",
    "start": "841420",
    "end": "849360"
  },
  {
    "text": "in general like Prometheus moved off from your PC format into a text format",
    "start": "849360",
    "end": "855160"
  },
  {
    "text": "with version 2 so hopefully we can still",
    "start": "855160",
    "end": "860350"
  },
  {
    "text": "optimize on that the previous cube state metric version would gzip all of this by",
    "start": "860350",
    "end": "865630"
  },
  {
    "text": "default as the Prometheus client library does that by default but we wanted to",
    "start": "865630",
    "end": "870640"
  },
  {
    "text": "play around with it a little bit and also add compression into the new version of chip state metrics so",
    "start": "870640",
    "end": "877030"
  },
  {
    "text": "different options were gzipping standard golang gzipping and york times gzipping new york times is they wrote gzip",
    "start": "877030",
    "end": "885280"
  },
  {
    "text": "handler library in the end which sounded quite promising so we wanted to try them",
    "start": "885280",
    "end": "891010"
  },
  {
    "text": "all out we haven't fully optimized for them so for sure we could have done a better job like tuning every single",
    "start": "891010",
    "end": "897970"
  },
  {
    "text": "thing on the compression options alright so just to be clear like we we compress",
    "start": "897970",
    "end": "906580"
  },
  {
    "text": "the metrics output and then send it over the network to Prometheus that's the that's the part we compress so on the",
    "start": "906580",
    "end": "914860"
  },
  {
    "text": "very top we are right now looking at CPU usage on the very top you see the old version",
    "start": "914860",
    "end": "921370"
  },
  {
    "text": "again we'll go a little bit into why this is so repetitive you'll see that",
    "start": "921370",
    "end": "926380"
  },
  {
    "text": "once we see the memory graph but underneath you see two green versions",
    "start": "926380",
    "end": "932650"
  },
  {
    "text": "like the the green and then on the very bottom a blue line there are two greens are the optimized cube state matrix",
    "start": "932650",
    "end": "941350"
  },
  {
    "text": "version which gzipping and well by surprise like it does use more CPU than",
    "start": "941350",
    "end": "947950"
  },
  {
    "text": "the lower version of course the low ones and doesn't do gzipping and yes compression takes to be you but",
    "start": "947950",
    "end": "954100"
  },
  {
    "text": "hopefully later on we'll see that our network is gonna be way faster let's",
    "start": "954100",
    "end": "960340"
  },
  {
    "text": "look in addition at memory we had quite a big cluster so the old cube state",
    "start": "960340",
    "end": "966100"
  },
  {
    "text": "matrix version were just simply oom because it would well take away too much memory you see where it's ramping up and",
    "start": "966100",
    "end": "972790"
  },
  {
    "text": "I'm just being killed that actually explains the CPU of the red line in the",
    "start": "972790",
    "end": "980020"
  },
  {
    "text": "previous graph but also now you see memory use it on the gzipping ones being",
    "start": "980020",
    "end": "987460"
  },
  {
    "text": "slightly higher than noticeably in general being very stable but you see",
    "start": "987460",
    "end": "992530"
  },
  {
    "text": "them being a lot higher and here need to explain a bit why does gzipping take",
    "start": "992530",
    "end": "997720"
  },
  {
    "text": "more memories and no gzipping well we can't really keep the gzip version in",
    "start": "997720",
    "end": "1002910"
  },
  {
    "text": "memory as we have our cache like structured as covenants objects for",
    "start": "1002910",
    "end": "1009000"
  },
  {
    "text": "example if a new pod or an updated pod comes in we want to generate the metrics",
    "start": "1009000",
    "end": "1014180"
  },
  {
    "text": "only for that pod and then change the the cache value for that pod and not for",
    "start": "1014180",
    "end": "1020370"
  },
  {
    "text": "all of it if we would change it for all of it we would generate all the metrics every single time over the entire cache",
    "start": "1020370",
    "end": "1025650"
  },
  {
    "text": "and that would be super expensive so what we actually need is we need access to like metrics of a single object that",
    "start": "1025650",
    "end": "1032069"
  },
  {
    "text": "way we can't keep a compressed version in cache but we need to compress every single time so that way you used we use",
    "start": "1032069",
    "end": "1039660"
  },
  {
    "text": "on cube state metrics more memory on the gzip a1 but still like this could all",
    "start": "1039660",
    "end": "1045870"
  },
  {
    "text": "still be perfect for for networking so let's look at those graphs in the end like scrape",
    "start": "1045870",
    "end": "1050980"
  },
  {
    "text": "durations is like the total like the network how long does it take a network and how long does it use on both sides",
    "start": "1050980",
    "end": "1056680"
  },
  {
    "text": "on in terms of on our CPU and again you see the red line like old cube set mixed",
    "start": "1056680",
    "end": "1062980"
  },
  {
    "text": "version being um killed it's taking zero seconds to scrape well because it's simply not there then you see on the",
    "start": "1062980",
    "end": "1069820"
  },
  {
    "text": "very top the two green lines and then the balloon line the green lines again are a gzipping enabled and the blue",
    "start": "1069820",
    "end": "1076300"
  },
  {
    "text": "lines are no gzip here you can see like while gzipping might have helped us in",
    "start": "1076300",
    "end": "1082540"
  },
  {
    "text": "terms of the network but it really didn't help us in terms of CPU time so here like the blue line no gzipping",
    "start": "1082540",
    "end": "1090490"
  },
  {
    "text": "really like sticks out as being the most performant one and that that was also",
    "start": "1090490",
    "end": "1096429"
  },
  {
    "text": "our conclusion in the end well first of all like with gzipping we increase our",
    "start": "1096429",
    "end": "1102580"
  },
  {
    "text": "size internally like memory our memory footprint then also the saved",
    "start": "1102580",
    "end": "1109140"
  },
  {
    "text": "improvement on the network isn't really not worth the extra CPU cycles that we waste on the compression itself",
    "start": "1109140",
    "end": "1115540"
  },
  {
    "text": "especially as we can't better compress i'll go a little bit into detail how we",
    "start": "1115540",
    "end": "1120550"
  },
  {
    "text": "could improve that in the future and yeah we have higher CPU utilization so",
    "start": "1120550",
    "end": "1126490"
  },
  {
    "text": "in the end what do you need to keep in mind cube state Metro is probably very close to your prometheus so that way",
    "start": "1126490",
    "end": "1131710"
  },
  {
    "text": "you're probably on a very high-performing network so probably that doesn't matter and now maybe you have a",
    "start": "1131710",
    "end": "1137290"
  },
  {
    "text": "lot of network congestion so you might want to enable this this is an optional now in cube state metrics so maybe you",
    "start": "1137290",
    "end": "1142870"
  },
  {
    "text": "want to go into that maybe not if you have more feedback on that on bigger clusters I would be very curious",
    "start": "1142870",
    "end": "1150029"
  },
  {
    "text": "now next to the caching and compression we are in a very specific use case this",
    "start": "1151520",
    "end": "1157250"
  },
  {
    "text": "actually happened on Prometheus upstream this is done by yarn from the upstream team what we can do what is quite",
    "start": "1157250",
    "end": "1165409"
  },
  {
    "text": "expensive is converting floats to strings of course not if you do one but if you do so many that you end up with",
    "start": "1165409",
    "end": "1172909"
  },
  {
    "text": "fifty megabytes of metrics you definitely want some optimizations here and what we can do is we probably have",
    "start": "1172909",
    "end": "1178760"
  },
  {
    "text": "default values like we probably have a lot of zeros and our metrics a lot of ones and our metrics so what we really",
    "start": "1178760",
    "end": "1185539"
  },
  {
    "text": "do is like check for those default values and render the strings about easier that makes it a lot faster in",
    "start": "1185539",
    "end": "1192740"
  },
  {
    "text": "terms of metric rendering then in addition I'll dive more into that in a",
    "start": "1192740",
    "end": "1199429"
  },
  {
    "text": "bit but what is also very important if you want to be very specific about how you",
    "start": "1199429",
    "end": "1204529"
  },
  {
    "text": "render your strings in golang you might want to look into strings builder it doesn't give you so many options like",
    "start": "1204529",
    "end": "1210860"
  },
  {
    "text": "for example the FMT package but it is more a lot more efficient in the end",
    "start": "1210860",
    "end": "1217399"
  },
  {
    "text": "like strings and golang are immutable that way every time you change a string you need to reallocate that memory",
    "start": "1217399",
    "end": "1223929"
  },
  {
    "text": "instead what we can do is we can use byte slices we can mute and that we can",
    "start": "1223929",
    "end": "1230179"
  },
  {
    "text": "address a lot better so strings builder simply leverages on that point which got",
    "start": "1230179",
    "end": "1235700"
  },
  {
    "text": "us quite far all right so comparing all that and a couple more versions a lot a",
    "start": "1235700",
    "end": "1242149"
  },
  {
    "text": "couple more stats here on the top you see the old version you have a scrape",
    "start": "1242149",
    "end": "1247370"
  },
  {
    "text": "duration of 22 seconds that was quite a bit of a problem we didn't get it all the way down to like insanity below one",
    "start": "1247370",
    "end": "1255320"
  },
  {
    "text": "second but a lot better for sure CPUs it is one that is just normalized",
    "start": "1255320",
    "end": "1260779"
  },
  {
    "text": "compared to the others with 2.5 gigabytes memory as a peak and then the",
    "start": "1260779",
    "end": "1266059"
  },
  {
    "text": "amount of percentage that we spent in garbage collection all of this was tested on a production cluster actually",
    "start": "1266059",
    "end": "1272390"
  },
  {
    "text": "by Bureau net SoundCloud which we are super thankful for like to have some production workload on this then the",
    "start": "1272390",
    "end": "1278840"
  },
  {
    "text": "next two versions optimize on the metric rendering itself not on the caching logic and here you",
    "start": "1278840",
    "end": "1285210"
  },
  {
    "text": "see for sure and improvement on scrape durations but still we generate all the metrics every single time Prometheus",
    "start": "1285210",
    "end": "1291870"
  },
  {
    "text": "grapes so it's not a huge improvement CPU usage got a lot better that's mostly",
    "start": "1291870",
    "end": "1296880"
  },
  {
    "text": "the use of strings builder and defaulting on float conversion and then also memory we could reduce s yeah on",
    "start": "1296880",
    "end": "1306840"
  },
  {
    "text": "the float reusing buffers one and then also the loot conversion got a lot more",
    "start": "1306840",
    "end": "1312510"
  },
  {
    "text": "efficient of course we spend less time in garbage collection this reduced memory allocation but a lot so that was",
    "start": "1312510",
    "end": "1319170"
  },
  {
    "text": "also very useful on the bottom two you see the caching optimizations here",
    "start": "1319170",
    "end": "1325200"
  },
  {
    "text": "there's actually the biggest improvement as I said earlier for our specific use case CPU usage dropped a huge amount",
    "start": "1325200",
    "end": "1333750"
  },
  {
    "text": "because simply we don't need to regenerate all the metrics every single time and also like memory dropped a lot",
    "start": "1333750",
    "end": "1341160"
  },
  {
    "text": "as like caching metrics is actually a lot more efficient and caching kubernetes objects in our use case as",
    "start": "1341160",
    "end": "1347010"
  },
  {
    "text": "we're throwing away most of the data anyways and again garbage collection hey of",
    "start": "1347010",
    "end": "1352020"
  },
  {
    "text": "course we're not throwing away all the metrics so garbage collector is really not that tough and yeah we save a lot of",
    "start": "1352020",
    "end": "1360570"
  },
  {
    "text": "CPU cycles through that as you saw earlier in the graphs right all this",
    "start": "1360570",
    "end": "1366360"
  },
  {
    "text": "resulted in version 1.5 that was a released I think this year maybe it January or something um got good",
    "start": "1366360",
    "end": "1374070"
  },
  {
    "text": "feedback pretty much no bug reports which I'm bit surprised but anyways that",
    "start": "1374070",
    "end": "1379590"
  },
  {
    "text": "worked quite well 1.5 now for the next one this is a",
    "start": "1379590",
    "end": "1385440"
  },
  {
    "text": "community project so we got a huge amount of new metrics coming in all the time so a lot of stuff piled in further",
    "start": "1385440",
    "end": "1394680"
  },
  {
    "text": "1.6 so we prepared everything for that and I did a bunch of regression tests",
    "start": "1394680",
    "end": "1399720"
  },
  {
    "text": "again just to make sure we don't like mess up all the work that we previously did um",
    "start": "1399720",
    "end": "1406180"
  },
  {
    "text": "and what I did is do a lot of profiling on this thing on again a test cluster",
    "start": "1406180",
    "end": "1411520"
  },
  {
    "text": "with varying workloads which was quite interesting keep state measures has a remote profiling endpoint so this is",
    "start": "1411520",
    "end": "1417820"
  },
  {
    "text": "super easy and go to profile your applications and wants to dive into a",
    "start": "1417820",
    "end": "1423160"
  },
  {
    "text": "couple of things that like stuck out for me and that might be interesting for you as well once you optimize some go on kubernetes",
    "start": "1423160",
    "end": "1430980"
  },
  {
    "text": "one is my profiles were like runtime",
    "start": "1430980",
    "end": "1436570"
  },
  {
    "text": "your slice was all over the place and I was a little bit like okay bro slicing",
    "start": "1436570",
    "end": "1441940"
  },
  {
    "text": "maybe we can optimize for this and I looked into runtime growth slice",
    "start": "1441940",
    "end": "1447550"
  },
  {
    "text": "there's simply the upstream like goal line core function what in the ended us",
    "start": "1447550",
    "end": "1454540"
  },
  {
    "text": "it grows the capacity of the slice for the people that don't write go very much",
    "start": "1454540",
    "end": "1459550"
  },
  {
    "text": "like a slices is not an array arrays are fixed types at compile time where slices",
    "start": "1459550",
    "end": "1466360"
  },
  {
    "text": "are variable sized at runtime so you can grow them in the end slices do use",
    "start": "1466360",
    "end": "1471460"
  },
  {
    "text": "arrays underneath but that's like an implementation detail okay so I thought",
    "start": "1471460",
    "end": "1477790"
  },
  {
    "text": "hmm we're spending quite a lot of CPU cycles in this go slice why are we doing this can't we like free allocate more",
    "start": "1477790",
    "end": "1483730"
  },
  {
    "text": "memory in advance can't we be more efficient on that and what you see a lot like we iterate through parts and we",
    "start": "1483730",
    "end": "1490390"
  },
  {
    "text": "iterate through the properties of parts or config Maps or ingressive and so on and we really like at the beginning we",
    "start": "1490390",
    "end": "1497380"
  },
  {
    "text": "allocate like an empty slice of metrics and I thought okay I'll be really smart",
    "start": "1497380",
    "end": "1502900"
  },
  {
    "text": "and well allocate a certain length pre allocate all of it do the allocation at",
    "start": "1502900",
    "end": "1508780"
  },
  {
    "text": "the beginning and then iterate and simply insert it into that pre-allocated slice we can't go with erase because we",
    "start": "1508780",
    "end": "1514960"
  },
  {
    "text": "don't know the size at the beginning that would be super nice maybe that then could even stay on the stack instead of",
    "start": "1514960",
    "end": "1520510"
  },
  {
    "text": "the heap but as at least like this is a good start",
    "start": "1520510",
    "end": "1525990"
  },
  {
    "text": "anyways testing all this you can ignore the blue line on the bottom for now you",
    "start": "1525990",
    "end": "1531700"
  },
  {
    "text": "see the red one and the green ones those is on the one hand the optimized version on the other hand are not optimized",
    "start": "1531700",
    "end": "1536830"
  },
  {
    "text": "version so pretty much this didn't help a lot I'd I've to live a deeper into the",
    "start": "1536830",
    "end": "1543280"
  },
  {
    "text": "language itself and what you can really see here is like gross slice actually",
    "start": "1543280",
    "end": "1549770"
  },
  {
    "text": "grows by doubling every single time pretty much there's a little bit more heuristics around it but actually what",
    "start": "1549770",
    "end": "1556370"
  },
  {
    "text": "we do is we don't grow slices so I'm often so this grows put exponentially",
    "start": "1556370",
    "end": "1561799"
  },
  {
    "text": "and simply like it doesn't happen that often so it's fine and that way my",
    "start": "1561799",
    "end": "1567110"
  },
  {
    "text": "optimization didn't really help a lot this is good to verify on on your clusters whenever like you can just run",
    "start": "1567110",
    "end": "1574159"
  },
  {
    "text": "the versions next to each other now I do want to go into the blue line on the bottom after some more profiling I saw",
    "start": "1574159",
    "end": "1583789"
  },
  {
    "text": "strings to slice bytes stick out quite a bit we wasted a lot of CPU cycle cycles on this and first",
    "start": "1583789",
    "end": "1593809"
  },
  {
    "text": "thought for me was like bite slices come on those are strings like that's that's just on the type system level like how",
    "start": "1593809",
    "end": "1600620"
  },
  {
    "text": "does that differ in any ways and as I mentioned earlier well strings are immutable bite slices or not to keep up",
    "start": "1600620",
    "end": "1608030"
  },
  {
    "text": "that guarantee imagine you have a string and you have multiple references to that string and now that one reference changes that",
    "start": "1608030",
    "end": "1614929"
  },
  {
    "text": "string or casts it into bite slice that way it has a mutable reference to it it",
    "start": "1614929",
    "end": "1620210"
  },
  {
    "text": "could mutated by slice and thus the other reference sees a string which is",
    "start": "1620210",
    "end": "1625580"
  },
  {
    "text": "being mutated which is quite a problem right so in order to keep up that",
    "start": "1625580",
    "end": "1631340"
  },
  {
    "text": "guarantee of strings being immutable golang actually takes the entire memory",
    "start": "1631340",
    "end": "1636400"
  },
  {
    "text": "memory section and copies it so that was super heavy on us and they'll actually",
    "start": "1636400",
    "end": "1642860"
  },
  {
    "text": "show where that happened we were simply like stupidly caching the metrics as",
    "start": "1642860",
    "end": "1648860"
  },
  {
    "text": "strings and then whenever Prometheus came in while the HTTP library the Cenacle HTTP library one byte slices and",
    "start": "1648860",
    "end": "1656900"
  },
  {
    "text": "we had to compare convert the strings to byte slices which is a bummer and then",
    "start": "1656900",
    "end": "1662450"
  },
  {
    "text": "lost quite a bit on that so what we did instead cash it at by slices as that is",
    "start": "1662450",
    "end": "1668600"
  },
  {
    "text": "not the hot path and then whenever Prometheus comes in we can like simply run out those bite",
    "start": "1668600",
    "end": "1674660"
  },
  {
    "text": "slices right away now this is actually the blue line at the bottom we we are",
    "start": "1674660",
    "end": "1680450"
  },
  {
    "text": "looking at the CPU the CPU consumption here and that saved us a lot for the 1.6",
    "start": "1680450",
    "end": "1687230"
  },
  {
    "text": "version all right all of this resulted in the 1.6 version we released that one",
    "start": "1687230",
    "end": "1693070"
  },
  {
    "text": "yeah and that is the current version you can check it out all open source all day yeah it's all",
    "start": "1693070",
    "end": "1701540"
  },
  {
    "text": "pretty much like goes down from one poor request so you can click through all of",
    "start": "1701540",
    "end": "1706640"
  },
  {
    "text": "it and yeah I wanted I've loved it into the future of cube state metric maybe",
    "start": "1706640",
    "end": "1712340"
  },
  {
    "text": "what we can do from a performance side and definitely this is also slide where I like this is just probably over",
    "start": "1712340",
    "end": "1719900"
  },
  {
    "text": "engineering on the lower parts of this but I'll dive into it anyways first of all like Fredrik in the front here is",
    "start": "1719900",
    "end": "1726380"
  },
  {
    "text": "working on sharding keep state metrics don't get me wrong that is not that's not the Olfa optimization this is",
    "start": "1726380",
    "end": "1732380"
  },
  {
    "text": "actually super useful on bigger clusters what you can do nowadays already you can chart based on kerbin edges types but if",
    "start": "1732380",
    "end": "1738770"
  },
  {
    "text": "you have a lot of parts and just one config map like sharding on that it's not super useful so what we what",
    "start": "1738770",
    "end": "1745010"
  },
  {
    "text": "Frederick instead developed is you can char it based on kubernetes object IDs and then you can distribute your shorts",
    "start": "1745010",
    "end": "1753110"
  },
  {
    "text": "or you can load balance on your charts a lot easier then in a in addition jimmy",
    "start": "1753110",
    "end": "1760400"
  },
  {
    "text": "pointed me to heap escapes to look more",
    "start": "1760400",
    "end": "1765410"
  },
  {
    "text": "into like when do stuff the stuff in cube state metrics escape to the heap when can we not stack allocate stack of",
    "start": "1765410",
    "end": "1772190"
  },
  {
    "text": "the location should be a lot more efficient and especially be a lot more efficient on the garbage collector so",
    "start": "1772190",
    "end": "1777650"
  },
  {
    "text": "there's not some nice tooling on go laying around that so I'll dive into that then also we could go further into",
    "start": "1777650",
    "end": "1785780"
  },
  {
    "text": "memory alignment on structs there's nice tooling or on that at least in the hot path that might save us a lot as we go",
    "start": "1785780",
    "end": "1792350"
  },
  {
    "text": "like allocates memory in buckets so like small performs optimizations on",
    "start": "1792350",
    "end": "1799400"
  },
  {
    "text": "the structs could push it into lower buckets which should a lot now this dot-dot-dot is they",
    "start": "1799400",
    "end": "1805700"
  },
  {
    "text": "saying like okay this is enough and then I think like the over-engineering would",
    "start": "1805700",
    "end": "1811820"
  },
  {
    "text": "start with like exploring commutativity on gzipping maybe someone has experience",
    "start": "1811820",
    "end": "1818120"
  },
  {
    "text": "so I would very much appreciate your thoughts but what we could do is we could in the cash keep the gzip versions",
    "start": "1818120",
    "end": "1825740"
  },
  {
    "text": "and somehow like make use of community activity of gzip so that it's still",
    "start": "1825740",
    "end": "1831530"
  },
  {
    "text": "performant even though we only G's the part of it but yeah that might be a bit tiny bit over-engineered yeah that is",
    "start": "1831530",
    "end": "1841010"
  },
  {
    "text": "the future of all of those in general maybe take away is huge date mixtures a",
    "start": "1841010",
    "end": "1846380"
  },
  {
    "text": "lot more demise nowadays so we had a lot like from upstream Prometheus but also",
    "start": "1846380",
    "end": "1851930"
  },
  {
    "text": "inside the Kuban as community on this but also when you look at it from a",
    "start": "1851930",
    "end": "1857030"
  },
  {
    "text": "different perspective as I said like s the alternative title from the talk takeaway is like first of all monitor",
    "start": "1857030",
    "end": "1864050"
  },
  {
    "text": "your cluster and application we have to say that a sec instrumentation here but also use your monitoring of course for",
    "start": "1864050",
    "end": "1871280"
  },
  {
    "text": "alerting but also use it for performance analysis and make this available to your",
    "start": "1871280",
    "end": "1876710"
  },
  {
    "text": "developers so they can debug regressions they can run their their software in",
    "start": "1876710",
    "end": "1882860"
  },
  {
    "text": "different versions and can compare it over long terms this was super helpful for me sorry and then last as I showed",
    "start": "1882860",
    "end": "1891020"
  },
  {
    "text": "you like not every optimization is very helpful and sometimes the gold memory",
    "start": "1891020",
    "end": "1896720"
  },
  {
    "text": "model documentation actually gives a very good tip on this like if you must read on on-the-go memory model so if you",
    "start": "1896720",
    "end": "1904370"
  },
  {
    "text": "have to read this document to understand your code and understand the program of your program you're being too clever and",
    "start": "1904370",
    "end": "1911450"
  },
  {
    "text": "don't be too clever yeah that's it thank you very much for you",
    "start": "1911450",
    "end": "1916530"
  },
  {
    "text": "[Applause] I think we have three minutes if there",
    "start": "1916530",
    "end": "1930120"
  },
  {
    "text": "are questions otherwise I'll stick around as well yeah thanks for the duck it's really good",
    "start": "1930120",
    "end": "1935550"
  },
  {
    "text": "my question is have you ever used go benchmarking tools because I can see you validate that on the way up reviews",
    "start": "1935550",
    "end": "1942660"
  },
  {
    "text": "which is fair and like its ultimate acceptance which is good but the problem is when you don't have this you know",
    "start": "1942660",
    "end": "1948950"
  },
  {
    "text": "even load that would you know kind of show test the thing and did you find the",
    "start": "1948950",
    "end": "1955080"
  },
  {
    "text": "gobiins useful did you use it so what we do in cube state measures actually at",
    "start": "1955080",
    "end": "1960720"
  },
  {
    "text": "the very beginning we made everything that cubes eight metrics talks to abstract it so we build stubs for",
    "start": "1960720",
    "end": "1966810"
  },
  {
    "text": "everything and so what we can do is test cube state metrics with the gold benchmark tool entirely with anything",
    "start": "1966810",
    "end": "1974070"
  },
  {
    "text": "around it and we did that both on like smaller scopes or unit testing as well",
    "start": "1974070",
    "end": "1979440"
  },
  {
    "text": "as integration or kind of into in testing in that version all right that's that's called an integration testing so",
    "start": "1979440",
    "end": "1985890"
  },
  {
    "text": "what we have now in our CI actually is as our CI automatically checks out your",
    "start": "1985890",
    "end": "1991440"
  },
  {
    "text": "patch and autom also checks our master runs the benchmarks overall both of them",
    "start": "1991440",
    "end": "1996930"
  },
  {
    "text": "and then there's a really cool tool on this tennard go that compares the two and then prints it out so every single",
    "start": "1996930",
    "end": "2003860"
  },
  {
    "text": "PR we can like check does this introduce any regressions we do use it on the CI",
    "start": "2003860",
    "end": "2009290"
  },
  {
    "text": "a-- yes actually all of this sorry I",
    "start": "2009290",
    "end": "2015590"
  },
  {
    "text": "should have mentioned this most of this stuff on committees upstream beyond was",
    "start": "2015590",
    "end": "2020630"
  },
  {
    "text": "mostly using it and that way I was like yes I need I need that as well good",
    "start": "2020630",
    "end": "2027340"
  },
  {
    "text": "thank you great great arc I'm wondering if you are still using the primitives",
    "start": "2030860",
    "end": "2036419"
  },
  {
    "text": "client go long the library to expose me to matrix we don't that was quite that",
    "start": "2036419",
    "end": "2046110"
  },
  {
    "text": "is definitely a big risk that chip state metrics went into like going away to a more performant version of the client",
    "start": "2046110",
    "end": "2053010"
  },
  {
    "text": "library for this use case like I'm not saying like the Prometheus plan library is not optimized it is way more optimized but this is specific for a use",
    "start": "2053010",
    "end": "2060419"
  },
  {
    "text": "case so this is a different some in some terms a different code but it tries to reuse most of the concepts that upstream",
    "start": "2060419",
    "end": "2067349"
  },
  {
    "text": "does but yes we're in we are a fork now in terms of Prometheus compatibility yes",
    "start": "2067349",
    "end": "2073500"
  },
  {
    "text": "this is this is not a general-purpose library so this is very much specific to",
    "start": "2073500",
    "end": "2079560"
  },
  {
    "text": "the coop statement Rick's use case other questions",
    "start": "2079560",
    "end": "2086060"
  },
  {
    "text": "alright cool thank you very much [Applause]",
    "start": "2086990",
    "end": "2092459"
  }
]