[
  {
    "text": "hello everyone my name is Jan cing I'm a software engineer at graphon Labs um and",
    "start": "320",
    "end": "6640"
  },
  {
    "text": "with me today here I have Alexander Magno hi everyone thank you for joining",
    "start": "6640",
    "end": "12040"
  },
  {
    "text": "with us in this session where we will share some experience about the open t",
    "start": "12040",
    "end": "17160"
  },
  {
    "text": "collector collector especially in tail sampling",
    "start": "17160",
    "end": "23199"
  },
  {
    "text": "feature wonderful so um we're covering here today A few usage um a few use",
    "start": "23599",
    "end": "30279"
  },
  {
    "text": "cases of the tail sampling uh processor for open Teter collector and um how",
    "start": "30279",
    "end": "36520"
  },
  {
    "text": "Pismo um use that to reduce their observability",
    "start": "36520",
    "end": "42280"
  },
  {
    "text": "costs well uh our company is under uh exponential growth and uh the cost of",
    "start": "42480",
    "end": "49399"
  },
  {
    "text": "observability was grow together and it was a problem that we need to handle and",
    "start": "49399",
    "end": "55160"
  },
  {
    "text": "take some actions to solve this kind of problem here the reason that why we need",
    "start": "55160",
    "end": "62840"
  },
  {
    "text": "to consider to implement some kind of sampling where our cost of observability traces was growing very fast so uh with",
    "start": "62840",
    "end": "72360"
  },
  {
    "text": "it it in mind was necessary to take some actions that we will share more details",
    "start": "72360",
    "end": "77600"
  },
  {
    "text": "in the next slides all right so um on this the next",
    "start": "77600",
    "end": "84040"
  },
  {
    "text": "20 minutes or so we're sharing uh what is sampling um what are the sampling",
    "start": "84040",
    "end": "89079"
  },
  {
    "text": "strategies that we have have available to for us and why Pismo needed sampling",
    "start": "89079",
    "end": "94200"
  },
  {
    "text": "and how they implemented sampling what were the outcomes they got and what holds for the future for uh Pismo when",
    "start": "94200",
    "end": "101399"
  },
  {
    "text": "it comes to sampling now um when we talk about sampling we can apply the same",
    "start": "101399",
    "end": "107759"
  },
  {
    "text": "ideas or the same uh Concepts to all of the signals right so we can apply uh some sampling strategies for logging we",
    "start": "107759",
    "end": "114719"
  },
  {
    "text": "can apply um the reduction of data for metrics and for traces so um I can with",
    "start": "114719",
    "end": "120280"
  },
  {
    "text": "a very simple definition of what is sampling and sampling is a technique that we can use to reduce the amount of",
    "start": "120280",
    "end": "127000"
  },
  {
    "text": "telemetry data that is generated or stored right so we either generate less",
    "start": "127000",
    "end": "133000"
  },
  {
    "text": "data or we store less data now uh we typically think about tracing when we",
    "start": "133000",
    "end": "139519"
  },
  {
    "text": "talk about sampling and because we have only 20 minutes here uh I'm not covering what is sampling for matrics and for",
    "start": "139519",
    "end": "145959"
  },
  {
    "text": "logs we're focusing on on traces here and sampling for tracing is basically",
    "start": "145959",
    "end": "151959"
  },
  {
    "text": "the idea of limiting the number of traces that we have by making a decision and then applying that decision to all",
    "start": "151959",
    "end": "158040"
  },
  {
    "text": "of these pens uh belonging to the same Trace right so we can make the decision either at the very beginning of the data",
    "start": "158040",
    "end": "164959"
  },
  {
    "text": "creation of the Telemetry data creation so at the root span as we call them we can apply that at the collector uh and",
    "start": "164959",
    "end": "172640"
  },
  {
    "text": "we can apply that after the trace has been finished um there is a small",
    "start": "172640",
    "end": "177959"
  },
  {
    "text": "difference between the the the the second and third points here uh that we are going to get later now Trace based",
    "start": "177959",
    "end": "184640"
  },
  {
    "text": "sampling or or head based sampling or just head sampling um is applied at the",
    "start": "184640",
    "end": "190879"
  },
  {
    "text": "very beginning of the trace and we typically use a probabilistic strategy for that now we can use other strategies",
    "start": "190879",
    "end": "197480"
  },
  {
    "text": "as well but typically when we when we talk about head sampling we are talking about making a decision based on some",
    "start": "197480",
    "end": "204280"
  },
  {
    "text": "probabilistic um so we say that for a service X we are uh sampling only or we",
    "start": "204280",
    "end": "210400"
  },
  {
    "text": "are selecting only 50% of the data that we are creating um and then we use that",
    "start": "210400",
    "end": "217040"
  },
  {
    "text": "decision um and we include that decision in the context that we propagate down to other spans for that uh for that same",
    "start": "217040",
    "end": "225280"
  },
  {
    "text": "Trace now um the advantages of doing same playing at the head uh at the head",
    "start": "225280",
    "end": "231239"
  },
  {
    "text": "or at the very beginning of the trace is that we don't have any transmission costs for nonsampled data right so we",
    "start": "231239",
    "end": "237200"
  },
  {
    "text": "can still perhaps generate depending on on the that we are using but we are not sending that data out it's relatively",
    "start": "237200",
    "end": "244360"
  },
  {
    "text": "easy to configure so when you are starting with a uh tracing you typically",
    "start": "244360",
    "end": "249480"
  },
  {
    "text": "are taught how to do sampling uh for for your services some libraries even come",
    "start": "249480",
    "end": "255680"
  },
  {
    "text": "with a sampling already on right so they can uh they can reduce the amount of data by",
    "start": "255680",
    "end": "261880"
  },
  {
    "text": "theault it's harder to apply it consistently though I mean um while you can configure for a service a a specific",
    "start": "261880",
    "end": "269440"
  },
  {
    "text": "number",
    "start": "269440",
    "end": "271720"
  },
  {
    "text": "on the trace ID so I can hash the trace ID do a um hash mod n and then we we can",
    "start": "329759",
    "end": "335479"
  },
  {
    "text": "make a decision consistent for all of these pans within the same Trace so we don't have to keep the trace the expens",
    "start": "335479",
    "end": "340639"
  },
  {
    "text": "in in memory to apply a decision the decision is consistent but then again we",
    "start": "340639",
    "end": "345960"
  },
  {
    "text": "have uh the same problem with uh probability um it is easy to configure",
    "start": "345960",
    "end": "351400"
  },
  {
    "text": "for all of the services at once because if we have all of the Telemetry data going to the same collector we can",
    "start": "351400",
    "end": "356560"
  },
  {
    "text": "centralize the configuration at that collector and change and tune based on our current needs um It's relatively",
    "start": "356560",
    "end": "363039"
  },
  {
    "text": "easy to configure but it does come with the operational costs of a collector and a collector is not very cheap to run",
    "start": "363039",
    "end": "369120"
  },
  {
    "text": "right I mean you you have to install you have to deploy it you have to maintain you have to to to take care of it uh and",
    "start": "369120",
    "end": "375680"
  },
  {
    "text": "again we have the same problem with Statistics now we have a second way of doing sampling at The Collector like",
    "start": "375680",
    "end": "381520"
  },
  {
    "text": "tail sampling uh and we typically use that for complex strategies so we can",
    "start": "381520",
    "end": "387560"
  },
  {
    "text": "apply different strategies depending on how the data look like um we",
    "start": "387560",
    "end": "393319"
  },
  {
    "text": "can this um one of the characteristics of this strategy is that we can we have",
    "start": "393319",
    "end": "398960"
  },
  {
    "text": "to keep data in memory for some time so we we keep all of the data in memory for",
    "start": "398960",
    "end": "404560"
  },
  {
    "text": "a specific amount of time and then once we deem a trace to be ready then we make a decision based on how the trace look",
    "start": "404560",
    "end": "411680"
  },
  {
    "text": "like what it means is it requires all of these pens to be sent to the same collector so if we send if we have like",
    "start": "411680",
    "end": "418599"
  },
  {
    "text": "10 collectors with we are scaling our our infra so we have 10 collectors and I send um and I do a wrong uh robbing uh",
    "start": "418599",
    "end": "426479"
  },
  {
    "text": "load balancing of my data then it means that parts of my traces are getting to one collector and parts to another",
    "start": "426479",
    "end": "432639"
  },
  {
    "text": "collector and when I run the decision uh I only make a decision based on partial traces and that decision might be wrong",
    "start": "432639",
    "end": "440599"
  },
  {
    "text": "so it requires again all of these fans from the same Trace to go to the same collector um advantages are I can really",
    "start": "440599",
    "end": "448319"
  },
  {
    "text": "pick um this exact traces that I want I can say I want 100% of the errors for",
    "start": "448319",
    "end": "454039"
  },
  {
    "text": "instance um I can apply multiple and complex conditions so not necessarily only probabilistic strategies uh but uh",
    "start": "454039",
    "end": "462080"
  },
  {
    "text": "combine probabilistic with errors and uh so on so forth it does come with even",
    "start": "462080",
    "end": "467280"
  },
  {
    "text": "higher operational cost as you're going to see in a couple of minutes um and by",
    "start": "467280",
    "end": "472639"
  },
  {
    "text": "now you have a stateful collector and scaling stateful workloads is somewhat",
    "start": "472639",
    "end": "477759"
  },
  {
    "text": "harder than uh stateless um and even I mean sampling has",
    "start": "477759",
    "end": "483960"
  },
  {
    "text": "downsides as well right so it's you don't use sampling by defa I would say um you only do sampling when you",
    "start": "483960",
    "end": "490440"
  },
  {
    "text": "actually need to do sampling because it's harder to make statistics after the fact so if you are dropping data out um",
    "start": "490440",
    "end": "497759"
  },
  {
    "text": "you don't you don't get that data back um so if you haven't collected statistics before dropping data it's",
    "start": "497759",
    "end": "504360"
  },
  {
    "text": "very harder it's very hard to uh understand what you actually missed what do you actually uh um uh",
    "start": "504360",
    "end": "510440"
  },
  {
    "text": "throughout it potentially misses important Elementary data so if you are waiting like 5 seconds for data to come",
    "start": "510440",
    "end": "516560"
  },
  {
    "text": "in uh so that make a decision and then you probably said okay so that Trace",
    "start": "516560",
    "end": "521839"
  },
  {
    "text": "there looks fine uh it the latency was very low and it was just more of the",
    "start": "521839",
    "end": "527080"
  },
  {
    "text": "same so I I just drop that data and then suddenly be U you receive a new span for",
    "start": "527080",
    "end": "532640"
  },
  {
    "text": "that now it becomes interesting but because you throw that Trace out you don't have that data anymore so you",
    "start": "532640",
    "end": "539519"
  },
  {
    "text": "potentially miss important Telemetry and it is indeed more complex to operate now you have to think about another layer of",
    "start": "539519",
    "end": "546079"
  },
  {
    "text": "collectors doing um load balancing for instance and how can you do that load balancing um and so other problems to",
    "start": "546079",
    "end": "553120"
  },
  {
    "text": "think about and now I'll pass it back to Alexandri to say why they needed sampling at",
    "start": "553120",
    "end": "560320"
  },
  {
    "text": "Pismo hi uh here we have uh three pillars that uh why we needed uh to",
    "start": "566920",
    "end": "572480"
  },
  {
    "text": "implement sampling in our case the exponential growth of the company and",
    "start": "572480",
    "end": "577760"
  },
  {
    "text": "hopefully hopefully uh without exponential observability costs while we",
    "start": "577760",
    "end": "583839"
  },
  {
    "text": "keep the visibility of our service so I will cover in more details here uh I as",
    "start": "583839",
    "end": "590720"
  },
  {
    "text": "I mentioned previous I work in at Pismo and the Pismo is a Prov uh Bank a",
    "start": "590720",
    "end": "596320"
  },
  {
    "text": "service of providing uh and and we have a large customer around the world and it becomes",
    "start": "596320",
    "end": "603360"
  },
  {
    "text": "our operations very critical so we need to have a good observability in",
    "start": "603360",
    "end": "609480"
  },
  {
    "text": "ourselves uh to cover and perform some troubleshoot when problems",
    "start": "609480",
    "end": "615640"
  },
  {
    "text": "happen here we have uh some numbers of P that uh the volume of money and",
    "start": "615640",
    "end": "621360"
  },
  {
    "text": "transactions that pass for our platform uh so here we can have idea about the",
    "start": "621360",
    "end": "628120"
  },
  {
    "text": "the the how much critical is this operation and the control uh",
    "start": "628120",
    "end": "635040"
  },
  {
    "text": "observability costs uh the the grow of the company uh could impact the",
    "start": "635040",
    "end": "640440"
  },
  {
    "text": "observability C it's uh sometimes become a big challenge to us to control this kind of situation so uh we've discussed",
    "start": "640440",
    "end": "650000"
  },
  {
    "text": "uh growing very fast it can impact the margin of the the the products that we",
    "start": "650000",
    "end": "655240"
  },
  {
    "text": "are uh sell uh so uh is necessary to to to take",
    "start": "655240",
    "end": "660760"
  },
  {
    "text": "some actions to solve this kind of situation keep uh uh Services observable",
    "start": "660760",
    "end": "668000"
  },
  {
    "text": "so when we try to implement some kind of sampling we don't can uh we cannot lost",
    "start": "668000",
    "end": "674880"
  },
  {
    "text": "the visibility of uh about our workloads when some kind of error some kind of",
    "start": "674880",
    "end": "681519"
  },
  {
    "text": "crash or incident critical happen you need to have good data to analyze and",
    "start": "681519",
    "end": "686600"
  },
  {
    "text": "perform to troubleshoots and become the he cover the the the platform as",
    "start": "686600",
    "end": "693480"
  },
  {
    "text": "well and how we did it in the the moment that we decide to implement sampling our",
    "start": "693480",
    "end": "700399"
  },
  {
    "text": "in our service we we had two options on the table that was head sampling or tail",
    "start": "700399",
    "end": "707360"
  },
  {
    "text": "sampling uh how we learn with to uh uh juras previously head sampling uh the",
    "start": "707360",
    "end": "713959"
  },
  {
    "text": "decision to store or not the the traces uh is taken on beginning of the trans",
    "start": "713959",
    "end": "719440"
  },
  {
    "text": "action so for this reason we discard this option because some could could",
    "start": "719440",
    "end": "725519"
  },
  {
    "text": "happen that we discard a critical information uh that can help us to solve",
    "start": "725519",
    "end": "731120"
  },
  {
    "text": "some kind of problem so we choose the tail sampling uh why tail samp tail sampling provide to us a good visibility",
    "start": "731120",
    "end": "738680"
  },
  {
    "text": "of uh our service in case of Crash uh find control with polic you have a lot",
    "start": "738680",
    "end": "744880"
  },
  {
    "text": "of possibilities with policies uh that you can use in ta simpl and tning uh the",
    "start": "744880",
    "end": "752880"
  },
  {
    "text": "open t as all there are a good metcs that you can analyze and perform uh",
    "start": "752880",
    "end": "759000"
  },
  {
    "text": "Improvement uh in in the collector in the configurations and perform some turnings uh that Bas it in the met that",
    "start": "759000",
    "end": "767279"
  },
  {
    "text": "uh have uh we have available in in open tement",
    "start": "767279",
    "end": "772760"
  },
  {
    "text": "collector here uh we have a simple diagram about the the the open Teter",
    "start": "772760",
    "end": "778639"
  },
  {
    "text": "where when 100% of the traces of the the the applications uh is sending to our",
    "start": "778639",
    "end": "785959"
  },
  {
    "text": "collector but uh the polic is applied and just uh uh sampling traces are sent",
    "start": "785959",
    "end": "793240"
  },
  {
    "text": "for our back end here uh we have some polls that we",
    "start": "793240",
    "end": "799079"
  },
  {
    "text": "have implemented uh the first PS uh is about errors if some uh Trace has some",
    "start": "799079",
    "end": "806240"
  },
  {
    "text": "kind of error this Trace will be stored in in in our application another",
    "start": "806240",
    "end": "812440"
  },
  {
    "text": "possibility is about the maybe you don't want to implement sampling in specific application so you can use this PS to to",
    "start": "812440",
    "end": "819839"
  },
  {
    "text": "don't Implement samply for that specific application another possibility that we",
    "start": "819839",
    "end": "825639"
  },
  {
    "text": "we have is about the Laten if there are some some latting uh happening your in",
    "start": "825639",
    "end": "832839"
  },
  {
    "text": "your environment you can store this data to analyze uh in in in another moment",
    "start": "832839",
    "end": "840360"
  },
  {
    "text": "and if everything is is running well uh you don't have error latting or any",
    "start": "840360",
    "end": "847519"
  },
  {
    "text": "exception uh we will use probabilistic sampling that will start until 25% of",
    "start": "847519",
    "end": "855040"
  },
  {
    "text": "the trace generating our platform another session of settings",
    "start": "855040",
    "end": "860880"
  },
  {
    "text": "that we have here is about the decision wait how much time we wait to decide if",
    "start": "860880",
    "end": "867600"
  },
  {
    "text": "we will store the trace are not 5 seconds and we start uh",
    "start": "867600",
    "end": "875120"
  },
  {
    "text": "120,000 uh Trace in the memory it's possible to to start this this number in",
    "start": "875120",
    "end": "880639"
  },
  {
    "text": "the memory to take a decision if we will start or not uh we don't have a magic",
    "start": "880639",
    "end": "888560"
  },
  {
    "text": "number about the number of Spun that each Trace has in some situations we have some traces that has uh 10 spun and",
    "start": "888560",
    "end": "897880"
  },
  {
    "text": "another moments we can have have Trace with more than 100 uh uh spun in the",
    "start": "897880",
    "end": "904959"
  },
  {
    "text": "same Trace here we have the ex uh chart",
    "start": "904959",
    "end": "910360"
  },
  {
    "text": "showing the volume of uh spun that is arriving in the in the collector uh how we can see",
    "start": "910360",
    "end": "917759"
  },
  {
    "text": "here is something near of 250,000 uh spun but when you look for",
    "start": "917759",
    "end": "926000"
  },
  {
    "text": "the another shart you can realize that uh the volume of Spun that we s for our",
    "start": "926000",
    "end": "932360"
  },
  {
    "text": "tool is less than 40,000 spun uh so it represent a good",
    "start": "932360",
    "end": "939480"
  },
  {
    "text": "save money for for us uh tradeoffs here is uh the instance",
    "start": "939480",
    "end": "946279"
  },
  {
    "text": "of the open collector is big but uh there are some reason for that for",
    "start": "946279",
    "end": "952000"
  },
  {
    "text": "example in our case we have some bad process that uh uh run during the night",
    "start": "952000",
    "end": "958680"
  },
  {
    "text": "where uh The Collector needs to have a a memory available to to handle with this",
    "start": "958680",
    "end": "964880"
  },
  {
    "text": "volume and another uh situation is in case of incident you will restore more",
    "start": "964880",
    "end": "971360"
  },
  {
    "text": "uh Trace so is necessary to to have uh memory available to to open Teter",
    "start": "971360",
    "end": "977720"
  },
  {
    "text": "collector handle with this volume of Trace so lessons learn that we have here",
    "start": "977720",
    "end": "985480"
  },
  {
    "text": "uh that is about the volume of Spun that collector can handle and we realize that",
    "start": "985480",
    "end": "991360"
  },
  {
    "text": "is a big volume of uh uh spun that uh open t can can handle and works",
    "start": "991360",
    "end": "1001519"
  },
  {
    "text": "well can yeah um but a collector Can Only",
    "start": "1003079",
    "end": "1008680"
  },
  {
    "text": "Hold um so much load right uh so they had uh 250,000 spins per second that's",
    "start": "1008680",
    "end": "1016040"
  },
  {
    "text": "quite something uh but then there's some limit um and they have",
    "start": "1016040",
    "end": "1023240"
  },
  {
    "text": "after doing vertical um escalation scaling of their collectors they found",
    "start": "1023240",
    "end": "1029360"
  },
  {
    "text": "out recently that um they hit the limit and looking at the previous chart you",
    "start": "1029360",
    "end": "1035400"
  },
  {
    "text": "probably saw like a spike and then a constant load uh after uh during the day",
    "start": "1035400",
    "end": "1041558"
  },
  {
    "text": "right so um that's also not very optimal usage of the resources um and then now",
    "start": "1041559",
    "end": "1048360"
  },
  {
    "text": "they hit they limit where um it's harder to scale um vertically now they have to",
    "start": "1048360",
    "end": "1054160"
  },
  {
    "text": "scale um horizontally as well so that's a second lesson learned even though the collector can take a very high load uh",
    "start": "1054160",
    "end": "1062280"
  },
  {
    "text": "there is a point where you have to plan for horizontal",
    "start": "1062280",
    "end": "1067000"
  },
  {
    "text": "scaling and uh another lesson that we learned that sometimes can happen that",
    "start": "1067440",
    "end": "1073280"
  },
  {
    "text": "uh open t collector crash but it's not a big problem because when uh uh open t",
    "start": "1073280",
    "end": "1080080"
  },
  {
    "text": "collector needs to be restart for any reason memory out of memory or some",
    "start": "1080080",
    "end": "1086679"
  },
  {
    "text": "reason uh open time collector take just four seconds to be ready again so the",
    "start": "1086679",
    "end": "1092159"
  },
  {
    "text": "volume of Spun can uh arrive again in The Collector that will be treated and",
    "start": "1092159",
    "end": "1098440"
  },
  {
    "text": "uh sent to our uh observability",
    "start": "1098440",
    "end": "1103840"
  },
  {
    "text": "tool so were anyone here yesterday at Rejects",
    "start": "1104640",
    "end": "1110760"
  },
  {
    "text": "yeah so yeah we had a a talk yesterday on on resiliency of collector pipelines so um the previous slide show that it's",
    "start": "1110760",
    "end": "1117960"
  },
  {
    "text": "okay to crash The Collector or for their uh case but uh of course the collector provides ways for people to make",
    "start": "1117960",
    "end": "1124000"
  },
  {
    "text": "resilient pipelines as well now the second uh the fourth uh lesson learned",
    "start": "1124000",
    "end": "1129080"
  },
  {
    "text": "is that the probability of 10% is not 10% so I I kind of alluded to that at the very beginning um and if you were",
    "start": "1129080",
    "end": "1135440"
  },
  {
    "text": "paying attention to the graphics um you saw that they are ingesting 244,000 per",
    "start": "1135440",
    "end": "1141440"
  },
  {
    "text": "second or that was their Peck uh on that specific time window and um if we look",
    "start": "1141440",
    "end": "1147799"
  },
  {
    "text": "at the number of sample data we got like 30 39.6k um if we do the math uh 39.6k is",
    "start": "1147799",
    "end": "1156679"
  },
  {
    "text": "not 25% which is what they had as probabilistic sampling it's 16% and if",
    "start": "1156679",
    "end": "1162799"
  },
  {
    "text": "we go back to the configuration they had they had like 100% of Errors 100% of those services in here",
    "start": "1162799",
    "end": "1169280"
  },
  {
    "text": "100% of high latency and 25% of the other things so I would",
    "start": "1169280",
    "end": "1174559"
  },
  {
    "text": "intuitively um think that we get more than 25% but it's not really the case it's",
    "start": "1174559",
    "end": "1180280"
  },
  {
    "text": "only 16 right so and I would and I I'm going to be honest with you when I saw those numbers here I came to Mar and",
    "start": "1180280",
    "end": "1186600"
  },
  {
    "text": "said that that's wrong I mean I would expect 25 I mean to be roughly 25 and he said no no yeah we looked into that and",
    "start": "1186600",
    "end": "1193000"
  },
  {
    "text": "it's really like 16 it's uh we increase and we decrease and we can see the effects but um and of course there's",
    "start": "1193000",
    "end": "1200760"
  },
  {
    "text": "other situation there where uh we specify a percentage of traces but then",
    "start": "1200760",
    "end": "1207640"
  },
  {
    "text": "the within the traces we have a different number of of spans so and the numbers that we have here are spans",
    "start": "1207640",
    "end": "1214440"
  },
  {
    "text": "right so what it means is we get um 16% of the spans been sampled but that is uh",
    "start": "1214440",
    "end": "1220480"
  },
  {
    "text": "not telling how many um how many traces were actually",
    "start": "1220480",
    "end": "1225640"
  },
  {
    "text": "sampled and uh the last lesson that we learn is about uh our currently policy",
    "start": "1227240",
    "end": "1234080"
  },
  {
    "text": "that we have implemented at Pismo uh is simple to detect when some kind incident",
    "start": "1234080",
    "end": "1241240"
  },
  {
    "text": "is happened because the volume of traces is stored in the The Tool uh raise so",
    "start": "1241240",
    "end": "1247360"
  },
  {
    "text": "fast and imagine that a critical incident uh will be stored more and more",
    "start": "1247360",
    "end": "1253400"
  },
  {
    "text": "trace and you can uh the simple way analyze the shart for example",
    "start": "1253400",
    "end": "1259240"
  },
  {
    "text": "and detect that the time of the problem start and you can analyze based",
    "start": "1259240",
    "end": "1266039"
  },
  {
    "text": "in good information because the the most part of information that will be stored",
    "start": "1266039",
    "end": "1271440"
  },
  {
    "text": "is about errors and uh as we know when we are trying to perform some",
    "start": "1271440",
    "end": "1276840"
  },
  {
    "text": "troubleshoot we look for errors and here is the results of after",
    "start": "1276840",
    "end": "1284279"
  },
  {
    "text": "the full implementation of tail sampling our company this is the chart of the financial team team of the Pismo and we",
    "start": "1284279",
    "end": "1293000"
  },
  {
    "text": "can realize that we are saving a lot of money here and uh the size of Pismo",
    "start": "1293000",
    "end": "1300080"
  },
  {
    "text": "today is much more than when we we implemented so uh it's complicated to",
    "start": "1300080",
    "end": "1307840"
  },
  {
    "text": "imagine where is uh discussed today because uh the volume of transaction",
    "start": "1307840",
    "end": "1313400"
  },
  {
    "text": "that we have today is much more than previously for example on 20 22 year for",
    "start": "1313400",
    "end": "1321120"
  },
  {
    "text": "example and the next steps as as you realize that our instance of uh open",
    "start": "1321120",
    "end": "1328080"
  },
  {
    "text": "Teter is big but not necessarily to keep big for the all day so we will Implement",
    "start": "1328080",
    "end": "1335799"
  },
  {
    "text": "a load balancer uh because today we don't use load balancer but it will be implemented and implemented HPA to scale",
    "start": "1335799",
    "end": "1344679"
  },
  {
    "text": "in case of uh we need to to scale automatically it will be bring more uh",
    "start": "1344679",
    "end": "1351000"
  },
  {
    "text": "save money for us in terms of instance uh to uh uh host the open Alarm",
    "start": "1351000",
    "end": "1357840"
  },
  {
    "text": "collector so it's all that uh I I would like to thank you for everyone and uh if",
    "start": "1357840",
    "end": "1365200"
  },
  {
    "text": "you have some question please let us",
    "start": "1365200",
    "end": "1369200"
  },
  {
    "text": "know we have microphones here here on the sides",
    "start": "1376559",
    "end": "1381919"
  },
  {
    "text": "um yeah I have a quick question given you're in in banking did most of the",
    "start": "1381919",
    "end": "1388120"
  },
  {
    "text": "sampling that you did not involve money movement as far as needing to keep um",
    "start": "1388120",
    "end": "1395360"
  },
  {
    "text": "transactional data did you mainly do non-critical",
    "start": "1395360",
    "end": "1402000"
  },
  {
    "text": "pathways uh well uh the most part of data uh um",
    "start": "1407520",
    "end": "1413720"
  },
  {
    "text": "is not so relevant for analyze some kind of problem because we have logs in a",
    "start": "1413720",
    "end": "1420159"
  },
  {
    "text": "good level of detail so uh logs complement the trace so we don't l a",
    "start": "1420159",
    "end": "1425799"
  },
  {
    "text": "data critical data we can search something at logs and we we will find uh",
    "start": "1425799",
    "end": "1432039"
  },
  {
    "text": "just to share uh we have a good strategy in terms of logs that we uh get the C ID",
    "start": "1432039",
    "end": "1439880"
  },
  {
    "text": "correlation ID of the transaction and it will be uh set in your uh line logs of",
    "start": "1439880",
    "end": "1447039"
  },
  {
    "text": "the all applications so if we need to perform some troubleshoot detect something we can just get the cig and we",
    "start": "1447039",
    "end": "1455159"
  },
  {
    "text": "can have a details based in logs but for traces we have a a tool where we can",
    "start": "1455159",
    "end": "1462760"
  },
  {
    "text": "analyze the specific request for database elastic cach and the any other",
    "start": "1462760",
    "end": "1470120"
  },
  {
    "text": "end point okay thank you thank you yeah thanks a lot thanks a lot for this great",
    "start": "1470120",
    "end": "1475320"
  },
  {
    "text": "talk uh just can you go back to Lesson Four because I was wondering on the probabilistic anomaly because the",
    "start": "1475320",
    "end": "1481960"
  },
  {
    "text": "interesting part here the max indeed has this weird thing but the mean is actually quite nicely",
    "start": "1481960",
    "end": "1488080"
  },
  {
    "text": "25% and that makes sense right because maximums are very sensitive to probalistic anomalies well the mean",
    "start": "1488080",
    "end": "1494799"
  },
  {
    "text": "isn't so yeah I'm I'm I'm not sure about the conclusion because it really in the mean",
    "start": "1494799",
    "end": "1500320"
  },
  {
    "text": "it's 25% and that's the overall period you're looking at yeah uh so that's um",
    "start": "1500320",
    "end": "1508000"
  },
  {
    "text": "so I guess the point is um you are configuring 25% uh but it's not",
    "start": "1508000",
    "end": "1513240"
  },
  {
    "text": "necessarily going to be 25% so it might be at most of the time but then you have to account for those uh those movements",
    "start": "1513240",
    "end": "1521440"
  },
  {
    "text": "right so it is 19 uh 16% uh if we compare just the Peaks but uh yeah I",
    "start": "1521440",
    "end": "1527640"
  },
  {
    "text": "mean ideally it would be around 25 yeah but in in your case I don't know",
    "start": "1527640",
    "end": "1532880"
  },
  {
    "text": "what the time frame was where you took this this I think that was one day right one day so at at one day you're at",
    "start": "1532880",
    "end": "1538880"
  },
  {
    "text": "exactly 25% yeah I think that's one day",
    "start": "1538880",
    "end": "1545919"
  },
  {
    "text": "yeah yeah I mean um you should you should we should",
    "start": "1545919",
    "end": "1551000"
  },
  {
    "text": "aim for 25 and that's um data should be within that close to that range but uh",
    "start": "1551000",
    "end": "1558399"
  },
  {
    "text": "it might not right so I guess that's the especially because and I think the the",
    "start": "1558399",
    "end": "1563880"
  },
  {
    "text": "the the when I was looking at that data uh the what I thought was we are",
    "start": "1563880",
    "end": "1570960"
  },
  {
    "text": "filtering traces so it might be 25% of the traces and I would hope that uh the",
    "start": "1570960",
    "end": "1576559"
  },
  {
    "text": "number of traces and uh the number of spans within traces would uh be almost",
    "start": "1576559",
    "end": "1581760"
  },
  {
    "text": "the same throughout the day uh and that might not be the case right so perhaps the traces that are generated from bat",
    "start": "1581760",
    "end": "1588640"
  },
  {
    "text": "they have more or less expense than the ones that are transactional throughout the day so the sampling that we appli",
    "start": "1588640",
    "end": "1594600"
  },
  {
    "text": "there might affect the the the number for the end of but yeah I mean over a longer the longer we we look at the data",
    "start": "1594600",
    "end": "1602279"
  },
  {
    "text": "uh the closer to 25 it should be yes thanks uh quick question one claim I",
    "start": "1602279",
    "end": "1609919"
  },
  {
    "text": "remember hearing a few years ago uh at a similar presentation was that for at least using an open source back end",
    "start": "1609919",
    "end": "1616039"
  },
  {
    "text": "where where the customer has to pay for the processing of traces so jger for example the claim was that for computed",
    "start": "1616039",
    "end": "1624000"
  },
  {
    "text": "memory costs tail based sampling would be roughly equivalent to just sending them all the way into the back end and",
    "start": "1624000",
    "end": "1629720"
  },
  {
    "text": "processing them can you speak to the amount of compute you had to throw at The Collector to achieve these gains it",
    "start": "1629720",
    "end": "1635360"
  },
  {
    "text": "looks like it was quite low which is good um I'll let him answer most of the",
    "start": "1635360",
    "end": "1641600"
  },
  {
    "text": "question uh but um so they had to have this bag of a of a",
    "start": "1641600",
    "end": "1648799"
  },
  {
    "text": "collector like so 25 CPUs and 56 gigs of RAM uh so that's quite Biffy um but then",
    "start": "1648799",
    "end": "1655840"
  },
  {
    "text": "they were paying where is the yeah so they were paying like almost 80k a month um and",
    "start": "1655840",
    "end": "1663919"
  },
  {
    "text": "they went down to 15K month with that collector right so that's um I would",
    "start": "1663919",
    "end": "1670039"
  },
  {
    "text": "expect a collector of that size not to cost what um such many case",
    "start": "1670039",
    "end": "1677240"
  },
  {
    "text": "yeah yeah but",
    "start": "1677240",
    "end": "1680919"
  },
  {
    "text": "perhaps yeah all right good thank you guys um how did you get to the number of",
    "start": "1682440",
    "end": "1688840"
  },
  {
    "text": "15 seconds and 120k of traces uh it was based in our test for",
    "start": "1688840",
    "end": "1696039"
  },
  {
    "text": "example uh 5 seconds for us is a high Laten because everything uh every",
    "start": "1696039",
    "end": "1703760"
  },
  {
    "text": "transaction in banking must to be done in in milliseconds so 5 seconds is",
    "start": "1703760",
    "end": "1710600"
  },
  {
    "text": "almost incident for us so this is a good number to to have uh and the volume of",
    "start": "1710600",
    "end": "1717640"
  },
  {
    "text": "traces that we have uh stor in memory in case 120,000 Trace is based in the critical",
    "start": "1717640",
    "end": "1726840"
  },
  {
    "text": "moments for example B process running that running during the the night and in",
    "start": "1726840",
    "end": "1732519"
  },
  {
    "text": "case of incidents when when we have some kind of incident is uh the customer per",
    "start": "1732519",
    "end": "1738240"
  },
  {
    "text": "far uh he try and he try and the volume of request is is Raising so fast so it",
    "start": "1738240",
    "end": "1744519"
  },
  {
    "text": "was necessary to put a big number to to to to be able handle with the volume of",
    "start": "1744519",
    "end": "1750760"
  },
  {
    "text": "data and storing our tool got it got it but you you mention like some batch operations working during night like and",
    "start": "1750760",
    "end": "1758440"
  },
  {
    "text": "if you have any like operation that's taking more than 5 seconds but like it's maybe a reporting generation for example",
    "start": "1758440",
    "end": "1765120"
  },
  {
    "text": "you want to trace this uh you are we going to to lose this traces because",
    "start": "1765120",
    "end": "1770519"
  },
  {
    "text": "this will be fragmented like how does this be handled no it's not fragmented",
    "start": "1770519",
    "end": "1776679"
  },
  {
    "text": "uh but the the volume of uh the memory that we allocate for the the The",
    "start": "1776679",
    "end": "1783120"
  },
  {
    "text": "Collector is necessary because we have a bad process but there are transactions",
    "start": "1783120",
    "end": "1788799"
  },
  {
    "text": "happening in the same moment so to guarantee the the the transactions that",
    "start": "1788799",
    "end": "1794000"
  },
  {
    "text": "is running in that moment is necessary to to allocate memory for this these",
    "start": "1794000",
    "end": "1799399"
  },
  {
    "text": "situations so to address part of your question um we we have a feature request",
    "start": "1799399",
    "end": "1805519"
  },
  {
    "text": "and we are well since like forever but uh the idea is uh to record on a cache",
    "start": "1805519",
    "end": "1811159"
  },
  {
    "text": "the decision for a specific Trace uh so if we have a trace that with a decision",
    "start": "1811159",
    "end": "1817080"
  },
  {
    "text": "to trace or to sample sorry uh then we we look at the cache and we send that data in and if the decision was not to",
    "start": "1817080",
    "end": "1824200"
  },
  {
    "text": "sample then we dropped this this data based on decisions that were made more than five seconds ago uh this is only a",
    "start": "1824200",
    "end": "1831039"
  },
  {
    "text": "cache so data can be you know after some time it is going to disappear but it is something that we have on the road map",
    "start": "1831039",
    "end": "1837039"
  },
  {
    "text": "for the load balancing uh sorry for the tail sampling processor that's cool so this means that my Trace might might",
    "start": "1837039",
    "end": "1842720"
  },
  {
    "text": "take longer than the my this time decision data that you lost you lost uh",
    "start": "1842720",
    "end": "1848480"
  },
  {
    "text": "but if you made a decision that decision is going to be applied consistently to the new or to the late uh late Spence",
    "start": "1848480",
    "end": "1855480"
  },
  {
    "text": "cool thank you guys all right yeah we are out of time so one question",
    "start": "1855480",
    "end": "1861880"
  },
  {
    "text": "so you're talking about uh scaling out the stateful sets right so the question is how often do you plan to scale out",
    "start": "1861880",
    "end": "1868039"
  },
  {
    "text": "those uh stateful collectors and how do you plan to handle the statefulness of those collectors right I mean when when",
    "start": "1868039",
    "end": "1874840"
  },
  {
    "text": "a new collector gets added or removed then you need to ensure that the spans of a particular Trace goes to the right",
    "start": "1874840",
    "end": "1880440"
  },
  {
    "text": "set of collectors right um the way that the so you would apply the load",
    "start": "1880440",
    "end": "1886200"
  },
  {
    "text": "balancing exporter in front of your like a layer of load balancing exporters and then the second layer is the same the",
    "start": "1886200",
    "end": "1891639"
  },
  {
    "text": "tail sampling processor so the tail sampling processor would not know about that change in the topology the load",
    "start": "1891639",
    "end": "1897600"
  },
  {
    "text": "balancing exporter would but then once you apply a new topology then you get data that is sent to a new location if",
    "start": "1897600",
    "end": "1904840"
  },
  {
    "text": "that's the case so there are some algorithms on the load balancing exporter making it um change only by 30%",
    "start": "1904840",
    "end": "1910760"
  },
  {
    "text": "of the trace ID uh range so only 30 30% would be affected by topology change uh",
    "start": "1910760",
    "end": "1916960"
  },
  {
    "text": "if it changed by one but then we are trying to come up with a solution similar to making a cash off decisions",
    "start": "1916960",
    "end": "1922799"
  },
  {
    "text": "for tail sampling we are thinking about the same for the load balancing exporter as well so that when a decision is made",
    "start": "1922799",
    "end": "1929000"
  },
  {
    "text": "for a specific Trace ID that decision is consistently applied uh in the future for other traces no matter the",
    "start": "1929000",
    "end": "1936240"
  },
  {
    "text": "topology and how often do you expect it to scale out or scale in what is that how often do you expect it to scale out",
    "start": "1936240",
    "end": "1942159"
  },
  {
    "text": "of scaling uh the out scale we we will Implement based in memory utilization so",
    "start": "1942159",
    "end": "1950159"
  },
  {
    "text": "we will set the number of the the the uh collector that can scale and basing the",
    "start": "1950159",
    "end": "1957000"
  },
  {
    "text": "memory utilization we will scale or not thank",
    "start": "1957000",
    "end": "1963480"
  },
  {
    "text": "you all right uh thank you very much and uh enjoy the",
    "start": "1963480",
    "end": "1969039"
  },
  {
    "text": "conference",
    "start": "1969039",
    "end": "1972039"
  }
]