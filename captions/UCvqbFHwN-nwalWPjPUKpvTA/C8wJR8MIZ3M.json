[
  {
    "text": "hi um welcome to the maintainers track six scheduling deep dive my name is Aldo",
    "start": "0",
    "end": "7080"
  },
  {
    "text": "cookie Condor I work for uh gke I'm based in Waterloo Canada",
    "start": "7080",
    "end": "12620"
  },
  {
    "text": "and I'm in the kubernetes side of things I'm a maintainer and TL for six",
    "start": "12620",
    "end": "19740"
  },
  {
    "text": "scheduling um yeah my name is Kent I worked for the",
    "start": "19740",
    "end": "26039"
  },
  {
    "text": "dark cloud based in Shanghai Channel and I also work on up Street kubernetes",
    "start": "26039",
    "end": "31199"
  },
  {
    "text": "Upstream together with erdo and I made it on 66 schedule here",
    "start": "31199",
    "end": "38179"
  },
  {
    "text": "so we're gonna have a quick overview scheduler for all for the ones who are",
    "start": "39000",
    "end": "44399"
  },
  {
    "text": "not familiar and the remaining is all about updates uh in the various projects",
    "start": "44399",
    "end": "49620"
  },
  {
    "text": "that the six gallon hosts um scheduler so let's start with the",
    "start": "49620",
    "end": "56280"
  },
  {
    "text": "scheduler um the scheduler uh uh is a single component is a controller",
    "start": "56280",
    "end": "64140"
  },
  {
    "text": "in in the in part of the control plane the kubernetes control plane and it's basically watching for uh pods which can",
    "start": "64140",
    "end": "73920"
  },
  {
    "text": "be in the high level into state in two states they could be admitted or",
    "start": "73920",
    "end": "79020"
  },
  {
    "text": "scheduled uh schedule pots pots are already assigned to nodes and Bots that",
    "start": "79020",
    "end": "85020"
  },
  {
    "text": "are new they they were not assigned to nodes yet so then the ports that are not assigned",
    "start": "85020",
    "end": "91200"
  },
  {
    "text": "yet they will go into a scheduling queue and the posts are scheduled will go into the internal scheduler cache",
    "start": "91200",
    "end": "99420"
  },
  {
    "text": "uh once uh once at a time uh we pop uh",
    "start": "99420",
    "end": "106799"
  },
  {
    "text": "uh pot from the from the queue um",
    "start": "106799",
    "end": "111899"
  },
  {
    "text": "and then we we execute all the scheduling algorithms to find first where the Pod can fit and among",
    "start": "111899",
    "end": "120540"
  },
  {
    "text": "all the possibilities which one is the best option according to the configuration uh and whether we need to",
    "start": "120540",
    "end": "127140"
  },
  {
    "text": "preempt any existing pods in them that are running because we have a higher",
    "start": "127140",
    "end": "132959"
  },
  {
    "text": "priority pod so if it's scalable uh it will enter The",
    "start": "132959",
    "end": "138900"
  },
  {
    "text": "Binding process which is simply telling the API server that the Pod is already",
    "start": "138900",
    "end": "144540"
  },
  {
    "text": "scheduled and where where it should go um if it doesn't you just will go back",
    "start": "144540",
    "end": "151200"
  },
  {
    "text": "to the queue um and even if if it doesn't it will get a",
    "start": "151200",
    "end": "157440"
  },
  {
    "text": "notification it will get a pot condition which says not scalable and other",
    "start": "157440",
    "end": "162480"
  },
  {
    "text": "components can react to it such as the cluster artist color for example uh if the binding is successful then",
    "start": "162480",
    "end": "170099"
  },
  {
    "text": "that's all that that keeps color does at that from from the point the the the no",
    "start": "170099",
    "end": "177239"
  },
  {
    "text": "the port has a a node assigned to it scheduler no longer owns the Pod so",
    "start": "177239",
    "end": "183900"
  },
  {
    "text": "anything that happens to it uh is responsibility of the cubelet except for",
    "start": "183900",
    "end": "190019"
  },
  {
    "text": "preemption uh which is when we need um to schedule a higher priority pot",
    "start": "190019",
    "end": "197159"
  },
  {
    "text": "um so uh over the years we worked in uh in a",
    "start": "197159",
    "end": "205019"
  },
  {
    "text": "framework to make scheduler more extensible uh because obviously different companies different users have",
    "start": "205019",
    "end": "212459"
  },
  {
    "text": "different needs so we uh we needed to uh to establish a structure where people",
    "start": "212459",
    "end": "219120"
  },
  {
    "text": "can contribute more easily to the scheduler so uh and as a result of that we",
    "start": "219120",
    "end": "225120"
  },
  {
    "text": "introduced a series of uh extension points uh where where people can um",
    "start": "225120",
    "end": "233360"
  },
  {
    "text": "extend the scheduler uh let's let's start in a green box uh the the",
    "start": "233360",
    "end": "238379"
  },
  {
    "text": "scheduling cycle um there are basically two two main processes here uh pre-filter and filter",
    "start": "238379",
    "end": "245959"
  },
  {
    "text": "filters are all about yes or no so uh we take a note we take a pot we take up a",
    "start": "245959",
    "end": "252480"
  },
  {
    "text": "node and we asked the question does can this go in here yes or no",
    "start": "252480",
    "end": "259139"
  },
  {
    "text": "um so we do that process if we ended up with all nodes for all for all the nodes",
    "start": "259139",
    "end": "265919"
  },
  {
    "text": "then we trigger the post filter which uh the typical example is the preemption uh",
    "start": "265919",
    "end": "272100"
  },
  {
    "text": "mechanism that would evict some parts if we have a set of uh if you have a set",
    "start": "272100",
    "end": "278580"
  },
  {
    "text": "of if we have more than one node that can fit that responded Yes to the",
    "start": "278580",
    "end": "283680"
  },
  {
    "text": "filters then we enter the scoring phase where we run a score for each of the",
    "start": "283680",
    "end": "290160"
  },
  {
    "text": "nodes uh multiple scores for each of the notes and whichever node has the highest",
    "start": "290160",
    "end": "295620"
  },
  {
    "text": "score is the one that is uh selected and then we enter The Binding cycle on the",
    "start": "295620",
    "end": "302040"
  },
  {
    "text": "right which communicates the decisions to API server somewhere in here there is",
    "start": "302040",
    "end": "309060"
  },
  {
    "text": "a mechanisms to uh uh uh to do volume binding as well but uh I won't get into",
    "start": "309060",
    "end": "317880"
  },
  {
    "text": "those details now we have introduced this new box uh on the left the purple",
    "start": "317880",
    "end": "324539"
  },
  {
    "text": "box uh which is a new uh a new uh extension point which we call Pre and Q",
    "start": "324539",
    "end": "331259"
  },
  {
    "text": "uh which is what it means is it will uh hold every hold any pod uh from from all",
    "start": "331259",
    "end": "340620"
  },
  {
    "text": "of the rest uh from the rest of the the mechanisms of the scalar",
    "start": "340620",
    "end": "347039"
  },
  {
    "text": "uh why is this useful well uh can't uh will will give us some some insight of",
    "start": "347039",
    "end": "353580"
  },
  {
    "text": "that but this is a new extension point that you can you can tweak um",
    "start": "353580",
    "end": "358919"
  },
  {
    "text": "the big difference between pre-inkue and filter let's say is that print queue is",
    "start": "358919",
    "end": "364740"
  },
  {
    "text": "a global decision it's a decision for the cluster uh filter is a decision for the node",
    "start": "364740",
    "end": "371100"
  },
  {
    "text": "um and also if you are failing the pre and queue you're not even entering the queue",
    "start": "371100",
    "end": "377280"
  },
  {
    "text": "so it's as if your pod didn't exist kind of",
    "start": "377280",
    "end": "382680"
  },
  {
    "text": "um so if your portfolio is a green queue for example the cluster autoscaler one",
    "start": "382680",
    "end": "388259"
  },
  {
    "text": "scale up that's uh one of the difference with Filter if a",
    "start": "388259",
    "end": "395340"
  },
  {
    "text": "pod fails filters then the cluster of the scaler can still react to it",
    "start": "395340",
    "end": "400740"
  },
  {
    "text": "um I will see why it's that useful in a second okay let's go through several updates",
    "start": "400740",
    "end": "408960"
  },
  {
    "text": "and improvements we made in the last several races so the first one as I said",
    "start": "408960",
    "end": "414419"
  },
  {
    "text": "is the post schedule so for most of the time when ports are created they will be",
    "start": "414419",
    "end": "420900"
  },
  {
    "text": "added to the schedule queue and popped out of the queue one by one for scheduling so this makes sense for",
    "start": "420900",
    "end": "428340"
  },
  {
    "text": "most of the time but not always the case like if the pause Mr essential",
    "start": "428340",
    "end": "434479"
  },
  {
    "text": "resources let's take a pod request for gpu4 example so if the GPU hasn't been",
    "start": "434479",
    "end": "440639"
  },
  {
    "text": "reduced in the cluster then the port will never be scheduled successfully right also like in a quota manager system and",
    "start": "440639",
    "end": "448979"
  },
  {
    "text": "let's say support with the lower polarity and there may be some",
    "start": "448979",
    "end": "454759"
  },
  {
    "text": "simulators like there is not enough resources for the pot so the portal will",
    "start": "454759",
    "end": "459900"
  },
  {
    "text": "still be stuck impending for a period of time however the group scheduler will commit",
    "start": "459900",
    "end": "466560"
  },
  {
    "text": "to reschedule support when it when it gets the chance so it somehow really to",
    "start": "466560",
    "end": "472740"
  },
  {
    "text": "the decoration of the scheduling throughput throughput so although we",
    "start": "472740",
    "end": "479280"
  },
  {
    "text": "have some like uh uh just uh mechanism underlying like we have the",
    "start": "479280",
    "end": "487080"
  },
  {
    "text": "like we have the back of algorithm which we are handing",
    "start": "487080",
    "end": "492419"
  },
  {
    "text": "for the Post schedule for scheduling failures but it will but we cannot avoid",
    "start": "492419",
    "end": "498479"
  },
  {
    "text": "it right so for another case it's about the uh",
    "start": "498479",
    "end": "503879"
  },
  {
    "text": "let's say we have several customer controllers who wants to make the decision together with the schedule",
    "start": "503879",
    "end": "510740"
  },
  {
    "text": "but but it does not want to modify the default schedule so without this feature",
    "start": "510740",
    "end": "517140"
  },
  {
    "text": "we cannot achieve this so we need a knob to help us control the scheduling",
    "start": "517140",
    "end": "523860"
  },
  {
    "text": "process that's the general idea of this cap so",
    "start": "523860",
    "end": "530700"
  },
  {
    "text": "we in this in this new feature we introduced the new Field named the schedule in case it's a set of strings",
    "start": "530700",
    "end": "539100"
  },
  {
    "text": "so when the schedule found a port carries the scaling case it will never",
    "start": "539100",
    "end": "544140"
  },
  {
    "text": "pop out support for scheduling",
    "start": "544140",
    "end": "548240"
  },
  {
    "text": "and this is a example about how this feature works",
    "start": "549240",
    "end": "554700"
  },
  {
    "text": "let's say we have a normal kubernetes and we have a customer web hook and",
    "start": "554700",
    "end": "560160"
  },
  {
    "text": "external controller so when we apply a port the portal will send the request to the API server and invoke in the web",
    "start": "560160",
    "end": "567660"
  },
  {
    "text": "hook then the web hooker will inject the scheduling case to the port",
    "start": "567660",
    "end": "573060"
  },
  {
    "text": "the same time Circle schedule watches the product create event and it found the port",
    "start": "573060",
    "end": "579779"
  },
  {
    "text": "carries the scaling Gates so it will never pop out the port",
    "start": "579779",
    "end": "585560"
  },
  {
    "text": "oh sorry it is for like removing the feature case",
    "start": "586580",
    "end": "593519"
  },
  {
    "text": "when it forms the post is already like the we we have enough resources for the",
    "start": "593519",
    "end": "599399"
  },
  {
    "text": "port in the course manager system so so external control will try to remove",
    "start": "599399",
    "end": "606000"
  },
  {
    "text": "the sketching pause and the cool schedule will watch the post updated event and it found the schedule case was",
    "start": "606000",
    "end": "613440"
  },
  {
    "text": "removed so it will pop out the port and yes when entering into the normal scheduling",
    "start": "613440",
    "end": "620220"
  },
  {
    "text": "process that's how it works in general",
    "start": "620220",
    "end": "624079"
  },
  {
    "text": "and the next step is about the mutable scheduling to relatives for suspended jobs",
    "start": "625500",
    "end": "632279"
  },
  {
    "text": "so the motivation for this is in best jobs usually posts will run with specific questions like they will run in",
    "start": "632279",
    "end": "639899"
  },
  {
    "text": "the same Zone maybe for the communication performance or say they",
    "start": "639899",
    "end": "645420"
  },
  {
    "text": "prefer some same model of gpus so job hopes to be mutable when when",
    "start": "645420",
    "end": "652079"
  },
  {
    "text": "it's suspended and also a high level job Control job Queen control",
    "start": "652079",
    "end": "658200"
  },
  {
    "text": "or can take advantage of this feature for better post placements",
    "start": "658200",
    "end": "664200"
  },
  {
    "text": "let's say let's look at the right maybe right side of the side so this these are",
    "start": "664200",
    "end": "671399"
  },
  {
    "text": "all the visible fields we support right now so not Affinity no selector",
    "start": "671399",
    "end": "676500"
  },
  {
    "text": "Toleration annotation levels and scheduling case the scheduling case was",
    "start": "676500",
    "end": "681779"
  },
  {
    "text": "it was introduced in the last series 1.27. so we didn't introduce any new apis in",
    "start": "681779",
    "end": "689279"
  },
  {
    "text": "this feature we just relax the API validations",
    "start": "689279",
    "end": "695660"
  },
  {
    "text": "so you see when the job is suspended we can enjoy this schedule Gates",
    "start": "696180",
    "end": "702620"
  },
  {
    "text": "and also Q is a sub project mostly sponsored by the scheduling take",
    "start": "703560",
    "end": "709740"
  },
  {
    "text": "advantage takes advantage of this feature for job query",
    "start": "709740",
    "end": "714680"
  },
  {
    "text": "so the next feature is about the mutable product scheduling directives when propagated so it's quite similar to the",
    "start": "715860",
    "end": "723060"
  },
  {
    "text": "previous one but it's for Port only and yes the motivation is quite similar",
    "start": "723060",
    "end": "728880"
  },
  {
    "text": "so it's going to work a lot of controllers can influence support placement with this feature",
    "start": "728880",
    "end": "735899"
  },
  {
    "text": "the the general idea is when when we found a port is gated by the scaling",
    "start": "735899",
    "end": "741899"
  },
  {
    "text": "Gates then we can inject the node selector and the node affinity but one thing I want to add here is not",
    "start": "741899",
    "end": "749220"
  },
  {
    "text": "any notice letter or not Affinity is allowed here because we",
    "start": "749220",
    "end": "754640"
  },
  {
    "text": "at a higher level so if there's another selector or another Infinity is empty",
    "start": "754640",
    "end": "760440"
  },
  {
    "text": "then anything is okay but if not only more restricted updates are allowed that",
    "start": "760440",
    "end": "766680"
  },
  {
    "text": "means uh the the node candidates should be a",
    "start": "766680",
    "end": "771720"
  },
  {
    "text": "subset of the original nodes yeah",
    "start": "771720",
    "end": "776360"
  },
  {
    "text": "so if if you have some customer jobs which has no suspended semantics so you",
    "start": "776820",
    "end": "783540"
  },
  {
    "text": "can also achieve the same behavior as kubernetes native job with this feature",
    "start": "783540",
    "end": "789379"
  },
  {
    "text": "uh there is a political spread plugin so this plugin helps to control pores",
    "start": "791519",
    "end": "797639"
  },
  {
    "text": "spreading across all the Clusters and we did a lot of work in the past",
    "start": "797639",
    "end": "802980"
  },
  {
    "text": "several races but I want but but I will not go into",
    "start": "802980",
    "end": "808680"
  },
  {
    "text": "that detail because we already did that in the last group called North America",
    "start": "808680",
    "end": "814440"
  },
  {
    "text": "so the first one is about the main domain it's a new field in under the to",
    "start": "814440",
    "end": "820980"
  },
  {
    "text": "produce spread the constraints so it helps to control the minimum number of two project domains and if there is not",
    "start": "820980",
    "end": "828420"
  },
  {
    "text": "enough domain the cluster Auto scale we are getting on board until there are",
    "start": "828420",
    "end": "833760"
  },
  {
    "text": "enough amounts and the second is about two new fees no",
    "start": "833760",
    "end": "840000"
  },
  {
    "text": "Affinity policy and another 10th policy so we can use these two fields to identify whether we should respect the",
    "start": "840000",
    "end": "847500"
  },
  {
    "text": "node Affinity or another 10 when calculating the spreading skill",
    "start": "847500",
    "end": "853320"
  },
  {
    "text": "so all these three figures are under the to produce spread constraints",
    "start": "853320",
    "end": "860600"
  },
  {
    "text": "and another feature is about the loading updates so in the before when a",
    "start": "861500",
    "end": "867480"
  },
  {
    "text": "deployment is loading update because the coupe schedule cannot distinguish with",
    "start": "867480",
    "end": "873000"
  },
  {
    "text": "other replicas and new replicas so when the other ones till the state will fall",
    "start": "873000",
    "end": "879480"
  },
  {
    "text": "into a balanced state once one solution for this is we can",
    "start": "879480",
    "end": "885480"
  },
  {
    "text": "deploy the schedule at the same time so if we use this feature enabled there",
    "start": "885480",
    "end": "892380"
  },
  {
    "text": "is another approach right now so this feature introduces a new Field named the mesh level kit here",
    "start": "892380",
    "end": "899880"
  },
  {
    "text": "it's a set of the level keys and it will not care about the level values so",
    "start": "899880",
    "end": "905880"
  },
  {
    "text": "for templates we can deploy a product template hash here so that for the",
    "start": "905880",
    "end": "910920"
  },
  {
    "text": "template hash is level added by the deployed control so it's child level",
    "start": "910920",
    "end": "917820"
  },
  {
    "text": "process will not overlap with each other",
    "start": "917820",
    "end": "922220"
  },
  {
    "text": "can use this you use this level keys to distinguish with otherwise and the new",
    "start": "924260",
    "end": "931139"
  },
  {
    "text": "ones and will follow into a balanced schedule results that's how it works generally",
    "start": "931139",
    "end": "938639"
  },
  {
    "text": "and yes we just uh I think we the blog about these three caps was just",
    "start": "938639",
    "end": "945199"
  },
  {
    "text": "published yesterday so you can if you have if you want to know more information about this you can clips",
    "start": "945199",
    "end": "953040"
  },
  {
    "text": "link for the details yeah",
    "start": "953040",
    "end": "957680"
  },
  {
    "text": "and beside these features we have several notable improvements at least to the three here so",
    "start": "958320",
    "end": "964500"
  },
  {
    "text": "the first one is about the skip status so if uh if if a pod has nothing to do",
    "start": "964500",
    "end": "973260"
  },
  {
    "text": "with a filter filter plugging then we think the filter plugin can be skipped",
    "start": "973260",
    "end": "978660"
  },
  {
    "text": "directly right let's take it let's take an example like a Paul carries no node",
    "start": "978660",
    "end": "984660"
  },
  {
    "text": "Affinity fields then the product sends the node Affinity plugin will have nothing to do with the",
    "start": "984660",
    "end": "992040"
  },
  {
    "text": "Pod so supplying can be skipped directly right now we can return a skip status in the",
    "start": "992040",
    "end": "999600"
  },
  {
    "text": "pre-filter exchanging point and the corresponding filter plugin can be executed it can be skipped",
    "start": "999600",
    "end": "1007459"
  },
  {
    "text": "foreign is quite similar to the previous one but it's for the",
    "start": "1007459",
    "end": "1013940"
  },
  {
    "text": "score extension point so you can also return skip statues in the",
    "start": "1013940",
    "end": "1020320"
  },
  {
    "text": "preschool extension point to skip the score probably yeah",
    "start": "1020320",
    "end": "1027220"
  },
  {
    "text": "the next one is about a new metric plugin evaluation total so if if if one",
    "start": "1028760",
    "end": "1036260"
  },
  {
    "text": "plugin is being called the value of the metric will plus over",
    "start": "1036260",
    "end": "1041480"
  },
  {
    "text": "so with this plugin we can have having an overview of how you plug in inflating",
    "start": "1041480",
    "end": "1047480"
  },
  {
    "text": "the schedule results I think it's it's quite useful",
    "start": "1047480",
    "end": "1052700"
  },
  {
    "text": "and then I handle handed over to Elder for several server projects updates yes",
    "start": "1052700",
    "end": "1060080"
  },
  {
    "text": "so a c scaling we our main component of course is the keep scheduler but we also",
    "start": "1060080",
    "end": "1066380"
  },
  {
    "text": "host a few sub-projects the first one is uh kind of not the",
    "start": "1066380",
    "end": "1072620"
  },
  {
    "text": "newest but one of the newest uh some projects it's a",
    "start": "1072620",
    "end": "1078380"
  },
  {
    "text": "uh soup component it's a a controller uh for uh job queuing",
    "start": "1078380",
    "end": "1085100"
  },
  {
    "text": "so we we offer a resource quota management uh with a borrowing and",
    "start": "1085100",
    "end": "1092059"
  },
  {
    "text": "preemption semantics um at the job level um",
    "start": "1092059",
    "end": "1098000"
  },
  {
    "text": "it also has a support for resource fungibility meaning that if you have an",
    "start": "1098000",
    "end": "1103700"
  },
  {
    "text": "heterogeneous cluster then you can you can establish quotas for different uh",
    "start": "1103700",
    "end": "1110780"
  },
  {
    "text": "resources for different flavors we call them in the cluster and then you can if",
    "start": "1110780",
    "end": "1116600"
  },
  {
    "text": "you submit a job you will decide if it fits in the in in certain quota it will",
    "start": "1116600",
    "end": "1123500"
  },
  {
    "text": "assign to that flavor otherwise it will assign to the next one that has available quota uh",
    "start": "1123500",
    "end": "1129980"
  },
  {
    "text": "we directly support the job API that we offer uh support for for MPI job as well",
    "start": "1129980",
    "end": "1137960"
  },
  {
    "text": "cubeflow and we are working on more Integrations we also uh published a",
    "start": "1137960",
    "end": "1144440"
  },
  {
    "text": "library for supporting any custom job crd that you might have",
    "start": "1144440",
    "end": "1150039"
  },
  {
    "text": "so you you can queue queued with q um here uh you might be wondering uh why",
    "start": "1150039",
    "end": "1159320"
  },
  {
    "text": "are we six scaling uh investing in a new uh scheduling uh",
    "start": "1159320",
    "end": "1166100"
  },
  {
    "text": "uh project um and the answer is that we are not we",
    "start": "1166100",
    "end": "1171740"
  },
  {
    "text": "we are not Reinventing Cube scheduler we uh we are following a separation of",
    "start": "1171740",
    "end": "1177340"
  },
  {
    "text": "concerns principle so Q can take decisions at the job level",
    "start": "1177340",
    "end": "1183200"
  },
  {
    "text": "um so it's a like let's let's call it a first scheduler and once the decision",
    "start": "1183200",
    "end": "1188419"
  },
  {
    "text": "has been taken for the entire job uh we delegate the rest of the scheduling to",
    "start": "1188419",
    "end": "1194780"
  },
  {
    "text": "the rest the existing components so Cube so Q is compatible with Cube scheduler",
    "start": "1194780",
    "end": "1200480"
  },
  {
    "text": "it's compatible with controller manager and it's also compatible with cluster autoscaler",
    "start": "1200480",
    "end": "1206660"
  },
  {
    "text": "uh and uh",
    "start": "1206660",
    "end": "1211880"
  },
  {
    "text": "so we just released the version 0.3 uh I think it was last week",
    "start": "1211880",
    "end": "1217880"
  },
  {
    "text": "um and some other highlights we uh we released the first beta quality API",
    "start": "1217880",
    "end": "1223940"
  },
  {
    "text": "meaning that from now we will respect uh the application policy established by",
    "start": "1223940",
    "end": "1229340"
  },
  {
    "text": "kubernetes um we increase some validation for better",
    "start": "1229340",
    "end": "1234679"
  },
  {
    "text": "use we added preemption support that was a big request from our users and",
    "start": "1234679",
    "end": "1242720"
  },
  {
    "text": "we finally have it for four different uh for different scenarios we added support",
    "start": "1242720",
    "end": "1248419"
  },
  {
    "text": "for MPI job uh the V1 beta 2 version of it we added this optional sequential",
    "start": "1248419",
    "end": "1255740"
  },
  {
    "text": "admission for quasi All or Nothing organ scheduling if you will uh we had a",
    "start": "1255740",
    "end": "1262880"
  },
  {
    "text": "support for limit ranges and runtime classes so we can better calculate usage",
    "start": "1262880",
    "end": "1268460"
  },
  {
    "text": "quota usage and we as I said uh we publish this library for integrating any",
    "start": "1268460",
    "end": "1275299"
  },
  {
    "text": "CRT with q if you want to learn more we did a session yesterday",
    "start": "1275299",
    "end": "1282160"
  },
  {
    "text": "on during the batch and HPC day you can find a link in the slides",
    "start": "1282160",
    "end": "1289100"
  },
  {
    "text": "hopefully the recording will be uploaded soon another project that we we host as",
    "start": "1289100",
    "end": "1294919"
  },
  {
    "text": "six scheduling is the the scheduler component which uh they just released uh",
    "start": "1294919",
    "end": "1303200"
  },
  {
    "text": "they just got a new logo so what is new in this scheduler uh first",
    "start": "1303200",
    "end": "1310340"
  },
  {
    "text": "of all they introduced the the V1 Alpha 2 API and the descaler framework which",
    "start": "1310340",
    "end": "1316700"
  },
  {
    "text": "uh is kind of a similar uh similar framework to the scheduler framework",
    "start": "1316700",
    "end": "1323600"
  },
  {
    "text": "um it's it's a plugin plugin based refactor of of the the project so more",
    "start": "1323600",
    "end": "1328640"
  },
  {
    "text": "people can more easily collaborate without without bumping into each other's features uh there is a new config API",
    "start": "1328640",
    "end": "1336880"
  },
  {
    "text": "the V1 Alpha One is deprecated and now they also introduce these scheduler",
    "start": "1336880",
    "end": "1342860"
  },
  {
    "text": "profiles similar as well to the to the scalware profiles in Cube scheduler I'll",
    "start": "1342860",
    "end": "1349299"
  },
  {
    "text": "expand on this a little bit in a bit um and they added this is a feature for the",
    "start": "1349299",
    "end": "1356539"
  },
  {
    "text": "node utilization plugin and now they have a namespace filter uh one inspiring thing that they wanted",
    "start": "1356539",
    "end": "1363200"
  },
  {
    "text": "to share is that they had more than 10 contributors in the latest release",
    "start": "1363200",
    "end": "1370000"
  },
  {
    "text": "so the Discover framework well what is the motivation that the project was",
    "start": "1370340",
    "end": "1375559"
  },
  {
    "text": "becoming too big too many contributors uh hopefully some of you here um",
    "start": "1375559",
    "end": "1381200"
  },
  {
    "text": "so and they all wanted to introduce new features new strategies and they were they were having uh issues managing",
    "start": "1381200",
    "end": "1389600"
  },
  {
    "text": "those changes so they decided to follow a similar pattern to keep scheduler uh",
    "start": "1389600",
    "end": "1395600"
  },
  {
    "text": "introduce the the this descaler framework it works a little different from the scalar framework and also a few",
    "start": "1395600",
    "end": "1403880"
  },
  {
    "text": "different names so let's say we have a profile here each bot is um",
    "start": "1403880",
    "end": "1409640"
  },
  {
    "text": "each spot is passed through all of these plugins each plugin implements Resort",
    "start": "1409640",
    "end": "1414980"
  },
  {
    "text": "sword and filter and they go through it there are two types of",
    "start": "1414980",
    "end": "1420280"
  },
  {
    "text": "plugins the D schedule which is kind of like observing one node at a time or",
    "start": "1420280",
    "end": "1425960"
  },
  {
    "text": "sorry one port at a time and you have the balance which is observing kind of like the entire",
    "start": "1425960",
    "end": "1432380"
  },
  {
    "text": "cluster at a time to take decisions so this could take decisions about let's say how long a pod has been running and",
    "start": "1432380",
    "end": "1439760"
  },
  {
    "text": "the balance can take a decision about like uh is my deployment spread so",
    "start": "1439760",
    "end": "1446659"
  },
  {
    "text": "and that's one profile each Port runs through every profile and if there is if",
    "start": "1446659",
    "end": "1453380"
  },
  {
    "text": "the profiles are different so the first profile can have can filter specific parts and the next profile can filter",
    "start": "1453380",
    "end": "1460580"
  },
  {
    "text": "other types of bots so that's that's the idea um",
    "start": "1460580",
    "end": "1466039"
  },
  {
    "text": "the scheduler uh project is uh looking is asking for use cases so if you have",
    "start": "1466039",
    "end": "1472280"
  },
  {
    "text": "any more uh use cases for Des scheduling um feel free to reach out uh and if you",
    "start": "1472280",
    "end": "1479720"
  },
  {
    "text": "want to learn more about the this color framework you can find a link down there to the original design",
    "start": "1479720",
    "end": "1486559"
  },
  {
    "text": "uh the next project is the schedule plugins this is a repository out of tree",
    "start": "1486559",
    "end": "1492679"
  },
  {
    "text": "uh so um there is basically one Cube scheduler in",
    "start": "1492679",
    "end": "1499520"
  },
  {
    "text": "kubernetes but we wanted to give the opportunity to the community to be able",
    "start": "1499520",
    "end": "1505039"
  },
  {
    "text": "to incubate uh certain plugins um with uh",
    "start": "1505039",
    "end": "1512000"
  },
  {
    "text": "with us a high quality as possible so um that's why we introduced this this",
    "start": "1512000",
    "end": "1518480"
  },
  {
    "text": "repo and there are quite some experimental and also quite production",
    "start": "1518480",
    "end": "1524720"
  },
  {
    "text": "ready uh plugins in here so this is kind",
    "start": "1524720",
    "end": "1529940"
  },
  {
    "text": "of like a staging uh project if uh if a plugging materials enough we",
    "start": "1529940",
    "end": "1538279"
  },
  {
    "text": "will consider the uh upstreaming to keep scheduler but it still needs to go",
    "start": "1538279",
    "end": "1543679"
  },
  {
    "text": "through the through the production well the cap the cap process and production Readiness",
    "start": "1543679",
    "end": "1549620"
  },
  {
    "text": "um reviews anyways I want to highlight uh the the plugins that there are are there today",
    "start": "1549620",
    "end": "1556360"
  },
  {
    "text": "uh starting with the most popular one probably the cost scheduling plugin which offers uh uh All or Nothing",
    "start": "1556360",
    "end": "1563480"
  },
  {
    "text": "scheduling scheduling a group of bots at the same time um I've been told that uh by the",
    "start": "1563480",
    "end": "1570320"
  },
  {
    "text": "contributors that open AI is one of the users of this of this plugin",
    "start": "1570320",
    "end": "1576679"
  },
  {
    "text": "the next one maybe this no resource topology it's about uh adding topology",
    "start": "1576679",
    "end": "1582380"
  },
  {
    "text": "awareness to scheduler uh topology inside the node like pneuma nodes for",
    "start": "1582380",
    "end": "1589220"
  },
  {
    "text": "example um capacity scheduling it's about elastic",
    "start": "1589220",
    "end": "1594440"
  },
  {
    "text": "quarters preemption Toleration it's variation on the preemption uh plugin",
    "start": "1594440",
    "end": "1602059"
  },
  {
    "text": "that offers certain um uh opting or up down opt out uh",
    "start": "1602059",
    "end": "1607460"
  },
  {
    "text": "capabilities and this the next one is trim around as it's actually a set of plugins which are kind of log aware",
    "start": "1607460",
    "end": "1616340"
  },
  {
    "text": "um scheduling plugins and uh the network our schedule implying as well",
    "start": "1616340",
    "end": "1622640"
  },
  {
    "text": "major changes in the the last few months a new component config API",
    "start": "1622640",
    "end": "1629539"
  },
  {
    "text": "similar updates to the no resources no resources topology topology match now",
    "start": "1629539",
    "end": "1634700"
  },
  {
    "text": "there is a scoring strategy uh new scoring is ready for these number nodes and there is an implementation of",
    "start": "1634700",
    "end": "1641000"
  },
  {
    "text": "reservations that aims to reduce conflicts with cubelet",
    "start": "1641000",
    "end": "1646179"
  },
  {
    "text": "and the apis for pod group and elasticquote announcer status so you can",
    "start": "1646179",
    "end": "1651320"
  },
  {
    "text": "subscribe to those events and more importantly there are breaking",
    "start": "1651320",
    "end": "1656659"
  },
  {
    "text": "changes in the next the newest release we change the API Group into",
    "start": "1656659",
    "end": "1662679"
  },
  {
    "text": "scheduling.x kubernetes dot IO so if you upgrade to the newest version you will have to uh update your crds",
    "start": "1662679",
    "end": "1671539"
  },
  {
    "text": "um yes okay so next project is about the cool scheduler simulator",
    "start": "1671539",
    "end": "1678679"
  },
  {
    "text": "so it can help to simulating the kubernetes schedule without a real class the more more importantly it can display",
    "start": "1678679",
    "end": "1685880"
  },
  {
    "text": "a scheduled decisions in detail so the major changes in the last list is about",
    "start": "1685880",
    "end": "1691520"
  },
  {
    "text": "we can display the scheduled results across all the extension points so but",
    "start": "1691520",
    "end": "1696679"
  },
  {
    "text": "it was in the before we can only support filter and score extension points",
    "start": "1696679",
    "end": "1702679"
  },
  {
    "text": "and one thing I want to mention here is a simulator has a web UI but up to now",
    "start": "1702679",
    "end": "1708799"
  },
  {
    "text": "we only support filter and score extension point for other extension points results you",
    "start": "1708799",
    "end": "1715520"
  },
  {
    "text": "have to revert you have to refer to the Pod annotations",
    "start": "1715520",
    "end": "1720799"
  },
  {
    "text": "and the next we are con where we will try to implement the similar based",
    "start": "1720799",
    "end": "1726620"
  },
  {
    "text": "simulation and Benchmark you can refer to the cap for more details so this is the general how it looks like",
    "start": "1726620",
    "end": "1734179"
  },
  {
    "text": "so we can see we have all the extension points scheduled results in the port",
    "start": "1734179",
    "end": "1740299"
  },
  {
    "text": "annotations yeah okay the next project is about",
    "start": "1740299",
    "end": "1747279"
  },
  {
    "text": "kubernetes without the couplet so this is a new project it's a toolkit to build",
    "start": "1747279",
    "end": "1753140"
  },
  {
    "text": "up a cluster and it can create thousands of nodes in seconds",
    "start": "1753140",
    "end": "1758360"
  },
  {
    "text": "yes it's lightweight is fast and it's also flexible so from the test from the",
    "start": "1758360",
    "end": "1764779"
  },
  {
    "text": "test report it can maintain thousands of nodes and 100 000 of nodes assumed as",
    "start": "1764779",
    "end": "1771740"
  },
  {
    "text": "hundreds of thousands of ports per laptop is quite a large amount of resources",
    "start": "1771740",
    "end": "1777880"
  },
  {
    "text": "and more information you can refer to the blog we are happy to announce that we just released the first version",
    "start": "1777919",
    "end": "1786460"
  },
  {
    "text": "and we have beyond the 1000 GitHub status is quite",
    "start": "1786500",
    "end": "1792440"
  },
  {
    "text": "popular so uh actually the architecture of the",
    "start": "1792440",
    "end": "1798620"
  },
  {
    "text": "clock is not quite complex there is no equivalent there is no node there is no data plan so all of this",
    "start": "1798620",
    "end": "1806000"
  },
  {
    "text": "will be marked by the cool cork so to achieve this cork has two controllers so",
    "start": "1806000",
    "end": "1811760"
  },
  {
    "text": "another control and the product control the node controller will responsible for simulating the another left Circle and",
    "start": "1811760",
    "end": "1818360"
  },
  {
    "text": "it also maintaining the node heartbeat to the API server",
    "start": "1818360",
    "end": "1825520"
  },
  {
    "text": "it's responsible for simulating the post lab Circle so you can Mark any node any port and as",
    "start": "1827080",
    "end": "1836679"
  },
  {
    "text": "any status at any time it's quite flexible because it can show this",
    "start": "1836679",
    "end": "1842299"
  },
  {
    "text": "because in Cork everything is API objects",
    "start": "1842299",
    "end": "1847600"
  },
  {
    "text": "yes Coke has all cork also has a client tool like Coke control it's as friendly",
    "start": "1848000",
    "end": "1854000"
  },
  {
    "text": "as canned so we think clock can cover several use",
    "start": "1854000",
    "end": "1860659"
  },
  {
    "text": "cases like the scheduling simulation so one interesting here is the Google",
    "start": "1860659",
    "end": "1866840"
  },
  {
    "text": "schedule simulator is planning to integrate with the clock that's quite interesting",
    "start": "1866840",
    "end": "1873260"
  },
  {
    "text": "and for scalability tests like if you want to have a performance for like five",
    "start": "1873260",
    "end": "1879500"
  },
  {
    "text": "thousand nodes you will no longer need a 5000 real nodes you can you you can use cork to and build up there and build up",
    "start": "1879500",
    "end": "1886520"
  },
  {
    "text": "them in your laptop and also for integration with the class also scalar you are no longer needed to",
    "start": "1886520",
    "end": "1893419"
  },
  {
    "text": "jump to the cloud providers you can boost up them in your laptop also",
    "start": "1893419",
    "end": "1899299"
  },
  {
    "text": "and yes functionality tests if you are developing a crd and you want to verify",
    "start": "1899299",
    "end": "1904700"
  },
  {
    "text": "whether the CID behave is aware so you can use clock okay pretty much for this session and I",
    "start": "1904700",
    "end": "1912260"
  },
  {
    "text": "guess someone must be interested with like join the community so if you are a",
    "start": "1912260",
    "end": "1918740"
  },
  {
    "text": "beginner you can refer to the first school issue or have a warranty issue and if you have any questions please",
    "start": "1918740",
    "end": "1925159"
  },
  {
    "text": "join your select we are all there and also we have bi-weekly meeting if you want to get more involved you can",
    "start": "1925159",
    "end": "1931640"
  },
  {
    "text": "talk with like the tech leader the coaches and yes we have a pen meeting",
    "start": "1931640",
    "end": "1937700"
  },
  {
    "text": "for the coach results from other Solutions if you want to know more about the",
    "start": "1937700",
    "end": "1944120"
  },
  {
    "text": "architecture of the Google schedule or you want to know how the officials are designed you can refer to the caps and",
    "start": "1944120",
    "end": "1951140"
  },
  {
    "text": "development dogs so last but not least thanks for all the contributors in the last list and yes we",
    "start": "1951140",
    "end": "1958279"
  },
  {
    "text": "are calling out for reveals since so the last part connect any questions",
    "start": "1958279",
    "end": "1966279"
  },
  {
    "text": "oh this is the microphone",
    "start": "1977120",
    "end": "1983200"
  },
  {
    "text": "thanks so uh thanks for the great presentation I wanted to ask about the features that allow delaying scheduling",
    "start": "1989960",
    "end": "1998539"
  },
  {
    "text": "of both so the scheduling Gates and the suspended pots so is there some sort of",
    "start": "1998539",
    "end": "2004299"
  },
  {
    "text": "status condition or time stamp well this status changes to pod becoming scheduleable I'm asking because for",
    "start": "2004299",
    "end": "2011380"
  },
  {
    "text": "example in Auto scaling the scheduling latency is is a major thing we monitor",
    "start": "2011380",
    "end": "2017260"
  },
  {
    "text": "and I'm wondering if if there is now a bitterly long time report scheduling can",
    "start": "2017260",
    "end": "2022960"
  },
  {
    "text": "be delayed is there is there a way for me to actually monitor the actual sort of",
    "start": "2022960",
    "end": "2030580"
  },
  {
    "text": "pod going through the system there is a pod condition uh",
    "start": "2030580",
    "end": "2035620"
  },
  {
    "text": "we I if I remember correctly it's called schedule scheduling gated so that it contains a timestamp",
    "start": "2035620",
    "end": "2042490"
  },
  {
    "text": "[Music] um but uh yeah you will you'll have to subscribe to that to be able to measure",
    "start": "2042490",
    "end": "2048820"
  },
  {
    "text": "like measure this allows right or slas uh thank you that's great thanks thank",
    "start": "2048820",
    "end": "2053980"
  },
  {
    "text": "you very much yes",
    "start": "2053980",
    "end": "2057419"
  },
  {
    "text": "I just wonder if there's a strategy or Plugin which allows to put parts dense",
    "start": "2060220",
    "end": "2065440"
  },
  {
    "text": "on notes so that they're not spread across nodes but that we that we utilize the nodes at the maximum Point yes uh",
    "start": "2065440",
    "end": "2073300"
  },
  {
    "text": "that's one of the modes for the uh no resources",
    "start": "2073300",
    "end": "2078638"
  },
  {
    "text": "no resources utilization plugin uh I don't remember the name at this time but",
    "start": "2078639",
    "end": "2084700"
  },
  {
    "text": "you can configure it to uh Target uh maximize utilization or",
    "start": "2084700",
    "end": "2091740"
  },
  {
    "text": "minimal utilization for spreading the default is minimum utilization so by",
    "start": "2091740",
    "end": "2097599"
  },
  {
    "text": "default Cube scalar spreads yeah that's that can be a problem because then you",
    "start": "2097599",
    "end": "2102880"
  },
  {
    "text": "get unutilized nodes and they cannot scale back because they know the ports are pinning the nodes yes uh",
    "start": "2102880",
    "end": "2110260"
  },
  {
    "text": "so some providers uh you know in gke we provide a multiple scheduling profiles",
    "start": "2110260",
    "end": "2116440"
  },
  {
    "text": "so you can uh you can basically select the description profile",
    "start": "2116440",
    "end": "2121599"
  },
  {
    "text": "that fits your use case better thank you so that that's that's the the",
    "start": "2121599",
    "end": "2128440"
  },
  {
    "text": "recommendation for cloud providers to provide",
    "start": "2128440",
    "end": "2134640"
  },
  {
    "text": "uh one okay in the back",
    "start": "2142900",
    "end": "2146338"
  },
  {
    "text": "a follow-up question to that um how does it compare to the D scheduler it could",
    "start": "2148780",
    "end": "2154780"
  },
  {
    "text": "also help with maximizing bin packing right by clearing out yes",
    "start": "2154780",
    "end": "2161640"
  },
  {
    "text": "unutilized notes no it's not about cleaning up it's the question was about scheduling at scheduling time to",
    "start": "2161640",
    "end": "2168640"
  },
  {
    "text": "maximize utilization uh and the I guess the benefit about",
    "start": "2168640",
    "end": "2175060"
  },
  {
    "text": "doing it during scheduling time is that well there's no preemptions no disruptions to the pods but of course yeah you can",
    "start": "2175060",
    "end": "2182680"
  },
  {
    "text": "still pair you can still deploy together scheduler anti-scheduler for maximum defragmentation",
    "start": "2182680",
    "end": "2191680"
  },
  {
    "text": "hi thanks for the talk a question about Q uh so you mentioned that decision is",
    "start": "2195280",
    "end": "2201700"
  },
  {
    "text": "made on the higher level for the job itself basically decision to schedule",
    "start": "2201700",
    "end": "2207220"
  },
  {
    "text": "not to schedule is done on the job how then preemption works uh so yeah so that's uh based on on the",
    "start": "2207220",
    "end": "2216099"
  },
  {
    "text": "suspend field of the the job uh the job the job API has a suspend feel uh that Q",
    "start": "2216099",
    "end": "2223300"
  },
  {
    "text": "leverages to uh either start or uh preempt a job as a whole so",
    "start": "2223300",
    "end": "2230440"
  },
  {
    "text": "the the the point here is that Q takes the decision at the job level and then it's the job controller that actually",
    "start": "2230440",
    "end": "2237579"
  },
  {
    "text": "executes the deletions and then cubelet handles the graceful termination and so",
    "start": "2237579",
    "end": "2242920"
  },
  {
    "text": "on um so basically the approach with taking in",
    "start": "2242920",
    "end": "2248740"
  },
  {
    "text": "queue is that anytime Upstream kubernetes doesn't work for us we go and change it",
    "start": "2248740",
    "end": "2254980"
  },
  {
    "text": "we talk we talk to sigaps we talk to um to see out scaling to any signal and we",
    "start": "2254980",
    "end": "2264280"
  },
  {
    "text": "work with them to uh to make this communication Channel or this like",
    "start": "2264280",
    "end": "2270280"
  },
  {
    "text": "separation of concerns work work well with us so as a result we might have a",
    "start": "2270280",
    "end": "2275980"
  },
  {
    "text": "little of a slower release but we are here for the long term",
    "start": "2275980",
    "end": "2281320"
  },
  {
    "text": "thank you yes and you can also refer to this session we uh yesterday about the",
    "start": "2281320",
    "end": "2287619"
  },
  {
    "text": "queue and the more detail about the player machine yeah thanks I for sure will review it thanks",
    "start": "2287619",
    "end": "2293578"
  },
  {
    "text": "all right",
    "start": "2295300",
    "end": "2297960"
  },
  {
    "text": "all right thank you steps everyone",
    "start": "2301359",
    "end": "2306720"
  }
]