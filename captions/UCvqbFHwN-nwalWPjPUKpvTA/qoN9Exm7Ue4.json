[
  {
    "text": "hello folks Welcome to our talk today we'll be sharing our story in running",
    "start": "840",
    "end": "6919"
  },
  {
    "text": "10,000 Argos applications and sharing our journey in tuning the performance of our Argus CID",
    "start": "6919",
    "end": "13400"
  },
  {
    "text": "instance my name is giri and this is my colag Udi both of us are infra Engineers",
    "start": "13400",
    "end": "20039"
  },
  {
    "text": "from go goto Financial goto Financial is a financial arm part of financial arm",
    "start": "20039",
    "end": "25880"
  },
  {
    "text": "part of uh goto group the leading digital ecosystem in Indonesia we provide various service offerings from",
    "start": "25880",
    "end": "33079"
  },
  {
    "text": "ride hailing service using motorcycle food delivery service package delivery service e-commerce platform and many",
    "start": "33079",
    "end": "39399"
  },
  {
    "text": "other services to start off um I'm going to give you a brief uh overview of the",
    "start": "39399",
    "end": "46640"
  },
  {
    "text": "state of current of our kubernetes and Argo City we maintain around 50 kubernetes",
    "start": "46640",
    "end": "54239"
  },
  {
    "text": "clusters across NWS gcp and private data center in Singapore and Indonesia region",
    "start": "54239",
    "end": "60440"
  },
  {
    "text": "this consists of 700 compute notes 15,000 CPUs 120 TB memories and more",
    "start": "60440",
    "end": "66600"
  },
  {
    "text": "than 30,000 pods this Argo City dashboard uh",
    "start": "66600",
    "end": "72119"
  },
  {
    "text": "snapshot we took uh two weeks back there's an interesting story behind this",
    "start": "72119",
    "end": "78000"
  },
  {
    "text": "snapshot when we were preparing for cubec con proposal this St few months",
    "start": "78000",
    "end": "83400"
  },
  {
    "text": "back we were at uh 7,000 aru CD apps and looking at the growth rate of our agdf",
    "start": "83400",
    "end": "89920"
  },
  {
    "text": "app creation we predicted today we would reach above 10,000 that's why the title 10,000 arusd",
    "start": "89920",
    "end": "98000"
  },
  {
    "text": "apps and today we made it it's now uh 11,000 arusd",
    "start": "98000",
    "end": "103479"
  },
  {
    "text": "applications these 11,000 applications are coming from 6,000 repositories",
    "start": "103479",
    "end": "109560"
  },
  {
    "text": "across 60 different projects Argo City watch uh more than 30 380,000 total",
    "start": "109560",
    "end": "116600"
  },
  {
    "text": "objects on our largest cluster we run 2,000 applications and Argo CD watch",
    "start": "116600",
    "end": "121960"
  },
  {
    "text": "over 40,000 objects we adopted a simple centralized Argo CD instance model which is",
    "start": "121960",
    "end": "129119"
  },
  {
    "text": "technically a push model or some of you refer it as Hub and spoke model in hubit",
    "start": "129119",
    "end": "134959"
  },
  {
    "text": "Spoke model we have management cluster where we run our single Argo CD instance",
    "start": "134959",
    "end": "141080"
  },
  {
    "text": "this Argo CD instance reads a common git repository sorry git uh git providers",
    "start": "141080",
    "end": "146920"
  },
  {
    "text": "and then using this uh the the the manifest stored in G repository the aroid instance push objects across all",
    "start": "146920",
    "end": "154920"
  },
  {
    "text": "the Clusters that get registered under aroid instance there's couple of benefits with",
    "start": "154920",
    "end": "161800"
  },
  {
    "text": "this simple centralized Argus cidd instance it's very easy to maintain and",
    "start": "161800",
    "end": "166920"
  },
  {
    "text": "upgrade because we only need to upgrade one argocd instance",
    "start": "166920",
    "end": "172920"
  },
  {
    "text": "regularly it's very easy to integrate with our automation at platform internally we maintain a developer",
    "start": "172920",
    "end": "179200"
  },
  {
    "text": "platform that is tightly uh related to argd instance so maintaining only one",
    "start": "179200",
    "end": "184920"
  },
  {
    "text": "argd version makes it very simple to write our integration logic in the platform it's very easy to manage",
    "start": "184920",
    "end": "191319"
  },
  {
    "text": "centralized arback because everything is in one place and we have a nice single",
    "start": "191319",
    "end": "196360"
  },
  {
    "text": "dashboard to view entire Argo CD applications that we have across entire clusters that we man",
    "start": "196360",
    "end": "203400"
  },
  {
    "text": "manage internally we have a developer platform that became the primary interface of our product Engineers we",
    "start": "203400",
    "end": "210879"
  },
  {
    "text": "don't let product Engineers to create argd application by themselves so um",
    "start": "210879",
    "end": "216799"
  },
  {
    "text": "this developer platform leverage uh the standardized Helm charts that we maintain the platform team maintains and",
    "start": "216799",
    "end": "223239"
  },
  {
    "text": "the platform generates the Manifest push it to uh repository as well as generate",
    "start": "223239",
    "end": "228680"
  },
  {
    "text": "the aroid applications our developer platform has some sort of grouping mechanism so that sets of aroid",
    "start": "228680",
    "end": "235640"
  },
  {
    "text": "applications point to the same repository in some cases one AR CD application can also point to one",
    "start": "235640",
    "end": "243360"
  },
  {
    "text": "repository the design uh in our platform is that one service can contain three to",
    "start": "243360",
    "end": "249200"
  },
  {
    "text": "five argocd applications these sets of applications can have different different life cycle so for instance if",
    "start": "249200",
    "end": "256560"
  },
  {
    "text": "a user or product engineer create service in uh through our developer platform we would uh generate one Canary",
    "start": "256560",
    "end": "264560"
  },
  {
    "text": "application and one stable application the stable application points to uh let's say a stable",
    "start": "264560",
    "end": "270880"
  },
  {
    "text": "container image V1 and then the canar application points to uh an updated",
    "start": "270880",
    "end": "276199"
  },
  {
    "text": "version of image for instance V2 the separations of application makes it very easy for us to perform our canar rollup",
    "start": "276199",
    "end": "284039"
  },
  {
    "text": "strategy and do promotion to the stable and uh canar roll back if necessary",
    "start": "284039",
    "end": "291280"
  },
  {
    "text": "makes it very easy now in our platform we enabled sto by default which means we inject esto side cars into every pot",
    "start": "291280",
    "end": "299120"
  },
  {
    "text": "that we make that we manage in our clusters we manage uho sidecar objects or this uho uh",
    "start": "299120",
    "end": "306280"
  },
  {
    "text": "sidecar configurations as a separate application so things like viral service object destination rule object are",
    "start": "306280",
    "end": "312080"
  },
  {
    "text": "maintained separately in another application in this this application we configure the traffic routing let's say",
    "start": "312080",
    "end": "318759"
  },
  {
    "text": "canar 5% or stable 95% the reason of this separation is is that we um each of",
    "start": "318759",
    "end": "325639"
  },
  {
    "text": "the application we can control it independently and in some cases if the service wants to expose domains to",
    "start": "325639",
    "end": "331800"
  },
  {
    "text": "public or third party Partners we expose it we control it uh we make configurations through another",
    "start": "331800",
    "end": "337840"
  },
  {
    "text": "application uh the Theo Gateway application which contains Gateway",
    "start": "337840",
    "end": "342960"
  },
  {
    "text": "object another use case that we l argd a lot is to maintain our cluster runtime",
    "start": "342960",
    "end": "349000"
  },
  {
    "text": "components in this cluster runtime components we have standardized components across the 50 clusters that",
    "start": "349000",
    "end": "354759"
  },
  {
    "text": "we that we have we left argocd app set or application set and the app of apps",
    "start": "354759",
    "end": "360039"
  },
  {
    "text": "pattern on a monor repo so we only have one repository that manage and",
    "start": "360039",
    "end": "366000"
  },
  {
    "text": "configures runtime components for all 50 clusters that we have in the root of repository we",
    "start": "366000",
    "end": "372199"
  },
  {
    "text": "maintain the the the root apps set which generates the parent app of each cluster so in this example there's cluster one",
    "start": "372199",
    "end": "379039"
  },
  {
    "text": "parent app each parent app manages the base cluster configuration as an app as",
    "start": "379039",
    "end": "384680"
  },
  {
    "text": "well as the runtime component as an app set this base app uh contains like basic",
    "start": "384680",
    "end": "390520"
  },
  {
    "text": "configurations like arbac or limit range and so on that are standardized across all the 50 clusters that we",
    "start": "390520",
    "end": "398120"
  },
  {
    "text": "have if um if we need to apply uh if we",
    "start": "398120",
    "end": "403240"
  },
  {
    "text": "need to customize the cluster beyond what is standardized we do it through another application which is uh server s",
    "start": "403240",
    "end": "410520"
  },
  {
    "text": "side apply patches application these sets of uh applications are replicated",
    "start": "410520",
    "end": "415919"
  },
  {
    "text": "for the rest of the Clusters that we have and by the way way we also manage Argo CD on Argo",
    "start": "415919",
    "end": "422960"
  },
  {
    "text": "CD there are few challenges with this simple centralized Argo CD",
    "start": "422960",
    "end": "428400"
  },
  {
    "text": "instance first challenge is this Argo CD instance require connectivity to all the",
    "start": "428400",
    "end": "433440"
  },
  {
    "text": "target clusters that it manages so the consequence of this is we need to establish tunnels or peering",
    "start": "433440",
    "end": "439919"
  },
  {
    "text": "connectivity between the management clusters and the entire workload Target clusters if and sometimes it's not",
    "start": "439919",
    "end": "446840"
  },
  {
    "text": "always possible to establish tunnels and peering we do it through a public network over",
    "start": "446840",
    "end": "452039"
  },
  {
    "text": "mtls another challenge is the limitation of Argo CD functionality first in Argo C we need to",
    "start": "452039",
    "end": "459759"
  },
  {
    "text": "maintain unique application name globally there's no separate uh so even",
    "start": "459759",
    "end": "465000"
  },
  {
    "text": "even for different different projects the application name cannot be the same for us luckily we always generate",
    "start": "465000",
    "end": "471919"
  },
  {
    "text": "application name from the platform so we we can have some sort of convention uh",
    "start": "471919",
    "end": "477440"
  },
  {
    "text": "to add let's say the customer name or team name as a suffix or prefix of uh aroid application name so each team um",
    "start": "477440",
    "end": "484319"
  },
  {
    "text": "in our platform can have conflicting namings and then uh the next challenge",
    "start": "484319",
    "end": "489479"
  },
  {
    "text": "is this uh centralized Aro CD instance is a single point of",
    "start": "489479",
    "end": "494599"
  },
  {
    "text": "failure another challenge that we encountered a lot is performance issues of central Argo CD instance along the",
    "start": "494599",
    "end": "502000"
  },
  {
    "text": "way we en encountered slow reconciliation problem and sync issues we look at the argocd or Q dep metrics",
    "start": "502000",
    "end": "510039"
  },
  {
    "text": "and the app reconcile metrics for this I'm pretty sure a lot of you are also experiencing this if you manage uh",
    "start": "510039",
    "end": "518399"
  },
  {
    "text": "more than thousand applications the UI starts loading very very slowly for us it took uh around one or two minutes to",
    "start": "518399",
    "end": "525240"
  },
  {
    "text": "load the entire homepage of arili dashboard this is quite obvious and we got a lot of complaints from the product",
    "start": "525240",
    "end": "531839"
  },
  {
    "text": "Engineers we Face frequent repo server o kills by looking at the cube events we",
    "start": "531839",
    "end": "538079"
  },
  {
    "text": "have an alert for this we Face a high rate of our git API calls",
    "start": "538079",
    "end": "544600"
  },
  {
    "text": "for both LS remote and fetch we look at git request metrics for this and we",
    "start": "544600",
    "end": "550600"
  },
  {
    "text": "encountered High repo catch Miss in the repo server by looking at the repo server locks it turns out later on we",
    "start": "550600",
    "end": "557160"
  },
  {
    "text": "found out that um this High repo catmas is the root cause of why our Argo CD is",
    "start": "557160",
    "end": "562440"
  },
  {
    "text": "making very high rate of git API calls and in our controller sharts we see",
    "start": "562440",
    "end": "567640"
  },
  {
    "text": "imbalanced resource consumption by each charts and and noisy cluster problem next uni is going to give us",
    "start": "567640",
    "end": "574880"
  },
  {
    "text": "walk through on our journey in tuning the performance of our Argo CD",
    "start": "574880",
    "end": "581200"
  },
  {
    "text": "instance thanks G now let's talk about our Performance Tuning journey in the next 20 minutes we'll discuss all the",
    "start": "582760",
    "end": "590000"
  },
  {
    "text": "config and parameter tunings we have done so far to support and scale our AR CD uh for 11k plus apps as a note the",
    "start": "590000",
    "end": "598760"
  },
  {
    "text": "tuning in this presentation are not in chronological order of when we implemented them instead we'll group",
    "start": "598760",
    "end": "605839"
  },
  {
    "text": "them into uh each of their own components so first let's take a look at",
    "start": "605839",
    "end": "612640"
  },
  {
    "text": "argu CD components so we'll use this diagram from the official Argo CID documentation in Argo cidd there are",
    "start": "612640",
    "end": "619760"
  },
  {
    "text": "four layers first layer uh we have the UI layer uh which is mainly uh for user",
    "start": "619760",
    "end": "625720"
  },
  {
    "text": "interaction here we have the web app uh and CLI the web app we uh use with our",
    "start": "625720",
    "end": "631600"
  },
  {
    "text": "web browser and CLI through allows us to interact with Argo CD through the terminal next we have the application",
    "start": "631600",
    "end": "638600"
  },
  {
    "text": "layer uh which consists of uh the API server or we also call it argu CD server",
    "start": "638600",
    "end": "644959"
  },
  {
    "text": "uh it serves API request from the UI layer and then uh next we have the core",
    "start": "644959",
    "end": "651920"
  },
  {
    "text": "uh layer uh these components are the main uh Argo CD functionality components",
    "start": "651920",
    "end": "657920"
  },
  {
    "text": "here we have app controller upset controller and repos server the app controller uh mainly uh reconciles and",
    "start": "657920",
    "end": "666839"
  },
  {
    "text": "uh synchronizes kubernetes objects uh according to their state in the repo and",
    "start": "666839",
    "end": "673560"
  },
  {
    "text": "then we have upset controller which is to generate applications based on templates and then uh we have the repo",
    "start": "673560",
    "end": "680320"
  },
  {
    "text": "serer which receives manifest generation request from the Argo CD server and the",
    "start": "680320",
    "end": "686200"
  },
  {
    "text": "amp controller and then uh next we have the infr layer uh Argo CD depends on",
    "start": "686200",
    "end": "692360"
  },
  {
    "text": "these components uh for its functionality we have Ries for caching Cube API to uh watch and uh apply Cube",
    "start": "692360",
    "end": "701320"
  },
  {
    "text": "objects and get Helm or customized repos and also deck for",
    "start": "701320",
    "end": "708440"
  },
  {
    "text": "authentication okay now let's uh start first with argu city server so as G mentioned before uh we",
    "start": "708440",
    "end": "716880"
  },
  {
    "text": "had very slow UI load uh so it could take anywhere between 15",
    "start": "716880",
    "end": "722920"
  },
  {
    "text": "seconds to 2 minutes for us to load the the Aro CD homepage uh depending on our",
    "start": "722920",
    "end": "729600"
  },
  {
    "text": "network connection so as a solution we enabled gzip compression feature uh in",
    "start": "729600",
    "end": "735839"
  },
  {
    "text": "Argo CD server so uh what we need is to set this environment variable to true so",
    "start": "735839",
    "end": "742959"
  },
  {
    "text": "uh in our case it improved load Time by uh in average by 5x and and uh data size",
    "start": "742959",
    "end": "750800"
  },
  {
    "text": "to seven times smaller next uh stillo Aro CD server and",
    "start": "750800",
    "end": "757800"
  },
  {
    "text": "it's actually not a tune but uh more of a tip for us the Argo CD UI users we can",
    "start": "757800",
    "end": "765519"
  },
  {
    "text": "actually use selectors to filter out only the applications we want to see",
    "start": "765519",
    "end": "770920"
  },
  {
    "text": "using their label values their projects or their name spaces uh and in this example we select",
    "start": "770920",
    "end": "779040"
  },
  {
    "text": "one project uh in one name space and we only show uh fraction of the the total",
    "start": "779040",
    "end": "786360"
  },
  {
    "text": "apps that we have uh which under uh our selectors here so uh one great thing is",
    "start": "786360",
    "end": "793639"
  },
  {
    "text": "that the selectors are safe the next time we load our argu CD Wii which is",
    "start": "793639",
    "end": "799480"
  },
  {
    "text": "quite handy next uh it's about kubernetes CPU",
    "start": "799480",
    "end": "806040"
  },
  {
    "text": "limits so this is actually not specific to Argo CD uh but more of a kubernetes mechanism",
    "start": "806040",
    "end": "814480"
  },
  {
    "text": "and it can be applied to other use cases uh outside C uh Argo CD so the problem",
    "start": "814480",
    "end": "819880"
  },
  {
    "text": "is that we noticed and uh through our monitoring uh system that all our Aro CD",
    "start": "819880",
    "end": "827279"
  },
  {
    "text": "components got throttled uh CPU throttled uh and uh it kind of slow had",
    "start": "827279",
    "end": "835120"
  },
  {
    "text": "an impact to our reconciliation uh latency so uh in kubernetes CPU request",
    "start": "835120",
    "end": "843600"
  },
  {
    "text": "and limits are implemented using c groups uh which uses the CFS or",
    "start": "843600",
    "end": "850279"
  },
  {
    "text": "completely fair schedular from the Linux kernel so uh CFS could guarantee or",
    "start": "850279",
    "end": "857600"
  },
  {
    "text": "could total container CPU depending on the proportion of the container CPU shares",
    "start": "857600",
    "end": "864240"
  },
  {
    "text": "or quota in in the N we we actually don't uh really have much time to talk",
    "start": "864240",
    "end": "869639"
  },
  {
    "text": "about more about uh this mechanism but if anyone is interested to have uh deeper look we attach some references so",
    "start": "869639",
    "end": "877399"
  },
  {
    "text": "uh as a solution to the CPU throttling we lifted our kubernetes limits we",
    "start": "877399",
    "end": "882680"
  },
  {
    "text": "decided to leave it and uh our app controller and uh other components uh",
    "start": "882680",
    "end": "888720"
  },
  {
    "text": "did not go throttled uh went throttled anymore next uh we move to the repo",
    "start": "888720",
    "end": "896000"
  },
  {
    "text": "serer so uh giri mentioned about oh I'm killed happened to our repo server so it",
    "start": "896000",
    "end": "903160"
  },
  {
    "text": "happened very frequently like then so as a solution we increase the replicas and",
    "start": "903160",
    "end": "910440"
  },
  {
    "text": "uh use HPA so that the repo serer Parts automatically scales with its memory",
    "start": "910440",
    "end": "915880"
  },
  {
    "text": "usage so uh with this we distribute more requests uh to into more pots so each",
    "start": "915880",
    "end": "923720"
  },
  {
    "text": "pot actually get uh less uh manifest generation request uh from the server",
    "start": "923720",
    "end": "930639"
  },
  {
    "text": "and app controller uh translating into less memory",
    "start": "930639",
    "end": "936880"
  },
  {
    "text": "usage or alternatively we can also use the uh parallelism limit flag on uh the",
    "start": "936880",
    "end": "943759"
  },
  {
    "text": "repo server to control how many manifest generation requests that can be served",
    "start": "943759",
    "end": "949079"
  },
  {
    "text": "uh in parallel to help avoid uh The Kills however there's one thread of of",
    "start": "949079",
    "end": "954160"
  },
  {
    "text": "this approach is that the throughput of manifest generation will also be low as we limit its",
    "start": "954160",
    "end": "962240"
  },
  {
    "text": "parallelism next uh so the Aro CD serfer and up controller",
    "start": "962600",
    "end": "970079"
  },
  {
    "text": "uh talks to repo serfer right they are the clients of repo serfer they uh talk to repo server for manifest generation",
    "start": "970079",
    "end": "977800"
  },
  {
    "text": "and uh they have timeout configurations so as we uh grow we started seeing uh",
    "start": "977800",
    "end": "986040"
  },
  {
    "text": "those clients timeout errors in our locks uh we see them when uh We sync and",
    "start": "986040",
    "end": "992920"
  },
  {
    "text": "refresh apps so as a solution we increase uh the configuration in both",
    "start": "992920",
    "end": "998880"
  },
  {
    "text": "the Argo CD server and the app controller one thing to note is that we really need to set on both uh components",
    "start": "998880",
    "end": "1006279"
  },
  {
    "text": "because I've seen people only setting it on uh app controller and uh Missing the",
    "start": "1006279",
    "end": "1013000"
  },
  {
    "text": "server so they still see uh the time out errors",
    "start": "1013000",
    "end": "1019839"
  },
  {
    "text": "next uh continuing on repos Surfer again uh so from our repo Surfer Matrix uh G",
    "start": "1019839",
    "end": "1026678"
  },
  {
    "text": "metric we consistently saw uh High very high git fetch requests so Argo CD",
    "start": "1026679",
    "end": "1035520"
  },
  {
    "text": "cashes generated manifest uh in redish and they uh have 24 hours expiry by",
    "start": "1035520",
    "end": "1044120"
  },
  {
    "text": "default so in uh cases when when remote change often even though the repository",
    "start": "1044120",
    "end": "1052200"
  },
  {
    "text": "T hasn't changed uh for example when we do get push force or when we update a",
    "start": "1052200",
    "end": "1060039"
  },
  {
    "text": "Helm chart with the same version uh shorter expiry uh time uh will be more",
    "start": "1060039",
    "end": "1066440"
  },
  {
    "text": "desirable uh to pick up those updates uh quicker uh faster than 24 hours but in",
    "start": "1066440",
    "end": "1072919"
  },
  {
    "text": "our use case uh our Helm uh customized and G remote uh references are already",
    "start": "1072919",
    "end": "1079240"
  },
  {
    "text": "hermatic so they uh we use the tack and we don't force push or uh force update",
    "start": "1079240",
    "end": "1085039"
  },
  {
    "text": "to attack so uh we can actually use higher uh value for uh the expired time",
    "start": "1085039",
    "end": "1092600"
  },
  {
    "text": "we can do it by setting this uh infir variable and after uh extending our",
    "start": "1092600",
    "end": "1098440"
  },
  {
    "text": "expiry time we uh immediately saw uh dramatic drop in our G request uh G",
    "start": "1098440",
    "end": "1106000"
  },
  {
    "text": "fetch request from the report server next uh we move on to monor repo so we",
    "start": "1106000",
    "end": "1112760"
  },
  {
    "text": "mentioned that we use uh app of apps pattern and the uh application Set uh in",
    "start": "1112760",
    "end": "1118559"
  },
  {
    "text": "our monor repo We additionally we also use the multisources apps feature uh of",
    "start": "1118559",
    "end": "1125240"
  },
  {
    "text": "Aro CD which allows us to have uh these uh multiple features uh when defining an app so uh",
    "start": "1125240",
    "end": "1134640"
  },
  {
    "text": "when we implemented this right after we immedately saw very high git Fetch and",
    "start": "1134640",
    "end": "1140280"
  },
  {
    "text": "git LS remote requests uh as seen in uh the two screenshots here so we",
    "start": "1140280",
    "end": "1147200"
  },
  {
    "text": "investigated it and uh found that it's potentially bug in Aro city so we implemented an an undocumented work",
    "start": "1147200",
    "end": "1154520"
  },
  {
    "text": "around for this which is uh can be accessed in details in this GitHub issue",
    "start": "1154520",
    "end": "1160159"
  },
  {
    "text": "and uh our GIF request Dro D dramatically after that uh which is",
    "start": "1160159",
    "end": "1165919"
  },
  {
    "text": "shown in at the right uh screenshot but we are still seeing High",
    "start": "1165919",
    "end": "1172000"
  },
  {
    "text": "uh LS remote request uh so we think this uh is still an open issue so if anyone",
    "start": "1172000",
    "end": "1178440"
  },
  {
    "text": "is interested uh please uh check out the GitHub issue still about monor repo uh so uh we",
    "start": "1178440",
    "end": "1186919"
  },
  {
    "text": "usually might want to use uh web Hook from our uh repository to notify argd so",
    "start": "1186919",
    "end": "1195320"
  },
  {
    "text": "argd can pick up uh updates uh faster uh every time we",
    "start": "1195320",
    "end": "1200720"
  },
  {
    "text": "commit in a monor repo uh Argo CD web hook server May refresh all applications",
    "start": "1200720",
    "end": "1208520"
  },
  {
    "text": "when it receive uh a web hook even though uh the news change uh might not",
    "start": "1208520",
    "end": "1215360"
  },
  {
    "text": "have any relation to uh all those applications at all uh it could be just",
    "start": "1215360",
    "end": "1220760"
  },
  {
    "text": "a subset uh of the parts that we really care of but uh the web hook will uh",
    "start": "1220760",
    "end": "1227799"
  },
  {
    "text": "refresh anyway all the apps in the refresh process Argo CD invalidates the",
    "start": "1227799",
    "end": "1233360"
  },
  {
    "text": "cash for all apps and calls kubernetes API to annotate all application objects",
    "start": "1233360",
    "end": "1239799"
  },
  {
    "text": "with a special annotation for Aro C refresh and uh this operation is a",
    "start": "1239799",
    "end": "1245840"
  },
  {
    "text": "network bound uh process which may slow the update process overall especially uh",
    "start": "1245840",
    "end": "1252240"
  },
  {
    "text": "when we start having uh more than 1,000 apps so uh we can actually filter out or",
    "start": "1252240",
    "end": "1260120"
  },
  {
    "text": "uh define specific Parts when we receive uh web hook so that Argo CD will only",
    "start": "1260120",
    "end": "1266559"
  },
  {
    "text": "refresh uh applications that are Chang uh in specific parts we can use uh this",
    "start": "1266559",
    "end": "1274080"
  },
  {
    "text": "special annotation called manifest generate parts and uh we Define the",
    "start": "1274080",
    "end": "1279559"
  },
  {
    "text": "specific Parts here after using this we spad up our uh refresh process after uh",
    "start": "1279559",
    "end": "1284799"
  },
  {
    "text": "getting a we request next let's move to the app",
    "start": "1284799",
    "end": "1290760"
  },
  {
    "text": "controller so one of the first uh problems that we had with app controller",
    "start": "1290760",
    "end": "1296600"
  },
  {
    "text": "is about uh work so uh our app controller work Q depth uh started to",
    "start": "1296600",
    "end": "1302720"
  },
  {
    "text": "pile up and it did not go down at all so uh we investigated and we decided to",
    "start": "1302720",
    "end": "1310520"
  },
  {
    "text": "increase the number of operation processes and Status processors uh an app controller because uh there are the",
    "start": "1310520",
    "end": "1317760"
  },
  {
    "text": "number number of concurrent uh reconciliation and synchronization that can be uh happening uh concurrently at",
    "start": "1317760",
    "end": "1324480"
  },
  {
    "text": "one time so we can use these parameters uh to tune them and in our case we use these numbers one rule of Thum we can",
    "start": "1324480",
    "end": "1332080"
  },
  {
    "text": "use uh for uh configuring these values is that for every 1,000 applications we",
    "start": "1332080",
    "end": "1338840"
  },
  {
    "text": "use 50 status processors and 25 operation",
    "start": "1338840",
    "end": "1344120"
  },
  {
    "text": "processors next uh about scaling so uh the app controller can actually be",
    "start": "1344120",
    "end": "1350159"
  },
  {
    "text": "scaled uh into multiple shards so it's horizontally",
    "start": "1350159",
    "end": "1356559"
  },
  {
    "text": "shable and uh the charging algorithm in is uh on the cluster level so different",
    "start": "1356559",
    "end": "1364559"
  },
  {
    "text": "set of clusters will be assigned and uh will be served by different",
    "start": "1364559",
    "end": "1370600"
  },
  {
    "text": "charts so to uh scale the app controller we need to increase the replicas uh of",
    "start": "1370600",
    "end": "1378279"
  },
  {
    "text": "the stat full set of app controller and we also need to set this uh environment",
    "start": "1378279",
    "end": "1383919"
  },
  {
    "text": "variable arusd controller replicas with the same value and after implementing",
    "start": "1383919",
    "end": "1389039"
  },
  {
    "text": "this we started to distribute uh into more Aro",
    "start": "1389039",
    "end": "1396080"
  },
  {
    "text": "Parts after implementing the uh sharding we noticed uneven sharts un even shart",
    "start": "1396200",
    "end": "1404080"
  },
  {
    "text": "CPU usages some sharts uh has higher CP usage and some has significantly lower",
    "start": "1404080",
    "end": "1411200"
  },
  {
    "text": "uh compared to others so as previously mentioned Aro CD shs per cluster not uh",
    "start": "1411200",
    "end": "1418440"
  },
  {
    "text": "on the up level so uh large clusters could be hosted uh by the same",
    "start": "1418440",
    "end": "1425600"
  },
  {
    "text": "Shard likewise smaller clusters could also be uh hosted by the same Shard as",
    "start": "1425600",
    "end": "1432440"
  },
  {
    "text": "well uh resulting in the uneven uh CPU usages the new round robbing Charing",
    "start": "1432440",
    "end": "1440440"
  },
  {
    "text": "algorithm uh that's just available recently uh might not help much either",
    "start": "1440440",
    "end": "1445960"
  },
  {
    "text": "for our use case here because there's still chance that large clusters could get into the same Shard when uh Argo CD",
    "start": "1445960",
    "end": "1453440"
  },
  {
    "text": "round Robins uh the Clusters into the sharts so we decided to do manual a",
    "start": "1453440",
    "end": "1458559"
  },
  {
    "text": "manual shart allocation instead to uh fine tune the our shart resources to do",
    "start": "1458559",
    "end": "1465679"
  },
  {
    "text": "this we can use the cluster Secret uh which is uh used by Aro CD and uh we add",
    "start": "1465679",
    "end": "1471880"
  },
  {
    "text": "a new field called shart and uh we put the shart number uh into the cluster",
    "start": "1471880",
    "end": "1479799"
  },
  {
    "text": "secret and after uh implementing it we saw more even CPU usages across all our",
    "start": "1479799",
    "end": "1488120"
  },
  {
    "text": "shards so perhaps one discussion about uh sharding on Argo CD uh sharding on",
    "start": "1488120",
    "end": "1494919"
  },
  {
    "text": "the application Level uh with think would be a really great feature and uh",
    "start": "1494919",
    "end": "1500559"
  },
  {
    "text": "might be a good uh solution for the uh noisy cluster problem we have here",
    "start": "1500559",
    "end": "1508000"
  },
  {
    "text": "there's an open discussion about this uh on GitHub uh if anyone is is interested",
    "start": "1508000",
    "end": "1513279"
  },
  {
    "text": "please check it out next uh even after all the app",
    "start": "1513279",
    "end": "1520600"
  },
  {
    "text": "controller tuning we did we still uh saw very high app controller CPU usages uh",
    "start": "1520600",
    "end": "1528559"
  },
  {
    "text": "and what we thought was uh I think we think was slow",
    "start": "1528559",
    "end": "1534159"
  },
  {
    "text": "reconciles so Argo C uh the app controller watches all field changes of",
    "start": "1534159",
    "end": "1540840"
  },
  {
    "text": "track objects and uh if any field changes and uh differs to the bifast",
    "start": "1540840",
    "end": "1548679"
  },
  {
    "text": "that's in the cach Argo CD will uh start refreshing the apps",
    "start": "1548679",
    "end": "1555200"
  },
  {
    "text": "uh those objects are kubernetes objects which uh maybe as we know kubernetes",
    "start": "1555200",
    "end": "1562399"
  },
  {
    "text": "Fields could get very concise and some Fields might get frequently updated uh",
    "start": "1562399",
    "end": "1568320"
  },
  {
    "text": "even even though those fields uh are fields that we don't really need for",
    "start": "1568320",
    "end": "1573720"
  },
  {
    "text": "Argo CD to reconcile so we actually only need the fields that we have in our uh",
    "start": "1573720",
    "end": "1579960"
  },
  {
    "text": "repositories uh in our manifest right those fields uh for ex example are the",
    "start": "1579960",
    "end": "1586440"
  },
  {
    "text": "uh status fields or the resource generation field or also the resource",
    "start": "1586440",
    "end": "1591840"
  },
  {
    "text": "version uh field so in Aro C uh there's this new",
    "start": "1591840",
    "end": "1597360"
  },
  {
    "text": "fairly new feature uh called ignore resource updates uh which was uh only",
    "start": "1597360",
    "end": "1603919"
  },
  {
    "text": "available since V 2.8 uh and ignore differences features",
    "start": "1603919",
    "end": "1609559"
  },
  {
    "text": "we can use these features to filter out uh the fields we don't really need uh for Aro to uh do that uh reconciliation",
    "start": "1609559",
    "end": "1619240"
  },
  {
    "text": "process uh or we can call them high turn objects uh so in our uh case here we use",
    "start": "1619240",
    "end": "1629679"
  },
  {
    "text": "uh this uh configuration uh I think it's a bit small but I hope uh you can see it",
    "start": "1629679",
    "end": "1637320"
  },
  {
    "text": "uh for example the uh we ignore HPA annotations uh which frequently updates",
    "start": "1637320",
    "end": "1643000"
  },
  {
    "text": "we also ignore some replica Set uh annotations we ignore and coin slice uh",
    "start": "1643000",
    "end": "1648840"
  },
  {
    "text": "which could get very noisy and uh we also ignore the whole",
    "start": "1648840",
    "end": "1654080"
  },
  {
    "text": "status Fields uh status field uh of our AG uh of our kubernetes objects one",
    "start": "1654080",
    "end": "1659799"
  },
  {
    "text": "thing to note about status is that uh if you have uh custom controllers your",
    "start": "1659799",
    "end": "1666080"
  },
  {
    "text": "custom controller might uh want to use some fields in the status field so uh you might want to specify more explicit",
    "start": "1666080",
    "end": "1674519"
  },
  {
    "text": "uh Parts here uh you want to ignore in our case we can uh just ignore the status fields and upon uh implementing",
    "start": "1674519",
    "end": "1681600"
  },
  {
    "text": "this our uh CPU usage uh dropped dramatically almost by half and for one",
    "start": "1681600",
    "end": "1689679"
  },
  {
    "text": "uh fairly simple feature I think this was uh really great feature and uh shout",
    "start": "1689679",
    "end": "1695600"
  },
  {
    "text": "out to the contributors and mainers maintainers for making this feature",
    "start": "1695600",
    "end": "1700960"
  },
  {
    "text": "happen next uh last but not least the uh about API client so our in-house",
    "start": "1700960",
    "end": "1706679"
  },
  {
    "text": "developer platform implements Argo CD AP library and as we scale we started",
    "start": "1706679",
    "end": "1712120"
  },
  {
    "text": "seeing http2 go away errors from our platform so we investigated found uh and",
    "start": "1712120",
    "end": "1719080"
  },
  {
    "text": "fixed a bu in the AP CL Library when using uh grpc web mode uh which is shown",
    "start": "1719080",
    "end": "1724519"
  },
  {
    "text": "in this uh GitHub issue we uh fix that error uh and also",
    "start": "1724519",
    "end": "1731960"
  },
  {
    "text": "actually for Argo CD CLI it may fall back to JPC web mode even though the flag is is not uh specified so you might",
    "start": "1731960",
    "end": "1739720"
  },
  {
    "text": "want to check your UHC config argd config file to see if there's any uh",
    "start": "1739720",
    "end": "1745120"
  },
  {
    "text": "configuration defer or alternatively we can just use the native grpc because this bu only uh",
    "start": "1745120",
    "end": "1753159"
  },
  {
    "text": "affects uh grpc web mode okay that's all the improvements uh",
    "start": "1753159",
    "end": "1760240"
  },
  {
    "text": "sorry the tuning that we can share and we do have uh more improvements to ourd that we look",
    "start": "1760240",
    "end": "1767080"
  },
  {
    "text": "for to which will be uh presented by gir so we explored uh different",
    "start": "1767080",
    "end": "1774279"
  },
  {
    "text": "alternatives to our centralized Aro CD setup the first alternative we look up to is the decentralized Argo CD model",
    "start": "1774279",
    "end": "1782360"
  },
  {
    "text": "instead of uh centralized model uh this is a pool model where each cluster uh",
    "start": "1782360",
    "end": "1788039"
  },
  {
    "text": "installs its own argd instance so if we have 50 different",
    "start": "1788039",
    "end": "1793840"
  },
  {
    "text": "clusters there will be 50 uh different arusd instances responsible to reconcile in its own local cluster there are a",
    "start": "1793840",
    "end": "1801279"
  },
  {
    "text": "couple of benefits which we uh which uh wins against the centralized aru City",
    "start": "1801279",
    "end": "1806799"
  },
  {
    "text": "the first benefit is the application control workloads gets distributed across the Clusters nicely so it's easy",
    "start": "1806799",
    "end": "1814279"
  },
  {
    "text": "to scale with this uh model and then second benefit the argd instance doesn't",
    "start": "1814279",
    "end": "1820080"
  },
  {
    "text": "need to have access to all the kubernetes API servers that we have in the infrastructure so this model has a",
    "start": "1820080",
    "end": "1826399"
  },
  {
    "text": "better security post than the centralized model however there are many challenges that we don't like um instead",
    "start": "1826399",
    "end": "1833919"
  },
  {
    "text": "of maintaining one instance we now need to maintain and upgrade 50 different Aro City instances for us small team of",
    "start": "1833919",
    "end": "1839799"
  },
  {
    "text": "platform Engineers we already regularly upgrades 50 clusters 50 esto uh",
    "start": "1839799",
    "end": "1845679"
  },
  {
    "text": "components and if we implement this we need to add more Works to our our small team and our platform if we implement",
    "start": "1845679",
    "end": "1853799"
  },
  {
    "text": "this we need to maintain multiple ago decline versions and know in order to talk to which cluster we need to uh use",
    "start": "1853799",
    "end": "1860799"
  },
  {
    "text": "different arus decline versions it makes unnecessary complexity in our platform logic and we no longer have centralized",
    "start": "1860799",
    "end": "1867320"
  },
  {
    "text": "dashboard to control and view Argo applications in a certain cluster we need to open different dashboards we",
    "start": "1867320",
    "end": "1873480"
  },
  {
    "text": "would be having 50 different argd dashboards and anyway if the cluster",
    "start": "1873480",
    "end": "1878960"
  },
  {
    "text": "gets large uh we still require tuning of that particular agus instance another",
    "start": "1878960",
    "end": "1884760"
  },
  {
    "text": "alternative that we saw was um agent based argu city or a hybrid model",
    "start": "1884760",
    "end": "1890159"
  },
  {
    "text": "between push and pull that we saw in aquity platform in this hybrid model",
    "start": "1890159",
    "end": "1896440"
  },
  {
    "text": "each of the cluster still has Aro CD instance but there's a central argd",
    "start": "1896440",
    "end": "1902080"
  },
  {
    "text": "control plane uh which oversees uh the entire application States across the Clusters and the dashboard is still",
    "start": "1902080",
    "end": "1909840"
  },
  {
    "text": "centralized we really hope this model gets uh contributed Upstream in the",
    "start": "1909840",
    "end": "1915480"
  },
  {
    "text": "community we've seen uh couple of work in progress in the community like the",
    "start": "1915480",
    "end": "1920600"
  },
  {
    "text": "first uh pool request got merged couple of months back it enables optional pool mechanism for application set with this",
    "start": "1920600",
    "end": "1928519"
  },
  {
    "text": "PR um the application set can generate application objects to a remote cluster",
    "start": "1928519",
    "end": "1934679"
  },
  {
    "text": "and then let the remote cluster reconciles on the application object that they belong locally in the cluster",
    "start": "1934679",
    "end": "1941480"
  },
  {
    "text": "now second uh second feature is still an open issue in GitHub it's uh it's an",
    "start": "1941480",
    "end": "1947240"
  },
  {
    "text": "effort to to to implement centralized UI for multiple Argo CD",
    "start": "1947240",
    "end": "1952399"
  },
  {
    "text": "instances and what we are also excited about is the Argo CD UI Improvement there's an open issue right now to",
    "start": "1952399",
    "end": "1959000"
  },
  {
    "text": "support server site pagination on the Argo CD dashboard which will significantly uh boost up Argo City uh",
    "start": "1959000",
    "end": "1966279"
  },
  {
    "text": "load page and the home page and this is already on aquid and they plan it to",
    "start": "1966279",
    "end": "1971320"
  },
  {
    "text": "bring it Upstream I hope soon it will be available Upstream we are heavily inspired by uh um Tik Tok and adobe's uh",
    "start": "1971320",
    "end": "1979080"
  },
  {
    "text": "journey in managing their thousands of arusd applications and we learn a lot uh from uh Argus CID best practices uh uh",
    "start": "1979080",
    "end": "1987200"
  },
  {
    "text": "from Alex and also um the amazing Argo CID documentation that provides us clear",
    "start": "1987200",
    "end": "1993480"
  },
  {
    "text": "guidance on the performance tuning uh Journey thank you for your time we are now happy to take questions we still",
    "start": "1993480",
    "end": "1999840"
  },
  {
    "text": "have uh two more minutes and please scan the QR here to give us feedback thank",
    "start": "1999840",
    "end": "2005960"
  },
  {
    "text": "you [Applause]",
    "start": "2005960",
    "end": "2013409"
  },
  {
    "text": "hi thanks for the talk uh I wanted to ask how fast and easy is it to short app and down on the application",
    "start": "2015559",
    "end": "2023200"
  },
  {
    "text": "server sorry I didn't get it how fast is that how easy and how easy and um fast",
    "start": "2023200",
    "end": "2029200"
  },
  {
    "text": "is it to short up or down on the application server how easy is it to sh",
    "start": "2029200",
    "end": "2035559"
  },
  {
    "text": "up or down right you were showing you had four application servers running",
    "start": "2035559",
    "end": "2040919"
  },
  {
    "text": "shed out controller sharts the controller sharts right okay it's very easy you can yeah uh actually it's a",
    "start": "2040919",
    "end": "2047840"
  },
  {
    "text": "simple just a simple uh replica uh change and to change the enironment",
    "start": "2047840",
    "end": "2053280"
  },
  {
    "text": "variable we uh shown in the uh presentation so aru CD app controller is",
    "start": "2053280",
    "end": "2061079"
  },
  {
    "text": "actually pretty stateless uh it can build up its own uh",
    "start": "2061079",
    "end": "2066280"
  },
  {
    "text": "State again uh from scratch so uh we might perhaps see some a little bit of",
    "start": "2066280",
    "end": "2073560"
  },
  {
    "text": "slow Rec consults right after we sh up or down uh the app controller but uh",
    "start": "2073560",
    "end": "2079079"
  },
  {
    "text": "it's like just two to three minutes uh in our experience I hope that answers your",
    "start": "2079079",
    "end": "2088200"
  },
  {
    "text": "question hi uh I have a question about the centralized Argo CD maintenance so",
    "start": "2088200",
    "end": "2094079"
  },
  {
    "text": "what type of guard rails do you have when migrating from from one Argo CD version to another say for suppose uh",
    "start": "2094079",
    "end": "2101359"
  },
  {
    "text": "recently we did a small migration from 2.7 to 2.8 and we found the performance",
    "start": "2101359",
    "end": "2107040"
  },
  {
    "text": "issues and we rolled it back and uh in in case of centralized Argo CD if",
    "start": "2107040",
    "end": "2112680"
  },
  {
    "text": "something breaks uh the whole deployment system might break so what kind of guardrails did you uh take when you are",
    "start": "2112680",
    "end": "2119280"
  },
  {
    "text": "maintaining this okay uh upgrading Argo C uh uh a",
    "start": "2119280",
    "end": "2126960"
  },
  {
    "text": "actually each argd version could have their own uh special treatment we need",
    "start": "2126960",
    "end": "2133000"
  },
  {
    "text": "to uh tackle so they're actually uh like described pretty",
    "start": "2133000",
    "end": "2139400"
  },
  {
    "text": "clearly uh in each upgrade uh documents for example I think from 2 uh 7 to 2.8",
    "start": "2139400",
    "end": "2148880"
  },
  {
    "text": "we had a pretty smooth uh upgrade and I think it was 2.8 to 2 2.5 to 2.6 right",
    "start": "2148880",
    "end": "2156480"
  },
  {
    "text": "where we we had uh some major changes and we needed to uh so it depends on the",
    "start": "2156480",
    "end": "2162240"
  },
  {
    "text": "the fs but generally uh it's not that",
    "start": "2162240",
    "end": "2167280"
  },
  {
    "text": "operationally intensive uh and uh they have pretty clear guidance for us to",
    "start": "2167280",
    "end": "2173640"
  },
  {
    "text": "do all right I think uh we are running out out of time uh if you still have questions let's do it in the hallway",
    "start": "2173640",
    "end": "2179480"
  },
  {
    "text": "thank you",
    "start": "2179480",
    "end": "2184880"
  }
]