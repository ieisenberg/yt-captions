[
  {
    "start": "0",
    "end": "37000"
  },
  {
    "text": "I guess let's get started so uh hello everyone welcome thanks for uh coming so",
    "start": "1040",
    "end": "6080"
  },
  {
    "text": "today I'm going to talk about the service match Journey at door Dash over the last two years uh we will discuss",
    "start": "6080",
    "end": "12280"
  },
  {
    "text": "what's been uh good what's been bad and yes even some a little bit some ugly stuff uh my name is H I have been a",
    "start": "12280",
    "end": "19640"
  },
  {
    "text": "software engineer at D cor infrastructure team since 2020 U mainly focusing on uh computer and traffic",
    "start": "19640",
    "end": "25920"
  },
  {
    "text": "infrastructure and before that I was working on buildings and distribu system for machine learning uh at several",
    "start": "25920",
    "end": "32279"
  },
  {
    "text": "startups until I realized uh that's just probably too hard for me U all right so",
    "start": "32279",
    "end": "38320"
  },
  {
    "start": "37000",
    "end": "120000"
  },
  {
    "text": "let's get started whyer smash for door Dash so in 2019 uh door Dash started",
    "start": "38320",
    "end": "44760"
  },
  {
    "text": "their effort to extract microservices from a single monistic application and",
    "start": "44760",
    "end": "50120"
  },
  {
    "text": "fast forward to the end of q1 2021 we found ourselves with over 50",
    "start": "50120",
    "end": "55920"
  },
  {
    "text": "microservices and you know with this growth came with those you know classic microservice challenges uh observability",
    "start": "55920",
    "end": "63359"
  },
  {
    "text": "has been challenging debuging was much harder our service topology became a mze",
    "start": "63359",
    "end": "68840"
  },
  {
    "text": "of complexity and no one really knows who is talking to whom and who ons what or microservices could talk to each",
    "start": "68840",
    "end": "75240"
  },
  {
    "text": "other in so many different ways they could use service IP they could use uh Headly service with client side Lo",
    "start": "75240",
    "end": "81400"
  },
  {
    "text": "balancing or they could just use load balancing Lo Baner and there was no standard way to do authentication and",
    "start": "81400",
    "end": "88479"
  },
  {
    "text": "authorization and bes that developers were just implementing so many Sim similar things in the application code",
    "start": "88479",
    "end": "95399"
  },
  {
    "text": "using so many different ways so uh in Q2 2021 we started",
    "start": "95399",
    "end": "101920"
  },
  {
    "text": "exploring service match which we believed has the potential to address the above challenges we believe that you",
    "start": "101920",
    "end": "109119"
  },
  {
    "text": "know by implementing so many features as the platform layer in a more standard way is far more efficient than",
    "start": "109119",
    "end": "115600"
  },
  {
    "text": "addressing them individually at the application layer so we started searching for Solutions",
    "start": "115600",
    "end": "122479"
  },
  {
    "start": "120000",
    "end": "356000"
  },
  {
    "text": "and we did explore various open source projects and probably you know before we",
    "start": "122479",
    "end": "128399"
  },
  {
    "text": "look at the initial deis uh let's take a quick overview of our requirements at the time so uh first thing first",
    "start": "128399",
    "end": "135680"
  },
  {
    "text": "scalability uh our microservices actually were around the time were all",
    "start": "135680",
    "end": "141120"
  },
  {
    "text": "deployed on one single big cuties clusters with more than 2,000 notes and",
    "start": "141120",
    "end": "146519"
  },
  {
    "text": "typically uh we felt uncomfortable whenever a cluster has more than 1,000 notes because a lot of Open Source tools",
    "start": "146519",
    "end": "152920"
  },
  {
    "text": "we used were basically tested with at most with a thousand notes and in",
    "start": "152920",
    "end": "158959"
  },
  {
    "text": "reality we did U you know observe some random scalability and reliability issues whenever we had a Custer more",
    "start": "158959",
    "end": "165560"
  },
  {
    "text": "than a, no and obviously we couldn't afford any outage of that single cluster",
    "start": "165560",
    "end": "170840"
  },
  {
    "text": "because that's the only thing we have so first thing first we wanted the solution to be able to support our scale at at",
    "start": "170840",
    "end": "177319"
  },
  {
    "text": "that time flexibility so we don't want to live with that single equivalent cluster",
    "start": "177319",
    "end": "183599"
  },
  {
    "text": "forever so I guess to make our life easier we implemented a console based",
    "start": "183599",
    "end": "188879"
  },
  {
    "text": "multicluster infrastructure and started their migration already to move some microservices to new clusters to put",
    "start": "188879",
    "end": "196000"
  },
  {
    "text": "less pressure on that old single kubernetes kubernetes clusters control plan so we needed the solution to be uh",
    "start": "196000",
    "end": "204000"
  },
  {
    "text": "less opinionated and more flexible enough to support our unique multicluster setup",
    "start": "204000",
    "end": "211680"
  },
  {
    "text": "and tools of course we use should have a mature community and solutions need to be supported by our successful user",
    "start": "211799",
    "end": "218760"
  },
  {
    "text": "storage supported by some other companies similar to our scale and it was also essential for our",
    "start": "218760",
    "end": "226400"
  },
  {
    "text": "solution to be easy enough to configure with some uh comprehensive documentation as well and lastly you know to uh we",
    "start": "226400",
    "end": "235519"
  },
  {
    "text": "need the solution to provide features in observability security reli liability",
    "start": "235519",
    "end": "240599"
  },
  {
    "text": "and traffic management to address those you know typical micr service challenges we mentioned",
    "start": "240599",
    "end": "247079"
  },
  {
    "text": "before and after spending a lot of time on you know exploring various open",
    "start": "247079",
    "end": "252799"
  },
  {
    "text": "source projects including those most popular ones at the time uh we settled on n as our our data plan while",
    "start": "252799",
    "end": "259759"
  },
  {
    "text": "developing our customer control plan to align with our specific needs and you are probably familiar with this",
    "start": "259759",
    "end": "265720"
  },
  {
    "text": "architecture today um which is still you know kind of most common and typical solution nowadays uh traffic redirection",
    "start": "265720",
    "end": "273160"
  },
  {
    "text": "is managed by IP tables and side car injection is managed by kubet mutating web hook data plan runs as a side car",
    "start": "273160",
    "end": "279720"
  },
  {
    "text": "container for each part uh handling all Ingress and egress traffic for http1 HTTP and grpc traffic or at that time we",
    "start": "279720",
    "end": "288000"
  },
  {
    "text": "don't manage uh we don't manage any uh storage related traffic the control plan",
    "start": "288000",
    "end": "293440"
  },
  {
    "text": "just manages the configurations for these NW side car containers using XDS API I guess it's worth to mention that",
    "start": "293440",
    "end": "300720"
  },
  {
    "text": "building our own control plan was not an easy deis at the time especially uh considering we have been always a",
    "start": "300720",
    "end": "306759"
  },
  {
    "text": "Smalling on this project usually uh with just one or two Engineers until very recently and while the service match",
    "start": "306759",
    "end": "314199"
  },
  {
    "text": "landscape has involved a lot nowadays um to be honest our choice might be uh",
    "start": "314199",
    "end": "320319"
  },
  {
    "text": "different today but given our uh unique requirements at that time uh this pass",
    "start": "320319",
    "end": "326160"
  },
  {
    "text": "was probably the most logical choice so for",
    "start": "326160",
    "end": "331720"
  },
  {
    "text": "adoption when we began the journey uh we knew we wanted so many features from C",
    "start": "331720",
    "end": "336840"
  },
  {
    "text": "smash to address those microservice challenges but however to be to be honest what exact features we wanted uh",
    "start": "336840",
    "end": "343319"
  },
  {
    "text": "to use was a little bit unclear uh and the plan was basically to onboard everyone to service match with minim",
    "start": "343319",
    "end": "349440"
  },
  {
    "text": "more features and then decide what additional features we want to support",
    "start": "349440",
    "end": "355560"
  },
  {
    "start": "356000",
    "end": "558000"
  },
  {
    "text": "later so however uh I guess our significant Turning Point occurred on June 19th 2021 that's when door Dash",
    "start": "356039",
    "end": "364000"
  },
  {
    "text": "experienced a complete outage lasting over more than uh 2 hours if you are interested in the",
    "start": "364000",
    "end": "371360"
  },
  {
    "text": "details I also linked the RCA in the slides as well but I guess the key takeaway from that RCA is that uh the",
    "start": "371360",
    "end": "379039"
  },
  {
    "text": "outage was actually caused by a typical cascading failure starting with some high lency issues initially in our",
    "start": "379039",
    "end": "385880"
  },
  {
    "text": "payment systems and as clients like Dasher service attempt retrice retrice",
    "start": "385880",
    "end": "391599"
  },
  {
    "text": "and probably retrice they recurred or retri stor eventually that further overloaded the already a struggling",
    "start": "391599",
    "end": "398440"
  },
  {
    "text": "payment service and uh eventually that caused a complete outage which cost us",
    "start": "398440",
    "end": "405440"
  },
  {
    "text": "to basically shut off the traffic from The Edge layer uh twice to put less",
    "start": "405440",
    "end": "411720"
  },
  {
    "text": "pressure on the payment service and give it just more time to to",
    "start": "411720",
    "end": "416599"
  },
  {
    "text": "recover we realized that this situation could have been prevented or at least mitigated with some standard best",
    "start": "417000",
    "end": "424440"
  },
  {
    "text": "practices that we always recommend to teams so for instance we should have what should have happened is that uh the",
    "start": "424440",
    "end": "430919"
  },
  {
    "text": "payment service itself could have implemented load shedding to proactively",
    "start": "430919",
    "end": "436759"
  },
  {
    "text": "reject some load when it's already in the degraded State like in this case high lency so that it could at least",
    "start": "436759",
    "end": "444039"
  },
  {
    "text": "prevent itself from a complete outage also so the clients of payment",
    "start": "444039",
    "end": "450360"
  },
  {
    "text": "service could have used uh circuit breakers to fail fast whenever payment service returned some higher error rate",
    "start": "450360",
    "end": "457800"
  },
  {
    "text": "and it could just periodically check whether payment service is back to normal and decide whether it's time to",
    "start": "457800",
    "end": "463039"
  },
  {
    "text": "talk back to payment service again so for these two reliability related features we did have them implemented in",
    "start": "463039",
    "end": "470120"
  },
  {
    "text": "some common codling libraries since that's the one uh that's our primary program language and however services",
    "start": "470120",
    "end": "477000"
  },
  {
    "text": "like the payment service are we still using using other program languages in this case python so it was kind of in",
    "start": "477000",
    "end": "483879"
  },
  {
    "text": "challenging to implement all these reliability features across the",
    "start": "483879",
    "end": "489159"
  },
  {
    "text": "world and unfortunately we had several similar instance before so this eventually led to many uh teams to have",
    "start": "489159",
    "end": "496960"
  },
  {
    "text": "a code freeze for amount focusing exclusively just on reliability related",
    "start": "496960",
    "end": "502520"
  },
  {
    "text": "uh tasks and that's also the point that folks started asking what's the status",
    "start": "502520",
    "end": "508639"
  },
  {
    "text": "of service which you know has the potential to implement these reliability features in",
    "start": "508639",
    "end": "515200"
  },
  {
    "text": "a language agnostic way so where did we stand with the project we well we",
    "start": "515200",
    "end": "521279"
  },
  {
    "text": "basically had nothing we just started the initial you know you know design and reviewed it and started the",
    "start": "521279",
    "end": "527600"
  },
  {
    "text": "implementation uh we didn't have any um operational experience with running And1",
    "start": "527600",
    "end": "533360"
  },
  {
    "text": "in production and there was basically no control plan at that time so so this outage basically made us",
    "start": "533360",
    "end": "541279"
  },
  {
    "text": "realize the urgency of shipping the project sooner so instead of waiting to",
    "start": "541279",
    "end": "546519"
  },
  {
    "text": "build a complete control plan and on board everyone first we understood the importance of addressing the most",
    "start": "546519",
    "end": "553000"
  },
  {
    "text": "immediate and pressing issues first So eventually we decided to shift our priority and let's look at their design",
    "start": "553000",
    "end": "559800"
  },
  {
    "start": "558000",
    "end": "664000"
  },
  {
    "text": "after the priority shift after the outage similarly traffic redirection and",
    "start": "559800",
    "end": "566519"
  },
  {
    "text": "side car injection is still unchanged still using IP and mutating web hook the biggest change is in their configuration",
    "start": "566519",
    "end": "573920"
  },
  {
    "text": "management system so instead of the API based configuration management uh system",
    "start": "573920",
    "end": "579000"
  },
  {
    "text": "we used a file based Dynamic configurations so users put config NW",
    "start": "579000",
    "end": "584240"
  },
  {
    "text": "configurations in our GitHub repository and then a CD pipeline just package and ship this configuration to an S3 bucket",
    "start": "584240",
    "end": "591839"
  },
  {
    "text": "uh which then just get pulled and mounted to thei car through our one existing internal service which we call",
    "start": "591839",
    "end": "599200"
  },
  {
    "text": "reinker in this diagram and mway can still hard restart Whenever there there's any updates in M configurations",
    "start": "599200",
    "end": "605880"
  },
  {
    "text": "so leveraging their existing CD Pipeline and that S3 thinker service saved us a",
    "start": "605880",
    "end": "611920"
  },
  {
    "text": "lot of time for the configuration management story and Eno was configured as a HTTP",
    "start": "611920",
    "end": "619200"
  },
  {
    "text": "pass through proxy using their nway uh original destination cluster uh instead of uh given no features we added two",
    "start": "619200",
    "end": "627120"
  },
  {
    "text": "reliability related features that concurrency for that load shedding behavior and out",
    "start": "627120",
    "end": "633000"
  },
  {
    "text": "detection to have a similar Behavior as our circuit breaking to basically help us prevent our similar outage from",
    "start": "633000",
    "end": "640360"
  },
  {
    "text": "happening again so this design as you can see had a very primitive configuration",
    "start": "640360",
    "end": "646959"
  },
  {
    "text": "management approach and the focus primarily was around the data plan specifically specifically for these two",
    "start": "646959",
    "end": "654079"
  },
  {
    "text": "reliability features provided by mway and actually at that time we started calling the project and C car uh rather",
    "start": "654079",
    "end": "661000"
  },
  {
    "text": "than service mesh so once we successfully implemented",
    "start": "661000",
    "end": "667000"
  },
  {
    "start": "664000",
    "end": "859000"
  },
  {
    "text": "and tested our solution in staging we unboarded two critical Pym Services which include the payment service which",
    "start": "667000",
    "end": "673920"
  },
  {
    "text": "uh was the one that cost the uh sitewide",
    "start": "673920",
    "end": "679160"
  },
  {
    "text": "outage a little bit about onboarding process uh requires two steps and firstly just similar to many open source",
    "start": "679160",
    "end": "686839"
  },
  {
    "text": "Solutions we need uh to add add some custom label to the name space and deployment spec and then we also need to",
    "start": "686839",
    "end": "695040"
  },
  {
    "text": "create raw NW configurations at the time which typically includes around 1,000 lines of configurations which as you can",
    "start": "695040",
    "end": "703040"
  },
  {
    "text": "are as you can imagine overwhelming for everyone at the time even though we just",
    "start": "703040",
    "end": "708839"
  },
  {
    "text": "had two customers so the good news for developers was that they didn't need to",
    "start": "708839",
    "end": "714920"
  },
  {
    "text": "modify any application code for the roll out strategy to",
    "start": "714920",
    "end": "721360"
  },
  {
    "text": "gradually roll out uh to gradually introduce the m c car we used the canary deployment approach users need to deploy",
    "start": "721360",
    "end": "728279"
  },
  {
    "text": "another independent kubernetes Clary deployment which uses the same applic application code as the production one",
    "start": "728279",
    "end": "734800"
  },
  {
    "text": "but runs with the n i car injected so the N the canary deployment shares the",
    "start": "734800",
    "end": "740079"
  },
  {
    "text": "same labels as the production one uh which matches the selector defined in the service object and that's how Sun",
    "start": "740079",
    "end": "746959"
  },
  {
    "text": "traffic could be sent to uh those Canary p as well and so this allowed users to",
    "start": "746959",
    "end": "752639"
  },
  {
    "text": "adjust their Canary deployments replica account to control their amount of traffic routed to the P running with the",
    "start": "752639",
    "end": "759079"
  },
  {
    "text": "side car and for these two Services we then baked the traffic for around two",
    "start": "759079",
    "end": "765519"
  },
  {
    "text": "weeks we were doing this super cautiously because the whole point was to prevent a outage like that and once",
    "start": "765519",
    "end": "772199"
  },
  {
    "text": "we failed uh confident about everything we then scale down the canary deployment and run the production one with the mide",
    "start": "772199",
    "end": "778519"
  },
  {
    "text": "car and injected so the roll out eventually was smooth and uh for these two Services they have their Extra",
    "start": "778519",
    "end": "784959"
  },
  {
    "text": "Protection without any code changes eventually so this I guess brings me to",
    "start": "784959",
    "end": "791880"
  },
  {
    "text": "the first lessons we learned along the way so looking back we believed that shifting our priority to build a this",
    "start": "791880",
    "end": "799320"
  },
  {
    "text": "you know very simplified solution was the right deis we initially had a very big dream but we needed to clarify our",
    "start": "799320",
    "end": "807000"
  },
  {
    "text": "immediate goals and start with something more so looking back on our journey actually many big changes were driven by",
    "start": "807000",
    "end": "814440"
  },
  {
    "text": "the motivation to solve some real world problems within the",
    "start": "814440",
    "end": "820160"
  },
  {
    "text": "arc so in q1 2022 the project the N Zar project uh reached G status uh we",
    "start": "820920",
    "end": "828079"
  },
  {
    "text": "created configuration templates instead of having developers to configure those raw and configurations which as you can",
    "start": "828079",
    "end": "834959"
  },
  {
    "text": "tell Miss imp possible we offered our common dashboard for networking Matrix provided common alerts and runbooks to",
    "start": "834959",
    "end": "842279"
  },
  {
    "text": "monitor common issues like you have higher rate or you just have you know High ly uh we expanded our user uh Base",
    "start": "842279",
    "end": "849800"
  },
  {
    "text": "by reaching out to uh more early adopters and onboarding services uh in",
    "start": "849800",
    "end": "855440"
  },
  {
    "text": "more uh different programming languages and with this successful user",
    "start": "855440",
    "end": "861480"
  },
  {
    "start": "859000",
    "end": "1071000"
  },
  {
    "text": "story from our initial customers and the announcement of that g status uh in 2022",
    "start": "861480",
    "end": "867560"
  },
  {
    "text": "we started hearing more feature request from pimps as well so one big ask was to support Zuma",
    "start": "867560",
    "end": "874199"
  },
  {
    "text": "Weare outing so typically our microservices are deployed across all different availability zoms in cries and",
    "start": "874199",
    "end": "881880"
  },
  {
    "text": "previously the default behavior is that the erress traffic from clients in service one in this diagram is load",
    "start": "881880",
    "end": "889040"
  },
  {
    "text": "balance between all destination or several parts of service two so it's a pattern of basically everyone talks to",
    "start": "889040",
    "end": "894880"
  },
  {
    "text": "everyone and the idea of Zoom overing is to Route the erress traffic of client to",
    "start": "894880",
    "end": "901560"
  },
  {
    "text": "its local availability Zone while ensuring their Ingress traffic received by each individual or several parts is",
    "start": "901560",
    "end": "908480"
  },
  {
    "text": "still balanced so staying with the same AVM has couple benefits and firstly it",
    "start": "908480",
    "end": "915480"
  },
  {
    "text": "saved us some cross a data transfer cost and reduced the impact of one a outage",
    "start": "915480",
    "end": "921279"
  },
  {
    "text": "and also making the communication more performant because uh we are now",
    "start": "921279",
    "end": "926759"
  },
  {
    "text": "connecting the clients to their NE and given its impact in all these",
    "start": "926759",
    "end": "932639"
  },
  {
    "text": "reliability efficiency and performance areas and given especially efficiency is",
    "start": "932639",
    "end": "938079"
  },
  {
    "text": "uh was our one of our engineering uh priority in 2022 we decided to support",
    "start": "938079",
    "end": "943279"
  },
  {
    "text": "this feature and that's the point that we could take their opport opportunity to",
    "start": "943279",
    "end": "950360"
  },
  {
    "text": "involve their configuration management system as well uh leading to this uh introduction of their API based Dynamic",
    "start": "950360",
    "end": "957000"
  },
  {
    "text": "configuration for all the Eds resources and now XTS resources and XTS servers",
    "start": "957000",
    "end": "962360"
  },
  {
    "text": "read the IP addresses from the source of shoes which in our case is console and ships all IP addresses with a",
    "start": "962360",
    "end": "969399"
  },
  {
    "text": "information back to the mvo iar so that you know given this information the data plan can just perform Zoom aware",
    "start": "969399",
    "end": "977480"
  },
  {
    "text": "outing it turns out Zoom aare outing was just the beginning uh we co-developed",
    "start": "978440",
    "end": "983560"
  },
  {
    "text": "many more features with our initial customers and throughout the year um this this process uh helped us to get a",
    "start": "983560",
    "end": "991519"
  },
  {
    "text": "better understanding and deeper understanding of our customer uh pain points and helped us prioritized",
    "start": "991519",
    "end": "997199"
  },
  {
    "text": "additional features Beyond those initial reliability related uh features so in our case many use cases we heard were",
    "start": "997199",
    "end": "1005000"
  },
  {
    "text": "related to Traffic management and with are very particular focus on header routing uh load balancing and traffic uh",
    "start": "1005000",
    "end": "1013079"
  },
  {
    "text": "policy so today all these features are in production but at that time with the",
    "start": "1013079",
    "end": "1019360"
  },
  {
    "text": "in introduction of all these new features we quickly realized that the",
    "start": "1019360",
    "end": "1024798"
  },
  {
    "text": "more the more services we adopt the more benefits we could have so we continue to",
    "start": "1024799",
    "end": "1030199"
  },
  {
    "text": "focus on increasing adoptions throughout the year and by the end of 2022 we",
    "start": "1030199",
    "end": "1036720"
  },
  {
    "text": "eventually unboarded around 100 Services which uh doesn't sound bad right uh",
    "start": "1036720",
    "end": "1042079"
  },
  {
    "text": "given that we we were really a smallart team with just one or two Engineers for helping people to onboard and but that's",
    "start": "1042079",
    "end": "1048679"
  },
  {
    "text": "also the point that people started asking when we can onboard all",
    "start": "1048679",
    "end": "1054600"
  },
  {
    "text": "services so unfortunately uh some back of the napkin mes quickly showed us that it would take",
    "start": "1054600",
    "end": "1060760"
  },
  {
    "text": "us several more years to onboard most services to service mesh and that you",
    "start": "1060760",
    "end": "1066000"
  },
  {
    "text": "know it became evident that we had to speed up the onboarding process so in Q4 2022 we decided to",
    "start": "1066000",
    "end": "1074880"
  },
  {
    "start": "1071000",
    "end": "1448000"
  },
  {
    "text": "review what could prevent us having uh moving faster in 23 and what changes we",
    "start": "1074880",
    "end": "1080720"
  },
  {
    "text": "should make before 23 the first thing I want to mention here is that in the initial adoption",
    "start": "1080720",
    "end": "1087280"
  },
  {
    "text": "phase we have discovered a lot of unknowns we found many special client",
    "start": "1087280",
    "end": "1092960"
  },
  {
    "text": "behaviors that was previously unnoticed but became apparent with the introduction of NW iar so here are",
    "start": "1092960",
    "end": "1099640"
  },
  {
    "text": "taking our first customer payment Service as an example so there was a",
    "start": "1099640",
    "end": "1104679"
  },
  {
    "text": "client which uh couldn't perform client side Rec Quest level load balancing for",
    "start": "1104679",
    "end": "1110280"
  },
  {
    "text": "the GPC traffic so before before the M car was injected to balance the traffic",
    "start": "1110280",
    "end": "1117280"
  },
  {
    "text": "it turns out that the payment service was just periodically recycling connections so that client could create",
    "start": "1117280",
    "end": "1124480"
  },
  {
    "text": "new connections to other parts in this case part two in the diagram and however injecting the side",
    "start": "1124480",
    "end": "1130480"
  },
  {
    "text": "car into payment service part disrupted this balance and now only",
    "start": "1130480",
    "end": "1137480"
  },
  {
    "text": "the connection between between payment service cyar and payment will be recreated and the client would just",
    "start": "1137480",
    "end": "1143840"
  },
  {
    "text": "always talk to payment service part one in this case leading to an Ingress traffic imbalance over time and",
    "start": "1143840",
    "end": "1150360"
  },
  {
    "text": "eventually we had to move the similar Behavior to the side car by adjusting the connection age uh configurations in",
    "start": "1150360",
    "end": "1157280"
  },
  {
    "text": "the servers n iar we also quickly realized that this",
    "start": "1157280",
    "end": "1163480"
  },
  {
    "text": "example was just the tip of the iceberg and we were basically in a phase where we didn't know what we didn't know so we",
    "start": "1163480",
    "end": "1170760"
  },
  {
    "text": "contined to uncover more special unnoticed client behaviors and the",
    "start": "1170760",
    "end": "1176400"
  },
  {
    "text": "introduction of the Nar broke whatever it just broke worked before this",
    "start": "1176400",
    "end": "1182000"
  },
  {
    "text": "highlighted that making their data plan always transparent isn't easy and we had",
    "start": "1182000",
    "end": "1188799"
  },
  {
    "text": "to uh kind of expect the unexpected in the initial adoption phase and that's",
    "start": "1188799",
    "end": "1194760"
  },
  {
    "text": "why we had to use that Canary deployment approach for a while well unfortunately",
    "start": "1194760",
    "end": "1200120"
  },
  {
    "text": "as we onboard more services by the end of Q4 22 we figured that we were not",
    "start": "1200120",
    "end": "1207080"
  },
  {
    "text": "seeing this kind of unknowns that often and that's the point we decided to take some",
    "start": "1207080",
    "end": "1212120"
  },
  {
    "text": "B we believe that we uncovered um most unknowns already and it's probably okay",
    "start": "1212120",
    "end": "1218640"
  },
  {
    "text": "to roll out faster as long as we can roll back fast so the first change we made was shifting from that Canary",
    "start": "1218640",
    "end": "1224880"
  },
  {
    "text": "deployment approach to uh where we also B traffic for dat to that you know uh uh",
    "start": "1224880",
    "end": "1231520"
  },
  {
    "text": "the native you know kubernetes building rolling update M this did significant",
    "start": "1231520",
    "end": "1236679"
  },
  {
    "text": "reduce onboarding time from days two hours we also realized many challenges",
    "start": "1236679",
    "end": "1243679"
  },
  {
    "text": "in developer experience could prevent us having a large scale adoption and firstly we used to ask teams to follow",
    "start": "1243679",
    "end": "1250840"
  },
  {
    "text": "some onboarding documentation to on board their service adding some labels and some NW configuration sounds easy",
    "start": "1250840",
    "end": "1257159"
  },
  {
    "text": "but actually every team needed to UNF follow some documentation and we are talking about 400 more services here so",
    "start": "1257159",
    "end": "1264640"
  },
  {
    "text": "we realized this decentralized onboarding approach doesn't scale for US",
    "start": "1264640",
    "end": "1270120"
  },
  {
    "text": "similarly we ask every individual team to manage their n i car resources which",
    "start": "1270120",
    "end": "1276640"
  },
  {
    "text": "doesn't SC as well so we decided to have the infro team to own the onboarding",
    "start": "1276640",
    "end": "1282600"
  },
  {
    "text": "process and the resource management story which is you know the team that is most for familar with the process and",
    "start": "1282600",
    "end": "1289840"
  },
  {
    "text": "the team that is most motivated to improve the process and eventually we",
    "start": "1289840",
    "end": "1295039"
  },
  {
    "text": "did streamline the onboarding process by preena all those n configurations and",
    "start": "1295039",
    "end": "1300080"
  },
  {
    "text": "labels for all services throughout the year we also",
    "start": "1300080",
    "end": "1306200"
  },
  {
    "text": "noticed we put our um primary focus on",
    "start": "1306200",
    "end": "1311320"
  },
  {
    "text": "onboarding and making the M iard transparent but we didn't put enough",
    "start": "1311320",
    "end": "1317400"
  },
  {
    "text": "time on educating our users and users were lack of some basic understanding of",
    "start": "1317400",
    "end": "1322760"
  },
  {
    "text": "Serv Smash and that eventually caused some confusions we decided to just enhance our documentation and investing",
    "start": "1322760",
    "end": "1329279"
  },
  {
    "text": "more time on in educating and enabling our service",
    "start": "1329279",
    "end": "1334360"
  },
  {
    "text": "owners the complexity of observability features is also another big one",
    "start": "1335039",
    "end": "1340240"
  },
  {
    "text": "networking issues still happened of course but the Matrix we provided were just overwhelming there were just too",
    "start": "1340240",
    "end": "1346880"
  },
  {
    "text": "many Matrix and it course super hard for our users to know what Matrix to look at",
    "start": "1346880",
    "end": "1352679"
  },
  {
    "text": "we exposed all terminologies uh used in N Matrix to our users and they have to",
    "start": "1352679",
    "end": "1359159"
  },
  {
    "text": "learn stuff like what is our Ingress egress Downstream Upstream local remote connections requests responses messages",
    "start": "1359159",
    "end": "1366240"
  },
  {
    "text": "JPC htp1 htb2 stuff like that all this stuff but not all Engineers enjoyed",
    "start": "1366240",
    "end": "1372919"
  },
  {
    "text": "looking into every single detail of all the data so we should have our you know",
    "start": "1372919",
    "end": "1378200"
  },
  {
    "text": "product mindset here and be more customer obsessed here so we invested some time in simplifying the dashboard",
    "start": "1378200",
    "end": "1384799"
  },
  {
    "text": "to make Matrix more user friendly by giving our users most important high level",
    "start": "1384799",
    "end": "1392200"
  },
  {
    "text": "Matrix we also noticed that ironically sometimes our with the introduction of",
    "start": "1393880",
    "end": "1400279"
  },
  {
    "text": "service match debugging was more complicated since the architecture became more complicated now we have",
    "start": "1400279",
    "end": "1406520"
  },
  {
    "text": "multiple mite in the cor pass and when some errors happened it wasn't always",
    "start": "1406520",
    "end": "1412799"
  },
  {
    "text": "clear whether they were triggered by service mesh or not their infrastructure team was just involved into two more",
    "start": "1412799",
    "end": "1418240"
  },
  {
    "text": "incidents to assist product teams to to debug and this sometimes caused",
    "start": "1418240",
    "end": "1423600"
  },
  {
    "text": "frustrations on both sides so to provide Clarity in identifying issues we",
    "start": "1423600",
    "end": "1429520"
  },
  {
    "text": "introduced service Mass availability sros to show all arrows that are originated from M iar we also introduced",
    "start": "1429520",
    "end": "1437360"
  },
  {
    "text": "distributor tracing to have a better view to show our users which component in the service graph is actually",
    "start": "1437360",
    "end": "1443799"
  },
  {
    "text": "returning the arrow service graph so we realized many",
    "start": "1443799",
    "end": "1451080"
  },
  {
    "text": "features are actually enabled only when the E dependency are defined features",
    "start": "1451080",
    "end": "1456159"
  },
  {
    "text": "like the zom a outing alight detection and even for those most basic upst",
    "start": "1456159",
    "end": "1462520"
  },
  {
    "text": "strring level Matrix require users to Define their n cluster erass Custer in",
    "start": "1462520",
    "end": "1468360"
  },
  {
    "text": "their configurations and however uh users are not uh sure about their service graph",
    "start": "1468360",
    "end": "1475720"
  },
  {
    "text": "and there was basically no tools available for this purpose So eventually",
    "start": "1475720",
    "end": "1480880"
  },
  {
    "text": "we decided to build an accurate service graph from the tracing data that we just",
    "start": "1480880",
    "end": "1486720"
  },
  {
    "text": "introduced and then build a tool to generate those erress configurations",
    "start": "1486720",
    "end": "1491799"
  },
  {
    "text": "based on the tracing data so for the execution of the uh plan",
    "start": "1491799",
    "end": "1500039"
  },
  {
    "text": "we started massive adoption and since we're doing okay overall onboarding was",
    "start": "1500039",
    "end": "1506120"
  },
  {
    "text": "much faster we had a few unexpected issues for some Services which is kind",
    "start": "1506120",
    "end": "1511200"
  },
  {
    "text": "of expected uh resulting in around two or three incidents but uh fortunately we",
    "start": "1511200",
    "end": "1516279"
  },
  {
    "text": "were able to roll back fast before things went uh wild and there is some",
    "start": "1516279",
    "end": "1521559"
  },
  {
    "text": "additional maintenance responsibility on their infrastructure team which is I guess still maintainable now",
    "start": "1521559",
    "end": "1528520"
  },
  {
    "text": "many features like client side Lo balancing zoom over routing and header based routing are also widely",
    "start": "1528520",
    "end": "1536440"
  },
  {
    "text": "adopted so here is a quick overview of our current state microservices are deployed on around 10 production cries",
    "start": "1537320",
    "end": "1544559"
  },
  {
    "text": "clusters we have more than 500 microservices deployed in five isolated",
    "start": "1544559",
    "end": "1550200"
  },
  {
    "text": "production mes within each MCH there are multiple clusters and there are today",
    "start": "1550200",
    "end": "1555600"
  },
  {
    "text": "more than 10,000 parts and around 5 million RPS managed by mesh",
    "start": "1555600",
    "end": "1562880"
  },
  {
    "text": "nowadays so here's a quick overview of our current and future work developer",
    "start": "1563520",
    "end": "1568640"
  },
  {
    "text": "velocity became a prominent concern more efforts are now directed towards having",
    "start": "1568640",
    "end": "1574960"
  },
  {
    "text": "their experience of configuring mway more developer friendly and improving",
    "start": "1574960",
    "end": "1580159"
  },
  {
    "text": "their user experience of uh leveraging or observability related",
    "start": "1580159",
    "end": "1585520"
  },
  {
    "text": "features for efficiency the way we are currently uh save the cost of compute",
    "start": "1585520",
    "end": "1592159"
  },
  {
    "text": "and Matrix usage is uh still manual and we need tools tools to eliminate the",
    "start": "1592159",
    "end": "1598240"
  },
  {
    "text": "waste here we also trying to leverage mesh to simplify some other uh traffic",
    "start": "1598240",
    "end": "1605440"
  },
  {
    "text": "infrastructures uh so in this case we are actively working on some um new uh",
    "start": "1605440",
    "end": "1611960"
  },
  {
    "text": "architecture to build the multicluster service Discovery solution and and of",
    "start": "1611960",
    "end": "1617840"
  },
  {
    "text": "course of course we should uh continue to add more features to support our uh",
    "start": "1617840",
    "end": "1623240"
  },
  {
    "text": "support our users so all right so here is a quick",
    "start": "1623240",
    "end": "1629720"
  },
  {
    "text": "recap of what we have discussed so before you start the journey we figured",
    "start": "1629720",
    "end": "1634840"
  },
  {
    "text": "that really understanding your requirements and the use cases is important and you can Cod develop your",
    "start": "1634840",
    "end": "1642159"
  },
  {
    "text": "features with your initial customers and when working with mway we noticed that",
    "start": "1642159",
    "end": "1647600"
  },
  {
    "text": "it's hard to make thei car always transparent so you probably have to expect the unexpected in the initial",
    "start": "1647600",
    "end": "1653440"
  },
  {
    "text": "adoption phase when onboarding services are",
    "start": "1653440",
    "end": "1658600"
  },
  {
    "text": "testing things gradually at the beginning it makes them well informed B",
    "start": "1658600",
    "end": "1663960"
  },
  {
    "text": "at the right time and decentralized onboarding doesn't scale even though the process required low effort from the",
    "start": "1663960",
    "end": "1670840"
  },
  {
    "text": "users and try to stringline stringline and automate the onboarding process instead when delivering product produ to",
    "start": "1670840",
    "end": "1678200"
  },
  {
    "text": "the rest of the engineering team Eno Matrix could be overwhelming and have a",
    "start": "1678200",
    "end": "1684279"
  },
  {
    "text": "product mindset and be more customer uh obsessed when you build the product",
    "start": "1684279",
    "end": "1689440"
  },
  {
    "text": "invest some time in training and uh enabling service owners uh but",
    "start": "1689440",
    "end": "1694480"
  },
  {
    "text": "increasing the velocity through some uh more simpler and more uh automated solution is actually more uh",
    "start": "1694480",
    "end": "1702120"
  },
  {
    "start": "1702000",
    "end": "1945000"
  },
  {
    "text": "important all right uh that's all I have thank you",
    "start": "1702120",
    "end": "1708690"
  },
  {
    "text": "[Applause] I think we still have some time for a",
    "start": "1708690",
    "end": "1713960"
  },
  {
    "text": "question and you can go to the",
    "start": "1713960",
    "end": "1717840"
  },
  {
    "text": "mic I uh have to ask would you still develop a service mesh architecture from",
    "start": "1722200",
    "end": "1729919"
  },
  {
    "text": "scratch today if you started to try to solve the same problem",
    "start": "1729919",
    "end": "1736240"
  },
  {
    "text": "again uh um so I",
    "start": "1736240",
    "end": "1741480"
  },
  {
    "text": "guess with the with the current architecture we probably so I guess the",
    "start": "1742080",
    "end": "1748039"
  },
  {
    "text": "first thing is that we have to have some use cases to motivate us to probably",
    "start": "1748039",
    "end": "1754080"
  },
  {
    "text": "move some new architecture and wait I'm saying put on your uh like pretend you",
    "start": "1754080",
    "end": "1761159"
  },
  {
    "text": "have nothing right you're starting 2021 with uh tools and solutions that",
    "start": "1761159",
    "end": "1768440"
  },
  {
    "text": "are available today what um would you still build your",
    "start": "1768440",
    "end": "1774240"
  },
  {
    "text": "own uh I guess we will have to evaluate",
    "start": "1774240",
    "end": "1780919"
  },
  {
    "text": "the all the solutions again uh it's been two years",
    "start": "1780919",
    "end": "1786399"
  },
  {
    "text": "already and honestly a lot of our other traffic related",
    "start": "1786399",
    "end": "1792240"
  },
  {
    "text": "infrastructures are actually improved by the current service match architecture so it's kind of hard to say that we we",
    "start": "1792240",
    "end": "1799640"
  },
  {
    "text": "are given what we have today because what we have today has service match kind of tightly coupled in in the",
    "start": "1799640",
    "end": "1806480"
  },
  {
    "text": "architecture um but we are actually are evaluating some other Solutions as well",
    "start": "1806480",
    "end": "1813240"
  },
  {
    "text": "because we do have some uh use cases uh for our case we wanted to introduce our",
    "start": "1813240",
    "end": "1819480"
  },
  {
    "text": "Network policy and that's why we are actually eval evaluating celum to uh",
    "start": "1819480",
    "end": "1825519"
  },
  {
    "text": "introduce it as our cni first and then evaluate maybe we could average some",
    "start": "1825519",
    "end": "1831039"
  },
  {
    "text": "more L7 level uh features okay yeah uh and you already",
    "start": "1831039",
    "end": "1838039"
  },
  {
    "text": "know of some gaps uh maybe between something like an sto or",
    "start": "1838039",
    "end": "1845200"
  },
  {
    "text": "celium uh that they wouldn't be suitable I guess uh to solve the same",
    "start": "1845200",
    "end": "1850760"
  },
  {
    "text": "problems that that that you developed for I",
    "start": "1850760",
    "end": "1855919"
  },
  {
    "text": "think uh it's been a while last time when I check all the solutions but I guess the common concern nowadays is",
    "start": "1855919",
    "end": "1862720"
  },
  {
    "text": "still around the developer experience uh aside how to make the your life of",
    "start": "1862720",
    "end": "1868159"
  },
  {
    "text": "configuration all the configurations easier uh that's I guess some common",
    "start": "1868159",
    "end": "1873360"
  },
  {
    "text": "concern that even now with our customer Solutions we are trying to trying to solve and our current solution is trying",
    "start": "1873360",
    "end": "1880639"
  },
  {
    "text": "to expose all uh this kind of experience through our common interface we are",
    "start": "1880639",
    "end": "1885880"
  },
  {
    "text": "trying to develop some uh we portal to manage all these configurations yeah okay thanks",
    "start": "1885880",
    "end": "1895000"
  },
  {
    "text": "yeah so I noticed that you talked a lot about how you're using console for",
    "start": "1895000",
    "end": "1900559"
  },
  {
    "text": "service Discovery did you evaluate using console connect as a service mesh we did",
    "start": "1900559",
    "end": "1907880"
  },
  {
    "text": "we did uh tried uh we did reach out to conso folks in the community um I",
    "start": "1907880",
    "end": "1917720"
  },
  {
    "text": "I guess I if I still remember correctly we were",
    "start": "1917720",
    "end": "1923080"
  },
  {
    "text": "mainly cons you know we wanted the mature the most mature Sol uh solution because we only we were really a small",
    "start": "1923080",
    "end": "1929799"
  },
  {
    "text": "team so we tend to be a little bit more conservative and just try the most",
    "start": "1929799",
    "end": "1935000"
  },
  {
    "text": "mature solution at the time so that's why we probably tried the wasar approach",
    "start": "1935000",
    "end": "1941080"
  },
  {
    "text": "um that makes sense thank you yeah hello um I kind of wondering like",
    "start": "1941080",
    "end": "1948360"
  },
  {
    "start": "1945000",
    "end": "2262000"
  },
  {
    "text": "given the limited engineer resource back in the 2019 what's the most motivation",
    "start": "1948360",
    "end": "1954000"
  },
  {
    "text": "for you to build your own control plan instead of using some other Solutions like Linker d or2 right we actually",
    "start": "1954000",
    "end": "1960639"
  },
  {
    "text": "spent a lot of time trying to make the existing open source Solutions work for us uh so I can probably give some",
    "start": "1960639",
    "end": "1969519"
  },
  {
    "text": "examples so we try Linker d uh first and as I mentioned we have a big cluster",
    "start": "1969519",
    "end": "1976080"
  },
  {
    "text": "that has 2,000 not and we were kind of asking around hey uh you know what's",
    "start": "1976080",
    "end": "1981240"
  },
  {
    "text": "your biggest cluster you can support and the story we heard was from other users was around 400 not which makes us a",
    "start": "1981240",
    "end": "1988399"
  },
  {
    "text": "little bit scared and the way Linker support the multicluster story uh is a",
    "start": "1988399",
    "end": "1995720"
  },
  {
    "text": "little bit open and that cannot support our own uh conso based solution so I",
    "start": "1995720",
    "end": "2001320"
  },
  {
    "text": "guess we were also another thing is our traffic team also work uh leveraging the",
    "start": "2001320",
    "end": "2008639"
  },
  {
    "text": "mway proxy to manage the edge traffic so it's probably more makes more sense to consolidate the the effort for the data",
    "start": "2008639",
    "end": "2015200"
  },
  {
    "text": "plan there uh is uh a couple of my colleagues tried",
    "start": "2015200",
    "end": "2020600"
  },
  {
    "text": "Isel but they were uh initially scared by their complexity of their all the",
    "start": "2020600",
    "end": "2027480"
  },
  {
    "text": "configurations to basically set up the most basic uh uh",
    "start": "2027480",
    "end": "2032760"
  },
  {
    "text": "configurations and we had some concerns about the control plan at the time that",
    "start": "2032760",
    "end": "2038840"
  },
  {
    "text": "was even before uh before uh I think",
    "start": "2038840",
    "end": "2043880"
  },
  {
    "text": "everyone we still move to the more listic related architecture So",
    "start": "2043880",
    "end": "2049480"
  },
  {
    "text": "eventually we had to uh decided to build our own yeah but that's a that's a good",
    "start": "2049480",
    "end": "2054960"
  },
  {
    "text": "point we we are actually trying to make us not uh be locked into this kind of",
    "start": "2054960",
    "end": "2062839"
  },
  {
    "text": "self uh you know inhouse solution and we are trying to leverage some uh opportunities to probably uh move to",
    "start": "2062839",
    "end": "2070599"
  },
  {
    "text": "some uh new architecture eventually if that's possible that's why I mentioned",
    "start": "2070599",
    "end": "2075679"
  },
  {
    "text": "we Pro we are evaluating using celum as the cni as the starting point got it",
    "start": "2075679",
    "end": "2081079"
  },
  {
    "text": "thank you yeah you mentioned availability Zone aware routing um could you elaborate on",
    "start": "2081079",
    "end": "2088760"
  },
  {
    "text": "that a little bit more is it uh preferring pods in the same a within a cluster and just how do you prevent",
    "start": "2088760",
    "end": "2096079"
  },
  {
    "text": "things from becoming imbalanced if say the source has more pods in one place destination a different place um so the",
    "start": "2096079",
    "end": "2104560"
  },
  {
    "text": "the traffic is controlled by the data plan so what we are doing in the control plan is to read all the IP addresses",
    "start": "2104560",
    "end": "2111280"
  },
  {
    "text": "with all the ad information from the source of shoose console in this case and then ship the all uh IP addresses",
    "start": "2111280",
    "end": "2119160"
  },
  {
    "text": "and uh you know all the those EDS resources back to the m car and actually",
    "start": "2119160",
    "end": "2125119"
  },
  {
    "text": "uh we are just leveraging the data to actually perform the zoom aware",
    "start": "2125119",
    "end": "2131880"
  },
  {
    "text": "outing okay",
    "start": "2132680",
    "end": "2136680"
  },
  {
    "text": "yeah so I was curious about the you mentioned you had some challenges when",
    "start": "2139520",
    "end": "2144599"
  },
  {
    "text": "you build some of these things developers didn't know how to debug so they were asking the product team to debug and you ended up adding some",
    "start": "2144599",
    "end": "2152640"
  },
  {
    "text": "observability so first of all was that enough were there still use cases where",
    "start": "2152640",
    "end": "2158119"
  },
  {
    "text": "developers had to debug no how did you that's definitely not enough I guess",
    "start": "2158119",
    "end": "2164000"
  },
  {
    "text": "that action item was basically to unblock us to move faster in 23 and the",
    "start": "2164000",
    "end": "2172359"
  },
  {
    "text": "debuging experience nowadays is still uh not great their current uh direction is",
    "start": "2172359",
    "end": "2180240"
  },
  {
    "text": "try to leverage unify unify all the data from all the different uh sources Matrix",
    "start": "2180240",
    "end": "2188079"
  },
  {
    "text": "logs and traces and uh just give a",
    "start": "2188079",
    "end": "2193280"
  },
  {
    "text": "better interface uh to our users and we are also actually evaluating building",
    "start": "2193280",
    "end": "2199880"
  },
  {
    "text": "some automate automated tools to help us to basically uh summarize what is going",
    "start": "2199880",
    "end": "2205680"
  },
  {
    "text": "on using uh using AI as well okay so one last question so looks like you made a",
    "start": "2205680",
    "end": "2211119"
  },
  {
    "text": "lot of design choices architectural choices was it just one team making these decisions were they multiple teams",
    "start": "2211119",
    "end": "2217960"
  },
  {
    "text": "and what kind of process and red tape did you have to go through to uh it's decided by their traffic and Compu team",
    "start": "2217960",
    "end": "2226319"
  },
  {
    "text": "are within the cor infrastructure or and we do actually whenever we make big",
    "start": "2226319",
    "end": "2231440"
  },
  {
    "text": "changes we do propose the RFC to to their engineering org",
    "start": "2231440",
    "end": "2238119"
  },
  {
    "text": "yeah uh honestly for the service match team usually it's been just one or two",
    "start": "2241640",
    "end": "2246680"
  },
  {
    "text": "Engineers uh for a while but now we have around you know it's drawing the uh we",
    "start": "2246680",
    "end": "2253280"
  },
  {
    "text": "move the service smesh project into the traffic team now and the traffic team currently has around 10ish Engineers",
    "start": "2253280",
    "end": "2261920"
  },
  {
    "text": "yeah",
    "start": "2261920",
    "end": "2264920"
  }
]