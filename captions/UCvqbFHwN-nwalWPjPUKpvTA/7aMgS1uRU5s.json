[
  {
    "text": "hi uh so welcome to our talk uh it's multi-cluster made reasonable envoy",
    "start": "80",
    "end": "5600"
  },
  {
    "text": "service mesh control plane so i'm paul fisher i'm the tech lead on",
    "start": "5600",
    "end": "11759"
  },
  {
    "text": "the compute platform at lyft and i've been working on kubernetes since its inception here at the company",
    "start": "11759",
    "end": "17600"
  },
  {
    "text": "it's been multiple years that we've been working on the platform and also today um ashley kasim will also be presenting",
    "start": "17600",
    "end": "26720"
  },
  {
    "text": "she'll take over later in the talk to do a demo of diplomat okay in the agenda for what",
    "start": "26720",
    "end": "35120"
  },
  {
    "text": "we're going to cover today i'll give an overview of lyft's environment as well as our kubernetes environment",
    "start": "35120",
    "end": "42399"
  },
  {
    "text": "in addition to covering the lift envoy environment and then actually we'll",
    "start": "42399",
    "end": "48559"
  },
  {
    "text": "cover our multi-cluster setup as well as diplomat which is our multi-cluster",
    "start": "48559",
    "end": "54480"
  },
  {
    "text": "control plane that she'll talk more about um and also go through a demo of diplomat",
    "start": "54480",
    "end": "62239"
  },
  {
    "text": "okay so those of you that might not be familiar with lyft particularly if you're in europe uh so we're a ride",
    "start": "66560",
    "end": "73360"
  },
  {
    "text": "share network that's in all 50 us states toronto ottawa and we recently launched in",
    "start": "73360",
    "end": "79680"
  },
  {
    "text": "vancouver as well and in addition to the car ride network we also have a scooter and bike",
    "start": "79680",
    "end": "88080"
  },
  {
    "text": "share networks in the u.s largest of which are a city bike in new",
    "start": "88080",
    "end": "93200"
  },
  {
    "text": "york city as well as bay wheels and the san francisco bay area",
    "start": "93200",
    "end": "98320"
  },
  {
    "text": "we also have transit partnerships and 11 markets that includes things like bus schedules",
    "start": "98320",
    "end": "104000"
  },
  {
    "text": "and subway times and the idea is just to get is to get you between points as",
    "start": "104000",
    "end": "109759"
  },
  {
    "text": "efficiently as possible and we also have uh autonomous musical partnerships",
    "start": "109759",
    "end": "114880"
  },
  {
    "text": "in las vegas and phoenix so now it talks about like sort of lift",
    "start": "114880",
    "end": "120799"
  },
  {
    "text": "scale there's also the kubernetes scale that that we have um so this kind of falls into three",
    "start": "120799",
    "end": "126640"
  },
  {
    "text": "categories uh the first that migrated onto our platform is sort of machine learning",
    "start": "126640",
    "end": "132879"
  },
  {
    "text": "which fits naturally with uh with kubernetes so our ml training jobs",
    "start": "132879",
    "end": "138160"
  },
  {
    "text": "trooper notebooks all of our cpu workloads exist there and they were the first to migrate over",
    "start": "138160",
    "end": "144480"
  },
  {
    "text": "there's also flight which was uh open sourced at the last kubecon us there's lots of",
    "start": "144480",
    "end": "151360"
  },
  {
    "text": "documentation out there about flight but that's how i run spark and hive and then sort of the largest set of",
    "start": "151360",
    "end": "157680"
  },
  {
    "text": "clusters that we run here at lyft are related to the rideshare platform",
    "start": "157680",
    "end": "163519"
  },
  {
    "text": "and that's about 600 plus stateless microservices we run multiple clusters in each",
    "start": "163519",
    "end": "170879"
  },
  {
    "text": "availability zone and those participate in a single envoy mesh uh in terms of scale uh it's about 30",
    "start": "170879",
    "end": "178879"
  },
  {
    "text": "000 or so pods that would rather aggressively scale up and down based on load",
    "start": "178879",
    "end": "184400"
  },
  {
    "text": "and then the number of containers is about an order of magnitude larger so we have lots of side cars",
    "start": "184400",
    "end": "189519"
  },
  {
    "text": "one of which is envoy and about 80 000 plus cores involved",
    "start": "189519",
    "end": "196560"
  },
  {
    "text": "with that",
    "start": "196560",
    "end": "199840"
  },
  {
    "text": "so in terms of timeline of kubernetes at lyft we sort of started the",
    "start": "202080",
    "end": "209120"
  },
  {
    "text": "containerization effort if you will in making use of docker back in 2015. we had a homegrown",
    "start": "209120",
    "end": "216480"
  },
  {
    "text": "system which sort of looked a bit like a docker swarm uh it wasn't until 2017 that we started",
    "start": "216480",
    "end": "222000"
  },
  {
    "text": "investigating options for running kubernetes on aws and in particular we realized that it",
    "start": "222000",
    "end": "227440"
  },
  {
    "text": "was necessary to have a networking platform that could interoperate with the existing",
    "start": "227440",
    "end": "233920"
  },
  {
    "text": "legacy stack or what we call the legacy stack today and so we worked aggressively to um",
    "start": "233920",
    "end": "241439"
  },
  {
    "text": "to get that platform in place uh we open sourced that work in december 2017",
    "start": "241439",
    "end": "246640"
  },
  {
    "text": "uh and that um it's part of our rcni stack um amazon also released their open",
    "start": "246640",
    "end": "252879"
  },
  {
    "text": "source cni stacker on the same time and the feature set is pretty similar between the two um and as i mentioned",
    "start": "252879",
    "end": "258720"
  },
  {
    "text": "before the batch and ml workloads were the first to sort of migrate over we've pretty much finished our migration",
    "start": "258720",
    "end": "266000"
  },
  {
    "text": "of all of our stateless services to kubernetes that was work that was done uh last year it's all the tier zero tier one",
    "start": "266000",
    "end": "272400"
  },
  {
    "text": "all the other core services run on kubernetes that lift and this year we're focused more on",
    "start": "272400",
    "end": "278880"
  },
  {
    "text": "stateful migrations combination of what we call semi-stateful and stateful semi-stateful",
    "start": "278880",
    "end": "286000"
  },
  {
    "text": "are things that just have um a fairly low tolerance for pod disruption and we don't aggressively",
    "start": "286000",
    "end": "292479"
  },
  {
    "text": "been packed there whereas the stateful services usually have like local ssd and",
    "start": "292479",
    "end": "297919"
  },
  {
    "text": "they're they're instances where we can't necessarily terminate the nodes without there being",
    "start": "297919",
    "end": "304160"
  },
  {
    "text": "impact to the service",
    "start": "304160",
    "end": "308080"
  },
  {
    "text": "okay so a little bit about uh our environment we're on kubernetes 116 and sort of a never-ending upgrade train",
    "start": "309280",
    "end": "315840"
  },
  {
    "text": "uh we're currently working to get uh updated to 118. uh we run",
    "start": "315840",
    "end": "321600"
  },
  {
    "text": "fedora 31 currently so we like came back one release from the current production version we",
    "start": "321600",
    "end": "329039"
  },
  {
    "text": "use cryo as our cri environment and we're currently in the process",
    "start": "329039",
    "end": "334400"
  },
  {
    "text": "of moving to core os we use ubuntu on the developer side",
    "start": "334400",
    "end": "341680"
  },
  {
    "text": "so it's basically like a fedora minimal shim os running a bunch of containers for the actual lift services",
    "start": "341680",
    "end": "348240"
  },
  {
    "text": "everything's mutable in the fedora world we use packer and in the core s world we use ignition",
    "start": "348240",
    "end": "355199"
  },
  {
    "text": "and all that's orchestrated via terraform uh so i think i mentioned earlier we're",
    "start": "355199",
    "end": "360720"
  },
  {
    "text": "in aws we use lots and lots of ec2 and ebs and s3 and our primary build outs are in us",
    "start": "360720",
    "end": "367199"
  },
  {
    "text": "east 1 and us west 2. so we have multiple render per ac clusters",
    "start": "367199",
    "end": "373600"
  },
  {
    "text": "so while we do have ongoing meshes that span those clusters the back planes of those clusters don't",
    "start": "373600",
    "end": "379360"
  },
  {
    "text": "communicate to each other and the goal there is to have the ability to do staggered rollouts and",
    "start": "379360",
    "end": "384639"
  },
  {
    "text": "limited flash radius so if we lose a single cluster it doesn't affect the lift service as a",
    "start": "384639",
    "end": "389919"
  },
  {
    "text": "whole it also means that if there are a z issues within aws it's limited to to one or just a set of",
    "start": "389919",
    "end": "396720"
  },
  {
    "text": "clusters um and as i mentioned before we have our rc knife back and this is really sort of",
    "start": "396720",
    "end": "402479"
  },
  {
    "text": "critical to allowing us to bridge multiple clusters together because the",
    "start": "402479",
    "end": "408960"
  },
  {
    "text": "pods are directly part of the envoy mesh",
    "start": "408960",
    "end": "413759"
  },
  {
    "text": "and related to that and the reason why we can sort of have this multi-cluster environment that works as",
    "start": "414560",
    "end": "421280"
  },
  {
    "text": "we have to keep well rather we've chosen to keep things as simple as possible",
    "start": "421280",
    "end": "426479"
  },
  {
    "text": "so there's no overlay networks there's no bgp we don't use nat we use very limited use of ingress we",
    "start": "426479",
    "end": "433280"
  },
  {
    "text": "don't use um cubeproxy as i mentioned before pods can communicate with pods uh",
    "start": "433280",
    "end": "438400"
  },
  {
    "text": "in any cluster and then envoy sort of binds all that together so a lot of the features that kubernetes uses for",
    "start": "438400",
    "end": "445599"
  },
  {
    "text": "development or proof of concepting or like makes it so easy to get up and running we tend not to use that in production as",
    "start": "445599",
    "end": "452080"
  },
  {
    "text": "we find it scales better to keep things simpler",
    "start": "452080",
    "end": "457039"
  },
  {
    "text": "so we have this this vpc native network where pods receive vpcip addresses",
    "start": "461199",
    "end": "467599"
  },
  {
    "text": "native network performance and the the two main plugin options there are our stack as well as the",
    "start": "467599",
    "end": "475599"
  },
  {
    "text": "aws stack but this is really what allows envoy to be able to um",
    "start": "475599",
    "end": "482240"
  },
  {
    "text": "or the animal we mesh to span multiple clusters",
    "start": "482240",
    "end": "486240"
  },
  {
    "text": "and i'll cover a bit of the lift envoy environment",
    "start": "488160",
    "end": "493599"
  },
  {
    "text": "so we have two onboard meshes there's one staging mesh and one very large",
    "start": "493599",
    "end": "498639"
  },
  {
    "text": "production mesh we're currently in the process of moving the production match to",
    "start": "498639",
    "end": "503919"
  },
  {
    "text": "a per um and that works ongoing by the network team to create",
    "start": "503919",
    "end": "509440"
  },
  {
    "text": "the split mesh environment and then we have one main envoy front proxy and that handles lift.com and then",
    "start": "509440",
    "end": "517839"
  },
  {
    "text": "fans out to other envoys as mentioned a lot of kubernetes clusters participate in the",
    "start": "517839",
    "end": "524000"
  },
  {
    "text": "mesh and we have one vpcip per kubernetes pod and then our control plane is a software",
    "start": "524000",
    "end": "532000"
  },
  {
    "text": "called envoy manager which actually we'll talk about here in a bit",
    "start": "532000",
    "end": "537920"
  },
  {
    "text": "to cover our cni stack and this sort of comes with trying to keep things as simple as possible",
    "start": "538000",
    "end": "544160"
  },
  {
    "text": "as we as we scale the clusters out it's a very minimal design there's",
    "start": "544160",
    "end": "551519"
  },
  {
    "text": "no demon sets there are no pods i don't have any runtimes it's just a handful of gobinaries",
    "start": "551519",
    "end": "556560"
  },
  {
    "text": "and most of the developments been done here at lyft with cryo datadog is also done a decent amount of",
    "start": "556560",
    "end": "563279"
  },
  {
    "text": "development and they run into production as well with container id but it's pretty much future complete and",
    "start": "563279",
    "end": "569040"
  },
  {
    "text": "it's been running for for three years and has really enabled",
    "start": "569040",
    "end": "575360"
  },
  {
    "text": "up to uh to work in terms of what the the mesh looks like on the production",
    "start": "575360",
    "end": "580560"
  },
  {
    "text": "side so we have five main production",
    "start": "580560",
    "end": "585839"
  },
  {
    "text": "clusters one per az for the for the stateless side and",
    "start": "585839",
    "end": "591279"
  },
  {
    "text": "that's where the bulk of the services are deployed to so when you deploy its service at lift",
    "start": "591279",
    "end": "596560"
  },
  {
    "text": "it ends up getting deployed across all of those core clusters and there's a bit of an abstraction",
    "start": "596560",
    "end": "602800"
  },
  {
    "text": "layer where end developers don't have to really be concerned or worried about",
    "start": "602800",
    "end": "608000"
  },
  {
    "text": "where their services are are deployed to in terms of when they interact with them",
    "start": "608000",
    "end": "613440"
  },
  {
    "text": "in addition to the mesh we still have our legacy stack which are ec2 vms",
    "start": "613440",
    "end": "619120"
  },
  {
    "text": "those participate in the mesh as well as they have ips just like the pods do",
    "start": "619120",
    "end": "624720"
  },
  {
    "text": "and then there's also the stable clusters as well that participate in the production mesh",
    "start": "624720",
    "end": "630320"
  },
  {
    "text": "and now i'll hand it over to ashley to talk about our multi-cluster setup",
    "start": "630320",
    "end": "636720"
  },
  {
    "text": "all right so first let's start by going over some terminology in particular where there's some naming",
    "start": "639360",
    "end": "646079"
  },
  {
    "text": "collisions across kubernetes and envoy so can get a little bit confusing",
    "start": "646079",
    "end": "651839"
  },
  {
    "text": "so there's two types of clusters we'll be dealing with kubernetes clusters which are a",
    "start": "651839",
    "end": "657680"
  },
  {
    "text": "collection of worker nodes running pods orchestrated by one or more api servers",
    "start": "657680",
    "end": "663360"
  },
  {
    "text": "and then there's envoy clusters onboard clusters are a collection of endpoints comprising a",
    "start": "663360",
    "end": "669200"
  },
  {
    "text": "service in diplomat we have mapped an envoy cluster to a kubernetes service an",
    "start": "669200",
    "end": "678160"
  },
  {
    "text": "endpoint is an envoy cluster member with a unique ip in port pairing which",
    "start": "678160",
    "end": "684000"
  },
  {
    "text": "in our implementation we've mapped to a kubernetes endpoint",
    "start": "684000",
    "end": "689360"
  },
  {
    "text": "and then as for envoy itself there's a few parts that we primarily are going to",
    "start": "689360",
    "end": "694399"
  },
  {
    "text": "be dealing with the first is xps or envoys dynamic discovery apis the two xds",
    "start": "694399",
    "end": "702560"
  },
  {
    "text": "services relevant to diplomat are cds or cluster discovery service which",
    "start": "702560",
    "end": "708240"
  },
  {
    "text": "registers onboard clusters and eds or endpoint discovery service",
    "start": "708240",
    "end": "713360"
  },
  {
    "text": "which registers envoy endpoints diplomat is implemented as an example in",
    "start": "713360",
    "end": "719360"
  },
  {
    "text": "go control plane which is a reference open source go implementation of the xds apis which can be used to",
    "start": "719360",
    "end": "726560"
  },
  {
    "text": "build custom envoy control planes such as diplomat or envoy manager",
    "start": "726560",
    "end": "734399"
  },
  {
    "text": "the control plane that lift runs internally as paul mentioned is called envoy manager envoy manager is",
    "start": "737040",
    "end": "744720"
  },
  {
    "text": "implemented on top of the go control plane it uses kubernetes informers and our in-house",
    "start": "744720",
    "end": "751440"
  },
  {
    "text": "kubernetes cluster discovery implementation to watch pod status and bridge the mesh across our end",
    "start": "751440",
    "end": "757600"
  },
  {
    "text": "kubernetes clusters we're often asked if we're ever going to",
    "start": "757600",
    "end": "762880"
  },
  {
    "text": "open source onboard manager and generally the answer that we have given is no",
    "start": "762880",
    "end": "768560"
  },
  {
    "text": "for the simple reason that envoy manager is very organization specific and encompasses a",
    "start": "768560",
    "end": "775680"
  },
  {
    "text": "lot of legacy functionality that reflects the evolution of infrastructure at lyft and therefore",
    "start": "775680",
    "end": "782959"
  },
  {
    "text": "making it unsuitable for direct adoption elsewhere in addition envoy manager predates the",
    "start": "782959",
    "end": "790000"
  },
  {
    "text": "control plane and as a consequence it has not yet been updated to take advantage",
    "start": "790000",
    "end": "795040"
  },
  {
    "text": "of all of go control plane's features which is another reason why it's unlikely to be open sourced",
    "start": "795040",
    "end": "801519"
  },
  {
    "text": "next we're going to dive into the implementation details of envoy manager a little bit",
    "start": "801519",
    "end": "808480"
  },
  {
    "text": "so here we have a diagram that describes the architecture of the environment manager deployment at",
    "start": "809839",
    "end": "815920"
  },
  {
    "text": "lyft lyft service pods run an on by sidecar container and these sidecars use",
    "start": "815920",
    "end": "821920"
  },
  {
    "text": "dns to discover envoy manager which is run as a cluster local headless kubernetes service lift",
    "start": "821920",
    "end": "828560"
  },
  {
    "text": "services run pods on n clusters which are independent and redundant such as we are resilient to a",
    "start": "828560",
    "end": "834320"
  },
  {
    "text": "cluster outage envoy manager is also fault tolerant or we have deployed multiple independent",
    "start": "834320",
    "end": "840800"
  },
  {
    "text": "envoy manager replicas across these n clusters and our envelope manager deployment can",
    "start": "840800",
    "end": "845839"
  },
  {
    "text": "auto scale with load environment manager's kubernetes support was originally implemented using",
    "start": "845839",
    "end": "851279"
  },
  {
    "text": "endpoint formers but has since switched to using pod informers there's going to be a link later to a",
    "start": "851279",
    "end": "857279"
  },
  {
    "text": "talk that goes more in depth as to this design decision later later on in this presentation",
    "start": "857279",
    "end": "863120"
  },
  {
    "text": "this currently runs a single service mesh where all source pods across these n clusters are part of the",
    "start": "863120",
    "end": "869360"
  },
  {
    "text": "same mesh um so let's take a closer look at what's happening in this diagram",
    "start": "869360",
    "end": "875600"
  },
  {
    "text": "here the service foo has two pods the envoy sidecar in each of them is",
    "start": "875600",
    "end": "881600"
  },
  {
    "text": "going to discover their cluster cluster local envoy manager via dns",
    "start": "881600",
    "end": "887040"
  },
  {
    "text": "where em.service.culture.local is a grpc load balanced across the envoy manager replicas in",
    "start": "887040",
    "end": "894800"
  },
  {
    "text": "that cluster each envoy manager is then talking to the api",
    "start": "894800",
    "end": "899839"
  },
  {
    "text": "servers of kubernetes clusters zero and one and this means that each of these envoy",
    "start": "899839",
    "end": "905680"
  },
  {
    "text": "managers has a complete picture of the whole state of the world across the clusters even though each kubernetes cluster is",
    "start": "905680",
    "end": "912480"
  },
  {
    "text": "remaining completely independent",
    "start": "912480",
    "end": "916160"
  },
  {
    "text": "and now on to diplomat diplomat is essentially a minimal envoy manager",
    "start": "920560",
    "end": "927680"
  },
  {
    "text": "implementation which simplifies the eds control loop and strips out all that legacy and",
    "start": "927680",
    "end": "934000"
  },
  {
    "text": "with specific implementation to make it more generic and adaptable to the needs of",
    "start": "934000",
    "end": "940160"
  },
  {
    "text": "other organizations and the open source community like envoy manager it is based on the go",
    "start": "940160",
    "end": "945920"
  },
  {
    "text": "control plane and it supports multi-cluster and currently we have implemented im authentication",
    "start": "945920",
    "end": "953680"
  },
  {
    "text": "and aws because that's what lift use the off implementation is extensible however so with a bit of code",
    "start": "953680",
    "end": "961360"
  },
  {
    "text": "other cloud providers could easily be",
    "start": "961360",
    "end": "964959"
  },
  {
    "text": "added so as i mentioned the implementation of",
    "start": "966839",
    "end": "972399"
  },
  {
    "text": "diplomat is similar to envoy manager at the start it registers the n kubernetes clusters",
    "start": "972399",
    "end": "979600"
  },
  {
    "text": "that is configured to be aware of and spawns kubernetes and point informers for each of them",
    "start": "979600",
    "end": "987360"
  },
  {
    "text": "the informers implement an a hook on endpoint update which collects the new",
    "start": "987360",
    "end": "994639"
  },
  {
    "text": "endpoint data and known and point data from the other clusters and then performs an update to the",
    "start": "994639",
    "end": "999759"
  },
  {
    "text": "diplomat's snapshot cache insert informing the service pod on voice sidecars",
    "start": "999759",
    "end": "1005120"
  },
  {
    "text": "of the new envoy or the new endpoint via the envoy endpoint discovery service and then in",
    "start": "1005120",
    "end": "1012160"
  },
  {
    "text": "this diagram we have n kubernetes clusters and each of them has an endpoint informer",
    "start": "1012160",
    "end": "1018399"
  },
  {
    "text": "when an informer detects a kubernetes endpoint update it triggers the diplomat xds update",
    "start": "1018399",
    "end": "1024480"
  },
  {
    "text": "logic if the update contains a new service that the service mesh did not previously",
    "start": "1024480",
    "end": "1031038"
  },
  {
    "text": "know about an envoy cds or clustered discovery service update is performed to",
    "start": "1031039",
    "end": "1038240"
  },
  {
    "text": "register the new service in the mesh if the informer update contains a modification to",
    "start": "1038240",
    "end": "1043760"
  },
  {
    "text": "a kubernetes endpoint then diplomat is going to trigger an envoy eds",
    "start": "1043760",
    "end": "1048880"
  },
  {
    "text": "or endpoint discovery service update you'll see an example on how this works",
    "start": "1048880",
    "end": "1054799"
  },
  {
    "text": "in our upcoming demo",
    "start": "1054799",
    "end": "1058080"
  },
  {
    "text": "there are a number of improvements in the envoy and kubernetes communities that are relevant to the",
    "start": "1062880",
    "end": "1069600"
  },
  {
    "text": "future roadmap for envoy manager and or diplomat the first is the beta",
    "start": "1069600",
    "end": "1075200"
  },
  {
    "text": "introduction of endpoint slices in kubernetes 117 which provide a more scalable",
    "start": "1075200",
    "end": "1080559"
  },
  {
    "text": "implementation for tracking larger numbers of network endpoints",
    "start": "1080559",
    "end": "1085600"
  },
  {
    "text": "and then as i had mentioned earlier lyft has kind of gone back and forth between watching pod and endpoint events",
    "start": "1085600",
    "end": "1091919"
  },
  {
    "text": "in envoy manager envoy manager is currently using a pod informer but we are aiming to move back to an",
    "start": "1091919",
    "end": "1098720"
  },
  {
    "text": "endpoint informer in the future an excellent deep dive of the trade-offs of these approaches and",
    "start": "1098720",
    "end": "1104960"
  },
  {
    "text": "our experience in making the list service mesh kubernetes compatible can be found in",
    "start": "1104960",
    "end": "1110000"
  },
  {
    "text": "the envoy con talk that's linked finally the envoy upstream community has an open issue",
    "start": "1110000",
    "end": "1116559"
  },
  {
    "text": "around envoy health checking being unaware of kubernetes pod planned removal this is",
    "start": "1116559",
    "end": "1122320"
  },
  {
    "text": "one of the main issues with using an endpoint informer with your onvoice control plane currently as kubernetes endpoints are updated only",
    "start": "1122320",
    "end": "1129600"
  },
  {
    "text": "once a pod is torn down so you may not be able to update eds endpoints fast enough",
    "start": "1129600",
    "end": "1135200"
  },
  {
    "text": "to prevent envoy from seeing failed health checks uh from the draining pods and thinking that the service",
    "start": "1135200",
    "end": "1140799"
  },
  {
    "text": "is in fact unhealthy",
    "start": "1140799",
    "end": "1144399"
  },
  {
    "text": "and now we have a demo for you",
    "start": "1146880",
    "end": "1151840"
  },
  {
    "text": "as you can see we have two eks clusters in this account in each of them we are running a few",
    "start": "1153679",
    "end": "1159280"
  },
  {
    "text": "components a front envoy proxy service one and service two services which we want to connect using our",
    "start": "1159280",
    "end": "1165039"
  },
  {
    "text": "service mesh and implement our onboard control plane that implements envoy eds for kubernetes endpoints",
    "start": "1165039",
    "end": "1173279"
  },
  {
    "text": "envoy sidecars are configured to receive endpoint updates from diplomat for clusters configured",
    "start": "1175039",
    "end": "1180799"
  },
  {
    "text": "here we see service wanted service 2 registered as envoy clusters",
    "start": "1180799",
    "end": "1192960"
  },
  {
    "text": "these pods have the corresponding endpoints",
    "start": "1192960",
    "end": "1197120"
  },
  {
    "text": "in addition the services we are adding to the mesh have a custom label xcs equals true on their service object that",
    "start": "1199280",
    "end": "1205600"
  },
  {
    "text": "diplomat will use to filter watchers for endpoint updates",
    "start": "1205600",
    "end": "1210720"
  },
  {
    "text": "if we enter the sidecar envoy container for one of our service pods we can see the endpoints envoy has added",
    "start": "1211200",
    "end": "1217280"
  },
  {
    "text": "for the service by curling the envoy admin end point",
    "start": "1217280",
    "end": "1222399"
  },
  {
    "text": "if we exec into front envoy we can send traffic to the service 1 or service 2 cluster",
    "start": "1232640",
    "end": "1238000"
  },
  {
    "text": "and it will be load balanced across the two ips each service has",
    "start": "1238000",
    "end": "1243520"
  },
  {
    "text": "now let's delete a pod for fun let's delete one that's in the other cluster so for example let's take a look at the",
    "start": "1243600",
    "end": "1250240"
  },
  {
    "text": "endpoint for service 2. as you can see it has an ip address now we can delete this pod",
    "start": "1250240",
    "end": "1255840"
  },
  {
    "text": "and the kubernetes deployment recreates the pod and updates the kubernetes endpoint object with a new pod ip",
    "start": "1255840",
    "end": "1261440"
  },
  {
    "text": "address the kubernetes endpoint informer implemented by diplomat sees this update event and triggers an",
    "start": "1261440",
    "end": "1267600"
  },
  {
    "text": "update to envoy eds so going back to our service 1 pod in the demo 2 cluster we can again curl the envoy admin",
    "start": "1267600",
    "end": "1274640"
  },
  {
    "text": "endpoint and see that the ips for service 2 have been updated to include the new ip and the updated endpoint",
    "start": "1274640",
    "end": "1282400"
  },
  {
    "text": "front envoy has also been informed of the update as well and if we go back to that container we",
    "start": "1284559",
    "end": "1289760"
  },
  {
    "text": "can continue to send traffic to service 2 with a new pod now registered in the",
    "start": "1289760",
    "end": "1296080"
  },
  {
    "text": "mesh",
    "start": "1302840",
    "end": "1305840"
  },
  {
    "text": "um all right um hopefully this was valuable in providing a pattern to build",
    "start": "1316840",
    "end": "1323840"
  },
  {
    "text": "multi kubernetes cluster envoy meshes uh and the corresponding control plane",
    "start": "1323840",
    "end": "1329360"
  },
  {
    "text": "uh the diplomat source code can be found on lyft's public fork of go control plane on the",
    "start": "1329360",
    "end": "1334640"
  },
  {
    "text": "diplomat branch contributions are welcome so please check it out in addition lyft is hiring and check out",
    "start": "1334640",
    "end": "1341520"
  },
  {
    "text": "the link here to see more information on the opportunities available and thank you very much for coming to",
    "start": "1341520",
    "end": "1346640"
  },
  {
    "text": "our talk",
    "start": "1346640",
    "end": "1361840"
  },
  {
    "text": "so not sure if there is audio or not on the demo if for some reason you didn't get",
    "start": "1370960",
    "end": "1377120"
  },
  {
    "text": "audio on that we can upload the demo to slack so you can watch it back later",
    "start": "1377120",
    "end": "1389840"
  },
  {
    "text": "okay um so i guess we should go through um some of the questions that came in um",
    "start": "1391840",
    "end": "1399440"
  },
  {
    "text": "whether or not we're using incremental xds that's a project that's in progress",
    "start": "1399440",
    "end": "1405440"
  },
  {
    "text": "right now but no not currently",
    "start": "1405440",
    "end": "1416640"
  },
  {
    "text": "uh question about",
    "start": "1416640",
    "end": "1420000"
  },
  {
    "text": "[Music] sorry about the lag go ahead ashley",
    "start": "1422880",
    "end": "1430000"
  },
  {
    "text": "yeah uh so i guess the next question is about uh envoys containing clusters containing",
    "start": "1430000",
    "end": "1435279"
  },
  {
    "text": "information about service endpoints from all clusters which means configs are getting huge",
    "start": "1435279",
    "end": "1440799"
  },
  {
    "text": "meaning memory consumption for services that talk a lot of services uh i mean yes",
    "start": "1440799",
    "end": "1445840"
  },
  {
    "text": "this is true um and this is i guess like one of the",
    "start": "1445840",
    "end": "1451039"
  },
  {
    "text": "reasons why um incremental xds is something that we may need to move towards um",
    "start": "1451039",
    "end": "1458080"
  },
  {
    "text": "and then in additional kubernetes is coming out with the endpoint slices and there is a proposal in upstream",
    "start": "1458080",
    "end": "1463840"
  },
  {
    "text": "envoy to implement something that's like similar to endpoint slices in order to kind of cut down",
    "start": "1463840",
    "end": "1469039"
  },
  {
    "text": "on the amount of information that is getting complicated",
    "start": "1469039",
    "end": "1474880"
  },
  {
    "text": "uh next questions related to diplomat being open source um yeah it's been merged into the upstream",
    "start": "1479360",
    "end": "1485279"
  },
  {
    "text": "go control plane um so yeah just google for left go control plane or rather envoy go control",
    "start": "1485279",
    "end": "1491440"
  },
  {
    "text": "plane um and you should be able to find it it's in the examples directory um one of the other kind of reasons for",
    "start": "1491440",
    "end": "1499360"
  },
  {
    "text": "diplomat be open source is it provides um a good sort of reference example of how",
    "start": "1499360",
    "end": "1505440"
  },
  {
    "text": "to assemble um your own control plane um so it's a good sort of like example",
    "start": "1505440",
    "end": "1512840"
  },
  {
    "text": "codebase",
    "start": "1512840",
    "end": "1515840"
  },
  {
    "text": "um for next questions related to um what's the typical network overhead",
    "start": "1526880",
    "end": "1532320"
  },
  {
    "text": "around broadcasting xps values um i've forgotten what the current time frame is for",
    "start": "1532320",
    "end": "1538000"
  },
  {
    "text": "convergence uh we can follow back up on slack i think a lot of our health",
    "start": "1538000",
    "end": "1544559"
  },
  {
    "text": "checks and stuff it's on the order of 20-ish seconds or so",
    "start": "1544559",
    "end": "1550640"
  },
  {
    "text": "for health checks but it's a couple like multiple round trips but we can follow up separately about",
    "start": "1550640",
    "end": "1555840"
  },
  {
    "text": "that uh so i can talk to the next two of them",
    "start": "1555840",
    "end": "1561600"
  },
  {
    "text": "the first is does diplomat talk to non-trade services this is currently not implemented",
    "start": "1561600",
    "end": "1566960"
  },
  {
    "text": "however this could be extended by adding you know whatever the equivalent is",
    "start": "1566960",
    "end": "1572080"
  },
  {
    "text": "that's doing um discovery for your non-kubernetes infrastructure",
    "start": "1572080",
    "end": "1577919"
  },
  {
    "text": "and then the second is can we use diplomat on an enterprise setup",
    "start": "1577919",
    "end": "1583120"
  },
  {
    "text": "diplomat is an example so i think that uh while it's a good basis",
    "start": "1583120",
    "end": "1588880"
  },
  {
    "text": "for building your own custom control plane it's not necessarily like tested at scale so depending on like your scalability",
    "start": "1588880",
    "end": "1596080"
  },
  {
    "text": "concerns it's probably not recommended to use just right out of the box",
    "start": "1596080",
    "end": "1601679"
  },
  {
    "text": "um yeah and then and then particularly we also have um the original sort of like open source",
    "start": "1604480",
    "end": "1611440"
  },
  {
    "text": "discovery service that came out when envoy originally came out and that's what um was used for our",
    "start": "1611440",
    "end": "1617520"
  },
  {
    "text": "legacy stack um and so that was the the discovery service was what like the original versions of",
    "start": "1617520",
    "end": "1622960"
  },
  {
    "text": "of envoy manager and the control plane didn't actually speak kubernetes at all and so since we had",
    "start": "1622960",
    "end": "1628159"
  },
  {
    "text": "um since all the pods have vpcips um the early versions of kubernetes that",
    "start": "1628159",
    "end": "1634720"
  },
  {
    "text": "lift would just register those ips into",
    "start": "1634720",
    "end": "1640399"
  },
  {
    "text": "envoy discovery and that's how everything got bridged together",
    "start": "1640399",
    "end": "1646000"
  },
  {
    "text": "performance issues uh some of these questions related to have we have we had any performance issues watching all",
    "start": "1653039",
    "end": "1658080"
  },
  {
    "text": "kubernetes clusters and control plane um so when we were using",
    "start": "1658080",
    "end": "1663760"
  },
  {
    "text": "um endpoint informers no um for pod informers um there's also uh got",
    "start": "1663760",
    "end": "1670480"
  },
  {
    "text": "mentioned in the talk but um there there's a prior kubecon talk um",
    "start": "1670480",
    "end": "1676000"
  },
  {
    "text": "uh from tom and lita at lyft that kind of goes into uh some more of the details on this but",
    "start": "1676000",
    "end": "1681919"
  },
  {
    "text": "with the pot informer there's just like a lot of events um and so there's a lot",
    "start": "1681919",
    "end": "1687279"
  },
  {
    "text": "of data that we have to filter out to get information that um",
    "start": "1687279",
    "end": "1692320"
  },
  {
    "text": "that we need um so no i wouldn't say there's like any real performance issue in in that sense",
    "start": "1692320",
    "end": "1700320"
  },
  {
    "text": "um there were some issues with the onboard manager in terms of like how quickly it could scale up and keep the state of the world",
    "start": "1700320",
    "end": "1706960"
  },
  {
    "text": "um but most of those issues have been um addressed of late it's pretty much i",
    "start": "1706960",
    "end": "1712559"
  },
  {
    "text": "think just the the size of the mesh that's concerned particularly with related to um memory usage uh within the",
    "start": "1712559",
    "end": "1720640"
  },
  {
    "text": "envoy sidecar and that's the reason for moving out to the to the split mesh approach",
    "start": "1720640",
    "end": "1727200"
  },
  {
    "text": "so the next question is how do you deal with new deployments for each cluster this is where cvs comes in",
    "start": "1729520",
    "end": "1737360"
  },
  {
    "text": "and a simple implementation is you know you annotate the deployment and then you use an informer",
    "start": "1737360",
    "end": "1743360"
  },
  {
    "text": "to watch for new employments and that deployments and then you generate an envoy cluster",
    "start": "1743360",
    "end": "1749520"
  },
  {
    "text": "for the new clusters that are discovered",
    "start": "1749520",
    "end": "1754480"
  },
  {
    "text": "okay next question is related to whether or not all of our clusters are in",
    "start": "1758399",
    "end": "1763440"
  },
  {
    "text": "the us and also gdpr um yeah our clusters are in the us and lyft",
    "start": "1763440",
    "end": "1768960"
  },
  {
    "text": "is uh gdpr compliant uh that was a large undertaking at the company",
    "start": "1768960",
    "end": "1774640"
  },
  {
    "text": "i guess like a couple years ago um but yeah we're gdpr compliant",
    "start": "1774640",
    "end": "1781840"
  },
  {
    "text": "i think that's it for all the questions that have come in",
    "start": "1783919",
    "end": "1789278"
  },
  {
    "text": "so if there aren't any other questions thanks everybody for attending and be sure to message us on slack if",
    "start": "1791039",
    "end": "1797760"
  },
  {
    "text": "you have any other questions",
    "start": "1797760",
    "end": "1801919"
  }
]