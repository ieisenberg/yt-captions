[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "all right good morning good morning",
    "start": "30",
    "end": "5420"
  },
  {
    "text": "thank you very much my name is Arun Gupta and today I'm here with my colleague ray alvarez and i work",
    "start": "5420",
    "end": "11429"
  },
  {
    "text": "with AWS and the specialist container solutions architect fantastic i'm a principal open source technologist",
    "start": "11429",
    "end": "17550"
  },
  {
    "text": "I'm also the CNC F board representative for Amazon so today we will talk about debug your kubernetes applications just",
    "start": "17550",
    "end": "25830"
  },
  {
    "text": "like anybody we have a twitter handle and we both work for AWS I know this is",
    "start": "25830",
    "end": "30869"
  },
  {
    "text": "cube con but now when we talk to customers about continued rising modernizing their application we always",
    "start": "30869",
    "end": "37020"
  },
  {
    "text": "ask them why they want to use kubernetes most of the time kubernetes ends up being the answer but we want to",
    "start": "37020",
    "end": "43110"
  },
  {
    "text": "emphasize the fact that kubernetes is not a golden hammer it may not be the solution of every problem that you're",
    "start": "43110",
    "end": "48239"
  },
  {
    "text": "looking we hope with us but it's not the case so make sure you are looking to solve the pain points the problems that",
    "start": "48239",
    "end": "54270"
  },
  {
    "text": "your customers are facing and then applying kubernetes to solve those problems another principle that I really",
    "start": "54270",
    "end": "61469"
  },
  {
    "start": "60000",
    "end": "85000"
  },
  {
    "text": "found recently I really connect with that the more you think you know about kubernetes the less you think you know",
    "start": "61469",
    "end": "68729"
  },
  {
    "text": "you know I mean every time you know you start peeling the onion and you know it's more tears coming out of your eyes so important part is to understand you",
    "start": "68729",
    "end": "75990"
  },
  {
    "text": "know we'll see that in the presentation this presentation is all about the different ways your apps can fail and",
    "start": "75990",
    "end": "81869"
  },
  {
    "text": "how do you recover from that essentially so in this talk what we intend to cover",
    "start": "81869",
    "end": "87090"
  },
  {
    "start": "85000",
    "end": "120000"
  },
  {
    "text": "is several aspects here we put asterisks on the last two topics because we think",
    "start": "87090",
    "end": "92820"
  },
  {
    "text": "just like kubernetes we are oversubscribed hopefully we can cover all the topics here so starting with",
    "start": "92820",
    "end": "98909"
  },
  {
    "text": "cluster design what are the basics what are the errors that we are seeing with our customers over there networking cube",
    "start": "98909",
    "end": "104460"
  },
  {
    "text": "kernel CLI pods so we'll talk about every places you know where there are errors happening in these areas and how",
    "start": "104460",
    "end": "110909"
  },
  {
    "text": "we can help debug those load-balancing ingress monitoring and then time permitted we'll look at resource",
    "start": "110909",
    "end": "116909"
  },
  {
    "text": "reservation and stateful sets as well so let's get started by cluster design you",
    "start": "116909",
    "end": "123420"
  },
  {
    "start": "120000",
    "end": "137000"
  },
  {
    "text": "know before we even jump there I want to get the basic concepts out there that when we are looking at",
    "start": "123420",
    "end": "129479"
  },
  {
    "text": "and how are they designing their cluster what are the basic principles that they need to think about one thing I do",
    "start": "129479",
    "end": "135810"
  },
  {
    "text": "remember is go down here alright so let's talk about the core kubernetes components at a very high level if you",
    "start": "135810",
    "end": "142319"
  },
  {
    "start": "137000",
    "end": "216000"
  },
  {
    "text": "think about a cluster set up you know you want to create a highly available cluster so add in Amazon language we",
    "start": "142319",
    "end": "147810"
  },
  {
    "text": "have this concept of a region which is a physical location around the world where we cluster data centers and in a region",
    "start": "147810",
    "end": "154769"
  },
  {
    "text": "we have multiple availability zones these availability zones are far separated from each other for H a and",
    "start": "154769",
    "end": "160590"
  },
  {
    "text": "disaster recovery so what you're showing here is how do you set up a kubernetes cluster in a region spread across",
    "start": "160590",
    "end": "167190"
  },
  {
    "text": "multiple availability zones so in terms of kubernetes components what you have is really of controller and there are a",
    "start": "167190",
    "end": "173129"
  },
  {
    "text": "bunch of controllers over there which will dig a little bit into deeper into it then HCD is where your state of the",
    "start": "173129",
    "end": "179220"
  },
  {
    "text": "cluster is persistent down there and then you have worker nodes which is where basically all of your containers",
    "start": "179220",
    "end": "185190"
  },
  {
    "text": "are running essentially so in terms of setting up a cluster you want to make sure that your controllers HCD and",
    "start": "185190",
    "end": "191160"
  },
  {
    "text": "workers are spread across multiple availability zones that gives you the basic principle of H a not a single",
    "start": "191160",
    "end": "198510"
  },
  {
    "text": "point of failure you know yes region is basically where you are deploying it but there are strategies around that but",
    "start": "198510",
    "end": "204389"
  },
  {
    "text": "essentially what you're doing is you're spreading your workload across multiple availability zones in case one availability zone goes down for whatever",
    "start": "204389",
    "end": "211200"
  },
  {
    "text": "reason geographical disaster or whatever it is okay so that's the key component that you want understand here so how do",
    "start": "211200",
    "end": "217169"
  },
  {
    "start": "216000",
    "end": "327000"
  },
  {
    "text": "you set up a kubernetes cluster first step is to really create your controller and when we are talking about controller",
    "start": "217169",
    "end": "222450"
  },
  {
    "text": "we're really talking about scheduler controller deployments that controller replica set controller all of those",
    "start": "222450",
    "end": "228510"
  },
  {
    "text": "controllers that need to run together together let's call them as master basically so you create a controller",
    "start": "228510",
    "end": "234450"
  },
  {
    "text": "node that is typically called as control plane then you need to setup HCD and the decision that you need to make over",
    "start": "234450",
    "end": "240629"
  },
  {
    "text": "there is will HCD be co-located with your master or will it be running on a separate node at Amazon we prefer that",
    "start": "240629",
    "end": "247169"
  },
  {
    "text": "it runs on a separate node because again you know it allows you to reduce your blast radius and allows you to upgrade",
    "start": "247169",
    "end": "253319"
  },
  {
    "text": "HCD by itself as well and also the needs for your controller which is more CPU",
    "start": "253319",
    "end": "258810"
  },
  {
    "text": "bound as opposed to HDD which is more disk bound you can set up the right see two instance types accordingly then",
    "start": "258810",
    "end": "265020"
  },
  {
    "text": "you have cube Caudill CLI of course by the way which you connect to your kubernetes cluster or to the control plane so that is your control plane",
    "start": "265020",
    "end": "271110"
  },
  {
    "text": "essentially and the only way you can talk to your cluster is using cue cuddle and directly talking to the control",
    "start": "271110",
    "end": "277410"
  },
  {
    "text": "plane then you install your worker nodes and the worker nodes is basically you know all of your ec2 instances where",
    "start": "277410",
    "end": "283260"
  },
  {
    "text": "your containers pods deployments etcetera all cannot run so you have your control plane then you have a data plane",
    "start": "283260",
    "end": "289590"
  },
  {
    "text": "and you attach that data plane to the control plane and then you deploy your",
    "start": "289590",
    "end": "295920"
  },
  {
    "text": "apps your add-ons whatever functionality you want on top of that kubernetes cluster using cube Carlo very simple",
    "start": "295920",
    "end": "302940"
  },
  {
    "text": "process but devil is in the details and that's where things start breaking apart hopefully you are making profit out of",
    "start": "302940",
    "end": "308100"
  },
  {
    "text": "that application and go back to the first slide remember what is the pain point what is the application that",
    "start": "308100",
    "end": "313830"
  },
  {
    "text": "you're trying to deploy over there how are you trying to solve that problem it should be in your mind this is just a",
    "start": "313830",
    "end": "319140"
  },
  {
    "text": "process as it was saying in the morning is not the destination it is a car to the destination so remember the",
    "start": "319140",
    "end": "325290"
  },
  {
    "text": "destination in the mind when you're designing your kubernetes cluster there",
    "start": "325290",
    "end": "331320"
  },
  {
    "start": "327000",
    "end": "398000"
  },
  {
    "text": "are a lot of considerations that needs to go into in your mind should I use control plane self-manage or should I",
    "start": "331320",
    "end": "338040"
  },
  {
    "text": "manage the cluster by myself when I'm designing the controller nodes what kind of ec2 instance type and I'm",
    "start": "338040",
    "end": "344490"
  },
  {
    "text": "just using AWS terminology but these are applicable to across the clouds what",
    "start": "344490",
    "end": "349710"
  },
  {
    "text": "kind of ec2 instance types should I use what if my number of pods on my worker",
    "start": "349710",
    "end": "355200"
  },
  {
    "text": "nodes increase how should i scale my data plane how should i size my lcd",
    "start": "355200",
    "end": "360440"
  },
  {
    "text": "securing my HDD backing up my HDD setting up a raft consensus protocol day",
    "start": "360440",
    "end": "366330"
  },
  {
    "text": "one is okay now I've got a hello world application up and running what about my day two upgrades patches CBE's who's",
    "start": "366330",
    "end": "373290"
  },
  {
    "text": "going to do all that is that my core competency always think about that part of it or you have that capability or is",
    "start": "373290",
    "end": "380100"
  },
  {
    "text": "this morale booster that's something goes on you resuming hey I'm a kubernetes sre so think about what is",
    "start": "380100",
    "end": "385710"
  },
  {
    "text": "the motivation over there it's okay to have that hundreds even by the way but some of the design considerations and",
    "start": "385710",
    "end": "391650"
  },
  {
    "text": "again these are very design considerations but you need to look at when you are designing your own kubernetes cluster that's where you know",
    "start": "391650",
    "end": "399139"
  },
  {
    "start": "398000",
    "end": "451000"
  },
  {
    "text": "we created Amazon eks Amazon eks is Amazon's version of a managed kubernetes service and essentially what we give you",
    "start": "399139",
    "end": "406009"
  },
  {
    "text": "is a managed control plane and in that control plane you don't need to worry about it what is the ec2 instance type",
    "start": "406009",
    "end": "412310"
  },
  {
    "text": "how often HDD is backed up how is it going to be a che all of those are given to you by default and on Monday actually",
    "start": "412310",
    "end": "419600"
  },
  {
    "text": "of this week we announced something called as managed nodes but what that means is the full cluster if you want it",
    "start": "419600",
    "end": "426340"
  },
  {
    "text": "including the control plane and the data plane is fully managed for you so that capability now exists in Amazon II cares",
    "start": "426340",
    "end": "432949"
  },
  {
    "text": "but if you want to bring your own custom worker nodes you can totally do that because that's how we initially launched",
    "start": "432949",
    "end": "439370"
  },
  {
    "text": "essentially ok so think in terms of when you're designing your kubernetes cluster or is it a che is it we our security",
    "start": "439370",
    "end": "446240"
  },
  {
    "text": "back hum what kind of instance type upgrades and all that capabilities now",
    "start": "446240",
    "end": "452030"
  },
  {
    "start": "451000",
    "end": "476000"
  },
  {
    "text": "you look up look at the concept of cute cuddle as we talked about it so your cluster is created so you take your Q",
    "start": "452030",
    "end": "457550"
  },
  {
    "text": "cuddle and that's how you talk to your cluster and then the cluster is really what is deploying or the API server",
    "start": "457550",
    "end": "462860"
  },
  {
    "text": "essentially which is listening to all those requests from Q cuddle understands those requests and then deploy your",
    "start": "462860",
    "end": "468919"
  },
  {
    "text": "objects into the data plane over here ok so that's to coddle is basically one CLI that manages your equipment at is",
    "start": "468919",
    "end": "474590"
  },
  {
    "text": "cluster let's look a little bit more into the kubernetes cluster design so",
    "start": "474590",
    "end": "479900"
  },
  {
    "start": "476000",
    "end": "560000"
  },
  {
    "text": "that we understand which what are the things that can potentially break and soon we will start getting into the real deal over there but if you think about",
    "start": "479900",
    "end": "486650"
  },
  {
    "text": "the master node components the first one is an API server which is sort of because if think about kubernetes is",
    "start": "486650",
    "end": "492349"
  },
  {
    "text": "nothing but a rest endpoint it listens to REST API that is being sent from the coop cube Cardle essentially so there is",
    "start": "492349",
    "end": "498800"
  },
  {
    "text": "an API server that needs to be sit there then there is a controller manager as we talked about earlier deployments that",
    "start": "498800",
    "end": "505340"
  },
  {
    "text": "controller replicas controller replica set controller persistence set controller you name it all of these",
    "start": "505340",
    "end": "511400"
  },
  {
    "text": "different controllers are existing and at a given point of time and the request is received they make sure that the",
    "start": "511400",
    "end": "517459"
  },
  {
    "text": "desired and the actual state are reconciled essentially so that's what they take care of then there is",
    "start": "517459",
    "end": "523250"
  },
  {
    "text": "scheduler which is also a controller so after all the status persisted into HCD the",
    "start": "523250",
    "end": "528500"
  },
  {
    "text": "scheduler kicks in and the scheduler says okay here is a part that has not been assigned to a node let me rank all",
    "start": "528500",
    "end": "535970"
  },
  {
    "text": "the worker nodes that are available based upon my predefined algorithm or if you are giving me a custom algorithm I'm",
    "start": "535970",
    "end": "541640"
  },
  {
    "text": "gonna pick a worker node I'm gonna assign a part to that and again persisted into HCD as well and then of",
    "start": "541640",
    "end": "548360"
  },
  {
    "text": "course there is a sed which is sort of the back end on where the entire state of the cluster is persisted so those are",
    "start": "548360",
    "end": "554570"
  },
  {
    "text": "sort of the core master node components over here and things can really break at all different levels over there in terms",
    "start": "554570",
    "end": "561260"
  },
  {
    "start": "560000",
    "end": "583000"
  },
  {
    "text": "of HDD design you know it follows the raft consensus protocol so once again the recommendation over there is to have",
    "start": "561260",
    "end": "566810"
  },
  {
    "text": "at least three @cd servers spread across multiple availability zones so typically",
    "start": "566810",
    "end": "571820"
  },
  {
    "text": "you know when the request comes in to the QB API server it writes to the one HCD server which is in the same availability zone and then it replicates",
    "start": "571820",
    "end": "578480"
  },
  {
    "text": "to other it's eventually consistent on",
    "start": "578480",
    "end": "584030"
  },
  {
    "start": "583000",
    "end": "644000"
  },
  {
    "text": "the worker node component site what we are looking at is there is cubelet that is sitting on each node think of it as a",
    "start": "584030",
    "end": "591050"
  },
  {
    "text": "node agent essentially and that really handles the communication between the API server and the worker node and that",
    "start": "591050",
    "end": "596720"
  },
  {
    "text": "is the one that is really running your node and communicating with the API server in terms of the status and the",
    "start": "596720",
    "end": "602060"
  },
  {
    "text": "what needs to be done and how many replicas etc well the replicas are designed by API server but cubelet is",
    "start": "602060",
    "end": "608270"
  },
  {
    "text": "responsible for the lifecycle of the container on the worker node then there is queue proxy which is basically",
    "start": "608270",
    "end": "613910"
  },
  {
    "text": "handles the communication between parts nodes and the outside world and last but not the least there is cree or container",
    "start": "613910",
    "end": "620270"
  },
  {
    "text": "runtime interface or cry which basically says okay there are multiple ways by",
    "start": "620270",
    "end": "625310"
  },
  {
    "text": "which the containers can be run I could use docker I could use container ID I can use cryo whatever technology Sokka",
    "start": "625310",
    "end": "632960"
  },
  {
    "text": "now it's a very kubernetes centric interface but gives you the independence from what the underlying runtime",
    "start": "632960",
    "end": "638540"
  },
  {
    "text": "technology could be now in terms of",
    "start": "638540",
    "end": "646040"
  },
  {
    "start": "644000",
    "end": "709000"
  },
  {
    "text": "creating a cluster the way we look at you know creating an Amazon eks cluster is very straightforward eks cuttle is",
    "start": "646040",
    "end": "652130"
  },
  {
    "text": "the official CLI which is created by one of our partners we've works so essentially what you say is eks kernel",
    "start": "652130",
    "end": "658370"
  },
  {
    "text": "create cluster we were looking at the keynote this morning talking about convention over configuration and this is exactly what",
    "start": "658370",
    "end": "664640"
  },
  {
    "text": "it follows the moment you say ich es caudal create cluster it just has a whole bunch of defaults what region how",
    "start": "664640",
    "end": "671060"
  },
  {
    "text": "many instance types how many worker what instance type what is the army all that default and then it",
    "start": "671060",
    "end": "676850"
  },
  {
    "text": "gives you a default eks cluster if you want to configure the cluster if you want to create a cluster by your",
    "start": "676850",
    "end": "683120"
  },
  {
    "text": "specific design I want to change you know the number of nodes the instance type or the SSH keys or what kind of",
    "start": "683120",
    "end": "691160"
  },
  {
    "text": "logging property properties it has sure you can specify them on the CLI but you can also specify them in a configuration",
    "start": "691160",
    "end": "697400"
  },
  {
    "text": "file so following the practices of how you create your ETS cluster persisted",
    "start": "697400",
    "end": "703130"
  },
  {
    "text": "into the database persisted into the github repo so essentially what you can do is you can create like a configuration file and in the",
    "start": "703130",
    "end": "710270"
  },
  {
    "start": "709000",
    "end": "747000"
  },
  {
    "text": "configuration file for example in this case I am saying my cluster name is debug gates and I was in the region US",
    "start": "710270",
    "end": "716210"
  },
  {
    "text": "East one then I'm specifying a node group and the node group has m5 extra-large instance type and I'm",
    "start": "716210",
    "end": "722600"
  },
  {
    "text": "creating a four node cluster I'm also enabling SSH and I'm saying use my US East one key so essentially that's",
    "start": "722600",
    "end": "729440"
  },
  {
    "text": "that's all and in the last fragment is where I'm saying enable these kind of logging so my control plane logging is already",
    "start": "729440",
    "end": "736550"
  },
  {
    "text": "available that is the most useful T useful tool typically when you get to when things go wrong so it's very",
    "start": "736550",
    "end": "742760"
  },
  {
    "text": "important to enable those kind of logging over there thanks Erin so let's",
    "start": "742760",
    "end": "749660"
  },
  {
    "text": "look at networking and when Erin reaches out to me and said we need to do this presentation and we need to find out all",
    "start": "749660",
    "end": "755360"
  },
  {
    "text": "the ways communities clusters have failed on AWS a result two or three friends inside Amazon and I started",
    "start": "755360",
    "end": "761060"
  },
  {
    "text": "asking them questions like what do you think are the number one issues that customers run into and unanimously",
    "start": "761060",
    "end": "766670"
  },
  {
    "text": "everyone came to me with the same response that the number one problem on kubernetes is generally related to",
    "start": "766670",
    "end": "773630"
  },
  {
    "text": "network so there are two core components that were we are looking at when it comes to networking problems the first",
    "start": "773630",
    "end": "780350"
  },
  {
    "text": "one is the C&I now in kubernetes kübra not provide its own networking stack you",
    "start": "780350",
    "end": "785930"
  },
  {
    "text": "can bring in your own network provider some have some functionalities like network policies",
    "start": "785930",
    "end": "792420"
  },
  {
    "text": "on eks we are using the Amazon VPC CNI which gives you the ability to create to",
    "start": "792420",
    "end": "798329"
  },
  {
    "text": "give your Pazza VPC IP address and the next thing was core DNS a lot of work",
    "start": "798329",
    "end": "804720"
  },
  {
    "text": "has been implemented in core DNS in the last few months in the fad last few releases since it became standard in in",
    "start": "804720",
    "end": "812000"
  },
  {
    "text": "1.11 so a lot of improvement has already happened but when you're running clusters that include thousands of pause",
    "start": "812000",
    "end": "819389"
  },
  {
    "text": "or hundreds of nodes it does not scale automatically and there are certain steps that you need to do so we're going",
    "start": "819389",
    "end": "825690"
  },
  {
    "text": "to take a look at some of those use cases today so let's look at the C&I what is Sienna",
    "start": "825690",
    "end": "833160"
  },
  {
    "start": "830000",
    "end": "883000"
  },
  {
    "text": "Sienna is a container networking interface and on eks we use Amazon V PCC",
    "start": "833160",
    "end": "839190"
  },
  {
    "text": "and I've a default and that gives you the ability to give your pods and your worker nodes the same IP address from a",
    "start": "839190",
    "end": "846089"
  },
  {
    "text": "subnet within your V PC and if you've ever used cops or any other thing you you might be familiar with things like",
    "start": "846089",
    "end": "852810"
  },
  {
    "text": "calico or flannel or Canal these are overlay and networking technologies that include a little bit of overhead the",
    "start": "852810",
    "end": "859980"
  },
  {
    "text": "compute overhead to run on the worker nodes but also now you have to deal with another layer of IP addressing that you",
    "start": "859980",
    "end": "867930"
  },
  {
    "text": "don't have to deal with in Amazon V PC and I because what the IPS that you're seeing for pods and your worker nodes",
    "start": "867930",
    "end": "874350"
  },
  {
    "text": "are real VPC IP address you can ping them you can connect directly to them and you can do things like BBC flow logs",
    "start": "874350",
    "end": "881130"
  },
  {
    "text": "etc so if you look at how the cni works in in eks c and i basically uses elastic",
    "start": "881130",
    "end": "889949"
  },
  {
    "start": "883000",
    "end": "976000"
  },
  {
    "text": "network interface which is a network interface that you attach or it's automatically attached to an ec2",
    "start": "889949",
    "end": "895319"
  },
  {
    "text": "instance and that's basically providing the networking capabilities of each ec2 instance and that's how you connect your",
    "start": "895319",
    "end": "901560"
  },
  {
    "text": "network so each ec2 instance gets an en I attached to it and Amazon BBC CNI runs",
    "start": "901560",
    "end": "908220"
  },
  {
    "text": "on top of en I what that means is it uses en is to give IP addresses to pods",
    "start": "908220",
    "end": "913620"
  },
  {
    "text": "and ec2 instance one of the IP address that you have in en I is reserved for",
    "start": "913620",
    "end": "920010"
  },
  {
    "text": "communication so typically use the max number you see deduct one because that's",
    "start": "920010",
    "end": "925649"
  },
  {
    "text": "what you're using for communication back and forth three VC pods as I said",
    "start": "925649",
    "end": "930660"
  },
  {
    "text": "receive an IP address directly from the subnet so that means that you now have a",
    "start": "930660",
    "end": "935879"
  },
  {
    "text": "limitation of the total number of paths that you can have in your cluster first limitation is the size of the subnet so",
    "start": "935879",
    "end": "943230"
  },
  {
    "text": "if you have only hundred IPs in the in that subnet guess what you can only create hundred pods anything that any",
    "start": "943230",
    "end": "949920"
  },
  {
    "text": "pod that's created after that is going to remain in pending state unless you have unless you add more IP address or",
    "start": "949920",
    "end": "957110"
  },
  {
    "text": "increase the instance size so you should really pay attention and plan for growth",
    "start": "957110",
    "end": "964470"
  },
  {
    "text": "before you're starting your cluster make sure that you get a idea about what is",
    "start": "964470",
    "end": "970199"
  },
  {
    "text": "going to be that upper limit for your pods and then create a network accordingly",
    "start": "970199",
    "end": "976429"
  },
  {
    "start": "976000",
    "end": "1057000"
  },
  {
    "text": "so let's see we create a deployment this is a very highly sophisticated",
    "start": "976429",
    "end": "981920"
  },
  {
    "text": "application that I've written and it's very highly sophisticated output it says",
    "start": "981920",
    "end": "986939"
  },
  {
    "text": "hello world to the world I'm very proud of it and I want to run it so I basically go to go to kubernetes and say",
    "start": "986939",
    "end": "995069"
  },
  {
    "text": "coop Ketel create my deployment which is a hello world application but of course",
    "start": "995069",
    "end": "1000709"
  },
  {
    "text": "I since this is such a high transaction very very important application I have",
    "start": "1000709",
    "end": "1006829"
  },
  {
    "text": "to make sure that it runs at scale so what I did was I said scale my replicator 240 let's see how is the",
    "start": "1006829",
    "end": "1013730"
  },
  {
    "text": "performance of print hello world at 240 replicas so I was very proud that it's",
    "start": "1013730",
    "end": "1019939"
  },
  {
    "text": "gonna work and then this happened I want to say get deployments and I was very",
    "start": "1019939",
    "end": "1025250"
  },
  {
    "text": "happy that two hundred twenty two of them were adopted but the remaining were orphans or pending states so I got very",
    "start": "1025250",
    "end": "1032000"
  },
  {
    "text": "curious I'm one of those users that learns by actually doing so when the",
    "start": "1032000",
    "end": "1037399"
  },
  {
    "text": "idea comes to my head I directly go into doing without skipping the very very low",
    "start": "1037399",
    "end": "1043100"
  },
  {
    "text": "importance task of reading documentation so I you know kubernetes works and I",
    "start": "1043100",
    "end": "1048350"
  },
  {
    "text": "just will go ahead and scale the two millions on unlimited scaling and I",
    "start": "1048350",
    "end": "1053430"
  },
  {
    "text": "who I should have that but it didn't happen I only have 222 pots what went wrong so I try to get get pods",
    "start": "1053430",
    "end": "1060600"
  },
  {
    "start": "1057000",
    "end": "1085000"
  },
  {
    "text": "and seems like some of the pods may be in pending state so here's another cool",
    "start": "1060600",
    "end": "1066240"
  },
  {
    "text": "thing if you don't know about field selectors these are really important especially when your cluster becomes",
    "start": "1066240",
    "end": "1071460"
  },
  {
    "text": "larger and you may be running hundreds or even thousands of pods fields lectures really come into play and you",
    "start": "1071460",
    "end": "1077010"
  },
  {
    "text": "can filter them so over here I'm just doing a give me all the pods that are pending and I see a bunch of",
    "start": "1077010",
    "end": "1082290"
  },
  {
    "text": "them are pending so I want to troubleshoot a little bit more now this time I can use get events cube Caudill",
    "start": "1082290",
    "end": "1088320"
  },
  {
    "start": "1085000",
    "end": "1098000"
  },
  {
    "text": "get events is going to give you all the events in the cluster but over here I'm not interested in anything that may be",
    "start": "1088320",
    "end": "1093840"
  },
  {
    "text": "information only so I'm only looking at warning and if I dig a little bit deeper into it I'm shown something that looks",
    "start": "1093840",
    "end": "1100650"
  },
  {
    "text": "like this and I see that there's an interesting part that's the part I'm interested in my hello world application",
    "start": "1100650",
    "end": "1106980"
  },
  {
    "text": "and I see that it says fail to assign IP address to container so that's the",
    "start": "1106980",
    "end": "1112440"
  },
  {
    "text": "number one issue people run into because guess what I did not read documentation and kubernetes was supposed to solve",
    "start": "1112440",
    "end": "1118440"
  },
  {
    "text": "world hunger and yet here it is it can't even create my 240 pods so I want to",
    "start": "1118440",
    "end": "1124260"
  },
  {
    "text": "take a look into it and I then started reading documentation because that's what you're supposed to do you work",
    "start": "1124260",
    "end": "1129600"
  },
  {
    "text": "first fill and then read documentation so I looked at this nifty component of",
    "start": "1129600",
    "end": "1134640"
  },
  {
    "start": "1131000",
    "end": "1200000"
  },
  {
    "text": "the CNI and it's called ipam d and it basically does two things first of all",
    "start": "1134640",
    "end": "1140610"
  },
  {
    "text": "it maintains a warm pool of IP address from the VPC so let's say we'll cache of",
    "start": "1140610",
    "end": "1147050"
  },
  {
    "text": "10-15-20 IP addresses and as pods are created is going to assign an IP address",
    "start": "1147050",
    "end": "1152850"
  },
  {
    "text": "to each one of them and as pods get destroyed is going to then take that IP address and keep it at this warm pool",
    "start": "1152850",
    "end": "1159510"
  },
  {
    "text": "so as you mentioned EA and I basically uses one IP address for communication so",
    "start": "1159510",
    "end": "1167100"
  },
  {
    "text": "the max number of I max number of IPs I can get in in an ec2 instance there's a",
    "start": "1167100",
    "end": "1172830"
  },
  {
    "text": "simple formula for that I don't know if you guys can read it so the formula",
    "start": "1172830",
    "end": "1177840"
  },
  {
    "text": "really is you take the the number of en eyes times that by the IP addresses that",
    "start": "1177840",
    "end": "1183660"
  },
  {
    "text": "each in that eni supports minus 1 that's the one it uses to communicate",
    "start": "1183660",
    "end": "1188870"
  },
  {
    "text": "back and the number of IP addresses you can get on that that ec2 instance is",
    "start": "1188870",
    "end": "1194930"
  },
  {
    "text": "going to be a min function so whichever is smaller that's going to be a limiting factor so in my cluster here I have an",
    "start": "1194930",
    "end": "1204050"
  },
  {
    "start": "1200000",
    "end": "1220000"
  },
  {
    "text": "m5 x-large ec2 instance type and it can support up to four yen eyes and Ichi and",
    "start": "1204050",
    "end": "1210350"
  },
  {
    "text": "I supports 15 IP addresses yes Oh",
    "start": "1210350",
    "end": "1215860"
  },
  {
    "text": "increase the font that could be a challenge we've never done that I don't",
    "start": "1217310",
    "end": "1225020"
  },
  {
    "start": "1220000",
    "end": "1234000"
  },
  {
    "text": "know what else is going to break yeah this is better than hello world so I",
    "start": "1225020",
    "end": "1234530"
  },
  {
    "start": "1234000",
    "end": "1343000"
  },
  {
    "text": "have an m5 x large cluster for en is 15 IP address I'm really bad at maths don't",
    "start": "1234530",
    "end": "1240200"
  },
  {
    "text": "let me do this year yes 456 was the number I was trying to get to so it can support up to 56 IP address where the",
    "start": "1240200",
    "end": "1248060"
  },
  {
    "text": "four times fifteen you get it so that's a 56 is the is a maximum en I that it",
    "start": "1248060",
    "end": "1254120"
  },
  {
    "text": "can support the default cluster that I created in that default subnet it",
    "start": "1254120",
    "end": "1260480"
  },
  {
    "text": "supports up to 8,000 192 IP address it's a slash 19 I knew it didn't need to read",
    "start": "1260480",
    "end": "1266120"
  },
  {
    "text": "it because that's how I calculate ciders so what the maximum number of pods that",
    "start": "1266120",
    "end": "1272870"
  },
  {
    "text": "I can run on this ec2 instance is going to be directly related to the maximum or",
    "start": "1272870",
    "end": "1278540"
  },
  {
    "text": "IPS that I get on this ec2 instance so over here and my limit is going to be 56",
    "start": "1278540",
    "end": "1284620"
  },
  {
    "text": "and 56 is a maximum or a pause that I can run inside this ec2 instance one",
    "start": "1284620",
    "end": "1291860"
  },
  {
    "text": "recommendation at 116 is that you should not run any more than 100 pods per node",
    "start": "1291860",
    "end": "1297230"
  },
  {
    "text": "and if you want to know why click on that link it should tell you exactly why",
    "start": "1297230",
    "end": "1303020"
  },
  {
    "text": "so in this situation I have 4 ec2 instances for m5x larges so that means",
    "start": "1303020",
    "end": "1310640"
  },
  {
    "text": "that 56 times 4 is going to be 240 is it going to be 224 224 and that's why my",
    "start": "1310640",
    "end": "1318890"
  },
  {
    "text": "pods stopped at 200 why did they stop at 220 because I may",
    "start": "1318890",
    "end": "1324800"
  },
  {
    "text": "be running other pods that are also consuming IP address so that's the number one thing to keep in mind is that",
    "start": "1324800",
    "end": "1329840"
  },
  {
    "text": "you always want to make sure that you are planning ahead you have enough IP addresses because if you don't your",
    "start": "1329840",
    "end": "1335870"
  },
  {
    "text": "application scaling is at one point going to come to a halt and your cluster",
    "start": "1335870",
    "end": "1341330"
  },
  {
    "text": "will break your apps one skill so in order to do that we've created the cni metrics helper open source tool and it",
    "start": "1341330",
    "end": "1348140"
  },
  {
    "start": "1343000",
    "end": "1362000"
  },
  {
    "text": "is a very easy tool that you can deploy inside your cluster and it can report back metrics like what does he know YP",
    "start": "1348140",
    "end": "1354620"
  },
  {
    "text": "address is available or are there any errors in my cluster so let's take a",
    "start": "1354620",
    "end": "1359750"
  },
  {
    "text": "look at deploying that and very simple first of all I'm going to create a policy this iron policy is basically",
    "start": "1359750",
    "end": "1365390"
  },
  {
    "start": "1362000",
    "end": "1372000"
  },
  {
    "text": "going to allow whoever assumes this role to write to clog watch directly so I",
    "start": "1365390",
    "end": "1370730"
  },
  {
    "text": "will write this policy the next thing I'm going to do is attach that to an ec2 instance which is my m5 Excel in this",
    "start": "1370730",
    "end": "1377930"
  },
  {
    "start": "1372000",
    "end": "1389000"
  },
  {
    "text": "situation you can alternatively also use what we release recently I am mapping to",
    "start": "1377930",
    "end": "1385250"
  },
  {
    "text": "a service account and you could do that too the next step I'm going to do is I'm going to apply this",
    "start": "1385250",
    "end": "1391070"
  },
  {
    "start": "1389000",
    "end": "1411000"
  },
  {
    "text": "yamo file which is going to create a bunch of resources inside your cluster but the most interesting thing is that",
    "start": "1391070",
    "end": "1397580"
  },
  {
    "text": "it creates a pod inside your coop system namespace which is CNI metrics helper",
    "start": "1397580",
    "end": "1403100"
  },
  {
    "text": "and this mattress helper is collecting information and then providing that to you on cloud watch so once I deploy this",
    "start": "1403100",
    "end": "1410150"
  },
  {
    "text": "I can go to my cloud watch and I can really quickly look at how many IP addresses I have available and and how",
    "start": "1410150",
    "end": "1418640"
  },
  {
    "start": "1411000",
    "end": "1427000"
  },
  {
    "text": "many IPAs are in use I can create some some really nice alerts to send me a",
    "start": "1418640",
    "end": "1423680"
  },
  {
    "text": "slack message in the middle of night that you're running out of IP addresses all right so let's look at core DNS now",
    "start": "1423680",
    "end": "1430730"
  },
  {
    "start": "1427000",
    "end": "1436000"
  },
  {
    "text": "this was a number to think that that people told me customers run into so let's look at core DNS core Deanna's use",
    "start": "1430730",
    "end": "1438200"
  },
  {
    "start": "1436000",
    "end": "1479000"
  },
  {
    "text": "was before core DNS came in we had cube DNS core DNS was g8 in 1.11 so if you",
    "start": "1438200",
    "end": "1445910"
  },
  {
    "text": "have clusters that you created there are 1.11 and you long forgot about it and they you should switch to core",
    "start": "1445910",
    "end": "1452450"
  },
  {
    "text": "DNS but if you create a cluster after a one-goal lemon by default you're getting core DNS and succour DNS is",
    "start": "1452450",
    "end": "1459049"
  },
  {
    "text": "highly scalable highly customizable DNS is a provider for kubernetes and it uses",
    "start": "1459049",
    "end": "1464509"
  },
  {
    "text": "a core file to control it's basically the configuration for fork or DNS and",
    "start": "1464509",
    "end": "1469909"
  },
  {
    "text": "the core file is stored in a config map like all good apps do and it's stored an",
    "start": "1469909",
    "end": "1476059"
  },
  {
    "text": "accordion s config Mac which you can directly manipulate so if my my if I'm facing some problems with DNS resolution",
    "start": "1476059",
    "end": "1482480"
  },
  {
    "start": "1479000",
    "end": "1513000"
  },
  {
    "text": "in my cluster the first thing I will do is go look at your pods go look at your DNS pods so in eks by default we'll run",
    "start": "1482480",
    "end": "1490730"
  },
  {
    "text": "two replicas of core DNS deployment in your situation maybe one maybe three we",
    "start": "1490730",
    "end": "1497389"
  },
  {
    "text": "recommend that you run at least two at a time if you're positive running and you're still not able to get named",
    "start": "1497389",
    "end": "1504320"
  },
  {
    "text": "resolution or you have latency the next thing I would do is check your service so there's a core DNS service that you",
    "start": "1504320",
    "end": "1509570"
  },
  {
    "text": "can look into and make sure that it's up and running if that doesn't work you can",
    "start": "1509570",
    "end": "1515539"
  },
  {
    "start": "1513000",
    "end": "1551000"
  },
  {
    "text": "then add logging to your core DNS and here what I'm going to just edit my core",
    "start": "1515539",
    "end": "1522679"
  },
  {
    "text": "DNS conflict map and I've added this you guys see this this yes that one you just",
    "start": "1522679",
    "end": "1533899"
  },
  {
    "text": "add a log there and it will automatically then start publishing logs pushing out logs you can then do cube",
    "start": "1533899",
    "end": "1540259"
  },
  {
    "text": "cuddle logs get in look at each pod and look at all the outputs of each pod or",
    "start": "1540259",
    "end": "1547840"
  },
  {
    "text": "sorry about that or you can just run this command which is it basically gets",
    "start": "1548139",
    "end": "1554330"
  },
  {
    "start": "1551000",
    "end": "1571000"
  },
  {
    "text": "all the pause in that are called core DNS and it will show you an output of all the all the pods that are running",
    "start": "1554330",
    "end": "1560840"
  },
  {
    "text": "inside your cluster so this is generally where you want you will find that most of the problems can be can be tapped",
    "start": "1560840",
    "end": "1566869"
  },
  {
    "text": "into and you can then act upon it some of the ways of fixing these problems and",
    "start": "1566869",
    "end": "1572240"
  },
  {
    "start": "1571000",
    "end": "1601000"
  },
  {
    "text": "the what first way is the easiest way to just scale your replicas and you can",
    "start": "1572240",
    "end": "1577519"
  },
  {
    "text": "scale them 2 3 4 5 but sometimes that still doesn't help so for that the",
    "start": "1577519",
    "end": "1582919"
  },
  {
    "text": "second option is a local DNS add-on that core DNS has and it creates a demon",
    "start": "1582919",
    "end": "1588350"
  },
  {
    "text": "that runs on each worker node and all the queries that come from any of those",
    "start": "1588350",
    "end": "1593899"
  },
  {
    "text": "pods are then sent to this local daemon set which enables faster name resolution",
    "start": "1593899",
    "end": "1600519"
  },
  {
    "text": "the next thing is the memory requirements for core DNS so cord e and",
    "start": "1600519",
    "end": "1605870"
  },
  {
    "start": "1601000",
    "end": "1659000"
  },
  {
    "text": "s comes with some same defaults so any case we've we say that at minimum core",
    "start": "1605870",
    "end": "1611659"
  },
  {
    "text": "DNS should have a 70 mid request and we've kept it to 170 mids but you if",
    "start": "1611659",
    "end": "1617690"
  },
  {
    "text": "you're running a larger cluster if you're running hundreds and thousands of pods then you should change it and the",
    "start": "1617690",
    "end": "1624559"
  },
  {
    "text": "calculation is pretty simple pods plus services by 1,000 plus 54 in some of you",
    "start": "1624559",
    "end": "1632120"
  },
  {
    "text": "may be thinking why 54 or 54 comes from 19 meg is what the core DNS application",
    "start": "1632120",
    "end": "1639230"
  },
  {
    "text": "needs and then 19 in then 30 mix for caching and then five Meg's for buffers",
    "start": "1639230",
    "end": "1646220"
  },
  {
    "text": "so that comes out to be 54 that's the default that's a recommendation in eks",
    "start": "1646220",
    "end": "1651470"
  },
  {
    "text": "you'll be using 70 but if you're running large cluster you might want to you",
    "start": "1651470",
    "end": "1656720"
  },
  {
    "text": "might want to customize this number another plug-in is or the auto path",
    "start": "1656720",
    "end": "1661820"
  },
  {
    "start": "1659000",
    "end": "1714000"
  },
  {
    "text": "plug-in auto path plug-in it has some improvements that can improve name resolutions for names that are external",
    "start": "1661820",
    "end": "1667940"
  },
  {
    "text": "to your cluster now I wouldn't recommend and recommend this enabling by default unless you're running into this problem",
    "start": "1667940",
    "end": "1674419"
  },
  {
    "text": "of external name resolution sometimes with Auto path plug-in the increase",
    "start": "1674419",
    "end": "1679970"
  },
  {
    "text": "could be 10 folds so it's really important if you have that requirement",
    "start": "1679970",
    "end": "1685159"
  },
  {
    "text": "in your cluster but the downside is that now you have significantly more memory",
    "start": "1685159",
    "end": "1690830"
  },
  {
    "text": "that your coordinates pods need so instead of possible services but by 1000 over you here you're dividing that by",
    "start": "1690830",
    "end": "1697370"
  },
  {
    "text": "250 so 4 times more memory requirement with Auto paths so that's the networking",
    "start": "1697370",
    "end": "1705409"
  },
  {
    "text": "section I'll give you back I'm not done yet",
    "start": "1705409",
    "end": "1711679"
  },
  {
    "text": "we're not done you're not done yet it's all right cube Caudill so cue puddle is",
    "start": "1711679",
    "end": "1717419"
  },
  {
    "start": "1714000",
    "end": "1803000"
  },
  {
    "text": "the primary way by which we interact with the kubernetes cluster how does Q",
    "start": "1717419",
    "end": "1723720"
  },
  {
    "text": "kernel work so let's try to understand the basic concept behind cube card oh well of course you need a cube config",
    "start": "1723720",
    "end": "1729149"
  },
  {
    "text": "file you know that's sort of the way it looks at it and default location for Q configures in your home directory in a",
    "start": "1729149",
    "end": "1734909"
  },
  {
    "text": "dot cube directory in config and that's where your configuration lives and what can happen over there is you know in",
    "start": "1734909",
    "end": "1741600"
  },
  {
    "text": "that configuration file you potentially have multiple clusters that are configured and for each cluster there is a config section and that you need to",
    "start": "1741600",
    "end": "1748230"
  },
  {
    "text": "refer to could be a local cluster or could be running on AWS or any other cloud or on-premise then you can use the",
    "start": "1748230",
    "end": "1755220"
  },
  {
    "text": "commands in cube kernel config use context specify the context that you want to use and use that to talk to that",
    "start": "1755220",
    "end": "1763080"
  },
  {
    "text": "particular cluster so that's the way well of course you can always look at your cube config environment variable so",
    "start": "1763080",
    "end": "1768629"
  },
  {
    "text": "oftentimes we have seen that hey you know what I'm trying to connect to the kubernetes cluster but I'm not able to connect to the kubernetes cluster so the",
    "start": "1768629",
    "end": "1775019"
  },
  {
    "text": "tip over there is make sure you don't have cube config environment variable point it to somewhere else",
    "start": "1775019",
    "end": "1781259"
  },
  {
    "text": "you know because if that is pointed and you still have dot cube config it doesn't matter it's going to take look look at the cube config environment",
    "start": "1781259",
    "end": "1787350"
  },
  {
    "text": "variable it basically communicates with your API server once the cube cuddle is",
    "start": "1787350",
    "end": "1793590"
  },
  {
    "text": "configured then it talks to the API server all the commands are given over there there's a beautiful post by",
    "start": "1793590",
    "end": "1800580"
  },
  {
    "text": "somebody working at github you know where it talks about how does cube quadrille really work and I have a link",
    "start": "1800580",
    "end": "1806850"
  },
  {
    "start": "1803000",
    "end": "1890000"
  },
  {
    "text": "after the end of the presentation it talks about all the details what happens",
    "start": "1806850",
    "end": "1812940"
  },
  {
    "text": "when you say cue cuddle create - F deployment it's a long process it's a",
    "start": "1812940",
    "end": "1819119"
  },
  {
    "text": "lot of things that go behind the scenes if you look at it on the client side for example there is a client side validation that is happening then it",
    "start": "1819119",
    "end": "1825450"
  },
  {
    "text": "infer your generators hey by the way you're trying to do a deployment so I have a deployment generator because the deployment generator knows how to look",
    "start": "1825450",
    "end": "1832259"
  },
  {
    "text": "at the deployment spec and create the rest representation of site then it creates a runtime object and that",
    "start": "1832259",
    "end": "1837509"
  },
  {
    "text": "runtime object is basically using that generator is going to create your rest representation then it does the API",
    "start": "1837509",
    "end": "1843480"
  },
  {
    "text": "version because you may be using a specific version of deployment and then it says hey does your API server really provide",
    "start": "1843480",
    "end": "1850169"
  },
  {
    "text": "that API version and during the first run of you know your cube cuddle by",
    "start": "1850169",
    "end": "1855360"
  },
  {
    "text": "connecting to the API server it downloads all the API is support and puts them into your local cache so you",
    "start": "1855360",
    "end": "1861450"
  },
  {
    "text": "can look at that you know if you go to into your cube config directory there's a cache directory over there as well so take a look at it it kind of gives you",
    "start": "1861450",
    "end": "1867480"
  },
  {
    "text": "the complete API versions that are supported over there and that's what it refers to for local optimization",
    "start": "1867480",
    "end": "1873419"
  },
  {
    "text": "eventually your rest request is created which is the rest request because end of the day is an API server rest request is",
    "start": "1873419",
    "end": "1879600"
  },
  {
    "text": "created authentication is done and the way authentication is done for eks is a bit different I will talk about that a",
    "start": "1879600",
    "end": "1885509"
  },
  {
    "text": "little bit later but now the request is off to the API server a lot more things",
    "start": "1885509",
    "end": "1892409"
  },
  {
    "start": "1890000",
    "end": "2018000"
  },
  {
    "text": "happen on the server side of course you know authentication authorization happens you're trying to create a deployment are you allowed to create a",
    "start": "1892409",
    "end": "1898710"
  },
  {
    "text": "deployment you're trying to delete a service update a service are you allowed to do that then admission controllers",
    "start": "1898710",
    "end": "1904409"
  },
  {
    "text": "get in very various say oh by the way if you're trying to create such as a deployment how many resources are",
    "start": "1904409",
    "end": "1910350"
  },
  {
    "text": "allowed the namespaces etc what kicks in over there then it really DC realizes the HTTP request just the way there are",
    "start": "1910350",
    "end": "1916259"
  },
  {
    "text": "generators there are objects on the server side which deserialize the request and persist it into HCD",
    "start": "1916259",
    "end": "1921840"
  },
  {
    "text": "initializers kick in and then your controller kicks in and all these controllers are doing is essentially the",
    "start": "1921840",
    "end": "1928500"
  },
  {
    "text": "deployment said replicas set the scheduler controller is they're looking at your HDD state and they say oh you",
    "start": "1928500",
    "end": "1935669"
  },
  {
    "text": "want to create a deployment so let me take a look at it and deployment means you want to create a replica set so I'm going to create a replica set persist",
    "start": "1935669",
    "end": "1942269"
  },
  {
    "text": "that into XE d then the reply replica set per controller gets up says I'm gonna create a pod so it purses the pod",
    "start": "1942269",
    "end": "1949529"
  },
  {
    "text": "in it CD nothing has been created yet this is the time when you're looking at your cue card L get pod this is",
    "start": "1949529",
    "end": "1956009"
  },
  {
    "text": "container creating all this is happening behind the scene for you then there's a",
    "start": "1956009",
    "end": "1961980"
  },
  {
    "text": "cubelet which is sitting on your node essentially which is querying your api",
    "start": "1961980",
    "end": "1967440"
  },
  {
    "text": "server every 20 seconds looking for it hey you know what is there anything for me to deploy is there anything for me to",
    "start": "1967440",
    "end": "1973200"
  },
  {
    "text": "deploy because that's where you know scheduler has already your worker nodes picked up where the",
    "start": "1973200",
    "end": "1979210"
  },
  {
    "text": "pod needs to be deployed and now the cubelet kicks in that oh there's a part that is attached to me that's scheduled",
    "start": "1979210",
    "end": "1984730"
  },
  {
    "text": "on me it brings it up identifies the CRI and then you know it gets CRI is a",
    "start": "1984730",
    "end": "1990550"
  },
  {
    "text": "darker runtime for example then it uses C and I gets an IP address then it pulls the images and all those things and then",
    "start": "1990550",
    "end": "1997120"
  },
  {
    "text": "it eventually lays out all the file system on your local worker node and then it says hey by the way the",
    "start": "1997120",
    "end": "2002790"
  },
  {
    "text": "container is up and running over here so I just want to highlight the complexity of the process that goes behind this",
    "start": "2002790",
    "end": "2010010"
  },
  {
    "text": "anything and everything can potentially break here and that's where you will see different status and we'll get into the",
    "start": "2010010",
    "end": "2016230"
  },
  {
    "text": "pod lifecycle a bit later but let's look a little bit further so one of the",
    "start": "2016230",
    "end": "2021660"
  },
  {
    "start": "2018000",
    "end": "2159000"
  },
  {
    "text": "common errors that we have seen from our customers is they say hey you know what I'm trying to do Q cuddle get serviced and saying the server doesn't have a",
    "start": "2021660",
    "end": "2030540"
  },
  {
    "text": "service type resource or resource type service service is a standard kubernetes object why is it doesn't support it the",
    "start": "2030540",
    "end": "2037710"
  },
  {
    "text": "error is misleading how oftentimes you know you see a good error and say oh this is excellent error now I can debug",
    "start": "2037710",
    "end": "2043530"
  },
  {
    "text": "it so what's really happening here is you're at least an ek as case your cube",
    "start": "2043530",
    "end": "2049950"
  },
  {
    "text": "Caudill is not authorized to talk to kubernetes cluster okay so the first tip",
    "start": "2049950",
    "end": "2055230"
  },
  {
    "text": "that we always look at the users is you should look at you know what version of cube cartel that you have that you have",
    "start": "2055230",
    "end": "2061020"
  },
  {
    "text": "so say cube Caudill version it gives you the cube Quarter version and in kubernetes there is a catabolic there is",
    "start": "2061020",
    "end": "2066389"
  },
  {
    "text": "a the amount of difference between cube Caudill CLI and the API server is",
    "start": "2066390",
    "end": "2071879"
  },
  {
    "text": "predefined so make sure you look at those API versions that you know they're in like not more than two versions apart",
    "start": "2071880",
    "end": "2077010"
  },
  {
    "text": "then also update your AWS I am Authenticator in AWS LAN the security is",
    "start": "2077010",
    "end": "2083310"
  },
  {
    "text": "all done using I am okay so if you have just like you know just a cluster",
    "start": "2083310",
    "end": "2089340"
  },
  {
    "text": "configure their you can't directly talk to the cluster I mean that cluster configure is generated by AWS CLI then",
    "start": "2089340",
    "end": "2095100"
  },
  {
    "text": "it's okay but otherwise the way it happens is from cube Cardle you know if",
    "start": "2095100",
    "end": "2100740"
  },
  {
    "text": "there is a I am role that you need to have that I am role behind the scene cube model invokes AWS I am",
    "start": "2100740",
    "end": "2106920"
  },
  {
    "text": "Authenticator which then talks to the SD service which gets a token then it gets",
    "start": "2106920",
    "end": "2112220"
  },
  {
    "text": "a token and that token is then passed on to cube Caudill to go talk to the kubernetes cluster behind the scene so",
    "start": "2112220",
    "end": "2118549"
  },
  {
    "text": "make sure your cube cartel and AWS I am Authenticator is to the latest version that's the typical source of error that",
    "start": "2118549",
    "end": "2124910"
  },
  {
    "text": "we have seen oftentimes what we have seen is you know maybe your config is not up-to-date because hey somebody else",
    "start": "2124910",
    "end": "2131779"
  },
  {
    "text": "created a cluster and they did not give me a config or they give me a config which is out of date now if we have the AWS CLI then of",
    "start": "2131779",
    "end": "2138920"
  },
  {
    "text": "course you can use AWS eks command over there to create a cube config over there",
    "start": "2138920",
    "end": "2144980"
  },
  {
    "text": "for you you need to have the I am role and the cluster endpoint but once you have those data then you can create your",
    "start": "2144980",
    "end": "2150349"
  },
  {
    "text": "new cube config for you or if the cluster was created using eks cartel which we said is the official CLI then",
    "start": "2150349",
    "end": "2157160"
  },
  {
    "text": "eks Caudill also has a tool here and the commands for generating your cube config file are specified over here so for",
    "start": "2157160",
    "end": "2163819"
  },
  {
    "start": "2159000",
    "end": "2203000"
  },
  {
    "text": "example you could use AWS eks update cube config another command is update",
    "start": "2163819",
    "end": "2169039"
  },
  {
    "text": "but essentially it gives you a new cube config that you can talk to so oftentimes what I would do is I would",
    "start": "2169039",
    "end": "2174769"
  },
  {
    "text": "create this command not generate this into my cube config file I will generate",
    "start": "2174769",
    "end": "2180049"
  },
  {
    "text": "it into a separate config file set up my cube config to that config file again I'm trying to isolate the problem over",
    "start": "2180049",
    "end": "2185779"
  },
  {
    "text": "here and then say now go talk to the cluster so can I help you debug your problem and isolate your issue and then",
    "start": "2185779",
    "end": "2192440"
  },
  {
    "text": "in case of eki scuttle then you can say eks cut all utils right cube config give me the cluster name and then it",
    "start": "2192440",
    "end": "2198589"
  },
  {
    "text": "generates the configuration file you have to specify the I am role as well",
    "start": "2198589",
    "end": "2203499"
  },
  {
    "start": "2203000",
    "end": "2243000"
  },
  {
    "text": "another thing that you wanna look at is is your cluster even accessible so you",
    "start": "2203650",
    "end": "2209569"
  },
  {
    "text": "can literally give the command curl - Kay HTTP and the cluster API endpoint and it shows you that hey this is what",
    "start": "2209569",
    "end": "2215930"
  },
  {
    "text": "is available now eks cluster uses I will just that uses I am authentication",
    "start": "2215930",
    "end": "2222739"
  },
  {
    "text": "essentially so if you were to give this from your desktop that will not work so",
    "start": "2222739",
    "end": "2228200"
  },
  {
    "text": "this command was really given from an ec2 instance that was set up in the V PC",
    "start": "2228200",
    "end": "2233569"
  },
  {
    "text": "the same V PC as the cluster was created because then you have this write privileges you know to be able to talk",
    "start": "2233569",
    "end": "2239089"
  },
  {
    "text": "to the IP API server endpoint and then you will see this response if you were to do this from your server",
    "start": "2239089",
    "end": "2245870"
  },
  {
    "text": "then you will actually get authorization error message not hey I can't talk to the server because first of all you are",
    "start": "2245870",
    "end": "2251960"
  },
  {
    "text": "using HTTP and the server has to be HTTP even if you give it a HTTP endpoint it",
    "start": "2251960",
    "end": "2257120"
  },
  {
    "text": "will not work so the way you know this would this could work is potentially you could use like AWS command to first of",
    "start": "2257120",
    "end": "2265880"
  },
  {
    "text": "all take the I am token eks command say there's a AWS eks get token command to",
    "start": "2265880",
    "end": "2271940"
  },
  {
    "text": "which you pass it a iya I am role or Arne of the I am role that'll get you a token then you use that token then you",
    "start": "2271940",
    "end": "2278750"
  },
  {
    "text": "pass it on curl that hey use that token to make that request because end of the day that's the token is looking for when",
    "start": "2278750",
    "end": "2285110"
  },
  {
    "text": "it's trying to do the authentication and then your command will work so my point is there are a lot of ways by which you",
    "start": "2285110",
    "end": "2291170"
  },
  {
    "text": "can debug if your service is accessible or not another error source of error",
    "start": "2291170",
    "end": "2298130"
  },
  {
    "start": "2295000",
    "end": "2357000"
  },
  {
    "text": "that we have seen is somebody else created a cluster and I'm using my I am",
    "start": "2298130",
    "end": "2303620"
  },
  {
    "text": "role and is saying I'm not allowed to access the I am role so the way it looks at it is in the kubernetes cluster we",
    "start": "2303620",
    "end": "2310400"
  },
  {
    "text": "have a config map called as AWS auth config map and in the config map the users and the roles that are allowed to",
    "start": "2310400",
    "end": "2317570"
  },
  {
    "text": "talk to the cluster and at what different auerbach capabilities are listed over there so what you want to do",
    "start": "2317570",
    "end": "2323870"
  },
  {
    "text": "is check be with your admin or the person who created the cluster or that hey that's my role or the I am really",
    "start": "2323870",
    "end": "2330500"
  },
  {
    "text": "exist in the auth config map as well and if not then you want to get it added over there actually so if you look at it",
    "start": "2330500",
    "end": "2336680"
  },
  {
    "text": "at the bottom of the screen there is a map roles and map users so make sure you are added right over there",
    "start": "2336680",
    "end": "2343130"
  },
  {
    "text": "now you may wonder this is not the right Arn format it should be are in coolin coolin coolin I am but the colon colon",
    "start": "2343130",
    "end": "2350240"
  },
  {
    "text": "was kind of messing up our formatting but make sure this is not syntactically valid but that's what it's supposed to",
    "start": "2350240",
    "end": "2355520"
  },
  {
    "text": "be so again as I said you know if you are not part of the AWS auth config map",
    "start": "2355520",
    "end": "2362060"
  },
  {
    "text": "then you will say hey you know you must be logged in to the server so you are unauthorized and it would not let you",
    "start": "2362060",
    "end": "2367070"
  },
  {
    "text": "communicate with the server so in this case you know what you're done is you'll get your own added to the AWS auth config map",
    "start": "2367070",
    "end": "2372560"
  },
  {
    "text": "and voila now your cube Caudill works and then you can say cube model version",
    "start": "2372560",
    "end": "2378200"
  },
  {
    "start": "2373000",
    "end": "2394000"
  },
  {
    "text": "it will give you the client version the server version or you can use Cupid cluster info then it gives you the",
    "start": "2378200",
    "end": "2384260"
  },
  {
    "text": "server endpoint and what more details about it you can get the component status and it gives you scheduler controller manager HDD status all that",
    "start": "2384260",
    "end": "2390950"
  },
  {
    "text": "so all those good things come along to you something else we have seen is that",
    "start": "2390950",
    "end": "2396920"
  },
  {
    "start": "2394000",
    "end": "2410000"
  },
  {
    "text": "hey I'm trying to do Q cuddle get nodes it's not even showing me the nodes this",
    "start": "2396920",
    "end": "2402920"
  },
  {
    "text": "is one of the capabilities that is there as again I'm using Amazon eks as an example over here but make sure your API",
    "start": "2402920",
    "end": "2411890"
  },
  {
    "start": "2410000",
    "end": "2459000"
  },
  {
    "text": "endpoint is publicly accessible because oftentimes our customers ask that I don't want my API server endpoint to be",
    "start": "2411890",
    "end": "2418850"
  },
  {
    "text": "publicly accessible I only want it to be accessible from within the VPC so this",
    "start": "2418850",
    "end": "2423890"
  },
  {
    "text": "is a snapshot of the AWS console so you can take a look at it at the bottom it says public access is disabled so you",
    "start": "2423890",
    "end": "2432020"
  },
  {
    "text": "need to select enable and then the public endpoint or the API server endpoint is accessible publicly and then",
    "start": "2432020",
    "end": "2439040"
  },
  {
    "text": "you can talk to it from your desktop a century so that's an important aspect so",
    "start": "2439040",
    "end": "2444380"
  },
  {
    "text": "some of the ways by which you need to look at you know what is happening in terms of cube cuddle to get work or to",
    "start": "2444380",
    "end": "2450140"
  },
  {
    "text": "just be able to talk to the server because if that's not working then no parts are gonna get deployed over there",
    "start": "2450140",
    "end": "2455750"
  },
  {
    "text": "at all and I'm doing this one as well all right pods all this work all this",
    "start": "2455750",
    "end": "2463670"
  },
  {
    "start": "2459000",
    "end": "2474000"
  },
  {
    "text": "creation of cluster all this cube kernel to work all this networking to work is to get that one pod running let's look",
    "start": "2463670",
    "end": "2470720"
  },
  {
    "text": "at pods so in terms of pod lifecycle",
    "start": "2470720",
    "end": "2476630"
  },
  {
    "start": "2474000",
    "end": "2555000"
  },
  {
    "text": "there's a community's documentation is awesome so it very clearly explains what are the different phases of pods and",
    "start": "2476630",
    "end": "2483110"
  },
  {
    "text": "what are the meanings of that so I'm not going to get into the details of it but essentially appending running succeeded",
    "start": "2483110",
    "end": "2489020"
  },
  {
    "text": "failed and unknown are the five stages the only happy stages are running and",
    "start": "2489020",
    "end": "2494120"
  },
  {
    "text": "succeeded that means you know that the power is running well and even in running depending upon how many",
    "start": "2494120",
    "end": "2500390"
  },
  {
    "text": "containers are in the pod it will tell you one out of one of 2 or 1 out of 2 so you wanna pay close attention to those numbers because",
    "start": "2500390",
    "end": "2507890"
  },
  {
    "text": "if it's one out of two then one container in that part is not running you wanna make sure is X out of X no it",
    "start": "2507890",
    "end": "2513590"
  },
  {
    "text": "should not be X out of y where x and y are different numbers succeded means you know you are probably running a job and",
    "start": "2513590",
    "end": "2519920"
  },
  {
    "text": "then the job has completed and is succeeded so that's good as well pending we talked about some of the reasons you know why it could happen",
    "start": "2519920",
    "end": "2525830"
  },
  {
    "text": "I'll dig a little bit deeper into that unknown typically happens when the cubelet is not able to communicate with",
    "start": "2525830",
    "end": "2532580"
  },
  {
    "text": "the server network partition multiple reasons that could happen and failed is",
    "start": "2532580",
    "end": "2537950"
  },
  {
    "text": "again could be the reason where we have seen most commonly happening when you're not able to pull the image for example",
    "start": "2537950",
    "end": "2544400"
  },
  {
    "text": "and that's where it starts failing so then again you kind of look at the details of the power that hey give me",
    "start": "2544400",
    "end": "2549830"
  },
  {
    "text": "more details and it tells you that the image not found and all those error messages start showing up for you so",
    "start": "2549830",
    "end": "2556790"
  },
  {
    "start": "2555000",
    "end": "2583000"
  },
  {
    "text": "what we're going to do is we're going to take the fantastic hello deployment that you created and we're going to create 8 deploy 8 replicas of that so I just",
    "start": "2556790",
    "end": "2563660"
  },
  {
    "text": "created a simple deployment here or we scale an existing deployment to a trap like us and I see that only four",
    "start": "2563660",
    "end": "2571010"
  },
  {
    "text": "replicas are available ok so again not a happy use case so let's take a little",
    "start": "2571010",
    "end": "2576980"
  },
  {
    "text": "bit more detail on why pod could be pending and what are the things that we could do basically for it to work around",
    "start": "2576980",
    "end": "2582380"
  },
  {
    "text": "that now when I look at say list pods and I say Keuka I'll get parts it says",
    "start": "2582380",
    "end": "2590030"
  },
  {
    "text": "yeah sure for pods are pending what are some of",
    "start": "2590030",
    "end": "2598100"
  },
  {
    "start": "2597000",
    "end": "2654000"
  },
  {
    "text": "the reasons why the pods may be pending the most common reason is not enough",
    "start": "2598100",
    "end": "2603620"
  },
  {
    "text": "resources in the cluster on a part you can specify resource limit port and",
    "start": "2603620",
    "end": "2611660"
  },
  {
    "text": "resource limit could be for CPU memory so there is most like the most common reason is there's not enough of those",
    "start": "2611660",
    "end": "2617330"
  },
  {
    "text": "resources in the cluster ok so that's one part of it something that Ray already covered is you know not enough",
    "start": "2617330",
    "end": "2623360"
  },
  {
    "text": "IP addresses in the cluster because the way we allocate IP addresses is each a",
    "start": "2623360",
    "end": "2629120"
  },
  {
    "text": "and I can give you certain number of IP addresses and you do that sophisticated math formula and then you say hey these",
    "start": "2629120",
    "end": "2635360"
  },
  {
    "text": "are the IP addresses available so you need to kind of scale your cluster if you wanna run sort of number of paths okay",
    "start": "2635360",
    "end": "2640820"
  },
  {
    "text": "you also wanna ensure that your all nodes are healthy so maybe you think you have a four node cluster but really what",
    "start": "2640820",
    "end": "2646460"
  },
  {
    "text": "you have is a two node cluster and the other nodes are not getting recycled for some reason so make sure that their",
    "start": "2646460",
    "end": "2652160"
  },
  {
    "text": "health is looking good okay so now what I'm gonna do is I'm gonna say describe a",
    "start": "2652160",
    "end": "2658730"
  },
  {
    "start": "2654000",
    "end": "2702000"
  },
  {
    "text": "pod I'm gonna take a look at the one specific part that was not healthy so I'm looking at it it says failed",
    "start": "2658730",
    "end": "2664790"
  },
  {
    "text": "scheduling is the reason for that and it says zero out of four nodes are available insufficient CPUs as I said on",
    "start": "2664790",
    "end": "2672980"
  },
  {
    "text": "a part you can define resources limit resources and limits and those are for",
    "start": "2672980",
    "end": "2679820"
  },
  {
    "text": "CPU and memory by default and we'll get into those those part of it actually next so the important part here is these",
    "start": "2679820",
    "end": "2686000"
  },
  {
    "text": "events even though I created a deployment these events are only available on pods not on deployments",
    "start": "2686000",
    "end": "2693140"
  },
  {
    "text": "replicas at a job because end of the day they are creating a part so you really need to say describe that part and then",
    "start": "2693140",
    "end": "2699710"
  },
  {
    "text": "you'll get all the details about it okay so now what I'm doing is okay get me all",
    "start": "2699710",
    "end": "2704930"
  },
  {
    "start": "2702000",
    "end": "2747000"
  },
  {
    "text": "the events and we looked at that detail already it says all the events that I'm getting is zero out of four nodes are",
    "start": "2704930",
    "end": "2710300"
  },
  {
    "text": "available so there is not sufficient CPU available for me now I want to only get",
    "start": "2710300",
    "end": "2715640"
  },
  {
    "text": "warning events that's important and again we looked at it earlier we also looked at say fuel selector one",
    "start": "2715640",
    "end": "2722420"
  },
  {
    "text": "of the things I also want to highlight is I can actually get involved object",
    "start": "2722420",
    "end": "2727520"
  },
  {
    "text": "dot name so now I'm actually specifying the specific pod name here that show me",
    "start": "2727520",
    "end": "2732890"
  },
  {
    "text": "all the events warning events for this spot simple tips I don't know which can",
    "start": "2732890",
    "end": "2737960"
  },
  {
    "text": "hopefully will go a long way and kind of debug in your problem and again we are purely using cube model we are not even",
    "start": "2737960",
    "end": "2743270"
  },
  {
    "text": "gone into any of the system tools yet another tool that we have found handy is",
    "start": "2743270",
    "end": "2749840"
  },
  {
    "start": "2747000",
    "end": "2761000"
  },
  {
    "text": "again cube curl Gary Vance show me you know list sorted by timestamp so I want to look at the recent event you know",
    "start": "2749840",
    "end": "2755990"
  },
  {
    "text": "flossed five last ten last twenty whatever that number is you can start specifying that okay so let's look at",
    "start": "2755990",
    "end": "2762770"
  },
  {
    "start": "2761000",
    "end": "2816000"
  },
  {
    "text": "you know the memory and CPU requirements what is the reason that my part may not be running so now if I look at the",
    "start": "2762770",
    "end": "2768760"
  },
  {
    "text": "describe deployment /hello it says sure it is a nginx latest image that I'm trying to deploy over there but if I",
    "start": "2768760",
    "end": "2775360"
  },
  {
    "text": "look at the limits it says two CPUs as the limit and two gigabyte as the memory",
    "start": "2775360",
    "end": "2781930"
  },
  {
    "text": "and similarly the request is to and memory is to a fundamental concept that",
    "start": "2781930",
    "end": "2787510"
  },
  {
    "text": "you wanna understand over here is the request is what it will initially get and limit is the maximum value that it",
    "start": "2787510",
    "end": "2793720"
  },
  {
    "text": "will go to okay so that's the way you want to think about it so essentially what I'm looking at here is I need if",
    "start": "2793720",
    "end": "2800500"
  },
  {
    "text": "I'm doing eight replicas of the pod then I need a whole bunch of memory you know some essentially I'm looking at sixteen",
    "start": "2800500",
    "end": "2806620"
  },
  {
    "text": "CPUs and I'm looking at about sixteen gigabyte hey I'm running a sixteen CPU cluster why this thing is not running I",
    "start": "2806620",
    "end": "2813220"
  },
  {
    "text": "have 16 CPUs let's look at that more now the default CPU request is 200 m and 200",
    "start": "2813220",
    "end": "2822190"
  },
  {
    "text": "m is think of it as the point to CPU and in terms of a V CPU if you think of in terms of AWS or any other cloud provider",
    "start": "2822190",
    "end": "2828460"
  },
  {
    "text": "and none on memory so if you create a power and you say describe the power to",
    "start": "2828460",
    "end": "2834340"
  },
  {
    "text": "me and then it'll show you that requester is 200 m and nothing on CPU and no limit okay so if I need 16 CPUs",
    "start": "2834340",
    "end": "2842830"
  },
  {
    "text": "and I am running a 16 CPU cluster or why those 16 CPUs are not available to me",
    "start": "2842830",
    "end": "2848970"
  },
  {
    "text": "now the way I can look at that is I can give a command called as Q corral top nodes and if I give that command it",
    "start": "2848970",
    "end": "2855670"
  },
  {
    "start": "2849000",
    "end": "2886000"
  },
  {
    "text": "shows me sort of the memory CPU utilization on my cluster okay so I'm gonna give that command but it says oh",
    "start": "2855670",
    "end": "2861400"
  },
  {
    "text": "this service is not available so the service is currently unable to handle the requests get nodes metrics gates API",
    "start": "2861400",
    "end": "2867910"
  },
  {
    "text": "now this is an API which gathers all the data from nodes and it gathers all the",
    "start": "2867910",
    "end": "2874660"
  },
  {
    "text": "data used to be heap store but now it's more Prometheus so it gathers all the data from your node and it publishes it",
    "start": "2874660",
    "end": "2880960"
  },
  {
    "text": "and that's where the data is being generated so what's really missing for me in this case is really a metric",
    "start": "2880960",
    "end": "2886090"
  },
  {
    "start": "2886000",
    "end": "2907000"
  },
  {
    "text": "server and again this is a very standard upstream kubernetes tool so what you can do is you can install the kubernetes",
    "start": "2886090",
    "end": "2891820"
  },
  {
    "text": "metric server over here just literally clone the repo and in the 1.8 plus",
    "start": "2891820",
    "end": "2898090"
  },
  {
    "text": "directory there are a bunch of deployment files go ahead deploy them to your cluster and once you deploy that then your",
    "start": "2898090",
    "end": "2904070"
  },
  {
    "text": "services up or at least the API is available and you can verify that your matrix API is available so you just say",
    "start": "2904070",
    "end": "2910490"
  },
  {
    "start": "2907000",
    "end": "2917000"
  },
  {
    "text": "get API service give it a number and it says yep looking good available and running for a while now when I say cue",
    "start": "2910490",
    "end": "2919580"
  },
  {
    "start": "2917000",
    "end": "2966000"
  },
  {
    "text": "plural top nodes I'm getting the right data so it's reporting the codes the CPU",
    "start": "2919580",
    "end": "2924770"
  },
  {
    "text": "so I said yeah you're running about you you're using about 28 M so about point zero two eight is what you're utilizing",
    "start": "2924770",
    "end": "2931780"
  },
  {
    "text": "zero percent is being utilized and it's using about 410 megabytes of memory is",
    "start": "2931780",
    "end": "2937070"
  },
  {
    "text": "about two percent of memory for all of the 16 gigabyte that I have available okay now remember all of 16 Giga 16 CPUs",
    "start": "2937070",
    "end": "2946670"
  },
  {
    "text": "are not available to you that's an important part of thing you are running the 16 CPU cluster but certain amount of",
    "start": "2946670",
    "end": "2953330"
  },
  {
    "text": "memory is required to run cubelet and certain amount of memory is run to run the operating system demon the CRI and",
    "start": "2953330",
    "end": "2959630"
  },
  {
    "text": "all those things so all of 16 CPU is not available let's look into that then",
    "start": "2959630",
    "end": "2966009"
  },
  {
    "start": "2966000",
    "end": "3031000"
  },
  {
    "text": "there is a concept of capacity memory and allocatable memory so the capacity",
    "start": "2966040",
    "end": "2973690"
  },
  {
    "text": "no top nose is top nose is only for the nose itself yeah the question is the top",
    "start": "2979330",
    "end": "2985520"
  },
  {
    "text": "notes command is is using the request and the limit no it's not top notes is just saying that how much memory and CPU",
    "start": "2985520",
    "end": "2990860"
  },
  {
    "text": "is available for the nodes correct and",
    "start": "2990860",
    "end": "2995920"
  },
  {
    "text": "so now what I'm saying is there's a concept of a capacity memory and allocatable memory the capacity memory",
    "start": "2995920",
    "end": "3001330"
  },
  {
    "text": "is basically how much capacity is available and allocatable memory is sure",
    "start": "3001330",
    "end": "3006700"
  },
  {
    "text": "you have a capacity of 16 gigabyte but certain amount of memory is already",
    "start": "3006700",
    "end": "3011980"
  },
  {
    "text": "being used for cubelet and other processes so I want to make sure that I'm giving you only the allocatable",
    "start": "3011980",
    "end": "3017620"
  },
  {
    "text": "memory and you can see there's a bit of a difference here is that here is about hundred one five nine five and it is one",
    "start": "3017620",
    "end": "3022780"
  },
  {
    "text": "five eight four so whatever that Delta is is being used for cube black okay the",
    "start": "3022780",
    "end": "3031930"
  },
  {
    "start": "3031000",
    "end": "3055000"
  },
  {
    "text": "way allocatable is calculated as a matter of fact you of course look at the node capacity so you remove cube",
    "start": "3031930",
    "end": "3038890"
  },
  {
    "text": "reserved memory you know whatever is required by the cube lat essentially you remove the system reserved memory and",
    "start": "3038890",
    "end": "3044650"
  },
  {
    "text": "you can also set up the hard eviction threshold memory which is the memory which you know at which point you know",
    "start": "3044650",
    "end": "3049690"
  },
  {
    "text": "it'll start ejecting parts you know because it knows it's right of the threshold essentially so one of the key",
    "start": "3049690",
    "end": "3058090"
  },
  {
    "start": "3055000",
    "end": "3109000"
  },
  {
    "text": "concepts is this is explained at node allocatable so it's worth reading on how all this thing work the allocatable and",
    "start": "3058090",
    "end": "3063820"
  },
  {
    "text": "the capacity memory because that is a fundamental part on how you are designing your cluster okay so now let's",
    "start": "3063820",
    "end": "3070390"
  },
  {
    "text": "take a look at it when I do the same thing for capacity and allocatable if the numbers don't change it says the",
    "start": "3070390",
    "end": "3076360"
  },
  {
    "text": "cpus are four and four if four is allocatable why that means I have 16",
    "start": "3076360",
    "end": "3081850"
  },
  {
    "text": "CPUs why I'm not able to allocate it okay now yes allocatable should be",
    "start": "3081850",
    "end": "3087910"
  },
  {
    "text": "calculated that way but when you are creating the cluster at that point of time you as a cluster operator cluster",
    "start": "3087910",
    "end": "3095830"
  },
  {
    "text": "designer has to specifically say oh carve out this memory for cubelet reserve and then it says I'm yeah",
    "start": "3095830",
    "end": "3102910"
  },
  {
    "text": "because you are asking now I'm gonna take that into consideration of calculating the allocatable part of it",
    "start": "3102910",
    "end": "3107950"
  },
  {
    "text": "okay so one of the things that we recently and this was literally done about two weeks ago when using eks now",
    "start": "3107950",
    "end": "3116350"
  },
  {
    "start": "3109000",
    "end": "3136000"
  },
  {
    "text": "when you're creating a kubernetes cluster we allow you to set those values eviction hard and cube reserved",
    "start": "3116350",
    "end": "3121660"
  },
  {
    "text": "basically those two values so those are set in the kubernetes cluster and that",
    "start": "3121660",
    "end": "3126670"
  },
  {
    "text": "is a clear indication that okay this is the amount of CPU and the memory that is available so those are allocated for",
    "start": "3126670",
    "end": "3133240"
  },
  {
    "text": "system processes so now it's evident that hey we don't have enough CPUs so",
    "start": "3133240",
    "end": "3139840"
  },
  {
    "start": "3136000",
    "end": "3176000"
  },
  {
    "text": "what we need to do is set up a cluster autoscaler cluster autoscaler is again an upstream component it serves two",
    "start": "3139840",
    "end": "3145930"
  },
  {
    "text": "purposes it basically makes sure that if any parts that are failing due to insufficient resources then it'll just",
    "start": "3145930",
    "end": "3152950"
  },
  {
    "text": "scale the cluster for you and then it will also recycle nodes that are underutilized for an extended period of",
    "start": "3152950",
    "end": "3159340"
  },
  {
    "text": "time so let's say you have a 10 word cluster and it recognizes that hey by the way all the nodes are running about",
    "start": "3159340",
    "end": "3164950"
  },
  {
    "text": "20% of CPU so what I'm gonna do is I'm gonna pods or straight lists I'm going",
    "start": "3164950",
    "end": "3170320"
  },
  {
    "text": "to move them to a different node and I'm going to recycle those pods I wanna make sure that your cluster size is optimal",
    "start": "3170320",
    "end": "3176130"
  },
  {
    "text": "so what we're gonna do is we're gonna install it and again for installing it we need to setup certain I am policies",
    "start": "3176130",
    "end": "3181630"
  },
  {
    "text": "attached the policy to the I am role we on the auto scaling group which are",
    "start": "3181630",
    "end": "3187360"
  },
  {
    "start": "3185000",
    "end": "3213000"
  },
  {
    "text": "behind the scene for us this is what is connected to so cluster autoscaler is a kubernetes concept but end of the day",
    "start": "3187360",
    "end": "3193180"
  },
  {
    "text": "they need to talk to auto scaling group because that's where the in AWS language where your cluster is scaled so you need",
    "start": "3193180",
    "end": "3199990"
  },
  {
    "text": "to make sure that you set up the correct tags over there and these are Kate's IO tag so once you set up those tags on the",
    "start": "3199990",
    "end": "3207160"
  },
  {
    "text": "auto scaling group that means auto scaling group can be automatically discovered by a cluster or a scaler",
    "start": "3207160",
    "end": "3212970"
  },
  {
    "text": "now you create the cluster autoscaler so you essentially need a certificate file then you create the cluster or a scalar",
    "start": "3212970",
    "end": "3221200"
  },
  {
    "start": "3213000",
    "end": "3223000"
  },
  {
    "text": "here okay I need to look at it that ok now I can look at my cluster autoscaler",
    "start": "3221200",
    "end": "3227380"
  },
  {
    "start": "3223000",
    "end": "3248000"
  },
  {
    "text": "logs essentially and if I look at my autoscaler logs they are the parts of the cluster or a scalar are running but",
    "start": "3227380",
    "end": "3233800"
  },
  {
    "text": "it says skipping node group over here because it says the max size is reached",
    "start": "3233800",
    "end": "3239380"
  },
  {
    "text": "sure I've installed the cluster or the scalar but as I said the cluster or the scalars behind-the-scene talked to auto",
    "start": "3239380",
    "end": "3244780"
  },
  {
    "text": "scaling so the next thing that we need to do really is update our auto scaling group limits so we go to our auto scaling",
    "start": "3244780",
    "end": "3251590"
  },
  {
    "start": "3248000",
    "end": "3307000"
  },
  {
    "text": "group and to the auto scaling itself I say oh by the way change my Mac size to",
    "start": "3251590",
    "end": "3257260"
  },
  {
    "text": "eight so now I can set up my auto scaling group to be eight one thing to",
    "start": "3257260",
    "end": "3262930"
  },
  {
    "text": "understand over here is the cluster or a scaler is being reactive in the sense that it looks at oh by the way the pods",
    "start": "3262930",
    "end": "3268990"
  },
  {
    "text": "are not being scheduled and now I need to scale it you can actually use auto scaling group use the auto scaling group",
    "start": "3268990",
    "end": "3275050"
  },
  {
    "text": "policies to being proactive in which you can set up in the auto scaling group that hey you know if the CPU utilization",
    "start": "3275050",
    "end": "3281740"
  },
  {
    "text": "goes above 50 percent automatically scaled my cluster and add more nodes to",
    "start": "3281740",
    "end": "3286780"
  },
  {
    "text": "it so you can use a combination of the two to make sure you know your pods are always like you know your cluster is an",
    "start": "3286780",
    "end": "3292330"
  },
  {
    "text": "optimal size and your pods can always be scheduled so now essentially if I run my",
    "start": "3292330",
    "end": "3300400"
  },
  {
    "text": "cube kernel get parts you know cluster a scale or a scaling group is set and all my parts are running basically at this",
    "start": "3300400",
    "end": "3306280"
  },
  {
    "text": "point of time similar use case I was running a machine learning workload on",
    "start": "3306280",
    "end": "3311470"
  },
  {
    "start": "3307000",
    "end": "3331000"
  },
  {
    "text": "this kubernetes cluster over there using cube flow and I saw that hey m missed inference I'm running an inference task",
    "start": "3311470",
    "end": "3317080"
  },
  {
    "text": "here and I say zero of one ready pending and let's describe this part a bit more",
    "start": "3317080",
    "end": "3322750"
  },
  {
    "text": "and as I described it it says all right you know insufficient NVIDIA GPU",
    "start": "3322750",
    "end": "3328000"
  },
  {
    "text": "available to me so essentially is my solution for that is set up my cluster autoscaler use the auto scaling group to",
    "start": "3328000",
    "end": "3335440"
  },
  {
    "start": "3331000",
    "end": "3370000"
  },
  {
    "text": "look at that matrix and say what scale my cluster and that way it can solve that problem",
    "start": "3335440",
    "end": "3342150"
  },
  {
    "text": "awesome so so now we have very easily created a cluster it was super simple",
    "start": "3342450",
    "end": "3349000"
  },
  {
    "text": "right we didn't run into any problems because we read the documentation first but now you have a pod add hello world",
    "start": "3349000",
    "end": "3355780"
  },
  {
    "text": "deployment running and Nick we want to show the rest of the world how cool it",
    "start": "3355780",
    "end": "3361150"
  },
  {
    "text": "is and how you know the amazing fonts that we've used in the CAD pictures that we've used we want to show it to the",
    "start": "3361150",
    "end": "3366850"
  },
  {
    "text": "rest of the world so a couple things you can do is you can use a tooth either the",
    "start": "3366850",
    "end": "3373480"
  },
  {
    "start": "3370000",
    "end": "3491000"
  },
  {
    "text": "service to expose your paths to the rest of the world so then will be accessible from outside your",
    "start": "3373480",
    "end": "3379250"
  },
  {
    "text": "bluster or you can use the ingress object so if you just do service type load balancer if you just say expose my",
    "start": "3379250",
    "end": "3387109"
  },
  {
    "text": "service then it's going to by default install create a classic load balancer",
    "start": "3387109",
    "end": "3392780"
  },
  {
    "text": "we have three types of load balancer Network load balancer which is there for classic does four and seven and an",
    "start": "3392780",
    "end": "3399020"
  },
  {
    "text": "application Allah balancer that does there are seven so if you but if you just say service type load balancer in",
    "start": "3399020",
    "end": "3405349"
  },
  {
    "text": "your service then you're going to get a classic load balancer if you wanted to use a newer Network load balancer then all you have to do is",
    "start": "3405349",
    "end": "3412160"
  },
  {
    "text": "annotate your service with just load balancer type NLB and e KS is going to",
    "start": "3412160",
    "end": "3417560"
  },
  {
    "text": "provision and NLB in the backend now you don't have to be using eks for that you could just be running kubernetes on AWS",
    "start": "3417560",
    "end": "3423830"
  },
  {
    "text": "and and then that should work as well the second option if you're operating in micro services environment is to use al",
    "start": "3423830",
    "end": "3430730"
  },
  {
    "text": "being rest controller and it creates the ingress resource inside your cluster and then you can basically say when you get",
    "start": "3430730",
    "end": "3437210"
  },
  {
    "text": "traffic on slash front-ends and send the traffic to this particular service that's running front-end and when you",
    "start": "3437210",
    "end": "3443000"
  },
  {
    "text": "get the traffic on slash back and send this to another set of service and eventually to the pods that are running",
    "start": "3443000",
    "end": "3448790"
  },
  {
    "text": "the backend service so you can use l being pressed controller you just install the controller inside will",
    "start": "3448790",
    "end": "3455119"
  },
  {
    "text": "provision and al be on your behalf in your account and that's it then you",
    "start": "3455119",
    "end": "3460730"
  },
  {
    "text": "start creating your ingress rules and you can start serving traffic in addition to al being rest control you",
    "start": "3460730",
    "end": "3467510"
  },
  {
    "text": "there are a bunch of open source and commercial ingress ingress is out there the most popular one that I've come",
    "start": "3467510",
    "end": "3474380"
  },
  {
    "text": "across is nginx so that the really works really well but it doesn't mean that",
    "start": "3474380",
    "end": "3480260"
  },
  {
    "text": "you're you know creating services you know you get the theme right we're not gonna just say create load balancer and",
    "start": "3480260",
    "end": "3486849"
  },
  {
    "text": "automatically it's going to happen because we're the worst users of course it will fail so when it fails if your",
    "start": "3486849",
    "end": "3493400"
  },
  {
    "start": "3491000",
    "end": "3562000"
  },
  {
    "text": "load balancer is not provisioning the first thing I would do is get checked tags so there are if you go to the",
    "start": "3493400",
    "end": "3498830"
  },
  {
    "text": "documentation hidden in very deep inside our documentation is this really important tag that you should put in",
    "start": "3498830",
    "end": "3505430"
  },
  {
    "text": "your subnets otherwise your load balancer is not going to work so make sure that you you have those tags in",
    "start": "3505430",
    "end": "3511099"
  },
  {
    "text": "your in your subnet if you still if you have the load bouncer enabled and you're still it's",
    "start": "3511099",
    "end": "3516870"
  },
  {
    "text": "still not working then the next step would be to go check your service and check the events check the status",
    "start": "3516870",
    "end": "3522300"
  },
  {
    "text": "sometimes we'll say load balancer provisioning failed and it can give you some interesting idea of what to do next",
    "start": "3522300",
    "end": "3528530"
  },
  {
    "text": "the last thing I would check is the eks service I am role so remember I said when use they say service type load",
    "start": "3528530",
    "end": "3535230"
  },
  {
    "text": "balancer eks is going to provision a load balancer in your account this role",
    "start": "3535230",
    "end": "3540450"
  },
  {
    "text": "gives us it gives us the ability to do that if you don't have this role then we basically don't have the ability to come",
    "start": "3540450",
    "end": "3547320"
  },
  {
    "text": "in your account and start the create a load balancer so make sure you give that permission to us now if you have your",
    "start": "3547320",
    "end": "3554400"
  },
  {
    "text": "load balancer it gets provisioned don't think that you're not going to run into some problems there so it's the most",
    "start": "3554400",
    "end": "3560490"
  },
  {
    "text": "common problems that I face is you know everything works but my application is",
    "start": "3560490",
    "end": "3565560"
  },
  {
    "start": "3562000",
    "end": "3600000"
  },
  {
    "text": "slow and this is without Kremlin's chaos the engineering this is just my hello world",
    "start": "3565560",
    "end": "3570720"
  },
  {
    "text": "you know this cannot be chaos engineered I thrive in chaos so first thing you",
    "start": "3570720",
    "end": "3576480"
  },
  {
    "text": "should do very easy to do is enable logging inside your load balancers it's",
    "start": "3576480",
    "end": "3581670"
  },
  {
    "text": "very easy to do it's a checkbox if you're lazy like me go in the console enable it that will give you some of the",
    "start": "3581670",
    "end": "3587310"
  },
  {
    "text": "metrics like latency and application metrics like 4x X 5 xx errors and those",
    "start": "3587310",
    "end": "3592740"
  },
  {
    "text": "are helpful you can see if there's a high error rate in your application that could be the reason why it is it's your",
    "start": "3592740",
    "end": "3599280"
  },
  {
    "text": "request is slow or failing if you see that there's everything is good inside your at you look in your load balancer",
    "start": "3599280",
    "end": "3606210"
  },
  {
    "text": "front the next thing I would do is start a part inside your inside your kubernetes cluster and access the",
    "start": "3606210",
    "end": "3613740"
  },
  {
    "text": "service right there and see what the metrics are so you can create a shell demo pod and it basically gives drops",
    "start": "3613740",
    "end": "3619950"
  },
  {
    "text": "you into a Debian or go into like environment where you can install some really good troubleshooting tools like",
    "start": "3619950",
    "end": "3626340"
  },
  {
    "text": "Carl and you can curl directly the IP or the fully qualified domain name and you",
    "start": "3626340",
    "end": "3632400"
  },
  {
    "text": "can see what it really is that that's causing problems so a lot of times you will see that the application is slow",
    "start": "3632400",
    "end": "3638100"
  },
  {
    "text": "either because your pods are spread out too wide nodes have latency in between them and this is a good tool to help you",
    "start": "3638100",
    "end": "3645240"
  },
  {
    "text": "identify now another thing that you can do is when you create a service you're also getting endpoints so you can also try to",
    "start": "3645240",
    "end": "3650880"
  },
  {
    "text": "ping the endpoints and see if one particular endpoint is is creating problems and the rest are fine and",
    "start": "3650880",
    "end": "3657140"
  },
  {
    "text": "lastly if you if you if you want if you can't change your app definitely",
    "start": "3657140",
    "end": "3663000"
  },
  {
    "text": "implements tracing in your application micro services without tracing is super hard to do because you're basically the",
    "start": "3663000",
    "end": "3669720"
  },
  {
    "text": "request could be coming in from one app and traversing through twelve different apps and then giving the result back so",
    "start": "3669720",
    "end": "3676020"
  },
  {
    "text": "if you have latency in one of the app in the twelve apps that you have you can't",
    "start": "3676020",
    "end": "3681120"
  },
  {
    "text": "see that unless you have some kind of tracing ability and traditionally tracing has been really hard to him to",
    "start": "3681120",
    "end": "3687960"
  },
  {
    "text": "implement but if you're using a service mesh tracing becomes really easy and I would highly recommend you use some kind",
    "start": "3687960",
    "end": "3694770"
  },
  {
    "text": "of tracing solution with service mesh whether it's a jogger open tracing with ISTE or x-ray with the app mesh",
    "start": "3694770",
    "end": "3702840"
  },
  {
    "text": "definitely use it because it's low effort and it will give you the ability to look at that request and see where",
    "start": "3702840",
    "end": "3708990"
  },
  {
    "text": "it's failing or what is the what's calling the slowness in your environment so clearly networking really really",
    "start": "3708990",
    "end": "3716670"
  },
  {
    "text": "important monitoring that really really important piece that you cannot skip if",
    "start": "3716670",
    "end": "3721980"
  },
  {
    "text": "you have to run a successful kubernetes cluster so that you asked a question",
    "start": "3721980",
    "end": "3728160"
  },
  {
    "text": "like ok seems like kubernetes has these millions of resources no I'm sorry",
    "start": "3728160",
    "end": "3734160"
  },
  {
    "text": "it's a billions of resources what should I really monitor and that becomes a problem it's too much to monitor and",
    "start": "3734160",
    "end": "3740550"
  },
  {
    "text": "there's too much data causes too much confusion you don't really know should I be paying attention to this API latency",
    "start": "3740550",
    "end": "3747300"
  },
  {
    "text": "metrics or this resync interval what is what is it really so we've tried to simplify it for you you can of course",
    "start": "3747300",
    "end": "3754860"
  },
  {
    "text": "monitor everything and you should monitor everything but you don't have to pay attention to every metric some of",
    "start": "3754860",
    "end": "3760140"
  },
  {
    "text": "the key things that you want to monitor are your worker nodes clearly they are running your worker or your application",
    "start": "3760140",
    "end": "3765420"
  },
  {
    "text": "so make sure that they're running healthy then it's going to be a cluster of components like ingress controllers",
    "start": "3765420",
    "end": "3771230"
  },
  {
    "text": "dashboards tools for monitoring cluster autoscaler all of those things and then",
    "start": "3771230",
    "end": "3776970"
  },
  {
    "text": "finally the apps so some of them metrics that we recommend that you monitor to begin with on the nodes",
    "start": "3776970",
    "end": "3784470"
  },
  {
    "text": "definitely have CPU network memory that's the basic cluster components at",
    "start": "3784470",
    "end": "3789870"
  },
  {
    "text": "CD is something you should monitor if you're not running eks because if you",
    "start": "3789870",
    "end": "3795060"
  },
  {
    "text": "are using it yes that's our responsibility Amazon will make sure that your at CD cluster is healthy and",
    "start": "3795060",
    "end": "3800760"
  },
  {
    "text": "up and running all the time but if you're running your own cluster highly recommended because if you're at CD",
    "start": "3800760",
    "end": "3806100"
  },
  {
    "text": "cluster goes it dies and then we'll say goodbye to your kubernetes cluster",
    "start": "3806100",
    "end": "3811640"
  },
  {
    "text": "advanced really important monitor the monitoring tools and then have",
    "start": "3811640",
    "end": "3817170"
  },
  {
    "text": "monitoring tools monitor them we must go level deeper so if you have monitoring tools or ingress controllers",
    "start": "3817170",
    "end": "3824160"
  },
  {
    "text": "load balancers dashboards make sure you monitor those components as well and the",
    "start": "3824160",
    "end": "3829200"
  },
  {
    "text": "last part is your application you definitely want to capture metrics that are relevant to your particular workload",
    "start": "3829200",
    "end": "3835250"
  },
  {
    "text": "and any of the error rates like if you're running a web-based application 5 X X 4 X X so how do you monitor well",
    "start": "3835250",
    "end": "3843330"
  },
  {
    "text": "the simplest answer today in communities world is Prometheus with Ravana everybody loves it it's super easy to",
    "start": "3843330",
    "end": "3850620"
  },
  {
    "text": "set up there's a there tons of community support and so that's the de facto tool",
    "start": "3850620",
    "end": "3855690"
  },
  {
    "text": "a couple months ago we also launched cloud watch container insights this is",
    "start": "3855690",
    "end": "3861270"
  },
  {
    "text": "and it really easy to use tool that you can install inside your cluster and we",
    "start": "3861270",
    "end": "3866340"
  },
  {
    "text": "have pre-built dashboards that can give you a lot of details on your cluster or you know your pod level and there's",
    "start": "3866340",
    "end": "3874110"
  },
  {
    "text": "certainly kubernetes dashboard you can use that although I'll stop there then",
    "start": "3874110",
    "end": "3880500"
  },
  {
    "text": "there are commercial solutions a bunch of them dynaTrace data dog-sitting are just some of them they can provide you",
    "start": "3880500",
    "end": "3887130"
  },
  {
    "text": "some really cool functionalities like some of them have machine learning that can predict problems before they actually happen let's start looking at",
    "start": "3887130",
    "end": "3894390"
  },
  {
    "text": "let's look at Prometheus so Prometheus again is very easy to install it comes with a helmet art so I would here I'll",
    "start": "3894390",
    "end": "3900690"
  },
  {
    "text": "just created Prometheus namespace and I'll say helm install Prometheus now I I",
    "start": "3900690",
    "end": "3905940"
  },
  {
    "text": "do want to customize some of the values for helm chart and over here I'm going to say that I want to persist the data",
    "start": "3905940",
    "end": "3913140"
  },
  {
    "text": "if you don't do that basically you're getting a femoral pod which if it just gets destroyed all of your metric data",
    "start": "3913140",
    "end": "3919470"
  },
  {
    "text": "will be lost so you don't want to do that you definitely want to persist that data so I've created a storage class",
    "start": "3919470",
    "end": "3925890"
  },
  {
    "text": "that's backed by a EBS that's the elastic block storage and over here I'm",
    "start": "3925890",
    "end": "3931080"
  },
  {
    "text": "using a GP to type volume that I created before and I'm saying from easiest please store your data into this volume",
    "start": "3931080",
    "end": "3939120"
  },
  {
    "text": "and then also I'm saying for alert manager which is a component on prometheus I'm saying you all so please go ahead and persist your data next",
    "start": "3939120",
    "end": "3946470"
  },
  {
    "text": "thing I want to do is once I create the the deploy the hunk chart is going to create a bunch of tools a bunch of pods",
    "start": "3946470",
    "end": "3953130"
  },
  {
    "text": "inside my cluster you'd see that I'm running a three node cluster here and for each cluster for each worker node",
    "start": "3953130",
    "end": "3960300"
  },
  {
    "text": "you'll see a node exporter here and then you'll see Prometheus server a alert",
    "start": "3960300",
    "end": "3965730"
  },
  {
    "text": "manager and state persistent pod so basically that's what you get when you install Prometheus if you want to check",
    "start": "3965730",
    "end": "3973200"
  },
  {
    "text": "for meteors you can you can I just do a cue card board forward which is going to",
    "start": "3973200",
    "end": "3978390"
  },
  {
    "text": "open a port on your localhost and if you go localhost port 8080 you will get Prometheus dashboard it's not really",
    "start": "3978390",
    "end": "3986220"
  },
  {
    "text": "helpful but you know operators are not supposed to forgive you information first hand so if you want to sleep nice",
    "start": "3986220",
    "end": "3993720"
  },
  {
    "text": "pretty graphs that there are pleasing to look at and you know make errors look",
    "start": "3993720",
    "end": "3998760"
  },
  {
    "text": "better then highly recommended that you install a graph on on top of it so again Ravana",
    "start": "3998760",
    "end": "4004310"
  },
  {
    "text": "you can use ham chart to install graph on in your in your cluster you don't",
    "start": "4004310",
    "end": "4009620"
  },
  {
    "text": "have to create a namespace from graph on a lot of people will just put this with coop system or prometheus namespace but",
    "start": "4009620",
    "end": "4016160"
  },
  {
    "text": "if you want to you can again I'm here saying that I want to persist my data on",
    "start": "4016160",
    "end": "4021740"
  },
  {
    "text": "a GP to storage class just like I did with Prometheus and then I'm I'm saying",
    "start": "4021740",
    "end": "4027260"
  },
  {
    "text": "that I also want the service type to be load balancer so in this situation what I don't want like Prometheus I don't",
    "start": "4027260",
    "end": "4034070"
  },
  {
    "text": "want to go and every time say to cuddle proxy or queue cuddle to open a port on",
    "start": "4034070",
    "end": "4039950"
  },
  {
    "text": "my laptop sometimes I may want to just access my cluster state outside my network",
    "start": "4039950",
    "end": "4045940"
  },
  {
    "text": "or not for my laptop where I don't have cube Caudill so over here when we say",
    "start": "4045940",
    "end": "4051190"
  },
  {
    "text": "service type load balancer it's actually going to provision a classic load balancer in the backend that we can just",
    "start": "4051190",
    "end": "4056500"
  },
  {
    "text": "go stable URL we don't have to expose any ports no dependency on cube Caudill and access brief on our dashboard so",
    "start": "4056500",
    "end": "4064329"
  },
  {
    "text": "again once I deploy the helm chart you get a one pod and that does everything",
    "start": "4064329",
    "end": "4069940"
  },
  {
    "text": "and the next thing I want to do is I want to I want to get the URL so I'll",
    "start": "4069940",
    "end": "4080079"
  },
  {
    "text": "you just query the the graph on a service and just get the load balancer",
    "start": "4080079",
    "end": "4085150"
  },
  {
    "text": "address which is going to give you the ELB URL and then you just get the the",
    "start": "4085150",
    "end": "4090160"
  },
  {
    "text": "password and then you get the dashboard now when you go to co-founder dashboard it's not gonna look anything like this",
    "start": "4090160",
    "end": "4096370"
  },
  {
    "text": "it's going to be very basic you can create your own dashboards we recommend",
    "start": "4096370",
    "end": "4101440"
  },
  {
    "text": "it but you probably just want to use the community driven 1 & 4 because de fauna is not a kubernetes specific thing they",
    "start": "4101440",
    "end": "4108068"
  },
  {
    "text": "have Redis the dashboards and they have Cassandra dashboard you can't just go and reuse some of the community created",
    "start": "4108069",
    "end": "4116258"
  },
  {
    "text": "dashboards over here we're looking at community dashboard 31 31 very easy if you go to Griffin ax you'll just say",
    "start": "4116259",
    "end": "4122258"
  },
  {
    "text": "import and you put in a number and you get this dashboard you don't have to do anything another dashboard that's useful",
    "start": "4122259",
    "end": "4128410"
  },
  {
    "text": "so 31 31 is a cluster level it gives you pods and it gives you nodes and cluster",
    "start": "4128410",
    "end": "4133810"
  },
  {
    "text": "level details 3146 it gives you pod level details so it's um and they're a",
    "start": "4133810",
    "end": "4139270"
  },
  {
    "text": "bunch of a bunch of dashboards out there that you can utilize inside your cluster",
    "start": "4139270",
    "end": "4144430"
  },
  {
    "text": "so you don't have to do any you know there's no learning curve with your fauna you can you don't have to note how",
    "start": "4144430",
    "end": "4149949"
  },
  {
    "text": "to create a dashboard just get the number and use it in your cluster some",
    "start": "4149949",
    "end": "4155588"
  },
  {
    "text": "of the important log files that you should also look at are going to be on note logs your communities control plane",
    "start": "4155589",
    "end": "4162040"
  },
  {
    "text": "logs scheduler logs audit logs these are really helpful if you are in in an",
    "start": "4162040",
    "end": "4167318"
  },
  {
    "text": "environment where you need to go back in time and tell what happened or they're really helpful in troubleshooting so",
    "start": "4167319",
    "end": "4172988"
  },
  {
    "text": "highly recommend that you keep these logs you can you don't have to store them forever but you know as long as you have to go back",
    "start": "4172989",
    "end": "4179380"
  },
  {
    "text": "for troubleshooting those are good enough now I want to talk a little bit about the container cloud watch",
    "start": "4179380",
    "end": "4186520"
  },
  {
    "text": "container insight so because cloud watch team has built this really nice tool that they think that will help a lot of",
    "start": "4186520",
    "end": "4192160"
  },
  {
    "text": "customers traditionally installing dashboards has been a little bit of pain point and then",
    "start": "4192160",
    "end": "4197500"
  },
  {
    "text": "learning how to use that dashboard effectively in your organization is another feat so we wanted to solve those",
    "start": "4197500",
    "end": "4203200"
  },
  {
    "text": "two challenges make it easier for you to collect summarized metrics and then consume metrics at the same time so if",
    "start": "4203200",
    "end": "4209530"
  },
  {
    "text": "you use cloud watches really easy to use you get some dashboards that are already built and all you have to do is just go",
    "start": "4209530",
    "end": "4216370"
  },
  {
    "text": "there and monitor and then you can put an alerting and trigger send slack message all those things so a we we use",
    "start": "4216370",
    "end": "4225340"
  },
  {
    "text": "cloud watch but on the cluster itself it's leveraging fluent D which means",
    "start": "4225340",
    "end": "4230410"
  },
  {
    "text": "they have all the logs inside your cluster assuming that it all centers standard out are collected by affluent D",
    "start": "4230410",
    "end": "4237430"
  },
  {
    "text": "and then fluently pipes or sends those logs to cloud watch cloud watch is known",
    "start": "4237430",
    "end": "4243340"
  },
  {
    "text": "not the only destination that fluently supports so in future if you'd like to say I don't like cloud watch I want to",
    "start": "4243340",
    "end": "4249130"
  },
  {
    "text": "go to elastic search all you have to do is in that in fluently configuration you're going to say now start sending",
    "start": "4249130",
    "end": "4254980"
  },
  {
    "text": "cloud it locks to cloud to elastic search and that's it you don't have to rip out your entire infrastructure there",
    "start": "4254980",
    "end": "4261000"
  },
  {
    "text": "so in order to deploy cloud watch container insight they all you have to",
    "start": "4261000",
    "end": "4266170"
  },
  {
    "text": "do is you import a yamo that we provide and it's going to create two daemon sets",
    "start": "4266170",
    "end": "4271300"
  },
  {
    "text": "one is going to be the fluency demon set the second one is going to be the cloud watch agent daemon set and that's it",
    "start": "4271300",
    "end": "4277330"
  },
  {
    "text": "once you do that next thing you open your cloud watch dashboard and you can",
    "start": "4277330",
    "end": "4283060"
  },
  {
    "text": "you can see this and that's just the one type of dashboard that we have we have",
    "start": "4283060",
    "end": "4288370"
  },
  {
    "text": "created custom dashboards and and we spoke with customers like you and I said what would you like to see as a first",
    "start": "4288370",
    "end": "4295480"
  },
  {
    "text": "thing and a lot of customers said we need cluster level details but I also want to be able to see my thing my pause",
    "start": "4295480",
    "end": "4301690"
  },
  {
    "text": "by namespaces I want to see my services and I want to be able to dig deeper into that cluster so for for that we created",
    "start": "4301690",
    "end": "4308500"
  },
  {
    "text": "a bunch of dashboards that will Attis fire most of the needs out there and you helps have the ability to create",
    "start": "4308500",
    "end": "4314829"
  },
  {
    "text": "your custom dashboards if you want it another thing it does is logs so along",
    "start": "4314829",
    "end": "4321280"
  },
  {
    "text": "with metrics that's cloud watch is collecting there are also logs that cloud watch can take so again we're",
    "start": "4321280",
    "end": "4326380"
  },
  {
    "text": "using fluent D send those logs those is stored durably in cloud wash so again you're not storing them in another",
    "start": "4326380",
    "end": "4333429"
  },
  {
    "text": "location or the stored in cloud watch and then you can use cloud watch logs insights queries and then query those",
    "start": "4333429",
    "end": "4340329"
  },
  {
    "text": "logs without having to provision any c2 instance or any hardware so for example",
    "start": "4340329",
    "end": "4345400"
  },
  {
    "text": "if I wanted to see what is the memory usage across my cluster but I don't want it relogin to cube cuddle I can just run",
    "start": "4345400",
    "end": "4351849"
  },
  {
    "text": "this query on my logs and it will tell me what is the current state of my cluster so there you go without",
    "start": "4351849",
    "end": "4357670"
  },
  {
    "text": "provisioning an ec2 instance without provisioning any hardware you can query logs very very easily so if you're if",
    "start": "4357670",
    "end": "4364989"
  },
  {
    "text": "you if you're running an application if you're creating a micro services environment monitoring and logging are",
    "start": "4364989",
    "end": "4372159"
  },
  {
    "text": "really really important a good and cloud native application is not only just that what that runs well but also is it's",
    "start": "4372159",
    "end": "4379570"
  },
  {
    "text": "also reporting what is happening you don't want to be running a black box you want to see what is running what are the",
    "start": "4379570",
    "end": "4385989"
  },
  {
    "text": "metrics how is the application running is it failing or not so for that there are certain things that you should do",
    "start": "4385989",
    "end": "4391920"
  },
  {
    "text": "first of all probes make sure you use probes as default make sure you have liveness and various",
    "start": "4391920",
    "end": "4398860"
  },
  {
    "text": "probe as a standard across the organization because they're really helpful you don't want to be running",
    "start": "4398860",
    "end": "4404139"
  },
  {
    "text": "into situations like your pod comes up and it's it's not functional for five",
    "start": "4404139",
    "end": "4409420"
  },
  {
    "text": "minutes and then your cluster is getting auto scale to millions of pods because you did not have a redness readiness",
    "start": "4409420",
    "end": "4415389"
  },
  {
    "text": "probe so make sure you have them implement tracing use service mesh or",
    "start": "4415389",
    "end": "4421150"
  },
  {
    "text": "use x-ray open tracing whatever is a tool you love and then also monitor process health and logs so don't just",
    "start": "4421150",
    "end": "4428170"
  },
  {
    "text": "don't just assume that because you have flu in DM clogged watch that all the logs are magically going to appear in",
    "start": "4428170",
    "end": "4434380"
  },
  {
    "text": "cloud watch make sure that all the components that you are running for example if you're running a Python app",
    "start": "4434380",
    "end": "4440739"
  },
  {
    "text": "that uses nginx make sure you capturing both Python law Flass logs and nginx logs so make sure",
    "start": "4440739",
    "end": "4447200"
  },
  {
    "text": "your all the logs are being collected not just your app but also supporting apps that you may have we can I talked",
    "start": "4447200",
    "end": "4464360"
  },
  {
    "text": "about this a little bit earlier already you know in terms of how do you do resource reservation so you can monitor",
    "start": "4464360",
    "end": "4471080"
  },
  {
    "text": "the cubelet on the worker and also let's say you have done the resource reservation you can dig into your worker",
    "start": "4471080",
    "end": "4477260"
  },
  {
    "text": "node so if you create your kubernetes cluster and let's say you have ssh key enabled over there then get into your",
    "start": "4477260",
    "end": "4483080"
  },
  {
    "text": "kubernetes cluster and say give me the details for a particular worker node and this is the way the way you are getting",
    "start": "4483080",
    "end": "4488270"
  },
  {
    "text": "it and we talked about this earlier already how we can use cube reserved and system reserved memory to how this value",
    "start": "4488270",
    "end": "4496370"
  },
  {
    "text": "can be set up and not only you can do this like if you're running a eks cluster of course you can use eks cut",
    "start": "4496370",
    "end": "4501740"
  },
  {
    "text": "all over there but let's say you are creating you know kubernetes cluster you know using some other tool so cops has",
    "start": "4501740",
    "end": "4507710"
  },
  {
    "text": "these capabilities as well or if you are creating using army then in the army",
    "start": "4507710",
    "end": "4513500"
  },
  {
    "text": "there is a concept of a user data and in the user data for example you can specify these commands as part of your",
    "start": "4513500",
    "end": "4519920"
  },
  {
    "text": "commands to the API server and then you can say that make sure my resources are appropriately allocated so if you look",
    "start": "4519920",
    "end": "4527060"
  },
  {
    "text": "at it over here for example we are highlighting that how you can set up that value in the eks coddle and this is",
    "start": "4527060",
    "end": "4533570"
  },
  {
    "text": "only specific to Amazon eks essentially so that's one of the areas where we are talking about so how you can have system",
    "start": "4533570",
    "end": "4539510"
  },
  {
    "text": "reserved cube reserved and eviction reserved because those are the three values that are taken into consideration",
    "start": "4539510",
    "end": "4544990"
  },
  {
    "text": "when it's looking at capacity and actually reporting the right value of allocatable so that yes you know it will",
    "start": "4544990",
    "end": "4552350"
  },
  {
    "text": "not be able to schedule your parts but at one you wanted to report the right value as well for CPU okay if you're",
    "start": "4552350",
    "end": "4561530"
  },
  {
    "text": "doing we have seen a lot of customers who are doing oversubscription where they say you know what you are sure no",
    "start": "4561530",
    "end": "4567730"
  },
  {
    "text": "CPU is not a compressible resource but memory is a compressible resource so I'm going to over subscribe it so one of the",
    "start": "4567730",
    "end": "4574610"
  },
  {
    "text": "things that we recommend to our customers is to avoid over subscription to specify sort of your CPU and the",
    "start": "4574610",
    "end": "4579770"
  },
  {
    "text": "memory limits over there and in this case for example you can see that the memory",
    "start": "4579770",
    "end": "4585650"
  },
  {
    "text": "requested is 64 megabyte for the memory and 128 megabyte is the limit so that",
    "start": "4585650",
    "end": "4591650"
  },
  {
    "text": "way you can totally calculating on how much max that the each worker node",
    "start": "4591650",
    "end": "4597140"
  },
  {
    "text": "should be able to handle another option over there another opinion over there is to be able to specify your resource",
    "start": "4597140",
    "end": "4604070"
  },
  {
    "text": "quotas or namespaces so in this case I'm just saying that for this namespace and",
    "start": "4604070",
    "end": "4609230"
  },
  {
    "text": "or depending upon your dev and the testing and the prod and the staging namespaces if that's how you are achieving multi-tenancy in kubernetes",
    "start": "4609230",
    "end": "4615370"
  },
  {
    "text": "setup your limits over there and it's not just for CPU as a matter of fact you",
    "start": "4615370",
    "end": "4622100"
  },
  {
    "text": "can set it for CPU the total amount of computer resources the total amount of resources how many services how many",
    "start": "4622100",
    "end": "4628790"
  },
  {
    "text": "parts can be created in this namespace so once you start specifying that then those and there are inheritance rules",
    "start": "4628790",
    "end": "4634280"
  },
  {
    "text": "available that if your pod spec does not specify any resource and limits that it inherits that from the namespace itself",
    "start": "4634280",
    "end": "4642700"
  },
  {
    "text": "now once the namespace on the coton the namespace is full of course I know it's treated as the same way that hey you",
    "start": "4643480",
    "end": "4649880"
  },
  {
    "text": "know you no longer have any more resources so then your cluster is not the scope now your namespace is the",
    "start": "4649880",
    "end": "4655490"
  },
  {
    "text": "scope so then you want to look at it that ok the error message is fortunately are much better here it says exceeded",
    "start": "4655490",
    "end": "4660560"
  },
  {
    "text": "quota and it will tell you exactly what quota is exceeded and then you start debugging it then you go back to your",
    "start": "4660560",
    "end": "4666290"
  },
  {
    "text": "cluster operator that increase my namespace limit will give me a different namespace a different cluster so that",
    "start": "4666290",
    "end": "4671570"
  },
  {
    "text": "you can start debugging what's the problem over there now by default containers run with unbounded compute",
    "start": "4671570",
    "end": "4678620"
  },
  {
    "text": "resources on a kubernetes cluster or in our namespace resource quota so limit",
    "start": "4678620",
    "end": "4683870"
  },
  {
    "text": "range is a policy to constrain resource by pod or container in our namespace so",
    "start": "4683870",
    "end": "4689750"
  },
  {
    "text": "in this case for example I'm creating a limit range and I'm saying that by default if the pod has no spec like no",
    "start": "4689750",
    "end": "4697490"
  },
  {
    "text": "resource no memory or CPU specified over there this is the memory that they're going to inherit so that way you can",
    "start": "4697490",
    "end": "4704840"
  },
  {
    "text": "start sorry",
    "start": "4704840",
    "end": "4707679"
  },
  {
    "text": "they're gonna inherit basically so that kind of gives you an idea that you know how you can start sizing your cluster",
    "start": "4711320",
    "end": "4716869"
  },
  {
    "text": "because cluster sizing is really important you know you don't want it to grow arbitrarily large so now what",
    "start": "4716869",
    "end": "4726349"
  },
  {
    "text": "you're doing is you're just saying keep Carol described limit ranges and then you can start getting those values and that kind of gives you an idea that you",
    "start": "4726349",
    "end": "4732800"
  },
  {
    "text": "know why your cluster may not really be getting created so from your perspective you're just saying cube Carol create",
    "start": "4732800",
    "end": "4739280"
  },
  {
    "text": "deployment and then you have to recognize that hey is there a namespace limit is there a limit range defined for",
    "start": "4739280",
    "end": "4745130"
  },
  {
    "text": "me so then you have to start looking at those objects and their default values kind of the reason that the part may not",
    "start": "4745130",
    "end": "4751219"
  },
  {
    "text": "really getting deployed I'm not gonna dig in deeper into this but essentially",
    "start": "4751219",
    "end": "4756860"
  },
  {
    "text": "the idea is if there are rules if limits and requests are defined on",
    "start": "4756860",
    "end": "4762260"
  },
  {
    "text": "the pod and on the limit ranges and how they really work but the whole idea is",
    "start": "4762260",
    "end": "4767510"
  },
  {
    "text": "that if nothing is defined on the pod then they inherit from the limit ranges that's sort of the key rule over here",
    "start": "4767510",
    "end": "4772520"
  },
  {
    "text": "but once again the key part being if your part is not getting deployed if your part is not coming up there are",
    "start": "4772520",
    "end": "4779210"
  },
  {
    "text": "things possibly going at multiple levels you know it could be limit rain could be namespace coda could be cluster",
    "start": "4779210",
    "end": "4785810"
  },
  {
    "text": "autoscaler we haven't even talked about as a matter of fact about horizontal part autoscaler and that is one thing",
    "start": "4785810",
    "end": "4791329"
  },
  {
    "text": "that can help you really with with the service latency which break kind of touched upon it briefly all right story",
    "start": "4791329",
    "end": "4800420"
  },
  {
    "text": "time so I very proud of creating my hello world application but I also create another application where I can",
    "start": "4800420",
    "end": "4806599"
  },
  {
    "text": "still store all my important data and pictures so I took the my my kids birds",
    "start": "4806599",
    "end": "4813349"
  },
  {
    "text": "pictures and baby pictures and stored all in a kubernetes pod right it's awesome and then the pod obviously got",
    "start": "4813349",
    "end": "4821150"
  },
  {
    "text": "deleted so guess what happens to all my data anyone any guesses the data was",
    "start": "4821150",
    "end": "4828440"
  },
  {
    "text": "gone yeah I was kidding it was not my baby pictures when wedding pictures much",
    "start": "4828440",
    "end": "4833960"
  },
  {
    "text": "worse trust me so pods are ephemeral by nature do not",
    "start": "4833960",
    "end": "4839510"
  },
  {
    "text": "store your wedding pictures do not store your tax documents in pods unless you are using stateful sets then you can",
    "start": "4839510",
    "end": "4846590"
  },
  {
    "text": "do it so stateful sets allow you to persist data using storage back-end that",
    "start": "4846590",
    "end": "4852290"
  },
  {
    "text": "again is abstracted away from the developers and users you don't need to be you don't need to care as a developer",
    "start": "4852290",
    "end": "4859220"
  },
  {
    "text": "what storage back-end should I use those are all done by hopefully an",
    "start": "4859220",
    "end": "4864410"
  },
  {
    "text": "administrator if you're the administrator then it's you you should do it your job so stateful sets allow you to persist",
    "start": "4864410",
    "end": "4872570"
  },
  {
    "text": "that data on a on a on a back end and it will container if a pod fails then the",
    "start": "4872570",
    "end": "4879140"
  },
  {
    "text": "data is persisted and the new stateful set container would come up pod will come up and start using that data so you",
    "start": "4879140",
    "end": "4885140"
  },
  {
    "text": "don't lose any data so stateful sets are really providing pods with that",
    "start": "4885140",
    "end": "4890150"
  },
  {
    "text": "persistent storage to persist data they also create percent volume claim",
    "start": "4890150",
    "end": "4896650"
  },
  {
    "text": "automatically on the fly and the reason why stateful sets are so different from",
    "start": "4896650",
    "end": "4902200"
  },
  {
    "text": "deployments and pods are because there they are each pod is going to get its",
    "start": "4902200",
    "end": "4908660"
  },
  {
    "text": "dedicated PVC which means that it's not sharing that volume with any other pod",
    "start": "4908660",
    "end": "4913850"
  },
  {
    "text": "it will have its own dedicated PVC it also creates a headless service and what",
    "start": "4913850",
    "end": "4920300"
  },
  {
    "text": "headless service really does is it gives you each each pod that's part of that",
    "start": "4920300",
    "end": "4926810"
  },
  {
    "text": "service a unique IP address so if you're running let's say Redis master and Redis",
    "start": "4926810",
    "end": "4932630"
  },
  {
    "text": "slave you can really easily identify the ordinality so like one would be the",
    "start": "4932630",
    "end": "4938120"
  },
  {
    "text": "master second would be the slave and so on and that ordering is maintained by by",
    "start": "4938120",
    "end": "4943460"
  },
  {
    "text": "kubernetes so if your pod or your your node fails if multiple nodes fail when",
    "start": "4943460",
    "end": "4948560"
  },
  {
    "text": "the new pods come up they are going to come up in that same order so it's kind of a little bit more resiliency that you",
    "start": "4948560",
    "end": "4954560"
  },
  {
    "text": "can put when you're working with stateful sets which is really necessary if you're dealing with databases for",
    "start": "4954560",
    "end": "4960200"
  },
  {
    "text": "example you may have a my sequel master database and a bunch of slaves this is going to make sure that you're always",
    "start": "4960200",
    "end": "4967120"
  },
  {
    "text": "replicating from master to slave and not otherwise so an eks there are a bunch of",
    "start": "4967120",
    "end": "4973850"
  },
  {
    "text": "options some of them I wonder highlight here are the EBS and EFS CSR drivers now CSI is a",
    "start": "4973850",
    "end": "4981570"
  },
  {
    "text": "container storage interface is much like networking where you don't where you can pick your own network provider you can",
    "start": "4981570",
    "end": "4989190"
  },
  {
    "text": "use a dub Amazon VP CC and I or you can use flannel similarly for storage you don't have to",
    "start": "4989190",
    "end": "4995550"
  },
  {
    "text": "use the same kind of storage provider you could use EBS you could use EFS there are a bunch of third-party",
    "start": "4995550",
    "end": "5001000"
  },
  {
    "text": "providers out there that you can also use again they're big what they conform to is the CSI spec and all of them",
    "start": "5001000",
    "end": "5008900"
  },
  {
    "text": "provide the basic functionality which is providing a volume and then you have a bunch of additional functionality like",
    "start": "5008900",
    "end": "5015080"
  },
  {
    "text": "snapshotting etc so if you are running databases if you're running pods that",
    "start": "5015080",
    "end": "5021440"
  },
  {
    "text": "need to process data make sure you're running stateful sets don't delete your wedding pictures make sure you're",
    "start": "5021440",
    "end": "5027890"
  },
  {
    "text": "reusing one of these CSI providers now",
    "start": "5027890",
    "end": "5033170"
  },
  {
    "text": "period a few generations ago in kubernetes there was an entry a storage",
    "start": "5033170",
    "end": "5039680"
  },
  {
    "text": "provider highly recommend that you don't use that because the community is moving away from it you want to use something",
    "start": "5039680",
    "end": "5045320"
  },
  {
    "text": "that provides that's using CSI so if you look at ETS EBS is the most common most",
    "start": "5045320",
    "end": "5053660"
  },
  {
    "text": "common storage type for customers that are running stateful sets in kubernetes so you can use that with eks or you can",
    "start": "5053660",
    "end": "5060530"
  },
  {
    "text": "just reduce it with cops it doesn't really matter but they will they will provide that persistent storage for your",
    "start": "5060530",
    "end": "5067460"
  },
  {
    "text": "pods in EBS these are again the functionality functions of the specific",
    "start": "5067460",
    "end": "5073340"
  },
  {
    "text": "CSI types of EBS CSI type support both dynamic and",
    "start": "5073340",
    "end": "5078490"
  },
  {
    "text": "Static assignment of volumes which means that either you can pre create a volume in EBS and say use this volume to store",
    "start": "5078490",
    "end": "5085640"
  },
  {
    "text": "all data or you can say as a pod comes up just dynamically create because I don't want to be managing volumes on",
    "start": "5085640",
    "end": "5092060"
  },
  {
    "text": "myself and you can also say that if the pod goes down if the data is replicated delete the data as well so you're not",
    "start": "5092060",
    "end": "5098240"
  },
  {
    "text": "paying anything any additional for for that pod that's not alive these volumes",
    "start": "5098240",
    "end": "5104300"
  },
  {
    "text": "can be resized so you don't need to guess the capacity ahead of time you can just increase from 10 to 20 GB if you wanted to",
    "start": "5104300",
    "end": "5110110"
  },
  {
    "text": "and they also support snapshots so you can take a snapshot if you're not running a pod continuously take a",
    "start": "5110110",
    "end": "5116290"
  },
  {
    "text": "snapshot and abs kill the pod there are two months later when you need it again you can start a new volume and hopefully",
    "start": "5116290",
    "end": "5123970"
  },
  {
    "text": "save a little bit on cost one important thing really really important thing with",
    "start": "5123970",
    "end": "5129910"
  },
  {
    "text": "EBS is currently EBS is only available in EBS volumes are only available in one",
    "start": "5129910",
    "end": "5136620"
  },
  {
    "text": "availability zone which means in a region like us west to where we have I",
    "start": "5136620",
    "end": "5142300"
  },
  {
    "text": "think three availability zone zones if you create an eps volume in availability",
    "start": "5142300",
    "end": "5148420"
  },
  {
    "text": "zone one it's not available in two and three so if you're creating stateful",
    "start": "5148420",
    "end": "5154600"
  },
  {
    "text": "sets or if you're giving your pods a volume then make sure that you have some",
    "start": "5154600",
    "end": "5160330"
  },
  {
    "text": "kind of logic that says this pod can only come up in this availability zone and that's typically done using node",
    "start": "5160330",
    "end": "5166780"
  },
  {
    "text": "labels so make sure you don't run into situation where your pod is now was running in us us West one and was using",
    "start": "5166780",
    "end": "5174580"
  },
  {
    "text": "an EBS volume in that availability zone it got deleted and now it's running in",
    "start": "5174580",
    "end": "5179980"
  },
  {
    "text": "West two waiting for that volume which is never gonna arrive so don't don't get run into that problem make sure you're",
    "start": "5179980",
    "end": "5186640"
  },
  {
    "text": "using your binding your stateful sets you make sure your binding your pods that need storage to a particular",
    "start": "5186640",
    "end": "5192280"
  },
  {
    "text": "availability zone now if you wanted to if you wanted your your pods across",
    "start": "5192280",
    "end": "5198600"
  },
  {
    "text": "availability zones to share the same volume use EFS EFS is available across",
    "start": "5198600",
    "end": "5204190"
  },
  {
    "text": "the region so it doesn't matter which AC you sit in all the volumes are replicated across so that's a really",
    "start": "5204190",
    "end": "5211330"
  },
  {
    "text": "good use case if you if you need that kind of distributed storage creating an",
    "start": "5211330",
    "end": "5216400"
  },
  {
    "text": "EBS storage class is really simple you will say provision or this is going to be AWS EBS and over here you can provide",
    "start": "5216400",
    "end": "5225160"
  },
  {
    "text": "some parameters so here I'm saying give me a general purpose volume and this is",
    "start": "5225160",
    "end": "5230320"
  },
  {
    "text": "going to provision EBS volume based on this configuration here and I don't have to go to EBS and say give me a volume I",
    "start": "5230320",
    "end": "5237400"
  },
  {
    "text": "can just create everything in my kubernetes cluster and then when the pod needs an idea",
    "start": "5237400",
    "end": "5243240"
  },
  {
    "text": "volume then storage class is automatically going to provision one assuming that it's doing it dynamically",
    "start": "5243240",
    "end": "5249630"
  },
  {
    "text": "and attach that to the pot so it becomes very seamless you only need to create a storage class",
    "start": "5249630",
    "end": "5254880"
  },
  {
    "text": "once and then the developers can just request that source class and request",
    "start": "5254880",
    "end": "5260960"
  },
  {
    "text": "volumes for their pots so one of the",
    "start": "5260960",
    "end": "5272190"
  },
  {
    "text": "things that I actually let me step back I was trying to look for failure scenarios for storage didn't really find",
    "start": "5272190",
    "end": "5279000"
  },
  {
    "text": "much which tells me either my google searching is really bad or these csr drivers work very well I'm proud of my",
    "start": "5279000",
    "end": "5286680"
  },
  {
    "text": "google searching skills so that's definitely not it it looks like it's a lot more stable environment now but",
    "start": "5286680",
    "end": "5292380"
  },
  {
    "text": "there are certain things that you you may run into one of them is this which is you mount a volume and you now want",
    "start": "5292380",
    "end": "5300120"
  },
  {
    "text": "to write write on it and it fails you know often customers we're gonna ask me",
    "start": "5300120",
    "end": "5305970"
  },
  {
    "text": "and say what if I do this this is a crazy thing in kubernetes is it going to",
    "start": "5305970",
    "end": "5311040"
  },
  {
    "text": "work and my answer is in Spanish see but in Spanish see also means if so my you",
    "start": "5311040",
    "end": "5316980"
  },
  {
    "text": "know they misunderstand me they think I'm saying yes which is C but what I'm really meaning is if if you read the",
    "start": "5316980",
    "end": "5323430"
  },
  {
    "text": "documentation right and if you do everything right and jump to 200 hoops it's going to work so this is one of",
    "start": "5323430",
    "end": "5329430"
  },
  {
    "text": "those scenarios where you may have mount a volume and you can't write to it the reason why it happens is because if",
    "start": "5329430",
    "end": "5336330"
  },
  {
    "text": "you're really interested Google search pause containers and they'll tell you a lot give you a lot of reading to do but",
    "start": "5336330",
    "end": "5342660"
  },
  {
    "text": "all the volumes are mounted as by a root user essentially so if you're not the",
    "start": "5342660",
    "end": "5348510"
  },
  {
    "text": "root you can't write and don't run your containers with root that's not what I'm trying to say but what you can do is you",
    "start": "5348510",
    "end": "5354630"
  },
  {
    "text": "can either use the security context in the pod which is going to use Linux security context and is going to allow",
    "start": "5354630",
    "end": "5362190"
  },
  {
    "text": "your containers or docker container in this case to assume that security",
    "start": "5362190",
    "end": "5367800"
  },
  {
    "text": "context and start writing or another thing you can do is in it can so a lot of people will say in it",
    "start": "5367800",
    "end": "5374300"
  },
  {
    "text": "container which is which runs which you can run as root mount the volume and then change the chain the permissions",
    "start": "5374300",
    "end": "5382670"
  },
  {
    "text": "which is Rancho noir chmod or something like that to now give your other containers which really need to write to",
    "start": "5382670",
    "end": "5389660"
  },
  {
    "text": "their volumes the ability to write to the CVS volume so that's those are two",
    "start": "5389660",
    "end": "5395240"
  },
  {
    "text": "things you can do if you want to put security context basically this is what",
    "start": "5395240",
    "end": "5400760"
  },
  {
    "text": "you need to do there's a little bit further reading required here if you're",
    "start": "5400760",
    "end": "5405770"
  },
  {
    "text": "not familiar with these two things but if you're dealing with storage and then it's highly recommended that you that",
    "start": "5405770",
    "end": "5412160"
  },
  {
    "text": "you implement this if not then in it containers could be a route now in containers not they're not",
    "start": "5412160",
    "end": "5418010"
  },
  {
    "text": "just for storage but anytime you need something done before your application comes up use your use in it containers",
    "start": "5418010",
    "end": "5424460"
  },
  {
    "text": "you can use ordering there and they're really helpful too and that's it for my site so this presentation is all",
    "start": "5424460",
    "end": "5438560"
  },
  {
    "text": "available online now this is the github repo on the top where all the content source is available and on the bottom is",
    "start": "5438560",
    "end": "5445280"
  },
  {
    "text": "where you know I mean we have been running a local version of it but if you go to the bottom link you can actually run this wherever you want to so feel",
    "start": "5445280",
    "end": "5452060"
  },
  {
    "text": "free to share contribute we would love to hear your stories on what happened to",
    "start": "5452060",
    "end": "5457220"
  },
  {
    "text": "your part why did your part fail and I think I'm gonna write a thesis on that sometime now you could actually very easily write",
    "start": "5457220",
    "end": "5462650"
  },
  {
    "text": "a story on this end the pr yes in a PR you can edit this presentation yourself yeah this is all on a github repo some",
    "start": "5462650",
    "end": "5469100"
  },
  {
    "text": "of the links that we want to share here is the last link is what I want to really highlight it's a simple link gates dot a fk8",
    "start": "5469100",
    "end": "5476780"
  },
  {
    "text": "s dot AF this is a link of kubernetes failure stories you know the multi-million ways your kubernetes",
    "start": "5476780",
    "end": "5483260"
  },
  {
    "text": "cluster can crash and burn so we all learn from our failures for more we have",
    "start": "5483260",
    "end": "5488450"
  },
  {
    "text": "some books that we want to share some of the books that we like and that are well used in the industry and that's a wrap",
    "start": "5488450",
    "end": "5495950"
  },
  {
    "text": "thank you so much [Applause]",
    "start": "5495950",
    "end": "5501749"
  }
]