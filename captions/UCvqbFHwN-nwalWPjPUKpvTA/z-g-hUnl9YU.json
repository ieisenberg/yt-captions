[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "text": "so let's start welcome to the deep dive of Prometheus some of you were here yesterday for the",
    "start": "3760",
    "end": "10220"
  },
  {
    "text": "intro now we're going to go a little bit deeper same people as last time and yeah",
    "start": "10220",
    "end": "23480"
  },
  {
    "text": "first half a spell I'm doing the first half alright so back to the same",
    "start": "23480",
    "end": "30380"
  },
  {
    "text": "questions who's heard of Prometheus yay",
    "start": "30380",
    "end": "36200"
  },
  {
    "text": "who's who's considering using Prometheus alright so who's already using it as",
    "start": "36200",
    "end": "43190"
  },
  {
    "start": "40000",
    "end": "106000"
  },
  {
    "text": "like a proof of concept or in testing few people good good and then who's who's shifted all the way",
    "start": "43190",
    "end": "49160"
  },
  {
    "text": "to production awesome so we're gonna have a lot of questions aren't we so back to previous 101 it was",
    "start": "49160",
    "end": "58760"
  },
  {
    "text": "inspired by a Google's internal monitoring system it's a numeric metric time series based",
    "start": "58760",
    "end": "68870"
  },
  {
    "text": "metric system it basically stores a series of data non numeric data points",
    "start": "68870",
    "end": "74360"
  },
  {
    "text": "over time it has a huge huge ecosystem of exporters and integrations hundreds",
    "start": "74360",
    "end": "81380"
  },
  {
    "text": "and hundreds of applications implement Prometheus or have or have converters what's it more than feel yeah more than",
    "start": "81380",
    "end": "88640"
  },
  {
    "text": "3-hundred it's not it's a it's a single tool for a single job it's not a fix",
    "start": "88640",
    "end": "96409"
  },
  {
    "text": "everything it doesn't do tracing it doesn't do logging it's not an event store it's",
    "start": "96409",
    "end": "102049"
  },
  {
    "text": "just for the first level of marring and our favorite our favorite plug-in tool",
    "start": "102049",
    "end": "109790"
  },
  {
    "start": "106000",
    "end": "152000"
  },
  {
    "text": "for Prometheus is go fauna it's usually the second thing I install after I install a Prometheus server the main",
    "start": "109790",
    "end": "116510"
  },
  {
    "text": "selling points over other metric space monitoring systems is it's super highly dynamic it's super efficient we have a",
    "start": "116510",
    "end": "123170"
  },
  {
    "text": "very powerful query language and it's super simple to operate a lot of people",
    "start": "123170",
    "end": "129229"
  },
  {
    "text": "come to us and say well I don't want to host my own monitoring it's a lot of work you know I've used Nagios and it's",
    "start": "129229",
    "end": "135690"
  },
  {
    "text": "always been and many many engineers worth of time running Nagios or or other",
    "start": "135690",
    "end": "141390"
  },
  {
    "text": "older metric systems like graphite prometheus is super easy to operate it's",
    "start": "141390",
    "end": "146940"
  },
  {
    "text": "not as big a deal as classic monitoring systems so yes or you want to say",
    "start": "146940",
    "end": "157860"
  },
  {
    "start": "152000",
    "end": "195000"
  },
  {
    "text": "something about this I can do it so now we are here cloud native con and",
    "start": "157860",
    "end": "162930"
  },
  {
    "text": "everything's everything's in container everything's a micro service everything has sidecars and all of a sudden you",
    "start": "162930",
    "end": "168450"
  },
  {
    "text": "have this one thing which is kind of a big thing in in in CNC F land and it's",
    "start": "168450",
    "end": "173700"
  },
  {
    "text": "one monolithic application and this is Prometheus and why are we doing this of course normally you would just",
    "start": "173700",
    "end": "179000"
  },
  {
    "text": "reasonably expect us to have like tons of small micro services and stuff which then provide what what we call",
    "start": "179000",
    "end": "186000"
  },
  {
    "text": "permittees but we're doing it exactly the other way around for a very very very simple reason and that is basically",
    "start": "186000",
    "end": "191700"
  },
  {
    "text": "resilience yeah yeah of course what you",
    "start": "191700",
    "end": "198840"
  },
  {
    "start": "195000",
    "end": "235000"
  },
  {
    "text": "really need for operations is if you if you look at what you actually provide on the physical level and and a little bit",
    "start": "198840",
    "end": "205739"
  },
  {
    "text": "above you need power and cooling for a data center you need network connectivity to connect to your services",
    "start": "205739",
    "end": "211230"
  },
  {
    "text": "and to debug and stuff and need observability AKA monitoring and that's about it the rest you can't fix so",
    "start": "211230",
    "end": "218269"
  },
  {
    "text": "because this is in the absolute critical path we decided to have one single",
    "start": "218269",
    "end": "223470"
  },
  {
    "text": "monolithic application which is really easy to run really easy to understand really easy to deploy and disregard all",
    "start": "223470",
    "end": "231810"
  },
  {
    "text": "the all the cool bits with bits and pieces on purpose yep yeah so one of the things that we",
    "start": "231810",
    "end": "238110"
  },
  {
    "start": "235000",
    "end": "301000"
  },
  {
    "text": "discovered as so I I used to be a site reliability engineer and one of the",
    "start": "238110",
    "end": "243480"
  },
  {
    "text": "things that we discovered that with complicated monitoring systems that depend on things like Cassandra or some",
    "start": "243480",
    "end": "249630"
  },
  {
    "text": "other cluster database or any kind of monitoring system that depended on other components those were the first things",
    "start": "249630",
    "end": "256019"
  },
  {
    "text": "to fail whenever there was a small glitch in the network and so we designed Prometheus to be a single standalone point that watches",
    "start": "256019",
    "end": "262800"
  },
  {
    "text": "everything and actually and you can run multiple of them and that way if there's",
    "start": "262800",
    "end": "268800"
  },
  {
    "text": "one prometheus server on one side of your network and you have another Prometheus server on another side of your network and there's a split brain",
    "start": "268800",
    "end": "274290"
  },
  {
    "text": "or a partition there's a recent outage that somebody github had where they had a split brain",
    "start": "274290",
    "end": "280170"
  },
  {
    "text": "in their network and they're they're clustering system just completely",
    "start": "280170",
    "end": "285540"
  },
  {
    "text": "itself and we we don't have that problem with Prometheus because it doesn't have",
    "start": "285540",
    "end": "292710"
  },
  {
    "text": "any external dependencies besides knowing where your targets are and that's an extremely extremely robust",
    "start": "292710",
    "end": "299790"
  },
  {
    "text": "thing to do so where are we at now with",
    "start": "299790",
    "end": "305850"
  },
  {
    "start": "301000",
    "end": "376000"
  },
  {
    "text": "Prometheus so we have the storage back-end about a year ago we developed a",
    "start": "305850",
    "end": "312300"
  },
  {
    "text": "new storage time series database to significantly reduce the overhead of",
    "start": "312300",
    "end": "317820"
  },
  {
    "text": "running Prometheus from a computer resource perspective and also it",
    "start": "317820",
    "end": "323330"
  },
  {
    "text": "significantly improved the performance of Prometheus when you're running kubernetes clusters where you're",
    "start": "323330",
    "end": "329310"
  },
  {
    "text": "launching and creating and destroying pods continuously so if you've got a CI pipeline the old prometheus had lots of",
    "start": "329310",
    "end": "336240"
  },
  {
    "text": "problems with that we also improved staleness handling one of the old problems with prometheus was as pods",
    "start": "336240",
    "end": "343350"
  },
  {
    "text": "came and went you'd have that you'd still see the pod for five minutes until",
    "start": "343350",
    "end": "348480"
  },
  {
    "text": "it would finally disappear we fixed that it's now much better and yeah we'll have",
    "start": "348480",
    "end": "356310"
  },
  {
    "text": "12 links to the specifics of that at the end of the talk and also we've finally",
    "start": "356310",
    "end": "361440"
  },
  {
    "text": "stabilized a remote read and write access to the database so that you can",
    "start": "361440",
    "end": "367590"
  },
  {
    "text": "take a Prometheus server and get access to the data or ship the data outside for",
    "start": "367590",
    "end": "373110"
  },
  {
    "text": "more in-depth analysis or long-term storage so Prometheus 1.0 had a bunch of",
    "start": "373110",
    "end": "380550"
  },
  {
    "start": "376000",
    "end": "463000"
  },
  {
    "text": "problems it had a single index for the entire lifespan of the Prometheus server",
    "start": "380550",
    "end": "385980"
  },
  {
    "text": "and this would grow and grow and grow as paas and that was a big problem the original",
    "start": "385980",
    "end": "393990"
  },
  {
    "text": "database was actually quite clever but in terms of like how fast we could we",
    "start": "393990",
    "end": "399419"
  },
  {
    "text": "could spin it up and get it going but it was it turns out that creating a file",
    "start": "399419",
    "end": "405180"
  },
  {
    "text": "for every single time series in a kubernetes cluster just didn't scale you",
    "start": "405180",
    "end": "410490"
  },
  {
    "text": "know we had at Sound Cloud we had some CI pipelines that would rotate something",
    "start": "410490",
    "end": "418800"
  },
  {
    "text": "like 500 pods five times a day and so",
    "start": "418800",
    "end": "425280"
  },
  {
    "text": "after a week we had like 50 million metric files on the on disk in it and we",
    "start": "425280",
    "end": "431820"
  },
  {
    "text": "actually over overflowed the ext4 inode table and so we had to reformat our",
    "start": "431820",
    "end": "438510"
  },
  {
    "text": "Prometheus servers with more I know it was really annoying and so that's so",
    "start": "438510",
    "end": "444000"
  },
  {
    "text": "that's what inspired the need to create a new time series database and so yeah",
    "start": "444000",
    "end": "452759"
  },
  {
    "text": "the selection is also a little bit of a problem because you had to go and read a",
    "start": "452759",
    "end": "458460"
  },
  {
    "text": "whole bunch of different files yeah so",
    "start": "458460",
    "end": "465240"
  },
  {
    "start": "463000",
    "end": "588000"
  },
  {
    "text": "the new tech new test set up to test the results of the new database what we were",
    "start": "465240",
    "end": "473340"
  },
  {
    "text": "gonna say about thank you okay so basically this is the test we did back",
    "start": "473340",
    "end": "479909"
  },
  {
    "text": "then when we introduced a new storage engine for 2.0 we had one cluster with",
    "start": "479909",
    "end": "486570"
  },
  {
    "text": "their imitated Prometheus notes sitting aside of the cluster and what we did is",
    "start": "486570",
    "end": "491729"
  },
  {
    "text": "we had 800 microservices we had 100 to 120 km birds per second 300 K active",
    "start": "491729",
    "end": "498419"
  },
  {
    "text": "time series and every 10 minutes 50 percent of those part died and were put",
    "start": "498419",
    "end": "503970"
  },
  {
    "text": "in a new and this was the test setup and the result was drumroll a 15-time",
    "start": "503970",
    "end": "509460"
  },
  {
    "text": "reduction in memory usage 6 times reduction in CPU usage up to a hundred time reduction in disk writes 5 time",
    "start": "509460",
    "end": "516330"
  },
  {
    "text": "reductions in on disk size four times reduction in latency and we actually reputed service results if anyone",
    "start": "516330",
    "end": "522550"
  },
  {
    "text": "doesn't trust us course people tend to not trust us on these numbers but they",
    "start": "522550",
    "end": "528430"
  },
  {
    "text": "are actually really real numbers yeah we had we had one we've had a couple of",
    "start": "528430",
    "end": "533710"
  },
  {
    "text": "times where people called us Liars for how good perfect Prometheus's time series database performance was but then",
    "start": "533710",
    "end": "539170"
  },
  {
    "text": "we showed that it was possible and other time series databases like in flux TV also we were able to produce the similar",
    "start": "539170",
    "end": "545260"
  },
  {
    "text": "results one of the other things that",
    "start": "545260",
    "end": "552460"
  },
  {
    "text": "we've introduced recently is as part of our our code review pipeline we actually",
    "start": "552460",
    "end": "559900"
  },
  {
    "text": "built in prom bench so that we can we can automatically spin up using a github",
    "start": "559900",
    "end": "565750"
  },
  {
    "text": "slash command to spin up a benchmark so anytime we introduce a time series database change or an integration change",
    "start": "565750",
    "end": "572500"
  },
  {
    "text": "we can just spin up a whole Prometheus benchmark to see if we have a performance degradation because",
    "start": "572500",
    "end": "578460"
  },
  {
    "text": "everyone's about we we we make a change the time series database things get worse so now now we can basically spin",
    "start": "578460",
    "end": "585760"
  },
  {
    "text": "up this benchmark anytime we want so also with the remote readwrite API this",
    "start": "585760",
    "end": "593410"
  },
  {
    "start": "588000",
    "end": "640000"
  },
  {
    "text": "is much basically stable now we have a whole bunch of integrations for external",
    "start": "593410",
    "end": "601000"
  },
  {
    "text": "time series databases and other related long-term storage methods so if you have",
    "start": "601000",
    "end": "608200"
  },
  {
    "text": "a very have a very dynamic cluster like you're running your Prometheus inside your kubernetes cluster you usually only",
    "start": "608200",
    "end": "614050"
  },
  {
    "text": "want to keep a few days or maybe a couple of weeks of data in inside the kubernetes cluster and you can use the",
    "start": "614050",
    "end": "620020"
  },
  {
    "text": "remote write API to ship all that data out to an external storage for long term",
    "start": "620020",
    "end": "625060"
  },
  {
    "text": "so you still maintain the robustness where you can get your up-to-date data and you're alerting done through that",
    "start": "625060",
    "end": "630940"
  },
  {
    "text": "integrated Prometheus server but for long term dumptruck data you can use the",
    "start": "630940",
    "end": "638140"
  },
  {
    "text": "external storage security and quality we went through a",
    "start": "638140",
    "end": "645180"
  },
  {
    "start": "640000",
    "end": "736000"
  },
  {
    "text": "security audit thanks to the cloud native computing foundation we've we",
    "start": "645180",
    "end": "650850"
  },
  {
    "text": "wanted them to focus on like making sure that we were had a good data model and",
    "start": "650850",
    "end": "659090"
  },
  {
    "text": "didn't have any really obvious security problems with Prometheus the biggest thing that they came back and said is",
    "start": "659090",
    "end": "665300"
  },
  {
    "text": "you really should have endpoint protection so we we are actually state",
    "start": "665300",
    "end": "671460"
  },
  {
    "text": "we took a vote at the last developer summit and said yes we should probably start doing endpoint protection so over",
    "start": "671460",
    "end": "678240"
  },
  {
    "text": "the next sometime it's now in our to-do list to actually start implementing TLS",
    "start": "678240",
    "end": "684330"
  },
  {
    "text": "encryption and authentication for export or endpoints so Prometheus has always",
    "start": "684330",
    "end": "689840"
  },
  {
    "text": "supported an encryption and authentication against targets but we we",
    "start": "689840",
    "end": "694860"
  },
  {
    "text": "never promoted that as something that we should do in every exporter because we",
    "start": "694860",
    "end": "701670"
  },
  {
    "text": "just didn't have a big enough team to be able to handle the workload of supporting and maintaining security and",
    "start": "701670",
    "end": "706950"
  },
  {
    "text": "of course there's a bunch of us in the Prometheus community that our security",
    "start": "706950",
    "end": "712290"
  },
  {
    "text": "aware and we didn't want to just implement something that looks like security but actually makes you list it",
    "start": "712290",
    "end": "717990"
  },
  {
    "text": "doesn't make you any more secure or makes it worse because we implemented bad security and you put your you left",
    "start": "717990",
    "end": "725700"
  },
  {
    "text": "you you open it up to the Internet and now you get owned because we implemented bad security so we're gonna implement",
    "start": "725700",
    "end": "732090"
  },
  {
    "text": "good security and it it should make everybody happy and then one of the",
    "start": "732090",
    "end": "739350"
  },
  {
    "start": "736000",
    "end": "801000"
  },
  {
    "text": "things we changed recently after the developer summit was we are doing a much",
    "start": "739350",
    "end": "745080"
  },
  {
    "text": "better job with a benchmarking and testing and we're starting to do a better release cycle so we're gonna do",
    "start": "745080",
    "end": "751080"
  },
  {
    "text": "or now doing every six weeks consistent release cycles oh am i jumping yes okay stabilizing oh",
    "start": "751080",
    "end": "761630"
  },
  {
    "text": "alright yeah so the of course like happens when you create a new time",
    "start": "761630",
    "end": "767040"
  },
  {
    "text": "series database the first version had a bunch of bugs so we spent quite a bit of",
    "start": "767040",
    "end": "772740"
  },
  {
    "text": "time focusing on stability and fixing crash bugs and other stability problems with",
    "start": "772740",
    "end": "780170"
  },
  {
    "text": "the time series database so and as you saw 2 3 2 is the first",
    "start": "780170",
    "end": "788480"
  },
  {
    "text": "version which is actually considered really stable so if you're using feel sorry if you're using anything older",
    "start": "788480",
    "end": "794540"
  },
  {
    "text": "than this one you should update course that's the first one which we consider stable within the 2 point X train so as",
    "start": "794540",
    "end": "803390"
  },
  {
    "start": "801000",
    "end": "823000"
  },
  {
    "text": "Ben already mentioned we are having a new release process where we cut an RC",
    "start": "803390",
    "end": "809990"
  },
  {
    "text": "every six weeks and this six weeks is hard so it doesn't take into account any",
    "start": "809990",
    "end": "815390"
  },
  {
    "text": "any time for the actual release or any any better testing or such the RC is cut",
    "start": "815390",
    "end": "821060"
  },
  {
    "text": "every six weeks and basically once we are happy with that and once it went to",
    "start": "821060",
    "end": "827390"
  },
  {
    "start": "823000",
    "end": "857000"
  },
  {
    "text": "testing and people came back with to us and said ok everything looks fine then and only then we cut a release no matter",
    "start": "827390",
    "end": "833690"
  },
  {
    "text": "how long this takes within that RC cycle after six weeks we start a new RC cycle also if there's any",
    "start": "833690",
    "end": "840980"
  },
  {
    "text": "really bad bugs what we decided to do is we'll call a moratorium on any new features to re-stabilize course that's",
    "start": "840980",
    "end": "848000"
  },
  {
    "text": "something we didn't do very well in two point X like maintain stability for for end-users and we definitely needed to",
    "start": "848000",
    "end": "855200"
  },
  {
    "text": "get better at that so also there's quite",
    "start": "855200",
    "end": "860240"
  },
  {
    "start": "857000",
    "end": "884000"
  },
  {
    "text": "some nice field and 2.4 2.5 are already under this new release cycle which is",
    "start": "860240",
    "end": "865340"
  },
  {
    "text": "why they're in this section so Brian did something really nice he made prom here really really quicker in a lot of cases",
    "start": "865340",
    "end": "871810"
  },
  {
    "text": "and this is already merged it's yeah you might not notice this unless you have",
    "start": "871810",
    "end": "877430"
  },
  {
    "text": "really large or complex queries but still does a really really nice change it especially in large deployments then",
    "start": "877430",
    "end": "885860"
  },
  {
    "start": "884000",
    "end": "909000"
  },
  {
    "text": "also there is this recurring thing of long term storage so most people who if",
    "start": "885860",
    "end": "891290"
  },
  {
    "text": "they have to voice one concern with Prometheus it's that they're not quite certain how to do long term storage",
    "start": "891290",
    "end": "897490"
  },
  {
    "text": "myself I just kept all my data and my servers have data since the late 2015 so",
    "start": "897490",
    "end": "903170"
  },
  {
    "text": "that works still it's not the ideal working model so",
    "start": "903170",
    "end": "909730"
  },
  {
    "start": "909000",
    "end": "918000"
  },
  {
    "text": "as we fundamentally designed fermitas to have distinct data islands that's not something we're about to change within",
    "start": "909930",
    "end": "916320"
  },
  {
    "text": "permeated itself by definition and by design but what we have is we have our",
    "start": "916320",
    "end": "924420"
  },
  {
    "start": "918000",
    "end": "935000"
  },
  {
    "text": "new storage which allows you to actually take snapshots and backups which wasn't possible with premises one not X so",
    "start": "924420",
    "end": "931560"
  },
  {
    "text": "that's already quite good if you need to restore something and also this remote",
    "start": "931560",
    "end": "937860"
  },
  {
    "text": "readwrite API allows you to export data to two other things and there is a ton",
    "start": "937860",
    "end": "943080"
  },
  {
    "text": "of projects which integrate with our remote readwrite API there's a total of 12 in the meantime - which came from",
    "start": "943080",
    "end": "952170"
  },
  {
    "text": "permeated itself our cortex antennas cortex basically distributes the index",
    "start": "952170",
    "end": "957360"
  },
  {
    "text": "and the ingesting on auto-scaling parts while tannaz does a totally different",
    "start": "957360",
    "end": "963270"
  },
  {
    "text": "thing and it puts all the other storage into object storage like SEF or something which is quite nice and we",
    "start": "963270",
    "end": "969360"
  },
  {
    "text": "actually expect those two to merge back again in some time and then you have something like a distributed premises",
    "start": "969360",
    "end": "975660"
  },
  {
    "text": "like a truly distributed communities but there's others as well and we definitely do not want to say okay we endorsed this",
    "start": "975660",
    "end": "982950"
  },
  {
    "text": "one thing and you shouldn't use any other of course we don't really want to be kingmakers we want to see what the",
    "start": "982950",
    "end": "989100"
  },
  {
    "text": "community comes up with and whatever is best should just will course we don't",
    "start": "989100",
    "end": "994260"
  },
  {
    "text": "have any profit motivation so we can just do whatever and would see what",
    "start": "994260",
    "end": "999600"
  },
  {
    "text": "works best looking beyond just 2.5 and beyond one thing which is still missing",
    "start": "999600",
    "end": "1007520"
  },
  {
    "start": "1005000",
    "end": "1020000"
  },
  {
    "text": "we thought we would have that earlier is like a proper asset database we have the automa see we have the consistency we",
    "start": "1007520",
    "end": "1013880"
  },
  {
    "text": "have the durability we are still missing the isolation there is a patch set which has been carried since early to dot X it",
    "start": "1013880",
    "end": "1023270"
  },
  {
    "start": "1020000",
    "end": "1057000"
  },
  {
    "text": "hasn't been merged yet but basically what we're going to do to have proper isolation as and you only get one full",
    "start": "1023270",
    "end": "1029630"
  },
  {
    "text": "scrape in your data set or you get or you don't get it but you never see a partial scrape result",
    "start": "1029630",
    "end": "1035209"
  },
  {
    "text": "that's what isolation means in this context is basically we just take a monolithic counter or monotonic counter",
    "start": "1035209",
    "end": "1041240"
  },
  {
    "text": "we count it up and every every ID or every every scrape gets one one ID and unless that is",
    "start": "1041240",
    "end": "1048660"
  },
  {
    "text": "committed you don't return that sample and no matter in which context it's relatively",
    "start": "1048660",
    "end": "1054180"
  },
  {
    "text": "easy in at least on this layer but there's more beyond just parameters of",
    "start": "1054180",
    "end": "1061080"
  },
  {
    "start": "1057000",
    "end": "1080000"
  },
  {
    "text": "course there I mean if you if you know the story of permeate is how how the",
    "start": "1061080",
    "end": "1066330"
  },
  {
    "text": "Titans stole the light from from the gods and gave it to humans there's quite some ambition in the name of Prometheus",
    "start": "1066330",
    "end": "1072420"
  },
  {
    "text": "and this is by design so yes we do actually want to change the world and actually change how the world does",
    "start": "1072420",
    "end": "1078120"
  },
  {
    "text": "monitoring we firmly believe that any hierarchical data structure in",
    "start": "1078120",
    "end": "1083850"
  },
  {
    "start": "1080000",
    "end": "1111000"
  },
  {
    "text": "monitoring is doing it wrong by default so labels we consider labels one of the",
    "start": "1083850",
    "end": "1089010"
  },
  {
    "text": "most powerful things which we have within premises and labels are encoded",
    "start": "1089010",
    "end": "1094080"
  },
  {
    "text": "in our exposition format so while there is a ton of political considerations about people supporting something called",
    "start": "1094080",
    "end": "1100980"
  },
  {
    "text": "Prometheus exposition format we actually want to push the system because that",
    "start": "1100980",
    "end": "1106290"
  },
  {
    "text": "means people will start using labels more and more so we came up with open metrics yeah that's basically it we're",
    "start": "1106290",
    "end": "1117450"
  },
  {
    "start": "1111000",
    "end": "1211000"
  },
  {
    "text": "spending on permissive exposition format it's already a CNC F sandbox project we",
    "start": "1117450",
    "end": "1122520"
  },
  {
    "text": "are aiming to have an IT IDF RFC publish the full test suite blah blah blah so we have something actually official to to",
    "start": "1122520",
    "end": "1129420"
  },
  {
    "text": "run against I'm currently actually writing the internet draft which will hopefully become the RFC and 2.5 already",
    "start": "1129420",
    "end": "1137070"
  },
  {
    "text": "has open metric support for me so 2.5 but it's experimental it might change it might break we might do away with it but",
    "start": "1137070",
    "end": "1143910"
  },
  {
    "text": "it already works and also our pile Python library already is able admitted it's able to emit 2.5 sorry emit open",
    "start": "1143910",
    "end": "1151800"
  },
  {
    "text": "metrics metrics so open matrix is just metrics but when we could already do is",
    "start": "1151800",
    "end": "1158970"
  },
  {
    "text": "we can attach exemplars in open metrics to certain histograms to a certain",
    "start": "1158970",
    "end": "1164100"
  },
  {
    "text": "buckets so basically you have a trace and you know this ran for let's say more",
    "start": "1164100",
    "end": "1169200"
  },
  {
    "text": "than 60 seconds and then you attach one of those traces from that bucket of latency to your data so you have a tire",
    "start": "1169200",
    "end": "1176640"
  },
  {
    "text": "link between your latency packet and your trace and if you have really high latency in some cases you already know",
    "start": "1176640",
    "end": "1183030"
  },
  {
    "text": "what trace I need to look at there's",
    "start": "1183030",
    "end": "1188600"
  },
  {
    "text": "some integrations like open census and stack travel for example they want to have this they want to support this they",
    "start": "1188600",
    "end": "1194010"
  },
  {
    "text": "want to ingest this data our are free for example promises to just drop this this reference to the exemplar course",
    "start": "1194010",
    "end": "1200700"
  },
  {
    "text": "currently within premises we don't really have a use for it this might change it might not change but so",
    "start": "1200700",
    "end": "1205980"
  },
  {
    "text": "basically open metrics is designed to allow people to do this but not force people to support this and we are",
    "start": "1205980",
    "end": "1213960"
  },
  {
    "start": "1211000",
    "end": "1259000"
  },
  {
    "text": "currently only looking at open metrics but we also want to have markers metrics",
    "start": "1213960",
    "end": "1219480"
  },
  {
    "text": "is usually the start of your monitoring story or your observer ability story but there's tons more to it so we also want",
    "start": "1219480",
    "end": "1225630"
  },
  {
    "text": "to have events we want to have traces we don't we want to have all these things within something which looks like open",
    "start": "1225630",
    "end": "1232530"
  },
  {
    "text": "metrics where you have labels where you have your well defined data structures basically to redo a new level of",
    "start": "1232530",
    "end": "1239540"
  },
  {
    "text": "observability across the whole ecosystem Tom Volk is actually already working on something which is basically for like",
    "start": "1239540",
    "end": "1245640"
  },
  {
    "text": "log files if you have if you imagine having premises for log files that's",
    "start": "1245640",
    "end": "1250950"
  },
  {
    "text": "what he's working on which is pretty awesome and just by happenstance it'll",
    "start": "1250950",
    "end": "1256590"
  },
  {
    "text": "support open metrics or whatever we end up calling the done stuff there's already quite a few people who are",
    "start": "1256590",
    "end": "1262440"
  },
  {
    "start": "1259000",
    "end": "1270000"
  },
  {
    "text": "companies who are committed to supporting open metrics these are just the first adopters because they're in",
    "start": "1262440",
    "end": "1268140"
  },
  {
    "text": "the meantime too many to list all of them and yes we do want to change the",
    "start": "1268140",
    "end": "1274200"
  },
  {
    "start": "1270000",
    "end": "1283000"
  },
  {
    "text": "world like seriously we will always commit to having simple and resilient",
    "start": "1274200",
    "end": "1280470"
  },
  {
    "text": "operation that will always be the case our most important thing will always be",
    "start": "1280470",
    "end": "1287010"
  },
  {
    "start": "1283000",
    "end": "1295000"
  },
  {
    "text": "to get from raw data through computation into alerts code step is what what",
    "start": "1287010",
    "end": "1292350"
  },
  {
    "text": "enables you to keep your services up and running we're talking to hardware vendors about intermitting premises and",
    "start": "1292350",
    "end": "1299790"
  },
  {
    "start": "1295000",
    "end": "1314000"
  },
  {
    "text": "or open metrics data some of them are already working on that and we always try to it's a little bit in your face",
    "start": "1299790",
    "end": "1307140"
  },
  {
    "text": "but still we are also being serious about that one we always want to be able to support 10x scale of today for tomorrow yeah there's",
    "start": "1307140",
    "end": "1315700"
  },
  {
    "start": "1314000",
    "end": "1325000"
  },
  {
    "text": "a few more talks you can also download these slides from from the schedule so you can just look at them and now we",
    "start": "1315700",
    "end": "1322570"
  },
  {
    "text": "have time for questions",
    "start": "1322570",
    "end": "1325200"
  },
  {
    "start": "1325000",
    "end": "1405000"
  },
  {
    "text": "thanks for sharing we use promise use in production and I found sometimes we",
    "start": "1333510",
    "end": "1340590"
  },
  {
    "text": "calculated the CPU usage right is no it's not correct because I found",
    "start": "1340590",
    "end": "1346270"
  },
  {
    "text": "the route KC's primitives collect metrics with the 10 step is the scrapped",
    "start": "1346270",
    "end": "1351640"
  },
  {
    "text": "scrapped to time note the real time step but if the couplet has a delay matrix and the",
    "start": "1351640",
    "end": "1363070"
  },
  {
    "text": "temperatures will correct that time the third high is not not correct that will",
    "start": "1363070",
    "end": "1369190"
  },
  {
    "text": "cause the matrices over the limit so we",
    "start": "1369190",
    "end": "1375010"
  },
  {
    "text": "try to find a solution we for custom knowledge spotter and imply implied CPU",
    "start": "1375010",
    "end": "1383680"
  },
  {
    "text": "usage right in the Nordics Potter - already this is shorthand but I don't",
    "start": "1383680",
    "end": "1389620"
  },
  {
    "text": "know whether it is the path the brightest Oh what's the open e about Rama huge",
    "start": "1389620",
    "end": "1396210"
  },
  {
    "text": "calculation about commuting value in the future so so you're collecting you're",
    "start": "1396210",
    "end": "1406090"
  },
  {
    "start": "1405000",
    "end": "1489000"
  },
  {
    "text": "collecting the CPU usage from the KU boot",
    "start": "1406090",
    "end": "1410279"
  },
  {
    "text": "ah oh yeah so so yeah so there's a lot",
    "start": "1411299",
    "end": "1418889"
  },
  {
    "text": "of indirection there and actually the best thing to do is to have your prometheus instead of going through the",
    "start": "1418889",
    "end": "1425460"
  },
  {
    "text": "couplet go directly to the node exporter go directly to see advisor and that way the the timestamp for when Prometheus",
    "start": "1425460",
    "end": "1433320"
  },
  {
    "text": "Connect collects the data from the target it's getting it as direct as possible and actually that's part of the",
    "start": "1433320",
    "end": "1439259"
  },
  {
    "text": "part of the design goal of previous is we didn't what we intentionally don't have a single server agent we design it",
    "start": "1439259",
    "end": "1448200"
  },
  {
    "text": "so that Prometheus can talk to every metrics endpoint directly as quickly as",
    "start": "1448200",
    "end": "1453239"
  },
  {
    "text": "possible with the least amount of toy and get them freshest data possible and",
    "start": "1453239",
    "end": "1458669"
  },
  {
    "text": "also a part of our scalability design is that a single agent like could take",
    "start": "1458669",
    "end": "1465600"
  },
  {
    "text": "several seconds just to return the results of all the things on a server whereas if you if you grant if you",
    "start": "1465600",
    "end": "1471600"
  },
  {
    "text": "create a granular set of metrics where you talk to your app and you talk to the",
    "start": "1471600",
    "end": "1476669"
  },
  {
    "text": "couplet and you talk to the see advisor you talk to the node exporter each of those happens in milliseconds so it",
    "start": "1476669",
    "end": "1482970"
  },
  {
    "text": "allows Prometheus to collect that data and really get it fine-grained and much more accurate",
    "start": "1482970",
    "end": "1489769"
  },
  {
    "text": "we know that from Athens earning support of float as his metrics type right and",
    "start": "1494990",
    "end": "1500650"
  },
  {
    "text": "and so I'm curious about what if I want to store a stream purses other metrics",
    "start": "1500650",
    "end": "1509110"
  },
  {
    "text": "okay so yeah so people want to store strings so we store strings so previous",
    "start": "1509380",
    "end": "1515690"
  },
  {
    "text": "does support strings it's all in our index so well we have a pattern called an info metric and what you do is you",
    "start": "1515690",
    "end": "1522620"
  },
  {
    "text": "take your string and you make a label for it so if you have something like there's Prometheus billed info contains",
    "start": "1522620",
    "end": "1530480"
  },
  {
    "text": "the the go compiler version the get commit of the server and the actual",
    "start": "1530480",
    "end": "1535880"
  },
  {
    "text": "Prometheus version string and so all those strings are all as labels on a single metric and the value of that",
    "start": "1535880",
    "end": "1542810"
  },
  {
    "text": "metric is one and because of the Prometheus compression storing one over",
    "start": "1542810",
    "end": "1549440"
  },
  {
    "start": "1544000",
    "end": "1574000"
  },
  {
    "text": "and over and over again in a database seems like it might be a lot of overhead but because of the Prometheus",
    "start": "1549440",
    "end": "1554750"
  },
  {
    "text": "compression that's only 20 bytes of storage for what every half an hour for",
    "start": "1554750",
    "end": "1561920"
  },
  {
    "text": "it ya know it's every hundred and twenty but 20 bytes for 120 samples so if",
    "start": "1561920",
    "end": "1567950"
  },
  {
    "text": "you're doing 15 seconds it's half an hour so so you're you're only talking like a few hundred bytes to store that",
    "start": "1567950",
    "end": "1577010"
  },
  {
    "text": "one over and over again for an entire day so the the overhead there is much smaller and we store all those strings",
    "start": "1577010",
    "end": "1582800"
  },
  {
    "text": "in our database and so this actually leads to a bunch of really interesting use cases for example if instead of",
    "start": "1582800",
    "end": "1589460"
  },
  {
    "text": "storing the kernel version like or the hardware version or the VM type in as a",
    "start": "1589460",
    "end": "1598280"
  },
  {
    "text": "label on every single metric we put them into these info labels and then you can use joins and prom ql2",
    "start": "1598280",
    "end": "1604910"
  },
  {
    "text": "if you want to correlate based on kernel version or vm type or hardware type or",
    "start": "1604910",
    "end": "1611560"
  },
  {
    "text": "some other one of these other strings or versions like if you have if you have",
    "start": "1611560",
    "end": "1616730"
  },
  {
    "text": "two versions running in production you can take and using a join and a group",
    "start": "1616730",
    "end": "1622880"
  },
  {
    "text": "left bring the version string of your app in - your cpu metric for the app",
    "start": "1622880",
    "end": "1628530"
  },
  {
    "text": "dynamically and then now you can do correlation of CPU usage by version and",
    "start": "1628530",
    "end": "1635080"
  },
  {
    "text": "so this is an extremely powerful thing that you would usually do in like an SQL thing but we can also support the same",
    "start": "1635080",
    "end": "1641560"
  },
  {
    "text": "kind of thing in prompt QL and so these the the string handling by having it these separate metrics leads to some",
    "start": "1641560",
    "end": "1648640"
  },
  {
    "text": "really powerful correlation possibilities I actually did that once in production where we had a JVM app and",
    "start": "1648640",
    "end": "1655960"
  },
  {
    "text": "we some of the JVM s on the cluster were using two CPUs and some of them were",
    "start": "1655960",
    "end": "1662290"
  },
  {
    "text": "using one CPU and it was the same binary it was the same JVM it was the same app and they're all running and we we",
    "start": "1662290",
    "end": "1669640"
  },
  {
    "text": "couldn't figure out why half the VMS or half the JVM s were using twice as much",
    "start": "1669640",
    "end": "1674950"
  },
  {
    "text": "CPU and so we started just using prom ql to correlate like well what if it's the hardware type so we had an info metric",
    "start": "1674950",
    "end": "1681310"
  },
  {
    "text": "that had the CPU type because we had some versions of these xeon cpus and we",
    "start": "1681310",
    "end": "1687340"
  },
  {
    "text": "had some versions of these xeon cpus no correlation so I threw in I did group",
    "start": "1687340",
    "end": "1693250"
  },
  {
    "start": "1689000",
    "end": "1712000"
  },
  {
    "text": "left on kernel name and so I brought the kernel name in and boom perfect correlation that the older kernel",
    "start": "1693250",
    "end": "1700870"
  },
  {
    "text": "version made the JVM twice as slow so guess what we have to upgrade the kernel and all these servers just happens so",
    "start": "1700870",
    "end": "1709000"
  },
  {
    "text": "that's that's that's the power of prom cool Walker's okay thank you",
    "start": "1709000",
    "end": "1718409"
  },
  {
    "start": "1712000",
    "end": "1761000"
  },
  {
    "text": "so previously I use the provisions with the remotest origin I establish a link",
    "start": "1718409",
    "end": "1724730"
  },
  {
    "text": "friend-friend DB so I found those performances not at courtesy and now I'm",
    "start": "1724730",
    "end": "1731009"
  },
  {
    "text": "using our craft now to query the primitives and as a problem for Patriots",
    "start": "1731009",
    "end": "1738269"
  },
  {
    "text": "refresh the slowly I'm not sure why so this is a puzzle question what",
    "start": "1738269",
    "end": "1745799"
  },
  {
    "text": "specifically is refreshing slowly sorry what solution what specifically is",
    "start": "1745799",
    "end": "1751230"
  },
  {
    "text": "refreshing slowly I didn't I means that you know if I use the remote storage I",
    "start": "1751230",
    "end": "1757379"
  },
  {
    "text": "found as a pro me who's acquired raise slow so yeah so yeah so Prometheus is a",
    "start": "1757379",
    "end": "1764759"
  },
  {
    "text": "very fast time series database and many of the remote storage options are actually slower than Prometheus and",
    "start": "1764759",
    "end": "1772700"
  },
  {
    "text": "you're better off just using Prometheus instead of using remote storage which is why we're like you know we're we're",
    "start": "1772700",
    "end": "1779549"
  },
  {
    "text": "we're experimenting with many different remote storages so whichever remote storage you're using which one did you",
    "start": "1779549",
    "end": "1784740"
  },
  {
    "text": "say you were using in flux I don't know that one",
    "start": "1784740",
    "end": "1791299"
  },
  {
    "text": "we don't have an official suggestion so yeah but if you want to have an",
    "start": "1796879",
    "end": "1804509"
  },
  {
    "text": "unofficial suggestion cortex and or tennis is probably your best bet cortex",
    "start": "1804509",
    "end": "1809519"
  },
  {
    "start": "1808000",
    "end": "1855000"
  },
  {
    "text": "and Thanos yeah there are two there's two different projects Quartet cortex is",
    "start": "1809519",
    "end": "1816419"
  },
  {
    "text": "a Prometheus time series data scalable time series data base and Thanos is a",
    "start": "1816419",
    "end": "1824149"
  },
  {
    "text": "different kind of scalable time series database so these are these are the two that I like the most but yeah many of",
    "start": "1824149",
    "end": "1832080"
  },
  {
    "text": "that many of the remote storage options are interesting but are actually slower",
    "start": "1832080",
    "end": "1837740"
  },
  {
    "text": "did you play did you say timescale DV",
    "start": "1837740",
    "end": "1842179"
  },
  {
    "text": "yeah timescale DV is based on Postgres so it's it's yeah it's slow it's slow",
    "start": "1843150",
    "end": "1853409"
  },
  {
    "start": "1855000",
    "end": "1889000"
  },
  {
    "text": "and so I'm I turn back to suss local",
    "start": "1855120",
    "end": "1860380"
  },
  {
    "text": "storage so I think a local storage is a good and it can better from performance but is it bring and that's a question so",
    "start": "1860380",
    "end": "1867910"
  },
  {
    "text": "after several days I mean one man sees that this car is a flow so I'm beans I",
    "start": "1867910",
    "end": "1876190"
  },
  {
    "text": "wanted to delay the some of our data it's the possible for the lovers the",
    "start": "1876190",
    "end": "1881919"
  },
  {
    "text": "load header you can tell from it is to persist for a maximum of time and you",
    "start": "1881919",
    "end": "1887740"
  },
  {
    "text": "also can't affirm it is to only use that much search and it will start deleting all by itself that's your best bet there",
    "start": "1887740",
    "end": "1893860"
  },
  {
    "start": "1889000",
    "end": "1937000"
  },
  {
    "text": "is also a possibility to to delete from the blocks but I it's this comes with",
    "start": "1893860",
    "end": "1900400"
  },
  {
    "text": "performance penalties so it's best to just cut off and just drop the data at",
    "start": "1900400",
    "end": "1905679"
  },
  {
    "text": "the end and basically you need to scale your your storage in a way that if you let let's say you want to have two weeks",
    "start": "1905679",
    "end": "1911980"
  },
  {
    "text": "or two months off data then you just need to do some calculations maybe give give some extra room and that's your",
    "start": "1911980",
    "end": "1918010"
  },
  {
    "text": "storage need and everything which is all your just drop that is a command line flag for Prometheus or or attach a",
    "start": "1918010",
    "end": "1924340"
  },
  {
    "text": "bigger volume yeah yeah",
    "start": "1924340",
    "end": "1929309"
  },
  {
    "text": "yes yeah yes so if you if you change the",
    "start": "1930960",
    "end": "1940299"
  },
  {
    "text": "time yet so because it's a command line flag you stop prometheus you change the flag you start it up again and then it",
    "start": "1940299",
    "end": "1946029"
  },
  {
    "text": "will garbage collect all the old data I think we have time for one more",
    "start": "1946029",
    "end": "1952860"
  },
  {
    "text": "Thank You Jimmy I've reviewed the codex and the saddles and but I think all those projects are",
    "start": "1952860",
    "end": "1960940"
  },
  {
    "text": "very complicated and the brewing is great operation burden yes Fran so I'm",
    "start": "1960940",
    "end": "1966840"
  },
  {
    "text": "wondering if Francis will plan to support local long term durability and",
    "start": "1966840",
    "end": "1974460"
  },
  {
    "text": "most importantly promise this town's gambling because if we have long term",
    "start": "1974460",
    "end": "1983140"
  },
  {
    "text": "durability storage we have one year or two year date we can't kill them without",
    "start": "1983140",
    "end": "1989309"
  },
  {
    "text": "some sampling so I'm wondering if from this 203 have a plan so yeah so we have",
    "start": "1989309",
    "end": "1999490"
  },
  {
    "start": "1997000",
    "end": "2037000"
  },
  {
    "text": "discussed so Santos has down sampling and it's really really powerful so it'll",
    "start": "1999490",
    "end": "2005399"
  },
  {
    "text": "it'll because it can do it in the background we're currently working on",
    "start": "2005399",
    "end": "2011010"
  },
  {
    "text": "the ability to do backfill and once we have so what and what I mean yeah so",
    "start": "2011010",
    "end": "2017789"
  },
  {
    "text": "what I mean by backfill is if you have a prometheus time series database right now there's no way to just add more data",
    "start": "2017789",
    "end": "2024179"
  },
  {
    "text": "to that without scraping right now and what we're working on is we're working on the ability to add a new just taken",
    "start": "2024179",
    "end": "2030539"
  },
  {
    "text": "and bulk of dumped new data sets into a Prometheus server and once we have last we put it here yeah so weak if you have",
    "start": "2030539",
    "end": "2038700"
  },
  {
    "start": "2037000",
    "end": "2169000"
  },
  {
    "text": "a bunch of data we can just insert actually not here but we can just add a whole bunch of new time series as",
    "start": "2038700",
    "end": "2044669"
  },
  {
    "text": "backfill and the two reasons we want to do that first is one so that we can do backfilled recording rules so actually",
    "start": "2044669",
    "end": "2051388"
  },
  {
    "text": "the best way to do down sampling right now is to do recording rules that create down sample data and then instead of",
    "start": "2051389",
    "end": "2057960"
  },
  {
    "text": "querying the the live data because that Prometheus 2.0 time-series",
    "start": "2057960",
    "end": "2063480"
  },
  {
    "text": "database is totally durable all on its own it's actually a really good durable time series database but it doesn't",
    "start": "2063480",
    "end": "2070440"
  },
  {
    "text": "support mutation it's it's a right once and so using recording rules is a good",
    "start": "2070440",
    "end": "2078030"
  },
  {
    "text": "way to do down sampling because you can court you can set your query to query a bunch of data and you can record it only",
    "start": "2078030",
    "end": "2084658"
  },
  {
    "text": "once a minutes because in Prometheus 2.0 we allowed her recording rule group",
    "start": "2084659",
    "end": "2090320"
  },
  {
    "text": "intervals so you could if you want to create a once every one minute or every five minute you can create those",
    "start": "2090320",
    "end": "2096270"
  },
  {
    "text": "recording rules that only record every five minutes also remains for a long",
    "start": "2096270",
    "end": "2111690"
  },
  {
    "text": "time yeah and it just takes up a lot of space the other thing you can do is you can use Federation is a simpler solution",
    "start": "2111690",
    "end": "2117570"
  },
  {
    "text": "for on-prem so you can create your recording rules and then and then use a",
    "start": "2117570",
    "end": "2122790"
  },
  {
    "text": "federated endpoint and have another have one Prometheus server that's dedicated to long term storage and this is what",
    "start": "2122790",
    "end": "2128670"
  },
  {
    "text": "Ritchie does for his company and and also we do this for the same thing at git lab so we have we have our the gate",
    "start": "2128670",
    "end": "2136350"
  },
  {
    "text": "lab runner pool and this is a huge auto scaling group and so and it creates a",
    "start": "2136350",
    "end": "2141600"
  },
  {
    "text": "lot of churning a lot of data and so we take and do pre-recorded data and we pull that in times over Oh times up we",
    "start": "2141600",
    "end": "2150030"
  },
  {
    "text": "will be heading down to yeah so we're going yeah we will be heading down to meet the maintainer right now so anyone",
    "start": "2150030",
    "end": "2156570"
  },
  {
    "text": "who has questions just follow us to the CTF booth forever you'll be staying for the next hour and also 1,500 to 1,600",
    "start": "2156570",
    "end": "2163320"
  },
  {
    "text": "we'll be there again",
    "start": "2163320",
    "end": "2166460"
  }
]