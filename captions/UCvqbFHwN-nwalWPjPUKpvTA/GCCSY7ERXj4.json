[
  {
    "start": "0",
    "end": "32000"
  },
  {
    "text": "um thanks thomas for the introduction uh thanks everyone for being here um hope",
    "start": "640",
    "end": "5759"
  },
  {
    "text": "you all had a good lunch also uh welcome to everyone who's online uh happy that you're able to participate as",
    "start": "5759",
    "end": "12799"
  },
  {
    "text": "well um like thomas said my name is david this is my colleague evan we're both software engineers on the compute",
    "start": "12799",
    "end": "19680"
  },
  {
    "text": "infrastructure team at airbnb um we're going to be talking about uh how we moved",
    "start": "19680",
    "end": "26320"
  },
  {
    "text": "how our kubernetes clusters have been evolving into heterogeneous clusters",
    "start": "26320",
    "end": "31679"
  },
  {
    "text": "over the past several years quick outline of our presentation evan's going to talk a little bit about",
    "start": "31679",
    "end": "38719"
  },
  {
    "start": "32000",
    "end": "96000"
  },
  {
    "text": "where we were before give you some historical context on what our clusters looked like",
    "start": "38719",
    "end": "45360"
  },
  {
    "text": "two three four years ago and we're going to do a deep dive into",
    "start": "45360",
    "end": "50559"
  },
  {
    "text": "uh three specific problems that we encountered we encountered a lot more than three but we picked three that we",
    "start": "50559",
    "end": "57199"
  },
  {
    "text": "thought were sort of the most interesting these are both sort of technical and organizational hurdles that we had to",
    "start": "57199",
    "end": "62800"
  },
  {
    "text": "overcome on this journey um i guess the other thing i'll mention here is this journey is ongoing some of the stuff",
    "start": "62800",
    "end": "69119"
  },
  {
    "text": "that we're talking about is done some of it is in progress some of it's not complete yet",
    "start": "69119",
    "end": "74479"
  },
  {
    "text": "but i think we've got a good plan at least and then lastly we're going to give you some future plans and lessons learned",
    "start": "74479",
    "end": "82000"
  },
  {
    "text": "the tldr here is that heterogeneous clusters are great they've already been",
    "start": "82000",
    "end": "87600"
  },
  {
    "text": "instrumental for airbnb and for our team so with that i'm going to hand it over to evan to talk a",
    "start": "87600",
    "end": "93520"
  },
  {
    "text": "little bit about some historical context that's great thanks david",
    "start": "93520",
    "end": "98960"
  },
  {
    "start": "96000",
    "end": "465000"
  },
  {
    "text": "yeah as david mentioned i'm going to start with a brief history of time of kubernetes at airbnb",
    "start": "98960",
    "end": "106960"
  },
  {
    "text": "in 2016 and 2017 we started evaluating kubernetes for production use",
    "start": "106960",
    "end": "113680"
  },
  {
    "text": "previously we were running chef on aws ec2 where each replica of each service",
    "start": "113680",
    "end": "120240"
  },
  {
    "text": "would have its own machine in 2017 and 2018 we started building one",
    "start": "120240",
    "end": "125920"
  },
  {
    "text": "touch which is airbnb's abstraction layer on top of kubernetes for developers",
    "start": "125920",
    "end": "132160"
  },
  {
    "text": "the philosophy here is that all of a services config lives in one place in",
    "start": "132160",
    "end": "138400"
  },
  {
    "text": "git and this config is then converted and applied as kubernetes manifests to the",
    "start": "138400",
    "end": "144720"
  },
  {
    "text": "cluster um in 2018 2019 there was a huge effort",
    "start": "144720",
    "end": "151200"
  },
  {
    "text": "across our engineering org to migrate uh 90 of our 700 plus services at the time",
    "start": "151200",
    "end": "159120"
  },
  {
    "text": "um to kubernetes uh initially our clusters were separated out by environment so we had a",
    "start": "159120",
    "end": "167040"
  },
  {
    "text": "prod cluster a staging cluster a dev cluster etc but this quickly grew out of hand as we",
    "start": "167040",
    "end": "174080"
  },
  {
    "text": "hit kubernetes single node single cluster node size limits and we're",
    "start": "174080",
    "end": "179360"
  },
  {
    "text": "forced to split these clusters into different cluster types so we had uh we we split our prod",
    "start": "179360",
    "end": "187920"
  },
  {
    "text": "cluster into multiple prod clusters our stage and cluster into multiple staging clusters um",
    "start": "187920",
    "end": "194000"
  },
  {
    "text": "and these clusters happen to be comprised of a single instance type",
    "start": "194000",
    "end": "200239"
  },
  {
    "text": "so single instance type clusters this is what a single instance type",
    "start": "200879",
    "end": "206239"
  },
  {
    "text": "cluster looks like we have a pod come into a scheduler",
    "start": "206239",
    "end": "212480"
  },
  {
    "text": "and the scheduler tries to schedule a pod on a node with enough resources",
    "start": "212480",
    "end": "218480"
  },
  {
    "text": "initially we only had c5d 9xls as our node type because this",
    "start": "218480",
    "end": "224239"
  },
  {
    "text": "happened to be the latest one we were using in ec2",
    "start": "224239",
    "end": "229519"
  },
  {
    "text": "and this was enough at first but as more and more workloads started migrating",
    "start": "230799",
    "end": "235840"
  },
  {
    "text": "some specialized workloads required different instance types so uh gpu workloads started migrating we're",
    "start": "235840",
    "end": "242720"
  },
  {
    "text": "forced to create a gpu cluster type workloads requiring a lot of memory",
    "start": "242720",
    "end": "248799"
  },
  {
    "text": "we created a high memory cluster type for and these cluster types were identical",
    "start": "248799",
    "end": "256000"
  },
  {
    "text": "in both setup and config and allowed us to expand them horizontally",
    "start": "256000",
    "end": "262560"
  },
  {
    "text": "um yeah so why single instance type um this was before we had cluster",
    "start": "262639",
    "end": "269360"
  },
  {
    "text": "autoscaler which allows you to dynamically scale cluster size based on",
    "start": "269360",
    "end": "274400"
  },
  {
    "text": "node capacity um without this multi-instance clusters",
    "start": "274400",
    "end": "279919"
  },
  {
    "text": "multiple instance type clusters were not yet feasible any scheduling and capacity errors",
    "start": "279919",
    "end": "286720"
  },
  {
    "text": "we ran into we would have to resolve manually which was quite a pain because we'd have to look at the different pod",
    "start": "286720",
    "end": "293840"
  },
  {
    "text": "requirements and manually determine which node types would fit those scheduling constraints",
    "start": "293840",
    "end": "302320"
  },
  {
    "text": "additionally no cluster auto scaler means a lack of automation",
    "start": "302320",
    "end": "307680"
  },
  {
    "text": "we were at the time manually scaling all of our asgs when we got alerted which was not great",
    "start": "307680",
    "end": "316160"
  },
  {
    "text": "yeah so back to the timeline in 2019 and 2020 [Music]",
    "start": "316160",
    "end": "321919"
  },
  {
    "text": "as more and more specialized workloads uh migrated we had a cluster type explosion and we went",
    "start": "321919",
    "end": "329199"
  },
  {
    "text": "from around five clusters to over 90 that we have now and uh we have over 30",
    "start": "329199",
    "end": "336639"
  },
  {
    "text": "cluster types this leads to a lot of painful operational overhead for our team",
    "start": "336639",
    "end": "343440"
  },
  {
    "text": "here's a screenshot of some 700 almost 700 alerts that we have",
    "start": "343440",
    "end": "350240"
  },
  {
    "text": "just for asg sizes which really isn't great um [Music]",
    "start": "350240",
    "end": "356240"
  },
  {
    "text": "yeah and this is our attempted solution to fix this uh multiple instance type",
    "start": "356240",
    "end": "361360"
  },
  {
    "text": "clusters um which look like this um",
    "start": "361360",
    "end": "366720"
  },
  {
    "text": "so we have the same thing as before where a pod comes into the scheduler but this time the scheduler has",
    "start": "366720",
    "end": "373680"
  },
  {
    "text": "many node types to choose from um it can schedule it on any node type that can fit the pod requirements of",
    "start": "373680",
    "end": "381199"
  },
  {
    "text": "in this case the four that it has but this time we also have cluster auto scaler which handles the complexity of",
    "start": "381199",
    "end": "389360"
  },
  {
    "text": "managing the asg sizes and it scales up the certain node types",
    "start": "389360",
    "end": "395520"
  },
  {
    "text": "uh when required by a pod but there's no capacity in the cluster",
    "start": "395520",
    "end": "401039"
  },
  {
    "text": "um so yeah why did we want to migrate well first it brings our team quite a",
    "start": "401120",
    "end": "406479"
  },
  {
    "text": "lot of quality of life improvements reducing the number of cluster types",
    "start": "406479",
    "end": "412160"
  },
  {
    "text": "to generalize clusters and make it so that we only have one process for cluster creation is really",
    "start": "412160",
    "end": "418560"
  },
  {
    "text": "beneficial for our team and it also reduces the number of alerts and alert fatigue",
    "start": "418560",
    "end": "423919"
  },
  {
    "text": "um secondly we wanted to be able to control cluster composition at a global",
    "start": "423919",
    "end": "429919"
  },
  {
    "text": "level it unlocks some cost savings for us when we're able to utilize more node",
    "start": "429919",
    "end": "436880"
  },
  {
    "text": "types rather than being locked into one gives us some contract flexibility with aws",
    "start": "436880",
    "end": "443440"
  },
  {
    "text": "in the future we are trying to utilize spot instances so we can run part of our",
    "start": "443440",
    "end": "448560"
  },
  {
    "text": "clusters on spot and also new instance generation upgrades we get",
    "start": "448560",
    "end": "455039"
  },
  {
    "text": "pretty much for free in multi-class the multi-cluster setting historically this has taken us",
    "start": "455039",
    "end": "460960"
  },
  {
    "text": "years to upgrade our entire fleet across new instance generations",
    "start": "460960",
    "end": "466960"
  },
  {
    "text": "um yeah and the next section is solutions and other problems um i'm gonna pass it back to david to talk",
    "start": "466960",
    "end": "472879"
  },
  {
    "text": "about the first one thanks evan um so",
    "start": "472879",
    "end": "478560"
  },
  {
    "text": "in this section we're going to talk about three of the major technical and organizational hurdles that we had to",
    "start": "478560",
    "end": "485039"
  },
  {
    "text": "overcome uh to do this migration to heterogeneous clusters",
    "start": "485039",
    "end": "490400"
  },
  {
    "text": "the first one i've entitled in which nobody knows what's going on specifically this is going to be",
    "start": "490400",
    "end": "495440"
  },
  {
    "start": "493000",
    "end": "521000"
  },
  {
    "text": "focusing on cluster autoscaler because it turns out that we didn't know what was going on with cluster autoscaler",
    "start": "495440",
    "end": "502960"
  },
  {
    "text": "when we started this process one of the key questions that all of our stakeholders asked us is how do we know",
    "start": "502960",
    "end": "509360"
  },
  {
    "text": "what's going to be running in our cluster like how do you know what mix of instance instance types you have how do",
    "start": "509360",
    "end": "515919"
  },
  {
    "text": "you know where things are going to get scheduled and we thought we had the answer to that",
    "start": "515919",
    "end": "521760"
  },
  {
    "text": "so this first diagram here is not actually what happens but it's what we thought happened",
    "start": "521760",
    "end": "527600"
  },
  {
    "text": "so a new pod comes in and cube scheduler looks at the pod specifications the resource requests etc",
    "start": "527600",
    "end": "535680"
  },
  {
    "text": "and it's going to do one of two things in the first case which i'm calling the happy path",
    "start": "535680",
    "end": "541279"
  },
  {
    "text": "there's a node somewhere which meets all of the requirements and cube scheduler",
    "start": "541279",
    "end": "546640"
  },
  {
    "text": "just binds the pod to that note i'm not going to talk about the happy path anymore",
    "start": "546640",
    "end": "552880"
  },
  {
    "start": "552000",
    "end": "651000"
  },
  {
    "text": "the unhappy path uh there are no nodes in the cluster where the pod can run and",
    "start": "552880",
    "end": "559680"
  },
  {
    "text": "so cube scheduler puts the pod into a pending state it marks it as unschedulable and cluster",
    "start": "559680",
    "end": "566560"
  },
  {
    "text": "auto scaler then monitors the list of unscheduleable pods and it's going to",
    "start": "566560",
    "end": "572000"
  },
  {
    "text": "try to spin up new ec2 instances that match the pod specifications it looks at",
    "start": "572000",
    "end": "577360"
  },
  {
    "text": "things like the resource request it looks at node selectors node labels",
    "start": "577360",
    "end": "582480"
  },
  {
    "text": "looks at things like the pod topology spread etc it's going to iterate through all of the",
    "start": "582480",
    "end": "589040"
  },
  {
    "text": "different node groups that it has and it's going to select one that matches the pod specifications and",
    "start": "589040",
    "end": "596880"
  },
  {
    "text": "it's going to spin up a new host from that node group so in our setting node groups correspond exactly to asgs",
    "start": "596880",
    "end": "605040"
  },
  {
    "text": "because in a node group you have to have all of the nodes identical so they have to have the",
    "start": "605040",
    "end": "611440"
  },
  {
    "text": "same cpus they have to have the same memory uh this goes down to the labels and everything that are applied to nodes",
    "start": "611440",
    "end": "618160"
  },
  {
    "text": "in that group as well and so we have a different asg that corresponds to each",
    "start": "618160",
    "end": "623519"
  },
  {
    "text": "different instance type that we want available in our cluster",
    "start": "623519",
    "end": "628480"
  },
  {
    "text": "so anyways once a cluster autoscaler spins up that node after it joins the cluster cube scheduler will then see oh",
    "start": "628880",
    "end": "635360"
  },
  {
    "text": "look there's a node that can accommodate my pod and it binds the pod to the node um nice and simple right",
    "start": "635360",
    "end": "641920"
  },
  {
    "text": "cluster autoscaler is making all these decisions about which node types to spin up and we",
    "start": "641920",
    "end": "647360"
  },
  {
    "text": "thought great we'll just hook into that process it's not quite as simple as we thought",
    "start": "647360",
    "end": "652720"
  },
  {
    "text": "this diagram here shows what's actually going on you can see that i've added a third",
    "start": "652720",
    "end": "658880"
  },
  {
    "text": "middle path here which i've entitled pause pods evicted",
    "start": "658880",
    "end": "663920"
  },
  {
    "text": "so what is a potus pod you might be asking well it turns out that for a variety of",
    "start": "663920",
    "end": "670480"
  },
  {
    "text": "reasons we want to be able to maintain additional overhead in our clusters this is useful in the event that we have",
    "start": "670480",
    "end": "676720"
  },
  {
    "text": "a spike in traffic or we have some sort of an outage and we need to fail over a service from one cluster to another",
    "start": "676720",
    "end": "683120"
  },
  {
    "text": "having this overhead gives us the ability to handle those bursts in traffic",
    "start": "683120",
    "end": "690000"
  },
  {
    "text": "seamlessly the way that we maintain this overhead is through something called the cluster",
    "start": "690000",
    "end": "695440"
  },
  {
    "text": "proportional autoscaler which is different than the cluster autoscaler what cluster proportional autoscaler",
    "start": "695440",
    "end": "701040"
  },
  {
    "text": "does is it spins up pause pods a pause pod is exactly what it sounds",
    "start": "701040",
    "end": "706880"
  },
  {
    "text": "like it just sits there on a node doing nothing but it reserves that extra capacity",
    "start": "706880",
    "end": "712639"
  },
  {
    "text": "all the pause pods are marked with a lower priority so we've got our regular priorities and then pause pods have a",
    "start": "712639",
    "end": "718560"
  },
  {
    "text": "lower priority so that when a new pod comes in cluster scheduler will look at that it'll say oh here's a node that has",
    "start": "718560",
    "end": "725920"
  },
  {
    "text": "a lower priority it'll evict that one and it'll make room for the new pod that",
    "start": "725920",
    "end": "732560"
  },
  {
    "text": "just came in we did a little bit of data analysis here and discovered that this middle",
    "start": "732560",
    "end": "739600"
  },
  {
    "text": "path here is taken approximately 98 of the time in our clusters which is great",
    "start": "739600",
    "end": "744720"
  },
  {
    "text": "it means that uh the proportional auto scaler is doing its job",
    "start": "744720",
    "end": "749760"
  },
  {
    "text": "we have enough capacity 98 of the time to handle our bursts and",
    "start": "749760",
    "end": "756000"
  },
  {
    "text": "traffic so what's the problem here well",
    "start": "756000",
    "end": "761519"
  },
  {
    "start": "760000",
    "end": "880000"
  },
  {
    "text": "in a single instance type world the way that we configured things is we applied",
    "start": "761519",
    "end": "767040"
  },
  {
    "text": "a instance type specific node selector to our pause pods so that",
    "start": "767040",
    "end": "772079"
  },
  {
    "text": "means that we're running c5d 9x larges our pause pods could only spin up on c5 d9x larges",
    "start": "772079",
    "end": "781040"
  },
  {
    "text": "this is for a variety of historical reasons and when we moved to a multiple instance",
    "start": "781040",
    "end": "787120"
  },
  {
    "text": "type world that started causing us problems because then we'd create new pause pods for uh m5d whatever whatever instance types",
    "start": "787120",
    "end": "794959"
  },
  {
    "text": "and i'm sure you can see where this is going when the new pod comes in it evicts a pause pod and gets bound to",
    "start": "794959",
    "end": "802480"
  },
  {
    "text": "the host where the pause pod used to be running the pause pod then transitions into a pending state",
    "start": "802480",
    "end": "808399"
  },
  {
    "text": "and cluster auto scaler says aha here's a pod that can't be scheduled let me try",
    "start": "808399",
    "end": "813920"
  },
  {
    "text": "to spin something up to satisfy all of its requests it has this instance type specific node selector on it and",
    "start": "813920",
    "end": "822639"
  },
  {
    "text": "there's only one node group that can satisfy that so cube scheduler doesn't know anything",
    "start": "822639",
    "end": "828639"
  },
  {
    "text": "about uh different instance types it doesn't know anything about",
    "start": "828639",
    "end": "833680"
  },
  {
    "text": "asgs or anything like that it's more or less picking pause pods to evict at random",
    "start": "833680",
    "end": "838720"
  },
  {
    "text": "um and what that means is that cluster autoscaler uh is entirely constrained",
    "start": "838720",
    "end": "847839"
  },
  {
    "text": "in its choices by our pause pods um when a pause pod gets evicted",
    "start": "847839",
    "end": "853839"
  },
  {
    "text": "cluster auto scaler is forced to launch a new node that exactly matches that pause pod",
    "start": "853839",
    "end": "860079"
  },
  {
    "text": "so this was kind of a problem because it meant that we had all of these",
    "start": "860079",
    "end": "865760"
  },
  {
    "text": "interacting control loops we had cluster autoscaler we had cluster proportional auto scaler we had cube scheduler all of",
    "start": "865760",
    "end": "871920"
  },
  {
    "text": "these things were in some sense fighting for control over our cluster composition",
    "start": "871920",
    "end": "877120"
  },
  {
    "text": "and this wasn't great so what do we do about it",
    "start": "877120",
    "end": "882480"
  },
  {
    "start": "880000",
    "end": "910000"
  },
  {
    "text": "on this slide uh i show a slight modification to the previous slide i'm going to highlight the differences here",
    "start": "882639",
    "end": "889199"
  },
  {
    "text": "um we ended up identifying two things that we could change um the first is",
    "start": "889199",
    "end": "894720"
  },
  {
    "text": "that we moved away from these instance type specific pause pods to what i call generic pause pods",
    "start": "894720",
    "end": "900240"
  },
  {
    "text": "i'll explain that in a moment and the second is that we built a custom expander plugin",
    "start": "900240",
    "end": "905600"
  },
  {
    "text": "for cluster autoscaler so let's dive into what each of those are",
    "start": "905600",
    "end": "911680"
  },
  {
    "start": "910000",
    "end": "1060000"
  },
  {
    "text": "so as you might expect uh the solution for pause pods is pretty straightforward you",
    "start": "911680",
    "end": "917440"
  },
  {
    "text": "just remove the instance type specific node selector so now we have what we call generic pause",
    "start": "917440",
    "end": "922959"
  },
  {
    "text": "pods they can run anywhere this is great um in terms of sizing uh",
    "start": "922959",
    "end": "928320"
  },
  {
    "text": "previously we had the pause pod just take up the entire node uh now",
    "start": "928320",
    "end": "933519"
  },
  {
    "text": "we're going to size the pause pods to be the smallest of the instance types in our cluster so",
    "start": "933519",
    "end": "940320"
  },
  {
    "text": "if we've got node types that have 32 cpus we've got node types that have 64 cpus",
    "start": "940320",
    "end": "946240"
  },
  {
    "text": "we'll size the pause pods to request 32 cpus this means that well maybe",
    "start": "946240",
    "end": "953120"
  },
  {
    "text": "a bigger node might have multiple pause pods running on it but that's fine we can tweak our",
    "start": "953120",
    "end": "959360"
  },
  {
    "text": "overhead parameters to accommodate that now the only sort of remaining concern",
    "start": "959360",
    "end": "966720"
  },
  {
    "text": "here is what do we do about our slas around scheduling as you recall",
    "start": "966720",
    "end": "972639"
  },
  {
    "text": "before 98 of the time we're evicting these pause pods in order to make room for new workloads that are coming in and",
    "start": "972639",
    "end": "980079"
  },
  {
    "text": "what that meant is that most of the time our services are getting scheduled in one to two seconds",
    "start": "980079",
    "end": "986320"
  },
  {
    "text": "and we really wanted to make sure that as we move into this multi multiple instance type world that we're able to",
    "start": "986320",
    "end": "993040"
  },
  {
    "text": "maintain that and this becomes a problem if we start",
    "start": "993040",
    "end": "998079"
  },
  {
    "text": "consolidating like our gpu clusters or some high memory workloads we try to mash all of those different things into",
    "start": "998079",
    "end": "1005120"
  },
  {
    "text": "a single cluster while you might have a job that comes along that's requesting 500 gigs of ram and our pause pods none",
    "start": "1005120",
    "end": "1013040"
  },
  {
    "text": "of them request 500 gigs of ram and so it might have to wait that job might",
    "start": "1013040",
    "end": "1018639"
  },
  {
    "text": "have to wait you know five minutes or 10 minutes or however long it takes for a new node to come up and join the cluster",
    "start": "1018639",
    "end": "1025520"
  },
  {
    "text": "and this could be a regression in performance so the solution that we've identified",
    "start": "1025520",
    "end": "1031280"
  },
  {
    "text": "this is a piece that we haven't implemented yet is that for jobs like",
    "start": "1031280",
    "end": "1037360"
  },
  {
    "text": "gpu workloads or high memory workloads we're going to build in a shadow capacity mechanism so that if",
    "start": "1037360",
    "end": "1045280"
  },
  {
    "text": "you've got a specific service that has a unusual resource request",
    "start": "1045280",
    "end": "1051120"
  },
  {
    "text": "essentially you're going to get pause pods that are specific to that service so that you can maintain those",
    "start": "1051120",
    "end": "1056880"
  },
  {
    "text": "scheduling guarantees all right so let's move on to the second",
    "start": "1056880",
    "end": "1062480"
  },
  {
    "start": "1060000",
    "end": "1279000"
  },
  {
    "text": "piece that we are introducing this is our custom expander plugin this is in progress we have a really basic proof of",
    "start": "1062480",
    "end": "1069039"
  },
  {
    "text": "concept right now but we expect this is going to get a lot more complex in the future",
    "start": "1069039",
    "end": "1074880"
  },
  {
    "text": "what we observed is that cluster autoscaler is performing a lot of common tasks and we really want to keep all of",
    "start": "1074880",
    "end": "1080559"
  },
  {
    "text": "that functionality in cluster autoscaler it does things like talk to the cloud provider it runs this simulation where it",
    "start": "1080559",
    "end": "1087120"
  },
  {
    "text": "determines all of the different places that a pod could go does a lot of really nice things for us",
    "start": "1087120",
    "end": "1093600"
  },
  {
    "text": "what it doesn't do necessarily is capture the business logic that we want",
    "start": "1093600",
    "end": "1099440"
  },
  {
    "text": "to determine what node types to spin up so how does cluster autoscaler actually",
    "start": "1099440",
    "end": "1104880"
  },
  {
    "text": "pick a node type to spin up well first it does this filtering step where it identifies all of the node groups that",
    "start": "1104880",
    "end": "1112240"
  },
  {
    "text": "potentially could run the incoming pod then it passes those node groups off to what it calls an expander",
    "start": "1112240",
    "end": "1119039"
  },
  {
    "text": "there are a number of expanders that are built in there's one that's just the random expander it's going to pick one",
    "start": "1119039",
    "end": "1124160"
  },
  {
    "text": "of those node groups at random the one that we use right now is called the priority expander so you rank all of",
    "start": "1124160",
    "end": "1129760"
  },
  {
    "text": "your node groups by priority and it picks the highest",
    "start": "1129760",
    "end": "1135200"
  },
  {
    "text": "available priority it's got a few others but what we",
    "start": "1135200",
    "end": "1140960"
  },
  {
    "text": "predict and what we're already seeing is that we're going to want even more control over the node groups that get",
    "start": "1140960",
    "end": "1147440"
  },
  {
    "text": "selected what we didn't want to have to happen is that we maintain a fork of cluster",
    "start": "1147440",
    "end": "1153200"
  },
  {
    "text": "autoscaler that has all this business logic and we wanted to be good uh",
    "start": "1153200",
    "end": "1158880"
  },
  {
    "text": "cncf citizens and be able to contribute some stuff back upstream and uh",
    "start": "1158880",
    "end": "1164559"
  },
  {
    "text": "we also wanted to make our lives easier and it turns out that i think with this custom expander plugin we can do all",
    "start": "1164559",
    "end": "1169760"
  },
  {
    "text": "three of those things so what we're going to do is patch cluster auto scaler so that it",
    "start": "1169760",
    "end": "1176559"
  },
  {
    "text": "can communicate over grpc with a separate process and that separate process is going to",
    "start": "1176559",
    "end": "1182960"
  },
  {
    "text": "encode all of our expander logic it's going to be all of the business logic around what instance",
    "start": "1182960",
    "end": "1188720"
  },
  {
    "text": "types we currently want to run as evan mentioned we want to",
    "start": "1188720",
    "end": "1194240"
  },
  {
    "text": "start dipping our toes in the spot world and so this custom expander can do things like",
    "start": "1194240",
    "end": "1200720"
  },
  {
    "text": "look at all of the prices for all of the spot markets that we're in and let's maybe pick one that's cheap right now or",
    "start": "1200720",
    "end": "1206960"
  },
  {
    "text": "that we think has a lot of availability um it can do things like uh",
    "start": "1206960",
    "end": "1212559"
  },
  {
    "text": "well we've got a lot of extra uh you know we've got a whole bunch of um",
    "start": "1212559",
    "end": "1220640"
  },
  {
    "text": "batch jobs that are going to be requesting a bunch of memory so maybe",
    "start": "1220640",
    "end": "1225760"
  },
  {
    "text": "for like bin packing reasons we should request some instances that have more memory or something like that all of",
    "start": "1225760",
    "end": "1232000"
  },
  {
    "text": "that business logic can get encoded in this expander that is separate from cluster auto scaler itself this is nice",
    "start": "1232000",
    "end": "1239039"
  },
  {
    "text": "for a couple of reasons it means that a we're not maintaining a fork of",
    "start": "1239039",
    "end": "1245120"
  },
  {
    "text": "cluster autoscaler and b we can upgrade the two out of band um we don't have to",
    "start": "1245120",
    "end": "1250880"
  },
  {
    "text": "if we if our aws contract changes and we need to fix some of that business logic we",
    "start": "1250880",
    "end": "1256080"
  },
  {
    "text": "don't have to deploy a new version of cluster auto scaler we can just deploy a new version of this service that we're running so like i mentioned this is in",
    "start": "1256080",
    "end": "1263280"
  },
  {
    "text": "progress uh we have a work in progress pr that i'll link at the end for this",
    "start": "1263280",
    "end": "1268480"
  },
  {
    "text": "expander but this is going to really be where we control the composition of our clusters",
    "start": "1268480",
    "end": "1276320"
  },
  {
    "text": "so that i'm going to hand it over to evan to talk a little bit more about some of the organizational issues we've",
    "start": "1276320",
    "end": "1281919"
  },
  {
    "start": "1279000",
    "end": "1299000"
  },
  {
    "text": "encountered yep thanks david yeah so david covered some of the",
    "start": "1281919",
    "end": "1288080"
  },
  {
    "text": "technical issues this section will be a bit more around",
    "start": "1288080",
    "end": "1293520"
  },
  {
    "text": "the organizational issues this chapter is titled in which customers are concerned",
    "start": "1293520",
    "end": "1300559"
  },
  {
    "start": "1299000",
    "end": "1331000"
  },
  {
    "text": "so because we're coming from single instance clusters uh service owners had",
    "start": "1300559",
    "end": "1305919"
  },
  {
    "text": "the expectation of identical pod performance across different machines",
    "start": "1305919",
    "end": "1311360"
  },
  {
    "text": "another issue we thought of was that [Music] our our service owners have a lot of",
    "start": "1311360",
    "end": "1317600"
  },
  {
    "text": "migration fatigue uh we currently have nearly a thousand services and with",
    "start": "1317600",
    "end": "1323039"
  },
  {
    "text": "every infrastructure config change leads to migration and every migration causes uh wastes a",
    "start": "1323039",
    "end": "1330080"
  },
  {
    "text": "lot of engineering time our first solution here was talking to people",
    "start": "1330080",
    "end": "1336080"
  },
  {
    "start": "1331000",
    "end": "1419000"
  },
  {
    "text": "we sent out surveys and talked with a lot of customer teams we realized here there were actually",
    "start": "1336080",
    "end": "1342159"
  },
  {
    "text": "quite a lot of nebulous concerns around performance without the teams knowing",
    "start": "1342159",
    "end": "1347760"
  },
  {
    "text": "exactly what requirements they had for their service around performance and most of them",
    "start": "1347760",
    "end": "1353600"
  },
  {
    "text": "hadn't really done any performance testing or performance tuning either they only came to us with sort of",
    "start": "1353600",
    "end": "1360720"
  },
  {
    "text": "like relative comparisons to their current performance whatever that performance may be",
    "start": "1360720",
    "end": "1366480"
  },
  {
    "text": "um out of these talks we created an extensible api",
    "start": "1366480",
    "end": "1371600"
  },
  {
    "text": "out of which was kind of two-fold it allows customers to specify",
    "start": "1371600",
    "end": "1376799"
  },
  {
    "text": "specific instance types if their workloads are actually super sensitive to to the different underlying node",
    "start": "1376799",
    "end": "1384080"
  },
  {
    "text": "types and it is also opt in which solves the",
    "start": "1384080",
    "end": "1389600"
  },
  {
    "text": "um or the the default is opted in meaning service owners would have to opt out um this solves a lot of the",
    "start": "1389600",
    "end": "1396000"
  },
  {
    "text": "migration issues because we expect this to work for all almost all services",
    "start": "1396000",
    "end": "1403200"
  },
  {
    "text": "it also allows for additive requirements when service owners are describing the",
    "start": "1403200",
    "end": "1408799"
  },
  {
    "text": "instance types they want and is therefore extensible for our future work",
    "start": "1408799",
    "end": "1414799"
  },
  {
    "text": "like i said before like utilizing spot instances and others",
    "start": "1414799",
    "end": "1419840"
  },
  {
    "start": "1419000",
    "end": "1486000"
  },
  {
    "text": "um to try to address customer concerns uh we did a bit of performance testing for",
    "start": "1419840",
    "end": "1426720"
  },
  {
    "text": "these new instance types we started with running some benchmark tests so we ran a",
    "start": "1426720",
    "end": "1432640"
  },
  {
    "text": "variety of sith sysbench synthetic tests and found performance uh",
    "start": "1432640",
    "end": "1439360"
  },
  {
    "text": "with uh between the instance types to be within 20 but obviously this isn't good enough for",
    "start": "1439360",
    "end": "1446080"
  },
  {
    "text": "production workloads we needed to use real workloads to test this so we took services test environments",
    "start": "1446080",
    "end": "1453919"
  },
  {
    "text": "with production traffic replay scheduled them onto different uh",
    "start": "1453919",
    "end": "1459840"
  },
  {
    "text": "empty nodes of the uh all the instance types we were testing uh to compare the",
    "start": "1459840",
    "end": "1464880"
  },
  {
    "text": "latency metrics and we did this for a representative set of service profiles that we have this",
    "start": "1464880",
    "end": "1472080"
  },
  {
    "text": "was across different languages so like java ruby and node latency requirements slos",
    "start": "1472080",
    "end": "1479679"
  },
  {
    "text": "and across different workload types so we had some like cpu intensive services we had some i o bounce services",
    "start": "1479679",
    "end": "1487840"
  },
  {
    "text": "here's an example of a very latency sensitive service that we have",
    "start": "1488240",
    "end": "1493919"
  },
  {
    "start": "1493000",
    "end": "1522000"
  },
  {
    "text": "the different colors here orange and purple represent the different",
    "start": "1493919",
    "end": "1499039"
  },
  {
    "text": "instance types and each grouping is the p95 and p99 of the latencies",
    "start": "1499039",
    "end": "1505039"
  },
  {
    "text": "you can see they're like really really similar we've actually found this to be the case",
    "start": "1505039",
    "end": "1510640"
  },
  {
    "text": "for all of our services and all end points we we found that",
    "start": "1510640",
    "end": "1516080"
  },
  {
    "text": "performance was within 10 which we deemed acceptable enough to move forward",
    "start": "1516080",
    "end": "1522720"
  },
  {
    "start": "1522000",
    "end": "1549000"
  },
  {
    "text": "um and yeah so as we started roll out we found quite a number of very very",
    "start": "1522720",
    "end": "1529600"
  },
  {
    "text": "technical issues with cluster autoscaler we don't have time to cover them all in this presentation but",
    "start": "1529600",
    "end": "1536720"
  },
  {
    "text": "we'll deep dive on one of them and then link the bug fixes and issues we filed",
    "start": "1536720",
    "end": "1542000"
  },
  {
    "text": "upstream at the end and i believe these slides are uploaded so you all can take a look if you're interested",
    "start": "1542000",
    "end": "1549520"
  },
  {
    "start": "1549000",
    "end": "1574000"
  },
  {
    "text": "um so yeah the problem we saw was that during our rollout we observed",
    "start": "1549520",
    "end": "1555200"
  },
  {
    "text": "that certain asgs weren't launching this was kind of strange because the",
    "start": "1555200",
    "end": "1561039"
  },
  {
    "text": "logs when we checked them showed pod topology spread failures",
    "start": "1561039",
    "end": "1566400"
  },
  {
    "text": "if you're not familiar pod topology spread all it does is guarantee that pods are spread evenly across availability zones",
    "start": "1566400",
    "end": "1575200"
  },
  {
    "start": "1574000",
    "end": "1609000"
  },
  {
    "text": "here's a screenshot of the logs you can see if that's not too small that the",
    "start": "1575200",
    "end": "1580840"
  },
  {
    "text": "m5d12 xl asg's in us east 1b",
    "start": "1580840",
    "end": "1586080"
  },
  {
    "text": "fails pod topology spread constraints and then later on in the logs",
    "start": "1586080",
    "end": "1592480"
  },
  {
    "text": "the c5 ad-12 xl in the same zone ends up succeeding and uh being chosen to scale",
    "start": "1592480",
    "end": "1599600"
  },
  {
    "text": "up um this is really weird because it's exactly the opposite of what pod",
    "start": "1599600",
    "end": "1605279"
  },
  {
    "text": "topology spread is uh trying to do um",
    "start": "1605279",
    "end": "1611039"
  },
  {
    "text": "and then this graph shows our uh gradual introduction of these new instance types",
    "start": "1611039",
    "end": "1616080"
  },
  {
    "text": "to one of our clusters um you can see these c580 12xls in the top in blue",
    "start": "1616080",
    "end": "1623200"
  },
  {
    "text": "gradually taking up a larger and larger proportion during our",
    "start": "1623200",
    "end": "1628240"
  },
  {
    "text": "rollout but c5 ads 16 xls and m5ds aren't launching",
    "start": "1628240",
    "end": "1634400"
  },
  {
    "text": "at all so after a while we realized that the",
    "start": "1634400",
    "end": "1642080"
  },
  {
    "start": "1635000",
    "end": "1666000"
  },
  {
    "text": "asus that weren't launching were empty asgs",
    "start": "1642080",
    "end": "1646960"
  },
  {
    "text": "in this case it was the yeah the m5d ones and the c580 16xls",
    "start": "1647200",
    "end": "1654000"
  },
  {
    "text": "and this was kind of strange to us because both of those asgs happen to be the very top priority in our expander",
    "start": "1654000",
    "end": "1661200"
  },
  {
    "text": "ladder and we'd expect them to launch before the other ones",
    "start": "1661200",
    "end": "1667200"
  },
  {
    "start": "1666000",
    "end": "1675000"
  },
  {
    "text": "so this was kind of strange we did quite a lot of code digging and we",
    "start": "1667200",
    "end": "1672399"
  },
  {
    "text": "we learned a couple things about cluster auto scalar so what's going on um",
    "start": "1672399",
    "end": "1679679"
  },
  {
    "text": "we have a cluster here with the four asgs",
    "start": "1679679",
    "end": "1685039"
  },
  {
    "text": "two of which are populated the c5d 18xls and the c5 ad-12xls",
    "start": "1685039",
    "end": "1690960"
  },
  {
    "text": "and then the two on the right are empty and we also have cluster auto scaler",
    "start": "1690960",
    "end": "1696880"
  },
  {
    "text": "which adds nodes to a cluster when incoming pods come in and there's no capacity",
    "start": "1696880",
    "end": "1702960"
  },
  {
    "text": "um cluster autoscaler undergoes a scheduling simulation when this happens",
    "start": "1702960",
    "end": "1709440"
  },
  {
    "text": "and looks at possible node types to expand and sees uh if the node types will fit",
    "start": "1709440",
    "end": "1717200"
  },
  {
    "text": "the scheduling requirements of the incoming pod so how does this scheduling simulator",
    "start": "1717200",
    "end": "1724480"
  },
  {
    "start": "1720000",
    "end": "1804000"
  },
  {
    "text": "simulation work when evaluating if a pod will fit",
    "start": "1724480",
    "end": "1729520"
  },
  {
    "text": "well it looks at the available node types it has in this case the four on the bottom",
    "start": "1729520",
    "end": "1736799"
  },
  {
    "text": "and it creates these fake nodes that populate um node info objects for each",
    "start": "1737600",
    "end": "1743520"
  },
  {
    "text": "type of node and it sees if this fake node will fit the scheduling constraint",
    "start": "1743520",
    "end": "1750559"
  },
  {
    "text": "um but that begs the question how does it populate this nodeinfo object",
    "start": "1750559",
    "end": "1756559"
  },
  {
    "text": "well it turns out there are two paths uh for asgs that have live nodes in the cluster it copies the node info directly",
    "start": "1756559",
    "end": "1763360"
  },
  {
    "text": "from existing nodes in this case the two on the left",
    "start": "1763360",
    "end": "1768559"
  },
  {
    "text": "but for the ones that are empty we have these things called asg templates which",
    "start": "1768559",
    "end": "1775440"
  },
  {
    "text": "whenever cluster autoscaler chooses to launch a new node it takes this asg template which describes the node config",
    "start": "1775440",
    "end": "1782799"
  },
  {
    "text": "before cubelet starts on a node",
    "start": "1782799",
    "end": "1787760"
  },
  {
    "text": "this seems fine right but i mean in most cases it is fine",
    "start": "1787919",
    "end": "1794799"
  },
  {
    "text": "but kibla actually applies quite a lot of labels and annotations to a node on startup",
    "start": "1794799",
    "end": "1800320"
  },
  {
    "text": "including architecture os",
    "start": "1800320",
    "end": "1805520"
  },
  {
    "start": "1804000",
    "end": "1819000"
  },
  {
    "text": "the host name the instance type name but then most importantly for the scenario the topology key topology zone key",
    "start": "1805520",
    "end": "1815440"
  },
  {
    "text": "this means that asg templates don't have that topology zone key",
    "start": "1815440",
    "end": "1820799"
  },
  {
    "start": "1819000",
    "end": "1877000"
  },
  {
    "text": "which explain the behavior we were seeing because the fake notes that were copied",
    "start": "1820799",
    "end": "1827440"
  },
  {
    "text": "from the asg templates don't have the topology zone key the ones from the existing nodes do have that key because",
    "start": "1827440",
    "end": "1832720"
  },
  {
    "text": "cube has already started on those existing nodes and thus we see our",
    "start": "1832720",
    "end": "1840559"
  },
  {
    "text": "behavior in the logs where the existing nodes even though it's a lower priority than the",
    "start": "1840559",
    "end": "1847279"
  },
  {
    "text": "than the other ones that were empty is chosen our short-term fix for this is to just",
    "start": "1847279",
    "end": "1854159"
  },
  {
    "text": "manually add the topology zone key to the asg template this is pretty counter-intuitive behavior because you",
    "start": "1854159",
    "end": "1860320"
  },
  {
    "text": "wouldn't expect to have to add that because cubelet does it for you the long-term fix here",
    "start": "1860320",
    "end": "1867600"
  },
  {
    "text": "is to modify clusterautoscaler to automatically add all of those cubelet tags",
    "start": "1867600",
    "end": "1874080"
  },
  {
    "text": "to the asg templates when it's evaluating um yeah so david you want to talk about",
    "start": "1874080",
    "end": "1880559"
  },
  {
    "text": "what we learned sure",
    "start": "1880559",
    "end": "1885360"
  },
  {
    "text": "so let's see what did we learn",
    "start": "1888320",
    "end": "1893840"
  },
  {
    "text": "testing there we go cool yeah sorry about that thanks evan um what did we learn from this whole process well the",
    "start": "1899600",
    "end": "1906240"
  },
  {
    "start": "1905000",
    "end": "1913000"
  },
  {
    "text": "short summary is that heterogeneous clusters are really great uh we should do more of those",
    "start": "1906240",
    "end": "1911519"
  },
  {
    "text": "um but you might be asking what's so great about them well i think the biggest benefit that we've gotten from",
    "start": "1911519",
    "end": "1917519"
  },
  {
    "start": "1913000",
    "end": "1977000"
  },
  {
    "text": "all of the work we've done so far we're still kind of in a half-finished state and we're already seeing this benefit is",
    "start": "1917519",
    "end": "1923279"
  },
  {
    "text": "it gives us a huge amount of flexibility in our team we've gone from as evan mentioned like",
    "start": "1923279",
    "end": "1928880"
  },
  {
    "text": "taking years to migrate from older generations of ec2 instances onto",
    "start": "1928880",
    "end": "1934720"
  },
  {
    "text": "the most recent ones to being able to do that with a single",
    "start": "1934720",
    "end": "1939919"
  },
  {
    "text": "config change in a couple of hours even",
    "start": "1939919",
    "end": "1945360"
  },
  {
    "text": "earlier this year it took you know several weeks to even just be able to add a new instance type into our cluster",
    "start": "1945360",
    "end": "1953840"
  },
  {
    "text": "and again that's now a couple hour task at most so it gives us a huge amount of flexibility to respond to changes in the",
    "start": "1953840",
    "end": "1962880"
  },
  {
    "text": "in our demands in the cloud provider markets um you name it uh we can now",
    "start": "1962880",
    "end": "1969200"
  },
  {
    "text": "handle all of these different uh changes without really overburdening our team so",
    "start": "1969200",
    "end": "1975279"
  },
  {
    "text": "that's great um second benefit is also this allows us to",
    "start": "1975279",
    "end": "1980559"
  },
  {
    "start": "1977000",
    "end": "1990000"
  },
  {
    "text": "future proof our systems a lot more um and this really boils down to just being able to add new instance types as aws",
    "start": "1980559",
    "end": "1988799"
  },
  {
    "text": "makes those available the third thing is really around cost um",
    "start": "1988799",
    "end": "1994320"
  },
  {
    "start": "1990000",
    "end": "2027000"
  },
  {
    "text": "historically we just picked an instance type and went with it because that seemed like",
    "start": "1994320",
    "end": "1999360"
  },
  {
    "text": "the thing to do now we can actually like critically analyze are we",
    "start": "1999360",
    "end": "2004480"
  },
  {
    "text": "getting the most bang for our buck with the instance types that we have in our cluster um evan and i have hinted",
    "start": "2004480",
    "end": "2010559"
  },
  {
    "text": "several times that we want to also start incorporating spot instances and all kinds of other stuff we're investigating",
    "start": "2010559",
    "end": "2016080"
  },
  {
    "text": "amd we're investigating graviton all of these different things we're now going to have the capability to",
    "start": "2016080",
    "end": "2022159"
  },
  {
    "text": "make these trade-off decisions um where we wouldn't be able to do that before",
    "start": "2022159",
    "end": "2027919"
  },
  {
    "start": "2027000",
    "end": "2063000"
  },
  {
    "text": "so what's next um we still have a number of changes to cluster autoscaler that we're trying to",
    "start": "2027919",
    "end": "2033440"
  },
  {
    "text": "get upstreamed i'll have a list of that a list of those on the next slide we're still in the process of rolling",
    "start": "2033440",
    "end": "2039120"
  },
  {
    "text": "this out as well like i mentioned some of these things are still in progress um we only have a",
    "start": "2039120",
    "end": "2046640"
  },
  {
    "text": "handful of our you know 50 60 odd clusters running uh in a multi-instance",
    "start": "2046640",
    "end": "2051919"
  },
  {
    "text": "type setting right now so we want to roll this out to the rest of airbnb and then our next big project after that",
    "start": "2051919",
    "end": "2057760"
  },
  {
    "text": "is running on spot instances this was really a prerequisite to that",
    "start": "2057760",
    "end": "2063440"
  },
  {
    "start": "2063000",
    "end": "2080000"
  },
  {
    "text": "and yeah so this is a list of existing prs and issues that we filed against",
    "start": "2063440",
    "end": "2069280"
  },
  {
    "text": "cluster autoscaler these are on the slides that are uploaded i think there's going to be like one or two more that are coming",
    "start": "2069280",
    "end": "2076320"
  },
  {
    "text": "that we just discovered in the last couple weeks um and with that i think i will conclude i",
    "start": "2076320",
    "end": "2083839"
  },
  {
    "start": "2080000",
    "end": "2090000"
  },
  {
    "text": "want to thank everybody again for attending um i'm happy to take any questions or evan any questions that you",
    "start": "2083839",
    "end": "2090398"
  },
  {
    "start": "2090000",
    "end": "2105000"
  },
  {
    "text": "all have um and just want to put out a shameless plug that we are hiring so if any of this stuff sounds interesting to",
    "start": "2090399",
    "end": "2096960"
  },
  {
    "text": "work on uh you can come talk to us after the presentation so thanks very much",
    "start": "2096960",
    "end": "2102390"
  },
  {
    "text": "[Applause]",
    "start": "2102390",
    "end": "2107730"
  }
]