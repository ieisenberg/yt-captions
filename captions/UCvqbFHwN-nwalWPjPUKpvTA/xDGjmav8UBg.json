[
  {
    "text": "okay hi everyone i hope you can hear me all the way in the back so my name is henrik i'm the moderator for these",
    "start": "240",
    "end": "5520"
  },
  {
    "text": "sessions we'll get started here in just a second just want to remind you uh once the session concludes we'll have a",
    "start": "5520",
    "end": "12400"
  },
  {
    "text": "microphone here in the middle for questions so please line up and ask any questions you might have i'll try and",
    "start": "12400",
    "end": "17600"
  },
  {
    "text": "get a couple of questions from the the virtual meeting room as well make sure those guys are included",
    "start": "17600",
    "end": "24240"
  },
  {
    "text": "and after the session please don't forget to to rate the session and if you have additional questions",
    "start": "24240",
    "end": "30640"
  },
  {
    "text": "after the session concludes and we're out of time please continue those discussions out in the hall so we can",
    "start": "30640",
    "end": "37120"
  },
  {
    "text": "clear the room and prepare for the next session and with that i'm going to leave it over to you thanks",
    "start": "37120",
    "end": "43200"
  },
  {
    "text": "good morning everybody can you give me a thumbs up if i'm",
    "start": "43200",
    "end": "49200"
  },
  {
    "text": "perfectly audible great okay so this is the title of our talk it was originally the subtitle was the",
    "start": "49200",
    "end": "56160"
  },
  {
    "text": "title of our talk but our pr person about had an aneurysm when they saw",
    "start": "56160",
    "end": "62000"
  },
  {
    "text": "that i was submitting that talk but i explained to her first of all",
    "start": "62000",
    "end": "68080"
  },
  {
    "text": "everybody at least all of our customers know we had an outage and second of all",
    "start": "68080",
    "end": "73119"
  },
  {
    "text": "everybody has outages it's not news when a sas provider has an outage so we",
    "start": "73119",
    "end": "78960"
  },
  {
    "text": "wanted to come here and share our experience with the community to maybe help you avoid an outage or deal with it",
    "start": "78960",
    "end": "86080"
  },
  {
    "text": "when it comes up so my name is rick i'm currently the vp",
    "start": "86080",
    "end": "92159"
  },
  {
    "text": "of product at influx data at the time of this incident i was the vp of engineering for the platform team",
    "start": "92159",
    "end": "99600"
  },
  {
    "text": "so i was there during the the day that we deleted and restored one of our",
    "start": "99600",
    "end": "106000"
  },
  {
    "text": "production environments and hi my name is wojciech i'm a platform engineer at the deployments",
    "start": "106000",
    "end": "111600"
  },
  {
    "text": "team at influx data and i've been involved in many parts of the incidents and follow-ups including post-mortem and",
    "start": "111600",
    "end": "118479"
  },
  {
    "text": "fixes around what broke and how we make sure that it doesn't happen again",
    "start": "118479",
    "end": "124399"
  },
  {
    "text": "okay so who are we why are we using kubernetes why might our experience be relevant",
    "start": "124560",
    "end": "132080"
  },
  {
    "text": "so at influx data we view ourselves as a development platform for writing time",
    "start": "132080",
    "end": "138319"
  },
  {
    "text": "series applications we don't really view ourselves as a kubernetes tools vendor",
    "start": "138319",
    "end": "144400"
  },
  {
    "text": "that said a lot of our customers use us to monitor their kubernetes clusters and",
    "start": "144400",
    "end": "149920"
  },
  {
    "text": "then we have different companies that have actually built sas solutions for monitoring kubernetes",
    "start": "149920",
    "end": "156720"
  },
  {
    "text": "on top of us so while we are an application development platform for time series at",
    "start": "156720",
    "end": "162879"
  },
  {
    "text": "the heart is a database called influx db it's an open source database that's",
    "start": "162879",
    "end": "168640"
  },
  {
    "text": "purpose built for time series but currently our flagship product is called",
    "start": "168640",
    "end": "173840"
  },
  {
    "text": "influx db cloud and this is a multi-tenant sas solution it's built on top of kubernetes and the",
    "start": "173840",
    "end": "180159"
  },
  {
    "text": "reason it's built on top of kubernetes is because we offer it on the top three",
    "start": "180159",
    "end": "185200"
  },
  {
    "text": "clouds and multiple regions in all those clouds so i think we have",
    "start": "185200",
    "end": "190480"
  },
  {
    "text": "like 12 or 15 production instances running around the world right now that we manage and kubernetes provided",
    "start": "190480",
    "end": "197519"
  },
  {
    "text": "that cloud abstraction layer that we needed to be able to manage the same application in all of those different",
    "start": "197519",
    "end": "204080"
  },
  {
    "text": "regions and clouds right so the timeline which is basically",
    "start": "204080",
    "end": "209360"
  },
  {
    "text": "how to delete your production in a few easy steps so let's as spring mention in fact the cloud is a",
    "start": "209360",
    "end": "215519"
  },
  {
    "text": "flagship product it's a stateful kubernetes based application and we use github's ncicd to keep it running and",
    "start": "215519",
    "end": "221920"
  },
  {
    "text": "keep it up to date we have multiple tiers in the application some of them are",
    "start": "221920",
    "end": "228319"
  },
  {
    "text": "stateful most of them are stateless so one of the key things that we have is the storage tier that's using pvcs and",
    "start": "228319",
    "end": "235040"
  },
  {
    "text": "keeps data in in kubernetes native volumes also using cloud native objects are like",
    "start": "235040",
    "end": "240400"
  },
  {
    "text": "s3 for long-term persistence but the key is that it's keeping the data so it can be queryable readable writable really",
    "start": "240400",
    "end": "247760"
  },
  {
    "text": "fast on disk then we use kafka and zookeeper for rider headlock meaning as soon as we",
    "start": "247760",
    "end": "253040"
  },
  {
    "text": "get the request to add new data it goes into kafka and then storage tier processes that as",
    "start": "253040",
    "end": "258560"
  },
  {
    "text": "soon as possible at the time of the incident we used cd for all of the metadata that we had so",
    "start": "258560",
    "end": "264240"
  },
  {
    "text": "things like bucket names which is where we put the data organizations users all of the all of the metadata for the",
    "start": "264240",
    "end": "270240"
  },
  {
    "text": "application right now moving away from that from that for postgres for various reasons but at the time it was at ce and",
    "start": "270240",
    "end": "275680"
  },
  {
    "text": "like i said we have multiple stateless micro services so when you run a query against influx db",
    "start": "275680",
    "end": "281440"
  },
  {
    "text": "it goes to our query engine which is entirely stateless it parses the query it sends the commands to proper storage",
    "start": "281440",
    "end": "286880"
  },
  {
    "text": "tier parts by knowing which charts to ask for the specific data gets it back reconstructs",
    "start": "286880",
    "end": "292639"
  },
  {
    "text": "it returns the data and most of our most of our components are are stateless and",
    "start": "292639",
    "end": "298240"
  },
  {
    "text": "we use argo cd for deploying all of our instances of infrared db cloud so we have mentioned 12 to 15 production ones",
    "start": "298240",
    "end": "305199"
  },
  {
    "text": "we have multiple staging or testing ones and we use githubs to manage all of them",
    "start": "305199",
    "end": "310240"
  },
  {
    "text": "we use ci cd to get everything built as soon as the code changes and see obviously to deploy them",
    "start": "310240",
    "end": "315919"
  },
  {
    "text": "we use argo cd's feature of auto sync which means that as soon as something shows up in our github config repository",
    "start": "315919",
    "end": "322639"
  },
  {
    "text": "it gets deployed and we also use prone which means if something was in that repository and it's no longer there",
    "start": "322639",
    "end": "328960"
  },
  {
    "text": "it'll get deleted and i think you'll know where this is going and now we use something called the app of apps pattern",
    "start": "328960",
    "end": "334880"
  },
  {
    "text": "which means we use argo cd to configure how and where argo cd deploys its things which is also",
    "start": "334880",
    "end": "341120"
  },
  {
    "text": "important so how it all started i'm suspecting this may be a bit of the focus but that's",
    "start": "341120",
    "end": "348240"
  },
  {
    "text": "actually by design so this pr was merged and this is basically just adding new",
    "start": "348240",
    "end": "353520"
  },
  {
    "text": "data there's like 500 new lines most of it generated yaml files it did not remove any single thing",
    "start": "353520",
    "end": "360400"
  },
  {
    "text": "but as soon as it got married within minutes we deleted all of our our entire single production environment",
    "start": "360400",
    "end": "367919"
  },
  {
    "text": "so i'll show what happened still out of focus but there's also by design so",
    "start": "367919",
    "end": "373520"
  },
  {
    "text": "what happened is we use our codename idp for the production so the thing we want",
    "start": "373520",
    "end": "379199"
  },
  {
    "text": "to keep running in all each of our clusters and we also have an open source project called ios that we wanted to deploy",
    "start": "379199",
    "end": "385840"
  },
  {
    "text": "alongside but what happens but you may not even see it because it's a tiny detail is",
    "start": "385840",
    "end": "391199"
  },
  {
    "text": "that we have a naming collision so if so the ios was deployed as idp",
    "start": "391199",
    "end": "396240"
  },
  {
    "text": "and you can see that the first arrow points that it should be iot and it's idp i'll show it again in a bit in a",
    "start": "396240",
    "end": "402319"
  },
  {
    "text": "better picture the point of this is it was really difficult to spot in code review same as it may be difficult to",
    "start": "402319",
    "end": "408240"
  },
  {
    "text": "spot for you now so what we wanted to how close how about argo cd works and what we wanted to get",
    "start": "408240",
    "end": "414720"
  },
  {
    "text": "to so on the left side we have the apps of apps button just for one cluster we have that for all of our clusters but just",
    "start": "414720",
    "end": "420880"
  },
  {
    "text": "for one of it what we had initially was our idp is our production environment",
    "start": "420880",
    "end": "426639"
  },
  {
    "text": "and there was an argo cd cargo city has its custom resource called application that defines",
    "start": "426639",
    "end": "432560"
  },
  {
    "text": "that has a name defines the locations of the git repository to get the definitions from",
    "start": "432560",
    "end": "438560"
  },
  {
    "text": "and then where it should be deployed to and also the path in that git repository so",
    "start": "438560",
    "end": "443759"
  },
  {
    "text": "then that repository is specified like a namespace and defines all the all the deployment stateful sets anything else",
    "start": "443759",
    "end": "449199"
  },
  {
    "text": "that you want deployed as part of that application and then what we wanted to do is wanted to add iocs alongside",
    "start": "449199",
    "end": "456080"
  },
  {
    "text": "so we added a new argo cd application we or we wanted to add a new organic application we wanted that to point to a",
    "start": "456080",
    "end": "462639"
  },
  {
    "text": "different path in the repository that would have all the objects for ios but",
    "start": "462639",
    "end": "467840"
  },
  {
    "text": "what really happened is because we had a typo and it was idp and",
    "start": "467840",
    "end": "473039"
  },
  {
    "text": "now it's on the left side in the middle in red because it was idp instead of iox which is what we wanted argot cd went in",
    "start": "473039",
    "end": "480319"
  },
  {
    "text": "and said okay i have two definitions for the idp cd application so i'm going to take the",
    "start": "480319",
    "end": "486400"
  },
  {
    "text": "next so i'm going to apply the last one which is actually pointing to the wrong git repository so then",
    "start": "486400",
    "end": "492960"
  },
  {
    "text": "argus if we apply that and then the actual lcd deployment for that application decided okay",
    "start": "492960",
    "end": "498240"
  },
  {
    "text": "i no longer have idp to deploy and i should be pruning everything so i'm just going to go ahead delete the whole the",
    "start": "498240",
    "end": "504240"
  },
  {
    "text": "whole production cluster and deploy this ios testing environment instead so",
    "start": "504240",
    "end": "509599"
  },
  {
    "text": "and this is a snippet from our from our uh the ins status page for the incident",
    "start": "509599",
    "end": "514959"
  },
  {
    "text": "once we had the postmortem in place this was exactly what happened due to some",
    "start": "514959",
    "end": "520880"
  },
  {
    "text": "mistakes when we were adding the ios we should have put io cd aws pro 01u",
    "start": "520880",
    "end": "526399"
  },
  {
    "text": "central one instead we added idp and this way we managed to basically delete the whole",
    "start": "526399",
    "end": "533519"
  },
  {
    "text": "thing and this is where the incident began",
    "start": "533519",
    "end": "538480"
  },
  {
    "text": "okay so i woke up at seven in the morning i",
    "start": "538720",
    "end": "544240"
  },
  {
    "text": "i live on the east coast and i don't for anyone here who manages a sas",
    "start": "545440",
    "end": "550720"
  },
  {
    "text": "environment you know that you never stop thinking about it and the first thing you check in the morning is like what happened at night and so this is uh what",
    "start": "550720",
    "end": "558160"
  },
  {
    "text": "i woke up to uh good times this by the way if you're trying to quit coffee this will help you",
    "start": "558160",
    "end": "564080"
  },
  {
    "text": "wake up without coffee [Music] so i just went straight to my computer",
    "start": "564080",
    "end": "569279"
  },
  {
    "text": "um and just started working on this so um",
    "start": "569279",
    "end": "574720"
  },
  {
    "text": "this is the first part this is when the damage was done this is actually all like right before i got up so the p",
    "start": "574720",
    "end": "580880"
  },
  {
    "text": "these are all times that are utc but i'm based on the east coast so um",
    "start": "580880",
    "end": "586560"
  },
  {
    "text": "the pr was merged and then all our monitoring systems",
    "start": "586560",
    "end": "591839"
  },
  {
    "text": "started to report api failures but we had a bit of monitoring fatigue",
    "start": "591839",
    "end": "597440"
  },
  {
    "text": "at the time so we told it like look you really need to wait a while before you start paging people because like these",
    "start": "597440",
    "end": "603120"
  },
  {
    "text": "bothering our staff with these transient api failures is uh just annoying people and it's not",
    "start": "603120",
    "end": "610079"
  },
  {
    "text": "helping much then a customer called in and then another customer and said hey it seems like something's not working so",
    "start": "610079",
    "end": "616480"
  },
  {
    "text": "the support team jumped in and said customers are reporting a problem and we",
    "start": "616480",
    "end": "622480"
  },
  {
    "text": "looked around and sure enough all the alerts started firing at that point and it was clear there was",
    "start": "622480",
    "end": "628880"
  },
  {
    "text": "something wrong so in our culture when we deliver code that causes a problem we",
    "start": "628880",
    "end": "636000"
  },
  {
    "text": "just revert it immediately and we really try to develop so that a revert of code is the",
    "start": "636000",
    "end": "641839"
  },
  {
    "text": "um [Music] path back to stability but this was not a code change this was an infrastructure",
    "start": "641839",
    "end": "648000"
  },
  {
    "text": "change so the developer actually followed that process submitted the pr",
    "start": "648000",
    "end": "653360"
  },
  {
    "text": "but then the team realized that like just reverting a change like that since",
    "start": "653360",
    "end": "658399"
  },
  {
    "text": "it was infrastructure change is probably not the way to do it but fortunately we also have a culture of",
    "start": "658399",
    "end": "665279"
  },
  {
    "text": "anyone being able to stop the line so we stopped all synchronizations we have this thing that we call internally the",
    "start": "665279",
    "end": "671040"
  },
  {
    "text": "big red button anyone can press it and it'll just stop deployments through all of our deployment environments",
    "start": "671040",
    "end": "678720"
  },
  {
    "text": "um then the engineering team said okay do we really need to start really planning",
    "start": "678720",
    "end": "683920"
  },
  {
    "text": "a proper recovery process and by the way did anybody update the status page and we",
    "start": "683920",
    "end": "689600"
  },
  {
    "text": "updated the status page to alert customers and then they started calling everybody like hey this is all hands on",
    "start": "689600",
    "end": "696240"
  },
  {
    "text": "deck uh you know when they really the penny dropped about the severity of the",
    "start": "696240",
    "end": "701360"
  },
  {
    "text": "problem okay so here i don't have individual times",
    "start": "701360",
    "end": "707200"
  },
  {
    "text": "because this phase really unfolded over the course of hours",
    "start": "707200",
    "end": "713120"
  },
  {
    "text": "but the first thing the team did was to create a deployment checklist and and double check that list and",
    "start": "713120",
    "end": "719680"
  },
  {
    "text": "that i was really impressed with that everyone's staying very calm and",
    "start": "719680",
    "end": "724720"
  },
  {
    "text": "uh instead of you know panicking they say okay like let's make a plan but like let's double check the plan",
    "start": "724720",
    "end": "731519"
  },
  {
    "text": "before we start stampeding and to make sure that we don't make things worse and",
    "start": "731519",
    "end": "736800"
  },
  {
    "text": "i actually credit the short time that it took us to rebuild the production environment to that",
    "start": "736800",
    "end": "743360"
  },
  {
    "text": "systematic uh mentality that the developers followed so",
    "start": "743360",
    "end": "748959"
  },
  {
    "text": "then we went through and we started to redeploy services carefully and in the proper order and the main thing was to",
    "start": "748959",
    "end": "757040"
  },
  {
    "text": "connect when we could the stateful services to their persistent volumes",
    "start": "757040",
    "end": "762880"
  },
  {
    "text": "because that saved us a lot of time in terms of not having to play back backup data right so the data was there we",
    "start": "762880",
    "end": "769519"
  },
  {
    "text": "could just reconnect it and start using it then um",
    "start": "769519",
    "end": "775680"
  },
  {
    "text": "additional services especially the state were stateful the stateless ones were redeployed in parallel with people going",
    "start": "775680",
    "end": "783839"
  },
  {
    "text": "and making sure like did we actually recover the volumes properly like is the data",
    "start": "783839",
    "end": "789680"
  },
  {
    "text": "actually safe and proper and it turned out it was um we had some services",
    "start": "789680",
    "end": "795360"
  },
  {
    "text": "that we just recovered from valero backups if the team thought you know okay the",
    "start": "795360",
    "end": "801440"
  },
  {
    "text": "best strategy is just to get that from valero but then also we're like you know what when we turn this back on",
    "start": "801440",
    "end": "807920"
  },
  {
    "text": "everybody's telegraph instances like some of the eight we we have an agent that people can use to write down flexdb",
    "start": "807920",
    "end": "814000"
  },
  {
    "text": "those are all going to start writing everybody's queries are going to start stampeding us everyone's going to be",
    "start": "814000",
    "end": "819680"
  },
  {
    "text": "trying to catch up so we scaled out especially the ingress tier but also also other tiers anticipating that surge",
    "start": "819680",
    "end": "827040"
  },
  {
    "text": "in traffic when we came back on and then finally the smoke started to",
    "start": "827040",
    "end": "833680"
  },
  {
    "text": "clear we enabled the right service and then let started accepting people's rights and spent some time to verify",
    "start": "833680",
    "end": "840000"
  },
  {
    "text": "that that was all working and that we hadn't lost any data and then we realized that we had we're",
    "start": "840000",
    "end": "846959"
  },
  {
    "text": "going to have another problem for careful influx db has something called a task system which means if you have a",
    "start": "846959",
    "end": "853839"
  },
  {
    "text": "script that is down sampling data or importing data",
    "start": "853839",
    "end": "859040"
  },
  {
    "text": "from another data source and joining with time series data or exporting to another data source",
    "start": "859040",
    "end": "864160"
  },
  {
    "text": "doing some kind of custom calculation and that's happening on a schedule you can push all that work down into our",
    "start": "864160",
    "end": "870079"
  },
  {
    "text": "platform and some people run those every second every minute every hour and we realized when the task system comes back",
    "start": "870079",
    "end": "877279"
  },
  {
    "text": "on it's going to realize it wasn't running tasks for the past few hours it's going to try to run all those tasks",
    "start": "877279",
    "end": "883360"
  },
  {
    "text": "and if it's trying to run all those tasks and users are trying to run queries at the same time",
    "start": "883360",
    "end": "888639"
  },
  {
    "text": "it's just going to be a complete collision you know there's a total traffic jam of queries so we decided to",
    "start": "888639",
    "end": "894560"
  },
  {
    "text": "do was before we turned on queries again we just let the task system run its course and fortunately it wasn't too",
    "start": "894560",
    "end": "901199"
  },
  {
    "text": "long so we let all those tasks run and fail",
    "start": "901199",
    "end": "906240"
  },
  {
    "text": "and then when the backlog was done and the tasks were caught up we went ahead and turned on the query service and at that",
    "start": "906240",
    "end": "913760"
  },
  {
    "text": "point really the smoke had cleared and we were back in business so uh",
    "start": "913760",
    "end": "920880"
  },
  {
    "text": "we alerted all our customers hey the the service is back we also went through and",
    "start": "920880",
    "end": "927600"
  },
  {
    "text": "collected a lot of information for customers like hey here's the idea of all your failed tasks here's a script",
    "start": "927600",
    "end": "933600"
  },
  {
    "text": "that you can run to rerun them if you want if they're still relevant and just other things that we could do",
    "start": "933600",
    "end": "939040"
  },
  {
    "text": "to help them recover during the course of the day i was busy just like writing down and logging what",
    "start": "939040",
    "end": "945360"
  },
  {
    "text": "happened so we were able to write an uh rca doc and put it on our status page within an hour or two after the incident",
    "start": "945360",
    "end": "952639"
  },
  {
    "text": "so that people could go back and you know see what really happened",
    "start": "952639",
    "end": "957680"
  },
  {
    "text": "from that we got an interesting piece of feedback from one of our big customers who said well",
    "start": "957680",
    "end": "963120"
  },
  {
    "text": "we're just glad it was automation and not somebody sitting in front of the terminal so",
    "start": "963120",
    "end": "968880"
  },
  {
    "text": "they actually was a little bit almost confidence building in a way that it wasn't somebody fat fingering at a terminal and it was our our automation",
    "start": "968880",
    "end": "977759"
  },
  {
    "text": "going crazy there so uh we did",
    "start": "978399",
    "end": "983839"
  },
  {
    "text": "obviously we spent a couple days during internal rcas um uh",
    "start": "983839",
    "end": "989759"
  },
  {
    "text": "so one of the things that we found during the rca process that we ran was",
    "start": "989759",
    "end": "995199"
  },
  {
    "text": "that our our cross team efforts were really effective right so sre team",
    "start": "995199",
    "end": "1000720"
  },
  {
    "text": "deployments team developers working together and i really chalked that up to our blameless culture",
    "start": "1000720",
    "end": "1007759"
  },
  {
    "text": "it was about like what was wrong with our systems it was never about any any person",
    "start": "1007920",
    "end": "1013040"
  },
  {
    "text": "no person made a mistake our systems were lacking and that",
    "start": "1013040",
    "end": "1018079"
  },
  {
    "text": "i could see really enabled that kind of collaboration while we had downtime we did not lose",
    "start": "1018079",
    "end": "1024798"
  },
  {
    "text": "any data and that i mean if anybody tried to write data while we were down",
    "start": "1024799",
    "end": "1030798"
  },
  {
    "text": "they got back a 404 or 500 or something right what's terrible is that they get back at 204 we got your data",
    "start": "1030799",
    "end": "1038400"
  },
  {
    "text": "but it didn't actually write the data because then they're in an inconsistent state and they don't know like how their application is going to",
    "start": "1038400",
    "end": "1044798"
  },
  {
    "text": "perform so in this case they could go back and decide if they wanted to backfill during the time that we were down or how they",
    "start": "1044799",
    "end": "1052240"
  },
  {
    "text": "wanted to handle it um we did avoid panic as uh as we mentioned",
    "start": "1052240",
    "end": "1059280"
  },
  {
    "text": "while we did like immediately sort of knee-jerk to that rollback attempt we were able to stop and create a plan",
    "start": "1059280",
    "end": "1066000"
  },
  {
    "text": "first props to valero also props to our sre team like the week before they had been",
    "start": "1066000",
    "end": "1072160"
  },
  {
    "text": "practicing valero backups so that was like right at their fingertips to uh how to use valero to to get back um",
    "start": "1072160",
    "end": "1081280"
  },
  {
    "text": "some things that we were not so happy about was um like how did our automation allow this",
    "start": "1081280",
    "end": "1087440"
  },
  {
    "text": "to happen right so like ci cd or like get ops it's a it's a very sharp knife",
    "start": "1087440",
    "end": "1093440"
  },
  {
    "text": "which they say a sharp knife is the safest but it cut us deeply in this case",
    "start": "1093440",
    "end": "1099440"
  },
  {
    "text": "and the other thing was we never anticipated deleting a cluster so our alerts were",
    "start": "1099440",
    "end": "1106240"
  },
  {
    "text": "tuned to errors in our code or some user doing something that we didn't anticipate but",
    "start": "1106240",
    "end": "1112080"
  },
  {
    "text": "we never imagined like we'd need to be alerted that the cluster was deleted and we had no run books for recovering the",
    "start": "1112080",
    "end": "1118320"
  },
  {
    "text": "cluster so we do now because we wrote it over the course of that day",
    "start": "1118320",
    "end": "1124880"
  },
  {
    "text": "okay so in terms of recovering some uh some like",
    "start": "1126080",
    "end": "1131200"
  },
  {
    "text": "recap and maybe some technical information so as rick mentioned our first instinct was to revert the change",
    "start": "1131200",
    "end": "1137200"
  },
  {
    "text": "like when you're doing a lot of small call changes and you're in a microservice based architecture that's what you often do you make small changes",
    "start": "1137200",
    "end": "1143360"
  },
  {
    "text": "that aren't breaking so they're easy to deploy incrementally and these roll back but in this case it wasn't a good idea",
    "start": "1143360",
    "end": "1149120"
  },
  {
    "text": "because we would be creating new volumes instead of reusing the volumes that that were that",
    "start": "1149120",
    "end": "1154400"
  },
  {
    "text": "were retained by the by kubernetes and the and the clouds um",
    "start": "1154400",
    "end": "1160080"
  },
  {
    "text": "so at that point as we mentioned the team stopped and we started creating a proper plan and the goal was that we",
    "start": "1160080",
    "end": "1165919"
  },
  {
    "text": "want to restore all the stateful guidance manually and then recreate the rest via cd with maybe some manual",
    "start": "1165919",
    "end": "1171520"
  },
  {
    "text": "tuning that we will get into in a bit so what we did and maybe some questions",
    "start": "1171520",
    "end": "1176880"
  },
  {
    "text": "around that it's like why didn't we just redeploy the storage there and storage theory is the the part of the system",
    "start": "1176880",
    "end": "1182559"
  },
  {
    "text": "that keeps all of the time series data so all of your metrics for the last 10 years and the reason for that is",
    "start": "1182559",
    "end": "1189039"
  },
  {
    "text": "if we were to do that and not and not like reattach the volumes that that we had that we still had",
    "start": "1189039",
    "end": "1195280"
  },
  {
    "text": "we would be we would need to fetch the data from the cloud native objects or like s3 or",
    "start": "1195280",
    "end": "1200720"
  },
  {
    "text": "google streamer storage system and replay that data and that would take several hours and keep in mind that we",
    "start": "1200720",
    "end": "1207600"
  },
  {
    "text": "were able to recover in just i believe it was six hours less than six hours i think even and if",
    "start": "1207600",
    "end": "1213200"
  },
  {
    "text": "we would do that this would probably roll over into more like 10 or 12 hours now the other question is",
    "start": "1213200",
    "end": "1219280"
  },
  {
    "text": "maybe i'll back up and just mention one thing so most of our volumes in that in that cluster were defined to be in",
    "start": "1219280",
    "end": "1225679"
  },
  {
    "text": "retain mode meaning that even if you delete the pvc the actual persistent volume and the actual underlying cloud",
    "start": "1225679",
    "end": "1232159"
  },
  {
    "text": "storage volume is kept there so but that wasn't the case for everything so for zookeeper that's just keeping the",
    "start": "1232159",
    "end": "1239120"
  },
  {
    "text": "state for kafka and it doesn't really have any changes or topics in kafka and everything in kafka is pretty much",
    "start": "1239120",
    "end": "1244320"
  },
  {
    "text": "static it's just a day-to-day kafka that changes so we were able to just restart it from the hourly backups and we and we",
    "start": "1244320",
    "end": "1250480"
  },
  {
    "text": "decided that for zookeeper that's that's enough to just have the backups in place for kafka and lcd uh we were we we since",
    "start": "1250480",
    "end": "1258400"
  },
  {
    "text": "i mentioned we had the persistent volumes we just restored them it was initially a manual process then once we",
    "start": "1258400",
    "end": "1264960"
  },
  {
    "text": "made sure that it works we could like script that and just run it we have a lot of pots in",
    "start": "1264960",
    "end": "1270799"
  },
  {
    "text": "in kafka at cdn storage so it would be painful to do it manually and prone to fat fingering that we didn't want once",
    "start": "1270799",
    "end": "1277120"
  },
  {
    "text": "we had that we could recreate the stateful sets and then and then by then kubernetes would recreate the parts",
    "start": "1277120",
    "end": "1283200"
  },
  {
    "text": "with storage it was pretty much the same thing the main difference is that the way our storage still works it has",
    "start": "1283200",
    "end": "1288720"
  },
  {
    "text": "to index the data at that point but basically as soon as it as every single as a storage spot wakes",
    "start": "1288720",
    "end": "1295280"
  },
  {
    "text": "up it re-indexes the data in the persistent volume in case it was shut down incorrectly so that have an up-to-date",
    "start": "1295280",
    "end": "1301679"
  },
  {
    "text": "index and then it can asynchronously inject the right ahead log data from kafka",
    "start": "1301679",
    "end": "1308159"
  },
  {
    "text": "last question why didn't we just enable everything at once as rick mentioned there are some tricks to that",
    "start": "1308159",
    "end": "1314240"
  },
  {
    "text": "so we started enabling parts of influx data as they started to work so even before the storage theory was fully",
    "start": "1314240",
    "end": "1320240"
  },
  {
    "text": "restored we could start enabling rights to the system because we because the rights just put the data in kafka and",
    "start": "1320240",
    "end": "1326880"
  },
  {
    "text": "report 204 if the right to kafka was successful so people could start writing data even though they were not able to",
    "start": "1326880",
    "end": "1332799"
  },
  {
    "text": "create yet and it wasn't properly persisted but the kafka with its availability and replication",
    "start": "1332799",
    "end": "1339440"
  },
  {
    "text": "allowed us to to be confident with that approach once we had all the stateful services in",
    "start": "1339440",
    "end": "1345120"
  },
  {
    "text": "place we deployed the remaining ones we increased the number of replicas and again in terms of what was",
    "start": "1345120",
    "end": "1351039"
  },
  {
    "text": "enabled or what was running we started with tasks we let all the tasks run we made sure that the backlog was empty",
    "start": "1351039",
    "end": "1357440"
  },
  {
    "text": "and then after that happened we enabled queries and at that point we were confident to just scale back and",
    "start": "1357440",
    "end": "1363520"
  },
  {
    "text": "configure the number of replicas to what it is on a day-to-day basis or what it should be",
    "start": "1363520",
    "end": "1369360"
  },
  {
    "text": "right so now after we've done that the obvious question is can we not delete production again please because it took a lot of",
    "start": "1369360",
    "end": "1376240"
  },
  {
    "text": "people a lot of time and everyone was had to stop whatever they were doing at the time or maybe like",
    "start": "1376240",
    "end": "1382400"
  },
  {
    "text": "use the text message instead of coffee for waking up so the first thing is like obviously we",
    "start": "1382400",
    "end": "1387760"
  },
  {
    "text": "don't want that thing to be merged again and just a quick introduction to how we do things",
    "start": "1387760",
    "end": "1393280"
  },
  {
    "text": "we use jsonnet which is a tool that allows rendering multiple well objects",
    "start": "1393280",
    "end": "1398400"
  },
  {
    "text": "basically it's not kubernetes specific but we use it for kubernetes objects prior to the incident we were just",
    "start": "1398400",
    "end": "1404159"
  },
  {
    "text": "writing everything to a single yaml file so the pr you've seen was that was just",
    "start": "1404159",
    "end": "1409440"
  },
  {
    "text": "a big yaml file that had object with the same with the same name and namespace",
    "start": "1409440",
    "end": "1415200"
  },
  {
    "text": "added to it so that was the problem we move that to house to have a single object in a single file and the file",
    "start": "1415200",
    "end": "1421919"
  },
  {
    "text": "name is generated from object properties so it's the api version it's the kind so",
    "start": "1421919",
    "end": "1426960"
  },
  {
    "text": "like a service deployment stateful set etc namespace or if this this is uh if this isn't if this is not a namespace",
    "start": "1426960",
    "end": "1434240"
  },
  {
    "text": "object then i think we use a static string there then the name and and that would prevent it because at",
    "start": "1434240",
    "end": "1440960"
  },
  {
    "text": "that point that pr would show that we're overwriting an object enamel at the preamble level and not adding one",
    "start": "1440960",
    "end": "1448159"
  },
  {
    "text": "but we went a bit further than that because as we include the api version in kubernetes you could have like v1 beta1",
    "start": "1448159",
    "end": "1454960"
  },
  {
    "text": "and v1 and that technically would be a separate file but at the kubernetes level would would generate would",
    "start": "1454960",
    "end": "1461120"
  },
  {
    "text": "override the object anyway so we use a tool called config to generate yaml from",
    "start": "1461120",
    "end": "1466880"
  },
  {
    "text": "jsonnet and we've added a smarter logic tool to detect those collisions in there and basically it's going to refuse to",
    "start": "1466880",
    "end": "1473200"
  },
  {
    "text": "generate the ammo files if you have a collision in there and i think that a big upside from that is now when we review prs we see which",
    "start": "1473200",
    "end": "1481120"
  },
  {
    "text": "object gets changed because sometimes when you have a complex deployment state full set and you just want to add a variable another container or something",
    "start": "1481120",
    "end": "1488720"
  },
  {
    "text": "it wasn't really clear what you were editing and you had to scroll up or down a lot so now it's now it's more",
    "start": "1488720",
    "end": "1493840"
  },
  {
    "text": "efficient in a lot of other things and then we looked at can we do something at argo cd level argos cd is a",
    "start": "1493840",
    "end": "1499600"
  },
  {
    "text": "great tool but you want to configure some of the things to make it even better and like make it",
    "start": "1499600",
    "end": "1505600"
  },
  {
    "text": "even safer to use it uh because of all the automation in place it's really easy to to do something really bad when you when",
    "start": "1505600",
    "end": "1512080"
  },
  {
    "text": "you when you make a mistake so argo cd has annotations that you can add to objects so basically any kubernetes",
    "start": "1512080",
    "end": "1518240"
  },
  {
    "text": "object and one of them is prone equals false i believe that's the annotation uh i mean that the value for the",
    "start": "1518240",
    "end": "1523679"
  },
  {
    "text": "annotation which will mean argos cd will never delete the underlying object so if you add it to your stateful set it don't",
    "start": "1523679",
    "end": "1529760"
  },
  {
    "text": "remember the annotation name even if you delete it in your yaml files or wherever argo cd is getting the",
    "start": "1529760",
    "end": "1537200"
  },
  {
    "text": "object definitions from argo cd will just leave it alone and will just no longer manage it one thing we learned as",
    "start": "1537200",
    "end": "1543120"
  },
  {
    "text": "well and i think this is a valuable lesson you need to set it for namespace as well because in one of our first",
    "start": "1543120",
    "end": "1549440"
  },
  {
    "text": "drills and of testing the this change we noticed that actually argo cd deleted namespace kubernetes",
    "start": "1549440",
    "end": "1555279"
  },
  {
    "text": "deleted all the stateful sets so it didn't really help without setting it to the namespace level we also added",
    "start": "1555279",
    "end": "1561279"
  },
  {
    "text": "annotations that make argo cd refuse to update a resource that already exists",
    "start": "1561279",
    "end": "1567120"
  },
  {
    "text": "and has annotations that specified it's managed by another cd application so if we if we would ever accidentally",
    "start": "1567120",
    "end": "1573440"
  },
  {
    "text": "clash at like an object level so someone else would create a same stateful state with",
    "start": "1573440",
    "end": "1579600"
  },
  {
    "text": "stateful set with the same name in the same name space argo cd would just fail to sync would just report an error and",
    "start": "1579600",
    "end": "1586559"
  },
  {
    "text": "this would show up in our alerting system immediately and",
    "start": "1586559",
    "end": "1592799"
  },
  {
    "text": "one last thing that we've done that was based on on rick's handling of the of the incident and the team's coordination",
    "start": "1592799",
    "end": "1599200"
  },
  {
    "text": "of that is well first thing we decided is we need to basically go back and delete an",
    "start": "1599200",
    "end": "1604559"
  },
  {
    "text": "environment again just to test everything but in this case we deleted a staging so testing environment another",
    "start": "1604559",
    "end": "1610240"
  },
  {
    "text": "production one we did that after obviously after we've written the run books but this was a way",
    "start": "1610240",
    "end": "1615679"
  },
  {
    "text": "to test them we also were doing some exercises fire drills around things like argo cd after",
    "start": "1615679",
    "end": "1622000"
  },
  {
    "text": "updating argo cd version does the annotation still work because at some point some of the configuration settings",
    "start": "1622000",
    "end": "1627840"
  },
  {
    "text": "changed in argo city itself and we learned a ton of new things about what happens when things go wrong when",
    "start": "1627840",
    "end": "1634640"
  },
  {
    "text": "we just basically did an equivalent of of chaos monkey and i",
    "start": "1634640",
    "end": "1639919"
  },
  {
    "text": "think everyone should think about doing this maybe don't start with production but like testing what happens when you get",
    "start": "1639919",
    "end": "1646320"
  },
  {
    "text": "up goes sideways and testing how you can potentially think about how you can potentially break it and making sure you",
    "start": "1646320",
    "end": "1651440"
  },
  {
    "text": "can't you can't do that is is a really important thing to think about we went back",
    "start": "1651440",
    "end": "1657600"
  },
  {
    "text": "and looked at all the volumes across a lot of environments so this was a relatively new environment that was done",
    "start": "1657600",
    "end": "1663840"
  },
  {
    "text": "when we have all the automation in place and all of the volumes were set to retain properly but we went back look",
    "start": "1663840",
    "end": "1669919"
  },
  {
    "text": "back at the first environments that were created manually and i believe in some cases we found some volumes that should be retained but",
    "start": "1669919",
    "end": "1676559"
  },
  {
    "text": "weren't or vice versa and we went back and and and updated all those settings",
    "start": "1676559",
    "end": "1682240"
  },
  {
    "text": "and one last but very important thing we looked at our processes and",
    "start": "1682240",
    "end": "1689039"
  },
  {
    "text": "made them more consistent and made them more easy to follow so this was our first incident that involved a large",
    "start": "1689039",
    "end": "1695440"
  },
  {
    "text": "number of customers that were affected by it so the first thing we learned is you need to have a way to list all the",
    "start": "1695440",
    "end": "1701600"
  },
  {
    "text": "customers that may be affected by an issue then make sure that you have all the valid contact points because",
    "start": "1701600",
    "end": "1707440"
  },
  {
    "text": "sometimes that may be a different person than the login that someone's using have a consistent way of conducting have",
    "start": "1707440",
    "end": "1713760"
  },
  {
    "text": "someone leading the the contacting effort and making sure we know when an incident is resolved and that we have to",
    "start": "1713760",
    "end": "1720080"
  },
  {
    "text": "proactively update all the customers about the incident status and that they are",
    "start": "1720080",
    "end": "1725200"
  },
  {
    "text": "updated all the way to it being resolved and that we just don't depend on someone refreshing our influx status impact data",
    "start": "1725200",
    "end": "1732080"
  },
  {
    "text": "status page and that being the only way they find out about how things are",
    "start": "1732080",
    "end": "1737440"
  },
  {
    "text": "and with that i believe that's it",
    "start": "1737440",
    "end": "1741200"
  },
  {
    "text": "so if you have any questions please make your way up to the microphone here in the middle and we have a few minutes",
    "start": "1751679",
    "end": "1756880"
  },
  {
    "text": "left for four questions while you do that i think i'm going to do one from",
    "start": "1756880",
    "end": "1762399"
  },
  {
    "text": "from the virtual attendees and see so i think you touched on this earlier but",
    "start": "1762399",
    "end": "1768159"
  },
  {
    "text": "someone is asking here in which cases valero is a better way to recover the application instead of just",
    "start": "1768159",
    "end": "1773919"
  },
  {
    "text": "redeploying the applications so i i think that really depends on what data you have and",
    "start": "1773919",
    "end": "1780000"
  },
  {
    "text": "how much time how much time it takes to recreate it and how accurate the data will be once",
    "start": "1780000",
    "end": "1785360"
  },
  {
    "text": "you do it in each way so in the case of zookeeper that data didn't really change often so just",
    "start": "1785360",
    "end": "1790960"
  },
  {
    "text": "reusing valero was fine because once you have kafka set up the data in valero doesn't really change often",
    "start": "1790960",
    "end": "1796640"
  },
  {
    "text": "so that was fine for example if we were to use hourly backups for data that our customers would be writing there would",
    "start": "1796640",
    "end": "1802799"
  },
  {
    "text": "be potential of losing the data for the up to last 60 minutes for example so i",
    "start": "1802799",
    "end": "1808080"
  },
  {
    "text": "think that's always a case-by-case choice when we were designing which volumes to",
    "start": "1808080",
    "end": "1813200"
  },
  {
    "text": "retain we chose to retain zookeeper for us there was about three levels the first is it's like if it's stateless valero doesn't",
    "start": "1813200",
    "end": "1819760"
  },
  {
    "text": "really help just let cd redeploy that if it's stateful and the data has not changed since the",
    "start": "1819760",
    "end": "1826960"
  },
  {
    "text": "last valero backup then valero can really help you out but if it's stateful and the data is constantly in flux like",
    "start": "1826960",
    "end": "1833679"
  },
  {
    "text": "it is for us people are constantly writing then you're going to lose all of the user data between the last",
    "start": "1833679",
    "end": "1840559"
  },
  {
    "text": "valero backup and where you are right now and so then just uh manually deploying and reconnecting to the pvcs",
    "start": "1840559",
    "end": "1847919"
  },
  {
    "text": "was a better option for us during the incident okay great thank you so see we'll see",
    "start": "1847919",
    "end": "1853919"
  },
  {
    "text": "some people lined up at the microphones we'll switch it over to room questions here so uh i wonder if you wouldn't have",
    "start": "1853919",
    "end": "1859919"
  },
  {
    "text": "stopped the git revert wouldn't it have been okay i mean if all",
    "start": "1859919",
    "end": "1866399"
  },
  {
    "text": "the volumes are correctly uh retained and your uh revert to the radiational",
    "start": "1866399",
    "end": "1872080"
  },
  {
    "text": "state seems to me like it could have worked right uh it",
    "start": "1872080",
    "end": "1878480"
  },
  {
    "text": "i think we in some cases we have been missing some objects i don't really remember at this point it was i believe",
    "start": "1878480",
    "end": "1884640"
  },
  {
    "text": "it was like eight months ago uh i think we've made a conscious",
    "start": "1884640",
    "end": "1889760"
  },
  {
    "text": "decision that something may go wrong so there's a chance things would just work out",
    "start": "1889760",
    "end": "1895519"
  },
  {
    "text": "the issue is it's always in those cases when you're risking losing data",
    "start": "1895519",
    "end": "1900559"
  },
  {
    "text": "i think it's safer to just go with the manual approach i believe it may have worked but i",
    "start": "1900559",
    "end": "1906080"
  },
  {
    "text": "remember we had to recreate some of the persistent volume objects or something like this so i mean one",
    "start": "1906080",
    "end": "1912080"
  },
  {
    "text": "thing that was that's missing in here is we use our own crd and our own controller to manage the storage tier we don't use stateful sets",
    "start": "1912080",
    "end": "1918640"
  },
  {
    "text": "for that and i believe this is one of the places where we may have to wait we may have needed to do something",
    "start": "1918640",
    "end": "1924640"
  },
  {
    "text": "slightly differently i just don't remember the details right now but right so this was a conscious decision that",
    "start": "1924640",
    "end": "1930240"
  },
  {
    "text": "you made a risk assessment and decided yes because if we just redeployed then argo would have said okay new storage",
    "start": "1930240",
    "end": "1937440"
  },
  {
    "text": "pods new pvcs and then we would have had like to",
    "start": "1937440",
    "end": "1943120"
  },
  {
    "text": "had a juggling act on our hands instead of just you know just re",
    "start": "1943120",
    "end": "1948720"
  },
  {
    "text": "creating the new pods and attaching hi thanks to the presentation um did you",
    "start": "1948720",
    "end": "1956399"
  },
  {
    "text": "manage did you investigate it in your ci cd deploy with argo to generate all",
    "start": "1956399",
    "end": "1963200"
  },
  {
    "text": "the um all the the objects and then run something like coop ctld to see",
    "start": "1963200",
    "end": "1970480"
  },
  {
    "text": "what will be applied and what would change and then review that instead of the code right so when we started",
    "start": "1970480",
    "end": "1976320"
  },
  {
    "text": "designing our cd process that was one of the discussions we had do we want to commit the generated files or have a tool",
    "start": "1976320",
    "end": "1983440"
  },
  {
    "text": "random and maybe also generate a diff at the pr level we chose to explicitly commit because",
    "start": "1983440",
    "end": "1990240"
  },
  {
    "text": "while this isn't the case in most most consensus but we're worried that the version of jsonnet in argo cd may differ",
    "start": "1990240",
    "end": "1997279"
  },
  {
    "text": "for the version of jsonnet that they're tooling other tooling may be using and then you may have those cell differences",
    "start": "1997279",
    "end": "2003360"
  },
  {
    "text": "that show up when you apply it's just it just seems like a safer way to do things and also this makes it that this is part",
    "start": "2003360",
    "end": "2009519"
  },
  {
    "text": "of the pr and in github you can configure which files are generated so when you open a pr in github it doesn't",
    "start": "2009519",
    "end": "2016080"
  },
  {
    "text": "show you the generated files but you can expand them but at least you see the number of additions and the number of deletions",
    "start": "2016080",
    "end": "2022159"
  },
  {
    "text": "so even with just the numbers you would catch that it's an override but often when we're touching stateful things",
    "start": "2022159",
    "end": "2027760"
  },
  {
    "text": "people open their yaml files and read them it just just seems more natural and seems safer",
    "start": "2027760",
    "end": "2032880"
  },
  {
    "text": "and that's what we decided at a company level and in all the ci cd that we do for all the tools that we have we commit",
    "start": "2032880",
    "end": "2038799"
  },
  {
    "text": "the ammo files hi",
    "start": "2038799",
    "end": "2045039"
  },
  {
    "text": "first question has this incident changed how your organization work for example",
    "start": "2045039",
    "end": "2051118"
  },
  {
    "text": "the priority of disaster recovery is actually higher sometimes than future",
    "start": "2051119",
    "end": "2057200"
  },
  {
    "text": "development um i'm going to say no because",
    "start": "2057200",
    "end": "2063599"
  },
  {
    "text": "for us safeguarding the user's data and not losing data is always the top priority",
    "start": "2063599",
    "end": "2069760"
  },
  {
    "text": "for everything that we do so there's like really no way for us to make it an even",
    "start": "2069760",
    "end": "2075118"
  },
  {
    "text": "even higher priority but we did learn a lot and because we have that um",
    "start": "2075119",
    "end": "2081839"
  },
  {
    "text": "as such a high priority the company was very tolerant to us",
    "start": "2081839",
    "end": "2086960"
  },
  {
    "text": "actually taking the time to do some more investigation and apply the learnings whereas you know",
    "start": "2086960",
    "end": "2094320"
  },
  {
    "text": "maybe other companies if they're under more feature pressure they they may not have had that luxury",
    "start": "2094320",
    "end": "2100240"
  },
  {
    "text": "so did that answer your question yes thank you i think great time for one final",
    "start": "2100240",
    "end": "2105839"
  },
  {
    "text": "question uh once a a little question what was the",
    "start": "2105839",
    "end": "2113520"
  },
  {
    "text": "first thought you had when you read the text message",
    "start": "2113520",
    "end": "2118640"
  },
  {
    "text": "it finally happened and one second question",
    "start": "2121440",
    "end": "2127839"
  },
  {
    "text": "how did you manage to deploy the different objects while you had one single big single file",
    "start": "2127839",
    "end": "2133839"
  },
  {
    "text": "with all the definitions did you separated the one file and single like batches to deploy or do you",
    "start": "2133839",
    "end": "2140240"
  },
  {
    "text": "mean during the recovery process yes uh i can answer that so basically",
    "start": "2140240",
    "end": "2145920"
  },
  {
    "text": "we i mean we're proficient with jsonnet so there are ways in which you can pick and choose which objects get generated",
    "start": "2145920",
    "end": "2152640"
  },
  {
    "text": "uh it's relatively easy to just generate a subset of the files at the end of the day json it is just an",
    "start": "2152640",
    "end": "2158000"
  },
  {
    "text": "array of objects i mean the the thing that we render at from those files is an array of objects and you can just filter",
    "start": "2158000",
    "end": "2164480"
  },
  {
    "text": "on that array of objects and choose the ones you want but i think just jamie did a lot of cube cut all right that's",
    "start": "2164480",
    "end": "2169760"
  },
  {
    "text": "another thing that you're like in the heater or you can just copy paste right i mean there are multiple ways to to get it it's not like it's a closed system",
    "start": "2169760",
    "end": "2176000"
  },
  {
    "text": "you can get like you can json eval and then you can do a lot of things in in one-liners as well so",
    "start": "2176000",
    "end": "2182320"
  },
  {
    "text": "this is beyond the scope of this but it's it's not difficult okay thank you",
    "start": "2182320",
    "end": "2187440"
  },
  {
    "text": "okay so i think that wraps it up so a big thank you to our presenters thanks everyone for coming",
    "start": "2187440",
    "end": "2194079"
  },
  {
    "text": "if i can please ask you to any do any additional questions outside the room and don't forget to rate the session",
    "start": "2197280",
    "end": "2203680"
  },
  {
    "text": "afterwards thanks everyone",
    "start": "2203680",
    "end": "2207319"
  },
  {
    "text": "you",
    "start": "2221760",
    "end": "2223839"
  }
]