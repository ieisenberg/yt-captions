[
  {
    "start": "0",
    "end": "243000"
  },
  {
    "text": "thanks everybody who else has a bit of a meat coma today anyways so this is log",
    "start": "0",
    "end": "9599"
  },
  {
    "text": "testing kubernetes and i'm gonna go through how to optimize your cluster resource allocation and",
    "start": "9599",
    "end": "16350"
  },
  {
    "text": "production so a little bit about me my name is Harrison and I'm a senior",
    "start": "16350",
    "end": "22859"
  },
  {
    "text": "software engineer at buffer and I focus on helping our product teams get more",
    "start": "22859",
    "end": "29820"
  },
  {
    "text": "stuff done and also some of the architecture work for for our system so",
    "start": "29820",
    "end": "38129"
  },
  {
    "text": "I'm gonna start with a little bit of a case study so we had a pre-existing endpoint in our monolith",
    "start": "38129",
    "end": "44670"
  },
  {
    "text": "it was done in PHP and it serves the number of times the link has been shared",
    "start": "44670",
    "end": "49860"
  },
  {
    "text": "within our product buffer so buffer is a social media tool and people like to",
    "start": "49860",
    "end": "57120"
  },
  {
    "text": "share tweets they'll they'll queue up a bunch of tweets and sometimes those tweets would contain a link so we keep",
    "start": "57120",
    "end": "64228"
  },
  {
    "text": "track of the number of times those things have been shared and then bloggers will have the buffer button",
    "start": "64229",
    "end": "70799"
  },
  {
    "text": "displayed so we serve a button that shares shows accounts so people can get an idea of how many times their link has",
    "start": "70799",
    "end": "79080"
  },
  {
    "text": "been shared so they can gauge interest in what they're they're writing about so",
    "start": "79080",
    "end": "84840"
  },
  {
    "text": "we settled upon a simple design or so we thought using node and DynamoDB to Tabac",
    "start": "84840",
    "end": "92070"
  },
  {
    "text": "the counts and we deployed the service to kubernetes and it had four replicas",
    "start": "92070",
    "end": "98640"
  },
  {
    "text": "and we manually verified using curl that",
    "start": "98640",
    "end": "104009"
  },
  {
    "text": "the service was operational so we were running pretty much stock kubernetes",
    "start": "104009",
    "end": "109369"
  },
  {
    "text": "using the stock services and we're deployed on AWS so we're out 1% of our",
    "start": "109369",
    "end": "119310"
  },
  {
    "text": "traffic from our existing application onto our kubernetes cluster things look",
    "start": "119310",
    "end": "124890"
  },
  {
    "text": "fantastic so we scale that up to 10% of all the",
    "start": "124890",
    "end": "130410"
  },
  {
    "text": "traffic was being routed into our new setup and then we moved up as",
    "start": "130410",
    "end": "136560"
  },
  {
    "text": "you'd expect to fifty percent and this is where things started to get interesting so the first thing that we",
    "start": "136560",
    "end": "144930"
  },
  {
    "text": "did was kind of freaked out and scaled up a replicas 5x so we scaled it up to",
    "start": "144930",
    "end": "150180"
  },
  {
    "text": "20 pods this helped but the pods just they just kept dying and it wasn't clear",
    "start": "150180",
    "end": "156690"
  },
  {
    "text": "because we were really new to corn eighties at the time what was going on so then we scaled the traffic back down",
    "start": "156690",
    "end": "163650"
  },
  {
    "text": "to 0% and spent some time investigating what could what could be happening so",
    "start": "163650",
    "end": "172250"
  },
  {
    "text": "the first thing was that I had copied and pasted a deployment from another service and I think it was just",
    "start": "172250",
    "end": "178530"
  },
  {
    "text": "something that I found on the Internet and shame on me but the the deployment",
    "start": "178530",
    "end": "184470"
  },
  {
    "text": "included resource limits and they weren't the right resource limits for for the application and the load that we",
    "start": "184470",
    "end": "191220"
  },
  {
    "text": "were doing with a little more investigation we found that we were getting om killed which basically means",
    "start": "191220",
    "end": "197190"
  },
  {
    "text": "that the container ran out of memory and kubernetes had basically killed it so",
    "start": "197190",
    "end": "205680"
  },
  {
    "text": "let's talk a little bit about resource limits so these are constraints that can",
    "start": "205680",
    "end": "211200"
  },
  {
    "text": "be set on both CPU and memory utilization and without these things set",
    "start": "211200",
    "end": "217139"
  },
  {
    "text": "at all the containers can run unbounded with the CPU and memory that they consume there are some things in place",
    "start": "217139",
    "end": "224579"
  },
  {
    "text": "now defaults that you can place on name namespaces but I'm not going to cover that in this talk but they are available",
    "start": "224579",
    "end": "232609"
  },
  {
    "text": "so kubernetes well so what when these thresholds are crossed so one of limits",
    "start": "232609",
    "end": "239129"
  },
  {
    "text": "cross kubernetes is going to restart the containers so how do we go about",
    "start": "239129",
    "end": "244440"
  },
  {
    "start": "243000",
    "end": "489000"
  },
  {
    "text": "optimally setting these things so it's important to understand what optimal",
    "start": "244440",
    "end": "251160"
  },
  {
    "text": "means here and that means that each pod has enough resources to complete their",
    "start": "251160",
    "end": "257639"
  },
  {
    "text": "tasks and this also means that individual nodes can run the maximum",
    "start": "257639",
    "end": "263250"
  },
  {
    "text": "number of so there's different ways that things",
    "start": "263250",
    "end": "269110"
  },
  {
    "text": "could things could go wrong and also things could go right so the first one",
    "start": "269110",
    "end": "274420"
  },
  {
    "text": "is under allocation which is what we were experiencing in our case this is where you don't your limits aren't high",
    "start": "274420",
    "end": "281860"
  },
  {
    "text": "enough so that when you you apply the load that you're getting from your traffic kubernetes just causes this",
    "start": "281860",
    "end": "288820"
  },
  {
    "text": "thing to to crash because you you haven't given it enough memory or maybe it ran out of CPU another one is over",
    "start": "288820",
    "end": "297190"
  },
  {
    "text": "allocation and this is where you set the limits to high this this is a trickier",
    "start": "297190",
    "end": "302740"
  },
  {
    "text": "problem to to spot because things aren't going to break in a very obvious way and",
    "start": "302740",
    "end": "308920"
  },
  {
    "text": "it really becomes a problem when you start to scale up replicas because you could imagine let's say you you waste 10",
    "start": "308920",
    "end": "317320"
  },
  {
    "text": "negative memory for every replicas you scale that up to a thousand you've made",
    "start": "317320",
    "end": "323260"
  },
  {
    "text": "your problem with thousand times worse and this this could be mean all the difference between running more more",
    "start": "323260",
    "end": "330520"
  },
  {
    "text": "pods on your nodes in this case you'd be running an extra pod if you had set your",
    "start": "330520",
    "end": "335650"
  },
  {
    "text": "constraints appropriately even",
    "start": "335650",
    "end": "340990"
  },
  {
    "text": "allocation so this is this is something that you should strive for this would be",
    "start": "340990",
    "end": "346570"
  },
  {
    "text": "a perfect allocation you're 100% you're utilizing 100% of your resources and and",
    "start": "346570",
    "end": "352810"
  },
  {
    "text": "your and you're saving your cost savings is maximized this is something that you",
    "start": "352810",
    "end": "358390"
  },
  {
    "text": "should work towards but again this is one of those that good enough is",
    "start": "358390",
    "end": "363970"
  },
  {
    "text": "probably good enough and striving for perfect might not be great in your in your use case so the next thing to think",
    "start": "363970",
    "end": "373420"
  },
  {
    "text": "about is the way that kubernetes does monitoring so this is kind of to",
    "start": "373420",
    "end": "379960"
  },
  {
    "text": "highlight the the system as a whole I'm gonna go through what this looks like on",
    "start": "379960",
    "end": "385180"
  },
  {
    "text": "an individual node but it's important to note that there's multiple nodes there's a master and there's also the storage",
    "start": "385180",
    "end": "391150"
  },
  {
    "text": "back-end that connects to this thing called heap stir so the very first layer",
    "start": "391150",
    "end": "397270"
  },
  {
    "text": "is see advisor and this runs on each of the nodes and what its responsibility is",
    "start": "397270",
    "end": "402610"
  },
  {
    "text": "is to collect metrics for each of the pods that are running from docker and it",
    "start": "402610",
    "end": "409600"
  },
  {
    "text": "collects metrics like CPU memory it collects information on the file system",
    "start": "409600",
    "end": "414970"
  },
  {
    "text": "and there's some other things that that it does as well but the important thing",
    "start": "414970",
    "end": "420580"
  },
  {
    "text": "here is now with that information the couplet makes decisions on what to do",
    "start": "420580",
    "end": "425950"
  },
  {
    "text": "with pods based off of what's the advisor tells it so then from there",
    "start": "425950",
    "end": "431970"
  },
  {
    "text": "something that you can it's an add-on but if you add heap stir unto your cluster it's going to aggregate all of",
    "start": "431970",
    "end": "439270"
  },
  {
    "text": "the metrics from the couplets and also has different backends for storage the",
    "start": "439270",
    "end": "445150"
  },
  {
    "text": "default is something called influx DB and heat stir allows you to visualize what's going on at the cluster level so",
    "start": "445150",
    "end": "455380"
  },
  {
    "text": "when you're setting limits and this is this is buffers approach it's we're",
    "start": "455380",
    "end": "461860"
  },
  {
    "text": "trying to understand just what one one pod can handle so one replica and we start with a conservative set of limits",
    "start": "461860",
    "end": "468550"
  },
  {
    "text": "and then we'll run some tests and I'll talk about what type of tests those are",
    "start": "468550",
    "end": "474240"
  },
  {
    "text": "and we'll adjust those limits until we find find the the limits that work for",
    "start": "474240",
    "end": "481660"
  },
  {
    "text": "us and we're only going to change one thing at a time and observe the changes that way you don't have too many",
    "start": "481660",
    "end": "487090"
  },
  {
    "text": "variables so there's a couple of different type of testing strategies that we employ so the first one is where",
    "start": "487090",
    "end": "495790"
  },
  {
    "start": "489000",
    "end": "530000"
  },
  {
    "text": "we slowly ramp up the traffic we start from no traffic and we slowly increase it until we find this point where it",
    "start": "495790",
    "end": "502810"
  },
  {
    "text": "breaks and then once we find the point where it breaks we're going to run",
    "start": "502810",
    "end": "509920"
  },
  {
    "text": "something called a duration test and we're gonna run that test just under the breaking point and at this point we're",
    "start": "509920",
    "end": "516820"
  },
  {
    "text": "going to be looking for things like memory leaks maybe unpredictable",
    "start": "516820",
    "end": "522940"
  },
  {
    "text": "maybe there's there's a queue that gets filled up there it depends on what you're building but everything kind of",
    "start": "522940",
    "end": "528550"
  },
  {
    "text": "has differ modes so I'm gonna do a live demo here",
    "start": "528550",
    "end": "535029"
  },
  {
    "start": "530000",
    "end": "593000"
  },
  {
    "text": "so alright and I'm gonna set limits on",
    "start": "536079",
    "end": "541399"
  },
  {
    "text": "at CD and this is a tool that we've got open source called coop scope I did this",
    "start": "541399",
    "end": "548690"
  },
  {
    "text": "demo last year and to get this metric or to get all this information before I had",
    "start": "548690",
    "end": "553790"
  },
  {
    "text": "to use port forwarding and get the information from C advisor directly this",
    "start": "553790",
    "end": "559220"
  },
  {
    "text": "tool gives you a little more information and so you can see here that I've got",
    "start": "559220",
    "end": "566300"
  },
  {
    "text": "CPU this red line is what the what the limits currently set and the blue line",
    "start": "566300",
    "end": "571639"
  },
  {
    "text": "is what the current utilization of the container so you can see that I've got",
    "start": "571639",
    "end": "577639"
  },
  {
    "text": "25 M of CPU and that's approximately 25 1,000th of a CPU core and also 5 mega",
    "start": "577639",
    "end": "588920"
  },
  {
    "text": "memory so I'm gonna apply some load and",
    "start": "588920",
    "end": "594009"
  },
  {
    "start": "593000",
    "end": "990000"
  },
  {
    "text": "we use a tool called loader IO and before I get into this at C D doesn't",
    "start": "594009",
    "end": "600620"
  },
  {
    "text": "really have a way to so the loader IO needs a token if you do decide to do this we've got a let me increase the",
    "start": "600620",
    "end": "609709"
  },
  {
    "text": "font here can everybody see that coming",
    "start": "609709",
    "end": "620569"
  },
  {
    "text": "into our cluster we have an engine X server that's sitting in front of that CD and that just allows us to serve our",
    "start": "620569",
    "end": "627889"
  },
  {
    "text": "loader tokens we can authenticate with loader do in order to run these load",
    "start": "627889",
    "end": "634040"
  },
  {
    "text": "tests so I'm gonna run this test",
    "start": "634040",
    "end": "640240"
  },
  {
    "text": "and what we should see here is that you",
    "start": "645410",
    "end": "650749"
  },
  {
    "text": "can already see that the memory utilization was pretty close container",
    "start": "650749",
    "end": "656929"
  },
  {
    "text": "has died that's exactly what I'm expecting here unfortunately I wasn't",
    "start": "656929",
    "end": "662869"
  },
  {
    "text": "quick enough but what what ended up happening there so if I describe the pod",
    "start": "662869",
    "end": "671109"
  },
  {
    "text": "Oh No",
    "start": "680730",
    "end": "683570"
  },
  {
    "text": "and I take a look at the",
    "start": "691770",
    "end": "695870"
  },
  {
    "text": "the last state so you could see the last terminated stay here and you can see that the reason was that we actually got",
    "start": "703490",
    "end": "710330"
  },
  {
    "text": "om killed so we can see that the memory we ran out of memory in this case so the",
    "start": "710330",
    "end": "715850"
  },
  {
    "text": "the next thing that we'll do is we'll edit the deployment and we're going to increase the memory here I'm going to",
    "start": "715850",
    "end": "722300"
  },
  {
    "text": "set up something so we can just watch",
    "start": "722300",
    "end": "725529"
  },
  {
    "text": "and you can also see that the @cd container was restarted here too that's",
    "start": "732430",
    "end": "737890"
  },
  {
    "text": "another indication that something went wrong so I'm gonna edit the deployment",
    "start": "737890",
    "end": "744990"
  },
  {
    "text": "and at this stage I'm making macro adjustments so I'm going to increase",
    "start": "746940",
    "end": "752530"
  },
  {
    "text": "this by a factor of 10 so we should see",
    "start": "752530",
    "end": "764770"
  },
  {
    "text": "that retainer get restarted",
    "start": "764770",
    "end": "768510"
  },
  {
    "text": "all right so the containers creating",
    "start": "778540",
    "end": "782820"
  },
  {
    "text": "so I'm going to go here and I'm going to find okay we've got the new @cd that's up and running you can see that before",
    "start": "795470",
    "end": "802760"
  },
  {
    "text": "this was five now this is set to 50 and now when we rerun this test we should",
    "start": "802760",
    "end": "811790"
  },
  {
    "text": "see that we've got much more room to breathe there and instead of just",
    "start": "811790",
    "end": "819170"
  },
  {
    "text": "crashing immediately this test is going to complete and you can see here so this",
    "start": "819170",
    "end": "826550"
  },
  {
    "text": "green line is the amount of traffic that's being sent over through loader and you can see that we've got a spike",
    "start": "826550",
    "end": "832580"
  },
  {
    "text": "here in the CPU and the blue line is the response time so you can see as the",
    "start": "832580",
    "end": "839240"
  },
  {
    "text": "response time as a request the number of requests simultaneously increases so",
    "start": "839240",
    "end": "846230"
  },
  {
    "text": "does the request time with this case that means we've probably got another bottleneck here and with Kubb scope here",
    "start": "846230",
    "end": "853970"
  },
  {
    "text": "you can tell that we are hitting we're definitely hitting a CPU bottleneck so",
    "start": "853970",
    "end": "859880"
  },
  {
    "text": "that'll be the next thing that we go and adjust so again and after this test",
    "start": "859880",
    "end": "865220"
  },
  {
    "text": "completes here I'm going to increase the CPU by a factor of 10 and see what",
    "start": "865220",
    "end": "870890"
  },
  {
    "text": "happens",
    "start": "870890",
    "end": "873220"
  },
  {
    "text": "so I'm going to set this to roughly a quarter of a CPU core and I'm expecting",
    "start": "887810",
    "end": "896010"
  },
  {
    "text": "this to break here but I've got a really really small cluster here with one node and in one CPU core so if I describe",
    "start": "896010",
    "end": "906030"
  },
  {
    "text": "that pot again you can see that in the",
    "start": "906030",
    "end": "915480"
  },
  {
    "text": "events log the default scheduler is telling us that we failed the schedule and you can see that it doesn't have",
    "start": "915480",
    "end": "922710"
  },
  {
    "text": "enough CPU so I've hit one of my natural limits with my system so I need no I",
    "start": "922710",
    "end": "929070"
  },
  {
    "text": "need to I can't quite give it 250 but I'd probably go back a little bit and",
    "start": "929070",
    "end": "936030"
  },
  {
    "text": "I'd add at the deployment again to",
    "start": "936030",
    "end": "941280"
  },
  {
    "text": "something more reasonable for this system so it was that 25 let's do 50",
    "start": "941280",
    "end": "948710"
  },
  {
    "text": "okay that's terminating and while that's starting up let's just take a look at",
    "start": "951500",
    "end": "957120"
  },
  {
    "text": "the response time here so we're looking at something like 1 second response times and well that's that's not great",
    "start": "957120",
    "end": "964200"
  },
  {
    "text": "but we should expect that if we actually are fixing our bottleneck that the response times are gonna go down and",
    "start": "964200",
    "end": "971010"
  },
  {
    "text": "since we gave roughly twice the resources for CPU and if that's actually",
    "start": "971010",
    "end": "976530"
  },
  {
    "text": "the bottleneck we'd expect that to increase somewhat linearly it's not going to be perfect and it really",
    "start": "976530",
    "end": "982170"
  },
  {
    "text": "depends on the system ok that's back up and running I'm gonna",
    "start": "982170",
    "end": "987330"
  },
  {
    "text": "run another load test so remember about a second let me go back and visualize",
    "start": "987330",
    "end": "998100"
  },
  {
    "start": "990000",
    "end": "1124000"
  },
  {
    "text": "this while this is going on too",
    "start": "998100",
    "end": "1001420"
  },
  {
    "text": "you can see that CPU is creeping up again [Music]",
    "start": "1008490",
    "end": "1014119"
  },
  {
    "text": "and if I were to compare this side by side with the other response times I",
    "start": "1021090",
    "end": "1026310"
  },
  {
    "text": "would expect yeah we're still creeping up because we've had our bottleneck but we're not creeping up quite as much if",
    "start": "1026310",
    "end": "1034439"
  },
  {
    "text": "all goes well we should see an average right around 5 to 600 milliseconds on the request but if if if I had more",
    "start": "1034440",
    "end": "1044579"
  },
  {
    "text": "resources I could keep doing this process over and over again until I hit this point where I'm where the CPU isn't",
    "start": "1044580",
    "end": "1052590"
  },
  {
    "text": "isn't pegged and wherever the whoever that wouldn't that that setting is you",
    "start": "1052590",
    "end": "1058440"
  },
  {
    "text": "kind of find this maximum this maximum amount of traffic that one container can",
    "start": "1058440",
    "end": "1064290"
  },
  {
    "text": "handle and once you've hit that maximum amount of traffic the next thing you'll do is you'll run a duration test and I'm",
    "start": "1064290",
    "end": "1071670"
  },
  {
    "text": "just going to run an example duration test here but you'll run just under that",
    "start": "1071670",
    "end": "1078810"
  },
  {
    "text": "breaking point for an extended period of time and again what you're looking for",
    "start": "1078810",
    "end": "1084120"
  },
  {
    "text": "things like memory leaks queues being filled variants and response times are",
    "start": "1084120",
    "end": "1091560"
  },
  {
    "text": "indications of these sorts of things to a lot of interesting stuff can happen at",
    "start": "1091560",
    "end": "1097620"
  },
  {
    "text": "this stage too and and while while you're doing these sorts of tests instead of modifying things by a factor",
    "start": "1097620",
    "end": "1104640"
  },
  {
    "text": "a tenure you're you're making smaller adjustments at this point you wouldn't necessarily want to increase things by a",
    "start": "1104640",
    "end": "1111300"
  },
  {
    "text": "factor at ten unless you had a really good reason to do it and again you might want to go back to your ramp up test",
    "start": "1111300",
    "end": "1116580"
  },
  {
    "text": "when you do this sort of thing so I'm",
    "start": "1116580",
    "end": "1122310"
  },
  {
    "text": "going to jump back in so while you're doing this it's important to keep a fail",
    "start": "1122310",
    "end": "1128970"
  },
  {
    "start": "1124000",
    "end": "1380000"
  },
  {
    "text": "log and that's something that you're going to want to share with the team this is both qualitative and",
    "start": "1128970",
    "end": "1136590"
  },
  {
    "text": "quantitative information about how the thing broke and this is going to be",
    "start": "1136590",
    "end": "1142320"
  },
  {
    "text": "really important when you're making your your run books because some you're not",
    "start": "1142320",
    "end": "1148560"
  },
  {
    "text": "always going to be on call somebody else can look at your at your fail log and",
    "start": "1148560",
    "end": "1153930"
  },
  {
    "text": "say okay we know that this thing is failing in an expected way I probably just need to",
    "start": "1153930",
    "end": "1159060"
  },
  {
    "text": "scale up but if it's failing in an unexpected way first off you want to update the fail log to make sure that",
    "start": "1159060",
    "end": "1165990"
  },
  {
    "text": "other people know that this is a particular failure mode but you might want to start looking at other things it",
    "start": "1165990",
    "end": "1172110"
  },
  {
    "text": "could be an issue with the code it could be another issue with the infrastructure with the related service but it's good",
    "start": "1172110",
    "end": "1179250"
  },
  {
    "text": "to keep track of those things so you can understand that so there's there's",
    "start": "1179250",
    "end": "1184800"
  },
  {
    "text": "different failure modes that you can observe one of them is where you've got",
    "start": "1184800",
    "end": "1189870"
  },
  {
    "text": "a memory leak in in memory slowly increasing and then you end up with that end up with that sawtooth pattern that",
    "start": "1189870",
    "end": "1197550"
  },
  {
    "text": "that's that's familiar another one is the CPU is pegged at 100% even after",
    "start": "1197550",
    "end": "1203670"
  },
  {
    "text": "you're running your load tests sometimes the soul this will happen may be accuse",
    "start": "1203670",
    "end": "1209100"
  },
  {
    "text": "filled up or maybe there's some process that just got hung another another classic one is you just see a bunch of",
    "start": "1209100",
    "end": "1215640"
  },
  {
    "text": "500s you see high response times and a",
    "start": "1215640",
    "end": "1222000"
  },
  {
    "text": "stranger one is a large variance and response times that one can happen with queuing and then the last one is just",
    "start": "1222000",
    "end": "1229920"
  },
  {
    "text": "requests just they just get dropped and you never get a response so some of the",
    "start": "1229920",
    "end": "1238710"
  },
  {
    "text": "stuff that we learned through this process was that scaling up your",
    "start": "1238710",
    "end": "1244320"
  },
  {
    "text": "replicas isn't going to solve scaling issues for stateless it's not always",
    "start": "1244320",
    "end": "1249930"
  },
  {
    "text": "going to solve scaling issues for a stateless compiler services and we also",
    "start": "1249930",
    "end": "1257400"
  },
  {
    "text": "learned that there's a lot of different ways that applications can fail and keeping a fail log is actually a really",
    "start": "1257400",
    "end": "1264870"
  },
  {
    "text": "good practice that keeps it keeps the teams closer together it gives something for systems and developers dev teams to",
    "start": "1264870",
    "end": "1274770"
  },
  {
    "text": "talk about so products product can work closer it's a communication point between product and systems",
    "start": "1274770",
    "end": "1283440"
  },
  {
    "text": "and really it's about increasing the predictability of your system if you're not setting limits on your containers",
    "start": "1283440",
    "end": "1290960"
  },
  {
    "text": "unexpected things can happen things can take more resources than you expected",
    "start": "1290960",
    "end": "1297470"
  },
  {
    "text": "so just looking ahead at kubernetes all the tools that we have right now are a",
    "start": "1297470",
    "end": "1303779"
  },
  {
    "text": "huge step forward for ops and cluster wide operations it's it there's never",
    "start": "1303779",
    "end": "1310289"
  },
  {
    "text": "been a better time to be an ops ops person but I think that there's still a",
    "start": "1310289",
    "end": "1315629"
  },
  {
    "text": "huge opportunity for developers to get involved here and if if ssh is or if if",
    "start": "1315629",
    "end": "1323940"
  },
  {
    "text": "coop cuttle is the new ssh these tools there's there's some tools that we can",
    "start": "1323940",
    "end": "1329129"
  },
  {
    "text": "build to help developers visualize what's going on and and instead set things like limits so that's that's kind",
    "start": "1329129",
    "end": "1336179"
  },
  {
    "text": "of why I built I hacked together Kubb scope I'm gonna be spending more time on",
    "start": "1336179",
    "end": "1341429"
  },
  {
    "text": "this and if it's something that anybody here is interested in looking for pull request feedback anything helps yeah I'd",
    "start": "1341429",
    "end": "1350850"
  },
  {
    "text": "like to open it up for any questions and thanks everybody [Applause]",
    "start": "1350850",
    "end": "1362029"
  },
  {
    "text": "so the question was it stopped me if I if I don't quite get this right is that",
    "start": "1379900",
    "end": "1386710"
  },
  {
    "start": "1380000",
    "end": "1510000"
  },
  {
    "text": "how do you judge how to set the actual",
    "start": "1386710",
    "end": "1391880"
  },
  {
    "text": "limits once I know where the breaking point is maybe I run more pods at half",
    "start": "1391880",
    "end": "1398540"
  },
  {
    "text": "of the breaking point I run twice as many pods is that so I",
    "start": "1398540",
    "end": "1404060"
  },
  {
    "text": "think that kind of depends on on your on your business case here for us we want",
    "start": "1404060",
    "end": "1411950"
  },
  {
    "text": "to run this few nodes as possible to keep costs down and we're running things",
    "start": "1411950",
    "end": "1417500"
  },
  {
    "text": "pretty close to the breaking point at all times and and that that keeps the",
    "start": "1417500",
    "end": "1422750"
  },
  {
    "text": "cost down for us if you have more capacity maybe you have more bursty data it might make more sense to have twice",
    "start": "1422750",
    "end": "1431150"
  },
  {
    "text": "as much capacity as you need for instance yes",
    "start": "1431150",
    "end": "1438160"
  },
  {
    "text": "what was that called pod okay so the",
    "start": "1444950",
    "end": "1454890"
  },
  {
    "text": "question was am i familiar with pod vertical sorry what was it again I'm not familiar with those by the way oops yeah",
    "start": "1454890",
    "end": "1463140"
  },
  {
    "text": "I'm not familiar with those yes",
    "start": "1463140",
    "end": "1468650"
  },
  {
    "text": "did I have a replica set two three oh",
    "start": "1477850",
    "end": "1481710"
  },
  {
    "text": "yeah yeah I had watch hooked up if there's three that would've been weird",
    "start": "1485460",
    "end": "1491789"
  },
  {
    "text": "yes",
    "start": "1492090",
    "end": "1495090"
  },
  {
    "start": "1510000",
    "end": "1590000"
  },
  {
    "text": "so that's one the question is how do I know when I've gone too far how do I know when I've given something too much",
    "start": "1511540",
    "end": "1517660"
  },
  {
    "text": "there's a couple of different ways that you can know that the first one is that even though you're giving a pod more CPU",
    "start": "1517660",
    "end": "1528880"
  },
  {
    "text": "for instance that's one of the things that we kind of ran out of resources because the cluster was small but if you",
    "start": "1528880",
    "end": "1534670"
  },
  {
    "text": "had more you could what you'll see is your what your requests will end up",
    "start": "1534670",
    "end": "1541330"
  },
  {
    "text": "staying the same you keep throwing more resources at it and your your response times will end up being about the same",
    "start": "1541330",
    "end": "1549160"
  },
  {
    "text": "as they were before yes yep",
    "start": "1549160",
    "end": "1556650"
  },
  {
    "text": "so the question is I might adjust that a little bit to be like what if I write an",
    "start": "1589730",
    "end": "1595070"
  },
  {
    "start": "1590000",
    "end": "1677000"
  },
  {
    "text": "app that is inherently performant but its resource limits are extremely high",
    "start": "1595070",
    "end": "1603070"
  },
  {
    "text": "that's a trickier one I think it I can't",
    "start": "1603190",
    "end": "1609230"
  },
  {
    "text": "I think it kind of depends on on the individual case like I'm thinking of compute heavy stuff like I don't know if",
    "start": "1609230",
    "end": "1615860"
  },
  {
    "text": "highly parallelizable tasks like machine learning might be an example of one of those things",
    "start": "1615860",
    "end": "1620900"
  },
  {
    "text": "I would start looking at like GPU acceleration in that case but it I think it really depends on the individual app",
    "start": "1620900",
    "end": "1627380"
  },
  {
    "text": "if it can't be split up maybe it does actually need that much resources and",
    "start": "1627380",
    "end": "1632540"
  },
  {
    "text": "and you just cut it's kind of the state of state of what the world you live in",
    "start": "1632540",
    "end": "1639580"
  },
  {
    "text": "yes",
    "start": "1639760",
    "end": "1642760"
  },
  {
    "start": "1677000",
    "end": "1805000"
  },
  {
    "text": "so I I read that question as have a ball have we automated this and if and also",
    "start": "1678019",
    "end": "1686690"
  },
  {
    "text": "could this be automated is that okay we haven't automated this process yet",
    "start": "1686690",
    "end": "1693049"
  },
  {
    "text": "and we have been looking in how to automate this and the tricky part isn't",
    "start": "1693049",
    "end": "1700789"
  },
  {
    "text": "actually running the tests it's actually the starting point and that that that requires some domain knowledge about",
    "start": "1700789",
    "end": "1708590"
  },
  {
    "text": "what language you're using like node versus Java containers are going to have a different starting point because I",
    "start": "1708590",
    "end": "1716320"
  },
  {
    "text": "suppose you could just start from a really small number and slowly increase it but like loader i/o for instance has",
    "start": "1716320",
    "end": "1722269"
  },
  {
    "text": "an API that you can trigger things and kick off these tests you you could",
    "start": "1722269",
    "end": "1727999"
  },
  {
    "text": "probably you could probably use that API and some tooling to adjust those those",
    "start": "1727999",
    "end": "1735619"
  },
  {
    "text": "limits so you run a test and if the the requests look ok you can look at the mean and variance and the average",
    "start": "1735619",
    "end": "1741289"
  },
  {
    "text": "requests and and see how that changes versus see how that changes you could",
    "start": "1741289",
    "end": "1748999"
  },
  {
    "text": "probably automate this with with the api's yes yeah this starting the",
    "start": "1748999",
    "end": "1757070"
  },
  {
    "text": "starting point is the tricky part I think yes",
    "start": "1757070",
    "end": "1765518"
  },
  {
    "text": "I don't quite follow what you're asking",
    "start": "1780679",
    "end": "1786970"
  },
  {
    "start": "1805000",
    "end": "1856000"
  },
  {
    "text": "yeah so the question is is there stuff that I could do before putting this out on kubernetes could I instrument this",
    "start": "1806710",
    "end": "1812980"
  },
  {
    "text": "locally and and make sure that my application is super performant",
    "start": "1812980",
    "end": "1818249"
  },
  {
    "text": "yeah I think I think that's that's the the other side of this as well this is",
    "start": "1818249",
    "end": "1823690"
  },
  {
    "text": "kind of like after you've got a container after you've got a container",
    "start": "1823690",
    "end": "1831850"
  },
  {
    "text": "and somebody it was kind of that assumption there that that the app is relatively performant then you go and do",
    "start": "1831850",
    "end": "1839980"
  },
  {
    "text": "this process but it would be it would be very good to do that before starting this process yes",
    "start": "1839980",
    "end": "1848850"
  },
  {
    "start": "1856000",
    "end": "1910000"
  },
  {
    "text": "so I think the question is would you would you run this in production for long so we actually run this in",
    "start": "1856270",
    "end": "1865400"
  },
  {
    "text": "production along with the rest of all of our stuff and the reason why is that",
    "start": "1865400",
    "end": "1871000"
  },
  {
    "text": "crosstalk between services is something that we also want to observe so if this pod fails and causes other things to",
    "start": "1871000",
    "end": "1878720"
  },
  {
    "text": "fail maybe like DNS is is causing issues",
    "start": "1878720",
    "end": "1884060"
  },
  {
    "text": "across the system we'd actually want to know that before before putting other",
    "start": "1884060",
    "end": "1890660"
  },
  {
    "text": "traffic onto it yes",
    "start": "1890660",
    "end": "1895450"
  },
  {
    "start": "1910000",
    "end": "1945000"
  },
  {
    "text": "we so the question is do we have something that's got more like non",
    "start": "1911820",
    "end": "1918779"
  },
  {
    "text": "non-trivial like maybe it's got does a lot of reads and writes maybe it makes a",
    "start": "1918779",
    "end": "1925090"
  },
  {
    "text": "request to a third party service maybe it's a database not in the same endpoint so like maybe you've got a complicated",
    "start": "1925090",
    "end": "1932049"
  },
  {
    "text": "endpoint that so we don't really do this",
    "start": "1932049",
    "end": "1951549"
  },
  {
    "start": "1945000",
    "end": "2024000"
  },
  {
    "text": "on the we're trying to avoid doing things like that because that that's",
    "start": "1951549",
    "end": "1958990"
  },
  {
    "text": "kind of our starting point like if you think of like our monolith is really is really that it's really difficult to",
    "start": "1958990",
    "end": "1965529"
  },
  {
    "text": "load test that without putting users on to it but it is it's much easier to say",
    "start": "1965529",
    "end": "1970600"
  },
  {
    "text": "focus on one of those endpoints pull it out and then load test that one endpoint which is kind of what what got us on the",
    "start": "1970600",
    "end": "1977799"
  },
  {
    "text": "micro-services journey in the first place yes",
    "start": "1977799",
    "end": "1984240"
  },
  {
    "text": "yeah that's a that's a good question we we actually don't test that and you I",
    "start": "1989450",
    "end": "1996860"
  },
  {
    "text": "think you could use this you'd have to have the place that the ingress",
    "start": "1996860",
    "end": "2002140"
  },
  {
    "text": "terminates to probably do something really like a hello world app or something but yeah that would be it",
    "start": "2002140",
    "end": "2008590"
  },
  {
    "text": "would be cool to see what that looks like",
    "start": "2008590",
    "end": "2011610"
  },
  {
    "text": "thanks everybody [Applause]",
    "start": "2020120",
    "end": "2026789"
  }
]