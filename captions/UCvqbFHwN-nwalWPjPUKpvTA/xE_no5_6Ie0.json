[
  {
    "text": "hello everyone I'm um I'm Von sear and this is my colleague Al we're both from",
    "start": "160",
    "end": "5920"
  },
  {
    "text": "Nvidia and we are part of the GeForce now Cloud um our talk is a tale of two",
    "start": "5920",
    "end": "11440"
  },
  {
    "text": "drivers so let's get started so in our talk we're going to",
    "start": "11440",
    "end": "16960"
  },
  {
    "text": "talk about what GeForce now cloud is what are the use cases we're trying to solve for it um how we run GPU workloads",
    "start": "16960",
    "end": "25160"
  },
  {
    "text": "with Device plugins what are the problems we see with it and then how do ra uh solves a lot of our issues and",
    "start": "25160",
    "end": "33520"
  },
  {
    "text": "then we'll end our talk with the demo and we'll have some QA after",
    "start": "33520",
    "end": "39360"
  },
  {
    "text": "that so gForce now is a cloud gaming service that enables end users",
    "start": "39360",
    "end": "47120"
  },
  {
    "text": "to run games on the cloud and we run our",
    "start": "47120",
    "end": "53160"
  },
  {
    "text": "Cloud on the kubernetes stack and we use a bunch of Open Source uh to enable us",
    "start": "53160",
    "end": "59160"
  },
  {
    "text": "so we use Cube work for VM management we use aen kubernetes for our cni fluent",
    "start": "59160",
    "end": "66240"
  },
  {
    "text": "bit Prometheus for our observability and then Opa and gatekeeper for policy management and we",
    "start": "66240",
    "end": "74400"
  },
  {
    "text": "have a pretty big scale we run about 40 data centers with about 30,000",
    "start": "74400",
    "end": "81200"
  },
  {
    "text": "nodes and 60,000 gpus and then we run about a million VMS daily",
    "start": "81200",
    "end": "90880"
  },
  {
    "text": "so I said we run virtual machines but uh kubernetes doesn't support it so we have",
    "start": "90920",
    "end": "97759"
  },
  {
    "text": "cuboard which runs VMS as part of kubernetes and",
    "start": "97759",
    "end": "103799"
  },
  {
    "text": "then this VM is just a custom resource that Cube word translates it into a pod",
    "start": "103799",
    "end": "110840"
  },
  {
    "text": "and then it runs the uh your it runs the virtual machine using the KVM hypervisor",
    "start": "110840",
    "end": "116920"
  },
  {
    "text": "on the nodes so some of the use cases that we have are",
    "start": "116920",
    "end": "123320"
  },
  {
    "text": "let's say you're a user and you want to play games with the highest FPS that is",
    "start": "123320",
    "end": "129000"
  },
  {
    "text": "possible and you want to have the highest Graphics that your games can support so the the way we run it for you",
    "start": "129000",
    "end": "136160"
  },
  {
    "text": "is we'd spin up a very large VM and we'll give it a full GPU",
    "start": "136160",
    "end": "142440"
  },
  {
    "text": "and do a pass through of the GPU to your VM and then we'll stream your game uh",
    "start": "142440",
    "end": "148200"
  },
  {
    "text": "from our data center to your compter computer now if you don't want that and",
    "start": "148200",
    "end": "154239"
  },
  {
    "text": "you just want uh let's say 60 FPS and",
    "start": "154239",
    "end": "160159"
  },
  {
    "text": "just about 1080p resolutions then we'd spin up a much smaller VM we'll give it",
    "start": "160159",
    "end": "166800"
  },
  {
    "text": "like half a GPU and then we'll stream it to you and then if you just want to try",
    "start": "166800",
    "end": "172920"
  },
  {
    "text": "what GeForce now is then we'll run a really small VM and then give it a",
    "start": "172920",
    "end": "178480"
  },
  {
    "text": "quarter GPU and and then you can play around and try to see what GeForce now",
    "start": "178480",
    "end": "185120"
  },
  {
    "text": "offers but when you play games we all know that uh these games are very lag",
    "start": "185120",
    "end": "191319"
  },
  {
    "text": "sensitive you don't want your games to stter if your experience needs to be",
    "start": "191319",
    "end": "196560"
  },
  {
    "text": "good these virtual machines that we run for you need to be highly optimized for",
    "start": "196560",
    "end": "203319"
  },
  {
    "text": "performance so what we have to do is uh get all the virtual machine to be fully",
    "start": "203319",
    "end": "210519"
  },
  {
    "text": "fully tuned for for Optimal Performance yeah so we have all these",
    "start": "210519",
    "end": "218040"
  },
  {
    "text": "use cases and then what it boils down to is that we have a bunch of different VM",
    "start": "218040",
    "end": "223599"
  },
  {
    "text": "shapes you want to run and then we have to configure our GPU to support each of",
    "start": "223599",
    "end": "229920"
  },
  {
    "text": "these VM shapes so we'd either give it a full GPU or we' carve it up into small",
    "start": "229920",
    "end": "236560"
  },
  {
    "text": "slices of virtual gpus and then run your VMS with it so how do we do that uh so",
    "start": "236560",
    "end": "242840"
  },
  {
    "text": "we know that device plugins if you're familiar with it lets you advertise",
    "start": "242840",
    "end": "248120"
  },
  {
    "text": "these gpus as custom resources so when when we use a device plugin to discover",
    "start": "248120",
    "end": "254560"
  },
  {
    "text": "a GPU we publish it to your APS server that we have this GPU and this is",
    "start": "254560",
    "end": "261720"
  },
  {
    "text": "account of that GPU and that's all the aps Serv Rees now for our use case we have to",
    "start": "261720",
    "end": "271000"
  },
  {
    "text": "have multiple device plugins for the same GPU because we have the all these different",
    "start": "271000",
    "end": "276520"
  },
  {
    "text": "configurations and and so we have one for pass through and then we have one",
    "start": "276520",
    "end": "282960"
  },
  {
    "text": "for slicing up the GPU into half and then we have one more for slicing it",
    "start": "282960",
    "end": "288120"
  },
  {
    "text": "into a quarter and finally uh we also need to align it with CPUs so we run yet another",
    "start": "288120",
    "end": "295800"
  },
  {
    "text": "device plugin that understands the CPUs on your note and publishes",
    "start": "295800",
    "end": "303199"
  },
  {
    "text": "that and to to let the API server know what the Numa topology of all these",
    "start": "303840",
    "end": "310639"
  },
  {
    "text": "devices are uh we use this other component called node feature Discovery",
    "start": "310639",
    "end": "317080"
  },
  {
    "text": "and we publish the Numa information from our plugins to node feature and then",
    "start": "317080",
    "end": "323440"
  },
  {
    "text": "this publishes Uh custom resource called node resource topology to the API server",
    "start": "323440",
    "end": "330479"
  },
  {
    "text": "Now using the resource information from the plugins and the node resource",
    "start": "330479",
    "end": "336600"
  },
  {
    "text": "topology we can now schedule workloads that uh find that pig nodes that uh that",
    "start": "336600",
    "end": "345199"
  },
  {
    "text": "are aligned on Numa now once we have our scheduler that",
    "start": "345199",
    "end": "352080"
  },
  {
    "text": "uh assigns your workload to a node on the Node itself the cube sees your",
    "start": "352080",
    "end": "358520"
  },
  {
    "text": "workload and it's sees what are the resources it needs and then it calls the corresponding device plugins to to",
    "start": "358520",
    "end": "365759"
  },
  {
    "text": "allocate your resource and then your device plugins do the work of configuring your GPU in in that",
    "start": "365759",
    "end": "372520"
  },
  {
    "text": "particular flavor and then it gives it back to the Pod and when your pod starts",
    "start": "372520",
    "end": "379160"
  },
  {
    "text": "we have cuboard that that detects all these resources that sees that we have a",
    "start": "379160",
    "end": "385479"
  },
  {
    "text": "VM that we want to run uh with this particular G CPU configuration with this",
    "start": "385479",
    "end": "392160"
  },
  {
    "text": "number of CPUs and with this memory and then it spins up a virtual machine uh on",
    "start": "392160",
    "end": "398759"
  },
  {
    "text": "your host uh using libber as its uh thin",
    "start": "398759",
    "end": "404960"
  },
  {
    "text": "library and then we have our guest running using all these devices that we",
    "start": "404960",
    "end": "411919"
  },
  {
    "text": "requested now on the surface all this looks great",
    "start": "411919",
    "end": "417599"
  },
  {
    "text": "U yeah so yeah everything works uh we know we have been running this for 2",
    "start": "417599",
    "end": "423240"
  },
  {
    "text": "years now so everything's been working but we know the pains over what we had to go through to get here and underneath",
    "start": "423240",
    "end": "430720"
  },
  {
    "text": "the surface we know that it's a hot mess and and something has to change and",
    "start": "430720",
    "end": "438360"
  },
  {
    "text": "what we found with Device plugins what are the problems we saw is that",
    "start": "438360",
    "end": "443960"
  },
  {
    "text": "first we have all these device plugins that we have to manage and it's all",
    "start": "443960",
    "end": "450639"
  },
  {
    "text": "advertising the same Hardware we have just one GPU but all these device plugins are are advertising these",
    "start": "450639",
    "end": "458039"
  },
  {
    "text": "different resources about the same Hardware so now these plugins need to",
    "start": "458039",
    "end": "463520"
  },
  {
    "text": "coordinate with each other because if you allocate a full GPU then none of",
    "start": "463520",
    "end": "468840"
  },
  {
    "text": "your other resources are going to be usable so they have to make sure that all those devices are de",
    "start": "468840",
    "end": "476280"
  },
  {
    "text": "advertised similarly if you have vgpu then if your vgpu is in use then none of",
    "start": "476280",
    "end": "482599"
  },
  {
    "text": "the other GPU types can be allocable so yeah these have to handhold each other",
    "start": "482599",
    "end": "488840"
  },
  {
    "text": "and make sure that they don't double book your",
    "start": "488840",
    "end": "494159"
  },
  {
    "text": "devices right so yeah now that you have allocated your device what happens when",
    "start": "495479",
    "end": "501199"
  },
  {
    "text": "your workload goes away right we need something to go and reconfigure your",
    "start": "501199",
    "end": "506360"
  },
  {
    "text": "gpus so that they go back to original state now we want all uh all the different GPU types to be",
    "start": "506360",
    "end": "514279"
  },
  {
    "text": "advertised so that a new workload can can pick any of these and that's our second problem that",
    "start": "514279",
    "end": "522159"
  },
  {
    "text": "there's no delate API in device plugins uh and to do this like in GeForce now we",
    "start": "522159",
    "end": "530360"
  },
  {
    "text": "had to write our own hacks and try to get these uh device plugins",
    "start": "530360",
    "end": "537839"
  },
  {
    "text": "to understand that we a pod went away so that now your GPU has to be freed up and",
    "start": "537839",
    "end": "544600"
  },
  {
    "text": "then they' go back and re-advertise all the devices that they",
    "start": "544600",
    "end": "550079"
  },
  {
    "text": "manage and then the next problem was we know that CU blet does any uh",
    "start": "550360",
    "end": "558360"
  },
  {
    "text": "device plug-in allocation serially it happens at the Pod admission time so any",
    "start": "558360",
    "end": "565120"
  },
  {
    "text": "new part that comes up has to wait for CU BL to allocate the device for the",
    "start": "565120",
    "end": "570519"
  },
  {
    "text": "previous part and then once that work is done that's when a new part can be",
    "start": "570519",
    "end": "576120"
  },
  {
    "text": "admitted but any GPU configuration we do is is very costly like we either we try",
    "start": "576120",
    "end": "583200"
  },
  {
    "text": "to switch the drivers uh or we try to slice that GPU it's going to take a",
    "start": "583200",
    "end": "588640"
  },
  {
    "text": "while and let's say you have a highly dense node with like 8 gpus and you want",
    "start": "588640",
    "end": "595240"
  },
  {
    "text": "to run like 32 quarter GPU workloads at the same time you have to wait for all",
    "start": "595240",
    "end": "602000"
  },
  {
    "text": "32 uh gpus to be allocated before you have all your workloads running and",
    "start": "602000",
    "end": "608040"
  },
  {
    "text": "that's going to take a really long time and that doesn't work for",
    "start": "608040",
    "end": "613680"
  },
  {
    "text": "us and then the other problem is that uh these device plugins they only work with",
    "start": "613959",
    "end": "621880"
  },
  {
    "text": "cuet but then we have our scheduler at the cluster level that's constantly",
    "start": "621880",
    "end": "628360"
  },
  {
    "text": "allocating workloads to nodes and we already said that we are ad double",
    "start": "628360",
    "end": "634880"
  },
  {
    "text": "advertising or triple advertising all our Hardware resources so the Schuler",
    "start": "634880",
    "end": "640079"
  },
  {
    "text": "needs to know that as soon as it allocated a device all the other types",
    "start": "640079",
    "end": "645279"
  },
  {
    "text": "are changing states and currently with Device plugins we can't do that so we end up with a lot of workloads that uh",
    "start": "645279",
    "end": "653959"
  },
  {
    "text": "that get wrongly scheduled and then they go to fail State and then we have to go back and redo all these",
    "start": "653959",
    "end": "661720"
  },
  {
    "text": "workloads and there are many more problems and I don't want to get into that right now but uh this brings us to",
    "start": "661720",
    "end": "671560"
  },
  {
    "text": "how we have Dr and how that actually solves all the problems that we just talked about and I'll take it from here",
    "start": "671560",
    "end": "679680"
  },
  {
    "text": "thank you so just a quick show off hands how many people here are familiar with uh Dr",
    "start": "679680",
    "end": "687720"
  },
  {
    "text": "heard about it tried it yeah quite a few um",
    "start": "687720",
    "end": "695200"
  },
  {
    "text": "so just a quick summary Dr is been worked on in the community Since U some",
    "start": "695200",
    "end": "702000"
  },
  {
    "text": "time now um it's a new API or new way of um allocating third- party um devices",
    "start": "702000",
    "end": "709880"
  },
  {
    "text": "like GPU um that handles many more uh use cases than what traditional device",
    "start": "709880",
    "end": "716279"
  },
  {
    "text": "plugin plugins did it's highly inspired from the PVC API uh",
    "start": "716279",
    "end": "724720"
  },
  {
    "text": "so you can request a device like a PVC and then users can get those devices",
    "start": "724720",
    "end": "733639"
  },
  {
    "text": "allocated at a high level the way this works is anytime an admin decides that",
    "start": "733639",
    "end": "740120"
  },
  {
    "text": "they want to have uh device being allowed in the cluster they bring on uh",
    "start": "740120",
    "end": "746560"
  },
  {
    "text": "resource Dr resource dver that driver first advertises all the",
    "start": "746560",
    "end": "753399"
  },
  {
    "text": "devices it can allocate U and the way it advertises is through",
    "start": "753399",
    "end": "760399"
  },
  {
    "text": "resource slice uh object uh so there is a per node object called resource slice",
    "start": "760399",
    "end": "766399"
  },
  {
    "text": "and in that there will be a list of uh devices that that driver can",
    "start": "766399",
    "end": "771800"
  },
  {
    "text": "manage so once you have the devices then a user comes along and they say hey I",
    "start": "771800",
    "end": "777199"
  },
  {
    "text": "want to use this device right the way they do it is they have an object called",
    "start": "777199",
    "end": "782920"
  },
  {
    "text": "resource claim and in that resource claim they can specify multiple different ways of uh selecting a device",
    "start": "782920",
    "end": "790760"
  },
  {
    "text": "that is done through uh attributes and that attributes are uh advertised by uh",
    "start": "790760",
    "end": "797360"
  },
  {
    "text": "by the a plug-in in the first place so a user comes along user says I",
    "start": "797360",
    "end": "804760"
  },
  {
    "text": "want uh this kind of a device they put it in the uh resource claim uh spec that",
    "start": "804760",
    "end": "811720"
  },
  {
    "text": "resource claim spec also references an object called device class device class has other options to configure that",
    "start": "811720",
    "end": "818440"
  },
  {
    "text": "device in U in the uh plugin and then user references this resource claim",
    "start": "818440",
    "end": "824240"
  },
  {
    "text": "object in the Pod and creates the workload so once a workload is created",
    "start": "824240",
    "end": "829959"
  },
  {
    "text": "and schedular sees it now schedular has a list of devices that were advertised",
    "start": "829959",
    "end": "835320"
  },
  {
    "text": "in the resource slice object and it has a p that wants uh that device to be",
    "start": "835320",
    "end": "840920"
  },
  {
    "text": "consumed so it will make a decision on which uh device it selected and put it",
    "start": "840920",
    "end": "846440"
  },
  {
    "text": "in the resource s status and then it will select the node for it and the Pod",
    "start": "846440",
    "end": "851639"
  },
  {
    "text": "lens on a node once it is on the Node the cubet calls node prepare API uh",
    "start": "851639",
    "end": "859160"
  },
  {
    "text": "that's where the Dr plugin will configure the device GPU in this case",
    "start": "859160",
    "end": "866480"
  },
  {
    "text": "and uh once it is allocated the part is running so with this context we plan to",
    "start": "866480",
    "end": "874199"
  },
  {
    "text": "use this in uh GeForce now stch so how do we do uh how do we do the",
    "start": "874199",
    "end": "882440"
  },
  {
    "text": "migration to Dr so the very first thing is bring along a GPU uh Dr driver that",
    "start": "882440",
    "end": "889680"
  },
  {
    "text": "can advertise this resources in the way uh gForce stch GeForce now stch wants so",
    "start": "889680",
    "end": "896279"
  },
  {
    "text": "the first thing is advertising uh pgp which is entire uh GPU so in the in the",
    "start": "896279",
    "end": "904320"
  },
  {
    "text": "resource slice spec you can see that the driver is uh advertising certain",
    "start": "904320",
    "end": "910360"
  },
  {
    "text": "attributes and a p GPU zero the attributes have um field called type",
    "start": "910360",
    "end": "917720"
  },
  {
    "text": "this is referencing the GPU um it has a PCI address the PCI address here is",
    "start": "917720",
    "end": "925279"
  },
  {
    "text": "important attribute because once the GPU is available able to the uh pod Cube",
    "start": "925279",
    "end": "932079"
  },
  {
    "text": "word has to do additional um additional work on it to pass that GPU to the",
    "start": "932079",
    "end": "940160"
  },
  {
    "text": "VM in order to do that it needs to use this PCI address and that's how it gives",
    "start": "940160",
    "end": "947160"
  },
  {
    "text": "it creates the right Dom XML for the VM to come up inside of the",
    "start": "947160",
    "end": "952720"
  },
  {
    "text": "P another important attribute here is the uh Numa uh d",
    "start": "952720",
    "end": "959920"
  },
  {
    "text": "SL Numa um with a string value of zero so what this shows is this",
    "start": "959920",
    "end": "967959"
  },
  {
    "text": "particular uh GPU is on an Numa node zero and one important thing to call out",
    "start": "967959",
    "end": "975560"
  },
  {
    "text": "is that there is a fully qualified uh domain in front of the attribute that is",
    "start": "975560",
    "end": "982199"
  },
  {
    "text": "important because multiple resource slices from different uh drivers can then share that",
    "start": "982199",
    "end": "990360"
  },
  {
    "text": "uh domain as uh attribute so that you can have a uh",
    "start": "990360",
    "end": "997440"
  },
  {
    "text": "CPU Dr driver with the same attribute and you can align GPU and CPU on the",
    "start": "997440",
    "end": "1003519"
  },
  {
    "text": "Numa node so moving along the same resource",
    "start": "1003519",
    "end": "1008600"
  },
  {
    "text": "slice has uh other devices which is vgpu this is of the type uh 2 is to one",
    "start": "1008600",
    "end": "1017839"
  },
  {
    "text": "you can see it in attribute class um the other attributes that have changed here",
    "start": "1017839",
    "end": "1024760"
  },
  {
    "text": "is the PCI address went away and it was replaced by uuid again this is important",
    "start": "1024760",
    "end": "1030760"
  },
  {
    "text": "because when the Pod uh gets this vgpu Cube word needs to use the uu ID to",
    "start": "1030760",
    "end": "1039640"
  },
  {
    "text": "convert and convert this into a Dom XML and pass it to the",
    "start": "1039640",
    "end": "1044760"
  },
  {
    "text": "VM similarly to uh similar to pgp it also has the Numa node and the type",
    "start": "1044760",
    "end": "1051799"
  },
  {
    "text": "becomes a vgpu the important thing to call out here is because of the use cases even in",
    "start": "1051799",
    "end": "1059240"
  },
  {
    "text": "with a GPU you could have a 4 is to1 GPU like in the yaml below and that becomes",
    "start": "1059240",
    "end": "1065440"
  },
  {
    "text": "a separate device than u a 2 is1 GPU so we have all of these gpus U being",
    "start": "1065440",
    "end": "1071880"
  },
  {
    "text": "advertised um in this resource slice the next thing now is advertis in",
    "start": "1071880",
    "end": "1079280"
  },
  {
    "text": "CPU because remember our use cases they want GPU and CPU aligned on the same",
    "start": "1079280",
    "end": "1085600"
  },
  {
    "text": "Numa so here we have uh attributes where like architecture the architecture uh",
    "start": "1085600",
    "end": "1092200"
  },
  {
    "text": "amd64 is important because um the CPUs",
    "start": "1092200",
    "end": "1097400"
  },
  {
    "text": "are um multicore and within each core they are hyper threaded so the thread ID",
    "start": "1097400",
    "end": "1106159"
  },
  {
    "text": "is represented in the ID attribute and the core ID is rep represented in the",
    "start": "1106159",
    "end": "1111960"
  },
  {
    "text": "parent ID attribute so you could have for example multiple threads so ID U 0",
    "start": "1111960",
    "end": "1119320"
  },
  {
    "text": "and ID 15 here both of them belonging to the same core and they are expressed as",
    "start": "1119320",
    "end": "1125159"
  },
  {
    "text": "separate devices again one thing to point out",
    "start": "1125159",
    "end": "1130840"
  },
  {
    "text": "because the Numa attribute is advertised here we can now align CPU and GPU uh",
    "start": "1130840",
    "end": "1137640"
  },
  {
    "text": "within a single okay so once the devices are uh",
    "start": "1137640",
    "end": "1144640"
  },
  {
    "text": "advertised how do how does workload request those device so I talked about",
    "start": "1144640",
    "end": "1151480"
  },
  {
    "text": "resource claim earlier um resource claim template is another object uh which",
    "start": "1151480",
    "end": "1157720"
  },
  {
    "text": "could be used it's basically a template uh that gets rendered into a a resource",
    "start": "1157720",
    "end": "1164120"
  },
  {
    "text": "Claim by by the system so in this uh when a resource",
    "start": "1164120",
    "end": "1169400"
  },
  {
    "text": "claim will be created from this template it will have two requests one a p GPU",
    "start": "1169400",
    "end": "1174880"
  },
  {
    "text": "and other a CPU with full Numa so in this case um we are requesting a pgp of",
    "start": "1174880",
    "end": "1182360"
  },
  {
    "text": "a specific device class and we want the attribute type uh to be equals to GPU",
    "start": "1182360",
    "end": "1190360"
  },
  {
    "text": "that's how this that's how we have defined uh full GPU in the resource",
    "start": "1190360",
    "end": "1195640"
  },
  {
    "text": "slice in the earlier slide so that will be selected then we have a configuration for full",
    "start": "1195640",
    "end": "1202280"
  },
  {
    "text": "Numa um here so if a numa has for example eight cores that will give 16 um",
    "start": "1202280",
    "end": "1210600"
  },
  {
    "text": "vgpus to this uh resource claim so now getting a pgp with a full Numa we do",
    "start": "1210600",
    "end": "1219240"
  },
  {
    "text": "want to align them on a a numa node so that is expressed underneath in the",
    "start": "1219240",
    "end": "1225159"
  },
  {
    "text": "constraints the constraints has match attribute feied which the Schuler will see and try to",
    "start": "1225159",
    "end": "1233679"
  },
  {
    "text": "and not try to and will allocate um select resources that are on that have",
    "start": "1233679",
    "end": "1240280"
  },
  {
    "text": "the same Numa node value okay so this is an example of how",
    "start": "1240280",
    "end": "1247240"
  },
  {
    "text": "to get a sliced 2 is to one vgpu so it's similar to a p GPU but the changes here",
    "start": "1247240",
    "end": "1255400"
  },
  {
    "text": "are that the attribute type becomes V GPU instead of uh uh GPU and we have to",
    "start": "1255400",
    "end": "1264240"
  },
  {
    "text": "mention the class because the vgpu 2 is to1 is different from vgpu 4 is to1 so",
    "start": "1264240",
    "end": "1270840"
  },
  {
    "text": "in the green box you can see a cell expression that selects uh that that",
    "start": "1270840",
    "end": "1277440"
  },
  {
    "text": "says that this resource claim will need to select one uh vgpu that is 2 is to",
    "start": "1277440",
    "end": "1283960"
  },
  {
    "text": "one type the next thing is um how Numa so in",
    "start": "1283960",
    "end": "1290200"
  },
  {
    "text": "this case because the GPU is sliced into half we might not need the entire set of",
    "start": "1290200",
    "end": "1297000"
  },
  {
    "text": "cores that are available in um in the Numa node so we want to slice it into",
    "start": "1297000",
    "end": "1302840"
  },
  {
    "text": "half so that when a second P GPU uh Second 2 is to1 vgpu workload comes in",
    "start": "1302840",
    "end": "1309440"
  },
  {
    "text": "it can occupy the other set of um CPUs and that way we can fit all of the",
    "start": "1309440",
    "end": "1315320"
  },
  {
    "text": "workloads in in a single Numa so this is an example of how you can request half",
    "start": "1315320",
    "end": "1321279"
  },
  {
    "text": "Numa and I can I'll talk about more um what goes into requesting that half",
    "start": "1321279",
    "end": "1328919"
  },
  {
    "text": "Noma CPU uh in in the demo similar to a PPU we have",
    "start": "1328919",
    "end": "1335960"
  },
  {
    "text": "constraints um in the resource claim template that tells the system that the",
    "start": "1335960",
    "end": "1342039"
  },
  {
    "text": "vgpu and the half Numa need to match uh the Numa attribute so they need to be",
    "start": "1342039",
    "end": "1347960"
  },
  {
    "text": "aligned on on the numo okay so that uh takes us to the",
    "start": "1347960",
    "end": "1355520"
  },
  {
    "text": "demo um so the first thing uh I'm going to",
    "start": "1355520",
    "end": "1362480"
  },
  {
    "text": "show is I've taken uh we have taken a",
    "start": "1362480",
    "end": "1368159"
  },
  {
    "text": "example from from a running a production workload and uh",
    "start": "1368159",
    "end": "1377039"
  },
  {
    "text": "this example shows that we have a CPU set so these eight CPU or 16 CPUs were",
    "start": "1381600",
    "end": "1390799"
  },
  {
    "text": "given to um to the VM and then we have uh a vfio PCI U GPU",
    "start": "1390799",
    "end": "1400120"
  },
  {
    "text": "here so this is an uh MD type so this is a vgpu this was taken from uh production",
    "start": "1400120",
    "end": "1408000"
  },
  {
    "text": "this this says that this particular workload has a vgpu with certain CPUs",
    "start": "1408000",
    "end": "1415480"
  },
  {
    "text": "that are aligned through uh device plugin okay now that we have context um",
    "start": "1415480",
    "end": "1421520"
  },
  {
    "text": "of this I want to move to how this this can be achieved via",
    "start": "1421520",
    "end": "1426919"
  },
  {
    "text": "Dr one disclaimer is that since Dr um was Alpha in 131 we have used all of",
    "start": "1426919",
    "end": "1435559"
  },
  {
    "text": "this to PO this is not uh something something that is currently running in uh",
    "start": "1435559",
    "end": "1442679"
  },
  {
    "text": "production so the next thing that is important to call out here",
    "start": "1442960",
    "end": "1449600"
  },
  {
    "text": "is how these devices get",
    "start": "1449600",
    "end": "1454440"
  },
  {
    "text": "allocated so what happens here is once the devices were available to to the Pod",
    "start": "1455240",
    "end": "1462480"
  },
  {
    "text": "the device plugin created this environment variable and this environment variable was read by Cubo to",
    "start": "1462480",
    "end": "1468399"
  },
  {
    "text": "generate the Dom XML uh for it that got us a result of of this",
    "start": "1468399",
    "end": "1473760"
  },
  {
    "text": "CPU similarly this is how the P GPU was read uh by Cube word and put it",
    "start": "1473760",
    "end": "1480320"
  },
  {
    "text": "generated uh this Dom XML moving AC moving to U the Dr PC",
    "start": "1480320",
    "end": "1489880"
  },
  {
    "text": "first um quick run through the environment we have so this is",
    "start": "1489880",
    "end": "1496279"
  },
  {
    "text": "a a node with two gpus it is running a kind cluster um",
    "start": "1496279",
    "end": "1504360"
  },
  {
    "text": "with 131 uh with one worker node and it has a",
    "start": "1504360",
    "end": "1511200"
  },
  {
    "text": "a POC version of cube word installed um",
    "start": "1511200",
    "end": "1516158"
  },
  {
    "text": "already there are two uh Dr drivers installed in this cluster um a CPU",
    "start": "1516600",
    "end": "1524440"
  },
  {
    "text": "driver which allocates those vcpus and a GPU driver which allocates all the pgp",
    "start": "1524440",
    "end": "1531480"
  },
  {
    "text": "and B gpus so with that context we can now look into the um resource claim",
    "start": "1531480",
    "end": "1541360"
  },
  {
    "text": "template so the resource claim template here again has uh two sections one for",
    "start": "1543559",
    "end": "1550200"
  },
  {
    "text": "the uh bgpu and then for CPU now you'll notice",
    "start": "1550200",
    "end": "1556799"
  },
  {
    "text": "that uh there are the the C CPUs that was requested in",
    "start": "1556799",
    "end": "1562679"
  },
  {
    "text": "this resource claim template is broken down into multiple counts of two this is",
    "start": "1562679",
    "end": "1568919"
  },
  {
    "text": "because in a hyperthreaded environment if we are requesting half Numa and if we",
    "start": "1568919",
    "end": "1574200"
  },
  {
    "text": "are requesting let's say um eight vcpus",
    "start": "1574200",
    "end": "1579520"
  },
  {
    "text": "you can have those eight CPUs be uh spread out from different cores so they",
    "start": "1579520",
    "end": "1586399"
  },
  {
    "text": "would be actual hyper threads from different codes and then when another um 2 is to one workload comes along it",
    "start": "1586399",
    "end": "1593799"
  },
  {
    "text": "would get the remaining uh vcpus from a separate code and what we want here is",
    "start": "1593799",
    "end": "1600039"
  },
  {
    "text": "to bin pack them so that all of the first eight uh CPUs were allocated from",
    "start": "1600039",
    "end": "1607679"
  },
  {
    "text": "uh first half so all sharing um two sharing each core so that way we have",
    "start": "1607679",
    "end": "1615320"
  },
  {
    "text": "the highest performance uh at the topology",
    "start": "1615320",
    "end": "1621200"
  },
  {
    "text": "level so the way we request this is we have counts of two and the counts of two",
    "start": "1621200",
    "end": "1629320"
  },
  {
    "text": "are all requesting match attribute parent ID from our earlier discussion",
    "start": "1629320",
    "end": "1634799"
  },
  {
    "text": "this parent ID represented the uh core where this uh vcpu was advertised from",
    "start": "1634799",
    "end": "1642559"
  },
  {
    "text": "so this is how all of those uh VC CPUs",
    "start": "1642559",
    "end": "1647640"
  },
  {
    "text": "will been packed inside of the workload and we'll get two perfectly U packed uh",
    "start": "1647640",
    "end": "1655320"
  },
  {
    "text": "vgpu 2 is to one workload running on on the",
    "start": "1655320",
    "end": "1660200"
  },
  {
    "text": "numano so quickly uh looking at",
    "start": "1660880",
    "end": "1666000"
  },
  {
    "text": "the the spec for the VM here you can see that we've um",
    "start": "1666000",
    "end": "1677519"
  },
  {
    "text": "this",
    "start": "1681880",
    "end": "1684080"
  },
  {
    "text": "one okay sorry so here you can see that the uh resource",
    "start": "1690000",
    "end": "1698120"
  },
  {
    "text": "claim the resource claim section uh requests for that resource claim which",
    "start": "1700200",
    "end": "1705640"
  },
  {
    "text": "which we just went through so this will be be a vgpu with half Numa and then in",
    "start": "1705640",
    "end": "1711559"
  },
  {
    "text": "the domain section we are actually describing that vgpu this is all needed for cubeo to generate the correct Dom",
    "start": "1711559",
    "end": "1719039"
  },
  {
    "text": "XML for it so with that spec uh we're going to",
    "start": "1719039",
    "end": "1726840"
  },
  {
    "text": "see we're going to start the VMI",
    "start": "1726840",
    "end": "1731799"
  },
  {
    "text": "what this did was what this did was it created a res",
    "start": "1743399",
    "end": "1752000"
  },
  {
    "text": "it created a pod underneath that had the same resource",
    "start": "1752000",
    "end": "1758480"
  },
  {
    "text": "claim so the resource claim was passed to the Pod so now this part has uh both",
    "start": "1758480",
    "end": "1765960"
  },
  {
    "text": "the vgpu with aligned uh vgpu 2 is to",
    "start": "1765960",
    "end": "1772080"
  },
  {
    "text": "one with aligned uh Numa aligned CPU in there and cubeb has generated the right",
    "start": "1772080",
    "end": "1778000"
  },
  {
    "text": "uh Dom XML for it so the way we find this is we look at the resource claim",
    "start": "1778000",
    "end": "1785640"
  },
  {
    "text": "template and in the status we can see that all of those",
    "start": "1788240",
    "end": "1795399"
  },
  {
    "text": "devices that were uh requested in the resource claim template are available",
    "start": "1795399",
    "end": "1801320"
  },
  {
    "text": "here now the vgpu and CPU all of them share the same attribute so we",
    "start": "1801320",
    "end": "1811600"
  },
  {
    "text": "so uh I shell script gave me that all of",
    "start": "1833640",
    "end": "1838840"
  },
  {
    "text": "those attributes um that were from the allocated devices and you can see here",
    "start": "1838840",
    "end": "1844120"
  },
  {
    "text": "that all of them have uh been selected from Numa node",
    "start": "1844120",
    "end": "1849360"
  },
  {
    "text": "zero so this is how we get a vgpu 2 is to one vgpu and U set of CPUs from Numa",
    "start": "1849360",
    "end": "1858360"
  },
  {
    "text": "node zero all of them perfectly been packed um into the right",
    "start": "1858360",
    "end": "1865080"
  },
  {
    "text": "course okay so that was it um with",
    "start": "1865080",
    "end": "1870880"
  },
  {
    "text": "this can move back so there are",
    "start": "1870880",
    "end": "1876120"
  },
  {
    "text": "um we are very excited about Dr uh thanks to everyone in the uh kubernetes",
    "start": "1876120",
    "end": "1882760"
  },
  {
    "text": "contributors who have helped move along this feature forward uh we are also very",
    "start": "1882760",
    "end": "1888880"
  },
  {
    "text": "excited at some of the new work uh that is being planned for this feature it",
    "start": "1888880",
    "end": "1894880"
  },
  {
    "text": "will help us achieve many more uh complex use cases and yeah with that uh we are um we",
    "start": "1894880",
    "end": "1905760"
  },
  {
    "text": "can take questions if you guys have",
    "start": "1905760",
    "end": "1909679"
  },
  {
    "text": "any yes uh the mic is should be available yeah okay so um just wondering",
    "start": "1911279",
    "end": "1917279"
  },
  {
    "text": "you seem like using ovn plugin the cni right so networking side I don't know",
    "start": "1917279",
    "end": "1923000"
  },
  {
    "text": "you have such a case where you need to add some kind of multiple network interfaces to the CER VM or n or I know",
    "start": "1923000",
    "end": "1930399"
  },
  {
    "text": "you're mostly focusing on the you know GPU side but you know is there any kind of a edge cases you had to deal with for",
    "start": "1930399",
    "end": "1937080"
  },
  {
    "text": "the networking part as well just want to know so for the purposes of um this",
    "start": "1937080",
    "end": "1942720"
  },
  {
    "text": "discussion we have not yet uh looked at how to make networking work work with Dr",
    "start": "1942720",
    "end": "1949799"
  },
  {
    "text": "but I know there was um some interest in trying to get uh SRI s devices for",
    "start": "1949799",
    "end": "1957799"
  },
  {
    "text": "example um work with uh Dr plug-in I believe um we have folks from networking",
    "start": "1957799",
    "end": "1966240"
  },
  {
    "text": "um we can have that discussion um if you're interested so are you using M cni",
    "start": "1966240",
    "end": "1971760"
  },
  {
    "text": "or some stuff nothing just one single interface no we are using m m solid for",
    "start": "1971760",
    "end": "1979039"
  },
  {
    "text": "we are using OBS ovn and then Ms for the multihomed okay um yeah thank",
    "start": "1979039",
    "end": "1986360"
  },
  {
    "text": "you hi thanks um so one of the problems we you mentioned was that uh the device",
    "start": "1986760",
    "end": "1995159"
  },
  {
    "text": "these multiple device plugins they they don't they can't clean up and you have to sort of take away resources when one",
    "start": "1995159",
    "end": "2001039"
  },
  {
    "text": "of the device plugins is G out um that's not solved with this implementation but could be solved by partitionable devices",
    "start": "2001039",
    "end": "2007399"
  },
  {
    "text": "right like so that that problem still exists right right that that problem exists but",
    "start": "2007399",
    "end": "2013120"
  },
  {
    "text": "we have something in the array that that's going to solve it in the future for us okay yeah I think um",
    "start": "2013120",
    "end": "2018679"
  },
  {
    "text": "partitionable devices I think we've always been thinking about them as like oh you've slicing something up but maybe it's something more hierarchical or that",
    "start": "2018679",
    "end": "2026720"
  },
  {
    "text": "you know you have a physical a mapping from a physical device to multiple uh realizations of that device",
    "start": "2026720",
    "end": "2034080"
  },
  {
    "text": "that needs to exclude the usage of others maybe but exactly okay cool thank you very much good",
    "start": "2034080",
    "end": "2040919"
  },
  {
    "text": "talk um as of 1.32 um the a lot of the uh CPU mapping",
    "start": "2041279",
    "end": "2051599"
  },
  {
    "text": "for um strict well guaranteed qos CPU management is already automated so it's",
    "start": "2051599",
    "end": "2060599"
  },
  {
    "text": "a if you you have to the CPU resource should be all you need to specify you wouldn't have to itemize all those uh",
    "start": "2060599",
    "end": "2069560"
  },
  {
    "text": "values it will actually um it will actually give you uh a CPU set that is",
    "start": "2069560",
    "end": "2076599"
  },
  {
    "text": "both chiplet and numer aligned mhm um and it will use whole cores um",
    "start": "2076599",
    "end": "2084440"
  },
  {
    "text": "inherently because that is the requirement of couet um and so you also",
    "start": "2084440",
    "end": "2090040"
  },
  {
    "text": "don't have to understand physical topology of the specific skew that you're working",
    "start": "2090040",
    "end": "2096800"
  },
  {
    "text": "with which currently looks like you need to so a lot of that is removed that's automated now um then the alignment for",
    "start": "2096800",
    "end": "2105280"
  },
  {
    "text": "um a physical device from an IO perspective um that's already supported",
    "start": "2105280",
    "end": "2111839"
  },
  {
    "text": "in kuet as well so it's part of the decision process the waiting for the for",
    "start": "2111839",
    "end": "2118560"
  },
  {
    "text": "the mapping so if the io device is not in the same numer it will I mean it will",
    "start": "2118560",
    "end": "2126240"
  },
  {
    "text": "detect the numer for the o his first and flag that as a as a preference and then",
    "start": "2126240",
    "end": "2131960"
  },
  {
    "text": "find the CPUs that match that preference and then you'll end up with a working",
    "start": "2131960",
    "end": "2138040"
  },
  {
    "text": "set and copert should just work right uh so so what we showed as an",
    "start": "2138040",
    "end": "2144760"
  },
  {
    "text": "example was just like a very base use case that we had uh we have a bunch of other use cases where uh we want to",
    "start": "2144760",
    "end": "2151680"
  },
  {
    "text": "share that Numa across all our VMS on that Numa and then we want to float our",
    "start": "2151680",
    "end": "2157400"
  },
  {
    "text": "V CPUs across all the cores and threads in that NOA um and then with the CPU",
    "start": "2157400",
    "end": "2164480"
  },
  {
    "text": "manager and guaranteed Q we don't get that ability and and for just to be clarified for to use Dr you do not need",
    "start": "2164480",
    "end": "2171599"
  },
  {
    "text": "to use cver this is just your example yeah",
    "start": "2171599",
    "end": "2177440"
  }
]