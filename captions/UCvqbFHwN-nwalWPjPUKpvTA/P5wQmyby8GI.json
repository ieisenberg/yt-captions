[
  {
    "start": "0",
    "end": "29000"
  },
  {
    "text": "hi everyone",
    "start": "2080",
    "end": "3600"
  },
  {
    "text": "i'm so excited to share our experience",
    "start": "3600",
    "end": "6160"
  },
  {
    "text": "of kubernetes management with you",
    "start": "6160",
    "end": "9599"
  },
  {
    "text": "i'm going to talk about the optimization",
    "start": "9599",
    "end": "12080"
  },
  {
    "text": "of stretch assignment for",
    "start": "12080",
    "end": "13840"
  },
  {
    "text": "distributed systems on kubernetes",
    "start": "13840",
    "end": "17520"
  },
  {
    "text": "we utilize the power scheduling features",
    "start": "17520",
    "end": "20080"
  },
  {
    "text": "of kubernetes",
    "start": "20080",
    "end": "21840"
  },
  {
    "text": "and we tune the scheduler under the",
    "start": "21840",
    "end": "24560"
  },
  {
    "text": "real-world disturbance factors",
    "start": "24560",
    "end": "30480"
  },
  {
    "start": "29000",
    "end": "80000"
  },
  {
    "text": "first let me introduce myself",
    "start": "30480",
    "end": "33840"
  },
  {
    "text": "i'm kenji morimoto i'm working for cyrus",
    "start": "33840",
    "end": "37520"
  },
  {
    "text": "and i have an account",
    "start": "37520",
    "end": "39360"
  },
  {
    "text": "morimoto cyborgs at github.com",
    "start": "39360",
    "end": "43280"
  },
  {
    "text": "cyborgs is a japanese i.t company",
    "start": "43280",
    "end": "46399"
  },
  {
    "text": "we provide groupware communication tools",
    "start": "46399",
    "end": "49120"
  },
  {
    "text": "as",
    "start": "49120",
    "end": "49440"
  },
  {
    "text": "cloud services and i've been working as",
    "start": "49440",
    "end": "52800"
  },
  {
    "text": "an",
    "start": "52800",
    "end": "53199"
  },
  {
    "text": "infrastructure engineer for eight years",
    "start": "53199",
    "end": "57280"
  },
  {
    "text": "we have more than 2000 servers for our",
    "start": "57280",
    "end": "59840"
  },
  {
    "text": "cloud services",
    "start": "59840",
    "end": "62320"
  },
  {
    "text": "our management tools were designed as a",
    "start": "62320",
    "end": "64878"
  },
  {
    "text": "collection of imperative commands",
    "start": "64879",
    "end": "67200"
  },
  {
    "text": "and we are facing a lot of manual",
    "start": "67200",
    "end": "69360"
  },
  {
    "text": "operation tasks",
    "start": "69360",
    "end": "70400"
  },
  {
    "text": "these days so we are now renewing our",
    "start": "70400",
    "end": "74080"
  },
  {
    "text": "infrastructure",
    "start": "74080",
    "end": "75280"
  },
  {
    "text": "using kubernetes and cloud native",
    "start": "75280",
    "end": "78080"
  },
  {
    "text": "technologies",
    "start": "78080",
    "end": "80799"
  },
  {
    "start": "80000",
    "end": "240000"
  },
  {
    "text": "this is today's agenda first",
    "start": "82000",
    "end": "85280"
  },
  {
    "text": "i introduce what we challenged",
    "start": "85280",
    "end": "88720"
  },
  {
    "text": "we tried to run a distributed storage",
    "start": "88720",
    "end": "91280"
  },
  {
    "text": "system on kubernetes",
    "start": "91280",
    "end": "94079"
  },
  {
    "text": "i'll recap the basic of volume",
    "start": "94079",
    "end": "96479"
  },
  {
    "text": "management in kubernetes",
    "start": "96479",
    "end": "98320"
  },
  {
    "text": "and define the problem",
    "start": "98320",
    "end": "101439"
  },
  {
    "text": "i'll describe a basic idea for the",
    "start": "101439",
    "end": "103600"
  },
  {
    "text": "program and then",
    "start": "103600",
    "end": "105200"
  },
  {
    "text": "inspect troubles in implementation",
    "start": "105200",
    "end": "109119"
  },
  {
    "text": "we got an unexpected result under the",
    "start": "109119",
    "end": "111759"
  },
  {
    "text": "real world disturbance",
    "start": "111759",
    "end": "113600"
  },
  {
    "text": "so we turned the kubes scheduler",
    "start": "113600",
    "end": "115680"
  },
  {
    "text": "configuration",
    "start": "115680",
    "end": "117920"
  },
  {
    "text": "finally i'll give a demonstration how",
    "start": "117920",
    "end": "120560"
  },
  {
    "text": "our tuning",
    "start": "120560",
    "end": "121280"
  },
  {
    "text": "improved the placement",
    "start": "121280",
    "end": "124719"
  },
  {
    "text": "first of all let's take a look at what",
    "start": "126159",
    "end": "129200"
  },
  {
    "text": "is a distributed stress system",
    "start": "129200",
    "end": "132080"
  },
  {
    "text": "a distributed system organizes node",
    "start": "132080",
    "end": "135200"
  },
  {
    "text": "local stretch devices in a computer",
    "start": "135200",
    "end": "137520"
  },
  {
    "text": "cluster",
    "start": "137520",
    "end": "138480"
  },
  {
    "text": "and provides a unified strategy resource",
    "start": "138480",
    "end": "141120"
  },
  {
    "text": "for users",
    "start": "141120",
    "end": "143120"
  },
  {
    "text": "in the case of ceph for example the",
    "start": "143120",
    "end": "145920"
  },
  {
    "text": "administrator defines object storage",
    "start": "145920",
    "end": "148400"
  },
  {
    "text": "devices",
    "start": "148400",
    "end": "149360"
  },
  {
    "text": "or osds in short using local disks",
    "start": "149360",
    "end": "152879"
  },
  {
    "text": "and safe gathers osds into a storage",
    "start": "152879",
    "end": "156080"
  },
  {
    "text": "resource",
    "start": "156080",
    "end": "158560"
  },
  {
    "text": "the management of the element disks in",
    "start": "158560",
    "end": "161120"
  },
  {
    "text": "the distributed stretch",
    "start": "161120",
    "end": "162640"
  },
  {
    "text": "is very tedious work",
    "start": "162640",
    "end": "166400"
  },
  {
    "text": "before seeing rook unsafe on kubernetes",
    "start": "168400",
    "end": "171599"
  },
  {
    "text": "let's recap the stretch architecture of",
    "start": "171599",
    "end": "173920"
  },
  {
    "text": "kubernetes",
    "start": "173920",
    "end": "176000"
  },
  {
    "text": "on kubernetes strategies are selected as",
    "start": "176000",
    "end": "179920"
  },
  {
    "text": "persistent volumes the strategies can be",
    "start": "179920",
    "end": "183920"
  },
  {
    "text": "local disks",
    "start": "183920",
    "end": "185040"
  },
  {
    "text": "network strategies in the cluster or",
    "start": "185040",
    "end": "187840"
  },
  {
    "text": "cloud strategies",
    "start": "187840",
    "end": "190400"
  },
  {
    "text": "pods can define their volumes by using",
    "start": "190400",
    "end": "193280"
  },
  {
    "text": "persistent burn claims as dead",
    "start": "193280",
    "end": "195599"
  },
  {
    "text": "sources persistent volume claims",
    "start": "195599",
    "end": "199200"
  },
  {
    "text": "specify their requirements for",
    "start": "199200",
    "end": "201360"
  },
  {
    "text": "underlying strategies",
    "start": "201360",
    "end": "202959"
  },
  {
    "text": "through storage class names resource",
    "start": "202959",
    "end": "205920"
  },
  {
    "text": "requests",
    "start": "205920",
    "end": "207040"
  },
  {
    "text": "selectors and so on",
    "start": "207040",
    "end": "210239"
  },
  {
    "text": "kubernetes finds or creates a matching",
    "start": "210239",
    "end": "212720"
  },
  {
    "text": "pv",
    "start": "212720",
    "end": "213280"
  },
  {
    "text": "for pvc and bind them together",
    "start": "213280",
    "end": "217920"
  },
  {
    "text": "we use rook to deploy a safe storage",
    "start": "219519",
    "end": "222000"
  },
  {
    "text": "cluster",
    "start": "222000",
    "end": "223920"
  },
  {
    "text": "rook has a mod to configure osds through",
    "start": "223920",
    "end": "227120"
  },
  {
    "text": "persistent volume claims",
    "start": "227120",
    "end": "230560"
  },
  {
    "text": "rook configures acquired pvs as osds for",
    "start": "230560",
    "end": "233840"
  },
  {
    "text": "safe",
    "start": "233840",
    "end": "234560"
  },
  {
    "text": "and then safe constructs a unified",
    "start": "234560",
    "end": "237040"
  },
  {
    "text": "strategic resource",
    "start": "237040",
    "end": "241840"
  },
  {
    "start": "240000",
    "end": "350000"
  },
  {
    "text": "what we wanted to do was to deploy a",
    "start": "242319",
    "end": "245040"
  },
  {
    "text": "distributed storage system on",
    "start": "245040",
    "end": "247200"
  },
  {
    "text": "on-premise servers we had experienced",
    "start": "247200",
    "end": "250959"
  },
  {
    "text": "that the management of thousands of",
    "start": "250959",
    "end": "253120"
  },
  {
    "text": "servers and disks",
    "start": "253120",
    "end": "254319"
  },
  {
    "text": "was an awful task if done manually",
    "start": "254319",
    "end": "258239"
  },
  {
    "text": "so we needed kubernetes help for",
    "start": "258239",
    "end": "261120"
  },
  {
    "text": "automatic management",
    "start": "261120",
    "end": "263199"
  },
  {
    "text": "however there was no standard profile to",
    "start": "263199",
    "end": "266240"
  },
  {
    "text": "deploy",
    "start": "266240",
    "end": "267520"
  },
  {
    "text": "distributed search system on kubernetes",
    "start": "267520",
    "end": "271840"
  },
  {
    "text": "we can expect that safe is responsible",
    "start": "272000",
    "end": "274880"
  },
  {
    "text": "for",
    "start": "274880",
    "end": "276160"
  },
  {
    "text": "replicating data across fair domains for",
    "start": "276160",
    "end": "280840"
  },
  {
    "text": "robustness",
    "start": "280840",
    "end": "282000"
  },
  {
    "text": "on the other hand distributing local",
    "start": "282000",
    "end": "284560"
  },
  {
    "text": "disks evenly",
    "start": "284560",
    "end": "285759"
  },
  {
    "text": "is a task for the administrators",
    "start": "285759",
    "end": "289440"
  },
  {
    "text": "so the challenge here is that we need to",
    "start": "289440",
    "end": "292800"
  },
  {
    "text": "distribute persistent volumes for local",
    "start": "292800",
    "end": "295280"
  },
  {
    "text": "disks evenly",
    "start": "295280",
    "end": "296400"
  },
  {
    "text": "through persistent volume claims",
    "start": "296400",
    "end": "300400"
  },
  {
    "text": "it's easy to achieve even distribution",
    "start": "302720",
    "end": "305520"
  },
  {
    "text": "if",
    "start": "305520",
    "end": "305919"
  },
  {
    "text": "all disks all servers and all racks are",
    "start": "305919",
    "end": "309199"
  },
  {
    "text": "evenly available",
    "start": "309199",
    "end": "312320"
  },
  {
    "text": "let's take six disks evenly from this",
    "start": "312320",
    "end": "316880"
  },
  {
    "text": "it's easy but",
    "start": "316880",
    "end": "320000"
  },
  {
    "text": "in reality this is often not true",
    "start": "320000",
    "end": "322400"
  },
  {
    "text": "because",
    "start": "322400",
    "end": "323039"
  },
  {
    "text": "for example a server node may have",
    "start": "323039",
    "end": "326160"
  },
  {
    "text": "broken disks",
    "start": "326160",
    "end": "328160"
  },
  {
    "text": "some disks some disks of the server may",
    "start": "328160",
    "end": "331280"
  },
  {
    "text": "already be assigned for other use or",
    "start": "331280",
    "end": "334479"
  },
  {
    "text": "rock may have fewer healthy servers than",
    "start": "334479",
    "end": "337039"
  },
  {
    "text": "other racks",
    "start": "337039",
    "end": "338960"
  },
  {
    "text": "so in the real world we need to consider",
    "start": "338960",
    "end": "342080"
  },
  {
    "text": "about",
    "start": "342080",
    "end": "342720"
  },
  {
    "text": "an even availability of local disks",
    "start": "342720",
    "end": "346320"
  },
  {
    "text": "this makes our challenge a hard one",
    "start": "346320",
    "end": "350320"
  },
  {
    "start": "350000",
    "end": "389000"
  },
  {
    "text": "in order to achieve even distribution",
    "start": "350960",
    "end": "353039"
  },
  {
    "text": "automatically",
    "start": "353039",
    "end": "354240"
  },
  {
    "text": "we need help of kubernetes the",
    "start": "354240",
    "end": "356800"
  },
  {
    "text": "kubernetes does not directly schedule",
    "start": "356800",
    "end": "359280"
  },
  {
    "text": "storage devices",
    "start": "359280",
    "end": "362400"
  },
  {
    "text": "in contrast to storage handling",
    "start": "362400",
    "end": "364639"
  },
  {
    "text": "kubernetes provides a",
    "start": "364639",
    "end": "366000"
  },
  {
    "text": "very rich set of features to schedule",
    "start": "366000",
    "end": "368560"
  },
  {
    "text": "pods",
    "start": "368560",
    "end": "370319"
  },
  {
    "text": "group scheduler is a kubernetes",
    "start": "370319",
    "end": "372080"
  },
  {
    "text": "component to schedule pods on nodes",
    "start": "372080",
    "end": "375440"
  },
  {
    "text": "it can schedule posts based on several",
    "start": "375440",
    "end": "377919"
  },
  {
    "text": "criteria",
    "start": "377919",
    "end": "378880"
  },
  {
    "text": "including resource requirements node",
    "start": "378880",
    "end": "381840"
  },
  {
    "text": "factors",
    "start": "381840",
    "end": "382960"
  },
  {
    "text": "port affinity and under affinity and",
    "start": "382960",
    "end": "385680"
  },
  {
    "text": "taints and traditions",
    "start": "385680",
    "end": "388960"
  },
  {
    "start": "389000",
    "end": "512000"
  },
  {
    "text": "now jump into the basic idea",
    "start": "389840",
    "end": "394160"
  },
  {
    "text": "our basic idea is to specify wait for",
    "start": "394160",
    "end": "397039"
  },
  {
    "text": "first consumer",
    "start": "397039",
    "end": "398319"
  },
  {
    "text": "for the volume binding mode in a stretch",
    "start": "398319",
    "end": "400720"
  },
  {
    "text": "class",
    "start": "400720",
    "end": "402400"
  },
  {
    "text": "but what is the boring binding mode",
    "start": "402400",
    "end": "404560"
  },
  {
    "text": "exactly",
    "start": "404560",
    "end": "406479"
  },
  {
    "text": "it can take up value of immediate or",
    "start": "406479",
    "end": "409520"
  },
  {
    "text": "wait for first consumer let's see the",
    "start": "409520",
    "end": "412720"
  },
  {
    "text": "behavior of kubernetes for each mode in",
    "start": "412720",
    "end": "414880"
  },
  {
    "text": "the following slides",
    "start": "414880",
    "end": "417840"
  },
  {
    "text": "the brain binding mode affects the",
    "start": "418000",
    "end": "419919"
  },
  {
    "text": "timing when the binding of a pvc and the",
    "start": "419919",
    "end": "423039"
  },
  {
    "text": "pv",
    "start": "423039",
    "end": "423599"
  },
  {
    "text": "is determined the default volume binding",
    "start": "423599",
    "end": "427520"
  },
  {
    "text": "mode is immediate",
    "start": "427520",
    "end": "429360"
  },
  {
    "text": "in this mode when a pvc is created",
    "start": "429360",
    "end": "432479"
  },
  {
    "text": "kubernetes immediately finds or creates",
    "start": "432479",
    "end": "434960"
  },
  {
    "text": "a matching pv",
    "start": "434960",
    "end": "436240"
  },
  {
    "text": "and bind them together this binding will",
    "start": "436240",
    "end": "440240"
  },
  {
    "text": "work as a topology constraint for kube",
    "start": "440240",
    "end": "442479"
  },
  {
    "text": "scheduler",
    "start": "442479",
    "end": "443280"
  },
  {
    "text": "when it schedules a port that uses this",
    "start": "443280",
    "end": "446000"
  },
  {
    "text": "pvc",
    "start": "446000",
    "end": "448400"
  },
  {
    "text": "in our case the pvc is bound to a local",
    "start": "448400",
    "end": "451280"
  },
  {
    "text": "disk",
    "start": "451280",
    "end": "452000"
  },
  {
    "text": "so the consumer port is bound to the",
    "start": "452000",
    "end": "454240"
  },
  {
    "text": "disks node in effect",
    "start": "454240",
    "end": "457599"
  },
  {
    "text": "the problem here is that we cannot",
    "start": "457599",
    "end": "460240"
  },
  {
    "text": "control the matching of a pvc and the pv",
    "start": "460240",
    "end": "463120"
  },
  {
    "text": "in terms of even distribution",
    "start": "463120",
    "end": "466879"
  },
  {
    "text": "another volume binding mode is wait for",
    "start": "468639",
    "end": "471199"
  },
  {
    "text": "first consumer",
    "start": "471199",
    "end": "473280"
  },
  {
    "text": "in this mode a pvc is not bound to a pv",
    "start": "473280",
    "end": "476879"
  },
  {
    "text": "until a port that uses the pvc is",
    "start": "476879",
    "end": "479360"
  },
  {
    "text": "scheduled",
    "start": "479360",
    "end": "482000"
  },
  {
    "text": "we can control the port scheduling by",
    "start": "482080",
    "end": "484080"
  },
  {
    "text": "several means",
    "start": "484080",
    "end": "486960"
  },
  {
    "text": "when binding a pvc to pv there is a",
    "start": "487280",
    "end": "490479"
  },
  {
    "text": "constraint that the pv",
    "start": "490479",
    "end": "492000"
  },
  {
    "text": "must be available from the node",
    "start": "492000",
    "end": "495120"
  },
  {
    "text": "as the pv is not local in our case",
    "start": "495120",
    "end": "498080"
  },
  {
    "text": "scheduling a pod",
    "start": "498080",
    "end": "499280"
  },
  {
    "text": "is equal to scheduling a pv",
    "start": "499280",
    "end": "503440"
  },
  {
    "text": "so we can control the location of a",
    "start": "503599",
    "end": "506000"
  },
  {
    "text": "local thread through port scheduling",
    "start": "506000",
    "end": "510080"
  },
  {
    "text": "go back to our basic idea by specifying",
    "start": "513120",
    "end": "516560"
  },
  {
    "text": "wait for first consumer for the volume",
    "start": "516560",
    "end": "518560"
  },
  {
    "text": "binding mode",
    "start": "518560",
    "end": "519760"
  },
  {
    "text": "we can translate the problem of stretch",
    "start": "519760",
    "end": "522159"
  },
  {
    "text": "allocation",
    "start": "522159",
    "end": "523039"
  },
  {
    "text": "into the problem of post scheduling and",
    "start": "523039",
    "end": "526399"
  },
  {
    "text": "as for the port scheduling kubernetes",
    "start": "526399",
    "end": "528640"
  },
  {
    "text": "provides a rich set of features",
    "start": "528640",
    "end": "530720"
  },
  {
    "text": "and we can utilize those features",
    "start": "530720",
    "end": "533760"
  },
  {
    "text": "now our challenge is translated to",
    "start": "533760",
    "end": "536640"
  },
  {
    "text": "distributing ports with pvcs evenly",
    "start": "536640",
    "end": "541680"
  },
  {
    "start": "541000",
    "end": "668000"
  },
  {
    "text": "well which type of scheduling criteria",
    "start": "542959",
    "end": "545839"
  },
  {
    "text": "should we use to distribute pose evenly",
    "start": "545839",
    "end": "549360"
  },
  {
    "text": "one candidate is anti-affinity",
    "start": "549360",
    "end": "552560"
  },
  {
    "text": "we can distribute one port per node by",
    "start": "552560",
    "end": "554959"
  },
  {
    "text": "using undefinity",
    "start": "554959",
    "end": "556800"
  },
  {
    "text": "but we want to use multiple pvs on any",
    "start": "556800",
    "end": "559920"
  },
  {
    "text": "one node",
    "start": "559920",
    "end": "561760"
  },
  {
    "text": "undefinity does not distinguish whether",
    "start": "561760",
    "end": "564640"
  },
  {
    "text": "there are two ports or three ports",
    "start": "564640",
    "end": "566800"
  },
  {
    "text": "or four five bubbles",
    "start": "566800",
    "end": "570640"
  },
  {
    "text": "much appropriate criterion is port",
    "start": "570640",
    "end": "572959"
  },
  {
    "text": "topology spread constraints",
    "start": "572959",
    "end": "575760"
  },
  {
    "text": "this feature was introduced in",
    "start": "575760",
    "end": "577680"
  },
  {
    "text": "kubernetes 1.16",
    "start": "577680",
    "end": "580240"
  },
  {
    "text": "became better in 1.18 and is now stable",
    "start": "580240",
    "end": "584080"
  },
  {
    "text": "in 1.19 a set of",
    "start": "584080",
    "end": "587760"
  },
  {
    "text": "participate constraints compute the",
    "start": "587760",
    "end": "589920"
  },
  {
    "text": "scheduling score",
    "start": "589920",
    "end": "591440"
  },
  {
    "text": "based on the skew so we can put a cap",
    "start": "591440",
    "end": "594640"
  },
  {
    "text": "on the difference of the numbers of pods",
    "start": "594640",
    "end": "599040"
  },
  {
    "text": "this figure shows how topology spread",
    "start": "600640",
    "end": "603279"
  },
  {
    "text": "constraints",
    "start": "603279",
    "end": "604000"
  },
  {
    "text": "work this is cited from kubernetes blog",
    "start": "604000",
    "end": "609360"
  },
  {
    "text": "you can specify max queue to describe",
    "start": "609360",
    "end": "612959"
  },
  {
    "text": "the degree to which post may be",
    "start": "612959",
    "end": "614959"
  },
  {
    "text": "unevenly distributed topology",
    "start": "614959",
    "end": "618079"
  },
  {
    "text": "key to group posed by node levels",
    "start": "618079",
    "end": "621760"
  },
  {
    "text": "one unsatisfiable to indicate what to do",
    "start": "621760",
    "end": "624640"
  },
  {
    "text": "if the constraints are not satisfiable",
    "start": "624640",
    "end": "627440"
  },
  {
    "text": "and level selector to find target pods",
    "start": "627440",
    "end": "632240"
  },
  {
    "text": "in this example zone one has two ports",
    "start": "632240",
    "end": "635519"
  },
  {
    "text": "and dom two has one pod the skew between",
    "start": "635519",
    "end": "639279"
  },
  {
    "text": "zones is one",
    "start": "639279",
    "end": "641360"
  },
  {
    "text": "now here comes a new part if this new",
    "start": "641360",
    "end": "644640"
  },
  {
    "text": "port is scheduled to zone 1",
    "start": "644640",
    "end": "646800"
  },
  {
    "text": "the skew becomes 3. this breaks the",
    "start": "646800",
    "end": "649440"
  },
  {
    "text": "constraint",
    "start": "649440",
    "end": "651600"
  },
  {
    "text": "if the port is scheduled to zone 2 the",
    "start": "651600",
    "end": "654079"
  },
  {
    "text": "skew becomes",
    "start": "654079",
    "end": "655120"
  },
  {
    "text": "0. this satisfies the constraint",
    "start": "655120",
    "end": "658399"
  },
  {
    "text": "so the new post should be scheduled to",
    "start": "658399",
    "end": "660880"
  },
  {
    "text": "zone 2.",
    "start": "660880",
    "end": "663040"
  },
  {
    "text": "as a result we can achieve even port",
    "start": "663040",
    "end": "665839"
  },
  {
    "text": "distribution",
    "start": "665839",
    "end": "668560"
  },
  {
    "text": "we can now distribute pose evenly",
    "start": "669920",
    "end": "672959"
  },
  {
    "text": "but when implementing distribution we",
    "start": "672959",
    "end": "676000"
  },
  {
    "text": "need to consider the case",
    "start": "676000",
    "end": "677600"
  },
  {
    "text": "where the constraints are not",
    "start": "677600",
    "end": "679200"
  },
  {
    "text": "satisfiable",
    "start": "679200",
    "end": "681600"
  },
  {
    "text": "let's see the details",
    "start": "681600",
    "end": "684639"
  },
  {
    "text": "to keep evenness strictly may not be",
    "start": "685760",
    "end": "688560"
  },
  {
    "text": "desirable in the real world",
    "start": "688560",
    "end": "691600"
  },
  {
    "text": "as described before there may be uneven",
    "start": "691600",
    "end": "694480"
  },
  {
    "text": "availability of local disks",
    "start": "694480",
    "end": "696640"
  },
  {
    "text": "a cybernode may have broken disks some",
    "start": "696640",
    "end": "699040"
  },
  {
    "text": "disks over server may already be",
    "start": "699040",
    "end": "700880"
  },
  {
    "text": "assigned for other use",
    "start": "700880",
    "end": "702399"
  },
  {
    "text": "or rack may have fewer healthy servers",
    "start": "702399",
    "end": "704959"
  },
  {
    "text": "than others",
    "start": "704959",
    "end": "707519"
  },
  {
    "text": "in this example there are six servers",
    "start": "707519",
    "end": "711200"
  },
  {
    "text": "each server has four disks",
    "start": "711200",
    "end": "714240"
  },
  {
    "text": "and server 3a has broken disks",
    "start": "714240",
    "end": "717519"
  },
  {
    "text": "it has one healthy disk and three broken",
    "start": "717519",
    "end": "719839"
  },
  {
    "text": "disks",
    "start": "719839",
    "end": "721839"
  },
  {
    "text": "let's start assignment lab1",
    "start": "721839",
    "end": "726399"
  },
  {
    "text": "i can sign six ports with six disks one",
    "start": "726399",
    "end": "730240"
  },
  {
    "text": "per node",
    "start": "730240",
    "end": "732399"
  },
  {
    "text": "lab2 i can assign 5 ports with",
    "start": "732399",
    "end": "736240"
  },
  {
    "text": "5 disks as you can see server 3a has no",
    "start": "736240",
    "end": "740880"
  },
  {
    "text": "available disk i cannot start lap 3",
    "start": "740880",
    "end": "745440"
  },
  {
    "text": "because assigning a body on the disk",
    "start": "745440",
    "end": "747839"
  },
  {
    "text": "will break the constraints",
    "start": "747839",
    "end": "749519"
  },
  {
    "text": "no matter which server i choose",
    "start": "749519",
    "end": "753519"
  },
  {
    "text": "do i need to stop assignment here it's",
    "start": "753519",
    "end": "756399"
  },
  {
    "text": "not desirable",
    "start": "756399",
    "end": "757600"
  },
  {
    "text": "i want to use as many disks as i can",
    "start": "757600",
    "end": "762160"
  },
  {
    "start": "762000",
    "end": "928000"
  },
  {
    "text": "we can relax the portability spread",
    "start": "763440",
    "end": "765440"
  },
  {
    "text": "constraints by using the parameter of",
    "start": "765440",
    "end": "768320"
  },
  {
    "text": "when unsatisfiable this parameter",
    "start": "768320",
    "end": "771040"
  },
  {
    "text": "indicates",
    "start": "771040",
    "end": "771680"
  },
  {
    "text": "what to do if a port does not satisfy",
    "start": "771680",
    "end": "774160"
  },
  {
    "text": "the constraints",
    "start": "774160",
    "end": "776639"
  },
  {
    "text": "according to the optional document the",
    "start": "776639",
    "end": "779279"
  },
  {
    "text": "behavior is described",
    "start": "779279",
    "end": "780800"
  },
  {
    "text": "like this if the constraints are not",
    "start": "780800",
    "end": "784000"
  },
  {
    "text": "satisfiable",
    "start": "784000",
    "end": "784880"
  },
  {
    "text": "and do not schedule is specified group",
    "start": "784880",
    "end": "787680"
  },
  {
    "text": "scheduler does not schedule the pod",
    "start": "787680",
    "end": "790079"
  },
  {
    "text": "this is the default behavior",
    "start": "790079",
    "end": "793200"
  },
  {
    "text": "if the constraints are not satisfiable",
    "start": "793200",
    "end": "795279"
  },
  {
    "text": "and the schedule anywhere is specified",
    "start": "795279",
    "end": "797680"
  },
  {
    "text": "cube scheduler still schedules the part",
    "start": "797680",
    "end": "800320"
  },
  {
    "text": "while prioritizing nodes",
    "start": "800320",
    "end": "802079"
  },
  {
    "text": "that minimize the skew",
    "start": "802079",
    "end": "805200"
  },
  {
    "text": "schedule anyway seemed optimal for us if",
    "start": "805200",
    "end": "808000"
  },
  {
    "text": "it really worked as advertised",
    "start": "808000",
    "end": "810800"
  },
  {
    "text": "so we tried schedule anyway not to limit",
    "start": "810800",
    "end": "814000"
  },
  {
    "text": "resource usage due to uneven local disks",
    "start": "814000",
    "end": "818560"
  },
  {
    "text": "we expected the behavior like this",
    "start": "820399",
    "end": "823440"
  },
  {
    "text": "if the constraints are satisfiable could",
    "start": "823440",
    "end": "826079"
  },
  {
    "text": "be scheduler",
    "start": "826079",
    "end": "826880"
  },
  {
    "text": "always scheduled the part within the",
    "start": "826880",
    "end": "829600"
  },
  {
    "text": "constraints",
    "start": "829600",
    "end": "831120"
  },
  {
    "text": "sounds obvious unfortunately it did not",
    "start": "831120",
    "end": "834560"
  },
  {
    "text": "work as expected",
    "start": "834560",
    "end": "837760"
  },
  {
    "text": "let's start with several ports already",
    "start": "838720",
    "end": "840880"
  },
  {
    "text": "running in the cluster",
    "start": "840880",
    "end": "843839"
  },
  {
    "text": "red leveled pods are running not on all",
    "start": "844079",
    "end": "846959"
  },
  {
    "text": "nodes",
    "start": "846959",
    "end": "848720"
  },
  {
    "text": "four nodes are used and the other two",
    "start": "848720",
    "end": "851360"
  },
  {
    "text": "nodes are completely free",
    "start": "851360",
    "end": "854000"
  },
  {
    "text": "the existing pods cpu resource to some",
    "start": "854000",
    "end": "857360"
  },
  {
    "text": "extent",
    "start": "857360",
    "end": "858160"
  },
  {
    "text": "say 60 and there is enough room for the",
    "start": "858160",
    "end": "862079"
  },
  {
    "text": "new pods we are now distributing",
    "start": "862079",
    "end": "865040"
  },
  {
    "text": "the new port will consume cpu resource",
    "start": "865040",
    "end": "867839"
  },
  {
    "text": "five percent",
    "start": "867839",
    "end": "870480"
  },
  {
    "text": "the constraint we use here is a simple",
    "start": "870480",
    "end": "872639"
  },
  {
    "text": "one keep the skew less than or equal to",
    "start": "872639",
    "end": "875440"
  },
  {
    "text": "one among all nodes",
    "start": "875440",
    "end": "878079"
  },
  {
    "text": "please note that this constraint is not",
    "start": "878079",
    "end": "880399"
  },
  {
    "text": "applied for the existing red",
    "start": "880399",
    "end": "882320"
  },
  {
    "text": "pods we apply the constraint in order to",
    "start": "882320",
    "end": "885279"
  },
  {
    "text": "distribute",
    "start": "885279",
    "end": "886000"
  },
  {
    "text": "stretch management pause evenly",
    "start": "886000",
    "end": "889120"
  },
  {
    "text": "the existing ports are just computing",
    "start": "889120",
    "end": "891600"
  },
  {
    "text": "they are not working for storage",
    "start": "891600",
    "end": "892959"
  },
  {
    "text": "management",
    "start": "892959",
    "end": "894880"
  },
  {
    "text": "then deploy six poles for stretch",
    "start": "894880",
    "end": "897600"
  },
  {
    "text": "management",
    "start": "897600",
    "end": "900240"
  },
  {
    "text": "the expected placement is like this",
    "start": "900639",
    "end": "904240"
  },
  {
    "text": "because the constraint is satisfiable",
    "start": "904240",
    "end": "906639"
  },
  {
    "text": "six poles would be distributed evenly",
    "start": "906639",
    "end": "909839"
  },
  {
    "text": "the skew would be zero here but",
    "start": "909839",
    "end": "913440"
  },
  {
    "text": "the actual placement is like this",
    "start": "913440",
    "end": "916639"
  },
  {
    "text": "all new pods are assigned to the unused",
    "start": "916639",
    "end": "919279"
  },
  {
    "text": "two nodes",
    "start": "919279",
    "end": "920800"
  },
  {
    "text": "the max skew is three this breaks the",
    "start": "920800",
    "end": "923760"
  },
  {
    "text": "constraint",
    "start": "923760",
    "end": "924480"
  },
  {
    "text": "even though it seems satisfiable",
    "start": "924480",
    "end": "928399"
  },
  {
    "text": "why are the new strategy management",
    "start": "929440",
    "end": "931440"
  },
  {
    "text": "parts scheduled in such a way",
    "start": "931440",
    "end": "934639"
  },
  {
    "text": "we inspected the source code of cube",
    "start": "934639",
    "end": "936560"
  },
  {
    "text": "scheduler and find the actual behavior",
    "start": "936560",
    "end": "940240"
  },
  {
    "text": "what we expected had two if clauses",
    "start": "940240",
    "end": "943759"
  },
  {
    "text": "if satisfiable blah blah blah and if not",
    "start": "943759",
    "end": "946959"
  },
  {
    "text": "satisfiable blah blah blah",
    "start": "946959",
    "end": "949839"
  },
  {
    "text": "the actual behavior is whether",
    "start": "949839",
    "end": "953040"
  },
  {
    "text": "the constraints are satisfiable or not",
    "start": "953040",
    "end": "955519"
  },
  {
    "text": "google scheduler does not treat the",
    "start": "955519",
    "end": "957600"
  },
  {
    "text": "constraining conditions as",
    "start": "957600",
    "end": "959279"
  },
  {
    "text": "real constraints instead the conditions",
    "start": "959279",
    "end": "962639"
  },
  {
    "text": "are treated as a part of the scoring",
    "start": "962639",
    "end": "965040"
  },
  {
    "text": "factors",
    "start": "965040",
    "end": "967120"
  },
  {
    "text": "so as a result fluttering cpu resource",
    "start": "967120",
    "end": "970240"
  },
  {
    "text": "usage",
    "start": "970240",
    "end": "970880"
  },
  {
    "text": "took a higher priority even though the",
    "start": "970880",
    "end": "973759"
  },
  {
    "text": "spread constraint was satisfiable",
    "start": "973759",
    "end": "975839"
  },
  {
    "text": "the existing computing ports prevented",
    "start": "975839",
    "end": "978560"
  },
  {
    "text": "even distribution",
    "start": "978560",
    "end": "981279"
  },
  {
    "text": "this came from the prioritizing",
    "start": "981279",
    "end": "983120"
  },
  {
    "text": "algorithm of cube scheduler",
    "start": "983120",
    "end": "985440"
  },
  {
    "text": "so we need to tune it",
    "start": "985440",
    "end": "988959"
  },
  {
    "start": "989000",
    "end": "1088000"
  },
  {
    "text": "the most important criteria in pod",
    "start": "990240",
    "end": "992399"
  },
  {
    "text": "scheduling for our case is",
    "start": "992399",
    "end": "994399"
  },
  {
    "text": "the topology spread constraints so we",
    "start": "994399",
    "end": "997120"
  },
  {
    "text": "turned",
    "start": "997120",
    "end": "997839"
  },
  {
    "text": "clever scheduler to weigh the",
    "start": "997839",
    "end": "999680"
  },
  {
    "text": "constraints more heavily",
    "start": "999680",
    "end": "1003040"
  },
  {
    "text": "because scheduler is rapidly evolving",
    "start": "1003040",
    "end": "1006079"
  },
  {
    "text": "it requires different tuning",
    "start": "1006079",
    "end": "1008000"
  },
  {
    "text": "configurations for its versions",
    "start": "1008000",
    "end": "1011759"
  },
  {
    "text": "for kubernetes 1.17 we adjusted the",
    "start": "1011759",
    "end": "1015199"
  },
  {
    "text": "scheduling policy",
    "start": "1015199",
    "end": "1017440"
  },
  {
    "text": "there is a weight weight parameter named",
    "start": "1017440",
    "end": "1020240"
  },
  {
    "text": "even participate priority",
    "start": "1020240",
    "end": "1022320"
  },
  {
    "text": "and its default value is one we",
    "start": "1022320",
    "end": "1025120"
  },
  {
    "text": "increased",
    "start": "1025120",
    "end": "1026160"
  },
  {
    "text": "we increased the weight to 500",
    "start": "1026160",
    "end": "1030720"
  },
  {
    "text": "the scheduling policy is a global",
    "start": "1030720",
    "end": "1032720"
  },
  {
    "text": "combination so this modification",
    "start": "1032720",
    "end": "1034880"
  },
  {
    "text": "requires",
    "start": "1034880",
    "end": "1035678"
  },
  {
    "text": "extreme caution in kubernetes 1.18",
    "start": "1035679",
    "end": "1040880"
  },
  {
    "text": "the feature of scheduling profiles is",
    "start": "1040880",
    "end": "1043199"
  },
  {
    "text": "introduced",
    "start": "1043199",
    "end": "1045438"
  },
  {
    "text": "google scheduler can handle multiple",
    "start": "1045439",
    "end": "1047918"
  },
  {
    "text": "profiles now",
    "start": "1047919",
    "end": "1049840"
  },
  {
    "text": "we can create a new profile and apply it",
    "start": "1049840",
    "end": "1052559"
  },
  {
    "text": "only to the storage management ports",
    "start": "1052559",
    "end": "1056320"
  },
  {
    "text": "from among the parameters in the profile",
    "start": "1056320",
    "end": "1059200"
  },
  {
    "text": "we set",
    "start": "1059200",
    "end": "1059760"
  },
  {
    "text": "port topology spreads weight to 500 in",
    "start": "1059760",
    "end": "1062799"
  },
  {
    "text": "kubernetes 1.18",
    "start": "1062799",
    "end": "1065760"
  },
  {
    "text": "in 1.19 the parameters are slightly",
    "start": "1065760",
    "end": "1068799"
  },
  {
    "text": "tuned by default",
    "start": "1068799",
    "end": "1070240"
  },
  {
    "text": "so disabling no resources branched",
    "start": "1070240",
    "end": "1072799"
  },
  {
    "text": "allocation is suitable for us",
    "start": "1072799",
    "end": "1076480"
  },
  {
    "text": "i attached two new configurations in",
    "start": "1076480",
    "end": "1078640"
  },
  {
    "text": "this slide",
    "start": "1078640",
    "end": "1080080"
  },
  {
    "text": "please check them from the sketch.com",
    "start": "1080080",
    "end": "1082160"
  },
  {
    "text": "link",
    "start": "1082160",
    "end": "1084480"
  },
  {
    "text": "skip the configurations",
    "start": "1085520",
    "end": "1088960"
  },
  {
    "start": "1088000",
    "end": "1304000"
  },
  {
    "text": "then now i'll give a demonstration",
    "start": "1090240",
    "end": "1093760"
  },
  {
    "text": "i prepared two environments one with",
    "start": "1093760",
    "end": "1096559"
  },
  {
    "text": "default",
    "start": "1096559",
    "end": "1097200"
  },
  {
    "text": "scheduler and then with tuned quest",
    "start": "1097200",
    "end": "1099760"
  },
  {
    "text": "schedule",
    "start": "1099760",
    "end": "1101760"
  },
  {
    "text": "let's see the default one first",
    "start": "1101760",
    "end": "1109840"
  },
  {
    "text": "and this is not tuned environment this",
    "start": "1111360",
    "end": "1113919"
  },
  {
    "text": "is a default",
    "start": "1113919",
    "end": "1115840"
  },
  {
    "text": "schedule",
    "start": "1115840",
    "end": "1118320"
  },
  {
    "text": "sorry",
    "start": "1120840",
    "end": "1123840"
  },
  {
    "text": "this is demonstration environment well",
    "start": "1131919",
    "end": "1137840"
  },
  {
    "text": "first let's see the notes there are four",
    "start": "1143520",
    "end": "1146720"
  },
  {
    "text": "notes here",
    "start": "1146720",
    "end": "1148880"
  },
  {
    "text": "and",
    "start": "1148880",
    "end": "1151840"
  },
  {
    "text": "two computing pods are running they are",
    "start": "1154160",
    "end": "1157120"
  },
  {
    "text": "running on two",
    "start": "1157120",
    "end": "1158080"
  },
  {
    "text": "nodes",
    "start": "1158080",
    "end": "1160480"
  },
  {
    "text": "they consume cp resource",
    "start": "1162000",
    "end": "1165440"
  },
  {
    "text": "on this cluster i deploy a distributed",
    "start": "1165440",
    "end": "1168160"
  },
  {
    "text": "solar system",
    "start": "1168160",
    "end": "1169200"
  },
  {
    "text": "rook and safe i already have destroyed",
    "start": "1169200",
    "end": "1172799"
  },
  {
    "text": "them because it takes too long",
    "start": "1172799",
    "end": "1179840"
  },
  {
    "text": "there are many posts running for rook",
    "start": "1179840",
    "end": "1181840"
  },
  {
    "text": "unsafe including non-operator",
    "start": "1181840",
    "end": "1184480"
  },
  {
    "text": "manager monitors and so on",
    "start": "1184480",
    "end": "1188240"
  },
  {
    "text": "persistent volumes are assigned for osd",
    "start": "1188240",
    "end": "1190840"
  },
  {
    "text": "ports i requested five persistent",
    "start": "1190840",
    "end": "1193600"
  },
  {
    "text": "volumes three",
    "start": "1193600",
    "end": "1194559"
  },
  {
    "text": "five ports and five claims",
    "start": "1194559",
    "end": "1198399"
  },
  {
    "text": "you know five oc pods and five pvcs",
    "start": "1199039",
    "end": "1204559"
  },
  {
    "text": "well let's see where the osd posts are",
    "start": "1205679",
    "end": "1209280"
  },
  {
    "text": "running",
    "start": "1209280",
    "end": "1209840"
  },
  {
    "text": "along with where the computing ports are",
    "start": "1209840",
    "end": "1215039"
  },
  {
    "text": "five osd ports and two computing ports",
    "start": "1218559",
    "end": "1224480"
  },
  {
    "text": "as you can see five osd posts are",
    "start": "1228640",
    "end": "1231679"
  },
  {
    "text": "distributed",
    "start": "1231679",
    "end": "1232559"
  },
  {
    "text": "only to three nodes although there are",
    "start": "1232559",
    "end": "1235600"
  },
  {
    "text": "four available nodes",
    "start": "1235600",
    "end": "1238799"
  },
  {
    "text": "it seems that the reciprocals avoided to",
    "start": "1240159",
    "end": "1242880"
  },
  {
    "text": "coexist",
    "start": "1242880",
    "end": "1243760"
  },
  {
    "text": "with the computing ports",
    "start": "1243760",
    "end": "1248240"
  },
  {
    "text": "this is because kubo scheduler put a",
    "start": "1248240",
    "end": "1250400"
  },
  {
    "text": "higher priority",
    "start": "1250400",
    "end": "1251520"
  },
  {
    "text": "on flattening cpu resource usage",
    "start": "1251520",
    "end": "1254559"
  },
  {
    "text": "and this is not desirable for",
    "start": "1254559",
    "end": "1256159"
  },
  {
    "text": "distributed search system",
    "start": "1256159",
    "end": "1259679"
  },
  {
    "text": "now let's move into another environment",
    "start": "1260240",
    "end": "1264480"
  },
  {
    "text": "the tuned cube scheduler is running here",
    "start": "1265840",
    "end": "1269919"
  },
  {
    "text": "let's see where the oc ports are running",
    "start": "1269919",
    "end": "1273440"
  },
  {
    "text": "along with where the computing puzzle",
    "start": "1273440",
    "end": "1277840"
  },
  {
    "text": "five osd ports and two computing ports",
    "start": "1278000",
    "end": "1282480"
  },
  {
    "text": "five osd ports are distributed well onto",
    "start": "1285360",
    "end": "1288320"
  },
  {
    "text": "four nodes",
    "start": "1288320",
    "end": "1290720"
  },
  {
    "text": "some of them coexist with the computing",
    "start": "1290720",
    "end": "1293200"
  },
  {
    "text": "ports",
    "start": "1293200",
    "end": "1295360"
  },
  {
    "text": "and this is what i wanted to do",
    "start": "1295360",
    "end": "1305840"
  },
  {
    "start": "1304000",
    "end": "1349000"
  },
  {
    "text": "let's wrap up with key takeaways",
    "start": "1305840",
    "end": "1309039"
  },
  {
    "text": "in order to deploy a distributed search",
    "start": "1309039",
    "end": "1311200"
  },
  {
    "text": "system using",
    "start": "1311200",
    "end": "1312080"
  },
  {
    "text": "rook unsafe on on-premise cluster",
    "start": "1312080",
    "end": "1315919"
  },
  {
    "text": "we translate the problem of local search",
    "start": "1315919",
    "end": "1318080"
  },
  {
    "text": "distribution",
    "start": "1318080",
    "end": "1318960"
  },
  {
    "text": "into the pawn scheduling problem using",
    "start": "1318960",
    "end": "1321760"
  },
  {
    "text": "word for",
    "start": "1321760",
    "end": "1322480"
  },
  {
    "text": "first consumer volume binding mode",
    "start": "1322480",
    "end": "1326080"
  },
  {
    "text": "as for bot scheduling we use the",
    "start": "1326080",
    "end": "1328240"
  },
  {
    "text": "portable display constraints for better",
    "start": "1328240",
    "end": "1330840"
  },
  {
    "text": "distribution",
    "start": "1330840",
    "end": "1332159"
  },
  {
    "text": "in order to cope with kuberschedular",
    "start": "1332159",
    "end": "1334320"
  },
  {
    "text": "scoring",
    "start": "1334320",
    "end": "1335360"
  },
  {
    "text": "we too need to prioritize the",
    "start": "1335360",
    "end": "1337360"
  },
  {
    "text": "constraints",
    "start": "1337360",
    "end": "1340000"
  },
  {
    "text": "our configuration for rook unsafe is",
    "start": "1340000",
    "end": "1342080"
  },
  {
    "text": "open source in github",
    "start": "1342080",
    "end": "1343840"
  },
  {
    "text": "please take a look that's all",
    "start": "1343840",
    "end": "1347600"
  },
  {
    "text": "thank you",
    "start": "1347600",
    "end": "1351679"
  }
]