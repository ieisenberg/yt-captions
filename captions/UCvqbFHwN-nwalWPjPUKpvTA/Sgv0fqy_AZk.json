[
  {
    "text": "hey everyone uh welcome to cube day Singapore today we are going to talk about uh handling billions of metrics",
    "start": "240",
    "end": "7279"
  },
  {
    "text": "with promethus and Thanos I'm Ravi Hari I work as a",
    "start": "7279",
    "end": "12360"
  },
  {
    "text": "principal software engineer at inot hey I'm Amit and I work as a senior software in in at",
    "start": "12360",
    "end": "18800"
  },
  {
    "text": "in at in we work on an AI driven expert platform which drives the work for all",
    "start": "18800",
    "end": "25680"
  },
  {
    "text": "the developers at inota and derives modern development experiences and this",
    "start": "25680",
    "end": "30759"
  },
  {
    "text": "platform is powered by kubernetes infrastructure and uh these are some of the metrics that signify the importance",
    "start": "30759",
    "end": "36440"
  },
  {
    "text": "of the platform uh our day and day our jobs is to ensure the platform scales as",
    "start": "36440",
    "end": "42320"
  },
  {
    "text": "well as solve Neo use cases here is the quick agenda we'll see",
    "start": "42320",
    "end": "48480"
  },
  {
    "text": "the metric Evolution at scale with Prometheus and Thanos and then we'll see different use cases with ar roll outs",
    "start": "48480",
    "end": "55039"
  },
  {
    "text": "for metrics and then how we leverage metrics for AA Ops and then how we are",
    "start": "55039",
    "end": "60320"
  },
  {
    "text": "using metrics for Argo CD uh metrics extension dashboard and then how we push",
    "start": "60320",
    "end": "66439"
  },
  {
    "text": "metrics for Golden signals uh to even best v c adapter so it all started like",
    "start": "66439",
    "end": "72000"
  },
  {
    "text": "this uh we have close to 320 kubernetes clusters uh at in and it's growing uh on",
    "start": "72000",
    "end": "79280"
  },
  {
    "text": "a day-to-day basis uh and then U uh as as Prometheus is a def facto standard",
    "start": "79280",
    "end": "85920"
  },
  {
    "text": "for kubernetes um for the metrics monitoring and uh uh these uh HP use",
    "start": "85920",
    "end": "92119"
  },
  {
    "text": "cases uh we thought of uh running kubernetes um uh Prometheus on top of kubernetes with Prometheus operator and",
    "start": "92119",
    "end": "99320"
  },
  {
    "text": "um we created a prome h pair uh where one of them is primary and the other one is secondary uh and then we install",
    "start": "99320",
    "end": "107520"
  },
  {
    "text": "service monitors uh that scrapes the data from services and collects the metrics from the services and stores in",
    "start": "107520",
    "end": "114119"
  },
  {
    "text": "Prometheus so we used to store this data for 36 hours because of various use cases and because of which um uh we had",
    "start": "114119",
    "end": "121439"
  },
  {
    "text": "to run Prometheus with an EBS volume uh because it's a standalone uh instance uh",
    "start": "121439",
    "end": "127880"
  },
  {
    "text": "each one of them and then uh we cannot store data uh in a object store with",
    "start": "127880",
    "end": "133480"
  },
  {
    "text": "Prometheus uh as of that time um so uh what we did then was we were collecting",
    "start": "133480",
    "end": "139840"
  },
  {
    "text": "the data and then we use a third party tool for visualization so we had to translate the Prometheus metrics into",
    "start": "139840",
    "end": "145360"
  },
  {
    "text": "custom metric format that this third party visualization tool supports so we created additional add-ons like uh the",
    "start": "145360",
    "end": "151319"
  },
  {
    "text": "storage adapter for Prometheus and then uh the S3 adapter that stores the data",
    "start": "151319",
    "end": "156640"
  },
  {
    "text": "in the custom rric format in the S3 bucket which then was leveraged by the custom uh you know visualization tool",
    "start": "156640",
    "end": "163560"
  },
  {
    "text": "for us and then uh you know the metric started growing and the number of",
    "start": "163560",
    "end": "169879"
  },
  {
    "text": "metrics that we had were overwhelming right and it was U difficult for the developers to always look at these",
    "start": "169879",
    "end": "176040"
  },
  {
    "text": "dashboards and figure out what's going wrong or stuff like that so what we thought of is coming up with few golden",
    "start": "176040",
    "end": "181879"
  },
  {
    "text": "signals uh leveraging this rate methodology uh the metrics such as rate",
    "start": "181879",
    "end": "187560"
  },
  {
    "text": "of requests uh error count and then uh latencies along with CPU and memory",
    "start": "187560",
    "end": "193280"
  },
  {
    "text": "utilization on the parts uh we we kind of looked into some few critical metrics for for the parts of the applications",
    "start": "193280",
    "end": "200799"
  },
  {
    "text": "and then uh we had written our own custom Prometheus rules on top of the base metrics and then we started",
    "start": "200799",
    "end": "206920"
  },
  {
    "text": "shipping the data out into even bus because we had a separate pipeline that reads data from this event bus and",
    "start": "206920",
    "end": "212640"
  },
  {
    "text": "processes uh the data and sends alerts for these golden signals so we had to",
    "start": "212640",
    "end": "217920"
  },
  {
    "text": "introduce a component called Prometheus CFA adapter we looked into the open source solution uh we liked it but it",
    "start": "217920",
    "end": "223879"
  },
  {
    "text": "missed some of the features so we had enhanced it we'll cover that in the subsequent",
    "start": "223879",
    "end": "230040"
  },
  {
    "text": "section so you know we had to adopt to Golden signals and we had to write the",
    "start": "230040",
    "end": "235200"
  },
  {
    "text": "data out from the Prometheus remote right into even bus this is additional use case right",
    "start": "235200",
    "end": "240480"
  },
  {
    "text": "and what happened was we started with uh some opening up 300 metrics per pod and",
    "start": "240480",
    "end": "246680"
  },
  {
    "text": "uh eventually it went all the way up to 1200 metrics per part that we had to scrape even that was not sufficient in",
    "start": "246680",
    "end": "252799"
  },
  {
    "text": "some cases we went all the way up to 4,000 metrics per pot that we had to scrape the metrics use cases have been",
    "start": "252799",
    "end": "259160"
  },
  {
    "text": "growing and the rate of request for the metric uh has been growing right so uh",
    "start": "259160",
    "end": "264600"
  },
  {
    "text": "at int we have uh seen that the rate of metrics in the last couple of years has been grown by",
    "start": "264600",
    "end": "270120"
  },
  {
    "text": "2x so single instance of Prometheus ha was not sufficient to handle all this",
    "start": "270120",
    "end": "276520"
  },
  {
    "text": "metrics we had to scale Prometheus horizontally and when we looked out for the solutions for it we came across",
    "start": "276520",
    "end": "283360"
  },
  {
    "text": "Thanos project and Thanos can scale Prometheus horizontally the way we have done this is we have run Thanos side car",
    "start": "283360",
    "end": "290960"
  },
  {
    "text": "along with Prometheus uh and then we this side car will write the data from",
    "start": "290960",
    "end": "296600"
  },
  {
    "text": "Prometheus into a Thanos S3 bucket that is shown on the top so so what we did",
    "start": "296600",
    "end": "302759"
  },
  {
    "text": "was we reduced the retention uh in the Prometheus instances that kind of reduced the size of the Prometheus",
    "start": "302759",
    "end": "307919"
  },
  {
    "text": "instances but we are storing the data in asbet and retrieving this data for up to",
    "start": "307919",
    "end": "313080"
  },
  {
    "text": "48 Hours uh using Thanos uh parts right uh so Thanos query is the front end and",
    "start": "313080",
    "end": "320360"
  },
  {
    "text": "Thanos query query is Thanos store which which retrieves data from uh S3 bucket and sends it back to the users so we had",
    "start": "320360",
    "end": "328280"
  },
  {
    "text": "to skill Prometheus horizont to handle all the additional metrics growth at in right so this is how uh we",
    "start": "328280",
    "end": "336520"
  },
  {
    "text": "started getting into Thanos and once we have opened the doors with this there are additional use cases that uh came",
    "start": "336520",
    "end": "343880"
  },
  {
    "text": "out so one of the challenges that we fac was um when we scaled Prometheus",
    "start": "343880",
    "end": "349440"
  },
  {
    "text": "horizontally uh we started seeing uh hpn not working appropriately the reason for",
    "start": "349440",
    "end": "355000"
  },
  {
    "text": "that is when we have multiple uh shots of Prometheus each shot of of Prometheus",
    "start": "355000",
    "end": "360800"
  },
  {
    "text": "uh was uh you know getting the data from a given name space only to a set of",
    "start": "360800",
    "end": "366319"
  },
  {
    "text": "Parts because um the Pod ipce is what we eventually uh you know derive from when",
    "start": "366319",
    "end": "372560"
  },
  {
    "text": "we query from the service of uh that particular name space uh and then uh",
    "start": "372560",
    "end": "377919"
  },
  {
    "text": "each pod is hashed to a particular Prometheus instance so the data uh for",
    "start": "377919",
    "end": "384520"
  },
  {
    "text": "the metrics uh is getting split even though they all belong to the same set of uh name space uh the parts belong to",
    "start": "384520",
    "end": "391520"
  },
  {
    "text": "the same name space the part distribution among the Prometheus instances is getting divided so when the",
    "start": "391520",
    "end": "397639"
  },
  {
    "text": "HPA metric we getting calculated via uh the kubernetes API uh the data that we",
    "start": "397639",
    "end": "404280"
  },
  {
    "text": "are getting is not accurate because it's not aggregating across multiple Prometheus instances in order to solve",
    "start": "404280",
    "end": "410039"
  },
  {
    "text": "that we had introduced another component of Thanos called Thanos ruler and Thanos ruler is a component that queries Thanos",
    "start": "410039",
    "end": "416960"
  },
  {
    "text": "query which internally can retrieve data data from Prometheus as well as it can",
    "start": "416960",
    "end": "422000"
  },
  {
    "text": "retrieve data from S3 bucket uh and then what we had done is we had uh evaluated this rules uh in Thanos ruler and we are",
    "start": "422000",
    "end": "429599"
  },
  {
    "text": "not evaluating the Prometheus rules in Prometheus anymore so that this Central",
    "start": "429599",
    "end": "434639"
  },
  {
    "text": "evaluation had solved us for the HP use cases uh once the rules are evaluated we",
    "start": "434639",
    "end": "440400"
  },
  {
    "text": "write it back to Thanos receive component which puts the metc back to S3 bucket right so this is how we solved",
    "start": "440400",
    "end": "446960"
  },
  {
    "text": "our HPA uh issues and then more and more use cases have uh",
    "start": "446960",
    "end": "453680"
  },
  {
    "text": "been um uh you know evolving at in so uh one of the use cases uh to use a Ops AI",
    "start": "453680",
    "end": "461560"
  },
  {
    "text": "has become a norm in the last couple of years and uh uh we at in started using this project called Numa Pro uh which uh",
    "start": "461560",
    "end": "469360"
  },
  {
    "text": "leverages metrics to solve for the anomaly detection uh with multiple",
    "start": "469360",
    "end": "475199"
  },
  {
    "text": "applications so one such application is argu rollouts uh we'll talk about that uh in detail uh the additional use case",
    "start": "475199",
    "end": "481879"
  },
  {
    "text": "that we have uh come across is U Arco CD Arco CD started extending um its",
    "start": "481879",
    "end": "489120"
  },
  {
    "text": "functionality so that uh uh use cases such as U uh config map and U uh uh",
    "start": "489120",
    "end": "496120"
  },
  {
    "text": "config Mac plugins uh or metric plugins and the new dashboards for roll outs are",
    "start": "496120",
    "end": "501759"
  },
  {
    "text": "all now integrated into Arco CD one of such extension is metric extension so users don't have to go to different",
    "start": "501759",
    "end": "508039"
  },
  {
    "text": "screens once they install Argo CD uh they can view events locks metrics in",
    "start": "508039",
    "end": "513560"
  },
  {
    "text": "the same uh dashboard so Argo CD also started uh you know leveraging metrics",
    "start": "513560",
    "end": "520760"
  },
  {
    "text": "from this Prometheus instance right so we had to call this Thanos query uh",
    "start": "520760",
    "end": "526360"
  },
  {
    "text": "instance um to retrieve the data but if you see all of these um different",
    "start": "526360",
    "end": "532640"
  },
  {
    "text": "workloads are putting pressure on Prometheus and scaling Prometheus is costly so this is uh some something that",
    "start": "532640",
    "end": "540040"
  },
  {
    "text": "that is kind of burning the cost for us now we started looking at how can we optimize the cost and not uh uh impact",
    "start": "540040",
    "end": "547360"
  },
  {
    "text": "Prometheus with all these different use cases because Prometheus has this remote rate functionality can we leverage uh it",
    "start": "547360",
    "end": "554360"
  },
  {
    "text": "in such a way that we can reduce the cost so what we did was we uh had",
    "start": "554360",
    "end": "560200"
  },
  {
    "text": "separation of concerns for each one of these use cases uh for the regular use cases we let uh the cluster admins and",
    "start": "560200",
    "end": "567839"
  },
  {
    "text": "the developers take care of of U the regular read query path uh which queries",
    "start": "567839",
    "end": "574800"
  },
  {
    "text": "the Thanos and then uh retries data from Prometheus because there are very few limited use cases for this uh the the",
    "start": "574800",
    "end": "582120"
  },
  {
    "text": "TPS is less whereas where the TPS is high for example for the aops use case",
    "start": "582120",
    "end": "587800"
  },
  {
    "text": "um we wanted to store data for 8 days and retrieving data for 8 Days uh means",
    "start": "587800",
    "end": "592880"
  },
  {
    "text": "that the Thanos query parts were overwhelming and it reads a lot of data from uh the back end right so we had to",
    "start": "592880",
    "end": "599800"
  },
  {
    "text": "add an additional component called query front end that kind of chunks the data into the interval that you want we have",
    "start": "599800",
    "end": "606480"
  },
  {
    "text": "chunked it for every 2 hours so that the uh Thanos query uh pods are not",
    "start": "606480",
    "end": "612240"
  },
  {
    "text": "exhausting right and it retries data back from the Thanos receive so the remote write here would write data back",
    "start": "612240",
    "end": "618160"
  },
  {
    "text": "to Thanos receive and here we store the data for 8 days in this use case for the AA Ops uh the primary reason we chose to",
    "start": "618160",
    "end": "625399"
  },
  {
    "text": "store it for H days here is because for any anomaly detect based on our study that we found in",
    "start": "625399",
    "end": "632040"
  },
  {
    "text": "order to get 95% confidence on that anomaly you need at least 8 days worth of data with 30 second scrape interval",
    "start": "632040",
    "end": "639399"
  },
  {
    "text": "of time right uh and other use cases for argocd here in order to show the live",
    "start": "639399",
    "end": "644959"
  },
  {
    "text": "metric because developers will be mostly interested in live metric uh when they are looking at Arco CD they wanted to",
    "start": "644959",
    "end": "650920"
  },
  {
    "text": "see immediately how the deployment went uh and how are the metrics coming from the new deployment and other things we",
    "start": "650920",
    "end": "656639"
  },
  {
    "text": "didn't want to store for too long right so so uh we stored uh in this uh",
    "start": "656639",
    "end": "661800"
  },
  {
    "text": "instance of the Thanos receive only for 6 hours of data so with these uh",
    "start": "661800",
    "end": "667360"
  },
  {
    "text": "different use cases we solve the problem uh by splitting into different pipelines",
    "start": "667360",
    "end": "673000"
  },
  {
    "text": "uh but the source of all these metrics is through Prometheus and Thanos has helped us solve this problems let's look",
    "start": "673000",
    "end": "678760"
  },
  {
    "text": "at one use case with argu roll outs so Argo rollouts is u a you know uh",
    "start": "678760",
    "end": "686519"
  },
  {
    "text": "a replacement uh for deployment from the open source kubernetes um so kubernetes",
    "start": "686519",
    "end": "693639"
  },
  {
    "text": "in the open source doesn't support blue green or Canary or Progressive roll outs so argu roll outs is created to solve",
    "start": "693639",
    "end": "699880"
  },
  {
    "text": "that problem and is made available in the open source right um and then uh we",
    "start": "699880",
    "end": "705279"
  },
  {
    "text": "leverage metrics primarily to Define when to promote uh a a deployment uh",
    "start": "705279",
    "end": "712160"
  },
  {
    "text": "from uh Canary to stable or shift it from blue to green and stuff like that right to make it stable we use this",
    "start": "712160",
    "end": "720320"
  },
  {
    "text": "analysis templates that run the analysis and qualify whether the new deployment or the new image that you have deployed",
    "start": "720320",
    "end": "726560"
  },
  {
    "text": "uh is running successfully or not a definition of analysis template looks like this and as you can see here we are",
    "start": "726560",
    "end": "732680"
  },
  {
    "text": "using a Prometheus query uh to you know query the data from Prometheus and ensure it meets a success condition",
    "start": "732680",
    "end": "740880"
  },
  {
    "text": "right and the way it works internally is that you have a stable deployment and",
    "start": "740880",
    "end": "746760"
  },
  {
    "text": "once you introduce a new image it creat creates a canary replica set and that creates a canary pod if let's say we",
    "start": "746760",
    "end": "753760"
  },
  {
    "text": "have five uh pods running for this replica set and one pod shifts into the",
    "start": "753760",
    "end": "759000"
  },
  {
    "text": "new replica set that percentage of traffic gets shifted into the new replica set and at that time we can run",
    "start": "759000",
    "end": "766360"
  },
  {
    "text": "the analysis template which triggers an analysis r that queries data from Prometheus aror supports other metric",
    "start": "766360",
    "end": "772880"
  },
  {
    "text": "providers but at end we primarily use Prometheus as a back end to retrieve the metric data and once this analysis run",
    "start": "772880",
    "end": "779440"
  },
  {
    "text": "is successful we can then subsequently promote the rest of the parts and make the canary as",
    "start": "779440",
    "end": "785959"
  },
  {
    "text": "stable so let's look at a quick demo for",
    "start": "785959",
    "end": "791240"
  },
  {
    "text": "this so here uh you can see the roll out definition uh in this roll out definition we are seeing that the name",
    "start": "799000",
    "end": "805199"
  },
  {
    "text": "of the roll out is roll out sto and uh it is running in metric analysis uh namespace and there are five replicas",
    "start": "805199",
    "end": "811519"
  },
  {
    "text": "and we use Canary as a strategy and we had defined different weights so initially 20% will shift and then run",
    "start": "811519",
    "end": "818800"
  },
  {
    "text": "the analysis template um and that triggers analysis run if that passes",
    "start": "818800",
    "end": "824000"
  },
  {
    "text": "through then the rest of the uh traffic gets shifted after 10 seconds each time",
    "start": "824000",
    "end": "829639"
  },
  {
    "text": "we can also run this analysis template in the background so that at every step this analysis step uh analysis template",
    "start": "829639",
    "end": "835279"
  },
  {
    "text": "gets executed and we uh ensure it is uh up and running successfully right uh",
    "start": "835279",
    "end": "840440"
  },
  {
    "text": "let's look at uh the analysis template definition as we have seen earlier uh we're calling this as a success rate uh",
    "start": "840440",
    "end": "846480"
  },
  {
    "text": "here we are ensuring uh that you know this runs every 30 seconds but uh we can",
    "start": "846480",
    "end": "852000"
  },
  {
    "text": "Define the count and the failure limit uh other fields as required uh we have just taken it as a base and simple thing",
    "start": "852000",
    "end": "858759"
  },
  {
    "text": "for the demo as one uh but you can also put the Timeout on the Prometheus if it is not responding you can you know time",
    "start": "858759",
    "end": "865639"
  },
  {
    "text": "it out and retry after some certain time and stuff like that right so uh here we are querying for the CP",
    "start": "865639",
    "end": "870880"
  },
  {
    "text": "utilization data in this namespace for this particular app and uh we have defined the success criteria for that",
    "start": "870880",
    "end": "877720"
  },
  {
    "text": "let's go ahead and uh you know quickly see that we have already created this",
    "start": "877720",
    "end": "882920"
  },
  {
    "text": "roll out um and uh we we'll just go ahead and update the image with uh a",
    "start": "882920",
    "end": "888959"
  },
  {
    "text": "newer image so that um the canary process starts so I've just zoomed in",
    "start": "888959",
    "end": "894800"
  },
  {
    "text": "here for the Easy View so as you can see the newer uh image starts getting",
    "start": "894800",
    "end": "900240"
  },
  {
    "text": "deployed and uh you know newer parts are getting provisioned if you look at the",
    "start": "900240",
    "end": "906720"
  },
  {
    "text": "analysis template this is the analysis template that we have defined and you can see that a new analysis T got",
    "start": "906720",
    "end": "912079"
  },
  {
    "text": "triggered and it is successful that is why it started promoting the uh pods uh in that names space if you look at the",
    "start": "912079",
    "end": "918440"
  },
  {
    "text": "value that we got from Prometheus for this query uh it is um 018 which is uh",
    "start": "918440",
    "end": "925199"
  },
  {
    "text": "lesser than our uh Target so it is the success criteria passed and it started",
    "start": "925199",
    "end": "930920"
  },
  {
    "text": "promoting and as you can see here it started shifting other parts uh and into",
    "start": "930920",
    "end": "936279"
  },
  {
    "text": "the new Canary and it will make the canary stable so this is the uh overall use case uh for leveraging metrics with",
    "start": "936279",
    "end": "944160"
  },
  {
    "text": "the ARG Lots Amit will cover the other use cases thanks Ri",
    "start": "944160",
    "end": "950639"
  },
  {
    "text": "so yeah I'll move forward with the uh next use case which is regarding AI Ops",
    "start": "950639",
    "end": "956880"
  },
  {
    "text": "with metrics so at into it we use automated detection of discrepancies and",
    "start": "956880",
    "end": "962240"
  },
  {
    "text": "issues uh something called uh anomaly score so it's a valuable tool for",
    "start": "962240",
    "end": "968040"
  },
  {
    "text": "developers to find out issues quicker than other traditional methods and also",
    "start": "968040",
    "end": "973759"
  },
  {
    "text": "it reduces MTD and mttr U so uh in in we use Aro roll outs along with this",
    "start": "973759",
    "end": "979519"
  },
  {
    "text": "anomaly score to automatically roll back changes if the quality of changes",
    "start": "979519",
    "end": "985399"
  },
  {
    "text": "doesn't meet the required standards so while implementing this we Face several",
    "start": "985399",
    "end": "991079"
  },
  {
    "text": "challenges so uh one of them was access to historical data and whenever this",
    "start": "991079",
    "end": "996560"
  },
  {
    "text": "Numa logic infrastructure was squaring this data we had to scale based on the load so uh we had this in cluster",
    "start": "996560",
    "end": "1003759"
  },
  {
    "text": "Prometheus but it was already doing uh some multiple operations like scraping compaction it was also suring HP metrics",
    "start": "1003759",
    "end": "1011759"
  },
  {
    "text": "and it was remoted writing to different things uh and also doing rule evaluation so we didn't want to put uh another read",
    "start": "1011759",
    "end": "1019199"
  },
  {
    "text": "heavy operation on Prometheus so we came up with this initial design where uh as",
    "start": "1019199",
    "end": "1024480"
  },
  {
    "text": "you can see we're pushing the data from Prometheus using Thanos side card to S3",
    "start": "1024480",
    "end": "1029798"
  },
  {
    "text": "bucket in the highlighted sections and we created a dedicated pipeline for this",
    "start": "1029799",
    "end": "1034839"
  },
  {
    "text": "Nal logic infrastructure to query the a data uh so in the his bucket we are",
    "start": "1034839",
    "end": "1040600"
  },
  {
    "text": "storing the data for 8 days and uh we are using this query front end to",
    "start": "1040600",
    "end": "1045640"
  },
  {
    "text": "efficiently fetch that data but uh here is some problem right uh because we",
    "start": "1045640",
    "end": "1051679"
  },
  {
    "text": "are storing all the data for 8 days in S3 bucket but for AI Ops we don't need",
    "start": "1051679",
    "end": "1057640"
  },
  {
    "text": "all the metrics right there are only few metrics that we are interested in also",
    "start": "1057640",
    "end": "1062679"
  },
  {
    "text": "this uh huge amount of data in S3 is incurring lot of cost and uh the way",
    "start": "1062679",
    "end": "1067960"
  },
  {
    "text": "tsdb blocks are designed whenever you are retrieving a particular metric it would load the entire tsdb Block in the",
    "start": "1067960",
    "end": "1075480"
  },
  {
    "text": "memory and uh it may contain some unnecessary metrics that you don't want",
    "start": "1075480",
    "end": "1081039"
  },
  {
    "text": "so so improve the design a bit and uh what we did was we replaced the uh store",
    "start": "1081039",
    "end": "1087799"
  },
  {
    "text": "pods with Thanos receive and here what we are doing we are only writing the",
    "start": "1087799",
    "end": "1093280"
  },
  {
    "text": "required metrics from Prometheus to the Thanos receive and we are keeping it there in its local tsdb and we keeping",
    "start": "1093280",
    "end": "1100840"
  },
  {
    "text": "it there for 8 days since the number of metrics are less so we can afford that",
    "start": "1100840",
    "end": "1106480"
  },
  {
    "text": "and whenever the numerology infrastructure is saring the data we are again going by the same path using",
    "start": "1106480",
    "end": "1113039"
  },
  {
    "text": "Thanos query front end and Thanos query but this time we fetching the data",
    "start": "1113039",
    "end": "1118159"
  },
  {
    "text": "directly from Thanos receives uh uh EVS volumes so that is uh much faster and uh",
    "start": "1118159",
    "end": "1125360"
  },
  {
    "text": "there is a significant cost reduction as well due to the volume of data that we have now so I'll quickly show a demo on",
    "start": "1125360",
    "end": "1133039"
  },
  {
    "text": "the performance of this new uh infrastructure uh so I recorded one let",
    "start": "1133039",
    "end": "1138679"
  },
  {
    "text": "me play it so uh as you can see at the top uh this is a query S3 instance that",
    "start": "1138679",
    "end": "1145240"
  },
  {
    "text": "means that it is connected to the S3 based Pipeline and uh uh I'll quickly",
    "start": "1145240",
    "end": "1151440"
  },
  {
    "text": "show you the store uh that is connected to so it says it is connected to a Thanos store",
    "start": "1151440",
    "end": "1157360"
  },
  {
    "text": "instance and we can quickly verify that using the IP so it's a qu in uh Thanos",
    "start": "1157360",
    "end": "1163480"
  },
  {
    "text": "sarus 3 store instance and let's try to get some data for 8",
    "start": "1163480",
    "end": "1170480"
  },
  {
    "text": "Days uh yeah as you can see it took around uh 4 seconds to fetch that data",
    "start": "1170480",
    "end": "1175679"
  },
  {
    "text": "here right and um so yeah let's try to uh see the newer approach using receive",
    "start": "1175679",
    "end": "1184159"
  },
  {
    "text": "uh we'll query the same data here so before that we'll quickly see disconnected to two receive instance um",
    "start": "1184159",
    "end": "1193159"
  },
  {
    "text": "and we can verify that using this command",
    "start": "1193159",
    "end": "1199200"
  },
  {
    "text": "so um yeah let's try to F the same data here and if we click on execute as you",
    "start": "1199200",
    "end": "1207000"
  },
  {
    "text": "can see it it only took around 300 milliseconds or 350 milliseconds to F",
    "start": "1207000",
    "end": "1212440"
  },
  {
    "text": "the data right so there is a significant performance Improvement on this so that",
    "start": "1212440",
    "end": "1217880"
  },
  {
    "text": "was the demo on this uh let me move forward with the next use case uh so as",
    "start": "1217880",
    "end": "1224679"
  },
  {
    "text": "ravie was talking about Argo City Life metrics right so we have this new extension is uh in Aro CD that allows",
    "start": "1224679",
    "end": "1232080"
  },
  {
    "text": "the developers to customize their key metrics and uh show it in the Argo CD UI",
    "start": "1232080",
    "end": "1237240"
  },
  {
    "text": "itself uh so you can configure metrics like CPU memory and some HTTP error",
    "start": "1237240",
    "end": "1242559"
  },
  {
    "text": "metrics and you can directly view it in the dashboard so it also helps providing a",
    "start": "1242559",
    "end": "1248039"
  },
  {
    "text": "unified view uh to the developers and identify any issues uh quicker so in the",
    "start": "1248039",
    "end": "1253480"
  },
  {
    "text": "diagram you can see that whenever there is a query from the argocd browser uh UI",
    "start": "1253480",
    "end": "1258880"
  },
  {
    "text": "app uh it goes to the Aro CD server and there is a extension service in place",
    "start": "1258880",
    "end": "1264320"
  },
  {
    "text": "which uh communicates to different argd extensions here that we are interested",
    "start": "1264320",
    "end": "1269400"
  },
  {
    "text": "in the metrix extension so when the query goes to the metrix extension it",
    "start": "1269400",
    "end": "1274840"
  },
  {
    "text": "tries to query the in cluster Prometheus to fetch the metrics but uh here we Face similar set of challenges as before like",
    "start": "1274840",
    "end": "1282240"
  },
  {
    "text": "the in cluster Prometheus are already doing multiple stuff and we don't want to add another read heavy operation to",
    "start": "1282240",
    "end": "1287880"
  },
  {
    "text": "it so we came up with a similar approach here uh uh yeah so came up with a",
    "start": "1287880",
    "end": "1294320"
  },
  {
    "text": "similar approach here so in the Highlight section we created a dedicated read path for this Aro live metrix and",
    "start": "1294320",
    "end": "1301720"
  },
  {
    "text": "as you can see we are remote writing the data from Prometheus only the required",
    "start": "1301720",
    "end": "1306799"
  },
  {
    "text": "metric to a separate Thanos receive uh tsdb instance so uh whenever this Argo",
    "start": "1306799",
    "end": "1313679"
  },
  {
    "text": "CD is squaring this data it is going via separate uh read path and this path can",
    "start": "1313679",
    "end": "1319760"
  },
  {
    "text": "scale independently and it doesn't affect the in cluster Prometheus so this is how it looks when",
    "start": "1319760",
    "end": "1326440"
  },
  {
    "text": "you configur the U metrics tab in Argo CD so you can see all the metrics in the",
    "start": "1326440",
    "end": "1332320"
  },
  {
    "text": "same tab as your application and uh coming to the next one uh so we wanted to send some golden",
    "start": "1332320",
    "end": "1339559"
  },
  {
    "text": "signal metrics to Kafka uh the need here was to uh send data to multiple Kafka",
    "start": "1339559",
    "end": "1345960"
  },
  {
    "text": "topics and uh we wanted to support tag based filtering as well uh the",
    "start": "1345960",
    "end": "1351720"
  },
  {
    "text": "challenges were uh there were some open source Solutions available but uh uh it doesn't support multiple topics also uh",
    "start": "1351720",
    "end": "1359919"
  },
  {
    "text": "there was no graceful shutdown and connection handling also if you wanted to use those Solutions uh we need to add",
    "start": "1359919",
    "end": "1367400"
  },
  {
    "text": "uh extra remote rights to Prometheus and that would add a extra 25% memory",
    "start": "1367400",
    "end": "1373080"
  },
  {
    "text": "overhead on Prometheus so uh what we did was we came up with our uh",
    "start": "1373080",
    "end": "1379400"
  },
  {
    "text": "custom Prometheus cafka adapter which which had a better uh connection",
    "start": "1379400",
    "end": "1384840"
  },
  {
    "text": "handling and all those things also we added a multi-topic support that means we are able to send uh metrics to",
    "start": "1384840",
    "end": "1392159"
  },
  {
    "text": "multiple topics based on something called a tag passer so a tag passer is",
    "start": "1392159",
    "end": "1397799"
  },
  {
    "text": "nothing but uh there is a image as well okay yeah so the tag passer is nothing",
    "start": "1397799",
    "end": "1404080"
  },
  {
    "text": "but a collection of labels and a matcher so whenever a metric coming from",
    "start": "1404080",
    "end": "1409640"
  },
  {
    "text": "Prometheus matches those uh tags here right um so it has to match this F equal",
    "start": "1409640",
    "end": "1416120"
  },
  {
    "text": "to uh anything and bar equal to true and that metric will go to CF Copic one uh",
    "start": "1416120",
    "end": "1423760"
  },
  {
    "text": "and anything which doesn't match any of the matches will be dropped so I'll",
    "start": "1423760",
    "end": "1429440"
  },
  {
    "text": "quickly show a demo on this uh so um let me play",
    "start": "1429440",
    "end": "1436520"
  },
  {
    "text": "this so we are using Telegraph to generate some mock Prometheus metrics uh",
    "start": "1436640",
    "end": "1443039"
  },
  {
    "text": "as you can see there is metric one with this labels uh Fu and bar there is",
    "start": "1443039",
    "end": "1448480"
  },
  {
    "text": "metric two with b equal to True uh so ideally metric 1 and Metric two should",
    "start": "1448480",
    "end": "1453720"
  },
  {
    "text": "go to cka topic 1 and two and this metric 3 should be uh dropped so uh we",
    "start": "1453720",
    "end": "1459880"
  },
  {
    "text": "are also creating some local Kafka uh instances using Docker",
    "start": "1459880",
    "end": "1465720"
  },
  {
    "text": "compose um and this is the same uh configuration that I showed in the uh",
    "start": "1465720",
    "end": "1471399"
  },
  {
    "text": "slide so uh let's try to uh start all these",
    "start": "1471399",
    "end": "1478039"
  },
  {
    "text": "components yeah let's start the telegraph first to generate the mock metrics and as you can see it uh started",
    "start": "1478679",
    "end": "1486000"
  },
  {
    "text": "generating some Moog data uh we are also starting the Kafka as well uh so uh let",
    "start": "1486000",
    "end": "1494799"
  },
  {
    "text": "it start and then we can start this uh CFA storage adapter which uh starts",
    "start": "1494799",
    "end": "1502399"
  },
  {
    "text": "accepting the remote right request from Telegraph and as you can see uh there is",
    "start": "1502399",
    "end": "1509120"
  },
  {
    "text": "uh uh all the metrics are going to the correct topic metrics one going to topic",
    "start": "1509120",
    "end": "1516520"
  },
  {
    "text": "one and similarly uh all the other metrics are going to the correct topic",
    "start": "1516520",
    "end": "1522240"
  },
  {
    "text": "based on the configuration yeah so that was the demo uh on this and I'll uh hand over to RI",
    "start": "1522240",
    "end": "1530279"
  },
  {
    "text": "to talk about Matrix component that into it so uh we have seen certain use cases",
    "start": "1530279",
    "end": "1535600"
  },
  {
    "text": "uh that are critical uh for us but uh in general there are many other components",
    "start": "1535600",
    "end": "1540840"
  },
  {
    "text": "at in that we uh leverage metrics for so uh here are some 15 add-ons that uh we",
    "start": "1540840",
    "end": "1546760"
  },
  {
    "text": "use at a high level some of them are inbuilt and open source via Kiko proach and some of them are consumed uh from",
    "start": "1546760",
    "end": "1553279"
  },
  {
    "text": "the open source like Prometheus and Thanos right so uh we have uh tools like active monitor that does synthetic",
    "start": "1553279",
    "end": "1559840"
  },
  {
    "text": "monitoring of the add-ons and self heals if the add-on uh is in a bad State and",
    "start": "1559840",
    "end": "1565080"
  },
  {
    "text": "then AWS metrix to check the resource limits on the cloud provider and then self heal if in case the limits are",
    "start": "1565080",
    "end": "1571320"
  },
  {
    "text": "breached uh and then we use Cube metric and then we use uh uh metric server uh",
    "start": "1571320",
    "end": "1577399"
  },
  {
    "text": "to Fitch uh CPM memory data for the parts and then uh we use uh alert",
    "start": "1577399",
    "end": "1583600"
  },
  {
    "text": "manager to create alerts through gitops and then we use uh node startup Monitor",
    "start": "1583600",
    "end": "1588919"
  },
  {
    "text": "and cost monitor to keep a check on the cost and uh the startup time on the noes and stuff like that so uh there are",
    "start": "1588919",
    "end": "1595840"
  },
  {
    "text": "there is a lot of uh uh you know depth at which uh we look at the metrics and",
    "start": "1595840",
    "end": "1601120"
  },
  {
    "text": "uh process them uh for for our use cases thank you any",
    "start": "1601120",
    "end": "1607640"
  },
  {
    "text": "questions yep good uh there is a mic in the",
    "start": "1607640",
    "end": "1615240"
  },
  {
    "text": "aisle",
    "start": "1616600",
    "end": "1619600"
  },
  {
    "text": "uh thanks for a very interesting talk uh I just have a question uh I believe that the Shing rule that will play a crucial",
    "start": "1622640",
    "end": "1630279"
  },
  {
    "text": "role in determining how effective your scaling effort is so I just want to ask",
    "start": "1630279",
    "end": "1635600"
  },
  {
    "text": "how was it design configure in order to adapt to the dynamic environment as communities in which the targets added",
    "start": "1635600",
    "end": "1642960"
  },
  {
    "text": "and remove constantly at the very high speed so how you design the Shing rule",
    "start": "1642960",
    "end": "1649120"
  },
  {
    "text": "Shing rule to ensure that the workload will be distributed evenly um among the",
    "start": "1649120",
    "end": "1654520"
  },
  {
    "text": "prome processes yeah thank so uh today uh our Shing is uh based on a giops",
    "start": "1654520",
    "end": "1660279"
  },
  {
    "text": "process so we get uh alerts if the instance of Prometheus or the sh Shard",
    "start": "1660279",
    "end": "1666519"
  },
  {
    "text": "one of the Prometheus that we start with if that reaches uh some on uh resources",
    "start": "1666519",
    "end": "1673360"
  },
  {
    "text": "uh we get alerted and then uh uh we add another Shard through giops process that creates a new Shard of Prometheus and",
    "start": "1673360",
    "end": "1681279"
  },
  {
    "text": "both of them starts processing",
    "start": "1681279",
    "end": "1684519"
  },
  {
    "text": "then so essentially if if your uh workload environment um spins up",
    "start": "1689519",
    "end": "1695399"
  },
  {
    "text": "multiple pods uh very quickly and stuff like that you might need to lower the threshold and scale it accordingly that",
    "start": "1695399",
    "end": "1702720"
  },
  {
    "text": "that's the solution that we have gone with but um do you do you evaluate whether the the workload distributed",
    "start": "1702720",
    "end": "1709960"
  },
  {
    "text": "evenly or is there any way you measure that uh do you mean in Prometheus like",
    "start": "1709960",
    "end": "1715960"
  },
  {
    "text": "uh let's say if there's a skill workload um can we detect that um we haven't uh",
    "start": "1715960",
    "end": "1721480"
  },
  {
    "text": "looked into uh that use case because we haven't come across such an issue where",
    "start": "1721480",
    "end": "1726600"
  },
  {
    "text": "one of the Prometheus instan is getting skewed and uh stuff like that we have not seen that yet but uh it would be",
    "start": "1726600",
    "end": "1732720"
  },
  {
    "text": "good to look into we'll watch out and let you know if there is anything like that right okay thank you",
    "start": "1732720",
    "end": "1739398"
  },
  {
    "text": "uh I saw in one of the other in talks that you've been relying on ISO as well",
    "start": "1744600",
    "end": "1750440"
  },
  {
    "text": "right in your service mesh yes are you uh leveraging any of the generated isometrics for your uh use Point use",
    "start": "1750440",
    "end": "1756679"
  },
  {
    "text": "cases as well and if so can you elaborate more thank you sure uh that's a great question so we useo primarily",
    "start": "1756679",
    "end": "1763399"
  },
  {
    "text": "for all our service M use cases uh and uh uh we we consume a lot of metrics",
    "start": "1763399",
    "end": "1769240"
  },
  {
    "text": "from uho generated ones and we have a different scrape limit because IO",
    "start": "1769240",
    "end": "1774840"
  },
  {
    "text": "generally emits a lot of metrics so uh the scrape limit on EO Parts is uh very",
    "start": "1774840",
    "end": "1780200"
  },
  {
    "text": "high um and U the golden signals that we were talking about it is one of the",
    "start": "1780200",
    "end": "1785840"
  },
  {
    "text": "primary use case for esto because there are so many metrics that esto generates we came up with some set of uh you know",
    "start": "1785840",
    "end": "1793679"
  },
  {
    "text": "five uh golden signals that uh we wanted to metrics that ISO generates and then uh",
    "start": "1793679",
    "end": "1801480"
  },
  {
    "text": "you know get you know consume uh them via the event bus and alert the user uh",
    "start": "1801480",
    "end": "1807279"
  },
  {
    "text": "based on the anomaly and other things uh two more follow-ups are you using it for the roll out your Argo roll out and if",
    "start": "1807279",
    "end": "1814240"
  },
  {
    "text": "so what kind of sto metrics that you use for that analyze yeah Soo Is Us in Argo",
    "start": "1814240",
    "end": "1819679"
  },
  {
    "text": "roll outs we use Virtual service to kind of uh uh leverage U Canary deployments",
    "start": "1819679",
    "end": "1824720"
  },
  {
    "text": "with ar and how do you maintain the balance between cardinality right is high cardinality matrics oh we we we",
    "start": "1824720",
    "end": "1830840"
  },
  {
    "text": "control it by the scrape limit in the prome thank you good no",
    "start": "1830840",
    "end": "1837960"
  },
  {
    "text": "problem I think we are at time but uh we are available uh outside uh if anyone of",
    "start": "1839399",
    "end": "1845159"
  },
  {
    "text": "you have questions please reach out to us thank you thank you",
    "start": "1845159",
    "end": "1853120"
  }
]