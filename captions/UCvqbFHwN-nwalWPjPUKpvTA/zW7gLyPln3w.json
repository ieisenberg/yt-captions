[
  {
    "start": "0",
    "end": "34000"
  },
  {
    "text": "hello everyone this is robert hodges and i'm happy to welcome you to my talk on five great ways to lose data on",
    "start": "80",
    "end": "6240"
  },
  {
    "text": "kubernetes and how to avoid them it's an honor to be presenting at",
    "start": "6240",
    "end": "11360"
  },
  {
    "text": "kubecon europe 2020 where this is a great conference a lot of really wonderful talks so we're",
    "start": "11360",
    "end": "17840"
  },
  {
    "text": "just totally happy that uh to be selected the topics that i'll be talking about are things that my company and",
    "start": "17840",
    "end": "24400"
  },
  {
    "text": "and my colleagues have dealt with from day to day over the last couple years so we hope that you will enjoy the",
    "start": "24400",
    "end": "30080"
  },
  {
    "text": "information we're about to share with you speaking of my company let's do some introductions",
    "start": "30080",
    "end": "36160"
  },
  {
    "start": "34000",
    "end": "34000"
  },
  {
    "text": "so my name again is robert hodges i am the alternative ceo um and but more relevantly for this talk",
    "start": "36160",
    "end": "44399"
  },
  {
    "text": "i have a background long background in data i started with my first database management system in 1983.",
    "start": "44399",
    "end": "50879"
  },
  {
    "text": "i've been working on them uh databases of one kind or another almost continuously since then with some short jogs into things like",
    "start": "50879",
    "end": "57600"
  },
  {
    "text": "virtualization and security i've been using kubernetes since 2018. my company altenity is an enterprise",
    "start": "57600",
    "end": "64799"
  },
  {
    "text": "provider for click house so we provide services and software for",
    "start": "64799",
    "end": "69920"
  },
  {
    "text": "the click house data warehouse it's a very popular very fast open source sql data warehouse",
    "start": "69920",
    "end": "76240"
  },
  {
    "text": "again most relevantly for this talk we are the implementers of the click house kubernetes operator and have made a",
    "start": "76240",
    "end": "81840"
  },
  {
    "text": "major commitment to operating data warehouses in a cloud native fashion on kubernetes and that work is the",
    "start": "81840",
    "end": "89600"
  },
  {
    "text": "source of a lot of the information we're going to provide in this talk today",
    "start": "89600",
    "end": "94640"
  },
  {
    "text": "before i get started i'd like to just define a couple of things so that we're we have the right frame of",
    "start": "97119",
    "end": "103040"
  },
  {
    "text": "reference for thinking about the problems of data loss so the first",
    "start": "103040",
    "end": "108880"
  },
  {
    "text": "thing is just to define what do we mean by data in a nutshell data is any information",
    "start": "108880",
    "end": "116479"
  },
  {
    "start": "109000",
    "end": "109000"
  },
  {
    "text": "and well for the purposes of this talk is any information that is used to either operate or guide a",
    "start": "116479",
    "end": "123119"
  },
  {
    "text": "business in the example i'm showing you right now we see a typical table from a from a",
    "start": "123119",
    "end": "128399"
  },
  {
    "text": "data warehouse containing sales data so data warehouses are used to answer questions that often go to company",
    "start": "128399",
    "end": "134959"
  },
  {
    "text": "strategy and tactics such as which products have the best gross margins over time if you can figure that",
    "start": "134959",
    "end": "140720"
  },
  {
    "text": "out you know which products make the most money and the ones that you want to focus on selling similarly if you can answer the question",
    "start": "140720",
    "end": "147599"
  },
  {
    "text": "which kinds of companies are most likely to buy sku556 that's a product then you can direct your sales people to",
    "start": "147599",
    "end": "154720"
  },
  {
    "text": "the places where they're most likely to make sales and bring home the bacon",
    "start": "154720",
    "end": "160080"
  },
  {
    "text": "similarly we want to define what we mean by the word lose well in the context of data lose is",
    "start": "160560",
    "end": "166319"
  },
  {
    "text": "actually a spectrum it's not a single thing so when we lose data we can talk about losing it temporarily so",
    "start": "166319",
    "end": "172400"
  },
  {
    "text": "it can be just gone for a while but can come back completely all the way to a state where it is",
    "start": "172400",
    "end": "179360"
  },
  {
    "text": "completely gone forever you will never see it again you will feel awful and it's there's",
    "start": "179360",
    "end": "185360"
  },
  {
    "text": "it's just vaporized so i call this the data loss arrow of evil and what's kind of interesting in",
    "start": "185360",
    "end": "191599"
  },
  {
    "text": "in when dealing with data is the value depending on what kind of data you have",
    "start": "191599",
    "end": "197040"
  },
  {
    "text": "the loss may be more or less severe at different points in the spectrum so for example if you're running my",
    "start": "197040",
    "end": "202640"
  },
  {
    "text": "sequel and using it to operate a an e-commerce site if my sequel is even avail unavailable",
    "start": "202640",
    "end": "210959"
  },
  {
    "text": "and the data is not not accessible it means you can't sell anything so you're basically at a standstill for",
    "start": "210959",
    "end": "216400"
  },
  {
    "text": "data warehouses that's not such a big deal they often offer many of them particularly if they're batch oriented or doing uh",
    "start": "216400",
    "end": "222879"
  },
  {
    "text": "answering strategic questions may run sort of on a nine to five basis so having it down for a while is not",
    "start": "222879",
    "end": "228560"
  },
  {
    "text": "such a big deal on the other hand losing some big chunk of data means that you lose vision on what's the state of",
    "start": "228560",
    "end": "234879"
  },
  {
    "text": "the business so that can be a very serious problem in both cases um and and for other",
    "start": "234879",
    "end": "241280"
  },
  {
    "text": "databases as well there is a point where it's all gone and the result may be in in many cases that the business",
    "start": "241280",
    "end": "247519"
  },
  {
    "text": "simply stops functioning so this is a very serious problem so with these two definitions in mind we",
    "start": "247519",
    "end": "253840"
  },
  {
    "text": "can now step in and look at some of the different ways that we can lose data on kubernetes",
    "start": "253840",
    "end": "258959"
  },
  {
    "text": "and how we avoid them so the first problem that i want to get",
    "start": "258959",
    "end": "264560"
  },
  {
    "text": "discuss is what i call the single copy catastrophe this is pretty obvious from the name but",
    "start": "264560",
    "end": "272080"
  },
  {
    "start": "271000",
    "end": "271000"
  },
  {
    "text": "first let's start by praising kubernetes one of the things that is wonderful about kubernetes is how easy it is to",
    "start": "272080",
    "end": "278720"
  },
  {
    "text": "set up complex applications in this particular example i'm going to use helm so helm is a popular tool",
    "start": "278720",
    "end": "285520"
  },
  {
    "text": "to create deployments in kubernetes bring them up you can upgrade them you",
    "start": "285520",
    "end": "291440"
  },
  {
    "text": "can take them away in this particular example i'm going to create a namespace my sql using cubecuddle i'm then going to run a",
    "start": "291440",
    "end": "298080"
  },
  {
    "text": "single line helm install command to install the um",
    "start": "298080",
    "end": "303600"
  },
  {
    "text": "a my sql server which will then pop up and run and a few minutes after even a few",
    "start": "303600",
    "end": "309840"
  },
  {
    "text": "seconds after i run this command i will see a database running in a pod",
    "start": "309840",
    "end": "315360"
  },
  {
    "start": "312000",
    "end": "312000"
  },
  {
    "text": "on kubernetes with an attached persistent volume unfortunately this database is kind of a",
    "start": "315360",
    "end": "321199"
  },
  {
    "text": "delicate flower and particularly when we're thinking about data loss so there's a number of bad",
    "start": "321199",
    "end": "326880"
  },
  {
    "text": "things that could possibly happen you could delete the pod that's usually not a big deal",
    "start": "326880",
    "end": "332240"
  },
  {
    "text": "when you install from helm but it's possible that if you delete the node you might lose the storage because in",
    "start": "332240",
    "end": "338479"
  },
  {
    "text": "fact the storage could you don't know for sure unless you check but the storage could be uh",
    "start": "338479",
    "end": "344479"
  },
  {
    "text": "allocated locally and for sure if you delete the persistent volume for example if you allocate block",
    "start": "344479",
    "end": "350400"
  },
  {
    "text": "storage your date is gone there's there's no replacement and this is kind of the same situation that",
    "start": "350400",
    "end": "357120"
  },
  {
    "text": "you have if you have a laptop and you drop it lose it or something just uh",
    "start": "357120",
    "end": "363039"
  },
  {
    "text": "something the storage dies so it's gone so databases have dealt with this",
    "start": "363039",
    "end": "368960"
  },
  {
    "text": "problem for a really long time and the solution in just about every single case is to have replicas",
    "start": "368960",
    "end": "375440"
  },
  {
    "start": "371000",
    "end": "371000"
  },
  {
    "text": "so you have different kinds of copies of data which mean that if you lose your main copy there's another copy you can",
    "start": "375440",
    "end": "381280"
  },
  {
    "text": "use instead and my sql is kind of instructive because it illustrates very well",
    "start": "381280",
    "end": "386960"
  },
  {
    "text": "the two main styles of replicas that we get with databases so my sql as you know if you've used",
    "start": "386960",
    "end": "394000"
  },
  {
    "text": "have experience directly with mysql yourself has very good built-in replication and",
    "start": "394000",
    "end": "399360"
  },
  {
    "text": "so replication enables you to have live copies you have a primary my sequel server",
    "start": "399360",
    "end": "404960"
  },
  {
    "text": "that would for your e-commerce site that would be where your transactions run it has storage attached to it and then",
    "start": "404960",
    "end": "410880"
  },
  {
    "text": "you have my sql replication enabled and it goes it as soon as a transaction commits in",
    "start": "410880",
    "end": "416479"
  },
  {
    "text": "the primary it it immediately transfers it to the replica it's very fast often a second or less to move",
    "start": "416479",
    "end": "422240"
  },
  {
    "text": "transactions across so what that means with this live copy is if you lose the primary you can just",
    "start": "422240",
    "end": "427599"
  },
  {
    "text": "promote one of the replicas and you're right back in business again the other kind of replica is a static",
    "start": "427599",
    "end": "433520"
  },
  {
    "text": "copy which is represented by a backup so you can take those replicas and this",
    "start": "433520",
    "end": "438720"
  },
  {
    "text": "is again a very common way that this is handled in my sql from time to time you'll run a backup",
    "start": "438720",
    "end": "443919"
  },
  {
    "text": "of the of the my of the mysql replica and then you'll store that back up in another location in case you need to",
    "start": "443919",
    "end": "450240"
  },
  {
    "text": "make more replicas at a later time so static copies in live and live replicas",
    "start": "450240",
    "end": "455919"
  },
  {
    "text": "these solve the problem of what to do when you lose your data in your in your primary database so",
    "start": "455919",
    "end": "462800"
  },
  {
    "text": "the issue though when you come to kubernetes is how do you actually implement this in kubernetes",
    "start": "462800",
    "end": "468319"
  },
  {
    "text": "because we're talking at the very least a complex deployment with a couple of different types of",
    "start": "468319",
    "end": "473759"
  },
  {
    "text": "servers running in separate pods we have backups and in fact",
    "start": "473759",
    "end": "478960"
  },
  {
    "text": "as we get to other types of databases these these may be far more complex modern databases are",
    "start": "478960",
    "end": "484720"
  },
  {
    "text": "typically very complex distributed applications so the question is how do we set it set",
    "start": "484720",
    "end": "491039"
  },
  {
    "text": "this up in in kubernetes especially in cases where we might have to",
    "start": "491039",
    "end": "497360"
  },
  {
    "text": "create dozens of resources to implement the database in kubernetes",
    "start": "497360",
    "end": "503360"
  },
  {
    "start": "504000",
    "end": "504000"
  },
  {
    "text": "the answer that's evolved over the last few years is something called kubernetes operators so the way that a kubernete what",
    "start": "504639",
    "end": "511520"
  },
  {
    "text": "kubernetes operator does is it allows you to create what are called custom resource definitions",
    "start": "511520",
    "end": "516800"
  },
  {
    "text": "so instead of thinking of the database as a collection of deployments stateful sets",
    "start": "516800",
    "end": "521919"
  },
  {
    "text": "services pods you instead have a single a new resource which represents the",
    "start": "521919",
    "end": "527839"
  },
  {
    "text": "entire structure of the database and and so what happens is you load this custom resource definition",
    "start": "527839",
    "end": "534640"
  },
  {
    "text": "into kubernetes it is kubernetes api recognized that it's handled by a particular operator type it dispatches",
    "start": "534640",
    "end": "541279"
  },
  {
    "text": "it to the operator which then looks at it and decides what changes it needs to make in kubernetes",
    "start": "541279",
    "end": "547120"
  },
  {
    "text": "to implement the database so what it then does is from there it will set up things like stateful sets or",
    "start": "547120",
    "end": "553279"
  },
  {
    "text": "services or pods or persistent storage define them in fcd and then the native controllers will take over",
    "start": "553279",
    "end": "559920"
  },
  {
    "text": "actually implementing the um the database out in the kubernetes",
    "start": "559920",
    "end": "565279"
  },
  {
    "text": "cluster itself and the result is a best practice uh deployment which contains all the pieces including",
    "start": "565279",
    "end": "572640"
  },
  {
    "text": "things like replication possibly backups that you need to ensure that you have",
    "start": "572640",
    "end": "577760"
  },
  {
    "text": "adequate replicas to protect your data so and just to illustrate the what we",
    "start": "577760",
    "end": "583600"
  },
  {
    "text": "show on the right here is a data warehouse setup these are relatively complicated compared",
    "start": "583600",
    "end": "588959"
  },
  {
    "text": "especially compared to my sequel let's just have a look at the crd you can see from this that in fact what",
    "start": "588959",
    "end": "596640"
  },
  {
    "start": "591000",
    "end": "591000"
  },
  {
    "text": "this does is it it is much simpler than for example having to do the deployments directly",
    "start": "596640",
    "end": "602720"
  },
  {
    "text": "and do all the low-level resources we have a single very simple resource file this is actually a real resource file that you",
    "start": "602720",
    "end": "608959"
  },
  {
    "text": "could load and it says let's have a cluster named ch01",
    "start": "608959",
    "end": "614800"
  },
  {
    "text": "actually with a configuration called replicated two shards two replicas and then point to zookeeper which is",
    "start": "614800",
    "end": "620160"
  },
  {
    "text": "used to keep uh to keep track of of state between the replicas that's it so",
    "start": "620160",
    "end": "625920"
  },
  {
    "text": "this illustrates how how much the operators simplify this and then also handle the issues of ensuring",
    "start": "625920",
    "end": "633360"
  },
  {
    "text": "proper replicas as well as many other things that are necessary for databases to function so this is a huge step forward and is",
    "start": "633360",
    "end": "640959"
  },
  {
    "text": "really one of the ways that we can implement replication and backups in",
    "start": "640959",
    "end": "646560"
  },
  {
    "text": "databases on kubernetes in a relatively straightforward manner",
    "start": "646560",
    "end": "651839"
  },
  {
    "text": "so that was the first so replicas are one of the first solutions to to losing data let's look at some of the",
    "start": "651839",
    "end": "658000"
  },
  {
    "text": "other ways that we can we can lose data so the next one is what i call blast radius blues",
    "start": "658000",
    "end": "664640"
  },
  {
    "text": "and let's talk about this term it's one of my favorite terms from from the high availability world the",
    "start": "664640",
    "end": "671200"
  },
  {
    "start": "666000",
    "end": "666000"
  },
  {
    "text": "notion is a blast radius is how far away do you have to be before you're not affected by a failure and there are",
    "start": "671200",
    "end": "678560"
  },
  {
    "text": "various you can think of this as a bunch of concentric circles for example if a host fails or something",
    "start": "678560",
    "end": "685120"
  },
  {
    "text": "on the host ceases to work anything running on that host and potentially any data attached to the",
    "start": "685120",
    "end": "690399"
  },
  {
    "text": "host may become available may just disappear so that's a that's a relatively",
    "start": "690399",
    "end": "696480"
  },
  {
    "text": "constrained failure but they can of course extend out in to be to cover much more",
    "start": "696480",
    "end": "701680"
  },
  {
    "text": "ground so for example uh hosts and racks racks are in data centers there it is possible to have failures",
    "start": "701680",
    "end": "707680"
  },
  {
    "text": "which make data completely inaccessible across a data center so for example the failure of",
    "start": "707680",
    "end": "712800"
  },
  {
    "text": "network attached storage or a failure of the network itself which can either cause data loss or",
    "start": "712800",
    "end": "719120"
  },
  {
    "text": "unavailability beyond that you can think of a failure at the level of a kubernetes",
    "start": "719120",
    "end": "724480"
  },
  {
    "text": "cluster so that things within that cluster become unavailable or unusable",
    "start": "724480",
    "end": "730079"
  },
  {
    "text": "and then of course a failure within an entire region so for example you can have failures in you there have",
    "start": "730079",
    "end": "736320"
  },
  {
    "text": "been historically failures in things like amazon that cost entire regions to lose access",
    "start": "736320",
    "end": "741600"
  },
  {
    "text": "to critical services and even data across across a number of data centers",
    "start": "741600",
    "end": "746880"
  },
  {
    "text": "so the really key thing with with to avoid blast radius problems is to get",
    "start": "746880",
    "end": "753680"
  },
  {
    "start": "752000",
    "end": "752000"
  },
  {
    "text": "distance between your replicas and for that there's an incredibly helpful concept in in kubernetes called affinity",
    "start": "753680",
    "end": "762880"
  },
  {
    "text": "which basically gives us the ability to move pods around as well as associated",
    "start": "762880",
    "end": "768480"
  },
  {
    "text": "resources like storage into different locations so that they are far enough away that if if one thing",
    "start": "768480",
    "end": "775360"
  },
  {
    "text": "fails it doesn't take all your data down with it so here's here's a simple example a very",
    "start": "775360",
    "end": "780959"
  },
  {
    "text": "simple example that illustrates affinity as well as it's it's um it's opposite which is",
    "start": "780959",
    "end": "787920"
  },
  {
    "text": "anti-affinity so in this particular picture we have a couple of zookeeper pods",
    "start": "787920",
    "end": "793360"
  },
  {
    "text": "zookeeper is a very popular distributed directory service that maintains consensus across a number",
    "start": "793360",
    "end": "800399"
  },
  {
    "text": "of nodes it's used for things like leader election as well as uh you know holding distributed logs",
    "start": "800399",
    "end": "806240"
  },
  {
    "text": "anything where a series of processes need to agree on what the state of things is it's really important with zookeeper",
    "start": "806240",
    "end": "812639"
  },
  {
    "text": "to move the to have the nodes separated so that a failure will not take all of",
    "start": "812639",
    "end": "817839"
  },
  {
    "text": "them down in this particular case what we can do is we can use anti-affinity",
    "start": "817839",
    "end": "823279"
  },
  {
    "text": "to keep two zookeepers from showing up on the same node in this case host 116",
    "start": "823279",
    "end": "828959"
  },
  {
    "text": "on the left similarly we can use affinity to drive the um that that zookeeper one",
    "start": "828959",
    "end": "837279"
  },
  {
    "text": "instance to a host that has a particular that has particular characteristics that interest",
    "start": "837279",
    "end": "843040"
  },
  {
    "text": "us one common use of affinity is to drive the pods basically to separate the pods",
    "start": "843040",
    "end": "849760"
  },
  {
    "text": "across availability zones what i'd like to do is show you the code that that implements this",
    "start": "849760",
    "end": "855920"
  },
  {
    "text": "because it's a very powerful feature inside kubernetes so let's first look at anti-affinity so",
    "start": "855920",
    "end": "862560"
  },
  {
    "start": "860000",
    "end": "860000"
  },
  {
    "text": "this is the this is pod anti-affinity which drives the basically it's going to drive the",
    "start": "862560",
    "end": "868000"
  },
  {
    "text": "the pods apart and keep them from landing on this being scheduled on the same hosts so what you see here is a part of a pod",
    "start": "868000",
    "end": "875279"
  },
  {
    "text": "definition and under the spec we have affinity rules and we have a rule for pod",
    "start": "875279",
    "end": "880639"
  },
  {
    "text": "anti-affinity and we say that this is a rule that is going to be used during scheduling so when we're",
    "start": "880639",
    "end": "886880"
  },
  {
    "text": "we're starting the pod but then we don't care later on while the pod is actually running and what this syntax says is that",
    "start": "886880",
    "end": "894800"
  },
  {
    "text": "there's a key hostname and we don't want to have two people basically two",
    "start": "894800",
    "end": "901440"
  },
  {
    "text": "pods in this case from a from one of our our data warehouses we",
    "start": "901440",
    "end": "907360"
  },
  {
    "text": "don't want to have two pods from the same data warehouse on the same host as you can see from",
    "start": "907360",
    "end": "912800"
  },
  {
    "text": "looking at this syntax and even hearing me read it it's a little bit non-intuitive and we'll get to that because that",
    "start": "912800",
    "end": "918320"
  },
  {
    "text": "actually turns out to be an issue but if you get this right what this will do is is drive the pods",
    "start": "918320",
    "end": "924000"
  },
  {
    "text": "off the you will never get two pods scheduled on the same host now what we can also do is is is move",
    "start": "924000",
    "end": "931360"
  },
  {
    "start": "928000",
    "end": "928000"
  },
  {
    "text": "pods to make them schedule in particular locations and for this node affinity is very helpful what we",
    "start": "931360",
    "end": "937680"
  },
  {
    "text": "see here is node affinity which is going to assign a pod to a particular",
    "start": "937680",
    "end": "945600"
  },
  {
    "text": "availability zone here us west 2. so the and in this case",
    "start": "945600",
    "end": "951920"
  },
  {
    "text": "what we see is the key is the failure domain blah blah blah zone that is a property that's automatically",
    "start": "951920",
    "end": "958079"
  },
  {
    "text": "set in most kubernetes clusters and we see that the value that we're looking for is us west 2a",
    "start": "958079",
    "end": "964639"
  },
  {
    "text": "so what this is going to do is any pod that contains this is going to want to land in in us west 2a when it gets scheduled",
    "start": "964639",
    "end": "972639"
  },
  {
    "text": "one of the things that's kind of interesting i should note here about about affinity is particularly availability zones is we",
    "start": "972639",
    "end": "979920"
  },
  {
    "text": "have found in our work that it's best if you're really concerned about distribution across azs it's best to be very explicit about",
    "start": "979920",
    "end": "987199"
  },
  {
    "text": "it as we show here there are sort of tricks you can use that for example we'll we'll get node affinity perhaps across a",
    "start": "987199",
    "end": "994720"
  },
  {
    "text": "stateful set but we find them difficult to configure and it's just better to be totally explicit",
    "start": "994720",
    "end": "1000480"
  },
  {
    "text": "about where you want things to land which means that different pods within a database cluster",
    "start": "1000480",
    "end": "1006000"
  },
  {
    "text": "will actually have different different affinity rules the result when you get it right is that",
    "start": "1006000",
    "end": "1012880"
  },
  {
    "start": "1010000",
    "end": "1010000"
  },
  {
    "text": "you get distance between your replicas so in this illustration right here we're",
    "start": "1012880",
    "end": "1018880"
  },
  {
    "text": "showing a couple of shards on the data warehouse that's click house and then we have three zookeeper pods so",
    "start": "1018880",
    "end": "1025038"
  },
  {
    "text": "that's a zookeeper ensemble we have the replicas neatly spread across the",
    "start": "1025039",
    "end": "1030079"
  },
  {
    "text": "availability zones us west 2 a b and c and um and and so that that spreads the",
    "start": "1030079",
    "end": "1038079"
  },
  {
    "text": "data the uh other thing that i want to draw your attention to is the backups",
    "start": "1038079",
    "end": "1043120"
  },
  {
    "text": "so if you take backups a really smart thing to do is stick them in object storage this is common just",
    "start": "1043120",
    "end": "1048558"
  },
  {
    "text": "because object storage is convenient and relatively inexpensive but the other thing about object storage",
    "start": "1048559",
    "end": "1054480"
  },
  {
    "text": "is that it is itself replicated and can be quite distant from the systems",
    "start": "1054480",
    "end": "1060400"
  },
  {
    "text": "that you're backing up so for example you can set up replication automatically across regions and what will happen is that means you",
    "start": "1060400",
    "end": "1066960"
  },
  {
    "text": "have a backup that's even outside the region that you're currently currently working in so the topology",
    "start": "1066960",
    "end": "1074320"
  },
  {
    "text": "that we're showing right here with the distance between replicas is actually quite common in kubernetes",
    "start": "1074320",
    "end": "1080000"
  },
  {
    "text": "and and is very easy to implement with things like cops with aussie ks",
    "start": "1080000",
    "end": "1086799"
  },
  {
    "text": "and other types of managed kubernetes so so definitely something that you should use to protect data",
    "start": "1086799",
    "end": "1094000"
  },
  {
    "start": "1093000",
    "end": "1093000"
  },
  {
    "text": "when we're talking about blast protection across regions or kubernetes in other words sort of",
    "start": "1094000",
    "end": "1099440"
  },
  {
    "text": "wider out in these concentric circles dealing with things like the loss of a kubernetes cluster",
    "start": "1099440",
    "end": "1104640"
  },
  {
    "text": "the loss of a region this is also this is no longer a data problem but more of a question of losing",
    "start": "1104640",
    "end": "1110480"
  },
  {
    "text": "it resources entirely so it's much more of an exercise for the",
    "start": "1110480",
    "end": "1116000"
  },
  {
    "text": "reader that doesn't just include data but includes things like hey how do we handle networking how do we handle dns",
    "start": "1116000",
    "end": "1124160"
  },
  {
    "text": "things like that so that's really beyond the scope of this talk although it's something that people",
    "start": "1124160",
    "end": "1129679"
  },
  {
    "text": "commonly do so what we'll do is we'll leave that here and let you work on that yourself",
    "start": "1129679",
    "end": "1135440"
  },
  {
    "text": "meanwhile what i'm going to do is proceed to another kind of data loss which is directly related to kubernetes",
    "start": "1135440",
    "end": "1142400"
  },
  {
    "text": "itself and that's what i call affinity afflictions so",
    "start": "1142400",
    "end": "1147760"
  },
  {
    "text": "affinity rules are great and uh but one thing you should do is you should definitely check that they're really",
    "start": "1147760",
    "end": "1153440"
  },
  {
    "text": "doing what you think they're doing here's an example so we have a zookeeper ensemble",
    "start": "1153440",
    "end": "1159520"
  },
  {
    "start": "1155000",
    "end": "1155000"
  },
  {
    "text": "which i set up as an experiment and what i've run is a cube cuddle command to show where the pods are running and",
    "start": "1159520",
    "end": "1165760"
  },
  {
    "text": "the first thing that i notice is even though this this uh this zookeeper was supposedly",
    "start": "1165760",
    "end": "1171440"
  },
  {
    "text": "properly set up and and is something i expect to run in production i notice that two of the pods are running on the same host so that's a",
    "start": "1171440",
    "end": "1177679"
  },
  {
    "text": "little bit spooky the other thing that i notice is if i want to go check",
    "start": "1177679",
    "end": "1183679"
  },
  {
    "text": "where those hosts are in in which azs i can see that they're spread out across",
    "start": "1183679",
    "end": "1189679"
  },
  {
    "text": "two az's so that's good what we can do is take these two things together and draw a picture",
    "start": "1189679",
    "end": "1195600"
  },
  {
    "text": "of how the uh how the zookeepers are actually distributed and what we see",
    "start": "1195600",
    "end": "1202000"
  },
  {
    "text": "is we've got two on one host one on another and the fact is that",
    "start": "1202000",
    "end": "1207120"
  },
  {
    "text": "the the distribution here is essentially random it's sort of luck of the draw that we got two on one machine",
    "start": "1207120",
    "end": "1212960"
  },
  {
    "text": "and and one on another the fact that they were just if they had come up as they could have",
    "start": "1212960",
    "end": "1218799"
  },
  {
    "text": "come up all on different machines or we could have even have had all pods on on a single machine",
    "start": "1218799",
    "end": "1224960"
  },
  {
    "text": "and what was actually happening here in this particular case is it just weren't any affinity rules um",
    "start": "1224960",
    "end": "1231600"
  },
  {
    "text": "and this is so so one of the things that you do want to check for is that is that if you think you have affinity",
    "start": "1231600",
    "end": "1236960"
  },
  {
    "text": "rules you want to make sure they really are there and in this particular example uh what i",
    "start": "1236960",
    "end": "1242400"
  },
  {
    "text": "did was run a variation in the cube cuddle get pods command and this shortened version with jq",
    "start": "1242400",
    "end": "1249120"
  },
  {
    "text": "allows me to pull out the affinity rules so that i can look at them quickly and what i can see here is that",
    "start": "1249120",
    "end": "1255440"
  },
  {
    "text": "they are that they're null there's simply no rules at all so obviously affinity doesn't work",
    "start": "1255440",
    "end": "1260559"
  },
  {
    "text": "if you don't use it but the other thing that that you need to do when you're checking these rules is to make sure",
    "start": "1260559",
    "end": "1265679"
  },
  {
    "text": "they're really doing what you expect as i mentioned before the syntax for affinity rules is",
    "start": "1265679",
    "end": "1271440"
  },
  {
    "text": "non-intuitive i think that's a kind of a kind word so it's pretty easy to to set things up",
    "start": "1271440",
    "end": "1277520"
  },
  {
    "text": "that look correct but then actually behave surprisingly when they're deployed so again",
    "start": "1277520",
    "end": "1282720"
  },
  {
    "text": "double checking the affinity rules and as well as the the actual implementation where where",
    "start": "1282720",
    "end": "1288320"
  },
  {
    "text": "things where the pods actually land is super important for using them correctly and protecting your data",
    "start": "1288320",
    "end": "1295679"
  },
  {
    "text": "let's move off affinity rules to another one of my favorites and this is a problem that i call the persistent",
    "start": "1296400",
    "end": "1302000"
  },
  {
    "text": "volume that wasn't and uh this is surprisingly common i've probably hit",
    "start": "1302000",
    "end": "1307919"
  },
  {
    "text": "this at least a dozen times in different forms let me talk about a couple of common ways that this can occur",
    "start": "1307919",
    "end": "1315200"
  },
  {
    "start": "1313000",
    "end": "1313000"
  },
  {
    "text": "before we go anywhere it's important to recognize that that ins in kubernetes",
    "start": "1315200",
    "end": "1321679"
  },
  {
    "text": "ephemeral storage is not a bad thing it is a feature not a bug that's in fact one of the",
    "start": "1321679",
    "end": "1328080"
  },
  {
    "text": "things that makes kubernetes so powerful is you can have different kinds of storage and we're going to take advantage of that",
    "start": "1328080",
    "end": "1334320"
  },
  {
    "text": "in in in just a couple of minutes but this is a picture of two kubernetes",
    "start": "1334320",
    "end": "1340080"
  },
  {
    "text": "nodes and one i call the bad pod i has a bad pod running on it and it has no it has",
    "start": "1340080",
    "end": "1345600"
  },
  {
    "text": "no block storage it's just writing to the ephemeral file system in the pod itself like overlay fs the other one is",
    "start": "1345600",
    "end": "1352559"
  },
  {
    "text": "a good pod and that's actually got nicely allocated block storage now what's interesting is as far as the",
    "start": "1352559",
    "end": "1357760"
  },
  {
    "text": "applications are concerned they can't tell the difference uh what kind of you know what kind of storage you have",
    "start": "1357760",
    "end": "1364640"
  },
  {
    "text": "the pods work in both cases the answer api calls so on and so forth everything looks good",
    "start": "1364640",
    "end": "1369840"
  },
  {
    "text": "what's even more interested is the pods themselves cannot necessarily tell whether they have truly persistent",
    "start": "1369840",
    "end": "1376640"
  },
  {
    "text": "storage because they just see it as a mounted in the file system so in kubernetes",
    "start": "1376640",
    "end": "1382960"
  },
  {
    "text": "to say something is bad and to say something is good well this is in the eye of the beholder if it's a web server of course it's it's",
    "start": "1382960",
    "end": "1389919"
  },
  {
    "text": "uh it's fine we don't really care whether the storage is ephemeral or not if it's a database of course that could",
    "start": "1389919",
    "end": "1395679"
  },
  {
    "text": "be quite a that could be quite a serious problem so one of the things that you want to do",
    "start": "1395679",
    "end": "1401840"
  },
  {
    "text": "is particularly when you're dependent on assuming that persistent volumes have been allocated",
    "start": "1401840",
    "end": "1407360"
  },
  {
    "text": "is to make sure they're really there so here's an example we can we run a cube cuddle get pvz",
    "start": "1407360",
    "end": "1414159"
  },
  {
    "text": "command and i've i've changed the output a little bit to",
    "start": "1414159",
    "end": "1419840"
  },
  {
    "text": "so that i can make sure that make sure that we really see what we",
    "start": "1421200",
    "end": "1427279"
  },
  {
    "text": "expect and when we when we look at the pvc we see that it has a storage class of cops ssd 117",
    "start": "1427279",
    "end": "1434799"
  },
  {
    "text": "that's the the storage class for cops running uh kubernetes uh uh 1.17 that looks pretty good",
    "start": "1434799",
    "end": "1443279"
  },
  {
    "text": "moreover under volume we see there's a real pv out there so persistent volume has been allocated",
    "start": "1443279",
    "end": "1448480"
  },
  {
    "text": "we could actually check that using a get pv command and and look at that another thing",
    "start": "1448480",
    "end": "1454799"
  },
  {
    "text": "that's important is to look at the storage class itself and make sure that it's using a",
    "start": "1454799",
    "end": "1460080"
  },
  {
    "text": "provisioner we expect so the provisioner is the thing that actually goes out and allocates the storage and again",
    "start": "1460080",
    "end": "1466400"
  },
  {
    "text": "our our class here is um is cops ssd117 its provisioner is the oz",
    "start": "1466400",
    "end": "1473520"
  },
  {
    "text": "ebs storage provisioner which goes out and allocates block storage so so far so good this actually sounds",
    "start": "1473520",
    "end": "1480320"
  },
  {
    "text": "really good this is allocating block storage it should stick around when pods get restarted everything looks copacetic however",
    "start": "1480320",
    "end": "1488640"
  },
  {
    "text": "that's not necessarily that's not to say that we're fully protected so because persistent volumes are only",
    "start": "1488640",
    "end": "1495360"
  },
  {
    "start": "1492000",
    "end": "1492000"
  },
  {
    "text": "good if you actually use them here's an example of a bug we hit with zookeeper it fortunately did not lead to a down",
    "start": "1495360",
    "end": "1502240"
  },
  {
    "text": "system because we detected it in time and were able to to correct it but what happened was due",
    "start": "1502240",
    "end": "1507919"
  },
  {
    "text": "to a misconfiguration problem in our zookeepers we had the ebs storage mounted as var",
    "start": "1507919",
    "end": "1514799"
  },
  {
    "text": "lib zk data but we accidentally set the configuration file so that zookeeper",
    "start": "1514799",
    "end": "1520640"
  },
  {
    "text": "wrote its snapshots to data and it's logged to data log in ephemeral storage and the problem is",
    "start": "1520640",
    "end": "1527120"
  },
  {
    "text": "you of course can't tell that this is happening until you actually restart the zookeeper node",
    "start": "1527120",
    "end": "1532720"
  },
  {
    "text": "now with zookeeper there's another interesting variation that it's not even enough to restart",
    "start": "1532720",
    "end": "1538320"
  },
  {
    "text": "zookeeper to see this because zookeeper replicates automatically so in this",
    "start": "1538320",
    "end": "1544159"
  },
  {
    "text": "particular case we had an ensemble of three zookeepers when individual pods restarted they",
    "start": "1544159",
    "end": "1549760"
  },
  {
    "text": "would then connect back to the other two pods get the data back and so it would be transparent that we had lost",
    "start": "1549760",
    "end": "1556240"
  },
  {
    "text": "data the the zookeeper replication behavior was hiding this it was only at a certain point that we um that we",
    "start": "1556240",
    "end": "1563919"
  },
  {
    "text": "actually accidentally noticed that the storage was not there and realized we had a real problem",
    "start": "1563919",
    "end": "1569520"
  },
  {
    "text": "so so you really want to be very careful about this and this is an",
    "start": "1569520",
    "end": "1574559"
  },
  {
    "start": "1574000",
    "end": "1574000"
  },
  {
    "text": "example of an area where the answer that you know to solving this kind of problem is testing",
    "start": "1574559",
    "end": "1581039"
  },
  {
    "text": "and a lot of testing so for example just be super paranoid check that kubernetes",
    "start": "1581039",
    "end": "1586559"
  },
  {
    "text": "resource definitions are out there as i as i showed before look at file system mounts uh make sure that the mounted",
    "start": "1586559",
    "end": "1593120"
  },
  {
    "text": "file system matches where you exp is is that the",
    "start": "1593120",
    "end": "1598320"
  },
  {
    "text": "you know that the pvs are mounted exactly where you expect them to and you're writing to them kill pods that's the best way of if if",
    "start": "1598320",
    "end": "1604960"
  },
  {
    "text": "they're just single databases they if they lose their data then uh then you'll detect that you can kill nodes",
    "start": "1604960",
    "end": "1611039"
  },
  {
    "text": "you can kill and restart all pods in a replicated database deleting volumes is a good thing so",
    "start": "1611039",
    "end": "1617919"
  },
  {
    "text": "take the volume away if the database is truly dependent on it and it's pointing to it and writing to it the database should stop working and",
    "start": "1617919",
    "end": "1624480"
  },
  {
    "text": "finally test with huge amounts of data many database problems are revealed only",
    "start": "1624480",
    "end": "1629919"
  },
  {
    "text": "when you have a large amount of data in this particular case if you add a lot of data it will fill the local file",
    "start": "1629919",
    "end": "1635440"
  },
  {
    "text": "system and then you'll quickly see that you've got a problem because you're not even writing to the to the to the block storage",
    "start": "1635440",
    "end": "1643600"
  },
  {
    "text": "so with that we're we've shown four ways you can lose data",
    "start": "1643919",
    "end": "1649679"
  },
  {
    "text": "here's the last one which i call fat fingers of fate and this is probably the saddest one",
    "start": "1649679",
    "end": "1655200"
  },
  {
    "text": "because really if you think about it the best way to lose data is to do it yourself and",
    "start": "1655200",
    "end": "1662880"
  },
  {
    "start": "1656000",
    "end": "1656000"
  },
  {
    "text": "the virtue of kubernetes of course that we can create things has",
    "start": "1662880",
    "end": "1667919"
  },
  {
    "text": "this mirror image that we can or sort of as equal and opposite effect that we can",
    "start": "1667919",
    "end": "1674720"
  },
  {
    "text": "take them away just as quickly as we can create them so this single line command will wipe out that mysql database that i",
    "start": "1674720",
    "end": "1681120"
  },
  {
    "text": "created as an example at the beginning so that's it data's gone so",
    "start": "1681120",
    "end": "1687039"
  },
  {
    "text": "one of the things that given how easy it is to destroy things accidentally one of the things that's worth thinking about",
    "start": "1687039",
    "end": "1693279"
  },
  {
    "text": "is how to prevent is how to protect your persistent volumes so they don't just evaporate and it",
    "start": "1693279",
    "end": "1699679"
  },
  {
    "text": "turns out that we can do this through what are called um",
    "start": "1699679",
    "end": "1704720"
  },
  {
    "text": "reclaim policies so we we list the pvs here and i show this",
    "start": "1704720",
    "end": "1709840"
  },
  {
    "text": "example you can see the policy is delete that means that when the the persistent volume claim that caused",
    "start": "1709840",
    "end": "1717279"
  },
  {
    "text": "the storage to be created goes away the storage just vaporizes so that's bad",
    "start": "1717279",
    "end": "1723039"
  },
  {
    "text": "what we can do if we have existing pvs is we can fix this by changing the reclaim policy to",
    "start": "1723039",
    "end": "1729120"
  },
  {
    "text": "be retained and this patch command shows how to do that another possibility if we want to do",
    "start": "1729120",
    "end": "1734799"
  },
  {
    "text": "this in a general way is to create a new storage class so the kubernetes is really flexible and",
    "start": "1734799",
    "end": "1742799"
  },
  {
    "text": "this is a great example of where that flexibility really works for you i've created in this definition a",
    "start": "1742799",
    "end": "1748159"
  },
  {
    "text": "storage class which is exactly the same as the normal cop storage class except that it by",
    "start": "1748159",
    "end": "1753520"
  },
  {
    "text": "default uses retain as the as the reclaim policy so any any persistent volumes",
    "start": "1753520",
    "end": "1759520"
  },
  {
    "text": "allocated with this will have retain as their policy at that point if i destroy my sql by",
    "start": "1759520",
    "end": "1765520"
  },
  {
    "text": "accident i can reclaim the storage just by making a persistent volume claim",
    "start": "1765520",
    "end": "1770720"
  },
  {
    "text": "that matches the uh the volume that we're trying to attach to in every respect",
    "start": "1770720",
    "end": "1776320"
  },
  {
    "text": "um and this is something where you have to be quite careful about it so for example you must have the volume name",
    "start": "1776320",
    "end": "1782960"
  },
  {
    "text": "stuck in your persistent volume otherwise it won't match and it'll just stay in a pending state but if you do this right",
    "start": "1782960",
    "end": "1788559"
  },
  {
    "text": "you can recover storage quite nicely and in fact here's the process that you go through to reclaim",
    "start": "1788559",
    "end": "1794240"
  },
  {
    "start": "1791000",
    "end": "1791000"
  },
  {
    "text": "is you've blown it away by accident just go in remove the claim ref from the pv that says hey it's it's now",
    "start": "1794240",
    "end": "1800240"
  },
  {
    "text": "free to be reallocated go ahead and run that definition again to create a new persistent volume",
    "start": "1800240",
    "end": "1805760"
  },
  {
    "text": "and then our persistent volume claim and then with helm it actually has a feature which allows it to reattach",
    "start": "1805760",
    "end": "1811840"
  },
  {
    "text": "to existing storage and you get your database back and that solves the problem",
    "start": "1811840",
    "end": "1818320"
  },
  {
    "text": "so with this we've covered five ways to lose data talked about",
    "start": "1818320",
    "end": "1824000"
  },
  {
    "text": "solutions i just like to summarize by thinking not so much about the ways to lose data but about the things you",
    "start": "1824000",
    "end": "1830159"
  },
  {
    "text": "want to do to solve those problems so the core of of solving problems with data loss in",
    "start": "1830159",
    "end": "1838159"
  },
  {
    "text": "kubernetes is just to be exceedingly paranoid this is something that's true of data in general and then beyond that",
    "start": "1838159",
    "end": "1845279"
  },
  {
    "text": "there are five specific things that you want to do first of all have replicas of data",
    "start": "1845279",
    "end": "1851120"
  },
  {
    "text": "always extra copies are always good separate them by distance use affinity rules so that you can lean",
    "start": "1851120",
    "end": "1858880"
  },
  {
    "text": "on the kubernetes capabilities that allow you to spread pods across uh you know sort of a",
    "start": "1858880",
    "end": "1865120"
  },
  {
    "text": "process across availability zones this is a great feature and keep them from clustering on single nodes use reclaim policies to protect",
    "start": "1865120",
    "end": "1872000"
  },
  {
    "text": "your storage and then finally just test the daylights out of everything if you're in data this is something you",
    "start": "1872000",
    "end": "1878080"
  },
  {
    "text": "just becomes part of you that anytime anytime you have data just do all kinds of tests on it because this",
    "start": "1878080",
    "end": "1884880"
  },
  {
    "text": "is the best way to define problems before they actually occur and then finally if you have operators available check",
    "start": "1884880",
    "end": "1892240"
  },
  {
    "text": "them out and use them they're a wonderful feature of kubernetes and and very uh very useful",
    "start": "1892240",
    "end": "1899279"
  },
  {
    "text": "so that's it um i hope you've enjoyed the talk it's been really fun putting this together",
    "start": "1899279",
    "end": "1905039"
  },
  {
    "text": "and and uh and presenting it i just want to say as a final uh note we are hiring like",
    "start": "1905039",
    "end": "1911200"
  },
  {
    "text": "any self-respecting startup if you like this talk or think you can do it better please give us a call we'd love to talk",
    "start": "1911200",
    "end": "1916640"
  },
  {
    "text": "to you our own operator is is shown here in the github reference",
    "start": "1916640",
    "end": "1923200"
  },
  {
    "text": "so check it out there's many other great operators percona has a great one that's good operators for zookeeper",
    "start": "1923200",
    "end": "1929760"
  },
  {
    "text": "it's a it's an increasingly powerful paradigm and definitely something worth learning about thanks and i look forward to",
    "start": "1929760",
    "end": "1936240"
  },
  {
    "text": "answering questions",
    "start": "1936240",
    "end": "1939679"
  }
]