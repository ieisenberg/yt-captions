[
  {
    "text": "good morning we're here today to talk",
    "start": "160",
    "end": "2320"
  },
  {
    "text": "about a new project in the Kubernetes",
    "start": "2320",
    "end": "4040"
  },
  {
    "text": "ecosystem sponsored by the serving",
    "start": "4040",
    "end": "6400"
  },
  {
    "text": "working group called the gateway API",
    "start": "6400",
    "end": "8639"
  },
  {
    "text": "inference",
    "start": "8639",
    "end": "10280"
  },
  {
    "text": "extension it takes any Kubernetes",
    "start": "10280",
    "end": "12880"
  },
  {
    "text": "gateway and turns it into an inference",
    "start": "12880",
    "end": "15120"
  },
  {
    "text": "gateway and an inference gateway helps",
    "start": "15120",
    "end": "18480"
  },
  {
    "text": "large platform teams small platform",
    "start": "18480",
    "end": "20480"
  },
  {
    "text": "teams self-host large language models",
    "start": "20480",
    "end": "23199"
  },
  {
    "text": "efficiently in production it's informed",
    "start": "23199",
    "end": "25439"
  },
  {
    "text": "by our experiences at Google and Bite",
    "start": "25439",
    "end": "27840"
  },
  {
    "text": "Dance and we're very excited to talk to",
    "start": "27840",
    "end": "29840"
  },
  {
    "text": "you today about",
    "start": "29840",
    "end": "31079"
  },
  {
    "text": "it 10 years ago someone told me that",
    "start": "31079",
    "end": "34480"
  },
  {
    "text": "Kubernetes wasn't going to be relevant",
    "start": "34480",
    "end": "36000"
  },
  {
    "text": "to the majority of users we'd all be",
    "start": "36000",
    "end": "38160"
  },
  {
    "text": "using functions as a service or maybe 12",
    "start": "38160",
    "end": "41120"
  },
  {
    "text": "factor managed platform as a service uh",
    "start": "41120",
    "end": "43840"
  },
  {
    "text": "in the",
    "start": "43840",
    "end": "44680"
  },
  {
    "text": "cloud now obviously as judging by all of",
    "start": "44680",
    "end": "47680"
  },
  {
    "text": "us here they were wrong but there was a",
    "start": "47680",
    "end": "50559"
  },
  {
    "text": "seed of truth in that it wasn't yet",
    "start": "50559",
    "end": "52800"
  },
  {
    "text": "clear that the majority of large",
    "start": "52800",
    "end": "54879"
  },
  {
    "text": "platform teams would have a diverse",
    "start": "54879",
    "end": "56719"
  },
  {
    "text": "range of workloads and that they would",
    "start": "56719",
    "end": "58879"
  },
  {
    "text": "need and demand the flexibility from of",
    "start": "58879",
    "end": "62600"
  },
  {
    "text": "Kubernetes as well as depend on a rich",
    "start": "62600",
    "end": "66080"
  },
  {
    "text": "ecosystem of composable automation",
    "start": "66080",
    "end": "68720"
  },
  {
    "text": "there's a similar timeline going on in",
    "start": "68720",
    "end": "70720"
  },
  {
    "text": "AI today two years ago it seemed that",
    "start": "70720",
    "end": "73840"
  },
  {
    "text": "all models would be proprietary in the",
    "start": "73840",
    "end": "76479"
  },
  {
    "text": "last year open models have dramatically",
    "start": "76479",
    "end": "79360"
  },
  {
    "text": "closed the",
    "start": "79360",
    "end": "80439"
  },
  {
    "text": "gap because larger models are more",
    "start": "80439",
    "end": "83000"
  },
  {
    "text": "flexible and smaller models are more",
    "start": "83000",
    "end": "85439"
  },
  {
    "text": "efficient to serve we believe there is a",
    "start": "85439",
    "end": "88159"
  },
  {
    "text": "meaningful and durable trade-off between",
    "start": "88159",
    "end": "90880"
  },
  {
    "text": "very smart frontier models and smaller",
    "start": "90880",
    "end": "94000"
  },
  {
    "text": "predictable building block open models",
    "start": "94000",
    "end": "97119"
  },
  {
    "text": "so we expect everyone hopefully will",
    "start": "97119",
    "end": "100479"
  },
  {
    "text": "eventually need to serve open models for",
    "start": "100479",
    "end": "102159"
  },
  {
    "text": "efficiency at scale while still continue",
    "start": "102159",
    "end": "105600"
  },
  {
    "text": "to depend on cutting edge models for",
    "start": "105600",
    "end": "107280"
  },
  {
    "text": "time to market",
    "start": "107280",
    "end": "109280"
  },
  {
    "text": "what flexibility and composable",
    "start": "109280",
    "end": "111000"
  },
  {
    "text": "automation will we all need when models",
    "start": "111000",
    "end": "113759"
  },
  {
    "text": "are a fundamental part of our",
    "start": "113759",
    "end": "115240"
  },
  {
    "text": "applications that's exactly the question",
    "start": "115240",
    "end": "117680"
  },
  {
    "text": "that we started the serving working",
    "start": "117680",
    "end": "119200"
  },
  {
    "text": "group in Kubernetes to answer a year ago",
    "start": "119200",
    "end": "121360"
  },
  {
    "text": "at",
    "start": "121360",
    "end": "122200"
  },
  {
    "text": "CubeConu and just like in the beginning",
    "start": "122200",
    "end": "124159"
  },
  {
    "text": "of Kubernetes we depend on experienced",
    "start": "124159",
    "end": "127439"
  },
  {
    "text": "platform",
    "start": "127439",
    "end": "128920"
  },
  {
    "text": "teams looking to build the next",
    "start": "128920",
    "end": "131039"
  },
  {
    "text": "generation of their platforms to guide",
    "start": "131039",
    "end": "133440"
  },
  {
    "text": "us we were very fortunate that bite",
    "start": "133440",
    "end": "136160"
  },
  {
    "text": "dance was ready to build the next",
    "start": "136160",
    "end": "137599"
  },
  {
    "text": "version of their platform they chose to",
    "start": "137599",
    "end": "139760"
  },
  {
    "text": "do it in the open with",
    "start": "139760",
    "end": "142959"
  },
  {
    "text": "us running LLM in Kubernetes sounds",
    "start": "143720",
    "end": "146800"
  },
  {
    "text": "simple in theory but in practice is",
    "start": "146800",
    "end": "149120"
  },
  {
    "text": "still challenging let's dive into",
    "start": "149120",
    "end": "151040"
  },
  {
    "text": "several production issues that shows LLM",
    "start": "151040",
    "end": "154000"
  },
  {
    "text": "inference challenges are truly unique",
    "start": "154000",
    "end": "156879"
  },
  {
    "text": "the first plot is from a byance",
    "start": "156879",
    "end": "159120"
  },
  {
    "text": "production LM service capturing the",
    "start": "159120",
    "end": "161440"
  },
  {
    "text": "daily request distribution so as we can",
    "start": "161440",
    "end": "164239"
  },
  {
    "text": "see the request valuation is spiking the",
    "start": "164239",
    "end": "167280"
  },
  {
    "text": "input prompt size vary widely and the",
    "start": "167280",
    "end": "169760"
  },
  {
    "text": "output length are unpredictable this",
    "start": "169760",
    "end": "172640"
  },
  {
    "text": "actually highlights one of the biggest",
    "start": "172640",
    "end": "174720"
  },
  {
    "text": "differences between traditional",
    "start": "174720",
    "end": "176599"
  },
  {
    "text": "microservices each request is different",
    "start": "176599",
    "end": "180000"
  },
  {
    "text": "here's another demonstration we can see",
    "start": "180000",
    "end": "182480"
  },
  {
    "text": "sudden strikes in GPU compute activity",
    "start": "182480",
    "end": "185760"
  },
  {
    "text": "even under constant QPS so at that at",
    "start": "185760",
    "end": "189040"
  },
  {
    "text": "that time we figure it out a batch of",
    "start": "189040",
    "end": "191599"
  },
  {
    "text": "requests with super long uh prompts",
    "start": "191599",
    "end": "194959"
  },
  {
    "text": "sneak in so for LLMs it's not just the",
    "start": "194959",
    "end": "198400"
  },
  {
    "text": "read of the requests that matters but",
    "start": "198400",
    "end": "200720"
  },
  {
    "text": "also their shape the length of the",
    "start": "200720",
    "end": "202720"
  },
  {
    "text": "prompts the number of the generated",
    "start": "202720",
    "end": "204640"
  },
  {
    "text": "tokens and the prompt structure these",
    "start": "204640",
    "end": "207200"
  },
  {
    "text": "all significantly affect the GPU load",
    "start": "207200",
    "end": "209680"
  },
  {
    "text": "and make it",
    "start": "209680",
    "end": "211400"
  },
  {
    "text": "unpredictable let's see another",
    "start": "211400",
    "end": "213280"
  },
  {
    "text": "challenge in binance we serve many",
    "start": "213280",
    "end": "215760"
  },
  {
    "text": "models in production well their traffic",
    "start": "215760",
    "end": "218480"
  },
  {
    "text": "patterns vary widely some models are",
    "start": "218480",
    "end": "221280"
  },
  {
    "text": "critical production models while others",
    "start": "221280",
    "end": "223519"
  },
  {
    "text": "are experimental with near zero",
    "start": "223519",
    "end": "226360"
  },
  {
    "text": "traffic given how resource intensive",
    "start": "226360",
    "end": "228959"
  },
  {
    "text": "these models are this kind of traffic",
    "start": "228959",
    "end": "231519"
  },
  {
    "text": "screw makes the problem even more",
    "start": "231519",
    "end": "233840"
  },
  {
    "text": "problematic it also makes the life cycle",
    "start": "233840",
    "end": "236640"
  },
  {
    "text": "management and resource allocation",
    "start": "236640",
    "end": "239040"
  },
  {
    "text": "extremely challenging and highlight the",
    "start": "239040",
    "end": "241519"
  },
  {
    "text": "need for resource usage aware",
    "start": "241519",
    "end": "243840"
  },
  {
    "text": "orchestration rather than just relying",
    "start": "243840",
    "end": "246400"
  },
  {
    "text": "on static deployments another uh",
    "start": "246400",
    "end": "249360"
  },
  {
    "text": "challenge we face is hardware",
    "start": "249360",
    "end": "251239"
  },
  {
    "text": "heterogeneity due to the machine",
    "start": "251239",
    "end": "253200"
  },
  {
    "text": "delivery timelines coder policies and",
    "start": "253200",
    "end": "255920"
  },
  {
    "text": "availability requirements our inference",
    "start": "255920",
    "end": "258320"
  },
  {
    "text": "clusters commonly end up with a mix of",
    "start": "258320",
    "end": "261040"
  },
  {
    "text": "different GPU types as show in the table",
    "start": "261040",
    "end": "264080"
  },
  {
    "text": "in 15,000 GPU clusters we have over",
    "start": "264080",
    "end": "267280"
  },
  {
    "text": "eight GPU types even the heterogeneous",
    "start": "267280",
    "end": "270320"
  },
  {
    "text": "pool help us serve a range of different",
    "start": "270320",
    "end": "272600"
  },
  {
    "text": "workloads we noticed it brings the",
    "start": "272600",
    "end": "275639"
  },
  {
    "text": "complexity sometimes our model has to",
    "start": "275639",
    "end": "278560"
  },
  {
    "text": "run on different GPU types due to the",
    "start": "278560",
    "end": "280720"
  },
  {
    "text": "capacity issues well due to GPU's",
    "start": "280720",
    "end": "283680"
  },
  {
    "text": "different tops and memory bandwidth it's",
    "start": "283680",
    "end": "286400"
  },
  {
    "text": "very hard to abstract the way those",
    "start": "286400",
    "end": "288479"
  },
  {
    "text": "hardware differences which makes the",
    "start": "288479",
    "end": "290639"
  },
  {
    "text": "management like routing even more",
    "start": "290639",
    "end": "292919"
  },
  {
    "text": "difficult all of these challenges like",
    "start": "292919",
    "end": "295440"
  },
  {
    "text": "different request shape model traffic",
    "start": "295440",
    "end": "297759"
  },
  {
    "text": "screw or hardware hydrogenerity directly",
    "start": "297759",
    "end": "300960"
  },
  {
    "text": "impact one shared infrastructure layer",
    "start": "300960",
    "end": "303520"
  },
  {
    "text": "which is routing and that's why it",
    "start": "303520",
    "end": "306080"
  },
  {
    "text": "becomes a bottleneck we have to",
    "start": "306080",
    "end": "308600"
  },
  {
    "text": "rethink to handle the unique challenges",
    "start": "308600",
    "end": "311199"
  },
  {
    "text": "of LM inference we need a new class of",
    "start": "311199",
    "end": "313840"
  },
  {
    "text": "routing solutions that go beyond",
    "start": "313840",
    "end": "315759"
  },
  {
    "text": "traditional load balancers bance and",
    "start": "315759",
    "end": "318320"
  },
  {
    "text": "Google have been working together over",
    "start": "318320",
    "end": "320320"
  },
  {
    "text": "the last year to pull our experiences in",
    "start": "320320",
    "end": "322800"
  },
  {
    "text": "serving to make Kubernetes better for",
    "start": "322800",
    "end": "325120"
  },
  {
    "text": "LLM inference what we found is that LLM",
    "start": "325120",
    "end": "329120"
  },
  {
    "text": "serving success really depends on three",
    "start": "329120",
    "end": "331360"
  },
  {
    "text": "things denser faster and automated it",
    "start": "331360",
    "end": "335039"
  },
  {
    "text": "all comes down to the self-hosted LLM",
    "start": "335039",
    "end": "338039"
  },
  {
    "text": "serving that teams need more control",
    "start": "338039",
    "end": "341039"
  },
  {
    "text": "flexibility and speed than ever so let's",
    "start": "341039",
    "end": "344080"
  },
  {
    "text": "get started by digging into the first",
    "start": "344080",
    "end": "345759"
  },
  {
    "text": "part",
    "start": "345759",
    "end": "346919"
  },
  {
    "text": "denser laura which stands for low rank",
    "start": "346919",
    "end": "349680"
  },
  {
    "text": "adaptation is a technique to fine-tune",
    "start": "349680",
    "end": "352000"
  },
  {
    "text": "the large pre-trained model efficiently",
    "start": "352000",
    "end": "354400"
  },
  {
    "text": "the core idea about the Laura is to",
    "start": "354400",
    "end": "356880"
  },
  {
    "text": "adapt the large pre-trained model to",
    "start": "356880",
    "end": "359440"
  },
  {
    "text": "specific tasks without needing to change",
    "start": "359440",
    "end": "361759"
  },
  {
    "text": "the entire model weights but just a",
    "start": "361759",
    "end": "363919"
  },
  {
    "text": "small set of the parameters called",
    "start": "363919",
    "end": "365600"
  },
  {
    "text": "adapters so these adapters commonly uh",
    "start": "365600",
    "end": "368800"
  },
  {
    "text": "only add 1% of the storage and memory",
    "start": "368800",
    "end": "371440"
  },
  {
    "text": "overhead compared to the original model",
    "start": "371440",
    "end": "373919"
  },
  {
    "text": "waste while at the same time it can",
    "start": "373919",
    "end": "376479"
  },
  {
    "text": "maintain the uh uh efficiencies and",
    "start": "376479",
    "end": "379759"
  },
  {
    "text": "accuracies without any",
    "start": "379759",
    "end": "381880"
  },
  {
    "text": "loss uh during the training phase Laura",
    "start": "381880",
    "end": "385680"
  },
  {
    "text": "frees the uh original model ways and",
    "start": "385680",
    "end": "388400"
  },
  {
    "text": "only fine-tune the two small matrix A",
    "start": "388400",
    "end": "390560"
  },
  {
    "text": "and B and uh in the inference phase it",
    "start": "390560",
    "end": "393680"
  },
  {
    "text": "get the output from the traditional uh",
    "start": "393680",
    "end": "395919"
  },
  {
    "text": "model and uh merge with the",
    "start": "395919",
    "end": "398440"
  },
  {
    "text": "multiplications of A and B to get final",
    "start": "398440",
    "end": "401600"
  },
  {
    "text": "uh inference results so that's the basic",
    "start": "401600",
    "end": "403759"
  },
  {
    "text": "of",
    "start": "403759",
    "end": "404680"
  },
  {
    "text": "Laura even Laura gives the resource uh",
    "start": "404680",
    "end": "407840"
  },
  {
    "text": "uh efficiencies and model flexibilities",
    "start": "407840",
    "end": "410639"
  },
  {
    "text": "managing it in Kubernetes is",
    "start": "410639",
    "end": "412639"
  },
  {
    "text": "surprisingly challenging why because",
    "start": "412639",
    "end": "415520"
  },
  {
    "text": "Laura has to be loaded alongside the",
    "start": "415520",
    "end": "417840"
  },
  {
    "text": "base model so that means Laura cannot be",
    "start": "417840",
    "end": "420560"
  },
  {
    "text": "deployed in separate containers those",
    "start": "420560",
    "end": "423039"
  },
  {
    "text": "breaks the Kubernetes principles right",
    "start": "423039",
    "end": "426240"
  },
  {
    "text": "now every single container may serve",
    "start": "426240",
    "end": "428720"
  },
  {
    "text": "multiple models that also bring the",
    "start": "428720",
    "end": "430880"
  },
  {
    "text": "problems to the uh request routing and",
    "start": "430880",
    "end": "434560"
  },
  {
    "text": "the original base models uh service",
    "start": "434560",
    "end": "436960"
  },
  {
    "text": "cannot be used to find the Laura anymore",
    "start": "436960",
    "end": "439520"
  },
  {
    "text": "and load balancer becomes even",
    "start": "439520",
    "end": "441720"
  },
  {
    "text": "challenging especially multiple loras",
    "start": "441720",
    "end": "444560"
  },
  {
    "text": "content for the shared GPU resources",
    "start": "444560",
    "end": "447199"
  },
  {
    "text": "within the same",
    "start": "447199",
    "end": "448599"
  },
  {
    "text": "pot luckily we solve these problems in",
    "start": "448599",
    "end": "451280"
  },
  {
    "text": "production let's use a concrete example",
    "start": "451280",
    "end": "453680"
  },
  {
    "text": "from Bidance to illustrate the benefits",
    "start": "453680",
    "end": "456160"
  },
  {
    "text": "of denser deployment in Bance we have",
    "start": "456160",
    "end": "459360"
  },
  {
    "text": "many database product like to integrate",
    "start": "459360",
    "end": "461759"
  },
  {
    "text": "AI capabilities to enhance user",
    "start": "461759",
    "end": "464160"
  },
  {
    "text": "productivity and lower the learning",
    "start": "464160",
    "end": "466479"
  },
  {
    "text": "curve text to SQL is one of the most",
    "start": "466479",
    "end": "469360"
  },
  {
    "text": "popular AI capabilities that translate",
    "start": "469360",
    "end": "471919"
  },
  {
    "text": "the uh natural language to SQL queries",
    "start": "471919",
    "end": "476160"
  },
  {
    "text": "in by dance we fine-tune the D6 33B",
    "start": "476160",
    "end": "478960"
  },
  {
    "text": "model to support our texttosql use case",
    "start": "478960",
    "end": "482000"
  },
  {
    "text": "however we have many business lines they",
    "start": "482000",
    "end": "484639"
  },
  {
    "text": "provide the SQL like query scenarios",
    "start": "484639",
    "end": "486960"
  },
  {
    "text": "such as log search or elastic search",
    "start": "486960",
    "end": "489840"
  },
  {
    "text": "while the query structure is very",
    "start": "489840",
    "end": "491840"
  },
  {
    "text": "similar to SQL but the syntax and the",
    "start": "491840",
    "end": "494720"
  },
  {
    "text": "semantics differ significantly so each",
    "start": "494720",
    "end": "497759"
  },
  {
    "text": "scenario needs their own uh fine-tune",
    "start": "497759",
    "end": "500160"
  },
  {
    "text": "model however if we support all these",
    "start": "500160",
    "end": "503120"
  },
  {
    "text": "scenarios with dedicated GPUs that will",
    "start": "503120",
    "end": "505599"
  },
  {
    "text": "be costly to address this resource issue",
    "start": "505599",
    "end": "508960"
  },
  {
    "text": "we adopt the Laura adapture solution we",
    "start": "508960",
    "end": "511840"
  },
  {
    "text": "fine-tune all these models in Laura way",
    "start": "511840",
    "end": "514159"
  },
  {
    "text": "and packing all the SQL like adapters",
    "start": "514159",
    "end": "518240"
  },
  {
    "text": "into one share deep models by following",
    "start": "518240",
    "end": "521680"
  },
  {
    "text": "this adapter sharing and routing",
    "start": "521680",
    "end": "523800"
  },
  {
    "text": "practice we can deploy the new adapters",
    "start": "523800",
    "end": "526560"
  },
  {
    "text": "in seconds achieve 1.5 to",
    "start": "526560",
    "end": "529880"
  },
  {
    "text": "4.7x GPU cost saving under different",
    "start": "529880",
    "end": "532880"
  },
  {
    "text": "traffic conditions so in this setup the",
    "start": "532880",
    "end": "535839"
  },
  {
    "text": "gateway plays a key role in minimizing",
    "start": "535839",
    "end": "538160"
  },
  {
    "text": "the number of the model servers and",
    "start": "538160",
    "end": "540080"
  },
  {
    "text": "intelligently select the least busy",
    "start": "540080",
    "end": "544320"
  },
  {
    "text": "instance bite Dance's experience",
    "start": "545000",
    "end": "547600"
  },
  {
    "text": "mirrored broad feedback from GKE's",
    "start": "547600",
    "end": "549839"
  },
  {
    "text": "generative AI customers large models are",
    "start": "549839",
    "end": "552160"
  },
  {
    "text": "slow and latency is important models",
    "start": "552160",
    "end": "555360"
  },
  {
    "text": "generate output tokens converted to text",
    "start": "555360",
    "end": "557680"
  },
  {
    "text": "at word or subword boundaries a bit",
    "start": "557680",
    "end": "560080"
  },
  {
    "text": "below human reading speed that's great",
    "start": "560080",
    "end": "562320"
  },
  {
    "text": "for chatbots but doesn't work so well",
    "start": "562320",
    "end": "564480"
  },
  {
    "text": "for other use cases to hit a specific",
    "start": "564480",
    "end": "566880"
  },
  {
    "text": "latency objective in online serving you",
    "start": "566880",
    "end": "569120"
  },
  {
    "text": "need to understand your traffic",
    "start": "569120",
    "end": "570480"
  },
  {
    "text": "distribution in terms of input and",
    "start": "570480",
    "end": "572000"
  },
  {
    "text": "output tokens you need to choose a",
    "start": "572000",
    "end": "574000"
  },
  {
    "text": "foundation model sized to have",
    "start": "574000",
    "end": "576399"
  },
  {
    "text": "acceptable quality at the lowest compute",
    "start": "576399",
    "end": "578640"
  },
  {
    "text": "cost select an accelerator configuration",
    "start": "578640",
    "end": "580959"
  },
  {
    "text": "for both your model and your traffic",
    "start": "580959",
    "end": "582720"
  },
  {
    "text": "that is cost effective and reserve",
    "start": "582720",
    "end": "584720"
  },
  {
    "text": "enough accelerators to handle your base",
    "start": "584720",
    "end": "586480"
  },
  {
    "text": "load and hopefully cover your burst load",
    "start": "586480",
    "end": "589680"
  },
  {
    "text": "and sending more requests to an",
    "start": "589680",
    "end": "591920"
  },
  {
    "text": "accelerator at the same time increases",
    "start": "591920",
    "end": "594240"
  },
  {
    "text": "throughput and the latency of all other",
    "start": "594240",
    "end": "597440"
  },
  {
    "text": "requests leading to the curve up here so",
    "start": "597440",
    "end": "600320"
  },
  {
    "text": "it's your latency tolerance not just",
    "start": "600320",
    "end": "602480"
  },
  {
    "text": "your traffic load that determines the",
    "start": "602480",
    "end": "604399"
  },
  {
    "text": "cost to serve we worked with teams",
    "start": "604399",
    "end": "606560"
  },
  {
    "text": "trying to solve this problem in with",
    "start": "606560",
    "end": "608800"
  },
  {
    "text": "multiple variables repeatedly new models",
    "start": "608800",
    "end": "611120"
  },
  {
    "text": "new hardware better software and",
    "start": "611120",
    "end": "612720"
  },
  {
    "text": "increasing prompt and output lengths all",
    "start": "612720",
    "end": "614800"
  },
  {
    "text": "led to high toil how can we help",
    "start": "614800",
    "end": "617519"
  },
  {
    "text": "optimize production",
    "start": "617519",
    "end": "619720"
  },
  {
    "text": "serving we start where it hurts the most",
    "start": "619720",
    "end": "622720"
  },
  {
    "text": "where the very nature of large model",
    "start": "622720",
    "end": "624240"
  },
  {
    "text": "serving leads to wasted resources and a",
    "start": "624240",
    "end": "626720"
  },
  {
    "text": "human can't be in the loop load",
    "start": "626720",
    "end": "628160"
  },
  {
    "text": "balancing we are moving from",
    "start": "628160",
    "end": "630000"
  },
  {
    "text": "microservices and web apps with very",
    "start": "630000",
    "end": "632399"
  },
  {
    "text": "small and very predictable requests to",
    "start": "632399",
    "end": "635200"
  },
  {
    "text": "very expensive and highly variable LLM",
    "start": "635200",
    "end": "637440"
  },
  {
    "text": "queries roundrobin load balancing",
    "start": "637440",
    "end": "639600"
  },
  {
    "text": "doesn't cut it anymore some accelerators",
    "start": "639600",
    "end": "642079"
  },
  {
    "text": "would sit idle and some requests would",
    "start": "642079",
    "end": "644240"
  },
  {
    "text": "be stuck waiting longer to get processed",
    "start": "644240",
    "end": "647040"
  },
  {
    "text": "we need to look at each request and",
    "start": "647040",
    "end": "648880"
  },
  {
    "text": "estimate where it will fit we also need",
    "start": "648880",
    "end": "651200"
  },
  {
    "text": "to know how full the servers are and how",
    "start": "651200",
    "end": "652959"
  },
  {
    "text": "sending one more request to that server",
    "start": "652959",
    "end": "655360"
  },
  {
    "text": "will impact the latency of all other",
    "start": "655360",
    "end": "656959"
  },
  {
    "text": "requests",
    "start": "656959",
    "end": "658480"
  },
  {
    "text": "we believe we can automate a significant",
    "start": "658480",
    "end": "660480"
  },
  {
    "text": "amount of the toil in generative AI",
    "start": "660480",
    "end": "662600"
  },
  {
    "text": "serving just based on these two ideas at",
    "start": "662600",
    "end": "665200"
  },
  {
    "text": "the load balancer model the cost of an",
    "start": "665200",
    "end": "668000"
  },
  {
    "text": "incoming LLM request and how it'll",
    "start": "668000",
    "end": "670160"
  },
  {
    "text": "impact other requests on that server and",
    "start": "670160",
    "end": "672640"
  },
  {
    "text": "build a real time snapshot of the",
    "start": "672640",
    "end": "674800"
  },
  {
    "text": "performance of each backend by",
    "start": "674800",
    "end": "676640"
  },
  {
    "text": "continuously gathering metrics capturing",
    "start": "676640",
    "end": "678880"
  },
  {
    "text": "the complex relationships between",
    "start": "678880",
    "end": "680920"
  },
  {
    "text": "hardware concurrency of traffic and",
    "start": "680920",
    "end": "683600"
  },
  {
    "text": "client visible latency the extra CPU",
    "start": "683600",
    "end": "686399"
  },
  {
    "text": "time spent gathering metrics and",
    "start": "686399",
    "end": "688399"
  },
  {
    "text": "precisely scheduling requests lets us",
    "start": "688399",
    "end": "690880"
  },
  {
    "text": "use more of the accelerator and expose a",
    "start": "690880",
    "end": "693680"
  },
  {
    "text": "better operational view to the service",
    "start": "693680",
    "end": "696040"
  },
  {
    "text": "owner even with just the simplest",
    "start": "696040",
    "end": "698480"
  },
  {
    "text": "version of this loop we see good results",
    "start": "698480",
    "end": "701680"
  },
  {
    "text": "as the number of model servers grows",
    "start": "701680",
    "end": "703600"
  },
  {
    "text": "random load balancing of non-uniform",
    "start": "703600",
    "end": "705920"
  },
  {
    "text": "requests increases the chance that one",
    "start": "705920",
    "end": "708079"
  },
  {
    "text": "model server is going to get multiple",
    "start": "708079",
    "end": "710560"
  },
  {
    "text": "very long requests in a row since a",
    "start": "710560",
    "end": "713200"
  },
  {
    "text": "model server needs GPU memory to",
    "start": "713200",
    "end": "715120"
  },
  {
    "text": "generate output tokens when memory fills",
    "start": "715120",
    "end": "717839"
  },
  {
    "text": "up that model server has to stop",
    "start": "717839",
    "end": "719680"
  },
  {
    "text": "accepting new requests which increases",
    "start": "719680",
    "end": "722279"
  },
  {
    "text": "latency just by steering requests to the",
    "start": "722279",
    "end": "725440"
  },
  {
    "text": "model server with the most unused GPU",
    "start": "725440",
    "end": "728079"
  },
  {
    "text": "memory we can achieve over 30% higher",
    "start": "728079",
    "end": "731200"
  },
  {
    "text": "QPS at constant",
    "start": "731200",
    "end": "733399"
  },
  {
    "text": "latency over a uh predictable traffic",
    "start": "733399",
    "end": "736160"
  },
  {
    "text": "load getting us closer to the maximum",
    "start": "736160",
    "end": "738240"
  },
  {
    "text": "utilization of the accelerator and this",
    "start": "738240",
    "end": "740240"
  },
  {
    "text": "is just a representative chat agent",
    "start": "740240",
    "end": "741839"
  },
  {
    "text": "workload the more workloads you add to a",
    "start": "741839",
    "end": "744560"
  },
  {
    "text": "shared set of model servers and the more",
    "start": "744560",
    "end": "746399"
  },
  {
    "text": "traffic patterns that overlap the",
    "start": "746399",
    "end": "749120"
  },
  {
    "text": "benefit of an algorithmic approach to",
    "start": "749120",
    "end": "750880"
  },
  {
    "text": "load balancing increases well we focused",
    "start": "750880",
    "end": "753600"
  },
  {
    "text": "on a single dimension for optimization",
    "start": "753600",
    "end": "755480"
  },
  {
    "text": "here we both see a huge number of",
    "start": "755480",
    "end": "758079"
  },
  {
    "text": "possible uh optimizations and research",
    "start": "758079",
    "end": "760240"
  },
  {
    "text": "and ecosystem that could be integrated",
    "start": "760240",
    "end": "762480"
  },
  {
    "text": "but how can we bring a highly",
    "start": "762480",
    "end": "763760"
  },
  {
    "text": "distributed ML ecosystem together",
    "start": "763760",
    "end": "767279"
  },
  {
    "text": "we're here at CubeCon because what needs",
    "start": "767279",
    "end": "769120"
  },
  {
    "text": "to be done is more important than",
    "start": "769120",
    "end": "770880"
  },
  {
    "text": "algorithms or hardware we need common",
    "start": "770880",
    "end": "773279"
  },
  {
    "text": "ground for operationalizing large models",
    "start": "773279",
    "end": "776240"
  },
  {
    "text": "as just another workload if we're all",
    "start": "776240",
    "end": "778320"
  },
  {
    "text": "going to be running large models in",
    "start": "778320",
    "end": "779760"
  },
  {
    "text": "production as a fundamental part of our",
    "start": "779760",
    "end": "781200"
  },
  {
    "text": "application infrastructure in a few",
    "start": "781200",
    "end": "782760"
  },
  {
    "text": "years we need to identify the APIs and",
    "start": "782760",
    "end": "785680"
  },
  {
    "text": "components that can be standardized and",
    "start": "785680",
    "end": "787920"
  },
  {
    "text": "reused we need common ground to bring",
    "start": "787920",
    "end": "790639"
  },
  {
    "text": "the latest research to production and a",
    "start": "790639",
    "end": "792399"
  },
  {
    "text": "common framework for innovation that",
    "start": "792399",
    "end": "794160"
  },
  {
    "text": "everyone can take to production",
    "start": "794160",
    "end": "796959"
  },
  {
    "text": "if there's a standard dynamic and",
    "start": "796959",
    "end": "798560"
  },
  {
    "text": "extensible load balancer it's Envoy we",
    "start": "798560",
    "end": "801440"
  },
  {
    "text": "chose to build our architecture around",
    "start": "801440",
    "end": "802959"
  },
  {
    "text": "Envoy because we knew it would work both",
    "start": "802959",
    "end": "804720"
  },
  {
    "text": "with and without Kubernetes and the rich",
    "start": "804720",
    "end": "807600"
  },
  {
    "text": "ecosystem of the gateway API would",
    "start": "807600",
    "end": "810000"
  },
  {
    "text": "ensure our extension could avoid",
    "start": "810000",
    "end": "812160"
  },
  {
    "text": "duplicating all of the regular load",
    "start": "812160",
    "end": "813600"
  },
  {
    "text": "balancing features that LLM service",
    "start": "813600",
    "end": "815760"
  },
  {
    "text": "owners also need we use the standard",
    "start": "815760",
    "end": "819040"
  },
  {
    "text": "Envoy X proc callout mechanism to",
    "start": "819040",
    "end": "822480"
  },
  {
    "text": "decouple our algorithms from the load",
    "start": "822480",
    "end": "824959"
  },
  {
    "text": "balancer you can deploy them",
    "start": "824959",
    "end": "826360"
  },
  {
    "text": "independently this also gives us the",
    "start": "826360",
    "end": "828399"
  },
  {
    "text": "freedom to have multiple implementations",
    "start": "828399",
    "end": "830320"
  },
  {
    "text": "and to allow for forking and",
    "start": "830320",
    "end": "831959"
  },
  {
    "text": "experimentation when very large platform",
    "start": "831959",
    "end": "834399"
  },
  {
    "text": "teams need something that the open",
    "start": "834399",
    "end": "836079"
  },
  {
    "text": "source project doesn't provide yet we",
    "start": "836079",
    "end": "839120"
  },
  {
    "text": "also worked to standardize the metrics",
    "start": "839120",
    "end": "840800"
  },
  {
    "text": "we would need in the top model servers",
    "start": "840800",
    "end": "842399"
  },
  {
    "text": "so that operators have a consistent",
    "start": "842399",
    "end": "843839"
  },
  {
    "text": "experience across the ecosystem and our",
    "start": "843839",
    "end": "846000"
  },
  {
    "text": "scheduler would need to do less our",
    "start": "846000",
    "end": "848399"
  },
  {
    "text": "focus is automating the boring parts of",
    "start": "848399",
    "end": "850240"
  },
  {
    "text": "going to production with LLM and",
    "start": "850240",
    "end": "852000"
  },
  {
    "text": "bringing you the best of ML research we",
    "start": "852000",
    "end": "854639"
  },
  {
    "text": "plug into a broad set of gateway",
    "start": "854639",
    "end": "856240"
  },
  {
    "text": "solutions we don't have any opinion on",
    "start": "856240",
    "end": "858000"
  },
  {
    "text": "how you deploy your model servers we",
    "start": "858000",
    "end": "860079"
  },
  {
    "text": "build in support for Laura for",
    "start": "860079",
    "end": "862639"
  },
  {
    "text": "prioritization and fairness and for",
    "start": "862639",
    "end": "864800"
  },
  {
    "text": "standard model rollouts so you can",
    "start": "864800",
    "end": "867120"
  },
  {
    "text": "safely share your model servers between",
    "start": "867120",
    "end": "868959"
  },
  {
    "text": "many different workloads for higher",
    "start": "868959",
    "end": "870600"
  },
  {
    "text": "utilization we want to be a loadbearing",
    "start": "870600",
    "end": "872959"
  },
  {
    "text": "part of your serving infrastructure",
    "start": "872959",
    "end": "875040"
  },
  {
    "text": "orchestrate all of us can depend on an",
    "start": "875040",
    "end": "877440"
  },
  {
    "text": "ecosystem driving optimization and the",
    "start": "877440",
    "end": "879680"
  },
  {
    "text": "control you need over your production",
    "start": "879680",
    "end": "881199"
  },
  {
    "text": "journey",
    "start": "881199",
    "end": "883040"
  },
  {
    "text": "yeah 2025 makes the year of production",
    "start": "883040",
    "end": "885600"
  },
  {
    "text": "scaling leading inference uh projects",
    "start": "885600",
    "end": "888000"
  },
  {
    "text": "like VM and control plan airs SG long",
    "start": "888000",
    "end": "891600"
  },
  {
    "text": "and tensor RT have all prioritize large",
    "start": "891600",
    "end": "894240"
  },
  {
    "text": "scale deployment in the road maps",
    "start": "894240",
    "end": "896399"
  },
  {
    "text": "putting efficient scalable inference at",
    "start": "896399",
    "end": "898720"
  },
  {
    "text": "the heart of their strategies so the",
    "start": "898720",
    "end": "900959"
  },
  {
    "text": "gateway API inference extension project",
    "start": "900959",
    "end": "903600"
  },
  {
    "text": "will play a critical role as the",
    "start": "903600",
    "end": "905600"
  },
  {
    "text": "foundation for LLM aware load balancer",
    "start": "905600",
    "end": "908079"
  },
  {
    "text": "in Kubernetes unlocking intelligent",
    "start": "908079",
    "end": "910480"
  },
  {
    "text": "traffic control for LM workloads looking",
    "start": "910480",
    "end": "913360"
  },
  {
    "text": "ahead we'll focus on enabling a full",
    "start": "913360",
    "end": "916399"
  },
  {
    "text": "suit of production readiness features",
    "start": "916399",
    "end": "919120"
  },
  {
    "text": "including fairness for multi-tenencies",
    "start": "919120",
    "end": "921760"
  },
  {
    "text": "heterogeneous weighted routing adaptive",
    "start": "921760",
    "end": "924160"
  },
  {
    "text": "SLO driven routing and KV casual wire",
    "start": "924160",
    "end": "927040"
  },
  {
    "text": "routing to support production workflows",
    "start": "927040",
    "end": "929440"
  },
  {
    "text": "at scale and with that we'll turn it",
    "start": "929440",
    "end": "932560"
  },
  {
    "text": "back to you we hope that you try the if",
    "start": "932560",
    "end": "935120"
  },
  {
    "text": "you're thinking about LLM serving in",
    "start": "935120",
    "end": "936639"
  },
  {
    "text": "production give us a try give us",
    "start": "936639",
    "end": "938959"
  },
  {
    "text": "feedback and help become part of the",
    "start": "938959",
    "end": "940800"
  },
  {
    "text": "community thank you",
    "start": "940800",
    "end": "944079"
  },
  {
    "text": "hey",
    "start": "945600",
    "end": "947839"
  }
]