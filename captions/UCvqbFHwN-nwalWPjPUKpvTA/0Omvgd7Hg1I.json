[
  {
    "start": "0",
    "end": "48000"
  },
  {
    "text": "please welcome Michael [Applause]",
    "start": "1360",
    "end": "10679"
  },
  {
    "text": "Rubin hello my name is Michael Rubin I am a tech lead and manager of the",
    "start": "10679",
    "end": "16240"
  },
  {
    "text": "kubernetes team or one of the kubernetes teams at Google and um my GitHub handle",
    "start": "16240",
    "end": "21359"
  },
  {
    "text": "is match tick this is a talk about the life of a packet trying to describe a um",
    "start": "21359",
    "end": "26760"
  },
  {
    "text": "traffic for kubernetes in the network it is consumes some basic kubernetes knowledge and it's packed it was",
    "start": "26760",
    "end": "35320"
  },
  {
    "text": "originally even longer than this and so because of that I'm going to probably be taking questions offline if that's okay",
    "start": "35320",
    "end": "40800"
  },
  {
    "text": "I'll be available um right after the talk if people want to know more or something is not clear so I appreciate",
    "start": "40800",
    "end": "46719"
  },
  {
    "text": "your time and let's dive into this um kubernetes is about clusters and because",
    "start": "46719",
    "end": "52199"
  },
  {
    "start": "48000",
    "end": "48000"
  },
  {
    "text": "it's about clusters this means the networking is pretty fundamental and important to the whole system our job in kubernetes is to make sure it",
    "start": "52199",
    "end": "58280"
  },
  {
    "text": "automatically just works fingers crossed and that your applications can talk to each other they can talk in and out of",
    "start": "58280",
    "end": "65119"
  },
  {
    "text": "the cluster and they can only talk when you want them to talk and not more nor less something I know that my family",
    "start": "65119",
    "end": "72000"
  },
  {
    "text": "wishes I would do more um we're going to dive into some of the fundamental constructs and philosophies behind the",
    "start": "72000",
    "end": "77479"
  },
  {
    "text": "kubernetes networking model starts with the IP perod model every pod has a real",
    "start": "77479",
    "end": "82960"
  },
  {
    "start": "81000",
    "end": "81000"
  },
  {
    "text": "IP address associated with it so this means that the Pod has an IP of its own",
    "start": "82960",
    "end": "88360"
  },
  {
    "text": "we're not going to play games with ports we're not going to play games with private machine IPS and the reason for this is that at Google and Borg we um",
    "start": "88360",
    "end": "95439"
  },
  {
    "text": "deploy tens of thousands of jobs on tens of thousands of computers and we found that mapping things to Ports hurts and",
    "start": "95439",
    "end": "102320"
  },
  {
    "text": "that overall Port collision and port management becomes a game in and of itself and one of the lessons that we had learned there was let's see if we",
    "start": "102320",
    "end": "108799"
  },
  {
    "text": "can think of doing something um in a more smoother way and we came up with this model and it looks like it's been working pretty well every pod should be",
    "start": "108799",
    "end": "115280"
  },
  {
    "text": "accessible to every other pod regardless of the VM they're on you shouldn't worry about what VM they're on that that unit of work should be schedulable across the",
    "start": "115280",
    "end": "121840"
  },
  {
    "text": "cluster and reachable to all other elements inside the cluster we're also going to be using traditional old school",
    "start": "121840",
    "end": "128920"
  },
  {
    "text": "Linux technology to implement this um old school Concepts basic IP routing and",
    "start": "128920",
    "end": "134760"
  },
  {
    "text": "systems and philosophies that have been around for a very long time for decades let's take a look and walk through some of this this is a machine",
    "start": "134760",
    "end": "142519"
  },
  {
    "start": "139000",
    "end": "139000"
  },
  {
    "text": "it is a box most of computers are boxes with boxes inside and lines um in this case we have a node you know a VM or",
    "start": "142519",
    "end": "149200"
  },
  {
    "text": "computer and an Ethernet device it could be virtual it could be real it doesn't really matter the concepts remain the same and are consistent but in truth",
    "start": "149200",
    "end": "156840"
  },
  {
    "text": "there's more to it they always is every node um has a network name space every connection or device also has a network",
    "start": "156840",
    "end": "163599"
  },
  {
    "text": "name space associated with it the tcpip stack the settings the context the idea that these are all of the pieces and um",
    "start": "163599",
    "end": "171720"
  },
  {
    "text": "the context that is associated with that network connection or with that Network environment now you want the Pod to feel",
    "start": "171720",
    "end": "178680"
  },
  {
    "text": "like it's got its own network name space you don't want the Pod to feel like it's sitting on a node you want the Pod to feel isolated um in that it has its own",
    "start": "178680",
    "end": "186400"
  },
  {
    "text": "domain and its own area to do the things that it wants to do so when you write your application you're free to do what you need to do you don't feel like",
    "start": "186400",
    "end": "193080"
  },
  {
    "text": "you're being hosted on a node so we can easily create another Network namespace for the first pod and we're going to",
    "start": "193080",
    "end": "198200"
  },
  {
    "text": "call that namespace pod one now you also want to be able to communicate between name spaces and to",
    "start": "198200",
    "end": "204640"
  },
  {
    "text": "do that we have a very old construct just a pipe pair but with Linux you can make it look a pipe pair look like a",
    "start": "204640",
    "end": "210360"
  },
  {
    "text": "virtual ethernet device here we've named one virtual ethernet device xx and another one virtual ethernet device y",
    "start": "210360",
    "end": "219120"
  },
  {
    "text": "y we connect them to the pod's root uh Network namespace and now the Pod can talk to the root name space and we name",
    "start": "219120",
    "end": "224799"
  },
  {
    "text": "it e zero so now this pod is really happy it's got its own network name space and it thinks it's got each zero",
    "start": "224799",
    "end": "230439"
  },
  {
    "text": "of its own so it feels like it's got its own setup we can do this many many times we can have one node handle and host",
    "start": "230439",
    "end": "237720"
  },
  {
    "text": "several name spaces for many many pods we want them to talk to each other and we're going to be using again a very",
    "start": "237720",
    "end": "243400"
  },
  {
    "text": "familiar construct to people in networking it's just a bridge bridge uses L2 technology Mac and ARP if uh you",
    "start": "243400",
    "end": "249640"
  },
  {
    "text": "don't feel comfortable with this stuff go Google it there's tons of documentation and tools associated with making sure it works in the world",
    "start": "249640",
    "end": "256840"
  },
  {
    "text": "outside so let's now track and see exactly how you can get a pod to talk to the other pod on the same",
    "start": "256840",
    "end": "263040"
  },
  {
    "start": "258000",
    "end": "258000"
  },
  {
    "text": "Noe pod one emits a packet sourc from pod one and it's going to go to pod two",
    "start": "263040",
    "end": "268919"
  },
  {
    "text": "it comes out of the Network namespace and crosses the pipe pair that's a really quick transversal goes through",
    "start": "268919",
    "end": "274880"
  },
  {
    "text": "the pipe pair and then goes to the bridge it's got an IP address just like every other Bridge using the L2 technology it finds the correct ethernet",
    "start": "274880",
    "end": "282000"
  },
  {
    "text": "device for Destination bammo and once it finds that",
    "start": "282000",
    "end": "288000"
  },
  {
    "text": "device pipe pair quick traversal again and boom it gets to the right Network namespace and the Pod that can handle",
    "start": "288000",
    "end": "294639"
  },
  {
    "text": "that traffic you're going to see this a lot in this talk it's going to get boring where other people you may find",
    "start": "294639",
    "end": "299720"
  },
  {
    "text": "this confusing beforehand you have a flat Network space inside of this model but you want to",
    "start": "299720",
    "end": "305280"
  },
  {
    "start": "302000",
    "end": "302000"
  },
  {
    "text": "also have the pods be reachable across nodes it shouldn't matter where the Pod is kubernetes should handle that you as",
    "start": "305280",
    "end": "310600"
  },
  {
    "text": "a user shouldn't have to care kubernetes doesn't care how we do routing in between pods you can use L2 L3 overlays",
    "start": "310600",
    "end": "318000"
  },
  {
    "text": "carrier pigeons as long as that traffic can reach one pod to the other even across nodes you assign a cider block to",
    "start": "318000",
    "end": "324720"
  },
  {
    "text": "each node so that each pod can then have its own IP address which we said before is important for kubernetes we're going",
    "start": "324720",
    "end": "330160"
  },
  {
    "text": "to show you how we do some of this routing in gcp a little bit and show you a simple example right now here's the",
    "start": "330160",
    "end": "335880"
  },
  {
    "start": "335000",
    "end": "335000"
  },
  {
    "text": "life of a packet pod top pod across a node the packet leaves Source uh pod one",
    "start": "335880",
    "end": "341840"
  },
  {
    "text": "it's going to go to pod four which is on that long way down to node two same path as before hits the bridge but this time",
    "start": "341840",
    "end": "348720"
  },
  {
    "text": "instead of thinking that that packet needs to go to a local um ethernet device it knows to go to the one on the",
    "start": "348720",
    "end": "354680"
  },
  {
    "text": "edge and so that the traffic will leave the node bammo it then hits the p uh the",
    "start": "354680",
    "end": "359919"
  },
  {
    "text": "network in between the nodes every network is going to have its own way to routing that packet between one node and",
    "start": "359919",
    "end": "364960"
  },
  {
    "text": "the other uh it's a great thing about open source you can choose what fits for you goes to the other node hits the",
    "start": "364960",
    "end": "370960"
  },
  {
    "text": "bridge and now it acts just like it did before it finds the bridge discovers again using RP L2 which is the correct",
    "start": "370960",
    "end": "377639"
  },
  {
    "text": "ethernet device from the ethernet device pipe pair boom we're in pod 4 and",
    "start": "377639",
    "end": "382720"
  },
  {
    "text": "straightforward should be simple understand overlays now overlays are a l",
    "start": "382720",
    "end": "388960"
  },
  {
    "text": "level of complexity that um one can employ to solve some problems uh why do you need an overlay",
    "start": "388960",
    "end": "396360"
  },
  {
    "start": "394000",
    "end": "394000"
  },
  {
    "text": "well an overlay network is basically an L2 Network built upon the network you already have why would you want to build",
    "start": "396360",
    "end": "402880"
  },
  {
    "text": "another level layer of complexity on top of the layer of complexity you already need you already been provided one",
    "start": "402880",
    "end": "408960"
  },
  {
    "text": "reason is maybe you don't have enough IP space given to all of the pods another reason could be maybe the network can handle all of the routes you want",
    "start": "408960",
    "end": "415039"
  },
  {
    "text": "between the nodes um one of the things that's great about kubernetes is that it marries the resources of the environment",
    "start": "415039",
    "end": "422599"
  },
  {
    "text": "to make life easier for the application developers and here with overlays you have an opportunity to make life easier",
    "start": "422599",
    "end": "428160"
  },
  {
    "text": "for the application developers based on the constraints of the network around you and one way to do this is",
    "start": "428160",
    "end": "433599"
  },
  {
    "text": "encapsulating the packet in a packet we're going to talk about that in more detail now but the point behind all of this is being able to more easily",
    "start": "433599",
    "end": "440120"
  },
  {
    "text": "Traverse the network between the nodes um you may not want to use an overlay all of the time uh sometimes",
    "start": "440120",
    "end": "447039"
  },
  {
    "text": "there's latency overhead for some Cloud providers there has to do with MTU settings and perhaps um you know just",
    "start": "447039",
    "end": "453080"
  },
  {
    "text": "things taking longer in the setup of that network uh there is more complexity associated with it and sometimes it's",
    "start": "453080",
    "end": "458240"
  },
  {
    "text": "not needed sometimes we get calls from people in kubernetes saying my overlay doesn't work and we say well what's you",
    "start": "458240",
    "end": "463400"
  },
  {
    "text": "doing it for and the response is I don't know don't I need one you don't always need one some past providers do have it",
    "start": "463400",
    "end": "469919"
  },
  {
    "text": "on by default though because it allows them to simplify their setup use it when you need it and make sure that it's",
    "start": "469919",
    "end": "475560"
  },
  {
    "text": "making sense so let's take a look at an example of an overlay Network in this case it's one provided by cor West",
    "start": "475560",
    "end": "480919"
  },
  {
    "start": "477000",
    "end": "477000"
  },
  {
    "text": "called flannel it's fairly straightforward and um we should say thank you that's why we do um in this",
    "start": "480919",
    "end": "486479"
  },
  {
    "text": "case you notice that there's a new virtual ethernet device called the flannel zero device we add that to our",
    "start": "486479",
    "end": "492120"
  },
  {
    "text": "root name space uh pod one is now going to send a packet to pod 4 goes through the same process as",
    "start": "492120",
    "end": "498840"
  },
  {
    "text": "before hits the bridge but now when it hits the bridge something interesting happens when it's at the bridge it needs",
    "start": "498840",
    "end": "505360"
  },
  {
    "text": "to go to pod four but ethernet zero um and the network in between the nodes doesn't know anything about pod IP",
    "start": "505360",
    "end": "511520"
  },
  {
    "text": "addresses we've been assigning these in a different way in an overlay network but flannel zero device does so the",
    "start": "511520",
    "end": "518399"
  },
  {
    "text": "packet instead is going to get routed to the flannel zero device now what happens inside that flannel zero device a lot of magic um",
    "start": "518399",
    "end": "526000"
  },
  {
    "text": "there's a mapping associated in user space where you can program all of the Pod IP addresses to the node IP",
    "start": "526000",
    "end": "531839"
  },
  {
    "text": "addresses where you can say hey look I know that these pods are found on these nodes and kubernetes will set this up",
    "start": "531839",
    "end": "537360"
  },
  {
    "text": "for you by Magic because flannel is integrated into kubernetes and what happens inside the flannel device is",
    "start": "537360",
    "end": "542600"
  },
  {
    "text": "that you then encapsulate you put a header on top of that packet to say even",
    "start": "542600",
    "end": "547959"
  },
  {
    "text": "though the packet knows it's supposed to go to pod 4 we know to get there it's going to have to find node 2 and it's",
    "start": "547959",
    "end": "553880"
  },
  {
    "text": "really coming from node one this is all done in the kernel often",
    "start": "553880",
    "end": "559320"
  },
  {
    "text": "so even though some of the mapping is done in user space in the data path it's generally very very fast because you don't have to trap user space or go up",
    "start": "559320",
    "end": "565560"
  },
  {
    "text": "and down for user space so here we go we rewrite the packet now has this nice sort of candy coating to help it find",
    "start": "565560",
    "end": "571279"
  },
  {
    "text": "its way hits ezero now because ezero is the device that is involved in routing node",
    "start": "571279",
    "end": "577800"
  },
  {
    "text": "traffic traverses across your network which knows how to handle node traffic finds e zero and now in E zero it has",
    "start": "577800",
    "end": "585399"
  },
  {
    "text": "this node 2 um destination and the bridge isn't going to know where that goes but flannel does goes to flannel",
    "start": "585399",
    "end": "592000"
  },
  {
    "text": "which decapsulates the packet bam we go through that other path that we had before and our packet is Happy Home in",
    "start": "592000",
    "end": "598760"
  },
  {
    "text": "pod for now you have to deal with change clusters are Dynamic and the great thing about kubernetes is as you Auto scale",
    "start": "598760",
    "end": "605600"
  },
  {
    "start": "600000",
    "end": "600000"
  },
  {
    "text": "the cluster up as you shrink it down as pods hang as VMS reboot as I don't know",
    "start": "605600",
    "end": "611240"
  },
  {
    "text": "apocalypses come and go you should be hopefully having kubernetes just to keep on ticking and you shouldn't have to",
    "start": "611240",
    "end": "616399"
  },
  {
    "text": "worry about it the whole point here is to make your life easier um often you don't want to keep track of all of these",
    "start": "616399",
    "end": "622120"
  },
  {
    "text": "pods coming up and down and what their addresses are how do we deal with the dynamic change going on with your pod",
    "start": "622120",
    "end": "627160"
  },
  {
    "text": "set and the work that you want to assign to it evenly and balanced across",
    "start": "627160",
    "end": "632839"
  },
  {
    "text": "Services a service is a way of a handle to say this is going to be a group of endpoints and here's how I want to get",
    "start": "632839",
    "end": "639120"
  },
  {
    "start": "634000",
    "end": "634000"
  },
  {
    "text": "to these endpoints uh a service will provide you a virtual IP address uh a sort of a doorway that you know when",
    "start": "639120",
    "end": "644480"
  },
  {
    "text": "your traffic hits that door it's going to be evenly distributed across the workers behind that door uh one way to",
    "start": "644480",
    "end": "649880"
  },
  {
    "text": "think of it is like a restaurant you know you have one waiter and a ton of Great Cooks right behind the door that's going to handle all the things you want",
    "start": "649880",
    "end": "655519"
  },
  {
    "text": "them to uh the set of the people the cooks in that kitchen they can come and go you're not going to care as long as",
    "start": "655519",
    "end": "661440"
  },
  {
    "text": "that waiter looks cool and um the food is good here's a service you're going to",
    "start": "661440",
    "end": "667680"
  },
  {
    "text": "create with gamble um what matters here is the selector once again you're selecting the type of PODS that you want",
    "start": "667680",
    "end": "673639"
  },
  {
    "text": "this traffic to go to associated with this named service you're creating a service that says this is the service",
    "start": "673639",
    "end": "679120"
  },
  {
    "text": "and when you associate work with this service it should go to these pods it will automatically create for",
    "start": "679120",
    "end": "686120"
  },
  {
    "text": "you in kues a distributed load balancer you don't have to do anything else and it's going to give you the IP address to",
    "start": "686120",
    "end": "691240"
  },
  {
    "text": "reach all of those Services as you get the um the feedback from the kubernetes system the result of the kubernetes",
    "start": "691240",
    "end": "696279"
  },
  {
    "text": "creation of the service now slight digression endpoints",
    "start": "696279",
    "end": "701440"
  },
  {
    "start": "699000",
    "end": "699000"
  },
  {
    "text": "when I first joined the kubernetes team everyone used the term endpoints to mean something different and it took me a while to realize endpoints is actually a",
    "start": "701440",
    "end": "708360"
  },
  {
    "text": "defined object in the API server itself and the word is used in many different ways I think I even use it a few",
    "start": "708360",
    "end": "714240"
  },
  {
    "text": "different ways in this talk an endpoint object is automatically created for you when you create a service it's just an",
    "start": "714240",
    "end": "720240"
  },
  {
    "text": "optimization it turns out that in kubernetes many controllers want to know the name uh the IP addresses of all the",
    "start": "720240",
    "end": "726160"
  },
  {
    "text": "pods associated with the service and instead and because we can have many many pods and kubernetes it can get",
    "start": "726160",
    "end": "731320"
  },
  {
    "text": "expensive to keep asking the API server to go find that selection instead we group them together",
    "start": "731320",
    "end": "737760"
  },
  {
    "text": "in an object called endpoints so that instead of constantly dering this information over and over again it's",
    "start": "737760",
    "end": "743120"
  },
  {
    "text": "sort of cached for us inside this object so at endpoint when it's um service is created you create an endpoint one to",
    "start": "743120",
    "end": "749040"
  },
  {
    "text": "one mapping generally with that service that has the same selector in this case store backend and then it will go off",
    "start": "749040",
    "end": "754760"
  },
  {
    "text": "and dynamically keep up to date the list of pods with those selectors this is what the ammo looks like and again you",
    "start": "754760",
    "end": "761199"
  },
  {
    "text": "don't generate this generally usually kubernetes will do this for you all right let's get off end points let's go",
    "start": "761199",
    "end": "766600"
  },
  {
    "start": "766000",
    "end": "766000"
  },
  {
    "text": "take a look now at the traffic of going from a pod to a service just like before the Pod sends",
    "start": "766600",
    "end": "771800"
  },
  {
    "text": "up to packet but this time the destination is not another pod it's a service goes through the pipe pair hits",
    "start": "771800",
    "end": "776959"
  },
  {
    "text": "the bridge and now things change we use IP tables uh everyone's favorite duct",
    "start": "776959",
    "end": "783120"
  },
  {
    "text": "tape for networking we say in IP tables remember what our goal is with the service to",
    "start": "783120",
    "end": "789399"
  },
  {
    "text": "evenly distribute the traffic across all of the pods IP tables will say I know all of the pods CU I know all of the end",
    "start": "789399",
    "end": "796160"
  },
  {
    "text": "points I'm going to pick one randomly bammo pod 99 that's going to be the one where this traffic is going to go it",
    "start": "796160",
    "end": "801920"
  },
  {
    "text": "doesn't really want to go to the service IP it's going to go to pod 99 and I'm going to do a destination that to make",
    "start": "801920",
    "end": "807000"
  },
  {
    "text": "sure that it gets there inside the IP table and then we're going to use contract what is contract this is a really cool",
    "start": "807000",
    "end": "813399"
  },
  {
    "text": "piece of the Linux kernel subsystem that again I urge people to go and check out again it's an old piece of the subsystem",
    "start": "813399",
    "end": "819079"
  },
  {
    "text": "inside of Linux and well documented but for all you need to know now is it's a connection tracker it knows that when",
    "start": "819079",
    "end": "825440"
  },
  {
    "text": "you're sending traffic through the Linux subsystem it's going to remember the source and then where it sent",
    "start": "825440",
    "end": "831279"
  },
  {
    "text": "it you'll see why it's important in a second so now we have a normal IP address routing pod one going off to pod",
    "start": "831279",
    "end": "838040"
  },
  {
    "text": "99 it goes leave the node it comes back really fast because pod 99 is a really happy no uh Happy pod and when it comes",
    "start": "838040",
    "end": "845079"
  },
  {
    "text": "back it hits the IP tables the IP tables remember using contract that this came from pod 99 and that really was meant to",
    "start": "845079",
    "end": "852320"
  },
  {
    "text": "be for service one we don't want to confuse pod one we love pod one we want this to be transparent to pod one so we",
    "start": "852320",
    "end": "858600"
  },
  {
    "text": "rewrite the source IP address unden it return it home and pod one is none the",
    "start": "858600",
    "end": "864040"
  },
  {
    "text": "none the wiser bit more on IP tables one of the very confusing things for me at least",
    "start": "864040",
    "end": "869759"
  },
  {
    "start": "866000",
    "end": "866000"
  },
  {
    "text": "and many other people when they first encounter the idea of a service is that you think a service is like a running",
    "start": "869759",
    "end": "875320"
  },
  {
    "text": "demon somewhere you're looking for that service code in truth it's kind of more of an idea it's an API object that",
    "start": "875320",
    "end": "881480"
  },
  {
    "text": "represents the the way you want to share and balance the load associated with a name and a selector in the API server",
    "start": "881480",
    "end": "887639"
  },
  {
    "text": "and it's implemented across every node in IP tables and the IP table rules look really scary but what they're really",
    "start": "887639",
    "end": "893680"
  },
  {
    "text": "just doing logically is saying if the traffic's going to be hitting the service IP and port pick one of the back",
    "start": "893680",
    "end": "899480"
  },
  {
    "text": "ends rewrite the IP address and send the traffic that way um another thing about uh the whole service concept is there's",
    "start": "899480",
    "end": "906399"
  },
  {
    "text": "a um a term floating around called Cube proxy people say it a lot and it's once",
    "start": "906399",
    "end": "911480"
  },
  {
    "text": "was a really really good name it's not a good name anymore and because it's not really a proxy at all but it is",
    "start": "911480",
    "end": "917839"
  },
  {
    "text": "associated with kubernetes so the Cube part is cool um it used to be a proxy it used to be when the traffic was routed",
    "start": "917839",
    "end": "924680"
  },
  {
    "text": "you'd pull it out of the kernel copy then you would miss um you would mangle the IP header and then put it back in",
    "start": "924680",
    "end": "931199"
  },
  {
    "text": "the kernel to send it out the door copy and no one likes copying data twice and it was a real proxy in user space doing",
    "start": "931199",
    "end": "939600"
  },
  {
    "text": "that for every connection and every piece of traffic involved in the service that's not cool and so as a result you",
    "start": "939600",
    "end": "945440"
  },
  {
    "text": "wanted it out of the data path in user space do all of this in IP tables in the kernel so Cube proxy now its main",
    "start": "945440",
    "end": "951399"
  },
  {
    "text": "purpose is to keep track like any other good controller in kubernetes of where these end points are and then update the",
    "start": "951399",
    "end": "959560"
  },
  {
    "text": "IP tables accordingly so that we can route that traffic accordingly also I got some people in the audience nodding",
    "start": "959560",
    "end": "965440"
  },
  {
    "text": "their heads some people who may have questions later that's fine this is not obvious and you'll see why in a",
    "start": "965440",
    "end": "971920"
  },
  {
    "text": "second um or actually well maybe you saw y already uh DNS is just another service",
    "start": "971920",
    "end": "979079"
  },
  {
    "text": "so services are every time you create a service it actually gets a DNS name of its own so you don't have to hardcode",
    "start": "979079",
    "end": "984759"
  },
  {
    "text": "that IP address into uh your source code if you want your pods code to find these Services instead",
    "start": "984759",
    "end": "993600"
  },
  {
    "text": "you get the name of that service hard code that into your pods code and know that once you create the service it can",
    "start": "993600",
    "end": "999880"
  },
  {
    "text": "find all of the pods associated with it by just hitting the DNS name and it'll be served as an A or SRV record and DNS",
    "start": "999880",
    "end": "1006360"
  },
  {
    "text": "itself it runs as a pod and a service of its own so we're just using the same kubernetes infrastructure the yaml is a",
    "start": "1006360",
    "end": "1012519"
  },
  {
    "text": "little bit different because we actually specify the IP address we want the DNS service to run on and DNS is a little",
    "start": "1012519",
    "end": "1018600"
  },
  {
    "text": "special for kubernetes so we have implementation that we've done to autoscale it the bigger the cluster gets",
    "start": "1018600",
    "end": "1025798"
  },
  {
    "text": "the more DNS services that are sprinkled around your kubernetes cluster we all love DNS DNS is oxygen and so none of us",
    "start": "1025799",
    "end": "1033120"
  },
  {
    "text": "like it when we make a DNS request and it takes a long time services are simple and Powerful um",
    "start": "1033120",
    "end": "1040280"
  },
  {
    "text": "you can use any port you want you don't have to worry about conflicts and remapping ports uh you can request a port uh an IP address if there's a",
    "start": "1040280",
    "end": "1046720"
  },
  {
    "text": "special one that you need and you can remap ports and not worry about it um the other piece to remember again is",
    "start": "1046720",
    "end": "1052880"
  },
  {
    "text": "it's just an abstraction it's not a demon of running code that sits across everything inside the node unless you",
    "start": "1052880",
    "end": "1059320"
  },
  {
    "text": "argu to look at the cube proxy as an implementer of the idea of the routing in the IP tables um but the big thing to",
    "start": "1059320",
    "end": "1065919"
  },
  {
    "text": "remember is a service is a way of setting it up so that all you need to do is hit that IP address and it magically",
    "start": "1065919",
    "end": "1071640"
  },
  {
    "text": "and evenly balanced way hits all of your pods let's talk about sending traffic",
    "start": "1071640",
    "end": "1077919"
  },
  {
    "start": "1076000",
    "end": "1076000"
  },
  {
    "text": "outside of your cluster to um the world uh we're going to focus on a gcp",
    "start": "1077919",
    "end": "1083200"
  },
  {
    "text": "implementation of this because it's fairly straightforward and um you know it it should be a good way to explain it",
    "start": "1083200",
    "end": "1089799"
  },
  {
    "text": "what if you want to reach traffic outside of the world egress every node in the cluster will",
    "start": "1089799",
    "end": "1094919"
  },
  {
    "start": "1093000",
    "end": "1093000"
  },
  {
    "text": "get an IP address or IP range from the 108 this is how gcp does it IP space and",
    "start": "1094919",
    "end": "1101360"
  },
  {
    "text": "the nodes can also have a public IP too so how does this work how are we going to make this work for the internet and",
    "start": "1101360",
    "end": "1107120"
  },
  {
    "start": "1107000",
    "end": "1107000"
  },
  {
    "text": "for our traffic well well you have your node that's inside a cluster and then you have a little system that does Nat",
    "start": "1107120",
    "end": "1112679"
  },
  {
    "text": "so that you can talk to the world outside your node will have a source internal IP address and it'll talk to",
    "start": "1112679",
    "end": "1118919"
  },
  {
    "text": "8888 which to me is a really friendly happy IP address that I use a lot at home to debug things and make sure",
    "start": "1118919",
    "end": "1125200"
  },
  {
    "text": "things work um that natat system for your cluster is going to rewrite that Source",
    "start": "1125200",
    "end": "1131480"
  },
  {
    "text": "um IP address because the outside world it doesn't know it doesn't shouldn't have to care about what's going on inside the",
    "start": "1131480",
    "end": "1137320"
  },
  {
    "text": "cluster packet leaves with this external IP address that's safe and well known it",
    "start": "1137320",
    "end": "1143400"
  },
  {
    "text": "returns really fast because 8888 has never let me down and um comes back and",
    "start": "1143400",
    "end": "1149960"
  },
  {
    "text": "okay cool one person's laughing at my joke just like the last talk um remaps to find inside of the cluster",
    "start": "1149960",
    "end": "1157280"
  },
  {
    "text": "where that traffic's going to go and away we go now let's do this with pods pod does the same path we've seen before",
    "start": "1157280",
    "end": "1164280"
  },
  {
    "text": "comes from pod one wants to go to our friend 8888 Bam Bam goes to that net",
    "start": "1164280",
    "end": "1169480"
  },
  {
    "text": "subsystem and it's dropped why is it dropped it's dropped because the PO IP doesn't equal the node IP and a lot of",
    "start": "1169480",
    "end": "1176000"
  },
  {
    "start": "1171000",
    "end": "1171000"
  },
  {
    "text": "the cluster systems out there don't just want willy-nilly Source IP traffic to go off and hit the world outside it wants",
    "start": "1176000",
    "end": "1182240"
  },
  {
    "text": "to have it registered with no uh with VMS or nodes this is because the world is still making that transition from",
    "start": "1182240",
    "end": "1188440"
  },
  {
    "text": "node to Containers um what do we do to fix it anybody's guess IP tables and",
    "start": "1188440",
    "end": "1195440"
  },
  {
    "text": "once again we sprinkled more IP tables and we mask into a source and so we do this for any packet that has got a",
    "start": "1195440",
    "end": "1202360"
  },
  {
    "text": "destination outside of our cluster so here we have a packet outside of our cluster we sprinkle our IP tables on it",
    "start": "1202360",
    "end": "1208480"
  },
  {
    "start": "1204000",
    "end": "1204000"
  },
  {
    "text": "rewrite the um Source IP address masquerading as the internal IP and we",
    "start": "1208480",
    "end": "1213760"
  },
  {
    "text": "follow the same pattern as before for a happy happy",
    "start": "1213760",
    "end": "1219480"
  },
  {
    "text": "ending um receiving external traffic there are generally two ways inside of kubernetes to receive external traffic",
    "start": "1221280",
    "end": "1227520"
  },
  {
    "start": "1222000",
    "end": "1222000"
  },
  {
    "text": "at The L4 level or at the L7 level these are the ways that we reason about it um we're going to talk about L4 now Ingress",
    "start": "1227520",
    "end": "1233760"
  },
  {
    "text": "was actually talked about in a talk that Tim and I gave it next and it's we don't have enough time to talk about it today",
    "start": "1233760",
    "end": "1239480"
  },
  {
    "text": "uh if you want to know about it please check out ins and outs of kubernetes networking where it's described in detail but we're going to talk about the",
    "start": "1239480",
    "end": "1245240"
  },
  {
    "text": "service load balancer for The L4 work today oh there it is a neat title to talk about what I just said we're going",
    "start": "1245240",
    "end": "1251240"
  },
  {
    "text": "to talk about um here's the service we use the same yaml to create it but this time we specify load balancer what what",
    "start": "1251240",
    "end": "1258240"
  },
  {
    "text": "does that do that creates an external IP address visible to the outside world you",
    "start": "1258240",
    "end": "1263799"
  },
  {
    "text": "shouldn't have to do this yourself kubernetes sets this up automatically for you and the idea here is that it",
    "start": "1263799",
    "end": "1269280"
  },
  {
    "text": "interacts with the load balancer in the environment that you're in with kubernetes again kubernetes makes the",
    "start": "1269280",
    "end": "1275600"
  },
  {
    "text": "integration with the network environment easier and it should evenly balance traffic coming in from that load",
    "start": "1275600",
    "end": "1281960"
  },
  {
    "text": "balancer across the pods so let's take a look external to",
    "start": "1281960",
    "end": "1287039"
  },
  {
    "text": "service um The Source packet is coming from a",
    "start": "1287039",
    "end": "1292880"
  },
  {
    "text": "client somewhere in the internet it hits the load balancer the load balancer chooses a node it's going to choose one",
    "start": "1292880",
    "end": "1298039"
  },
  {
    "text": "with pods on it it chooses node one sends the packet to node",
    "start": "1298039",
    "end": "1303600"
  },
  {
    "text": "one but most load balancers only know about nodes they're not pod aware or",
    "start": "1303600",
    "end": "1309360"
  },
  {
    "start": "1304000",
    "end": "1304000"
  },
  {
    "text": "container aware and nodes don't map one to one with pods this creates some complexity I've heard that I think eBay",
    "start": "1309360",
    "end": "1316240"
  },
  {
    "text": "may have a container aware load balance but I think this is also emerging technology that we will learn more about",
    "start": "1316240",
    "end": "1322279"
  },
  {
    "text": "hopefully as the years go by um if we're assuming that load balancer Only Hits",
    "start": "1322279",
    "end": "1327520"
  },
  {
    "text": "nodes and it only knows about nodes well this is what we have we have an imbalance problem 50% of the traffic is",
    "start": "1327520",
    "end": "1333080"
  },
  {
    "text": "going to hit one node and 50% is going to hit the other node well that means that we're not",
    "start": "1333080",
    "end": "1338640"
  },
  {
    "text": "balancing things evenly anymore ah um the node three pods are going to get a",
    "start": "1338640",
    "end": "1344240"
  },
  {
    "text": "quarter of the traffic not half so how do we fix this IP tables",
    "start": "1344240",
    "end": "1349919"
  },
  {
    "text": "once again duct tape coming to the rescue um I always have this image that when I give these talks people assume my",
    "start": "1349919",
    "end": "1355200"
  },
  {
    "text": "house is like just covered in duct tape kind of is um so once again we choose a pod traffic",
    "start": "1355200",
    "end": "1364240"
  },
  {
    "start": "1360000",
    "end": "1360000"
  },
  {
    "text": "comes in and this is something that's a little tricky to understand even though the packet has hit node one it's not",
    "start": "1364240",
    "end": "1371039"
  },
  {
    "text": "really hitting the pod on node one the node is being used as part of the service um extraction to figure out",
    "start": "1371039",
    "end": "1377799"
  },
  {
    "text": "which which node is really the destination um that holds the",
    "start": "1377799",
    "end": "1383080"
  },
  {
    "text": "Pod so the IP tables will make a decision and say I want to pick the",
    "start": "1383080",
    "end": "1388360"
  },
  {
    "text": "random pod that this traffic should go to and it may not be on the same Noe it'll go to pod two it then sends the",
    "start": "1388360",
    "end": "1395400"
  },
  {
    "text": "traffic to pod two it's a hop pod two takes the traffic pulls it",
    "start": "1395400",
    "end": "1402000"
  },
  {
    "text": "out it's getting ready to send it back to the client that doesn't work because the client is going to go what this IP",
    "start": "1402000",
    "end": "1408480"
  },
  {
    "text": "address from pod 2 I don't know who that is and any strong smart client is going to say no no no no no I don't talk to",
    "start": "1408480",
    "end": "1414919"
  },
  {
    "text": "strangers so let's go back and see what we should do what we end up doing um the traffic hits IP tables it then picks pod",
    "start": "1414919",
    "end": "1422480"
  },
  {
    "text": "two and then pod two has to send it all the way back to node one for the IB",
    "start": "1422480",
    "end": "1428080"
  },
  {
    "text": "tables to do the dnat with contract find out the right client IP and send it out",
    "start": "1428080",
    "end": "1433360"
  },
  {
    "text": "this is cool because we preserve the balancing problem we solve that issue but but it's a trade-off we have a",
    "start": "1433360",
    "end": "1439720"
  },
  {
    "start": "1437000",
    "end": "1437000"
  },
  {
    "text": "double hop which incl in um induces induces latency who likes latency in their",
    "start": "1439720",
    "end": "1445919"
  },
  {
    "text": "Network traffic and then also it hides the client IPS from the users now this was sort of an interesting philosophical",
    "start": "1445919",
    "end": "1451880"
  },
  {
    "text": "problem that some users really really cared about balance some users really really cared about the client IP and",
    "start": "1451880",
    "end": "1458080"
  },
  {
    "text": "latency they wanted to make the trade-off themselves hence only local we've added an annotation to the service",
    "start": "1458080",
    "end": "1464200"
  },
  {
    "text": "to let the users specify what they feel is more important it preserves client IP and it may risk imbalance but it also um",
    "start": "1464200",
    "end": "1471720"
  },
  {
    "text": "removes the double hop how does that work well realize first you don't even have to deal with this if you have the",
    "start": "1471720",
    "end": "1477320"
  },
  {
    "start": "1473000",
    "end": "1473000"
  },
  {
    "text": "right distribution of PODS to nodes if you have many many many more pods than you've got nodes going to be fine the",
    "start": "1477320",
    "end": "1484080"
  },
  {
    "text": "imbalances are very very small because the Delta of the work um spread across them is minimal if you have more nodes",
    "start": "1484080",
    "end": "1490880"
  },
  {
    "text": "than pods you're also going to be fine because you're only going to have one pod per node because you've got more",
    "start": "1490880",
    "end": "1496640"
  },
  {
    "text": "nodes it's when you have situations like this when they're about the same which can happen especially when you have pods",
    "start": "1496640",
    "end": "1501919"
  },
  {
    "text": "coming up and coming down where things get confusing um so the network load",
    "start": "1501919",
    "end": "1508399"
  },
  {
    "text": "balancer then goes and checks to see which node to send the traffic to doesn't check node two because you",
    "start": "1508399",
    "end": "1513480"
  },
  {
    "text": "consider node two because it knows there's no pods there so the traffic comes down and we're going to pick node",
    "start": "1513480",
    "end": "1518640"
  },
  {
    "text": "three in this case because that's the interesting one and with only local enabled instead of the IP tables",
    "start": "1518640",
    "end": "1524919"
  },
  {
    "text": "possibly sending that traffic to node one it's going to say nope going to keep that traffic on this",
    "start": "1524919",
    "end": "1531360"
  },
  {
    "text": "node that's part of my routing decision and then it's going to pick one of the Pods at random going to pick pod two I",
    "start": "1531360",
    "end": "1537120"
  },
  {
    "text": "like pod two we never pick pod three it's kind of lonely and sad um we do the dnat comes back out we send it back out",
    "start": "1537120",
    "end": "1545320"
  },
  {
    "text": "to the client and away we go we don't have a double hop and it keeps and preserves the client IP",
    "start": "1545320",
    "end": "1550799"
  },
  {
    "text": "address so this allows the user to make these trade-off decisions we're going to end the talk",
    "start": "1550799",
    "end": "1557720"
  },
  {
    "text": "talk with the last uh subject which is Network policy a lot of um applications that",
    "start": "1557720",
    "end": "1565960"
  },
  {
    "start": "1563000",
    "end": "1563000"
  },
  {
    "text": "I've seen in kubernetes they end up having tiers of setup for their microservices they want to be able to",
    "start": "1565960",
    "end": "1571840"
  },
  {
    "text": "scale in different dimensions based on how they divide up the work and this is a pattern that I've seen even before",
    "start": "1571840",
    "end": "1577600"
  },
  {
    "text": "containers and it's actually really one of the wonderful places where I think um containers shine you have a front end",
    "start": "1577600",
    "end": "1583360"
  },
  {
    "text": "you have a back end and you have a database you wrap them all up in containers so that you can possibly have different environments and different",
    "start": "1583360",
    "end": "1588960"
  },
  {
    "text": "ways to scale them um often though you want to lock down the network you don't",
    "start": "1588960",
    "end": "1594840"
  },
  {
    "text": "want them all to communicate with each other you want to control the communication between",
    "start": "1594840",
    "end": "1600960"
  },
  {
    "text": "them you want then this could be for security concerns or this could just be for debugging concerns or mostly it's",
    "start": "1601120",
    "end": "1607279"
  },
  {
    "text": "the I want to know what's going on inside my system concern um a frontend in this case should never really be",
    "start": "1607279",
    "end": "1613279"
  },
  {
    "text": "talking to the database and a front end should never be talking to other front- ends the the other issue here and this",
    "start": "1613279",
    "end": "1618840"
  },
  {
    "text": "is where I think kubernetes is really really powerful is it's a dynamic system like we've described pods come up pods",
    "start": "1618840",
    "end": "1625039"
  },
  {
    "text": "come down pods hang nodes come up nodes come down we don't want the users to have to deal with that we want",
    "start": "1625039",
    "end": "1631480"
  },
  {
    "text": "kubernetes to just magically make sure that all of these rules and policies are preserved for you in this dynamic",
    "start": "1631480",
    "end": "1640200"
  },
  {
    "text": "system I'm going to introduce another concept kind of at the last minute here it's a little funky some people may know",
    "start": "1640200",
    "end": "1645520"
  },
  {
    "text": "about it already the idea is called the name space this is another one of those words in computer science that is used",
    "start": "1645520",
    "end": "1650720"
  },
  {
    "text": "all over the place it's heavily overloaded um it's used in C++ it's used in languages it's used it just it's a",
    "start": "1650720",
    "end": "1657279"
  },
  {
    "text": "good handle um the thing to remember is that most of the objects in kubernetes",
    "start": "1657279",
    "end": "1662360"
  },
  {
    "text": "can be namespaced and that means that they can kind of be just grouped together so that you can apply um",
    "start": "1662360",
    "end": "1668480"
  },
  {
    "text": "actions or policies on a group of them all at once so this is a nam space you",
    "start": "1668480",
    "end": "1674000"
  },
  {
    "text": "can put a bunch of pods in there you can put a bunch of services in there and almost all of the objects but not all",
    "start": "1674000",
    "end": "1679840"
  },
  {
    "text": "are namespaced and this is how you can create it inside when you create the object you can specify the namespace",
    "start": "1679840",
    "end": "1686279"
  },
  {
    "text": "with dasn and if you don't sometimes it can be put in the default namespace it's it's something to be aware",
    "start": "1686279",
    "end": "1693518"
  },
  {
    "text": "of um when you first create your set of PODS and we said this before but we",
    "start": "1693840",
    "end": "1699080"
  },
  {
    "text": "didn't really call it out exactly this explicitly the network policy is all of them can talk to each other remember",
    "start": "1699080",
    "end": "1704760"
  },
  {
    "text": "when we first talked about iPod per policy all of pods should be able to reach each other bir",
    "start": "1704760",
    "end": "1711440"
  },
  {
    "text": "directionally what we want to do here is describe the links of relationships between the pods that we want to allow",
    "start": "1711880",
    "end": "1718880"
  },
  {
    "text": "not the ones we want to disallow the ones we want to allow so here we want to say the front ends should talk to the",
    "start": "1718880",
    "end": "1724480"
  },
  {
    "text": "backends and the backends should be allowed to talk to the database no more no",
    "start": "1724480",
    "end": "1730360"
  },
  {
    "text": "less here's how you do that in yaml there's another object called the network policy you can you know create",
    "start": "1730360",
    "end": "1736760"
  },
  {
    "text": "the yaml and store this in the API server and here there's three things to know about the network policy object um",
    "start": "1736760",
    "end": "1743320"
  },
  {
    "text": "one of them well okay four or five um all right more but we're going to stick",
    "start": "1743320",
    "end": "1748399"
  },
  {
    "text": "with maybe five in this talk um one you want to say the pods that are the",
    "start": "1748399",
    "end": "1754120"
  },
  {
    "text": "subject of the policy and then you want to say the pods that are allowed to send",
    "start": "1754120",
    "end": "1760760"
  },
  {
    "text": "traffic to the subject of this policy just giving that a moment to",
    "start": "1760760",
    "end": "1768320"
  },
  {
    "text": "in who can talk to the subject of the you know the selector of the",
    "start": "1768320",
    "end": "1773679"
  },
  {
    "text": "policy and then on that incoming traffic what ports are allowed or you know one",
    "start": "1773679",
    "end": "1779559"
  },
  {
    "text": "port or set of ports now I didn't really write about it here and maybe for you know you don't have to think about this",
    "start": "1779559",
    "end": "1785200"
  },
  {
    "text": "that much but there's also the idea that you can say instead of the pods that can talk to the subject of the policy you",
    "start": "1785200",
    "end": "1790399"
  },
  {
    "text": "could even specify the name space that can talk to the set of these pods but if",
    "start": "1790399",
    "end": "1795480"
  },
  {
    "text": "you don't if that hurts your head don't worry about it you can look about it and think about it later the other thing to think about too is if you notice we",
    "start": "1795480",
    "end": "1801960"
  },
  {
    "text": "didn't say Services we focused on pods because we realize that we don't really gain that much when we add um a service",
    "start": "1801960",
    "end": "1808640"
  },
  {
    "text": "as part of the policy subject and that this this keeps things simple and",
    "start": "1808640",
    "end": "1814360"
  },
  {
    "text": "consistent also these policies are per name space so you have to associate the",
    "start": "1814360",
    "end": "1821320"
  },
  {
    "text": "policy with a namespace it could be the default namespace or it could be one that you've specified already when you put your pods into that name",
    "start": "1821320",
    "end": "1827919"
  },
  {
    "text": "space so now we've created our policy we've described our",
    "start": "1827919",
    "end": "1833080"
  },
  {
    "text": "links now what you need to do next is take the set of pods in a name space and",
    "start": "1833399",
    "end": "1839120"
  },
  {
    "text": "say default deny isolate the traffic nobody can talk to anybody",
    "start": "1839120",
    "end": "1846080"
  },
  {
    "text": "anymore you have to turn off the ability to have the traffic reach each other now if you have a policy",
    "start": "1846279",
    "end": "1853159"
  },
  {
    "text": "associated with that Nam space some magic happens",
    "start": "1853159",
    "end": "1859120"
  },
  {
    "text": "here's how we set default deny and now assume that we've already made a network policy associated with that name space",
    "start": "1859120",
    "end": "1865679"
  },
  {
    "text": " it will apply that policy to allow some holes in that default deny the",
    "start": "1865679",
    "end": "1871760"
  },
  {
    "text": "default is no traffic but with the policy it says oh wait I'll let this traffic",
    "start": "1871760",
    "end": "1878440"
  },
  {
    "text": "occur now realize too if you didn't specify the name space correctly on your policy and you do the default deny you",
    "start": "1879600",
    "end": "1886360"
  },
  {
    "text": "have basically worked all of your pod communication with each other I highly highly recommend from my own experience",
    "start": "1886360",
    "end": "1892760"
  },
  {
    "text": "do not do this in a production system for the first time especially if like you put your",
    "start": "1892760",
    "end": "1899360"
  },
  {
    "text": "family's photo albums inside your kubernetes cluster it's just not a good idea um the other thing to remember",
    "start": "1899360",
    "end": "1905240"
  },
  {
    "text": "about Network policy is it's additive I kind of cheated a little bit you set one set of pod selector at a time inside",
    "start": "1905240",
    "end": "1910679"
  },
  {
    "text": "your policy and then you can stack them up on top of each other and there's no way to specify deny in network policy um",
    "start": "1910679",
    "end": "1916760"
  },
  {
    "text": "we don't want want to reimplement IP tables in yaml you know that you shouldn't have to do that this is also",
    "start": "1916760",
    "end": "1923399"
  },
  {
    "text": "implemented at the L3 and L4 level not L7 um that said there's a company I",
    "start": "1923399",
    "end": "1929320"
  },
  {
    "text": "think coent that I saw downstairs and I think um igera and I I there's a lot of",
    "start": "1929320",
    "end": "1934440"
  },
  {
    "text": "other providers and companies that are interested in the Sig networking right now at making L7 policy occur so I",
    "start": "1934440",
    "end": "1941159"
  },
  {
    "text": "wouldn't be surprised um reasonably soon if you start seeing proposals to make Network policy work at the L7 level It's",
    "start": "1941159",
    "end": "1947679"
  },
  {
    "text": "Tricky though for a variety of reasons also this is where kubernetes",
    "start": "1947679",
    "end": "1952960"
  },
  {
    "start": "1951000",
    "end": "1951000"
  },
  {
    "text": "shines you have more options than one could possibly imagine to implement this",
    "start": "1952960",
    "end": "1959080"
  },
  {
    "text": "policy like by default you have the API but I mean you've got Calico con open",
    "start": "1959080",
    "end": "1964919"
  },
  {
    "text": "shift Romana trene weet I think I found two more downstairs I didn't know about",
    "start": "1964919",
    "end": "1970200"
  },
  {
    "text": "yesterday um for some reason this is like a really cool systems problem that like lots of systems Engineers want to",
    "start": "1970200",
    "end": "1975799"
  },
  {
    "text": "get into and they have a lot of extra flavor that they can provide outside of the basic API in order to make this work",
    "start": "1975799",
    "end": "1983039"
  },
  {
    "text": "and so it's a very neat crowded space and as a user it's wonderful you can pick many different options to employ in",
    "start": "1983039",
    "end": "1989519"
  },
  {
    "text": "your kubernetes cluster for implementing the policy and then other added functionality that you know you might be",
    "start": "1989519",
    "end": "1995200"
  },
  {
    "text": "able to add to it and um you know create value this is beta in 1.6 and expected",
    "start": "1995200",
    "end": "2001240"
  },
  {
    "text": "to be GA in 1.7 fingers crossed um that's pretty much it um uh",
    "start": "2001240",
    "end": "2007399"
  },
  {
    "text": "you know networking in kubernetes is a moving Target there's a lot of work going on the open source it's kind of",
    "start": "2007399",
    "end": "2013919"
  },
  {
    "text": "amazing the coordination that the Sig has created and the the simple interface and The Primitives that allow us to",
    "start": "2013919",
    "end": "2020519"
  },
  {
    "text": "implement all of this functionality and what I really like about it is that it",
    "start": "2020519",
    "end": "2025880"
  },
  {
    "text": "often just works um there's a whole bunch of people who don't know a lot about networking but are able to really leverage the system in order to get a",
    "start": "2025880",
    "end": "2033120"
  },
  {
    "text": "lot of value and you know it's good um we will probably hopefully in the next",
    "start": "2033120",
    "end": "2038760"
  },
  {
    "text": "cubec con have more to talk about with what's going on in networking and I really appreciate your time and want to say thank you um kubernetes is open and",
    "start": "2038760",
    "end": "2046519"
  },
  {
    "text": "this wouldn't happen without all of you in the audience and all of the other people contributing it it's it's magic",
    "start": "2046519",
    "end": "2052520"
  },
  {
    "text": "it's very very cool thank you very",
    "start": "2052520",
    "end": "2056320"
  },
  {
    "text": "much",
    "start": "2057800",
    "end": "2060800"
  }
]