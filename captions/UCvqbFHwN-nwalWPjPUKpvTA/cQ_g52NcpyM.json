[
  {
    "start": "0",
    "end": "28000"
  },
  {
    "text": "hello and welcome to this tech demo my",
    "start": "399",
    "end": "3120"
  },
  {
    "text": "name is joshua moody",
    "start": "3120",
    "end": "4319"
  },
  {
    "text": "i'm a staff software engineer working",
    "start": "4319",
    "end": "6240"
  },
  {
    "text": "for venturelabs which is now a part of",
    "start": "6240",
    "end": "8080"
  },
  {
    "text": "souza",
    "start": "8080",
    "end": "9360"
  },
  {
    "text": "i work on project longhorn longhorn is a",
    "start": "9360",
    "end": "12639"
  },
  {
    "text": "distributed plug storage engine",
    "start": "12639",
    "end": "14320"
  },
  {
    "text": "built for and on kubernetes in today's",
    "start": "14320",
    "end": "17520"
  },
  {
    "text": "tech demo",
    "start": "17520",
    "end": "18560"
  },
  {
    "text": "i'm going to show you why you would want",
    "start": "18560",
    "end": "20880"
  },
  {
    "text": "to use longhorns persistent storage",
    "start": "20880",
    "end": "23439"
  },
  {
    "text": "for your edge deployments to make sure",
    "start": "23439",
    "end": "25680"
  },
  {
    "text": "that your data is safe",
    "start": "25680",
    "end": "28800"
  },
  {
    "start": "28000",
    "end": "126000"
  },
  {
    "text": "let's give a quick overview of how long",
    "start": "28800",
    "end": "30560"
  },
  {
    "text": "one works",
    "start": "30560",
    "end": "31840"
  },
  {
    "text": "long conceptually consists of two",
    "start": "31840",
    "end": "33760"
  },
  {
    "text": "components",
    "start": "33760",
    "end": "34960"
  },
  {
    "text": "the control plane which we can see on",
    "start": "34960",
    "end": "36640"
  },
  {
    "text": "this slide",
    "start": "36640",
    "end": "38000"
  },
  {
    "text": "as well as the data plane which we can",
    "start": "38000",
    "end": "39600"
  },
  {
    "text": "see on the following slide",
    "start": "39600",
    "end": "41360"
  },
  {
    "text": "the main component for longhorn's",
    "start": "41360",
    "end": "42879"
  },
  {
    "text": "control plane is the longhorn manager",
    "start": "42879",
    "end": "44719"
  },
  {
    "text": "it's responsible for orchestration of",
    "start": "44719",
    "end": "46480"
  },
  {
    "text": "all volumes",
    "start": "46480",
    "end": "48000"
  },
  {
    "text": "creation of backups in initiation of",
    "start": "48000",
    "end": "50480"
  },
  {
    "text": "restores etc",
    "start": "50480",
    "end": "52719"
  },
  {
    "text": "we also have two auxiliary components",
    "start": "52719",
    "end": "54719"
  },
  {
    "text": "which is the lawrence's eye plugin",
    "start": "54719",
    "end": "56559"
  },
  {
    "text": "as well as the logon ui both of these",
    "start": "56559",
    "end": "58559"
  },
  {
    "text": "interface we are with the manager via",
    "start": "58559",
    "end": "60320"
  },
  {
    "text": "the longhorn api",
    "start": "60320",
    "end": "63280"
  },
  {
    "text": "the login csi plugin is acts as a bridge",
    "start": "63520",
    "end": "66159"
  },
  {
    "text": "between kubernetes and longhorn",
    "start": "66159",
    "end": "68240"
  },
  {
    "text": "manager this way kubernetes can request",
    "start": "68240",
    "end": "71200"
  },
  {
    "text": "volume provisioning as well as",
    "start": "71200",
    "end": "73200"
  },
  {
    "text": "attachment",
    "start": "73200",
    "end": "73840"
  },
  {
    "text": "creation of snapshots etc",
    "start": "73840",
    "end": "77439"
  },
  {
    "text": "the long data plane consists of two",
    "start": "77439",
    "end": "79600"
  },
  {
    "text": "parts",
    "start": "79600",
    "end": "80799"
  },
  {
    "text": "the longhorn engine which you can think",
    "start": "80799",
    "end": "82640"
  },
  {
    "text": "of as the volume controller",
    "start": "82640",
    "end": "84320"
  },
  {
    "text": "it's responsible for processing all i o",
    "start": "84320",
    "end": "86720"
  },
  {
    "text": "requests",
    "start": "86720",
    "end": "87680"
  },
  {
    "text": "as well as communication with the",
    "start": "87680",
    "end": "89200"
  },
  {
    "text": "replicas which you can think of as the",
    "start": "89200",
    "end": "91600"
  },
  {
    "text": "volume's data",
    "start": "91600",
    "end": "93280"
  },
  {
    "text": "here on the slide you can see two",
    "start": "93280",
    "end": "95680"
  },
  {
    "text": "workloads",
    "start": "95680",
    "end": "96799"
  },
  {
    "text": "where each one having their own volume",
    "start": "96799",
    "end": "99920"
  },
  {
    "text": "each volume",
    "start": "99920",
    "end": "100799"
  },
  {
    "text": "has only one engine and two replicas",
    "start": "100799",
    "end": "105119"
  },
  {
    "text": "per volume the amount of vapor cost per",
    "start": "105119",
    "end": "107840"
  },
  {
    "text": "volume is configurable",
    "start": "107840",
    "end": "109040"
  },
  {
    "text": "each volume can have a different amount",
    "start": "109040",
    "end": "110880"
  },
  {
    "text": "depending on your application workload's",
    "start": "110880",
    "end": "112880"
  },
  {
    "text": "requirements",
    "start": "112880",
    "end": "114560"
  },
  {
    "text": "you can also see that the replicas got",
    "start": "114560",
    "end": "117600"
  },
  {
    "text": "randomly scheduled onto different nodes",
    "start": "117600",
    "end": "120640"
  },
  {
    "text": "this allows for the case where one of",
    "start": "120640",
    "end": "122560"
  },
  {
    "text": "your nodes fails",
    "start": "122560",
    "end": "123840"
  },
  {
    "text": "that your data is still safe if you look",
    "start": "123840",
    "end": "127040"
  },
  {
    "start": "126000",
    "end": "180000"
  },
  {
    "text": "on the left",
    "start": "127040",
    "end": "127840"
  },
  {
    "text": "you can see my messy test setup if you",
    "start": "127840",
    "end": "130720"
  },
  {
    "text": "want to replicate this",
    "start": "130720",
    "end": "131920"
  },
  {
    "text": "you will need four raspberry pi's i use",
    "start": "131920",
    "end": "134640"
  },
  {
    "text": "the four gigabyte version",
    "start": "134640",
    "end": "136959"
  },
  {
    "text": "for 128 gigabyte sd card or bigger",
    "start": "136959",
    "end": "140239"
  },
  {
    "text": "to provide storage to the operating",
    "start": "140239",
    "end": "142480"
  },
  {
    "text": "system as well as the cluster",
    "start": "142480",
    "end": "145120"
  },
  {
    "text": "afterwards you need to install a 64-bit",
    "start": "145120",
    "end": "147440"
  },
  {
    "text": "arm operating system",
    "start": "147440",
    "end": "148640"
  },
  {
    "text": "in my case i use support server if you",
    "start": "148640",
    "end": "151280"
  },
  {
    "text": "want to",
    "start": "151280",
    "end": "152319"
  },
  {
    "text": "use rwx volumes you need to ensure",
    "start": "152319",
    "end": "155280"
  },
  {
    "text": "availability of an nfs4 client",
    "start": "155280",
    "end": "158239"
  },
  {
    "text": "you can do this on ubuntu by installing",
    "start": "158239",
    "end": "160400"
  },
  {
    "text": "nfs comments",
    "start": "160400",
    "end": "161599"
  },
  {
    "text": "for an operating system there's",
    "start": "161599",
    "end": "163040"
  },
  {
    "text": "alternative packages",
    "start": "163040",
    "end": "165760"
  },
  {
    "text": "once your pipes are prepared you",
    "start": "165760",
    "end": "168959"
  },
  {
    "text": "need to install k3s on each of them to",
    "start": "168959",
    "end": "171040"
  },
  {
    "text": "form a cluster",
    "start": "171040",
    "end": "173440"
  },
  {
    "text": "at this point you can optionally add",
    "start": "173440",
    "end": "175040"
  },
  {
    "text": "that cluster to your existing venture",
    "start": "175040",
    "end": "176720"
  },
  {
    "text": "installation",
    "start": "176720",
    "end": "179040"
  },
  {
    "text": "let's start with the tag demo we start",
    "start": "179040",
    "end": "181680"
  },
  {
    "start": "180000",
    "end": "261000"
  },
  {
    "text": "by logging into rancho",
    "start": "181680",
    "end": "185840"
  },
  {
    "text": "here you can see my existing clusters",
    "start": "186159",
    "end": "189519"
  },
  {
    "text": "for the tech demo we're going to use the",
    "start": "189519",
    "end": "190959"
  },
  {
    "text": "k3 as ubuntu edge cluster",
    "start": "190959",
    "end": "194560"
  },
  {
    "text": "let's have a look at the notes these are",
    "start": "195200",
    "end": "197840"
  },
  {
    "text": "my four recipe pies",
    "start": "197840",
    "end": "200239"
  },
  {
    "text": "and you can verify that your clusters",
    "start": "200239",
    "end": "202239"
  },
  {
    "text": "are working correctly by checking the",
    "start": "202239",
    "end": "203920"
  },
  {
    "text": "system information",
    "start": "203920",
    "end": "206720"
  },
  {
    "text": "now that we verified the clusters",
    "start": "207040",
    "end": "208640"
  },
  {
    "text": "working correctly let's start by",
    "start": "208640",
    "end": "210879"
  },
  {
    "text": "installing longhorn onto it",
    "start": "210879",
    "end": "214400"
  },
  {
    "text": "we can use rancher to deploy",
    "start": "215760",
    "end": "220159"
  },
  {
    "text": "the longhorn app from the catalog",
    "start": "220159",
    "end": "227840"
  },
  {
    "text": "now that long is up and running we can",
    "start": "253280",
    "end": "256239"
  },
  {
    "text": "have a look at the longer ui",
    "start": "256239",
    "end": "259680"
  },
  {
    "start": "261000",
    "end": "342000"
  },
  {
    "text": "for this cluster we want to configure a",
    "start": "262000",
    "end": "265120"
  },
  {
    "text": "specific setting",
    "start": "265120",
    "end": "266720"
  },
  {
    "text": "that allows longhorn to delete parts on",
    "start": "266720",
    "end": "270000"
  },
  {
    "text": "failed nodes by default kubernetes will",
    "start": "270000",
    "end": "274080"
  },
  {
    "text": "not",
    "start": "274080",
    "end": "274720"
  },
  {
    "text": "delete parts on failed nodes since it's",
    "start": "274720",
    "end": "276960"
  },
  {
    "text": "the responsibility of cubelet to do so",
    "start": "276960",
    "end": "279919"
  },
  {
    "text": "but if the node is down cubed isn't",
    "start": "279919",
    "end": "281919"
  },
  {
    "text": "running so it cannot delete the part",
    "start": "281919",
    "end": "283520"
  },
  {
    "text": "from the api server",
    "start": "283520",
    "end": "284800"
  },
  {
    "text": "which means the part is stuck and any",
    "start": "284800",
    "end": "288080"
  },
  {
    "text": "volume that's used by the port is stuck",
    "start": "288080",
    "end": "289919"
  },
  {
    "text": "as well",
    "start": "289919",
    "end": "292000"
  },
  {
    "text": "so in longhorn we provide options for",
    "start": "292000",
    "end": "294160"
  },
  {
    "text": "deleting the stateful set parts",
    "start": "294160",
    "end": "295840"
  },
  {
    "text": "deployment ports or both in this case",
    "start": "295840",
    "end": "299040"
  },
  {
    "text": "we're going to choose both",
    "start": "299040",
    "end": "302000"
  },
  {
    "text": "besides the part deletion we have a",
    "start": "302240",
    "end": "304240"
  },
  {
    "text": "prior implementation that is less",
    "start": "304240",
    "end": "306240"
  },
  {
    "text": "aggressive",
    "start": "306240",
    "end": "307280"
  },
  {
    "text": "but only applies to deployments",
    "start": "307280",
    "end": "310960"
  },
  {
    "text": "in the volume attachment recovery policy",
    "start": "310960",
    "end": "313199"
  },
  {
    "text": "implementation",
    "start": "313199",
    "end": "314160"
  },
  {
    "text": "we leave the failed port on the api",
    "start": "314160",
    "end": "316320"
  },
  {
    "text": "server but only clean up the kubernetes",
    "start": "316320",
    "end": "318720"
  },
  {
    "text": "volume attachment object",
    "start": "318720",
    "end": "320880"
  },
  {
    "text": "science on this cluster we enable both",
    "start": "320880",
    "end": "323600"
  },
  {
    "text": "deletion for stateful set as well as",
    "start": "323600",
    "end": "325280"
  },
  {
    "text": "deployment ports",
    "start": "325280",
    "end": "326479"
  },
  {
    "text": "we don't need this so we can disable",
    "start": "326479",
    "end": "328479"
  },
  {
    "text": "this feature",
    "start": "328479",
    "end": "330960"
  },
  {
    "text": "now that we configured our settings",
    "start": "334240",
    "end": "336720"
  },
  {
    "text": "let's go back to rancho",
    "start": "336720",
    "end": "338720"
  },
  {
    "text": "to deploy our application workloads",
    "start": "338720",
    "end": "342560"
  },
  {
    "start": "342000",
    "end": "422000"
  },
  {
    "text": "let's start by deploying our data",
    "start": "342560",
    "end": "344400"
  },
  {
    "text": "aggregator workload",
    "start": "344400",
    "end": "346000"
  },
  {
    "text": "you can do this by using cubectl to",
    "start": "346000",
    "end": "349039"
  },
  {
    "text": "deploy the yaml file",
    "start": "349039",
    "end": "350880"
  },
  {
    "text": "or using wrenches import yaml feature",
    "start": "350880",
    "end": "353919"
  },
  {
    "text": "for this example we're going to use",
    "start": "353919",
    "end": "355600"
  },
  {
    "text": "renshaw's import yaml feature",
    "start": "355600",
    "end": "359199"
  },
  {
    "text": "the data creator consists of a service",
    "start": "360639",
    "end": "365039"
  },
  {
    "text": "persistent volume in this case we only",
    "start": "365680",
    "end": "368240"
  },
  {
    "text": "need a single instance of this",
    "start": "368240",
    "end": "369840"
  },
  {
    "text": "data aggregator if you needed multiple",
    "start": "369840",
    "end": "372479"
  },
  {
    "text": "instances",
    "start": "372479",
    "end": "373280"
  },
  {
    "text": "you would use a persistent volume claim",
    "start": "373280",
    "end": "375039"
  },
  {
    "text": "template as part of the",
    "start": "375039",
    "end": "377160"
  },
  {
    "text": "specification for the stateful set",
    "start": "377160",
    "end": "381840"
  },
  {
    "text": "the only interesting aspect in this",
    "start": "381919",
    "end": "385039"
  },
  {
    "text": "data aggregator configuration is that we",
    "start": "385039",
    "end": "387919"
  },
  {
    "text": "set the termination craze period",
    "start": "387919",
    "end": "389440"
  },
  {
    "text": "to 10 seconds as well as overwrite the",
    "start": "389440",
    "end": "392160"
  },
  {
    "text": "default tolerations of kubernetes that",
    "start": "392160",
    "end": "394319"
  },
  {
    "text": "community sets",
    "start": "394319",
    "end": "395840"
  },
  {
    "text": "to allow for faster failover but a fault",
    "start": "395840",
    "end": "398639"
  },
  {
    "text": "community sets these tolerations at 5",
    "start": "398639",
    "end": "400639"
  },
  {
    "text": "minutes which means",
    "start": "400639",
    "end": "402400"
  },
  {
    "text": "a part will not be evicted for 5 minutes",
    "start": "402400",
    "end": "405840"
  },
  {
    "text": "from a failed node you can also",
    "start": "405840",
    "end": "409039"
  },
  {
    "text": "configure these",
    "start": "409039",
    "end": "410319"
  },
  {
    "text": "defaults as part of the cube api server",
    "start": "410319",
    "end": "413520"
  },
  {
    "text": "command line",
    "start": "413520",
    "end": "415199"
  },
  {
    "text": "or during the k3s installation",
    "start": "415199",
    "end": "422800"
  },
  {
    "start": "422000",
    "end": "464000"
  },
  {
    "text": "let's wait for the data aggregator to be",
    "start": "422800",
    "end": "425039"
  },
  {
    "text": "up and running",
    "start": "425039",
    "end": "427759"
  },
  {
    "text": "we can see that the volume has already",
    "start": "427919",
    "end": "429840"
  },
  {
    "text": "been provisioned",
    "start": "429840",
    "end": "432720"
  },
  {
    "text": "and we can see that the data aggregator",
    "start": "435120",
    "end": "437199"
  },
  {
    "text": "is now up and running",
    "start": "437199",
    "end": "439840"
  },
  {
    "text": "let's continue by deploying our data",
    "start": "439840",
    "end": "441919"
  },
  {
    "text": "collector workload",
    "start": "441919",
    "end": "444720"
  },
  {
    "text": "the data collector is a stateless demon",
    "start": "444720",
    "end": "447039"
  },
  {
    "text": "set that runs on each of the raspberry",
    "start": "447039",
    "end": "449039"
  },
  {
    "text": "pi worker nodes",
    "start": "449039",
    "end": "450639"
  },
  {
    "text": "and is responsible for collecting sensor",
    "start": "450639",
    "end": "452720"
  },
  {
    "text": "data",
    "start": "452720",
    "end": "453680"
  },
  {
    "text": "then transmitting that sensor data to",
    "start": "453680",
    "end": "455280"
  },
  {
    "text": "the data aggregator service",
    "start": "455280",
    "end": "457120"
  },
  {
    "text": "where it can be processed and stored",
    "start": "457120",
    "end": "464400"
  },
  {
    "start": "464000",
    "end": "585000"
  },
  {
    "text": "now that both our application workloads",
    "start": "464400",
    "end": "466080"
  },
  {
    "text": "are deployed let's take a minute to",
    "start": "466080",
    "end": "467840"
  },
  {
    "text": "discuss",
    "start": "467840",
    "end": "468400"
  },
  {
    "text": "why one would want to use persistent",
    "start": "468400",
    "end": "469919"
  },
  {
    "text": "storage for the data aggregate to",
    "start": "469919",
    "end": "471520"
  },
  {
    "text": "service",
    "start": "471520",
    "end": "473440"
  },
  {
    "text": "in the case where one uses local storage",
    "start": "473440",
    "end": "476319"
  },
  {
    "text": "should the worker node where the data",
    "start": "476319",
    "end": "478080"
  },
  {
    "text": "aggregator is deployed fail for whatever",
    "start": "478080",
    "end": "481120"
  },
  {
    "text": "reason",
    "start": "481120",
    "end": "482160"
  },
  {
    "text": "one would lose the data that has",
    "start": "482160",
    "end": "484160"
  },
  {
    "text": "currently been stored",
    "start": "484160",
    "end": "486800"
  },
  {
    "text": "in the case of longhorn since we use",
    "start": "486800",
    "end": "489440"
  },
  {
    "text": "three replicas by default",
    "start": "489440",
    "end": "491120"
  },
  {
    "text": "with heart anti-affinity each replica is",
    "start": "491120",
    "end": "494400"
  },
  {
    "text": "scheduled on a different",
    "start": "494400",
    "end": "495919"
  },
  {
    "text": "worker node we would still be able to",
    "start": "495919",
    "end": "499120"
  },
  {
    "text": "just reschedule the data aggregator",
    "start": "499120",
    "end": "501120"
  },
  {
    "text": "service",
    "start": "501120",
    "end": "501759"
  },
  {
    "text": "onto a different worker node and",
    "start": "501759",
    "end": "503759"
  },
  {
    "text": "continue processing the priorly existing",
    "start": "503759",
    "end": "505919"
  },
  {
    "text": "data",
    "start": "505919",
    "end": "506639"
  },
  {
    "text": "without any loss of data",
    "start": "506639",
    "end": "510080"
  },
  {
    "text": "let's show this in action we start by",
    "start": "510479",
    "end": "512800"
  },
  {
    "text": "identifying the worker node where the",
    "start": "512800",
    "end": "514479"
  },
  {
    "text": "data",
    "start": "514479",
    "end": "514959"
  },
  {
    "text": "aggregate is currently deployed then",
    "start": "514959",
    "end": "518080"
  },
  {
    "text": "we're going to",
    "start": "518080",
    "end": "518719"
  },
  {
    "text": "ssh into the node to turn it off",
    "start": "518719",
    "end": "522560"
  },
  {
    "text": "let's wait for node to get marked",
    "start": "530080",
    "end": "535200"
  },
  {
    "text": "the time that is required for node to",
    "start": "535200",
    "end": "537440"
  },
  {
    "text": "get mocked",
    "start": "537440",
    "end": "538240"
  },
  {
    "text": "has failed it can be configured",
    "start": "538240",
    "end": "541600"
  },
  {
    "text": "by setting the node monitoring interval",
    "start": "541600",
    "end": "544320"
  },
  {
    "text": "as well as the cubelet post status",
    "start": "544320",
    "end": "546720"
  },
  {
    "text": "you can do this during the kts",
    "start": "546720",
    "end": "548880"
  },
  {
    "text": "installation",
    "start": "548880",
    "end": "551120"
  },
  {
    "text": "now that the worker node has been marked",
    "start": "551120",
    "end": "552720"
  },
  {
    "text": "as failed let's have a look at our",
    "start": "552720",
    "end": "555040"
  },
  {
    "text": "deployment",
    "start": "555040",
    "end": "557600"
  },
  {
    "text": "currently the data aggregator is still",
    "start": "559040",
    "end": "561120"
  },
  {
    "text": "on unknown status",
    "start": "561120",
    "end": "562800"
  },
  {
    "text": "because we're still within the",
    "start": "562800",
    "end": "564560"
  },
  {
    "text": "toleration seconds",
    "start": "564560",
    "end": "566800"
  },
  {
    "text": "once that aggregator gets marked for",
    "start": "566800",
    "end": "569279"
  },
  {
    "text": "eviction",
    "start": "569279",
    "end": "571360"
  },
  {
    "text": "longhorn will go and delete false delete",
    "start": "571360",
    "end": "574480"
  },
  {
    "text": "the part",
    "start": "574480",
    "end": "575680"
  },
  {
    "text": "which allows to be the part to be",
    "start": "575680",
    "end": "577360"
  },
  {
    "text": "rescheduled onto a different worker node",
    "start": "577360",
    "end": "579760"
  },
  {
    "text": "now the data aggregator is up and",
    "start": "579760",
    "end": "581519"
  },
  {
    "text": "running on k2s worker 3",
    "start": "581519",
    "end": "583760"
  },
  {
    "text": "let's have a look at the loan volume we",
    "start": "583760",
    "end": "586399"
  },
  {
    "start": "585000",
    "end": "741000"
  },
  {
    "text": "can see that the status is now degraded",
    "start": "586399",
    "end": "589200"
  },
  {
    "text": "this is because we have a failed replica",
    "start": "589200",
    "end": "591920"
  },
  {
    "text": "on worker world",
    "start": "591920",
    "end": "592880"
  },
  {
    "text": "node 1. if in the case where our cluster",
    "start": "592880",
    "end": "595920"
  },
  {
    "text": "would be bigger",
    "start": "595920",
    "end": "597120"
  },
  {
    "text": "longhorn would now go and rebuild",
    "start": "597120",
    "end": "599200"
  },
  {
    "text": "another replica",
    "start": "599200",
    "end": "600160"
  },
  {
    "text": "on a different work node but since we",
    "start": "600160",
    "end": "602160"
  },
  {
    "text": "only have three worker nodes in this",
    "start": "602160",
    "end": "603360"
  },
  {
    "text": "cluster",
    "start": "603360",
    "end": "604000"
  },
  {
    "text": "and we have part on the affinity we will",
    "start": "604000",
    "end": "606640"
  },
  {
    "text": "run integrated status with only two",
    "start": "606640",
    "end": "609120"
  },
  {
    "text": "replicas let's recap as you have seen",
    "start": "609120",
    "end": "613360"
  },
  {
    "text": "a single failure of a raspberry pi did",
    "start": "613360",
    "end": "616240"
  },
  {
    "text": "not lead to any data loss",
    "start": "616240",
    "end": "619279"
  },
  {
    "text": "besides that you also seen that longhorn",
    "start": "619279",
    "end": "622640"
  },
  {
    "text": "cleaned up the workload port",
    "start": "622640",
    "end": "624480"
  },
  {
    "text": "so that kubernetes can reschedule it",
    "start": "624480",
    "end": "626240"
  },
  {
    "text": "onto a different worker node",
    "start": "626240",
    "end": "629200"
  },
  {
    "text": "had you used local storage you would",
    "start": "629200",
    "end": "632320"
  },
  {
    "text": "have lost your data",
    "start": "632320",
    "end": "634480"
  },
  {
    "text": "and your data aggregator would have been",
    "start": "634480",
    "end": "636560"
  },
  {
    "text": "stuck",
    "start": "636560",
    "end": "638160"
  },
  {
    "text": "let's make this even better by setting",
    "start": "638160",
    "end": "641120"
  },
  {
    "text": "up a recurring backup schedule for the",
    "start": "641120",
    "end": "642959"
  },
  {
    "text": "volume",
    "start": "642959",
    "end": "644480"
  },
  {
    "text": "we can ensure that even in the case",
    "start": "644480",
    "end": "648560"
  },
  {
    "text": "where the whole cluster dies we will not",
    "start": "648560",
    "end": "651519"
  },
  {
    "text": "lose any data",
    "start": "651519",
    "end": "652560"
  },
  {
    "text": "because we can restore from our backup",
    "start": "652560",
    "end": "654560"
  },
  {
    "text": "on a different cluster",
    "start": "654560",
    "end": "657600"
  },
  {
    "text": "in the background you can notice i have",
    "start": "657600",
    "end": "659200"
  },
  {
    "text": "replaced the faulty power",
    "start": "659200",
    "end": "660720"
  },
  {
    "text": "which allowed longhorn to rebuild one of",
    "start": "660720",
    "end": "662320"
  },
  {
    "text": "the volume rubber cars",
    "start": "662320",
    "end": "663920"
  },
  {
    "text": "and transition the volume back into a",
    "start": "663920",
    "end": "665519"
  },
  {
    "text": "healthy state",
    "start": "665519",
    "end": "668079"
  },
  {
    "text": "let's continue by setting up our backup",
    "start": "668079",
    "end": "670000"
  },
  {
    "text": "target lauren supports any s3 compatible",
    "start": "670000",
    "end": "672800"
  },
  {
    "text": "endpoint",
    "start": "672800",
    "end": "673440"
  },
  {
    "text": "as well as an nfs4 server for this demo",
    "start": "673440",
    "end": "676480"
  },
  {
    "text": "we're going to use amazon s3",
    "start": "676480",
    "end": "679920"
  },
  {
    "text": "we set the s3 endpoint as well as the",
    "start": "681200",
    "end": "684480"
  },
  {
    "text": "previously created s3 secret",
    "start": "684480",
    "end": "689839"
  },
  {
    "text": "we can now set up the recurring backup",
    "start": "691839",
    "end": "693519"
  },
  {
    "text": "schedule for this volume",
    "start": "693519",
    "end": "696720"
  },
  {
    "text": "for the schedule we want to take it back",
    "start": "701360",
    "end": "703279"
  },
  {
    "text": "up daily",
    "start": "703279",
    "end": "705920"
  },
  {
    "text": "and we want to retain seven backups so",
    "start": "708000",
    "end": "710079"
  },
  {
    "text": "that we have backups for the whole week",
    "start": "710079",
    "end": "713839"
  },
  {
    "text": "to start off we're to kick off a manual",
    "start": "714000",
    "end": "716000"
  },
  {
    "text": "backup",
    "start": "716000",
    "end": "718399"
  },
  {
    "text": "we're going to add a custom label",
    "start": "719120",
    "end": "722560"
  },
  {
    "text": "you can use labels to categorize",
    "start": "724399",
    "end": "726639"
  },
  {
    "text": "different backups and",
    "start": "726639",
    "end": "728399"
  },
  {
    "text": "differentiate between them",
    "start": "728399",
    "end": "731839"
  },
  {
    "text": "we can see the backups in the progress",
    "start": "733440",
    "end": "735120"
  },
  {
    "text": "of being created",
    "start": "735120",
    "end": "736800"
  },
  {
    "text": "let's take a look at our backup store",
    "start": "736800",
    "end": "740560"
  },
  {
    "text": "you can see that i share this backup",
    "start": "742000",
    "end": "744160"
  },
  {
    "text": "store with different cluster we already",
    "start": "744160",
    "end": "745920"
  },
  {
    "text": "have",
    "start": "745920",
    "end": "746480"
  },
  {
    "text": "volume backups for different volumes",
    "start": "746480",
    "end": "750240"
  },
  {
    "text": "let's have a look for our volume and the",
    "start": "750320",
    "end": "751920"
  },
  {
    "text": "backup that we created",
    "start": "751920",
    "end": "754959"
  },
  {
    "text": "we can manually restore this backup",
    "start": "756560",
    "end": "759600"
  },
  {
    "text": "to new volume inside of this cluster",
    "start": "759600",
    "end": "763440"
  },
  {
    "text": "if the original volume would have been",
    "start": "763440",
    "end": "765360"
  },
  {
    "text": "lost we could also",
    "start": "765360",
    "end": "766959"
  },
  {
    "text": "recreate it",
    "start": "766959",
    "end": "773839"
  },
  {
    "text": "as you can see the volume is currently",
    "start": "777760",
    "end": "779440"
  },
  {
    "text": "not ready for workloads",
    "start": "779440",
    "end": "781120"
  },
  {
    "text": "because it's in the process of",
    "start": "781120",
    "end": "784399"
  },
  {
    "text": "restoration",
    "start": "784839",
    "end": "787200"
  },
  {
    "text": "we can see the restore process and once",
    "start": "787200",
    "end": "789920"
  },
  {
    "text": "it's completed",
    "start": "789920",
    "end": "790800"
  },
  {
    "text": "the volume will be available for",
    "start": "790800",
    "end": "792320"
  },
  {
    "text": "workloads",
    "start": "792320",
    "end": "794959"
  },
  {
    "text": "let's also demonstrate the ability of",
    "start": "796399",
    "end": "798720"
  },
  {
    "text": "setting up a dr volume",
    "start": "798720",
    "end": "800560"
  },
  {
    "text": "for one of the volumes that is currently",
    "start": "800560",
    "end": "802639"
  },
  {
    "text": "not present in this cluster",
    "start": "802639",
    "end": "806079"
  },
  {
    "start": "805000",
    "end": "856000"
  },
  {
    "text": "you can see this is a different volume",
    "start": "806320",
    "end": "808399"
  },
  {
    "text": "that is currently not present in this",
    "start": "808399",
    "end": "811839"
  },
  {
    "text": "cluster",
    "start": "812839",
    "end": "814959"
  },
  {
    "text": "the volume information is recorded so we",
    "start": "814959",
    "end": "818079"
  },
  {
    "text": "can use the same name",
    "start": "818079",
    "end": "819279"
  },
  {
    "text": "size and replica count that was present",
    "start": "819279",
    "end": "821279"
  },
  {
    "text": "on the other cluster",
    "start": "821279",
    "end": "828639"
  },
  {
    "text": "you can see this marker that shows you",
    "start": "828639",
    "end": "831360"
  },
  {
    "text": "that this is a dr volume",
    "start": "831360",
    "end": "834000"
  },
  {
    "text": "what the dr volume does is continuously",
    "start": "834000",
    "end": "836399"
  },
  {
    "text": "monitor the backup store",
    "start": "836399",
    "end": "838240"
  },
  {
    "text": "and always restore the latest backup for",
    "start": "838240",
    "end": "840880"
  },
  {
    "text": "this volume",
    "start": "840880",
    "end": "841760"
  },
  {
    "text": "that is present on the backup store in",
    "start": "841760",
    "end": "844880"
  },
  {
    "text": "the failure case of the auto cluster",
    "start": "844880",
    "end": "847279"
  },
  {
    "text": "we can activate this volume and it will",
    "start": "847279",
    "end": "850000"
  },
  {
    "text": "become available for workloads",
    "start": "850000",
    "end": "857040"
  },
  {
    "start": "856000",
    "end": "867000"
  },
  {
    "text": "this concludes this tech demo if you",
    "start": "857040",
    "end": "859360"
  },
  {
    "text": "want to learn more",
    "start": "859360",
    "end": "860480"
  },
  {
    "text": "about longhorn and the upcoming features",
    "start": "860480",
    "end": "863760"
  },
  {
    "text": "come join us on the longhorn cncf slack",
    "start": "863760",
    "end": "866519"
  },
  {
    "text": "channel",
    "start": "866519",
    "end": "869519"
  }
]