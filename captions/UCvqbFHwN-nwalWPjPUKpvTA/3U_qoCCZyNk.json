[
  {
    "text": "okay hey everyone thanks for coming to our talk today on rapidly scaling for breaking news with Carpenter and kada my",
    "start": "320",
    "end": "7040"
  },
  {
    "text": "name is Mel con I use any pronouns I'm a senior software engineer at the New York Times in delivery engineering which is a",
    "start": "7040",
    "end": "13679"
  },
  {
    "text": "platform team that is building the shared platform in the New York Times and I'm specifically on the team that",
    "start": "13679",
    "end": "19000"
  },
  {
    "text": "manages and maintains and adds features to our shared kubernetes clusters in",
    "start": "19000",
    "end": "25160"
  },
  {
    "text": "AWS hi everyone my name is deuk I'm als my pronoun Heim I'm also on the same",
    "start": "25160",
    "end": "31160"
  },
  {
    "text": "team that better I'm also on the same team as Mel working on the same shared",
    "start": "31160",
    "end": "37800"
  },
  {
    "text": "platform get right into it so what's a breaking news alert so a breaking news alert is a push notification that we",
    "start": "37800",
    "end": "43960"
  },
  {
    "text": "send to our users and I'm sure you've gotten some of these or might even get one get one during the talk but as you",
    "start": "43960",
    "end": "49280"
  },
  {
    "text": "expect this leads to a sudden influx of traffic of people to the website and the mobile",
    "start": "49280",
    "end": "55160"
  },
  {
    "text": "app and this leads some interesting traffic patterns we see sudden spiky traffic and this is an example of an",
    "start": "55160",
    "end": "62120"
  },
  {
    "text": "average traffic Spike usually caused by a push notification we regularly see traffic Spike to 2 to 3x and this",
    "start": "62120",
    "end": "67960"
  },
  {
    "text": "happens within a minute in the past we've over-provisioned to deal with this but that cost a lot of money and we want",
    "start": "67960",
    "end": "73920"
  },
  {
    "text": "to minimize the amount of infrastructure that we're using so we want to need a way to scale down and scale up",
    "start": "73920",
    "end": "79880"
  },
  {
    "text": "quickly this is another example of a traffic pattern that we see this is a result of a daily release of a game and",
    "start": "80360",
    "end": "87960"
  },
  {
    "text": "this also leads to about a 3X increase in traffic in a very short time span this traffic is regular so we can we",
    "start": "87960",
    "end": "94640"
  },
  {
    "text": "know when to expect this and we know how long it lasts so we can we can also",
    "start": "94640",
    "end": "99920"
  },
  {
    "text": "scale predictively scale for this so now Mel will dive deeper into",
    "start": "99920",
    "end": "105360"
  },
  {
    "text": "our architecture and what we're scaling okay so why do we see this",
    "start": "105360",
    "end": "111399"
  },
  {
    "text": "traffic Spike the New York Times has a unified HTP Ingress this is a relatively recent thing that handles some of our",
    "start": "111399",
    "end": "117759"
  },
  {
    "text": "internal and external traffic it's the goal is to have everything eventually pass through the Ingress but the Ingress",
    "start": "117759",
    "end": "125159"
  },
  {
    "text": "controller needs to scale to handle the traffic spikes from the bnas in games so we often see the 3x increase in terms of",
    "start": "125159",
    "end": "134239"
  },
  {
    "text": "3x number of replicas of the unified HTTP Ingress and this is running on the",
    "start": "134239",
    "end": "140680"
  },
  {
    "text": "kubernetes Clusters so when a user opens the New York Times website or app the request is routed through our HTP",
    "start": "140680",
    "end": "147160"
  },
  {
    "text": "Ingress the Ingress sends the request to the Cor Upstream for the appropriate services that said these Services often",
    "start": "147160",
    "end": "154160"
  },
  {
    "text": "need to make calls to other New York Times Services behind our Ingress for example the front page service will make a call to our off service to check if a",
    "start": "154160",
    "end": "161200"
  },
  {
    "text": "user is logged in and has a subscription or a call to our personalization service which customizes the organization of",
    "start": "161200",
    "end": "166560"
  },
  {
    "text": "stories presented to a user and then those Services need to make calls back to the front Ed front page service this",
    "start": "166560",
    "end": "172560"
  },
  {
    "text": "means that there's a spike in user traffic but there's also a spike in internal traffic which makes the traffic",
    "start": "172560",
    "end": "177920"
  },
  {
    "text": "Spike for Ingress even bigger so in this diagram this is a portion of",
    "start": "177920",
    "end": "184319"
  },
  {
    "text": "our shared platform specifically the shared kubernetes clusters in AWS we run",
    "start": "184319",
    "end": "190120"
  },
  {
    "text": "a multi-tenant kubernetes runtime with clusters in multiple regions and environments including a Sandbox cluster",
    "start": "190120",
    "end": "195360"
  },
  {
    "text": "we use for testing changes and teams are provided with a tenant cloud account",
    "start": "195360",
    "end": "200760"
  },
  {
    "text": "along with access to a team Nam space in the cluster in the dev stage and prod clusters so whenever we need to scale we",
    "start": "200760",
    "end": "208000"
  },
  {
    "text": "need to scale across multiple clusters obvously prod is a bigger scale but it's all of",
    "start": "208000",
    "end": "214400"
  },
  {
    "text": "them so now we'll talk about how we're scaling with Carpenter and how we're scaling our nodes so why wouldn't we just use",
    "start": "214400",
    "end": "221200"
  },
  {
    "text": "autoscaler so first Carpenter is a native kubernetes autoscaler so it's installed as a crd and we're already",
    "start": "221200",
    "end": "227519"
  },
  {
    "text": "using a kuet based workflow for installing our other parts of our cluster so we're able to configure Carpenter Vio as part of this workflow",
    "start": "227519",
    "end": "235120"
  },
  {
    "text": "with autoscaler we'd had a lot of terraform that have to be run separately to be installed on each of our environments",
    "start": "235120",
    "end": "240439"
  },
  {
    "text": "so and that had to be maintained separately now I'm going to hand it back to meel to talk further about",
    "start": "240439",
    "end": "246360"
  },
  {
    "text": "Carpenter yeah so Carpenter is able to scale at the Pod level by finding the optimal way to pack pods into nodes",
    "start": "246360",
    "end": "253159"
  },
  {
    "text": "using a bin packing algorithm while the cluster autoscaler is more naive in its approach it it scales at the node level",
    "start": "253159",
    "end": "259680"
  },
  {
    "text": "based on overall resource demand so for example if you have resource limits on a pod and you need to increase the number",
    "start": "259680",
    "end": "266000"
  },
  {
    "text": "number of replicas it looks at the resource limits for that pods and sees where it can schedule it there's a bit of an algorithm there but",
    "start": "266000",
    "end": "271759"
  },
  {
    "text": "it's more naive um and then Carpenter also takes into consideration all instance types that meet the",
    "start": "271759",
    "end": "277160"
  },
  {
    "text": "requirements you specify so we're going to show an example in a minute but you can give multiple instance types as",
    "start": "277160",
    "end": "282520"
  },
  {
    "text": "options which is what really gets a lot of the power of carpenter because carpenter has multiple algorithms that",
    "start": "282520",
    "end": "288039"
  },
  {
    "text": "are like optimizing from a cost perspective which thing to spin up so if you look at the diagram in Carpenter if",
    "start": "288039",
    "end": "295080"
  },
  {
    "text": "you have pending kubernetes pods if they can be fit on an existing node they go ahead and get scheduled by the scheduler",
    "start": "295080",
    "end": "301280"
  },
  {
    "text": "if they're not then Carpenter will spin up a new node that said Carpenter will",
    "start": "301280",
    "end": "306720"
  },
  {
    "text": "do node consolidation and so if it realizes okay I can get a bigger node",
    "start": "306720",
    "end": "311840"
  },
  {
    "text": "that's cheaper than the total cost of the other nodes then it will spin up that node and move things over",
    "start": "311840",
    "end": "318759"
  },
  {
    "text": "there so Carpenter uses grouplist autoscaling so the migration was a little complicated we went from manage",
    "start": "319080",
    "end": "325639"
  },
  {
    "text": "node groups to this grouplist autoscaling that leverages is uh the security groups and uh security groups",
    "start": "325639",
    "end": "334720"
  },
  {
    "text": "and the subnets it adds a tag so that it knows where to spin stuff up um but it's",
    "start": "334720",
    "end": "340039"
  },
  {
    "text": "grouplist because of the different instance types uh and it can't be used with the cluster",
    "start": "340039",
    "end": "346440"
  },
  {
    "text": "autoscaler they explicitly say not to so provisioners which now as of very",
    "start": "346440",
    "end": "351759"
  },
  {
    "text": "recently are with carpent are graduating to Beta like a week and a half ago are being renamed to uh node pools which",
    "start": "351759",
    "end": "358759"
  },
  {
    "text": "that's going to be fun when we upgrade um and they're the equivalent of a node",
    "start": "358759",
    "end": "364199"
  },
  {
    "text": "group in eks so this example right here is one of the provisioners that we use",
    "start": "364199",
    "end": "370240"
  },
  {
    "text": "in our cluster specifically for tenants so we have a bit more flexibility here than say the provisioner for our unified",
    "start": "370240",
    "end": "376120"
  },
  {
    "text": "HTTP Ingress and you can see you have the different instance types and you",
    "start": "376120",
    "end": "382039"
  },
  {
    "text": "have the um the part for celium specifically like the startup taint and",
    "start": "382039",
    "end": "388520"
  },
  {
    "text": "then you can we also have our nodes expire every 14 days part of the reason is spot instances can't be replaced and",
    "start": "388520",
    "end": "396720"
  },
  {
    "text": "like changed to a different size they're just deleted and so we want to cycle the nodes and then it's also like fun you know chaos engineering to have our nodes",
    "start": "396720",
    "end": "403960"
  },
  {
    "text": "cycle every 14 days and it's at Max 14 days sometimes they go away sooner than that um and then we have consolidation",
    "start": "403960",
    "end": "412080"
  },
  {
    "text": "enabled which you don't have to do and I'll go into a bit more but we don't do",
    "start": "412080",
    "end": "417120"
  },
  {
    "text": "that for our unified hvv Ingress",
    "start": "417120",
    "end": "421400"
  },
  {
    "text": "uh so consolidation so this is kind of like the big way that we save money so",
    "start": "424639",
    "end": "430360"
  },
  {
    "text": "carpenter has three ways to consolidate nodes it reduces the cost by either deleting or replacing the nodes either",
    "start": "430360",
    "end": "436879"
  },
  {
    "text": "if they're empty if their workloads can run on other nodes or if they can be replaced with cheaper nodes and by",
    "start": "436879",
    "end": "442039"
  },
  {
    "text": "cheaper I it could possibly mean one node replacing multiple nodes or one node replacing one node so in order it",
    "start": "442039",
    "end": "450319"
  },
  {
    "text": "goes through and it sees can it delete any empty nodes or it tries to delete multiple nodes and launch a cheaper",
    "start": "450319",
    "end": "456639"
  },
  {
    "text": "single node or it deletes a single node and then tries and replace with a cheaper node on the right hand side this",
    "start": "456639",
    "end": "463039"
  },
  {
    "text": "is something taken from the carpenter dock I took a portion of it this is",
    "start": "463039",
    "end": "468120"
  },
  {
    "text": "actually the node pool which has all these new uh pieces in the spec and everything too um but it allows you to",
    "start": "468120",
    "end": "475199"
  },
  {
    "text": "Define how disruption works so you have this consolidation policy either you say when underutilized or when empty and",
    "start": "475199",
    "end": "482479"
  },
  {
    "text": "then you say how long it will wait before it makes a decision and then you",
    "start": "482479",
    "end": "487840"
  },
  {
    "text": "can also set it to expire after a certain amount of time which is the same thing that you saw on the previous slide",
    "start": "487840",
    "end": "493120"
  },
  {
    "text": "with the provisioner that we had the 14 days okay so this is code taken from the",
    "start": "493120",
    "end": "501000"
  },
  {
    "text": "carpenter GitHub repository I'm not going to walk through like every line just the high level so this is the cool part with spot instances so part of the",
    "start": "501000",
    "end": "507240"
  },
  {
    "text": "reason we M migrated to Carpenter it was a lot easier to do spot instances there than it was to do with our manage node",
    "start": "507240",
    "end": "512518"
  },
  {
    "text": "groups and we really wanted to use spot instances and so this filter unwanted",
    "start": "512519",
    "end": "518080"
  },
  {
    "text": "spot what it does is it filters out spot types that are more expensive than the cheapest on demand type that we could",
    "start": "518080",
    "end": "525160"
  },
  {
    "text": "launch so if the on demand type is going to be just as the same price or like",
    "start": "525160",
    "end": "531760"
  },
  {
    "text": "cheaper then it doesn't make sense to do a spot instance we also for anyone who doesn't know a lot about spot instances",
    "start": "531760",
    "end": "537360"
  },
  {
    "text": "you essentially can like bid on them and and so people could like out bid you we don't Bid And so it's kind of like an I",
    "start": "537360",
    "end": "542920"
  },
  {
    "text": "even how we do it so it's either it's cheaper it's not the end um and it also",
    "start": "542920",
    "end": "549200"
  },
  {
    "text": "is going to find like the cheapest offering for the spot instances from the options that you give it so if you don't",
    "start": "549200",
    "end": "555079"
  },
  {
    "text": "give it as many options you're not going to have as big of a like bang for your buck because it doesn't have as many options to choose from and that also",
    "start": "555079",
    "end": "561560"
  },
  {
    "text": "makes consolidation harder if you don't have as many options to choose from",
    "start": "561560",
    "end": "567800"
  },
  {
    "text": "uh cool so now we have a quick demo on how Carpenter",
    "start": "568120",
    "end": "573519"
  },
  {
    "text": "works it should autoplay and I can't see it on my laptop so I'll explain it from here but on the left side just have a",
    "start": "573519",
    "end": "580440"
  },
  {
    "text": "quick HTP bin deployment that I'll be scaling up and on the right side just tailing some logs from the carpenter",
    "start": "580440",
    "end": "586800"
  },
  {
    "text": "controller so we can see what Carpenter is doing so right now you can see that carpenter has might be a little small",
    "start": "586800",
    "end": "592680"
  },
  {
    "text": "but carpenter has some spot instance pricing that it's aware of it's aware of what instances it it can pick from when",
    "start": "592680",
    "end": "598519"
  },
  {
    "text": "scaling and on the left side just scaling up say we have a breaking news event something big scaling up to 500",
    "start": "598519",
    "end": "605240"
  },
  {
    "text": "Deo uh replicas and we're going to watch that scale up on the right side once once the pods",
    "start": "605240",
    "end": "612720"
  },
  {
    "text": "scale up and Carpenters aware you should be able to see that Carpenter will decide first what node which go through",
    "start": "612720",
    "end": "619480"
  },
  {
    "text": "the algorithm see which no pick an instance type and see which it can provision and we should",
    "start": "619480",
    "end": "625760"
  },
  {
    "text": "see on the right side yeah so now we can see that carpenter has decided a provision a new node it picked a",
    "start": "625760",
    "end": "631480"
  },
  {
    "text": "specific instance type and on the left side we can see now that the node is there and we can also see and we can see",
    "start": "631480",
    "end": "639760"
  },
  {
    "text": "that it's actually provisioning a second note also to meet the demand to schedule all the",
    "start": "639760",
    "end": "646279"
  },
  {
    "text": "pods and then if you look at the tags on the on the Node we can see that",
    "start": "646279",
    "end": "652240"
  },
  {
    "text": "Carpenter picked a certain instance type and it's tagged by Carpenter and now then important part is",
    "start": "652240",
    "end": "659880"
  },
  {
    "text": "that we want to be able to also scale down if we scale up so I'm going to try scaling down on the left and scaling",
    "start": "659880",
    "end": "666800"
  },
  {
    "text": "back down to one so yeah now we're scaling",
    "start": "666800",
    "end": "675240"
  },
  {
    "text": "down",
    "start": "680200",
    "end": "683200"
  },
  {
    "text": "second yeah this part I was trying to see if there was pod the node but command didn't work but",
    "start": "686120",
    "end": "693600"
  },
  {
    "text": "yep waiting for the node to be ready but yeah just waiting for the waiting for me to scale it",
    "start": "698079",
    "end": "705560"
  },
  {
    "text": "down sorry we didn't do a live demo we didn't want to test the demo Gods but this was recorded",
    "start": "708440",
    "end": "714680"
  },
  {
    "text": "yesterday close enough to live",
    "start": "714680",
    "end": "720959"
  },
  {
    "text": "so yeah now we can see that the this is me showing that the pods have been scheduled on that note that was",
    "start": "727160",
    "end": "734120"
  },
  {
    "text": "provisioned then and part of what you're saying is it takes a little bit for it decide to disrupt the node and get rid",
    "start": "739040",
    "end": "745360"
  },
  {
    "text": "of it um it needs to like move around the pods and then also get rid of all the demon set pods yep and now we're",
    "start": "745360",
    "end": "751120"
  },
  {
    "text": "scaling it down to one and then we can see that the deployment now is one replica so so we should see that",
    "start": "751120",
    "end": "757800"
  },
  {
    "text": "Carpenter is not going to go through that consolidation algorithm where it's going to either delete the node or try to replace with a single node but in",
    "start": "757800",
    "end": "764079"
  },
  {
    "text": "this case it's likely just going to delete both nodes since we've scaled down from 500 pods into one pod so on",
    "start": "764079",
    "end": "770000"
  },
  {
    "text": "the right side soon we'll see the Carpenters going decide to delete each of those not that it's created and we'll",
    "start": "770000",
    "end": "775519"
  },
  {
    "text": "be back to our original state very quickly part of the other reason we decided to",
    "start": "775519",
    "end": "781720"
  },
  {
    "text": "use Carpenter was also it's faster than the cluster Auto scaler so we did like some benchmarking and I want to say",
    "start": "781720",
    "end": "786920"
  },
  {
    "text": "cluster autoscaler maybe takes like two minutes and Carpenter is like 90 seconds sometimes even 60 seconds a little",
    "start": "786920",
    "end": "792880"
  },
  {
    "text": "longer to schedule something on it and that sounds like not a lot but when you have the traffic spikes we have that is",
    "start": "792880",
    "end": "798120"
  },
  {
    "text": "a lot and also is still not really enough um so yeah",
    "start": "798120",
    "end": "804360"
  },
  {
    "text": "cool that's the demo now we'll talk about",
    "start": "804360",
    "end": "810120"
  },
  {
    "text": "more thank you and now we'll talk more about how",
    "start": "810120",
    "end": "815760"
  },
  {
    "text": "we're scaling with kada so just to give a overview of kada kada is an event based scaler and it",
    "start": "815760",
    "end": "823160"
  },
  {
    "text": "extends to the HPA so it configures the HPA for your deployment instead of it",
    "start": "823160",
    "end": "828600"
  },
  {
    "text": "doesn't replace the HPA so scaling is set up via VIA scaled object and the scaled object defines which deployment",
    "start": "828600",
    "end": "835360"
  },
  {
    "text": "is the Target and then it also Define what the trigger is so trigger could be a bunch of event sources that kada",
    "start": "835360",
    "end": "840600"
  },
  {
    "text": "provides out of the box so let's revisit some of those",
    "start": "840600",
    "end": "846720"
  },
  {
    "text": "traffic Spike so this is the this is the release traffic Spike the predictable traffic Spike so we needed a way to make",
    "start": "846720",
    "end": "853240"
  },
  {
    "text": "sure we can scale up in time and we know when these spikes are so we're able to use K's KRON event type to scale our",
    "start": "853240",
    "end": "858720"
  },
  {
    "text": "Ingress controller so essentially we specify specify time for it to SC scale",
    "start": "858720",
    "end": "863920"
  },
  {
    "text": "up to a certain replica amount and then kadas patches the HPA to scale it up for this time period and this would have",
    "start": "863920",
    "end": "870600"
  },
  {
    "text": "been very doable with the kubernetes KRON job also but kada allows us to Define our scaling logic in one yl and",
    "start": "870600",
    "end": "875800"
  },
  {
    "text": "it also modifies the replicas for us so this is just a diagram combining",
    "start": "875800",
    "end": "883320"
  },
  {
    "text": "car printer and kada and how we scale together so when a when a when a drop",
    "start": "883320",
    "end": "889160"
  },
  {
    "text": "happens or lease happens we can see that kada first adjusts our Ingress controller replica so it modifies the",
    "start": "889160",
    "end": "896320"
  },
  {
    "text": "HPA which scaled up to deployment and then Carpenter in response has to be able to schedule all these pods that",
    "start": "896320",
    "end": "902480"
  },
  {
    "text": "have been that have been created so Carpenter adjusts the node pool that manages the Ingress controller and then",
    "start": "902480",
    "end": "908480"
  },
  {
    "text": "optimizes for cost by picking the best in instance and once the K Chron event",
    "start": "908480",
    "end": "913639"
  },
  {
    "text": "ends we know that carp K will scale down the deployment and in response Carpenter",
    "start": "913639",
    "end": "919440"
  },
  {
    "text": "will either delete the nodes or replace the nodes with cheaper",
    "start": "919440",
    "end": "923880"
  },
  {
    "text": "nodes and we've also explored other ways to scale so this is the revisiting the",
    "start": "924920",
    "end": "930199"
  },
  {
    "text": "breaking news alert so we know when we're going to send a breaking news alerts we need a way to like scale an in",
    "start": "930199",
    "end": "936959"
  },
  {
    "text": "response to this event but these aren't predictable we don't know like it's going to happen at the same time every day so these spikes are more random and",
    "start": "936959",
    "end": "943120"
  },
  {
    "text": "more frequent so we can't use the cron solution so kada supports an external push trigger so we essentially set up",
    "start": "943120",
    "end": "949399"
  },
  {
    "text": "the server ourself to se send an alert to K so as you can see in this diagram we",
    "start": "949399",
    "end": "955880"
  },
  {
    "text": "have we first have like the breaking news aler going out after the article is published and then the web hook server",
    "start": "955880",
    "end": "962519"
  },
  {
    "text": "that that the team that sends out the notification sends an alert to and then in order to set up the external server",
    "start": "962519",
    "end": "969079"
  },
  {
    "text": "we have to set up there needs to be a grpc server that Kens to so that's where the external push goes so the the",
    "start": "969079",
    "end": "976519"
  },
  {
    "text": "request gets sent to the server webbook server which gets sent to the grp server and then once it hits a grp server kada",
    "start": "976519",
    "end": "983360"
  },
  {
    "text": "knows to scale up and it would be a similar type of solution where kada would scale up to an x amount once it",
    "start": "983360",
    "end": "988800"
  },
  {
    "text": "receiv receives the alert that a breaking news alert has gone out and now we're going to discuss some",
    "start": "988800",
    "end": "995279"
  },
  {
    "text": "lessons learned from this process so cost savings um we were",
    "start": "995279",
    "end": "1002279"
  },
  {
    "text": "looking to save money by not over provisioning as much so this is really hard given the way that the breaking",
    "start": "1002279",
    "end": "1007800"
  },
  {
    "text": "news alerts work and what deu was talking about with like getting a heads up the heads up is like a couple seconds",
    "start": "1007800",
    "end": "1014040"
  },
  {
    "text": "and when your Spike happens in like 20 to 40 seconds and last for only 2 and a half minutes minutes and is so huge and",
    "start": "1014040",
    "end": "1020920"
  },
  {
    "text": "it basically means that like by the time you scale up you have to scale down um",
    "start": "1020920",
    "end": "1025959"
  },
  {
    "text": "so you know over pring was kind of what we were able to do and um to be honest we still do it somewhat um but a lot",
    "start": "1025959",
    "end": "1032319"
  },
  {
    "text": "less now and a lot of that is to do with the crown job that he mentioned but also",
    "start": "1032319",
    "end": "1037360"
  },
  {
    "text": "the things in Carpenter so there's a no consolidation so that means less over provisioning right because it's going to",
    "start": "1037360",
    "end": "1043640"
  },
  {
    "text": "pick like the cheaper nodes it's important that we have like a variety of nodes that it can pick from otherwise",
    "start": "1043640",
    "end": "1049600"
  },
  {
    "text": "like if you say okay you can only do this specific instance type then it can't do anything it's either like okay",
    "start": "1049600",
    "end": "1055679"
  },
  {
    "text": "it's spot or it's on demand the end and we do have some provisioners that are only on demand because they're like more critical",
    "start": "1055679",
    "end": "1061360"
  },
  {
    "text": "workloads um yeah and so with the Cron job we don't need to over-provision",
    "start": "1061360",
    "end": "1066799"
  },
  {
    "text": "things as much and can adjust the scaled object Max replicas based on observed traffic patterns so for example um",
    "start": "1066799",
    "end": "1073160"
  },
  {
    "text": "recently with some of the releases of the newer games there has been like a record number of downloads of of games",
    "start": "1073160",
    "end": "1079559"
  },
  {
    "text": "and so was like huh I wonder what's going oh okay cool um so because we",
    "start": "1079559",
    "end": "1085000"
  },
  {
    "text": "scale up ahead of time we also have fewer alerts and things breaking in the cluster since it's not attempting to",
    "start": "1085000",
    "end": "1090520"
  },
  {
    "text": "scale on the Fly and we also have more instance types to choose from so with the manage node",
    "start": "1090520",
    "end": "1096480"
  },
  {
    "text": "group you have like an instance type with Carpenter you can give a set of instance",
    "start": "1096480",
    "end": "1102080"
  },
  {
    "text": "types so further improvements oh and I forgot to mention we saw a 25 decrease",
    "start": "1102080",
    "end": "1107320"
  },
  {
    "text": "25% decrease in cost which is exciting um further improvements so our unified",
    "start": "1107320",
    "end": "1112960"
  },
  {
    "text": "HTP Ingress because of how important it is and because of like the networking",
    "start": "1112960",
    "end": "1118080"
  },
  {
    "text": "and compute needs has one instance type that it can do and also we only do on demand so obviously we can't really",
    "start": "1118080",
    "end": "1124840"
  },
  {
    "text": "leverage Carpenter for that and also we don't do node consolidation for it so we're already looking into ways that we can either like have more options for",
    "start": "1124840",
    "end": "1131640"
  },
  {
    "text": "nodes or maybe like switch from like a network optimized instance type to compute optimized instance type and just",
    "start": "1131640",
    "end": "1137640"
  },
  {
    "text": "like really do more investigation into that and reminder that this unified HTP Andress is like maybe a year old year",
    "start": "1137640",
    "end": "1143120"
  },
  {
    "text": "and a half so like yeah um and then we also the external BNA scaler",
    "start": "1143120",
    "end": "1150679"
  },
  {
    "text": "we're trying to figure out like how to do that because of the little bit of heads up that does it's not really worth",
    "start": "1150679",
    "end": "1156919"
  },
  {
    "text": "it to like Implement a whole scaler when it's not even a little less o over",
    "start": "1156919",
    "end": "1162280"
  },
  {
    "text": "provisioning would be good but it's not worth it given the like little bit of heads up and the way that the workflow",
    "start": "1162280",
    "end": "1167320"
  },
  {
    "text": "works is it's kind of like unexpected there's a whole process of getting an article approved and I think it's like",
    "start": "1167320",
    "end": "1172960"
  },
  {
    "text": "in slack they post like hey please give us a breaking news alert now and it's like there are other teams that are",
    "start": "1172960",
    "end": "1178559"
  },
  {
    "text": "doing different things and so by the time it gets to us it's like um and then we also want to have",
    "start": "1178559",
    "end": "1184840"
  },
  {
    "text": "potentially kada as a service for tenants so right now we have kada rolled out to the tenants in our clusters um",
    "start": "1184840",
    "end": "1190320"
  },
  {
    "text": "but we don't really have many who are using it and I think part of the reason is because they don't know necessarily how or why um I know deepo said said",
    "start": "1190320",
    "end": "1198720"
  },
  {
    "text": "that we they have different metrics that you can scale on so there's a lot of stuff built in so data dog all the",
    "start": "1198720",
    "end": "1204919"
  },
  {
    "text": "different clouds like CPU pretty much anything you can imagine and like we said you can write your own um so",
    "start": "1204919",
    "end": "1210520"
  },
  {
    "text": "ideally we'd have something like in the shared platform where it's like hey do you want a scaler for this deployment and like give us a bit of the",
    "start": "1210520",
    "end": "1216840"
  },
  {
    "text": "configuration and then we'll do it for you so with kada and Carpenter together we were able to save significant money",
    "start": "1216840",
    "end": "1223039"
  },
  {
    "text": "and um our lives are a lot easier too get paged way less often",
    "start": "1223039",
    "end": "1229360"
  },
  {
    "text": "okay so the other big thing is the shared platform so shared platform is centralization and standardization right",
    "start": "1229360",
    "end": "1236280"
  },
  {
    "text": "so before the shared platform people were running and managing their own infrastructure understandably there's",
    "start": "1236280",
    "end": "1241919"
  },
  {
    "text": "going to be differences in that infrastructure not only because of the needs of the team but also because of the expertise and what the team prefers",
    "start": "1241919",
    "end": "1249240"
  },
  {
    "text": "um but now that we have a lot of our services on the shared platform and",
    "start": "1249240",
    "end": "1255960"
  },
  {
    "text": "specifically the shared cluster we're able to establish a Baseline and more easily compare across services and then",
    "start": "1255960",
    "end": "1262919"
  },
  {
    "text": "detect outliers so for example we can ask questions like why does this one service have significantly higher RPS",
    "start": "1262919",
    "end": "1269520"
  },
  {
    "text": "sometimes the answer is the scale is larger and that's fine um but other times it may be because there's like not",
    "start": "1269520",
    "end": "1274880"
  },
  {
    "text": "an optimal traffic flow and something needs to be changed like maybe it's making too many too many requests maybe it's not cashing",
    "start": "1274880",
    "end": "1282200"
  },
  {
    "text": "whatever speaker notes are cut off from the bottom so I guess I'm just gonna",
    "start": "1287039",
    "end": "1292240"
  },
  {
    "text": "wing this um um so I already said some tenants",
    "start": "1292240",
    "end": "1298600"
  },
  {
    "text": "make significantly more requests I talked about earlier this kind of like you have to make calls to other services",
    "start": "1298600",
    "end": "1304240"
  },
  {
    "text": "within the New York Times and then they make calls back and how that contributes to the spike um but there are some that",
    "start": "1304240",
    "end": "1310600"
  },
  {
    "text": "are making more requests than others and it's we haven't like fully figured out what's going on but it's a lot easier to",
    "start": "1310600",
    "end": "1316640"
  },
  {
    "text": "tell um and also with there being us scaling the architecture rather than the",
    "start": "1316640",
    "end": "1321919"
  },
  {
    "text": "tenants needing to and like I said this like standardization we have a much better idea like okay this is",
    "start": "1321919",
    "end": "1328480"
  },
  {
    "text": "potentially something with the cluster versus this is something going on with the tenants like this is expected behavior all of that um and then it also",
    "start": "1328480",
    "end": "1336600"
  },
  {
    "text": "means they don't have to worry about it so in the past a lot of different teams scaled up for these breaking news alerts",
    "start": "1336600",
    "end": "1342200"
  },
  {
    "text": "or for the game's release because like for example like o or uh",
    "start": "1342200",
    "end": "1349480"
  },
  {
    "text": "I there I gota be careful off or say like personalization or algorithms stuff",
    "start": "1349480",
    "end": "1354799"
  },
  {
    "text": "like that uh and now they don't need to do that anymore I think we mentioned",
    "start": "1354799",
    "end": "1360720"
  },
  {
    "text": "early on that not all of our services are even behind the Ingress which you",
    "start": "1360720",
    "end": "1366200"
  },
  {
    "text": "don't have to be running your service in our shared cluster to be behind the Ingress you can be elsewhere",
    "start": "1366200",
    "end": "1372600"
  },
  {
    "text": "um so we're hoping that the more people that move to the platform that that",
    "start": "1372600",
    "end": "1378200"
  },
  {
    "text": "means it'll be easier to see some of these outliers and then it'll be good for everyone they can go back we can go back to the team we can advise them as",
    "start": "1378200",
    "end": "1384600"
  },
  {
    "text": "SME and say like Hey we're seeing this can you tell us why this might be happening can we help you improve your",
    "start": "1384600",
    "end": "1392960"
  },
  {
    "text": "service okay um so with that thank you very much um we had a couple talks",
    "start": "1395000",
    "end": "1402039"
  },
  {
    "text": "earlier this week from other of our co-workers they already happened but one at argoon and one at multitenancy con I",
    "start": "1402039",
    "end": "1407360"
  },
  {
    "text": "encourage you to check it out if you get the [Applause]",
    "start": "1407360",
    "end": "1412400"
  },
  {
    "text": "chance thank you",
    "start": "1412400",
    "end": "1415720"
  },
  {
    "text": "[Applause]",
    "start": "1418080",
    "end": "1422559"
  },
  {
    "text": "everyone very very good talk one of the best talk of the whole ccon seriously",
    "start": "1423480",
    "end": "1428679"
  },
  {
    "text": "it's the last day the last talk and like it's full appreciate it also both of our",
    "start": "1428679",
    "end": "1434720"
  },
  {
    "text": "first talks At cubec Con so",
    "start": "1434720",
    "end": "1439880"
  },
  {
    "text": "so still I have a question um so you you're using K it's scaling the number",
    "start": "1444240",
    "end": "1450960"
  },
  {
    "text": "it's using the HP to scale let say 500 pods some of them are going to be",
    "start": "1450960",
    "end": "1456159"
  },
  {
    "text": "skittled because you already have the nods but then Carpenter is going to create new nod it can take some time so",
    "start": "1456159",
    "end": "1463039"
  },
  {
    "text": "do you have any way to kind of or how do you manage to have some resour versus",
    "start": "1463039",
    "end": "1468840"
  },
  {
    "text": "like when Carpenters scale down the number of nodes you still have some some balloon you can you can scedule pods",
    "start": "1468840",
    "end": "1476440"
  },
  {
    "text": "directly say that last part again the last part of it you were like",
    "start": "1476440",
    "end": "1481559"
  },
  {
    "text": "how do you manage to blah how do you manage to the the the the number of",
    "start": "1481559",
    "end": "1486840"
  },
  {
    "text": "nodes or the compute power you still have when carpent moves down so you can accommodate at least some of the SP some",
    "start": "1486840",
    "end": "1494399"
  },
  {
    "text": "of the pods were when a new scaling happened okay so first of all part of",
    "start": "1494399",
    "end": "1499919"
  },
  {
    "text": "the like benefit of the crown job is rather than doing like node by node it can start up like a bunch of nodes um",
    "start": "1499919",
    "end": "1505399"
  },
  {
    "text": "and you're specifically asking like when it's scaling down how we still have",
    "start": "1505399",
    "end": "1512720"
  },
  {
    "text": "power",
    "start": "1515880",
    "end": "1518880"
  },
  {
    "text": "y so the answer is it doesn't so it's it's basically like they're kind of working separately so Carpenter is",
    "start": "1524640",
    "end": "1530200"
  },
  {
    "text": "working like on the Node level and then kada is working on the like pod HBA level so there's not going to be nodes",
    "start": "1530200",
    "end": "1535840"
  },
  {
    "text": "just sitting there unless there are nodes that are underutilized um in general because of the node consolidation and everything we",
    "start": "1535840",
    "end": "1542520"
  },
  {
    "text": "don't really have many nodes that are like at a high under utilization unless it's the HTP Ingress so what that means",
    "start": "1542520",
    "end": "1550039"
  },
  {
    "text": "is as it's increasing the replicas with kada it's like okay I need to do schedule this many more replicas",
    "start": "1550039",
    "end": "1555679"
  },
  {
    "text": "Carpenter sees it's not interacting with with kada it just sees that it needs to schedule more pods and then it will spin",
    "start": "1555679",
    "end": "1561720"
  },
  {
    "text": "up the notes which is part of the reason why this is like difficult to do with a BNA because like like we would have to",
    "start": "1561720",
    "end": "1568919"
  },
  {
    "text": "either like purposely create them go ahead so let me rephrase yeah can can",
    "start": "1568919",
    "end": "1574240"
  },
  {
    "text": "you tell Carpenter to keep that number of notes when it's scaled down yeah yeah",
    "start": "1574240",
    "end": "1580000"
  },
  {
    "text": "yeah you can so you can say so you can either say like uh you could say keep them around for longer um you would have",
    "start": "1580000",
    "end": "1585760"
  },
  {
    "text": "to have like a a like different configuration for the provisioner so potentially like have a different provisioner specifically for this event",
    "start": "1585760",
    "end": "1594320"
  },
  {
    "text": "which you can't overlap the provisioner so you can't have like a provisioner that would apply to where a pod could be",
    "start": "1594320",
    "end": "1601000"
  },
  {
    "text": "like scheduled based on one or the other um and so we just and I just lost my",
    "start": "1601000",
    "end": "1607919"
  },
  {
    "text": "train of thought um but yeah so it's basically like we don't have extra ahead",
    "start": "1607919",
    "end": "1615520"
  },
  {
    "text": "of time it's just like they're there and then they're gone and we would have to do extra work to keep them around and in",
    "start": "1615520",
    "end": "1621520"
  },
  {
    "text": "general we don't really want to keep them around it does it's pretty smart about like it's you saw it took a little bit of time for it to scale back down um",
    "start": "1621520",
    "end": "1629240"
  },
  {
    "text": "but yeah we have seen some errors on scale down because you can imagine like it's going like this and then going like that and so it's a",
    "start": "1629240",
    "end": "1635840"
  },
  {
    "text": "lot go ahead thank you hello guys I have a question like is it okay to run cluster after scaron and Carpenter at",
    "start": "1635840",
    "end": "1643039"
  },
  {
    "text": "the same time no you have to run only one tool correct um so usually what you",
    "start": "1643039",
    "end": "1648919"
  },
  {
    "text": "do is you scale down the cluster autoscaler to zero and you do that after you have Carpenter set up so you can't",
    "start": "1648919",
    "end": "1654640"
  },
  {
    "text": "have the two together because they are both scaling the same thing in different ways and so they know they explicitly",
    "start": "1654640",
    "end": "1661159"
  },
  {
    "text": "says in the carpent documentation to not have it okay thank you thank you um have you had",
    "start": "1661159",
    "end": "1668480"
  },
  {
    "text": "to make any changes to cube scheduler or have you observed any weird interactions",
    "start": "1668480",
    "end": "1675159"
  },
  {
    "text": "between Carpenter and Cube Schuler cuz I can I with carpenters bin packing and",
    "start": "1675159",
    "end": "1680320"
  },
  {
    "text": "Cube scheduler doing whatever the heck Cube scheduler does uh I can imagine that those are going to fight uh yeah",
    "start": "1680320",
    "end": "1688120"
  },
  {
    "text": "yeah so we haven't made any changes to cube scheduler um some of the stuff that",
    "start": "1688120",
    "end": "1693720"
  },
  {
    "text": "we've seen that's kind of weird is we had to like Implement these Interruption cues specifically because there was not",
    "start": "1693720",
    "end": "1699399"
  },
  {
    "text": "like graceful termination of stuff especially with like the demon sets and so it was essentially like sometimes",
    "start": "1699399",
    "end": "1704799"
  },
  {
    "text": "weird things would happen where like it would struggle to like drain and delete the node or for some reason like",
    "start": "1704799",
    "end": "1710039"
  },
  {
    "text": "something wasn't scheduled wasn't at the right number of replicas and so that's how we've gotten around it but yeah you're totally right um so I think it's",
    "start": "1710039",
    "end": "1716799"
  },
  {
    "text": "kind of like wonky stuff happens but it's like the way it is right now it's like okay Carpenter can kind of like",
    "start": "1716799",
    "end": "1723120"
  },
  {
    "text": "figure it out and fix it as in like say the cube scheduler like naively schedules something on a node and",
    "start": "1723120",
    "end": "1728720"
  },
  {
    "text": "Carpenter's like I don't know I want one node for all of that instead then it's able to like delete the node and then",
    "start": "1728720",
    "end": "1734279"
  },
  {
    "text": "schedule things on that node and you saw in the laog that it will show you like how many pods it's scheduling on the no",
    "start": "1734279",
    "end": "1740760"
  },
  {
    "text": "thank you yep fantastic talk thank you are you concerned about capacity limitations in",
    "start": "1740760",
    "end": "1747440"
  },
  {
    "text": "the cloud so you're moving from pre-scaling from having an over provisioner by the sounds of it and",
    "start": "1747440",
    "end": "1753200"
  },
  {
    "text": "you're now scaling on demand based on breaking news as we enter the holiday periods the clouds get busier are you",
    "start": "1753200",
    "end": "1759960"
  },
  {
    "text": "concerned about running out of capacity when breaking news occurs and Cuda Carpenter Can't scale anymore I'm going",
    "start": "1759960",
    "end": "1767039"
  },
  {
    "text": "to answer this very carefully yes um I think in general well",
    "start": "1767039",
    "end": "1774519"
  },
  {
    "text": "there are a couple things to consider so first of all like we are currently migrating more people to the Clusters um",
    "start": "1774519",
    "end": "1780480"
  },
  {
    "text": "and the Ingress which means more scaling needed um I would say holidays are not",
    "start": "1780480",
    "end": "1787320"
  },
  {
    "text": "as big of a thing it's really the election and I would say the big one first big one that's coming up is Iowa caucus which I think is in January um so",
    "start": "1787320",
    "end": "1794360"
  },
  {
    "text": "in general what we do is we have um a lot of stress testing and load testing and in particular this is like a",
    "start": "1794360",
    "end": "1800559"
  },
  {
    "text": "separate thing but we have an in-house uh load testing set up specifically using k6 which allows people to like",
    "start": "1800559",
    "end": "1808000"
  },
  {
    "text": "configure their load test and they can run stuff themselves but there's a whole team that does elections Readiness to",
    "start": "1808000",
    "end": "1813519"
  },
  {
    "text": "make sure that we're like in a good situation for this but yeah you're totally right that's brilliant I would",
    "start": "1813519",
    "end": "1818799"
  },
  {
    "text": "say it's a fantastic talk for the next qcom thank you thank",
    "start": "1818799",
    "end": "1823600"
  },
  {
    "text": "you hi uh with mixing up different instance sizes how are you managing um",
    "start": "1824720",
    "end": "1830600"
  },
  {
    "text": "demon set scaling like setting their resources just like statically to the largest instance size you run or is",
    "start": "1830600",
    "end": "1837480"
  },
  {
    "text": "there like any Dynamic scaling there so I think",
    "start": "1837480",
    "end": "1842559"
  },
  {
    "text": "that I think so specifically what it'll do is it it doesn't consider like aside",
    "start": "1842559",
    "end": "1848440"
  },
  {
    "text": "from knowing how much capacity it needs for the demon sets it doesn't consider that when it's spinning up new nodes so it's just like what it knows okay I need",
    "start": "1848440",
    "end": "1854919"
  },
  {
    "text": "like maybe there are like seven demon sets so I need seven pods that are like this resource constraints that need to",
    "start": "1854919",
    "end": "1860480"
  },
  {
    "text": "go on that node but it will spin up the new node based on the other stuff that",
    "start": "1860480",
    "end": "1866279"
  },
  {
    "text": "needs to get scheduled yeah I'm asking like um for larger nodes you may need more demon more demon set resources and",
    "start": "1866279",
    "end": "1873720"
  },
  {
    "text": "for smaller nodes you just provision like for the larger sizes at all times yeah so",
    "start": "1873720",
    "end": "1879000"
  },
  {
    "text": "we this is not ideal but currently what we do is we've been shifting it so basically like Carpenter doesn't do",
    "start": "1879000",
    "end": "1885960"
  },
  {
    "text": "stuff exactly the same every time but it seems to kind of pick similar instances and so what we've had to do over time as",
    "start": "1885960",
    "end": "1891480"
  },
  {
    "text": "more stuff has migrated as the like node size has increased is we've have to change those resource limits and we've",
    "start": "1891480",
    "end": "1897360"
  },
  {
    "text": "actually recently removed the CPU limit for the data dog demon set specifically",
    "start": "1897360",
    "end": "1903200"
  },
  {
    "text": "because of the way that like CPU starvation fails it's like a silent fail compared to memory starvation um and I",
    "start": "1903200",
    "end": "1909679"
  },
  {
    "text": "think ideally and I know we're doing this in the shared cluster we have in GK we want to just like completely remove",
    "start": "1909679",
    "end": "1916679"
  },
  {
    "text": "CPU this is me speaking I would like to completely remove CPU limits um for the",
    "start": "1916679",
    "end": "1922279"
  },
  {
    "text": "deployments all the deployments tenants and everything because there's been some gnarly stuff because of the CPU",
    "start": "1922279",
    "end": "1928360"
  },
  {
    "text": "starvation thanks hi I was wondering um as you sort",
    "start": "1928360",
    "end": "1935480"
  },
  {
    "text": "of migrating to this like really fast cluster Auto scaling how much did your application start of time uh play into",
    "start": "1935480",
    "end": "1942200"
  },
  {
    "text": "that like if you had a few applications that were really slow to start up that were really like blocking it would you",
    "start": "1942200",
    "end": "1947799"
  },
  {
    "text": "see something where all of your applications would like you know be high CPU usage and then it would scale even",
    "start": "1947799",
    "end": "1953360"
  },
  {
    "text": "more yeah so that's a great question um so part of the difficulty is like we want to live in a world with a shared",
    "start": "1953360",
    "end": "1959360"
  },
  {
    "text": "platform where people don't have to worry about the infrastructure that's not the world we live in and honestly like if you have a shared platform like that congratulations because that I",
    "start": "1959360",
    "end": "1966320"
  },
  {
    "text": "don't know um and so essentially what we've had to do is be really clear in our documentation and then also when we",
    "start": "1966320",
    "end": "1972039"
  },
  {
    "text": "see like outliers and advise people and basically be like hey so here naively is what could super is doing like pods can",
    "start": "1972039",
    "end": "1978519"
  },
  {
    "text": "come and go all of that you don't know where they're going to get scheduled so you need to be more resilient so we've had stuff happen in the past where",
    "start": "1978519",
    "end": "1985159"
  },
  {
    "text": "tenants are like having to change things but usually because it's in one place we look at it we go help them we talk to",
    "start": "1985159",
    "end": "1991720"
  },
  {
    "text": "them we get an understanding and we've also done some of these migrations ourselves like with the team um but yeah",
    "start": "1991720",
    "end": "1997440"
  },
  {
    "text": "it's a great question it's it's going to shift cool yeah thank you um I have a couple of questions so",
    "start": "1997440",
    "end": "2005080"
  },
  {
    "text": "the first one is uh about the consolidation so I see your No poor",
    "start": "2005080",
    "end": "2010519"
  },
  {
    "text": "configuration you set up the consolidate after 30 seconds so have you ever seen",
    "start": "2010519",
    "end": "2016960"
  },
  {
    "text": "um the N that got the um consolidation replaced to the small no and in the next",
    "start": "2016960",
    "end": "2023399"
  },
  {
    "text": "five minutes it terminated senal and spin up the big Sizer and that created a lot of trigger",
    "start": "2023399",
    "end": "2031440"
  },
  {
    "text": "a lot of the uh part get the rescheduling yeah um so that specific",
    "start": "2031440",
    "end": "2037679"
  },
  {
    "text": "thing maybe not but like something similar as in we've definitely seen situations where like especially because",
    "start": "2037679",
    "end": "2043039"
  },
  {
    "text": "of these like really fast short-lived spikes where nodes will start get started up and then it'll be like oh",
    "start": "2043039",
    "end": "2049800"
  },
  {
    "text": " wait I need like I have now like five nodes that could be one node I'm going to consolidate them I would say it",
    "start": "2049800",
    "end": "2055960"
  },
  {
    "text": "doesn't happen that fast and there's also the fact that like we're not doing node consolidation for the htb Ingress",
    "start": "2055960",
    "end": "2062599"
  },
  {
    "text": "and so we don't have to worry about that happening with it and that is the main thing that has to scale just because of the scale with everything going through",
    "start": "2062599",
    "end": "2069398"
  },
  {
    "text": "it uh yeah I guess because we since we have a uh continuously delivery um every",
    "start": "2069399",
    "end": "2077440"
  },
  {
    "text": "minute so we have like a 10K user running our platform so that deployment",
    "start": "2077440",
    "end": "2083839"
  },
  {
    "text": "is actually triggering the consolidation very frequently and users is actually",
    "start": "2083839",
    "end": "2089320"
  },
  {
    "text": "complain for that um the other question is about your Spar instance so how do",
    "start": "2089320",
    "end": "2094480"
  },
  {
    "text": "you dealing with The Spar instance get recycled from the AWS as in like when",
    "start": "2094480",
    "end": "2099680"
  },
  {
    "text": "they're like recalled you mean or when they're reced to recyc to ask you your",
    "start": "2099680",
    "end": "2105480"
  },
  {
    "text": "Spar instance get back to the AWS pool yeah um so the answer is we we just the",
    "start": "2105480",
    "end": "2113280"
  },
  {
    "text": "the interruption queue has been important the interruption queue I mention mentioned before but in general",
    "start": "2113280",
    "end": "2118480"
  },
  {
    "text": "I think we would have to do like significant more work to deal with that gracefully because we're not doing like",
    "start": "2118480",
    "end": "2123520"
  },
  {
    "text": "the bidding and all of that so in general what will happen is that like gets recalled or whatever it's called",
    "start": "2123520",
    "end": "2128760"
  },
  {
    "text": "that node goes away and then it like gracefully terminates and then it's going to like spit up new nodes to schedule the stuff and to my knowledge",
    "start": "2128760",
    "end": "2135960"
  },
  {
    "text": "it's not going to like Des schedu the pods before it like spins up new node but I'm not 100% on that got it um so um",
    "start": "2135960",
    "end": "2144119"
  },
  {
    "text": "regarding to the Kat uh component um this for for the predictable use case",
    "start": "2144119",
    "end": "2151640"
  },
  {
    "text": "for the breaking news what about the unpredict unpredictable thing news like",
    "start": "2151640",
    "end": "2156920"
  },
  {
    "text": "a earthqu gunshot is that also possible to use the kada to scale that yeah so",
    "start": "2156920",
    "end": "2163920"
  },
  {
    "text": "currently we're mainly using it for the pral use case but like the the custom external push thing trigger I was",
    "start": "2163920",
    "end": "2170640"
  },
  {
    "text": "showing that's what we're prototyping and that would account for like okay A Team tells us they're about to send out",
    "start": "2170640",
    "end": "2176200"
  },
  {
    "text": "like a news notification they know they're we're probably going to get more traffic that would we would account for that and then we and other than that we",
    "start": "2176200",
    "end": "2182560"
  },
  {
    "text": "used use kada for like the normal CPU and metrics that you can do with HPA so rely on the some external metries to",
    "start": "2182560",
    "end": "2189200"
  },
  {
    "text": "trigger that oh not currently we are prototype that's like what we're working on but we will sorry last just to like",
    "start": "2189200",
    "end": "2196319"
  },
  {
    "text": "walk back to what I said during the presentation so the reason we're not doing it right now is because it's so little time it's just not worth it like",
    "start": "2196319",
    "end": "2202280"
  },
  {
    "text": "I think it's literally a couple seconds and then also like we're one of the downstream services and so it's like other stuff is doing things um I know",
    "start": "2202280",
    "end": "2209839"
  },
  {
    "text": "that like the team that sends out the push alerts have has an idea of audience that they're sending it out to and that's really helpful um but but it's",
    "start": "2209839",
    "end": "2216920"
  },
  {
    "text": "it's basically like H it hasn't been worth it there are other things to work on but we definitely want to do it and honestly ideally we would probably talk",
    "start": "2216920",
    "end": "2222400"
  },
  {
    "text": "to the folks sending out the push notifications or the ones requesting them and be like we really need a heads up because I think historically maybe",
    "start": "2222400",
    "end": "2229040"
  },
  {
    "text": "like years ago we got like 30 minutes now they're just like whatever couple seconds there you",
    "start": "2229040",
    "end": "2234359"
  },
  {
    "text": "go so sorry last question no these are great questions about the uh carent",
    "start": "2234359",
    "end": "2240480"
  },
  {
    "text": "controller so when I first deployed that I actually uh made up a lot of com",
    "start": "2240480",
    "end": "2246680"
  },
  {
    "text": "deoration it hand reading me to the terminat out of my um Cub yes I remember",
    "start": "2246680",
    "end": "2253720"
  },
  {
    "text": "when we migrated so have you had any alert or how do you prevent that yeah um",
    "start": "2253720",
    "end": "2261920"
  },
  {
    "text": "so we definitely have alerting didn't you set up some of the alerting for this yeah we have we have alerting and we also have multiple node groups so we",
    "start": "2261920",
    "end": "2267960"
  },
  {
    "text": "have or node pool sorry no node pool for the actual Ingress controller but we don't use spot instances cuz that's more",
    "start": "2267960",
    "end": "2275200"
  },
  {
    "text": "that's more resilient and then we have like other node groups that manage tenant workloads I think that's also how we separated it based on the compon uh",
    "start": "2275200",
    "end": "2283160"
  },
  {
    "text": "log or you based on the carent controller Matrix the controller metric",
    "start": "2283160",
    "end": "2288200"
  },
  {
    "text": "Matrix okay and we also I don't remember the specifics of it but there are like certain anonymes that we look for that we page on or at least post to like a",
    "start": "2288200",
    "end": "2294040"
  },
  {
    "text": "slack channel that we can look at so it's like ah you know this this node hasn't gone away like what's going on",
    "start": "2294040",
    "end": "2299200"
  },
  {
    "text": "stuff like that um but yeah it's not it's not perfect it's not ideal cool",
    "start": "2299200",
    "end": "2304640"
  },
  {
    "text": "thanks thanks thank you Carpenter is considering the node types",
    "start": "2304640",
    "end": "2310400"
  },
  {
    "text": "in order to do the Autos scaling is it also considering the region or the Zone uh so you have you can do that that was",
    "start": "2310400",
    "end": "2316880"
  },
  {
    "text": "actually something I didn't talk about this is a great question um so in general I'm pretty sure it balances stuff there's a bit of weirdness in that",
    "start": "2316880",
    "end": "2324200"
  },
  {
    "text": "especially with these scaling events and that like say we have you know us East 1 a b and c right the availability zones",
    "start": "2324200",
    "end": "2330800"
  },
  {
    "text": "it will balance them across and I think this is something like in the configuration that we did the problem is is that if it's like oh okay we have 20",
    "start": "2330800",
    "end": "2337280"
  },
  {
    "text": "and one 40 and one and 20 and the other it's like well let's spin up 20 in the other two but then if it like takes them",
    "start": "2337280",
    "end": "2342440"
  },
  {
    "text": "down then it's like wait we have to get rid of the other ones and then start up new ones and then it's like this constant Loop of like wait we don't have",
    "start": "2342440",
    "end": "2347720"
  },
  {
    "text": "the right number that was one of the weird behaviors we saw so I'm trying to remember what we did for it I think it just got into like a gnarly State at",
    "start": "2347720",
    "end": "2353960"
  },
  {
    "text": "some point and we just had to fix it um but I think part of that was for like the really the like regular predictable",
    "start": "2353960",
    "end": "2361960"
  },
  {
    "text": "Spike and so we were able to kind of be like okay wait I think there was some setting like with rebalancing so it",
    "start": "2361960",
    "end": "2367200"
  },
  {
    "text": "wasn't as naive about how it did it that we were able to use cool thank you thank",
    "start": "2367200",
    "end": "2372680"
  },
  {
    "text": "you so with uh the really short discreet",
    "start": "2372680",
    "end": "2378520"
  },
  {
    "text": "uh traffic uh spikes that you get have you and I'm not really sure how like",
    "start": "2378520",
    "end": "2385599"
  },
  {
    "text": "Amazon exactly like bills like when it starts when it ends have y all thought about integrating like fargate uh which",
    "start": "2385599",
    "end": "2393359"
  },
  {
    "text": "might have a faster like startup and shut down I I think initially we did consider",
    "start": "2393359",
    "end": "2399319"
  },
  {
    "text": "fargate but we never actually used it in production I think we might have done a PC so I not sure I think initially we",
    "start": "2399319",
    "end": "2406240"
  },
  {
    "text": "might have if you remember yeah okay it's like specifically like",
    "start": "2406240",
    "end": "2413640"
  },
  {
    "text": "the if you if we've used fargate to also solve the similar type of",
    "start": "2417119",
    "end": "2423000"
  },
  {
    "text": "problem uh great talk by the way it was one of the most valuable ones I think so far that I've I've been in um because we",
    "start": "2423000",
    "end": "2429800"
  },
  {
    "text": "use Carpenter we we're just starting our journey with that yeah um have you lived through any availability Zone gray",
    "start": "2429800",
    "end": "2436240"
  },
  {
    "text": "failures and how does Carpenter handle that when you say gray failures can you be specific about what you mean yeah",
    "start": "2436240",
    "end": "2441560"
  },
  {
    "text": "when you start you me an availability Zone goes down or just can't launch and ah",
    "start": "2441560",
    "end": "2448480"
  },
  {
    "text": "um I don't think we've had availability Zone gray failures or at least not to",
    "start": "2448480",
    "end": "2453720"
  },
  {
    "text": "knowledge and I'm like we should set up an alert for that cuz I would not be surprised if that's not happening so maybe I should go do that now the fun",
    "start": "2453720",
    "end": "2460319"
  },
  {
    "text": "part of that with Amazon is you don't get an alert you find out about it later I would set up an alert to like figure",
    "start": "2460319",
    "end": "2467079"
  },
  {
    "text": "out their Shenanigans is what I'm saying like I'd have to look at it and see what's going on but that's like a great point like I I should go look at that so",
    "start": "2467079",
    "end": "2473440"
  },
  {
    "text": "I can figure what I would think you'd probably have to do is is tell Carpenter",
    "start": "2473440",
    "end": "2478839"
  },
  {
    "text": "to not deal with the availability Zone and do it manually which is a problem but I was just wondering if you've seen",
    "start": "2478839",
    "end": "2484400"
  },
  {
    "text": "that before yeah and for what it's worth we we do have like multi- region so we have clusters in multiple regions and as",
    "start": "2484400",
    "end": "2491240"
  },
  {
    "text": "of recently we have a cross region surface smashh and the unified Ingress",
    "start": "2491240",
    "end": "2496680"
  },
  {
    "text": "is set up to fail over so that it's not exactly because if there's like one availability Zone that's having problems but if there's something like really",
    "start": "2496680",
    "end": "2502440"
  },
  {
    "text": "wonky happening it will just fail over to the other region and so maybe then",
    "start": "2502440",
    "end": "2508560"
  },
  {
    "text": "but like yeah it's still not great gotcha thank you thank you for a great talk um does",
    "start": "2508560",
    "end": "2515640"
  },
  {
    "text": "Carpenter take take into account compute savings plan if you have someone that's a great question that I",
    "start": "2515640",
    "end": "2520920"
  },
  {
    "text": "actually thought about today when I was uh doing the talk and I was like I don't know the answer to that and I don't have the time to look it up I don't think it",
    "start": "2520920",
    "end": "2526319"
  },
  {
    "text": "does especially as someone who has like weighted around a lot in AWS cost Explorer which is confusing very",
    "start": "2526319",
    "end": "2533440"
  },
  {
    "text": "confusing when you do savings plans and spot instances and all of that I don't think it does um and then also part of",
    "start": "2533440",
    "end": "2539280"
  },
  {
    "text": "the problem and I know for sure it doesn't in the sense of if we tell it hey do these like you can pick from",
    "start": "2539280",
    "end": "2545079"
  },
  {
    "text": "these node types it's not smart enough to be like you have a savings plan for this specific node type so I'm going to",
    "start": "2545079",
    "end": "2550440"
  },
  {
    "text": "make sure I use that node type it's a little more naive but that's that's a great question can we maybe tune the",
    "start": "2550440",
    "end": "2556440"
  },
  {
    "text": "weights on the not types to say that this is more preferable I don't know I want to say",
    "start": "2556440",
    "end": "2564040"
  },
  {
    "text": "this is something that came up before and either like we couldn't do it or it was not like we we couldn't figure out",
    "start": "2564040",
    "end": "2569920"
  },
  {
    "text": "exactly how to do it because like mind you Carpenter just graduated to Beta um so I'm not sure thank",
    "start": "2569920",
    "end": "2578240"
  },
  {
    "text": "you we got thank you everyone go ahead I have one more question no",
    "start": "2578240",
    "end": "2584480"
  },
  {
    "text": "worries I know you guys were mentioning about scale the by parts right or by notes I'm wondering what you guys to do",
    "start": "2584480",
    "end": "2590559"
  },
  {
    "text": "with other resources like uh DB or like a messaging system how do you guys scale",
    "start": "2590559",
    "end": "2595760"
  },
  {
    "text": "those like database or yeah like other resource not only parts or notes as in",
    "start": "2595760",
    "end": "2601160"
  },
  {
    "text": "like how do we scale like say a database that's not running in the cluster that like a application is communicating with",
    "start": "2601160",
    "end": "2607000"
  },
  {
    "text": "like I was thinking like uh when you guys to get a breaking use right like not only requires a parts level other",
    "start": "2607000",
    "end": "2614760"
  },
  {
    "text": "resources right maybe other resource dependent resources you guys may need how you guys scale up those",
    "start": "2614760",
    "end": "2621559"
  },
  {
    "text": "like so there is like the like built into the HPA right there's like the CPU",
    "start": "2621559",
    "end": "2627920"
  },
  {
    "text": "and memory but in terms of like the database all that I'm not sure but I do know that for stuff that's outside of",
    "start": "2627920",
    "end": "2634359"
  },
  {
    "text": "the cluster which is usually like the more common thing thing is that people are running like their main app in the cluster and maybe they're like communicating out to a database or",
    "start": "2634359",
    "end": "2640760"
  },
  {
    "text": "something that a lot of times they have to scale up ahead of time there are like multiple other teams are dealing with a sar problem okay all right thank you",
    "start": "2640760",
    "end": "2648160"
  },
  {
    "text": "thank you sorry to keep everyone here Al um so in your demonstration you kind",
    "start": "2648160",
    "end": "2654720"
  },
  {
    "text": "of mentioned that you use the Cron job part of K however I believe it does require that you give it like an actual",
    "start": "2654720",
    "end": "2661599"
  },
  {
    "text": "number to scale up to so like how did you derive that number do you use hysterical data um I'm guessing this is",
    "start": "2661599",
    "end": "2667680"
  },
  {
    "text": "for the regularly occurring spikes so yeah yeah we use historical data and it's like obviously like like slightly",
    "start": "2667680",
    "end": "2674240"
  },
  {
    "text": "over provisioned so it's like historical data plus like 20% but yeah it's a set number of replicas that we scale our",
    "start": "2674240",
    "end": "2679720"
  },
  {
    "text": "Ingress controller up to and to be clear the number in the slide was not the number we do because I'm not going to put that in the",
    "start": "2679720",
    "end": "2685800"
  },
  {
    "text": "slide kind of on the flip side so when you have those sudden spikes what do you actually use to determine the number of",
    "start": "2685800",
    "end": "2693520"
  },
  {
    "text": "PODS to scale up to is it a CP you memory like resourcing or is there an",
    "start": "2693520",
    "end": "2698640"
  },
  {
    "text": "external metric because you're saying that previously you got like a 30 minute heads up but now you get a few seconds",
    "start": "2698640",
    "end": "2703800"
  },
  {
    "text": "um that's assuming that you have some sort of coordination between the breaking news alert versus like if",
    "start": "2703800",
    "end": "2709680"
  },
  {
    "text": "there's an earthquake sudden you know everyone's going to check their phone and Google you know is there an earthquake near me so how do you manage",
    "start": "2709680",
    "end": "2715520"
  },
  {
    "text": "the sudden spikes rather than the regular spikes so the answer is um just",
    "start": "2715520",
    "end": "2721079"
  },
  {
    "text": "hope for the I mean okay that's a little bit of an over simplification um but we are like continuously trying to decrease",
    "start": "2721079",
    "end": "2726920"
  },
  {
    "text": "the amount of time it takes to scale so I think at this point we've we've gotten like most of what we're going to get out",
    "start": "2726920",
    "end": "2733359"
  },
  {
    "text": "of it but we're hoping the stuff improves over time and I do really think that the main way that's going to like",
    "start": "2733359",
    "end": "2738720"
  },
  {
    "text": "help with this problem is either getting a headset from the team that requests the push alert or uh doing this external",
    "start": "2738720",
    "end": "2745520"
  },
  {
    "text": "scaler in kada that's like the custom scaler that uh subscribes to the web hook uh because they're really",
    "start": "2745520",
    "end": "2751000"
  },
  {
    "text": "unpredictable and also you don't necessarily know what's going to go out I think the only thing that we have is like a slack Channel and then maybe you could get some idea um but in general",
    "start": "2751000",
    "end": "2758160"
  },
  {
    "text": "like if an election is coming up we're like oh we know we're going to get more traffic so we're probably going to over-provision some because like I'm",
    "start": "2758160",
    "end": "2765280"
  },
  {
    "text": "sure for when the debate was happening right we probably saw a really big spike I wasn't paying attention because I wasn't on call and I was here but yeah",
    "start": "2765280",
    "end": "2773079"
  },
  {
    "text": "and also the the expected traffic you were showing that scale that spike is much quicker like that's like a few",
    "start": "2773079",
    "end": "2779559"
  },
  {
    "text": "seconds versus like 30 seconds a minute on the other one so it's like we're able to scale for that yeah just curious if",
    "start": "2779559",
    "end": "2784680"
  },
  {
    "text": "like CPU or was an adequate signal to scale up in that rapid time period because you know you have pod startup",
    "start": "2784680",
    "end": "2791319"
  },
  {
    "text": "and scheduling and all that good stuff so just wondering thank you so much thank",
    "start": "2791319",
    "end": "2796760"
  },
  {
    "text": "you",
    "start": "2798640",
    "end": "2801640"
  }
]