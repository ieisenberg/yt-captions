[
  {
    "text": "good morning welcome to our session i'm Yen Chen from",
    "start": "160",
    "end": "6160"
  },
  {
    "text": "Nvidia okay let's echo from another room uh I'm working and with the Nvidia DJX",
    "start": "6359",
    "end": "14000"
  },
  {
    "text": "cloud to build Kubernetes optimized AI infrastructure platform and for AI and",
    "start": "14000",
    "end": "21039"
  },
  {
    "text": "GPU workload hello I'm Chen uh a senior research scientist from uh IBM research",
    "start": "21039",
    "end": "27519"
  },
  {
    "text": "i've been a very active contributors to kubernetes and six kubernetes including the autoscaling scheduling community and",
    "start": "27519",
    "end": "34480"
  },
  {
    "text": "now uh I'm contributing to the working group of serving as well uh like this benchmarking tool",
    "start": "34480",
    "end": "42600"
  },
  {
    "text": "okay so in today's session we are going to walk through how to run model",
    "start": "42600",
    "end": "49360"
  },
  {
    "text": "inference benchmarking use the Triton inference server and uh FM performance",
    "start": "49360",
    "end": "56480"
  },
  {
    "text": "tools we will also cover how to run the GPU intensive workload in this talk",
    "start": "56480",
    "end": "62239"
  },
  {
    "text": "we'll also cover some of the monitoring tool like the Nvidia SMI and another",
    "start": "62239",
    "end": "67600"
  },
  {
    "text": "tool called GPU stats uh we'll briefly and uh overview give an overview of all",
    "start": "67600",
    "end": "73439"
  },
  {
    "text": "the benchmark tools hopefully and uh you will find this session helpful",
    "start": "73439",
    "end": "80759"
  },
  {
    "text": "okay let me start it and uh so firstly the Triton inference server is very",
    "start": "81040",
    "end": "86479"
  },
  {
    "text": "popular and uh inference server and it's open source tool can run and uh a bunch",
    "start": "86479",
    "end": "92320"
  },
  {
    "text": "of different models and uh from like uh the typical and uh uh all this and",
    "start": "92320",
    "end": "99439"
  },
  {
    "text": "popular models and you can use it it supported x86 and ARM also provide uh uh",
    "start": "99439",
    "end": "106320"
  },
  {
    "text": "some client library and the tools including uh running on the edge and",
    "start": "106320",
    "end": "111600"
  },
  {
    "text": "mobile devices uh also recently Nvidia announced and advanced version called",
    "start": "111600",
    "end": "117799"
  },
  {
    "text": "Dynamo it extend and uh optimize the Triton uh with advanced features for",
    "start": "117799",
    "end": "124880"
  },
  {
    "text": "improved scalability and performance so in order to set up a triton uh",
    "start": "124880",
    "end": "132400"
  },
  {
    "text": "inference so there are three and key steps the firstly and you need to create",
    "start": "132400",
    "end": "137840"
  },
  {
    "text": "a model repository and populate and with all kind of different models I will",
    "start": "137840",
    "end": "143040"
  },
  {
    "text": "cover next the some details how to do that then the second is deploy the",
    "start": "143040",
    "end": "148480"
  },
  {
    "text": "Triton server instance itself also and Triton provide and there are some client",
    "start": "148480",
    "end": "155440"
  },
  {
    "text": "or workload generation tool and the performance uh collection tool uh core",
    "start": "155440",
    "end": "160800"
  },
  {
    "text": "performance analyzer can collect send request and collect all this performance",
    "start": "160800",
    "end": "166160"
  },
  {
    "text": "data so I will go through each of them one by one so for model and uh",
    "start": "166160",
    "end": "172519"
  },
  {
    "text": "uh deployment and Triton support and all different models from PyTorch and uh to",
    "start": "172519",
    "end": "179440"
  },
  {
    "text": "like the standard OMX models and not VLM",
    "start": "179440",
    "end": "184560"
  },
  {
    "text": "so some example models like Nama and FCON and other thing so if you look at",
    "start": "184560",
    "end": "190599"
  },
  {
    "text": "the the the structure of the model repository basically and you can",
    "start": "190599",
    "end": "196000"
  },
  {
    "text": "populate and deploy and all different models so each model is a subdirectory",
    "start": "196000",
    "end": "201680"
  },
  {
    "text": "you name and the models under each model there are configuration file also you",
    "start": "201680",
    "end": "207040"
  },
  {
    "text": "can have multiple versions for each type of models so for example just one two three name it so on the right side here",
    "start": "207040",
    "end": "214480"
  },
  {
    "text": "is the examples for example you have text detection model version one right you have another one is text recognition",
    "start": "214480",
    "end": "222080"
  },
  {
    "text": "and uh yeah under this directory is the real model file and uh yeah there are",
    "start": "222080",
    "end": "228480"
  },
  {
    "text": "the Triton supports like I mentioned all different and uh a variety and uh uh",
    "start": "228480",
    "end": "234799"
  },
  {
    "text": "type or format of the models so this is you populate it and the models so a lot",
    "start": "234799",
    "end": "240319"
  },
  {
    "text": "of models and available online you can just download it and uh yeah store and",
    "start": "240319",
    "end": "245439"
  },
  {
    "text": "uh in a storage or file system so the second step is uh set up",
    "start": "245439",
    "end": "251040"
  },
  {
    "text": "configure the Triton server and we have containerized the version so the Triton server right and have some client API re",
    "start": "251040",
    "end": "259040"
  },
  {
    "text": "uh receive the request it support both the HTTP and gRPC protocol then inside",
    "start": "259040",
    "end": "266000"
  },
  {
    "text": "their flow process request send back the the response so this is a sample the YAML",
    "start": "266000",
    "end": "273759"
  },
  {
    "text": "file and very simple and the most important thing right is you set up the",
    "start": "273759",
    "end": "279520"
  },
  {
    "text": "Triton server ports it support both the like the HTTP protocol which port we",
    "start": "279520",
    "end": "284960"
  },
  {
    "text": "listening to also the RP GIC the port so then you specify most important thing is",
    "start": "284960",
    "end": "291440"
  },
  {
    "text": "the model repository right you can mount and from your local host on this simple example but it can be anywhere right in",
    "start": "291440",
    "end": "298479"
  },
  {
    "text": "the distributed system additionally you set up service right listening to the port and where you want to get and uh",
    "start": "298479",
    "end": "306400"
  },
  {
    "text": "listen and receive the request like here right we have GIC protocol port and as",
    "start": "306400",
    "end": "312160"
  },
  {
    "text": "well as the HTTP one so the third step right you have to",
    "start": "312160",
    "end": "318320"
  },
  {
    "text": "generate some workload and send the workload and a very nice tool is called a performance and analyzer it can",
    "start": "318320",
    "end": "325520"
  },
  {
    "text": "generate the request sent to the triton and the inference server it's also support tensson flow and the torch ser",
    "start": "325520",
    "end": "332320"
  },
  {
    "text": "as well so they're all different mode and uh you can and send the request and",
    "start": "332320",
    "end": "337919"
  },
  {
    "text": "from the like the specified concurrency level and the specified the read or you",
    "start": "337919",
    "end": "343840"
  },
  {
    "text": "can also the interval right how you want to send and the the request also how you want to measure the performance here is",
    "start": "343840",
    "end": "351520"
  },
  {
    "text": "some sample and the output right how many requests you send also the latency",
    "start": "351520",
    "end": "357039"
  },
  {
    "text": "different percentile percentile 50 percentile 99 percentile and detail and",
    "start": "357039",
    "end": "362240"
  },
  {
    "text": "also the different concurrency level so there are tons of different configurations you can try and uh check",
    "start": "362240",
    "end": "368319"
  },
  {
    "text": "out the the reference for how to use this and performance and analyzer",
    "start": "368319",
    "end": "374560"
  },
  {
    "text": "yeah if you want to run a containerized and a client generation this is very simple example file you can see and uh",
    "start": "374560",
    "end": "382479"
  },
  {
    "text": "the the most important thing you specify here is you want to send a gc or http",
    "start": "382479",
    "end": "388240"
  },
  {
    "text": "request and also which model you want to send the request to as you see and early",
    "start": "388240",
    "end": "393280"
  },
  {
    "text": "so for the Triton server we can populate the the Triton inference server with a bunch of model here specify this and",
    "start": "393280",
    "end": "399840"
  },
  {
    "text": "rest that 50 as the model and also request here is very simple just specify",
    "start": "399840",
    "end": "405520"
  },
  {
    "text": "I want to send a constant and request 200 and request per second okay another",
    "start": "405520",
    "end": "412000"
  },
  {
    "text": "thing I want to mention yeah recently as part of the performance uh analyzer and",
    "start": "412000",
    "end": "418080"
  },
  {
    "text": "they are new and workload generate called GI PF so it's a specific tool for",
    "start": "418080",
    "end": "425280"
  },
  {
    "text": "measure and do the benchmark for the generative AI models so it's still under",
    "start": "425280",
    "end": "430639"
  },
  {
    "text": "active development but please check out if you're interesting it can provide a lot of the metrics and the data for the",
    "start": "430639",
    "end": "437840"
  },
  {
    "text": "JI specific and uh workloads yeah like the time to first token and then the",
    "start": "437840",
    "end": "443680"
  },
  {
    "text": "inter latency between tokens are the thing it could be very and help helpful",
    "start": "443680",
    "end": "449440"
  },
  {
    "text": "useful tools to understand the behavior of this and JI and the model",
    "start": "449440",
    "end": "455160"
  },
  {
    "text": "inference so as part of the benchmark we know right monitoring tool and other",
    "start": "455160",
    "end": "460240"
  },
  {
    "text": "things is important so of course if you are familiar with the GPU setup the most useful or you should consider to use",
    "start": "460240",
    "end": "466960"
  },
  {
    "text": "Nvidia SMI Nvidia system management interface to collect the data and it can",
    "start": "466960",
    "end": "472560"
  },
  {
    "text": "show you this is A100 which each load have eight and GPU devices right like",
    "start": "472560",
    "end": "478080"
  },
  {
    "text": "the GPU utilization temperature memory usage or other thing another useful tools I used and from open source called",
    "start": "478080",
    "end": "485599"
  },
  {
    "text": "GPU stats I think it's a very and useful tools can show you real time right the",
    "start": "485599",
    "end": "492080"
  },
  {
    "text": "each of the GPU devices temperature GPU utilization as well as the memory",
    "start": "492080",
    "end": "497360"
  },
  {
    "text": "consumptions so you can try it so I also list some of the reference",
    "start": "497360",
    "end": "503000"
  },
  {
    "text": "here okay so use cases actually we use this and Triton and the inference server",
    "start": "503000",
    "end": "508800"
  },
  {
    "text": "uh my colleague Kevin Cruz and I gave a presentation last CubeCon at CubeCon so",
    "start": "508800",
    "end": "514159"
  },
  {
    "text": "click city and we do a benchmark study compare different GPU share strategy",
    "start": "514159",
    "end": "519599"
  },
  {
    "text": "like time slice ing and MPS multiprocess services compare the trade off and uh uh",
    "start": "519599",
    "end": "526480"
  },
  {
    "text": "different sharing strategy for different workload so if I interesting check out and we gain a lot of the insight by",
    "start": "526480",
    "end": "533760"
  },
  {
    "text": "using this and Triton uh uh benchmark and uh inference server so okay so next",
    "start": "533760",
    "end": "540959"
  },
  {
    "text": "let me see hopefully I still have time okay I will try to give a live demo even I recorded and I know live demo is risky",
    "start": "540959",
    "end": "548880"
  },
  {
    "text": "but uh I think it's probably more interesting so let me see if I",
    "start": "548880",
    "end": "554040"
  },
  {
    "text": "can make it work",
    "start": "554040",
    "end": "558600"
  },
  {
    "text": "so I hope can you still hear me hello okay so now and",
    "start": "559279",
    "end": "567519"
  },
  {
    "text": "uh this is actually on my uh workstation",
    "start": "567880",
    "end": "573519"
  },
  {
    "text": "so like I mentioned you need and populate a model first so here I already",
    "start": "573519",
    "end": "579360"
  },
  {
    "text": "know populate a bunch of the models and like I show you Ernie and uh the model",
    "start": "579360",
    "end": "584880"
  },
  {
    "text": "and uh you can see each model right and",
    "start": "584880",
    "end": "589519"
  },
  {
    "text": "uh yeah there will be a model like the label and file yeah whatever and uh yeah you",
    "start": "590760",
    "end": "598000"
  },
  {
    "text": "define it then under the model and you have I I I only for this one have a",
    "start": "598000",
    "end": "603120"
  },
  {
    "text": "version one right if you look at that's the real model right it's quite big and uh Yeah that's basically what you need",
    "start": "603120",
    "end": "609760"
  },
  {
    "text": "and we have a bunch of model already here so now and I deploy this model",
    "start": "609760",
    "end": "614800"
  },
  {
    "text": "right and uh as I show earlier yeah this is the YAML file very",
    "start": "614800",
    "end": "622399"
  },
  {
    "text": "straightforward and there are already image provided and in our Nvidia and registry and you specify the the mount",
    "start": "622399",
    "end": "629760"
  },
  {
    "text": "of the model and a bunch of metrics you want to connect it and specify the port and other thing okay",
    "start": "629760",
    "end": "638680"
  },
  {
    "text": "okay as you see it's running okay let's see and uh",
    "start": "642000",
    "end": "650120"
  },
  {
    "text": "Okay that's the the log of the server Triton server as you see here a bunch of the model already populated right we",
    "start": "656000",
    "end": "662640"
  },
  {
    "text": "have like uh uh nine or different models also with hue right it's listening to",
    "start": "662640",
    "end": "668240"
  },
  {
    "text": "the different port for different request okay now let's see the client and",
    "start": "668240",
    "end": "675279"
  },
  {
    "text": "uh okay the client was very quite simple this example one right you just run also",
    "start": "677880",
    "end": "683200"
  },
  {
    "text": "their containerized version you just run the performance and uh analyzer specify",
    "start": "683200",
    "end": "689040"
  },
  {
    "text": "the port you want to send also the model name And I want to measure the 95 percentile and use GPC protocol request",
    "start": "689040",
    "end": "696640"
  },
  {
    "text": "rate is 200 right okay let's see",
    "start": "696640",
    "end": "701519"
  },
  {
    "text": "and use the yeah Nvidia SMI you can see the",
    "start": "710200",
    "end": "716880"
  },
  {
    "text": "utilization here right both and are running and also",
    "start": "716880",
    "end": "723320"
  },
  {
    "text": "yeah that's the tool you can you you can see because I send the client send a constant request so the GP utilization",
    "start": "723440",
    "end": "730160"
  },
  {
    "text": "is 21 22% yeah I just run a very short period of time it's done now if you go",
    "start": "730160",
    "end": "736320"
  },
  {
    "text": "back to look at the output in the",
    "start": "736320",
    "end": "743360"
  },
  {
    "text": "client okay you will see this and the detail of the like this one of course the batch size one you can specify other",
    "start": "746120",
    "end": "753120"
  },
  {
    "text": "thing how many request is greater send like 3,600 request or different",
    "start": "753120",
    "end": "758959"
  },
  {
    "text": "percentile of the latency and throughput or other thing so this is just the basic set of the benchmark and hopefully you",
    "start": "758959",
    "end": "765519"
  },
  {
    "text": "find yeah it's quite straightforward that containerized the version you populate the model you just start a",
    "start": "765519",
    "end": "771360"
  },
  {
    "text": "triton server you can use this performance analyzer to generate a workload collect the detail of the",
    "start": "771360",
    "end": "777240"
  },
  {
    "text": "information okay so now I will and",
    "start": "777240",
    "end": "782639"
  },
  {
    "text": "uh switch back and give it to Chen to",
    "start": "783240",
    "end": "788839"
  },
  {
    "text": "present the",
    "start": "788839",
    "end": "792839"
  },
  {
    "text": "Okay the foundation model performance hello um so today I'm bringing a project",
    "start": "795279",
    "end": "802639"
  },
  {
    "text": "uh initially uh developed by uh IBM research and we uh actually open source",
    "start": "802639",
    "end": "808160"
  },
  {
    "text": "it in uh the open source community and later donated to the uh serving working",
    "start": "808160",
    "end": "813839"
  },
  {
    "text": "group uh to consolidate like opensource community effort so as you already see",
    "start": "813839",
    "end": "820320"
  },
  {
    "text": "that you showed the general workflow of the benchmarking process you need to prepare the server deployment you need",
    "start": "820320",
    "end": "827360"
  },
  {
    "text": "to prepare the load testing uh job deployment you need to collect the data necessary data so this framework is",
    "start": "827360",
    "end": "835279"
  },
  {
    "text": "exactly help you to ease that process by uh providing you a pro uh programming",
    "start": "835279",
    "end": "841680"
  },
  {
    "text": "interface in Python and let's go through the details of it so um what is fmpperf",
    "start": "841680",
    "end": "848560"
  },
  {
    "text": "right uh it's a python based benchmarking tooling library designed specifically for uh evaluating",
    "start": "848560",
    "end": "855600"
  },
  {
    "text": "performance uh efficiency energy efficiency of uh large language model or",
    "start": "855600",
    "end": "860720"
  },
  {
    "text": "genai serving frameworks such as the uh server VM server or TJ server uh which",
    "start": "860720",
    "end": "867600"
  },
  {
    "text": "is the IBM folk version of TGI server and then why we want to use FMerve uh",
    "start": "867600",
    "end": "873279"
  },
  {
    "text": "because it kind of provide you a simple but very powerful Python API so you can",
    "start": "873279",
    "end": "879279"
  },
  {
    "text": "uh so it allows you to deploy inference servers as Kubernetes deployment and services without the hassle to define",
    "start": "879279",
    "end": "886639"
  },
  {
    "text": "all your YAML deployment uh service uh YAML etc and then it gave you the simple",
    "start": "886639",
    "end": "893279"
  },
  {
    "text": "Python interface to create different types of load testing and within the",
    "start": "893279",
    "end": "898320"
  },
  {
    "text": "Kubernetes or open shift environment uh so why we target Kubernetes so of course",
    "start": "898320",
    "end": "904240"
  },
  {
    "text": "this is KubeC everybody is concerned about Kubernetes but the initial objective was is um Kubernetes",
    "start": "904240",
    "end": "911440"
  },
  {
    "text": "involvement actually allow you to do consistent and reproducible uh",
    "start": "911440",
    "end": "917360"
  },
  {
    "text": "performance benchmarking and testing across different types of servers different types of load testers and also",
    "start": "917360",
    "end": "924959"
  },
  {
    "text": "different types of infrastructure so this is the high level architecture of FMF basically you have a",
    "start": "924959",
    "end": "933360"
  },
  {
    "text": "module called FMerve library which um you can use uh in a simple Python script",
    "start": "933360",
    "end": "939600"
  },
  {
    "text": "to configure what server you want to benchmark what types of load tester you want and then uh how long you want to",
    "start": "939600",
    "end": "946240"
  },
  {
    "text": "run the experiment right and then um behind the scene it actually used the Kubernetes uh client library to allow",
    "start": "946240",
    "end": "953920"
  },
  {
    "text": "you to to deploy different server deployment uh create corresponding services mounted the necessary",
    "start": "953920",
    "end": "960399"
  },
  {
    "text": "persistent volumes and here we specifically uh by default it will mount",
    "start": "960399",
    "end": "965600"
  },
  {
    "text": "the uh slashmodels for your model um uh files and the slash request for",
    "start": "965600",
    "end": "971279"
  },
  {
    "text": "different types of data traces you want to do the load testing and then um for the uh for the server deployment right",
    "start": "971279",
    "end": "978720"
  },
  {
    "text": "now we already support uh VM and TJS and we expect to support more like Triton",
    "start": "978720",
    "end": "984560"
  },
  {
    "text": "and uh other servers for example SGAN so uh here is a list of initial",
    "start": "984560",
    "end": "989600"
  },
  {
    "text": "contributors from IBM research i really want to thank uh to them i'm the uh one of the first adopter of this uh FMPF uh",
    "start": "989600",
    "end": "998399"
  },
  {
    "text": "library it helps me a lot in terms of benchmarking uh performance across different hardware uh servers and",
    "start": "998399",
    "end": "1005680"
  },
  {
    "text": "different versions of the servers for optimizations so um key features of FFM perf uh first",
    "start": "1005680",
    "end": "1013360"
  },
  {
    "text": "it's um automated deployment using simple python code I will show the",
    "start": "1013360",
    "end": "1018800"
  },
  {
    "text": "example and then it provide uh a simple load testers for different scenarios",
    "start": "1018800",
    "end": "1024798"
  },
  {
    "text": "like homogeneous workload scenario hegenius workload scenario and also uh we have some models behind the scene uh",
    "start": "1024799",
    "end": "1032640"
  },
  {
    "text": "based on our real production traces uh so you have more realistic workload uh",
    "start": "1032640",
    "end": "1039280"
  },
  {
    "text": "testing and then right now it support multiple inference servers and we will expand the list uh in the future and",
    "start": "1039280",
    "end": "1046319"
  },
  {
    "text": "then we extract uh LM specific metrics as Yan already mentioned the important",
    "start": "1046319",
    "end": "1051600"
  },
  {
    "text": "thing uh for example time to first token inter token latencies throughput and",
    "start": "1051600",
    "end": "1056640"
  },
  {
    "text": "then uh in addition we actually collect the metrics from GPU as well using the tool uh you mentioned so we have some",
    "start": "1056640",
    "end": "1063679"
  },
  {
    "text": "energy efficiency metrics uh available as well and uh because you have a simple",
    "start": "1063679",
    "end": "1069919"
  },
  {
    "text": "uh Python library you can write your script and then it can also help you for example you are developing and",
    "start": "1069919",
    "end": "1075679"
  },
  {
    "text": "optimizing a server framework and you can also uh use this library to write",
    "start": "1075679",
    "end": "1080960"
  },
  {
    "text": "your own Python script to integrate with your CI/CD pipeline so every version of",
    "start": "1080960",
    "end": "1086720"
  },
  {
    "text": "optimization you did you can kind of compare and automate the performance benchmarking for your servers",
    "start": "1086720",
    "end": "1094480"
  },
  {
    "text": "so um I will then uh dive into how can we start using it is very very simple we",
    "start": "1094480",
    "end": "1100720"
  },
  {
    "text": "just need to define three types of um Python classes the first one is cluster",
    "start": "1100720",
    "end": "1106960"
  },
  {
    "text": "of course they support local and remote uh Kubernetes servers as long as your",
    "start": "1106960",
    "end": "1112080"
  },
  {
    "text": "client laptop has access to the all connection to your clusters uh you can",
    "start": "1112080",
    "end": "1117360"
  },
  {
    "text": "define it using for example the uh load cube config uh in Kubernetes client to",
    "start": "1117360",
    "end": "1122559"
  },
  {
    "text": "initialize the uh cluster uh class and then the second one is workload spec",
    "start": "1122559",
    "end": "1129280"
  },
  {
    "text": "workload spec allows you to define like what load testers you want to use for example here we use the default FM perf",
    "start": "1129280",
    "end": "1136880"
  },
  {
    "text": "uh uh load tester uh image but you can change it to uh Triton client libraries",
    "start": "1136880",
    "end": "1143520"
  },
  {
    "text": "or other types of load tester libraries and it it provides three types of um",
    "start": "1143520",
    "end": "1150160"
  },
  {
    "text": "workflow specs including homogeneous workload spec uh which just defines the",
    "start": "1150160",
    "end": "1155679"
  },
  {
    "text": "uh fixed input output tokens uh for the stressing uh testing purposes so it's",
    "start": "1155679",
    "end": "1161280"
  },
  {
    "text": "more consistent and reproducible benchmarking use cases and then you can define the heterogeneous workloads back",
    "start": "1161280",
    "end": "1168160"
  },
  {
    "text": "that allows you to uh plug in different random distributions for your uh input",
    "start": "1168160",
    "end": "1173600"
  },
  {
    "text": "output tokens for example and then it's ideal for testing performance under some diverse request patterns and then again",
    "start": "1173600",
    "end": "1181520"
  },
  {
    "text": "realistic workflows back is uh something we contributed to the open source from",
    "start": "1181520",
    "end": "1187600"
  },
  {
    "text": "uh FMerf uh it really uh learned the the trace patterns from our production logs",
    "start": "1187600",
    "end": "1194480"
  },
  {
    "text": "and then we provide statistical modeling fitted to those uh production logs so uh",
    "start": "1194480",
    "end": "1200080"
  },
  {
    "text": "you see more realistic pythons so the third one is model spec",
    "start": "1200080",
    "end": "1205520"
  },
  {
    "text": "of course you want to understand which model you want to benchmark and then some configurations of models whether",
    "start": "1205520",
    "end": "1211600"
  },
  {
    "text": "you want to enable uh for example the quantization uh some different",
    "start": "1211600",
    "end": "1216720"
  },
  {
    "text": "configurations in model compilations and um uh we specifically right now support",
    "start": "1216720",
    "end": "1223120"
  },
  {
    "text": "uh TJS model spec and VR model spec because they allow different input",
    "start": "1223120",
    "end": "1228320"
  },
  {
    "text": "argument configurations for the servers and um of course you can also uh define",
    "start": "1228320",
    "end": "1234559"
  },
  {
    "text": "for example what GPUs you want to benchmark this uh server on uh what is",
    "start": "1234559",
    "end": "1239600"
  },
  {
    "text": "the CPU and memory allocations you want the deployment to be so those are the",
    "start": "1239600",
    "end": "1246240"
  },
  {
    "text": "just three simple uh class definitions or declarations you want to have in your",
    "start": "1246240",
    "end": "1251679"
  },
  {
    "text": "Python script and then the only function you need to uh call is called run",
    "start": "1251679",
    "end": "1257360"
  },
  {
    "text": "benchmark so basically you pass the cluster model back workflow spec into",
    "start": "1257360",
    "end": "1262480"
  },
  {
    "text": "this function and then you configure how many times you want to repeat the experiments and then uh how many",
    "start": "1262480",
    "end": "1268480"
  },
  {
    "text": "concurrent users you want to emulate or simulate in your benchmarking and then",
    "start": "1268480",
    "end": "1273840"
  },
  {
    "text": "how long you want to run the experiments about and then uh the all the FM perf",
    "start": "1273840",
    "end": "1279600"
  },
  {
    "text": "library will help you go through the workflow of deploying a model server generated the workload and runs",
    "start": "1279600",
    "end": "1286799"
  },
  {
    "text": "evaluator parts to uh collect the data and finally you just need to check the",
    "start": "1286799",
    "end": "1293720"
  },
  {
    "text": "data so and then uh for the for the results we actually provide two types of",
    "start": "1293720",
    "end": "1299840"
  },
  {
    "text": "uh results one is the CSV uh summary of all your experiments uh in different",
    "start": "1299840",
    "end": "1305520"
  },
  {
    "text": "reputations what are the statistics of uh for example throughoot prefill latencies um inter token latencies and",
    "start": "1305520",
    "end": "1314320"
  },
  {
    "text": "tune latencies etc and then if you want to get into the details we also have uh",
    "start": "1314320",
    "end": "1319440"
  },
  {
    "text": "the the load tester pod will actually write some detailed information about",
    "start": "1319440",
    "end": "1324559"
  },
  {
    "text": "the time stamp of each token and then uh the uh the the inter token latencies in",
    "start": "1324559",
    "end": "1331360"
  },
  {
    "text": "a JSON file so you can later review all those results",
    "start": "1331360",
    "end": "1338640"
  },
  {
    "text": "so okay so compared to popular those uh",
    "start": "1339159",
    "end": "1346240"
  },
  {
    "text": "popular benchmarks you may heard of like ML perve um uh the the distinct capabilities",
    "start": "1346240",
    "end": "1353200"
  },
  {
    "text": "FMerve is providing is really um Kubernetes integration so Kubernetes",
    "start": "1353200",
    "end": "1358880"
  },
  {
    "text": "involvement allow you to do the repeated uh reproducible results of benchmarking",
    "start": "1358880",
    "end": "1364320"
  },
  {
    "text": "and then for example ML perf is more focused on giving you some accuracy and the latency constraint and then let",
    "start": "1364320",
    "end": "1371200"
  },
  {
    "text": "different uh submitters to uh improve their optimizations uh to to to show",
    "start": "1371200",
    "end": "1378000"
  },
  {
    "text": "which server has better performance however uh those are not not necessarily easy to reproduce and then LM Perf um",
    "start": "1378000",
    "end": "1386799"
  },
  {
    "text": "run load test again the existing uh um services especially commercial services",
    "start": "1386799",
    "end": "1392320"
  },
  {
    "text": "so it doesn't give you the choice to deploy uh your own um optimized server",
    "start": "1392320",
    "end": "1397799"
  },
  {
    "text": "versions and then fmperf is really uh for yourself to compare for example",
    "start": "1397799",
    "end": "1403520"
  },
  {
    "text": "different versions of your inference servers and different versions of your hardwares uh to actually give you an",
    "start": "1403520",
    "end": "1410480"
  },
  {
    "text": "idea on whether you are uh improving your server performance or you are uh",
    "start": "1410480",
    "end": "1415600"
  },
  {
    "text": "you got some regressions in your server performance so there are also a lot of",
    "start": "1415600",
    "end": "1421200"
  },
  {
    "text": "other open-source um um benchmarking tools from uh different communities uh",
    "start": "1421200",
    "end": "1429039"
  },
  {
    "text": "we actually in the uh working group of serving we started all of those and then",
    "start": "1429039",
    "end": "1434960"
  },
  {
    "text": "uh we decided to consolidate the effort into a standard way uh to contribute and",
    "start": "1434960",
    "end": "1440799"
  },
  {
    "text": "then FMerve we actually donate FMP to this community so uh we help other",
    "start": "1440799",
    "end": "1447120"
  },
  {
    "text": "companies or other contributors to not repeat the same tooling process like",
    "start": "1447120",
    "end": "1452559"
  },
  {
    "text": "creating YAML uh for for the deployment and jobs and then um so the the unique",
    "start": "1452559",
    "end": "1458799"
  },
  {
    "text": "part is uh we um we we focus on the code first approach so we provide code",
    "start": "1458799",
    "end": "1465679"
  },
  {
    "text": "library to easily launch the ben benchmarking and then uh we we want to",
    "start": "1465679",
    "end": "1471440"
  },
  {
    "text": "this class to be extendable tensible so we can easily extended to more types of servers right and then we focus on uh",
    "start": "1471440",
    "end": "1478960"
  },
  {
    "text": "kubernetes uh native integrations so I will show a quick",
    "start": "1478960",
    "end": "1485159"
  },
  {
    "text": "demo on how we can easily",
    "start": "1485159",
    "end": "1489600"
  },
  {
    "text": "use fmperf",
    "start": "1490840",
    "end": "1494840"
  },
  {
    "text": "So here we uh have some local folders mounted to the current cluster uh on the",
    "start": "1496080",
    "end": "1501440"
  },
  {
    "text": "mount models and mount uh request and then we first just check if the uh the",
    "start": "1501440",
    "end": "1507880"
  },
  {
    "text": "both volumes are properly",
    "start": "1507880",
    "end": "1512960"
  },
  {
    "text": "mounted and then here I I will show a simple like VM uh benchmarking script so",
    "start": "1513159",
    "end": "1519760"
  },
  {
    "text": "you can see all you need to do is to define the clusters back workloads back",
    "start": "1519760",
    "end": "1525440"
  },
  {
    "text": "initialize the cluster right configure some security context and",
    "start": "1525440",
    "end": "1532440"
  },
  {
    "text": "then configure how long you want to run the experiment and just run benchmark those are all the things you need to",
    "start": "1532440",
    "end": "1540520"
  },
  {
    "text": "do then we run it you can see like in the cube control get parts monitoring uh",
    "start": "1540520",
    "end": "1547919"
  },
  {
    "text": "we already uh started creating the uh deployment of this VM",
    "start": "1547919",
    "end": "1554320"
  },
  {
    "text": "server and then of course it takes a while for the VM server to boot up and",
    "start": "1554919",
    "end": "1561120"
  },
  {
    "text": "when it's ready it will continue checking your workload back",
    "start": "1561120",
    "end": "1566760"
  },
  {
    "text": "configurations and start um another job right here is the FM generator so",
    "start": "1566760",
    "end": "1576320"
  },
  {
    "text": "this workload spec is using the uh the the native FM load",
    "start": "1576320",
    "end": "1583200"
  },
  {
    "text": "testers and after configuring what data set you want to import into your load",
    "start": "1585159",
    "end": "1590400"
  },
  {
    "text": "testers we will launch a pod called evaluator the evaluator is really just sending the request to the server and",
    "start": "1590400",
    "end": "1597279"
  },
  {
    "text": "then summarizing the results so we actually launched two",
    "start": "1597279",
    "end": "1603520"
  },
  {
    "text": "experiments here one is emulating one user keeps continuous sending request",
    "start": "1603520",
    "end": "1609039"
  },
  {
    "text": "the other is two concurrent users sending request so here are the results of the",
    "start": "1609039",
    "end": "1615919"
  },
  {
    "text": "summary this is the CSV um and then you can configure a parameter called repetation um so you can conduct the",
    "start": "1615919",
    "end": "1624000"
  },
  {
    "text": "same experiment multiple times with the same configuration you get so you get very robust performance benchmarking",
    "start": "1624000",
    "end": "1631640"
  },
  {
    "text": "results and then here is a detailed uh the detailed result JSON file that gives",
    "start": "1631640",
    "end": "1638159"
  },
  {
    "text": "you each tokens timestamp uh decoding times amount prevailing time stamp and",
    "start": "1638159",
    "end": "1643440"
  },
  {
    "text": "how long it takes so this is really a quick demo and the the demo video is available on YouTube as well um so as I",
    "start": "1643440",
    "end": "1653279"
  },
  {
    "text": "mentioned so we we actually donate this we actually donate this uh library to",
    "start": "1653720",
    "end": "1659760"
  },
  {
    "text": "the inference perf uh project and right now it supports the python library it",
    "start": "1659760",
    "end": "1666080"
  },
  {
    "text": "supports the VRM uh model server and TJS model server and then um I will ignore",
    "start": "1666080",
    "end": "1672159"
  },
  {
    "text": "all of those as uh you may already heard it from uh uh from the working group of",
    "start": "1672159",
    "end": "1677360"
  },
  {
    "text": "serving talk yesterday and in the future because uh the serving is not just a",
    "start": "1677360",
    "end": "1683679"
  },
  {
    "text": "server if you think about it I it's a inference cluster and then we in the",
    "start": "1683679",
    "end": "1689120"
  },
  {
    "text": "open source have a lot of uh orchestrators inference cluster orchestrators like production stack AI",
    "start": "1689120",
    "end": "1695360"
  },
  {
    "text": "brick or even Dynamo right so in the future we want to expand this uh benchmarking",
    "start": "1695360",
    "end": "1701679"
  },
  {
    "text": "libraries to not only uh benchmarking the server performance but also",
    "start": "1701679",
    "end": "1706720"
  },
  {
    "text": "different use cases like long context QA summarization agent and then our focus",
    "start": "1706720",
    "end": "1712480"
  },
  {
    "text": "will be expanded to uh not only the server performance optimizations but",
    "start": "1712480",
    "end": "1717840"
  },
  {
    "text": "what kind of routers performance we should have and then whether we can uh",
    "start": "1717840",
    "end": "1723279"
  },
  {
    "text": "better reuse the KV cache etc okay I will hand it back to Yan okay",
    "start": "1723279",
    "end": "1730720"
  },
  {
    "text": "thanks Ch and for giving so yeah we are running out of time i just quick mention",
    "start": "1730840",
    "end": "1736640"
  },
  {
    "text": "if you want do some GPU and intensive workload benchmark the GPU burn is a",
    "start": "1736640",
    "end": "1742240"
  },
  {
    "text": "very useful tool you can try it out it's open source also have the containerized version unfortunately I don't think I",
    "start": "1742240",
    "end": "1748240"
  },
  {
    "text": "have time to demo it but very straightforward you can specify how much memory and uh it will consume also how",
    "start": "1748240",
    "end": "1755600"
  },
  {
    "text": "long you want to run right just try it and you also can use the monitoring tool I mentioned earlier like the Nvidia SMI",
    "start": "1755600",
    "end": "1762799"
  },
  {
    "text": "or GPU stats to collect all this information we also use this benchmark to study the different GPU sharing",
    "start": "1762799",
    "end": "1769600"
  },
  {
    "text": "strategy in our previous cubecon please check it out so I will skip the demo so",
    "start": "1769600",
    "end": "1775520"
  },
  {
    "text": "also there are other benchmarks and train already compare and uh uh I'd like to highlight It's a few of them very",
    "start": "1775520",
    "end": "1782399"
  },
  {
    "text": "compre comprehensive set of the ML benchmark for all the machine learning HPC and generative AI is called ML proof",
    "start": "1782399",
    "end": "1791039"
  },
  {
    "text": "benchmark also and recently Nvidia announced the DJX cloud benchmarking service please check it out as well so",
    "start": "1791039",
    "end": "1798240"
  },
  {
    "text": "okay to summarize our talk uh benchmarking is essential to for understanding optimized AI workload so",
    "start": "1798240",
    "end": "1805679"
  },
  {
    "text": "the key takeaway is there already a variety and a set of tools available and",
    "start": "1805679",
    "end": "1811360"
  },
  {
    "text": "also benchmark and software so we should try it but moving forward I I I think we",
    "start": "1811360",
    "end": "1817039"
  },
  {
    "text": "we need and probably and provide the software tool can do all this more realistic end to end performance right",
    "start": "1817039",
    "end": "1823919"
  },
  {
    "text": "it's not just about training or inferencing and it's the end solutions even data processing post-processing",
    "start": "1823919",
    "end": "1830799"
  },
  {
    "text": "other thing another thing is not only right the synthetic workload for what if analysis also can replay and some real",
    "start": "1830799",
    "end": "1838720"
  },
  {
    "text": "traces that's important also simulation emulination are used for technical",
    "start": "1838720",
    "end": "1843760"
  },
  {
    "text": "for deeper and insight and finally I also want to mention right DI is the new",
    "start": "1843760",
    "end": "1848799"
  },
  {
    "text": "API to request allocated GPU and you should take a look at it and uh finally",
    "start": "1848799",
    "end": "1854399"
  },
  {
    "text": "I want to highlight the AI workload and uh not only about performance right reliability and feed recovery very",
    "start": "1854399",
    "end": "1861679"
  },
  {
    "text": "important how to support that and evaluate this and feature and we be a critical features hopefully community",
    "start": "1861679",
    "end": "1868640"
  },
  {
    "text": "can work together to improve it finally all this tool you have seen so far right",
    "start": "1868640",
    "end": "1874240"
  },
  {
    "text": "are publicly available and definitely we should make it open source and the community can use it and work together",
    "start": "1874240",
    "end": "1880480"
  },
  {
    "text": "to enhance and developing the new features so yeah we list a bunch of the",
    "start": "1880480",
    "end": "1885520"
  },
  {
    "text": "reference gear and uh you can take a look so I will be and at NVIDIA booth this afternoon yeah also we're available",
    "start": "1885520",
    "end": "1892159"
  },
  {
    "text": "and after the talk so please come talk to us and uh yeah any question comments or feedback thank you very much",
    "start": "1892159",
    "end": "1901279"
  }
]