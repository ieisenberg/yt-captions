[
  {
    "text": "okay good morning uh my name is gaurav Singh I am product manager in openshift",
    "start": "299",
    "end": "5940"
  },
  {
    "text": "and I have done with me if you're interested oh yes my name is Darren Don I'm with IBM research I'm the",
    "start": "5940",
    "end": "12660"
  },
  {
    "text": "computational patterning team lead for uh Albany nanotech as well as the Thomas",
    "start": "12660",
    "end": "18480"
  },
  {
    "text": "J Watson Research Center so uh",
    "start": "18480",
    "end": "23640"
  },
  {
    "text": "give you a view of what I'm hearing from my customers regarding batch and and",
    "start": "23640",
    "end": "29699"
  },
  {
    "text": "this type of activities so I've talked to a lot of customers these are predominant customer who comes to me",
    "start": "29699",
    "end": "36000"
  },
  {
    "text": "around bash type of Niche right so we talked to life science customer they",
    "start": "36000",
    "end": "41340"
  },
  {
    "text": "they say we want to run genome simulations we talk to manufacturing",
    "start": "41340",
    "end": "46440"
  },
  {
    "text": "customers they want to do computational food Dynamics uh we have very big presence in FSI or",
    "start": "46440",
    "end": "53579"
  },
  {
    "text": "financial sector they want to run their characteristics analysis and then we have this roads is Red Hat data science",
    "start": "53579",
    "end": "60899"
  },
  {
    "text": "and they have this use case of running uh training jobs uh um on top of a batch",
    "start": "60899",
    "end": "67680"
  },
  {
    "text": "system so when I hear uh all these customer everything boils on into three",
    "start": "67680",
    "end": "73380"
  },
  {
    "text": "characteristics they come and repeat you know the characters of the batch workload they're looking for one is uh",
    "start": "73380",
    "end": "80220"
  },
  {
    "text": "asynchronous run what they say is we have this jobs the jobs needs to run uh",
    "start": "80220",
    "end": "85979"
  },
  {
    "text": "asynchronously and complete to give me the result these jobs are computational heavy uh",
    "start": "85979",
    "end": "93900"
  },
  {
    "text": "you know require gpus fast nectares fast Network fast storage and these are",
    "start": "93900",
    "end": "99960"
  },
  {
    "text": "massive scale and since they're running on cloud they need some type of elasticity",
    "start": "99960",
    "end": "107180"
  },
  {
    "text": "um so listening to those customers uh we are trying to address few few problems",
    "start": "107880",
    "end": "113520"
  },
  {
    "text": "either from red hat or openshift side or from Community side one is job Q uh is I",
    "start": "113520",
    "end": "120540"
  },
  {
    "text": "think the font is very small but I'll talk through it so what what I've seen is uh what customer is asking is hey",
    "start": "120540",
    "end": "126840"
  },
  {
    "text": "these are my jobs uh put it in a cube so before it goes as",
    "start": "126840",
    "end": "133200"
  },
  {
    "text": "scheduled and that queue needs to be dynamic in the sense that uh the first job does not mean that it",
    "start": "133200",
    "end": "140520"
  },
  {
    "text": "needs to execute first I should able to prioritize things in the cube uh so that",
    "start": "140520",
    "end": "146819"
  },
  {
    "text": "you know I'm I want to make sure my higher priority job is running all the time uh I think Community is working on",
    "start": "146819",
    "end": "153239"
  },
  {
    "text": "something called Q we have uh we internally openshift is working on",
    "start": "153239",
    "end": "158940"
  },
  {
    "text": "something called mcad which is basically a app wrapper and dispatcher for queuing",
    "start": "158940",
    "end": "166200"
  },
  {
    "text": "then a dynamic infrastructure right so we talked about Q when a job comes into",
    "start": "166200",
    "end": "172500"
  },
  {
    "text": "the queue what customer wants is a cluster is Spinner for them right these",
    "start": "172500",
    "end": "178440"
  },
  {
    "text": "these are very expensive infrastructure they do not want it to spin uh beforehand like think about GPU nodes",
    "start": "178440",
    "end": "184739"
  },
  {
    "text": "and all these are expensive so they want whenever a job comes in the queue they want an infrastructure to be spent and",
    "start": "184739",
    "end": "191640"
  },
  {
    "text": "when there's no job just deprovision the whole infrastructure so that you know you do there's no cost incurred second",
    "start": "191640",
    "end": "198000"
  },
  {
    "text": "thing is um they need to have specialized",
    "start": "198000",
    "end": "204260"
  },
  {
    "text": "specialized Hardware into the infrastructure for example gpus or you have a fast uh infrastructures or fast",
    "start": "204260",
    "end": "212519"
  },
  {
    "text": "Network that you want to use in and there's a way to enable that within the",
    "start": "212519",
    "end": "217800"
  },
  {
    "text": "platform so things like operators we use operator in openshift how to enable",
    "start": "217800",
    "end": "223200"
  },
  {
    "text": "those operators within the infrastructure looking at the job characteristics saying that hey this job",
    "start": "223200",
    "end": "229920"
  },
  {
    "text": "needs gpus so the infrastructure needs a GPU go install the gpus and get the",
    "start": "229920",
    "end": "235140"
  },
  {
    "text": "ready infrastructure before the job can be executed ah third thing is the gang scheduling so",
    "start": "235140",
    "end": "243299"
  },
  {
    "text": "what what customer says is let's say I have 10 jobs and infrastructure has capacity",
    "start": "243299",
    "end": "250080"
  },
  {
    "text": "for five to run five jobs make sure the jobs weighs a weight until all the",
    "start": "250080",
    "end": "258060"
  },
  {
    "text": "resources are available in within the infrastructure so all kind of All or Nothing uh run all jobs or nothing in",
    "start": "258060",
    "end": "267240"
  },
  {
    "text": "the gang fashion um these other trends that I'm seeing by",
    "start": "267240",
    "end": "273240"
  },
  {
    "text": "talking to all these customers so I mean all these customer use cloud in one way",
    "start": "273240",
    "end": "279180"
  },
  {
    "text": "of form for their application now they see benefit of it they want to use the",
    "start": "279180",
    "end": "284340"
  },
  {
    "text": "same benefit uh for their AI ML and HPC application uh multi-hardware",
    "start": "284340",
    "end": "292020"
  },
  {
    "text": "architecture so what I've seen is what customers saying is hey to save the cost I would be able to run my uh",
    "start": "292020",
    "end": "300960"
  },
  {
    "text": "run my management on the arm architecture and compute on",
    "start": "300960",
    "end": "308220"
  },
  {
    "text": "x86 right so you need to have multiple multi architecture in there accelerators",
    "start": "308220",
    "end": "314040"
  },
  {
    "text": "we talk about uh they want to use GPU smart connects fast Network everything that clouds present today uh they want",
    "start": "314040",
    "end": "322139"
  },
  {
    "text": "to leverage that into their uh into their workload uh containers these",
    "start": "322139",
    "end": "329340"
  },
  {
    "text": "uh now I'm seeing a shift where customers uh you know old HPC type of",
    "start": "329340",
    "end": "335520"
  },
  {
    "text": "customer running on uh HPC workload on-prem on a bare metal they are",
    "start": "335520",
    "end": "341520"
  },
  {
    "text": "transitioning uh mostly re-hosting and looking forward to re-platforming their",
    "start": "341520",
    "end": "348720"
  },
  {
    "text": "HPC type of workload into the containers ah we we talk about HPC a lot but I've",
    "start": "348720",
    "end": "356340"
  },
  {
    "text": "seen a very good use case coming on from AIML site where AIML customers asking",
    "start": "356340",
    "end": "363180"
  },
  {
    "text": "that hey I want this uh batch thing for to to run my AML jobs so rather than HPC",
    "start": "363180",
    "end": "371340"
  },
  {
    "text": "side I'm seeing a lot of uh requests coming from the AIML uh side",
    "start": "371340",
    "end": "377039"
  },
  {
    "text": "of the business uh uh not on the AML and HPC but I've",
    "start": "377039",
    "end": "383639"
  },
  {
    "text": "seen some another uh demand coming out from gaming networking and Telco side",
    "start": "383639",
    "end": "390180"
  },
  {
    "text": "that they want to run on these uh uh on on on on on on a batch system",
    "start": "390180",
    "end": "397940"
  },
  {
    "text": "uh Nexus Darren he's going to talk about how we're running uh uh um",
    "start": "397940",
    "end": "404960"
  },
  {
    "text": "Eda Eda worker on openshift or kubernetes so I'll welcome Darren yeah",
    "start": "404960",
    "end": "412039"
  },
  {
    "text": "okay so for my part of the talk I'm going to give you a little bit of background about the Two Worlds that I",
    "start": "413639",
    "end": "420720"
  },
  {
    "text": "Bridge um I won't expect that anybody here knows the workload that I'm going to",
    "start": "420720",
    "end": "425759"
  },
  {
    "text": "talk about so I'm going to go into a little bit of detail so everybody understands I will also uh go into a",
    "start": "425759",
    "end": "432240"
  },
  {
    "text": "little bit of detail about why we want to do Cloud burst it's a very similar story to what everybody else wants to do",
    "start": "432240",
    "end": "438240"
  },
  {
    "text": "and some of the benefits for us particularly in semiconductor research and design for doing so and then I'll go",
    "start": "438240",
    "end": "444000"
  },
  {
    "text": "over some scaling results from our Optical proximity correction workload using uh",
    "start": "444000",
    "end": "450539"
  },
  {
    "text": "service in the iCloud so semiconductor research and",
    "start": "450539",
    "end": "455819"
  },
  {
    "text": "development my role is to span Two Worlds I span process development and",
    "start": "455819",
    "end": "462180"
  },
  {
    "text": "research I also span hybrid cloud and my sole role is to bring these two worlds",
    "start": "462180",
    "end": "467520"
  },
  {
    "text": "together so that we can be more efficient and leverage more opportunities so when you think about doing",
    "start": "467520",
    "end": "472740"
  },
  {
    "text": "semiconductor research and design you could boil it down to three main things you need to be able to design",
    "start": "472740",
    "end": "479520"
  },
  {
    "text": "new circuits and chips you need to be able to transfer what you've designed",
    "start": "479520",
    "end": "485039"
  },
  {
    "text": "to photo lithography masks there's no way you can do anything without being",
    "start": "485039",
    "end": "490860"
  },
  {
    "text": "able to transfer a design to wafer with lithography and other processing processes once you've done that",
    "start": "490860",
    "end": "498000"
  },
  {
    "text": "you're not done you need to develop processes that are capable of actually processing the lithography pattern that",
    "start": "498000",
    "end": "505020"
  },
  {
    "text": "you have transferring it to a workable circuit on on the wafer today what I'm",
    "start": "505020",
    "end": "510660"
  },
  {
    "text": "going to do is I'm going to focus mostly on the transfer process and I'm going to",
    "start": "510660",
    "end": "516300"
  },
  {
    "text": "talk about a project or process which involves what's called retargeting",
    "start": "516300",
    "end": "522120"
  },
  {
    "text": "which is taking a design shape and actually changing it slightly so that we can actually print it and then using a",
    "start": "522120",
    "end": "530519"
  },
  {
    "text": "massively parallel embarrassingly parallel hvc workload",
    "start": "530519",
    "end": "535980"
  },
  {
    "text": "called Optical proximity correction to actually transfer the shapes uh to uh",
    "start": "535980",
    "end": "542060"
  },
  {
    "text": "photolithography mask assemble it and then write it to a photolithography mask",
    "start": "542060",
    "end": "547680"
  },
  {
    "text": "and let the process development teams take over so the tool set that I'm going to talk",
    "start": "547680",
    "end": "554339"
  },
  {
    "text": "about today um we we use multiple tools today specifically I'm going to talk about a",
    "start": "554339",
    "end": "560279"
  },
  {
    "text": "tool called caliber which is made by Siemens these are commercial tools typically an engineering design",
    "start": "560279",
    "end": "566399"
  },
  {
    "text": "automation workflows most of the primary tool sets are commercial so one of the",
    "start": "566399",
    "end": "571620"
  },
  {
    "text": "challenges that you have it's taking a commercial tool that has been written",
    "start": "571620",
    "end": "577260"
  },
  {
    "text": "for a more typical HPC Linux cluster environment containerizing it and then",
    "start": "577260",
    "end": "582839"
  },
  {
    "text": "finding efficient ways to run it with orchestrated containers on openshift or",
    "start": "582839",
    "end": "588019"
  },
  {
    "text": "bear more vanilla kubernetes services so when we talk about this what we're",
    "start": "588019",
    "end": "594240"
  },
  {
    "text": "really going to be talking about is taking this application this application is meant to manage the transfer through",
    "start": "594240",
    "end": "600779"
  },
  {
    "text": "the photolithography mask from euv light or 1938 to an actual wafer when you break",
    "start": "600779",
    "end": "609600"
  },
  {
    "text": "this process down actually um it it it's a consists of many",
    "start": "609600",
    "end": "615120"
  },
  {
    "text": "different tool flows when we finish with design what ends up happening is we get",
    "start": "615120",
    "end": "620399"
  },
  {
    "text": "a database file format either gds2 or Oasis that file format contains a",
    "start": "620399",
    "end": "627660"
  },
  {
    "text": "representation of of design shapes that may contain upwards of 10 billion design shapes in a particular layer that we're",
    "start": "627660",
    "end": "634740"
  },
  {
    "text": "going to transfer as part of the process of figuring out ahead of time how we can",
    "start": "634740",
    "end": "640980"
  },
  {
    "text": "take a design and print it on wafer we have to go through several steps one of them is called design for manufacturing",
    "start": "640980",
    "end": "647160"
  },
  {
    "text": "that's actually taking the ground rules that were established for the technology figuring out what actually has to happen",
    "start": "647160",
    "end": "654300"
  },
  {
    "text": "what's printable what's transferable and if we need to change design rules go ahead and do that we then need to fill",
    "start": "654300",
    "end": "661200"
  },
  {
    "text": "use use other shapes to fill the gaps in a design without impacting the",
    "start": "661200",
    "end": "666660"
  },
  {
    "text": "performance of the design and that's a tool and a technology all to itself in",
    "start": "666660",
    "end": "672540"
  },
  {
    "text": "many cases what we're doing is we're decomposing a design into multiple colors and printing a color at the same",
    "start": "672540",
    "end": "679320"
  },
  {
    "text": "time and the reason for that is is that as we get beyond the three nanometer known five nanometer node you know even",
    "start": "679320",
    "end": "686279"
  },
  {
    "text": "starting a 10 nanometer we can't print the entire design with a single color or",
    "start": "686279",
    "end": "691680"
  },
  {
    "text": "a single exposure we need to find ways to break it apart so that we can actually Pat what we need to do and then",
    "start": "691680",
    "end": "698640"
  },
  {
    "text": "we talked about retargeting where we take those shapes we change them slightly so that we can actually print",
    "start": "698640",
    "end": "704100"
  },
  {
    "text": "them we will put insert features through code called sub-resolution assist",
    "start": "704100",
    "end": "709860"
  },
  {
    "text": "features those are required to help the lithography actually print particularly intermediate to isolated shapes and then",
    "start": "709860",
    "end": "717300"
  },
  {
    "text": "we finally do Optical proximity correction and we also verify that the",
    "start": "717300",
    "end": "722760"
  },
  {
    "text": "shape that we've created will actually print on wafer using modeling to within",
    "start": "722760",
    "end": "729060"
  },
  {
    "text": "a certain tolerance or expected a variance so in a nutshell what is",
    "start": "729060",
    "end": "735300"
  },
  {
    "text": "Optical proximity correction you can think about this as an optimization problem prior to about the 90 nanometer",
    "start": "735300",
    "end": "742800"
  },
  {
    "text": "technology node you could actually take a drawn shape multiply it by a scaling factor and",
    "start": "742800",
    "end": "748560"
  },
  {
    "text": "transfer it to a lithography mask and you'd have a pretty good uh probability",
    "start": "748560",
    "end": "754560"
  },
  {
    "text": "of hitting the target for the design and and people would come up with some rules-based changes but typically it was",
    "start": "754560",
    "end": "761640"
  },
  {
    "text": "just a straight shot starting in 90 nanometer that wasn't true so if you look at this diagram",
    "start": "761640",
    "end": "767160"
  },
  {
    "text": "without OPC that's the drawn shape you would bring it you you could think about",
    "start": "767160",
    "end": "772380"
  },
  {
    "text": "the lithography process as a series of transforms one transform is to transform it to the mass the other one is to",
    "start": "772380",
    "end": "778860"
  },
  {
    "text": "expose it and the other one is to etch it on wafer what typically happened at",
    "start": "778860",
    "end": "784200"
  },
  {
    "text": "90 nanometer and Beyond is the shape that we brought in as the design shape no longer was represented on on the",
    "start": "784200",
    "end": "791100"
  },
  {
    "text": "wafer in fact in some cases some shapes wouldn't print it off so there was a technique called Optical proximity",
    "start": "791100",
    "end": "797279"
  },
  {
    "text": "correction that was developed in which you bring in the design shape you",
    "start": "797279",
    "end": "802440"
  },
  {
    "text": "partition it into vertices and edges you then expose that partition partition",
    "start": "802440",
    "end": "808860"
  },
  {
    "text": "shape um or calculate what the aerial image would be which is a very physical process and you apply a",
    "start": "808860",
    "end": "817500"
  },
  {
    "text": "an empirical resist model to it to predict what the shape will print on wafer and then you iterate until you",
    "start": "817500",
    "end": "824519"
  },
  {
    "text": "either hit a fixed number of iterations or a cost function has been matched and",
    "start": "824519",
    "end": "829860"
  },
  {
    "text": "you then output that onto the uh the photolithography mask so you end up with",
    "start": "829860",
    "end": "835860"
  },
  {
    "text": "a lot of very complex serif shapes when you do this so how how is this represented from a",
    "start": "835860",
    "end": "842519"
  },
  {
    "text": "compute process so this is actually a two-part compute you have a primary path process that reads in the design layout",
    "start": "842519",
    "end": "849120"
  },
  {
    "text": "chops it up into tiles and creates a cue or a heap once it's done with that it",
    "start": "849120",
    "end": "855360"
  },
  {
    "text": "spins up a bunch of remote workers uh on a on a traditional Linux cluster in our",
    "start": "855360",
    "end": "860459"
  },
  {
    "text": "case on an open shifter kubernetes cluster and then it starts parsing out work tile by tile to all of the workers",
    "start": "860459",
    "end": "867779"
  },
  {
    "text": "and it does this in a round robin fashion until the work until all of the tiles in the queue or the Heap have been",
    "start": "867779",
    "end": "874440"
  },
  {
    "text": "exhausted there's no need for the remotes or the worker processes to",
    "start": "874440",
    "end": "879839"
  },
  {
    "text": "communicate with one another so it's a very traditional Hub and spoke process",
    "start": "879839",
    "end": "886320"
  },
  {
    "text": "so when we when we talk about this in a traditional sense and and bursting to the cloud what",
    "start": "887519",
    "end": "893519"
  },
  {
    "text": "problem are we trying to solve what we're really trying to do is expand our opportunity Horizons to work on more and",
    "start": "893519",
    "end": "899160"
  },
  {
    "text": "more projects you can think about this as a triangle we have three main resources that we're trying to optimize",
    "start": "899160",
    "end": "905279"
  },
  {
    "text": "one of them is compute one of them are commercial licenses for the tools and the other one is people",
    "start": "905279",
    "end": "912300"
  },
  {
    "text": "okay the most sticky component that you have are people it's not easy to expand",
    "start": "912300",
    "end": "918420"
  },
  {
    "text": "a design team or an OPC team or a patterning team in short periods of time",
    "start": "918420",
    "end": "923760"
  },
  {
    "text": "the two things that you can expand are compute and licenses so if you think",
    "start": "923760",
    "end": "929579"
  },
  {
    "text": "about the semiconductor interest industry as a whole there are more opportunities than capacity to take care",
    "start": "929579",
    "end": "935459"
  },
  {
    "text": "of but and and time to Market is huge So the faster that you can realize an",
    "start": "935459",
    "end": "941160"
  },
  {
    "text": "opportunity the more opportunities you can work on so people spend a lot of time trying to figure out how do I",
    "start": "941160",
    "end": "946860"
  },
  {
    "text": "maximize my opportunities there's two ways that that you could do this",
    "start": "946860",
    "end": "952440"
  },
  {
    "text": "um and one is is to increase your compute the other one is to increase the number of licenses and these two have to",
    "start": "952440",
    "end": "958440"
  },
  {
    "text": "go concurrently okay so if we have a smaller uh opportunity Horizon that's referenced",
    "start": "958440",
    "end": "966120"
  },
  {
    "text": "in the in the yellow but we've got a lot of projects that we really want to explore what are the best ways for us to",
    "start": "966120",
    "end": "971820"
  },
  {
    "text": "do this is to burst to the cloud but in order to do this as was mentioned earlier",
    "start": "971820",
    "end": "977579"
  },
  {
    "text": "designers and OPC Engineers are not Cloud engineers and Cloud Engineers definitely are not designers or or OPC",
    "start": "977579",
    "end": "985800"
  },
  {
    "text": "Engineers there's a communication gap and and there are there's an expectation Gap so if you could come up with a way",
    "start": "985800",
    "end": "992459"
  },
  {
    "text": "that that people are submitting jobs to a compute resource and you can expand",
    "start": "992459",
    "end": "998399"
  },
  {
    "text": "the the compute license and people triangle to maximize your opportunities",
    "start": "998399",
    "end": "1003500"
  },
  {
    "text": "and and deliver more in the same period of time so our Cloud burst strategy from the",
    "start": "1003500",
    "end": "1009199"
  },
  {
    "text": "beginning is to use OPC or Optical proximity correction as a proxy workload we want to create the same on-prem",
    "start": "1009199",
    "end": "1016160"
  },
  {
    "text": "infrastructure as we as we would use in the cloud and we want to Target managed kubernetes Services what that does is",
    "start": "1016160",
    "end": "1022940"
  },
  {
    "text": "that lets us minimize the amount of configuration and tweaking that we do when we burst to the cloud and helps us",
    "start": "1022940",
    "end": "1029418"
  },
  {
    "text": "realize this vision of sane compute on-premise we have in the cloud we also",
    "start": "1029419",
    "end": "1035058"
  },
  {
    "text": "want to centralize license servers in the cloud to avoid splitting pools or having gaps in time from when we can",
    "start": "1035059",
    "end": "1041000"
  },
  {
    "text": "leverage the cloud or not and also develop our controllers and operators to make public Cloud bursts with openshift",
    "start": "1041000",
    "end": "1047839"
  },
  {
    "text": "transparent to the engineering teams so this is a pictorial diagram of the type",
    "start": "1047839",
    "end": "1053540"
  },
  {
    "text": "of infrastructure that we're talking about on the upper left-hand side we have our on-prem compute we have",
    "start": "1053540",
    "end": "1059059"
  },
  {
    "text": "interactive compute to set up jobs to view results to also change uh do job",
    "start": "1059059",
    "end": "1066020"
  },
  {
    "text": "management and we also have a number of openshift clusters that we maintain internally with shared uh distributed",
    "start": "1066020",
    "end": "1073880"
  },
  {
    "text": "file space when we burst into the cloud we target our a service called rocks",
    "start": "1073880",
    "end": "1080360"
  },
  {
    "text": "which is Red Hat openshift kubernetes service in the IBM Cloud you know if you're used to",
    "start": "1080360",
    "end": "1086799"
  },
  {
    "text": "AWS you might think of this as Rosa and in the cloud what we do is we build some",
    "start": "1086799",
    "end": "1092780"
  },
  {
    "text": "some compute nodes and job management nodes and we want to run across as many data centers in a cloud burst geography",
    "start": "1092780",
    "end": "1099260"
  },
  {
    "text": "as we can so we can maximize our capacity leverage Auto scalers where we can so we can scale up and scale down to",
    "start": "1099260",
    "end": "1105679"
  },
  {
    "text": "save costs and centralize the license managers in in a cloud geography that's",
    "start": "1105679",
    "end": "1111559"
  },
  {
    "text": "accessible to all one of the key things that you you could do in almost any cloud is connect different regions via",
    "start": "1111559",
    "end": "1119059"
  },
  {
    "text": "networking that exists solely in the cloud itself so what we've done is within IBM Cloud we use Transit gateways",
    "start": "1119059",
    "end": "1125600"
  },
  {
    "text": "to connect different geographies and maximize our ability to to burst",
    "start": "1125600",
    "end": "1130880"
  },
  {
    "text": "anywhere in the IBM cloud from anywhere within IBM",
    "start": "1130880",
    "end": "1137500"
  },
  {
    "text": "so how does how does our workflow work based on the description I gave you earlier",
    "start": "1137720",
    "end": "1143419"
  },
  {
    "text": "um how do we set this up in kubernetes and I'm sure people here could come up",
    "start": "1143419",
    "end": "1148760"
  },
  {
    "text": "with many different ways to do this we chose because of the nature of these",
    "start": "1148760",
    "end": "1154039"
  },
  {
    "text": "tools to try to mimic um as much as we possibly could the way",
    "start": "1154039",
    "end": "1160100"
  },
  {
    "text": "that these jobs run on a native Linux bare metal Linux cluster and the reason why we wanted to do this was twofold one",
    "start": "1160100",
    "end": "1167000"
  },
  {
    "text": "it solves a problem of OPC Engineers or design Engineers understanding how their",
    "start": "1167000",
    "end": "1172640"
  },
  {
    "text": "jobs are running and two with very you know small tweaks to a uh to a controller or an operator",
    "start": "1172640",
    "end": "1180500"
  },
  {
    "text": "in kubernetes we can build in resilience to this workload by making sure that we",
    "start": "1180500",
    "end": "1185840"
  },
  {
    "text": "leverage inherent capabilities and kubernetes to keep worker nodes for instance up if they go down make sure",
    "start": "1185840",
    "end": "1193039"
  },
  {
    "text": "that we can checkpoint and and start and and restart jobs scale them down so that",
    "start": "1193039",
    "end": "1198860"
  },
  {
    "text": "we can prioritize jobs so what happens is we use kubernetes job types we have",
    "start": "1198860",
    "end": "1204140"
  },
  {
    "text": "two we have a primary job type which um it starts initially what it does is",
    "start": "1204140",
    "end": "1210260"
  },
  {
    "text": "exactly what I talked about before it reads in the layout chops it up into tiles and creates a queue once it's done",
    "start": "1210260",
    "end": "1216380"
  },
  {
    "text": "it then talks to a distributed controller",
    "start": "1216380",
    "end": "1222340"
  },
  {
    "text": "that then spins up a number of workers and when I talk about workers in this case we're talking about running at",
    "start": "1222340",
    "end": "1229160"
  },
  {
    "text": "fairly large scale table Stakes for an OPC run is typically between 8 000 pods",
    "start": "1229160",
    "end": "1235460"
  },
  {
    "text": "and sixteen thousand pods when you start talking to foundries that's like a twenty thousand pod run to to make time",
    "start": "1235460",
    "end": "1243200"
  },
  {
    "text": "um or to make most efficient use of time so what we wanted to do was was go out and say for us to use this internally as",
    "start": "1243200",
    "end": "1250220"
  },
  {
    "text": "research we need to be able to get to 10 000 pods reliably let's go out figure out how to assemble",
    "start": "1250220",
    "end": "1256640"
  },
  {
    "text": "job flows like this and demonstrate scaling from a thousand to ten thousand pods if we could do that in a fairly",
    "start": "1256640",
    "end": "1263480"
  },
  {
    "text": "linear fashion then we have confidence then we can go and do this at more scale one of the other reasons for breaking",
    "start": "1263480",
    "end": "1269840"
  },
  {
    "text": "this apart are breaking our job flows apart is it also lets us address the",
    "start": "1269840",
    "end": "1275780"
  },
  {
    "text": "differences between running let's say four twenty five hundred pod runs and one ten thousand pod run on the same",
    "start": "1275780",
    "end": "1282980"
  },
  {
    "text": "cluster and also lets us address some of the shared file system issues that we",
    "start": "1282980",
    "end": "1288919"
  },
  {
    "text": "have so one of the things to keep in mind is for for many of these tools and many HPC workloads",
    "start": "1288919",
    "end": "1295640"
  },
  {
    "text": "um a shared high performance file system is really key all of the worker pods need",
    "start": "1295640",
    "end": "1302840"
  },
  {
    "text": "to see the same file system read and write to it and Stat it that the primary pod sees and they're going to do this on",
    "start": "1302840",
    "end": "1310580"
  },
  {
    "text": "with irregular patterns so one of the ways that we've made this made our work",
    "start": "1310580",
    "end": "1316460"
  },
  {
    "text": "happen here is to explore both open source high performance file systems",
    "start": "1316460",
    "end": "1321679"
  },
  {
    "text": "like SEF um or or internal file systems like gpfs",
    "start": "1321679",
    "end": "1327620"
  },
  {
    "text": "or Spectrum scale and we what we tend to do is offer",
    "start": "1327620",
    "end": "1333100"
  },
  {
    "text": "Seth gluster or gpfs as persistent",
    "start": "1333100",
    "end": "1338299"
  },
  {
    "text": "volume claims that are mounted by all of the pods in in the run and we keep those",
    "start": "1338299",
    "end": "1344299"
  },
  {
    "text": "static and there's also another advantage to that is when we have to go back and debug logs everybody can see",
    "start": "1344299",
    "end": "1350240"
  },
  {
    "text": "the same thing they can pick up the logs in the same place in whatever Cloud environment we're talking about and with",
    "start": "1350240",
    "end": "1355760"
  },
  {
    "text": "asynchronous file management we can bring it anywhere so these jobs are",
    "start": "1355760",
    "end": "1360860"
  },
  {
    "text": "heavily compute dependent and they're heavily i o dependent both at the network level and at the file system",
    "start": "1360860",
    "end": "1367460"
  },
  {
    "text": "level so you know one of the things that that",
    "start": "1367460",
    "end": "1374000"
  },
  {
    "text": "I get asked I I get some skeptical questions up front with new OPC",
    "start": "1374000",
    "end": "1380179"
  },
  {
    "text": "engineers and new designers is yeah this is all great it looks convenient but is",
    "start": "1380179",
    "end": "1385940"
  },
  {
    "text": "it going to perform how do I know that if I just you know give you a kubernetes",
    "start": "1385940",
    "end": "1391220"
  },
  {
    "text": "job then I'm going to get it in the time that I need to do it so over the last two years or so we've been we've been",
    "start": "1391220",
    "end": "1398000"
  },
  {
    "text": "looking at scaling and OPC is is embarrassingly parallel it should scale",
    "start": "1398000",
    "end": "1403760"
  },
  {
    "text": "fairly linearly but there are parts of the job that need to do i o need to do",
    "start": "1403760",
    "end": "1409039"
  },
  {
    "text": "some checking in in the hierarchy management the primary pod so we don't expect it to to be pure linear so what",
    "start": "1409039",
    "end": "1416600"
  },
  {
    "text": "I've done here in this in these results is plot a couple of things I plotted um",
    "start": "1416600",
    "end": "1422360"
  },
  {
    "text": "the speed up that you would expect if everything was linear which is really just this line the black line that you",
    "start": "1422360",
    "end": "1429020"
  },
  {
    "text": "see here the next thing that we've added is a speed up assuming you have a Serial",
    "start": "1429020",
    "end": "1435020"
  },
  {
    "text": "fraction of about 15 that's something more attainable than pure linear given",
    "start": "1435020",
    "end": "1440480"
  },
  {
    "text": "the amount of file and networking i o that we need to do and then what I did",
    "start": "1440480",
    "end": "1445640"
  },
  {
    "text": "was um plot our speed up in the blue curve for OPC runs",
    "start": "1445640",
    "end": "1453020"
  },
  {
    "text": "that span a thousand pods to ten thousand pods for our Nano sheet node which you can think of as three",
    "start": "1453020",
    "end": "1459080"
  },
  {
    "text": "nanometers to two nanometers depending on who you talk about or talk to for a a",
    "start": "1459080",
    "end": "1464539"
  },
  {
    "text": "back end wiring level back in wiring levels and I apologize for using jargon",
    "start": "1464539",
    "end": "1470780"
  },
  {
    "text": "are are the is the first wiring level that's most important for signals and",
    "start": "1470780",
    "end": "1476299"
  },
  {
    "text": "and you know that they're called The Thin wire levels and there's a couple of them but the first one is the most",
    "start": "1476299",
    "end": "1481640"
  },
  {
    "text": "important so what we're showing here is that with in the blue curve scaling from",
    "start": "1481640",
    "end": "1487280"
  },
  {
    "text": "one thousand to ten thousand pods we're achieving a pretty close correlation with about a 15 i o",
    "start": "1487280",
    "end": "1494240"
  },
  {
    "text": "um uh or serial fraction which is actually quite good",
    "start": "1494240",
    "end": "1499520"
  },
  {
    "text": "um where by no means done with this we believe that these points that you see here around six to seven thousand pods",
    "start": "1499520",
    "end": "1505760"
  },
  {
    "text": "we understand how to fix and we think that we can definitely with some very",
    "start": "1505760",
    "end": "1511039"
  },
  {
    "text": "simple tweaks start to increase the slope of this curve so that we're somewhere between the 15 serial fraction",
    "start": "1511039",
    "end": "1517419"
  },
  {
    "text": "and the the actual hypothetical linear scaling result",
    "start": "1517419",
    "end": "1523460"
  },
  {
    "text": "um so to summarize we we've successfully demonstrated running Optical proximity",
    "start": "1523460",
    "end": "1529039"
  },
  {
    "text": "correction at scale using a managed openshift service we are targeting management managed",
    "start": "1529039",
    "end": "1535820"
  },
  {
    "text": "openshift Services because it minimizes setup and tear down in public Cloud infrastructure and we're applying what",
    "start": "1535820",
    "end": "1541520"
  },
  {
    "text": "we've learned using optical proximity correction to many other tools in the design change",
    "start": "1541520",
    "end": "1547279"
  },
  {
    "text": "this design tool chain so we can apply this to the tools that we're actually using to do register transistor logic",
    "start": "1547279",
    "end": "1554740"
  },
  {
    "text": "synthesis place in route timing and enclosure and performance we've got a",
    "start": "1554740",
    "end": "1561559"
  },
  {
    "text": "lot of opportunities to build logic and custom controllers into this flow I you know we view this as as the first step",
    "start": "1561559",
    "end": "1569840"
  },
  {
    "text": "um we do use for all of our Mass delivery into Albany nanotech today",
    "start": "1569840",
    "end": "1575779"
  },
  {
    "text": "um openshift and and a managed kubernetes service um so we're in in a sense and this is",
    "start": "1575779",
    "end": "1581539"
  },
  {
    "text": "something that we always have to do within IBM our management team always insists that we eat our own cooking so",
    "start": "1581539",
    "end": "1588080"
  },
  {
    "text": "we can't go out and say hey everybody should use kubernetes or everybody should use openshift we actually",
    "start": "1588080",
    "end": "1594860"
  },
  {
    "text": "have to do that and make sure that it works for us and that we could stand behind it and say yep here's the data I",
    "start": "1594860",
    "end": "1600679"
  },
  {
    "text": "can scale from a thousand to ten thousand pods I get good results I can do this reliably",
    "start": "1600679",
    "end": "1606620"
  },
  {
    "text": "um and I think what this is also going to do is enable us to make smarter use of public cloud services the IBM cloud",
    "start": "1606620",
    "end": "1613460"
  },
  {
    "text": "in particular but also not to limit ourselves to that we're also a hybrid Cloud company we want to be able to",
    "start": "1613460",
    "end": "1619039"
  },
  {
    "text": "enable this in other people's clouds so that we can make the greatest use of cloud resources and also deliver new",
    "start": "1619039",
    "end": "1625580"
  },
  {
    "text": "technology on time so thank you very much [Applause]",
    "start": "1625580",
    "end": "1632730"
  },
  {
    "text": "thank you uh nice talk uh I was curious it sounds like you have a lot of like",
    "start": "1640659",
    "end": "1645799"
  },
  {
    "text": "workflow management going on with this uh workload do you use any of the cncf",
    "start": "1645799",
    "end": "1651679"
  },
  {
    "text": "projects for uh like Argo workflows or anything like that not yet we we would",
    "start": "1651679",
    "end": "1656779"
  },
  {
    "text": "like to um we started very simply because um you know we we had to solve some",
    "start": "1656779",
    "end": "1664460"
  },
  {
    "text": "problems or some some I won't say problems we had to optimize our containerization",
    "start": "1664460",
    "end": "1670400"
  },
  {
    "text": "uh flow and in order to keep things simple on the compute side we didn't we",
    "start": "1670400",
    "end": "1675500"
  },
  {
    "text": "didn't try to do anything more sophisticated that's one area where we'd like to go is is use",
    "start": "1675500",
    "end": "1682220"
  },
  {
    "text": "um particularly on the on the operator side and the scheduling side better uh uh um or leverage open open source",
    "start": "1682220",
    "end": "1689960"
  },
  {
    "text": "projects to actually do that better within within openshift and kubernetes just want to add one more thing so",
    "start": "1689960",
    "end": "1696380"
  },
  {
    "text": "um also things like crd changes custom uh uh configuration on the Node so for",
    "start": "1696380",
    "end": "1702679"
  },
  {
    "text": "that uh we are looking at Argo CD things like that when whenever you have job coming and before the infrastructure",
    "start": "1702679",
    "end": "1708260"
  },
  {
    "text": "spin up you have Argo CD comes in and you know prep the infrastructure so things like that you're looking at the",
    "start": "1708260",
    "end": "1713419"
  },
  {
    "text": "Argo City in that way thank you for the talk very inspiring uh",
    "start": "1713419",
    "end": "1718640"
  },
  {
    "text": "question you've been attached for two years which means you probably went to a number of openshift versions",
    "start": "1718640",
    "end": "1725179"
  },
  {
    "text": "I'm sorry I'm hard hearing you've been doing this for two years yeah you've probably been using several different",
    "start": "1725179",
    "end": "1730760"
  },
  {
    "text": "openshift versions yes what are your conclusions when upgrading did it really improve your performance or not",
    "start": "1730760",
    "end": "1738760"
  },
  {
    "text": "um can you repeat it one more time I'm sorry so you probably went from openshift 3 or",
    "start": "1740299",
    "end": "1746779"
  },
  {
    "text": "maybe opposite four or one to whatever 10 yeah 11 away or today right I assume",
    "start": "1746779",
    "end": "1752779"
  },
  {
    "text": "there would have been differences when you repeat the same tests with newer versions can you share something about that oh yes yes yes",
    "start": "1752779",
    "end": "1760279"
  },
  {
    "text": "um we we do see differences um we I we've seen an improvement um it definitely in the four series of",
    "start": "1760279",
    "end": "1766760"
  },
  {
    "text": "releases um in particular with the with our Network performance and our ability to",
    "start": "1766760",
    "end": "1772520"
  },
  {
    "text": "opt to tune file systems um I I think we need to be a little more rigorous about Version Control and how",
    "start": "1772520",
    "end": "1778940"
  },
  {
    "text": "we Benchmark different versions we're not doing that today but that's one of the things that's on our list but I have",
    "start": "1778940",
    "end": "1784580"
  },
  {
    "text": "seen some improvements definitely from three to four with networking and and our file system performance and our",
    "start": "1784580",
    "end": "1789980"
  },
  {
    "text": "ability to tune them um the compute is also better um but I you know it's hard for me to",
    "start": "1789980",
    "end": "1795440"
  },
  {
    "text": "quantify it because these workloads um don't make very good use of hyper",
    "start": "1795440",
    "end": "1801080"
  },
  {
    "text": "threading so when we when we do position workloads we're looking more for physical cores and trying to drive the",
    "start": "1801080",
    "end": "1807860"
  },
  {
    "text": "workload to physical cores um I think you know we have some some some work to do to optimize it and make",
    "start": "1807860",
    "end": "1815539"
  },
  {
    "text": "better use of of what of what openshift offers and and other you know kubernetes",
    "start": "1815539",
    "end": "1821080"
  },
  {
    "text": "so just to uh one more thing so openshift is 100 kubernetes we are also",
    "start": "1821080",
    "end": "1827059"
  },
  {
    "text": "making Improvement on Upstream as well so things like C then c groups V2 these",
    "start": "1827059",
    "end": "1832760"
  },
  {
    "text": "are coming uh going forward which will enhance the other performance I mean we",
    "start": "1832760",
    "end": "1838279"
  },
  {
    "text": "have seen better results in in the lab and we are going forward to implement this uh within openshift as well so",
    "start": "1838279",
    "end": "1845299"
  },
  {
    "text": "going forward you will see good uh I mean I will working with Darren to see how the graph looks like when we",
    "start": "1845299",
    "end": "1851720"
  },
  {
    "text": "Implement those",
    "start": "1851720",
    "end": "1854200"
  },
  {
    "text": "your slide listed gpfs and ceph running with openshift was that running in cluster or was that external",
    "start": "1858080",
    "end": "1865760"
  },
  {
    "text": "they are so this is a managed service they are running external we're building them",
    "start": "1865760",
    "end": "1872299"
  },
  {
    "text": "within the same VPC but the control plane for the the cluster is is separate",
    "start": "1872299",
    "end": "1878480"
  },
  {
    "text": "and it's different so when we do the mounts what we do is is we're provisioning",
    "start": "1878480",
    "end": "1885559"
  },
  {
    "text": "um virtual machine nodes in the VPC and the control plane is managing all the",
    "start": "1885559",
    "end": "1890720"
  },
  {
    "text": "connections so if I understand your your question as being external the the file systems are in the same are",
    "start": "1890720",
    "end": "1898760"
  },
  {
    "text": "in one of the data centers that the VPC encapsulates the block storage that",
    "start": "1898760",
    "end": "1904399"
  },
  {
    "text": "we're building it on is usually local so we'll build it over local nvme nodes or",
    "start": "1904399",
    "end": "1910580"
  },
  {
    "text": "um bare metal or virtualized bare metal instances with",
    "start": "1910580",
    "end": "1916340"
  },
  {
    "text": "local storage so the The Operators aren't running in cluster and are like Rook Seth or using",
    "start": "1916340",
    "end": "1922640"
  },
  {
    "text": "uh cnsa for the gpfs layer no these are you know static persistent volume claims",
    "start": "1922640",
    "end": "1928880"
  },
  {
    "text": "that that mount this or the file system that we built within a customer within the VPC with the with the um with the",
    "start": "1928880",
    "end": "1937220"
  },
  {
    "text": "differences between SEF and gpfs did you see any performance differences in your workload we did so and it's really not a",
    "start": "1937220",
    "end": "1945980"
  },
  {
    "text": "it's not really a fair comparison Seth Seth performs much better than almost",
    "start": "1945980",
    "end": "1952039"
  },
  {
    "text": "any of the other open source file systems that that we've tried thus far and you know truth in advertising we",
    "start": "1952039",
    "end": "1958159"
  },
  {
    "text": "haven't tried luster yet but you know between gluster um Bayer NFS built over block storage",
    "start": "1958159",
    "end": "1964419"
  },
  {
    "text": "Seth outperforms them all gpfs is really optimized for HPC and it has some",
    "start": "1964419",
    "end": "1972620"
  },
  {
    "text": "features that Seth doesn't have which allows it allows us to tune metadata and",
    "start": "1972620",
    "end": "1978140"
  },
  {
    "text": "and just performance um in much more finely than we can with SEF so gpfs will outperform CEF",
    "start": "1978140",
    "end": "1987679"
  },
  {
    "text": "um but that being said from an open source perspective we're getting very good performance from Seth we still have a",
    "start": "1987679",
    "end": "1993500"
  },
  {
    "text": "lot of work to do but we're getting very good performance from",
    "start": "1993500",
    "end": "1997960"
  },
  {
    "text": "Seth uh thank you great talk uh you mentioned that for this SEF you have uh",
    "start": "2001360",
    "end": "2007179"
  },
  {
    "text": "the there's something missing for the HPC so I'm curious what is your views that Seth Visa with HPC",
    "start": "2007179",
    "end": "2016679"
  },
  {
    "text": "gpfs is really it's really kind of like a Ferrari I mean there are things that",
    "start": "2023679",
    "end": "2029260"
  },
  {
    "text": "it does very very well and it's and you can tune it to do certain things very well so in some",
    "start": "2029260",
    "end": "2036399"
  },
  {
    "text": "sense it's it's not as it's not a true Apples to Apples comparison it's more if I'm",
    "start": "2036399",
    "end": "2044440"
  },
  {
    "text": "willing to go out and get Spectrum scale take the time to configure it and and there are managed services for gpfs",
    "start": "2044440",
    "end": "2051460"
  },
  {
    "text": "today or Spectrum scale um but like any anything else you've got to tune in and so it just has more",
    "start": "2051460",
    "end": "2058720"
  },
  {
    "text": "capability under the hood than Seth does out of the box and I think that's the main way to explain it it's it and and",
    "start": "2058720",
    "end": "2066280"
  },
  {
    "text": "it it it's often um used in in",
    "start": "2066280",
    "end": "2071398"
  },
  {
    "text": "universities National Data Centers so there's there's this entire body of work where people have figured out for this",
    "start": "2071399",
    "end": "2078638"
  },
  {
    "text": "workload here's how gpfs gets configured so so it's it I expect it to outperform",
    "start": "2078639",
    "end": "2084760"
  },
  {
    "text": "Seth um out of the box now maybe there's somebody who's a SEF better at Seth than",
    "start": "2084760",
    "end": "2091358"
  },
  {
    "text": "I am who could get very close or beat gpfs but you know gpfs is really",
    "start": "2091359",
    "end": "2096940"
  },
  {
    "text": "starting with a you know a huge Arsenal ahead of time a huge priority advantage",
    "start": "2096940",
    "end": "2104800"
  },
  {
    "text": "all right thank you",
    "start": "2104800",
    "end": "2108119"
  }
]