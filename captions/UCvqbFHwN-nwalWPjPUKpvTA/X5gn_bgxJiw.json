[
  {
    "text": "hi everyone welcome to my talk on gpu's monitoring gpus at scale",
    "start": "80",
    "end": "8559"
  },
  {
    "text": "for ai ml and hpc clusters my name is bharti agarwal i'm a software",
    "start": "8639",
    "end": "15599"
  },
  {
    "text": "architect at nvidia working on logging and monitoring stack in the nvidia saturn 5 data center",
    "start": "15599",
    "end": "24960"
  },
  {
    "text": "we manage the nvidia saturn 5 data center using kubernetes this supports thousands of gpu servers",
    "start": "26480",
    "end": "33680"
  },
  {
    "text": "which supports hundreds of users running their machine learning training jobs these jobs use",
    "start": "33680",
    "end": "40079"
  },
  {
    "text": "terabytes of data we also have stakeholders and other users with observability needs",
    "start": "40079",
    "end": "46800"
  },
  {
    "text": "how to support monitoring at this scale for our agenda today we will first look",
    "start": "46800",
    "end": "53280"
  },
  {
    "text": "at what the saturn 5 data center is then we will review the various users we",
    "start": "53280",
    "end": "60239"
  },
  {
    "text": "have to support for observability we will look at the size and the scale",
    "start": "60239",
    "end": "67360"
  },
  {
    "text": "of the data center and the requirements that come out of that next we will go over the stack and a",
    "start": "71760",
    "end": "78320"
  },
  {
    "text": "couple of architectures of the stack that we set up to meet these requirements we will also cover some details of gpu",
    "start": "78320",
    "end": "85840"
  },
  {
    "text": "matrix collection we will look at the scale challenges we hit and how we solve for them",
    "start": "85840",
    "end": "92799"
  },
  {
    "text": "finally we will look at some example user views",
    "start": "92799",
    "end": "97439"
  },
  {
    "text": "let's look at the nvidia saturn 5 data center so what is the nvidia saturn 5 data",
    "start": "97920",
    "end": "104240"
  },
  {
    "text": "center or nsv for short at nvidia we have a lot of users who",
    "start": "104240",
    "end": "109920"
  },
  {
    "text": "need to run their artificial intelligence and machine learning jobs on the gpu server",
    "start": "109920",
    "end": "115920"
  },
  {
    "text": "they can set up their own server to do this but this limits them in the size of the job and the technology it would also add",
    "start": "115920",
    "end": "123759"
  },
  {
    "text": "cost to maintain these nodes in order to facilitate these users our team set up clusters",
    "start": "123759",
    "end": "130640"
  },
  {
    "text": "with hundreds of gpu nodes with the latest technologies for the user to deploy their jobs on",
    "start": "130640",
    "end": "138480"
  },
  {
    "text": "we also added a cpu control plane to allow us to add a scheduler for the user",
    "start": "138480",
    "end": "144080"
  },
  {
    "text": "for the user jobs and to support observability",
    "start": "144080",
    "end": "149040"
  },
  {
    "text": "on top of this we added the nsv cloud control plane for the users to interact with the users",
    "start": "149599",
    "end": "156959"
  },
  {
    "text": "use the nsv api to schedule their jobs on the nsv cloud control plane",
    "start": "156959",
    "end": "163200"
  },
  {
    "text": "the request is then sent to this cpu control plane which forwards it on",
    "start": "163200",
    "end": "170959"
  },
  {
    "text": "to the scheduler the scheduler sends metrics back to the cpu control",
    "start": "170959",
    "end": "178400"
  },
  {
    "text": "plane the users are also able to leverage the ngc registry which contains",
    "start": "178400",
    "end": "185280"
  },
  {
    "text": "containers data sets and pre-trained models for them to use this is great users get the latest",
    "start": "185280",
    "end": "192640"
  },
  {
    "text": "technology and are able to deploy their jobs and be guaranteed that they will have resources",
    "start": "192640",
    "end": "199280"
  },
  {
    "text": "to run on however we need to ensure we can meet this guarantee observability is the key to this",
    "start": "199280",
    "end": "206080"
  },
  {
    "text": "guarantee let's look at the setup and the use cases we need to support for this",
    "start": "206080",
    "end": "213840"
  },
  {
    "text": "next let's look at the users the size and the scale as we saw in the nsp data center we have",
    "start": "217519",
    "end": "224879"
  },
  {
    "text": "the kubernetes cluster with gpu nodes and cpu control plane from the observability perspective apart",
    "start": "224879",
    "end": "231840"
  },
  {
    "text": "from the end users we have various users accessing this system",
    "start": "231840",
    "end": "237200"
  },
  {
    "text": "we have the admin user who would like to get a view on the overall health of the cluster and get real-time alerts",
    "start": "237200",
    "end": "244480"
  },
  {
    "text": "in slack and page of duty we have the machine learning and artificial intelligence",
    "start": "244480",
    "end": "249920"
  },
  {
    "text": "end users who would like to see the performance of their jobs in terms of the job telemetry",
    "start": "249920",
    "end": "255280"
  },
  {
    "text": "and the application telemetry we have the stakeholders these are the product managers managers",
    "start": "255280",
    "end": "263040"
  },
  {
    "text": "architects and other org leaders who would like to get an overall occupancy",
    "start": "263040",
    "end": "268479"
  },
  {
    "text": "and yield of the system for capacity planning purposes then we have the nsp developers",
    "start": "268479",
    "end": "275120"
  },
  {
    "text": "these are developers like me who worked on the scheduler and the observability stack",
    "start": "275120",
    "end": "281440"
  },
  {
    "text": "these users would like to be able to get a view into the health and resource usage of the components",
    "start": "281440",
    "end": "287360"
  },
  {
    "text": "and be able to root cause issues",
    "start": "287360",
    "end": "291039"
  },
  {
    "text": "how do we how do these use cases map into requirements for the end user who wants to see how",
    "start": "293600",
    "end": "300160"
  },
  {
    "text": "efficiently their jobs are running in terms of resource usage and their custom metrics we need to",
    "start": "300160",
    "end": "306560"
  },
  {
    "text": "collect the resources resource and application metrics and send them to the nsv cloud control plan",
    "start": "306560",
    "end": "313199"
  },
  {
    "text": "for the nsv admin user who wants to monitor the overall health of the system",
    "start": "313199",
    "end": "318320"
  },
  {
    "text": "get real-time notifications on incidents and be able to root cause them effectively we need to centralize the",
    "start": "318320",
    "end": "325680"
  },
  {
    "text": "metrics and logs in real time the real-time alerts also need to be sent to slack and creativity",
    "start": "325680",
    "end": "332720"
  },
  {
    "text": "we need to ensure that the stack is highly available and can scale horizontally as nodes get added to the clusters",
    "start": "332720",
    "end": "340479"
  },
  {
    "text": "for the stakeholders we want to get metrics aggregation at the job level and retain these for one year we also",
    "start": "340479",
    "end": "347440"
  },
  {
    "text": "need to show you an efficiency in a dashboard and weekly reports finally the nsp",
    "start": "347440",
    "end": "355440"
  },
  {
    "text": "developer for them we need to collect and centralize the metrics and logs of the nsv components our",
    "start": "355440",
    "end": "366160"
  },
  {
    "text": "system needs to support multiple production clusters each with up to 600 nodes 250 of which",
    "start": "366160",
    "end": "373600"
  },
  {
    "text": "contain 2 000 gpus along with this we have several non-part",
    "start": "373600",
    "end": "379280"
  },
  {
    "text": "clusters where we need to support the same logging and monitoring staff we use ci",
    "start": "379280",
    "end": "384800"
  },
  {
    "text": "cd processed for promotion to manage these these clusters keep growing",
    "start": "384800",
    "end": "392560"
  },
  {
    "text": "let's go over the system requirements that come out of this",
    "start": "392960",
    "end": "397600"
  },
  {
    "text": "for the functional requirements we need to centrally collect the metrics for all levels",
    "start": "401120",
    "end": "407120"
  },
  {
    "text": "the cluster the node the kubernetes system and jobs at scale we need to meet the",
    "start": "407120",
    "end": "414080"
  },
  {
    "text": "sla to allow users to get real-time metrics that is to say less than one minute from",
    "start": "414080",
    "end": "419440"
  },
  {
    "text": "node to system these metrics have to be forwarded to external systems",
    "start": "419440",
    "end": "425360"
  },
  {
    "text": "the growing data needs to be retained to support long-term views for the non-functional requirements",
    "start": "425360",
    "end": "433440"
  },
  {
    "text": "we we need to consider data durability that is to say there should be no data loss",
    "start": "433440",
    "end": "438880"
  },
  {
    "text": "the system should be highly available the stack should scale as we add nodes",
    "start": "438880",
    "end": "444319"
  },
  {
    "text": "and it should be resilient we also need to keep security in mind when we send metrics to the external",
    "start": "444319",
    "end": "450240"
  },
  {
    "text": "cloud",
    "start": "450240",
    "end": "452638"
  },
  {
    "text": "let's now look at the stack and the architecture",
    "start": "455440",
    "end": "459759"
  },
  {
    "text": "these are the components our stack consists of we decided to use kubernetes for orchestration",
    "start": "460560",
    "end": "466560"
  },
  {
    "text": "and lifecycle management of the system the system uses kubernetes device plugin",
    "start": "466560",
    "end": "473680"
  },
  {
    "text": "for gpu enablement under communities we developed the nsv scheduler which is",
    "start": "473680",
    "end": "479360"
  },
  {
    "text": "the kubernetes custom controller for the observability stack we have the nvidia data center gpu manager component",
    "start": "479360",
    "end": "486800"
  },
  {
    "text": "for gpu metrics we decided to use prometheus operator for metrics",
    "start": "486800",
    "end": "492000"
  },
  {
    "text": "collection and alerting this stack comes with grafana for visualizing",
    "start": "492000",
    "end": "497440"
  },
  {
    "text": "this is native to kubernetes and widely used with a lot of community support behind it it is highly performant in collecting",
    "start": "497440",
    "end": "504960"
  },
  {
    "text": "metrics and writing to remote storage to support in-cluster data persistence",
    "start": "504960",
    "end": "510879"
  },
  {
    "text": "we use influx tv thanos and swift stack object store are used for data durability and storage",
    "start": "510879",
    "end": "519039"
  },
  {
    "text": "fluentd agent is used for the log collection and sending blogs to the central login service",
    "start": "519039",
    "end": "525200"
  },
  {
    "text": "which can be influx tv or relog we use nginx ingress controller for",
    "start": "525200",
    "end": "530880"
  },
  {
    "text": "exposing the stack endpoints to the users in some clusters based on requirements",
    "start": "530880",
    "end": "537760"
  },
  {
    "text": "we are using greylog instead of influx gb for logs",
    "start": "537760",
    "end": "542720"
  },
  {
    "text": "this is how it all comes together in terms of the overall architecture we use all open source components",
    "start": "545040",
    "end": "552399"
  },
  {
    "text": "on the left hand side we can see how the metrics are centralized node exporter is running on the compute",
    "start": "552399",
    "end": "558800"
  },
  {
    "text": "worker nodes these can be either gpu or cpu knobs it exposes node and gpu level metrics",
    "start": "558800",
    "end": "566959"
  },
  {
    "text": "applications can also expose service metrics at an end point these metrics are scraped by prometheus",
    "start": "566959",
    "end": "573839"
  },
  {
    "text": "running in the cpu control plane at a 30 second interval these metrics",
    "start": "573839",
    "end": "579279"
  },
  {
    "text": "are sent to influx cv for class in cluster persistence from",
    "start": "579279",
    "end": "584480"
  },
  {
    "text": "here they are forwarded to the metrics transporter via the n clutch cv subscription setup this transporter then",
    "start": "584480",
    "end": "591839"
  },
  {
    "text": "forwards them to the external cloud systems having matrix transporter on a different",
    "start": "591839",
    "end": "598080"
  },
  {
    "text": "node from the source node with a separate vpc allowed us to introduce a level of",
    "start": "598080",
    "end": "604800"
  },
  {
    "text": "security to ensure that the data was not would not be",
    "start": "604800",
    "end": "610160"
  },
  {
    "text": "hacked we also have thanos sidecar running on",
    "start": "610160",
    "end": "616399"
  },
  {
    "text": "the system that sends these metrics to switch stack object store for long term storage and data",
    "start": "616399",
    "end": "622399"
  },
  {
    "text": "durability grafana is used to visualize these metrics for the nsv developers",
    "start": "622399",
    "end": "627519"
  },
  {
    "text": "and the admin users alerts are evaluated by prometheus and forward it to alert",
    "start": "627519",
    "end": "632640"
  },
  {
    "text": "manager which will then forward them to slack and pagerduty on the right hand side we see the flow",
    "start": "632640",
    "end": "639040"
  },
  {
    "text": "for logs a fluency agent is running on the nodes to collect the system and send the system logs to influxdb",
    "start": "639040",
    "end": "647120"
  },
  {
    "text": "an application can also add a fluency login sidecar and send their logs to the same",
    "start": "647120",
    "end": "652320"
  },
  {
    "text": "centralized service as well the users will this will see this data in the nvc",
    "start": "652320",
    "end": "657839"
  },
  {
    "text": "ui and the stakeholders will use the elk stack on the data lake to visualize the job",
    "start": "657839",
    "end": "663680"
  },
  {
    "text": "metrics stakeholders also get a weekly yield and showback report that uses the data from the data lake",
    "start": "663680",
    "end": "672480"
  },
  {
    "text": "here is a variation of the architecture we set up for another cluster the only change here is that we do not",
    "start": "673279",
    "end": "679440"
  },
  {
    "text": "need to send metrics to external systems so we were able to replace influx tv with greylock for logs",
    "start": "679440",
    "end": "686240"
  },
  {
    "text": "this has care better for us for our loads we also introduced the thanos receiver",
    "start": "686240",
    "end": "693200"
  },
  {
    "text": "to give us better data durabilities as you can see we were able to adapt our",
    "start": "693200",
    "end": "699839"
  },
  {
    "text": "architecture for a slightly different set of requirements",
    "start": "699839",
    "end": "704720"
  },
  {
    "text": "let's take a quick look at the details of how we collect gpu metrics",
    "start": "705519",
    "end": "710720"
  },
  {
    "text": "in this diagram you will see on the gpu node on the bottom we have several jobs running",
    "start": "710720",
    "end": "717120"
  },
  {
    "text": "node exporter pod has a data center gpu manager or dcgm exporter as a container that",
    "start": "717120",
    "end": "724079"
  },
  {
    "text": "collects gpu metrics from the dcgm library running in the pod as well",
    "start": "724079",
    "end": "730240"
  },
  {
    "text": "to get hard level details we have hot exporter that talks to cubelet to see which part",
    "start": "730240",
    "end": "736560"
  },
  {
    "text": "is using with gpu with this integration we can get",
    "start": "736560",
    "end": "741839"
  },
  {
    "text": "job level metrics",
    "start": "741839",
    "end": "745839"
  },
  {
    "text": "in prometheus in the monitoring nodes we will scrape these metrics at every 30 seconds these then get sent",
    "start": "746959",
    "end": "754639"
  },
  {
    "text": "via influx tv to the metrics transporter and from there to the cloud systems",
    "start": "754639",
    "end": "759920"
  },
  {
    "text": "some of the key gpu metrics are listed here",
    "start": "759920",
    "end": "764320"
  },
  {
    "text": "to break the flow a bit i want to show one example of view that comes",
    "start": "770160",
    "end": "776240"
  },
  {
    "text": "out of the first architecture we covered this is a view of the job telemetry the end user will see",
    "start": "776240",
    "end": "782560"
  },
  {
    "text": "at the top you can see the job runtime and the time the gpus are active and the",
    "start": "782560",
    "end": "788880"
  },
  {
    "text": "average gpu utilization across the 8 gpus being used by this job",
    "start": "788880",
    "end": "794160"
  },
  {
    "text": "below that you can see a line graph for the average gpu metrics over time like gpu active tensor core",
    "start": "794160",
    "end": "802160"
  },
  {
    "text": "active gpu memory and gpu power in this job you can see that gpu is being kept busy",
    "start": "802160",
    "end": "809120"
  },
  {
    "text": "with tensor core at 25 the overlay shows the breakdown of the",
    "start": "809120",
    "end": "814320"
  },
  {
    "text": "metrics at any point in time you can see details like the pcie",
    "start": "814320",
    "end": "819519"
  },
  {
    "text": "read-write bandwidth values the in-wind link bandwidth values the cpu and memory usage the heat map",
    "start": "819519",
    "end": "827120"
  },
  {
    "text": "below shows gpu utilization and the tensor core utilization for each gpu",
    "start": "827120",
    "end": "834079"
  },
  {
    "text": "let's now look at the scale challenges",
    "start": "834880",
    "end": "843440"
  },
  {
    "text": "as you saw in the architecture diagram we used a lot of cncf components",
    "start": "843440",
    "end": "848959"
  },
  {
    "text": "these are great as there are many solutions that exist and independently they solve for a lot of our functional",
    "start": "848959",
    "end": "854839"
  },
  {
    "text": "requirements they're all native cloud technologies and have strong community support behind",
    "start": "854839",
    "end": "860240"
  },
  {
    "text": "them this allows them to continuously evolve and with the growing needs of the community",
    "start": "860240",
    "end": "866560"
  },
  {
    "text": "this has been a lifesaver for us many a times just an upgrade of the stack got us the resolution up",
    "start": "866560",
    "end": "872880"
  },
  {
    "text": "for an issue and improved the performance all the components are also easily configurable",
    "start": "872880",
    "end": "878959"
  },
  {
    "text": "why i help the stable helm charts however there was a lot of trial and error involved in finding the right",
    "start": "878959",
    "end": "885600"
  },
  {
    "text": "configuration for us for our requirements the community has many references for the",
    "start": "885600",
    "end": "892079"
  },
  {
    "text": "integrations and we were able to leverage this we took a lot of the best practice and adapted them for our loads",
    "start": "892079",
    "end": "899279"
  },
  {
    "text": "we had to tune for size high availability scalability and data durability",
    "start": "899279",
    "end": "904880"
  },
  {
    "text": "not only did we have to optimize the configurations but we needed to get a good understanding of how to adapt them",
    "start": "904880",
    "end": "911839"
  },
  {
    "text": "as the clusters grew let's now look at how we optimize the",
    "start": "911839",
    "end": "918560"
  },
  {
    "text": "components for our scale for prometheus and other components we had to understand the resource requests",
    "start": "918560",
    "end": "924639"
  },
  {
    "text": "and limits that would work well for our loads with room to grow to size each component well we had to",
    "start": "924639",
    "end": "931279"
  },
  {
    "text": "understand how it used the resources prometheus for example keeps two hours",
    "start": "931279",
    "end": "936959"
  },
  {
    "text": "of data in memory the till it is compacted to this",
    "start": "936959",
    "end": "942240"
  },
  {
    "text": "prometheus size is dependent on the total number of series it is supporting you can see some of our",
    "start": "942240",
    "end": "948160"
  },
  {
    "text": "load numbers here it restarts at restart it also loads",
    "start": "948160",
    "end": "954160"
  },
  {
    "text": "wall files which depending on the load can be large this impacts the restart time of",
    "start": "954160",
    "end": "959839"
  },
  {
    "text": "prometheus as well as the memory usage we did rigorous load tests to get a baseline for the size of each component",
    "start": "959839",
    "end": "967040"
  },
  {
    "text": "this helped us avoid frequent failures in production however due to the unfair",
    "start": "967040",
    "end": "972600"
  },
  {
    "text": "unpredictability of production environments we kept a close eye on the systems",
    "start": "972600",
    "end": "978160"
  },
  {
    "text": "in production to ensure that we did not hit auto memory or cpu throttling errors",
    "start": "978160",
    "end": "984800"
  },
  {
    "text": "we do regular evaluations of our system to ensure that the sizing is working well",
    "start": "984800",
    "end": "990160"
  },
  {
    "text": "as we add nodes to the cluster we review these settings to make the system highly available we",
    "start": "990160",
    "end": "996800"
  },
  {
    "text": "had to learn what load could be supported by prometheus with acceptable restart times",
    "start": "996800",
    "end": "1002480"
  },
  {
    "text": "versus where we needed to add another replica for scaling from ethios we rely on",
    "start": "1002480",
    "end": "1008560"
  },
  {
    "text": "sizing of the instances from our load test so far we've been able to manage each cluster with two instances",
    "start": "1008560",
    "end": "1015279"
  },
  {
    "text": "with tuning on the resources there were some trickier challenges we had the first of this",
    "start": "1015279",
    "end": "1021680"
  },
  {
    "text": "was data persistence we needed the metrics to always be available",
    "start": "1021680",
    "end": "1028079"
  },
  {
    "text": "for viewing for this we needed to add in cluster persistence with replication",
    "start": "1028079",
    "end": "1034160"
  },
  {
    "text": "we used influx tv with for remote read and write to cover this",
    "start": "1034160",
    "end": "1039438"
  },
  {
    "text": "this worked well up to a certain load after which we started seeing data gaps",
    "start": "1039439",
    "end": "1045199"
  },
  {
    "text": "to resolve this we have to look into the code of prometheus and the downstream components we found",
    "start": "1045199",
    "end": "1050799"
  },
  {
    "text": "that prometheus would drop metrics when its buffer was full or if the downstream components were not expecting",
    "start": "1050799",
    "end": "1057280"
  },
  {
    "text": "metrics so there were several points of failure with the 2.8 version of prometheus where",
    "start": "1057280",
    "end": "1063840"
  },
  {
    "text": "prometheus introduced wall for remote drive this occurrence was reduced but prior to that we had to go we had to",
    "start": "1063840",
    "end": "1071120"
  },
  {
    "text": "optimize the downstream components as well as the buffer of the prometheus so metrics did not get",
    "start": "1071120",
    "end": "1076799"
  },
  {
    "text": "dropped i will go into some details of the downstream components in the next five",
    "start": "1076799",
    "end": "1081919"
  },
  {
    "text": "slide for the data durability we had to introduce thanos side car",
    "start": "1081919",
    "end": "1087919"
  },
  {
    "text": "in one instance and thanos receiver in another thanos sideka worked well for us but",
    "start": "1087919",
    "end": "1094559"
  },
  {
    "text": "thanos receiver was new and we hit a lot of challenges with it",
    "start": "1094559",
    "end": "1099679"
  },
  {
    "text": "with the updated version of prometheus we had to optimize the max samples max shards and capacity size to optimize",
    "start": "1099679",
    "end": "1106880"
  },
  {
    "text": "based on the samples size of the cluster load tests helped us",
    "start": "1106880",
    "end": "1111919"
  },
  {
    "text": "get to the optimal setting for this",
    "start": "1111919",
    "end": "1116080"
  },
  {
    "text": "the influx db tuning for sizing ha and scalability was similar to prometheus influx tv keeps a",
    "start": "1121440",
    "end": "1129039"
  },
  {
    "text": "lot of data in memory and restarts times along as the container needs reads the wall files",
    "start": "1129039",
    "end": "1135520"
  },
  {
    "text": "we also found that influx db did not scale well for prometheus data which has a lot of",
    "start": "1135520",
    "end": "1141280"
  },
  {
    "text": "labels which need to be indexed sizing had to consider this due to long",
    "start": "1141280",
    "end": "1146720"
  },
  {
    "text": "restart times for ha we needed to add replicas we added influx tv relay for application",
    "start": "1146720",
    "end": "1153440"
  },
  {
    "text": "however influx db relay is not supported anymore so we had to support it ourselves for",
    "start": "1153440",
    "end": "1159360"
  },
  {
    "text": "our setup this also helped for the data persistence requirements",
    "start": "1159360",
    "end": "1164880"
  },
  {
    "text": "sizing of the resources helped us support the scale however influx db did not scale to our",
    "start": "1164880",
    "end": "1171280"
  },
  {
    "text": "minimum requirement of two weeks retention we could only support three",
    "start": "1171280",
    "end": "1176320"
  },
  {
    "text": "days of retention with confidence for getting longer attention we introduced thanos sidecar",
    "start": "1176320",
    "end": "1181760"
  },
  {
    "text": "in one setup and moved away from influx tv to thanos receiver in another as i",
    "start": "1181760",
    "end": "1187600"
  },
  {
    "text": "mentioned we had added input tv in our architecture for data persistence in the cluster",
    "start": "1187600",
    "end": "1192880"
  },
  {
    "text": "since this is a kubernetes cluster parts can restart for various reasons and land on new nodes",
    "start": "1192880",
    "end": "1199760"
  },
  {
    "text": "to process data for influx tv we are using host path persistent volumes with pod restarts we",
    "start": "1200160",
    "end": "1206960"
  },
  {
    "text": "could easily have parts land on different nodes and blues data our ha solution of in cluster",
    "start": "1206960",
    "end": "1212480"
  },
  {
    "text": "application helped here here we had to ensure that the data we are sending to external",
    "start": "1212480",
    "end": "1218880"
  },
  {
    "text": "system had no gaps as well this turned out to be a very hard issue to debug and",
    "start": "1218880",
    "end": "1224159"
  },
  {
    "text": "solve for and needed detailed investigation into the component codes to understand how best to solve it the",
    "start": "1224159",
    "end": "1231039"
  },
  {
    "text": "solution involved updates to the code of the services",
    "start": "1231039",
    "end": "1236080"
  },
  {
    "text": "tunnels which we use for data durability comes with many components sizing for tunnels involved",
    "start": "1237440",
    "end": "1242960"
  },
  {
    "text": "understanding each of these and how best to optimize them for our loads for example thanos store for",
    "start": "1242960",
    "end": "1250159"
  },
  {
    "text": "thunderstorm we needed to set max cache size for thanos receiver we discovered that",
    "start": "1250159",
    "end": "1255679"
  },
  {
    "text": "setting the tstb min block and match block to 15 minute would reduce the wall size",
    "start": "1255679",
    "end": "1261280"
  },
  {
    "text": "which allowed us to reduce the restart times and have thanos receiver not use increasing amounts of memory",
    "start": "1261280",
    "end": "1268400"
  },
  {
    "text": "aj was easily supported by thanos by optimizing the replicas for the components as needed",
    "start": "1268400",
    "end": "1274400"
  },
  {
    "text": "for scale in chronos receiver we had to move to the grpc version",
    "start": "1274400",
    "end": "1279919"
  },
  {
    "text": "thanos helped us solve the issue of data durability and data persistence data in a kubernetes cluster is",
    "start": "1279919",
    "end": "1286080"
  },
  {
    "text": "ephemeral thundersidecar sends data to an object story for the tree",
    "start": "1286080",
    "end": "1291520"
  },
  {
    "text": "this component is fairly robust and gave us data durability for data persisted to disk this is prometheus this",
    "start": "1291520",
    "end": "1299039"
  },
  {
    "text": "in prometheus is every two hours and we use influx db for the immediate time window this",
    "start": "1299039",
    "end": "1305600"
  },
  {
    "text": "worked well however in a variant architecture that i showed where we used thanos receiver we had a",
    "start": "1305600",
    "end": "1312480"
  },
  {
    "text": "lot of issues with load we had to delve into the code and figure out the root cause",
    "start": "1312480",
    "end": "1318240"
  },
  {
    "text": "and then see how best to solve for it luckily we were able to upgrade prometheus and thanos versions and tune",
    "start": "1318240",
    "end": "1325360"
  },
  {
    "text": "the configurations to solve for our loads we had to do it",
    "start": "1325360",
    "end": "1330799"
  },
  {
    "text": "we had to do rigorous load tests to set up the max shards max samples and capacity for prometheus",
    "start": "1330799",
    "end": "1337679"
  },
  {
    "text": "report drive for our loads down sampling allowed us to support longer data retention and querying",
    "start": "1337679",
    "end": "1347840"
  },
  {
    "text": "in some of our deployments we use greylock for logging greylock uses elastic search and mongodb",
    "start": "1350080",
    "end": "1356640"
  },
  {
    "text": "underneath each had to be separately undisturbed and tuned load tests helped us but we also had to",
    "start": "1356640",
    "end": "1364000"
  },
  {
    "text": "have we also have a lot of battle scars for sizing",
    "start": "1364000",
    "end": "1369039"
  },
  {
    "text": "we had to look at how to tune each component these are written in java so we had to",
    "start": "1369039",
    "end": "1374240"
  },
  {
    "text": "optimize the heap size and the container sizes we also learned that we had to set cpu",
    "start": "1374240",
    "end": "1379600"
  },
  {
    "text": "limit as without it java would default to just one cpu",
    "start": "1379600",
    "end": "1388000"
  },
  {
    "text": "for high availability initially we added replicas though elasticsearch and mongodb did",
    "start": "1388000",
    "end": "1394240"
  },
  {
    "text": "fairly well with the default replicas greylock performance seemed to do better with more of the bills",
    "start": "1394240",
    "end": "1400080"
  },
  {
    "text": "as our lords grew we quickly hit scale issues with greyhound just adding replicas did not help",
    "start": "1400080",
    "end": "1407919"
  },
  {
    "text": "we had to look at sizing as well we learned there was a heap size recommended for",
    "start": "1407919",
    "end": "1412960"
  },
  {
    "text": "elasticsearch that made it perform optimally for data persistence we added the",
    "start": "1412960",
    "end": "1419440"
  },
  {
    "text": "persistent volume for greylock journal data written to elasticsearch was already well managed relog and",
    "start": "1419440",
    "end": "1426960"
  },
  {
    "text": "elasticsearch handle data as derivative let's now look at some user views",
    "start": "1426960",
    "end": "1435039"
  },
  {
    "text": "here is an example of the nsv admin view this is a capacity dashboard",
    "start": "1435679",
    "end": "1441360"
  },
  {
    "text": "it shows the cluster capacity with total nodes in the cluster and total number of code",
    "start": "1441360",
    "end": "1447039"
  },
  {
    "text": "nodes cordoned on the top and below that it shows nodes coordinate by node type on the right it shows the",
    "start": "1447039",
    "end": "1455120"
  },
  {
    "text": "availability of the different node types with this the admin users can get a good view",
    "start": "1455120",
    "end": "1460640"
  },
  {
    "text": "into the cluster capacity this view shows us the nsv developer's",
    "start": "1460640",
    "end": "1467919"
  },
  {
    "text": "view for the job controller the pie charts at the top show counts of jobs in the different states and below",
    "start": "1467919",
    "end": "1474240"
  },
  {
    "text": "that we see the timelines for jobs in various states running queued task lost and fair",
    "start": "1474240",
    "end": "1481679"
  },
  {
    "text": "this gives a good view to the developers for the current state of the dog controller",
    "start": "1481679",
    "end": "1487679"
  },
  {
    "text": "this is a stakeholders view the pie chart at the top left shows the jobs in different states at the moment",
    "start": "1487679",
    "end": "1494159"
  },
  {
    "text": "then we have timelines of jobs running by different aspects by time by gpu allocated running",
    "start": "1494159",
    "end": "1500720"
  },
  {
    "text": "duration or theme registry etc",
    "start": "1500720",
    "end": "1506080"
  },
  {
    "text": "and finally we have the yield view it shows year by jobs the green bars",
    "start": "1507520",
    "end": "1515200"
  },
  {
    "text": "or gpu hours the blue line you can see the dips in both when we have issues in",
    "start": "1515200",
    "end": "1520720"
  },
  {
    "text": "our cluster for example when koi io was down users could not download their job containers",
    "start": "1520720",
    "end": "1527919"
  },
  {
    "text": "this in turn calls our year to drop",
    "start": "1527919",
    "end": "1531759"
  },
  {
    "text": "thank you for your for attending i will now take questions",
    "start": "1533520",
    "end": "1543840"
  },
  {
    "text": "can you hear me",
    "start": "1638159",
    "end": "1641840"
  },
  {
    "text": "am i on",
    "start": "1646320",
    "end": "1648960"
  },
  {
    "text": "hi everyone okay so we have a few questions here um",
    "start": "1652080",
    "end": "1659278"
  },
  {
    "text": "okay let's see um",
    "start": "1660799",
    "end": "1665278"
  },
  {
    "text": "are the grafana dashboards available anywhere for folks to look at",
    "start": "1666559",
    "end": "1672000"
  },
  {
    "text": "the ones i've shared here was done by us they're custom dashboards but if you",
    "start": "1672000",
    "end": "1677840"
  },
  {
    "text": "download the prometheus operator helm chart it comes with some predefined",
    "start": "1677840",
    "end": "1683440"
  },
  {
    "text": "dashboards that you'll get out of the box so you should be able to kind of install",
    "start": "1683440",
    "end": "1688640"
  },
  {
    "text": "it and look at it",
    "start": "1688640",
    "end": "1696720"
  },
  {
    "text": "no uh can we use the nsv scheduler is it open source unfortunately not uh it is an internal",
    "start": "1696720",
    "end": "1703120"
  },
  {
    "text": "scheduler that we are using within nvidia sorry about that",
    "start": "1703120",
    "end": "1711840"
  },
  {
    "text": "how is the gpu usage calculated for different applications running",
    "start": "1714399",
    "end": "1719600"
  },
  {
    "text": "in the kts so that their sras are satisfied so um basically we are monitoring the",
    "start": "1719600",
    "end": "1727600"
  },
  {
    "text": "actual utilization of the gpu um so we have several uh profiling metrics that we grab",
    "start": "1727600",
    "end": "1734559"
  },
  {
    "text": "we grab the percentage usage uh percentage active for the gpu the percentage active of the tensor",
    "start": "1734559",
    "end": "1741360"
  },
  {
    "text": "core so while the job is running we are monitoring it um almost every second but we're collecting the metric at every 30 second",
    "start": "1741360",
    "end": "1748159"
  },
  {
    "text": "rate so engineers can see that what rate they're using the gpu add and optimize",
    "start": "1748159",
    "end": "1754720"
  },
  {
    "text": "it so it's really up to the engineers to kind of do the tuning we help guide them some of our senior ml",
    "start": "1754720",
    "end": "1762640"
  },
  {
    "text": "engineers will help guide them how to tune it better but the application is really the one",
    "start": "1762640",
    "end": "1768159"
  },
  {
    "text": "that needs to kind of satisfy their own slas",
    "start": "1768159",
    "end": "1773440"
  },
  {
    "text": "let's see what else",
    "start": "1775760",
    "end": "1781840"
  },
  {
    "text": "okay let's see if i have any more",
    "start": "1796640",
    "end": "1800720"
  },
  {
    "text": "questions",
    "start": "1806840",
    "end": "1809840"
  },
  {
    "text": "okay we wait for a little bit longer see if any more questions come in",
    "start": "1820480",
    "end": "1837840"
  },
  {
    "text": "okay so let's see there are some medium level ones that are coming in",
    "start": "1842640",
    "end": "1851840"
  },
  {
    "text": "so this for the slides you guys can kind of download it just from the um the ui i think um so",
    "start": "1854960",
    "end": "1862559"
  },
  {
    "text": "here's the response that they have click the handouts widget below to download the slide deck",
    "start": "1862559",
    "end": "1870159"
  },
  {
    "text": "so that's that",
    "start": "1870159",
    "end": "1873039"
  },
  {
    "text": "um see oh would you mind sharing okay",
    "start": "1876840",
    "end": "1885200"
  },
  {
    "text": "and then you have um is gpu okay hold on there's questions coming in",
    "start": "1887440",
    "end": "1895440"
  },
  {
    "text": "how about if there happen anomaly and causes to use gpu more than the normal",
    "start": "1895440",
    "end": "1901120"
  },
  {
    "text": "level um i think that's more about just gpu usage",
    "start": "1901120",
    "end": "1909120"
  },
  {
    "text": "um i lost the question",
    "start": "1909120",
    "end": "1916398"
  },
  {
    "text": "just a second um so i i mean that's really up to the engineers",
    "start": "1916799",
    "end": "1922000"
  },
  {
    "text": "to kind of resolve those types of things we are just about um setting up the scheduler to allow them to run their",
    "start": "1922000",
    "end": "1928720"
  },
  {
    "text": "jobs um you know they can get the resources but optimizing and tuning the",
    "start": "1928720",
    "end": "1934320"
  },
  {
    "text": "application um so i guess what you're asking is if there's an infrastructure anomaly",
    "start": "1934320",
    "end": "1940799"
  },
  {
    "text": "that potentially causes them to use gpu more than the normal",
    "start": "1940799",
    "end": "1947600"
  },
  {
    "text": "level i am not really clear on what that anomaly would look like the",
    "start": "1947600",
    "end": "1952880"
  },
  {
    "text": "the issues that might happen at the infrastructure level would be like um they may not be able to mount um",
    "start": "1952880",
    "end": "1959840"
  },
  {
    "text": "their cf volumes or their data sources and then that is when we would step in and try to address that",
    "start": "1959840",
    "end": "1966000"
  },
  {
    "text": "but if in terms of um because they're in a in a pod in a",
    "start": "1966000",
    "end": "1971360"
  },
  {
    "text": "container i don't you know usually they have to tell us exactly how many gpus they want so",
    "start": "1971360",
    "end": "1977279"
  },
  {
    "text": "they're restricted to the number of gpus so they can't really i use more than they're asking for",
    "start": "1977279",
    "end": "1985120"
  },
  {
    "text": "i hope that answers your question i'm not i'm sorry if it doesn't answer it very well um okay",
    "start": "1985440",
    "end": "1992559"
  },
  {
    "text": "so this is uh why influx tv wasn't thanos enough to satisfy long-term",
    "start": "1992559",
    "end": "1998000"
  },
  {
    "text": "storage um at the time that we did it the first install that we did it",
    "start": "1998000",
    "end": "2003519"
  },
  {
    "text": "thanos was not quite there yet it had just started like six months after we did the first implementation so at",
    "start": "2003519",
    "end": "2010240"
  },
  {
    "text": "that time the industry most of the community was using influx db for the long-term retention",
    "start": "2010240",
    "end": "2015840"
  },
  {
    "text": "from um prometheus so that's why we started with infrastb and we did see scaling problems with it",
    "start": "2015840",
    "end": "2022720"
  },
  {
    "text": "so our second iteration the second architecture that i shared with you has thanos in it it doesn't have influx tv",
    "start": "2022720",
    "end": "2029600"
  },
  {
    "text": "so we quickly upgraded to thanos as soon as as soon as it was ready",
    "start": "2029600",
    "end": "2035440"
  },
  {
    "text": "um is gpu sharing talked into account taken into account in your",
    "start": "2038080",
    "end": "2045440"
  },
  {
    "text": "jazz scores um gpu sharing is not yet taken into",
    "start": "2045440",
    "end": "2051520"
  },
  {
    "text": "account in the dashboards um we we are just talking about um",
    "start": "2051520",
    "end": "2056960"
  },
  {
    "text": "gpu affinity in our scheduler um we haven't really talked about gpu sharing as such between",
    "start": "2056960",
    "end": "2063679"
  },
  {
    "text": "i assume you mean between multiple jobs by a team of users um i think what we're gonna do is in the",
    "start": "2063679",
    "end": "2070240"
  },
  {
    "text": "next inter next upgrade we'll be introducing some of those capabilities but that's not in",
    "start": "2070240",
    "end": "2075679"
  },
  {
    "text": "place yet so once it's there then i'm sure the dashboards will be there for that",
    "start": "2075679",
    "end": "2082000"
  },
  {
    "text": "they're not there right now would it be possible to go",
    "start": "2082000",
    "end": "2087760"
  },
  {
    "text": "granular that is to schedule to gpu cores and monitor many users",
    "start": "2087760",
    "end": "2095118"
  },
  {
    "text": "and how they're using these cores how about scheduling yeah so we are trying to go more granular by splitting the gpu",
    "start": "2095119",
    "end": "2102160"
  },
  {
    "text": "and you know schedule at the gpu um i mean i think what the way the",
    "start": "2102160",
    "end": "2108560"
  },
  {
    "text": "design is i think this is public that we're going to make it part of the upstream infra",
    "start": "2108560",
    "end": "2115280"
  },
  {
    "text": "architecture for gpu sharing um which is to split the gpu you know",
    "start": "2115280",
    "end": "2120480"
  },
  {
    "text": "and then the jobs will define how many of those splits it's using",
    "start": "2120480",
    "end": "2126000"
  },
  {
    "text": "um and then the scheduler will also be supporting that so you should be able to",
    "start": "2126000",
    "end": "2131040"
  },
  {
    "text": "do it at that level and and once it's that is in place then we'll be able to kind of monitor it as",
    "start": "2131040",
    "end": "2137040"
  },
  {
    "text": "well um so i hope that answers all the bits of your question",
    "start": "2137040",
    "end": "2143200"
  },
  {
    "text": "um thank you all for joining um i think we'll be wrapping it up now",
    "start": "2143200",
    "end": "2152560"
  }
]