[
  {
    "text": "okay hello everyone I guess we can start this talk my name is Mark Eva booths I'm",
    "start": "3290",
    "end": "10910"
  },
  {
    "text": "an engineer at Google and today I'm here with my colleague magic to give you a deep dive into auto-scaling in",
    "start": "10910",
    "end": "18349"
  },
  {
    "text": "kubernetes and this is the continuation of yesterday talks and yesterday we on",
    "start": "18349",
    "end": "27500"
  },
  {
    "text": "sticky auto-scaling introduction we discuss what outer scale inquiry is why",
    "start": "27500",
    "end": "32870"
  },
  {
    "text": "do you need outer scaling how can you auto scale your application more",
    "start": "32870",
    "end": "38149"
  },
  {
    "text": "specifically for application of the scaling we mentioned about two options you may use the first one was horizontal",
    "start": "38149",
    "end": "45679"
  },
  {
    "text": "auto scaling in horizontal auto scaling you change the number of application",
    "start": "45679",
    "end": "52159"
  },
  {
    "text": "replicas based on the current load if the load increase you add replicas and if the load is decreased some of early",
    "start": "52159",
    "end": "59389"
  },
  {
    "text": "replicas are removed of course you don't want to do it manually and for that",
    "start": "59389",
    "end": "66110"
  },
  {
    "text": "purpose in kubernetes we have horizontal pot autoscaler does does this job for",
    "start": "66110",
    "end": "71870"
  },
  {
    "text": "you horizontal pot autoscaler were was covered in details yesterday so we will",
    "start": "71870",
    "end": "77780"
  },
  {
    "text": "not talk more about it the other option to scale your application is to scale",
    "start": "77780",
    "end": "83960"
  },
  {
    "text": "your application vertically with vertical auto scaling you increase our decrease the size of instances based on",
    "start": "83960",
    "end": "92540"
  },
  {
    "text": "the current and past load for quite a long time there was no automatic support",
    "start": "92540",
    "end": "98060"
  },
  {
    "text": "for this type of activity in kubernetes and so the user had to manually specify",
    "start": "98060",
    "end": "103360"
  },
  {
    "text": "the pot sizes and come with these good pod sizes in the first place luckily",
    "start": "103360",
    "end": "110210"
  },
  {
    "text": "this spring we launched vertical pot autoscaler which is hope to set to make size",
    "start": "110210",
    "end": "117260"
  },
  {
    "text": "setting automatically for you vertical pot autoscaler is not about a reaction",
    "start": "117260",
    "end": "123740"
  },
  {
    "text": "for today the changes in in traffic one of the reasons why we started this project was that many users had no clue",
    "start": "123740",
    "end": "131329"
  },
  {
    "text": "how to set pot size right or didn't set it at all or set it partial",
    "start": "131329",
    "end": "137030"
  },
  {
    "text": "or set it too big too small and that was a huge problem for them so one of the",
    "start": "137030",
    "end": "143000"
  },
  {
    "text": "vp8 goals is to help in this case as well to explain how VPI exactly does the",
    "start": "143000",
    "end": "150890"
  },
  {
    "text": "job let's start with the API the API is currently available as a customer source",
    "start": "150890",
    "end": "156290"
  },
  {
    "text": "definition and will probably stay as such for foreseeable future this summer",
    "start": "156290",
    "end": "162170"
  },
  {
    "text": "we had a quite extensive discussion with SiC architecture about whether to include vertical path autoscaler in the",
    "start": "162170",
    "end": "169220"
  },
  {
    "text": "core kubernetes along with HP api this decision was made not that due to more",
    "start": "169220",
    "end": "176209"
  },
  {
    "text": "complex nature of vertical path autoscaler it should remain to be developed as a kind of separate project",
    "start": "176209",
    "end": "183470"
  },
  {
    "text": "and for that reason the API will remain a CRD",
    "start": "183470",
    "end": "188350"
  },
  {
    "text": "at the vertebral Evan half VPA looks more or less like every other",
    "start": "189200",
    "end": "196140"
  },
  {
    "text": "kubernetes object it has a name in object matter it has a namespace",
    "start": "196140",
    "end": "203010"
  },
  {
    "text": "information you can add labels or annotation to it exactly as sports or",
    "start": "203010",
    "end": "208790"
  },
  {
    "text": "deployments or replica set and other objects in kubernetes but more importantly in a similar fashion as",
    "start": "208790",
    "end": "215190"
  },
  {
    "text": "other object it contains pack and Status I will cover it right now so let's start",
    "start": "215190",
    "end": "222150"
  },
  {
    "text": "with specification in specification you have three fields the first one is selector which allows",
    "start": "222150",
    "end": "228780"
  },
  {
    "text": "you to tell which part you would like to be to have all two scaled containers",
    "start": "228780",
    "end": "235470"
  },
  {
    "text": "with the same name in the pods selected with this selector will because the dirt",
    "start": "235470",
    "end": "240840"
  },
  {
    "text": "has equivalent and will receive the same recommendations the next item is the update policy it",
    "start": "240840",
    "end": "248310"
  },
  {
    "text": "will tell what exactly you would like VP to do the first option for update policy",
    "start": "248310",
    "end": "254940"
  },
  {
    "text": "is to set it to off in that case a VP I will only provide you with the information what the pot size should be",
    "start": "254940",
    "end": "261450"
  },
  {
    "text": "however it will execute no action the other option is to set it update mode to",
    "start": "261450",
    "end": "268050"
  },
  {
    "text": "initial in that case the recommendation will be applied only to newly created",
    "start": "268050",
    "end": "273360"
  },
  {
    "text": "pots and the existing pots will be left alone if you want to go full speed with",
    "start": "273360",
    "end": "279270"
  },
  {
    "text": "vertical pot of the scalar you can set up the motor auto in that case the recommendation S will be applied both to",
    "start": "279270",
    "end": "286470"
  },
  {
    "text": "the new pots as well as to the existing pots inside of your cluster",
    "start": "286470",
    "end": "293360"
  },
  {
    "text": "the first element of spec allows you to specify which containers should be",
    "start": "294860",
    "end": "301500"
  },
  {
    "text": "actually affected by VP a so there is a container name here and you can also",
    "start": "301500",
    "end": "308789"
  },
  {
    "text": "specify what should be the minimum and maximum amount of resources that you",
    "start": "308789",
    "end": "314430"
  },
  {
    "text": "want to give to individual containers and of course you can specify whether you would like s container to be auto",
    "start": "314430",
    "end": "322110"
  },
  {
    "text": "scale or not why is it important because you don't have to specify it for every",
    "start": "322110",
    "end": "327569"
  },
  {
    "text": "single container in your application you may use a wildcard pattern asterisk here",
    "start": "327569",
    "end": "333449"
  },
  {
    "text": "and in that case you will be applying vertical path autoscaler to all",
    "start": "333449",
    "end": "340050"
  },
  {
    "text": "containers inside of your pots if you are applying here or vpa to all containers in your pots you may want to",
    "start": "340050",
    "end": "346500"
  },
  {
    "text": "exclude some of these containers from VPA and for that reason we have this field here",
    "start": "346500",
    "end": "353400"
  },
  {
    "text": "which can either be set to out or off and enough PA will not provide",
    "start": "353400",
    "end": "360630"
  },
  {
    "text": "recommendation or touch the containers I mentioned here so we know what we can",
    "start": "360630",
    "end": "366510"
  },
  {
    "text": "ask you a vertical photo scaler for now it's the time to show what vpa can give",
    "start": "366510",
    "end": "371910"
  },
  {
    "text": "us in return in the status structure of the object so in the status we have two",
    "start": "371910",
    "end": "379860"
  },
  {
    "text": "fields the first one conditions give us information about whether the VP is",
    "start": "379860",
    "end": "385740"
  },
  {
    "text": "working at all has it provide its recommendation or maybe it has not enough samples to provide anything",
    "start": "385740",
    "end": "392250"
  },
  {
    "text": "meaningful to you the other field is the recommendation",
    "start": "392250",
    "end": "399009"
  },
  {
    "text": "itself it starts with the container name which tells you to which container this",
    "start": "399009",
    "end": "404800"
  },
  {
    "text": "recommendation can be applied then we have four sets of number the first one",
    "start": "404800",
    "end": "410860"
  },
  {
    "text": "is relatively easy it gives you information what should be the container",
    "start": "410860",
    "end": "417009"
  },
  {
    "text": "size the three others are slightly more complex so lower bound and upper bound",
    "start": "417009",
    "end": "423370"
  },
  {
    "text": "is the minimum and maximum amount of resource maximum minimum and maximum",
    "start": "423370",
    "end": "429490"
  },
  {
    "text": "size of the pot that we pay is still considered okay so if you are running VP",
    "start": "429490",
    "end": "434590"
  },
  {
    "text": "in the fully automatic mode and the pot is within this lower and upper bound it",
    "start": "434590",
    "end": "440470"
  },
  {
    "text": "will not be updated if it's outside of this boundaries it means that the size of the pot is wrong and needs to be",
    "start": "440470",
    "end": "446680"
  },
  {
    "text": "updated the last field tells you what",
    "start": "446680",
    "end": "452820"
  },
  {
    "text": "recommended what should be the pot size in the ideal case if you didn't provide",
    "start": "452820",
    "end": "458020"
  },
  {
    "text": "any constraints and there were no other obstacles how much resources would VPI allocate to this particular pot for",
    "start": "458020",
    "end": "465159"
  },
  {
    "text": "example VP I may think that the pot",
    "start": "465159",
    "end": "471639"
  },
  {
    "text": "should get in the container in the pot should get five gigabytes of memory however in spec you put three gigabytes",
    "start": "471639",
    "end": "478810"
  },
  {
    "text": "of memory as the upper limit so in target you will get three gigabytes of memory however to give you information",
    "start": "478810",
    "end": "486639"
  },
  {
    "text": "that the pot size should be bigger this 5 gigabyte is passed here in uncapped target and",
    "start": "486639",
    "end": "494770"
  },
  {
    "text": "obviously when VP will be applying a recommendation 3 gigabytes will be used",
    "start": "494770",
    "end": "500430"
  },
  {
    "text": "ok now hopefully everyone kind of understands what the API is all about",
    "start": "500430",
    "end": "506949"
  },
  {
    "text": "let's see how it works in practice probably the most tricky part of this",
    "start": "506949",
    "end": "512740"
  },
  {
    "text": "design is how we update part request as you may know for example deployment",
    "start": "512740",
    "end": "518380"
  },
  {
    "text": "contain history of the updates made to them so if you update deployment you change the image or pod size it creates",
    "start": "518380",
    "end": "526150"
  },
  {
    "text": "a new history in the history which is basically new replica set until a VPI gets a good",
    "start": "526150",
    "end": "534350"
  },
  {
    "text": "grasp on the right size of the autant containers in it it may update the pods",
    "start": "534350",
    "end": "540050"
  },
  {
    "text": "multiple times we don't consider this changes important enough to go to the deployment history which for example may",
    "start": "540050",
    "end": "548210"
  },
  {
    "text": "have limited capacity for example it may be limited to last 9 entries and if we",
    "start": "548210",
    "end": "553720"
  },
  {
    "text": "updated deployment with every beep a change will simply be polluting it with",
    "start": "553720",
    "end": "560440"
  },
  {
    "text": "with the unimportant changes and important changes would be lost",
    "start": "560440",
    "end": "566260"
  },
  {
    "text": "so we also don't want to limit vp8 on a specific set of controllers like only",
    "start": "566470",
    "end": "573800"
  },
  {
    "text": "two deployments or only two State foot set so we decided to go to slightly different paths then horizontal path",
    "start": "573800",
    "end": "579650"
  },
  {
    "text": "autoscaler and not to update any controller objects at all instead we use",
    "start": "579650",
    "end": "585710"
  },
  {
    "text": "admission plugins and the pod size is basically override during the admission",
    "start": "585710",
    "end": "593050"
  },
  {
    "text": "pie-faced when the pot is created it goes through multiple plugins which",
    "start": "593050",
    "end": "598100"
  },
  {
    "text": "checks whether you have enough quota to create the part whether you have enough privileges to create the part or",
    "start": "598100",
    "end": "606040"
  },
  {
    "text": "something else in this change you can also plug in your own plugin in our case",
    "start": "606040",
    "end": "613880"
  },
  {
    "text": "we are plugging our VP admission plug-in and this plugin checks all pods that are",
    "start": "613880",
    "end": "619340"
  },
  {
    "text": "created in the system whenever there is a matching VP object to the pot the pot is updated and it gets the recommended",
    "start": "619340",
    "end": "626960"
  },
  {
    "text": "size okay now a little bit about the recommendations which are obviously very",
    "start": "626960",
    "end": "632669"
  },
  {
    "text": "important in this picture the recommendations are calculated based on the observed with real-world usage for",
    "start": "632669",
    "end": "640289"
  },
  {
    "text": "that VP a recommender component periodically checks the usage of the",
    "start": "640289",
    "end": "645569"
  },
  {
    "text": "pots that are under VP a because the data from the metric server recalculate",
    "start": "645569",
    "end": "652529"
  },
  {
    "text": "the recommendations and put these recommendations back into VP a status",
    "start": "652529",
    "end": "657779"
  },
  {
    "text": "object which I showed you a couple minutes ago if you allow vp8 to work in",
    "start": "657779",
    "end": "664139"
  },
  {
    "text": "a fully automatic mode it will try to update the pots currently the only way to update the pot that is absolutely",
    "start": "664139",
    "end": "670949"
  },
  {
    "text": "running is unfortunately to kill it and recreate it so VP a constantly analyzes",
    "start": "670949",
    "end": "677339"
  },
  {
    "text": "both set of VPS and the list of pots if some of the pots that are under VP a",
    "start": "677339",
    "end": "682529"
  },
  {
    "text": "have requests outside of the allowed a lower and upper range VP updater kills",
    "start": "682529",
    "end": "688349"
  },
  {
    "text": "the spots and allows the pot controller to recreate the pot and once the pot is",
    "start": "688349",
    "end": "694409"
  },
  {
    "text": "recreated it goes through the admission plug-in chain and gets the appropriate",
    "start": "694409",
    "end": "699419"
  },
  {
    "text": "size through the VPI admission plugin",
    "start": "699419",
    "end": "704839"
  },
  {
    "text": "hopefully one day VPA will be more gentle to the pods and if",
    "start": "705800",
    "end": "710940"
  },
  {
    "text": "there is enough capacity on the machine that is the pot is actually running it may execute the in-place upgrades right",
    "start": "710940",
    "end": "719519"
  },
  {
    "text": "now there is not an option however there are a couple of designs how to do it and so hopefully in the near future we'll",
    "start": "719519",
    "end": "725880"
  },
  {
    "text": "have it okay so now a little bit about status and we are graduating via",
    "start": "725880",
    "end": "733380"
  },
  {
    "text": "vertical path autoscaler to beta right now what does it mean for you first of all it means that VP a actually works",
    "start": "733380",
    "end": "740040"
  },
  {
    "text": "and does its job while in alpha it has been tested by a bunch of clients and their feedback has been applied to the",
    "start": "740040",
    "end": "745889"
  },
  {
    "text": "product being beta also means that the API is relatively stable we may change",
    "start": "745889",
    "end": "751800"
  },
  {
    "text": "something in the future however the changes will not be drastic and they will be preceded by with the appropriate",
    "start": "751800",
    "end": "757769"
  },
  {
    "text": "and application period it also means that you can slowly start using VP in",
    "start": "757769",
    "end": "763199"
  },
  {
    "text": "your production environment if you want to do it you should start",
    "start": "763199",
    "end": "769270"
  },
  {
    "text": "using VPN product by running it in recommendation on remote keep it in such",
    "start": "769270",
    "end": "774490"
  },
  {
    "text": "state for couple days and check whether this record its recommendations work for you it is important that during this run",
    "start": "774490",
    "end": "783130"
  },
  {
    "text": "your pods get legitimate real-world traffic in the appropriate quantity otherwise the recommendations of VP will",
    "start": "783130",
    "end": "791080"
  },
  {
    "text": "not be valid you should also not blind fully copy vpa or recommendations from",
    "start": "791080",
    "end": "797710"
  },
  {
    "text": "your test or staging environment because the amount of traffic in this environment may not match the real world",
    "start": "797710",
    "end": "804950"
  },
  {
    "text": "so this recommendation may simply be too small if you are not running on gke and",
    "start": "804950",
    "end": "811880"
  },
  {
    "text": "just set VP mode to outer we strongly advise you to put size limits on your",
    "start": "811880",
    "end": "817250"
  },
  {
    "text": "containers so that they don't go beyond node capacity in your cluster and in",
    "start": "817250",
    "end": "823310"
  },
  {
    "text": "such a way become unschedulable and obviously we recommend using cluster autoscaler will be PA because it may",
    "start": "823310",
    "end": "830089"
  },
  {
    "text": "give you capacity when the pods grow if you are on Decatur is be tighter",
    "start": "830089",
    "end": "837110"
  },
  {
    "text": "integration of VP and cluster autoscaler and scheduler and you may execute slightly less caution that's all for",
    "start": "837110",
    "end": "844940"
  },
  {
    "text": "heavy PA you can find more information about how to install it and use it on",
    "start": "844940",
    "end": "850010"
  },
  {
    "text": "our github page in or github repository",
    "start": "850010",
    "end": "855019"
  },
  {
    "text": "and if you try it please let us know what you think about it and now it's the time for the second part of the",
    "start": "855019",
    "end": "861110"
  },
  {
    "text": "presentation the one about cluster autoscaler yes so let's talk about",
    "start": "861110",
    "end": "868850"
  },
  {
    "text": "cluster to Sky data so very quickly we talked about it a bit yesterday but it's",
    "start": "868850",
    "end": "875089"
  },
  {
    "text": "the component that makes sure that you always have the right number of nodes in your cluster so basically if there is",
    "start": "875089",
    "end": "881660"
  },
  {
    "text": "not enough space to run some of your thoughts cluster autoscaler will add modern nodes if there are some nodes",
    "start": "881660",
    "end": "888110"
  },
  {
    "text": "that are underutilized and can be safely removed cluster to scale will remove",
    "start": "888110",
    "end": "893269"
  },
  {
    "text": "those to save you some money so cluster autoscaler has been in GA for over a",
    "start": "893269",
    "end": "898610"
  },
  {
    "text": "year now I think since kubernetes 1.8 and currently it supports for cloud",
    "start": "898610",
    "end": "905329"
  },
  {
    "text": "providers Google cloud platform AWS agile and Alibaba cloud and hopefully",
    "start": "905329",
    "end": "912380"
  },
  {
    "text": "there are more coming so yeah let's let's see how it works and",
    "start": "912380",
    "end": "917540"
  },
  {
    "text": "to talk about crystal to scalar we always have to talk is to start talking about the scheduler so the scheduler is",
    "start": "917540",
    "end": "926750"
  },
  {
    "text": "the kubernetes component that decides which port should run on which node and",
    "start": "926750",
    "end": "932920"
  },
  {
    "text": "for the class notice Kara it's quite important how scheduler does it",
    "start": "932920",
    "end": "938079"
  },
  {
    "text": "so without going too deeply but a bit deeply scheduler has a process of",
    "start": "938079",
    "end": "944709"
  },
  {
    "text": "deciding which note is the best fit for there for each pot so imagine we have a",
    "start": "944709",
    "end": "951189"
  },
  {
    "text": "new pot that has just been created and it needs to be scheduled so basically",
    "start": "951189",
    "end": "956410"
  },
  {
    "text": "the two steps in the schedule the first one is the predicate functions and those",
    "start": "956410",
    "end": "961569"
  },
  {
    "text": "are sort of how the requirements of the pot basically things that must be to my",
    "start": "961569",
    "end": "967779"
  },
  {
    "text": "folder pot to ever be able to run on a given note so some example predicates",
    "start": "967779",
    "end": "973089"
  },
  {
    "text": "would be added enough resources left on the note to satisfy the pot resource",
    "start": "973089",
    "end": "978879"
  },
  {
    "text": "requests does the note match pot note selector of note affinity I'll all the",
    "start": "978879",
    "end": "984850"
  },
  {
    "text": "port affinity and anti fi ET rules observed and so on and so forth this I",
    "start": "984850",
    "end": "990339"
  },
  {
    "text": "think between 10 and 20 of those and basically every single predicate must pass from the pot to be able to schedule",
    "start": "990339",
    "end": "997540"
  },
  {
    "text": "on a given node so once we filter out the pots that cannot be scheduled at the",
    "start": "997540",
    "end": "1003540"
  },
  {
    "text": "nodes that cannot be used for a given pot and there is the second phase and it's about priority function so those",
    "start": "1003540",
    "end": "1010920"
  },
  {
    "text": "are the functions that basically rate each node to say how desirable it is to",
    "start": "1010920",
    "end": "1016769"
  },
  {
    "text": "run a specific pot on this node and then they are used to select exactly which",
    "start": "1016769",
    "end": "1022470"
  },
  {
    "text": "node will be finally used but I won't go into more details of priorities because",
    "start": "1022470",
    "end": "1027839"
  },
  {
    "text": "we actually don't care about those at all in auto-scaling the important part is the predicate so",
    "start": "1027839",
    "end": "1035699"
  },
  {
    "text": "basically the way that cast out two scalawags is it guarantees that for every pot there will be at least one",
    "start": "1035699",
    "end": "1042058"
  },
  {
    "text": "node that passes all predicates so there will be a note that makes it possible to",
    "start": "1042059",
    "end": "1047819"
  },
  {
    "text": "schedule this pot so this one concept that's going to be very important in our thinking about craft autoscaler",
    "start": "1047819",
    "end": "1054510"
  },
  {
    "text": "and then there is another concept an old group so this is specifically crafted",
    "start": "1054510",
    "end": "1060450"
  },
  {
    "text": "out of schedule concept and the node group is a set of identical notes so maybe all your two CPU nodes in the",
    "start": "1060450",
    "end": "1067889"
  },
  {
    "text": "cluster will be one node group and then you'll follow CPU notes be another notable and the way crashed",
    "start": "1067889",
    "end": "1074190"
  },
  {
    "text": "out of scale actually works is it to the sizes don't know those note boobs so if",
    "start": "1074190",
    "end": "1079259"
  },
  {
    "text": "it wants to add two more to CPU notes will cluster it will just resize the",
    "start": "1079259",
    "end": "1085769"
  },
  {
    "text": "corresponding node group and increase its size by two so with all those",
    "start": "1085769",
    "end": "1093769"
  },
  {
    "text": "concepts let's go into CA propel so very",
    "start": "1093769",
    "end": "1099629"
  },
  {
    "text": "quickly what does cast out the scaler actually do so it monitors ilk raster",
    "start": "1099629",
    "end": "1104909"
  },
  {
    "text": "and it looks for pending pots so the pots that cannot be scheduled because",
    "start": "1104909",
    "end": "1110309"
  },
  {
    "text": "there is not a single node that passes all predicates once it finds such nodes",
    "start": "1110309",
    "end": "1115860"
  },
  {
    "text": "it's actually simulates kubernetes scheduler to see if adding some new",
    "start": "1115860",
    "end": "1121500"
  },
  {
    "text": "nodes would solve the issue if it would it uses the node groups it resizes the",
    "start": "1121500",
    "end": "1127409"
  },
  {
    "text": "node group based on the result of this simulation and a lot of misconceptions",
    "start": "1127409",
    "end": "1134789"
  },
  {
    "text": "about cross delta scalar a lot of things that people expect it to do which actually doesn't do so the node group is",
    "start": "1134789",
    "end": "1141899"
  },
  {
    "text": "basically implemented on the cloud provider level and the resizing it just creates the VM cross Delta scalar",
    "start": "1141899",
    "end": "1148289"
  },
  {
    "text": "doesn't register this VM in kubernetes it doesn't configure it it doesn't do anything with it it actually like the",
    "start": "1148289",
    "end": "1154919"
  },
  {
    "text": "only thing it does is ask cloud provider for the new VM and it just assumes that",
    "start": "1154919",
    "end": "1160110"
  },
  {
    "text": "that will somehow results in a node being created another common",
    "start": "1160110",
    "end": "1165629"
  },
  {
    "text": "misconception is what it looks at because it's based on scheduling it doesn't actually care about resource",
    "start": "1165629",
    "end": "1173190"
  },
  {
    "text": "usage if you set your pot resource request very low and it's booming or it's burning the node just using way",
    "start": "1173190",
    "end": "1180240"
  },
  {
    "text": "more cpu than it requested crust out or scaleable will not react because it's still schedulable it doesn't impact",
    "start": "1180240",
    "end": "1186690"
  },
  {
    "text": "scheduling so for that sort of thing you need pot outer scales",
    "start": "1186690",
    "end": "1192070"
  },
  {
    "text": "and because of how tightly we'll integrate it with scheduler we don't support any custom scheduling so if use",
    "start": "1192070",
    "end": "1199300"
  },
  {
    "text": "that then unfortunately you cannot discuss tile to scale",
    "start": "1199300",
    "end": "1204000"
  },
  {
    "text": "and okay so now let's proceed and sort of look how it all works now that we",
    "start": "1204379",
    "end": "1210110"
  },
  {
    "text": "know what it does and the basically cross delta scalar is made of three components there is the crowd provided",
    "start": "1210110",
    "end": "1216919"
  },
  {
    "text": "module so that basically needs to be implemented separately for each cloud provider and we currently have four",
    "start": "1216919",
    "end": "1223039"
  },
  {
    "text": "implementations as I said and that basically is the implementation of the node group concept so this is the",
    "start": "1223039",
    "end": "1230779"
  },
  {
    "text": "interface between class Delta Skylar and cloud provider so we have some common",
    "start": "1230779",
    "end": "1237070"
  },
  {
    "text": "abstraction that we can use the other part is the imported scheduler code so",
    "start": "1237070",
    "end": "1242359"
  },
  {
    "text": "the way we simulate scheduler is we literally just take and import the predicate code from schedule the good",
    "start": "1242359",
    "end": "1249679"
  },
  {
    "text": "thing about it is any new feature that is ever added to schedule is automatically supported by class delta",
    "start": "1249679",
    "end": "1255440"
  },
  {
    "text": "scalar because we will just import it the best thing is is actually important",
    "start": "1255440",
    "end": "1260840"
  },
  {
    "text": "to learn the say like the version of the cross delta scalar that is the same as the version of the scheduler that is",
    "start": "1260840",
    "end": "1266989"
  },
  {
    "text": "running in your cluster so what we do is we release new class delta scalar for",
    "start": "1266989",
    "end": "1272690"
  },
  {
    "text": "each new kubernetes release so if you're using kubernetes 1.12 you should be using cross Delta scale at one point for",
    "start": "1272690",
    "end": "1278720"
  },
  {
    "text": "12 with 1.15 we will release class Delta scalar one pinpoint 15 and so on and so",
    "start": "1278720",
    "end": "1285499"
  },
  {
    "text": "forth okay so I've been talking a bit about",
    "start": "1285499",
    "end": "1291580"
  },
  {
    "text": "simulations here and there and I wanted to quickly show you how this algorithm",
    "start": "1291580",
    "end": "1296900"
  },
  {
    "text": "works so imagine you have a bunch of painting pots that crossed out to scale",
    "start": "1296900",
    "end": "1303169"
  },
  {
    "text": "already notice there is one large pot and two smallish pots and we need to decide how many nodes we want to add to",
    "start": "1303169",
    "end": "1309830"
  },
  {
    "text": "help those pots so basically what cast out the scaler does is sort of greedy bin packing algorithm we just see what",
    "start": "1309830",
    "end": "1317720"
  },
  {
    "text": "would happen if you added one node and we take the first pot and we want the scheduler pedicles that we took from the",
    "start": "1317720",
    "end": "1324320"
  },
  {
    "text": "scheduler and we see what will happen so in this case the Potter fits on the node so we're all good but there's still two",
    "start": "1324320",
    "end": "1331010"
  },
  {
    "text": "more pending pots so we try to schedule them as well and now the problem happens",
    "start": "1331010",
    "end": "1336169"
  },
  {
    "text": "because this pot won't fit anymore because the first pot basically took the whole note so okay one note is clearly",
    "start": "1336169",
    "end": "1342980"
  },
  {
    "text": "not enough let's add one more and let's see what happens then ah the pot can",
    "start": "1342980",
    "end": "1349309"
  },
  {
    "text": "schedule okay so already helped two of our three pots by adding two notes we have won multiple remaining we try it",
    "start": "1349309",
    "end": "1357470"
  },
  {
    "text": "with the face node doesn't work second one it's schedules okay two notes are",
    "start": "1357470",
    "end": "1364460"
  },
  {
    "text": "enough we can help all our pots by adding two notes let's do that and",
    "start": "1364460",
    "end": "1370120"
  },
  {
    "text": "that's basically how crust outer scale works",
    "start": "1370120",
    "end": "1375270"
  },
  {
    "text": "I want to go into as much details on scale it down because it's slightly more",
    "start": "1375270",
    "end": "1381540"
  },
  {
    "text": "complex but the basic idea is we also monitor every single node in the cluster",
    "start": "1381540",
    "end": "1387660"
  },
  {
    "text": "and to decide if we actually need it and there is a bunch of criteria we use so",
    "start": "1387660",
    "end": "1393500"
  },
  {
    "text": "first of all note is unneeded if the utilization is below 50% well it may be",
    "start": "1393500",
    "end": "1400320"
  },
  {
    "text": "an idiot if the utilization is below 50% we may change this threshold in future so don't get to use to the exact number",
    "start": "1400320",
    "end": "1407610"
  },
  {
    "text": "but another important criteria is that actually all the ports that are running",
    "start": "1407610",
    "end": "1412770"
  },
  {
    "text": "on the node can be moved to somewhere else in the cluster we once again use scheduler code and simulate scheduling",
    "start": "1412770",
    "end": "1419580"
  },
  {
    "text": "to see if we remove this nodes with all the pods be schedulable somewhere else",
    "start": "1419580",
    "end": "1424860"
  },
  {
    "text": "and finally there are some kinds of ports that we are basically too afraid",
    "start": "1424860",
    "end": "1430530"
  },
  {
    "text": "to touch and will never remove a node that has one of those ports so we don't touch the stem ports because restarting",
    "start": "1430530",
    "end": "1437040"
  },
  {
    "text": "those can cause some sort of system outage we don't touch ports with storage",
    "start": "1437040",
    "end": "1442140"
  },
  {
    "text": "because we local storage because we're afraid that we may destroy some data and",
    "start": "1442140",
    "end": "1447840"
  },
  {
    "text": "we also have some specific mechanism that allow you to say that port should",
    "start": "1447840",
    "end": "1452940"
  },
  {
    "text": "never ever be restarted by class Delta scalar because of whatever reason you have to specify it",
    "start": "1452940",
    "end": "1460380"
  },
  {
    "text": "and we fall of that it's actually quite",
    "start": "1460380",
    "end": "1465570"
  },
  {
    "text": "a complex Fink so I have a few I have a section about how to see what's going on",
    "start": "1465570",
    "end": "1471240"
  },
  {
    "text": "with class Delta scalar and how to debug it and the first thing is we have this",
    "start": "1471240",
    "end": "1476550"
  },
  {
    "text": "class Delta scalars stateís config map and it's basically automatically written",
    "start": "1476550",
    "end": "1482460"
  },
  {
    "text": "and refreshed by class Delta scalar and it gives you the current status of your cluster as perceived by class Delta",
    "start": "1482460",
    "end": "1489150"
  },
  {
    "text": "scalar so it describes how your cluster looks like how every node group looks like you can actually find what's the",
    "start": "1489150",
    "end": "1496230"
  },
  {
    "text": "actual size of the node group as in how many nodes that are in kubernetes versus how many VMs have been requested for",
    "start": "1496230",
    "end": "1502980"
  },
  {
    "text": "crowd provider so for example you can see that maybe some of the VMS haven't",
    "start": "1502980",
    "end": "1508590"
  },
  {
    "text": "managed to start up successfully and so on and it also tells about crustal to",
    "start": "1508590",
    "end": "1513900"
  },
  {
    "text": "scaling operations so for example in this case you can see there is scale up in progress there are currently two notes but",
    "start": "1513900",
    "end": "1519510"
  },
  {
    "text": "crascell to scale it requested for more nodes so that's one thing that's quite",
    "start": "1519510",
    "end": "1524580"
  },
  {
    "text": "useful to see what's going on with class Delta scalar another things are cubed",
    "start": "1524580",
    "end": "1530880"
  },
  {
    "text": "netis events we write a lot of those so basically whenever crossed out the schedule does scale up a scaled down",
    "start": "1530880",
    "end": "1536820"
  },
  {
    "text": "basically any operation there will be an event describing what just happened so",
    "start": "1536820",
    "end": "1544020"
  },
  {
    "text": "you can use those to basically trace the history of class delta scheduled operations another useful event is",
    "start": "1544020",
    "end": "1550560"
  },
  {
    "text": "basically we write events on ports so if a port was a reason for scale-up there",
    "start": "1550560",
    "end": "1555990"
  },
  {
    "text": "will be an event saying that if we restarted a pod because we did a scale down there will be an event and finally",
    "start": "1555990",
    "end": "1562740"
  },
  {
    "text": "there is an event that will be on the pot if we see that the pod is pending",
    "start": "1562740",
    "end": "1567810"
  },
  {
    "text": "but we cannot actually find any skylab that would help that pot that also will result in an event so you can look for",
    "start": "1567810",
    "end": "1574680"
  },
  {
    "text": "those if you think it's not working correctly and more info is in our",
    "start": "1574680",
    "end": "1580080"
  },
  {
    "text": "repository there is all the code that is Vietnamese separate for each cloud provider that says exactly how to run",
    "start": "1580080",
    "end": "1587160"
  },
  {
    "text": "across the autoscaler on each cloud provider and we have a very extensive FAQ that sort of goes into",
    "start": "1587160",
    "end": "1593940"
  },
  {
    "text": "why is crust autoscaler not removing some notes that I think it should remove why is it not adding some notes and so",
    "start": "1593940",
    "end": "1601740"
  },
  {
    "text": "on so some sort of common troubleshooting problems are covered in our FAQ which is also in our lab",
    "start": "1601740",
    "end": "1609840"
  },
  {
    "text": "and I think that's about it about the class delta scale itself we one small",
    "start": "1609840",
    "end": "1617070"
  },
  {
    "text": "invite you to join our Sigma Tings which are every Monday at 4 p.m. Central",
    "start": "1617070",
    "end": "1622170"
  },
  {
    "text": "European Time we're very happy to discuss basically any feature request",
    "start": "1622170",
    "end": "1627270"
  },
  {
    "text": "problems or anything else we encountered while using any auto scaling features in kubernetes if the hour is not very good",
    "start": "1627270",
    "end": "1634710"
  },
  {
    "text": "for you or you just have some quick question you don't want to wait until Monday we quite active on our slack",
    "start": "1634710",
    "end": "1641460"
  },
  {
    "text": "channel on kubernetes slack so feel free to ask us any question there and finally",
    "start": "1641460",
    "end": "1647520"
  },
  {
    "text": "thank you if you have any questions we'll try to ask to answer them",
    "start": "1647520",
    "end": "1652640"
  },
  {
    "text": "you thank you for a talk to questions",
    "start": "1654710",
    "end": "1661670"
  },
  {
    "text": "Frances for the first stop is there any conflict if we use horizontal autoscaler",
    "start": "1661670",
    "end": "1668420"
  },
  {
    "text": "in vertical scale at the same time yes there is conflict basically they are",
    "start": "1668420",
    "end": "1674660"
  },
  {
    "text": "trying to do exactly the same thing so if you are right now using HP and VP at",
    "start": "1674660",
    "end": "1680240"
  },
  {
    "text": "the same moment they will be trying to both for example increase the pot and increase the pot count or decrease the",
    "start": "1680240",
    "end": "1686990"
  },
  {
    "text": "pot count and decrease the pot size so at this very moment we don't recommend you using HPA based on CPU utilization",
    "start": "1686990",
    "end": "1695750"
  },
  {
    "text": "and vertical pot autoscaler at the same time however if you point your",
    "start": "1695750",
    "end": "1701560"
  },
  {
    "text": "horizontal pot autoscaler at custom metric for example at the number of QP",
    "start": "1701560",
    "end": "1707660"
  },
  {
    "text": "aces then you can use it because with HP you are guaranteed that for example you",
    "start": "1707660",
    "end": "1713060"
  },
  {
    "text": "are passing only 100 QP s is to every instances and then vertical pot autoscaler and it tries to adjust the",
    "start": "1713060",
    "end": "1719720"
  },
  {
    "text": "amount of CPU and memory that is needed to handle this traffic for 100k Pierce's",
    "start": "1719720",
    "end": "1729220"
  },
  {
    "text": "second question cysts as far as I know then the autoscaler will always go up",
    "start": "1729770",
    "end": "1736799"
  },
  {
    "text": "one not a that IRA because in my use",
    "start": "1736799",
    "end": "1743340"
  },
  {
    "text": "case then I have an issue when the horizontal autoscaler they kick in for",
    "start": "1743340",
    "end": "1750660"
  },
  {
    "text": "some services and they need to apply for a symbol like 100 mob hearts and then in",
    "start": "1750660",
    "end": "1757049"
  },
  {
    "text": "the case that even though cluster Auto scalars come up new not it will not be",
    "start": "1757049",
    "end": "1763080"
  },
  {
    "text": "scheduled all of the parts total new parts on a new not against Costa also",
    "start": "1763080",
    "end": "1770070"
  },
  {
    "text": "scalar just stuck at that site it it cannot scale up anymore so I mean I",
    "start": "1770070",
    "end": "1776520"
  },
  {
    "text": "would have to look at like the details of the configuration but in general it does the sort of being packing algorithm",
    "start": "1776520",
    "end": "1782549"
  },
  {
    "text": "I described to decide how many notes are needed and actually it adds all don't",
    "start": "1782549",
    "end": "1788309"
  },
  {
    "text": "all those notes in one go so it resizes the back mEagle a SGO whatever you have",
    "start": "1788309",
    "end": "1794520"
  },
  {
    "text": "behind the scenes by a given amount in one shot there are some issues if you're using",
    "start": "1794520",
    "end": "1800669"
  },
  {
    "text": "things like pod affinity that sometimes may cause it to add them one by one but",
    "start": "1800669",
    "end": "1807390"
  },
  {
    "text": "in general it should add them in like multi notes in one go sometimes the",
    "start": "1807390",
    "end": "1814470"
  },
  {
    "text": "second go is nudity if we underestimate the need for four for the notes however",
    "start": "1814470",
    "end": "1820260"
  },
  {
    "text": "in most cases on the one scale up is need a tent in one scaled up you will get as many notes as you already need",
    "start": "1820260",
    "end": "1828530"
  },
  {
    "text": "finally if you think it's not working for you just like us on slack or something we'll discuss that",
    "start": "1828530",
    "end": "1837169"
  },
  {
    "text": "I have a question for the autoscaler so",
    "start": "1839700",
    "end": "1844980"
  },
  {
    "text": "you mentioned that if I understand correctly you put the scheduler as part of the library for the autoscaler you",
    "start": "1844980",
    "end": "1851429"
  },
  {
    "text": "simulate the results from as the scheduler and the primary goal is to have reducers scheduling pressure have",
    "start": "1851429",
    "end": "1858539"
  },
  {
    "text": "you ever considered other design where you put these autoscaler functionality into the scatter itself or design a API",
    "start": "1858539",
    "end": "1866090"
  },
  {
    "text": "between consuming the updates from the scheduler trying to understand the",
    "start": "1866090",
    "end": "1871950"
  },
  {
    "text": "design reactionary behind that thank you okay I can hang loose so historically a",
    "start": "1871950",
    "end": "1877370"
  },
  {
    "text": "cluster autoscaler was started as a separate project because we didn't want",
    "start": "1877370",
    "end": "1882480"
  },
  {
    "text": "to pollute scheduler code so we want to keep kubernetes core elements as minimal",
    "start": "1882480",
    "end": "1888179"
  },
  {
    "text": "as possible and don't add functionality that may not be needed for all of the",
    "start": "1888179",
    "end": "1894450"
  },
  {
    "text": "users as you could see a cluster of the scalar requires some plugins to work so",
    "start": "1894450",
    "end": "1899940"
  },
  {
    "text": "every cloud provider that you want to use needs a special driver and all of",
    "start": "1899940",
    "end": "1905730"
  },
  {
    "text": "the libraries and probably you don't want to include all of these libraries in the default kubernetes and",
    "start": "1905730",
    "end": "1913700"
  },
  {
    "text": "distribution and to the default code of scheduler because well we have currently support for I know 30 different cloud",
    "start": "1913700",
    "end": "1920760"
  },
  {
    "text": "providers and all of them would want to be in the core kubernetes and for that",
    "start": "1920760",
    "end": "1926220"
  },
  {
    "text": "reasons we decided that we will have a cluster autoscaler running separately so that it schedule",
    "start": "1926220",
    "end": "1934409"
  },
  {
    "text": "remains relatively simple and cluster autoscaler can be developed at its own",
    "start": "1934409",
    "end": "1939870"
  },
  {
    "text": "pace in the idea world of course it would be better if you didn't have to integrate",
    "start": "1939870",
    "end": "1947270"
  },
  {
    "text": "if we could integrate cluster of the scanner with scheduler however due to multiple reasons I described right now",
    "start": "1947270",
    "end": "1954960"
  },
  {
    "text": "it's not the best idea and regarding the",
    "start": "1954960",
    "end": "1960390"
  },
  {
    "text": "API part why would because you could think that like one possible design bit for the scheduler to have some sort of a",
    "start": "1960390",
    "end": "1966779"
  },
  {
    "text": "delight and mode so the problem is with not actually running against the actual cluster because we're adding those nodes",
    "start": "1966779",
    "end": "1972929"
  },
  {
    "text": "in our simulation so the state of the cluster changes so we actually what we need is to answer if the pot would be",
    "start": "1972929",
    "end": "1980880"
  },
  {
    "text": "schedulable in some sort of fictional scenario well we have in memory node objects and then I mean first of all",
    "start": "1980880",
    "end": "1987990"
  },
  {
    "text": "that's not supported by scheduler that heavily relies on caching and other things that makes that difficult and",
    "start": "1987990",
    "end": "1993510"
  },
  {
    "text": "also secondly for we run scheduler platicas a lot more than scheduler does",
    "start": "1993510",
    "end": "1999539"
  },
  {
    "text": "without really a lot of those and then the cost of serializing the whole cluster state and sending it to",
    "start": "1999539",
    "end": "2005570"
  },
  {
    "text": "scheduler is just prohibitive we have been paying that we don't have any more",
    "start": "2005570",
    "end": "2011620"
  },
  {
    "text": "time for questions if you have questions feel free to ask them in in the lobby we",
    "start": "2011620",
    "end": "2019070"
  },
  {
    "text": "are there for you so don't hesitate to approach us thank you thank you",
    "start": "2019070",
    "end": "2025010"
  },
  {
    "text": "[Applause]",
    "start": "2025010",
    "end": "2027490"
  }
]