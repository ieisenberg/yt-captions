[
  {
    "text": "thank you everyone for joining today we'll be talking about KU which is essentially how you run Ray on kubernetes to do AI applications um do",
    "start": "80",
    "end": "8960"
  },
  {
    "text": "we have a clicker no um hi everyone I'm Winston I'm a product manager at Google and I'm joined",
    "start": "8960",
    "end": "14320"
  },
  {
    "text": "by I'm archit I'm an engineered any scale yeah so quick agenda we'll do",
    "start": "14320",
    "end": "22119"
  },
  {
    "text": "short little inro intro talk about Ray talk about kubernetes and then KUB which is essentially Ray and kubernetes um the",
    "start": "22119",
    "end": "28519"
  },
  {
    "text": "benefits you'll get by building your platform with ring kubernetes and no presentation found without a demo so",
    "start": "28519",
    "end": "35120"
  },
  {
    "text": "we'll definitely include one two maybe three of those uh firstly who here took an Uber",
    "start": "35120",
    "end": "41520"
  },
  {
    "text": "this weekend or week um or listened to Spotify bought something online yeah AI",
    "start": "41520",
    "end": "47760"
  },
  {
    "text": "is around us everywhere you are interacting with machine learning models every day and coincidentally or not",
    "start": "47760",
    "end": "55320"
  },
  {
    "text": "coincidentally those machine learning models were built using Ray",
    "start": "55320",
    "end": "60640"
  },
  {
    "text": "and we're just getting started um as we're all witnessing now these models are getting larger and larger and as",
    "start": "60640",
    "end": "66240"
  },
  {
    "text": "they get larger they are now demonstrating human level capabilities with that it's opening up completely new",
    "start": "66240",
    "end": "71799"
  },
  {
    "text": "opportunities to apply ml in everyday task as I mentioned before you're already surrounded by AIML and it's only",
    "start": "71799",
    "end": "78400"
  },
  {
    "text": "going to get it's going to grow more and more um of course as models get larger",
    "start": "78400",
    "end": "85520"
  },
  {
    "text": "they demand more data they are computationally hungry and it's no",
    "start": "85520",
    "end": "90680"
  },
  {
    "text": "longer just the largest of companies worrying about the scale every company now is thinking about how do I get these",
    "start": "90680",
    "end": "96439"
  },
  {
    "text": "high performance compute how do I distribute my workloads um and achieve such scale and of course as scale",
    "start": "96439",
    "end": "102600"
  },
  {
    "text": "becomes matter of fact then we're all worrying about costs because these are expensive computational resources we",
    "start": "102600",
    "end": "108640"
  },
  {
    "text": "can't let them run on um but you know beyond scale Beyond cost it's about future proofing um even businesses that",
    "start": "108640",
    "end": "116039"
  },
  {
    "text": "have been doing AI for years have invested tons of money in building the AI capabilities they are still",
    "start": "116039",
    "end": "122039"
  },
  {
    "text": "challenged and struggling to take advantage of these new large language models and generative AI I think maybe",
    "start": "122039",
    "end": "127799"
  },
  {
    "text": "your company has experienced that um even at Google you know we're doing many things to better bring these generative",
    "start": "127799",
    "end": "133560"
  },
  {
    "text": "AI capabilities to um other customers and users and so how do you build for",
    "start": "133560",
    "end": "139200"
  },
  {
    "text": "that right AI is not done changing we're only getting started now and so how do you prepare to uptake those challenges",
    "start": "139200",
    "end": "146239"
  },
  {
    "text": "and changing requirements with to capture these new values of these AI",
    "start": "146239",
    "end": "152280"
  },
  {
    "text": "Trends um and so what we're seeing is many customers adopting a platform and building a unified platform on top of",
    "start": "152280",
    "end": "159159"
  },
  {
    "text": "Ray and kubernetes um as many of you already know customers have made a strategic bet on kubernetes as their",
    "start": "159159",
    "end": "165800"
  },
  {
    "text": "platform of choice to build in that flexibility and future proofing for all the new application and developer needs",
    "start": "165800",
    "end": "172640"
  },
  {
    "text": "going forward we're seeing from customers making that same strategic bet for Ray in order to build that AI",
    "start": "172640",
    "end": "178760"
  },
  {
    "text": "capabilities AI is changing rapidly and the challenges continue to grow with Ray and kubernetes it'll enable you to take",
    "start": "178760",
    "end": "186280"
  },
  {
    "text": "advantage of that rapid pace of AI so should tell us more about Ray thanks",
    "start": "186280",
    "end": "192920"
  },
  {
    "text": "Winston um so quick show of hands how many people are familiar with Ry okay so some of you um so we'll cover",
    "start": "192920",
    "end": "201360"
  },
  {
    "text": "some of the basics uh Ray is an open source unified compute framework that makes it easy to scale Ai and python",
    "start": "201360",
    "end": "207239"
  },
  {
    "text": "workloads from reinforcement learning to Deep learning to tuning and model serving Ray provides the compute layer",
    "start": "207239",
    "end": "214040"
  },
  {
    "text": "for parallel processing so that you don't need to be a distributed systems expert Ray minimizes the complexity of",
    "start": "214040",
    "end": "219599"
  },
  {
    "text": "running your distributed individual and end to-end machine learning workflows with these components so first at the",
    "start": "219599",
    "end": "225920"
  },
  {
    "text": "top of this diagram we have scalable libraries for common machine learning tasks such as data pre-processing",
    "start": "225920",
    "end": "231959"
  },
  {
    "text": "distributed training hyperparameter tuning reinforcement learning and model serving these libraries sit on top of",
    "start": "231959",
    "end": "238439"
  },
  {
    "text": "pythonic distributed computer Primitives for paralyzing and scaling python applications and we call this Ray core",
    "start": "238439",
    "end": "245480"
  },
  {
    "text": "we'll see some example code in later slides and finally at the bottom here Ray provides Integrations and utilities",
    "start": "245480",
    "end": "252439"
  },
  {
    "text": "for deploying a ray cluster with existing tools and infrastructure such as kubernetes or directly on VMS on AWS",
    "start": "252439",
    "end": "258919"
  },
  {
    "text": "gcp and Azure Ray reduces friction between development and production by enabling",
    "start": "258919",
    "end": "264400"
  },
  {
    "text": "the same P python code to scale seamlessly from a laptop to a large cluster to enable this Ray handles key",
    "start": "264400",
    "end": "270600"
  },
  {
    "text": "processes under the hood such as orchestration scheduling fault tolerance and auto",
    "start": "270600",
    "end": "276400"
  },
  {
    "text": "scaling let's quickly dive into what each of the ray AI libraries does Ray data for scalable framework agnostic",
    "start": "276400",
    "end": "283360"
  },
  {
    "text": "data loading and transformation across training tuning and prediction Ray train is for distributed multi-node and",
    "start": "283360",
    "end": "289919"
  },
  {
    "text": "multi-core model training with fault tolerance that integrates with popular training libraries next Ray tune is for scalable",
    "start": "289919",
    "end": "296639"
  },
  {
    "text": "hyperparameter tuning to optimize model performance Ray serve is for scalable and",
    "start": "296639",
    "end": "302080"
  },
  {
    "text": "programmable serving to deploy models from on uh online inference with optional micro batching to improve",
    "start": "302080",
    "end": "307800"
  },
  {
    "text": "performance and Ray serve also provides streaming support for large language models which is used by the library Ray",
    "start": "307800",
    "end": "313880"
  },
  {
    "text": "llm and finally we have rib which is for scalable distributed reinforcement learning",
    "start": "313880",
    "end": "320319"
  },
  {
    "text": "workloads so when would you use Ray and the ray AI libraries one use case is you might wish",
    "start": "320319",
    "end": "326639"
  },
  {
    "text": "to scale only a single workload say data ingestion or hyper parameter optimization or",
    "start": "326639",
    "end": "332919"
  },
  {
    "text": "training or rather than a single workload you might want to build a complete endtoend ml application and use",
    "start": "332919",
    "end": "338520"
  },
  {
    "text": "all the libraries they work together cohesively and composedly or you might want to use",
    "start": "338520",
    "end": "345199"
  },
  {
    "text": "external ml libraries integrated in a unified manner or finally you might wish to",
    "start": "345199",
    "end": "350840"
  },
  {
    "text": "build your own ml platform a top these Ray AI libraries this has been done by industry leaders such as instacart door",
    "start": "350840",
    "end": "357520"
  },
  {
    "text": "Dash Pinterest and so on and you can see their blog posts about this or watch uh YouTube videos of some of their talks",
    "start": "357520",
    "end": "363240"
  },
  {
    "text": "from this year's Ray Summit to learn more so now we know how to use Ray or",
    "start": "363240",
    "end": "368400"
  },
  {
    "text": "when to use it so let's take a quick look at the ray core API that enables all of",
    "start": "368400",
    "end": "374680"
  },
  {
    "text": "this so here we have a couple simple python functions the Primitive that Ray exposes",
    "start": "374680",
    "end": "380800"
  },
  {
    "text": "is this ray. remote decorator which takes an arbitrary python function and designates it as a remote",
    "start": "380800",
    "end": "388039"
  },
  {
    "text": "function remote functions can then be called with function name. remote which",
    "start": "388039",
    "end": "393160"
  },
  {
    "text": "will return immediately with the future in the form of an object ID that represents the",
    "start": "393160",
    "end": "398400"
  },
  {
    "text": "result in the background Ray will schedule the function invocation somewhere in the cluster and execute",
    "start": "398400",
    "end": "406240"
  },
  {
    "text": "it in this way many different functions can run at the same",
    "start": "406280",
    "end": "412039"
  },
  {
    "text": "time you can also construct computation graphs using this API see here we have",
    "start": "412039",
    "end": "417400"
  },
  {
    "text": "two outputs being used as input for another function then if you actually want to",
    "start": "417400",
    "end": "423479"
  },
  {
    "text": "get the results you can call ray. getet which will block until the tasks have finished executing and will retrieve the",
    "start": "423479",
    "end": "430039"
  },
  {
    "text": "results this API enables the straightforward parallelization of a lot of python",
    "start": "430039",
    "end": "436479"
  },
  {
    "text": "applications so that was the first key feature remote functions which we call aray tasks now I'll introduce another",
    "start": "436720",
    "end": "443840"
  },
  {
    "text": "keyray feature actors which are essentially remote classes as an example here's a simple",
    "start": "443840",
    "end": "450840"
  },
  {
    "text": "python class with a method for incrementing a counter you can add the remote decorator to convert it into an",
    "start": "450840",
    "end": "458479"
  },
  {
    "text": "actor then when we instantiate the class a new worker process or actor is created",
    "start": "458479",
    "end": "464479"
  },
  {
    "text": "somewhere in the cluster and each method call creates tasks that are scheduled on that worker process so in this example",
    "start": "464479",
    "end": "471800"
  },
  {
    "text": "these tasks are executed sequentially on the actor there's no parallelism among these tasks and they all Shar the",
    "start": "471800",
    "end": "477440"
  },
  {
    "text": "mutable state of the actor you can also easily schedule these tasks",
    "start": "477440",
    "end": "483840"
  },
  {
    "text": "on gpus by specifying resource requests this allows users to easily parallelize",
    "start": "483840",
    "end": "489319"
  },
  {
    "text": "AI applications you can read more about all this on the ray documentation and so now we've seen some",
    "start": "489319",
    "end": "495800"
  },
  {
    "text": "of the language integrated Primitives that enable our higher level Ray AI libraries to be written and we've made",
    "start": "495800",
    "end": "502159"
  },
  {
    "text": "reference to array cluster but what is array cluster actually let's take a quick look at the architecture and and",
    "start": "502159",
    "end": "508440"
  },
  {
    "text": "this will be the last bit of our array overview so array cluster consists of a head node and one or more worker",
    "start": "508440",
    "end": "515240"
  },
  {
    "text": "nodes so on each node is a raet which is a process that manages shared resources on each",
    "start": "515240",
    "end": "521200"
  },
  {
    "text": "node then on the head node we have the global global control service which manages cluster level",
    "start": "521200",
    "end": "528839"
  },
  {
    "text": "metadata on each node you have one or more worker processes and these perform",
    "start": "529600",
    "end": "535240"
  },
  {
    "text": "the execution of the ray tasks and actors that we just saw and and finally you'll have a driver process which is a",
    "start": "535240",
    "end": "542160"
  },
  {
    "text": "special worker process that executes um the main function basically and for more detailed",
    "start": "542160",
    "end": "549079"
  },
  {
    "text": "information on the architecture you can read the ray white paper um and you can just search up Ray white paper to get a",
    "start": "549079",
    "end": "554800"
  },
  {
    "text": "link to that so quick recap this is the picture you want to have in mind we just gave an",
    "start": "554800",
    "end": "560279"
  },
  {
    "text": "overview of the ray core API and now we're going to move down the stack and explain how Ray can be used with",
    "start": "560279",
    "end": "565880"
  },
  {
    "text": "kubernetes and for this next section I'll hand it over to Winston thanks archet so won't spend too much time",
    "start": "565880",
    "end": "572480"
  },
  {
    "text": "going as deeply with kubernetes think many ways preaching to the choir here uh but kubernetes is the decto deao",
    "start": "572480",
    "end": "579079"
  },
  {
    "text": "standard for you know deploying applications for compute platforms um",
    "start": "579079",
    "end": "584240"
  },
  {
    "text": "Rey is going to get you you know that simple python experience that data scientists love but kubernetes will",
    "start": "584240",
    "end": "590160"
  },
  {
    "text": "bring the operational excellence necessary for that reliable scalable compute platform um I won't go through",
    "start": "590160",
    "end": "597079"
  },
  {
    "text": "every little piece here but essentially you you know it's a trusted standard for deploying operating and managing your",
    "start": "597079",
    "end": "603640"
  },
  {
    "text": "platform um so yeah both Ray and kuties you could call them as resource orchestrators but they're actually",
    "start": "603640",
    "end": "609040"
  },
  {
    "text": "focused on very different aspects Ray is all about the machine learning computation it handles the tasks and",
    "start": "609040",
    "end": "615200"
  },
  {
    "text": "actors that are actually in the distributed application whereas kubernetes is geared more towards the deployment and it's focused on managing",
    "start": "615200",
    "end": "622360"
  },
  {
    "text": "the Pod as its scheduling unit what this does is that allows for a separation of uh interest or responsibility between",
    "start": "622360",
    "end": "629240"
  },
  {
    "text": "data scientist or your ml practitioner with your platform engineer and so Ray with Ray the data scientists are",
    "start": "629240",
    "end": "634920"
  },
  {
    "text": "focusing on their python they're focusing on their you know their training application or in their models",
    "start": "634920",
    "end": "640320"
  },
  {
    "text": "whereas kubernetes they will focus on the resource life cycle and just make sure that you have that performance",
    "start": "640320",
    "end": "645839"
  },
  {
    "text": "scale and reliability and so the best way to run Ray on kubernetes is kubra Ku is an",
    "start": "645839",
    "end": "653760"
  },
  {
    "text": "operator um and it will manage the entire life cycle of the different Ray resources on top of kuber",
    "start": "653760",
    "end": "659920"
  },
  {
    "text": "um again kubra will enable that kind of separation or principle where your ml scientists will focus on the experiments",
    "start": "659920",
    "end": "666040"
  },
  {
    "text": "on the machine learning while your infrastructure Engineers can focus on the resources and the Integrations with",
    "start": "666040",
    "end": "671680"
  },
  {
    "text": "the other operational like kind of needs Enterprise needs that you have um and kubay does this with three",
    "start": "671680",
    "end": "679160"
  },
  {
    "text": "custom resources that's the ray cluster the ray job and Race Service um",
    "start": "679160",
    "end": "685440"
  },
  {
    "text": "essentially you're going to use these for three different slightly I guess different workflows um so for",
    "start": "685440",
    "end": "691560"
  },
  {
    "text": "instance the ray cluster that will you create a persistent Ray cluster where you might need more debugging or an",
    "start": "691560",
    "end": "696639"
  },
  {
    "text": "interactive experience to prototype and try new things or solve what is wrong",
    "start": "696639",
    "end": "702040"
  },
  {
    "text": "with a specific like model that you might be working with um the ray job or the kubra ray job is actually a ray",
    "start": "702040",
    "end": "708800"
  },
  {
    "text": "cluster plus a ray job and that's when you'll start to operationalize on using Ural clusters um and you're just",
    "start": "708800",
    "end": "715040"
  },
  {
    "text": "submitting jobs that will go into a queue um and you might have many people submitting to the same place and Ray and",
    "start": "715040",
    "end": "720160"
  },
  {
    "text": "kubernetes will then help you cue that up and start the resources as needed um",
    "start": "720160",
    "end": "725240"
  },
  {
    "text": "and then lastly the race service that'll focus more on the inference side where you need higher",
    "start": "725240",
    "end": "730399"
  },
  {
    "text": "availability uh and it it essentially is a ray cluster that stands up with added features to make sure that it has fault",
    "start": "730399",
    "end": "736480"
  },
  {
    "text": "tolerance and availability cool so we talked about Ray",
    "start": "736480",
    "end": "741680"
  },
  {
    "text": "why you have Ray and kubernetes and ultimately what KUB is um and the we expect that you'll get these benefits",
    "start": "741680",
    "end": "747839"
  },
  {
    "text": "out of using kubra for for your unified AI platform it goes back to what are the challenges right scale cost and future",
    "start": "747839",
    "end": "753440"
  },
  {
    "text": "proofing and so these kind of four benefits map back to scale cost and future proofing um so first one",
    "start": "753440",
    "end": "759639"
  },
  {
    "text": "scalability and performance oh there's sorry I can't see it uh so I mean we all know that kuber",
    "start": "759639",
    "end": "766600"
  },
  {
    "text": "9s gets you that scale going from one to a th to 15,000 nodes and Ray will help",
    "start": "766600",
    "end": "772399"
  },
  {
    "text": "you achieve that through Distributing your application right so you're getting distributed application on distributed compute to the latest and greatest",
    "start": "772399",
    "end": "779480"
  },
  {
    "text": "accelerators um and so that's scale and performance um and so as I was just",
    "start": "779480",
    "end": "785639"
  },
  {
    "text": "saying you know kubernetes um you can get to 15,000 clusters scale all the way up get access to your most powerful",
    "start": "785639",
    "end": "792720"
  },
  {
    "text": "compute this really tiny here sorry um and oh and pretty much many of the",
    "start": "792720",
    "end": "799839"
  },
  {
    "text": "largest ml models today were all trained using kubernetes and",
    "start": "799839",
    "end": "804920"
  },
  {
    "text": "Ray um specifically with Ray service um as an example it has two key performance",
    "start": "804920",
    "end": "810639"
  },
  {
    "text": "meth um features that help achieve greater stability the first one is zero dime time so it'll monitor for unhealthy",
    "start": "810639",
    "end": "817399"
  },
  {
    "text": "nodes or changes to the config and automatically bring up a new Ray cluster shift the traffic and then recycle the",
    "start": "817399",
    "end": "823519"
  },
  {
    "text": "old cluster for you and that helps achieve zero DME time the second side is Fault tolerance and so one of the",
    "start": "823519",
    "end": "829839"
  },
  {
    "text": "challenges that uh one might have with the ray cluster is that there's a lot of dependency in the ray head if the ray",
    "start": "829839",
    "end": "835199"
  },
  {
    "text": "head goes down then you can't access the dashboard you lose your metadata with the ray service it will make sure",
    "start": "835199",
    "end": "842360"
  },
  {
    "text": "to allow your traffic to continue to flow to the ray workers as it brings back up a new Ray heads and repairs",
    "start": "842360",
    "end": "850160"
  },
  {
    "text": "itself and again cost right so you scale up to achieve scalability you want to",
    "start": "850440",
    "end": "855880"
  },
  {
    "text": "scale back down to only pay for what you use um again Ry will do this on the application layer and then kubernetes",
    "start": "855880",
    "end": "862199"
  },
  {
    "text": "will do this on your resource layer um there's also a ton of features built in to make sure that you have all your",
    "start": "862199",
    "end": "867320"
  },
  {
    "text": "workload right sizing your cost optimizations being able to use spot VMS",
    "start": "867320",
    "end": "872399"
  },
  {
    "text": "uh R ultimately does this on any of the clouds with kubernetes um and then you can actually like scale up whether it's",
    "start": "872399",
    "end": "877920"
  },
  {
    "text": "like a hyperparameter tuning sweep to use um spot as it's available and scale down if spot is not and tons of",
    "start": "877920",
    "end": "883920"
  },
  {
    "text": "different features to help you achieve that cost Advantage um and then specifically so",
    "start": "883920",
    "end": "890399"
  },
  {
    "text": "you're all familiar with kubernetes auto scaling tons of ways to do that to uh you know balance performance with cost",
    "start": "890399",
    "end": "897120"
  },
  {
    "text": "one of the things about with Ray and Auto scaling is that Ray itself is technically the application so it knows",
    "start": "897120",
    "end": "902680"
  },
  {
    "text": "things about your code about the application that kubernetes does not and because of that it has certain advantages in order to achieve even",
    "start": "902680",
    "end": "910000"
  },
  {
    "text": "smarter autoscaling in combination with the different pod and cluster Auto",
    "start": "910000",
    "end": "916279"
  },
  {
    "text": "scaling and lastly again future proofing so one of the things we all love about kubernetes is that it has a robust OSS",
    "start": "917519",
    "end": "925279"
  },
  {
    "text": "ecosystem and no vendor no group no single service is ever going to keep up",
    "start": "925279",
    "end": "931399"
  },
  {
    "text": "with the pace of Open Source and so with kubernetes we know that we're always going to be future proofed towards that",
    "start": "931399",
    "end": "937199"
  },
  {
    "text": "and then Ray is ultimately it's just python so it's equally just as future proof and it's customizable and Ray",
    "start": "937199",
    "end": "943040"
  },
  {
    "text": "brings in a lot of these different AI Frameworks into that single unified experience so Ray plus kubernetes you",
    "start": "943040",
    "end": "948560"
  },
  {
    "text": "know that you're always going to be ready for the next Evolution the next change in AI in order to bring that into your",
    "start": "948560",
    "end": "955040"
  },
  {
    "text": "platform and you know future proofing again portability you write once you can use it anywhere whether it's on premise",
    "start": "955040",
    "end": "962120"
  },
  {
    "text": "different clouds on your laptop you can use Ray and kubernetes on any of these different",
    "start": "962120",
    "end": "968759"
  },
  {
    "text": "environments cool so we talked a lot about all the benefits um at Google we offer a couple like kind of quick start",
    "start": "969240",
    "end": "975600"
  },
  {
    "text": "templates if you just search Ray on gke um it's a GitHub and it has some solution templates with built-in",
    "start": "975600",
    "end": "981480"
  },
  {
    "text": "different service Integrations to hopefully get you started earlier if you want to see a demo there actually is another talk um shown here on tomorrow",
    "start": "981480",
    "end": "989839"
  },
  {
    "text": "and we hope that it'll help customers like yourself basically achieve that faster and more costeffective benefits",
    "start": "989839",
    "end": "996440"
  },
  {
    "text": "that we're talking about with kubra but you don't really need to take it from us can see these kind of crazy awesome",
    "start": "996440",
    "end": "1003279"
  },
  {
    "text": "metrics that all the different customers are reporting on this AR thanks",
    "start": "1003279",
    "end": "1008959"
  },
  {
    "text": "Winston yeah and just to build off that here we also have a quote from Niantic saying how they were able to use kubra successfully in production um you can",
    "start": "1008959",
    "end": "1015600"
  },
  {
    "text": "check out their blog post and their talk recording from race Summit of of this year to learn more about",
    "start": "1015600",
    "end": "1021880"
  },
  {
    "text": "that and yeah just talking more about the kubra usage um it's really been growing we've seen an average of 37%",
    "start": "1021880",
    "end": "1028720"
  },
  {
    "text": "month uh per month growth in the number of unique cubra clusters running over the last six",
    "start": "1028720",
    "end": "1034079"
  },
  {
    "text": "months and this past year has seen four major releases hundreds of commits we also have more than 100 contributors now",
    "start": "1034079",
    "end": "1040520"
  },
  {
    "text": "which is on par with other popular kubernetes operators and we have several blog posts",
    "start": "1040520",
    "end": "1046000"
  },
  {
    "text": "from industry leaders using kubra and production um I'll just share some screenshots",
    "start": "1046000",
    "end": "1053240"
  },
  {
    "text": "here so with all this said uh we're delighted to announce that KUB is generally available um we hope you'll",
    "start": "1053799",
    "end": "1059280"
  },
  {
    "text": "try it out this is the version 1.0 release culmination of a lot of",
    "start": "1059280",
    "end": "1064480"
  },
  {
    "text": "[Applause] work so now for the last part of our talk um we're going to see kubra in",
    "start": "1065710",
    "end": "1071640"
  },
  {
    "text": "action with the demo of what we're calling the llm life cycle so I'm sure all of you are familiar with llms uh",
    "start": "1071640",
    "end": "1078640"
  },
  {
    "text": "sort of taken the World by storm recently so first let's compare the differences between the traditional ml",
    "start": "1078640",
    "end": "1085039"
  },
  {
    "text": "model life cycle and the llm model life cycle so before llms the most expensive",
    "start": "1085039",
    "end": "1090400"
  },
  {
    "text": "part of the life cycle would be training because it requires lots of gpus for serving the traditional ml workload",
    "start": "1090400",
    "end": "1096520"
  },
  {
    "text": "would be more latency sensitive models wouldn't be very large you could use single or small GPU instances for",
    "start": "1096520",
    "end": "1102159"
  },
  {
    "text": "inference or even CPUs in some cases and the bottleneck typically would be",
    "start": "1102159",
    "end": "1107320"
  },
  {
    "text": "computation the llm model life cycle is a bit different pre- training an llm is",
    "start": "1107320",
    "end": "1112600"
  },
  {
    "text": "pretty expensive May cost millions of dollars therefore only a few big tech companies can really afford to pre-train",
    "start": "1112600",
    "end": "1118760"
  },
  {
    "text": "in llm so in the llm model life cycle serving becomes much more expensive and",
    "start": "1118760",
    "end": "1125720"
  },
  {
    "text": "important you may need multiple large expensive gpus and the gpus may not even be available when you need",
    "start": "1125720",
    "end": "1133159"
  },
  {
    "text": "them so what I want to show you in these last few minutes is that kubra can manage the entire enta llm life cycle on",
    "start": "1133159",
    "end": "1140320"
  },
  {
    "text": "kubernetes and there's two main reasons why it's the best solution here the first is that it supports autoscaling",
    "start": "1140320",
    "end": "1147360"
  },
  {
    "text": "it's hard to predict traffic and like we saw earlier kubra supports Auto scaling which adjusts dynamically based on the",
    "start": "1147360",
    "end": "1153880"
  },
  {
    "text": "load to save costs for your ml platform and finally kubra supports",
    "start": "1153880",
    "end": "1159159"
  },
  {
    "text": "heterogeneous compute resources on a single cluster or workload so supporting things like gpus tpus and so",
    "start": "1159159",
    "end": "1167240"
  },
  {
    "text": "on so now we'll move to our demo portion of just showing how some of these things",
    "start": "1167240",
    "end": "1173240"
  },
  {
    "text": "can be used with KUB so here's a quick diagram of one example of like a sample llm workflow",
    "start": "1173240",
    "end": "1180840"
  },
  {
    "text": "and you can see how each piece can be done with Ray so prototyping for example could be done with say a Jupiter",
    "start": "1180840",
    "end": "1187080"
  },
  {
    "text": "Notebook on the head node of Ray cluster then you could move into Data preparation using Ray data or Ray core",
    "start": "1187080",
    "end": "1194000"
  },
  {
    "text": "Primitives fine-tuning using array job running array train worklad Lo serving",
    "start": "1194000",
    "end": "1199919"
  },
  {
    "text": "using the ray service custom resource uh running Ray llm and finally you could expose it as an application uh with a UI",
    "start": "1199919",
    "end": "1207320"
  },
  {
    "text": "using lank chain and gradio so for this talk we're just going",
    "start": "1207320",
    "end": "1212360"
  },
  {
    "text": "to focus on the fine tuning the serving and the final portion with",
    "start": "1212360",
    "end": "1218520"
  },
  {
    "text": "gradio so first we're going to run some fine-tuning using the ray job custom resource you can see some technical",
    "start": "1218520",
    "end": "1224360"
  },
  {
    "text": "details here but for the application code we're not going to show it on screen but it's just using a tutorial from the ray train documentation um so",
    "start": "1224360",
    "end": "1231799"
  },
  {
    "text": "no one's typing that link but there's a QR code which which points to that URL um if you want to see the application",
    "start": "1231799",
    "end": "1237360"
  },
  {
    "text": "code which is uh just Ray python code ultimately um so let's go ahead and see",
    "start": "1237360",
    "end": "1244200"
  },
  {
    "text": "the demo there's the",
    "start": "1244200",
    "end": "1250200"
  },
  {
    "text": "cursor all right so here we're installing the kubra operator using Helm",
    "start": "1250760",
    "end": "1256320"
  },
  {
    "text": "you can see that it's just been deployed we're just checking to see that the",
    "start": "1256320",
    "end": "1261440"
  },
  {
    "text": "kubernetes operator has been started so now we're going to run a ray",
    "start": "1261440",
    "end": "1266760"
  },
  {
    "text": "job that will run our fine tuning for us using Ray train and deep",
    "start": "1266760",
    "end": "1272278"
  },
  {
    "text": "speed so the ray job will in turn launch a ray cluster and so now we've just",
    "start": "1274200",
    "end": "1280640"
  },
  {
    "text": "we're watching the GPU pods come up this is just coup C get pod um we're checking the logs for the",
    "start": "1280640",
    "end": "1286919"
  },
  {
    "text": "fine-tuning job as it's running uh for this demo we're just running one Epoch and at the end it's going to save",
    "start": "1286919",
    "end": "1293240"
  },
  {
    "text": "a checkpoint to S3 which we'll load later into our service um there's the",
    "start": "1293240",
    "end": "1298919"
  },
  {
    "text": "checkpoint so that's sort of the output of this step finally we'll delete the ray job to clean it up and this will",
    "start": "1298919",
    "end": "1304880"
  },
  {
    "text": "also recycle the ray",
    "start": "1304880",
    "end": "1308400"
  },
  {
    "text": "cluster for the next part this is just the same QR code as before in case you didn't see it earlier uh links to the",
    "start": "1311360",
    "end": "1316640"
  },
  {
    "text": "same template on the r docs uh with our application codee um so this is the next step which is serving from that",
    "start": "1316640",
    "end": "1323039"
  },
  {
    "text": "checkpoint using array service which is deploying Ray",
    "start": "1323039",
    "end": "1328960"
  },
  {
    "text": "llm so now we're just going to create a ray service using coul apply with our Ray service EML which contains",
    "start": "1333000",
    "end": "1338600"
  },
  {
    "text": "application code from the link in the previous slide so creating the ray service custom resource and what this is",
    "start": "1338600",
    "end": "1345559"
  },
  {
    "text": "going to do is it's going to spin up another ray cluster to serve our model and our Ray cluster is going to have one",
    "start": "1345559",
    "end": "1352200"
  },
  {
    "text": "head node and one worker node with a GPU on it so we'll just wait for those pods",
    "start": "1352200",
    "end": "1358320"
  },
  {
    "text": "to come up and once the cluster is running we'll",
    "start": "1358320",
    "end": "1363679"
  },
  {
    "text": "run Cube C get service to get the name of the service that we want here it's called",
    "start": "1363679",
    "end": "1369360"
  },
  {
    "text": "Avary uh we'll do port forwarding so we can query the model and we'll send a simple uh math",
    "start": "1369360",
    "end": "1376679"
  },
  {
    "text": "question to the model once the port foring is done uh math question similar to the ones in the data set and we'll",
    "start": "1376679",
    "end": "1384279"
  },
  {
    "text": "get a response from the model uh which is some some math",
    "start": "1384279",
    "end": "1389760"
  },
  {
    "text": "answer so for the last step here we're going to throw a web UI over it using gradio and Lang",
    "start": "1390600",
    "end": "1396799"
  },
  {
    "text": "chain um so gradio will provide the UI Lang chain will do the interfacing with our model converting the The Prompt the",
    "start": "1396799",
    "end": "1404279"
  },
  {
    "text": "answer to something human readable um so here's a sample math problem details are not too",
    "start": "1404279",
    "end": "1411559"
  },
  {
    "text": "important um we'll just show you what this UI looks",
    "start": "1411559",
    "end": "1417080"
  },
  {
    "text": "like so we're checking the graphical user interface um provided by gradio and",
    "start": "1421600",
    "end": "1427720"
  },
  {
    "text": "you can see we input a question and get an answer so this is an example of of",
    "start": "1427720",
    "end": "1433400"
  },
  {
    "text": "the sort of end to end LM life cycle and we've done all parts of it using Ray and cubra",
    "start": "1433400",
    "end": "1439880"
  },
  {
    "text": "so that concludes our talk thanks so much for for attending um you can follow to learn more about Ray at this link up",
    "start": "1441600",
    "end": "1448640"
  },
  {
    "text": "there we have an active Ray slack uh you can follow these two channels in particular for questions about kubay um",
    "start": "1448640",
    "end": "1454720"
  },
  {
    "text": "and some links about Ray on GK thanks so much and archard's got stickers yeah and",
    "start": "1454720",
    "end": "1461679"
  },
  {
    "text": "we have stickers and a limited amount of Ray swag which I'll keep up here for",
    "start": "1461679",
    "end": "1467279"
  },
  {
    "text": "people who want to come after the talk or to ask ask questions remember one really cool fact to get this way yeah",
    "start": "1467279",
    "end": "1472600"
  },
  {
    "text": "yeah please be engaged okay this a QR code for feedback thanks yeah and we'll we'll take",
    "start": "1472600",
    "end": "1479440"
  },
  {
    "text": "questions he uh quick question um how do you distribute your code and code",
    "start": "1479640",
    "end": "1484840"
  },
  {
    "text": "dependencies between like the driver and the uh or whatever components or what components need your code and how do you",
    "start": "1484840",
    "end": "1492520"
  },
  {
    "text": "distribute it and is there a way to separate your sizing information from that code",
    "start": "1492520",
    "end": "1498880"
  },
  {
    "text": "uh you said size information like GP number gpus number nodes all that kind of stuff or or rather than hard coding",
    "start": "1498880",
    "end": "1504919"
  },
  {
    "text": "it into the code I'm just wondering if yeah got it so the size information uh I",
    "start": "1504919",
    "end": "1510000"
  },
  {
    "text": "guess yeah you can think of them as like resource requirements those are just defined straight in your python code so these are just parameters that you'll",
    "start": "1510000",
    "end": "1516039"
  },
  {
    "text": "pass directly to Ray so whatever way you want to use to manage those um like it's",
    "start": "1516039",
    "end": "1522480"
  },
  {
    "text": "pretty flexible as for the dependencies Ray has two ways of handling that so one is you could",
    "start": "1522480",
    "end": "1528200"
  },
  {
    "text": "pre-bake all of them into say a Docker image so your ray image and specify that in your uh your yl spec for cubay or you",
    "start": "1528200",
    "end": "1535200"
  },
  {
    "text": "could specify it at runtime um in the python code itself using a feature called Ray runtime environments so you",
    "start": "1535200",
    "end": "1541399"
  },
  {
    "text": "can specify say a list of uh you know python pip packages environment variables and so on and those will",
    "start": "1541399",
    "end": "1546559"
  },
  {
    "text": "actually be installed dynamically at runtime so depending on your use case and your constraints you could use either one of those thanks",
    "start": "1546559",
    "end": "1554880"
  },
  {
    "text": "mhm thank you for the great talk I have two questions so first of one I think you mentioned about Advanced metrics uh",
    "start": "1554919",
    "end": "1562559"
  },
  {
    "text": "collection and scale your instances based on the GPU utilization and CPU utilization so can you elaborate a",
    "start": "1562559",
    "end": "1569320"
  },
  {
    "text": "little bit what are the difference between what kubernets provides out of the box and what Ray can do with the",
    "start": "1569320",
    "end": "1574880"
  },
  {
    "text": "metrics uh with metrics I see um I I don't have too many details on what",
    "start": "1574880",
    "end": "1580080"
  },
  {
    "text": "exactly the the AI libraries do on that front but I think the general picture is you can see that the the GPU resource",
    "start": "1580080",
    "end": "1586440"
  },
  {
    "text": "requests are happening at a granular level of like tasks and actors and that's something that kubernetes can't",
    "start": "1586440",
    "end": "1592720"
  },
  {
    "text": "see into but but Ray can provide so Ray will request the the Pod only when it's",
    "start": "1592720",
    "end": "1598320"
  },
  {
    "text": "needed by by that task or actor so that's the level of like a function or a class so does this mean like basically",
    "start": "1598320",
    "end": "1604039"
  },
  {
    "text": "if user like how many gpus user set in advance basically array will scale up or",
    "start": "1604039",
    "end": "1609480"
  },
  {
    "text": "scale down the instance right exactly yeah all right but you don't really analyze the runtime information for my",
    "start": "1609480",
    "end": "1615320"
  },
  {
    "text": "model right so basically which means like when training is is executing mhm you don't analyze how many gpus I need",
    "start": "1615320",
    "end": "1621880"
  },
  {
    "text": "because imagine what if I want to start my job but I just don't know how many GPU I need right yeah that's something that'll have to be done at the",
    "start": "1621880",
    "end": "1627679"
  },
  {
    "text": "application Level so resources from raise perspective are uh just logical resources so it's request based on what",
    "start": "1627679",
    "end": "1634240"
  },
  {
    "text": "the the user what the user provides right yeah exactly yeah thank you and a second question about race service so do",
    "start": "1634240",
    "end": "1639320"
  },
  {
    "text": "you support SS based models basically if I want to scale down my instance all the",
    "start": "1639320",
    "end": "1644720"
  },
  {
    "text": "way to zero and scale up when traffic comes into the model yeah resport scale to zero yeah all right what kind of like",
    "start": "1644720",
    "end": "1650720"
  },
  {
    "text": "technology are you using behind the scenes like how you scale up your um inference L",
    "start": "1650720",
    "end": "1656240"
  },
  {
    "text": "inference is it like done inside the ray like aray implementation yeah yeah so it's all built on top of Ray core so at",
    "start": "1656240",
    "end": "1662480"
  },
  {
    "text": "its core the serve deployments are just Ray actors um scheduled in such a way that you know they're spread out among",
    "start": "1662480",
    "end": "1668240"
  },
  {
    "text": "the nodes and so on they can scale up and down Mak sense thank you great talk hello yeah first thanks we a great",
    "start": "1668240",
    "end": "1675360"
  },
  {
    "text": "presentation here I have question about uh when you assign the results to the function of the actor here like a remote",
    "start": "1675360",
    "end": "1683679"
  },
  {
    "text": "and then the results and the mount there but you know it it's totally defined",
    "start": "1683679",
    "end": "1689360"
  },
  {
    "text": "controlled by the software engineer like uh I for me I totally most of the case I",
    "start": "1689360",
    "end": "1695120"
  },
  {
    "text": "don't have idea maybe I give it more results maybe I give it less results like uh that that does really could like",
    "start": "1695120",
    "end": "1702240"
  },
  {
    "text": "do some optimization behind it or it's just uh totally mechanical yeah yeah no",
    "start": "1702240",
    "end": "1707880"
  },
  {
    "text": "so that that kind of thing is not done at the level of of Ray so Ray doesn't uh",
    "start": "1707880",
    "end": "1713480"
  },
  {
    "text": "so you would have to monitor the usage yourself and you can do this using tools such as the ray dashboard um Prometheus",
    "start": "1713480",
    "end": "1719200"
  },
  {
    "text": "grafana and so on um but Ray won't handle that automatically okay tracking utilization and using it to schedule",
    "start": "1719200",
    "end": "1727360"
  },
  {
    "text": "MH uh thanks for the presentation um quick question I think Auto scaling is",
    "start": "1727360",
    "end": "1732600"
  },
  {
    "text": "great uh however in the large language model use cases the Cod start time is a",
    "start": "1732600",
    "end": "1738600"
  },
  {
    "text": "kind of big headache right like especially you uh for the motor loading I'm wondering in Ray area is there any",
    "start": "1738600",
    "end": "1745919"
  },
  {
    "text": "way or optimization has been done uh to to optimize that I see so you're talking um",
    "start": "1745919",
    "end": "1754120"
  },
  {
    "text": "cold start time of the of the model the instances um I don't have the answer to",
    "start": "1754120",
    "end": "1759200"
  },
  {
    "text": "that off the top of my head I think you can probably ask on slack um some people who are more familiar with the ray llm",
    "start": "1759200",
    "end": "1764640"
  },
  {
    "text": "project would be able to help I see uh also uh another quick question is regarding the M uh multiple models uh",
    "start": "1764640",
    "end": "1772159"
  },
  {
    "text": "that's r support let's say uh a single interface for for multiple models um",
    "start": "1772159",
    "end": "1777919"
  },
  {
    "text": "yeah definitely you can um you can serve multiple models on the same Ray cluster yeah all using the same",
    "start": "1777919",
    "end": "1784440"
  },
  {
    "text": "interface right cool thank you yeah thanks nice presentation I just",
    "start": "1784440",
    "end": "1790039"
  },
  {
    "text": "wanted to chat a little bit about plasma do you guys know a lot about plasma how it's implemented and how it may run well",
    "start": "1790039",
    "end": "1796000"
  },
  {
    "text": "or not on kubernetes so you're talking about the plasma memory store the data store yeah yeah um",
    "start": "1796000",
    "end": "1801760"
  },
  {
    "text": "I'm unfortunately not the best person to that's kind of like a a lower level Ray cor question um but we did in the past",
    "start": "1801760",
    "end": "1807320"
  },
  {
    "text": "and possibly still use yeah plasma store for shared memory for distributed memory yeah so I think like probably in the",
    "start": "1807320",
    "end": "1812919"
  },
  {
    "text": "full like training pre-training flow it's much more effective right because",
    "start": "1812919",
    "end": "1818039"
  },
  {
    "text": "you're doing a lot more intense work there but on the serving side have you seen any use cases where you're like",
    "start": "1818039",
    "end": "1823480"
  },
  {
    "text": "actually using plasma for serving or no um unfortunately I I don't know off the",
    "start": "1823480",
    "end": "1828880"
  },
  {
    "text": "top of my head it might be a better question for a slack and I can um I can CC the appropriate people to to get into",
    "start": "1828880",
    "end": "1835640"
  },
  {
    "text": "maybe catch up with someone thank you thanks hi uh just a question so do you",
    "start": "1835640",
    "end": "1842320"
  },
  {
    "text": "know if uh CU like plays well with like notebooks under Cloud Solutions like Q flow or like jupyter Hub because at",
    "start": "1842320",
    "end": "1848840"
  },
  {
    "text": "least like when we try to set it up uh the user have to figure out you know like the networking side of things like where to connect like which service of",
    "start": "1848840",
    "end": "1855320"
  },
  {
    "text": "the cluster to connect like is there like tools nowadays that's helps with that process I see",
    "start": "1855320",
    "end": "1862159"
  },
  {
    "text": "um so I I'm not aware of any like third",
    "start": "1862159",
    "end": "1867720"
  },
  {
    "text": "party tool that does the setup for you you're talking about like setting up just like say user spins up a qf flow",
    "start": "1867720",
    "end": "1874760"
  },
  {
    "text": "notebook right and they want to like play with the ray cluster interactively so like then like to set that up they have to figure out you know like the",
    "start": "1874760",
    "end": "1880880"
  },
  {
    "text": "service like where to connect to like the URLs things like that like is there more a little bit more native like yeah",
    "start": "1880880",
    "end": "1886480"
  },
  {
    "text": "as far as I know I think that's still something the user has to Dos um but I I can get back to you and and do a little",
    "start": "1886480",
    "end": "1892039"
  },
  {
    "text": "bit more research on that yeah thanks",
    "start": "1892039",
    "end": "1899279"
  },
  {
    "text": "yeah um yeah you're talking about like optimizations with that the raid data Library uses um I can put you in contact",
    "start": "1908200",
    "end": "1915200"
  },
  {
    "text": "with someone from ra sorry I don't I know that technical details there but yeah it's a good",
    "start": "1915200",
    "end": "1921760"
  },
  {
    "text": "question I see I see yeah um yeah reach out on Race slack we can definitely get people who are experts to to help out",
    "start": "1928519",
    "end": "1934840"
  },
  {
    "text": "with thater the ray operator to theace or",
    "start": "1934840",
    "end": "1941960"
  },
  {
    "text": "cluster scope to the namespace or the cluster um uh let me ask my question do",
    "start": "1941960",
    "end": "1947840"
  },
  {
    "text": "you want to answer",
    "start": "1947840",
    "end": "1950320"
  },
  {
    "text": "that you can do either so I think we're just at time",
    "start": "1958320",
    "end": "1963519"
  },
  {
    "text": "here we'll stick around informally to answer questions and I'll bring up the the race swag but thanks again for",
    "start": "1963519",
    "end": "1970320"
  },
  {
    "text": "attending",
    "start": "1971480",
    "end": "1974480"
  }
]