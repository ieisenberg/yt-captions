[
  {
    "text": "I think I'm going to get started so um hi everyone thanks for joining um me",
    "start": "760",
    "end": "5799"
  },
  {
    "text": "today welcome to my session uh enable uh GPU acceleration without worrying about",
    "start": "5799",
    "end": "11040"
  },
  {
    "text": "managing device drivers I know it's a mouthful sorry about that uh my name is Chris deso I work at um as a software",
    "start": "11040",
    "end": "17840"
  },
  {
    "text": "engineer at Nvidia uh I work on our Cloud native team uh we work on enabling",
    "start": "17840",
    "end": "22880"
  },
  {
    "text": "gpus and containers and kubernetes so in this talk I'm going to",
    "start": "22880",
    "end": "28599"
  },
  {
    "text": "be strictly discussing Linux device drivers right I'll give a quick definition of what I mean by a device",
    "start": "28599",
    "end": "33960"
  },
  {
    "text": "driver and what are some methods for installing them in kubernetes today or typical methods for installing them um",
    "start": "33960",
    "end": "41520"
  },
  {
    "text": "I'll Focus the latter half of my talk on some of the day two challenges with managing drivers especially with a",
    "start": "41520",
    "end": "47239"
  },
  {
    "text": "larger cluster um I'll present some solutions we've built especially for for NVIDIA gpus um and then I'll end with a",
    "start": "47239",
    "end": "56680"
  },
  {
    "text": "demo so what is a device driver we'll simply put a a driver abstracts a piece of Hardware right on a Linux system",
    "start": "57120",
    "end": "64158"
  },
  {
    "text": "typically it's a kernel space component which is a loadable kernel module that you build uh against your kernel and",
    "start": "64159",
    "end": "70119"
  },
  {
    "text": "have to recompile on on kernel updates um and then you also have along with it",
    "start": "70119",
    "end": "75360"
  },
  {
    "text": "some user space driver libraries that provide abstractions in user space right and so these are usually versioned",
    "start": "75360",
    "end": "80680"
  },
  {
    "text": "together and you need both of these to to leverage uh your device or accelerator so for my talk when I say",
    "start": "80680",
    "end": "86720"
  },
  {
    "text": "device driver I mean both the user space uh libraries Shar libraries and the the kernel",
    "start": "86720",
    "end": "92640"
  },
  {
    "text": "module so what are some if we were to install drivers and on your local system you would just you know either build",
    "start": "92640",
    "end": "99200"
  },
  {
    "text": "from Source or for your respected distribution go to your package manager and install a package right invid a",
    "start": "99200",
    "end": "105240"
  },
  {
    "text": "driver package um but you can't do that in kubernetes we need to install this on lots of machines so what are some",
    "start": "105240",
    "end": "111799"
  },
  {
    "text": "strategies uh the first you can use SSH right so you can use some common tool or some familiar familiar tools like",
    "start": "111799",
    "end": "118399"
  },
  {
    "text": "ansible or puppet um have a common recipe and go one by one on your nodes and install drivers right directly on",
    "start": "118399",
    "end": "124439"
  },
  {
    "text": "the host uh this is simple it's maybe familiar to your team or tooling that's familiar to your team but obviously",
    "start": "124439",
    "end": "131239"
  },
  {
    "text": "we're not using kubernetes to manage our drivers so I'm not going to focus on the for the rest of my talk right you need",
    "start": "131239",
    "end": "136599"
  },
  {
    "text": "separate tooling you need to maintain it um and you know separate tooling for monitoring and logging right your your",
    "start": "136599",
    "end": "143040"
  },
  {
    "text": "your drivers second approach is to build your own operating system image for your GPU",
    "start": "143040",
    "end": "149640"
  },
  {
    "text": "work ER nodes and embed the driver in it right so have your own image maintain it um and this is simple to understand it's",
    "start": "149640",
    "end": "156239"
  },
  {
    "text": "reliable it's also secure because you're not installing things at runtime um the the major con here is",
    "start": "156239",
    "end": "163000"
  },
  {
    "text": "it's not flexible uh as flexible because if you want to deploy new drivers you",
    "start": "163000",
    "end": "168519"
  },
  {
    "text": "typically have to rebuild your OS image right so maybe that takes time for your organization also if you have existing",
    "start": "168519",
    "end": "174800"
  },
  {
    "text": "nodes you need to reprovision them and that may be a lengthy process um additionally if you have CPU and GPU",
    "start": "174800",
    "end": "180920"
  },
  {
    "text": "nodes in your cluster you may need to maintain different images for each of those and that also adds",
    "start": "180920",
    "end": "187599"
  },
  {
    "text": "overhead third approach is to use containers so this is not a new approach but you can package your driver and a",
    "start": "187599",
    "end": "194879"
  },
  {
    "text": "set of install scripts into a container image and deploy it everywhere right and install load your kernel modules and",
    "start": "194879",
    "end": "201840"
  },
  {
    "text": "make your user space driver libraries available on the host somewhere um this is allows us to do you know reproducible",
    "start": "201840",
    "end": "207760"
  },
  {
    "text": "Builds on your cluster um you can manage it via kubernetes which is great and why I'll focus on this method for for the",
    "start": "207760",
    "end": "214319"
  },
  {
    "text": "remainder of my talk and just as important this also works on container optimized operating systems like coros",
    "start": "214319",
    "end": "220280"
  },
  {
    "text": "or flat car Linux those that are um the the roof file is mostly readon so you",
    "start": "220280",
    "end": "225760"
  },
  {
    "text": "can't just SSH and install a package right it's meant to be a run an operating system for running containers",
    "start": "225760",
    "end": "232239"
  },
  {
    "text": "on um obviously I mean one con is you do need to run a pod with elevated",
    "start": "232239",
    "end": "237439"
  },
  {
    "text": "privileges we are loading a kernel module right so it's like you're running running as root so something to be cognizant of when you're using driver",
    "start": "237439",
    "end": "245000"
  },
  {
    "text": "containers as we call them so for the rest of my talk because we can manage this through kubernetes",
    "start": "245000",
    "end": "251120"
  },
  {
    "text": "I'm going to focus on this method but the other ones are are valid and you're you're free to use them so quickly like what is a driver",
    "start": "251120",
    "end": "258759"
  },
  {
    "text": "container or a pod right if we deployed in kubernetes it's just a container that does a few things one it compiles your",
    "start": "258759",
    "end": "264240"
  },
  {
    "text": "driver against the kernel it loads the kernel modules and then it also mounts",
    "start": "264240",
    "end": "270080"
  },
  {
    "text": "part or all of the containers root file system onto the host somewhere using a host path volume that's because we need",
    "start": "270080",
    "end": "275840"
  },
  {
    "text": "to make sure that device nodes driver libraries are made available in the host somewhere so that we can reuse them so",
    "start": "275840",
    "end": "282320"
  },
  {
    "text": "that containers that want to use a driver use a GPU for example can have the driver libraries um injected into",
    "start": "282320",
    "end": "289000"
  },
  {
    "text": "them and once that once that's done this can this is done they can just",
    "start": "289000",
    "end": "294199"
  },
  {
    "text": "sleep we can take this pod we can deploy it via demon set so that we get one driver installed all your nodes with",
    "start": "294199",
    "end": "300600"
  },
  {
    "text": "your Hardware in this case I have some nice Nvidia djx servers with lots of gpus and I install One driver on each",
    "start": "300600",
    "end": "307400"
  },
  {
    "text": "one of them now uh a demon set is we can deploy a static demon set and that's",
    "start": "307400",
    "end": "313039"
  },
  {
    "text": "fine but for complicated devices like nid gpus configuration can be a little",
    "start": "313039",
    "end": "318360"
  },
  {
    "text": "bit complex and so we um and and managing the life cycle of it can also be complex so we we would prefer to",
    "start": "318360",
    "end": "325000"
  },
  {
    "text": "maybe build an operator to automate and make an easier user experience for for for enabling certain",
    "start": "325000",
    "end": "330840"
  },
  {
    "text": "features um so Nvidia GPU specifically we have an operator I think we've it's been mentioned at this cubec Con Nvidia",
    "start": "330840",
    "end": "337280"
  },
  {
    "text": "GPU operator right it automates the the deployment and life cycle of like most",
    "start": "337280",
    "end": "343000"
  },
  {
    "text": "of the Nvidia software you need to use gpus and kubernetes um so it includes the driver that I just showed but it has",
    "start": "343000",
    "end": "348319"
  },
  {
    "text": "a lot of other components um like runtime components our device plug-in monitoring tools Etc um I highly",
    "start": "348319",
    "end": "355240"
  },
  {
    "text": "encourage you to attend a talk tomorrow by a few of my colleagues who are going to go over this operator in much more detail I'm just going to focus on the",
    "start": "355240",
    "end": "361800"
  },
  {
    "text": "driver aspect and what we offer and and some things that uh I think you can find useful for for driver",
    "start": "361800",
    "end": "369199"
  },
  {
    "text": "management so like what did we support today with our operator and NVIDIA drivers so some some some key features",
    "start": "369639",
    "end": "375960"
  },
  {
    "text": "that I want to highlight so the first thing is for any supported distribution that our operator claims to support we",
    "start": "375960",
    "end": "381759"
  },
  {
    "text": "build and publish containerized driver images for them on our Nvidia container registry this means if you install the",
    "start": "381759",
    "end": "387919"
  },
  {
    "text": "operator on your cluster automatically we will pull a container image for that",
    "start": "387919",
    "end": "394400"
  },
  {
    "text": "distri for your distribution for your worker nodes and compile and load the driver on them um so this is nice um we",
    "start": "394400",
    "end": "402599"
  },
  {
    "text": "also are um building and supporting pre-compiled driver images that is",
    "start": "402599",
    "end": "407759"
  },
  {
    "text": "driver images that have pre-compiled drivers built into them so you don't have to compile at runtime there are",
    "start": "407759",
    "end": "413400"
  },
  {
    "text": "some nice benefits of that you don't have to compile so it's quicker to deploy right quicker deployment time uh",
    "start": "413400",
    "end": "419319"
  },
  {
    "text": "there's less runtime dependencies right you don't need to pull kernel headers to compile against that kernel you don't need certain parts of the tool chain and",
    "start": "419319",
    "end": "425919"
  },
  {
    "text": "because of that easier to deploy an air gapped environments we don't depend on the network anymore also easier to",
    "start": "425919",
    "end": "431280"
  },
  {
    "text": "support secure boot because um you can sign your drivers at compilation time",
    "start": "431280",
    "end": "436560"
  },
  {
    "text": "beforehand and so uh we we we we we've started to publish some images for certain distributions uh in particular",
    "start": "436560",
    "end": "442960"
  },
  {
    "text": "bunched 224 and um some kernel flavors um there and the operator when you",
    "start": "442960",
    "end": "448520"
  },
  {
    "text": "enable it will detect like what kernels you have in the cluster and deploy a demon set for each specific kernel",
    "start": "448520",
    "end": "454080"
  },
  {
    "text": "version so the right um image gets pulled so we also support other features",
    "start": "454080",
    "end": "460039"
  },
  {
    "text": "so virtual GPU drivers this is a feature at Nvidia where you can share a GPU with",
    "start": "460039",
    "end": "465800"
  },
  {
    "text": "multiple virtual machines and so there's a host and a guest driver component for that and we support containerizing those",
    "start": "465800",
    "end": "471479"
  },
  {
    "text": "and managing them through through the operator uh we also support the open GPU kernel modules those the open sourced",
    "start": "471479",
    "end": "478199"
  },
  {
    "text": "Nvidia modules that were announced uh I think in 2022 so we also support those in driver containers and we also support",
    "start": "478199",
    "end": "484680"
  },
  {
    "text": "other Advanced features like GP direct um RDMA storage these are things that allow you these are technologies that",
    "start": "484680",
    "end": "490479"
  },
  {
    "text": "allow you to communicate do data transfer between the GPU and other external devices like uh infin devices",
    "start": "490479",
    "end": "497360"
  },
  {
    "text": "or storage U and bypass the CPU and there's kernel modules that are required to be loaded on your system to enable",
    "start": "497360",
    "end": "503960"
  },
  {
    "text": "these um and if you if you want to opt into these our operator will actually build and load these modules for you so",
    "start": "503960",
    "end": "510599"
  },
  {
    "text": "it's quite nice so next few slides I'm going to",
    "start": "510599",
    "end": "515719"
  },
  {
    "text": "kind of just go through so now that we have an overview of the operator and some of the features um how does it actually work at a fundamental level",
    "start": "515719",
    "end": "523120"
  },
  {
    "text": "so um like what is a driver container how works for for NVIDIA gpus so you have an OS you have a kernel you have a",
    "start": "523120",
    "end": "529480"
  },
  {
    "text": "distribution with a runtime there uh and then you deploy a container uh the first thing it does is it uh builds the kernel",
    "start": "529480",
    "end": "538000"
  },
  {
    "text": "modules loads them to the kernel if you have pre-compiled packages it's just going to link and load them uh",
    "start": "538000",
    "end": "543360"
  },
  {
    "text": "instead of instead of compiling uh we run this demon called Nvidia persistence d uh for for NVIDIA",
    "start": "543360",
    "end": "550240"
  },
  {
    "text": "gpus essentially it keeps the handle open with the driver um it's mostly used for performance reasons so uh quicker",
    "start": "550240",
    "end": "557120"
  },
  {
    "text": "startup times for your GPU applications um and the container lifetime is tied to this demon so it will run as long as the",
    "start": "557120",
    "end": "563880"
  },
  {
    "text": "container is running on your host like I said before we have to mount",
    "start": "563880",
    "end": "569480"
  },
  {
    "text": "we have to use a host path volume to make sure that the driver installation that's done in the container is available in the host and so that",
    "start": "569480",
    "end": "576040"
  },
  {
    "text": "applications that run later can have access to them so we typically Mount this somewhere that's writable on Linux system so somewhere under run for",
    "start": "576040",
    "end": "582040"
  },
  {
    "text": "example in this case uh run Nvidia driver um before we can actually run a",
    "start": "582040",
    "end": "588800"
  },
  {
    "text": "GPU container there's a project called Nvidia container toolkit that we maintain that enables um gpus and",
    "start": "588800",
    "end": "594680"
  },
  {
    "text": "containers uh has a set of tools for uh different run it's runtime IC but make",
    "start": "594680",
    "end": "599920"
  },
  {
    "text": "sure that your runtime whether it's cryo or container D can support running uh GPU containers so we have to install",
    "start": "599920",
    "end": "606120"
  },
  {
    "text": "that and then you can run your Cuda container I'm sort of bypass I'm sort of skipping kubernetes scheduling here",
    "start": "606120",
    "end": "611480"
  },
  {
    "text": "assume assume a pod gets gets scheduled here and you're running a Cuda container that requests at one or more gpus then",
    "start": "611480",
    "end": "618279"
  },
  {
    "text": "the driver installation the container run sign will make sure that the files under run that need to be injected",
    "start": "618279",
    "end": "624800"
  },
  {
    "text": "whether the device nodes driver libraries they're made accessible to your container and your container uh can",
    "start": "624800",
    "end": "630040"
  },
  {
    "text": "run just fine and leverage Cuda and and our driver okay so we have an overview of",
    "start": "630040",
    "end": "636839"
  },
  {
    "text": "how driver container Works um and I want to sort of focus on the next next part",
    "start": "636839",
    "end": "642000"
  },
  {
    "text": "of my presentation on some some challenges with managing the life cycle so there's a couple challenges I want to",
    "start": "642000",
    "end": "647240"
  },
  {
    "text": "highlight in my talk the first is that clusters are not all the same so you may have GPU nodes that have different um",
    "start": "647240",
    "end": "654320"
  },
  {
    "text": "Hardware or different system configurations and so it may be necessary to maintain or manage",
    "start": "654320",
    "end": "659600"
  },
  {
    "text": "different driver versions or different driver configurations and doing that at scale can be a little difficult the",
    "start": "659600",
    "end": "666760"
  },
  {
    "text": "second thing is upgrades uh I don't need to I don't need to explain why upgrades are difficult um so right uh in",
    "start": "666760",
    "end": "673519"
  },
  {
    "text": "particular driver upgrades are more challenging because it's like performing node maintenance right you're unloading a kernel module and loading it so it is",
    "start": "673519",
    "end": "679959"
  },
  {
    "text": "disruptive to applications running on the Node that are using the GPU so there is some some care that needs to be",
    "start": "679959",
    "end": "686279"
  },
  {
    "text": "taken so I'm going to quickly cover heterogeneous cluster is this this sort of problem and what we've built to solve",
    "start": "686279",
    "end": "691720"
  },
  {
    "text": "it um so as a cluster administrator what are some some some possibilities that",
    "start": "691720",
    "end": "696800"
  },
  {
    "text": "you would like uh to do with your GPU nodes the first thing is maybe deploy different driver versions in your",
    "start": "696800",
    "end": "702279"
  },
  {
    "text": "cluster uh an example of this is you have a cluster with older gpus and then",
    "start": "702279",
    "end": "707959"
  },
  {
    "text": "you want to add more capacity with the latest and greatest and maybe the the drivers that are supported on the older ones are not the same as the newer the",
    "start": "707959",
    "end": "714399"
  },
  {
    "text": "newer generation so you have to maintain right different driver versions for for those sets of a sets of",
    "start": "714399",
    "end": "721279"
  },
  {
    "text": "nodes the other scenario is maybe you have different operating system versions running maybe you're adding new capacity",
    "start": "721279",
    "end": "726320"
  },
  {
    "text": "with a bun 20204 but you still have some nodes that are running up on to 2004 or you're um upgrading existing nodes and",
    "start": "726320",
    "end": "734240"
  },
  {
    "text": "you want to make sure that the drivers still work during the upgrade right so you want to make sure you have GPU drivers running on both operating system",
    "start": "734240",
    "end": "741399"
  },
  {
    "text": "versions maybe you have some nodes that have um some some some RDMA um devices",
    "start": "741399",
    "end": "748560"
  },
  {
    "text": "and they require some of these additional uh drivers to to leverage GPU direct and some others",
    "start": "748560",
    "end": "754760"
  },
  {
    "text": "don't so there's some heterogenity there um and then the list kind of goes on and",
    "start": "754760",
    "end": "760079"
  },
  {
    "text": "on I'm not going to go through but I think if your cluster is not built the same uh you may need to deploy different",
    "start": "760079",
    "end": "767040"
  },
  {
    "text": "different drivers and different configurations so using an operator what what did we build so we have a crd",
    "start": "767040",
    "end": "774160"
  },
  {
    "text": "called Nvidia driver that allows you to Simply WR um create different instances",
    "start": "774160",
    "end": "780160"
  },
  {
    "text": "so um if you have different uh pools of nodes that have different configurations you can deploy one MD driver instance",
    "start": "780160",
    "end": "787320"
  },
  {
    "text": "Pro pool allows you as an admin to sort of logically partition your GPU nodes in terms of what drivers you want on them",
    "start": "787320",
    "end": "793320"
  },
  {
    "text": "um it supports right different driver versions uh you can deploy different uh uh drivers on different operating system",
    "start": "793320",
    "end": "799199"
  },
  {
    "text": "versions and even driver types right you can deploy virtual GPU drivers like I mentioned earlier or or or not um so in",
    "start": "799199",
    "end": "806240"
  },
  {
    "text": "this example right we have four nodes and the first two are getting their driver from the the red configuration",
    "start": "806240",
    "end": "812560"
  },
  {
    "text": "and the the node 3 and four are getting their driver from the purple the purple instance of of the Nvidia driver",
    "start": "812560",
    "end": "819399"
  },
  {
    "text": "CR so more concrete example right um we have two actual simple uh Nvidia driver",
    "start": "819399",
    "end": "827160"
  },
  {
    "text": "instances uh one of named Kepler because I have a a a a group of nodes that are",
    "start": "827160",
    "end": "832240"
  },
  {
    "text": "running uh the K have k80 gpus attached to them which are very old um and",
    "start": "832240",
    "end": "837360"
  },
  {
    "text": "they're only supported up to the 470 Drive ever and on the right I have a instance called mere with some 00s which",
    "start": "837360",
    "end": "844360"
  },
  {
    "text": "are much a much later generation of GPU and there're suppored by the latest which is 550 right and so this allows me",
    "start": "844360",
    "end": "850440"
  },
  {
    "text": "to sort of logically partition right I have some old gpus I have some new gpus I need to manage them differently and",
    "start": "850440",
    "end": "856000"
  },
  {
    "text": "this makes it quite simple to do that um and also the operator right will when you give it a node selector saying okay",
    "start": "856000",
    "end": "862160"
  },
  {
    "text": "this is my label that identifies my pool of nodes um it will go through them see",
    "start": "862160",
    "end": "867480"
  },
  {
    "text": "okay what are the operating system versions on those hosts and if it does detect that you have different versions",
    "start": "867480",
    "end": "873800"
  },
  {
    "text": "maybe of auntu it will be smart enough to deploy a different demon Set uh per",
    "start": "873800",
    "end": "879399"
  },
  {
    "text": "operating system version so you do get that out of the box when you use um this",
    "start": "879399",
    "end": "884839"
  },
  {
    "text": "uh crd so that was quite simple so I'm going to focus now on driver upgrades um",
    "start": "884839",
    "end": "891639"
  },
  {
    "text": "which I'll spend a little bit more time on so what are some basic requirements when you want to upgrade your driver",
    "start": "891639",
    "end": "896959"
  },
  {
    "text": "well first of all pretty obviously you want to maintain availability of your cluster and of applications that are using the GPU um but at the same time",
    "start": "896959",
    "end": "904720"
  },
  {
    "text": "you want to be able to do um well not not at the same time but you want to be",
    "start": "904720",
    "end": "909800"
  },
  {
    "text": "able to perform the upgrade in a controlled fashion if you have critical workloads that are using the GPU that can't be disrupted uh we want to be able",
    "start": "909800",
    "end": "917040"
  },
  {
    "text": "to selectively you know not drain nodes or or kill workloads during an upgrade",
    "start": "917040",
    "end": "923000"
  },
  {
    "text": "we want to sort of wait for um critical work to to finish um at the same time",
    "start": "923000",
    "end": "928240"
  },
  {
    "text": "you still want to be able to do this a you know have automation built for this right we don't want to have to manually",
    "start": "928240",
    "end": "933639"
  },
  {
    "text": "go node by node and and and and start maintenance on them at the same time uh",
    "start": "933639",
    "end": "939360"
  },
  {
    "text": "we also want to do it automatically and be able to mon monitor progress and and if failures",
    "start": "939360",
    "end": "946120"
  },
  {
    "text": "happen um so the next few slides I want to actually walk through an example of uh what the demon set controller can",
    "start": "946120",
    "end": "952040"
  },
  {
    "text": "offer us so like the demon set controller so we're deploying drivers via demon set and the demon set you can",
    "start": "952040",
    "end": "958360"
  },
  {
    "text": "upgrade demon set so what what are some upgrade strategies and and will it suff will it actually meet all of those",
    "start": "958360",
    "end": "964040"
  },
  {
    "text": "requirements that I just laid out so there's two update strategies there's I'll use my pointer here uh there's the",
    "start": "964040",
    "end": "970920"
  },
  {
    "text": "rolling update and the on delete so on delete is that the demon controller doesn't do anything you you update the version of the demon set and you have to",
    "start": "970920",
    "end": "977440"
  },
  {
    "text": "actually go in node by node and delete the old pod for the new version to get rolled out right so there's no actually",
    "start": "977440",
    "end": "982560"
  },
  {
    "text": "automation there uh rolling update is just a traditional rolling update as you know it right so the demon set controller will go in and select nodes",
    "start": "982560",
    "end": "989480"
  },
  {
    "text": "to upgrade and roll out new versions of your pod or or of your driver in this case so because it's automated let's go",
    "start": "989480",
    "end": "996079"
  },
  {
    "text": "through a quick example um and see if if if it satisfies our requirements so uh",
    "start": "996079",
    "end": "1002120"
  },
  {
    "text": "we want to upgrade the driver so we patch the demon set maybe say I want I want the latest version of the Nvidia",
    "start": "1002120",
    "end": "1008120"
  },
  {
    "text": "driver uh the Dem controller will pick a node to upgrade we'll delete the old version of the Pod and roll out the new",
    "start": "1008120",
    "end": "1014360"
  },
  {
    "text": "one this might work so it'll compile the driver when it tries to load your drivers it will fail because you have",
    "start": "1014360",
    "end": "1021600"
  },
  {
    "text": "drivers already loaded so it's going to we have to go back to our our driver pod definition and add some logic to unload",
    "start": "1021600",
    "end": "1028678"
  },
  {
    "text": "a previous install right so if there is something already installed uninstall it quite",
    "start": "1028679",
    "end": "1033959"
  },
  {
    "text": "simple but what if there's something what if there applications using the GPU right so then you're going to also fail",
    "start": "1033959",
    "end": "1039199"
  },
  {
    "text": "to unload because the modules are busy so we have to add more logic to our driver pod which is okay if if we can't",
    "start": "1039199",
    "end": "1045720"
  },
  {
    "text": "unload let's we need to drain we need to kick workloads off the off the node so",
    "start": "1045720",
    "end": "1051320"
  },
  {
    "text": "if we do that um then we can right install the new version of the driver successfully and repeat this on all of",
    "start": "1051320",
    "end": "1057640"
  },
  {
    "text": "our other nodes so what I showed is that we can use a d controller for driver upgrades",
    "start": "1057640",
    "end": "1064080"
  },
  {
    "text": "but you don't really have the control to um you have to drain the node so that",
    "start": "1064080",
    "end": "1069360"
  },
  {
    "text": "can be disruptive if you have long running for example training um you don't want to lose lots of work that's",
    "start": "1069360",
    "end": "1076000"
  },
  {
    "text": "been been been um been worked on on right so you want to either wait for a",
    "start": "1076000",
    "end": "1081240"
  },
  {
    "text": "training Epoch to finish or something to be checkpointed right before um draining the node and and migrating that workload",
    "start": "1081240",
    "end": "1088400"
  },
  {
    "text": "um so this will not suffice for for for managing GPU driver upgrades um what about the on delete",
    "start": "1088400",
    "end": "1095360"
  },
  {
    "text": "strategy right well that is sort of The Other Extreme right it gives you the full control so you can as an admin go",
    "start": "1095360",
    "end": "1100840"
  },
  {
    "text": "one by no one by one on your nodes uh Cordon them drain that or wait for jobs to finish and then initiate the upgrade",
    "start": "1100840",
    "end": "1108480"
  },
  {
    "text": "obvious obviously this is not this is entirely manual so we're not going to go with this approach but it's it's entirely valid this is what people do",
    "start": "1108480",
    "end": "1114840"
  },
  {
    "text": "with our drivers if they deploy via demon set they typically do this approach right very careful they have their own set of processes in place to",
    "start": "1114840",
    "end": "1121640"
  },
  {
    "text": "they know what's running they know which nodes are sort of ready to to upgrade and they'll go in and coordinate and and",
    "start": "1121640",
    "end": "1127440"
  },
  {
    "text": "and and Trigger upgrades themselves um but they may also build their own Automation and we don't want",
    "start": "1127440",
    "end": "1133400"
  },
  {
    "text": "teams sort of duplicating this right that we want to have a common solution to to to perform upgrades",
    "start": "1133400",
    "end": "1139400"
  },
  {
    "text": "so um how do we solve this problem so we in our operator built a controller to uh",
    "start": "1139400",
    "end": "1147640"
  },
  {
    "text": "facilitate this upgrade uh the core idea right is we configure our driver demon",
    "start": "1147640",
    "end": "1152960"
  },
  {
    "text": "sets with the on delete update strategy because we don't want the demon set controller to actually carry out the upgrade instead we have our controller",
    "start": "1152960",
    "end": "1160280"
  },
  {
    "text": "that goes in and cordin nodes um waits for jobs to finish which is a",
    "start": "1160280",
    "end": "1165840"
  },
  {
    "text": "configurable policy that you can have as an admin and and then right proceeds with the upgrade and what's nice is the",
    "start": "1165840",
    "end": "1172760"
  },
  {
    "text": "controller emits a metrics during uh the upgrade to give you some uh idea what the progress is and if any failures",
    "start": "1172760",
    "end": "1179159"
  },
  {
    "text": "happen on nodes um and like I said the upgrade behavior is configurable through through a",
    "start": "1179159",
    "end": "1185559"
  },
  {
    "text": "CR um something else to know is that we we worked with another team at Nvidia the the network operator team so they",
    "start": "1185559",
    "end": "1192240"
  },
  {
    "text": "their operator helps um configure all the networking stack for GPU direct RDMA",
    "start": "1192240",
    "end": "1197880"
  },
  {
    "text": "and so they also manage drivers and so they have the same problem and so we sort of collaborate on on a shared package that we can both use in our",
    "start": "1197880",
    "end": "1203799"
  },
  {
    "text": "controllers to manage upgrades this is just an example of some",
    "start": "1203799",
    "end": "1209400"
  },
  {
    "text": "configuration you can do um so you can have like you know what what how many parallel upgrades you you're okay with",
    "start": "1209400",
    "end": "1215600"
  },
  {
    "text": "having at one particular time Max on availability of the cluster during an upgrade some some basic configuration in",
    "start": "1215600",
    "end": "1221480"
  },
  {
    "text": "terms of what pods you want to wait to complete for before proceeding with an upgrade and so an empty selector means",
    "start": "1221480",
    "end": "1227520"
  },
  {
    "text": "wait for for all gpus jobs to finish and you can specify a timeout so if you're okay waiting forever wait forever",
    "start": "1227520",
    "end": "1233520"
  },
  {
    "text": "otherwise specify something not zero um and some other you can opt into drain or disable drain if you don't want to drain",
    "start": "1233520",
    "end": "1239559"
  },
  {
    "text": "at all during an upgrade so here's a quick example of of what what a our controller will look",
    "start": "1239559",
    "end": "1246600"
  },
  {
    "text": "like when it's facilitating an upgrade so like I said we configure the demon set as on delete and we have our",
    "start": "1246600",
    "end": "1252880"
  },
  {
    "text": "controller that's going to facilitate this so uh the first thing that our controller does during reconciliation is",
    "start": "1252880",
    "end": "1258400"
  },
  {
    "text": "it goes through all of your nodes with drivers and checks is an upgrade",
    "start": "1258400",
    "end": "1263720"
  },
  {
    "text": "required right right now the drivers are in have the same revision as the parent demon set so the nothing to do right",
    "start": "1263720",
    "end": "1270480"
  },
  {
    "text": "they're they're they're in sync so we label uh we we we label each note with",
    "start": "1270480",
    "end": "1275640"
  },
  {
    "text": "like what state they're in so in this case their you know upgrade is done that's the that's the value of this label that we give on the noes so the",
    "start": "1275640",
    "end": "1282559"
  },
  {
    "text": "the the the label is just representing what what state the node is in the driver on the Node is in so right now",
    "start": "1282559",
    "end": "1288080"
  },
  {
    "text": "everything is is is done there's no nothing to do as an admin we go in and we upgrade",
    "start": "1288080",
    "end": "1294120"
  },
  {
    "text": "the the demon set and on the next reconciliation this label will change all of a sudden now the the Pod is out",
    "start": "1294120",
    "end": "1300640"
  },
  {
    "text": "of sync with the demon set and an upgrade is required so now these get",
    "start": "1300640",
    "end": "1305799"
  },
  {
    "text": "relabeled um the controller Will based on your policy will initiate a rolling upgrade so pick a pod or pick a node to",
    "start": "1305799",
    "end": "1313000"
  },
  {
    "text": "upgrade in this case it's picking the first one and the very first the next state is to coordin the node",
    "start": "1313000",
    "end": "1320039"
  },
  {
    "text": "after that we wait for GPU jobs to complete um based on the polic based on",
    "start": "1320039",
    "end": "1325960"
  },
  {
    "text": "your your configurable policy once that job completes we go to another state",
    "start": "1325960",
    "end": "1331080"
  },
  {
    "text": "which is sort of drain but just drain any any remaining GPU jobs if there are",
    "start": "1331080",
    "end": "1336400"
  },
  {
    "text": "any then restart the the driver pod so that the new one gets rolled",
    "start": "1336400",
    "end": "1341559"
  },
  {
    "text": "out do any validation to make sure that the new driver is is healthy uncor in the node and then",
    "start": "1341559",
    "end": "1348159"
  },
  {
    "text": "Market is done and then repeat so pretty",
    "start": "1348159",
    "end": "1354840"
  },
  {
    "text": "simple okay how are we on time pretty good so I'm just going to do",
    "start": "1354840",
    "end": "1360240"
  },
  {
    "text": "a quick demo showing both of the controllers to give you a better idea so I already recorded",
    "start": "1360240",
    "end": "1368080"
  },
  {
    "text": "this this is too small okay it might be a little little",
    "start": "1371919",
    "end": "1378559"
  },
  {
    "text": "small but I'm going to highlight some things and and walk you through it so I have a a cluster on AKs for this demo I",
    "start": "1378559",
    "end": "1386200"
  },
  {
    "text": "created two node pools so one is a T4 pool that has two nodes each with a T4",
    "start": "1386200",
    "end": "1392440"
  },
  {
    "text": "Nvidia T4 GPU and I have another pool with a V100 node pool with two nodes as",
    "start": "1392440",
    "end": "1399240"
  },
  {
    "text": "well each with a v00 so I gives me a total of four um GPU",
    "start": "1399240",
    "end": "1405120"
  },
  {
    "text": "nodes in my cluster and I have a a little metric on my dashboard just to show that I've also um beforehand I've",
    "start": "1405120",
    "end": "1412960"
  },
  {
    "text": "installed uh the latest GPU operator Helm chart um and I've actually already",
    "start": "1412960",
    "end": "1418520"
  },
  {
    "text": "sort of brought up the entire Stacks I've already installed drivers so what I'm going to show you is that uh this",
    "start": "1418520",
    "end": "1425279"
  },
  {
    "text": "metric is showing how many nodes have the 525 driver installed I have two and I have um a number of nodes with the 535",
    "start": "1425279",
    "end": "1432159"
  },
  {
    "text": "driver I also have two so I've basically deployed different driver versions on my different pools and we'll show that",
    "start": "1432159",
    "end": "1440440"
  },
  {
    "text": "momentarily so yeah I've already pre-created the driver Nvidia driver instances one named T4 for my T4 pool",
    "start": "1441760",
    "end": "1449000"
  },
  {
    "text": "right I'm deploying the 525 driver I'm also using pre-compiled drivers because I have images available for these",
    "start": "1449000",
    "end": "1454520"
  },
  {
    "text": "kernels so that's why we see two 525 drivers installed in my cluster for my",
    "start": "1454520",
    "end": "1460120"
  },
  {
    "text": "v100s same spec essentially but I'm installing the 535 driver also using pre-compiled images and that's why we",
    "start": "1460120",
    "end": "1466399"
  },
  {
    "text": "see two over here um if we inspect I'm going to inspect the uh the actual demon",
    "start": "1466399",
    "end": "1473279"
  },
  {
    "text": "set spec for one of these and show you the actual image tag that gets resolved",
    "start": "1473279",
    "end": "1478559"
  },
  {
    "text": "it has the the driver version as well as the the kernel version in it um so here",
    "start": "1478559",
    "end": "1483880"
  },
  {
    "text": "are the demon sets in our in our namespace we have a bunch of uh demon sets and then we also have two driver demon sets one is for each",
    "start": "1483880",
    "end": "1490559"
  },
  {
    "text": "pool and uh this is for the T4 this is a demon set for the T4 pool I'm installing",
    "start": "1490559",
    "end": "1496679"
  },
  {
    "text": "the 525 driver for this specific Azure kernel uh running a bunch of 20204 so",
    "start": "1496679",
    "end": "1502640"
  },
  {
    "text": "the operator automatically detects what kernel you're running in this pool and constructs the tag",
    "start": "1502640",
    "end": "1508600"
  },
  {
    "text": "appropriately um for the rest of my demo I'm actually going to trigger an upgrade for my T4 pool I'm going to move them",
    "start": "1508600",
    "end": "1514600"
  },
  {
    "text": "off of the 525 driver and onto the 535 driver right before I do that before I",
    "start": "1514600",
    "end": "1520480"
  },
  {
    "text": "start the upgrade I'm just going to launch a very silly job to keep my t4s",
    "start": "1520480",
    "end": "1525600"
  },
  {
    "text": "busy my GPU is busy so I'm just running a a a job that's going to sleep for three",
    "start": "1525600",
    "end": "1531120"
  },
  {
    "text": "minutes right um on both of my",
    "start": "1531120",
    "end": "1537559"
  },
  {
    "text": "t4s so I'll skip a little bit I think I yeah so the pods are running they're",
    "start": "1538399",
    "end": "1543480"
  },
  {
    "text": "going to be running for 3 minutes I'm going to go to my T4 instance and I'm going to manually right just change the",
    "start": "1543480",
    "end": "1549840"
  },
  {
    "text": "version from 525 to 535 to trigger an upgrade and you'll see me type",
    "start": "1549840",
    "end": "1557240"
  },
  {
    "text": "it immediately right my T4 instan is my T4 driver is not ready anymore uh right",
    "start": "1557240",
    "end": "1563760"
  },
  {
    "text": "the the spec is is out of sync with what's actually install in the nodes we see here we have so let me explain this",
    "start": "1563760",
    "end": "1569880"
  },
  {
    "text": "dashboard these are just metrics from our driver upgrade controller saying how many nodes uh upgrades are done for so",
    "start": "1569880",
    "end": "1576760"
  },
  {
    "text": "initially all four of my nodes are in sync so like the drivers are in sync with the spec the the the configuration",
    "start": "1576760",
    "end": "1582440"
  },
  {
    "text": "so initially right uh four of them were you know there was no you know no upgrades were pending and no upgrades",
    "start": "1582440",
    "end": "1588440"
  },
  {
    "text": "were in progress but now when I upgraded my T4 um now it shows that",
    "start": "1588440",
    "end": "1595080"
  },
  {
    "text": "two um two nodes only two nodes in my cluster have finished upgrading so two",
    "start": "1595080",
    "end": "1601200"
  },
  {
    "text": "two have upgrades required um shortly after we see our",
    "start": "1601200",
    "end": "1608679"
  },
  {
    "text": "upgrade controller has already cordoned one node so it's already picked one T4 node to start an upgrade and so one is",
    "start": "1608679",
    "end": "1616320"
  },
  {
    "text": "you know label as upgrade required the other one's already waiting for the GPU job to finish completing so it's sort of",
    "start": "1616320",
    "end": "1621840"
  },
  {
    "text": "stuck at that State um so I'm going to skip a little",
    "start": "1621840",
    "end": "1628960"
  },
  {
    "text": "bit so I'm sort of just wait waiting for my job to complete typing some commands",
    "start": "1628960",
    "end": "1634240"
  },
  {
    "text": "on the screen waiting eventually my job does complete",
    "start": "1634240",
    "end": "1642039"
  },
  {
    "text": "and the node gets moved to the next stage so it's actually restarting the driver pod I'm just showing that and",
    "start": "1642039",
    "end": "1647080"
  },
  {
    "text": "showing um the new driver being uh",
    "start": "1647080",
    "end": "1652080"
  },
  {
    "text": "deployed shortly after we see my gauge has changed so now there's only one 525 driver installed my cluster",
    "start": "1652840",
    "end": "1660760"
  },
  {
    "text": "um it's validating it's validating the new install it's done now my other gauge",
    "start": "1660760",
    "end": "1667360"
  },
  {
    "text": "has changed so now we have three instances of the 535 driver and uh my my",
    "start": "1667360",
    "end": "1672760"
  },
  {
    "text": "little charts here are updating um so three nodes are done we have one left to upgrade and this one",
    "start": "1672760",
    "end": "1679159"
  },
  {
    "text": "already is cordoned right the controller has already moved on to it um it should upgrade pretty quick because there's no",
    "start": "1679159",
    "end": "1685679"
  },
  {
    "text": "jobs running anymore so it already is getting restarted the driver",
    "start": "1685679",
    "end": "1691159"
  },
  {
    "text": "pod gauge has changed the other one's waiting to change as",
    "start": "1691159",
    "end": "1696398"
  },
  {
    "text": "well and there we go so everything's done all my charts are back to to what",
    "start": "1696559",
    "end": "1702480"
  },
  {
    "text": "they were initially and now we're all upgraded from 525 to 535 so",
    "start": "1702480",
    "end": "1710880"
  },
  {
    "text": "yeah okay just to summarize so managing device driver at scale is difficult",
    "start": "1729240",
    "end": "1735200"
  },
  {
    "text": "right um driver containers allow to manage the life cycle through kubernetes",
    "start": "1735200",
    "end": "1740919"
  },
  {
    "text": "which allows us to build some cool Solutions on top to automate some of the pain points right so what I demonstrated",
    "start": "1740919",
    "end": "1747440"
  },
  {
    "text": "in this talk is where we can deploy and manage different driver versions or configurations through controllers uh as",
    "start": "1747440",
    "end": "1754600"
  },
  {
    "text": "well as manage upgrades in a controlled yet automated",
    "start": "1754600",
    "end": "1760080"
  },
  {
    "text": "fashion so yeah so that concludes my talk I'm happy to answer any questions you guys",
    "start": "1760080",
    "end": "1766440"
  },
  {
    "text": "have",
    "start": "1767039",
    "end": "1770039"
  },
  {
    "text": "yeah I think we need to give you a mic first I have two questions the first one",
    "start": "1775080",
    "end": "1782799"
  },
  {
    "text": "is uh do the gpus require special uh firmware and how are firmware updates",
    "start": "1782799",
    "end": "1788960"
  },
  {
    "text": "handled and the second question is if secq boot is switched on you need to",
    "start": "1788960",
    "end": "1794039"
  },
  {
    "text": "sign the drivers how are the drivers signed",
    "start": "1794039",
    "end": "1799360"
  },
  {
    "text": "so yeah you do need firmware uh and the answer to how they are managed we don't manage firmware updates I think that's",
    "start": "1799360",
    "end": "1806320"
  },
  {
    "text": "something that's been asked but we haven't supported that with our operator right um so maybe that's something we",
    "start": "1806320",
    "end": "1812240"
  },
  {
    "text": "will consider supporting in the future or maybe having a separate operator dedicated to doing like firmware updates",
    "start": "1812240",
    "end": "1819000"
  },
  {
    "text": "uh on kubernetes and for secure boot uh yes you're right you have to have signed",
    "start": "1819000",
    "end": "1824320"
  },
  {
    "text": "driver packages so some when I was talking about pre-compile driver containers we for for a buntu canonical",
    "start": "1824320",
    "end": "1832200"
  },
  {
    "text": "signs are drivers so all of the tags that we have on our registry for pre-compiled driver images are actually",
    "start": "1832200",
    "end": "1837799"
  },
  {
    "text": "signed so if you run if you run them on a secure boot system with the buntu 2204 they'll be able to load just",
    "start": "1837799",
    "end": "1844320"
  },
  {
    "text": "fine um if you're running non- precompiled images you sort of have to",
    "start": "1844320",
    "end": "1849760"
  },
  {
    "text": "build you know add your your your trusted Keys into the image so at build",
    "start": "1849760",
    "end": "1855559"
  },
  {
    "text": "time they're signed but we not many people people are doing that typically pre-compiled is the way to go so once we",
    "start": "1855559",
    "end": "1861159"
  },
  {
    "text": "add more support for different distributions like real I think we're in discussions with them for how to get",
    "start": "1861159",
    "end": "1866279"
  },
  {
    "text": "them to sign the packages and give them to us to to build container images with them right",
    "start": "1866279",
    "end": "1872880"
  },
  {
    "text": "included yeah yeah hello um I I think in in the",
    "start": "1872880",
    "end": "1879440"
  },
  {
    "text": "second part of the talk when you started talking about the demon set the drivers you were referring to were the user",
    "start": "1879440",
    "end": "1886840"
  },
  {
    "text": "space driver drivers right the Cardinal mods K mods you still need to to add",
    "start": "1886840",
    "end": "1894559"
  },
  {
    "text": "those to the Cardinal in the node itself so everything I showed is both so",
    "start": "1894559",
    "end": "1900240"
  },
  {
    "text": "like the when I was doing an upgrade I was actually unloading the old version of the kernel module and then loading",
    "start": "1900240",
    "end": "1906600"
  },
  {
    "text": "the new version as well as having making the new version of the driver libraries available on the host and the demon set",
    "start": "1906600",
    "end": "1912600"
  },
  {
    "text": "does all that takes care of it's running as privileged because it has to load and unload kernel modules but it's is doing",
    "start": "1912600",
    "end": "1918519"
  },
  {
    "text": "that yeah oh and that's why you said that the path needs to be exposed to the",
    "start": "1918519",
    "end": "1923799"
  },
  {
    "text": "kernel right so before to the operating system that's the part where you said",
    "start": "1923799",
    "end": "1928919"
  },
  {
    "text": "that you need to show what's inside container to the operating system um the reason I mentioned like a",
    "start": "1928919",
    "end": "1936679"
  },
  {
    "text": "host path volume so mounting something from the container to the host is so that you can make the driver",
    "start": "1936679",
    "end": "1942320"
  },
  {
    "text": "installation so everything that device nodes for example and um like like shared libraries they need to be",
    "start": "1942320",
    "end": "1948440"
  },
  {
    "text": "somewhere on the host that you can share them with other containers that want to use the device and the driver okay",
    "start": "1948440",
    "end": "1955679"
  },
  {
    "text": "thanks",
    "start": "1955679",
    "end": "1958398"
  },
  {
    "text": "y hi uh do you think that there could be any issues updating the network",
    "start": "1964159",
    "end": "1971760"
  },
  {
    "text": "driver the same approach as he used to update the video driver maybe we could",
    "start": "1971760",
    "end": "1977279"
  },
  {
    "text": "expand it and upgrade the network card as well do you see any challenges with",
    "start": "1977279",
    "end": "1982480"
  },
  {
    "text": "that I mean like I mentioned our Network operator team that installs like the the mofed stack they're they're using this",
    "start": "1982480",
    "end": "1989240"
  },
  {
    "text": "approach just fine but i' I'd rather have you reach out to them to see if they has any particular challenges there",
    "start": "1989240",
    "end": "1995799"
  },
  {
    "text": "that we don't encounter for GPU drivers because I'm not aware not",
    "start": "1995799",
    "end": "2001000"
  },
  {
    "text": "today actually on my side I also have like two two questions for you so we are already using this approach for almost a",
    "start": "2001000",
    "end": "2007600"
  },
  {
    "text": "year here and then was kind of for cash but then uh for example now the 2410 the",
    "start": "2007600",
    "end": "2013440"
  },
  {
    "text": "new buntu version will be available I mean in almost a month right how fast are you providing drivers in in your",
    "start": "2013440",
    "end": "2021480"
  },
  {
    "text": "Nvidia registry and now coming uh the second one is also related slightly to the",
    "start": "2021480",
    "end": "2027320"
  },
  {
    "text": "secure boot uh yes you have some sign drivers in the registry but I mean not",
    "start": "2027320",
    "end": "2032679"
  },
  {
    "text": "all the drivers are signed so really a lot of them are not signed and then for example for big company",
    "start": "2032679",
    "end": "2038360"
  },
  {
    "text": "it's actually difficult because for example we have Services which are using GPU drivers across I don't know 20 teams",
    "start": "2038360",
    "end": "2046440"
  },
  {
    "text": "right so then we always want to be in sync but then in some parts we cannot because actually the drivers are not",
    "start": "2046440",
    "end": "2052118"
  },
  {
    "text": "signed and it's always like a challenge for us to do it so do do you have any plans to try to sign all the drives",
    "start": "2052119",
    "end": "2059560"
  },
  {
    "text": "because I don't see any downside on your side as long as we get packages from",
    "start": "2059560",
    "end": "2067520"
  },
  {
    "text": "like like operating system vendors that are signed like canonical or red hat but these are the main distributions that we",
    "start": "2067520",
    "end": "2072960"
  },
  {
    "text": "support as long as we get packages from them then we are all on board to provide images that contain those signed",
    "start": "2072960",
    "end": "2078800"
  },
  {
    "text": "pre-compiled packages um for a bunto we do so that's why we've been automatically sort of when when new",
    "start": "2078800",
    "end": "2084679"
  },
  {
    "text": "kernels come out we are publishing tags right we have some automation to to do that relatively quickly after a new",
    "start": "2084679",
    "end": "2090480"
  },
  {
    "text": "kernel releases for Red Hat we're still sort of in the process of getting",
    "start": "2090480",
    "end": "2095560"
  },
  {
    "text": "getting that sorted out so it may take some time I forgot your first question so I don't I'm sorry about",
    "start": "2095560",
    "end": "2103800"
  },
  {
    "text": "that you can catch me out there if you want to ask your first question know who else hi uh may I inquire question about",
    "start": "2103800",
    "end": "2113200"
  },
  {
    "text": "uh uh did you suffer some uh device uh",
    "start": "2113200",
    "end": "2118640"
  },
  {
    "text": "number coordination Challenge and also the update come with the system crash",
    "start": "2118640",
    "end": "2126160"
  },
  {
    "text": "and the any uh uh R back suggestion",
    "start": "2126160",
    "end": "2132480"
  },
  {
    "text": "yeah so what do you mean by device number um issue like device major and",
    "start": "2132480",
    "end": "2138720"
  },
  {
    "text": "minor number issues yeah yeah yeah because um we used to to update the phone world and uh uh sometimes we we",
    "start": "2138720",
    "end": "2146520"
  },
  {
    "text": "need to identify with the serial number and the device number like kind but uh",
    "start": "2146520",
    "end": "2153440"
  },
  {
    "text": "uh in container uh some way will come with fail so",
    "start": "2153440",
    "end": "2160920"
  },
  {
    "text": "yeah um I'm not I'm not aware of any issues but maybe you can give me more context after about what exact numbers",
    "start": "2161319",
    "end": "2168880"
  },
  {
    "text": "were different in the container than on the host um what was your what was your",
    "start": "2168880",
    "end": "2174079"
  },
  {
    "text": "second question U after iate the phone Weare um",
    "start": "2174079",
    "end": "2180640"
  },
  {
    "text": "uh is there any uh r r suggestion can be",
    "start": "2180640",
    "end": "2186319"
  },
  {
    "text": "can you suggest in the container dcker fa to do the",
    "start": "2186319",
    "end": "2193760"
  },
  {
    "text": "preparation for firware avoid after excusion come with the uh system crash",
    "start": "2193760",
    "end": "2200720"
  },
  {
    "text": "yeah um for driver upgrades I mean we can do a a roll back with the demon set",
    "start": "2200720",
    "end": "2207599"
  },
  {
    "text": "like rolling back the version of demon set for if there's a system crash we don't really have good failure recovery",
    "start": "2207599",
    "end": "2214280"
  },
  {
    "text": "currently with with GPU nodes like our device plug-in sort of marks your note as unhealthy if there's some sort of",
    "start": "2214280",
    "end": "2220640"
  },
  {
    "text": "driver or Hardware issue um sort of marks as unhealthy so you can't schedule jobs on it and it requires you to sort",
    "start": "2220640",
    "end": "2226880"
  },
  {
    "text": "of manually go in and fix the problem and then bring back the device plugin so we don't really have good failure",
    "start": "2226880",
    "end": "2232839"
  },
  {
    "text": "recovery if that's if that's answers your question I",
    "start": "2232839",
    "end": "2237119"
  },
  {
    "text": "hope all right I think I'm getting kicked out so thank you all for attending appreciate it",
    "start": "2241760",
    "end": "2249520"
  }
]