[
  {
    "text": "hi everyone Um thank you for being here today Uh we're going to talk about the",
    "start": "80",
    "end": "6560"
  },
  {
    "text": "quest for GPU custody and how we want to use DRA for efficient GPU",
    "start": "6560",
    "end": "11800"
  },
  {
    "text": "sharing The session will be presented by me My name is Donna I work at CERN I'm a computing engineer and Yan Philip over",
    "start": "11800",
    "end": "19520"
  },
  {
    "text": "here who's a principal engineer at Nvidia All right So let's start with the",
    "start": "19520",
    "end": "25199"
  },
  {
    "text": "easy part What is CERN so CERN is the European Organization for Nuclear",
    "start": "25199",
    "end": "30320"
  },
  {
    "text": "Research This is a very special place located both in France and Switzerland",
    "start": "30320",
    "end": "35600"
  },
  {
    "text": "and it hosts the biggest particle accelerator there is Uh the particle accelerator is called LHC or Large",
    "start": "35600",
    "end": "42399"
  },
  {
    "text": "Hadron Collider and it's a 27 kilometers long tunnel underground where particle",
    "start": "42399",
    "end": "47840"
  },
  {
    "text": "beams are accelerated to close to the speed of light and they are made to collide They're collided at four points",
    "start": "47840",
    "end": "55120"
  },
  {
    "text": "along this ring where which is where we have detectors So CMS LHCB Atlas and",
    "start": "55120",
    "end": "62199"
  },
  {
    "text": "Alice So as I said at CERN we specialize in particles And when particle beams are",
    "start": "62199",
    "end": "69119"
  },
  {
    "text": "made to collide a lot of data is being produced Of course we cannot store all",
    "start": "69119",
    "end": "74640"
  },
  {
    "text": "the data that is being produced So we have systems in place which are called triggers uh that use simple criteria to",
    "start": "74640",
    "end": "82000"
  },
  {
    "text": "decide which information to store or discard Once we decided which information we want to store for our",
    "start": "82000",
    "end": "88400"
  },
  {
    "text": "long-term storage we need a way to actually process all this data we need a",
    "start": "88400",
    "end": "93759"
  },
  {
    "text": "way to make sense of all these particles And in order to do this we use",
    "start": "93759",
    "end": "99799"
  },
  {
    "text": "chips Um yeah I'm this is wrong chips No no no We're in UK So let's fix this",
    "start": "99799",
    "end": "106159"
  },
  {
    "text": "We're gonna fix this Uh let's see if it works Okay Yeah Perfect Perfect Right Um",
    "start": "106159",
    "end": "114880"
  },
  {
    "text": "right No I mean I love potato chips but I think GPU chips are much better So for",
    "start": "114880",
    "end": "120479"
  },
  {
    "text": "this presentation we're going to stick to GPUs for now Uh jokes aside we do",
    "start": "120479",
    "end": "126000"
  },
  {
    "text": "need GPUs and we need a lot of them and they're expensive They're hard to get",
    "start": "126000",
    "end": "131360"
  },
  {
    "text": "And as a result at CERN we decided to centralize our GPUs into a centralized GPU platform So basically what it means",
    "start": "131360",
    "end": "139120"
  },
  {
    "text": "it means that we will have a GPUs in the same common pool uh for simulations",
    "start": "139120",
    "end": "144640"
  },
  {
    "text": "inference CI runners training or anything else you can think of As a user you request a GPU uh you come to this",
    "start": "144640",
    "end": "152080"
  },
  {
    "text": "pool you get the GPU you use it but once you are done you release it back to the pool so that other users can use it as",
    "start": "152080",
    "end": "158480"
  },
  {
    "text": "well Uh this means the GPUs are always in use which is very good but also it means",
    "start": "158480",
    "end": "164959"
  },
  {
    "text": "that we can extend our offering in terms of accelerators beyond what we have on premises and also have accelerators on",
    "start": "164959",
    "end": "171840"
  },
  {
    "text": "public clouds and this is especially important for accelerators that we can never get on premises in",
    "start": "171840",
    "end": "179160"
  },
  {
    "text": "general No perfect the GPUs are always in use The problem is that still even",
    "start": "179160",
    "end": "184879"
  },
  {
    "text": "though they're technically in use uh many workloads have sub-optimal code many users were they're just doing",
    "start": "184879",
    "end": "191840"
  },
  {
    "text": "development work So there is actually not a lot of activity on the GPU itself",
    "start": "191840",
    "end": "197040"
  },
  {
    "text": "and sometimes the workloads are just spiky and all of this introduces a lot",
    "start": "197040",
    "end": "202239"
  },
  {
    "text": "of idle time into our system And to fight it we want to use GPU sharing so",
    "start": "202239",
    "end": "208000"
  },
  {
    "text": "that we make sure that we minimize the idle time that we have in our",
    "start": "208000",
    "end": "213080"
  },
  {
    "text": "system To actually do GPU sharing there are multiple options One of them is time slicing With time slicing theuler gives",
    "start": "213080",
    "end": "221120"
  },
  {
    "text": "an equal amount of time to all the GPU processes that are scheduled on the GPU",
    "start": "221120",
    "end": "227200"
  },
  {
    "text": "uh the implication is that because only one process is running on the GPU at any point in time uh we have an overhead",
    "start": "227200",
    "end": "234720"
  },
  {
    "text": "because of a context switching and this is something we cannot ignore Another thing to keep in mind is that with time",
    "start": "234720",
    "end": "240480"
  },
  {
    "text": "slicing the memory is divided between all the processes So in theory one process can starve other processes that",
    "start": "240480",
    "end": "247519"
  },
  {
    "text": "are running on the same GPU Configuring this is rather easy So",
    "start": "247519",
    "end": "253200"
  },
  {
    "text": "with your GPU operator the device plug-in will reference a config map And in this config map you can add",
    "start": "253200",
    "end": "259120"
  },
  {
    "text": "configuration for time slicing So in this case I'm saying slice 4 will mean that I want GPU sharing with time",
    "start": "259120",
    "end": "266000"
  },
  {
    "text": "slicing into four replicas Now I don't do anything with this configuration by",
    "start": "266000",
    "end": "272080"
  },
  {
    "text": "default It's just in my cluster But to actually use it I need to decide which GPU I want to share and find out what is",
    "start": "272080",
    "end": "279280"
  },
  {
    "text": "the node hosting this GPU And once I know this information I can go ahead and",
    "start": "279280",
    "end": "284479"
  },
  {
    "text": "label my node saying that the configuration for the device plugin needs to be in this case slice 4 And",
    "start": "284479",
    "end": "290160"
  },
  {
    "text": "when I do this my node will pick it up and instead of advertising one GPU that",
    "start": "290160",
    "end": "295360"
  },
  {
    "text": "can be allocated it will start advertising my shared resource",
    "start": "295360",
    "end": "300680"
  },
  {
    "text": "instead Uh NPS is a bit different If with NPS we have spatial sharing So in",
    "start": "300680",
    "end": "306240"
  },
  {
    "text": "this case all the processes are running on the GPU at the same time and also we get an extra process which makes sure",
    "start": "306240",
    "end": "313199"
  },
  {
    "text": "that every client consumes only the compute and the memory that it is allowed to consume And in this case we",
    "start": "313199",
    "end": "320080"
  },
  {
    "text": "don't have context switching anymore Configuring it is very similar",
    "start": "320080",
    "end": "325120"
  },
  {
    "text": "to time slicing It's just instead of sharing with time slicing I'm saying that I want sharing with MPS and then",
    "start": "325120",
    "end": "330960"
  },
  {
    "text": "everything rest is the same I just again go and I label my node Very important you need to label your node And once you",
    "start": "330960",
    "end": "337600"
  },
  {
    "text": "do this you'll see that your node will start advertising your shared resource instead of a full",
    "start": "337600",
    "end": "343800"
  },
  {
    "text": "GPU And lastly we have multi- instance GPU or MIG And with MIG uh we have",
    "start": "343800",
    "end": "350240"
  },
  {
    "text": "partitioning into up to seven uh independent isolated instances isolated",
    "start": "350240",
    "end": "356080"
  },
  {
    "text": "in terms of memory cache compute cores and in this case all the processes are",
    "start": "356080",
    "end": "361199"
  },
  {
    "text": "running on the GPU at the same time but they cannot influence each other anymore which is something that is very",
    "start": "361199",
    "end": "368680"
  },
  {
    "text": "convenient Uh configuring kit is similar to time slicing and MPS but still a little bit different In the ca in this",
    "start": "368680",
    "end": "375600"
  },
  {
    "text": "case we have a MIG manager and in the config map that we referenced in our GPU",
    "start": "375600",
    "end": "380800"
  },
  {
    "text": "operator we're going to say that these are the layouts that we want to allow in our cluster for MIG uh in this case I'm",
    "start": "380800",
    "end": "388080"
  },
  {
    "text": "saying I want one per one one instance of 3G 20GB and I want two instances of",
    "start": "388080",
    "end": "393120"
  },
  {
    "text": "2G 10GB and then I go ahead and I find the node that has a MIG capable GPU and",
    "start": "393120",
    "end": "399600"
  },
  {
    "text": "I'm labeling this node and I will see in maybe up to a minute because in case of",
    "start": "399600",
    "end": "404880"
  },
  {
    "text": "MIG some processes need to be restarted uh but in no time I'll see that my node starts advertising the MIG instances",
    "start": "404880",
    "end": "411360"
  },
  {
    "text": "instead of a full GPU uh one thing to note here is that you are already running process that are using the GPU",
    "start": "411360",
    "end": "418319"
  },
  {
    "text": "you'll have to make sure that you evict them first All right so I think everyone can",
    "start": "418319",
    "end": "424160"
  },
  {
    "text": "agree that sharing is fairly easy It usually works It's very well documented",
    "start": "424160",
    "end": "429599"
  },
  {
    "text": "is available but if it all be green we wouldn't be here today talking about a",
    "start": "429599",
    "end": "436319"
  },
  {
    "text": "different way of doing things And the reality is that there are some pain points that we cannot ignore",
    "start": "436319",
    "end": "442639"
  },
  {
    "text": "So pain point number one is that partitioning repartitioning those are manual steps for the system",
    "start": "442639",
    "end": "449080"
  },
  {
    "text": "administrators So in my case I need to think in advance how many GPUs I want to make available partitioned with MIG how",
    "start": "449080",
    "end": "456240"
  },
  {
    "text": "many GPUs I want to have available in my system as full GPUs available to my users and how many I want to have time",
    "start": "456240",
    "end": "462479"
  },
  {
    "text": "slice and so on and every time this changes someone needs to go and releabel the node so that this can take into",
    "start": "462479",
    "end": "469039"
  },
  {
    "text": "action and then we can satisfy the needs of our users Another option would have to have an external solution like an",
    "start": "469039",
    "end": "476160"
  },
  {
    "text": "external software that monitors the requests monitors the users and then decides on the flight how to configure",
    "start": "476160",
    "end": "482960"
  },
  {
    "text": "the GPU into your cluster But of course this is far from ideal as well because it's an external thing that needs to be",
    "start": "482960",
    "end": "490280"
  },
  {
    "text": "supported Another pain point is that the configuration is per node So as I said we need to go and label the node This is",
    "start": "490280",
    "end": "497280"
  },
  {
    "text": "fine if you have one GPU per one node The reality is that this is not always",
    "start": "497280",
    "end": "502319"
  },
  {
    "text": "the case In our case at CERN we have nodes that have two four or even eight",
    "start": "502319",
    "end": "507639"
  },
  {
    "text": "GPUs And I don't want to configure all the GPUs on the node in the same way If",
    "start": "507639",
    "end": "513518"
  },
  {
    "text": "I want to enable MIG on only one GPU I go and I label my node and actually eight GPUs are configured with MIG using",
    "start": "513519",
    "end": "520959"
  },
  {
    "text": "the same layout which is not what I want I want a way to be able to manage GPUs",
    "start": "520959",
    "end": "527000"
  },
  {
    "text": "individually and I want a way to be able to manage GPUs dynamically and this is where DRA comes",
    "start": "527000",
    "end": "533680"
  },
  {
    "text": "into handy So with the array I do not need to preconfigure and prepartition my",
    "start": "533680",
    "end": "539680"
  },
  {
    "text": "GPUs they can be available in the system as full dedicated GPUs and then when",
    "start": "539680",
    "end": "545600"
  },
  {
    "text": "there is a user that requests the GPU in a certain way this can be done dynamically so that the user gets access",
    "start": "545600",
    "end": "552480"
  },
  {
    "text": "to exactly what they need and since we don't need to preconfigure or pre-artition anything",
    "start": "552480",
    "end": "559440"
  },
  {
    "text": "the problem with per node configuration disappears as well so basically before the array I was",
    "start": "559440",
    "end": "566160"
  },
  {
    "text": "thinking in advance and making sure like okay I have so many GPUs available in a certain way Now I just have all the GPUs",
    "start": "566160",
    "end": "572959"
  },
  {
    "text": "I can get in the system and then where is the request they're being partitioned in the way that they need to be",
    "start": "572959",
    "end": "578720"
  },
  {
    "text": "partition to satisfy the needs of my users So with this in mind let's find out more about the array and why this is",
    "start": "578720",
    "end": "586080"
  },
  {
    "text": "the way to do it Cool Thank you Diana So uh I'm Yan Phillip I very",
    "start": "586080",
    "end": "592320"
  },
  {
    "text": "recently joined Nvidia and it's my pleasure to give you a brief quick overview over dynamic resource",
    "start": "592320",
    "end": "599760"
  },
  {
    "text": "allocation or DRA for short Um is John here like I I like one of John's uh",
    "start": "599760",
    "end": "606240"
  },
  {
    "text": "mission statements from one of the the talks I watched from last year Um enra",
    "start": "606240",
    "end": "612320"
  },
  {
    "text": "enables simple and efficient uh configuration and sharing and allocation of accelerators and other specialized",
    "start": "612320",
    "end": "619519"
  },
  {
    "text": "devices Uh another more bold mission statement I think it was actually Pat who said this in one of the talks uh I",
    "start": "619519",
    "end": "626720"
  },
  {
    "text": "watched to get up to speed with DRA Um as DRA is here to redefine Kubernetes",
    "start": "626720",
    "end": "632720"
  },
  {
    "text": "relationship with hardware Um quickly about some key people",
    "start": "632720",
    "end": "638399"
  },
  {
    "text": "involved Uh this has mainly been John from Google I don't know if you're here Uh Patrick and and Kevin from Nvidia um",
    "start": "638399",
    "end": "646160"
  },
  {
    "text": "who have been um yeah driving this initiative over the last few years and",
    "start": "646160",
    "end": "651200"
  },
  {
    "text": "there is a a device management working group uh formed last year um with the",
    "start": "651200",
    "end": "657279"
  },
  {
    "text": "mission to to bring uh DRA towards GA",
    "start": "657279",
    "end": "663000"
  },
  {
    "text": "um I'm oversimplifying here but I think that's a good way to look at it what",
    "start": "663399",
    "end": "668720"
  },
  {
    "text": "what does DRA add really at the high level there are two relevant dimensions to this for for device vendor ers",
    "start": "668720",
    "end": "675839"
  },
  {
    "text": "there's a new way to describe devices And um as users you you will probably",
    "start": "675839",
    "end": "681120"
  },
  {
    "text": "also not even look at this As as users you will mainly care about uh the new way or ways to request devices um or or",
    "start": "681120",
    "end": "688800"
  },
  {
    "text": "groups of devices even or or fractions thereof Um and um just quickly to cover the describing",
    "start": "688800",
    "end": "696240"
  },
  {
    "text": "devices part it's it's important to know that um the task of of doing the device",
    "start": "696240",
    "end": "704079"
  },
  {
    "text": "description is of course vendor specific and vendors need to add their own drivers We call them DRA drivers And uh",
    "start": "704079",
    "end": "710160"
  },
  {
    "text": "without drivers DRA doesn't really do anything That's just a very important thing to understand about DRA There are",
    "start": "710160",
    "end": "716079"
  },
  {
    "text": "a few drivers out there in the world Intel is building drivers for their hardware We at NVIDIA are building the",
    "start": "716079",
    "end": "721440"
  },
  {
    "text": "DA driver for GPUs There's a a reference implementation I've been linking here that I think mainly Kevin and Patek have",
    "start": "721440",
    "end": "728480"
  },
  {
    "text": "been building Um beyond that in my own efforts to search the web I think there",
    "start": "728480",
    "end": "733680"
  },
  {
    "text": "is not much yet Is that right um but yeah we're hoping to to grow that ecosystem of",
    "start": "733680",
    "end": "740519"
  },
  {
    "text": "course Um most importantly DR is there It's ready to be tried out by all of you",
    "start": "740519",
    "end": "747360"
  },
  {
    "text": "it's still hidden behind a feature gate How you enable that depends on how you deploy Kubernetes Um we have managed to",
    "start": "747360",
    "end": "755279"
  },
  {
    "text": "promote DRA towards beta in back in December and uh we're on our way to to",
    "start": "755279",
    "end": "761120"
  },
  {
    "text": "G8 hopefully by the end of the year Diana described quite vividly what",
    "start": "761120",
    "end": "767040"
  },
  {
    "text": "the limitations are of the system we largely all of us use today The device",
    "start": "767040",
    "end": "772240"
  },
  {
    "text": "plug-in approach like the current or traditional way of doing things Maybe in my own words uh that interface is is",
    "start": "772240",
    "end": "778320"
  },
  {
    "text": "highly limited There are three degrees of freedom You have a resource name a resource count and everything else you",
    "start": "778320",
    "end": "785760"
  },
  {
    "text": "you want to do in terms of mapping complexity uh you have to squeeze into node labels and corresponding node",
    "start": "785760",
    "end": "792079"
  },
  {
    "text": "selectors And that's like actually I'm I'm surprised that we got so far with",
    "start": "792079",
    "end": "797760"
  },
  {
    "text": "that very limited model because yeah you you just cannot map device properties",
    "start": "797760",
    "end": "803600"
  },
  {
    "text": "like individual device properties and you also cannot uh really meaningfully",
    "start": "803600",
    "end": "809519"
  },
  {
    "text": "describe any more abstract resources So a little more GPU oriented",
    "start": "809519",
    "end": "816480"
  },
  {
    "text": "point of view on those limitations you can only really have one type of GPU per node If you have",
    "start": "816480",
    "end": "823600"
  },
  {
    "text": "different types there's no way to map that in a meaning meaningful way Uh you as a user you cannot really ask for",
    "start": "823600",
    "end": "829920"
  },
  {
    "text": "something like give me a device with at least so and so much memory uh you cannot also I mean Nvidia GPU hardware",
    "start": "829920",
    "end": "837680"
  },
  {
    "text": "has really nice features like for example those multi-instance GPUs where you can actually split a device into you",
    "start": "837680",
    "end": "845199"
  },
  {
    "text": "know mini GPUs so to say but in Kubernetes there's no meaningful way to ask for this so far and uh you also",
    "start": "845199",
    "end": "852800"
  },
  {
    "text": "cannot request device groups and when it's about sharing like control sharing of an individual device between say a",
    "start": "852800",
    "end": "860880"
  },
  {
    "text": "couple of containers or pods there is no meaningful way to do this so far and DRA",
    "start": "860880",
    "end": "866160"
  },
  {
    "text": "addresses all these well-known limitations systematically So it enables making",
    "start": "866160",
    "end": "872399"
  },
  {
    "text": "proper use of MC devices like the dynamic subdivision of large devices is now possible You can request a specific",
    "start": "872399",
    "end": "878800"
  },
  {
    "text": "device configuration which is then uh actuated for you on the fly meaning including proper cleanup after your",
    "start": "878800",
    "end": "885279"
  },
  {
    "text": "workload completes you can request groups of devices and um DRRA also",
    "start": "885279",
    "end": "891440"
  },
  {
    "text": "enables um the the concept of a resource that is not tied to the concept of a",
    "start": "891440",
    "end": "897440"
  },
  {
    "text": "node anymore like the node is not as important anymore We can define single abstract resources that are spanning",
    "start": "897440",
    "end": "904000"
  },
  {
    "text": "multiple nodes or even something that I think John called a node resource if I",
    "start": "904000",
    "end": "910000"
  },
  {
    "text": "listen to him properly Um as users you would mainly care about the",
    "start": "910000",
    "end": "916560"
  },
  {
    "text": "new primitives for uh requesting devices and that's largely the resource claim",
    "start": "916560",
    "end": "922560"
  },
  {
    "text": "which is introduced by DRA and the corresponding resource claim template and from here uh I like for the",
    "start": "922560",
    "end": "931680"
  },
  {
    "text": "for the rest of the of the talk I want to take the time to to go through four examples and uh hopefully also have the",
    "start": "931680",
    "end": "939279"
  },
  {
    "text": "time to conclude that with a little demo So um the first example is really just a",
    "start": "939279",
    "end": "947199"
  },
  {
    "text": "a very very basic example for controlled sharing of a single GPU device among two",
    "start": "947199",
    "end": "954160"
  },
  {
    "text": "containers uh compared to the traditional way like we are all very aware that it looks a",
    "start": "954160",
    "end": "961839"
  },
  {
    "text": "little more bulky but let's walk through it step by step Do I have a pointer no I don't Okay",
    "start": "961839",
    "end": "968880"
  },
  {
    "text": "no pointer On the right hand side um we can see both containers referring to a",
    "start": "968880",
    "end": "974079"
  },
  {
    "text": "claim with a with a name called GPU right and that and that claim is defined at the bottom of the",
    "start": "974079",
    "end": "980600"
  },
  {
    "text": "YAML And uh it's important to know that in DRA a resource claim represents one",
    "start": "980600",
    "end": "985920"
  },
  {
    "text": "one single physical resource So that's that's the systematic precise way of saying I want both containers use the",
    "start": "985920",
    "end": "992639"
  },
  {
    "text": "same device and the and that device through a layer of indirection is actually defined by the resource claim",
    "start": "992639",
    "end": "999040"
  },
  {
    "text": "resource claim template and it's basically just saying give me one GPU one Nvidia GPU",
    "start": "999040",
    "end": "1005360"
  },
  {
    "text": "um um let's have a look at the output if we run this like really from both",
    "start": "1005360",
    "end": "1010480"
  },
  {
    "text": "containers point of view uh this is the same device as identified by device uu",
    "start": "1010480",
    "end": "1017880"
  },
  {
    "text": "ID um so in this case maybe the traditional way would have been a little",
    "start": "1017880",
    "end": "1023759"
  },
  {
    "text": "uh more compact to describe a similar thing but it would not be as precise and",
    "start": "1023759",
    "end": "1028959"
  },
  {
    "text": "the additional verbosity allows for flexibility that we will see in the the next example for example we don't need",
    "start": "1028959",
    "end": "1035360"
  },
  {
    "text": "to understand all details here and I'm also not showing um a workload consuming this but but you can see that you can",
    "start": "1035360",
    "end": "1042880"
  },
  {
    "text": "you can put complexity and flexibility in the in the way to request devices so here we we have two things going on that",
    "start": "1042880",
    "end": "1049919"
  },
  {
    "text": "are interesting uh we can we can see that there's um a specific configuration",
    "start": "1049919",
    "end": "1055520"
  },
  {
    "text": "of the device that we're requesting towards the bottom of the document in terms of in this case time slicing where",
    "start": "1055520",
    "end": "1060640"
  },
  {
    "text": "we don't need to really understand that now It's just that we we're asking for a specifically configured device and that's actuated by the system And then",
    "start": "1060640",
    "end": "1068160"
  },
  {
    "text": "we can uh in this case we can see that we can also apply um complex logic um in",
    "start": "1068160",
    "end": "1075360"
  },
  {
    "text": "this case asking for a specific product name that matches a a text",
    "start": "1075360",
    "end": "1082080"
  },
  {
    "text": "pattern Third example and this I think the one we should really",
    "start": "1082200",
    "end": "1090039"
  },
  {
    "text": "uh yeah appreciate in terms of the value added Um this the first two examples I",
    "start": "1090039",
    "end": "1097039"
  },
  {
    "text": "was showing are working as of today with Kubernetes 132 and our most recent DRA",
    "start": "1097039",
    "end": "1104000"
  },
  {
    "text": "driver release This one here will only work in the in Kubernetes 133 Uh and it",
    "start": "1104000",
    "end": "1110000"
  },
  {
    "text": "will allow for dynamically provisioning a MC device based on the user's request",
    "start": "1110000",
    "end": "1115200"
  },
  {
    "text": "Again Mick is this and and Diana also mentioned this is this is this very uh smart way of being able to split a",
    "start": "1115200",
    "end": "1121679"
  },
  {
    "text": "powerful GPU into you know smaller fractions and um traditionally it",
    "start": "1121679",
    "end": "1128480"
  },
  {
    "text": "requires some kind of manual pre-provisioning and that's of course something that the orchestration layer",
    "start": "1128480",
    "end": "1133840"
  },
  {
    "text": "can do for you and uh in in Kubernetes 133 and the corresponding additions to",
    "start": "1133840",
    "end": "1139520"
  },
  {
    "text": "DRA we will finally get there so that when you request uh say a mini GPU of",
    "start": "1139520",
    "end": "1146559"
  },
  {
    "text": "you know these are arbitrary units almost you know 4G compute and 20 GB of memory then this is going to be set up",
    "start": "1146559",
    "end": "1153120"
  },
  {
    "text": "for the workload and the that corresponding um state is also torn down properly when",
    "start": "1153120",
    "end": "1160000"
  },
  {
    "text": "the workload finishes and um Diana added chips and so",
    "start": "1160000",
    "end": "1168720"
  },
  {
    "text": "just to honor the holy land that we're meeting in I wanted to add some",
    "start": "1168720",
    "end": "1175600"
  },
  {
    "text": "fish at this point So not lose your attention Um the fourth example is maybe",
    "start": "1175960",
    "end": "1184080"
  },
  {
    "text": "a little more involved technically but it's I hope I'm I'm not losing you on this because it's a very important",
    "start": "1184080",
    "end": "1189840"
  },
  {
    "text": "technical thing Um as I hope most of you know that and for for bigger training",
    "start": "1189840",
    "end": "1195440"
  },
  {
    "text": "workloads what's really important is to have communicate have GPUs communicate across machine boundaries at at really",
    "start": "1195440",
    "end": "1201919"
  },
  {
    "text": "high bandwidth right and uh in especially in this um in the compute",
    "start": "1201919",
    "end": "1208240"
  },
  {
    "text": "racks that that Nvidia is building those GB200 NVL uh 72 uh we have uh a number",
    "start": "1208240",
    "end": "1215679"
  },
  {
    "text": "of racks but but all the GPUs in there they can they can communicate at really high bandwidth The question is just how",
    "start": "1215679",
    "end": "1221200"
  },
  {
    "text": "do we expose that in in Kubernetes in a way that it's also useful for say a",
    "start": "1221200",
    "end": "1226600"
  },
  {
    "text": "multi-tenant multi-user environment where potentially you even have um some",
    "start": "1226600",
    "end": "1232080"
  },
  {
    "text": "security constraints around the uh communication between workloads And so",
    "start": "1232080",
    "end": "1237600"
  },
  {
    "text": "to enable that in a in a meaningful way we've recently added to this DRA driver that we are building uh a new primitive",
    "start": "1237600",
    "end": "1244640"
  },
  {
    "text": "that we call compute domain And uh that really allows you to to set",
    "start": "1244640",
    "end": "1250720"
  },
  {
    "text": "up this communication um these communication primitives between the parts of your workload the",
    "start": "1250720",
    "end": "1258799"
  },
  {
    "text": "individual pods in a secure way so that just your workload can communicate with",
    "start": "1258799",
    "end": "1264159"
  },
  {
    "text": "each other and other parties on the same cluster are isolated from that um under",
    "start": "1264159",
    "end": "1270640"
  },
  {
    "text": "the hood that just requires a little bit of work that you can also do manually like running an IMX demon so-called IMAX",
    "start": "1270640",
    "end": "1277120"
  },
  {
    "text": "demon which is an implementation detail detail I'm just throwing out the name uh you have to run the demon on on the",
    "start": "1277120",
    "end": "1282960"
  },
  {
    "text": "nodes involved in the workload and you have to set up an IMAX channel and and all of that requires some orchestration",
    "start": "1282960",
    "end": "1289760"
  },
  {
    "text": "that that we're now taking care for you in an ephemeral way almost you know the",
    "start": "1289760",
    "end": "1295760"
  },
  {
    "text": "the computer domain construct ensures that all the required components are set up for your workload and then also torn",
    "start": "1295760",
    "end": "1302320"
  },
  {
    "text": "down properly when your workload finishes And in this case as you can see on the left hand side here um you just",
    "start": "1302320",
    "end": "1309440"
  },
  {
    "text": "say I I need this computer main you give it a name and then you refer to that",
    "start": "1309440",
    "end": "1315360"
  },
  {
    "text": "name in your in your workload and uh the rest is done by the",
    "start": "1315360",
    "end": "1321320"
  },
  {
    "text": "system And I want to quickly demonstrate that for you Um I thought about how to do this at the",
    "start": "1321320",
    "end": "1328880"
  },
  {
    "text": "core But what what I found convincing is for for demonstrating that this works at at the core you want to have one GPU",
    "start": "1328880",
    "end": "1336799"
  },
  {
    "text": "allocate some memory and then make that memory allocation exportable to a",
    "start": "1336799",
    "end": "1342080"
  },
  {
    "text": "different GPU on a different node Right and in raw CUDA API terms like this is",
    "start": "1342080",
    "end": "1349280"
  },
  {
    "text": "what you need to do what I'm showing here on on the right hand side So one pod is actually in the first line you see it's allocating some memory with",
    "start": "1349280",
    "end": "1355600"
  },
  {
    "text": "qmem create and then uh we have the API call qm export to sharable handle of a",
    "start": "1355600",
    "end": "1361840"
  },
  {
    "text": "specific type and that type qme handle type fabric is precisely what you need",
    "start": "1361840",
    "end": "1367280"
  },
  {
    "text": "to make this multi-node nvlink memory sharing at high bandwidth possible and",
    "start": "1367280",
    "end": "1372880"
  },
  {
    "text": "that handle is after all just a tiny blob of data you bring it to another node to a GPU there and you can import",
    "start": "1372880",
    "end": "1379360"
  },
  {
    "text": "it and the import will only succeed when the underlying orchestration was done so",
    "start": "1379360",
    "end": "1384679"
  },
  {
    "text": "that those two GPUs have a so-called shared IMAX channel So if this works our",
    "start": "1384679",
    "end": "1390080"
  },
  {
    "text": "computer main construct works Um let's see quick",
    "start": "1390080",
    "end": "1397039"
  },
  {
    "text": "demo So uh on the right hand side we we I had a YAML document showing some",
    "start": "1398520",
    "end": "1404520"
  },
  {
    "text": "work a job comprised of two pods and that's what I'm uh hopefully",
    "start": "1404520",
    "end": "1412320"
  },
  {
    "text": "deploying here right now just the job two pods they are running on two different nodes we can have a can",
    "start": "1413919",
    "end": "1423480"
  },
  {
    "text": "thank you yeah so we can have a look at the pod status okay they are now both like you know they are both on two",
    "start": "1424960",
    "end": "1430400"
  },
  {
    "text": "different nodes we see that here right two different host names they're now running and now we probably already have",
    "start": "1430400",
    "end": "1438559"
  },
  {
    "text": "some log output so one of the pods actually allocated memory on the",
    "start": "1438559",
    "end": "1444440"
  },
  {
    "text": "GPU exported a handle and we're looking at the the data here The handle is some some you know some bytes hex notation",
    "start": "1444440",
    "end": "1450640"
  },
  {
    "text": "just to make it um",
    "start": "1450640",
    "end": "1455480"
  },
  {
    "text": "uh vis to to visualize it And then on the other node the other GPU we just see",
    "start": "1456320",
    "end": "1461679"
  },
  {
    "text": "that the import from sharable handle API call succeeded which means that the",
    "start": "1461679",
    "end": "1466799"
  },
  {
    "text": "underlying um IMAX channel was actually available And all that was handled by the",
    "start": "1466799",
    "end": "1473880"
  },
  {
    "text": "um computer main logic of our DRA driver And now we can have a quick look",
    "start": "1473880",
    "end": "1479880"
  },
  {
    "text": "at the log output Oh no I was too slow you know the the the the the part that I",
    "start": "1479880",
    "end": "1487039"
  },
  {
    "text": "was actually inspecting right now the demon running the IMAX demon uh is already gone anyway we would have seen",
    "start": "1487039",
    "end": "1493840"
  },
  {
    "text": "that you know the demon confirmed what we saw basically the memory",
    "start": "1493840",
    "end": "1499400"
  },
  {
    "text": "exchange okay in terms of time I think that was",
    "start": "1499400",
    "end": "1504880"
  },
  {
    "text": "pretty good um quick outlook",
    "start": "1504880",
    "end": "1512320"
  },
  {
    "text": "um what I said like the partitional devices story will work properly in the next release of Kubernetes 133 and we're",
    "start": "1512320",
    "end": "1518159"
  },
  {
    "text": "going to hopefully have a like de meaning enabled by default by the end of",
    "start": "1518159",
    "end": "1523600"
  },
  {
    "text": "the year and if you want to learn way more about all this that I just talked",
    "start": "1523600",
    "end": "1529120"
  },
  {
    "text": "about I really want to recommend Kevin and Patrick's talk on Thursday and uh",
    "start": "1529120",
    "end": "1534880"
  },
  {
    "text": "also Diana is going to have a more exhaustive overview what they are doing",
    "start": "1534880",
    "end": "1540080"
  },
  {
    "text": "at CERN on Thursday Thank you so much [Applause]",
    "start": "1540080",
    "end": "1546390"
  },
  {
    "text": "[Music] [Applause] Do we have questions yep If you have any",
    "start": "1546390",
    "end": "1553919"
  },
  {
    "text": "questions we have a microphone over there You can attend Yep Please head over in the",
    "start": "1553919",
    "end": "1561360"
  },
  {
    "text": "middle and we have a microphone to ask questions from",
    "start": "1561360",
    "end": "1566520"
  },
  {
    "text": "Yeah",
    "start": "1572640",
    "end": "1575640"
  },
  {
    "text": "Hi Can you hear me yep Yep Hey how's it going yeah Thanks for the presentation I'm over here Oh yeah",
    "start": "1592400",
    "end": "1599679"
  },
  {
    "text": "Yeah Um I think you mentioned like in 1.32 that the feature gates are",
    "start": "1599679",
    "end": "1604799"
  },
  {
    "text": "available to use DRA So what's happening in 1.32 and what's happening in 1.33",
    "start": "1604799",
    "end": "1610799"
  },
  {
    "text": "uh so yeah 1.32 which you know has been released a while ago the feature gate is available So you know if you just",
    "start": "1610799",
    "end": "1616400"
  },
  {
    "text": "install Kubernetes and set the feature gate you can use DRA meaning you can deploy for example",
    "start": "1616400",
    "end": "1621480"
  },
  {
    "text": "Nvidia's uh DRA driver for GPUs and it will do its thing um in 133 I think that",
    "start": "1621480",
    "end": "1629760"
  },
  {
    "text": "is not going to change yet So you still have to manually opt in to using DRA and",
    "start": "1629760",
    "end": "1636320"
  },
  {
    "text": "the the big achievement in 134 maybe would would be that it it just is enabled by default",
    "start": "1636320",
    "end": "1643520"
  },
  {
    "text": "Does that answer the question yeah that's perfect Thanks And another question is um so you've talked about",
    "start": "1643520",
    "end": "1650080"
  },
  {
    "text": "kind of having NV link work between or like across nodes right but you need to make sure those nodes are on the same",
    "start": "1650080",
    "end": "1656400"
  },
  {
    "text": "server rack right so are there any kind of conversations with public cloud providers for you know allowing that",
    "start": "1656400",
    "end": "1662960"
  },
  {
    "text": "kind of feature yeah I mean you're absolutely right So this only makes sense when they're when the when the nodes are connected with those NVLink",
    "start": "1662960",
    "end": "1669880"
  },
  {
    "text": "switches and um like internally this is depicted by a so-called like domain ID",
    "start": "1669880",
    "end": "1676320"
  },
  {
    "text": "So the system knows when there is NVLink linkage between the nodes So um",
    "start": "1676320",
    "end": "1683520"
  },
  {
    "text": "realistically this only makes sense you know in these well what makes makes most of the sense in these huge racks like",
    "start": "1683520",
    "end": "1689200"
  },
  {
    "text": "NVL 72 and um maybe you maybe you can also",
    "start": "1689200",
    "end": "1694640"
  },
  {
    "text": "get to connectivity beyond that but that would then fall back to slower connection",
    "start": "1694640",
    "end": "1699840"
  },
  {
    "text": "Nice Thank you so much Pleasure So in your demo um from my understanding",
    "start": "1699840",
    "end": "1707120"
  },
  {
    "text": "I saw you have two pods running on two nodes connected to two different GPUs",
    "start": "1707120",
    "end": "1713039"
  },
  {
    "text": "and they're connected using an IMAX channel and that's one application Is that correct right And so what happens",
    "start": "1713039",
    "end": "1719760"
  },
  {
    "text": "if you lost one of those pods what if you killed one of them",
    "start": "1719760",
    "end": "1725200"
  },
  {
    "text": "so um from from a application point of view of course you know when you have vivid",
    "start": "1725200",
    "end": "1732559"
  },
  {
    "text": "communication going on between the GPUs um so you would need to handle that I",
    "start": "1732559",
    "end": "1739440"
  },
  {
    "text": "would say on the uh application level and I I think you know in many HPC scenarios maybe you have some kind of",
    "start": "1739440",
    "end": "1745760"
  },
  {
    "text": "restart point to go back to okay so you so what you're saying is like it's usually on the application to",
    "start": "1745760",
    "end": "1754159"
  },
  {
    "text": "have to kind the application will be responsible for handling this kind of failure scenario This is the expectation",
    "start": "1754159",
    "end": "1761760"
  },
  {
    "text": "Yeah So I mean you can of course have Kubernetes just restart your pods but I would say if you really do meaningful",
    "start": "1761760",
    "end": "1767679"
  },
  {
    "text": "work uh you don't just want to restart all over but you want to have some kind",
    "start": "1767679",
    "end": "1772799"
  },
  {
    "text": "of checkpointing going on and maybe you can also make it so that if your application crashes and then maybe",
    "start": "1772799",
    "end": "1779360"
  },
  {
    "text": "Kubernetes restarts your containers that you pick up where you left off but that",
    "start": "1779360",
    "end": "1785120"
  },
  {
    "text": "would still something I guess that you would need to build yourself Cool Yeah because I'm wondering",
    "start": "1785120",
    "end": "1791200"
  },
  {
    "text": "too even now like you got to think about the rescheduling of the new pot Does it go back to the same node does use the",
    "start": "1791200",
    "end": "1797919"
  },
  {
    "text": "same IMAX channel the same GPU does it go somewhere else i don't know Like is now you have the application who has",
    "start": "1797919",
    "end": "1804559"
  },
  {
    "text": "some understanding and expectation that it could be preempted and then Kubernetes would probably have some",
    "start": "1804559",
    "end": "1809919"
  },
  {
    "text": "other expectation of what how it's going to be scheduled I don't I'm just thinking I mean yeah you're you're",
    "start": "1809919",
    "end": "1815520"
  },
  {
    "text": "certainly right that this is probably um you know especially when you have I don't know a 10,000 GPU training",
    "start": "1815520",
    "end": "1822120"
  },
  {
    "text": "job failures will happen and that story needs to be like well thought through",
    "start": "1822120",
    "end": "1827200"
  },
  {
    "text": "right yeah yeah okay I was just wonder sure yeah cool thank you yeah I think",
    "start": "1827200",
    "end": "1834799"
  },
  {
    "text": "that's all the time we have for questions sorry but we can take rest of your questions in the hallway track does",
    "start": "1834799",
    "end": "1840640"
  },
  {
    "text": "that work the the rest of the questions can follow in the hallway track People can find you Yep Sure Thank you Thank",
    "start": "1840640",
    "end": "1847200"
  },
  {
    "text": "you Dash Thank you John",
    "start": "1847200",
    "end": "1851120"
  }
]