[
  {
    "text": "this is mauro pecina and welcome to my session which is getting the optimal service efficiency that auto skaters",
    "start": "240",
    "end": "6799"
  },
  {
    "text": "won't give you these are the contents we will cover today we start by discussing the problem",
    "start": "6799",
    "end": "13920"
  },
  {
    "text": "we are going to address and the kubernetes challenges for ensuring application performance and reliability",
    "start": "13920",
    "end": "19760"
  },
  {
    "text": "we will then introduce the new approach we implemented in moviri which leverages ai to automate",
    "start": "19760",
    "end": "25519"
  },
  {
    "text": "the optimization process and we'll do by they do that by providing real world examples",
    "start": "25519",
    "end": "31119"
  },
  {
    "text": "finally we will conclude with the some take away the quiz before proceeding please allow me to",
    "start": "31119",
    "end": "37360"
  },
  {
    "text": "introduce myself my name is mauro and i'm the head of performance engineering services align",
    "start": "37360",
    "end": "42719"
  },
  {
    "text": "in mobility and performance engineering has always been my passion since i graduated",
    "start": "42719",
    "end": "50320"
  },
  {
    "text": "so let's start with a quick introduction about what the problem is as you all know applications are",
    "start": "51680",
    "end": "58239"
  },
  {
    "text": "evolving monoliths are quickly disappearing and modern and cloud native applications are features",
    "start": "58239",
    "end": "64239"
  },
  {
    "text": "dozens or even hundreds of microservices spanning over a wide set of technologies",
    "start": "64239",
    "end": "69520"
  },
  {
    "text": "all these technologies comes with the tunable parameters and option themselves and as",
    "start": "69520",
    "end": "75040"
  },
  {
    "text": "a result we must deal with hydrate of on of thousands of possible configurations",
    "start": "75040",
    "end": "80560"
  },
  {
    "text": "so how do you find the configuration that that best suits our workload",
    "start": "80560",
    "end": "87840"
  },
  {
    "text": "speaking of kubernetes itself there are more and more stories about how difficult it is to ensure",
    "start": "88159",
    "end": "94320"
  },
  {
    "text": "performance and stability for kubernetes application as you may have seen also on speeches before mine",
    "start": "94320",
    "end": "102240"
  },
  {
    "text": "the kubernetes fellow stories for example is a website that was specifically created to check kubernetes",
    "start": "102240",
    "end": "108479"
  },
  {
    "text": "incident reports with the goal to learn how to prevent them many of these stories describe",
    "start": "108479",
    "end": "114479"
  },
  {
    "text": "teams that are struggling with kubernetes application performance and stability issues such as unexpected cpu",
    "start": "114479",
    "end": "120240"
  },
  {
    "text": "slowdowns or even sudden container termination but why is it so difficult to manage",
    "start": "120240",
    "end": "126479"
  },
  {
    "text": "application performance stability and efficiency on kubernetes the simple answer is that kubernetes is",
    "start": "126479",
    "end": "133520"
  },
  {
    "text": "a great platform to run containerized applications but it requires applications to be carefully configured",
    "start": "133520",
    "end": "139920"
  },
  {
    "text": "to ensure high performance and stability let's now",
    "start": "139920",
    "end": "145760"
  },
  {
    "text": "get back to the fundamentals and see how kubernetes resource management works to better understand the main parameters",
    "start": "145760",
    "end": "151599"
  },
  {
    "text": "that impact kubernetes application performance stability and cost efficiency we will go through the six main key",
    "start": "151599",
    "end": "158160"
  },
  {
    "text": "facts and their implication starting from the first which is resource request",
    "start": "158160",
    "end": "163760"
  },
  {
    "text": "you may have heard many talks before mine talking about this topic but let's make a quick recap",
    "start": "163760",
    "end": "170800"
  },
  {
    "text": "when developers define a pod they have the possibilities to specify also resource requests those are the amount",
    "start": "170800",
    "end": "176480"
  },
  {
    "text": "of cpu and memory uh which are uh granted to the pod or orbit or better a",
    "start": "176480",
    "end": "182640"
  },
  {
    "text": "container within the pod and kubernetes will schedule the pod on a node where the request requested",
    "start": "182640",
    "end": "188800"
  },
  {
    "text": "resources are actually available in the example you can see the pod day which requires two cpus and is scheduled on a",
    "start": "188800",
    "end": "195200"
  },
  {
    "text": "four cpu node when a new part b of the same size is created it can be also",
    "start": "195200",
    "end": "200480"
  },
  {
    "text": "scheduled on the same node this is not this node now has all of the four cpus requested if a pod c is",
    "start": "200480",
    "end": "207920"
  },
  {
    "text": "created kubernetes won't schedule it on this node and as its capacity is full",
    "start": "207920",
    "end": "213920"
  },
  {
    "text": "this means that those numbers a developers puts in a yaml file are accurately used by kubernetes to manage",
    "start": "213920",
    "end": "220720"
  },
  {
    "text": "a real cluster capacity a quite surprising fact is that kubernetes there is no other commitment",
    "start": "220720",
    "end": "228159"
  },
  {
    "text": "on request you cannot request more cpus than those available in the cluster this is very different from for",
    "start": "228159",
    "end": "234959"
  },
  {
    "text": "example virtualization where you could create vms with many more cpus than the real physical ones",
    "start": "234959",
    "end": "241360"
  },
  {
    "text": "and another very important fact to notice is that the resource requests are not equal to utilization if the public",
    "start": "241360",
    "end": "248159"
  },
  {
    "text": "requests are much higher than the accuracy usage for example you will end up with a cluster that can be full can",
    "start": "248159",
    "end": "255120"
  },
  {
    "text": "be fully scheduled even though it's a cp utilization is as low as 10 percent for",
    "start": "255120",
    "end": "260320"
  },
  {
    "text": "example so the takeaway is here is that a set improper pod request is needed to ensure kubernetes",
    "start": "260320",
    "end": "267600"
  },
  {
    "text": "cost efficiency the second important concept is resource",
    "start": "267600",
    "end": "272720"
  },
  {
    "text": "limits resource requests are the guaranteed resources a container will get but the",
    "start": "272720",
    "end": "278479"
  },
  {
    "text": "usage can be higher resource limits is the mechanism that allows you to define the maximum amount",
    "start": "278479",
    "end": "283759"
  },
  {
    "text": "of resources a container can use like two cpus or one gigabyte of memory uh all this is great but what happens",
    "start": "283759",
    "end": "291600"
  },
  {
    "text": "when resource usage is the limit kubernetes tweets in cpu and memory",
    "start": "291600",
    "end": "296800"
  },
  {
    "text": "differently here so when cpu usage approaches the limit the container gets throttled",
    "start": "296800",
    "end": "303280"
  },
  {
    "text": "this means that the cpu is artificially restricted and this can result in application performance issues instead the web when",
    "start": "303280",
    "end": "311199"
  },
  {
    "text": "memory usage is the limit the container could get terminated so there is no applica no no applications slow down due",
    "start": "311199",
    "end": "317360"
  },
  {
    "text": "to paging or swapping or so on but with kubernetes your body will simply disappear and you may face serious",
    "start": "317360",
    "end": "324000"
  },
  {
    "text": "application stability issues the third factor is about an important",
    "start": "324000",
    "end": "329280"
  },
  {
    "text": "and less known effect cpu limits have on application performance cpu limits work by throttling cpu",
    "start": "329280",
    "end": "335120"
  },
  {
    "text": "performance and you may think that this happens only when cpu usage hits the limit but surprisingly the",
    "start": "335120",
    "end": "342000"
  },
  {
    "text": "reality is that the cpu throttling starts when the cpu usage is well below the limit we did quite a bit of research",
    "start": "342000",
    "end": "348639"
  },
  {
    "text": "ourselves and we found out that the cpu throttling can start when cpu usage is",
    "start": "348639",
    "end": "354160"
  },
  {
    "text": "as low as 30 percent of the cpu limit this is due but to some configuration of",
    "start": "354160",
    "end": "360160"
  },
  {
    "text": "some kernel parameters but let's uh let's talk about what happens with aggressive cpu throttling has a",
    "start": "360160",
    "end": "367600"
  },
  {
    "text": "huge impact on service performance you can get sudden latency spike that may breach your slos without any apparent",
    "start": "367600",
    "end": "374479"
  },
  {
    "text": "reason and even at low cpu usage so now some people including the",
    "start": "374479",
    "end": "380479"
  },
  {
    "text": "engineers at buffer for example tried to remove cpu limits what they got you can see it in the chart on the right was an",
    "start": "380479",
    "end": "386960"
  },
  {
    "text": "impressive reduction in service latency so is it is it actually a good idea to",
    "start": "386960",
    "end": "393120"
  },
  {
    "text": "get rid of cpu limits well it depends the answer could be no",
    "start": "393120",
    "end": "398240"
  },
  {
    "text": "because cpu limits is exist for the purpose of ensuring that application run fine and consists with other",
    "start": "398240",
    "end": "405360"
  },
  {
    "text": "applications so if cpu limits are removed a single runway application can completely disrupt the performance and",
    "start": "405360",
    "end": "412400"
  },
  {
    "text": "expected availability of your most critical services this best practice obviously is also",
    "start": "412400",
    "end": "418800"
  },
  {
    "text": "recommended by google so tuning these resources is really",
    "start": "418800",
    "end": "424000"
  },
  {
    "text": "important you have to think properly before doing it",
    "start": "424000",
    "end": "428879"
  },
  {
    "text": "coming to the auto scalers so we have vpas we have hpas let's talk about vpas",
    "start": "429039",
    "end": "435840"
  },
  {
    "text": "the vertical protoscaler basically provides recommendation on cpu and memory request based on the observed",
    "start": "435840",
    "end": "442479"
  },
  {
    "text": "powder source usage however our experience with the vpa is mixed",
    "start": "442479",
    "end": "448319"
  },
  {
    "text": "in this example you can see a kubernetes microservices which is serving a typical journal traffic",
    "start": "448319",
    "end": "454720"
  },
  {
    "text": "the top right chart shows the latency of this service and it's a service level",
    "start": "454720",
    "end": "459759"
  },
  {
    "text": "objective a while below you can see the resource request in terms of cpu and memory and",
    "start": "459759",
    "end": "465120"
  },
  {
    "text": "the corresponding resource utilization it's interesting to see okay that we",
    "start": "465120",
    "end": "471039"
  },
  {
    "text": "left the service running for a couple of days with some initial resources sizing and then we activated the vpa and let it",
    "start": "471039",
    "end": "477440"
  },
  {
    "text": "apply the new recommended settings to the pod it's interesting to see that the vpa",
    "start": "477440",
    "end": "482479"
  },
  {
    "text": "immediately decided to reduce the sound resources in particular it cut enough the cpu",
    "start": "482479",
    "end": "488319"
  },
  {
    "text": "request but this is like likely due to some apparently over provisioning of",
    "start": "488319",
    "end": "493840"
  },
  {
    "text": "this service as the cpu utilization was below 50 however with this new setting suggested",
    "start": "493840",
    "end": "500560"
  },
  {
    "text": "by the vpa the latency of the microservices skyrocketed with the microservices no longer no longer able",
    "start": "500560",
    "end": "507280"
  },
  {
    "text": "to meet its reliability what is the lesson learned here",
    "start": "507280",
    "end": "512399"
  },
  {
    "text": "vpa autoscaler is based on resource usage and does not consider application level metrics like response time for",
    "start": "512399",
    "end": "519039"
  },
  {
    "text": "example we need to evaluate the the effects on of the recommender settings",
    "start": "519039",
    "end": "524080"
  },
  {
    "text": "as be as they might be somewhat aggressive and cause service uh severe",
    "start": "524080",
    "end": "529120"
  },
  {
    "text": "service performance or reliability degradation but we do also have hpa the horizontal",
    "start": "529120",
    "end": "536080"
  },
  {
    "text": "pad auto scaler what about it in this example we wish a",
    "start": "536080",
    "end": "541680"
  },
  {
    "text": "situation where hbr hpa scales out when specific memory or cpu consumptions are",
    "start": "541680",
    "end": "547040"
  },
  {
    "text": "met as a result from one pod we get two identical pods",
    "start": "547040",
    "end": "552160"
  },
  {
    "text": "that provide the same service this is great so we have two times the resources we had before to complete the service",
    "start": "552160",
    "end": "558000"
  },
  {
    "text": "what could possibly go wrong well things may go wrong because it's if",
    "start": "558000",
    "end": "563760"
  },
  {
    "text": "for some reason you didn't properly set up your memory limit for example you undersized it and you will simply get",
    "start": "563760",
    "end": "569680"
  },
  {
    "text": "into a situation where you get twice the out of memory kills you had before but also on the other hand you may face",
    "start": "569680",
    "end": "575760"
  },
  {
    "text": "a situation where you set your cpu limits to height but your pod can find such resources",
    "start": "575760",
    "end": "581600"
  },
  {
    "text": "directly impacting your service latency because of throttling but let's say you wanted to play safe",
    "start": "581600",
    "end": "588000"
  },
  {
    "text": "and over provision all your environment and your pods everything works properly functionally speaking but at what cost",
    "start": "588000",
    "end": "595360"
  },
  {
    "text": "at least you may be wasting two times the resources you are wasting before the scale out if your pod isn't tuned",
    "start": "595360",
    "end": "601440"
  },
  {
    "text": "properly as we have seen so far optimizing",
    "start": "601440",
    "end": "606959"
  },
  {
    "text": "microservices application on kubernetes is a real challenge given the complexity of tuning kubernetes resources and many",
    "start": "606959",
    "end": "614000"
  },
  {
    "text": "moving parts we we have in modern applications a new approach is required",
    "start": "614000",
    "end": "620240"
  },
  {
    "text": "to successfully solve this problem and this is where ai can help air have",
    "start": "620240",
    "end": "626000"
  },
  {
    "text": "revolutionized the entire industries and the good news is that they can be used also in the performance tuning process",
    "start": "626000",
    "end": "633120"
  },
  {
    "text": "ai can automate the tuning of the many parameters we have in the software stack",
    "start": "633120",
    "end": "638240"
  },
  {
    "text": "with the goal to optimize application performance resiliency and cost in this section i would like to",
    "start": "638240",
    "end": "644480"
  },
  {
    "text": "introduce you to this new ai powered openstack methodology by leveraging a",
    "start": "644480",
    "end": "649680"
  },
  {
    "text": "real world scenario we have seen so the real world case is about an",
    "start": "649680",
    "end": "655360"
  },
  {
    "text": "european sas provider of financial services whose java based micro service application are running on an a on",
    "start": "655360",
    "end": "662160"
  },
  {
    "text": "either azure or aws kubernetes service the target system of the optimization is",
    "start": "662160",
    "end": "668240"
  },
  {
    "text": "the b2b actualization service running on azure a business critical service that",
    "start": "668240",
    "end": "673440"
  },
  {
    "text": "interacts with all applications powering the digital services provided by the company",
    "start": "673440",
    "end": "678640"
  },
  {
    "text": "the challenge of the customer was to avoid the spending and achieve the best cost efficiency possible by enabling",
    "start": "678640",
    "end": "685440"
  },
  {
    "text": "developing teams to optimize their application while keeping on releasing application updates to introduce new",
    "start": "685440",
    "end": "691760"
  },
  {
    "text": "business functionalities and align to new regulations",
    "start": "691760",
    "end": "696560"
  },
  {
    "text": "what the results were almost impossible to achieve with the tuning practice they had in place since they it was manual",
    "start": "697120",
    "end": "704000"
  },
  {
    "text": "and it took almost two months to tune one single macro service uh with mixed results obviously because",
    "start": "704000",
    "end": "711279"
  },
  {
    "text": "it was always too difficult to to find the right trade-off between performance resiliency and obviously cost",
    "start": "711279",
    "end": "718000"
  },
  {
    "text": "by design the approach would was the good old over provisioning but as a result what did they have the",
    "start": "718000",
    "end": "725279"
  },
  {
    "text": "result was a huge of a spending and a lack of performance operational",
    "start": "725279",
    "end": "730399"
  },
  {
    "text": "efficiency and business agility so how did we address this customer i",
    "start": "730399",
    "end": "737279"
  },
  {
    "text": "would like to introduce quickly our ai power and optimization methodology in practice the process is fully automated and it",
    "start": "737279",
    "end": "744639"
  },
  {
    "text": "works in five steps the first step first step is to apply the new configuration ai suggested to our target",
    "start": "744639",
    "end": "751760"
  },
  {
    "text": "system this is typically done leveraging kubernetes apis and to set the new value",
    "start": "751760",
    "end": "757519"
  },
  {
    "text": "for the parameters for example the cpu of the request of the container",
    "start": "757519",
    "end": "762639"
  },
  {
    "text": "the second step is to apply a workload to the target system in order to assess the performance of the new configuration",
    "start": "762639",
    "end": "769839"
  },
  {
    "text": "this is typically done by leveraging performance testing tools in this case we we use the geometer test that was",
    "start": "769839",
    "end": "777360"
  },
  {
    "text": "previously built to stress the application with a realistic workload we'll see it later",
    "start": "777360",
    "end": "783040"
  },
  {
    "text": "and the third step is to collect kpis related to the target system the typical approach here is is to leverage",
    "start": "783040",
    "end": "790079"
  },
  {
    "text": "observability platforms and in this case we integrated elastic apm",
    "start": "790079",
    "end": "796720"
  },
  {
    "text": "which is the monitoring solution used by the customer the fourth step",
    "start": "796720",
    "end": "802079"
  },
  {
    "text": "is to analyze the result of the performance test and assign a score based on the specific goal that we set",
    "start": "802079",
    "end": "810240"
  },
  {
    "text": "in this case the goal was the cloud cost of the application container considering the prices of visual cloud",
    "start": "810240",
    "end": "817360"
  },
  {
    "text": "the last step is where ai kicks in by taking the score of the test",
    "start": "817360",
    "end": "822639"
  },
  {
    "text": "configuration as an input as input and by producing as output the most promising configuration to be tested in",
    "start": "822639",
    "end": "828560"
  },
  {
    "text": "the next iteration of the same process and so we start again we can start again by testing a new configuration",
    "start": "828560",
    "end": "836880"
  },
  {
    "text": "so i i've said that we have to to have a goal we can define",
    "start": "837279",
    "end": "843279"
  },
  {
    "text": "a goal in this scenario the goal was to reduce the cloud the cost required to run the",
    "start": "843279",
    "end": "848480"
  },
  {
    "text": "authentication service on kubernetes as you can see the optimization glo is declarative okay",
    "start": "848480",
    "end": "854560"
  },
  {
    "text": "at the same time you also wanted to ensure that the service would always meet its reliability targets which are",
    "start": "854560",
    "end": "861519"
  },
  {
    "text": "expressed as latency throughput and error rate slos so how can we leverage ai to achieve",
    "start": "861519",
    "end": "867680"
  },
  {
    "text": "this high level business goal in the ai powered optimization methodology the ai will change the",
    "start": "867680",
    "end": "873920"
  },
  {
    "text": "parameters of the system to improve a metric a metric that we have defined in this case the ai goal is simply to",
    "start": "873920",
    "end": "880079"
  },
  {
    "text": "minimize the application cost this is a metric that represents the cloud cost we will pay to run the application on the",
    "start": "880079",
    "end": "886800"
  },
  {
    "text": "cloud which depends on the value of cpu and memory sources that are allocated to the microservices",
    "start": "886800",
    "end": "894639"
  },
  {
    "text": "the the methodology that we used also allows to set constraints to define which configuration are",
    "start": "894639",
    "end": "901440"
  },
  {
    "text": "acceptable in this case we state that the system throughput response time and error rate should not degrade more than",
    "start": "901440",
    "end": "908720"
  },
  {
    "text": "10 than the baseline so to assess the performance and cost",
    "start": "908720",
    "end": "914480"
  },
  {
    "text": "efficiency of a new configuration suggested by the ai optimizer we stress test the siemens the system with a load",
    "start": "914480",
    "end": "919920"
  },
  {
    "text": "test in this case you can see the load testing scenario that we used it was designed according",
    "start": "919920",
    "end": "925839"
  },
  {
    "text": "to the performance engineering best practices the traffic pattern reproduces the behaviors in production including",
    "start": "925839",
    "end": "932079"
  },
  {
    "text": "api call distribution and think times",
    "start": "932079",
    "end": "936480"
  },
  {
    "text": "before looking at the results it's worth commenting on how the application was initially configured by",
    "start": "937920",
    "end": "944639"
  },
  {
    "text": "the customer we called that the baseline configuration let's look at the kubernetes setting first this container",
    "start": "944639",
    "end": "952480"
  },
  {
    "text": "powering the application was configured with the resources of resource requests sorry of a 1.5 cpus",
    "start": "952480",
    "end": "958880"
  },
  {
    "text": "and 3.4 gigabytes of memory and the team also specified their source limits of two cpus and 4.4 gigabytes of memory",
    "start": "958880",
    "end": "967920"
  },
  {
    "text": "remember the requests are the guaranteed resources that kubernetes will use for scheduling the capacity management to",
    "start": "967920",
    "end": "974000"
  },
  {
    "text": "the cluster in this case requests are lower than the limit which is a common approach to guarantee resources for the",
    "start": "974000",
    "end": "980880"
  },
  {
    "text": "application to run properly but at the same time allow some room for unexpected growth",
    "start": "980880",
    "end": "986880"
  },
  {
    "text": "besides looking at the container settings it's important to also see how the application runtime is configured",
    "start": "986880",
    "end": "993600"
  },
  {
    "text": "the runtime is what ultimately powers our application and for java application we know that the jvm settings",
    "start": "993600",
    "end": "1001120"
  },
  {
    "text": "might play a big role in application performance but the same and happens for other languages so for",
    "start": "1001120",
    "end": "1007440"
  },
  {
    "text": "example golang and so on so the jvm it was configured with a minimum hip of half a gig",
    "start": "1007440",
    "end": "1014240"
  },
  {
    "text": "and a maxip of four gigs okay notice that the max ship is higher than the memory request",
    "start": "1014240",
    "end": "1021199"
  },
  {
    "text": "which means that the jvm can use more memory than the amount request requested",
    "start": "1021199",
    "end": "1026798"
  },
  {
    "text": "as we are going to see this configuration will have an impact on how the application behaves under load and",
    "start": "1026799",
    "end": "1033120"
  },
  {
    "text": "and the associated resiliency attach the cost",
    "start": "1033120",
    "end": "1038520"
  },
  {
    "text": "okay we have covered how the applications is configured okay let's now look at the behavior of the",
    "start": "1038559",
    "end": "1043600"
  },
  {
    "text": "application when subject to the load test that we have shown before with the baseline configuration",
    "start": "1043600",
    "end": "1049280"
  },
  {
    "text": "in this chart you can see the application throughput the response time and the number of replicas that were created by the autoscaler",
    "start": "1049280",
    "end": "1056000"
  },
  {
    "text": "two facts are important to notice first one when load increases the autoscaler triggers a scale out event",
    "start": "1056000",
    "end": "1062640"
  },
  {
    "text": "which creates a new replica this event causes a big spike on response time which impacts service",
    "start": "1062640",
    "end": "1069440"
  },
  {
    "text": "reliability and performance this is due to the height cpu usage and cpu throttling obviously during the jvm",
    "start": "1069440",
    "end": "1076160"
  },
  {
    "text": "startup phase when the load drops the number of replicas that does not scale down despite the container cpu usage may",
    "start": "1076160",
    "end": "1083440"
  },
  {
    "text": "be idle as in this case it's interesting to understand that why",
    "start": "1083440",
    "end": "1088559"
  },
  {
    "text": "this is happening this is caused by configuration of the container sources the jvm running inside and the",
    "start": "1088559",
    "end": "1094880"
  },
  {
    "text": "autoscaler policy in particular for the memory source the autoscaler in this case is not",
    "start": "1094880",
    "end": "1101840"
  },
  {
    "text": "scaling down because the memory usage of the container is higher than the configured threshold of the seventy",
    "start": "1101840",
    "end": "1108320"
  },
  {
    "text": "percent usage of the memory request this might be due the jvo and maxip",
    "start": "1108320",
    "end": "1113919"
  },
  {
    "text": "being higher than the memory request as in this case uh as we've seen before but it may also",
    "start": "1113919",
    "end": "1119360"
  },
  {
    "text": "be due to a change in the application memory footprint for example after deployment",
    "start": "1119360",
    "end": "1124960"
  },
  {
    "text": "this effect clearly impacts the cloud bill as more instances are up than the",
    "start": "1124960",
    "end": "1130720"
  },
  {
    "text": "required ones but shows that configuring kubernetes apps for reliability and cost",
    "start": "1130720",
    "end": "1136559"
  },
  {
    "text": "efficiency is actually a tricky process so what did we do",
    "start": "1136559",
    "end": "1142799"
  },
  {
    "text": "and we know we now know how the baseline behaves let's start optimizing what we did was starting experimenting with",
    "start": "1142799",
    "end": "1149200"
  },
  {
    "text": "different kubernetes and java configurations suggested suggested by our ai algorithm and measure the results",
    "start": "1149200",
    "end": "1157360"
  },
  {
    "text": "let's have a look at the best configuration identified by our ai with respect to the defined cost efficiency",
    "start": "1157919",
    "end": "1164000"
  },
  {
    "text": "goal this was fine found at experiment number 34 after about 90 hours of trials",
    "start": "1164000",
    "end": "1170640"
  },
  {
    "text": "and provides a 49 improvement on the cost with respect to the baseline",
    "start": "1170640",
    "end": "1176480"
  },
  {
    "text": "first of all it's interesting to notice how our optimization increased the both memory and cpu request limits",
    "start": "1176480",
    "end": "1184160"
  },
  {
    "text": "which is not all obvious especially for kubernetes because kubernetes is often",
    "start": "1184160",
    "end": "1189360"
  },
  {
    "text": "considered well suited for a small and highly scalable applications we will deep dive on this into a minute",
    "start": "1189360",
    "end": "1196240"
  },
  {
    "text": "the other notable changes are related to the jvm options ai peaked the maxip was",
    "start": "1196240",
    "end": "1202640"
  },
  {
    "text": "increased by 20 percent and is now well within the container memory request which was increased to",
    "start": "1202640",
    "end": "1209440"
  },
  {
    "text": "five gigabytes the midi min him if size also has been adjusted to be almost equal to the max",
    "start": "1209440",
    "end": "1216400"
  },
  {
    "text": "sip which is a configuration that can avoid garbage collection cycles especially in the startup phase of the",
    "start": "1216400",
    "end": "1222960"
  },
  {
    "text": "jvm so let's now see how the application performs with the new configuration",
    "start": "1222960",
    "end": "1228080"
  },
  {
    "text": "identified and how it compares to baseline there are two two important differences",
    "start": "1228080",
    "end": "1233440"
  },
  {
    "text": "here autoscaling is not triggered in this configuration as the full load is",
    "start": "1233440",
    "end": "1239120"
  },
  {
    "text": "sustained by one pod this is clearly beneficial in terms of cost",
    "start": "1239120",
    "end": "1245200"
  },
  {
    "text": "response time always remains within the response time slos and there are no more peaks so this configuration not only",
    "start": "1245200",
    "end": "1251520"
  },
  {
    "text": "improves on cost but also is beneficial in terms of performance and resilience",
    "start": "1251520",
    "end": "1258880"
  },
  {
    "text": "let's also compare in detail the best configuration with respect to the baseline here we can we can notice that the pod",
    "start": "1259280",
    "end": "1266320"
  },
  {
    "text": "is significantly larger in terms of both cpu and memory especially for the request",
    "start": "1266320",
    "end": "1272240"
  },
  {
    "text": "this configuration has the effects as the effect of triggering the auto scaler less often",
    "start": "1272240",
    "end": "1277760"
  },
  {
    "text": "okay as we have seen but interestingly well this implies a kind of fixed cost",
    "start": "1277760",
    "end": "1283039"
  },
  {
    "text": "considering the pricing of the container resources it turns out being much cheaper than a configuration where auto",
    "start": "1283039",
    "end": "1289120"
  },
  {
    "text": "scaling is triggered and these also avoid performance issues the container and runtime configuration",
    "start": "1289120",
    "end": "1296159"
  },
  {
    "text": "are now better aligned the jvm max hip is now below the memory request and this has beneficial effects",
    "start": "1296159",
    "end": "1303600"
  },
  {
    "text": "and it also enables the scale down of the application should the scaling be triggered by higher loads",
    "start": "1303600",
    "end": "1311200"
  },
  {
    "text": "let's now have a look at another configuration found by ai at the experiment number 14 at after",
    "start": "1312400",
    "end": "1319039"
  },
  {
    "text": "about eight hours we level we labeled this configuration high resilience for a reason that will",
    "start": "1319039",
    "end": "1324720"
  },
  {
    "text": "be clear in a minute the score of this configuration while not as good as the best configuration",
    "start": "1324720",
    "end": "1330400"
  },
  {
    "text": "also provides about 16 percent of cost reduction so this can be considered also an",
    "start": "1330400",
    "end": "1336000"
  },
  {
    "text": "interest configuration with respect to the cost efficiency goal as regarded parameters what is worth",
    "start": "1336000",
    "end": "1342080"
  },
  {
    "text": "noticing is that this time ai picked different settings that significantly change the shape of the container it is",
    "start": "1342080",
    "end": "1349039"
  },
  {
    "text": "it is now has a much smaller cpu request than the baseline but the memory is",
    "start": "1349039",
    "end": "1354080"
  },
  {
    "text": "still pretty large which is interesting the gvm option will also changed in",
    "start": "1354080",
    "end": "1359679"
  },
  {
    "text": "particular the garbage collector was switched to a different one to parallel which is a collector that in some cases",
    "start": "1359679",
    "end": "1365840"
  },
  {
    "text": "can be much more efficient and on the use of cpus and memory",
    "start": "1365840",
    "end": "1372240"
  },
  {
    "text": "but let's now compare the behavior of this configuration with respect to the baseline there there are two important",
    "start": "1372240",
    "end": "1378000"
  },
  {
    "text": "differences here the peak on the response time upon the scaling out is significantly lower",
    "start": "1378000",
    "end": "1384880"
  },
  {
    "text": "it is still higher than the response time slows however the peak is less than half the value for the baseline",
    "start": "1384880",
    "end": "1391120"
  },
  {
    "text": "configuration this clearly improves the service resilience and now auto scaling works properly",
    "start": "1391120",
    "end": "1397360"
  },
  {
    "text": "after high load phase replicas are scaled back to 1. this behavior is what we expect from an",
    "start": "1397360",
    "end": "1404240"
  },
  {
    "text": "auto scaling system that works properly notice that the response time peak could be far to reduce the it's simply a",
    "start": "1404240",
    "end": "1410880"
  },
  {
    "text": "matter of creating a new optimization with a new goal of minimizing the response time metric for example instead",
    "start": "1410880",
    "end": "1417200"
  },
  {
    "text": "of application cost let's now also compare and detail the high resilient configuration with",
    "start": "1417200",
    "end": "1422960"
  },
  {
    "text": "respect to the baseline quite interestingly this configuration has higher memory requests and lower cpu",
    "start": "1422960",
    "end": "1429919"
  },
  {
    "text": "request but higher limits than the baseline as you may remember the lowest cost configuration instead had the higher cpu",
    "start": "1429919",
    "end": "1437039"
  },
  {
    "text": "request than the baseline without getting too much into the analysis of this specific configuration",
    "start": "1437039",
    "end": "1443360"
  },
  {
    "text": "what the with this result show is that the optimization goal changes and cpu",
    "start": "1443360",
    "end": "1449520"
  },
  {
    "text": "memory requests and limits may need to be increased or decreased and that multiple parameters at",
    "start": "1449520",
    "end": "1456159"
  },
  {
    "text": "kubernetes and jvm level also need to be tuned accordingly this is a clear clearly a confirmation",
    "start": "1456159",
    "end": "1464400"
  },
  {
    "text": "of the perceived complexity of tuning kubernetes microservices applications",
    "start": "1464400",
    "end": "1470640"
  },
  {
    "text": "as we we're here just discussing just one micro services but think about tuning hundreds of microservices",
    "start": "1470640",
    "end": "1478720"
  },
  {
    "text": "in terms of customer result what did we get uh the red sizing of the service spots",
    "start": "1479039",
    "end": "1484799"
  },
  {
    "text": "allowed a huge cost reduction in the kubernetes environment but where's more we allowed them to automate the tuning",
    "start": "1484799",
    "end": "1492720"
  },
  {
    "text": "of their services or services in a matter of hours instead of months",
    "start": "1492720",
    "end": "1498240"
  },
  {
    "text": "enabling not only the cost reduction but also enabling an improvement in the latency of our application and overall a",
    "start": "1498240",
    "end": "1506159"
  },
  {
    "text": "better user experience experience for the customer",
    "start": "1506159",
    "end": "1510880"
  },
  {
    "text": "there are many interesting configurations that we found out with our study",
    "start": "1512240",
    "end": "1517840"
  },
  {
    "text": "and they maybe were discussing but i think it's time to conclude with a few takeaways",
    "start": "1517840",
    "end": "1524720"
  },
  {
    "text": "first one tune tune tune any inefficiency is not going to be",
    "start": "1524960",
    "end": "1530159"
  },
  {
    "text": "addressed by your kubernetes cluster if you don't think about how to optimize",
    "start": "1530159",
    "end": "1535200"
  },
  {
    "text": "your platform nobody else will and the second takeaway is to this",
    "start": "1535200",
    "end": "1540799"
  },
  {
    "text": "application are too complex you simply don't have time to optimize everything",
    "start": "1540799",
    "end": "1546400"
  },
  {
    "text": "ai powered optimization works and can be the solution to your optimization needs",
    "start": "1546400",
    "end": "1553360"
  },
  {
    "text": "thank you",
    "start": "1553360",
    "end": "1556520"
  },
  {
    "text": "i have a microphone for questions if you're leaving do so very quietly",
    "start": "1562799",
    "end": "1570039"
  },
  {
    "text": "hi hi thanks thanks a lot this is for me at least very very interesting um",
    "start": "1574400",
    "end": "1580960"
  },
  {
    "text": "one question can you speak a bit more about the ai uses like black box",
    "start": "1580960",
    "end": "1586000"
  },
  {
    "text": "optimization like here are some parameters and just find the combination that seems to work best or",
    "start": "1586000",
    "end": "1592559"
  },
  {
    "text": "yeah so we could talk our about the the approach we had i'll just share some",
    "start": "1592559",
    "end": "1597760"
  },
  {
    "text": "insights so base basically uh that is based on referencement learning and we provide as input the parameters",
    "start": "1597760",
    "end": "1605679"
  },
  {
    "text": "that we apply so the value of the parameters that we apply and the score so the score of the last",
    "start": "1605679",
    "end": "1611600"
  },
  {
    "text": "iteration that we had then after analyzing the such score of the iteration the parameters",
    "start": "1611600",
    "end": "1618159"
  },
  {
    "text": "the algorithms the algorithms that we've used learns how it has to behave and",
    "start": "1618159",
    "end": "1623200"
  },
  {
    "text": "propose the new iteration if you want more details we can we can discuss separately because it's a long",
    "start": "1623200",
    "end": "1629200"
  },
  {
    "text": "topic as you may as you may imagine",
    "start": "1629200",
    "end": "1632960"
  },
  {
    "text": "hi hi thanks um how did you took into consideration",
    "start": "1634240",
    "end": "1640399"
  },
  {
    "text": "the cpu memory ratio of the node like",
    "start": "1640399",
    "end": "1645600"
  },
  {
    "text": "you had at some point like the high resiliency was",
    "start": "1645600",
    "end": "1650799"
  },
  {
    "text": "up some something like two cpus and five six gigabytes of memory",
    "start": "1650799",
    "end": "1656880"
  },
  {
    "text": "so i i didn't i didn't really get the question so like you have no the nodes in okay",
    "start": "1656880",
    "end": "1662640"
  },
  {
    "text": "aks or something like that so the nodes in the cluster like have a",
    "start": "1662640",
    "end": "1668240"
  },
  {
    "text": "cpu to memory ratio okay and that kind of like if you have like",
    "start": "1668240",
    "end": "1674880"
  },
  {
    "text": "uh the pod with two cpus and six gigabytes of ram okay",
    "start": "1674880",
    "end": "1681360"
  },
  {
    "text": "like uh you can only put one pod on the node yeah",
    "start": "1681360",
    "end": "1686399"
  },
  {
    "text": "so like i need to take that into consideration the cpu to memory ratio okay so i got it so",
    "start": "1686399",
    "end": "1693039"
  },
  {
    "text": "in this particular case uh what we did is uh that was focused on on the pod",
    "start": "1693039",
    "end": "1698960"
  },
  {
    "text": "sizing not on the let's say on the node sizing or we didn't think about uh auto scaling of",
    "start": "1698960",
    "end": "1706159"
  },
  {
    "text": "the nodes but what we usually see is that the experimental approach",
    "start": "1706159",
    "end": "1711360"
  },
  {
    "text": "allows us to monitor how the system behaves in terms of node capacity while we run so we are able to let's say",
    "start": "1711360",
    "end": "1719200"
  },
  {
    "text": "identify if we have a shortage in the cpu and memory utilization on specific nodes and",
    "start": "1719200",
    "end": "1726720"
  },
  {
    "text": "doing this experimental approach we can do it on one single microservice or we can do with",
    "start": "1726720",
    "end": "1733120"
  },
  {
    "text": "let's say full stack for every microservices which is running a specific node so every container which",
    "start": "1733120",
    "end": "1738720"
  },
  {
    "text": "is which runs in a specific node i don't know if i answered your question but we",
    "start": "1738720",
    "end": "1743919"
  },
  {
    "text": "can discuss it further if you want hi thank you i think my question is a",
    "start": "1743919",
    "end": "1750640"
  },
  {
    "text": "bit similar to the last one it's like if we um if we always this example seems to",
    "start": "1750640",
    "end": "1756799"
  },
  {
    "text": "optimize versus one service but i think a lot of times",
    "start": "1756799",
    "end": "1762320"
  },
  {
    "text": "to minimize the cost you need to optimize against all the servers otherwise i guess the one of the example is if it has um",
    "start": "1762399",
    "end": "1769919"
  },
  {
    "text": "it gets a very large pod uh three cpu and if you're you're not",
    "start": "1769919",
    "end": "1775440"
  },
  {
    "text": "might mean your node cannot um accommodate any other pods and but i",
    "start": "1775440",
    "end": "1781679"
  },
  {
    "text": "imagine if you start tuning against all the micro services that becomes a huge parameter space and then the ai becomes",
    "start": "1781679",
    "end": "1788159"
  },
  {
    "text": "very hard to run or maybe even inefficient yeah how does that work in practice yeah basically it may take longer to find a",
    "start": "1788159",
    "end": "1795360"
  },
  {
    "text": "suitable situation but we can test multiple services together also i can",
    "start": "1795360",
    "end": "1800559"
  },
  {
    "text": "add in this specific case constraints related to the sizing of the node so if",
    "start": "1800559",
    "end": "1805679"
  },
  {
    "text": "i want let's say to be safe in terms of not allocation i can add more constraints in terms of okay you don't",
    "start": "1805679",
    "end": "1812320"
  },
  {
    "text": "use much more of let's say the 30 percent of the capacity on the load but i can i can add it as a constraint",
    "start": "1812320",
    "end": "1819360"
  },
  {
    "text": "in order to let's say define a limit in the space that you have to search against",
    "start": "1819360",
    "end": "1826320"
  },
  {
    "text": "so there's a lot of people following online a couple of questions came in i'm going to read one out does the ai",
    "start": "1827039",
    "end": "1832559"
  },
  {
    "text": "provide a predictive ability yeah yeah the idea is to predict the possible",
    "start": "1832559",
    "end": "1838720"
  },
  {
    "text": "configuration that could lead to an improvement okay so that is an iterative",
    "start": "1838720",
    "end": "1844559"
  },
  {
    "text": "and predictive approach if if you have followed also the the talk before mine",
    "start": "1844559",
    "end": "1850320"
  },
  {
    "text": "uh they were talking about predictive approach this is the case i mean this is a predictive and iterative approach okay",
    "start": "1850320",
    "end": "1859679"
  },
  {
    "text": "hey in one of your slides you said that you found that if you disabled the cpu limits the",
    "start": "1859679",
    "end": "1866480"
  },
  {
    "text": "response times actually reduced yeah as far as i remember there was a linux linux kernel back in 2020 if i'm not",
    "start": "1866480",
    "end": "1874480"
  },
  {
    "text": "mistaken and there was already a bug fix about that and afterwards",
    "start": "1874480",
    "end": "1879679"
  },
  {
    "text": "presumably setting the limits again would actually eliminate the bug",
    "start": "1879679",
    "end": "1886080"
  },
  {
    "text": "and fix the bug actually so my question would be do you still think that uh",
    "start": "1886080",
    "end": "1892080"
  },
  {
    "text": "turning off the limits is the way to go or okay",
    "start": "1892080",
    "end": "1897679"
  },
  {
    "text": "uh in my opinion i mean uh i know that in some cases many kernels fix that that",
    "start": "1897679",
    "end": "1904559"
  },
  {
    "text": "issue on the throttling the idea is that",
    "start": "1904559",
    "end": "1909840"
  },
  {
    "text": "this argument of removing the request and the resource limits",
    "start": "1910080",
    "end": "1915519"
  },
  {
    "text": "is a some kind of um it's by the way it's something like uh a a",
    "start": "1915519",
    "end": "1922960"
  },
  {
    "text": "war between the people who wants to remove the meets and the people who says oh no you have to keep that limit",
    "start": "1922960",
    "end": "1928720"
  },
  {
    "text": "uh the nice part of what we do is that we don't need to take let's say",
    "start": "1928720",
    "end": "1935360"
  },
  {
    "text": "a position since our approach is experimental so if we find out that",
    "start": "1935360",
    "end": "1941200"
  },
  {
    "text": "removing the limits has better improvements on the on the on our",
    "start": "1941200",
    "end": "1947200"
  },
  {
    "text": "application we can remove it obviously by removing the limits we take the risk associated to it okay",
    "start": "1947200",
    "end": "1955518"
  },
  {
    "text": "okay two minutes left hi um i have a question",
    "start": "1955760",
    "end": "1962480"
  },
  {
    "text": "on uh the problem of overfitting to the data so you know in machine learning you",
    "start": "1962480",
    "end": "1967600"
  },
  {
    "text": "typically have this problem that uh you can you know you can your classifier",
    "start": "1967600",
    "end": "1972880"
  },
  {
    "text": "or whatever can uh you know become too sensitive to the training data and not",
    "start": "1972880",
    "end": "1978240"
  },
  {
    "text": "perform well on the real data so i see you're using we are using uh",
    "start": "1978240",
    "end": "1983519"
  },
  {
    "text": "performance tests so some artificial tests right to to set the parameters so like how does",
    "start": "1983519",
    "end": "1989760"
  },
  {
    "text": "it behave well in production and you know how do you solve this problem of being",
    "start": "1989760",
    "end": "1995519"
  },
  {
    "text": "too uh too fitted to the performance test i basically the the",
    "start": "1995519",
    "end": "2000640"
  },
  {
    "text": "health of the accuracy of the test is very important yeah basically uh i mean i i come from a",
    "start": "2000640",
    "end": "2007360"
  },
  {
    "text": "long practice of performance testing so uh performance testing is what i do for a living since started working and",
    "start": "2007360",
    "end": "2013200"
  },
  {
    "text": "basically uh the requirement to to this approach is to find a way to shape a proper mod to",
    "start": "2013200",
    "end": "2020080"
  },
  {
    "text": "shape a proper model of of the of the let's say of the performance test so you",
    "start": "2020080",
    "end": "2025120"
  },
  {
    "text": "do have obviously to use uh the same version of application the same",
    "start": "2025120",
    "end": "2031360"
  },
  {
    "text": "version of the operative system comparable let's say sizing",
    "start": "2031360",
    "end": "2036799"
  },
  {
    "text": "in the production environment where i test the configuration so that is everything that you have to take",
    "start": "2036799",
    "end": "2043120"
  },
  {
    "text": "into account uh and for sure that's the problem of of how significant",
    "start": "2043120",
    "end": "2049679"
  },
  {
    "text": "significative is that is your performance test so uh our this approach is uh well",
    "start": "2049679",
    "end": "2055919"
  },
  {
    "text": "suited for a customer for for people who are very into performance engineering and know how to",
    "start": "2055919",
    "end": "2062720"
  },
  {
    "text": "shape your traffic well but we are also investigating the field of doing it",
    "start": "2062720",
    "end": "2068240"
  },
  {
    "text": "let's say in production with the real data",
    "start": "2068240",
    "end": "2072159"
  },
  {
    "text": "okay last question uh with the jvm based application",
    "start": "2073280",
    "end": "2079040"
  },
  {
    "text": "usually you have a problem when you try to base uh your request to your actual",
    "start": "2079040",
    "end": "2084158"
  },
  {
    "text": "utilization because of the startup time yeah when the application start and the",
    "start": "2084159",
    "end": "2089520"
  },
  {
    "text": "cpu usage go up yeah how you could solve this and the other thing is",
    "start": "2089520",
    "end": "2095520"
  },
  {
    "text": "when i bought this resource and usage actually almost the same i will",
    "start": "2095520",
    "end": "2103040"
  },
  {
    "text": "need to scale fast right so i will need have to ability to scale fast which i still need to add",
    "start": "2103040",
    "end": "2109839"
  },
  {
    "text": "more nodes and yeah so basically uh what we did with the with the jvm topic is that in this",
    "start": "2109839",
    "end": "2116960"
  },
  {
    "text": "particular case because it's not let's say a one fit for all solution okay in",
    "start": "2116960",
    "end": "2122079"
  },
  {
    "text": "this particular case case obviously the best configuration was the one we didn't trigger the replica but if",
    "start": "2122079",
    "end": "2129359"
  },
  {
    "text": "you see into the other configuration which led to a some kind of 15 reduction",
    "start": "2129359",
    "end": "2136160"
  },
  {
    "text": "in cost and that configuration we managed to find a sizing of both the jvm",
    "start": "2136160",
    "end": "2142640"
  },
  {
    "text": "in terms of mini pen money and maxip which were increased where",
    "start": "2142640",
    "end": "2148480"
  },
  {
    "text": "the the gvm wasn't suffering let's say in terms of throttling at the startup of",
    "start": "2148480",
    "end": "2154000"
  },
  {
    "text": "the jvm obviously ijvm always suffers in terms of cpu and starts but",
    "start": "2154000",
    "end": "2159839"
  },
  {
    "text": "that specific configuration in terms of minip maxip and memory request on",
    "start": "2159839",
    "end": "2165839"
  },
  {
    "text": "on on the pod allowed us to cut let's say the the spike in half but that is",
    "start": "2165839",
    "end": "2171440"
  },
  {
    "text": "something you will have to face ever i mean in every situation if you don't want to",
    "start": "2171440",
    "end": "2178480"
  },
  {
    "text": "let's say over um on over oversize your environment you will",
    "start": "2178480",
    "end": "2184480"
  },
  {
    "text": "have you will have some kind of spike when a gvm starts",
    "start": "2184480",
    "end": "2189680"
  },
  {
    "text": "okay okay we're on out of time thank you for being here and get in touch soon",
    "start": "2190480",
    "end": "2198839"
  }
]