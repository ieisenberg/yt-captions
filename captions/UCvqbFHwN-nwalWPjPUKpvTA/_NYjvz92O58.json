[
  {
    "text": "hi welcome to kubecon 2021 in los angeles you're about to learn about some",
    "start": "799",
    "end": "6480"
  },
  {
    "text": "cool new tech that lets kubernetes workloads consume virtualized service from a pool of gpus a gpu does not have",
    "start": "6480",
    "end": "14880"
  },
  {
    "text": "to be installed in the same physical worker node running the pod that is consuming the",
    "start": "14880",
    "end": "20160"
  },
  {
    "text": "gpu i'm steve wong co-chair of the kubernetes vmware user group also",
    "start": "20160",
    "end": "26160"
  },
  {
    "text": "presenting is going to be miles gray from the uk but because of travel difficulties he couldn't be here but he",
    "start": "26160",
    "end": "33760"
  },
  {
    "text": "composed a great recording showing a deep dive with a demo of this technology",
    "start": "33760",
    "end": "40160"
  },
  {
    "text": "a word about the user group we're inclusive of all users running all",
    "start": "40160",
    "end": "45360"
  },
  {
    "text": "uh forms of kubernetes on vmware infrastructure so what i'm talking about",
    "start": "45360",
    "end": "50960"
  },
  {
    "text": "doesn't just apply to people running a vmware distribution uh this should be useful to you if you're running",
    "start": "50960",
    "end": "58480"
  },
  {
    "text": "distributions like anthos eks anywhere openshift rancher or pure upstream uh",
    "start": "58480",
    "end": "65360"
  },
  {
    "text": "kubernetes open source we'll start by talking about why you",
    "start": "65360",
    "end": "71119"
  },
  {
    "text": "might want to use a gpu in your kubernetes workloads then miles is going to show you how to set this up with a",
    "start": "71119",
    "end": "78159"
  },
  {
    "text": "step-by-step demo using kubernetes after you'll get details on how you can join",
    "start": "78159",
    "end": "83600"
  },
  {
    "text": "the user group and a link to download this deck",
    "start": "83600",
    "end": "88159"
  },
  {
    "text": "gpus got the name graphics processing units because they were originally designed to manipulate pixels for images",
    "start": "88640",
    "end": "96560"
  },
  {
    "text": "and video people realize that the parallel processing capabilities could also be applied to many other general",
    "start": "96560",
    "end": "103600"
  },
  {
    "text": "problems if they could be broken down to allow parallel processing and use of",
    "start": "103600",
    "end": "109040"
  },
  {
    "text": "parallel algorithmic solutions machine learning is a common application but there are many more",
    "start": "109040",
    "end": "116479"
  },
  {
    "text": "super computer pioneer seymour cray is said to have coined the marketing line",
    "start": "116719",
    "end": "122000"
  },
  {
    "text": "which would you rather use a couple of strong oxen or a thousand chickens he",
    "start": "122000",
    "end": "127040"
  },
  {
    "text": "said this back in the 90s well advancements in both hardware and software have",
    "start": "127040",
    "end": "133680"
  },
  {
    "text": "changed the terrain and today you want the chickens now before some nitpicker in the audience looks at that picture",
    "start": "133680",
    "end": "140560"
  },
  {
    "text": "carefully and points out that those are actually ducks and not chickens give me a little leeway and i want you to paint",
    "start": "140560",
    "end": "147520"
  },
  {
    "text": "an even different picture visualize this in your head you can keep the ox",
    "start": "147520",
    "end": "152959"
  },
  {
    "text": "but you know lose the ducks or chickens but i want you to bring in a shark",
    "start": "152959",
    "end": "158720"
  },
  {
    "text": "and a school of piranha uh now imagine somebody is giving you the",
    "start": "158720",
    "end": "164480"
  },
  {
    "text": "goal of creating a museum exhibit of an ox skeleton",
    "start": "164480",
    "end": "171280"
  },
  {
    "text": "at the la uh county natural history museum and that museum by the way is",
    "start": "171280",
    "end": "176480"
  },
  {
    "text": "just a short distance that way and i highly recommend it living in los angeles if you've got some spare time to",
    "start": "176480",
    "end": "182400"
  },
  {
    "text": "kill but back to the mission we start with an ox and we want an ox skeleton",
    "start": "182400",
    "end": "189360"
  },
  {
    "text": "you could take that shark oops accidentally invents my slide you could",
    "start": "189360",
    "end": "194720"
  },
  {
    "text": "take that shark put it with the ox in a pool and that shark is going to take big",
    "start": "194720",
    "end": "200080"
  },
  {
    "text": "bites and digest them bigger bites than a piranha could but if you unleash that school of piranha",
    "start": "200080",
    "end": "207599"
  },
  {
    "text": "they are going to be able to access that from the pool on all the sides the top and the bottom you're just going to see",
    "start": "207599",
    "end": "214720"
  },
  {
    "text": "a cloud burst of activity and poof an ox skeleton",
    "start": "214720",
    "end": "221120"
  },
  {
    "text": "this is this is what this gpu scenario of parallel processing is like",
    "start": "221120",
    "end": "226720"
  },
  {
    "text": "for the right kind of job this is a big winner",
    "start": "226720",
    "end": "231840"
  },
  {
    "text": "another reason for gpus the growth rate in conventional cpus has been leveling",
    "start": "232159",
    "end": "237360"
  },
  {
    "text": "off compared to gpus uh maybe cpus are just reaching a material maturity level",
    "start": "237360",
    "end": "244000"
  },
  {
    "text": "and the gpu situation is still young so they're they're growing faster",
    "start": "244000",
    "end": "251599"
  },
  {
    "text": "and by the way this isn't just time to result uh there are",
    "start": "252319",
    "end": "257919"
  },
  {
    "text": "often energy advantages to using these gpus if you can get the same job done",
    "start": "257919",
    "end": "263440"
  },
  {
    "text": "by while burning fewer kilowatt hours it helps the planet another possible tie-in",
    "start": "263440",
    "end": "269600"
  },
  {
    "text": "that might save energy if you're doing something like deep learning applications usually the",
    "start": "269600",
    "end": "275360"
  },
  {
    "text": "training stage is very uh gpu intensive but then after you've done the training",
    "start": "275360",
    "end": "281919"
  },
  {
    "text": "and take it to the run time to actually execute using your model",
    "start": "281919",
    "end": "287600"
  },
  {
    "text": "you can get by on a fraction of a gpu's capacity what if you could eliminate the wasted",
    "start": "287600",
    "end": "293919"
  },
  {
    "text": "money and wasted energy by pooling capacity to increase your efficiency",
    "start": "293919",
    "end": "300960"
  },
  {
    "text": "so i'm going to move on to the demo by miles",
    "start": "300960",
    "end": "305520"
  },
  {
    "text": "i hope you're having a great q con so far so over the next couple of minutes i am going to demo to you our",
    "start": "307199",
    "end": "314720"
  },
  {
    "text": "aiml framework for running those kinds of applications on top of kubernetes on top of docker",
    "start": "314720",
    "end": "322560"
  },
  {
    "text": "even nvms if you want but you know we're focusing on the kubernetes aspect of this uh today so what we're going to do",
    "start": "322560",
    "end": "329280"
  },
  {
    "text": "is take a brief look at the lab that this is running in so it's running on all ga code all fully released code",
    "start": "329280",
    "end": "335360"
  },
  {
    "text": "uh it's vsphere 6.7 6.7 vsphere 7.0 u2",
    "start": "335360",
    "end": "341600"
  },
  {
    "text": "um and we're running a tantu community edition and then on top of 102 community edition we're going to deploy this",
    "start": "341600",
    "end": "347919"
  },
  {
    "text": "application and the bitfusion integration for kubernetes so let's have a look at just the vms which is this",
    "start": "347919",
    "end": "354720"
  },
  {
    "text": "sort of base level that we're at here you can see that we have four bit fusion servers here and if i go into edit one",
    "start": "354720",
    "end": "361120"
  },
  {
    "text": "of these what you will see inside the bitfusion servers is that they have a gpu attached so each",
    "start": "361120",
    "end": "368240"
  },
  {
    "text": "one of these has a nvidia tesla d4 attached to it and in case you're not sort of aware of",
    "start": "368240",
    "end": "374080"
  },
  {
    "text": "how bitfusion works but fusion is a client server model so these servers in",
    "start": "374080",
    "end": "379120"
  },
  {
    "text": "this instance are vms and they have a gpu passed up into them",
    "start": "379120",
    "end": "385280"
  },
  {
    "text": "your clients then can dial into the servers and request gpu resources so say",
    "start": "385280",
    "end": "392240"
  },
  {
    "text": "you have a application and you say i would like it to consume half of the gpu",
    "start": "392240",
    "end": "398080"
  },
  {
    "text": "then the bitfusion client running in either inside the container or on the nodes and we'll get into that in in a",
    "start": "398080",
    "end": "404800"
  },
  {
    "text": "second here is going to request from bit fusion you know half of a gpu if it can fulfill",
    "start": "404800",
    "end": "411360"
  },
  {
    "text": "that request then what will happen is bitfusion will intercept all the cuda calls that go to the cuda apis",
    "start": "411360",
    "end": "418639"
  },
  {
    "text": "that would need gpu acceleration and instead sends those over the network to the bitfusion server which does the",
    "start": "418639",
    "end": "425039"
  },
  {
    "text": "calculation on the gpu and sends back the results so bitfusion abstracts",
    "start": "425039",
    "end": "431120"
  },
  {
    "text": "the gpu from the workload itself so there's no need to have a gpu mounted",
    "start": "431120",
    "end": "436639"
  },
  {
    "text": "into every single vm that is consuming or has a workload that would like to consume a gpu",
    "start": "436639",
    "end": "442639"
  },
  {
    "text": "instead you can consolidate your gpus into as we have here we've got one gpu in",
    "start": "442639",
    "end": "448240"
  },
  {
    "text": "each host therefore we have four bit fusion server vms each with one gpu and",
    "start": "448240",
    "end": "453360"
  },
  {
    "text": "that means that we don't have to have a one-to-one mapping from gpu to",
    "start": "453360",
    "end": "458880"
  },
  {
    "text": "application instead we can slice up that gpu into an arbitrary",
    "start": "458880",
    "end": "464560"
  },
  {
    "text": "number of frame buffer allocations so each of these gpus has 16 gigs of vram or frame",
    "start": "464560",
    "end": "471919"
  },
  {
    "text": "buffer more accurately so you can divide that up into however many slices that you like and gpu and bitfusion will",
    "start": "471919",
    "end": "479360"
  },
  {
    "text": "allow the client to claim that much of the gpu so you'll see as we get into this why this",
    "start": "479360",
    "end": "485759"
  },
  {
    "text": "is particularly beneficial to kubernetes workloads given the non-deterministic",
    "start": "485759",
    "end": "490879"
  },
  {
    "text": "placement of them the fact that there are many of those and only you know a limited subset of gpus",
    "start": "490879",
    "end": "498800"
  },
  {
    "text": "so anyway we've got our bitfusion servers there they are going to be the things that are going to do the actual",
    "start": "498800",
    "end": "504240"
  },
  {
    "text": "processing of our application code that requires acceleration",
    "start": "504240",
    "end": "509840"
  },
  {
    "text": "inside this resource pool here you can see tce management and a workload cluster",
    "start": "509840",
    "end": "515440"
  },
  {
    "text": "here tanzu community edition that is so steve that set this up for me very kindly last week so you can see we've",
    "start": "515440",
    "end": "520959"
  },
  {
    "text": "got our management cluster and we've got our workload cluster here and we've also got rancher in here as",
    "start": "520959",
    "end": "526640"
  },
  {
    "text": "well this is not um a bit fusion that is bitfusion is not specific to tanzu this is not something",
    "start": "526640",
    "end": "533360"
  },
  {
    "text": "that only works with tanza this will work with basically any distro of kubernetes that you have as long as it's",
    "start": "533360",
    "end": "538560"
  },
  {
    "text": "vanilla compliant so that's the vm view out of the way let's",
    "start": "538560",
    "end": "543760"
  },
  {
    "text": "have a look at bit fusion itself before we get into this so here you can see",
    "start": "543760",
    "end": "549519"
  },
  {
    "text": "the current total number of gpus that are available so you can see at the top here in purple it says we have four gpus",
    "start": "549519",
    "end": "556320"
  },
  {
    "text": "available total allocation is zero we're not actually running any workloads on the left hand side you see the the",
    "start": "556320",
    "end": "562800"
  },
  {
    "text": "servers that's the same servers that we just saw and clients it says we have 177. now you might look at that and go",
    "start": "562800",
    "end": "568800"
  },
  {
    "text": "hmm i don't really understand why there's 177 clients but nothing is being consumed",
    "start": "568800",
    "end": "574320"
  },
  {
    "text": "so the way that fusion works is it assumes a gpu as a service model so",
    "start": "574320",
    "end": "580160"
  },
  {
    "text": "we're assuming that the gpus are sort of being leased out to these tenants or internal",
    "start": "580160",
    "end": "587279"
  },
  {
    "text": "customers for and and their builds based on how much time they have used those for so",
    "start": "587279",
    "end": "593120"
  },
  {
    "text": "each of these clients is actually kept as a historical record so you can say okay this consumed 30 minutes of gpu",
    "start": "593120",
    "end": "599760"
  },
  {
    "text": "this consumed an hour of gpu so you can do chargeback and that kind of stuff so that's why we keep all of our clients",
    "start": "599760",
    "end": "605360"
  },
  {
    "text": "there whether or not they're actively consuming gpu so the first thing that we're going to do here is we're going to add bitfusion",
    "start": "605360",
    "end": "613920"
  },
  {
    "text": "or the bitfusion credentials more accurately to our kubernetes cluster to our tce cluster",
    "start": "613920",
    "end": "620720"
  },
  {
    "text": "and this is brand new in bitfusion4 and to be honest i think this is a really really killer feature so if we go",
    "start": "620720",
    "end": "626399"
  },
  {
    "text": "kubernetes clusters we click add it says cubeconfig and i've got my tceq config",
    "start": "626399",
    "end": "631680"
  },
  {
    "text": "file here so click upload and we'll call it tce01",
    "start": "631680",
    "end": "637200"
  },
  {
    "text": "and you can see it's grabbed the ip address from the cubeconfig file automatically connected and pull back",
    "start": "637200",
    "end": "642959"
  },
  {
    "text": "the namespaces so it already knows what namespaces are in there so i want to add my bitfusion",
    "start": "642959",
    "end": "649680"
  },
  {
    "text": "credentials so that would be what are the server ip addresses what is the ca search for the servers that i can",
    "start": "649680",
    "end": "656320"
  },
  {
    "text": "validate it and what are my client credentials for actually connecting to the servers so i'm going to add that to",
    "start": "656320",
    "end": "662480"
  },
  {
    "text": "three namespaces we're going to add it to cube system we're going to add it to flower market which is where we're",
    "start": "662480",
    "end": "668160"
  },
  {
    "text": "actually going to deploy our application and we're going to deploy it to the bitfusion",
    "start": "668160",
    "end": "673680"
  },
  {
    "text": "with kubernetes integration namespace as well so we clicked add okay so now these are",
    "start": "673680",
    "end": "679519"
  },
  {
    "text": "our target clusters right so um you can see here this is sort of a",
    "start": "679519",
    "end": "685360"
  },
  {
    "text": "one-to-many relationship you can have many kubernetes clusters and you can even have different tokens",
    "start": "685360",
    "end": "691360"
  },
  {
    "text": "for different name spaces inside the same cluster depends what way you want to do your permissions model so we've added our cluster and now we",
    "start": "691360",
    "end": "698079"
  },
  {
    "text": "need to create a client token so that those workloads can actually connect to bitfusion and consume some gpu resources",
    "start": "698079",
    "end": "705600"
  },
  {
    "text": "so we'll click create and we'll just call it flower market",
    "start": "705600",
    "end": "712320"
  },
  {
    "text": "token and we're just going to add it to all three of those you can see you can pick and choose here so you can have many",
    "start": "712480",
    "end": "719279"
  },
  {
    "text": "tokens if you want to to do individual permissions models and that kind of thing but we're just going to",
    "start": "719279",
    "end": "724800"
  },
  {
    "text": "pick all of them and it says activate token after creation so that means that it'll deploy the token to the kubernetes",
    "start": "724800",
    "end": "731680"
  },
  {
    "text": "cluster in the form of some secrets and actually let me show you that before we do it so if i do k get secret",
    "start": "731680",
    "end": "740000"
  },
  {
    "text": "and hopefully we won't see yeah so there should be three secrets in",
    "start": "740000",
    "end": "746560"
  },
  {
    "text": "here though and you'll see them uh whenever they pop up but basically that will be the ca servers and the client",
    "start": "746560",
    "end": "752399"
  },
  {
    "text": "configuration file so they do not exist here so what it's asking us here when we",
    "start": "752399",
    "end": "757839"
  },
  {
    "text": "say activate token after creation is once it's populated those secrets into kubernetes is that token able to make",
    "start": "757839",
    "end": "764480"
  },
  {
    "text": "requests of the bitfusion server and you can disable it at any time as well so if",
    "start": "764480",
    "end": "770000"
  },
  {
    "text": "you've decided that you need the gpu resources for something else you can deactivate that token without",
    "start": "770000",
    "end": "776320"
  },
  {
    "text": "actually deleting it and changing the configuration but anytime someone connects they'll just get it no sorry",
    "start": "776320",
    "end": "782079"
  },
  {
    "text": "your token has been deactivated so we're going to go ahead and click create you can see it says it's activated it's",
    "start": "782079",
    "end": "788079"
  },
  {
    "text": "added it to these namespaces so if we go back into our kate's cluster we should hopefully see straight away the three new secrets and",
    "start": "788079",
    "end": "794880"
  },
  {
    "text": "we do so we've got our like i said our ca cert our client configuration and our",
    "start": "794880",
    "end": "800760"
  },
  {
    "text": "servers.conf so that's good that means that we can now mount those secrets into an",
    "start": "800760",
    "end": "807279"
  },
  {
    "text": "integration that allows us to connect to the bitfusion instance and the nice part of this about it is it's dynamic right",
    "start": "807279",
    "end": "814480"
  },
  {
    "text": "you just add the cube config file you choose your namespaces and it auto populates it's not you know download",
    "start": "814480",
    "end": "820320"
  },
  {
    "text": "credentials create generic secret all that kind of stuff and you can rotate secrets a lot easier with this the way",
    "start": "820320",
    "end": "826560"
  },
  {
    "text": "the integration is built so the first thing we're going to do is we're going to take a look at the",
    "start": "826560",
    "end": "832160"
  },
  {
    "text": "bitfusion with kubernetes integration and i'll just pull up the github page here",
    "start": "832160",
    "end": "838800"
  },
  {
    "text": "so you can get this at github.com vmware bitfusion with kubernetes integration",
    "start": "838800",
    "end": "845360"
  },
  {
    "text": "and the bit that you're interested in here is the bitfusiondevice plugin so kate's has a concept of device plugins",
    "start": "845360",
    "end": "852000"
  },
  {
    "text": "it's generally used for like hardware that you have that has specific drivers that need to run on the gas os",
    "start": "852000",
    "end": "858560"
  },
  {
    "text": "of the the node itself alongside cubelet to make them you know work so we have a",
    "start": "858560",
    "end": "864399"
  },
  {
    "text": "similar sort of thing with bitfusion here it goes about things a little bit differently but it does install some libraries and some drivers that are",
    "start": "864399",
    "end": "870959"
  },
  {
    "text": "required um but basically it's in two parts right and we'll look at the the architecture here to just to exemplify",
    "start": "870959",
    "end": "878160"
  },
  {
    "text": "that so what you see here is your standard in green by the way is your deployment and",
    "start": "878160",
    "end": "883920"
  },
  {
    "text": "the job for that deployment if you're doing job-based uh batch processing or whatever",
    "start": "883920",
    "end": "889279"
  },
  {
    "text": "and then you've got your bitfusion device plug-in and that is essentially doing the connection between the worker",
    "start": "889279",
    "end": "894800"
  },
  {
    "text": "node and the bitfusion server so that creates that pipe between cuda on the",
    "start": "894800",
    "end": "900160"
  },
  {
    "text": "client and cuda on the server to pass those calls over the network so you can see here it says it consists",
    "start": "900160",
    "end": "906320"
  },
  {
    "text": "of two bits you've got the device plugin itself which is the thing that actually does you know the the data path bit",
    "start": "906320",
    "end": "913680"
  },
  {
    "text": "and then you've also got the bitfusion web hook and the bitfusion web hook is looking for a certain type of object",
    "start": "913680",
    "end": "921680"
  },
  {
    "text": "in kubernetes with some annotations and if it sees those annotations it'll do some transformation on it because it's a",
    "start": "921680",
    "end": "927600"
  },
  {
    "text": "mutating web hook and it'll rewrite them to the kubernetes api and we'll have a look at that now as well",
    "start": "927600",
    "end": "932959"
  },
  {
    "text": "so uh i've already deployed this because it takes a little bit to deploy and i'm just going to show you how this actually",
    "start": "932959",
    "end": "938800"
  },
  {
    "text": "mounts the secrets in just that you're aware of where the integration from bitfusion server in 4.0 and the",
    "start": "938800",
    "end": "945519"
  },
  {
    "text": "bitfusion kubernetes integration is so if we look at our actual deployment yaml",
    "start": "945519",
    "end": "950639"
  },
  {
    "text": "file here it's passed in as a config map and if we go down to the bottom you can see here it's mounting in ca",
    "start": "950639",
    "end": "958000"
  },
  {
    "text": "client and servers.conf so that is then telling the device plugin here's how you connect a bit",
    "start": "958000",
    "end": "964639"
  },
  {
    "text": "fusion here are the servers here your credentials and this is how you authenticate the connection right",
    "start": "964639",
    "end": "970079"
  },
  {
    "text": "so we've already deployed the fusion with kubernetes integration we're not going to look at that what we're going",
    "start": "970079",
    "end": "975279"
  },
  {
    "text": "to do is deploy an app and i'm going to show you the docker file just to show you that we don't have any pre-baked",
    "start": "975279",
    "end": "982000"
  },
  {
    "text": "bits in there right there's there's no bit fusion client in there there's nothing like that it's just a a standard",
    "start": "982000",
    "end": "987839"
  },
  {
    "text": "container just runs python that's it and what the bit fusion with kubernetes integration is going to do",
    "start": "987839",
    "end": "994320"
  },
  {
    "text": "is look for the annotations and we'll look at the annotations in a second and if it sees those it will automatically",
    "start": "994320",
    "end": "1000320"
  },
  {
    "text": "inject a init container onto that deployment and put the fusion client into it and",
    "start": "1000320",
    "end": "1007120"
  },
  {
    "text": "then append all of the the entry point for that docker file",
    "start": "1007120",
    "end": "1013519"
  },
  {
    "text": "onto the end of the command essentially proxying the entire container's connection to to cuda through the",
    "start": "1013519",
    "end": "1020079"
  },
  {
    "text": "network completely transparently it requires no changes to your app code whatsoever",
    "start": "1020079",
    "end": "1025600"
  },
  {
    "text": "so what we're going to do is just look at the docker file to begin with because it's really really simple you can see we're making some very",
    "start": "1025600",
    "end": "1031600"
  },
  {
    "text": "minimal changes we're taking the nvidia tensorflow container from",
    "start": "1031600",
    "end": "1036640"
  },
  {
    "text": "upstream on nvcr.io we are then telling it we want to execute this benchmark python script",
    "start": "1036640",
    "end": "1043520"
  },
  {
    "text": "right it's just a a standard tensorflow benchmark uh it uses gpu",
    "start": "1043520",
    "end": "1049280"
  },
  {
    "text": "acceleration but it's not actually doing any real work it's just demonstrating you know work on the gpu",
    "start": "1049280",
    "end": "1055360"
  },
  {
    "text": "so we're specifying the number of batches the batch size data format you know model all that kind of stuff so",
    "start": "1055360",
    "end": "1061840"
  },
  {
    "text": "this is essentially just setting up tensorflow and telling it what benchmark are you going to run what model are you going to use and what are you going to",
    "start": "1061840",
    "end": "1069360"
  },
  {
    "text": "do the processing on so local parameter device gpu so it's going to do it use a gpu",
    "start": "1069360",
    "end": "1075440"
  },
  {
    "text": "if we come down we just pip install some packages because this exposes some information to",
    "start": "1075440",
    "end": "1081360"
  },
  {
    "text": "prometheus we install a prometheus client uh we then clone the repo for itself down so that it can get the",
    "start": "1081360",
    "end": "1089919"
  },
  {
    "text": "python script and then we execute the python script so you can see here entry point python and then the name of the",
    "start": "1090480",
    "end": "1096720"
  },
  {
    "text": "file along with all the other model stuff that we saw further up as well as telling it to run on a gpu so",
    "start": "1096720",
    "end": "1103520"
  },
  {
    "text": "that is the container we're working with right it's essentially a base nvidia tensorflow container it's just",
    "start": "1103520",
    "end": "1110160"
  },
  {
    "text": "running python that's it there's no other bits or pieces in there so this is like your standard machine learning",
    "start": "1110160",
    "end": "1116400"
  },
  {
    "text": "container now what i want to show you is that we can take that standard machine learning",
    "start": "1116400",
    "end": "1122160"
  },
  {
    "text": "container which if we just ran it on the cluster without any gpus attached it",
    "start": "1122160",
    "end": "1127919"
  },
  {
    "text": "would fail right because it's trying to use a gpu and there's no gpu directly attached to the cluster the gpu is",
    "start": "1127919",
    "end": "1134240"
  },
  {
    "text": "mounted you know somewhere else in the infrastructure entirely so this wouldn't actually work",
    "start": "1134240",
    "end": "1139440"
  },
  {
    "text": "but what i want to show you is that whenever we deploy it with the kubernetes integration for bit fusion",
    "start": "1139440",
    "end": "1145600"
  },
  {
    "text": "it will dynamically inject a bit fusion client into it and allow those gpu calls",
    "start": "1145600",
    "end": "1150960"
  },
  {
    "text": "to succeed so let's have a look at our manifests here",
    "start": "1150960",
    "end": "1157600"
  },
  {
    "text": "the only one we're going to look at is the deployment all the rest of it is fairly uninteresting we've got our name",
    "start": "1157600",
    "end": "1162799"
  },
  {
    "text": "namespace service monitor service horizontal pod autoscaler that kind of stuff but the uh the bit that actually",
    "start": "1162799",
    "end": "1169760"
  },
  {
    "text": "ties this to the bitfusion integration is just some annotations here",
    "start": "1169760",
    "end": "1174799"
  },
  {
    "text": "so you can see this is just a standard deployment it's called a new hope wookie there's a whole other presentation on",
    "start": "1174799",
    "end": "1181520"
  },
  {
    "text": "that which you'll find if you google the dutch v-mug from from last year",
    "start": "1181520",
    "end": "1186960"
  },
  {
    "text": "you'll find what this whole story is about but essentially we've got our same container",
    "start": "1186960",
    "end": "1192640"
  },
  {
    "text": "that we were just looking at here the a new hope worker latest and you can see the the python command is exactly the",
    "start": "1192640",
    "end": "1198799"
  },
  {
    "text": "same as the one that was that was in the container itself and we've added these three annotations so we've got auto",
    "start": "1198799",
    "end": "1205120"
  },
  {
    "text": "management bit fusion set to yes so that essentially says enable the bit fusion",
    "start": "1205120",
    "end": "1210640"
  },
  {
    "text": "integration for this uh this deployment please what os would you like it to use to do",
    "start": "1210640",
    "end": "1216880"
  },
  {
    "text": "the bootstrapping so i just chose ubuntu 20 because it's the latest one and what version of the bitfusion client",
    "start": "1216880",
    "end": "1222720"
  },
  {
    "text": "would you like to use this generally needs to be matched to your server so my server is running 4.0.1 so my bitfusion",
    "start": "1222720",
    "end": "1229360"
  },
  {
    "text": "client version is 4.01 so that's essentially it we then add our",
    "start": "1229360",
    "end": "1235120"
  },
  {
    "text": "limits to our container just under our resources so you can see we are requesting bit fusion gpu and one so",
    "start": "1235120",
    "end": "1241440"
  },
  {
    "text": "that's the number of gpus so one gpu and what percentage of that gpu would we",
    "start": "1241440",
    "end": "1246640"
  },
  {
    "text": "like to use so we're asking for 100 so we are essentially expecting to get a full gpu allocation",
    "start": "1246640",
    "end": "1254799"
  },
  {
    "text": "here so i'm going to go ahead and do a k apply f and it's in",
    "start": "1254799",
    "end": "1261679"
  },
  {
    "text": "i'm in the wrong folder so let's fix that first so it's the new hope app",
    "start": "1261679",
    "end": "1268240"
  },
  {
    "text": "and we'll do k apply dash f manifests",
    "start": "1268240",
    "end": "1273280"
  },
  {
    "text": "all right so you can see i had most of the rest of this already deployed out here horizontal plot",
    "start": "1275039",
    "end": "1280320"
  },
  {
    "text": "autoscaler always says configured for some reason and you can see our deployment has been created so if i do",
    "start": "1280320",
    "end": "1286720"
  },
  {
    "text": "k get deploy you can see that it's 15 seconds old and",
    "start": "1286720",
    "end": "1292640"
  },
  {
    "text": "it's already up and running and if i do a k get pod",
    "start": "1292640",
    "end": "1297919"
  },
  {
    "text": "you can see that that container is running now what i want to show you before we look",
    "start": "1298080",
    "end": "1303679"
  },
  {
    "text": "at the container logs or maybe maybe we'll look at the container logs really quick i will do k logs",
    "start": "1303679",
    "end": "1309440"
  },
  {
    "text": "and it's going to be a new hub and we'll follow",
    "start": "1309440",
    "end": "1316400"
  },
  {
    "text": "so you can see here it says adding visible gpu devices zero",
    "start": "1316400",
    "end": "1323120"
  },
  {
    "text": "that might look weird as if it added zero gpu devices that's the number of the gpu the index",
    "start": "1323120",
    "end": "1329440"
  },
  {
    "text": "of the gpu so it's gpu with index zero right so the the first gpu or the zeroth",
    "start": "1329440",
    "end": "1335280"
  },
  {
    "text": "gpu if we go a little further up you should be able to see",
    "start": "1335280",
    "end": "1341280"
  },
  {
    "text": "where it mounts the gpu in",
    "start": "1341280",
    "end": "1347440"
  },
  {
    "text": "there you go so you can see the bitfusion client stuff has already been mounted into the container so that was all done by an",
    "start": "1347440",
    "end": "1354559"
  },
  {
    "text": "init container that was injected into this so if we do a k describe",
    "start": "1354559",
    "end": "1359760"
  },
  {
    "text": "po a new hope",
    "start": "1359760",
    "end": "1363760"
  },
  {
    "text": "so if we have a look up here you should see that there a",
    "start": "1368880",
    "end": "1375039"
  },
  {
    "text": "container wookie which is the standard one that's this one that is in the manifest here as you can see",
    "start": "1375039",
    "end": "1381919"
  },
  {
    "text": "uh but we should also see a init container as well here you go so",
    "start": "1381919",
    "end": "1388720"
  },
  {
    "text": "there's our init container and you can see that i have not i've not specified an init container in",
    "start": "1388720",
    "end": "1394559"
  },
  {
    "text": "this manifest anywhere that's been injected by the integration so you can see it's doing some command line foo",
    "start": "1394559",
    "end": "1400480"
  },
  {
    "text": "here it's copying the bitfusion client service.conf as well as the uh",
    "start": "1400480",
    "end": "1407600"
  },
  {
    "text": "the debian file or the bits to actually run the client it's copying those into the workload container and then it's",
    "start": "1407600",
    "end": "1414080"
  },
  {
    "text": "running it's adjusting the command so you can see here it says the",
    "start": "1414080",
    "end": "1420320"
  },
  {
    "text": "command for the wookie now is actually not just the python stuff that we had",
    "start": "1420320",
    "end": "1425840"
  },
  {
    "text": "already specified but rather it has injected the bitfusion client in and is",
    "start": "1425840",
    "end": "1430960"
  },
  {
    "text": "now executing the command through bit bit fusion so you can see it's doing a bit fusion run dash n1",
    "start": "1430960",
    "end": "1437520"
  },
  {
    "text": "number of gpus one and how much of the gpus so dash p is partial 1.0 so it",
    "start": "1437520",
    "end": "1443440"
  },
  {
    "text": "means the whole gpu so you can see that is injected the",
    "start": "1443440",
    "end": "1449440"
  },
  {
    "text": "bitfusion client and it is now running that benchmark through the um the gpu there so if you look up",
    "start": "1449440",
    "end": "1456720"
  },
  {
    "text": "at the top here you can see it says requesting gpus with 15 gigs of memory",
    "start": "1456720",
    "end": "1462080"
  },
  {
    "text": "right it's a 16 gig gpu obviously they're using different units here but you can see that that's now",
    "start": "1462080",
    "end": "1467760"
  },
  {
    "text": "running in the background if we go into our ui and we go to our cluster",
    "start": "1467760",
    "end": "1475600"
  },
  {
    "text": "you can now see that we've got pit fusion server one of them is having its",
    "start": "1475600",
    "end": "1480720"
  },
  {
    "text": "full gpu allocated out and you can see the client here one of one gpu is fully allocated so if we click on the client",
    "start": "1480720",
    "end": "1487600"
  },
  {
    "text": "it'll take us through there you go currently allocated one of one if we change that to five",
    "start": "1487600",
    "end": "1493520"
  },
  {
    "text": "minutes so you can see it's on the ramp up and there it's using a full gpu and it must have finished so it's on its",
    "start": "1493520",
    "end": "1499279"
  },
  {
    "text": "ramp back down again so you can see that that has successfully mounted a remote gpu over the network",
    "start": "1499279",
    "end": "1507679"
  },
  {
    "text": "from a bitfusion server into a container that had no knowledge of bitfusion or the",
    "start": "1507679",
    "end": "1514480"
  },
  {
    "text": "bitfusion client whatsoever so if we go back in here we should see",
    "start": "1514480",
    "end": "1519520"
  },
  {
    "text": "once it's finished you know process so many images per second and that kind of thing from the benchmark script but you can",
    "start": "1519520",
    "end": "1526400"
  },
  {
    "text": "see here that essentially this container has zero knowledge of bit fusion",
    "start": "1526400",
    "end": "1531919"
  },
  {
    "text": "whatsoever right it's just a python container and the bit fusion with kubernetes integration along with the bitfusion4 um",
    "start": "1531919",
    "end": "1540799"
  },
  {
    "text": "you know token copy mechanism allows uh these containers to connect to a",
    "start": "1540799",
    "end": "1546880"
  },
  {
    "text": "bitfusion server without any knowledge of the client without having to be adjusted in any way",
    "start": "1546880",
    "end": "1553039"
  },
  {
    "text": "uh i'll just show you for for clarity how i used to have to do this so",
    "start": "1553039",
    "end": "1558320"
  },
  {
    "text": "if we go into my docker file here you can see i have a whole bunch of stuff that's been commented out here and that's the",
    "start": "1558320",
    "end": "1564880"
  },
  {
    "text": "way that we had used to have to do it with bitfusion 3.5 so i had to to bake",
    "start": "1564880",
    "end": "1570320"
  },
  {
    "text": "into the container the bitfusion client bits as well as all the configuration for those so",
    "start": "1570320",
    "end": "1576159"
  },
  {
    "text": "i had to bake in the number of gpus how much of the gpu any variables along with",
    "start": "1576159",
    "end": "1582080"
  },
  {
    "text": "like the servers file the ca file it made it really fragile um it worked",
    "start": "1582080",
    "end": "1588080"
  },
  {
    "text": "but it meant that any time uh the servers list changed you'd have to rebuild your",
    "start": "1588080",
    "end": "1594000"
  },
  {
    "text": "container right or if you needed to issue a new token you had to rebuild your container it's just stuff that you",
    "start": "1594000",
    "end": "1599840"
  },
  {
    "text": "don't want to have to deal with it wasn't necessary so you can see all of this complexity around installing the",
    "start": "1599840",
    "end": "1605520"
  },
  {
    "text": "bitfusion client everything like that is completely removed from the container and instead now we can take off the",
    "start": "1605520",
    "end": "1611120"
  },
  {
    "text": "shelf gpu accelerated containers pass them through the bit fusion with kubernetes integration that we've",
    "start": "1611120",
    "end": "1617200"
  },
  {
    "text": "released again github.com vmware bitfusion with kubernetes",
    "start": "1617200",
    "end": "1622320"
  },
  {
    "text": "integration and this will just automatically bootstrap bit fusion into any of them and allow you to",
    "start": "1622320",
    "end": "1628720"
  },
  {
    "text": "uh allocate gpus to containers without any modification whatsoever and",
    "start": "1628720",
    "end": "1634559"
  },
  {
    "text": "i think with kubernetes this is particularly powerful because with you",
    "start": "1634559",
    "end": "1640000"
  },
  {
    "text": "know more standard uh step steady state workloads like uh vms or even you know",
    "start": "1640000",
    "end": "1646000"
  },
  {
    "text": "containers running in vms under docker or something like that where it's it's not being constantly spun up spun",
    "start": "1646000",
    "end": "1651760"
  },
  {
    "text": "down descheduled or it's not job-based or you know that kind of thing where you could just mount a gpu into a vm and it",
    "start": "1651760",
    "end": "1658159"
  },
  {
    "text": "was fine to live there with kubernetes because placement is you know pseudo non-deterministic that means it could",
    "start": "1658159",
    "end": "1665039"
  },
  {
    "text": "land anywhere in the cluster even on a different cluster or whatever so instead of having to buy gpus for every single",
    "start": "1665039",
    "end": "1670559"
  },
  {
    "text": "one of those just in case well now that you can have a consolidated set of gpus",
    "start": "1670559",
    "end": "1676720"
  },
  {
    "text": "and just allow the bitfusion client to do all of the connection stuff for you so it means",
    "start": "1676720",
    "end": "1683840"
  },
  {
    "text": "that you don't have to continually you know unmount and remount gpus into things or",
    "start": "1683840",
    "end": "1689600"
  },
  {
    "text": "you know worry about is this being underutilized over here are they actually using that resource",
    "start": "1689600",
    "end": "1695039"
  },
  {
    "text": "so that was our kind of uh quick high level whistle stop tour of the",
    "start": "1695039",
    "end": "1700640"
  },
  {
    "text": "fusion 4 integration with kubernetes so that's this bit here under our tokens",
    "start": "1700640",
    "end": "1705760"
  },
  {
    "text": "you can now add kubernetes clusters direct to bit fusion as well as the open source bit fusion with kubernetes",
    "start": "1705760",
    "end": "1712399"
  },
  {
    "text": "integration project that allows you to take standard off-the-shelf unmodified containers and run them with gpu",
    "start": "1712399",
    "end": "1718960"
  },
  {
    "text": "acceleration so i hope that's been useful if you need any more information we'll put some stuff",
    "start": "1718960",
    "end": "1725120"
  },
  {
    "text": "in the chat so that you can have a look at each of these repositories and recreate this on your own clusters",
    "start": "1725120",
    "end": "1730720"
  },
  {
    "text": "thanks for watching thanks mile so these are just some links to some of",
    "start": "1730720",
    "end": "1737520"
  },
  {
    "text": "the things miles uh showed i did add one thing that he didn't cover this was based on a conversation i had a hallway",
    "start": "1737520",
    "end": "1745520"
  },
  {
    "text": "track here i wanted to practice my delivery of this so i showed it to somebody with a lot of",
    "start": "1745520",
    "end": "1750799"
  },
  {
    "text": "experience using uh gpu resources in an organization",
    "start": "1750799",
    "end": "1756080"
  },
  {
    "text": "and the person planted the seeds for this aha moment where he pointed out that the organization has a lot of users",
    "start": "1756080",
    "end": "1763279"
  },
  {
    "text": "running these jupiter notebooks you know it's a it's sort of like a ui for data scientists",
    "start": "1763279",
    "end": "1769120"
  },
  {
    "text": "and one way to do that is to install this provision individual users with gpus in",
    "start": "1769120",
    "end": "1776080"
  },
  {
    "text": "high-end laptops but it the jupiter notebook just is a web application so",
    "start": "1776080",
    "end": "1781200"
  },
  {
    "text": "you could centrally deploy that in a server farm and there happens to be a",
    "start": "1781200",
    "end": "1788320"
  },
  {
    "text": "the jupiter project itself has put up a helm chart for running this at scale in",
    "start": "1788320",
    "end": "1793360"
  },
  {
    "text": "kubernetes you know provision a thousand jupiter notebook users and i kind of",
    "start": "1793360",
    "end": "1798640"
  },
  {
    "text": "think that using this as a back end of that because the scenario with that jupiter notebook",
    "start": "1798640",
    "end": "1805200"
  },
  {
    "text": "is people are typing things in almost like a spreadsheet with a lot of dead time you know it's think time for the",
    "start": "1805200",
    "end": "1812159"
  },
  {
    "text": "user until they kick off whatever the operation is it's not going to use a gpu",
    "start": "1812159",
    "end": "1818159"
  },
  {
    "text": "this technique of pooling it might be saving money energy",
    "start": "1818159",
    "end": "1823440"
  },
  {
    "text": "uh maintenance overhead so uh if you like this presentation um this",
    "start": "1823440",
    "end": "1831840"
  },
  {
    "text": "was sponsored by the kubernetes vmware user group we have meetings",
    "start": "1831840",
    "end": "1838559"
  },
  {
    "text": "i'll tell you in the next slide how to join it it's me miles we have a couple user",
    "start": "1838559",
    "end": "1844960"
  },
  {
    "text": "sponsors one of them joe cersei of t-mobile i don't see him in the room but he's",
    "start": "1844960",
    "end": "1850960"
  },
  {
    "text": "walking around uh the conference oh there he is right there but um you know we we have these monthly",
    "start": "1850960",
    "end": "1858159"
  },
  {
    "text": "meetings sometimes there's a speaker giving talks on a topic i don't know like this other times we just declare",
    "start": "1858159",
    "end": "1865519"
  },
  {
    "text": "them bring us your problems or presentations on best practices once again these apply",
    "start": "1865519",
    "end": "1871679"
  },
  {
    "text": "to using any form of kubernetes on top of the vmware infrastructure so picture",
    "start": "1871679",
    "end": "1878080"
  },
  {
    "text": "the base vsphere as the equivalent of a public cloud platform you can bring any kubernetes distro you like there this",
    "start": "1878080",
    "end": "1884960"
  },
  {
    "text": "isn't this isn't uh vmware product centric this is whatever kind of",
    "start": "1884960",
    "end": "1890000"
  },
  {
    "text": "kubernetes you got if you're running it on vsphere on-prem this group should be of interest to you",
    "start": "1890000",
    "end": "1897360"
  },
  {
    "text": "so that link is how you join the group it's a mailing list and the first step we actually gate",
    "start": "1897360",
    "end": "1904320"
  },
  {
    "text": "access to the documents and things by membership in that group",
    "start": "1904320",
    "end": "1909760"
  },
  {
    "text": "if you we have a slack channel listed at the bottom so you can ask",
    "start": "1909760",
    "end": "1916480"
  },
  {
    "text": "questions there and join the zoom meetings if you like",
    "start": "1916480",
    "end": "1921519"
  },
  {
    "text": "those zoom meetings are recorded and there's a they're in a playlist under the",
    "start": "1921519",
    "end": "1927760"
  },
  {
    "text": "kubernetes user channel um if you want to get a hold of the",
    "start": "1927760",
    "end": "1932960"
  },
  {
    "text": "speakers these are our github and twitter handles but to be honest with you the",
    "start": "1932960",
    "end": "1938240"
  },
  {
    "text": "uh it's probably easier to get a hold of us in that slack channel and",
    "start": "1938240",
    "end": "1944240"
  },
  {
    "text": "i think we only have a couple minutes for q a and miles is not physically here",
    "start": "1944240",
    "end": "1950159"
  },
  {
    "text": "but he was going to be monitoring in that kubernetes slack channel which is our normal one so even if you have",
    "start": "1950159",
    "end": "1957440"
  },
  {
    "text": "questions after this is over later today next week go ahead and ask them in that channel and then this is a link where",
    "start": "1957440",
    "end": "1964640"
  },
  {
    "text": "you can download this deck a lot of the things that i showed up here on the screen are hot links behind them so you",
    "start": "1964640",
    "end": "1972000"
  },
  {
    "text": "should be able to conveniently go to these resources that we just talked about and with that said maybe we have a",
    "start": "1972000",
    "end": "1979039"
  },
  {
    "text": "minute for a question or two if we run out i'll hang around in the hallway yes",
    "start": "1979039",
    "end": "1984159"
  },
  {
    "text": "maybe you wait till you get the mic",
    "start": "1984159",
    "end": "1989080"
  },
  {
    "text": "right over there",
    "start": "1990240",
    "end": "1992880"
  },
  {
    "text": "so what are the performance implications of sending those you know requests to the gpu over the network",
    "start": "1998080",
    "end": "2004240"
  },
  {
    "text": "versus using the system bus well you want to have fast network so i would not recommend this if you have less than 10",
    "start": "2004240",
    "end": "2010799"
  },
  {
    "text": "gig e and probably 40 is better 100 100 gig ethernet even better",
    "start": "2010799",
    "end": "2017120"
  },
  {
    "text": "and it depends what's going on is really you're feeding a pipeline that goes to the gpu and usually what comes back is",
    "start": "2017120",
    "end": "2024480"
  },
  {
    "text": "relatively small compared to what has to go over there and it's a scenario where there's a",
    "start": "2024480",
    "end": "2029840"
  },
  {
    "text": "little latency on getting that pipeline filled but if you've got jobs that run",
    "start": "2029840",
    "end": "2034960"
  },
  {
    "text": "you know this this probably wouldn't uh pay off for jobs that run",
    "start": "2034960",
    "end": "2040640"
  },
  {
    "text": "100 milliseconds or something but it's my understanding that many of these",
    "start": "2040640",
    "end": "2045760"
  },
  {
    "text": "applications some of them might even run for a day or more even with the gpu many of them run for minutes and hours",
    "start": "2045760",
    "end": "2052560"
  },
  {
    "text": "and that initial pipeline effort is just kind of a one-time",
    "start": "2052560",
    "end": "2059358"
  },
  {
    "text": "thing it is algo it is dependent on your algorithm and the ratio of kind of gpu think time to",
    "start": "2059359",
    "end": "2067520"
  },
  {
    "text": "actual data that has to be loaded into that fast access frame buffer over on",
    "start": "2067520",
    "end": "2073118"
  },
  {
    "text": "the gpu so your mileage would would vary but uh",
    "start": "2073119",
    "end": "2079358"
  },
  {
    "text": "it works for very many popular jobs and it's my understanding that when you get",
    "start": "2079359",
    "end": "2085919"
  },
  {
    "text": "gpu service and some of the public cloud providers that you might have a similar thing going on too depending on how they",
    "start": "2085919",
    "end": "2092398"
  },
  {
    "text": "chose to implement it under the covers",
    "start": "2092399",
    "end": "2096078"
  },
  {
    "text": "thank you for the presentation i had a question regarding the functionality that might not be available things like",
    "start": "2101359",
    "end": "2107599"
  },
  {
    "text": "gpu profiling like cuda profiling having a tensorboard for your tensorflow",
    "start": "2107599",
    "end": "2113200"
  },
  {
    "text": "training is there any limitation when you're doing this kind of thing of sharing gpus um",
    "start": "2113200",
    "end": "2118960"
  },
  {
    "text": "i'm afraid i don't want to tell you more than i know and and i don't know that but miles might",
    "start": "2118960",
    "end": "2124320"
  },
  {
    "text": "uh so uh come back on the slack channel and we'll we'll answer that all right thank you",
    "start": "2124320",
    "end": "2132960"
  },
  {
    "text": "we've got one over here",
    "start": "2133359",
    "end": "2137160"
  },
  {
    "text": "i think this will be the last question by the way because we're going over are you using pre-trained models or are you",
    "start": "2147440",
    "end": "2153280"
  },
  {
    "text": "training on a data set uh to do inferencing oh this is up to you it's",
    "start": "2153280",
    "end": "2158640"
  },
  {
    "text": "like you just write it as if you were running your machine learning deep learning app",
    "start": "2158640",
    "end": "2165119"
  },
  {
    "text": "inside docker or inside a kubernetes pod and",
    "start": "2165119",
    "end": "2170240"
  },
  {
    "text": "this is just taking on virtualizing a cuda interface out of",
    "start": "2170240",
    "end": "2176000"
  },
  {
    "text": "out of physical gpus that aren't necessarily on your uh worker node so",
    "start": "2176000",
    "end": "2182560"
  },
  {
    "text": "in terms of what goes on in the stages if you're using it for deep learning",
    "start": "2182560",
    "end": "2188800"
  },
  {
    "text": "this doesn't get there that that would have to be in the code that you'd implement in the kubernetes pod",
    "start": "2188800",
    "end": "2194960"
  },
  {
    "text": "yeah can you also use other like dl frameworks like pandas uh i think anything that um",
    "start": "2194960",
    "end": "2203359"
  },
  {
    "text": "can be accelerated with a cuda interface will work so we are ver we are not virtualizing the",
    "start": "2203359",
    "end": "2211040"
  },
  {
    "text": "physical hardware here we're virtualizing the cuda interface so anything that works uh and can take",
    "start": "2211040",
    "end": "2217599"
  },
  {
    "text": "advantage of cuda will work with this technology are there any",
    "start": "2217599",
    "end": "2223200"
  },
  {
    "text": "performance uh like trade-offs with that well kind of i think it was like the",
    "start": "2223200",
    "end": "2228800"
  },
  {
    "text": "question brought up earlier that there is a potential cost with uh sending",
    "start": "2228800",
    "end": "2235920"
  },
  {
    "text": "these data flows over a network versus having a physical card",
    "start": "2235920",
    "end": "2241680"
  },
  {
    "text": "you know literally mounted in a pci express box on the very box you're on you know there isn't this isn't quite up",
    "start": "2241680",
    "end": "2248480"
  },
  {
    "text": "to the same bandwidth and there is going to be latency associated with the network how",
    "start": "2248480",
    "end": "2254880"
  },
  {
    "text": "big a deal that makes is kind of application dependent so",
    "start": "2254880",
    "end": "2260160"
  },
  {
    "text": "longer running jobs i think that might not show up so much and jobs that don't need",
    "start": "2260160",
    "end": "2266079"
  },
  {
    "text": "you know the less data that has to flow in there to fill the frame buffers the better off this is going to be compared",
    "start": "2266079",
    "end": "2272880"
  },
  {
    "text": "to using a physical gpu and you there's not one answer for every application",
    "start": "2272880",
    "end": "2279760"
  },
  {
    "text": "okay and does it support batch processing yeah it should uh",
    "start": "2279760",
    "end": "2285599"
  },
  {
    "text": "i can't say i personally tried it but i think that's orthogonal to this as well",
    "start": "2285599",
    "end": "2290960"
  },
  {
    "text": "thank you okay so i'll get you in the hallway i see we have one more question but i think we're",
    "start": "2290960",
    "end": "2297040"
  },
  {
    "text": "over time so catch me in the hallway but thanks for attending uh like i say if if you have",
    "start": "2297040",
    "end": "2303839"
  },
  {
    "text": "more and you can't meet me in the hallway just reach out join the group",
    "start": "2303839",
    "end": "2308960"
  },
  {
    "text": "join our monthly meetings but thanks",
    "start": "2308960",
    "end": "2313400"
  }
]