[
  {
    "text": "I'm Kenneth Owens formerly of Google currently at Breck's this is Janet Koh",
    "start": "30",
    "end": "5150"
  },
  {
    "text": "it's still at Google we're two of the co-chairs of sig apps",
    "start": "5150",
    "end": "10980"
  },
  {
    "text": "and co-chairs are really just in SIG's kind of an administrative burden like we organize meetings basically in that",
    "start": "10980",
    "end": "16590"
  },
  {
    "text": "function we're also two of the tech leads and they think we're the set of people that are both chairs and tech leads so that",
    "start": "16590",
    "end": "23939"
  },
  {
    "text": "means we're code owners for the workloads api's there's another one right there he's from Red Hat you can go at him",
    "start": "23939",
    "end": "30240"
  },
  {
    "text": "later so for the say gasps deep dive we'd usually do a couple of things one we try",
    "start": "30240",
    "end": "36540"
  },
  {
    "text": "to make it interactive and we want to hear about your opinions and what the stakes should be doing - we try to teach",
    "start": "36540",
    "end": "41700"
  },
  {
    "text": "so we're gonna start with going over some of the code that we actually maintain in trade and some of the code",
    "start": "41700",
    "end": "48690"
  },
  {
    "text": "that we maintain out of tree inside of siga apps basically to give an overview of how to write controllers then at the",
    "start": "48690",
    "end": "54930"
  },
  {
    "text": "end of it we're going to point to the work that needs to be done in the community and ask you to contribute and",
    "start": "54930",
    "end": "60059"
  },
  {
    "text": "and talk about that a lot try to get Europeans on how we prioritize it as well if there are any questions as we go",
    "start": "60059",
    "end": "66750"
  },
  {
    "text": "please feel free to be interactive this is our agenda so I'll start with on",
    "start": "66750",
    "end": "74100"
  },
  {
    "text": "walking through the controller what globe controller internals so we take",
    "start": "74100",
    "end": "79619"
  },
  {
    "text": "three of the workload controllers to just give you do a code walkthrough to",
    "start": "79619",
    "end": "86340"
  },
  {
    "text": "help new contributors or existing contributors to understand the controller code structure so I'll start",
    "start": "86340",
    "end": "94829"
  },
  {
    "text": "with on the basic reconciliation loop of a controller so for a given controller",
    "start": "94829",
    "end": "102570"
  },
  {
    "text": "in kubernetes we usually have the desired state and then the current state and based on that",
    "start": "102570",
    "end": "110850"
  },
  {
    "text": "we make some changes and we keep doing that in the inner loop and this change",
    "start": "110850",
    "end": "118649"
  },
  {
    "text": "is level triggered which means that we're not looking at the diff between",
    "start": "118649",
    "end": "124369"
  },
  {
    "text": "the div in the state we're just looking at the current state and what's the",
    "start": "124369",
    "end": "130020"
  },
  {
    "text": "expected state in the resource pack so let's take on",
    "start": "130020",
    "end": "138060"
  },
  {
    "text": "deployment controller as an example so to give you a quick recap on what",
    "start": "138060",
    "end": "145950"
  },
  {
    "text": "deployment does is that it creates replica sets and replicas that then",
    "start": "145950",
    "end": "151959"
  },
  {
    "text": "create pods and deployment supports on a feature called rolling update so you can",
    "start": "151959",
    "end": "160200"
  },
  {
    "text": "rolling update your containers by on changing this back of a deployment and",
    "start": "160200",
    "end": "167739"
  },
  {
    "text": "then what it does internally is that it will scale up and scale down replica sets and let's take a look at the source",
    "start": "167739",
    "end": "176560"
  },
  {
    "text": "code of that okay that's probably too",
    "start": "176560",
    "end": "185800"
  },
  {
    "text": "small good enough okay so let's start with",
    "start": "185800",
    "end": "196280"
  },
  {
    "text": "this piece so all the controllers are",
    "start": "196280",
    "end": "201769"
  },
  {
    "text": "just part of the kubernetes controller manager we ship it as a binary and",
    "start": "201769",
    "end": "208180"
  },
  {
    "text": "deployment controller is one of them and the other controller code lives in the",
    "start": "208180",
    "end": "214609"
  },
  {
    "text": "package in the kubernetes kubernetes repository you can find it under package",
    "start": "214609",
    "end": "220400"
  },
  {
    "text": "slash controller and under there there will one of it is deployment and you can",
    "start": "220400",
    "end": "226519"
  },
  {
    "text": "see other controllers here so if you go inside the deployment controller code",
    "start": "226519",
    "end": "233030"
  },
  {
    "text": "you can see the new deployment controller that's the start of this whole controller code and let's talk",
    "start": "233030",
    "end": "243049"
  },
  {
    "text": "about a bunch of things in this controller first let's talk about",
    "start": "243049",
    "end": "251500"
  },
  {
    "text": "something called shared informer so you can see that we passed some some",
    "start": "251500",
    "end": "259539"
  },
  {
    "text": "parameters into this deployment controller and it's it's using a",
    "start": "259539",
    "end": "265669"
  },
  {
    "text": "framework in kubernetes that's called share informer so what it does is that",
    "start": "265669",
    "end": "271270"
  },
  {
    "text": "the controllers can use that to receive notifications of the changes to the",
    "start": "271270",
    "end": "279289"
  },
  {
    "text": "resource for example if I have the deployment informer I can get the events",
    "start": "279289",
    "end": "286130"
  },
  {
    "text": "the notification of the creation the update or a deletion of the deployment",
    "start": "286130",
    "end": "292550"
  },
  {
    "text": "and from there I can access a shared",
    "start": "292550",
    "end": "298639"
  },
  {
    "text": "cache so this is not required for all controllers it's just a optimization",
    "start": "298639",
    "end": "305509"
  },
  {
    "text": "because without this then you'll need to keep making requests to API server and",
    "start": "305509",
    "end": "313030"
  },
  {
    "text": "it doesn't scale as you can imagine so instead of that we have this informer",
    "start": "313030",
    "end": "318710"
  },
  {
    "text": "framework that allows you to I received notification on updates and",
    "start": "318710",
    "end": "324960"
  },
  {
    "text": "you can an update the cache and then you only read our list from the cache in the",
    "start": "324960",
    "end": "332729"
  },
  {
    "text": "controller and it's called shared Informer because this cache can be shared among different controllers and",
    "start": "332729",
    "end": "341900"
  },
  {
    "text": "then one of the cavea is that when you",
    "start": "341900",
    "end": "347759"
  },
  {
    "text": "write code in controller you should never ever mutate objects in the cache",
    "start": "347759",
    "end": "355009"
  },
  {
    "text": "because when you do that it will affect other controllers that's also reading",
    "start": "355009",
    "end": "361169"
  },
  {
    "text": "from this cache so when you get resource from the cache if you need to make some",
    "start": "361169",
    "end": "367650"
  },
  {
    "text": "changes to it you just do a deep coffee a deep copy and then when you do the",
    "start": "367650",
    "end": "374310"
  },
  {
    "text": "copy and don't forget to not do shallow",
    "start": "374310",
    "end": "379560"
  },
  {
    "text": "copy because there's resource like annotations which is a map if you do",
    "start": "379560",
    "end": "384960"
  },
  {
    "text": "shallow copy you can still mess up with others code and then and then if you",
    "start": "384960",
    "end": "393810"
  },
  {
    "text": "scroll down here so you can see that I have the D Informer is the deployment",
    "start": "393810",
    "end": "400050"
  },
  {
    "text": "Informer and then replica set Informer and the pot Informer but just means that",
    "start": "400050",
    "end": "405840"
  },
  {
    "text": "the deployment controller cares well cares about this resources and then",
    "start": "405840",
    "end": "412400"
  },
  {
    "text": "underneath here there's something called sink handler it's just the the real business logic in",
    "start": "412400",
    "end": "421229"
  },
  {
    "text": "this deployment controller so after a receive some events from all those",
    "start": "421229",
    "end": "429630"
  },
  {
    "text": "resources that it watches it will try to sync the deployment and the sink is just",
    "start": "429630",
    "end": "437570"
  },
  {
    "text": "the loop that I have mentioned here ok",
    "start": "437570",
    "end": "444270"
  },
  {
    "text": "and and then we have something called work you so you can see here there's a",
    "start": "444270",
    "end": "453360"
  },
  {
    "text": "function called in queue at the point and and there's in queue so what does",
    "start": "453360",
    "end": "459539"
  },
  {
    "text": "the in queue mean it means that we are storing the deployment resource in a",
    "start": "459539",
    "end": "466590"
  },
  {
    "text": "queue we call it work you it's another framework in kubernetes so",
    "start": "466590",
    "end": "471900"
  },
  {
    "text": "the benefit of using that is that you can queue all the changes that you need",
    "start": "471900",
    "end": "477360"
  },
  {
    "text": "to make who to you you can include the changes to a resource and then you can",
    "start": "477360",
    "end": "483270"
  },
  {
    "text": "work on that that later in the reconcile in the in this case it's the same",
    "start": "483270",
    "end": "488849"
  },
  {
    "text": "deployment so you can have multiple workers that takes task from takes",
    "start": "488849",
    "end": "495960"
  },
  {
    "text": "resource from the work you and then you can reconcile it by running sync",
    "start": "495960",
    "end": "501629"
  },
  {
    "text": "deployment later and this work you will make sure that no two deployments will",
    "start": "501629",
    "end": "508139"
  },
  {
    "text": "be operated by the same worker at a time and then so in here you can see that we",
    "start": "508139",
    "end": "521610"
  },
  {
    "text": "run this controller it will run start some workers and you can specify how",
    "start": "521610",
    "end": "528750"
  },
  {
    "text": "many number of workers you want for this controller and then you can see it's",
    "start": "528750",
    "end": "535350"
  },
  {
    "text": "running this thing called worker and then if you scroll down there's a",
    "start": "535350",
    "end": "545940"
  },
  {
    "text": "function called synced appointment",
    "start": "545940",
    "end": "550699"
  },
  {
    "text": "that's the sync handler thing that I just mentioned ok so now we have the",
    "start": "552769",
    "end": "560790"
  },
  {
    "text": "same deployment that has all the real business logic of this controller and",
    "start": "560790",
    "end": "567949"
  },
  {
    "text": "you can see that we first got a thing called key the key is just a index in",
    "start": "567949",
    "end": "577649"
  },
  {
    "text": "the work queue for you to fight this resource and the controller and the",
    "start": "577649",
    "end": "583350"
  },
  {
    "text": "deployment controller just used that namespace and then as key to fight this deployment this works",
    "start": "583350",
    "end": "589460"
  },
  {
    "text": "because we only store deployment in this work you we don't store other kinds of",
    "start": "589460",
    "end": "594890"
  },
  {
    "text": "resource so if you have multiple kinds of resource then you'll need a different",
    "start": "594890",
    "end": "600730"
  },
  {
    "text": "different function to define here for this work you and then and we can scroll",
    "start": "600730",
    "end": "612620"
  },
  {
    "text": "down a little bit you can see that I'm getting and this",
    "start": "612620",
    "end": "617960"
  },
  {
    "text": "I'm finding this given this key I'm finding this deployment from the key I'm",
    "start": "617960",
    "end": "625640"
  },
  {
    "text": "splitting the namespace and name from the key and then get the deployment and",
    "start": "625640",
    "end": "631810"
  },
  {
    "text": "notice that this is listing from the cache so we're not there right it directly talking to API server and then",
    "start": "631810",
    "end": "639980"
  },
  {
    "text": "we got the deployment from the cache and we don't want to mutate it by accident",
    "start": "639980",
    "end": "646970"
  },
  {
    "text": "so we call this deep copy here and then",
    "start": "646970",
    "end": "652000"
  },
  {
    "text": "let's see there's anything interesting so here is the deployment deletion",
    "start": "652000",
    "end": "661730"
  },
  {
    "text": "timestamp is not nil that means that this department will be deleted so we don't need to reconcile it anymore we",
    "start": "661730",
    "end": "669410"
  },
  {
    "text": "don't need to like create or scale up scale down replica shots we just need to",
    "start": "669410",
    "end": "675080"
  },
  {
    "text": "think the status status of it so we can just return sync status only and then",
    "start": "675080",
    "end": "681620"
  },
  {
    "text": "and then scroll scroll down there's a function called check if it's paused so",
    "start": "681620",
    "end": "689420"
  },
  {
    "text": "in deployment controller when you pause a deployment and you can stop the",
    "start": "689420",
    "end": "697430"
  },
  {
    "text": "controller from doing the rollout but then when you do when you pause the",
    "start": "697430",
    "end": "704750"
  },
  {
    "text": "deployment we still allow you to scale up or scale down the deployment but in",
    "start": "704750",
    "end": "711290"
  },
  {
    "text": "order to do it safely the scale up or down will be proportionally for example",
    "start": "711290",
    "end": "718970"
  },
  {
    "text": "if you have and two replicas of the old pots and",
    "start": "718970",
    "end": "725029"
  },
  {
    "text": "then three replicas of a new pot then if you want to scale it up by two times",
    "start": "725029",
    "end": "732020"
  },
  {
    "text": "then you need to scale them proportional so that's safer because you don't want",
    "start": "732020",
    "end": "737210"
  },
  {
    "text": "to suddenly roll out the new replicas too quickly and then if it's on if it's",
    "start": "737210",
    "end": "747560"
  },
  {
    "text": "rolled back so we we have a feature called raw back in deployment but it's",
    "start": "747560",
    "end": "753860"
  },
  {
    "text": "now deprecated we still keep it here for backward compatibility and then finally",
    "start": "753860",
    "end": "760960"
  },
  {
    "text": "here if it's a scaling event is it even if on the deployment is not paused we",
    "start": "760960",
    "end": "768770"
  },
  {
    "text": "run the same function to scale where proportionally and the last piece is",
    "start": "768770",
    "end": "774920"
  },
  {
    "text": "that finally we can do the deployment rollout so deployment supports two",
    "start": "774920",
    "end": "782690"
  },
  {
    "text": "different types of rollout strategy the first one is the recreate that means",
    "start": "782690",
    "end": "789500"
  },
  {
    "text": "that I will delete all your old parts completely and after all those old has",
    "start": "789500",
    "end": "796220"
  },
  {
    "text": "are deleted you can create new ones and then for rolling update is just rolling",
    "start": "796220",
    "end": "802540"
  },
  {
    "text": "and scaling up and down old ones and new ones so that's it and because there's a",
    "start": "802540",
    "end": "811790"
  },
  {
    "text": "lot of details here but after this walkthrough it should be pretty easy for",
    "start": "811790",
    "end": "816830"
  },
  {
    "text": "you to dive deeper into deployment and then let's go back to here the next",
    "start": "816830",
    "end": "826339"
  },
  {
    "text": "controller that I want to talk about is replica set because it's related to",
    "start": "826339",
    "end": "831350"
  },
  {
    "text": "deployment and what replica set does is that it just creates on a number of",
    "start": "831350",
    "end": "838220"
  },
  {
    "text": "identical pods from a part template and then the source code lives here it's",
    "start": "838220",
    "end": "845959"
  },
  {
    "text": "under your package control area roughly at and then and interesting so the I can show you",
    "start": "845959",
    "end": "855030"
  },
  {
    "text": "like the basic of this replica set",
    "start": "855030",
    "end": "860580"
  },
  {
    "text": "controller but you can see that a lot of code is very similar for example it also",
    "start": "860580",
    "end": "866790"
  },
  {
    "text": "has the informer framework and it also",
    "start": "866790",
    "end": "873270"
  },
  {
    "text": "has the sync Handler and it's listing from the cache and one thing that's",
    "start": "873270",
    "end": "881190"
  },
  {
    "text": "particularly interesting in a replica set controller is that it has something",
    "start": "881190",
    "end": "887520"
  },
  {
    "text": "called expectations and if I scroll down to the sink let's find a sink handler",
    "start": "887520",
    "end": "899540"
  },
  {
    "text": "thing replica set okay so if you go down",
    "start": "899540",
    "end": "908760"
  },
  {
    "text": "to the business logic of replica set you can find that it's using something",
    "start": "908760",
    "end": "917370"
  },
  {
    "text": "called expectations and I find it oh",
    "start": "917370",
    "end": "925220"
  },
  {
    "text": "yeah you can see that's doing something like deleting expectations creating",
    "start": "925220",
    "end": "931260"
  },
  {
    "text": "expectations so expectation is is a pair",
    "start": "931260",
    "end": "937740"
  },
  {
    "text": "of atomic counters for you to track on the the creation and deletion of",
    "start": "937740",
    "end": "944820"
  },
  {
    "text": "resource and the in this case and replica set controller is using this to",
    "start": "944820",
    "end": "950580"
  },
  {
    "text": "track the creation and deletion of parts and you know you can also find it in",
    "start": "950580",
    "end": "958890"
  },
  {
    "text": "other controllers like jobs and daemon set and why do we need this why don't we",
    "start": "958890",
    "end": "964710"
  },
  {
    "text": "need it and deployment controller for example this is because that if you",
    "start": "964710",
    "end": "970380"
  },
  {
    "text": "still remember we are reading from cache and the cache could be outdated when you",
    "start": "970380",
    "end": "977430"
  },
  {
    "text": "read it so imagine that when the replica set controller creates it for example if",
    "start": "977430",
    "end": "984510"
  },
  {
    "text": "told to create three parts and then in the next sink loop it's looking at",
    "start": "984510",
    "end": "992540"
  },
  {
    "text": "number the number of current parts in the system and doesn't find those three",
    "start": "992540",
    "end": "998519"
  },
  {
    "text": "parts because the cash hasn't been updated so the replica set controller",
    "start": "998519",
    "end": "1004610"
  },
  {
    "text": "will then try to create three more parts so this problem will eventually be",
    "start": "1004610",
    "end": "1011779"
  },
  {
    "text": "reconciled but you don't want that to happen too often so to solve this",
    "start": "1011779",
    "end": "1018529"
  },
  {
    "text": "problem we have the expectations so with this expectations you can say in",
    "start": "1018529",
    "end": "1025910"
  },
  {
    "text": "this loop I expect for example three more parts and then the controller will",
    "start": "1025910",
    "end": "1031938"
  },
  {
    "text": "not be synced on until the expectations",
    "start": "1031939",
    "end": "1037938"
  },
  {
    "text": "are met so you can set a counter to creation equal to three and then when",
    "start": "1037939",
    "end": "1045350"
  },
  {
    "text": "the controller observes that the part is being created from the informer from the",
    "start": "1045350",
    "end": "1050809"
  },
  {
    "text": "event then every time it will check the expectations like I saw this expectation",
    "start": "1050809",
    "end": "1057890"
  },
  {
    "text": "I saw this creation and then after three creations and the expectations will be",
    "start": "1057890",
    "end": "1065690"
  },
  {
    "text": "satisfied then the sink sink and and the sink replica set can be executed so in",
    "start": "1065690",
    "end": "1079370"
  },
  {
    "text": "the rebel set controller there's also something called look filtered active",
    "start": "1079370",
    "end": "1084410"
  },
  {
    "text": "parts and we have this because we don't want for example if there's a node duck",
    "start": "1084410",
    "end": "1092960"
  },
  {
    "text": "unit and the no diet no pot can be scheduled then or maybe there is there's",
    "start": "1092960",
    "end": "1101150"
  },
  {
    "text": "a race condition then the cubelet may reject the part to be scheduled on that",
    "start": "1101150",
    "end": "1106760"
  },
  {
    "text": "node so the part will be failed but then if Rebecca said doesn't ignore that high",
    "start": "1106760",
    "end": "1113690"
  },
  {
    "text": "which is felled then it will have on last number of active pods forever so",
    "start": "1113690",
    "end": "1121559"
  },
  {
    "text": "to to compensate that problem we ignore",
    "start": "1121559",
    "end": "1126809"
  },
  {
    "text": "the inactive pods and finally the real",
    "start": "1126809",
    "end": "1132510"
  },
  {
    "text": "logic lives in the managed replica set manage replicas and in in this function",
    "start": "1132510",
    "end": "1141780"
  },
  {
    "text": "it just scale create and delete parts based on the expectations so for example",
    "start": "1141780",
    "end": "1150120"
  },
  {
    "text": "it calculates how many parts it needs to create then it will set that I expect",
    "start": "1150120",
    "end": "1156350"
  },
  {
    "text": "creations and then till it which replica set expect this creation and the number",
    "start": "1156350",
    "end": "1164940"
  },
  {
    "text": "of creation it expects so the next one",
    "start": "1164940",
    "end": "1170309"
  },
  {
    "text": "will be stable set let me hand it off to Ken to talk about it so the skeleton for",
    "start": "1170309",
    "end": "1178650"
  },
  {
    "text": "staple said like no we already saw setup code for replica set and deployment",
    "start": "1178650",
    "end": "1184100"
  },
  {
    "text": "it's basically the same as deployment in replica set you can think of staple set as the controller with increased",
    "start": "1187250",
    "end": "1194370"
  },
  {
    "text": "guarantees and lowered expectations as Janet was saying with replica said we're",
    "start": "1194370",
    "end": "1201450"
  },
  {
    "text": "trying to get a heuristic number of pods running but you can go past that scaling point right and you can be under at any",
    "start": "1201450",
    "end": "1207720"
  },
  {
    "text": "point I'm the guarantees of stateful set are we going to give you no more than this many pods running so if you have a",
    "start": "1207720",
    "end": "1213720"
  },
  {
    "text": "stateful set with four replicas we're never going to give you five right we're gonna give you four replicas or at least",
    "start": "1213720",
    "end": "1219659"
  },
  {
    "text": "attempt to with names that you can actually recognize and that you can find inside of dns and we'll do storage",
    "start": "1219659",
    "end": "1226230"
  },
  {
    "text": "provisioning for you in some ways it made the controller a lot simpler in some ways it got a little bit more",
    "start": "1226230",
    "end": "1231330"
  },
  {
    "text": "complex so that's just a skeleton code",
    "start": "1231330",
    "end": "1236549"
  },
  {
    "text": "the interesting pieces in staple set are actually in staple set control",
    "start": "1236549",
    "end": "1244309"
  },
  {
    "text": "yeah so at the top these things controller revisions",
    "start": "1247720",
    "end": "1254120"
  },
  {
    "text": "this was an interesting problem so replica set came first right this is",
    "start": "1254120",
    "end": "1259730"
  },
  {
    "text": "just a way to give you a fungible number of replicas of the same pod made pod useful what was deployment really about",
    "start": "1259730",
    "end": "1265880"
  },
  {
    "text": "deployment was about having low disruption updates for your replica sets",
    "start": "1265880",
    "end": "1271309"
  },
  {
    "text": "for your app right so inherently in deployment when I'm scaling this replica setup and scaling this replica set down",
    "start": "1271309",
    "end": "1278000"
  },
  {
    "text": "in order to do a rolling update if I need to do a rollback a low disruption rollback all I'm doing is scaling the",
    "start": "1278000",
    "end": "1284120"
  },
  {
    "text": "other direction so it became a problem of looking at the template looking at the template of a replica set and saying oh well these be",
    "start": "1284120",
    "end": "1291320"
  },
  {
    "text": "this replica set actually implements the Declaration of the current intent of the deployment so instead of doing a",
    "start": "1291320",
    "end": "1297950"
  },
  {
    "text": "rollover and creating a new replica set and just wiping everything which is usually what you don't want when you're",
    "start": "1297950",
    "end": "1304040"
  },
  {
    "text": "doing a rollback at two o'clock in the morning will actually say oh well this replica sets already halfway up and it",
    "start": "1304040",
    "end": "1309500"
  },
  {
    "text": "already matches the spec so we're gonna we're gonna scale this one up and then scale this failing one down now stateful",
    "start": "1309500",
    "end": "1317420"
  },
  {
    "text": "SEC doesn't have an intermediate representation like replica set it works directly on pods so we needed a way to",
    "start": "1317420",
    "end": "1323120"
  },
  {
    "text": "achieve the same thing hence controller revision now I think the demon set and staples that are still",
    "start": "1323120",
    "end": "1330920"
  },
  {
    "text": "the only actual like entry controllers that use it but if you are right in your own controller and you need a way to",
    "start": "1330920",
    "end": "1336559"
  },
  {
    "text": "solve that problem to store off the Deathlord of intent that you can detect later in order to do graceful roll backs you can use this object and its entry so",
    "start": "1336559",
    "end": "1344780"
  },
  {
    "text": "what we're doing at the top right here is we're actually gests if you go to",
    "start": "1344780",
    "end": "1351400"
  },
  {
    "text": "this all we're doing is we're pulling down the previous revisions of the",
    "start": "1351670",
    "end": "1357170"
  },
  {
    "text": "staple set that have been stored and then we have a comparison function okay",
    "start": "1357170",
    "end": "1363080"
  },
  {
    "text": "who tries to attacked detective quality between the existing revision and the",
    "start": "1363080",
    "end": "1368720"
  },
  {
    "text": "staple set spy template",
    "start": "1368720",
    "end": "1372130"
  },
  {
    "text": "so here we're just getting the current revision of the staple set the update version the staple set and the collision",
    "start": "1383340",
    "end": "1390600"
  },
  {
    "text": "count so the current revision basically is here's the controller revision that realizes the declaratory user at this",
    "start": "1390600",
    "end": "1396059"
  },
  {
    "text": "point in time the update revision is this is the what the users are requesting actually let me rephrase that",
    "start": "1396059",
    "end": "1402240"
  },
  {
    "text": "differently the update revision is saying here's what the user wants now the current revision is saying here's",
    "start": "1402240",
    "end": "1407669"
  },
  {
    "text": "what I'm currently trying to reconcile so the current revision is what the staple set controller thinks the previous declan intent of the user is",
    "start": "1407669",
    "end": "1414419"
  },
  {
    "text": "the update revision is the version that you've declared immediately",
    "start": "1414419",
    "end": "1420230"
  },
  {
    "text": "so drunk history truncate history is basically doing the same thing that say crime job controller does with its jobs",
    "start": "1427190",
    "end": "1436140"
  },
  {
    "text": "or what deployment does with replica sets we can't store an unbounded number",
    "start": "1436140",
    "end": "1441300"
  },
  {
    "text": "of controller revisions we can't store an unbounded number of replica sets we can't store them bounded number of jobs",
    "start": "1441300",
    "end": "1446820"
  },
  {
    "text": "though we used to in all cases um at any rate so what this is doing is it's just",
    "start": "1446820",
    "end": "1453060"
  },
  {
    "text": "going through the history of controller revisions in saint's okay well for what you've declared in this in the template",
    "start": "1453060",
    "end": "1458970"
  },
  {
    "text": "in the spec of the staple set we're going to bound the list of controller",
    "start": "1458970",
    "end": "1464280"
  },
  {
    "text": "revisions that you can possibly roll back to to that number and it just truncates them and deletes the ones that are no longer there so this is the most",
    "start": "1464280",
    "end": "1477660"
  },
  {
    "text": "interesting function so what word can",
    "start": "1477660",
    "end": "1482670"
  },
  {
    "text": "you guys hear me by the way is this lamb enough okay alright then the volume seems to go up down of the ante rate so",
    "start": "1482670",
    "end": "1491310"
  },
  {
    "text": "we update staple set what we're doing is we're gonna look at the current revision we're going to look at the update",
    "start": "1491310",
    "end": "1496350"
  },
  {
    "text": "revision we set the we set the",
    "start": "1496350",
    "end": "1502950"
  },
  {
    "text": "generation update the number of replicas and now we check the pods so this is",
    "start": "1502950",
    "end": "1511050"
  },
  {
    "text": "where things get hairy right so for staple set we have to actually sort the pods in the ordinal order right and",
    "start": "1511050",
    "end": "1517800"
  },
  {
    "text": "they're two different rollout strategies that we support so the first strategy which was the original one was ordered",
    "start": "1517800",
    "end": "1524280"
  },
  {
    "text": "ready ordered ready is what you would want to do like let's say you're running something like my sequel right and let's",
    "start": "1524280",
    "end": "1529980"
  },
  {
    "text": "use Postgres people that's that's popular these days everyone hates my sequel everyone loves Postgres so let's",
    "start": "1529980",
    "end": "1535110"
  },
  {
    "text": "say you're rolling out Postgres and you actually want to set up a replication topology if you were using staples set",
    "start": "1535110",
    "end": "1541680"
  },
  {
    "text": "to do this one way you can do it using a little bit of duct tape and spit you roll out pod 0 that'll be your master",
    "start": "1541680",
    "end": "1547980"
  },
  {
    "text": "when you roll out pod 1 you pass the ordinal into that pod you have a configuration script that says ok I'm",
    "start": "1547980",
    "end": "1552990"
  },
  {
    "text": "pod 1 I'll go look at pod 0 that's going to be my master by default and I'm gonna",
    "start": "1552990",
    "end": "1558480"
  },
  {
    "text": "put myself into the replic and chain there and then you set up the replication topology thatwe ordered",
    "start": "1558480",
    "end": "1564870"
  },
  {
    "text": "ready was meant to support that type of rollout it's still used in like Vitesse for instance which is based primarily on",
    "start": "1564870",
    "end": "1571679"
  },
  {
    "text": "MySQL they use staple sets and I'm pretty sure they still use ordered ready policy to roll up their replicas they do",
    "start": "1571679",
    "end": "1576900"
  },
  {
    "text": "shard it and replicated cockroach I believe still does the same thing with theirs as well as cockroach is like",
    "start": "1576900",
    "end": "1583280"
  },
  {
    "text": "Postgres the test kind of it actually does some cooler stuff with leveldb",
    "start": "1583280",
    "end": "1588570"
  },
  {
    "text": "under the hood but same thing for other things like zookeeper you don't need it",
    "start": "1588570",
    "end": "1593910"
  },
  {
    "text": "so for a zookeeper allow burst would be",
    "start": "1593910",
    "end": "1604040"
  },
  {
    "text": "true right so when allow burst is true all of that kind of goes out the window",
    "start": "1604040",
    "end": "1610309"
  },
  {
    "text": "you don't get ordered ready anymore instead of doing ordered ready it's just",
    "start": "1610309",
    "end": "1615330"
  },
  {
    "text": "going to go ahead and throw them all out there at once simultaneously zookeeper doesn't care like there's a well-known",
    "start": "1615330",
    "end": "1622230"
  },
  {
    "text": "theorem that basically you can't do leader election in an anonymous ring anyway so they all have to be configured",
    "start": "1622230",
    "end": "1627600"
  },
  {
    "text": "with an ID and they have to be configured with network coordinates of their peers right so you throw them all",
    "start": "1627600",
    "end": "1633030"
  },
  {
    "text": "out there and they will pretty much just figure it out I think the default kafka staple set from confluent actually still",
    "start": "1633030",
    "end": "1639750"
  },
  {
    "text": "does ordered ready I think they still do ordered ready for zookeeper but one of the things I've pushed on them and what",
    "start": "1639750",
    "end": "1644790"
  },
  {
    "text": "I do personally in production is turn it off and just do burstable because generally Kafka figures it out and",
    "start": "1644790",
    "end": "1649890"
  },
  {
    "text": "generally as you keeper figures it out the bigger problems with Kafka and staple sets are more along the lines of",
    "start": "1649890",
    "end": "1655350"
  },
  {
    "text": "when I elect my broker and then I do the other thing that staple set does which is roll and update I can have many many",
    "start": "1655350",
    "end": "1662220"
  },
  {
    "text": "many leader elections and switch that broker leader over again that's kind of one of the big problems with staple set",
    "start": "1662220",
    "end": "1669360"
  },
  {
    "text": "today we don't have a way of saying kill this specific pod we always say we go in",
    "start": "1669360",
    "end": "1675480"
  },
  {
    "text": "reverse order of the pods that we rolled out anything else interesting",
    "start": "1675480",
    "end": "1684320"
  },
  {
    "text": "ah so the is terminating function so the other thing about staple set is and this",
    "start": "1685470",
    "end": "1691260"
  },
  {
    "text": "this you can't turn off it but the other",
    "start": "1691260",
    "end": "1701250"
  },
  {
    "text": "thing about people say is that if you get a terminating pot in the middle of a",
    "start": "1701250",
    "end": "1706559"
  },
  {
    "text": "rollout so let's say i roll up i 0 i roll up hot one I roll up high - I'm not gonna move to pot 4 until a pod 3 is up",
    "start": "1706559",
    "end": "1713549"
  },
  {
    "text": "and ready right now burstable allows me to bypass that for the purposes of turning them up and",
    "start": "1713549",
    "end": "1718710"
  },
  {
    "text": "tearing them down but what I'm doing a roll in update I'm never gonna bypass that this is the thinking behind that is",
    "start": "1718710",
    "end": "1724830"
  },
  {
    "text": "like okay sure if you're gonna go and turn them all up and you're saying just",
    "start": "1724830",
    "end": "1730440"
  },
  {
    "text": "do it all at the same time we're gonna let you know that it makes sense updates",
    "start": "1730440",
    "end": "1737159"
  },
  {
    "text": "need to be more control and that's basically just old-school always when",
    "start": "1737159",
    "end": "1742470"
  },
  {
    "text": "you're doing a rollout do it gradually don't just go gung-ho you get a time",
    "start": "1742470",
    "end": "1758039"
  },
  {
    "text": "check",
    "start": "1758039",
    "end": "1760070"
  },
  {
    "text": "okay all right so we plan to talking about a whole bunch of other stuff but",
    "start": "1766140",
    "end": "1771150"
  },
  {
    "text": "this is the relevant stuff where staple set I mean basically it is most interesting in contrast to the play man",
    "start": "1771150",
    "end": "1776980"
  },
  {
    "text": "we knew we didn't have time to talk about a demon's set which is based on your selectors so there are a bunch of other things you want to talk about in",
    "start": "1776980",
    "end": "1782740"
  },
  {
    "text": "terms of contributions that we were looking at going forward in 2019 but I",
    "start": "1782740",
    "end": "1788380"
  },
  {
    "text": "mean you only have five minutes left so are there any I'd also like to give you guys times ask question what well I also",
    "start": "1788380",
    "end": "1794650"
  },
  {
    "text": "like to give you guys time to ask questions if you have any we were if we",
    "start": "1794650",
    "end": "1836110"
  },
  {
    "text": "had more time we'd actually plan to talk about the currency of the applications CID is gif or is it to be duped beta we",
    "start": "1836110",
    "end": "1842890"
  },
  {
    "text": "know it's beta but it's play early yeah it's still being contributed to we were going to use that as an example of how you can use coupie builder to build your",
    "start": "1842890",
    "end": "1849250"
  },
  {
    "text": "own controllers out of tree one of the takeaways like the two takeaways we want you to have heard here our controllers",
    "start": "1849250",
    "end": "1854440"
  },
  {
    "text": "here's how they work and here are the things you should think about if you're going to contribute to the injury controllers we also wanted to get su",
    "start": "1854440",
    "end": "1860230"
  },
  {
    "text": "here's how we do out of treatment rollers because the way this things are working right now we're more than",
    "start": "1860230",
    "end": "1866320"
  },
  {
    "text": "willing to take in Shepard contributions that live out of tree and the general direction from Sagarika texture is that",
    "start": "1866320",
    "end": "1871750"
  },
  {
    "text": "we'd like to see you they can really do everything out of tree if possible but we'd love to get deployment we'd love to get daemon set and those core",
    "start": "1871750",
    "end": "1878200"
  },
  {
    "text": "controllers and how they live is sub contributions that we just suck into the binary for a release that's the that's",
    "start": "1878200",
    "end": "1883720"
  },
  {
    "text": "the dream whether we get there or not is another story but for newer stuff we'd love to do it out of tree and like the",
    "start": "1883720",
    "end": "1890350"
  },
  {
    "text": "application CRD if you want to go look at the code provides one example that stay gaps are ones of how you can",
    "start": "1890350",
    "end": "1895630"
  },
  {
    "text": "so for deployment I rolled back definition it's not about the feature",
    "start": "1895630",
    "end": "1901390"
  },
  {
    "text": "it's not working it's mostly because the controller is doing something that's",
    "start": "1901390",
    "end": "1906530"
  },
  {
    "text": "sort of anti-pattern so how the rollback is done is that you set us and reveal a",
    "start": "1906530",
    "end": "1914480"
  },
  {
    "text": "row back to which revision in the controller's back I mean in the deployments back and then the controller",
    "start": "1914480",
    "end": "1921410"
  },
  {
    "text": "will then change the part template in this back to the older version so the",
    "start": "1921410",
    "end": "1928580"
  },
  {
    "text": "controller itself is mutating on the deployments back which is not what we",
    "start": "1928580",
    "end": "1935000"
  },
  {
    "text": "recommend people to do because the spec should be change by users so we're thinking how we can implement",
    "start": "1935000",
    "end": "1942350"
  },
  {
    "text": "this better so we deprecate this feature just for that reason and I want to say",
    "start": "1942350",
    "end": "1948530"
  },
  {
    "text": "something that I forgot to mention earlier about the expectation so on why",
    "start": "1948530",
    "end": "1954500"
  },
  {
    "text": "we have that in Rebecca say instead of having it but not having it and deployment is because when deployment",
    "start": "1954500",
    "end": "1961400"
  },
  {
    "text": "creates Rebecca sets and it creates Rebecca said with an M generating a hash",
    "start": "1961400",
    "end": "1969920"
  },
  {
    "text": "from the pot template so the name is always deterministic but when Rebecca",
    "start": "1969920",
    "end": "1976640"
  },
  {
    "text": "said generates part the department is just a random stream so that's why we",
    "start": "1976640",
    "end": "1982760"
  },
  {
    "text": "need the expectation",
    "start": "1982760",
    "end": "1987520"
  },
  {
    "text": "more than a year I think",
    "start": "2052520",
    "end": "2056270"
  },
  {
    "text": "[Music]",
    "start": "2070360",
    "end": "2073530"
  },
  {
    "text": "so we are still kind of trying to push for with API machinery we're a way to do this entry that",
    "start": "2103680",
    "end": "2111480"
  },
  {
    "text": "I think sorry",
    "start": "2166890",
    "end": "2173630"
  },
  {
    "text": "so I think there's some work and CLI for the porn you",
    "start": "2228759",
    "end": "2315880"
  },
  {
    "text": "[Music]",
    "start": "2332480",
    "end": "2335550"
  },
  {
    "text": "so I think there are some controllers that has resort policy set to always",
    "start": "2369150",
    "end": "2376270"
  },
  {
    "text": "because we expect those counselors to be used to run the the services that always",
    "start": "2376270",
    "end": "2385150"
  },
  {
    "text": "that should always run never finish so we have jobs that don't have restocked",
    "start": "2385150",
    "end": "2392260"
  },
  {
    "text": "policy as always yeah for jobs is different yeah to answer your question",
    "start": "2392260",
    "end": "2397990"
  },
  {
    "text": "there's there's this project within six scheduling called D scheduler and as one",
    "start": "2397990",
    "end": "2405760"
  },
  {
    "text": "of the the reasons for the scheduler is to be able to Wan rebalance your cluster",
    "start": "2405760",
    "end": "2412840"
  },
  {
    "text": "evenly if you're calling scaling up or down or I'm not sure if that's fixed but",
    "start": "2412840",
    "end": "2420580"
  },
  {
    "text": "there is a plan for having a fixed that if your pod is in a crash looking for",
    "start": "2420580",
    "end": "2426460"
  },
  {
    "text": "whatever reason on the particular node it will get affected and it will go",
    "start": "2426460",
    "end": "2431830"
  },
  {
    "text": "through again a a scheduling phase which might fix the problem currently if a pod",
    "start": "2431830",
    "end": "2439810"
  },
  {
    "text": "gets scheduled once it will never get scheduled again that's why the d",
    "start": "2439810",
    "end": "2445900"
  },
  {
    "text": "scheduler is only doing evict from a power vic from a node and go through a",
    "start": "2445900",
    "end": "2451380"
  },
  {
    "text": "regular scheduling once again and see if it works I can add to that concisely we",
    "start": "2451380",
    "end": "2458650"
  },
  {
    "text": "will probably never make the core workloads controllers that run services anything other than run always the the",
    "start": "2458650",
    "end": "2466270"
  },
  {
    "text": "run always is actually capturing the users declared up in ten and don't get me wrong we could be moved from this but",
    "start": "2466270",
    "end": "2472420"
  },
  {
    "text": "like you'd have to have a real clear story about what it means to have a service that runs to completion like",
    "start": "2472420",
    "end": "2479530"
  },
  {
    "text": "batch workloads run to completion stateless serving workloads or stateful storage servers don't run to completion",
    "start": "2479530",
    "end": "2485110"
  },
  {
    "text": "and to add on to more route to it he said like we there are still troubles in",
    "start": "2485110",
    "end": "2490720"
  },
  {
    "text": "terms of like mediating interaction between the scheduler and the controllers that's something that we",
    "start": "2490720",
    "end": "2496270"
  },
  {
    "text": "definitely want to focus on going forward it's not clear right now like so for instance what is it uh security",
    "start": "2496270",
    "end": "2503440"
  },
  {
    "text": "policies right so I can declare security policies inside of a replica set that will get scheduled onto a node that",
    "start": "2503440",
    "end": "2509470"
  },
  {
    "text": "don't support the security policy that I've configured right like that's a bad thing right now our answer is don't do",
    "start": "2509470",
    "end": "2517000"
  },
  {
    "text": "that we need to we need a better answer there right so figuring out how to deal with that is currently a topic that's in",
    "start": "2517000",
    "end": "2523510"
  },
  {
    "text": "flight so no we probably won't do anything but restart always but yes we are continuing to address and be",
    "start": "2523510",
    "end": "2530410"
  },
  {
    "text": "concerned with the general interaction between the components the core components of the cluster and trying to",
    "start": "2530410",
    "end": "2535750"
  },
  {
    "text": "improve that experience okay so now",
    "start": "2535750",
    "end": "2544119"
  },
  {
    "text": "Schilling for the cig we meet every Monday if you guys aren't aware every",
    "start": "2544119",
    "end": "2549190"
  },
  {
    "text": "every mother Monday now 9:00 a.m. Pt yeah we have a slack channel we have a",
    "start": "2549190",
    "end": "2556059"
  },
  {
    "text": "community page if there's any other questions you want to ask feel free to come to the meeting and ask them if",
    "start": "2556059",
    "end": "2561069"
  },
  {
    "text": "there's any contributions you want to make or get started on it's a great place to kind of come and check out if",
    "start": "2561069",
    "end": "2567660"
  },
  {
    "text": "ya just come and say hi if you like thank you thanks",
    "start": "2567660",
    "end": "2573250"
  },
  {
    "text": "[Applause]",
    "start": "2573250",
    "end": "2577760"
  }
]