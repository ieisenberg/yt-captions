[
  {
    "text": "hi everybody my name is Marius Gregorio I'm a senior dev manager at Nordstrom in",
    "start": "0",
    "end": "5940"
  },
  {
    "text": "the spaces of kubernetes see icd build tools and hello my name is emmanuel",
    "start": "5940",
    "end": "12389"
  },
  {
    "text": "gomes i'm a principal engineer at nordstrom and just want to say there's a ton of really great content going on so",
    "start": "12389",
    "end": "18900"
  },
  {
    "text": "it's exciting to see you all here thanks for coming and it's an honor to be here to present for you today yes and welcome",
    "start": "18900",
    "end": "24779"
  },
  {
    "text": "to 101 ways to crash your cluster so the two of us have been heavily involved",
    "start": "24779",
    "end": "30510"
  },
  {
    "text": "with the adoption of kubernetes at nordstrom all the way from incubation to",
    "start": "30510",
    "end": "37380"
  },
  {
    "text": "implementation and now at scaling and Nordstrom is a fashion specialty",
    "start": "37380",
    "end": "43829"
  },
  {
    "text": "retailer found in 1901 based over in Seattle Washington and we've got",
    "start": "43829",
    "end": "50690"
  },
  {
    "text": "hundreds of stores nationwide and recently opened some stores in Puerto Rico and up in Canada and we love",
    "start": "50690",
    "end": "59399"
  },
  {
    "text": "kubernetes we're serious about using kubernetes over at Nordstrom and so we're using it for a wide gamut of micro",
    "start": "59399",
    "end": "67049"
  },
  {
    "text": "services that are powering different parts of our business reviews gift cards purchase orders and so on and and even",
    "start": "67049",
    "end": "74250"
  },
  {
    "text": "parts of nordstrom.com that are a bit more recently updated and",
    "start": "74250",
    "end": "80990"
  },
  {
    "text": "and we also rely on kubernetes to run a bunch of the dev tools so things that",
    "start": "80990",
    "end": "89850"
  },
  {
    "text": "our engineers use oh hello I'm an auto somehow auto-advance I'm not sure how or",
    "start": "89850",
    "end": "97170"
  },
  {
    "text": "why this might be scary okay so anyway not only do we use the dev tools but",
    "start": "97170",
    "end": "104040"
  },
  {
    "text": "also the the enterprise logging and monitoring stack parts of that are",
    "start": "104040",
    "end": "111000"
  },
  {
    "text": "running on kubernetes so we have we use kubernetes for a lot but if you're",
    "start": "111000",
    "end": "116969"
  },
  {
    "text": "interested in stories with happy endings and you'd be better off going to some other talk don't get me wrong I mean",
    "start": "116969",
    "end": "124200"
  },
  {
    "text": "kubernetes is great but just go to any other talk you'll hear about how wonderful it is it is our solemn duty to",
    "start": "124200",
    "end": "131970"
  },
  {
    "text": "bring to light tales of mishap bad luck and calamity we'll be telling a",
    "start": "131970",
    "end": "137689"
  },
  {
    "text": "series of short stories beginning with unfortunate events down",
    "start": "137689",
    "end": "143299"
  },
  {
    "text": "at the node level and we'll work our way up to disastrous cluster wide incidents",
    "start": "143299",
    "end": "148519"
  },
  {
    "text": "and you know as we start off don't discount those node level events because",
    "start": "148519",
    "end": "153769"
  },
  {
    "text": "before you know it they can spread across your cluster and you'll have a major problem on your hand you know you",
    "start": "153769",
    "end": "160010"
  },
  {
    "text": "and the WoW okay power plant fail so you and your audience have no obligation to",
    "start": "160010",
    "end": "166459"
  },
  {
    "text": "remain honestly I advise you if you have a weak stomach to turn immediately and find something more pleasant instead so",
    "start": "166459",
    "end": "174500"
  },
  {
    "text": "our first tale begins with the way most most tales and issues tend to happen is",
    "start": "174500",
    "end": "181609"
  },
  {
    "text": "when you see a node go not ready and when when that happens the first thing that we do is is we just go cube CTL",
    "start": "181609",
    "end": "190250"
  },
  {
    "text": "described node to see what's going on in this case qiblas stopped posting status not uncommon at all and just so happens",
    "start": "190250",
    "end": "198379"
  },
  {
    "text": "that our end users also said hey our applications running on this node have stopped responding so we decided we",
    "start": "198379",
    "end": "204859"
  },
  {
    "text": "wanted to go take a look see what's going off that node login first hand and",
    "start": "204859",
    "end": "210109"
  },
  {
    "text": "so we go ssh into that and just sit around wait wait wait it looks like sshd is also unresponsive so meanwhile we go",
    "start": "210109",
    "end": "217099"
  },
  {
    "text": "digging through our logs and unfortunately our logs are scrolling by so fast and we're not really sure what we're looking for and eventually ssh",
    "start": "217099",
    "end": "226129"
  },
  {
    "text": "gets through and everything is nice and good and so the node seems to have",
    "start": "226129",
    "end": "231829"
  },
  {
    "text": "become happy again by the time we got there so unfortunately we couldn't see anything when we finally got onto the",
    "start": "231829",
    "end": "237620"
  },
  {
    "text": "node but then we looked into the past so we collect our metrics they go into Prometheus and and so what we found out",
    "start": "237620",
    "end": "245090"
  },
  {
    "text": "was memory utilization on this node reached up into the 90s and then right before we were able to log back in",
    "start": "245090",
    "end": "251120"
  },
  {
    "text": "there's this crash and a lot of memory got freed up and suddenly somehow that",
    "start": "251120",
    "end": "256370"
  },
  {
    "text": "got correlated so we knew that we were looking for we were looking for something that was related to memory",
    "start": "256370",
    "end": "263000"
  },
  {
    "text": "utilization or memory memory kills so now we were able to query our for something more appropriate and so we",
    "start": "263000",
    "end": "270380"
  },
  {
    "text": "found something like this in the kernel messages we found oom kills and we found page allocation stalls on that node and",
    "start": "270380",
    "end": "277360"
  },
  {
    "text": "so we wanted to really understand the mechanism through which this was going on because we didn't run out of memory",
    "start": "277360",
    "end": "283130"
  },
  {
    "text": "not entirely so we went to Google about page allocation stalls and back then",
    "start": "283130",
    "end": "290240"
  },
  {
    "text": "there was nothing if you actually do that now you'll find some other unfortunate kubernetes user who's running into the same problem but there",
    "start": "290240",
    "end": "297380"
  },
  {
    "text": "was not a lot of information so we cracked open the Linux source code search for that string and read I think",
    "start": "297380",
    "end": "303830"
  },
  {
    "text": "is like page a Lexi and so what that tells us is that when you go and",
    "start": "303830",
    "end": "309320"
  },
  {
    "text": "allocate a page of Rams kernel is going to go look for it it may need to perform memory compaction and if your memory",
    "start": "309320",
    "end": "316340"
  },
  {
    "text": "utilization is really high well then then then it could take a really long",
    "start": "316340",
    "end": "323419"
  },
  {
    "text": "time to get that memory meanwhile none of your applications are running because everything is is frozen now eventually",
    "start": "323419",
    "end": "331580"
  },
  {
    "text": "that's gonna timeout if it takes too long and that's when the own killer gets involved so not a pleasant experience",
    "start": "331580",
    "end": "338630"
  },
  {
    "text": "and it turns out the way this happened is while we were running Prometheus in our node Prometheus gobbles gobs of RAM",
    "start": "338630",
    "end": "344349"
  },
  {
    "text": "and then we had a ton of flink pods also on the same note that the debt that were",
    "start": "344349",
    "end": "350150"
  },
  {
    "text": "dormant and one of our users would initiate a job and all the pods are just wake up at the same time and they would",
    "start": "350150",
    "end": "358849"
  },
  {
    "text": "wake up at the same time and then bursts in terms of memory our memory utilization goes sky-high and then the",
    "start": "358849",
    "end": "365120"
  },
  {
    "text": "machine would freeze so that's that's what happened in the way we got to that information is just really following a",
    "start": "365120",
    "end": "371599"
  },
  {
    "text": "standard checklist of troubleshooting steps now this is really pared down I mean if no not ready this is in the",
    "start": "371599",
    "end": "378349"
  },
  {
    "text": "event of high memory utilization you know we look at the nodes we figure out what's going on cubelets stops posting",
    "start": "378349",
    "end": "385010"
  },
  {
    "text": "nodes status and then so then we look",
    "start": "385010",
    "end": "392539"
  },
  {
    "text": "for signs of high resource utilization and finally the own kills so what was the fix the fix was to set a qubit flags",
    "start": "392539",
    "end": "399979"
  },
  {
    "text": "correctly primarily eviction thresholds hard eviction actually so and there's",
    "start": "399979",
    "end": "407819"
  },
  {
    "text": "documentation that tells you all about this I suggest you read it very closely because it's easy to get wrong and so",
    "start": "407819",
    "end": "417080"
  },
  {
    "text": "what you want to do is you want the cubelets to enforce a memory limit global across all pods and once that",
    "start": "417080",
    "end": "423749"
  },
  {
    "text": "hits you want it to evict immediately and want to evict early because it takes time for the qiblah to respond to memory",
    "start": "423749",
    "end": "430830"
  },
  {
    "text": "pressure and so you don't want it's bumping up against the Linux um killer and then your node going unresponsive so",
    "start": "430830",
    "end": "438629"
  },
  {
    "text": "once you're doing that then I would also recommend you look into excuse me I'm gonna make a little edit or hit the",
    "start": "438629",
    "end": "447719"
  },
  {
    "text": "button here I think this is going to help alright okay",
    "start": "447719",
    "end": "458140"
  },
  {
    "text": "so you should probably also look at cube reserved and maybe even system reserved flags to make sure that your node agents",
    "start": "458140",
    "end": "465490"
  },
  {
    "text": "and your system demons all have enough resources to operate so that was that",
    "start": "465490",
    "end": "473080"
  },
  {
    "text": "was our kind of like our small story and was fortunate that doesn't spread to multiple nodes it was fairly contained",
    "start": "473080",
    "end": "479830"
  },
  {
    "text": "but you can imagine that somebody could write the page allocation stall or daemon set that's just ripped through",
    "start": "479830",
    "end": "486850"
  },
  {
    "text": "the cluster and cause havoc well we didn't need that for something",
    "start": "486850",
    "end": "492640"
  },
  {
    "text": "to rip through the cluster and cause havoc we ended up seeing this one day and there were more nodes down there",
    "start": "492640",
    "end": "499390"
  },
  {
    "text": "which actually were ready these are the not ready ones which is about half of it",
    "start": "499390",
    "end": "504520"
  },
  {
    "text": "and so you know first thing you want to do is panic and then the second thing you want to do is not panic so we have",
    "start": "504520",
    "end": "513729"
  },
  {
    "text": "to resort back to our to our run book and and you know simplified down we take a look what's a node up to a look at",
    "start": "513729",
    "end": "520450"
  },
  {
    "text": "stopped posting status yet again we want to look for signs of high resource utilization and in this case we found",
    "start": "520450",
    "end": "527770"
  },
  {
    "text": "that resource utilization was not consistent across the nodes that were affected and in fact a lot of the nodes",
    "start": "527770",
    "end": "534760"
  },
  {
    "text": "were not utilized at all so it couldn't have been that you know and so when you",
    "start": "534760",
    "end": "540580"
  },
  {
    "text": "see a lot of nodes just kind of pop out of existence all at the same time it makes you wonder maybe this is like a",
    "start": "540580",
    "end": "545920"
  },
  {
    "text": "networking problem cuz like nuts split from the IRC days and so we go to a",
    "start": "545920",
    "end": "551770"
  },
  {
    "text": "cloud provider happens to be AWS we take a look we say what's the networking status is it green yes it's green or",
    "start": "551770",
    "end": "558580"
  },
  {
    "text": "they claim it's green it's always a network right I always blamed on the",
    "start": "558580",
    "end": "564010"
  },
  {
    "text": "network so there's only one way to find out so we actually go try to log in machines that are currently posting not",
    "start": "564010",
    "end": "569710"
  },
  {
    "text": "ready no problem we're able to log in and then we say well let's try to hit the API server from these machines",
    "start": "569710",
    "end": "575350"
  },
  {
    "text": "because maybe that connection is somehow down and no problem we're able to reach the API server from these nodes so",
    "start": "575350",
    "end": "581850"
  },
  {
    "text": "what's going on doesn't look like a network ignition must be something else so we look through our cubelets logs we",
    "start": "581850",
    "end": "587110"
  },
  {
    "text": "look through our API server logs we just find a lot of a lot of information a lot of red",
    "start": "587110",
    "end": "595060"
  },
  {
    "text": "herrings absolutely nothing that we could identify as a cause for this and then eventually everything just kind of",
    "start": "595060",
    "end": "601240"
  },
  {
    "text": "cleared up and we can walk away we can pat ourselves on the back job well done",
    "start": "601240",
    "end": "607770"
  },
  {
    "text": "except for when it happened again at some point like months later on on a different cluster and then again and",
    "start": "608820",
    "end": "616150"
  },
  {
    "text": "again and this was a really hard one to track down but eventually we were we were tracking down the clues each time",
    "start": "616150",
    "end": "622870"
  },
  {
    "text": "this happened and we were eventually able to put something together and it looks something like this",
    "start": "622870",
    "end": "628020"
  },
  {
    "text": "usually 50% of the nodes go not ready all at the same time but sometimes 33%",
    "start": "628020",
    "end": "633730"
  },
  {
    "text": "or even 25% go not ready and then it every time this happens it's always all",
    "start": "633730",
    "end": "640360"
  },
  {
    "text": "the nodes kind of pop at the same time and they also happen to come back at the",
    "start": "640360",
    "end": "646150"
  },
  {
    "text": "same time and in fact every time they go not ready and then get ready again that",
    "start": "646150",
    "end": "652000"
  },
  {
    "text": "time window the time period where they are not ready always happens to be somehow just like almost exactly 15",
    "start": "652000",
    "end": "658570"
  },
  {
    "text": "minutes now it's starting to feel like we're dealing with a time out you know",
    "start": "658570",
    "end": "664870"
  },
  {
    "text": "maybe maybe it is Network related after all but maybe we looked at it the wrong way",
    "start": "664870",
    "end": "670230"
  },
  {
    "text": "so we scratch our headers trying to say well what did we not look at yet and it turns out well we didn't look at our",
    "start": "670230",
    "end": "675640"
  },
  {
    "text": "load balance or logs and so let's go to those ELB logs so a little bit of our",
    "start": "675640",
    "end": "680830"
  },
  {
    "text": "architecture we have high availability masters which means that we have multiple IP addresses for the API",
    "start": "680830",
    "end": "687940"
  },
  {
    "text": "servers and the cubelets needs to go talk to one of them and well we've got an ELB classic ELB in between and so we",
    "start": "687940",
    "end": "696280"
  },
  {
    "text": "look at the the EOB logs and they said oh we're gonna scale down one of the instances behind the scenes transparent",
    "start": "696280",
    "end": "704080"
  },
  {
    "text": "to you you don't know about this until you know about it and and that preceded the incident every",
    "start": "704080",
    "end": "711640"
  },
  {
    "text": "single time so what is this all about how is this happening well it turns out",
    "start": "711640",
    "end": "719170"
  },
  {
    "text": "that there was actually multiple issues over on get but this is the more general case",
    "start": "719170",
    "end": "725329"
  },
  {
    "text": "because it turns out that this issue is not an AWS specific problem the problem",
    "start": "725329",
    "end": "730610"
  },
  {
    "text": "was that the cubelet didn't have a timeout in the heartbeat over back to the API server and so the 15 minutes",
    "start": "730610",
    "end": "736850"
  },
  {
    "text": "without TCP retransmission the quick fix",
    "start": "736850",
    "end": "743660"
  },
  {
    "text": "for us was to use network load balancer the claim behind that is they can handle",
    "start": "743660",
    "end": "748670"
  },
  {
    "text": "connections that are open for months or years I'm not sure how they tested the open for years part maybe maybe a time",
    "start": "748670",
    "end": "759829"
  },
  {
    "text": "machine time machines solve all problems but but it was finally fixed in 1 7 8",
    "start": "759829",
    "end": "765820"
  },
  {
    "text": "and so if you're running a current version of kubernetes you probably don't",
    "start": "765820",
    "end": "771139"
  },
  {
    "text": "have to worry about this instance of this happening but you never know so I'm",
    "start": "771139",
    "end": "777170"
  },
  {
    "text": "actually gonna hand it over to Emanuele now who's gonna tell us a story about the day that our robots turned against",
    "start": "777170",
    "end": "783050"
  },
  {
    "text": "us so yeah so here we move from node level outages to well",
    "start": "783050",
    "end": "789589"
  },
  {
    "text": "multi node level outages to cluster components going haywire so as Marius mentioned we run on iOS as",
    "start": "789589",
    "end": "797839"
  },
  {
    "text": "a cloud provider and we have run the cluster autoscaler component as a way to",
    "start": "797839",
    "end": "804050"
  },
  {
    "text": "manage our spend and kind of keep our infrastructure provisioned according to our workloads and one day we came to",
    "start": "804050",
    "end": "812120"
  },
  {
    "text": "find that our cluster was shrinking actually we found out after the fact our",
    "start": "812120",
    "end": "817160"
  },
  {
    "text": "cluster shrank to something around a third of its original size and this came up as a user report of my workloads not",
    "start": "817160",
    "end": "824240"
  },
  {
    "text": "running that's never a happy moment as a cluster operator so first stop we",
    "start": "824240",
    "end": "836560"
  },
  {
    "text": "took a look at the obviously consulted the API server to see what our note you",
    "start": "837540",
    "end": "843149"
  },
  {
    "text": "know what nodes were being reported found the the shrank size and then proceeded to go and check out our AWS",
    "start": "843149",
    "end": "850170"
  },
  {
    "text": "ASG the auto scaling group scaling history which helpfully reported that yes those nodes had been terminated and",
    "start": "850170",
    "end": "856589"
  },
  {
    "text": "yes that had been requested and happily complied with let's see and the next top",
    "start": "856589",
    "end": "866459"
  },
  {
    "text": "was to go check the cluster autoscaler because knowing that this component is running and interacting with the auto",
    "start": "866459",
    "end": "872519"
  },
  {
    "text": "scaling group api's that was a logical culprit and autoscaler group are the",
    "start": "872519",
    "end": "879600"
  },
  {
    "text": "autoscaler logs reported that utilization was observed at 0.0 so we",
    "start": "879600",
    "end": "885660"
  },
  {
    "text": "have the autoscaler set to both scale up and scale down when it observes 0 utilization it's gonna happily scale",
    "start": "885660",
    "end": "891480"
  },
  {
    "text": "down problem was utilization was not 0 so we were scratching our heads about",
    "start": "891480",
    "end": "902459"
  },
  {
    "text": "this for a while we had this one incident which was catice significantly",
    "start": "902459",
    "end": "909480"
  },
  {
    "text": "impactful our cluster was still operable but number of workloads were impacted but it didn't recur at the same scale",
    "start": "909480",
    "end": "917190"
  },
  {
    "text": "and this is truly a sad tale because we have not been able to determined true",
    "start": "917190",
    "end": "922860"
  },
  {
    "text": "root cause on this issue a couple of things that made that difficult was that we had diagnostic data aged out in terms",
    "start": "922860",
    "end": "930630"
  },
  {
    "text": "of the numeric telemetry and logs that were aggregated we were trying to you",
    "start": "930630",
    "end": "935730"
  },
  {
    "text": "know use that data while it was available to try and understand the issue but we didn't get to the root",
    "start": "935730",
    "end": "942510"
  },
  {
    "text": "cause during that period of time and we didn't do the work to durably capture that in a long-term way to be able to go back and look at it so furthermore we",
    "start": "942510",
    "end": "949440"
  },
  {
    "text": "didn't have the data to go open an upstream issue about this and potentially save somebody else the",
    "start": "949440",
    "end": "954750"
  },
  {
    "text": "trouble now a couple of things about how we worked around it we well as it says",
    "start": "954750",
    "end": "965640"
  },
  {
    "text": "here we extended the smoothing function so we so we slowed down its scale and behavior both",
    "start": "965640",
    "end": "970960"
  },
  {
    "text": "reducing the number of nodes it would scale in it at once as well as the interval between scale and events so",
    "start": "970960",
    "end": "977140"
  },
  {
    "text": "that gave us time to respond so that we wouldn't get it sort of bulk bulk killed",
    "start": "977140",
    "end": "982300"
  },
  {
    "text": "which is clearly a workaround but at least gives us you know even if this is a middle-of-the-night incident gives us",
    "start": "982300",
    "end": "988180"
  },
  {
    "text": "time to kind of get somebody at a keyboard and intervene before before we're in a catastrophic situation in",
    "start": "988180",
    "end": "996280"
  },
  {
    "text": "addition we upgraded the cluster auto scalar component to a version which supports a significantly better observability in terms of the metrics",
    "start": "996280",
    "end": "1002670"
  },
  {
    "text": "that it exports and makes visible but there's still a couple of metrics that we haven't yet been able to introduce to",
    "start": "1002670",
    "end": "1010620"
  },
  {
    "text": "the cluster autoscaler we'd like it to be able to report what will happen it reports what's unneeded but not the actual intended scale in that it's going",
    "start": "1010620",
    "end": "1019110"
  },
  {
    "text": "to perform this is something that we know how to do we just haven't gotten a chance to get to yet and along the way",
    "start": "1019110",
    "end": "1026910"
  },
  {
    "text": "we learned some interesting things specifically and this may be germane",
    "start": "1026910",
    "end": "1032610"
  },
  {
    "text": "it's somewhat specular because we can couldn't conclusively prove this but this I suspect is germane to to what hit",
    "start": "1032610",
    "end": "1039900"
  },
  {
    "text": "us that day that so first first interesting thing we learned in this",
    "start": "1039900",
    "end": "1046170"
  },
  {
    "text": "investigation was that the kubernetes service the cluster IP service that kubernetes creates by default to be able",
    "start": "1046170",
    "end": "1053340"
  },
  {
    "text": "to address the API server often it shows up as kubernetes default service cluster local there is session affinity set on",
    "start": "1053340",
    "end": "1060780"
  },
  {
    "text": "that service implicitly you don't create that service that service is created by the API server if you're running in a",
    "start": "1060780",
    "end": "1067920"
  },
  {
    "text": "high availability configuration and you have multiple API servers present in your cluster there's a number of",
    "start": "1067920",
    "end": "1073140"
  },
  {
    "text": "different knobs and possible configurations you could be in you could be advertising a load balancer but if you're connecting directly to an API",
    "start": "1073140",
    "end": "1078450"
  },
  {
    "text": "server your clients that are addressing it in that way will be pinned to one",
    "start": "1078450",
    "end": "1085080"
  },
  {
    "text": "specific API server instance on the face of it that's not so surprising there's a",
    "start": "1085080",
    "end": "1091080"
  },
  {
    "text": "number of reasons why that's the case this issue explains a bit however the second thing we learned is that the",
    "start": "1091080",
    "end": "1096660"
  },
  {
    "text": "behavior of API servers when you're running with the API server account flag is surprising",
    "start": "1096660",
    "end": "1105060"
  },
  {
    "text": "so in short what happens here is readiness is not respected when the API",
    "start": "1105060",
    "end": "1111900"
  },
  {
    "text": "server count flag is in play so in our case we run five node control plane five instances of Etsy d5 API servers",
    "start": "1111900",
    "end": "1119600"
  },
  {
    "text": "scheduler and controller manager in HT configurations and we run with the API servers having the API server account",
    "start": "1119600",
    "end": "1126570"
  },
  {
    "text": "set to five seems seemed logical now it turns out that the behavior with that",
    "start": "1126570",
    "end": "1132770"
  },
  {
    "text": "setting is that all five of those API servers remain in the endpoints set for that service at all times so if you're",
    "start": "1132770",
    "end": "1138960"
  },
  {
    "text": "in a maintenance event and you don't change the count value bringing down one of your API servers 20% of your traffic",
    "start": "1138960",
    "end": "1146160"
  },
  {
    "text": "that is addressing those API servers over that cluster service are going to fail and it could actually be more",
    "start": "1146160",
    "end": "1153210"
  },
  {
    "text": "because of the client because of the session affinity based on client IP you can end up in situations where you have hotspots because the clients could",
    "start": "1153210",
    "end": "1159980"
  },
  {
    "text": "during maintenance operations you could consolidate down to less than your total set of API servers now the API server",
    "start": "1159980",
    "end": "1168120"
  },
  {
    "text": "account issue the referenced pull request here helpfully suggests that the",
    "start": "1168120",
    "end": "1174360"
  },
  {
    "text": "behavior with API server account set greater to one is worse than just leaving it set to one I tend to agree I",
    "start": "1174360",
    "end": "1183240"
  },
  {
    "text": "think basically this is something that I there are there is upstream work to change but it's something to be aware of",
    "start": "1183240",
    "end": "1189570"
  },
  {
    "text": "where you have components address in the API server over that service we've seen a couple of issues around the schedule",
    "start": "1189570",
    "end": "1194940"
  },
  {
    "text": "and controller manager as well especially during maintenance operations this is where it's come up when we're maintaining the control plane nodes and",
    "start": "1194940",
    "end": "1202020"
  },
  {
    "text": "when we have to take one of those out of service we've well developed our maintenance",
    "start": "1202020",
    "end": "1207420"
  },
  {
    "text": "procedures to work around this because we got hit by a couple times so something to be aware of and the further",
    "start": "1207420",
    "end": "1214230"
  },
  {
    "text": "speculation getting getting out past what I can strongly assert but one possible explanation we sort of",
    "start": "1214230",
    "end": "1221130"
  },
  {
    "text": "hypothesized about this cluster autoscaler incident and the zero point zero utilization observed in the logs is",
    "start": "1221130",
    "end": "1226290"
  },
  {
    "text": "that under some set of circumstances which I can't name the cluster autoscaler when talking to",
    "start": "1226290",
    "end": "1232140"
  },
  {
    "text": "the API servers metrics API had some network connectivity which was",
    "start": "1232140",
    "end": "1239420"
  },
  {
    "text": "deserialized into in-memory representations and go and that goes behavior of representing",
    "start": "1239420",
    "end": "1247970"
  },
  {
    "text": "uninitialized values as the zero value of the type could potentially and again",
    "start": "1247970",
    "end": "1253500"
  },
  {
    "text": "speculative here but could potentially have resulted in an uninitialized flow being reported as a zero point zero and",
    "start": "1253500",
    "end": "1259440"
  },
  {
    "text": "then acted upon resulting in the scale and behavior we described don't let this",
    "start": "1259440",
    "end": "1265170"
  },
  {
    "text": "happen to you and as a side note we have actually so we mentioned that the smoothing function",
    "start": "1265170",
    "end": "1272870"
  },
  {
    "text": "where were we here yes that the smoothing function slowdown we've",
    "start": "1272870",
    "end": "1278160"
  },
  {
    "text": "upgraded to the newest release of the cluster autoscaler and we haven't been hit by this behavior but we have seen intermittent only very occasional but we",
    "start": "1278160",
    "end": "1287460"
  },
  {
    "text": "have once or twice seen reported in the cluster autoscaler logs that observe utilization was zero when in fact it was not",
    "start": "1287460",
    "end": "1293430"
  },
  {
    "text": "so I thrown around a little bit of flood there and I guess I don't personally",
    "start": "1293430",
    "end": "1298440"
  },
  {
    "text": "completely understand the issue but um it may still be lurking out there in the woods careful",
    "start": "1298440",
    "end": "1304970"
  },
  {
    "text": "now I'm going to move on we had a truly catastrophic outage which we calling the",
    "start": "1304970",
    "end": "1312990"
  },
  {
    "text": "split personality SCD cluster so I was describing a bit about how our control",
    "start": "1312990",
    "end": "1319290"
  },
  {
    "text": "plane maintenance procedures had grown to kind of work around this interaction of the API server account and session",
    "start": "1319290",
    "end": "1326880"
  },
  {
    "text": "stickiness or session affinity of the API server communications from",
    "start": "1326880",
    "end": "1332100"
  },
  {
    "text": "components that talk to the API server over the built-in cluster IP service so",
    "start": "1332100",
    "end": "1337110"
  },
  {
    "text": "our procedure for performing control plane node maintenance which we occasionally have to do due to",
    "start": "1337110",
    "end": "1343430"
  },
  {
    "text": "configuration updates we treat our nodes control plane nodes included as sort of",
    "start": "1343430",
    "end": "1349590"
  },
  {
    "text": "immutable and when we need to update the underlying node configuration that's typically will almost universally we",
    "start": "1349590",
    "end": "1357120"
  },
  {
    "text": "will terminate one by one and to roll through the roll through the set and replace them with the with",
    "start": "1357120",
    "end": "1362680"
  },
  {
    "text": "nodes of the new configuration so given given what I just described that means removing a member from the NCD quorum",
    "start": "1362680",
    "end": "1369270"
  },
  {
    "text": "starting a new machine it also involves actually decrementing the API server API",
    "start": "1369270",
    "end": "1376510"
  },
  {
    "text": "server account flag on all the running API servers then I say starting a new",
    "start": "1376510",
    "end": "1382120"
  },
  {
    "text": "machine we start a new machine that new machine then we we manually add that to",
    "start": "1382120",
    "end": "1388000"
  },
  {
    "text": "the sed quorum and then go back to all of the API servers and increment that API server count flag and that allows us",
    "start": "1388000",
    "end": "1395590"
  },
  {
    "text": "to work around the negative behavior that I described a moment ago I'm telling you all this it's gonna it's",
    "start": "1395590",
    "end": "1400810"
  },
  {
    "text": "gonna come back later now in this incident we had a situation where",
    "start": "1400810",
    "end": "1409450"
  },
  {
    "text": "nothing made sense we were deeply confused we were on cube control get",
    "start": "1409450",
    "end": "1415060"
  },
  {
    "text": "pods and we would get two different sets of data and it would alternate back and",
    "start": "1415060",
    "end": "1420250"
  },
  {
    "text": "forth we would run cube control get nodes and we would get two different sets of data and it would alternate back",
    "start": "1420250",
    "end": "1427270"
  },
  {
    "text": "and forth and we couldn't make sense of it it seemed like there were two",
    "start": "1427270",
    "end": "1432910"
  },
  {
    "text": "opinions of what the state of the cluster was what was going on and here's",
    "start": "1432910",
    "end": "1439480"
  },
  {
    "text": "how it came up to us our user this is this is not ever a conversation you want",
    "start": "1439480",
    "end": "1446590"
  },
  {
    "text": "to have as a cluster operator and not because this is a self-identified Cubs fan but because pods starting up and",
    "start": "1446590",
    "end": "1456190"
  },
  {
    "text": "vanishing it's not supposed to something that's supposed to happen here so when we went and looked we'd see this and",
    "start": "1456190",
    "end": "1463780"
  },
  {
    "text": "then this and that flashed by pretty quickly so it would go back and forth",
    "start": "1463780",
    "end": "1469300"
  },
  {
    "text": "between these two states something like this although obviously depending what resources you were looking at you'd see different things this was confusing to",
    "start": "1469300",
    "end": "1478480"
  },
  {
    "text": "say the least as I already said and it wasn't just about cube control and reporting to our sort of interactive",
    "start": "1478480",
    "end": "1485110"
  },
  {
    "text": "query of the state of the cluster the control loop started misbehaving so this manifested as the controller manager",
    "start": "1485110",
    "end": "1491790"
  },
  {
    "text": "acting on incorrect data and this was thousands of pods",
    "start": "1491790",
    "end": "1497249"
  },
  {
    "text": "getting spun up and terminated I don't that basically the cluster or the",
    "start": "1497249",
    "end": "1504029"
  },
  {
    "text": "controller manager would be querying API servers and getting alternating sets of",
    "start": "1504029",
    "end": "1509789"
  },
  {
    "text": "data and then hopefully rapidly acting on alternating sets of data trying to",
    "start": "1509789",
    "end": "1516059"
  },
  {
    "text": "converge to some state but it's hard to converge when you're getting conflicting",
    "start": "1516059",
    "end": "1521429"
  },
  {
    "text": "views of the world the next set of you know horrible symptoms asserted manifesting was you know service",
    "start": "1521429",
    "end": "1527340"
  },
  {
    "text": "endpoints were thrashing our English controller started to do very bad things to the traffic that should have been",
    "start": "1527340",
    "end": "1532980"
  },
  {
    "text": "flowing through it or rather the ingress controller itself was reasonably okay",
    "start": "1532980",
    "end": "1538529"
  },
  {
    "text": "but it just it was responding to these conflicting signals so there's bad news",
    "start": "1538529",
    "end": "1544649"
  },
  {
    "text": "here this was a full cluster outage on our primary production cluster we",
    "start": "1544649",
    "end": "1549690"
  },
  {
    "text": "weren't service simply out of service it wasn't just sort of went away but it was sort of violently wrong and our time to",
    "start": "1549690",
    "end": "1558119"
  },
  {
    "text": "resolution was brutal it was about a four hour outage for our until we were able to restore cluster or restore",
    "start": "1558119",
    "end": "1563970"
  },
  {
    "text": "service by way of a replacement cluster there are a number of contributing",
    "start": "1563970",
    "end": "1569369"
  },
  {
    "text": "factors that kind of led to that significant of a tender resolution one",
    "start": "1569369",
    "end": "1575159"
  },
  {
    "text": "was you know simply confusion but also we were reluctant to replace the cluster and spend in quite a while",
    "start": "1575159",
    "end": "1580619"
  },
  {
    "text": "troubleshooting and diagnosing we knew that replacement would mean read",
    "start": "1580619",
    "end": "1586889"
  },
  {
    "text": "basically redeploying applications on the new cluster and there was a fairly distributed relationship in terms of",
    "start": "1586889",
    "end": "1592769"
  },
  {
    "text": "teams utilizing features that are challenging to simply migrate pick up",
    "start": "1592769",
    "end": "1598919"
  },
  {
    "text": "and move from cluster cluster we didn't end up replacing the cluster so Oliver",
    "start": "1598919",
    "end": "1604049"
  },
  {
    "text": "lectins ended up being for naught and that was also contributing contributed to the delay and the duration of our",
    "start": "1604049",
    "end": "1610679"
  },
  {
    "text": "outage so some specific things here volumes are challenging volumes are exclusively by own to one node so when",
    "start": "1610679",
    "end": "1617820"
  },
  {
    "text": "we brought up workloads on the new cluster we had to sort of go manually one by one and make sure that those were freed from the errant cluster in order",
    "start": "1617820",
    "end": "1624990"
  },
  {
    "text": "to be able to bind them on the new cluster but actually load balancers were",
    "start": "1624990",
    "end": "1631159"
  },
  {
    "text": "somewhat more challenging because although they were much fewer in number the teams that were using load balancers",
    "start": "1631159",
    "end": "1637980"
  },
  {
    "text": "a handful of teams had created DNS records that pointed to the sort of",
    "start": "1637980",
    "end": "1643769"
  },
  {
    "text": "ephemeral name that kubernetes creates the load balancer with and they had to manually update those DNS records to",
    "start": "1643769",
    "end": "1649830"
  },
  {
    "text": "point to the new load balancer names that were created on the new cluster and",
    "start": "1649830",
    "end": "1655549"
  },
  {
    "text": "one could potentially wish for a little bit better support on kubernetes side in terms of making it possible to kind of",
    "start": "1655820",
    "end": "1662789"
  },
  {
    "text": "claim ownership of an existing load balancer I think it's maybe possible but it's not a supported use case so",
    "start": "1662789",
    "end": "1670080"
  },
  {
    "text": "basically migrating load balancers is not realistic there's other ways to work around this in terms of managing the DNS",
    "start": "1670080",
    "end": "1676620"
  },
  {
    "text": "records on the community side is is something that we've looked at but haven't gotten in a place yet it's",
    "start": "1676620",
    "end": "1682190"
  },
  {
    "text": "there's some other things that make that challenging there's some good news though so this happened during working",
    "start": "1682190",
    "end": "1688559"
  },
  {
    "text": "hours hooray nobody got woke up in the middle of the night so we had full team presence and we were able to kind of",
    "start": "1688559",
    "end": "1694860"
  },
  {
    "text": "bring our full resources to bear and another bit of good news is that we were able to analyze this one and and",
    "start": "1694860",
    "end": "1700200"
  },
  {
    "text": "actually get down to a root cause and and that root cause understanding did Drive or did lead to significant",
    "start": "1700200",
    "end": "1706529"
  },
  {
    "text": "improvements in our understanding of the system its behavior under this failure",
    "start": "1706529",
    "end": "1711809"
  },
  {
    "text": "mode our code and and our procedures around that now you may be asking but",
    "start": "1711809",
    "end": "1718649"
  },
  {
    "text": "wait how could this possibly happen that's it is a consistent key value store right that's the whole point it is",
    "start": "1718649",
    "end": "1724200"
  },
  {
    "text": "it is and I'm not making any claim we didn't observe sprit split-brain as described which is a breakage of the",
    "start": "1724200",
    "end": "1731129"
  },
  {
    "text": "consensus model itself but what we did see was stale data and",
    "start": "1731129",
    "end": "1738450"
  },
  {
    "text": "it's not super widely known I don't think but it is indeed documented that",
    "start": "1738450",
    "end": "1743520"
  },
  {
    "text": "etsy D will return steal data there are known conditions under which that can occur and typically this is very small",
    "start": "1743520",
    "end": "1750030"
  },
  {
    "text": "increments where the sed members I have you know some small degree of replication lag and between them a",
    "start": "1750030",
    "end": "1755820"
  },
  {
    "text": "leader will accept rights and then replicate those two followers but under",
    "start": "1755820",
    "end": "1761190"
  },
  {
    "text": "certain sets of scenarios that lag can get very very significant and it can also get if you have very significant",
    "start": "1761190",
    "end": "1767700"
  },
  {
    "text": "lag you can also in inter situations where you're having leader thrashing because leader election timeouts can be",
    "start": "1767700",
    "end": "1774930"
  },
  {
    "text": "repeatedly crossed causing multiple elections back to back to back and like",
    "start": "1774930",
    "end": "1779970"
  },
  {
    "text": "I said this is documented and in fact we had even read the manual and knew that",
    "start": "1779970",
    "end": "1786900"
  },
  {
    "text": "there was at least some theoretical possibility of this at the time which is to say earlier this year six months ago",
    "start": "1786900",
    "end": "1794240"
  },
  {
    "text": "there wasn't widespread agreement that this that oh sorry let me back up for",
    "start": "1794240",
    "end": "1799830"
  },
  {
    "text": "just a second so one piece of mitigation here resolution actually is to always",
    "start": "1799830",
    "end": "1805320"
  },
  {
    "text": "ensure demand that HCD perform quorum operation when reads are are happening",
    "start": "1805320",
    "end": "1811800"
  },
  {
    "text": "so stale data is in the read path not in the right path so rights are always",
    "start": "1811800",
    "end": "1816810"
  },
  {
    "text": "consistent you'll always get an act that reflects the quorum of the cluster the NCD cluster but on the read path etsy d",
    "start": "1816810",
    "end": "1824820"
  },
  {
    "text": "can return data that's only known to the local host the the local ad CD instance that you're querying and that's the case",
    "start": "1824820",
    "end": "1831900"
  },
  {
    "text": "unless you expressly request a quorum read quorum read involves a quorum of",
    "start": "1831900",
    "end": "1837630"
  },
  {
    "text": "the cluster and it happens to be the case that kubernetes api server and its",
    "start": "1837630",
    "end": "1842820"
  },
  {
    "text": "default configuration does not request a quorum read and it's not mentioned in",
    "start": "1842820",
    "end": "1847950"
  },
  {
    "text": "the a date a che Docs or at least until october of this year it wasn't mentioned in the a che docs there's another may of",
    "start": "1847950",
    "end": "1854820"
  },
  {
    "text": "culpa here we after having observed this and then immediately going after",
    "start": "1854820",
    "end": "1859950"
  },
  {
    "text": "resolving the issue immediately going to the HJ docs and scratching our head saying how did we miss that found that",
    "start": "1859950",
    "end": "1866880"
  },
  {
    "text": "it wasn't there it is now so read the docs read the box like Sunday at noon like it was",
    "start": "1866880",
    "end": "1874500"
  },
  {
    "text": "church like those are changing and you have to stay up-to-date and some of the",
    "start": "1874500",
    "end": "1880320"
  },
  {
    "text": "some of the reason that that wasn't a recommended configuration from the beginning is that there was concerns",
    "start": "1880320",
    "end": "1885750"
  },
  {
    "text": "about the performance impact when when you're performing a quorum read on the right path you have to talk to you know",
    "start": "1885750",
    "end": "1891269"
  },
  {
    "text": "a quorum so majority of your Etsy team members and xev 3.1 greatly reduces the",
    "start": "1891269",
    "end": "1899070"
  },
  {
    "text": "impact of that because reads don't have to hit discs anymore previously that meant a disk hit on on you know majority",
    "start": "1899070",
    "end": "1906059"
  },
  {
    "text": "of your got CD cluster and and 1.9 quorum reads become the default behavior",
    "start": "1906059",
    "end": "1911909"
  },
  {
    "text": "so if you're running 1.9 you're safe if you're not running 1.9 you may not be",
    "start": "1911909",
    "end": "1918450"
  },
  {
    "text": "safe there yeah this might be something to go look at in your configs if you're",
    "start": "1918450",
    "end": "1925409"
  },
  {
    "text": "running your own API server configs so in our case we know what happened we got",
    "start": "1925409",
    "end": "1934169"
  },
  {
    "text": "hit by right latency so exit II is extremely sensitive to right latency this is healthy right latency we're",
    "start": "1934169",
    "end": "1941220"
  },
  {
    "text": "talking single-digit milliseconds in this case this is this is a happy Etsy the instance or a cluster actually this is five Exedy instances this this is not",
    "start": "1941220",
    "end": "1949320"
  },
  {
    "text": "a happy at CD instance so if you if you start crossing double",
    "start": "1949320",
    "end": "1954750"
  },
  {
    "text": "digit or multiple hundreds of milliseconds of right latency and this is specifically the database sync lag",
    "start": "1954750",
    "end": "1961580"
  },
  {
    "text": "you're you're entering a world of hurt and that CDs gonna start to do well if",
    "start": "1961580",
    "end": "1967590"
  },
  {
    "text": "you're not using core memories you're potentially going to start to see behavior like we saw even with corn",
    "start": "1967590",
    "end": "1973260"
  },
  {
    "text": "beats you're gonna start to see you know tremendously degraded performance because every operation is gonna see well many of your operations are gonna",
    "start": "1973260",
    "end": "1981060"
  },
  {
    "text": "see significant lag so how is it that we have missed such an important thing on",
    "start": "1981060",
    "end": "1988350"
  },
  {
    "text": "such a critical database and the answer is really that there's just so much to",
    "start": "1988350",
    "end": "1993990"
  },
  {
    "text": "to go figure out if you're running a kubernetes cluster and to kind of",
    "start": "1993990",
    "end": "1999570"
  },
  {
    "text": "demonstrate kind of what i'm trying to talk about i mean let's let's talk about what happens at the node level so this",
    "start": "1999570",
    "end": "2007340"
  },
  {
    "text": "is from a great site post by Brendan Gregg who posts a lot of topics about",
    "start": "2007340",
    "end": "2014210"
  },
  {
    "text": "Linux performance and if you need to figure out how to troubleshoot performance issues on a Linux node this",
    "start": "2014210",
    "end": "2021980"
  },
  {
    "text": "is a pretty good kind of road map to that and as you can see there's just a lot of different components you have to",
    "start": "2021980",
    "end": "2027560"
  },
  {
    "text": "worry yourself about and for each component there's a different set of tools right so now as cluster operators",
    "start": "2027560",
    "end": "2033380"
  },
  {
    "text": "we don't run one node we run dozens of nodes and dozens of nodes each in dozens",
    "start": "2033380",
    "end": "2038960"
  },
  {
    "text": "of clusters right and and so you really need to stay on top of this but this",
    "start": "2038960",
    "end": "2045080"
  },
  {
    "text": "isn't a small right this is zoomed in down to the node if we take a zoom all the way back out to the ecosystem this",
    "start": "2045080",
    "end": "2052398"
  },
  {
    "text": "is what we have to deal with and in fact Linux isn't even anywhere on the slide so so not only needs to deal with the",
    "start": "2052399",
    "end": "2060378"
  },
  {
    "text": "operating system but the cloud level and provisioning you need to know the quirks of your cloud providers because maybe",
    "start": "2060379",
    "end": "2065960"
  },
  {
    "text": "they're going to transparently replace or scale in some of the nodes behind",
    "start": "2065960",
    "end": "2071120"
  },
  {
    "text": "your load balancer doing crazy things to your network traffic you need to know about your container runtime you need to",
    "start": "2071120",
    "end": "2080270"
  },
  {
    "text": "know about that overlay network plugin that you're running how that performing for you you need to know",
    "start": "2080270",
    "end": "2086560"
  },
  {
    "text": "obviously about kubernetes itself how the scheduler interacts with the API server and how the cubelets talk to each",
    "start": "2086560",
    "end": "2093250"
  },
  {
    "text": "other and the controller managers and everything and then don't forget that super critical 300 megabyte database",
    "start": "2093250",
    "end": "2101560"
  },
  {
    "text": "called Etsy D that's really sensitive too right Layton sees and and as you can",
    "start": "2101560",
    "end": "2108280"
  },
  {
    "text": "tell there's a the surface area is huge absolutely huge but here's the thing",
    "start": "2108280",
    "end": "2117030"
  },
  {
    "text": "full-stack devops teams have had to deal with something like this for years because you know it may not be",
    "start": "2117030",
    "end": "2124630"
  },
  {
    "text": "kubernetes but they still have to deal with their applications that deployments the cloud the provisioning the operating",
    "start": "2124630",
    "end": "2131800"
  },
  {
    "text": "systems of virtual machines the networking the DNS load balancers all of that stuff and what kubernetes provides",
    "start": "2131800",
    "end": "2138970"
  },
  {
    "text": "for the application teams at least is a simple interface through which to run their applications and then push all",
    "start": "2138970",
    "end": "2145510"
  },
  {
    "text": "that responsibility on to somebody else the cluster operators so that's the good",
    "start": "2145510",
    "end": "2151300"
  },
  {
    "text": "news good for them as for us cluster operators well any number of small",
    "start": "2151300",
    "end": "2158020"
  },
  {
    "text": "things could possibly turn into a big issue and so surely out of this diagram",
    "start": "2158020",
    "end": "2165700"
  },
  {
    "text": "and the one before there must be 101 or more ways for your cluster to come",
    "start": "2165700",
    "end": "2171850"
  },
  {
    "text": "tumbling down thank you [Applause]",
    "start": "2171850",
    "end": "2181479"
  }
]