[
  {
    "text": "hello everyone how are you doing fine hmm",
    "start": "170",
    "end": "6029"
  },
  {
    "text": "I assume that that is good good land my name is Eduardo Silva",
    "start": "6029",
    "end": "11040"
  },
  {
    "text": "I'm a principal engineer at arm and as you know we are in the deep dive session",
    "start": "11040",
    "end": "16560"
  },
  {
    "text": "of flew indie project and this deep dive session will focus on the flu and bed",
    "start": "16560",
    "end": "22130"
  },
  {
    "text": "project which is a sub break of the flu in the ecosystem as I said I'm engineer",
    "start": "22130",
    "end": "30449"
  },
  {
    "text": "at arm we used to be treasured data the company who created flu Indy and I'm a maintainer of this project which is",
    "start": "30449",
    "end": "36780"
  },
  {
    "text": "called flowing bed first of all do we have any flu and bed users here please",
    "start": "36780",
    "end": "41969"
  },
  {
    "text": "raise your hand oh that's good so I think that I will owe you some stickers t-shirts stuff well a we will cover",
    "start": "41969",
    "end": "51960"
  },
  {
    "text": "something that is really new here it's about called stream processing but",
    "start": "51960",
    "end": "57539"
  },
  {
    "text": "before jumping to that we are going to get to the basics of logging when",
    "start": "57539",
    "end": "62670"
  },
  {
    "text": "application generates a message that message is composed as a record that's",
    "start": "62670",
    "end": "69240"
  },
  {
    "text": "like the technical term internal either influenza or flu in bed and that record",
    "start": "69240",
    "end": "74549"
  },
  {
    "text": "comes is not just a message it also comes with some metadata and that",
    "start": "74549",
    "end": "79590"
  },
  {
    "text": "metadata means the timestamp of when this message was created plus the same",
    "start": "79590",
    "end": "85500"
  },
  {
    "text": "message that now this toy is going to be sterilized it you can imagine for",
    "start": "85500",
    "end": "90540"
  },
  {
    "text": "example a JSON map but on this case we use message back Jason is like a human",
    "start": "90540",
    "end": "96659"
  },
  {
    "text": "readable format but message back is binary so you can you can think about",
    "start": "96659",
    "end": "101790"
  },
  {
    "text": "the performance that you have working with JSON internally in step 2 and beta flow in D versus message effect so an",
    "start": "101790",
    "end": "111299"
  },
  {
    "text": "application generate a message that message becomes a lot that low close to",
    "start": "111299",
    "end": "116640"
  },
  {
    "text": "the file system maybe over the network and in the other side we have to embed generating a record with this",
    "start": "116640",
    "end": "122850"
  },
  {
    "text": "information that becomes a binary record then inside in the internal part the",
    "start": "122850",
    "end": "131069"
  },
  {
    "text": "records can become our group right for example if you are consuming",
    "start": "131069",
    "end": "136110"
  },
  {
    "text": "logs from an input section and this input section means a couple of log files maybe these log files a has many",
    "start": "136110",
    "end": "143820"
  },
  {
    "text": "information and you want to group them by tags so you're saying all of these messages are coming from Apache or all",
    "start": "143820",
    "end": "150660"
  },
  {
    "text": "of these other messages are coming from syslog and why grouping them is really",
    "start": "150660",
    "end": "156060"
  },
  {
    "text": "important when you want to do login stuff because you need to sort and have",
    "start": "156060",
    "end": "161760"
  },
  {
    "text": "some order for these logs because you're you're in as we said in the keynote yesterday well it's not logging log in",
    "start": "161760",
    "end": "168630"
  },
  {
    "text": "as the same any time is boring but what you want to do in reality is data analysis for troubleshooting learning or",
    "start": "168630",
    "end": "175320"
  },
  {
    "text": "whatever once the data of the records goes to the storage phase this can be",
    "start": "175320",
    "end": "180930"
  },
  {
    "text": "either in memory or the filesystem it goes to the rotor the rotor basically",
    "start": "180930",
    "end": "186690"
  },
  {
    "text": "said where are these records going to be delivered and as you can see you can",
    "start": "186690",
    "end": "192540"
  },
  {
    "text": "define mayn't multiple output plugins or configurations output from where you",
    "start": "192540",
    "end": "197550"
  },
  {
    "text": "would like to ship these locks out and where you're going to ship it come your elasticsearch database your cloud",
    "start": "197550",
    "end": "204270"
  },
  {
    "text": "service or whatever and if you look carefully the rotor does a really",
    "start": "204270",
    "end": "209280"
  },
  {
    "text": "special job here because it matched the tag from each record with a match",
    "start": "209280",
    "end": "214680"
  },
  {
    "text": "section define it in the configuration so the rotors say okay all this bunch of",
    "start": "214680",
    "end": "220890"
  },
  {
    "text": "record had this tag I'm going to look for the matching rules so this data is going to this place or maybe that same",
    "start": "220890",
    "end": "227640"
  },
  {
    "text": "data is going into multiple places that can be done too without any problem in",
    "start": "227640",
    "end": "234209"
  },
  {
    "text": "the community space it's like this I'm sure that most of you using kubernetes",
    "start": "234209",
    "end": "239700"
  },
  {
    "text": "or are familiar with docker but in kubernetes we have the concept of the master api server and the node in a",
    "start": "239700",
    "end": "247770"
  },
  {
    "text": "cluster we have many nodes right but if you have applications deployed in a node you need to have your login processor to",
    "start": "247770",
    "end": "255540"
  },
  {
    "text": "be able to consume the login information from the other ports or containers so what we do is that we deploy fluent pip",
    "start": "255540",
    "end": "262560"
  },
  {
    "text": "or flew indeed it doesn't matter what where is the case reading we more the volume of the note looks and then we",
    "start": "262560",
    "end": "269389"
  },
  {
    "text": "let the lock processor consume those blocks now how this correlate with the main",
    "start": "269389",
    "end": "277700"
  },
  {
    "text": "topic of this session which is called a stream processing are you familiar with Apache spark or Kafka case equal okay so",
    "start": "277700",
    "end": "287780"
  },
  {
    "text": "with one bit one that one release we focus on two major things one was",
    "start": "287780",
    "end": "293630"
  },
  {
    "text": "Windows support and you can realize that many windows service running and many",
    "start": "293630",
    "end": "299510"
  },
  {
    "text": "people has been asking for Windows support windows support maybe people is not really interested to do a huge",
    "start": "299510",
    "end": "305389"
  },
  {
    "text": "processing in Windows environments but they want take out the logs out of the",
    "start": "305389",
    "end": "310460"
  },
  {
    "text": "box to a UNIX box for processing irrigation or whatever so we have Windows support in beta so if you are a",
    "start": "310460",
    "end": "317270"
  },
  {
    "text": "Windows user you are administrating windows services please a built from dip from that it's in a beta beta phase so",
    "start": "317270",
    "end": "325130"
  },
  {
    "text": "any feedback you can provide us is really important and the second part is a stream processing so what is the",
    "start": "325130",
    "end": "334130"
  },
  {
    "text": "stream processing who can describe a stream processing please raise your hand let's try to make it more more friendly",
    "start": "334130",
    "end": "340840"
  },
  {
    "text": "it doesn't matter if you're wrong or you're right maybe I'm wrong but what",
    "start": "340840",
    "end": "346220"
  },
  {
    "text": "means for your stream processing leh",
    "start": "346220",
    "end": "351820"
  },
  {
    "text": "Oh microphone it means I want to",
    "start": "352730",
    "end": "359690"
  },
  {
    "text": "transform my data as it is flowing through my pipeline yes from the data as far as I get it into my pipelines",
    "start": "359690",
    "end": "366080"
  },
  {
    "text": "perfect so this is not just streams of data so estranged data can be anything",
    "start": "366080",
    "end": "372680"
  },
  {
    "text": "but the stream processing is ability to transfer the data while the data is",
    "start": "372680",
    "end": "377990"
  },
  {
    "text": "still in motion okay so if you're going to process that in your database your",
    "start": "377990",
    "end": "384140"
  },
  {
    "text": "data already was collected processes aggregated indexed set in the database",
    "start": "384140",
    "end": "389210"
  },
  {
    "text": "and then you're doing queries but here we are talking about the stream processing on the edge side who of you",
    "start": "389210",
    "end": "398030"
  },
  {
    "text": "are using elasticsearch please raise your hand okay so all of you use elasticsearch not just to use Kabana",
    "start": "398030",
    "end": "405650"
  },
  {
    "text": "have fancy you know - works right you're using it because you need to index your data you need to query your data but I'm",
    "start": "405650",
    "end": "412850"
  },
  {
    "text": "sure that at some point a most of you are not quite happy with the performance or the results so the performance of",
    "start": "412850",
    "end": "419870"
  },
  {
    "text": "your results and not because of elastic search because indexing expensive right",
    "start": "419870",
    "end": "425180"
  },
  {
    "text": "need to get the data go to the database they can attach flash blah blah blah blah blah and sometimes you need to get",
    "start": "425180",
    "end": "432470"
  },
  {
    "text": "some feedback loop faster than indexing so stream processing on the edge it's an",
    "start": "432470",
    "end": "439280"
  },
  {
    "text": "ability to have data selection filtering by patterns perform data aggregation",
    "start": "439280",
    "end": "445360"
  },
  {
    "text": "okay this sounds familiar because fluid the effluent bit already kind of doing that or maybe this is kind of you know",
    "start": "445360",
    "end": "452180"
  },
  {
    "text": "create new streams of data based on query results maybe this sounds familiar",
    "start": "452180",
    "end": "458420"
  },
  {
    "text": "to people who ingest data into character into a half storage then pull this data",
    "start": "458420",
    "end": "464210"
  },
  {
    "text": "into an with analog processor and create a new stream based on results but here",
    "start": "464210",
    "end": "471140"
  },
  {
    "text": "we are talking about a Java stack right if we talk about lassic search you means Java if you talk about casca is Java so",
    "start": "471140",
    "end": "478790"
  },
  {
    "text": "sometimes we need something more lightweight and something that can provide us data as full as soon as",
    "start": "478790",
    "end": "484550"
  },
  {
    "text": "possible as something is happening imagine that you have your cluster and maybe you're ready",
    "start": "484550",
    "end": "489680"
  },
  {
    "text": "in a credit card transaction and somebody paid for some item twice how do",
    "start": "489680",
    "end": "496250"
  },
  {
    "text": "you detect type in two seconds can you do it with elasticsearch no maybe in two",
    "start": "496250",
    "end": "502850"
  },
  {
    "text": "into two seconds after the indexing took ten minutes right but that it's not",
    "start": "502850",
    "end": "508460"
  },
  {
    "text": "ideal and the point of this talk is not to blame any database because that's not",
    "start": "508460",
    "end": "513530"
  },
  {
    "text": "the point but the point that indexing takes time indexing is expensive we cannot get rid",
    "start": "513530",
    "end": "520610"
  },
  {
    "text": "of indexing because we need it but also we we can improve how that how do we",
    "start": "520610",
    "end": "526430"
  },
  {
    "text": "operate and create queries so all of this is about performance performance in",
    "start": "526430",
    "end": "533060"
  },
  {
    "text": "technical terms in computing time performance in networking bandwidth and also performance in how fast do I get my",
    "start": "533060",
    "end": "540410"
  },
  {
    "text": "data back and in login in general we have many pains okay accessing the file",
    "start": "540410",
    "end": "546830"
  },
  {
    "text": "system is expensive it doesn't matter what kind of storage you have well phone data parsing is even more",
    "start": "546830",
    "end": "554870"
  },
  {
    "text": "expensive you know we are not dealing with binary data usually if you have a",
    "start": "554870",
    "end": "560870"
  },
  {
    "text": "JSON map or a JSON log that is really expensive to parse okay it's not magic",
    "start": "560870",
    "end": "566600"
  },
  {
    "text": "it's expensive because you need your parser to go by by by checking oh this is a map this is an array did it end",
    "start": "566600",
    "end": "572420"
  },
  {
    "text": "they didn't know maybe have a start to try to check if this is a valid or not and then you have your database indexing",
    "start": "572420",
    "end": "581089"
  },
  {
    "text": "so expensive expensive plus times expensive and then you have your data",
    "start": "581089",
    "end": "587710"
  },
  {
    "text": "okay so we work on this area on flu in the",
    "start": "587710",
    "end": "592820"
  },
  {
    "text": "affluent bit we are flexible we are really fast both can integrate each other or we can work that as a standard",
    "start": "592820",
    "end": "599810"
  },
  {
    "text": "on solutions and that's perfect but what can we do in the other scenario to help",
    "start": "599810",
    "end": "608470"
  },
  {
    "text": "to reduce the load on the database for some queries that we maybe we can do on",
    "start": "608470",
    "end": "614150"
  },
  {
    "text": "this side on the edge so we are going to do a really quick",
    "start": "614150",
    "end": "619520"
  },
  {
    "text": "a basically I have 1 million reports in just one locked file it's a hundred",
    "start": "619520",
    "end": "625790"
  },
  {
    "text": "megabytes file but anyways is expensive and every line is in JSON format and I'm",
    "start": "625790",
    "end": "631820"
  },
  {
    "text": "going to process the data and index it in a database please guess how much time",
    "start": "631820",
    "end": "637190"
  },
  {
    "text": "it will take right we can't speak",
    "start": "637190",
    "end": "643270"
  },
  {
    "text": "milliseconds one second was it okay but we have file system",
    "start": "644470",
    "end": "651850"
  },
  {
    "text": "parsing indexing one second okay can you",
    "start": "651850",
    "end": "665990"
  },
  {
    "text": "see the screen right okay",
    "start": "665990",
    "end": "670930"
  },
  {
    "text": "I'm going to stop the database we started cleaning up everything as I said",
    "start": "672600",
    "end": "685020"
  },
  {
    "text": "many times don't do this at home the delete command is really dangerous plus",
    "start": "685020",
    "end": "693270"
  },
  {
    "text": "with all okay I just wiped the database okay",
    "start": "693270",
    "end": "698370"
  },
  {
    "text": "I'm using Coral a common line to query elasticsearch I'm not going to use",
    "start": "698370",
    "end": "703470"
  },
  {
    "text": "Cabana because this point doesn't make sense okay so where's my file my file is",
    "start": "703470",
    "end": "712260"
  },
  {
    "text": "called cube Conn espied that look maybe",
    "start": "712260",
    "end": "721410"
  },
  {
    "text": "we can do something like this oh I'm going to break it so every record has",
    "start": "721410",
    "end": "726600"
  },
  {
    "text": "like this a date an IP address a word and a country name a flag and a number",
    "start": "726600",
    "end": "732360"
  },
  {
    "text": "between 1 and at 100 so it's nothing complex if you can realize the record is",
    "start": "732360",
    "end": "737430"
  },
  {
    "text": "quite small so what I'm going to do is to process this log file with flu embed",
    "start": "737430",
    "end": "744840"
  },
  {
    "text": "and I'm going to send it to elasticsearch so and please try to come",
    "start": "744840",
    "end": "749970"
  },
  {
    "text": "up with to realize how much time that could take so here I'm going to query",
    "start": "749970",
    "end": "756840"
  },
  {
    "text": "the elastic search database the indexes some people are smiling because they",
    "start": "756840",
    "end": "762270"
  },
  {
    "text": "know that it will fail right in something like this if I'm right",
    "start": "762270",
    "end": "771320"
  },
  {
    "text": "can you read that okay what we care about is the number of",
    "start": "778530",
    "end": "786340"
  },
  {
    "text": "documents that we have in database we have nothing now and going to my full and bit configuration I'm going to",
    "start": "786340",
    "end": "796180"
  },
  {
    "text": "delete that did some cleanup okay this is a setup",
    "start": "796180",
    "end": "801930"
  },
  {
    "text": "friendly for elasticsearch okay I'm going to do something I'm going to be a",
    "start": "802230",
    "end": "810280"
  },
  {
    "text": "really bad logging tool that is going to process the data really fast and send that every fast very fast to the",
    "start": "810280",
    "end": "816010"
  },
  {
    "text": "database right no limits and here I'm going to send the data to elasticsearch",
    "start": "816010",
    "end": "823750"
  },
  {
    "text": "for all records in logstash format okay I think that that should be enough",
    "start": "823750",
    "end": "832320"
  },
  {
    "text": "they're going to run fluent bit from the command line okay so we see that a",
    "start": "832320",
    "end": "842890"
  },
  {
    "text": "number of documents is increasing you're counting right 1 2 3 4 5 ok 1 million ok",
    "start": "842890",
    "end": "855580"
  },
  {
    "text": "so something happened here the numbers are not working because our",
    "start": "855580",
    "end": "864430"
  },
  {
    "text": "file has a million so we sent a 4000 and it's stuck",
    "start": "864430",
    "end": "870870"
  },
  {
    "text": "so what about here let's try to reach that erase oops",
    "start": "870870",
    "end": "878620"
  },
  {
    "text": "I don't get a response from indicted Alice I'm running local fuzz right but",
    "start": "878620",
    "end": "883720"
  },
  {
    "text": "I'm not doing anything like crazy Oh here influent bit I can cut my",
    "start": "883720",
    "end": "890080"
  },
  {
    "text": "connection rejected that's why we get a ticket in our github say hey my data",
    "start": "890080",
    "end": "895600"
  },
  {
    "text": "slows I cannot flush the data ok we need to do some troubleshooting but here I'm",
    "start": "895600",
    "end": "900880"
  },
  {
    "text": "not able to correct wait where's where's",
    "start": "900880",
    "end": "909790"
  },
  {
    "text": "my elasticsearch",
    "start": "909790",
    "end": "912449"
  },
  {
    "text": "it's question and we sent how many",
    "start": "917610",
    "end": "923260"
  },
  {
    "text": "records for well 5,000 so in the cloud",
    "start": "923260",
    "end": "929560"
  },
  {
    "text": "space tanning and buffering is everything I'm not saying that elasticsearch is bad I'm saying that if",
    "start": "929560",
    "end": "936550"
  },
  {
    "text": "you send adjust a little bit of load you can crush things well but our golden",
    "start": "936550",
    "end": "942700"
  },
  {
    "text": "this talk is not to crush the things right so we're going to properly stop",
    "start": "942700",
    "end": "948580"
  },
  {
    "text": "elasticsearch and we're going to study the game but I'm going to stop fooling",
    "start": "948580",
    "end": "953890"
  },
  {
    "text": "bit so it would stop sending data and I'm going to wipe the database again how",
    "start": "953890",
    "end": "965740"
  },
  {
    "text": "much data it index it Oh 23,000 not too bad I'm going to delete everything okay",
    "start": "965740",
    "end": "980770"
  },
  {
    "text": "database is empty now we are going to be a good citizen we're going to apply this",
    "start": "980770",
    "end": "988690"
  },
  {
    "text": "which is called memory buffer limit so we've got a problem that when we send a low process of flu in the affluent bits",
    "start": "988690",
    "end": "994390"
  },
  {
    "text": "and data too fast but that is not a problem the problem says sometimes database cannot process all the data so",
    "start": "994390",
    "end": "1001830"
  },
  {
    "text": "what we do is that we put some limit instead I'm going to take data process",
    "start": "1001830",
    "end": "1007830"
  },
  {
    "text": "this data memory I'm going to have ten megabytes try to ingest it Sarah but I'm not going to consume more data until",
    "start": "1007830",
    "end": "1014760"
  },
  {
    "text": "these ten megabytes are flooded so with this we'll provide some kind of",
    "start": "1014760",
    "end": "1020400"
  },
  {
    "text": "workaround for the database to process our indexes data okay so let's run in a",
    "start": "1020400",
    "end": "1026459"
  },
  {
    "text": "game okay we are doing better as you can",
    "start": "1026459",
    "end": "1036209"
  },
  {
    "text": "see in this number here then the number of documents is going up",
    "start": "1036209",
    "end": "1042600"
  },
  {
    "text": "what we did is just to put some limit some kind of threshold in the fluent bit",
    "start": "1042600",
    "end": "1048060"
  },
  {
    "text": "to send the data more slowly but not slowly but again we sent one million and",
    "start": "1048060",
    "end": "1055050"
  },
  {
    "text": "we are having like 10 seconds 15 seconds and so on and we have not done any query",
    "start": "1055050",
    "end": "1060230"
  },
  {
    "text": "just indexing so I think that you got my point okay so if you're going to move data you need",
    "start": "1060230",
    "end": "1066270"
  },
  {
    "text": "to prepare for failures and also we can improve on many areas so let's do let's",
    "start": "1066270",
    "end": "1072300"
  },
  {
    "text": "finish say 800 thousands it should reach 1 million so shortly",
    "start": "1072300",
    "end": "1080030"
  },
  {
    "text": "there you go 1 million cool it looks like a minute or so just one connection",
    "start": "1080360",
    "end": "1089360"
  },
  {
    "text": "ok so so how much time did it take 1",
    "start": "1089360",
    "end": "1097230"
  },
  {
    "text": "minute or more so here we have some challenges why I'm carrying too much",
    "start": "1097230",
    "end": "1104250"
  },
  {
    "text": "about data indexing the indexing allows you to query your data faster that's the",
    "start": "1104250",
    "end": "1109590"
  },
  {
    "text": "whole goal but if you need you want to query your data and indexing is becoming a blocker for you we need to think is",
    "start": "1109590",
    "end": "1116700"
  },
  {
    "text": "something different and that is flu embedded with stream processing capabilities we was releasing in just",
    "start": "1116700",
    "end": "1122730"
  },
  {
    "text": "the last week but what you can see there is the log in pipeline so what we did is like after",
    "start": "1122730",
    "end": "1127860"
  },
  {
    "text": "the input site where we collect the data we parse the data filter we have the storage side before to wrote the data",
    "start": "1127860",
    "end": "1135750"
  },
  {
    "text": "out what we do we put a new engine from a stream processor and the stream",
    "start": "1135750",
    "end": "1142380"
  },
  {
    "text": "processor can do a lot of queries and this is really good because we do it",
    "start": "1142380",
    "end": "1148530"
  },
  {
    "text": "with SQL straight to query range language so SQL used to be boring but",
    "start": "1148530",
    "end": "1153540"
  },
  {
    "text": "nowadays I think that is quite fun but you say but what do you put any database",
    "start": "1153540",
    "end": "1158850"
  },
  {
    "text": "on fluent 8 or so no there's no database because this is a stream processing everything is happening in memory",
    "start": "1158850",
    "end": "1165360"
  },
  {
    "text": "there's no tables there's no concept of indexing so for flow embed every input",
    "start": "1165360",
    "end": "1171990"
  },
  {
    "text": "plug-in is a stream of data and we can do also do data aggregation",
    "start": "1171990",
    "end": "1178410"
  },
  {
    "text": "so let's do us a second test I had the same file but I just want to retrieve",
    "start": "1178410",
    "end": "1186070"
  },
  {
    "text": "the fields called IP country unknown which are the JSON keys but with the",
    "start": "1186070",
    "end": "1191950"
  },
  {
    "text": "conditions where the field num is greater than 80 and the field country equals to chill so if I want to do that",
    "start": "1191950",
    "end": "1199560"
  },
  {
    "text": "with a database it would take a minute or more which are very load load but how",
    "start": "1199560",
    "end": "1206500"
  },
  {
    "text": "can I accomplish the same thing but with fluid",
    "start": "1206500",
    "end": "1212159"
  },
  {
    "text": "so here you have a really simple SQL query select IP country name from the",
    "start": "1221260",
    "end": "1228890"
  },
  {
    "text": "stream coupon because in the configuration I call all my data that come from the queue Koons ample files is",
    "start": "1228890",
    "end": "1234919"
  },
  {
    "text": "called coupon so there's no table notion might ever become my string of data were",
    "start": "1234919",
    "end": "1241250"
  },
  {
    "text": "none really than 80 and country equals two to chill and this is a stream file",
    "start": "1241250",
    "end": "1248390"
  },
  {
    "text": "and this stream file needs to be configured here stream kept calm and we",
    "start": "1248390",
    "end": "1262460"
  },
  {
    "text": "are going to do something here let me check something okay right now we are",
    "start": "1262460",
    "end": "1269210"
  },
  {
    "text": "not going to ship the data to a database just to the standard output but from the engine so I'm going to modify my output",
    "start": "1269210",
    "end": "1276309"
  },
  {
    "text": "to know and I'm going to increase my buffers for reading because I want to",
    "start": "1276309",
    "end": "1283549"
  },
  {
    "text": "become faster we don't need me so there",
    "start": "1283549",
    "end": "1297740"
  },
  {
    "text": "you go so you start getting the data right out of the box in your tamela well",
    "start": "1297740",
    "end": "1303890"
  },
  {
    "text": "that is useful for debugging purposes right you cannot do anything with that but you already got what you want it",
    "start": "1303890",
    "end": "1310179"
  },
  {
    "text": "well it got all the records so it worried the 1 million in just a fraction of seconds that is what we are trying to",
    "start": "1310179",
    "end": "1317809"
  },
  {
    "text": "achieve with the strain processing on the edge reduce the number of CPU that you have to invest in your cloud side",
    "start": "1317809",
    "end": "1323809"
  },
  {
    "text": "but defer it to the H the H can be your kubernetes node can be your raspberry pi",
    "start": "1323809",
    "end": "1328850"
  },
  {
    "text": "your IOT device or anything that is running through embed okay do you think",
    "start": "1328850",
    "end": "1335480"
  },
  {
    "text": "that this is good yeah cool now lead to something better",
    "start": "1335480",
    "end": "1342250"
  },
  {
    "text": "these results were sent to the standard output but now what we're going to do is",
    "start": "1342250",
    "end": "1348290"
  },
  {
    "text": "create a new stream of data in fluent bit using the results of the string processor are you familiar",
    "start": "1348290",
    "end": "1355020"
  },
  {
    "text": "with K sequel of Carta some of you well so we will do create stream M cube con s",
    "start": "1355020",
    "end": "1367500"
  },
  {
    "text": "so the new stream of data will be called cube con and all the data on that stream",
    "start": "1367500",
    "end": "1372870"
  },
  {
    "text": "will comes from the output of the query but we're going to do something better",
    "start": "1372870",
    "end": "1378510"
  },
  {
    "text": "with do you know that we talked about tagging matching a rhodium tag equals a",
    "start": "1378510",
    "end": "1386809"
  },
  {
    "text": "results so all the results from that",
    "start": "1386809",
    "end": "1394860"
  },
  {
    "text": "query will be created as a new stream with attack call that results go to my",
    "start": "1394860",
    "end": "1404010"
  },
  {
    "text": "fluent bit configuration I'm going to send the data where elastic search and",
    "start": "1404010",
    "end": "1412890"
  },
  {
    "text": "match everything by we're going to match only results because we care just about",
    "start": "1412890",
    "end": "1418830"
  },
  {
    "text": "the data that we just process it we don't care about the others and yeah",
    "start": "1418830",
    "end": "1426360"
  },
  {
    "text": "we're going to use the non friendly set up maybe it works maybe not I don't know so are we good now it should work okay",
    "start": "1426360",
    "end": "1436010"
  },
  {
    "text": "let's wipe our preferred database",
    "start": "1438740",
    "end": "1443780"
  },
  {
    "text": "Oh had to change my stream name so the stream cube cone already exists because",
    "start": "1451330",
    "end": "1458739"
  },
  {
    "text": "the main configuration I put an alias cube con okay",
    "start": "1458739",
    "end": "1464049"
  },
  {
    "text": "so that's why I got an error cube click",
    "start": "1464049",
    "end": "1472989"
  },
  {
    "text": "on this - that is normal and that",
    "start": "1472989",
    "end": "1479169"
  },
  {
    "text": "happens every day so okay let's give it a try so it will take the same amount of",
    "start": "1479169",
    "end": "1485859"
  },
  {
    "text": "time to ingest the data in the standard output but to elasticsearch so here",
    "start": "1485859",
    "end": "1492129"
  },
  {
    "text": "we're just getting the final result and it ended up on 60000 because that's a",
    "start": "1492129",
    "end": "1497679"
  },
  {
    "text": "result of our query we don't get to one second as the lady won it right but we",
    "start": "1497679",
    "end": "1503649"
  },
  {
    "text": "didn't in three or five seconds so with this approach we can extend too many",
    "start": "1503649",
    "end": "1509049"
  },
  {
    "text": "possibilities do you use in my sequel post-acute from the command line one of",
    "start": "1509049",
    "end": "1515200"
  },
  {
    "text": "the things that we are well our thoughts is okay how we can implement some",
    "start": "1515200",
    "end": "1520330"
  },
  {
    "text": "client-server model on this in mind the URL in your cluster your notes you don't",
    "start": "1520330",
    "end": "1525399"
  },
  {
    "text": "know what's happening with your data on certain nodes for some type port or some type container imagine that you can",
    "start": "1525399",
    "end": "1532179"
  },
  {
    "text": "connect remotely to flu and paid directly today to the string processor create your query and get your data",
    "start": "1532179",
    "end": "1538600"
  },
  {
    "text": "remotely you're skipping a database you're skipping everything so the",
    "start": "1538600",
    "end": "1543669"
  },
  {
    "text": "possibilities here are many even we are considering to have some kind of storage not behave like a full database but",
    "start": "1543669",
    "end": "1551679"
  },
  {
    "text": "something that can provide more reliability for chorister need to run for a long time okay",
    "start": "1551679",
    "end": "1560200"
  },
  {
    "text": "so this is just one of the use cases but there are other use cases do you know",
    "start": "1560200",
    "end": "1567249"
  },
  {
    "text": "the concept of data aggregation for simply have a table of the range of",
    "start": "1567249",
    "end": "1573669"
  },
  {
    "text": "relational database and you want to get some an average value of certain fields",
    "start": "1573669",
    "end": "1579669"
  },
  {
    "text": "that has so much in criteria and so on let me show this I'm going to split a",
    "start": "1579669",
    "end": "1584779"
  },
  {
    "text": "configuration here we're going to prove this for example just so you can get a",
    "start": "1584779",
    "end": "1590630"
  },
  {
    "text": "notion of what's going on here okay who",
    "start": "1590630",
    "end": "1597559"
  },
  {
    "text": "can explain this this query average okay exactly we're",
    "start": "1597559",
    "end": "1613909"
  },
  {
    "text": "going to select okay we we are going to spray in a different way we are going to",
    "start": "1613909",
    "end": "1619909"
  },
  {
    "text": "create a window of time that means every 20 seconds calculate I get me the whole",
    "start": "1619909",
    "end": "1628669"
  },
  {
    "text": "records that the country is Chile and group by country but calculate the",
    "start": "1628669",
    "end": "1635600"
  },
  {
    "text": "average number of all of them where this use case a what is useful IOT embedded",
    "start": "1635600",
    "end": "1644750"
  },
  {
    "text": "gateways you have a bunch of sensor do you want to send thousands of vehicles to the cloud to calculate something",
    "start": "1644750",
    "end": "1651559"
  },
  {
    "text": "what's the average temperature in the at home in the last week no maybe you can",
    "start": "1651559",
    "end": "1657260"
  },
  {
    "text": "do the calculation on the edge and just send out the results pretty much as we did with elasticsearch some minutes ago",
    "start": "1657260",
    "end": "1665769"
  },
  {
    "text": "so I'm going to did I set it now oh cool yeah we're going to fix this because a",
    "start": "1665769",
    "end": "1672440"
  },
  {
    "text": "in the configuration flowing bit is very strict everybody's just one line but I",
    "start": "1672440",
    "end": "1677750"
  },
  {
    "text": "think that we this use case is is painful but anyway so our stream here is",
    "start": "1677750",
    "end": "1684649"
  },
  {
    "text": "called results it we used to be cube conducts to with the tag results okay",
    "start": "1684649",
    "end": "1691360"
  },
  {
    "text": "and we are going to change the irrigation file for simplicity I put the",
    "start": "1691360",
    "end": "1699950"
  },
  {
    "text": "stream files so the string queries in separate files but you can have many tasks and it's something I had to have",
    "start": "1699950",
    "end": "1707510"
  },
  {
    "text": "time to show here but what you can do also is create a new query based on the results of the previous task the",
    "start": "1707510",
    "end": "1714110"
  },
  {
    "text": "previous query because every stream that you query you create a new one so you can do some kind",
    "start": "1714110",
    "end": "1720289"
  },
  {
    "text": "of chain in between them so irrigation and we're going to put the results and",
    "start": "1720289",
    "end": "1727390"
  },
  {
    "text": "in the standard output just to get a notion about how it works this is not",
    "start": "1727390",
    "end": "1733640"
  },
  {
    "text": "necessary irrigation okay so we have to",
    "start": "1733640",
    "end": "1745160"
  },
  {
    "text": "wait to in a second right because it has a wind of twenty seconds waiting for data to be processes tic-tac tic-tac",
    "start": "1745160",
    "end": "1756580"
  },
  {
    "text": "I should have put 10 right well there we go so we've got the record output",
    "start": "1760630",
    "end": "1768120"
  },
  {
    "text": "irrigate it and they average number between 20 seconds we've got 1 million",
    "start": "1768120",
    "end": "1773500"
  },
  {
    "text": "records so we process it all of them we look it up for Chile we did some aggregation window and the average",
    "start": "1773500",
    "end": "1781090"
  },
  {
    "text": "number is 50 for all of them so these kind of things are very flexible what",
    "start": "1781090",
    "end": "1787990"
  },
  {
    "text": "enough futures are missing at the moment for example join streams if you have multiple streams",
    "start": "1787990",
    "end": "1793570"
  },
  {
    "text": "maybe you want to do some query with a join between them pretty much as you do with a table in a relational database",
    "start": "1793570",
    "end": "1802049"
  },
  {
    "text": "and what that was all about a stream processing and if you have questions",
    "start": "1803909",
    "end": "1810250"
  },
  {
    "text": "please raise your hand and happy to answer all of again",
    "start": "1810250",
    "end": "1815879"
  },
  {
    "text": "now when announcement we have some t-shirts here so before you leave please",
    "start": "1824470",
    "end": "1829640"
  },
  {
    "text": "speak in them and we have a ladies size too so if you want please feel free to",
    "start": "1829640",
    "end": "1835310"
  },
  {
    "text": "wear you're fluent the t-shirt hi so inside the square you just showed",
    "start": "1835310",
    "end": "1840470"
  },
  {
    "text": "us there's this time window sometimes you wanna backfill for example log files",
    "start": "1840470",
    "end": "1845930"
  },
  {
    "text": "can I access there the timestamp value and say okay group by the timestamp",
    "start": "1845930",
    "end": "1850970"
  },
  {
    "text": "value because now the fluent bit was waiting 20 real-time seconds but I want",
    "start": "1850970",
    "end": "1856040"
  },
  {
    "text": "to have 20 seconds slices maybe off the timestamp is it possible 20 seconds so",
    "start": "1856040",
    "end": "1861230"
  },
  {
    "text": "it's like to worry the timestamp or yeah usually you have your box in a",
    "start": "1861230",
    "end": "1866450"
  },
  {
    "text": "chronological order with the timestamp but maybe you want to backfill it sometime later so I'm going to parse the",
    "start": "1866450",
    "end": "1874580"
  },
  {
    "text": "timestamp from the what file for example and then say okay group it by this 22nd",
    "start": "1874580",
    "end": "1880400"
  },
  {
    "text": "slices that was already recorded maybe even years ago okay so you want to have",
    "start": "1880400",
    "end": "1887120"
  },
  {
    "text": "Hadoop in fluent it and you know what I'm going to be honest",
    "start": "1887120",
    "end": "1894960"
  },
  {
    "text": "and we are taking the best ideas of what is in the market and what are the needs",
    "start": "1894960",
    "end": "1900570"
  },
  {
    "text": "because what you just said is being between a purchase part with Kafka right",
    "start": "1900570",
    "end": "1906950"
  },
  {
    "text": "yeah because that's because right now everything is happening in memory but I",
    "start": "1906950",
    "end": "1912570"
  },
  {
    "text": "think that will not be hard toward some kind of storage like Kafka because it's like a pen look wait time son where we",
    "start": "1912570",
    "end": "1920460"
  },
  {
    "text": "can do that actually you wouldn't need any more than distance which really just still be a stream processing but instead",
    "start": "1920460",
    "end": "1927960"
  },
  {
    "text": "of waiting for this 20 real-time seconds see okay how much or get the notion of",
    "start": "1927960",
    "end": "1936240"
  },
  {
    "text": "time from the stream of timestamps okay okay so get us some insights from",
    "start": "1936240",
    "end": "1943080"
  },
  {
    "text": "the timestamp to perform quite interesting yeah I never thought about that okay we'll check in the count I",
    "start": "1943080",
    "end": "1951540"
  },
  {
    "text": "haven't been working with through a bit for a while because one feature that I had been missing was tech rewrite this",
    "start": "1951540",
    "end": "1957990"
  },
  {
    "text": "looks like it solves it doesn't it sorry so I can I can use the stream",
    "start": "1957990",
    "end": "1963450"
  },
  {
    "text": "processing to rewrite my text from my input sources so we render rejects the",
    "start": "1963450",
    "end": "1969630"
  },
  {
    "text": "text the text yes happened that extreme precision happens can be done because we",
    "start": "1969630",
    "end": "1976470"
  },
  {
    "text": "already do the conversion from a structured data to structure because we parse the data so we have a binary map",
    "start": "1976470",
    "end": "1983760"
  },
  {
    "text": "so we know the fields that we have on every record okay so you can do whatever",
    "start": "1983760",
    "end": "1989760"
  },
  {
    "text": "you want in terms of query on top of this all is possible right now we extend",
    "start": "1989760",
    "end": "1995220"
  },
  {
    "text": "in the language not the language but the support is SQL because right now support",
    "start": "1995220",
    "end": "2000530"
  },
  {
    "text": "very basic stuff comparison aggregation windows count some minimum but it can be",
    "start": "2000530",
    "end": "2006020"
  },
  {
    "text": "expanded it looks super powerful yeah and actually to be honest I would like",
    "start": "2006020",
    "end": "2012440"
  },
  {
    "text": "to because you are the users so if you can go to our github repository create a",
    "start": "2012440",
    "end": "2017900"
  },
  {
    "text": "new issue like SP which there short-tempered stream processor like we",
    "start": "2017900",
    "end": "2023510"
  },
  {
    "text": "need this that would be really helpful because we don't know old a the use case",
    "start": "2023510",
    "end": "2028970"
  },
  {
    "text": "that you have thank you welcome",
    "start": "2028970",
    "end": "2034030"
  },
  {
    "text": "Thanks I'm just curious if you support multiple levels and depth in JSON or it",
    "start": "2055200",
    "end": "2064060"
  },
  {
    "text": "must be a flat like a JSON map a hives",
    "start": "2064060",
    "end": "2069850"
  },
  {
    "text": "okay the question is if you want to query show me all the records that kubernetes labels up or something like",
    "start": "2069850",
    "end": "2079629"
  },
  {
    "text": "that something like a for example like this yeah maybe yeah so so can you",
    "start": "2079630",
    "end": "2091899"
  },
  {
    "text": "address that in your secure not yet we are going to implement that which means",
    "start": "2091900",
    "end": "2097090"
  },
  {
    "text": "to query this sub a well this is like a erased right and we would binary that",
    "start": "2097090",
    "end": "2103090"
  },
  {
    "text": "yeah we're going to spend that for the next version all right thanks the other just note that in Lua script filters you",
    "start": "2103090",
    "end": "2110710"
  },
  {
    "text": "can do stuff like yeah what you actually talking about Lua you know that in SQL",
    "start": "2110710",
    "end": "2116710"
  },
  {
    "text": "you have in this kind of concept in databases stored procedures and you have these user-defined functions so we're",
    "start": "2116710",
    "end": "2123700"
  },
  {
    "text": "going to implement that so you can create your own functions for the stream processor using the Lua scripting and he",
    "start": "2123700",
    "end": "2132610"
  },
  {
    "text": "wants to have a question",
    "start": "2132610",
    "end": "2135930"
  },
  {
    "text": "for now excuse me for the question you can take a note of the address for the stream processor guide which is like a",
    "start": "2139700",
    "end": "2145440"
  },
  {
    "text": "separate document and there's a little hands-on about how to get started this so in this new world of micro services",
    "start": "2145440",
    "end": "2152670"
  },
  {
    "text": "and lots of containers lots of pots and everything we're talking about lots and lots of data would you recommend running",
    "start": "2152670",
    "end": "2159059"
  },
  {
    "text": "fluent built for example as a sidecar container running near the pod that",
    "start": "2159059",
    "end": "2164309"
  },
  {
    "text": "would be one log producer but if I'm interested for example if I'm having",
    "start": "2164309",
    "end": "2171690"
  },
  {
    "text": "stylus services they do the same job they deliver the same data I'm interested of summing the averages of",
    "start": "2171690",
    "end": "2179579"
  },
  {
    "text": "CPU or whatever from all these sources would you recommend putting a fluent bit aggregator before of the sink because we",
    "start": "2179579",
    "end": "2188970"
  },
  {
    "text": "are currently facing exactly this problem we are sinking into a lastik search and there's a huge amount of data",
    "start": "2188970",
    "end": "2194099"
  },
  {
    "text": "and we are going to move into to put a fluent aggregator before of it but",
    "start": "2194099",
    "end": "2200460"
  },
  {
    "text": "that's fluent traditional flu not one bit yes so how to optimize and where to put",
    "start": "2200460",
    "end": "2208430"
  },
  {
    "text": "the data collector for irrigation in general aim I think that sadly the",
    "start": "2208430",
    "end": "2214230"
  },
  {
    "text": "answer is always the same that we have it depends of a use case based on data",
    "start": "2214230",
    "end": "2219630"
  },
  {
    "text": "how much data you generating what kind of data and what your load overall in",
    "start": "2219630",
    "end": "2225420"
  },
  {
    "text": "your cluster because some people said is configuring for example when talking fluent paid to elasticsearch what they",
    "start": "2225420",
    "end": "2231809"
  },
  {
    "text": "are doing is putting an memory buffer limit of 50 kilobytes pay attention 50",
    "start": "2231809",
    "end": "2238380"
  },
  {
    "text": "kilobytes per note but happens that there are clusters that had 200 note 300",
    "start": "2238380",
    "end": "2243450"
  },
  {
    "text": "nodes and sometimes that is too much load for elastic so I would say that",
    "start": "2243450",
    "end": "2250309"
  },
  {
    "text": "just give it a try we as a sidecar as a daemon set and there's not like a magic",
    "start": "2250309",
    "end": "2256020"
  },
  {
    "text": "world for that because as you know data processors or logging tools we are really fast but that can mess up your",
    "start": "2256020",
    "end": "2263130"
  },
  {
    "text": "environment because database or whatever is not so fast or your network doesn't",
    "start": "2263130",
    "end": "2268650"
  },
  {
    "text": "have enough bandwidth so I would say that and just give it a try as a sidecar or",
    "start": "2268650",
    "end": "2274349"
  },
  {
    "text": "just give it a try sad Iman said but there's no like a straight rule for that",
    "start": "2274349",
    "end": "2280280"
  },
  {
    "text": "if you have time for the last question oh we don't have more time so thank you",
    "start": "2282290",
    "end": "2288510"
  },
  {
    "text": "so much and they for coming [Music]",
    "start": "2288510",
    "end": "2294179"
  }
]