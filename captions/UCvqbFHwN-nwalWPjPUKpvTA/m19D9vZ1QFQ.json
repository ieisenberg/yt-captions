[
  {
    "start": "0",
    "end": "97000"
  },
  {
    "text": "you've ever been to a talk where you watched it later at 2x speed this is not gonna be that one because I'm gonna be",
    "start": "30",
    "end": "6120"
  },
  {
    "text": "speaking to XP to get through all these slides so at cruise we are building the",
    "start": "6120",
    "end": "11400"
  },
  {
    "text": "most advanced autonomous vehicles and running back in on kubernetes we have self-driving cars in San Francisco right",
    "start": "11400",
    "end": "18000"
  },
  {
    "text": "now and there's a couple hundred of them navigating the streets autonomously and",
    "start": "18000",
    "end": "23250"
  },
  {
    "text": "pretty soon we'll have some of those available for you in a ride hailing service but here I'm not going to talk",
    "start": "23250",
    "end": "29939"
  },
  {
    "text": "about the cars too much mostly talking about the backend which runs on kubernetes so this is just an example of",
    "start": "29939",
    "end": "35969"
  },
  {
    "text": "how our cars talk to kubernetes in the backend there's a couple different ways either over the internet or over our",
    "start": "35969",
    "end": "42510"
  },
  {
    "text": "private fiber backbone through some physical routers online in our data",
    "start": "42510",
    "end": "48690"
  },
  {
    "text": "centers or in our garages and then over interconnects for high-speed transfer data because it turns out you can't send",
    "start": "48690",
    "end": "55680"
  },
  {
    "text": "enough data over LTE or even 5g when you're recording with so many different",
    "start": "55680",
    "end": "60930"
  },
  {
    "text": "devices on the cars so a little bit about cruises paths we have a",
    "start": "60930",
    "end": "66600"
  },
  {
    "text": "constellation of different components that make up our path so that includes",
    "start": "66600",
    "end": "72510"
  },
  {
    "text": "bits about ingress and egress security observability deployments and management",
    "start": "72510",
    "end": "78360"
  },
  {
    "text": "infrastructure a lot of these are open source tools or SAS products that we integrate with and a bunch of them we",
    "start": "78360",
    "end": "85080"
  },
  {
    "text": "brought ourselves some of them are not yet open source or still still private",
    "start": "85080",
    "end": "90619"
  },
  {
    "text": "glance over a bunch of them in slides in a few minutes rather than talk about",
    "start": "90619",
    "end": "96659"
  },
  {
    "text": "them all right now so we have a little bit of scale there's a little bit of",
    "start": "96659",
    "end": "101909"
  },
  {
    "start": "97000",
    "end": "347000"
  },
  {
    "text": "irony in at scale here but we generally have our largest multi-tenant clusters",
    "start": "101909",
    "end": "107280"
  },
  {
    "text": "or around a thousand nodes this is sort of reaching capacity where real workloads start to struggle on",
    "start": "107280",
    "end": "113369"
  },
  {
    "text": "kubernetes and there are larger synthetic tests that show you can get to like 3,000 or",
    "start": "113369",
    "end": "119100"
  },
  {
    "text": "5,000 nodes but generally what we find is that when you get above a thousand",
    "start": "119100",
    "end": "124320"
  },
  {
    "text": "all a bunch of different things start to break down so this is where we are and",
    "start": "124320",
    "end": "129390"
  },
  {
    "text": "most of our load input and runs on one of these big clusters this blue line these are pod count graph",
    "start": "129390",
    "end": "135680"
  },
  {
    "text": "and it's showing we have a high dynamicism with pods that get scheduled and deleted pretty frequently along with",
    "start": "135680",
    "end": "143090"
  },
  {
    "text": "more consistent workloads we have a very wide variety of workloads including",
    "start": "143090",
    "end": "149150"
  },
  {
    "text": "services and jobs and agents that process jobs and databases and semi",
    "start": "149150",
    "end": "156230"
  },
  {
    "text": "stateful storage a whole bunch of things so we like to use multi-tenancy because",
    "start": "156230",
    "end": "161240"
  },
  {
    "text": "it gets us a little more higher utilization on our nodes among other things so a little bit about multi-tenancy first because it's poorly",
    "start": "161240",
    "end": "168680"
  },
  {
    "text": "understood or communicated in different ways by different people when I talk about multi-tenancy I talk about",
    "start": "168680",
    "end": "174350"
  },
  {
    "text": "multiple applications operating in a shared environment and usually when you have multiple applications you also have",
    "start": "174350",
    "end": "180380"
  },
  {
    "text": "multiple operators so there's a little bit of contention over whether the applications or the tenants or the operators or the tenants but it doesn't",
    "start": "180380",
    "end": "186890"
  },
  {
    "text": "really matter because you're gonna have more of these anyway and chances are everybody who's running kubernetes is",
    "start": "186890",
    "end": "192200"
  },
  {
    "text": "running a lot of different applications on them not just one application so you can call the application the tenant you",
    "start": "192200",
    "end": "197690"
  },
  {
    "text": "can call a system of applications attend it's really up to you how you define this but the real the the crux of",
    "start": "197690",
    "end": "204020"
  },
  {
    "text": "multi-tenancy is the logical isolation in order to get from logical isolation or rather to build logical isolation you",
    "start": "204020",
    "end": "212300"
  },
  {
    "text": "have to put in a whole bunch of components on top of kubernetes kubernetes has some of this isolation built in and some of it you bring",
    "start": "212300",
    "end": "217670"
  },
  {
    "text": "yourself if you go all the way to physical isolation then you're not really in multi-tenancy anymore if you",
    "start": "217670",
    "end": "223550"
  },
  {
    "text": "have full physical isolation that's really just single tenancy or multi instance so the more physical",
    "start": "223550",
    "end": "230240"
  },
  {
    "text": "integration you have the harder it is to preserve logical isolation and the more expensive it is with more sorry the less",
    "start": "230240",
    "end": "238430"
  },
  {
    "text": "expensive it is so if everything is running on the same hardware it's cheaper to run until it breaks and then",
    "start": "238430",
    "end": "245720"
  },
  {
    "text": "if everything is separate then it's more expensive to run and you've sort of",
    "start": "245720",
    "end": "251180"
  },
  {
    "text": "reached the edges of multi-tenancy so why multi-tenancy obviously cloud cloud",
    "start": "251180",
    "end": "256670"
  },
  {
    "text": "cost is one of them I won't go too deep into that one because it's a little obvious lower operational cost is another one if you're starting up a team",
    "start": "256670",
    "end": "262669"
  },
  {
    "text": "a platform team with like five people you might not be able to operate five clusters realistically much less 30",
    "start": "262669",
    "end": "269580"
  },
  {
    "text": "to 100 clusters so having fewer clusters means fewer people needed to manage it but you have to invest in being able to",
    "start": "269580",
    "end": "276990"
  },
  {
    "text": "do multi-tenant securely with good operations and security practices high scale validation is sort of unique to us",
    "start": "276990",
    "end": "283470"
  },
  {
    "text": "is that we wanted to get to a higher scale and see what the limits were before we got customers on our platform",
    "start": "283470",
    "end": "289440"
  },
  {
    "text": "so we do say we're running in production right now because we have cars on the street that talk to our back-end but",
    "start": "289440",
    "end": "294770"
  },
  {
    "text": "it's not necessarily full production because we don't have customers in those cars yet so we still have a little bit",
    "start": "294770",
    "end": "300570"
  },
  {
    "text": "of time to fix up everything before that happens and we wanted to make sure that we tested the edges before we got there",
    "start": "300570",
    "end": "305790"
  },
  {
    "text": "in terms of higher consistency this is just the standard idea that a rising tide lifts all boats we wanted to make",
    "start": "305790",
    "end": "312120"
  },
  {
    "text": "in our investments in a focused area so that it applied to as many of our internal tenants as possible to get them",
    "start": "312120",
    "end": "318990"
  },
  {
    "text": "to production readiness specifically so I talked about multi-tenancy in layers these are not necessarily checkboxes",
    "start": "318990",
    "end": "326430"
  },
  {
    "text": "where you like oh I did this and done I never have to touch this again more like areas of concern in isolation so",
    "start": "326430",
    "end": "333660"
  },
  {
    "text": "starting at the bottom these ones are kind of the things everybody needs in their multi-tenant stack and at the top",
    "start": "333660",
    "end": "338970"
  },
  {
    "text": "you're getting to the point where not very many people have these because they're harder to do they're more expensive to do and I'll go through each",
    "start": "338970",
    "end": "345360"
  },
  {
    "text": "one of these in a minute I would really like to be on a beach right now that's",
    "start": "345360",
    "end": "350430"
  },
  {
    "start": "347000",
    "end": "607000"
  },
  {
    "text": "part of my identity but in kubernetes terms what we're really talking about is authentication so we have user identity",
    "start": "350430",
    "end": "356910"
  },
  {
    "text": "and service identity so these are kind of the bread and butter the table stakes of authentication so I won't go too deep",
    "start": "356910",
    "end": "363690"
  },
  {
    "text": "into them but in our case we're using G suite because we use gke that's baked in and we use octave for single sign-on and",
    "start": "363690",
    "end": "370560"
  },
  {
    "text": "duo for two back door off for service identity kubernetes service accounts you're probably familiar with GCB",
    "start": "370560",
    "end": "376830"
  },
  {
    "text": "service accounts is because we're on that platform and they integrate together with gke specifically then for",
    "start": "376830",
    "end": "384510"
  },
  {
    "text": "logging into service accounts usually use a signed certificate or a JWT with some sort of refresh token and that",
    "start": "384510",
    "end": "390030"
  },
  {
    "text": "refresh token allows you to log in until it expires so you have some sort of time window for how they can log-in these are just basic",
    "start": "390030",
    "end": "396210"
  },
  {
    "text": "security principles my coworker Mike Ruth talked yesterday one of the day",
    "start": "396210",
    "end": "402600"
  },
  {
    "text": "zero conferences about this so you can pull up his talk to go deeper one of the tools the infrastructure team worked on",
    "start": "402600",
    "end": "408360"
  },
  {
    "text": "is Daytona which is the sidecar application this is one of our early open-source projects and it allows you",
    "start": "408360",
    "end": "415410"
  },
  {
    "text": "to bind your secrets to your application containers with a in-memory volume so it",
    "start": "415410",
    "end": "422790"
  },
  {
    "text": "never gets written to disk so you're not Luke leaking your secrets and it gets these from vault using logging in with",
    "start": "422790",
    "end": "429389"
  },
  {
    "text": "the kubernetes service account and then you can do this thing called identity translation that's my word not",
    "start": "429389",
    "end": "435660"
  },
  {
    "text": "necessarily an industry industry term yet but the idea is I can get from kubernetes service account to GCP",
    "start": "435660",
    "end": "440700"
  },
  {
    "text": "service account to AWS service account to whatever kind of service accounts you're using without having to inject",
    "start": "440700",
    "end": "446520"
  },
  {
    "text": "them all at configuration time it allows for a little bit looser configuration",
    "start": "446520",
    "end": "451950"
  },
  {
    "text": "and it pulls us in at a 10-8 time or deploy time you can also pull them in at runtime if you want to and if you have",
    "start": "451950",
    "end": "458970"
  },
  {
    "text": "tight integration with vault you can do that but it requires a little more investment so we do it at deploy time for the most part k rail is another tool",
    "start": "458970",
    "end": "466740"
  },
  {
    "text": "that we built specifically to allow for policy validation so with the existing",
    "start": "466740",
    "end": "474060"
  },
  {
    "text": "tools in the marketplace we found it difficult to validate on existing",
    "start": "474060",
    "end": "479070"
  },
  {
    "text": "clusters figure out who was violating those policies apply exemptions and then",
    "start": "479070",
    "end": "485220"
  },
  {
    "text": "rollout to an enforcement policy and then backfill the people who were in",
    "start": "485220",
    "end": "490260"
  },
  {
    "text": "exemptions to get them to stop doing those crazy things and some of those crazy things are like you know mounting",
    "start": "490260",
    "end": "496530"
  },
  {
    "text": "the dr. socket which is completely insecure or using bind mounts to places",
    "start": "496530",
    "end": "502260"
  },
  {
    "text": "that are on the operating system that you don't want them to have access to or acting as host pin I won't go through",
    "start": "502260",
    "end": "507870"
  },
  {
    "text": "all of these but one of the things we noticed when we did this was that the default docker or second profile gets disabled by kubernetes because of some",
    "start": "507870",
    "end": "514500"
  },
  {
    "text": "reverse compatibility concerns so we go and enable that with with the default",
    "start": "514500",
    "end": "519830"
  },
  {
    "text": "mutating web hook so the web hook applies these things when the pods are created",
    "start": "519830",
    "end": "525779"
  },
  {
    "text": "then they automatically get these security features for free that's also an open-source tool and it's",
    "start": "525779",
    "end": "532560"
  },
  {
    "text": "available on github so a next layer of isolation is domain isolation there's a lot of different ways to slice this so",
    "start": "532560",
    "end": "539189"
  },
  {
    "text": "when I talk about domain isolation I'm talking about environmental organizational and architectural domains",
    "start": "539189",
    "end": "544800"
  },
  {
    "text": "so kubernetes you basically have two options for slicing these orthogonal you have clusters and you have namespaces",
    "start": "544800",
    "end": "551540"
  },
  {
    "text": "the-the-the sig for multi-tenancy is working on hierarchical namespaces but they're not there yet so what you have",
    "start": "551540",
    "end": "557579"
  },
  {
    "text": "now is a choice you got to pick two of these so this this island has fit beyond",
    "start": "557579",
    "end": "563309"
  },
  {
    "text": "one and logs on the other just so you're aware the the way we've sliced this is originally environmental versus",
    "start": "563309",
    "end": "570360"
  },
  {
    "text": "organization also we have clusters for our environments in different BP seas and then we have namespaces for the",
    "start": "570360",
    "end": "576870"
  },
  {
    "text": "teams so we thought that's worked great for awhile until we realized that in hyper-growth your teams change really frequently and then you have to refer it",
    "start": "576870",
    "end": "583559"
  },
  {
    "text": "refactor all your namespaces all the time so what we switch to instead was a project-based namespace which allows for",
    "start": "583559",
    "end": "589529"
  },
  {
    "text": "more granularity for tighter security and still gets you the benefit of an",
    "start": "589529",
    "end": "595019"
  },
  {
    "text": "orthogonal domain we started with one cluster in each environment and we moved",
    "start": "595019",
    "end": "603300"
  },
  {
    "text": "to multi cluster as we've as we've needed but I'll talk about that a little later so permission isolation is the next",
    "start": "603300",
    "end": "609660"
  },
  {
    "start": "607000",
    "end": "861000"
  },
  {
    "text": "layer of multi-tenancy and this really comes down to authorization and authorization management kubernetes",
    "start": "609660",
    "end": "616740"
  },
  {
    "text": "gives you the tools to do our back to do our back but it doesn't really give you",
    "start": "616740",
    "end": "622139"
  },
  {
    "text": "the tools to manage your permissions above that so oftentimes you need some other layer on top of that to make that",
    "start": "622139",
    "end": "629100"
  },
  {
    "text": "work one of the tools that we wrote to help this out was our back sync and our",
    "start": "629100",
    "end": "634529"
  },
  {
    "text": "beck sync allows us to bind groups Google groups specifically to role",
    "start": "634529",
    "end": "639540"
  },
  {
    "text": "bindings sorry use row bindings on groups to give them roles to give them permissions",
    "start": "639540",
    "end": "645269"
  },
  {
    "text": "because the roles are a set of permissions and then this one other feature we threw into this was the",
    "start": "645269",
    "end": "651029"
  },
  {
    "text": "ability to add service accounts to groups so we can do the same thing for service accounts there is a new feature you can sort of see also at the bottom",
    "start": "651029",
    "end": "657360"
  },
  {
    "text": "that gke has immigration recently with Google Groups but they still don't have service",
    "start": "657360",
    "end": "663390"
  },
  {
    "text": "accounts in groups yet so if you want that we still provide it here and this is also an open-source project vault",
    "start": "663390",
    "end": "669840"
  },
  {
    "text": "workspaces so in terms of managing your secrets right now with kubernetes you're effectively managing them at the",
    "start": "669840",
    "end": "675660"
  },
  {
    "text": "namespace level which if you're having a team level might not be as secure as you want if you had a project level it might",
    "start": "675660",
    "end": "680970"
  },
  {
    "text": "be fine it kind of depends we wanted more security than that and more isolation so we isolate our secrets at",
    "start": "680970",
    "end": "687839"
  },
  {
    "text": "the application level or at the workload level so one of the ways we do that is with a standard hierarchical path in",
    "start": "687839",
    "end": "693750"
  },
  {
    "text": "volt and then that allows us to have an application space in vault and then your application has its own service account",
    "start": "693750",
    "end": "700050"
  },
  {
    "text": "and then it can log in and grab those secrets for your application but nobody else is so in terms of managing these we",
    "start": "700050",
    "end": "707730"
  },
  {
    "text": "have to do some get ups of vault permissions and that's not quite as easy as it could be but it effectively allows",
    "start": "707730",
    "end": "713760"
  },
  {
    "text": "us to subdivide these even more than kubernetes default allows and then at the higher level because you're at the",
    "start": "713760",
    "end": "719790"
  },
  {
    "text": "namespace boundaries you can give your teams permissions to read or write those secrets based on your needs you might",
    "start": "719790",
    "end": "726120"
  },
  {
    "text": "just only need like lists to be able to see that they exist but you might also need your own call to be able to update",
    "start": "726120",
    "end": "732030"
  },
  {
    "text": "them in runtime or you might have CI go through and push these out live isopod",
    "start": "732030",
    "end": "738540"
  },
  {
    "text": "is a tool that we wrote specifically for add-on management and one of those add-ons was managing permissions for",
    "start": "738540",
    "end": "745170"
  },
  {
    "text": "awhile so this is similar to helm in the",
    "start": "745170",
    "end": "750540"
  },
  {
    "text": "idea that it's used for deploying things and but instead of templates we write code in skylark which is a Python subset",
    "start": "750540",
    "end": "758550"
  },
  {
    "text": "so skylark allows us to have a nearly Turing complete language in order to",
    "start": "758550",
    "end": "764930"
  },
  {
    "text": "programmatically make templates for our add-ons because we didn't really want to",
    "start": "764930",
    "end": "769950"
  },
  {
    "text": "go through the hassle of releasing every single atom when we're running 20 per cluster and after deploy them took 20 clusters so this allows us just to push",
    "start": "769950",
    "end": "778709"
  },
  {
    "text": "out master to the top of our set and deploy it everywhere all at once which may not be your requirements because you",
    "start": "778709",
    "end": "784080"
  },
  {
    "text": "might have more diversity than that it works for us also Alessi ammo makes people happy sometimes and the helmet",
    "start": "784080",
    "end": "792299"
  },
  {
    "text": "iller alternative was insecure at the time thankfully home 3 is out now and",
    "start": "792299",
    "end": "798629"
  },
  {
    "text": "the terraform kubernetes plugin at the time we started this wasn't really usable i've heard they've invested in a",
    "start": "798629",
    "end": "805499"
  },
  {
    "text": "little more we need to check on it again but this is a fun tool and it's also open source next thing we wrote was Juno",
    "start": "805499",
    "end": "811939"
  },
  {
    "text": "unfortunately for you guys this guy this is not open source but it's an internal project for now and this allows us to",
    "start": "811939",
    "end": "818369"
  },
  {
    "text": "configure a lot of different things for our tenants to increase their isolation and and improve our operational speed so",
    "start": "818369",
    "end": "825059"
  },
  {
    "text": "for example here we see we can enable GCP projects I can add them to a share BBC I can create a vault workspace and I",
    "start": "825059",
    "end": "832379"
  },
  {
    "text": "can create a Koopa Nettie's namespace so we hope to allow this to self-service to",
    "start": "832379",
    "end": "838739"
  },
  {
    "text": "create namespaces on demand for our users and ideally you can either you",
    "start": "838739",
    "end": "844649"
  },
  {
    "text": "know authorize people to do that by using groups or you can do a sort of request approval process where the",
    "start": "844649",
    "end": "850980"
  },
  {
    "text": "platform team approves these resources granted because each of these resources comes with an effective quota and you",
    "start": "850980",
    "end": "857220"
  },
  {
    "text": "need to do some capacity planning around it which is still it yet to be isolated so resource isolation is sort of the",
    "start": "857220",
    "end": "864389"
  },
  {
    "start": "861000",
    "end": "1012000"
  },
  {
    "text": "next tier here and there's some built-in types of the CPU GPU memory everybody",
    "start": "864389",
    "end": "871709"
  },
  {
    "text": "kind of knows those when they use good burn it is the ones that are sort of less isolated is persistent storage and",
    "start": "871709",
    "end": "877230"
  },
  {
    "text": "ephemeral storage let me step back on the persistent storage for a second you can isolate that really well because",
    "start": "877230",
    "end": "882899"
  },
  {
    "text": "it's one disk to one application unless you're using local persistent storage",
    "start": "882899",
    "end": "888299"
  },
  {
    "text": "and then it gets a little more complicated yet to mount you have to pre mount your applications or have some way to runtime mountain mount sorry pre",
    "start": "888299",
    "end": "894239"
  },
  {
    "text": "mount your drives or have some some runtime drive mounting it's pretty complicated to manage and the storage",
    "start": "894239",
    "end": "899489"
  },
  {
    "text": "classes are sorry the the quota is all storage class specific so if you have like two storage classes because they're",
    "start": "899489",
    "end": "905819"
  },
  {
    "text": "in two different zones but your quota is regional at the project level then you have to like add those together to get your quota makes capacity planning a",
    "start": "905819",
    "end": "912779"
  },
  {
    "text": "little harder the ephemeral storage is the real bugger here though because there's quota and usage but you only get",
    "start": "912779",
    "end": "917819"
  },
  {
    "text": "usage if you have a quota applied and there's no pod level usage yet so you can't really enforce the quota",
    "start": "917819",
    "end": "924139"
  },
  {
    "text": "because people come to you and they're like how much am I using what should I set my request and limits do and you're like I have no idea so we're working",
    "start": "924139",
    "end": "931009"
  },
  {
    "text": "with Google and data dog to help improve the ephemeral storage mapping up here but another thing you have to concern",
    "start": "931009",
    "end": "936559"
  },
  {
    "text": "yourself with the ephemeral storage here is that all these storage volumes here in the middle all share your root Drive",
    "start": "936559",
    "end": "941929"
  },
  {
    "text": "unless you've subdivided them some more so you might want to consider isolating those so that your doctor images don't take a ball your space on your storage",
    "start": "941929",
    "end": "948499"
  },
  {
    "text": "volumes or your storage volumes don't you know block you from be able to launch more pods or your OS crashes",
    "start": "948499",
    "end": "954230"
  },
  {
    "text": "because one of your pods is using too much disk we had a fun bug where we crashed docker because we were using too",
    "start": "954230",
    "end": "960379"
  },
  {
    "text": "much disk and then doctor slowed down and the health check for docker waited 30 seconds and then killed it and then",
    "start": "960379",
    "end": "965689"
  },
  {
    "text": "waited for it to come back up but if you're using too much disk it would never come back up so you ended up with",
    "start": "965689",
    "end": "970939"
  },
  {
    "text": "a cascade failure where you get evicted from the node and crash the entire cluster so don't do that we're still",
    "start": "970939",
    "end": "978079"
  },
  {
    "text": "waiting for quality service here another step here for quotas and limits you can",
    "start": "978079",
    "end": "983420"
  },
  {
    "text": "put quotas and limits with the the default resources but if you want to manage defaults those on the namespace there's no current tool for that so we",
    "start": "983420",
    "end": "990649"
  },
  {
    "text": "do this with Juno or isopod and allows us to set a default quota for each",
    "start": "990649",
    "end": "995749"
  },
  {
    "text": "namespace when we create the namespace and then we set a really low limit for resource limits on CPU and memory to",
    "start": "995749",
    "end": "1003279"
  },
  {
    "text": "force people to specify that creates a feedback mechanism that requires them to",
    "start": "1003279",
    "end": "1008439"
  },
  {
    "text": "know about resources or have some tool that does that for them next level is",
    "start": "1008439",
    "end": "1013569"
  },
  {
    "start": "1012000",
    "end": "1139000"
  },
  {
    "text": "network isolation I wish our data centers look like this they're really cool got a nice air gap there we're gonna",
    "start": "1013569",
    "end": "1019089"
  },
  {
    "text": "have that instead we have shared tunnels so share tunnels here one two call out",
    "start": "1019089",
    "end": "1025058"
  },
  {
    "text": "is nat gateways not gateways like to fall over especially be using them with",
    "start": "1025059",
    "end": "1031089"
  },
  {
    "text": "static IPS static IPS then get wait-listed by people who are doing things that maybe they shouldn't do then",
    "start": "1031089",
    "end": "1037360"
  },
  {
    "text": "you have wait lists strewn across everybody and you can't auto scale your or now that's because then you'd have to",
    "start": "1037360",
    "end": "1043870"
  },
  {
    "text": "go update all the white lists or things break so don't do that either white lists are bad but the we switched from a",
    "start": "1043870",
    "end": "1053020"
  },
  {
    "text": "bill sorry a self deployed terraform module managed in that gateway that didn't",
    "start": "1053020",
    "end": "1058120"
  },
  {
    "text": "scare scale very well to cloud nap when that was offered by Google and so that's worked a little bit better for us but it",
    "start": "1058120",
    "end": "1064000"
  },
  {
    "text": "still has the same whitelist problem ingress/egress QPS is completely uh nice elated so unless you are running",
    "start": "1064000",
    "end": "1071620"
  },
  {
    "text": "something like Sto you basically don't get QPS or request isolation between",
    "start": "1071620",
    "end": "1077529"
  },
  {
    "text": "containers or pods on the same node so what we did was we went from Broadwell to sky lake and got twice as much",
    "start": "1077529",
    "end": "1083649"
  },
  {
    "text": "network speed to avoid the problem but hopefully we're working towards increasing QPS isolation or just",
    "start": "1083649",
    "end": "1090399"
  },
  {
    "text": "ingress/egress worth noting if you're using Network shared network storage",
    "start": "1090399",
    "end": "1096370"
  },
  {
    "text": "that is shared on the network like it's in the name so your network and your storage are now on isolated from each",
    "start": "1096370",
    "end": "1102100"
  },
  {
    "text": "other much less the pods they're on so there's a little bit of complexity there that needs to be separated and you might want to use like actual physically",
    "start": "1102100",
    "end": "1108309"
  },
  {
    "text": "attached disks instead of network attacks disk if you need more isolation bandwidth here there's a CNI bye and",
    "start": "1108309",
    "end": "1113649"
  },
  {
    "text": "bandwidth plugin but it doesn't work on gke because the version of calico isn't",
    "start": "1113649",
    "end": "1119110"
  },
  {
    "text": "new enough so we've asked them to increase that and we'll try that out pretty soon in the meantime i working on sto where we can get rate limits with",
    "start": "1119110",
    "end": "1125919"
  },
  {
    "text": "the layer 7 load balancer worth calling out that layer 7 load balancing is slower than layer layer 4 almost",
    "start": "1125919",
    "end": "1131740"
  },
  {
    "text": "universally so if you inject this in you've got to make sure that you qualify it and make sure it's not going to break",
    "start": "1131740",
    "end": "1137470"
  },
  {
    "text": "your applications so speaking of this do network policies built into kubernetes",
    "start": "1137470",
    "end": "1144159"
  },
  {
    "start": "1139000",
    "end": "1328000"
  },
  {
    "text": "are great except for when they're not which is when you're managing them because managing IP blocks is",
    "start": "1144159",
    "end": "1149950"
  },
  {
    "text": "effectively managing whitelists it's no fun namespace selecting and pod selectors tends to be a blunt instrument",
    "start": "1149950",
    "end": "1155950"
  },
  {
    "text": "and it's not really great unless you have very fine-grain namespaces where your namespaces are your application and",
    "start": "1155950",
    "end": "1161980"
  },
  {
    "text": "then you want to you know control them between we like the sto model a little bit better although it's kind of",
    "start": "1161980",
    "end": "1167139"
  },
  {
    "text": "complicated but you can effectively do denier list checker that's basically black lists and white lists you can also",
    "start": "1167139",
    "end": "1173500"
  },
  {
    "text": "do service authentication which allows you to say you know this service account has access to access this service run by",
    "start": "1173500",
    "end": "1181450"
  },
  {
    "text": "there's other service account and then make policies that enforce what can talk to what",
    "start": "1181450",
    "end": "1187440"
  },
  {
    "text": "integration isolation this one is the biggest unsolved problems I think when",
    "start": "1187500",
    "end": "1192670"
  },
  {
    "text": "you have kubernetes kubernetes is not enough and you need a whole bunch of other things like I said a constellation",
    "start": "1192670",
    "end": "1197920"
  },
  {
    "text": "of components and applications and operators all sorts of stuff we mix in a bunch of SAS products - you might not",
    "start": "1197920",
    "end": "1203620"
  },
  {
    "text": "but each one of those probably isn't as isolated as kubernetes is so one of",
    "start": "1203620",
    "end": "1210130"
  },
  {
    "text": "these examples is load balancing we've been using the nginx controller for a long time",
    "start": "1210130",
    "end": "1215680"
  },
  {
    "text": "we found that we didn't like it running on the same nodes as the application so we moved into its own node pool and then",
    "start": "1215680",
    "end": "1221230"
  },
  {
    "text": "we found that we didn't like isolating our own sorry make managing a new node",
    "start": "1221230",
    "end": "1226240"
  },
  {
    "text": "pool and having it you know have different rules and everybody else and also then have two layers of auto",
    "start": "1226240",
    "end": "1231940"
  },
  {
    "text": "scaling so we moved it out to GCE now we have the load balancers running on VMs and then we have one layer of",
    "start": "1231940",
    "end": "1238330"
  },
  {
    "text": "auto scaling and it's much more performant and just as easy to manage there's some so so you can give this is",
    "start": "1238330",
    "end": "1246010"
  },
  {
    "text": "kind of a lesson in in isolation you can you can give it your on node pool you can give it its own nodes you can give",
    "start": "1246010",
    "end": "1251290"
  },
  {
    "text": "it its own VMs you can you can make them per tenant unfortunately there's no current mechanism for spinning up or requesting",
    "start": "1251290",
    "end": "1258130"
  },
  {
    "text": "a new actual physical load balancer for every namespace so I know there's an investment in the API machinery sig to",
    "start": "1258130",
    "end": "1266380"
  },
  {
    "text": "work on gateways and gateway classes so I'm hoping that pans out pretty soon so",
    "start": "1266380",
    "end": "1272650"
  },
  {
    "text": "that we can say I need to new one of these for this namespace or this tenant another problem here that we don't have",
    "start": "1272650",
    "end": "1278980"
  },
  {
    "text": "very well isolated is DNS this is just just kind of a lull slide in how DNS is",
    "start": "1278980",
    "end": "1285310"
  },
  {
    "text": "is set up because you have a cache in front of a cache in front of a cache in front of a cache in front of DNS and so",
    "start": "1285310",
    "end": "1293940"
  },
  {
    "text": "this is just private zones but basically your DNS on the cluster is really easy to overwhelm with any of your tenants",
    "start": "1293940",
    "end": "1301030"
  },
  {
    "text": "and that will affect every other tenant and unfortunately that's just the way it is you can slice these by by node pool",
    "start": "1301030",
    "end": "1307740"
  },
  {
    "text": "sometimes if you if you have some way to set up the DNS differently like node labels on GCP for example but there's",
    "start": "1307740",
    "end": "1315370"
  },
  {
    "text": "not a good way to isolate this / tenant because there's no way to say this name bass only can use this note pool we've",
    "start": "1315370",
    "end": "1322570"
  },
  {
    "text": "been thinking about using a validating webhook to enforce that but there's no way in kubernetes to lock those two together right now this one I'm not",
    "start": "1322570",
    "end": "1330910"
  },
  {
    "start": "1328000",
    "end": "1477000"
  },
  {
    "text": "going to have too deep into but shared observability is a big problem all the logs all the metrics all the tracing",
    "start": "1330910",
    "end": "1336850"
  },
  {
    "text": "tends to go to a SAS provider or if you're running your own it all goes in the same place so there's there's",
    "start": "1336850",
    "end": "1342790"
  },
  {
    "text": "usually the SAS providers will have multi-tenancy because you are a tenant but they don't have two layers of",
    "start": "1342790",
    "end": "1348760"
  },
  {
    "text": "multi-tenancy so you can't give tenancy to your users underneath you or that",
    "start": "1348760",
    "end": "1354790"
  },
  {
    "text": "you're hosting for so if you're running your own also like Prometheus and Ravana",
    "start": "1354790",
    "end": "1361000"
  },
  {
    "text": "have the same problems they're not really designed for it they might have authentication low-level multi-tenancy but they don't have authorization rules",
    "start": "1361000",
    "end": "1367900"
  },
  {
    "text": "for most of their stuff this is just a fun hack we made to get logs into a different project in GCP so we we",
    "start": "1367900",
    "end": "1375280"
  },
  {
    "text": "actually double admit container logs so that they can get into a team project and that team has access to do log based",
    "start": "1375280",
    "end": "1381400"
  },
  {
    "text": "metrics that way they can self-service their metrics and those metrics then go to data dog and then they can do the alert flow the way they normally would",
    "start": "1381400",
    "end": "1388090"
  },
  {
    "text": "rather than having to come through us and do some like get ops management of the log based metrics also more",
    "start": "1388090",
    "end": "1395020"
  },
  {
    "text": "critically they can do dashboards in their project whereas we don't want to give them edit to our project because",
    "start": "1395020",
    "end": "1401770"
  },
  {
    "text": "then they'd stomp on each other's toes that's like the opposite of isolation so as was mentioned you can go from no",
    "start": "1401770",
    "end": "1408910"
  },
  {
    "text": "isolation sharing sharing a cluster admin account which I don't recommend do logical isolation which has been called",
    "start": "1408910",
    "end": "1416170"
  },
  {
    "text": "sort of soft multi-tenancy before hard multi-tenancy is is kind of the idea that you don't trust the people that are",
    "start": "1416170",
    "end": "1422290"
  },
  {
    "text": "underneath you I don't recommend that for a kubernetes install because of all these sort of isolation constraints or",
    "start": "1422290",
    "end": "1429760"
  },
  {
    "text": "problems or yet to be implemented features and then system isolation is really single tenant to you or multi",
    "start": "1429760",
    "end": "1436360"
  },
  {
    "text": "instance strategy so you choose what layer you want to do and what works for you may not work for everybody so",
    "start": "1436360",
    "end": "1442810"
  },
  {
    "text": "there's a lot of complexity in that and it's hard to provide it like a single done and sold solution you can isolate",
    "start": "1442810",
    "end": "1450010"
  },
  {
    "text": "at the machine level you can isolate it the node pool level you can isolate at the class you can isolate at the network level any",
    "start": "1450010",
    "end": "1455200"
  },
  {
    "text": "one of these would be real system isolation the components it's good to",
    "start": "1455200",
    "end": "1461290"
  },
  {
    "text": "call out that each one of these components I listed you can get to scale issues pretty quickly so if you are",
    "start": "1461290",
    "end": "1467380"
  },
  {
    "text": "running in a multi-tenant environment you get more scaled just by default and then you might hit some of these boundaries and then you had to back it",
    "start": "1467380",
    "end": "1473230"
  },
  {
    "text": "off and give them separate clusters in order to isolate those components so was",
    "start": "1473230",
    "end": "1478870"
  },
  {
    "start": "1477000",
    "end": "1710000"
  },
  {
    "text": "it worth it sure the the goal of this",
    "start": "1478870",
    "end": "1485710"
  },
  {
    "text": "whole strategy was to use save us money lower operational cost get scale",
    "start": "1485710",
    "end": "1491950"
  },
  {
    "text": "validation and you know Raisa tired of all boats so I think we did that the",
    "start": "1491950",
    "end": "1497320"
  },
  {
    "text": "biggest cost was shared downtime so anyone users bugs or anyone users system",
    "start": "1497320",
    "end": "1503640"
  },
  {
    "text": "like running into somebody else's bugs or you know running into an isolation",
    "start": "1503640",
    "end": "1508660"
  },
  {
    "text": "boundary that doesn't exist and they can they can take out everybody else so that",
    "start": "1508660",
    "end": "1513820"
  },
  {
    "text": "becomes a security issue if you get compromised but it's also an availability issue just with your",
    "start": "1513820",
    "end": "1519460"
  },
  {
    "text": "regular tenants so generally it's good but you have to make make sure that your",
    "start": "1519460",
    "end": "1524950"
  },
  {
    "text": "use cases are good for it so what we've done is we started big one big cluster I mean we have Deb stage prod but our",
    "start": "1524950",
    "end": "1530770"
  },
  {
    "text": "problem is the biggest and we put everybody on that until it hurts and then we move them out so we have a",
    "start": "1530770",
    "end": "1536890"
  },
  {
    "text": "couple other tenants with their own clusters but I wouldn't call them single tenant clusters because it's really like a team they get their cluster and then",
    "start": "1536890",
    "end": "1542920"
  },
  {
    "text": "that team has different projects they get different namespaces so we really have a bunch of multi dining clusters they're just you know",
    "start": "1542920",
    "end": "1548650"
  },
  {
    "text": "isolated on different boundaries another thing we learned here is that we got a",
    "start": "1548650",
    "end": "1555010"
  },
  {
    "text": "bunch of security improvements prioritized because they were also operational and isolation 'el concerns",
    "start": "1555010",
    "end": "1560590"
  },
  {
    "text": "and I think that's a win for everybody is investing in these security tools which might might have been overlooked",
    "start": "1560590",
    "end": "1567820"
  },
  {
    "text": "otherwise and then expertise building we we have broken kubernetes many times and",
    "start": "1567820",
    "end": "1573790"
  },
  {
    "text": "so that allows us to know where the edges are and what not to do in the future or what to work around what to prioritize in terms of fixes and I think",
    "start": "1573790",
    "end": "1579910"
  },
  {
    "text": "that was valuable especially before we have customers that we would embarrass ourselves in front of",
    "start": "1579910",
    "end": "1585539"
  },
  {
    "text": "there the cost here is incompatible tooling turns out that not just integrations but tooling tend to be incompatible one of them I mentioned",
    "start": "1585539",
    "end": "1591509"
  },
  {
    "text": "earlier was like helm tiller it didn't have any officee in it so it was really like you install a helm and it can",
    "start": "1591509",
    "end": "1597059"
  },
  {
    "text": "install the whole cluster bypassing your tenancy rules or you install it in the namespace and you run multiple incidents",
    "start": "1597059",
    "end": "1602190"
  },
  {
    "text": "it's not very great helm 3 now has the option to basically you see our DS and",
    "start": "1602190",
    "end": "1607979"
  },
  {
    "text": "our back but you had to carefully configure that so we haven't done that yet but we're looking into it another",
    "start": "1607979",
    "end": "1613169"
  },
  {
    "text": "one I'd call it his cube flow it has its own sort of tenancy model so I didn't really match with ours so we can't",
    "start": "1613169",
    "end": "1618869"
  },
  {
    "text": "really install it with everybody else's namespaces for example because it creates and deletes namespaces so it",
    "start": "1618869",
    "end": "1625919"
  },
  {
    "text": "basically has to have an equivalent to cluster admin or more permissions than a tenant would have so you can't put it in",
    "start": "1625919",
    "end": "1631049"
  },
  {
    "text": "a tenancy box so we give that as its own clusters to see are the installations",
    "start": "1631049",
    "end": "1638729"
  },
  {
    "text": "another one the platform team has to manage the IDs because they're outside of the namespace and they're shared by the whole cluster it's a global resource",
    "start": "1638729",
    "end": "1644429"
  },
  {
    "text": "and this is a little bit unfortunate because we like people to self-service their own operators but at the same time",
    "start": "1644429",
    "end": "1649859"
  },
  {
    "text": "it gives us a option to review the operators they're installing to make",
    "start": "1649859",
    "end": "1655080"
  },
  {
    "text": "sure they're not going to do something that's going to break down all the other tenants and so one of the things that",
    "start": "1655080",
    "end": "1660179"
  },
  {
    "text": "this also gives us in general is not just to see IDs but the chance to review tenants that we pull into the system and",
    "start": "1660179",
    "end": "1666210"
  },
  {
    "text": "do some capacity planning with them and make sure that we have quota for them make sure that they are not doing anything crazy like a lift and shift",
    "start": "1666210",
    "end": "1672899"
  },
  {
    "text": "without understanding what cloud native means but that's that's a time intensive process and so it also slows down the",
    "start": "1672899",
    "end": "1681179"
  },
  {
    "text": "the self-service aspect so that it's kind of a challenge pro and con and kubernetes itself it's hard not just for",
    "start": "1681179",
    "end": "1688379"
  },
  {
    "text": "the platform team but for everyone using it and so they all have to learn it they all have to learn the pads tools that we integrate with and we're constantly",
    "start": "1688379",
    "end": "1694679"
  },
  {
    "text": "building new abstractions to that they either have to learn in parallel or that you know hide some complexity from them",
    "start": "1694679",
    "end": "1700649"
  },
  {
    "text": "but there's no one system that the overlays that makes this simple that is also multi-tenant so we can just pull",
    "start": "1700649",
    "end": "1706019"
  },
  {
    "text": "one off the shelf alrighty that was it got through that pretty",
    "start": "1706019",
    "end": "1713309"
  },
  {
    "start": "1710000",
    "end": "1994000"
  },
  {
    "text": "quick [Applause]",
    "start": "1713309",
    "end": "1718740"
  },
  {
    "text": "so we have a few more minutes for questions any questions I was first hi",
    "start": "1721310",
    "end": "1733500"
  },
  {
    "text": "Carl's thanks for sharing this with us and a very informative so I'm Yan Cheng from JD dot-com I have",
    "start": "1733500",
    "end": "1738960"
  },
  {
    "text": "two questions the first one is can you talk a little bit more about your use cases workloads",
    "start": "1738960",
    "end": "1744330"
  },
  {
    "text": "the second one is in terms of resource isolation and do you see the the disk",
    "start": "1744330",
    "end": "1750870"
  },
  {
    "text": "i/o and like the Panda way offender is a key change there and that's a problem you know a multi-tenant sender that is",
    "start": "1750870",
    "end": "1758130"
  },
  {
    "text": "platform Thanks so I guess two questions one is on whether the bandwidth and QPS isolation",
    "start": "1758130",
    "end": "1764370"
  },
  {
    "text": "is required or a problem and the other one was based on our workload styles or",
    "start": "1764370",
    "end": "1769800"
  },
  {
    "text": "types so we have a lot of different workloads we have standard web applications micro services we also have",
    "start": "1769800",
    "end": "1775890"
  },
  {
    "text": "jobs that we get kicked off by an external scheduler we have cron jobs that happen nightly for batch job stuff",
    "start": "1775890",
    "end": "1782280"
  },
  {
    "text": "we have sort of semi stateless stuff like elastic search or a cache a layer",
    "start": "1782280",
    "end": "1790040"
  },
  {
    "text": "we also read its databases for example we also have full stateful databases",
    "start": "1790040",
    "end": "1795390"
  },
  {
    "text": "only a couple of them and then we have a bunch of agents that run jobs or launch",
    "start": "1795390",
    "end": "1801780"
  },
  {
    "text": "pods and effectively ask act as like custom schedulers except they're",
    "start": "1801780",
    "end": "1807450"
  },
  {
    "text": "internal so those are usually fed by a pub sub q and then want to scale based",
    "start": "1807450",
    "end": "1813630"
  },
  {
    "text": "on the depth of the queue so we have all those systems together and a couple more stragglers that I didn't mention but",
    "start": "1813630",
    "end": "1819390"
  },
  {
    "text": "it's it's enough that we can get pretty good isolation but we also have some",
    "start": "1819390",
    "end": "1824760"
  },
  {
    "text": "jobs that like want to use sixty CPU we had to give them their own namespace in order to isolate them a little more otherwise they'll block scheduling or",
    "start": "1824760",
    "end": "1831540"
  },
  {
    "text": "won't be able to schedule themselves and then we have in terms of the bandwidth",
    "start": "1831540",
    "end": "1836910"
  },
  {
    "text": "and QPS we sort of solved the immediate problem that we had by giving more",
    "start": "1836910",
    "end": "1841980"
  },
  {
    "text": "bandwidth but it's not isolation so it's it's a security problem because you can das a system by taking down a node",
    "start": "1841980",
    "end": "1848870"
  },
  {
    "text": "you can you can take out the Nats gateways and that's a security problem but also it's an availability problem so",
    "start": "1848870",
    "end": "1854990"
  },
  {
    "text": "I think the the disk isolation and network isolation that basically don't exist or have very rudimentary solutions",
    "start": "1854990",
    "end": "1862090"
  },
  {
    "text": "are some of the biggest win areas to invest in kubernetes isolation I how",
    "start": "1862090",
    "end": "1871640"
  },
  {
    "text": "come the multi-tenancy working group help you in terms of native features for kubernetes especially for security so",
    "start": "1871640",
    "end": "1881990"
  },
  {
    "text": "there's a little bit of complexity with us running on gke which means that we're not running an upstream kubernetes",
    "start": "1881990",
    "end": "1887360"
  },
  {
    "text": "version and gk is a little a little behind on versions and we're behind on",
    "start": "1887360",
    "end": "1892580"
  },
  {
    "text": "their versions and so if there's a new patch into kubernetes it'll take like a year to get to us so that's that's",
    "start": "1892580",
    "end": "1899240"
  },
  {
    "text": "something that needs to be improved both by upstream and Google but if it's an",
    "start": "1899240",
    "end": "1904280"
  },
  {
    "text": "external controller that we can apply that's a much easier way to do it and I think that's that extensibility with",
    "start": "1904280",
    "end": "1910010"
  },
  {
    "text": "operators is the way to go I have been interested in the hierarchical namespace",
    "start": "1910010",
    "end": "1915790"
  },
  {
    "text": "operator but it over overlaps with a bunch of the stuff we're doing so I'd",
    "start": "1915790",
    "end": "1921770"
  },
  {
    "text": "also kind of like smaller tools that I can slap on to add to an existing set of tools rather than something that tries",
    "start": "1921770",
    "end": "1929330"
  },
  {
    "text": "to do everything hi how you are managing",
    "start": "1929330",
    "end": "1934790"
  },
  {
    "text": "the project level CID pipelines in stuff like is there any segregation is there",
    "start": "1934790",
    "end": "1941630"
  },
  {
    "text": "as a slide I had to leave after time we have a couple different CI systems and",
    "start": "1941630",
    "end": "1947660"
  },
  {
    "text": "our primary CD system is spinnaker right now we wrote a spinnaker operator internally to manage accounts to have",
    "start": "1947660",
    "end": "1955780"
  },
  {
    "text": "accounts per namespace per cluster which means we have hundreds of accounts and",
    "start": "1955780",
    "end": "1961160"
  },
  {
    "text": "then we have to have a tool that manages that for the for us so we have like a spinnaker account CRD that you make",
    "start": "1961160",
    "end": "1967850"
  },
  {
    "text": "resources for and we have to pre provision those with some-some get ops and then the spinnaker account sets up",
    "start": "1967850",
    "end": "1973760"
  },
  {
    "text": "through the operator to inject and restart spinnaker running on how your intern inside the cluster",
    "start": "1973760",
    "end": "1980430"
  },
  {
    "text": "and then that alone is also another use case I didn't mention that spans a lot of resources running continuous",
    "start": "1980430",
    "end": "1986070"
  },
  {
    "text": "deployment okay we are at a time so if you have more questions feel free to reach Karl thank you",
    "start": "1986070",
    "end": "1994380"
  },
  {
    "text": "[Applause]",
    "start": "1994380",
    "end": "1996689"
  }
]