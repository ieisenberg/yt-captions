[
  {
    "start": "0",
    "end": "92000"
  },
  {
    "text": "thanks for coming guys hope you haven't heard hope you haven't heard too much about scale over the last",
    "start": "0",
    "end": "5940"
  },
  {
    "text": "two or three days but it's a pretty popular topic find it extremely",
    "start": "5940",
    "end": "10950"
  },
  {
    "text": "interesting myself my name is Jeremy Durr I work for Red Hat currently and I",
    "start": "10950",
    "end": "16550"
  },
  {
    "text": "co lead a team of performance and scalability engineers we're currently working on OpenShift the larger team at",
    "start": "16550",
    "end": "24000"
  },
  {
    "text": "Red Hat works on covers performance and scale for every Red Hat product so it's",
    "start": "24000",
    "end": "29220"
  },
  {
    "text": "a fairly large team today we're going to talk about some work we did in late summer on the",
    "start": "29220",
    "end": "37170"
  },
  {
    "text": "CNC F lab to scale test Red Hat's Enterprise ready",
    "start": "37170",
    "end": "43530"
  },
  {
    "text": "distribution of kubernetes which is called open shift and I did a talk at Linux con about workload",
    "start": "43530",
    "end": "51980"
  },
  {
    "text": "classification as well so I'm going to try and bring in some of the performance aspects which is my traditional",
    "start": "51980",
    "end": "58590"
  },
  {
    "text": "background is making workloads go fast I'll share some of what we're working on",
    "start": "58590",
    "end": "65309"
  },
  {
    "text": "today and when I say we I mean I co-lead the team with Tim Sinclair who's somewhere hopefully in the room and",
    "start": "65309",
    "end": "71659"
  },
  {
    "text": "he's the best and then yeah then we'll go through some details about the lab",
    "start": "71659",
    "end": "77210"
  },
  {
    "text": "work that we did and if all goes well show you a quick demo",
    "start": "77210",
    "end": "82470"
  },
  {
    "text": "of some work we're doing in the context of cig scale to more",
    "start": "82470",
    "end": "88380"
  },
  {
    "text": "formally define what scale actually means so this is our group in kind of a simple",
    "start": "88380",
    "end": "94470"
  },
  {
    "text": "workflow we take inputs from the field we might get requirements from customers or potential customers about what they",
    "start": "94470",
    "end": "100860"
  },
  {
    "text": "need the product to do and we'll try and make sure that that occurs you know that work correctly and it's properly",
    "start": "100860",
    "end": "105930"
  },
  {
    "text": "supported we do upstream development we do test execution and design of test plans and",
    "start": "105930",
    "end": "113479"
  },
  {
    "text": "we work very closely with the quality engineering team at Red Hat to ensure",
    "start": "113479",
    "end": "118829"
  },
  {
    "text": "that they run these things formally in regression test plans so we feel like we've got decent coverage and we can",
    "start": "118829",
    "end": "124770"
  },
  {
    "text": "always do better and what I'm one of the things I'd like to hear from you guys today is what this scale mean to you and",
    "start": "124770",
    "end": "130349"
  },
  {
    "text": "what this performance means you so if any of these slides are you know if you have any comments",
    "start": "130349",
    "end": "135780"
  },
  {
    "text": "about these slides please just just shout oh and there's a link I'll put these slides on SlideShare afterwards or on Twitter",
    "start": "135780",
    "end": "142440"
  },
  {
    "text": "but there's a hyperlink here to our Trello board which is public all of RedHat OpenShift Engineering's Trello",
    "start": "142440",
    "end": "148320"
  },
  {
    "text": "boards so our agile workflow backlogs are on Trello and publicly available so you can you can see exactly",
    "start": "148320",
    "end": "154590"
  },
  {
    "text": "what we're doing listen to your apps so here it'll do a",
    "start": "154590",
    "end": "160950"
  },
  {
    "text": "couple slides on performance people's ask a Red Hat how do i optimize",
    "start": "160950",
    "end": "166620"
  },
  {
    "text": "my environment premature optimization is usually not a good idea what we do try",
    "start": "166620",
    "end": "171690"
  },
  {
    "text": "and make the best guesses that are generic gerrant generally applicable to most workloads",
    "start": "171690",
    "end": "177320"
  },
  {
    "text": "that's actually part of what we want to do for what the scale mean in kubernetes is widening the workloads that run in",
    "start": "177320",
    "end": "185310"
  },
  {
    "text": "that test plan to include things that customers might use and so",
    "start": "185310",
    "end": "191600"
  },
  {
    "text": "who here today is using kubernetes who's using more than a hundred node",
    "start": "191600",
    "end": "196620"
  },
  {
    "text": "cluster of kubernetes do you work for Google",
    "start": "196620",
    "end": "202250"
  },
  {
    "text": "okay so anyone running hundred nodes or more",
    "start": "202850",
    "end": "208350"
  },
  {
    "text": "that doesn't isn't doing like research and development okay so we're the skill",
    "start": "208350",
    "end": "214620"
  },
  {
    "text": "the skill guys want to do significant scale and rightfully so we want these",
    "start": "214620",
    "end": "219750"
  },
  {
    "text": "huge environments we have these ideas grandiose ideas of what is really cool",
    "start": "219750",
    "end": "225630"
  },
  {
    "text": "and sexy to work on but I want to temper that with some reality here so and the",
    "start": "225630",
    "end": "231120"
  },
  {
    "text": "applications that are currently running in and that kubernetes is designed to run our web applications and",
    "start": "231120",
    "end": "236630"
  },
  {
    "text": "incidentally if you're interested in running non web applications high performance applications things that",
    "start": "236630",
    "end": "242489"
  },
  {
    "text": "don't use Ethernet perhaps or IP come see me after the talk there's some",
    "start": "242489",
    "end": "248700"
  },
  {
    "text": "potential there to expand what kubernetes can do right now so",
    "start": "248700",
    "end": "254900"
  },
  {
    "text": "this slide goes in every deck that I do it's a food group analogy slide I want to give you guys a sense of how I",
    "start": "254900",
    "end": "261150"
  },
  {
    "start": "258000",
    "end": "505000"
  },
  {
    "text": "characterize a work load when it comes to me for example if a new application is under development at Red Hat and they",
    "start": "261150",
    "end": "266730"
  },
  {
    "text": "say formance engineering guys you need to take a look at this and tell us if our architecture is sound or not",
    "start": "266730",
    "end": "273710"
  },
  {
    "text": "these are some of the things that we look at these are four food groups of performance",
    "start": "273710",
    "end": "279590"
  },
  {
    "text": "every application can be decomposed into these resources and then you further optimize into each resource",
    "start": "279590",
    "end": "286070"
  },
  {
    "text": "I've heard some really interesting stuff in the CPU space about proactively monitoring in like cycles per",
    "start": "286070",
    "end": "293340"
  },
  {
    "text": "instruction on on Google's cloud etc invisible some pretty sexy stuff but these are things",
    "start": "293340",
    "end": "300330"
  },
  {
    "text": "we do internally and then this is kind of how these things need to be in a healthy balance if any one of them is contended",
    "start": "300330",
    "end": "306090"
  },
  {
    "text": "your application is not going to perform correctly so here with my enterprise and Red Hat",
    "start": "306090",
    "end": "311639"
  },
  {
    "text": "hat on there's some additional concerns that you might have outside of the",
    "start": "311639",
    "end": "317160"
  },
  {
    "text": "performance realm that are equal or even more important than the actual performance and these are things that we",
    "start": "317160",
    "end": "322169"
  },
  {
    "text": "take extremely seriously at Red Hat and this is why we call ourselves enterprise the things like support and security we",
    "start": "322169",
    "end": "328560"
  },
  {
    "text": "take extremely seriously part of our major contributions to kubernetes have been in this area and",
    "start": "328560",
    "end": "335090"
  },
  {
    "text": "and of course stability reliability bug fixing being able to pick up the phone and call someone etc so take the Red Hat",
    "start": "335090",
    "end": "342539"
  },
  {
    "text": "corporate hat off for a minute decomposing a particular workload let's say we have a build form Red Hat has",
    "start": "342539",
    "end": "348539"
  },
  {
    "text": "decided to compose all of our containers that we shipped to customers using open shift so we use openshift as a build",
    "start": "348539",
    "end": "354720"
  },
  {
    "text": "farm internally to compose like if you do dr. pol rel seven or whatever",
    "start": "354720",
    "end": "359840"
  },
  {
    "text": "you get an image that was run through this open chef based build farm and this",
    "start": "359840",
    "end": "365340"
  },
  {
    "text": "is publicly available to be available of course like everything we do so we did",
    "start": "365340",
    "end": "370440"
  },
  {
    "text": "some work on optimizing that environment we found issues with",
    "start": "370440",
    "end": "375889"
  },
  {
    "text": "docker not being parallel safe for builds for example running into into IO",
    "start": "375889",
    "end": "382700"
  },
  {
    "text": "contention and some of it was learning how much IO does it actually take to do a build is there more optimal ways can",
    "start": "382700",
    "end": "389340"
  },
  {
    "text": "we do some caching to to shield the the storage subsystem from having to do",
    "start": "389340",
    "end": "394500"
  },
  {
    "text": "repetitive IO and we also had to implement some queues in front of the",
    "start": "394500",
    "end": "400410"
  },
  {
    "text": "actual docker processes so that we wouldn't flood the",
    "start": "400410",
    "end": "406090"
  },
  {
    "text": "docker API I think about all of these kinds of things",
    "start": "406090",
    "end": "411910"
  },
  {
    "text": "when I'm characterizing a workload and so as you look at this slide think to yourself what does my application look",
    "start": "411910",
    "end": "418600"
  },
  {
    "text": "like on this chart do you need live migration",
    "start": "418600",
    "end": "423630"
  },
  {
    "text": "does it run Windows these are things you would want to want to know about your Atlas Lee you know these things about",
    "start": "423630",
    "end": "429040"
  },
  {
    "text": "your application and what level you modulate the level of concern that's what the left side means and that these",
    "start": "429040",
    "end": "434310"
  },
  {
    "text": "icons mean incidentally a an hour long talk on this",
    "start": "434310",
    "end": "439600"
  },
  {
    "text": "topic is already on SlideShare if you just search for my name but in this case",
    "start": "439600",
    "end": "445150"
  },
  {
    "text": "a build farm isn't necessarily sensitive to network latency but it is sensitive to network throughput because I'm doing pushes and pulls and maybe I'm getting",
    "start": "445150",
    "end": "451630"
  },
  {
    "text": "stuff from yum or wherever else to actually compose my container so these are the types of things that I",
    "start": "451630",
    "end": "458440"
  },
  {
    "text": "keep in the back of my head and at least from my point of view the only thing that matters is making your",
    "start": "458440",
    "end": "464650"
  },
  {
    "text": "apps run well so that's kind of the performance side of",
    "start": "464650",
    "end": "471130"
  },
  {
    "text": "it does that does that make sense is that methodology ring true at all",
    "start": "471130",
    "end": "476520"
  },
  {
    "text": "who can give me one example of an application that's not a web-based application that they want to run and coupon cube",
    "start": "476520",
    "end": "483570"
  },
  {
    "text": "awesome so do you use GPUs for that",
    "start": "484560",
    "end": "489660"
  },
  {
    "text": "yeah that's right up my alley and I believe we will do something with that I",
    "start": "489660",
    "end": "496570"
  },
  {
    "text": "hope we can do something with that RIT has perspective standardized everything",
    "start": "496570",
    "end": "501880"
  },
  {
    "text": "the value of devops cannot be realized without portability and the standards",
    "start": "501880",
    "end": "508950"
  },
  {
    "start": "505000",
    "end": "709000"
  },
  {
    "text": "standardization of at least the at least the container image format we're not able to build a successful ecosystem of",
    "start": "508950",
    "end": "515590"
  },
  {
    "text": "ISVs software ifvs around kubernetes in the container platform without some sort",
    "start": "515590",
    "end": "520719"
  },
  {
    "text": "of standardization they're not going to run and do we know these Java like if you're selling a java app you deploy it",
    "start": "520719",
    "end": "526180"
  },
  {
    "text": "as like you sell it as a zip file you don't pack it as it as it Deb or an RPM",
    "start": "526180",
    "end": "531480"
  },
  {
    "text": "so the idea being there was no portability at the time when we were",
    "start": "531480",
    "end": "536770"
  },
  {
    "text": "doing these package managers for the last 20 years we hope that the container image format can solve that and",
    "start": "536770",
    "end": "543100"
  },
  {
    "text": "incidentally Red Hat plays a somewhat of a role in both of these both of these consortiums or at least I would call",
    "start": "543100",
    "end": "549880"
  },
  {
    "text": "them foundations so what do we do that's different than what the or in in",
    "start": "549880",
    "end": "555760"
  },
  {
    "text": "addition to what the kubernetes and community do for scale testing of open ship of kubernetes",
    "start": "555760",
    "end": "562590"
  },
  {
    "text": "back to the word Enterprise so you have kubernetes what else do you need to make it actually work you need things like a",
    "start": "562590",
    "end": "568390"
  },
  {
    "text": "registry you need an SDN you need",
    "start": "568390",
    "end": "575339"
  },
  {
    "text": "some sort of storage on it as well so these are things that Red Hat sells",
    "start": "576480",
    "end": "582610"
  },
  {
    "text": "additional products for and these are types of things that we are going to add on top of the normal kubernetes scale",
    "start": "582610",
    "end": "587620"
  },
  {
    "text": "tests so we're gonna do you know build performance I mentioned the render for",
    "start": "587620",
    "end": "592750"
  },
  {
    "text": "the build form earlier we're gonna do registry scalability logging and metrics OpenShift has like",
    "start": "592750",
    "end": "600790"
  },
  {
    "text": "an e FK stack to be as fluent d which I noticed was was just accepted into CNC F so all of those things are in our realm",
    "start": "600790",
    "end": "607600"
  },
  {
    "text": "that aren't necessarily part of the base cube distribution but that you get when you purchase a distribution of kubernetes from an enterprise vendor",
    "start": "607600",
    "end": "615600"
  },
  {
    "text": "so onto the the actual lab tests and I guess may or something we we found",
    "start": "616350",
    "end": "624760"
  },
  {
    "text": "out about this cluster and we Alec and they were allocated 300 nodes on it because we had a goal of a thousand node",
    "start": "624760",
    "end": "631990"
  },
  {
    "text": "kubernetes cluster on this level you might have seen this blog but it's on the CNCs blog but since we were only",
    "start": "631990",
    "end": "638620"
  },
  {
    "text": "allocated 300 of those nodes we had to do some sort of virtualization which was not in our original plan we decided to put OpenStack on there",
    "start": "638620",
    "end": "646150"
  },
  {
    "text": "perhaps OpenStack distribution which was I think version 8 so like Liberty if you're familiar with the code names and",
    "start": "646150",
    "end": "651960"
  },
  {
    "text": "so that was our cloud infrastructure on the CNC lab",
    "start": "651960",
    "end": "657090"
  },
  {
    "text": "the point of the exercise was to push it till it breaks which is every",
    "start": "657090",
    "end": "663100"
  },
  {
    "text": "performance engine favorite thing to do is to find the failure scenarios and",
    "start": "663100",
    "end": "668780"
  },
  {
    "text": "we also wanted to come up with a reference design which we've done around a scalable kubernetes environment",
    "start": "668780",
    "end": "674730"
  },
  {
    "text": "environment that is also highly available",
    "start": "674730",
    "end": "678980"
  },
  {
    "text": "so we put together a list of best practices out of this we filed 30 plus",
    "start": "681140",
    "end": "688350"
  },
  {
    "text": "bugs in different scenarios certain things are fixed certain things are still underway certain things are just",
    "start": "688350",
    "end": "695010"
  },
  {
    "text": "the nature of kubernetes so I mentioned we not only do we file",
    "start": "695010",
    "end": "701820"
  },
  {
    "text": "these issues but like Tim and some of his guys will work with the upstream SIG's to actually fix them",
    "start": "701820",
    "end": "709070"
  },
  {
    "start": "709000",
    "end": "959000"
  },
  {
    "text": "if you haven't seen the CNC F cluster is a pretty pretty sexy resource there's",
    "start": "709070",
    "end": "714270"
  },
  {
    "text": "500 of these nodes now with 500 more coming online real soon and what's cool about these nodes and we",
    "start": "714270",
    "end": "721080"
  },
  {
    "text": "took advantage of was that each node has an end this is donated to it by Intel by the way to the CNC F anyway each node",
    "start": "721080",
    "end": "728130"
  },
  {
    "text": "has an nvme device in it which is extremely hard to come by and if we try to do that on a public cloud provider",
    "start": "728130",
    "end": "734670"
  },
  {
    "text": "they don't offer those things yet and even if they did it would be insanely expensive it's one cool thing about this",
    "start": "734670",
    "end": "739950"
  },
  {
    "text": "lab is you get really high end stuff at zero cost",
    "start": "739950",
    "end": "745459"
  },
  {
    "text": "I guess if I had a want for this thing they'd have more ram and they'd have 40 gig Nick's but other than that it's like",
    "start": "745550",
    "end": "751770"
  },
  {
    "text": "you know gift horse in the mouth type of thing if you want to if you want to schedule yourself on this lab there's a",
    "start": "751770",
    "end": "757260"
  },
  {
    "text": "FIFO queue that's managed in github /c n CF slash cluster if you want to apply",
    "start": "757260",
    "end": "763290"
  },
  {
    "text": "for it and incidentally I look through some of our next like what's next in a minute but we're gonna get on this lab",
    "start": "763290",
    "end": "770400"
  },
  {
    "text": "again at some point",
    "start": "770400",
    "end": "773420"
  },
  {
    "text": "so once we're on the lab this is what we built and the aspect ratio makes this",
    "start": "775850",
    "end": "781260"
  },
  {
    "text": "diagram what's really her orrible but I mean obviously be used right now it's",
    "start": "781260",
    "end": "786390"
  },
  {
    "text": "product stack but really the architecture diagram is what I wanted to point out here we did the whole thing highly available",
    "start": "786390",
    "end": "792420"
  },
  {
    "text": "so there's there's no single point of failure one thing that's actually missing from this dire is a load balancer that goes",
    "start": "792420",
    "end": "799110"
  },
  {
    "text": "in front of the master the API servers so we've got three API servers in this",
    "start": "799110",
    "end": "804450"
  },
  {
    "text": "scenario one load balancer VM and the queue let's talk to the load",
    "start": "804450",
    "end": "810150"
  },
  {
    "text": "balancer a 3-node fcd cluster routers registries and a thousand VMs",
    "start": "810150",
    "end": "819530"
  },
  {
    "text": "the VM inch if you're interested the VM configs down here below but it's not really that important",
    "start": "819530",
    "end": "824780"
  },
  {
    "text": "so yeah so that's what we built and then ended up being fed into parts of it we're fed into a recent reference",
    "start": "824780",
    "end": "831360"
  },
  {
    "text": "architecture document that RIT has put out how to run OpenShift on top of OpenStack we've also learned got there's",
    "start": "831360",
    "end": "837480"
  },
  {
    "text": "four of them how to run OpenShift on OpenStack on VMware on GCE and on AWS",
    "start": "837480",
    "end": "842490"
  },
  {
    "text": "they just came out this week so you'll be able to see the very latest and greatest from exercises like this and",
    "start": "842490",
    "end": "848640"
  },
  {
    "text": "our counterpart in my in in my team and the system design and engineering group that builds the reference architectures",
    "start": "848640",
    "end": "855690"
  },
  {
    "text": "so if you go to access right comm you'll be able to find these reference architectures",
    "start": "855690",
    "end": "862370"
  },
  {
    "text": "ok so we've got this environment built out which was its own set of hilarious",
    "start": "865280",
    "end": "872070"
  },
  {
    "text": "jokes but after it was after it was up and running I guys that America's talk",
    "start": "872070",
    "end": "877170"
  },
  {
    "text": "yesterday and then he said he had this picture of a nuke and it it really is that we we kind of call ourselves",
    "start": "877170",
    "end": "882300"
  },
  {
    "text": "crawling over broken glass as a service this is all and the idea is that is what",
    "start": "882300",
    "end": "888270"
  },
  {
    "text": "we want to do that's what we get paid for the idea is that after we're done crawling and and you know afterward",
    "start": "888270",
    "end": "895100"
  },
  {
    "text": "abused the products are so much better we have",
    "start": "895100",
    "end": "900740"
  },
  {
    "text": "you know we have products that have run the full gamut of some pretty hardcore tests I mean like I said we can always",
    "start": "900740",
    "end": "905940"
  },
  {
    "text": "do better and we will but we feel like we have a decent mix and here's what we here's what we have if you go to github",
    "start": "905940",
    "end": "911640"
  },
  {
    "text": "openshift SBT you'll find our repository of tests and i'm hopeful and it done this has been",
    "start": "911640",
    "end": "919110"
  },
  {
    "text": "negotiated but my personal hope is that we that we can port all of this stuff into upstream kubernetes and that we",
    "start": "919110",
    "end": "925200"
  },
  {
    "text": "would pick and choose pieces of it potentially to use in this scalability",
    "start": "925200",
    "end": "931440"
  },
  {
    "text": "finishin ultimately we would want to gate merges on scale tests if we had infinite budget",
    "start": "931440",
    "end": "937440"
  },
  {
    "text": "so I don't know if that will ever happen but I think we can do something with like mocking large environments and",
    "start": "937440",
    "end": "943950"
  },
  {
    "text": "maybe learn a lot more I know that's what there's a tool called cube mark which does that right now anyway so I",
    "start": "943950",
    "end": "949890"
  },
  {
    "text": "think the main most there's a there's a this repo here is currently has a couple of network tests in it from from the",
    "start": "949890",
    "end": "956010"
  },
  {
    "text": "from the Google guys we've got this weird scenario where we're trying to use the Cooper who's",
    "start": "956010",
    "end": "961470"
  },
  {
    "start": "959000",
    "end": "1019000"
  },
  {
    "text": "familiar with the core Nettie's test harness called ete the ete test harness currently doesn't",
    "start": "961470",
    "end": "968130"
  },
  {
    "text": "support being accessed from outside the EE and so we've got just a branch down",
    "start": "968130",
    "end": "974970"
  },
  {
    "text": "here until the cig testing guys can figure out how to let us let us access ete from outside e to e or we",
    "start": "974970",
    "end": "982050"
  },
  {
    "text": "reimplemented of e to e that we're actually using in this tool called cluster loader which is our main utility",
    "start": "982050",
    "end": "988880"
  },
  {
    "text": "so we've also got things like reliability and longevity testing where we'll run for a couple of weeks and make",
    "start": "988880",
    "end": "994110"
  },
  {
    "text": "sure we don't have any memory leaks etc these are things to kind of harden the environment and we found we found some",
    "start": "994110",
    "end": "999870"
  },
  {
    "text": "leaks and back so I think that's all pretty cool I think you know so cholesterol order is part one and I'll go through it in a minute and then the",
    "start": "999870",
    "end": "1005540"
  },
  {
    "text": "other piece is after the cluster is up and running we need to fire a bunch of load at it and then see if it can really",
    "start": "1005540",
    "end": "1011990"
  },
  {
    "text": "perform the way we expect it to at scale so performance at scale",
    "start": "1011990",
    "end": "1018730"
  },
  {
    "text": "we were building this environment the first step to doing it was we need to go to an image based primigi",
    "start": "1018730",
    "end": "1026300"
  },
  {
    "start": "1019000",
    "end": "1199000"
  },
  {
    "text": "based deployment model if you're scaling to a thousand you",
    "start": "1026300",
    "end": "1032750"
  },
  {
    "text": "can't have each VM go out to the registry and pull the 15 containers it takes to actually make things work and you also want to not it would also have",
    "start": "1032750",
    "end": "1040069"
  },
  {
    "text": "want to have updates baked in etc so we've we built out a bunch of ansible to do this and it will spit out a cue cow",
    "start": "1040069",
    "end": "1047170"
  },
  {
    "text": "which we used on OpenStack and continues on OpenStack or Rev and it will also",
    "start": "1047170",
    "end": "1053600"
  },
  {
    "text": "spit out potentially an ami so it will go out to Amazon create an instance do",
    "start": "1053600",
    "end": "1060320"
  },
  {
    "text": "the whole thing and then the image will be available for us to use I think the main steps I mention - earlier we do",
    "start": "1060320",
    "end": "1066019"
  },
  {
    "text": "some filesystem juggling like partition resizing etc and then we do you know",
    "start": "1066019",
    "end": "1071299"
  },
  {
    "text": "make sure we have all the updates and containers pre polled and then we actually load the rpms as well onto the",
    "start": "1071299",
    "end": "1076549"
  },
  {
    "text": "system so everything as much as we can is preloaded and those are big wins because otherwise we couldn't do the",
    "start": "1076549",
    "end": "1082340"
  },
  {
    "text": "full scale it would take forever we use these images and we can do massive installs of OpenShift really quickly",
    "start": "1082340",
    "end": "1089679"
  },
  {
    "text": "and that helps because we felt like we learned that we had to it we had to be capable of that because we knew we",
    "start": "1089679",
    "end": "1095120"
  },
  {
    "text": "started to have to iterate a lot do these installs over and over and over again so",
    "start": "1095120",
    "end": "1100389"
  },
  {
    "text": "the cluster loader itself takes a llamó config like this and these templates are somewhat",
    "start": "1100389",
    "end": "1107539"
  },
  {
    "text": "controversial right now but essentially it can take a number of projects or namespaces which our tenants and",
    "start": "1107539",
    "end": "1114380"
  },
  {
    "text": "OpenShift land and it will create however many of whatever type of templates that you have so if you were",
    "start": "1114380",
    "end": "1121940"
  },
  {
    "text": "to download this guy today and you have templatized and moved your app into cube already you take your you take your",
    "start": "1121940",
    "end": "1128179"
  },
  {
    "text": "template you feed it into this you Hamill file and it will build out your environment exactly as you with your",
    "start": "1128179",
    "end": "1134149"
  },
  {
    "text": "application what we've done is take the exam of the examples from kubernetes and and make them in you know and put them",
    "start": "1134149",
    "end": "1141110"
  },
  {
    "text": "into this repo so i mentioned they're controversial because cube-like doesn't have a concept",
    "start": "1141110",
    "end": "1148130"
  },
  {
    "text": "of templates right now openshift does and there's other technology like there's",
    "start": "1148130",
    "end": "1153380"
  },
  {
    "text": "probably a dozen other like templating technologies out there but anyway so we're reporting as many as we can we",
    "start": "1153380",
    "end": "1159500"
  },
  {
    "text": "like to build up a pretty big corpus of other applications that we could you know throw at these",
    "start": "1159500",
    "end": "1165320"
  },
  {
    "text": "environments these configs can also include like a persistent volume they",
    "start": "1165320",
    "end": "1170389"
  },
  {
    "text": "can get as complicated as you want it could be as simple as like a pause pod if you're familiar with that or it could be as complex as a multi-tiered",
    "start": "1170389",
    "end": "1176600"
  },
  {
    "text": "application that includes persistent storage and in fact those are the ones that we've got here so the Redis example",
    "start": "1176600",
    "end": "1182450"
  },
  {
    "text": "the guestbook example with persistent back in storage in a database and",
    "start": "1182450",
    "end": "1187480"
  },
  {
    "text": "it will also set up all of the routing and secrets and build configs etc so that's",
    "start": "1187480",
    "end": "1195620"
  },
  {
    "text": "what we did on the on the open our sorry on the CN CF cluster",
    "start": "1195620",
    "end": "1200740"
  },
  {
    "text": "so maybe it'll make a little bit more sense if I show you what what it is",
    "start": "1200929",
    "end": "1206890"
  },
  {
    "text": "okay so in this scenario what we found with OpenShift version two was that",
    "start": "1239640",
    "end": "1245640"
  },
  {
    "text": "this is not kubernetes version of open shipped to the previous version which had our own implementation of containers",
    "start": "1245640",
    "end": "1251070"
  },
  {
    "text": "we found that flooding the API server was not a good idea so we've got this these control loops and back offs and",
    "start": "1251070",
    "end": "1257320"
  },
  {
    "text": "delays that are built into it in this scenario I'm not doing a tremendous amount of load on the system so I don't",
    "start": "1257320",
    "end": "1263230"
  },
  {
    "text": "really need those those in there but that's what they're for so and then this",
    "start": "1263230",
    "end": "1268330"
  },
  {
    "text": "config is pluggable as if you know you can put whatever applications you want in here so in this scenario just nginx",
    "start": "1268330",
    "end": "1274540"
  },
  {
    "text": "guestbook elasticsearch etc we're only gonna do one of each so it'll be one namespace with one of each of these",
    "start": "1274540",
    "end": "1279940"
  },
  {
    "text": "applications running in it",
    "start": "1279940",
    "end": "1283559"
  },
  {
    "text": "so just right now got kind of the main the normal kubernetes stuff in here so",
    "start": "1288330",
    "end": "1293370"
  },
  {
    "text": "I'll go ahead and run this horrendous command",
    "start": "1293370",
    "end": "1301230"
  },
  {
    "text": "there's not really much to it make it verbose use the cluster loader portion of the ete test so the IDI test says",
    "start": "1306840",
    "end": "1312540"
  },
  {
    "text": "like 400 and plus tests in it we only want to run the cluster loader portion of it and then they use Viper for it",
    "start": "1312540",
    "end": "1317700"
  },
  {
    "text": "this is written by a co-worker of mine named Sebastian uke and so it uses Viper and then you pointed at",
    "start": "1317700",
    "end": "1323910"
  },
  {
    "text": "a config file the idea is this config file could be could represent your environment so",
    "start": "1323910",
    "end": "1329930"
  },
  {
    "text": "it's going through and creating all these containers incidentally the images are pre-loaded like they're in the",
    "start": "1331880",
    "end": "1337260"
  },
  {
    "text": "docker already",
    "start": "1337260",
    "end": "1340100"
  },
  {
    "text": "so not terribly sexy but it did it did do it everything's running and the idea",
    "start": "1346430",
    "end": "1352170"
  },
  {
    "text": "here is that when we were on the CNC F lab we would increase those integers and be able to scale out as wide as we possibly as wide as we wanted to is why",
    "start": "1352170",
    "end": "1359070"
  },
  {
    "text": "does the hardware and software is capable of doing so the single applet so the single test harness can do you know",
    "start": "1359070",
    "end": "1364440"
  },
  {
    "text": "as much scale as as you might want so pretty simple demo but we're hopeful",
    "start": "1364440",
    "end": "1370710"
  },
  {
    "text": "that we can find a way to get that upstream into kubernetes said you guys",
    "start": "1370710",
    "end": "1376380"
  },
  {
    "text": "can all right now it's on github and it's in Python but we're we're gonna try and get into upstream cube proper contribute back to the community so",
    "start": "1376380",
    "end": "1384560"
  },
  {
    "text": "any questions about that utility does anyone think they could make use of",
    "start": "1384560",
    "end": "1390270"
  },
  {
    "text": "it yeah that's what it is",
    "start": "1390270",
    "end": "1395600"
  },
  {
    "start": "1394000",
    "end": "1444000"
  },
  {
    "text": "so what did we learn we fired this we fired the the cluster loader at at this",
    "start": "1395600",
    "end": "1401700"
  },
  {
    "text": "big at this big open shift environment what was some of the takeaways well this is our configure we ended up with end up",
    "start": "1401700",
    "end": "1408000"
  },
  {
    "text": "with a thousand nodes 13,000 tenants or projects whatever 52,000",
    "start": "1408000",
    "end": "1413450"
  },
  {
    "text": "pods and all these other objects were loaded into at CD",
    "start": "1413450",
    "end": "1419180"
  },
  {
    "text": "in this hidden incidentally we plan to go a lot higher but we did hit a bug in the API server that had very recently",
    "start": "1419180",
    "end": "1425580"
  },
  {
    "text": "been filed upstream at the time so the API server like was crashing and at that",
    "start": "1425580",
    "end": "1431760"
  },
  {
    "text": "time our time was up on the CNC F lab so we didn't get a chance to continue but we'll do that you know a bigger scale",
    "start": "1431760",
    "end": "1438180"
  },
  {
    "text": "the next time we get on that gear so a",
    "start": "1438180",
    "end": "1446180"
  },
  {
    "start": "1444000",
    "end": "1534000"
  },
  {
    "text": "couple things we found a lot of our teams output is actually capacity",
    "start": "1446180",
    "end": "1452160"
  },
  {
    "text": "planning information and that gets into our product documentation so",
    "start": "1452160",
    "end": "1457790"
  },
  {
    "text": "probably the initial conversation that we have with customers is how much gear do I need to buy or how much cloud",
    "start": "1457790",
    "end": "1463650"
  },
  {
    "text": "resources so I need to provision and so we create these formulas based on our actual tests that they can use to",
    "start": "1463650",
    "end": "1470160"
  },
  {
    "text": "extrapolate and decide how much gear they need in the scenario on the Left we",
    "start": "1470160",
    "end": "1475590"
  },
  {
    "text": "found that loading all of that stuff that I just showed you into ed CD takes a fair amount of space on disk more than",
    "start": "1475590",
    "end": "1481800"
  },
  {
    "text": "we had originally documented and so we've gone ahead and updated that documentation and used this as",
    "start": "1481800",
    "end": "1486930"
  },
  {
    "text": "justification to say look you need to give 40 gigs to at CD if you're gonna do anything at scale fortunately F CD is",
    "start": "1486930",
    "end": "1492570"
  },
  {
    "text": "really easy to move like it's got you know just that one database files you just like my sequel you can just ship them around if you can handle an outage",
    "start": "1492570",
    "end": "1499890"
  },
  {
    "text": "but if you want to size everything accurately up front you know that so that's what the best value out of some",
    "start": "1499890",
    "end": "1506220"
  },
  {
    "text": "of these tests in terms of ED CD and disk incidentally this is at CD version 2 in",
    "start": "1506220",
    "end": "1512640"
  },
  {
    "text": "case you were curious and on the right hand side we've got some other gharana graphs I think the only interesting part about this is that you notice this",
    "start": "1512640",
    "end": "1519180"
  },
  {
    "text": "drop-off in CPU and that's actually when the API server fell over and",
    "start": "1519180",
    "end": "1524330"
  },
  {
    "text": "luckily I Chi said that bug had been fixed so and this is an alpha version of openshift 3 3 so this bug never shipped",
    "start": "1524330",
    "end": "1530460"
  },
  {
    "text": "in the product question yeah",
    "start": "1530460",
    "end": "1536510"
  },
  {
    "start": "1534000",
    "end": "1609000"
  },
  {
    "text": "open ships installer is based on is written in ansible which is incredibly",
    "start": "1537410",
    "end": "1542700"
  },
  {
    "text": "flexible totally paralyzed and not really sure how we would survive without it so we also use ansible for",
    "start": "1542700",
    "end": "1550230"
  },
  {
    "text": "every other thing in the world one thing we noticed is that when you're gathering so ansible will go out to each node",
    "start": "1550230",
    "end": "1555540"
  },
  {
    "text": "gather what see what it calls facts about that node and then suck them back over the network and hash them in memory",
    "start": "1555540",
    "end": "1561750"
  },
  {
    "text": "what happens is when it does that hashing across thousands of nodes you get a tremendous amount of CPU",
    "start": "1561750",
    "end": "1567120"
  },
  {
    "text": "utilization which you need to know about because if you",
    "start": "1567120",
    "end": "1573540"
  },
  {
    "text": "run out of CPU sometimes those those installed portions could fail and so we worked with the",
    "start": "1573540",
    "end": "1580020"
  },
  {
    "text": "ansible guys to optimize the crap out of that part and",
    "start": "1580020",
    "end": "1586280"
  },
  {
    "text": "we can actually shove those results into a database at this point instead of having them in a flat file it's a little",
    "start": "1586280",
    "end": "1591990"
  },
  {
    "text": "bit more optimal just sequel Lite it actually helps us perform this problem significantly and again this is just the",
    "start": "1591990",
    "end": "1597000"
  },
  {
    "text": "Installer if you're doing at a huge scale ya",
    "start": "1597000",
    "end": "1603620"
  },
  {
    "text": "against the personal machines so I",
    "start": "1603620",
    "end": "1611360"
  },
  {
    "start": "1609000",
    "end": "2069000"
  },
  {
    "text": "guess we can start talking about that so that was the end of the performance tests for for CN CF that all makes sense",
    "start": "1611360",
    "end": "1618840"
  },
  {
    "text": "was there any questions do you think you can use some of that",
    "start": "1618840",
    "end": "1624840"
  },
  {
    "text": "information",
    "start": "1624840",
    "end": "1627169"
  },
  {
    "text": "[Music]",
    "start": "1632320",
    "end": "1635519"
  },
  {
    "text": "yeah right so we ran into when we did the initial OpenStack deployment we used",
    "start": "1640370",
    "end": "1648060"
  },
  {
    "text": "Swift and which we regretted but in the",
    "start": "1648060",
    "end": "1654150"
  },
  {
    "text": "next iteration we're gonna use we're already using SEF moved from volumes with snapshots so we expect that prob we",
    "start": "1654150",
    "end": "1660090"
  },
  {
    "text": "had a problem with Nova that a lot like it just took forever to blow out these these 1,000 VMs and so we're trying to",
    "start": "1660090",
    "end": "1665700"
  },
  {
    "text": "optimize that and so for our next strobe our next runs we'll have those optimized we won't have to keep the VMS around",
    "start": "1665700",
    "end": "1672210"
  },
  {
    "text": "we'll be able to whack them the same like we uninstall openshift",
    "start": "1672210",
    "end": "1677240"
  },
  {
    "text": "it wasn't the scheduler it was just pushing all the images already yeah just shipping those images around on on disk",
    "start": "1678320",
    "end": "1684840"
  },
  {
    "text": "to 300 nodes just took a long time it was like an hour just of shipping images",
    "start": "1684840",
    "end": "1689850"
  },
  {
    "text": "around and then the install actually kicked off was there another question",
    "start": "1689850",
    "end": "1694340"
  },
  {
    "text": "that's specifically about the density test that's in the IDI so that's what those definitions apply to OpenShift as",
    "start": "1708460",
    "end": "1714950"
  },
  {
    "text": "well we take Qbert we take kubernetes as is we had a couple things that weren't yet upstream but our going upstream",
    "start": "1714950",
    "end": "1721640"
  },
  {
    "text": "things like for a while the security policies weren't upstream we were using them but my point is that these those",
    "start": "1721640",
    "end": "1729140"
  },
  {
    "text": "SaaS are are part of all this testing",
    "start": "1729140",
    "end": "1734710"
  },
  {
    "text": "yeah so it's not in this it's not in this debt but we our team has built a tool called P bench which is a",
    "start": "1752620",
    "end": "1758750"
  },
  {
    "text": "distributed system analysis tool and it'll gather all of the CPU profiles like go P prof. data and so we have that",
    "start": "1758750",
    "end": "1767120"
  },
  {
    "text": "for every run we do across all of the math all the important nodes on the system have full tracing enabled we have",
    "start": "1767120",
    "end": "1773210"
  },
  {
    "text": "all this graphs and stuff so I showed you a graph on a graph of CPU that's one example of that but there's also p prof",
    "start": "1773210",
    "end": "1779210"
  },
  {
    "text": "data and that's the one those are the ones where we you know like tim and other guys on open chef team would go in and optimize code paths or algorithms in",
    "start": "1779210",
    "end": "1786790"
  },
  {
    "text": "kubernetes code to help with scale",
    "start": "1786790",
    "end": "1790840"
  },
  {
    "text": "oh",
    "start": "1793159",
    "end": "1795159"
  },
  {
    "text": "yeah yeah yeah so I think I think the open tracing guys could probably play a",
    "start": "1808599",
    "end": "1814609"
  },
  {
    "text": "role here because the the events that cube throws are probably not as granular as we need and they're also the time",
    "start": "1814609",
    "end": "1822529"
  },
  {
    "text": "stamps aren't accurate enough so there's improvement to be done there what I find",
    "start": "1822529",
    "end": "1827840"
  },
  {
    "text": "personally and they feel free to disagree is that the only thing that matters is the is the user experience is",
    "start": "1827840",
    "end": "1833539"
  },
  {
    "text": "how long ultimately we want to know along the way how long each subsection",
    "start": "1833539",
    "end": "1838820"
  },
  {
    "text": "took we want to make sure that the total time to stand up a pod is within those five-second bounds as well with pre",
    "start": "1838820",
    "end": "1844580"
  },
  {
    "text": "pulled images and at scale it turns out that you end up violating those around the",
    "start": "1844580",
    "end": "1850090"
  },
  {
    "text": "60,000 pod range or something like that so and a lot of this is subjective it",
    "start": "1850090",
    "end": "1855649"
  },
  {
    "text": "depends on your i/o subsystem performance so",
    "start": "1855649",
    "end": "1861220"
  },
  {
    "text": "absolutely and so we every time we do one of these runs the actual activity is",
    "start": "1865119",
    "end": "1870440"
  },
  {
    "text": "analyzing all of the data that gets recorded out of it and finding out where we cpu-bound in that ansible scenario I",
    "start": "1870440",
    "end": "1877249"
  },
  {
    "text": "actually showed you the good graph the bad graph was a disaster and we fixed all those bugs and ansible along the way",
    "start": "1877249",
    "end": "1883369"
  },
  {
    "text": "but the those graphs were instrumental in telling us what portion of the ansible playbooks actually had those",
    "start": "1883369",
    "end": "1890779"
  },
  {
    "text": "kinds of issues so in the ansible scenario that's one thing and then the the other thing is in the API server we",
    "start": "1890779",
    "end": "1896929"
  },
  {
    "text": "might see huge CPU spikes and then we can go in and look at the p prof data that correlates with that time frame and",
    "start": "1896929",
    "end": "1903039"
  },
  {
    "text": "get back traces basically that tells us how much CPU time was spent in a certain function or whatever and then we you",
    "start": "1903039",
    "end": "1908899"
  },
  {
    "text": "know it's just like it's you peeling an onion so we do we do have like extensive debug and tracing and in all of these",
    "start": "1908899",
    "end": "1915919"
  },
  {
    "text": "runs",
    "start": "1915919",
    "end": "1918158"
  },
  {
    "text": "yep",
    "start": "1928089",
    "end": "1931089"
  },
  {
    "text": "we agree with that I mean I would say that there's we bring so there's there's",
    "start": "1933879",
    "end": "1939499"
  },
  {
    "text": "enough low-hanging fruit actually is really when it comes down to there's also other teams that are working on locking scenarios in in the go language",
    "start": "1939499",
    "end": "1946460"
  },
  {
    "text": "itself that we need to improve like the fact is go is new it doesn't have ideal tracing facilities right now and there's",
    "start": "1946460",
    "end": "1953479"
  },
  {
    "text": "some pretty big shows I don't want to say showstoppers but it we wish it would be see and that we could debug it like a",
    "start": "1953479",
    "end": "1960649"
  },
  {
    "text": "normal language but here nor there we have everything in go at this point I was going to run through a couple of",
    "start": "1960649",
    "end": "1967969"
  },
  {
    "text": "tuning subsist yeah it wasn't yep",
    "start": "1967969",
    "end": "1971679"
  },
  {
    "text": "yes yes it was a single switch with a single 10",
    "start": "1981869",
    "end": "1989889"
  },
  {
    "text": "gig drop to every node and a flat VLAN",
    "start": "1989889",
    "end": "1996779"
  },
  {
    "text": "if you're talking about VX LAN you know single night and actually",
    "start": "1998429",
    "end": "2006149"
  },
  {
    "text": "visibility into the networking tier is something we specifically asked the CNC F to give us so they were managing the",
    "start": "2006149",
    "end": "2012539"
  },
  {
    "text": "hardware we ran into like firmware issues on the NEX we ran into a bunch of ugly before we could even get OpenStack installed and that was",
    "start": "2012539",
    "end": "2019889"
  },
  {
    "text": "specific to that lab so when they bring more gear on we're gonna have to probably go through the same thing but",
    "start": "2019889",
    "end": "2026809"
  },
  {
    "text": "it was it was and we we know that wasn't optimal we did we did do the double V X",
    "start": "2026809",
    "end": "2032399"
  },
  {
    "text": "LAN encapsulation here the next time we go through we will use flannel one of the things we like about open ship is it",
    "start": "2032399",
    "end": "2038309"
  },
  {
    "text": "has multi-tenant secure networking out of the box we need OVS to do that if we drop back to flannel the trade-off is",
    "start": "2038309",
    "end": "2044700"
  },
  {
    "text": "you don't get that multi-tenant security anymore depends on the site whether or not that matters",
    "start": "2044700",
    "end": "2050480"
  },
  {
    "text": "or another pluggable network implementation so that's another thing we work with all these partners and",
    "start": "2050480",
    "end": "2055950"
  },
  {
    "text": "vendors and the load balancer is pluggable the networking subsystem is pluggable as well totally pluggable and",
    "start": "2055950",
    "end": "2061378"
  },
  {
    "text": "it's fully supported so thanks for bringing that up the networking performance is really important to meet you that's where I kind of came from in",
    "start": "2061379",
    "end": "2068608"
  },
  {
    "text": "my history anyway so for CPU bound workloads in in cube one three one of the main features that went in I guess",
    "start": "2068609",
    "end": "2074908"
  },
  {
    "start": "2069000",
    "end": "2444000"
  },
  {
    "text": "this is really all you need to know about performance management on cube is that there's a quality of service tiers that have been enabled and so there's",
    "start": "2074909",
    "end": "2081690"
  },
  {
    "text": "there's a what is it batch best best effort guaranteed and I think there's a",
    "start": "2081690",
    "end": "2086760"
  },
  {
    "text": "third one batch I think and the idea there is that if there's a contended resource the guaranteed pod would get",
    "start": "2086760",
    "end": "2093270"
  },
  {
    "text": "scheduled and run on the CPUs more often than the best effort pod the idea there is to increase utilization and also have",
    "start": "2093270",
    "end": "2099869"
  },
  {
    "text": "some relative priorities between pods that are running on the same system so if you have a performance sensitive",
    "start": "2099869",
    "end": "2106170"
  },
  {
    "text": "workload you would want to give you would want to place that in the guaranteed tier that's really the only takeaway you also want to make sure that",
    "start": "2106170",
    "end": "2113460"
  },
  {
    "text": "there's a limit syndrome Quest's capabilities and kubernetes as well you",
    "start": "2113460",
    "end": "2118530"
  },
  {
    "text": "want to make sure that those are sized to fit your application a lot of times people will undersized it and wonder why",
    "start": "2118530",
    "end": "2125970"
  },
  {
    "text": "things are running slow and the reality is they're you know they're being throttled intent like by the design of the system so for a lot of these",
    "start": "2125970",
    "end": "2132839"
  },
  {
    "text": "scenarios will turn off those those or make everything guaranteed or make it set no limits on the on the pods back",
    "start": "2132839",
    "end": "2142130"
  },
  {
    "text": "other applications might have might need specific like CPU features or",
    "start": "2143750",
    "end": "2151010"
  },
  {
    "text": "run on specific cores and there's there's kind of weird ways that you can manage this now with labels on nodes and",
    "start": "2151010",
    "end": "2157819"
  },
  {
    "text": "upcoming features like taints and toleration so eventually we'd like to get to a place where you have a little",
    "start": "2157819",
    "end": "2165000"
  },
  {
    "text": "bit more fine-grained control over where things run pneumatology PCI locality etc",
    "start": "2165000",
    "end": "2170640"
  },
  {
    "text": "we don't know if that's gonna be manageable or not but it is something we're throwing around the ideas for",
    "start": "2170640",
    "end": "2176180"
  },
  {
    "text": "memory latency again is around Numa and limits",
    "start": "2176180",
    "end": "2180920"
  },
  {
    "text": "it did have a slide here about networking so a lot of this is like it's totally infrastructure dependent if",
    "start": "2181280",
    "end": "2188490"
  },
  {
    "text": "you're running on Google's cloud you have their Sdn if you're running on",
    "start": "2188490",
    "end": "2193799"
  },
  {
    "text": "Amazon you have their Sdn if you're running on OpenStack you have that s the end etc each one of those has their own",
    "start": "2193799",
    "end": "2199410"
  },
  {
    "text": "characteristics in terms of scale throughput and packets per second type of latency numbers we've found that running on public",
    "start": "2199410",
    "end": "2206160"
  },
  {
    "text": "clouds is we basically cannot do successful regression testing on public clouds",
    "start": "2206160",
    "end": "2212609"
  },
  {
    "text": "because there's too much variance and this is something we're in constant struggles with because they're extremely",
    "start": "2212609",
    "end": "2218220"
  },
  {
    "text": "popular the public clouds but we have too much noisy neighbor problems and also buying the amount of resources it",
    "start": "2218220",
    "end": "2225270"
  },
  {
    "text": "takes to actually get huge guests that have all the provisioned I ops that you might want and then these placement",
    "start": "2225270",
    "end": "2232859"
  },
  {
    "text": "groups or equivalent technologies on different clouds just the budget just runs right through the roof so we've",
    "start": "2232859",
    "end": "2238530"
  },
  {
    "text": "fallen back to bare metal because performance analysis needs that tight control and if you don't have it then",
    "start": "2238530",
    "end": "2244170"
  },
  {
    "text": "you end up chasing red herrings all over the place and it's a waste of time for storage",
    "start": "2244170",
    "end": "2250340"
  },
  {
    "text": "rahat and I think Google have introduced something called storage classes which is a way of tearing your storage so you",
    "start": "2250340",
    "end": "2256500"
  },
  {
    "text": "could say some people may already have like slow SATA disks in their data",
    "start": "2256500",
    "end": "2261570"
  },
  {
    "text": "center and they've got other workloads be databases whatever that need nvme disks since you've got this tearing fame",
    "start": "2261570",
    "end": "2267930"
  },
  {
    "text": "concept already in your data center most likely bringing that to cube with something called storage classes and you",
    "start": "2267930",
    "end": "2273390"
  },
  {
    "text": "could maybe you have like gold silver and bronze and the more critical workloads have",
    "start": "2273390",
    "end": "2279410"
  },
  {
    "text": "the gold tier of storage which is analogous to the croisé tiers of CPU and",
    "start": "2279410",
    "end": "2284940"
  },
  {
    "text": "memory that I just mentioned guaranteed best-effort etc so there's a blog up on kubernetes from",
    "start": "2284940",
    "end": "2291390"
  },
  {
    "text": "last month about that one if you want to read more",
    "start": "2291390",
    "end": "2295070"
  },
  {
    "text": "any questions about all that stuff",
    "start": "2297890",
    "end": "2301880"
  },
  {
    "text": "yeah we the build farm is the short short lived container case for us those",
    "start": "2314270",
    "end": "2319440"
  },
  {
    "text": "containers usually live like one minute but it's not like functions as a service where things run for a couple hundred",
    "start": "2319440",
    "end": "2325230"
  },
  {
    "text": "milliseconds that's not the type of thing we're looking at yet but the build farm analysis was very high churn and in",
    "start": "2325230",
    "end": "2332880"
  },
  {
    "text": "that scenario I think the use case is going to be kubernetes scheduler throughput and doing something better",
    "start": "2332880",
    "end": "2338610"
  },
  {
    "text": "than increasing the parallelism that's possible with docker we've thought about",
    "start": "2338610",
    "end": "2344790"
  },
  {
    "text": "having multiple docker daemons to kind of parallelized things but the reality is it's not it's not highly parallel I",
    "start": "2344790",
    "end": "2350430"
  },
  {
    "text": "would call it mildly parallel safe at the moment but you can run into situations and until docker 1:12 if you",
    "start": "2350430",
    "end": "2357420"
  },
  {
    "text": "overwhelm the docker daemon you are screwed like you keep your all your apps are going down things are much better",
    "start": "2357420",
    "end": "2363990"
  },
  {
    "text": "with 1:12 where if the data you know the Damons not the parents anymore so if you",
    "start": "2363990",
    "end": "2370230"
  },
  {
    "text": "kill the daemon you don't actually lose your abs but",
    "start": "2370230",
    "end": "2374839"
  },
  {
    "text": "the short-lived containers is to build form scenario I shared earlier",
    "start": "2375320",
    "end": "2380600"
  },
  {
    "text": "Swift was for the OpenStack VM storage so the mountain unmount times are there",
    "start": "2394259",
    "end": "2400990"
  },
  {
    "text": "in there I mean yeah we had it's actually not that big of a deal it actually takes more time to allocate a",
    "start": "2400990",
    "end": "2406690"
  },
  {
    "text": "veeth than then a block device quite honestly I mean we're using device mapper for our storage here but overlay",
    "start": "2406690",
    "end": "2413380"
  },
  {
    "text": "is one scenario we investigated we did some tricks recently it's like overlay would get us faster storage faster graph",
    "start": "2413380",
    "end": "2419559"
  },
  {
    "text": "driver because it has memory page cache sharing and so you have less memory",
    "start": "2419559",
    "end": "2424569"
  },
  {
    "text": "usage it also doesn't allocate a kernel device so you don't go through those code paths as well but basically we started using",
    "start": "2424569",
    "end": "2430589"
  },
  {
    "text": "device mapper with a little bit of a bind mount trick to get rid of both of those scenarios and still deliver POSIX",
    "start": "2430589",
    "end": "2436539"
  },
  {
    "text": "compliance so there's a blog on that on developer blog dot reddit com",
    "start": "2436539",
    "end": "2443220"
  },
  {
    "text": "so what we're gonna do next the gentleman asked about the normal density API",
    "start": "2443220",
    "end": "2448259"
  },
  {
    "start": "2444000",
    "end": "2663000"
  },
  {
    "text": "response time latency is we need to work on hopefully in the next couple releases of cube we need to formalize that a",
    "start": "2448259",
    "end": "2454900"
  },
  {
    "text": "little bit more and extend it some of my ideas if I was just spitballing is we need to know like CPI kind of",
    "start": "2454900",
    "end": "2461319"
  },
  {
    "text": "measurements around all of the subsystems that are involved we need to actually test real applications and run",
    "start": "2461319",
    "end": "2467589"
  },
  {
    "text": "workloads on them so if you grab our git repository we also have a workload generator that spins so it will deploy",
    "start": "2467589",
    "end": "2473170"
  },
  {
    "text": "all the apps with cluster loader then it will come back and spin up a dynamic number of jmeter pods and fire off the",
    "start": "2473170",
    "end": "2478269"
  },
  {
    "text": "tests against those and we'll go and query the API server for the routes and it will build a jmeter test plan",
    "start": "2478269",
    "end": "2484420"
  },
  {
    "text": "dynamically and then fire load at it and then build graphs so all that's in there I would love to see something like that",
    "start": "2484420",
    "end": "2490029"
  },
  {
    "text": "added and then of course we want to make sure that it's actually tested against a full AJ environment if possible which",
    "start": "2490029",
    "end": "2495640"
  },
  {
    "text": "means multi-master multi sed etc",
    "start": "2495640",
    "end": "2500369"
  },
  {
    "text": "I think that's pretty much it and we have some STD testing of course that's always ongoing oh and another thing is hyper-converged storage so Brad sells a",
    "start": "2503460",
    "end": "2510060"
  },
  {
    "text": "product called container native storage which is Gluster and it's running on top of OpenShift or on top of kubernetes and",
    "start": "2510060",
    "end": "2516120"
  },
  {
    "text": "the it provides the dynamic provisioning of persistent volumes directly to kubernetes and it runs within kubernetes",
    "start": "2516120",
    "end": "2521880"
  },
  {
    "text": "as pods itself so that's something we're gonna take a look at over the next couple weeks",
    "start": "2521880",
    "end": "2526700"
  },
  {
    "text": "the guys are extremely busy with features and I know Connor is here he's doing this",
    "start": "2527780",
    "end": "2534810"
  },
  {
    "text": "one pod level cgroups right now I mean it's not about the individual people but the idea is we're tackling many many",
    "start": "2534810",
    "end": "2539880"
  },
  {
    "text": "different things huge page support for certain applications we've also introduced certain sis controls the ones",
    "start": "2539880",
    "end": "2545280"
  },
  {
    "text": "that are namespace aware we've introduced already in kubernetes the croisé stuff and then",
    "start": "2545280",
    "end": "2550790"
  },
  {
    "text": "another thing we're probably going to need is a way to feed Hardware features into the scheduler at least this is my",
    "start": "2550790",
    "end": "2555810"
  },
  {
    "text": "opinion so you can schedule against certain hardware resources like GPUs or FPGA s or whatever",
    "start": "2555810",
    "end": "2564170"
  },
  {
    "text": "so I heard clapping from the room next door must mean I'm over time",
    "start": "2564230",
    "end": "2570140"
  },
  {
    "text": "hopefully that was helpful to you guys questions any last questions",
    "start": "2571550",
    "end": "2578210"
  },
  {
    "text": "it's on our github so it's github slash openshift /s V T and again these slides",
    "start": "2592700",
    "end": "2599670"
  },
  {
    "text": "are going SlideShare and Twitter and a half an hour or something okay",
    "start": "2599670",
    "end": "2606500"
  },
  {
    "text": "anything else yes",
    "start": "2606500",
    "end": "2611240"
  },
  {
    "text": "in this testing",
    "start": "2613820",
    "end": "2617450"
  },
  {
    "text": "yeah so let me do one thing cig scale is the place to go for information about",
    "start": "2623870",
    "end": "2629090"
  },
  {
    "text": "all of this stuff SIG's gal suit scheduling in cig note if you're not participating in a COO Brunetti cig and you want to get into this stuff I would",
    "start": "2629090",
    "end": "2634280"
  },
  {
    "text": "highly recommend you go there in terms of the issues here where we're kind of slowly socializing the full list of",
    "start": "2634280",
    "end": "2641330"
  },
  {
    "text": "stuff that we want to add to kubernetes and so it's it's all sort of internal information now but we're gonna slowly",
    "start": "2641330",
    "end": "2647540"
  },
  {
    "text": "start publicizing that stuff and that's actually what this conference is about it's making those connections",
    "start": "2647540",
    "end": "2654160"
  },
  {
    "text": "so cool thanks everybody",
    "start": "2654160",
    "end": "2660150"
  },
  {
    "text": "[Applause]",
    "start": "2660150",
    "end": "2665569"
  }
]