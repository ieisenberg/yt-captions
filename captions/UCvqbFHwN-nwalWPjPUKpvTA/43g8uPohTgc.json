[
  {
    "start": "0",
    "end": "320000"
  },
  {
    "text": "hi everyone welcome to our session on group less auto skilling with carpenter my name is priti gogia and i am here",
    "start": "240",
    "end": "6640"
  },
  {
    "text": "with alistan we both work in the ek scalability team at aws",
    "start": "6640",
    "end": "12639"
  },
  {
    "text": "we have been working on the carpenter project for last few months and today we will be talking about the current auto",
    "start": "12639",
    "end": "18400"
  },
  {
    "text": "scaling landscape in kubernetes how carpenter works and a quick short demo",
    "start": "18400",
    "end": "25680"
  },
  {
    "text": "there are multiple ways a part can be created in a cluster users can use a clr tool like cube ctl",
    "start": "26800",
    "end": "33840"
  },
  {
    "text": "or there could be a cron job running or there could be an auto scaler tool such as hpa cada k native which",
    "start": "33840",
    "end": "40879"
  },
  {
    "text": "could add these part into the cluster once these spots are added they need resources to be able to run",
    "start": "40879",
    "end": "49440"
  },
  {
    "text": "these resources can either be added by a manual effort or by an auto scaler capacity planning",
    "start": "50000",
    "end": "56399"
  },
  {
    "text": "is hard and manual scaling can be slow error prone and cause significant delays as compared",
    "start": "56399",
    "end": "62719"
  },
  {
    "text": "to an auto scaler auto skiller has benefits over manual and today",
    "start": "62719",
    "end": "68000"
  },
  {
    "text": "cluster auto scaler has become the de facto for auto scaling in kubernetes",
    "start": "68000",
    "end": "75200"
  },
  {
    "text": "so what are the some of the benefits of cluster autoscaler as part scale up load increases in the",
    "start": "75200",
    "end": "82240"
  },
  {
    "text": "cluster cluster autoscaler can request more capacity from the underlying provider",
    "start": "82240",
    "end": "87759"
  },
  {
    "text": "and when the load reduces the cluster auto scaler can also request to terminate these instances from the",
    "start": "87759",
    "end": "94560"
  },
  {
    "text": "provider which in turn helps user reduce their infrastructure costs and remove the",
    "start": "94560",
    "end": "100320"
  },
  {
    "text": "burden of capacity planning there are other advantages such as closer autoscaler is vendor neutral so",
    "start": "100320",
    "end": "107520"
  },
  {
    "text": "has support for all the major crowd providers the approach is battle tested and widely",
    "start": "107520",
    "end": "112960"
  },
  {
    "text": "adopted and it works great for cluster sizes about 1000 nodes",
    "start": "112960",
    "end": "118719"
  },
  {
    "text": "so let's take a look at how closer autoscaler works cluster autoscaler works around this",
    "start": "118880",
    "end": "125040"
  },
  {
    "text": "concept of node groups which is an auto scaling group or an asg in",
    "start": "125040",
    "end": "130319"
  },
  {
    "text": "case of an in case of aws it looks for pending parts sets the",
    "start": "130319",
    "end": "136239"
  },
  {
    "text": "desired count of instances based on the number of pending parts on a node group",
    "start": "136239",
    "end": "142239"
  },
  {
    "text": "once the underlying provider provisions the capacity based on the desired count cube scheduler places these parts onto",
    "start": "142239",
    "end": "148879"
  },
  {
    "text": "these new nodes being created node group here plays critical role by",
    "start": "148879",
    "end": "154720"
  },
  {
    "text": "maintaining the desired number of nodes which are requested by the auto scaler",
    "start": "154720",
    "end": "161760"
  },
  {
    "text": "so let's try to understand what node group is node group is a logical grouping of nodes which can be auto scaled and",
    "start": "162000",
    "end": "168560"
  },
  {
    "text": "managed together cluster admins can create these multiple node groups specifying instance type options and",
    "start": "168560",
    "end": "175360"
  },
  {
    "text": "some other configurations and also set a minimum and a max node count",
    "start": "175360",
    "end": "180959"
  },
  {
    "text": "node groups are not auto cell feeling it's done by an auto skillet by setting",
    "start": "180959",
    "end": "186159"
  },
  {
    "text": "the desired maximum",
    "start": "186159",
    "end": "189280"
  },
  {
    "text": "there are some challenges with this approach though let's see what are those challenges",
    "start": "192640",
    "end": "198319"
  },
  {
    "text": "crystal autoscaler assumes that the instance types are all identical in a given group",
    "start": "198319",
    "end": "204720"
  },
  {
    "text": "and if you want to use multiple instance types you need to create separate groups and",
    "start": "204720",
    "end": "210560"
  },
  {
    "text": "furthermore if you need to spread parts across acs multi multiply the number of groups by",
    "start": "210560",
    "end": "217680"
  },
  {
    "text": "each a z some of the other challenges here are if there is an error creating a node in a",
    "start": "217680",
    "end": "223280"
  },
  {
    "text": "node group cluster or scalar will not know until it times out and this causes delays for an application when a part",
    "start": "223280",
    "end": "229360"
  },
  {
    "text": "stays pending there are users creating clusters larger than thousand nodes today",
    "start": "229360",
    "end": "237040"
  },
  {
    "text": "but there have been some efforts from the community but unfortunately either they are deprecated or not maintained",
    "start": "237200",
    "end": "242879"
  },
  {
    "text": "and one of such efforts is from cerebral which is cerebral which was created because",
    "start": "242879",
    "end": "249280"
  },
  {
    "text": "cluster order scale was slow another one is escalator which is a batch workload specific",
    "start": "249280",
    "end": "255439"
  },
  {
    "text": "and works on a selected set of groups in tandem with the cluster autoscaler to",
    "start": "255439",
    "end": "262479"
  },
  {
    "text": "fix some of the challenges we just discussed another interesting approach is from",
    "start": "262479",
    "end": "267919"
  },
  {
    "text": "zelando which tries to improve and build upon cluster autoscaler they improved how",
    "start": "267919",
    "end": "274000"
  },
  {
    "text": "cluster autoscaler generates template nodes for groups to decide what kind of node",
    "start": "274000",
    "end": "279199"
  },
  {
    "text": "to expect when scaling up they added reliable back of logic in case of",
    "start": "279199",
    "end": "285600"
  },
  {
    "text": "in case a node group phase to scale up nodes and they made some more additional changes and some of their efforts",
    "start": "285600",
    "end": "292400"
  },
  {
    "text": "which have been upstreamed to cluster order scaler which makes it more robust and production ready",
    "start": "292400",
    "end": "298080"
  },
  {
    "text": "but one thing to observe here is all these different approaches are trying to solve around node groups",
    "start": "298080",
    "end": "304960"
  },
  {
    "text": "keeping these challenges in mind now i would like to hand it over to alice to talk about if",
    "start": "304960",
    "end": "310720"
  },
  {
    "text": "we can do any better in terms of auto scaling",
    "start": "310720",
    "end": "315599"
  },
  {
    "text": "thanks pratik so we spent a fair amount of time in",
    "start": "317840",
    "end": "323280"
  },
  {
    "start": "320000",
    "end": "514000"
  },
  {
    "text": "this node auto scaling space and we're trying to understand what was exactly the root cause of all these issues",
    "start": "323280",
    "end": "329680"
  },
  {
    "text": "some of them were just code bugs in our cloud provider implementation things that we could fix you know things that we did fix",
    "start": "329680",
    "end": "336160"
  },
  {
    "text": "sometimes it was just customer configuration since these things are really hard to get right it's really easy to misconfigure",
    "start": "336160",
    "end": "341840"
  },
  {
    "text": "uh node auto scaling it's one of the things that we we decided we really wanted to focus on to make easier",
    "start": "341840",
    "end": "347520"
  },
  {
    "text": "and finally part of it was just architectural by nature of having node groups you",
    "start": "347520",
    "end": "352560"
  },
  {
    "text": "you you take on some assumptions and that was where we decided that a lot of",
    "start": "352560",
    "end": "358000"
  },
  {
    "text": "the inefficiencies were coming from so we really started to play with this idea of what would happen if you remove",
    "start": "358000",
    "end": "364160"
  },
  {
    "text": "the group what if we provisioned capacity directly the idea is that when some pending pods",
    "start": "364160",
    "end": "371199"
  },
  {
    "text": "come in you can compute the resource requirements of all those pods just like",
    "start": "371199",
    "end": "376240"
  },
  {
    "text": "the cluster auto scaler might but instead of simulating from an existing set of static templates of",
    "start": "376240",
    "end": "382160"
  },
  {
    "text": "a set of nodes that could be created you just ask ec2 for exactly the instance that you want",
    "start": "382160",
    "end": "388080"
  },
  {
    "text": "so we know all of the instance types we can simulate those pods on you know we can pick an m5 large or or",
    "start": "388080",
    "end": "394080"
  },
  {
    "text": "any instance that makes sense for those those pods whether or not it's been seen before and additionally the",
    "start": "394080",
    "end": "399680"
  },
  {
    "text": "the other node properties like the availability zone and the operating system all of these things we can compute dynamically so we've shifted from this",
    "start": "399680",
    "end": "406639"
  },
  {
    "text": "static template world to this dynamically generated template world which which reduces the configuration",
    "start": "406639",
    "end": "413599"
  },
  {
    "text": "complexity pretty significantly so once we've decided what instance we want we just tell our cloud provider",
    "start": "413599",
    "end": "419520"
  },
  {
    "text": "please give us this instance of course one of the value props of groups is they",
    "start": "419520",
    "end": "425280"
  },
  {
    "text": "they track nodes so that you can you know you can roll out a change or you can delete them all at once uh so we decided to track nodes using",
    "start": "425280",
    "end": "432000"
  },
  {
    "text": "kubernetes labels which we thought was a little bit more kubernetes friendly and native to the other tooling that you're using",
    "start": "432000",
    "end": "438960"
  },
  {
    "text": "so the result of all this is you have pretty significantly reduced configuration burden we've seen customers deploying over 50",
    "start": "438960",
    "end": "445199"
  },
  {
    "text": "asgs in production which is the cross product of instance type capacity type availability zone and scheduling",
    "start": "445199",
    "end": "451280"
  },
  {
    "text": "properties which can blow up really quickly so you just don't need any of those anymore it's all done dynamically",
    "start": "451280",
    "end": "458240"
  },
  {
    "text": "also the api load is pretty significantly reduced since for every single one of those asgs",
    "start": "458240",
    "end": "463280"
  },
  {
    "text": "you have to ask the cloud provider uh you know what is what is the current status of this thing what is the what is the value",
    "start": "463280",
    "end": "469360"
  },
  {
    "text": "and those api requests really they really stack up uh and you can run into api",
    "start": "469360",
    "end": "475360"
  },
  {
    "text": "limits and and the whole system grinds to a halt so instead we just make calls when we're creating and deleting capacity",
    "start": "475360",
    "end": "481360"
  },
  {
    "text": "and otherwise we rely on the kubernetes api server which can support much higher throughput",
    "start": "481360",
    "end": "487520"
  },
  {
    "text": "it also reduces the simulation complexity we no longer have to to look at such a large number of",
    "start": "487520",
    "end": "493440"
  },
  {
    "text": "templates we can collapse the simulation space to just instance types because we can prescribe",
    "start": "493440",
    "end": "499039"
  },
  {
    "text": "the availability zone and scheduling properties which really simplifies things for example instead of looking at for at",
    "start": "499039",
    "end": "506000"
  },
  {
    "text": "all the possible groups and saying which one has this zone we can just tell our cloud provider run",
    "start": "506000",
    "end": "511039"
  },
  {
    "text": "an instance in this zone so we ended up with this architecture",
    "start": "511039",
    "end": "516800"
  },
  {
    "start": "514000",
    "end": "605000"
  },
  {
    "text": "which should seem fairly familiar if you're familiar with the cluster auto scaler we start up at the top left where you",
    "start": "516800",
    "end": "522000"
  },
  {
    "text": "have pods that were created and haven't been scheduled yet and the coupe scheduler gets the first crack at it so it looks at the existing",
    "start": "522000",
    "end": "528480"
  },
  {
    "text": "capacity and the and the pending pods and it tries to find places to schedule those pots",
    "start": "528480",
    "end": "534240"
  },
  {
    "text": "and if for some reason their resources aren't available to schedule them the coop scheduler sets a status",
    "start": "534240",
    "end": "539839"
  },
  {
    "text": "condition in the pod that the pod cannot be schedulable and that's when we kick in",
    "start": "539839",
    "end": "544959"
  },
  {
    "text": "so we have a process called the allocator that looks at pods that have that unscheduled status condition and then we attempt to create",
    "start": "544959",
    "end": "552000"
  },
  {
    "text": "new capacity that can fit those pods we try to make sure that capacity is",
    "start": "552000",
    "end": "558240"
  },
  {
    "text": "as efficiently packed as possible and that we can bring it up as quickly as possible and then that capacity joins the cluster",
    "start": "558240",
    "end": "563920"
  },
  {
    "text": "so your existing capacity and your provision capacity just now or your new cluster state over",
    "start": "563920",
    "end": "569600"
  },
  {
    "text": "time you can get you can get holes in the capacity you can get inefficient utilization so we have another controller that looks",
    "start": "569600",
    "end": "576240"
  },
  {
    "text": "globally at the cluster and attempts to optimize it over time",
    "start": "576240",
    "end": "581600"
  },
  {
    "text": "so where we end up with is two controllers one is a fact fast acting latency sensitive controller",
    "start": "582160",
    "end": "588560"
  },
  {
    "text": "the allocator and the other is a slow acting cost sensitive controller called the reallocator",
    "start": "588560",
    "end": "593680"
  },
  {
    "text": "and together these work to both schedule pods as quickly as possible",
    "start": "593680",
    "end": "599040"
  },
  {
    "text": "and optimize the resource utilization of your cluster",
    "start": "599040",
    "end": "605839"
  },
  {
    "start": "605000",
    "end": "750000"
  },
  {
    "text": "so there's a couple ways to launch capacity directly well there's many ways to launch capacity in ec2 at least so previously",
    "start": "606480",
    "end": "613279"
  },
  {
    "text": "we were relying on asg we gave it a template and we gave it an instance count and it was responsible",
    "start": "613279",
    "end": "618720"
  },
  {
    "text": "for launching all of those instances now we've removed asg and so we want to use ec2's",
    "start": "618720",
    "end": "624160"
  },
  {
    "text": "direct capacity launch apis there's two of these one is called fleet and the other is called run",
    "start": "624160",
    "end": "629360"
  },
  {
    "text": "instances they're roughly the same we could actually use either of them and i imagine that other cloud providers",
    "start": "629360",
    "end": "636560"
  },
  {
    "text": "they might have multiple options as well we know that every cloud provider does have the ability to run instances in general",
    "start": "636560",
    "end": "643680"
  },
  {
    "text": "but we for our use case and for our cloud provider we select the fleet api because we think that it gives us the most flexibility so",
    "start": "643680",
    "end": "650720"
  },
  {
    "text": "for example once when deciding what node we want to create for a given set of pods we don't actually limit",
    "start": "650720",
    "end": "657600"
  },
  {
    "text": "ourselves to one availability zone and one instance type and this is constrained to our cloud provider's implementation you could",
    "start": "657600",
    "end": "664000"
  },
  {
    "text": "have another implementation that selected the specific instance type in the specific availability zone",
    "start": "664000",
    "end": "669279"
  },
  {
    "text": "but we actually can get it some additional value by being flexible so we tell ec2",
    "start": "669279",
    "end": "674959"
  },
  {
    "text": "these pods will fit in any of these instance types and given their topology constraints",
    "start": "674959",
    "end": "680160"
  },
  {
    "text": "it'll run in any of these availability zones and by giving it that flexibility ec2 can actually pick the cheapest",
    "start": "680160",
    "end": "687040"
  },
  {
    "text": "instances given the constraints the more flexibility you give easy to the better of a job that can do",
    "start": "687040",
    "end": "692720"
  },
  {
    "text": "to pick the right instance type given your constraints so it's almost like it's almost like a run instances request",
    "start": "692720",
    "end": "699360"
  },
  {
    "text": "that's been expanded and made more flexible but at the end of the day you are still getting one node that those",
    "start": "699360",
    "end": "704640"
  },
  {
    "text": "pods get packed onto so we get pretty massively increased node flexibility here we get this really",
    "start": "704640",
    "end": "710480"
  },
  {
    "text": "nice failover behavior if a particular instance type is out of stock",
    "start": "710480",
    "end": "715519"
  },
  {
    "text": "it also pretty significantly reduces the node provisioning latency so systems systems like auto scaling",
    "start": "715519",
    "end": "721680"
  },
  {
    "text": "groups and and other group systems there's there's a little bit of overhead to them so there's some reconciliation",
    "start": "721680",
    "end": "727600"
  },
  {
    "text": "loop that's happening in the implementation of that thing and you know maybe it's really fast or maybe it's not very fast but",
    "start": "727600",
    "end": "733279"
  },
  {
    "text": "we found that with asg it was adding about 30 seconds from the time that you increment the node count to the time that the node is",
    "start": "733279",
    "end": "739839"
  },
  {
    "text": "brought up so by removing that and asking ec2 to create fleet directly",
    "start": "739839",
    "end": "745200"
  },
  {
    "text": "we get instances 30 seconds faster which is a pretty big deal",
    "start": "745200",
    "end": "750079"
  },
  {
    "start": "750000",
    "end": "909000"
  },
  {
    "text": "scheduling is also critical to the provisioning workflow so like i mentioned earlier we work in tandem with the cube scheduler we give",
    "start": "751279",
    "end": "757839"
  },
  {
    "text": "the coupe scheduler the first shot and when it fails we kick in afterwards but one of the corollaries to this",
    "start": "757839",
    "end": "763680"
  },
  {
    "text": "is that is the realization that provisioning decisions are scheduling decisions by nature of picking a new node for pods",
    "start": "763680",
    "end": "770639"
  },
  {
    "text": "to schedule onto you are making a scheduling decision whether or not you like it or whether you like it or not it's it's",
    "start": "770639",
    "end": "776560"
  },
  {
    "text": "it's unavoidable it's unavoidable and so one of the differences that we were able to do because we're creating",
    "start": "776560",
    "end": "782880"
  },
  {
    "text": "the capacity directly is we can actually create the node objects so rather",
    "start": "782880",
    "end": "788000"
  },
  {
    "text": "we make the the create instance request we get an instance id back we're able to create the node object",
    "start": "788000",
    "end": "794000"
  },
  {
    "text": "immediately and then bind the pods immediately and so we've enforced a scheduling",
    "start": "794000",
    "end": "801120"
  },
  {
    "text": "decision kind of outside of the loop of the kube scheduler and that's a fairly interesting pivot um from the way that",
    "start": "801120",
    "end": "807920"
  },
  {
    "text": "the cluster autoscaler works which is it brings up the new capacity and then waits for the kube scheduler to do the",
    "start": "807920",
    "end": "813279"
  },
  {
    "text": "scheduling but we decided that if we're already making a scheduling decision we might as",
    "start": "813279",
    "end": "818560"
  },
  {
    "text": "well enforce it and there are some benefits to that so for example because we've already",
    "start": "818560",
    "end": "824959"
  },
  {
    "text": "bound the pod the second that the node comes online you can start pulling the image because",
    "start": "824959",
    "end": "830079"
  },
  {
    "text": "you already know the binding decision you don't have to wait for the node to become ready you don't have to wait for the cube",
    "start": "830079",
    "end": "835199"
  },
  {
    "text": "scheduler to schedule it so that was just kind of a nice dual optimization we did so one is we enforced that the decision",
    "start": "835199",
    "end": "841600"
  },
  {
    "text": "that we made is actually in is actually uh it actually happens in the cluster and",
    "start": "841600",
    "end": "847519"
  },
  {
    "text": "we also get some you know about five seconds of latency improvement um which is which is quite nice",
    "start": "847519",
    "end": "854000"
  },
  {
    "text": "the other thing that we realized is uh that the cluster auto scaler is limited in its version compatibility so they",
    "start": "854000",
    "end": "861120"
  },
  {
    "text": "copy this coupe scheduler directly into their code base and use it and so because of this there's a",
    "start": "861120",
    "end": "866639"
  },
  {
    "text": "recommendation that the coupe scheduler must be the same in the cluster auto scalers code base as",
    "start": "866639",
    "end": "872480"
  },
  {
    "text": "as you have running upstream or as you have running in your masters rather and so this this creates a",
    "start": "872480",
    "end": "878480"
  },
  {
    "text": "tight version coupling between those two things because they're not enforcing the the binding decision and so",
    "start": "878480",
    "end": "885680"
  },
  {
    "text": "by doing this binding binding decision we now free ourselves from this version",
    "start": "885680",
    "end": "891040"
  },
  {
    "text": "compatibility because the the coupe scheduler is its own system that has its own set of responsibilities",
    "start": "891040",
    "end": "896320"
  },
  {
    "text": "and the carpenter allocator scheduler is is is the same it's it also has its own responsibilities and",
    "start": "896320",
    "end": "903199"
  },
  {
    "text": "so you no longer have this type coupling and versions of carpenter can work across many versions of kubernetes",
    "start": "903199",
    "end": "910079"
  },
  {
    "start": "909000",
    "end": "986000"
  },
  {
    "text": "another core layer we have here is the scheduling api conformance and so by nature of writing a scheduler",
    "start": "910399",
    "end": "916000"
  },
  {
    "text": "or writing logic that decides new nodes to provision we have to support all the common things like resource requests and",
    "start": "916000",
    "end": "922000"
  },
  {
    "text": "selectors and affinity and topology um and so this is this is just kind of a burden that we take on as part of trying",
    "start": "922000",
    "end": "928480"
  },
  {
    "text": "to play in the salt in this problem space um and specifically the resource requests need to are get a little bit",
    "start": "928480",
    "end": "935040"
  },
  {
    "text": "complicated so when you have uh i'm sure you guys are familiar with running gpus or running potentially",
    "start": "935040",
    "end": "942079"
  },
  {
    "text": "other custom instance or custom resource types your cloud provider effectively needs to have",
    "start": "942079",
    "end": "947920"
  },
  {
    "text": "knowledge of how to launch instances with those particular instance types or resource types sorry",
    "start": "947920",
    "end": "955199"
  },
  {
    "text": "and so there's kind of an ongoing burden here of the cloud providers to ensure that the uh the particular",
    "start": "955199",
    "end": "962639"
  },
  {
    "text": "properties of the instances that the pods are requesting are actually able to be launched and we've built a bunch of machinery",
    "start": "962639",
    "end": "968639"
  },
  {
    "text": "that's that's vendor neutral that supports the node selectors and all of the scheduling uh the the non-non-vendor",
    "start": "968639",
    "end": "975199"
  },
  {
    "text": "specific scheduling constraints but there is there is somewhat of an ongoing maintenance burden um as as new types of",
    "start": "975199",
    "end": "981519"
  },
  {
    "text": "resources enter the enter the scene",
    "start": "981519",
    "end": "985680"
  },
  {
    "start": "986000",
    "end": "1034000"
  },
  {
    "text": "we also operate based off of this concept of well-known labels so for example uh this is how we",
    "start": "987199",
    "end": "992480"
  },
  {
    "text": "communicate that a pod needs to run in a particular zone or in a particular instance type",
    "start": "992480",
    "end": "998480"
  },
  {
    "text": "there's this concept that if in your pods node selector you use for example topology dot",
    "start": "998480",
    "end": "1004079"
  },
  {
    "text": "kubernetes dot io zone and you state that zone that's a signal to carpenter that the the node that you",
    "start": "1004079",
    "end": "1011680"
  },
  {
    "text": "need must be constrained that then it must have a label of that zone so we use these well-known labels to",
    "start": "1011680",
    "end": "1017600"
  },
  {
    "text": "apply additional constraints there's also a set of defaults so that if you don't you don't define these or",
    "start": "1017600",
    "end": "1024160"
  },
  {
    "text": "you don't have to define these in all cases but if you do need to do apply additional constraints",
    "start": "1024160",
    "end": "1029918"
  },
  {
    "text": "you would do that via node selectors and well-known labels",
    "start": "1029919",
    "end": "1034640"
  },
  {
    "text": "so once we've sorted out our scheduling you end up with uh kind of groups of pods imagine you",
    "start": "1035520",
    "end": "1041120"
  },
  {
    "text": "have a hundred pods that need to be scheduled and some of them have one node selector and some of them have another node",
    "start": "1041120",
    "end": "1046798"
  },
  {
    "text": "selector you end up with these what we're calling constraint groups and these are pods that that can be scheduled together",
    "start": "1046799",
    "end": "1053360"
  },
  {
    "text": "according to their scheduling constraints but we still we have another problem here which is how do we pick the instance",
    "start": "1053360",
    "end": "1060080"
  },
  {
    "text": "that can fit all of those pods so we sum up all of the resource requests of the pods",
    "start": "1060080",
    "end": "1065600"
  },
  {
    "text": "and then we do what's called bin packing this is a classical computer science np complete problem",
    "start": "1065600",
    "end": "1071039"
  },
  {
    "text": "there's a couple variants the one that this is is called online bin packing which means we're looking at the subset of the",
    "start": "1071039",
    "end": "1076960"
  },
  {
    "text": "overall pods in the cluster and we're attempting to pack them into the existing capacity",
    "start": "1076960",
    "end": "1082960"
  },
  {
    "text": "there's a bunch of research in this space and we're actually super excited for all the innovation we can do here",
    "start": "1082960",
    "end": "1088080"
  },
  {
    "text": "but the algorithm that we've picked right now is called first fit descending and what that basically means",
    "start": "1088080",
    "end": "1093120"
  },
  {
    "text": "is we sort the pods by size and we try to pack the pods biggest to smallest and what",
    "start": "1093120",
    "end": "1099760"
  },
  {
    "text": "this does is it kind of has this nice effect it's called first fit descending has this nice effect where",
    "start": "1099760",
    "end": "1105200"
  },
  {
    "text": "you attempt to pack pods and if you ever can't as you go down this list this descending list if you ever can't fit a",
    "start": "1105200",
    "end": "1111039"
  },
  {
    "text": "pod you set it aside and you go smaller and smaller and smaller and so as you get to the smaller pods",
    "start": "1111039",
    "end": "1117360"
  },
  {
    "text": "they tend to fit in the gaps that you've created by starting with the largest one and this is actually there are some great",
    "start": "1117360",
    "end": "1123039"
  },
  {
    "text": "analogs here to actual uh container shipping and and uh how uh containers are are packed in that",
    "start": "1123039",
    "end": "1130720"
  },
  {
    "text": "world um but the the the main idea is that",
    "start": "1130720",
    "end": "1136960"
  },
  {
    "text": "you take a set of pods that can all be scheduled on a set of nodes together they have the same scheduling constraints",
    "start": "1136960",
    "end": "1143120"
  },
  {
    "text": "and the result is you end up with a set of nodes that can fit those pods",
    "start": "1143120",
    "end": "1148320"
  },
  {
    "text": "you might ask well how do you order the pods because it's not a single dimensional problem",
    "start": "1148320",
    "end": "1153520"
  },
  {
    "text": "there's a couple approaches to this you can either prioritize cpu or memory or do the euclidean which is",
    "start": "1153520",
    "end": "1158799"
  },
  {
    "text": "the the sum of or the root of the sum of the squares um and so we found that the euclidean is is",
    "start": "1158799",
    "end": "1165520"
  },
  {
    "text": "kind of a best effort simulation of kind of weighting memory and cpu equally",
    "start": "1165520",
    "end": "1171360"
  },
  {
    "text": "when when sorting them and that it tends to work fairly well for bin packing but like i said earlier we're really excited",
    "start": "1171360",
    "end": "1177280"
  },
  {
    "text": "to innovate in the space and figure out better algorithms we've talked a little bit about genetic algorithms which could be really",
    "start": "1177280",
    "end": "1183360"
  },
  {
    "text": "interesting and there's a whole field of research here",
    "start": "1183360",
    "end": "1187919"
  },
  {
    "text": "we're not just responsible for bringing up nodes we're also responsible for terminating them it's you know what what goes up must come",
    "start": "1189840",
    "end": "1196160"
  },
  {
    "text": "down uh and so we we have we have another controller it's part of the or rather it's part of the reallocator controller",
    "start": "1196160",
    "end": "1202960"
  },
  {
    "text": "we look at nodes that don't have any pods scheduled to them anymore and we apply a ttl and if those pods if those nodes don't",
    "start": "1202960",
    "end": "1208880"
  },
  {
    "text": "get any pod scheduled to them for a period of time which we default to five minutes those nodes are eventually scaled down",
    "start": "1208880",
    "end": "1215280"
  },
  {
    "text": "we also respond to other events that might cause instances to be terminated so for example when ec2 notices that your virtual",
    "start": "1215280",
    "end": "1222720"
  },
  {
    "text": "machine is unhealthy for some reason it'll send an event and so we respond to those and we we drain the node as",
    "start": "1222720",
    "end": "1228559"
  },
  {
    "text": "quickly as possible and try to try to heal the situation similarly",
    "start": "1228559",
    "end": "1234320"
  },
  {
    "text": "there's a couple of other signals called spot rebalance and spot termination rebalance is kind of a heads up from",
    "start": "1234320",
    "end": "1240320"
  },
  {
    "text": "spot that your node is going to be preempted and so you can do your best to drain and reschedule everything",
    "start": "1240320",
    "end": "1246080"
  },
  {
    "text": "termination happens after the rebalance so you're not guaranteed to get a rebalance request but you",
    "start": "1246080",
    "end": "1251360"
  },
  {
    "text": "typically do you are guaranteed to get a termination signal and that termination signal means you have two minutes to",
    "start": "1251360",
    "end": "1256799"
  },
  {
    "text": "drain so ideally we've we've managed to terminate the nodes and reschedule everything by the rebalance but if for some reason",
    "start": "1256799",
    "end": "1263840"
  },
  {
    "text": "it happens so quickly or something's getting gummed up then it has to be done before that that node termination",
    "start": "1263840",
    "end": "1270730"
  },
  {
    "text": "[Music] the other thing that we've we've realized is that you can apply a node ttl of",
    "start": "1270730",
    "end": "1276320"
  },
  {
    "text": "a long time frame so for example there's there's some value in saying i want this node to live for 90 days and",
    "start": "1276320",
    "end": "1284000"
  },
  {
    "text": "this is super useful in the upgrade case as long as our allocator is creating nodes on the latest kubernetes version",
    "start": "1284000",
    "end": "1290159"
  },
  {
    "text": "and as long as the nodes only live for 90 days your cluster is basically always up to date given the kubernetes",
    "start": "1290159",
    "end": "1296640"
  },
  {
    "text": "three-month release cycle there's some other compliance use cases where you might care about how long the",
    "start": "1296640",
    "end": "1301840"
  },
  {
    "text": "node lives maybe you want to roll your fleet every month or so but this is another feature that we've built in that",
    "start": "1301840",
    "end": "1307200"
  },
  {
    "text": "that we think will be super useful for upgrade you know potentially weird memory leaks potentially compliance cases of course",
    "start": "1307200",
    "end": "1314559"
  },
  {
    "text": "you can not use the ttl and your nodes will live forever",
    "start": "1314559",
    "end": "1319840"
  },
  {
    "start": "1320000",
    "end": "1387000"
  },
  {
    "text": "so previously i mentioned that we will only drain nodes that are empty that don't have any pods on them",
    "start": "1321200",
    "end": "1326960"
  },
  {
    "text": "but you could also imagine that over time most of the pods get deleted but not all of them and this can end up with what we call",
    "start": "1326960",
    "end": "1333120"
  },
  {
    "text": "defragmentation where you have nodes that are really poorly utilized and this is where this this reallocator",
    "start": "1333120",
    "end": "1339039"
  },
  {
    "text": "process kicks in in addition to the really easy case where we just scale down an empty node",
    "start": "1339039",
    "end": "1345120"
  },
  {
    "text": "we can look at poorly utilized nodes and ask the question is there a better selection of nodes",
    "start": "1345120",
    "end": "1351520"
  },
  {
    "text": "within the cluster that we can replace our current nodes with that will more efficiently pack these pods",
    "start": "1351520",
    "end": "1357520"
  },
  {
    "text": "this is actually a really challenging problem because pods are immutable they can't be moved you have to be really",
    "start": "1357520",
    "end": "1362880"
  },
  {
    "text": "careful about deleting a pod and waiting for the new one to be created and then enforcing your repacking so",
    "start": "1362880",
    "end": "1368240"
  },
  {
    "text": "this is still very much in the design phase we're still trying to figure out how to do this but we're committed to the idea that you",
    "start": "1368240",
    "end": "1374559"
  },
  {
    "text": "shouldn't be stuck with the the initial state that things were provisioned in you should have another controller",
    "start": "1374559",
    "end": "1380000"
  },
  {
    "text": "that's always monitoring always trying to improve the situation so we're exploring different algorithms",
    "start": "1380000",
    "end": "1386640"
  },
  {
    "text": "here also one of the most important pieces of",
    "start": "1386840",
    "end": "1392320"
  },
  {
    "start": "1387000",
    "end": "1475000"
  },
  {
    "text": "this problem is one that doesn't really come up very often but we realized is absolutely critical it's how quickly are these nodes",
    "start": "1392320",
    "end": "1398720"
  },
  {
    "text": "launched there's a whole bunch of steps they're all listed here in in the",
    "start": "1398720",
    "end": "1404240"
  },
  {
    "text": "critical path of a node coming online and right now in aws that's about two minutes in different clouds i think it's",
    "start": "1404240",
    "end": "1410799"
  },
  {
    "text": "it's actually fairly common for it to be about two minutes and when it's this slow you end up with this use case where",
    "start": "1410799",
    "end": "1416480"
  },
  {
    "text": "you you actually want to over provision you can't wait those two minutes so you'd like extra nodes to be in the",
    "start": "1416480",
    "end": "1421760"
  },
  {
    "text": "cluster so that the the pods can be more scheduled more rapidly and so we've decided to really attack",
    "start": "1421760",
    "end": "1428240"
  },
  {
    "text": "this problem we think we can get it down to maybe 50 to 30 or 15 seconds if we do a ton of",
    "start": "1428240",
    "end": "1433360"
  },
  {
    "text": "work and so we're really excited to to attack this this latency problem",
    "start": "1433360",
    "end": "1438880"
  },
  {
    "text": "which we think will mitigate the need to do over provisioning of course you know there are going to be",
    "start": "1438880",
    "end": "1444320"
  },
  {
    "text": "use cases where you need to over provision which we recommend doing at the pod level and in the very worst",
    "start": "1444320",
    "end": "1449440"
  },
  {
    "text": "case scenario if you can't over provision at the pod level and you can't tolerate um the the latency at whatever we've",
    "start": "1449440",
    "end": "1455279"
  },
  {
    "text": "optimized for the point in time we recommend that you just statically provision your capacity",
    "start": "1455279",
    "end": "1463278"
  },
  {
    "text": "of course we're always open to new ideas and we've actually had some discussions about some sort of feature about leaving",
    "start": "1463840",
    "end": "1470240"
  },
  {
    "text": "headroom but that's not fully baked yet",
    "start": "1470240",
    "end": "1474720"
  },
  {
    "start": "1475000",
    "end": "1503000"
  },
  {
    "text": "i also want to mention quickly that we do this using kubernetes custom resources this is kind of the modern standard way",
    "start": "1476320",
    "end": "1482640"
  },
  {
    "text": "to write controllers and kubernetes we found that using command line arguments and a single global operator is kind of",
    "start": "1482640",
    "end": "1489440"
  },
  {
    "text": "untenable so all of these configuration parameters the taints the labels the cluster that the new nodes connect",
    "start": "1489440",
    "end": "1496320"
  },
  {
    "text": "to all of these things are encoded in a custom resource",
    "start": "1496320",
    "end": "1502240"
  },
  {
    "start": "1503000",
    "end": "1558000"
  },
  {
    "text": "this project is of course open source and vendor neutral it's currently incubating in the aws",
    "start": "1505120",
    "end": "1510480"
  },
  {
    "text": "labs github repo and we're excited to figure out where it's going to go after that",
    "start": "1510480",
    "end": "1516159"
  },
  {
    "text": "um we have a roadmap that you should come check out to see our 0.2 0.3 and",
    "start": "1516159",
    "end": "1521520"
  },
  {
    "text": "0.4 releases that we have planned we're planning on executing about the rate that kubernetes does so releasing",
    "start": "1521520",
    "end": "1528320"
  },
  {
    "text": "as the crew as kubernetes releases and the other thing is we have a bi-weekly working group where",
    "start": "1528320",
    "end": "1533919"
  },
  {
    "text": "people come from segato scaling or from our customer base or or really just people who are curious and we we talk",
    "start": "1533919",
    "end": "1540240"
  },
  {
    "text": "about the project and talk about things that we could do to make it better or prioritize features",
    "start": "1540240",
    "end": "1546000"
  },
  {
    "text": "or design reviews so if you're interested in that we'd love to see you",
    "start": "1546000",
    "end": "1551760"
  },
  {
    "text": "we'd love to see you drop by there's a qr code here if you're interested",
    "start": "1551760",
    "end": "1557840"
  },
  {
    "start": "1558000",
    "end": "2180000"
  },
  {
    "text": "and now i'll hand it off to petite who's got a demo for us",
    "start": "1558960",
    "end": "1563520"
  },
  {
    "text": "thanks alice so let's go ahead and see some of the concepts that alice has explained in action i have already provisioned an",
    "start": "1567360",
    "end": "1574960"
  },
  {
    "text": "eks cluster it's running a single node and i have also deployed carpenter",
    "start": "1574960",
    "end": "1581440"
  },
  {
    "text": "onto these on this one node so at the moment we have carpenter running some of the components which carpenter",
    "start": "1581440",
    "end": "1588080"
  },
  {
    "text": "needs and some of the system parts",
    "start": "1588080",
    "end": "1592879"
  },
  {
    "text": "so as a next step let's go ahead and keep a watch on all the nodes on this cluster at this",
    "start": "1593120",
    "end": "1599360"
  },
  {
    "text": "point we are ready to configure carpenter to be able to talk to the api server",
    "start": "1599360",
    "end": "1606480"
  },
  {
    "text": "so these are the configurations that we need to provide to carpenter to be able to talk to the api server",
    "start": "1606480",
    "end": "1612960"
  },
  {
    "text": "and the other configurations i have here are optional we provide a list of instance types to",
    "start": "1612960",
    "end": "1619360"
  },
  {
    "text": "limit the num the instance types carpenter can use when requesting for the capacity we are also limiting",
    "start": "1619360",
    "end": "1626720"
  },
  {
    "text": "the zones in which carpenter can create the capacity and detail seconds tell",
    "start": "1626720",
    "end": "1632720"
  },
  {
    "text": "carpenter for how long to wait when it detects an un underutilized nodes before it terminates",
    "start": "1632720",
    "end": "1638799"
  },
  {
    "text": "the node so let's go ahead and create the crd object which carpenter can",
    "start": "1638799",
    "end": "1644720"
  },
  {
    "text": "watch and and using all this information carpenter will be able to talk to the api server",
    "start": "1644720",
    "end": "1652159"
  },
  {
    "text": "so at this point we are ready to add some workload into the cluster but since we have these",
    "start": "1652159",
    "end": "1658880"
  },
  {
    "text": "this node tainted and it's only running system critical components no other workload can be scheduled on",
    "start": "1658880",
    "end": "1664799"
  },
  {
    "text": "this one node we will need additional capacity added to the cluster",
    "start": "1664799",
    "end": "1670240"
  },
  {
    "text": "so let's go ahead and add a deployment into the cluster",
    "start": "1670240",
    "end": "1675279"
  },
  {
    "text": "this deployment contains a single container and replicas right now are zero and it",
    "start": "1678880",
    "end": "1684799"
  },
  {
    "text": "uses a pause image with cpu request for one core",
    "start": "1684799",
    "end": "1689919"
  },
  {
    "text": "and before we start to scale up let's keep a watch on the pods which are",
    "start": "1689919",
    "end": "1695120"
  },
  {
    "text": "getting created as part of this let's also go ahead and tell the carpenter logs to see",
    "start": "1695120",
    "end": "1701919"
  },
  {
    "text": "the actions carpenter is taking at this point we are ready to scale up",
    "start": "1701919",
    "end": "1707600"
  },
  {
    "text": "our workloads and let's start with a single replica so once we create the replica",
    "start": "1707600",
    "end": "1714080"
  },
  {
    "text": "what's going to happen is this particular part is going to go in the pending state because there is not enough capacity in the",
    "start": "1714080",
    "end": "1720799"
  },
  {
    "text": "cluster and carpenter will request for more capacity from the underlying provider which is",
    "start": "1720799",
    "end": "1726480"
  },
  {
    "text": "aws in this case so as we scaled up the part went into",
    "start": "1726480",
    "end": "1732159"
  },
  {
    "text": "the pending state",
    "start": "1732159",
    "end": "1734960"
  },
  {
    "text": "and carpenter detected that there is one part which is pending and it requested a node from ec2",
    "start": "1737279",
    "end": "1744559"
  },
  {
    "text": "and it binded this one part on this particular node so let's give it few seconds and this is",
    "start": "1744559",
    "end": "1750240"
  },
  {
    "text": "the node which is coming up and in another few seconds we should be able to see the version",
    "start": "1750240",
    "end": "1756080"
  },
  {
    "text": "instance type zone and architecture so here it is we are running on an x large in usb 2c",
    "start": "1756080",
    "end": "1763520"
  },
  {
    "text": "and amd64 and as soon as the note comes up the",
    "start": "1763520",
    "end": "1768559"
  },
  {
    "text": "container will start getting deployed and will be running in few seconds",
    "start": "1768559",
    "end": "1774320"
  },
  {
    "text": "so in about 49 seconds we had the container running on this node which was requested by",
    "start": "1776799",
    "end": "1783679"
  },
  {
    "text": "carpenter because there was a pending point let's try to add",
    "start": "1783679",
    "end": "1788720"
  },
  {
    "text": "two more parts of this in the same deployment and see what happens",
    "start": "1788720",
    "end": "1793760"
  },
  {
    "text": "so once we add two additional parts what happened was because this extra large node had",
    "start": "1793760",
    "end": "1801360"
  },
  {
    "text": "had sufficient capacity these parts went into running state and they were never detected by",
    "start": "1801360",
    "end": "1806840"
  },
  {
    "text": "carpenter let's take it a step further and try to",
    "start": "1806840",
    "end": "1812480"
  },
  {
    "text": "detect and try to deploy some additional parts so once we add",
    "start": "1812480",
    "end": "1817760"
  },
  {
    "text": "four more parts and carpenter detected all four parts and requested another",
    "start": "1817760",
    "end": "1823600"
  },
  {
    "text": "instance from ec2 to add these four parts and this is the the ec2 instance which is coming up",
    "start": "1823600",
    "end": "1831760"
  },
  {
    "text": "similarly in the next few seconds as cubelet comes up we should be seeing the version instance",
    "start": "1831760",
    "end": "1838320"
  },
  {
    "text": "type zone and architecture and these parts should go in the container creating mode and",
    "start": "1838320",
    "end": "1845279"
  },
  {
    "text": "so at this point the image for these parts is being pulled and once the image is there on the node",
    "start": "1845279",
    "end": "1851440"
  },
  {
    "text": "they should go into the running state and the difference to note here is carpenter this time selected a 2x",
    "start": "1851440",
    "end": "1858399"
  },
  {
    "text": "large because we needed four cores for the four parts we were creating and then",
    "start": "1858399",
    "end": "1864559"
  },
  {
    "text": "the zone it selected was 2b so at this point we have all the seven",
    "start": "1864559",
    "end": "1871919"
  },
  {
    "text": "parts running let's also go ahead and check how bin packing works in carpenter so this time i'm going to",
    "start": "1871919",
    "end": "1879039"
  },
  {
    "text": "add 100 parts in the same deployment and once these parts go in pending state carpenter",
    "start": "1879039",
    "end": "1885840"
  },
  {
    "text": "detected that there are 55 pending parts and it tried to bind all these parts",
    "start": "1885840",
    "end": "1892480"
  },
  {
    "text": "onto the same node and then it detected there are 35 parts on the second",
    "start": "1892480",
    "end": "1898000"
  },
  {
    "text": "and try to bind these 35 parts on the second node it requested so the way this works is carpenter runs",
    "start": "1898000",
    "end": "1904480"
  },
  {
    "text": "in a reconciler loop every 10 seconds checking for the pending parts",
    "start": "1904480",
    "end": "1909760"
  },
  {
    "text": "and in the first reconciliation loop it detected 55 parts in the second one it",
    "start": "1909760",
    "end": "1915120"
  },
  {
    "text": "found out 35 parts 55 parts it requested a larger instance type and for the 35 it requested a 12x",
    "start": "1915120",
    "end": "1921600"
  },
  {
    "text": "large we can verify this so a 97 is a 2012 x",
    "start": "1921600",
    "end": "1927360"
  },
  {
    "text": "large and 174 139 is a 24 x large so as these instances are now coming up",
    "start": "1927360",
    "end": "1934799"
  },
  {
    "text": "we can see these parts are getting into container creating some of the parts are already going into",
    "start": "1934799",
    "end": "1940399"
  },
  {
    "text": "the running state here so let's give it a few seconds and all these parts should",
    "start": "1940399",
    "end": "1946480"
  },
  {
    "text": "just go into running state",
    "start": "1946480",
    "end": "1949840"
  },
  {
    "text": "at this point we can quickly check how many are in the running state out of 100 so we have 95 give it few more seconds",
    "start": "1962960",
    "end": "1969840"
  },
  {
    "text": "and see once we reach 100 so we have 100 parts and we can go ahead and",
    "start": "1969840",
    "end": "1976159"
  },
  {
    "text": "watch the parts again so it took about two minutes for all these parts to be created across",
    "start": "1976159",
    "end": "1983440"
  },
  {
    "text": "two nodes which is which are 12x large and 24x large",
    "start": "1983440",
    "end": "1989120"
  },
  {
    "text": "next let's see how scale down works so let's say the load has degrees and the replica",
    "start": "1989120",
    "end": "1994799"
  },
  {
    "text": "count has been set to 0 now at this point all the parts are being",
    "start": "1994799",
    "end": "2000000"
  },
  {
    "text": "terminated across all the four instances that we just created",
    "start": "2000000",
    "end": "2005279"
  },
  {
    "text": "and once all the parts are being completely removed carpenter will detect that these nodes",
    "start": "2005279",
    "end": "2011840"
  },
  {
    "text": "are under utilized and carpenter will wait for 10 seconds that was the ttl seconds we configured",
    "start": "2011840",
    "end": "2018559"
  },
  {
    "text": "earlier and remove these nodes by draining these nodes and then and then terminating these",
    "start": "2018559",
    "end": "2025919"
  },
  {
    "text": "nodes as a result so we started terminating one by one and two of the nodes are being removed",
    "start": "2025919",
    "end": "2032799"
  },
  {
    "text": "so we are left with two more nodes which are 12 x large and a 24 x large",
    "start": "2032799",
    "end": "2038799"
  },
  {
    "text": "so at this point all the parts are being terminated and and carpenter has already removed",
    "start": "2042960",
    "end": "2049280"
  },
  {
    "text": "three nodes and we are just waiting for the last node to be removed from the cluster and these nodes are",
    "start": "2049280",
    "end": "2057040"
  },
  {
    "text": "also being deleted at the ec2 level so they are being terminated in aws also",
    "start": "2057040",
    "end": "2064878"
  },
  {
    "text": "so now we are back to where we started with a single node and no workload at all let's say as a",
    "start": "2066000",
    "end": "2072720"
  },
  {
    "text": "use case you want to schedule some of your parts on a different architecture for a node let's say",
    "start": "2072720",
    "end": "2077839"
  },
  {
    "text": "you want to use an arm 64. so let's repurpose our deployment and",
    "start": "2077839",
    "end": "2083679"
  },
  {
    "text": "try to run the parts on the on in this deployment now on an arm64",
    "start": "2083679",
    "end": "2089599"
  },
  {
    "text": "let's keep in mind we never created any auto scaling group or a node group for auto scaling based on particular",
    "start": "2089599",
    "end": "2097280"
  },
  {
    "text": "architecture or instance type and let's go ahead and create two replicas",
    "start": "2097280",
    "end": "2102480"
  },
  {
    "text": "for these parts and they should come up on an arm 64 type architecture node so they both went",
    "start": "2102480",
    "end": "2109599"
  },
  {
    "text": "into the pending as expected carpenter detected two two provisionable parts",
    "start": "2109599",
    "end": "2114720"
  },
  {
    "text": "which need capacity and it requested a node from ec2 and binded two parts on",
    "start": "2114720",
    "end": "2121200"
  },
  {
    "text": "on this particular node and in this case the architecture it requested was an arm",
    "start": "2121200",
    "end": "2127280"
  },
  {
    "text": "64. so in another few seconds once cubelet is up we should be able to",
    "start": "2127280",
    "end": "2133280"
  },
  {
    "text": "see the version instance type and the zone again in which this",
    "start": "2133280",
    "end": "2138320"
  },
  {
    "text": "instance is being created which is an arm based node and these two parts should get created",
    "start": "2138320",
    "end": "2144079"
  },
  {
    "text": "on that particular node",
    "start": "2144079",
    "end": "2147119"
  },
  {
    "text": "so it took about 56 seconds and the instance type it selected was a t4g",
    "start": "2149920",
    "end": "2154960"
  },
  {
    "text": "extra large and it and the zone it picked was a usb 2c and we can see that the container are",
    "start": "2154960",
    "end": "2161520"
  },
  {
    "text": "getting are in the container creating more and the image is being pulled in the background",
    "start": "2161520",
    "end": "2167759"
  },
  {
    "text": "so they both are in the running states on an arm architecture",
    "start": "2171280",
    "end": "2176320"
  },
  {
    "text": "so yeah this is all i had for the demo thanks for joining our session",
    "start": "2176320",
    "end": "2182560"
  }
]