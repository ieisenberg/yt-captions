[
  {
    "start": "0",
    "end": "30000"
  },
  {
    "text": "[Music]",
    "start": "3120",
    "end": "6219"
  },
  {
    "text": "i'm a software engineer from vmware i've been working on a project called pks um",
    "start": "12639",
    "end": "19119"
  },
  {
    "text": "since its beginning for almost for almost two years now um so today i'm going to talk about load",
    "start": "19119",
    "end": "24800"
  },
  {
    "text": "balancing especially in the context of cal native",
    "start": "24800",
    "end": "31800"
  },
  {
    "start": "30000",
    "end": "262000"
  },
  {
    "text": "okay um so here's what i'm going to do firstly i'll give you a brief introduction of all the concepts related",
    "start": "31840",
    "end": "38399"
  },
  {
    "text": "to load balancer and then i'll walk you through the life cycle of a simple packet and point out",
    "start": "38399",
    "end": "45520"
  },
  {
    "text": "where and how load balancing is happening also our list of other techniques can be used to implement some concrete",
    "start": "45520",
    "end": "52160"
  },
  {
    "text": "solutions i do think that sometimes it really helps to get your hands dirty to gain a",
    "start": "52160",
    "end": "59120"
  },
  {
    "text": "better understanding of all the concepts that's why i prepared a demo at last um",
    "start": "59120",
    "end": "64640"
  },
  {
    "text": "to show you how to build a reliable and scalable l4 balancer by running a bunch",
    "start": "64640",
    "end": "70720"
  },
  {
    "text": "of very simple bash scripts okay let's get started",
    "start": "70720",
    "end": "77119"
  },
  {
    "text": "so i'm pretty sure everyone here are already familiar with the term load balancing but what does it mean it can't",
    "start": "77119",
    "end": "82720"
  },
  {
    "text": "native contacts right um so around five years ago i mean even for",
    "start": "82720",
    "end": "89439"
  },
  {
    "text": "now some big companies are still depending on very expensive box boxes to implement this or to fulfill",
    "start": "89439",
    "end": "96400"
  },
  {
    "text": "this load balancing functionality they are great but they do come with a bunch of disadvantages",
    "start": "96400",
    "end": "102399"
  },
  {
    "text": "first of all they are very expensive the upgrade is very costly and it takes lots of training and money to",
    "start": "102399",
    "end": "109600"
  },
  {
    "text": "get the internal working of the very fancy boxes also",
    "start": "109600",
    "end": "115360"
  },
  {
    "text": "as our as our application scales up it's very hard to meet the aha",
    "start": "115360",
    "end": "120640"
  },
  {
    "text": "requirement because the redundancy model here it provides is one plus one which",
    "start": "120640",
    "end": "125840"
  },
  {
    "text": "means you have one active low balancer running to serve other requests and the other one",
    "start": "125840",
    "end": "131280"
  },
  {
    "text": "is standing by um it's also syncing states with this active one um also it brings itself up",
    "start": "131280",
    "end": "137120"
  },
  {
    "text": "once it detects a failure of the active one but it's just good not good enough right also um if we're using hardware or",
    "start": "137120",
    "end": "144800"
  },
  {
    "text": "some software in the hardware to implement this it lacks the program programmability and flexibility um for",
    "start": "144800",
    "end": "152319"
  },
  {
    "text": "quick iteration um the same way we do for our cognitive applications okay so we like to have our load",
    "start": "152319",
    "end": "158720"
  },
  {
    "text": "balancer defined in software and to scale our load balancer itself",
    "start": "158720",
    "end": "165120"
  },
  {
    "text": "the same way as we scale our applications we want to keep this software as",
    "start": "165120",
    "end": "171280"
  },
  {
    "text": "stateless as possible right then we have this scale lab model which provides earn plus one redundancy",
    "start": "171280",
    "end": "177920"
  },
  {
    "text": "also the department itself is extremely thin simplified because we can deploy the",
    "start": "177920",
    "end": "183040"
  },
  {
    "text": "load balancer instance the same way we deploy our application we can just add more load balancer instances to provide",
    "start": "183040",
    "end": "190800"
  },
  {
    "text": "a better look balancing functionality with that help we can also short the",
    "start": "190800",
    "end": "197519"
  },
  {
    "text": "load balancer instances um to provide performance isolation to implement different",
    "start": "197519",
    "end": "203440"
  },
  {
    "text": "sla for example right container native",
    "start": "203440",
    "end": "208480"
  },
  {
    "text": "if you think about a vanilla kubernetes deployment where you have at least three microwaves",
    "start": "208480",
    "end": "214239"
  },
  {
    "text": "and you deploy this one container that is serving one service now you expose this service and you have",
    "start": "214239",
    "end": "221040"
  },
  {
    "text": "corresponding cluster ip service ingress and before the traffic hits the target container if you really think about it",
    "start": "221040",
    "end": "227519"
  },
  {
    "text": "it's very likely that it'll hit the bm firstly and firmware the routing decision will be made again",
    "start": "227519",
    "end": "234319"
  },
  {
    "text": "and the request will be routed to the other vm where this container is actually running",
    "start": "234319",
    "end": "239360"
  },
  {
    "text": "so it just adds a bunch of extra latency and overhead",
    "start": "239360",
    "end": "244480"
  },
  {
    "text": "and we don't want that right so by being container native um from can load balance load balancers perspective um",
    "start": "244480",
    "end": "251680"
  },
  {
    "text": "other containers are the same endpoints just same as vms right",
    "start": "251680",
    "end": "257680"
  },
  {
    "text": "then we can count all the latency and overhead",
    "start": "257680",
    "end": "262560"
  },
  {
    "start": "262000",
    "end": "336000"
  },
  {
    "text": "it's also very important to be the load dancer to be able to um interoperate with other",
    "start": "262880",
    "end": "270080"
  },
  {
    "text": "cloud services by being cloud native for example um some security related features provided by cloud provider like",
    "start": "270080",
    "end": "277440"
  },
  {
    "text": "ddos mitigation right or caching like cal cdn or some identity-based authentication",
    "start": "277440",
    "end": "285600"
  },
  {
    "text": "it's also very important to not ask our clients or customers to do any manual provisioning of platform",
    "start": "285600",
    "end": "292960"
  },
  {
    "text": "related resources in this case loop dancer itself as customer's application",
    "start": "292960",
    "end": "298320"
  },
  {
    "text": "scales up we want the load dancer cell to be automatically deployed and scaled",
    "start": "298320",
    "end": "304160"
  },
  {
    "text": "to be consistent with our application also in cognitive world as we know that",
    "start": "304160",
    "end": "310720"
  },
  {
    "text": "our backhand containers they come and go so it's a very um it's a highly um",
    "start": "310720",
    "end": "315919"
  },
  {
    "text": "dynamic environment right and to be able to run requests to",
    "start": "315919",
    "end": "321680"
  },
  {
    "text": "the endpoints on currency endpoints we need our load balancer",
    "start": "321680",
    "end": "327680"
  },
  {
    "text": "to be highly configurable to adapt to other changes spontaneously",
    "start": "327680",
    "end": "333840"
  },
  {
    "text": "so with all this being said let's take a look at the basic traffic flow or the lifecycle",
    "start": "333840",
    "end": "341120"
  },
  {
    "start": "336000",
    "end": "385000"
  },
  {
    "text": "package here this picture demonstrates a very classic model where we have kinds",
    "start": "341120",
    "end": "346720"
  },
  {
    "text": "um all the packets are generating the kind and before it hits a target backhand server it'll reach middleware",
    "start": "346720",
    "end": "353759"
  },
  {
    "text": "called load balancer in cognitive world we tend to model",
    "start": "353759",
    "end": "360000"
  },
  {
    "text": "independent modules into separate microservices and depending on",
    "start": "360000",
    "end": "365039"
  },
  {
    "text": "how they interact uh with each other it can be very complex but if you really think about it",
    "start": "365039",
    "end": "370880"
  },
  {
    "text": "the service that is making a request is actually acting as a kind so we still have this old classic model where three",
    "start": "370880",
    "end": "377919"
  },
  {
    "text": "phases low balancing can happen right client load balancer and service so now let's take a look at them one by",
    "start": "377919",
    "end": "384560"
  },
  {
    "text": "one on client side if you have config control or if you",
    "start": "384560",
    "end": "390479"
  },
  {
    "start": "385000",
    "end": "515000"
  },
  {
    "text": "have partial control over kinds you can start low balancing already right in this case you can choose to use",
    "start": "390479",
    "end": "396800"
  },
  {
    "text": "grpc by making calls to grpc library in your application application code and grpc",
    "start": "396800",
    "end": "403280"
  },
  {
    "text": "itself is smart enough to directly routing requests to different back-end servers",
    "start": "403280",
    "end": "408960"
  },
  {
    "text": "if you feel like that's too much okay grpc shouldn't be responsible for this load balancing decision",
    "start": "408960",
    "end": "415199"
  },
  {
    "text": "that's fine they also provide something called look aside load balancer so that means",
    "start": "415199",
    "end": "420800"
  },
  {
    "text": "this grpc kind can talk to a third party entity that implements grpc lb protocol",
    "start": "420800",
    "end": "428639"
  },
  {
    "text": "so it accidents entity for rotting decision then this responsibility can be fully",
    "start": "428639",
    "end": "434479"
  },
  {
    "text": "dedicated to the third party it sounds all good but the problem is",
    "start": "434479",
    "end": "440319"
  },
  {
    "text": "not everyone understand this grpc lb protocol so",
    "start": "440319",
    "end": "445360"
  },
  {
    "text": "in istio the way they expose the data plane is",
    "start": "445360",
    "end": "450400"
  },
  {
    "text": "called something universal data ping api right so it abstracts out all the data",
    "start": "450400",
    "end": "455520"
  },
  {
    "text": "pane in all aspects so grpc is also considering implement",
    "start": "455520",
    "end": "460960"
  },
  {
    "text": "this universal data pane api and in that case the grpc kind will make requests",
    "start": "460960",
    "end": "466240"
  },
  {
    "text": "directly to let's say istio control ping for routing decision and after that the",
    "start": "466240",
    "end": "471520"
  },
  {
    "text": "request will be automatically routed to different backend servers if you don't have any control over",
    "start": "471520",
    "end": "477039"
  },
  {
    "text": "clients let's say you have an application that is an internal service that talks to other",
    "start": "477039",
    "end": "482639"
  },
  {
    "text": "services and in this case it's a kind itself but you just don't have control over this this um this code or let's say",
    "start": "482639",
    "end": "488400"
  },
  {
    "text": "a jar package whatever it runs just fine and you don't want to touch it um in this case we can adopt a model",
    "start": "488400",
    "end": "495039"
  },
  {
    "text": "called sidecar proxy i believe everyone um are very familiar with this um it's",
    "start": "495039",
    "end": "500319"
  },
  {
    "text": "actually the way istio is leveraging online policy right you debug proxy",
    "start": "500319",
    "end": "505680"
  },
  {
    "text": "together with all the clients they talk to each other through some local networking or sockets and then the proxy",
    "start": "505680",
    "end": "512800"
  },
  {
    "text": "will make the writing decisions but most time we just don't have any",
    "start": "512800",
    "end": "518880"
  },
  {
    "start": "515000",
    "end": "746000"
  },
  {
    "text": "control on clients and most time load balancing actually happens in the middleware",
    "start": "518880",
    "end": "526000"
  },
  {
    "text": "of course our old friend dns can be used here you can define a a record",
    "start": "527279",
    "end": "533760"
  },
  {
    "text": "to map your on survey's name to different ip or even one ip in that case any cost can",
    "start": "533760",
    "end": "540320"
  },
  {
    "text": "be used this ip itself can be backed up by different servers that deployed across the world actually",
    "start": "540320",
    "end": "546880"
  },
  {
    "text": "and depending on where the server is deployed the request can be routed to the closest location",
    "start": "546880",
    "end": "552880"
  },
  {
    "text": "i'm going to show you later um how to how to do this um",
    "start": "552880",
    "end": "559040"
  },
  {
    "text": "in terms vip and ecmp so vip here means virtualized ip right instead of thinking",
    "start": "559040",
    "end": "565440"
  },
  {
    "text": "as something that um we assigned to a physical nic or a virtualized nic on vm",
    "start": "565440",
    "end": "571040"
  },
  {
    "text": "specifically um it's more like a abstraction right in l4 networking it's like a name it's like",
    "start": "571040",
    "end": "577440"
  },
  {
    "text": "the dns name we can actually map this ip address to a bunch of clusters or a",
    "start": "577440",
    "end": "582480"
  },
  {
    "text": "bunch of servers and ecmp itself or equal cost multiple",
    "start": "582480",
    "end": "587600"
  },
  {
    "text": "path is way to scale one ip or one virtualized ip",
    "start": "587600",
    "end": "592800"
  },
  {
    "text": "so the idea is quite simple now the cluster is backed up by this router right that's serving this vip and",
    "start": "592800",
    "end": "599839"
  },
  {
    "text": "we have a bunch of routers that are still in the same vip",
    "start": "599839",
    "end": "605320"
  },
  {
    "text": "we asked these routers to broadcast this vip to the upstream to the same upstream",
    "start": "605360",
    "end": "610720"
  },
  {
    "text": "router then from this upstream router's perspective this vip is actually associated with a bunch of next hops",
    "start": "610720",
    "end": "619040"
  },
  {
    "text": "right so it's able to pick up anyone from the routers and just send the traffic to that because they are all",
    "start": "619040",
    "end": "625279"
  },
  {
    "text": "serving the same vip the challenge here is how to reduce the",
    "start": "625279",
    "end": "632160"
  },
  {
    "text": "connection disruption as the number of load balancer instance and back-end",
    "start": "632160",
    "end": "637279"
  },
  {
    "text": "server number change right if that happens we want to keep all the packets",
    "start": "637279",
    "end": "642560"
  },
  {
    "text": "from the same stream or sync connection go into the same backend server",
    "start": "642560",
    "end": "649200"
  },
  {
    "text": "and consistent hashing is a technique we use here so it gives you this guarantee that",
    "start": "649279",
    "end": "656399"
  },
  {
    "text": "the number of disruptive connections will be at most as a one over n if the total number of",
    "start": "657120",
    "end": "663120"
  },
  {
    "text": "servers is n it's good enough sometimes for example in google cloud um this is actually is a",
    "start": "663120",
    "end": "669600"
  },
  {
    "text": "mac that the picture here shows that software called macdab um it's actually the paper published by google and this is",
    "start": "669600",
    "end": "676720"
  },
  {
    "text": "the thing that's supporting google cloud networking right to them load balancing is very important",
    "start": "676720",
    "end": "682160"
  },
  {
    "text": "so they don't care that much about connection disruption but depending on application sometimes it's very",
    "start": "682160",
    "end": "687600"
  },
  {
    "text": "important so in gitlab for example they took some other techniques",
    "start": "687600",
    "end": "692720"
  },
  {
    "text": "together with consistent hashing to minimize the number of disrupted connections",
    "start": "692720",
    "end": "699360"
  },
  {
    "text": "with all the tasks with all these techniques um it's all good but sometimes we still have the limitation",
    "start": "701519",
    "end": "707440"
  },
  {
    "text": "that the load balancer instance itself has to be deployed in the same l2 network with",
    "start": "707440",
    "end": "713920"
  },
  {
    "text": "our back-end servers um i'll give give you more details why this is a limitation um but for now let's just",
    "start": "713920",
    "end": "720720"
  },
  {
    "text": "assume this limitation so now we want to like scale beyond this l2 networking",
    "start": "720720",
    "end": "726160"
  },
  {
    "text": "right we don't want all our backhand server to deploy in the same l2 networking or on the scene switch so the",
    "start": "726160",
    "end": "732560"
  },
  {
    "text": "way to do that is very simple we use encapsulation or just tunneling right we wrap the original packet use another",
    "start": "732560",
    "end": "739760"
  },
  {
    "text": "packet and directly download the connection to a remotely robbed boat server",
    "start": "739760",
    "end": "745839"
  },
  {
    "start": "746000",
    "end": "829000"
  },
  {
    "text": "from service to service of course dns can still be used in vip virtualized ip or cluster ip in",
    "start": "747920",
    "end": "754959"
  },
  {
    "text": "kubernetes term right it's mostly man using a bunch of ip table rules",
    "start": "754959",
    "end": "760800"
  },
  {
    "text": "in l4 as we know except ip we also have part number right so here the port",
    "start": "760800",
    "end": "766079"
  },
  {
    "text": "number itself is also part of the address the abstraction right so it's very important sometimes to be able to",
    "start": "766079",
    "end": "772720"
  },
  {
    "text": "expose the same port number across a bunch of servers and when you send requests to either one of the vms they",
    "start": "772720",
    "end": "779200"
  },
  {
    "text": "can be routed to the same set of containers right so it's called node port in kubernetes or",
    "start": "779200",
    "end": "785279"
  },
  {
    "text": "writing match in docker swarm and they are mostly implemented by something called ipvs which i'm going to",
    "start": "785279",
    "end": "791920"
  },
  {
    "text": "demonstrate how to use it later with a bunch of details",
    "start": "791920",
    "end": "797040"
  },
  {
    "text": "of course service mesh can be used here it provides something fancier like l7 um",
    "start": "797040",
    "end": "802880"
  },
  {
    "text": "low balancing for example it's able to rot your request based on some fields or",
    "start": "802880",
    "end": "807920"
  },
  {
    "text": "for example url path right in http protocol it's able to route different requests based on your path to different",
    "start": "807920",
    "end": "813920"
  },
  {
    "text": "backend servers so now um with this concept being said i mean",
    "start": "813920",
    "end": "820880"
  },
  {
    "text": "this concept is just very dry here and really talking is very cheap so let's",
    "start": "820880",
    "end": "826480"
  },
  {
    "text": "just get into demo right as we can see from the previous list",
    "start": "826480",
    "end": "831839"
  },
  {
    "start": "829000",
    "end": "867000"
  },
  {
    "text": "here all these techniques listed here the l4 load balancing um is actually the",
    "start": "831839",
    "end": "839120"
  },
  {
    "text": "most fundamental building block right and that's what i'm going to do or i'm going to show in this demo um is to help",
    "start": "839120",
    "end": "846000"
  },
  {
    "text": "teach you how to extend one ip to make it an name or an abstraction in terms of",
    "start": "846000",
    "end": "852480"
  },
  {
    "text": "l4 networking and i'm going to do this by using some widely available open source softwares",
    "start": "852480",
    "end": "859360"
  },
  {
    "text": "like ipvs or just dummy interface or and bgp or ecmp",
    "start": "859360",
    "end": "867079"
  },
  {
    "start": "867000",
    "end": "922000"
  },
  {
    "text": "before i start demo i'd like to give you a very brief introduction of the ipvs yourself so ipvs is called ip virtual server",
    "start": "870399",
    "end": "878480"
  },
  {
    "text": "it's actually something that has existing linux kernel for over a decade surprisingly right and it's widely used",
    "start": "878480",
    "end": "885600"
  },
  {
    "text": "now in kubernetes and docker to implement the things i just mentioned like cluster ip and running mesh and no",
    "start": "885600",
    "end": "891920"
  },
  {
    "text": "port like that and what it provides is essentially just l forward balancing",
    "start": "891920",
    "end": "898160"
  },
  {
    "text": "with many algorithms like round robin this connection um destination hashing",
    "start": "898160",
    "end": "903360"
  },
  {
    "text": "blah blah blah it's also able to stay on to sync states with other ips instances",
    "start": "903360",
    "end": "910639"
  },
  {
    "text": "so that if one of the ipvs goes down um we can still keep the states",
    "start": "910639",
    "end": "916320"
  },
  {
    "text": "they are doing that through multicast but it only supports ipv4 now",
    "start": "916320",
    "end": "921759"
  },
  {
    "start": "922000",
    "end": "1181000"
  },
  {
    "text": "now let's start a demo so in this demo i'm going to start from",
    "start": "922079",
    "end": "927279"
  },
  {
    "text": "the very simple department here can everyone see the see the graph here",
    "start": "927279",
    "end": "932320"
  },
  {
    "text": "or i can just uh describe what it is okay so the top box here",
    "start": "932320",
    "end": "938639"
  },
  {
    "text": "this guy here is the roger so it's actually my look my laptop here right and deploy two other vms the first",
    "start": "938639",
    "end": "945920"
  },
  {
    "text": "one has ipvs running on that that serves as a load balancer and this guy here as",
    "start": "945920",
    "end": "951120"
  },
  {
    "text": "a server it runs a very simple golden hp server and the client here is actually also my",
    "start": "951120",
    "end": "956800"
  },
  {
    "text": "laptop i'm going to make a bunch of requests to this guy here and the first mode supported by ipvs is",
    "start": "956800",
    "end": "963680"
  },
  {
    "text": "called net mode or networking address translation idea is very simple that when the request",
    "start": "963680",
    "end": "970480"
  },
  {
    "text": "comes to the ipvs the destination ip will be translated to this real server's ip",
    "start": "970480",
    "end": "978399"
  },
  {
    "text": "and the default routine for this server will be set to this guy here so when the response so the response will be routed",
    "start": "978399",
    "end": "984160"
  },
  {
    "text": "back and the the net will be armed down and so the response can be sent right back to",
    "start": "984160",
    "end": "990160"
  },
  {
    "text": "client now let's see how it works",
    "start": "990160",
    "end": "995800"
  },
  {
    "text": "all right",
    "start": "1001759",
    "end": "1004160"
  },
  {
    "text": "so here i have the first vm um that has ipvs running on that so and",
    "start": "1008959",
    "end": "1015120"
  },
  {
    "text": "the same box here um i have the second vm then i'm gonna start this server",
    "start": "1015120",
    "end": "1020240"
  },
  {
    "text": "right um so what this step does really is as you can see here u2 step one a",
    "start": "1020240",
    "end": "1025438"
  },
  {
    "text": "step two sorry it's too small",
    "start": "1025439",
    "end": "1029038"
  },
  {
    "text": "it removes the default routing which is 33.1 which is my router i want to change it to the 33.2 which is the ipvs vm",
    "start": "1034319",
    "end": "1041839"
  },
  {
    "text": "right and also i started server it's quite simple actually then i start this ipvs on the first vm and i",
    "start": "1041839",
    "end": "1050000"
  },
  {
    "text": "add this server into ipvs",
    "start": "1050000",
    "end": "1053840"
  },
  {
    "text": "as you can see here the virtual ip is configured on this ipvs",
    "start": "1058480",
    "end": "1063600"
  },
  {
    "text": "and we also add the server into the ipvs now when a request goes to this",
    "start": "1063600",
    "end": "1069360"
  },
  {
    "text": "ipbs it's supposed to send a request to the backend server here which is the other vm i configured",
    "start": "1069360",
    "end": "1077440"
  },
  {
    "text": "now let me start the clients",
    "start": "1078400",
    "end": "1082080"
  },
  {
    "text": "so what the class does is it's just making curl requests to the endpoint with the virtual ip",
    "start": "1083679",
    "end": "1090799"
  },
  {
    "text": "and it slips one second every every time for for this one thousand times which is uh",
    "start": "1090799",
    "end": "1097520"
  },
  {
    "text": "hopefully this demo can end in 18 minutes okay",
    "start": "1097520",
    "end": "1103120"
  },
  {
    "text": "let's see great",
    "start": "1103120",
    "end": "1109720"
  },
  {
    "text": "as we can see the server starts serving requests here right that's all good",
    "start": "1113919",
    "end": "1120320"
  },
  {
    "text": "let's go back to the ppt and see what's wrong with that",
    "start": "1120320",
    "end": "1125919"
  },
  {
    "text": "so net is very pain and simple but it comes with a bunch of disadvantages",
    "start": "1129679",
    "end": "1135880"
  },
  {
    "text": "for one thing all the packets for this connection have to go through the same ipvs right that's a load answer it's not",
    "start": "1136240",
    "end": "1142720"
  },
  {
    "text": "like something everyone send traffic to that uh let's say you are download a huge",
    "start": "1142720",
    "end": "1147760"
  },
  {
    "text": "file so in that case the request itself is way lighter than the response right",
    "start": "1147760",
    "end": "1152799"
  },
  {
    "text": "it doesn't make any sense for your response to all go go through this loop answer so that's one thing the other",
    "start": "1152799",
    "end": "1158160"
  },
  {
    "text": "thing is we also add extra overhead for the critical path here because we need to",
    "start": "1158160",
    "end": "1163600"
  },
  {
    "text": "modify each packet right we are doing dna here for each packet",
    "start": "1163600",
    "end": "1169280"
  },
  {
    "text": "oh thank you for each packet",
    "start": "1169280",
    "end": "1175200"
  },
  {
    "text": "so we would like to let the server send the response directly back to client and how to do",
    "start": "1175200",
    "end": "1180960"
  },
  {
    "text": "that here comes the second mode supported by ipvs which is called direct routing",
    "start": "1180960",
    "end": "1187600"
  },
  {
    "start": "1181000",
    "end": "1362000"
  },
  {
    "text": "the idea is this server here this ipvs here will send the packet in l2",
    "start": "1187600",
    "end": "1195120"
  },
  {
    "text": "directly to this server it doesn't do any ip level change it sends this packet in",
    "start": "1195120",
    "end": "1201600"
  },
  {
    "text": "l2 right a switch level directly to this guy here and we have a dummy interface",
    "start": "1201600",
    "end": "1206720"
  },
  {
    "text": "configured on this guy with the virtual ip then we configure routing",
    "start": "1206720",
    "end": "1211840"
  },
  {
    "text": "to the default routing to this guy so the request can be uh sorry so response can be sent back to the kind",
    "start": "1211840",
    "end": "1219039"
  },
  {
    "text": "this technique is called directly server return with the assumption that with the",
    "start": "1219039",
    "end": "1224960"
  },
  {
    "text": "assumption that the server has to deploy in the same l2 network with the loop answer let's see how it works",
    "start": "1224960",
    "end": "1232760"
  },
  {
    "text": "now i'm gonna start a second on vm",
    "start": "1240320",
    "end": "1245080"
  },
  {
    "text": "what this step does is it sets up this virtual ip on the diamond interface also it starts this",
    "start": "1248480",
    "end": "1255200"
  },
  {
    "text": "golden server and then i'm going to add this server into my load balancer",
    "start": "1255200",
    "end": "1263158"
  },
  {
    "text": "okay as you can see here this server is added into the banner right 33.4",
    "start": "1267840",
    "end": "1273639"
  },
  {
    "text": "if we go to this this page here we can see it starts serving requests as we waste that but you might be wondering",
    "start": "1276559",
    "end": "1282640"
  },
  {
    "text": "why this only is only the second server that's serving the request and reason is the default algorithm is called wlc or",
    "start": "1282640",
    "end": "1289280"
  },
  {
    "text": "weighted list connection so the load balancer itself will send all the requests",
    "start": "1289280",
    "end": "1294720"
  },
  {
    "text": "to the server with the list connection let me let me do a watch on the load",
    "start": "1294720",
    "end": "1299919"
  },
  {
    "text": "balancer stats and see what happens",
    "start": "1299919",
    "end": "1304120"
  },
  {
    "text": "as you can see here the active connection for the first one is 64. the second one",
    "start": "1316320",
    "end": "1321440"
  },
  {
    "text": "is um 55 so the second one is still catching up and before we reach the same",
    "start": "1321440",
    "end": "1326799"
  },
  {
    "text": "number the load answer will send all the incoming required to the second one",
    "start": "1326799",
    "end": "1332919"
  },
  {
    "text": "with this direct quality mode we just liberate ourselves from the limitation that puts on us right the",
    "start": "1338559",
    "end": "1345440"
  },
  {
    "text": "response can be sent directly back to client and we are okay to download some huge files in this case but still with",
    "start": "1345440",
    "end": "1351840"
  },
  {
    "text": "the assumption that we have to we have to deploy loopbanger and server in the same l2 network",
    "start": "1351840",
    "end": "1359039"
  },
  {
    "text": "and we want to scale beyond l2 the way to do that is a third mode",
    "start": "1359039",
    "end": "1365120"
  },
  {
    "start": "1362000",
    "end": "1508000"
  },
  {
    "text": "supported by ipvs called tunnel mode the idea just like what what i described",
    "start": "1365120",
    "end": "1370720"
  },
  {
    "text": "before it encapsulates this packet with another packet and it tunnels this connection to a",
    "start": "1370720",
    "end": "1377039"
  },
  {
    "text": "remotely rockball server so here the vm is deployed actually in another subnet",
    "start": "1377039",
    "end": "1382559"
  },
  {
    "text": "if you can see that it's deploying 34.2 these three are deploying 33 subnet",
    "start": "1382559",
    "end": "1390960"
  },
  {
    "text": "right so it's just remotely rubble it's not in the same l2 network and the way to configure that is",
    "start": "1390960",
    "end": "1397120"
  },
  {
    "text": "to set vip on this tunnel 0 interface which is the default linux default ipip",
    "start": "1397120",
    "end": "1404559"
  },
  {
    "text": "um interface okay so ipip is a protocol used by ipvs here for encapsulation",
    "start": "1404559",
    "end": "1411039"
  },
  {
    "text": "which means to wrap one ip packet with a ip packet",
    "start": "1411039",
    "end": "1416240"
  },
  {
    "text": "on that step they stopped",
    "start": "1416240",
    "end": "1419960"
  },
  {
    "text": "now i start a third server um so what this command does is it just sets up tunnel mode right on the server",
    "start": "1436960",
    "end": "1443360"
  },
  {
    "text": "like i described before and it starts this guillain server okay then i'm going to add it into the load",
    "start": "1443360",
    "end": "1449440"
  },
  {
    "text": "bancer",
    "start": "1449440",
    "end": "1452440"
  },
  {
    "text": "as you can see 34.2 is also added into the load balancer if we go to the",
    "start": "1458799",
    "end": "1463919"
  },
  {
    "text": "client here oh my god",
    "start": "1463919",
    "end": "1467600"
  },
  {
    "text": "well i don't know what's happening here um it looks like there's some failure but that's fine because we are going to",
    "start": "1470320",
    "end": "1475919"
  },
  {
    "text": "have health check on the load balancer later in the demo so now um the requests are serving by these",
    "start": "1475919",
    "end": "1482240"
  },
  {
    "text": "servers even with some temporary video probably because i'm not working",
    "start": "1482240",
    "end": "1487279"
  },
  {
    "text": "in this building i don't know if you look at this picture here they all have the active connections",
    "start": "1487279",
    "end": "1493440"
  },
  {
    "text": "around 10 or 20. okay",
    "start": "1493440",
    "end": "1499799"
  },
  {
    "text": "so now everything is good right we are able to scale beyond l2 what's the problem then if you look at the",
    "start": "1503279",
    "end": "1508400"
  },
  {
    "text": "department here the load balancer instance itself becomes a single point failure it's a bottleneck here if this",
    "start": "1508400",
    "end": "1514320"
  },
  {
    "text": "guy goes down it brings down all the other servers so we want to scale beyond this ip this",
    "start": "1514320",
    "end": "1520320"
  },
  {
    "text": "virtual ip that is supported by this servers right so the packing used here is called ecmp",
    "start": "1520320",
    "end": "1526840"
  },
  {
    "text": "um the idea is this guy will run go bgp to broadcast this",
    "start": "1526840",
    "end": "1533039"
  },
  {
    "text": "virtual ip to upstream router and we are going to replicate this deployment here",
    "start": "1533039",
    "end": "1538159"
  },
  {
    "text": "to have a second load balancer instance with other servers and the second load",
    "start": "1538159",
    "end": "1543440"
  },
  {
    "text": "balancer instance is going to broadcast the same vip to the upstream router so that from upstream router it act it",
    "start": "1543440",
    "end": "1549840"
  },
  {
    "text": "actually has have um two routers with the same supported by the same vip let me first replicate the",
    "start": "1549840",
    "end": "1557440"
  },
  {
    "text": "department here so now i'm gonna deploy two more vms",
    "start": "1557440",
    "end": "1567799"
  },
  {
    "text": "oh",
    "start": "1571440",
    "end": "1573679"
  },
  {
    "text": "what okay oh nice um i'm going to deploy two more",
    "start": "1577200",
    "end": "1583600"
  },
  {
    "text": "vms here um in different subnets and then i'm going to have a second load balancer instance",
    "start": "1583600",
    "end": "1591480"
  },
  {
    "text": "um then i'll set up the second little bandster instance",
    "start": "1600559",
    "end": "1605440"
  },
  {
    "text": "as you can see here the second little dancer oh sorry",
    "start": "1609600",
    "end": "1613840"
  },
  {
    "text": "the second little bouncer is here with two other servers configured um now let's say if it works i'll change the",
    "start": "1622559",
    "end": "1628240"
  },
  {
    "text": "default routing on kind to the second load balancer",
    "start": "1628240",
    "end": "1633120"
  },
  {
    "text": "so now we just change the default lock into 33.5 which is second load balancer and ideally request will be routed to",
    "start": "1639039",
    "end": "1645600"
  },
  {
    "text": "the other two servers okay nice so as you can see these two guys start serving requests",
    "start": "1645600",
    "end": "1651360"
  },
  {
    "text": "okay and now we want to scale beyond this virtual ip these two load balancer instances and we're going to start go",
    "start": "1651360",
    "end": "1657840"
  },
  {
    "text": "bgp on both instances",
    "start": "1657840",
    "end": "1662440"
  },
  {
    "text": "so what this command starts is oh okay um is it starts go bgp on both ipvs",
    "start": "1678799",
    "end": "1686960"
  },
  {
    "text": "and ideally it should add",
    "start": "1686960",
    "end": "1692000"
  },
  {
    "text": "it should broadcast the virtual ip",
    "start": "1692399",
    "end": "1697159"
  },
  {
    "text": "let me run this manually",
    "start": "1699679",
    "end": "1703480"
  },
  {
    "text": "okay so we just started go bgp on both uh load balancers and they broadcast the",
    "start": "1711840",
    "end": "1717279"
  },
  {
    "text": "same vip to its peer which is the upstream router and on this host you can see the routing",
    "start": "1717279",
    "end": "1723120"
  },
  {
    "text": "table here it actually has this vip associated with two next hops and this",
    "start": "1723120",
    "end": "1729600"
  },
  {
    "text": "this guy will pick up one of the two to send all the traffic",
    "start": "1729600",
    "end": "1735200"
  },
  {
    "text": "and to make this guy work we need to change the default routine to to the upstream router which is 33.6",
    "start": "1735840",
    "end": "1745240"
  },
  {
    "text": "and now if you go to here let's see okay cool um so all the requests are started",
    "start": "1745600",
    "end": "1750720"
  },
  {
    "text": "serving by the first little dancer but in this case um",
    "start": "1750720",
    "end": "1755760"
  },
  {
    "text": "even even even if even if the first one fails um",
    "start": "1755760",
    "end": "1760799"
  },
  {
    "text": "we have no problem because we just use go bgp to backup this virtual ip by two routers okay let's",
    "start": "1760799",
    "end": "1768159"
  },
  {
    "text": "then mimic the failure here to by shutting down the vm and before that",
    "start": "1768159",
    "end": "1774080"
  },
  {
    "text": "i'll add some health check okay um",
    "start": "1774080",
    "end": "1780640"
  },
  {
    "text": "so health check is very important to the bouncer because you need to detail the failed server to stop sending incoming",
    "start": "1780640",
    "end": "1786159"
  },
  {
    "text": "requests to the failed one and in this case we can only support we can imagine the scalability and availability we",
    "start": "1786159",
    "end": "1792880"
  },
  {
    "text": "mentioned before so let me add some health check on a load balancer",
    "start": "1792880",
    "end": "1798080"
  },
  {
    "text": "so the health check here is also very simple some batch functions what it does really is it makes requests",
    "start": "1801360",
    "end": "1807919"
  },
  {
    "text": "on this endpoint if it fails it's going to run this command if it succeeds it's going to run the second",
    "start": "1807919",
    "end": "1813440"
  },
  {
    "text": "one the first one is to remove this endpoint from the load balancer the second one is",
    "start": "1813440",
    "end": "1818480"
  },
  {
    "text": "to add it back okay so it's going to keep querying the end point and either add it or remove it",
    "start": "1818480",
    "end": "1826480"
  },
  {
    "text": "now we just started the health check and we can see the house check",
    "start": "1831840",
    "end": "1837279"
  },
  {
    "text": "request is showing up on these three servers let me mimic the failure by shutting this guy down",
    "start": "1837279",
    "end": "1845278"
  },
  {
    "text": "cool as you can see here the sec the third one is removed automatically and from clients",
    "start": "1846960",
    "end": "1853200"
  },
  {
    "text": "perspective there's no disruption at all all the requests are served right by these two the third one is even though",
    "start": "1853200",
    "end": "1859760"
  },
  {
    "text": "it fails it's removed automatically by the health check function and let me just remove the other two",
    "start": "1859760",
    "end": "1866240"
  },
  {
    "text": "and see if this traffic switch to the second guy here",
    "start": "1866240",
    "end": "1871840"
  },
  {
    "text": "okay let me shut this guy down now it's done and you can see it's also",
    "start": "1876960",
    "end": "1883840"
  },
  {
    "text": "removed from the from the ipvs that's here let me also shut this guy down",
    "start": "1883840",
    "end": "1890960"
  },
  {
    "text": "oh nice so there's no disruption at all",
    "start": "1890960",
    "end": "1897039"
  },
  {
    "text": "as you can see when i shot this guy it's 619 or something like that and all the requests are automatically",
    "start": "1897039",
    "end": "1904240"
  },
  {
    "text": "routed to the second load balancer right they are served by this cluster servers",
    "start": "1904240",
    "end": "1909679"
  },
  {
    "text": "so if you go back to here you can see",
    "start": "1909679",
    "end": "1915679"
  },
  {
    "text": "the first load balancer is automatically removed from the upstream router okay",
    "start": "1915679",
    "end": "1920799"
  },
  {
    "text": "so in this way we just build a very stable and reliable l4 load balancer",
    "start": "1920799",
    "end": "1925919"
  },
  {
    "text": "since we are running up time i'm going to stop here feel free to ask me any questions",
    "start": "1925919",
    "end": "1931840"
  },
  {
    "text": "also the setup is available on github if you have any questions just ask me or",
    "start": "1931840",
    "end": "1936880"
  },
  {
    "text": "send me an email thanks to everyone for joining and bearing with me for this very long demo",
    "start": "1936880",
    "end": "1943840"
  },
  {
    "text": "thank you [Applause]",
    "start": "1943840",
    "end": "1949279"
  },
  {
    "text": "so what's the difference between between 20ws and the ros",
    "start": "1949279",
    "end": "1954960"
  },
  {
    "text": "so what's the difference between ipos and the rms uh sorry can you see i i uh second one",
    "start": "1957279",
    "end": "1964000"
  },
  {
    "text": "was the second one l yes lvs yes and another open source note balance",
    "start": "1964000",
    "end": "1971120"
  },
  {
    "text": "obvious i'm i'm not aware of that sorry okay yeah",
    "start": "1971120",
    "end": "1976640"
  },
  {
    "text": "lbs i i think it's just linux virtual server or something maybe they are the same",
    "start": "1976640",
    "end": "1982480"
  },
  {
    "text": "thing right yeah they are same thing just the name names are different it's the same thing",
    "start": "1982480",
    "end": "1988960"
  },
  {
    "text": "ipvs or lvs sometimes it's obvious okay thank you",
    "start": "1988960",
    "end": "1994720"
  },
  {
    "text": "any question okay thank you guys",
    "start": "1997360",
    "end": "2003720"
  }
]