[
  {
    "start": "0",
    "end": "12000"
  },
  {
    "text": "so before we get started I'm just curious how many about you out there have migrated your clusters or systems",
    "start": "560",
    "end": "7799"
  },
  {
    "text": "to kubernetes or docker anybody know so",
    "start": "7799",
    "end": "13320"
  },
  {
    "start": "12000",
    "end": "34000"
  },
  {
    "text": "basically in this talk my I'm going to talk through what we used to do why we",
    "start": "13320",
    "end": "18510"
  },
  {
    "text": "decided to move to docker while we move to kubernetes the problems we faced and sort of the system that we have now",
    "start": "18510",
    "end": "25289"
  },
  {
    "text": "which works across multiple different cloud providers and we can install it on on-premises as well but first a little",
    "start": "25289",
    "end": "32520"
  },
  {
    "text": "bit about me I'm Patrick mcquigg and I worked at algorithm yeah we run a mixed",
    "start": "32520",
    "end": "38309"
  },
  {
    "start": "34000",
    "end": "55000"
  },
  {
    "text": "hardware cluster primarily on AWS with CPUs and GPUs on a daily basis we create",
    "start": "38309",
    "end": "43950"
  },
  {
    "text": "and destroy some ten thousands of containers I don't know the exact number and I also ran our migration to",
    "start": "43950",
    "end": "49800"
  },
  {
    "text": "kubernetes in late of 2016 what we do is we're basically a surrealist AI cloud",
    "start": "49800",
    "end": "56910"
  },
  {
    "start": "55000",
    "end": "104000"
  },
  {
    "text": "the idea is that we have a common API for algorithms functions machine learning models to run as micro services",
    "start": "56910",
    "end": "63270"
  },
  {
    "text": "users can create algorithms and seven or eight different languages whenever they push to a git repo it automatically",
    "start": "63270",
    "end": "69750"
  },
  {
    "text": "creates an version API endpoint which then anybody can call its set up and as",
    "start": "69750",
    "end": "76830"
  },
  {
    "text": "a marketplace so users can publish publicly if they desire to you could set a royalty fee or you can make it private",
    "start": "76830",
    "end": "83820"
  },
  {
    "text": "you could share it within an organization there's a whole lot of different options that happen there the",
    "start": "83820",
    "end": "89820"
  },
  {
    "text": "key is that we will scale our cloud both the number of replicas of a particular",
    "start": "89820",
    "end": "95909"
  },
  {
    "text": "algorithm that exists and the number of servers that we have running at any given point in time based on demand as",
    "start": "95909",
    "end": "101280"
  },
  {
    "text": "it happens so for example we have this one algorithm for doing filtering like",
    "start": "101280",
    "end": "107729"
  },
  {
    "start": "104000",
    "end": "142000"
  },
  {
    "text": "you might see in snapchat or anything like that it has a whole bunch of different possible filters so you take an input picture it gives you give it a",
    "start": "107729",
    "end": "114750"
  },
  {
    "text": "path to save it and one of the filter names and you get this nice little stylized picture of my dog thousands of",
    "start": "114750",
    "end": "120990"
  },
  {
    "text": "algorithms out there there's 50,000 different users anyone can create anything some things or utilities or",
    "start": "120990",
    "end": "126570"
  },
  {
    "text": "sentiment analysis and LP it kind of goes all over the place but we do support all the major deep learning frameworks tensorflow MX",
    "start": "126570",
    "end": "133520"
  },
  {
    "text": "Caffe torciano and Karis I'm sure I'm forgetting some but the point is we're",
    "start": "133520",
    "end": "139190"
  },
  {
    "text": "trying to do anything all right so with that in mind uh I'd like to first start",
    "start": "139190",
    "end": "144890"
  },
  {
    "start": "142000",
    "end": "165000"
  },
  {
    "text": "by saying what we used to do and then why we moved to docker and kubernetes",
    "start": "144890",
    "end": "150170"
  },
  {
    "text": "getting to the problems and then how we generalized for multiple cloud providers and then I'll finish up with the things",
    "start": "150170",
    "end": "156890"
  },
  {
    "text": "I'm sort of excited about with kubernetes I'm sure you've heard a lot so far this week on where things are going and I'll talk a little bit about",
    "start": "156890",
    "end": "162770"
  },
  {
    "text": "what I'm personally excited about so this may look familiar to you it's a pretty traditional web application",
    "start": "162770",
    "end": "168980"
  },
  {
    "start": "165000",
    "end": "263000"
  },
  {
    "text": "diagram you know we have DNS that has entries for the website and for the API",
    "start": "168980",
    "end": "174980"
  },
  {
    "text": "servers those will point to load balancers we were using AWS so that's ELB behind the load balancers you have a",
    "start": "174980",
    "end": "181790"
  },
  {
    "text": "bunch of virtual machines that are running servers for example a web server maybe on port 9000 and an API server on",
    "start": "181790",
    "end": "188240"
  },
  {
    "text": "port 9100 when an API request comes in it goes to the load balancer round",
    "start": "188240",
    "end": "193310"
  },
  {
    "text": "robins to one of the virtual machines that you know does some authentication and we have our own schedule or to",
    "start": "193310",
    "end": "198770"
  },
  {
    "text": "figure out where we actually wanted to run that work it's pretty standard but just in case some of you haven't",
    "start": "198770",
    "end": "205300"
  },
  {
    "text": "experienced this before here's how the deployment might work so first of all we",
    "start": "205300",
    "end": "210410"
  },
  {
    "text": "have to use write a whole bunch of our own small bash or Python scripts to do each step of the deployment which was",
    "start": "210410",
    "end": "216410"
  },
  {
    "text": "managed by a third party called estel√≠ and the way that I would typically work is you have our three servers we pick",
    "start": "216410",
    "end": "224240"
  },
  {
    "text": "one of them to deregister from the load balancer the load balancer will stop sending new connections to that server",
    "start": "224240",
    "end": "229820"
  },
  {
    "text": "we wait for work to drain off of that machine whenever it's done servicing all active requests it gets deep registered",
    "start": "229820",
    "end": "237200"
  },
  {
    "text": "fully we can then update the application binary and then reattach to the load",
    "start": "237200",
    "end": "242209"
  },
  {
    "text": "bouncer and start responding to health checks after 30 seconds or whatever our configured interval is of healthy things",
    "start": "242209",
    "end": "249080"
  },
  {
    "text": "then the load balancers start sending work to it again this is pretty standard this is what people have been doing and",
    "start": "249080",
    "end": "254750"
  },
  {
    "text": "some variation of this for the you know 10 15 years in any sort of web serving architecture but it has a whole bunch of",
    "start": "254750",
    "end": "261680"
  },
  {
    "text": "limitations to it in particular you can really only run one copy of a service per machine",
    "start": "261680",
    "end": "267260"
  },
  {
    "start": "263000",
    "end": "310000"
  },
  {
    "text": "and a big reason for that is if you have a server running on port 9000 there's only one port 9000 and if the load",
    "start": "267260",
    "end": "273110"
  },
  {
    "text": "balancer is configured to ping port 9000 well you're kind of out of luck if you want to want to run multiple copies you",
    "start": "273110",
    "end": "279140"
  },
  {
    "text": "can do things like run a CH a proxy to balance between ports or something along those lines but it starts to get a",
    "start": "279140",
    "end": "284660"
  },
  {
    "text": "little bit more messy and it's usually just easier to create more machines running one copy of the service",
    "start": "284660",
    "end": "290170"
  },
  {
    "text": "additionally it can be really difficult to add services you have to install libraries on the host you may need to",
    "start": "290170",
    "end": "296060"
  },
  {
    "text": "configure your particular directories we have to write the deployment pipeline we have to figure out what's going to",
    "start": "296060",
    "end": "301820"
  },
  {
    "text": "monitor it if the process crashes how does it come back up how do we know that it crashed there's a whole host of",
    "start": "301820",
    "end": "307400"
  },
  {
    "text": "problems there on top of that like I said we have a public marketplace but",
    "start": "307400",
    "end": "314210"
  },
  {
    "start": "310000",
    "end": "360000"
  },
  {
    "text": "the customers came to us who a public marketplace doesn't always work for them they may have data privacy requirements",
    "start": "314210",
    "end": "320270"
  },
  {
    "text": "such as a HIPAA requirement or maybe they're an EU and there's data privacy laws where you can't send data across or",
    "start": "320270",
    "end": "326390"
  },
  {
    "text": "outside of the EU there's also a bandwidth or latency requirements users who are sort of in the big data space",
    "start": "326390",
    "end": "332060"
  },
  {
    "text": "they have all of their data in a certain data center why should they send all of that from say Europe to our data center",
    "start": "332060",
    "end": "338780"
  },
  {
    "text": "and US East one just to do some processing really they'd like it to be within their own data center so but",
    "start": "338780",
    "end": "346370"
  },
  {
    "text": "those motivations we felt like we can't just handle or can't only have a public marketplace but wanted to have an",
    "start": "346370",
    "end": "352310"
  },
  {
    "text": "enterprise offering where we would install our system and whatever cloud whatever region on premise that the",
    "start": "352310",
    "end": "357950"
  },
  {
    "text": "users wanted but before we can do that there's a whole bunch of barriers we",
    "start": "357950",
    "end": "364670"
  },
  {
    "start": "360000",
    "end": "400000"
  },
  {
    "text": "have to figure out how you're actually going to deliver your applications you have to figure out how to work on different operating systems based on the",
    "start": "364670",
    "end": "370970"
  },
  {
    "text": "customers requirements like I said we have third-party software for called Esteli for doing our deployment",
    "start": "370970",
    "end": "377420"
  },
  {
    "text": "management we would also use some things like Algol II as a power search on our website there's a high proof of concept",
    "start": "377420",
    "end": "384170"
  },
  {
    "text": "and the list kind of goes on but what I'd like to talk about first is why",
    "start": "384170",
    "end": "389270"
  },
  {
    "text": "docker so docker gives us the first two of these barriers where we want to talk",
    "start": "389270",
    "end": "394430"
  },
  {
    "text": "about the operating system and how to actually bundle the application and deliver it to people",
    "start": "394430",
    "end": "400220"
  },
  {
    "start": "400000",
    "end": "464000"
  },
  {
    "text": "so the benefits of container izing is that in general you're no longer tied to the host libraries or operating system",
    "start": "400220",
    "end": "406940"
  },
  {
    "text": "with a small asterisk I'll come back to it gives you a easy way to distribute and upgrade the application images you",
    "start": "406940",
    "end": "413630"
  },
  {
    "text": "can simply take the docker image you can push it to a registry you can dump it to a tar file you could send it via email",
    "start": "413630",
    "end": "419560"
  },
  {
    "text": "whatever you'd like to do there's multiple ways to distribute it it also gives a unified way to view logs and",
    "start": "419560",
    "end": "426470"
  },
  {
    "text": "kill and manage your applications so for example if you're running H a proxy and some shell scripts and a web server",
    "start": "426470",
    "end": "432830"
  },
  {
    "text": "using Apache and a different web server and Python or whatever else they might all right logs to random spots on the",
    "start": "432830",
    "end": "439250"
  },
  {
    "text": "operating system with docker single docker logs you can view application",
    "start": "439250",
    "end": "444260"
  },
  {
    "text": "logs for anything it also makes it somewhat easier to run multiple copies on the same machine but not fully",
    "start": "444260",
    "end": "449920"
  },
  {
    "text": "so I dr. raster system or completely docker has your system in August of 2016",
    "start": "449920",
    "end": "455690"
  },
  {
    "text": "roughly prior to that we had used docker for certain components but it wasn't a fully fledged entire ecosystem there's a",
    "start": "455690",
    "end": "465260"
  },
  {
    "start": "464000",
    "end": "582000"
  },
  {
    "text": "couple caveats as I said you aren't really completely free from knowing what",
    "start": "465260",
    "end": "470450"
  },
  {
    "text": "operating system you're running on fully for us when we're using GPU devices and other drivers those actually need to be",
    "start": "470450",
    "end": "477080"
  },
  {
    "text": "installed on the host operating system itself it's not something that you can only have existing in the container because you have kernel modules and",
    "start": "477080",
    "end": "483500"
  },
  {
    "text": "other things that you're dependent on then the kernel is not a part of the container it exists outside of the container additionally and this is a",
    "start": "483500",
    "end": "491540"
  },
  {
    "text": "little more convoluted but if you have a container and you want to spawn more containers you would give that container",
    "start": "491540",
    "end": "498950"
  },
  {
    "text": "access to the docker UNIX socket which is from the docker daemon its VAR on docker dot sock since this is a file on",
    "start": "498950",
    "end": "505730"
  },
  {
    "text": "the file system and has a user ID and a group ID and the user who's running your application inside of the container has",
    "start": "505730",
    "end": "511910"
  },
  {
    "text": "to either match the user ID or the group ID for this file or won't have permissions to write to it well it turns",
    "start": "511910",
    "end": "518750"
  },
  {
    "text": "out that in Ubuntu or sense OS or a couple other different operating systems when you install docker they get a",
    "start": "518750",
    "end": "525140"
  },
  {
    "text": "different group ID and that means that your container your application is running with a group ID of say 999 that",
    "start": "525140",
    "end": "531890"
  },
  {
    "text": "means that it can only actually access the dock socket on other hosts that have docker group 999 if you are using fully using",
    "start": "531890",
    "end": "539690"
  },
  {
    "text": "kubernetes you don't really have to worry about that because usually you're not interacting with docker directly you're gonna interact with the",
    "start": "539690",
    "end": "545360"
  },
  {
    "text": "kubernetes api server or something like that and when you're using the kubernetes api server",
    "start": "545360",
    "end": "550640"
  },
  {
    "text": "either's like role-based access controls and tokens that will handle all the access that you need but if there's some",
    "start": "550640",
    "end": "557300"
  },
  {
    "text": "sort of middle ground where you are using docker but not fully using kubernetes or some other Orchestrator this might be a problem if you need to",
    "start": "557300",
    "end": "564200"
  },
  {
    "text": "target different operating systems we worked around that by sort of customizing the the base image that",
    "start": "564200",
    "end": "570680"
  },
  {
    "text": "we're going to use even if we're using Ubuntu or sent to us or anything we install the docker and have a specific",
    "start": "570680",
    "end": "577190"
  },
  {
    "text": "group that we use for all of our a.m. is that we share with customers there's a",
    "start": "577190",
    "end": "582830"
  },
  {
    "start": "582000",
    "end": "639000"
  },
  {
    "text": "couple other issues and it's something where dr. people like to think that it's very easy but in some ways it can be",
    "start": "582830",
    "end": "589070"
  },
  {
    "text": "complicated and have sort of unknown performance issues when you go to figure",
    "start": "589070",
    "end": "594470"
  },
  {
    "text": "out what storage driver you want to use to with docker you kind of get this whole little matrix here if you're using",
    "start": "594470",
    "end": "600740"
  },
  {
    "text": "a Bunty Aur Deb Ewing or whatever there's just this whole sort of list and you're like okay well which one do I",
    "start": "600740",
    "end": "606110"
  },
  {
    "text": "choose turns out the recommended driver has changed multiple times over the last three years that we've used docker from",
    "start": "606110",
    "end": "612440"
  },
  {
    "text": "device mappers to AU FS and now it's at overlay - and that's something that they",
    "start": "612440",
    "end": "617750"
  },
  {
    "text": "the community sort of realized oh this one has performance issues or that one has performance issues and it kind of",
    "start": "617750",
    "end": "622940"
  },
  {
    "text": "keeps evolving for us we ended up using a ufs we might be changing that but",
    "start": "622940",
    "end": "630410"
  },
  {
    "text": "there are some tricky bits with a particular storage driver that they may not explain to you very clearly in some",
    "start": "630410",
    "end": "636590"
  },
  {
    "text": "of the documentation for example with a ufs it does require an extra kernel",
    "start": "636590",
    "end": "642380"
  },
  {
    "start": "639000",
    "end": "680000"
  },
  {
    "text": "package to be installed and this is documented but we were automatically upgrading our kernel versions for",
    "start": "642380",
    "end": "648680"
  },
  {
    "text": "security reasons or whatever else and then when we rebooted a host well our process we actually forgot to",
    "start": "648680",
    "end": "654650"
  },
  {
    "text": "update the Linux extras package so your reboot the host the kernels knew we were missing this extra kernel package and",
    "start": "654650",
    "end": "660560"
  },
  {
    "text": "then the docker daemon is that they're crashed looping can easily be fixed by you know being a little more intelligent",
    "start": "660560",
    "end": "665900"
  },
  {
    "text": "and now you're up creating your packages and making sure you upgrade everything you really need to but it's something that sort of",
    "start": "665900",
    "end": "671170"
  },
  {
    "text": "caught us off-guard when we you know a month or so after we're using docker and our system so I'd only had these weird",
    "start": "671170",
    "end": "676390"
  },
  {
    "text": "edge cases we were hitting secondly with",
    "start": "676390",
    "end": "682240"
  },
  {
    "start": "680000",
    "end": "763000"
  },
  {
    "text": "a AWS specifically EBS volume performance is really bad until the images or until the volume is fully",
    "start": "682240",
    "end": "688900"
  },
  {
    "text": "warmed up depending on how big your volumes are this can take several hours to read all the data you don't actually",
    "start": "688900",
    "end": "695440"
  },
  {
    "text": "have to write anything you just have to read and scan the entire volume in our case it was taking 45 minutes to an hour",
    "start": "695440",
    "end": "701650"
  },
  {
    "text": "and a half to actually fully read the volume and when you're trying to be an auto scaling system bringing up and down",
    "start": "701650",
    "end": "708100"
  },
  {
    "text": "lots of hosts an hour and a half you can't wait that's not reactive in the way we want to be because of that issue",
    "start": "708100",
    "end": "715300"
  },
  {
    "text": "we found that with very write heavy workloads like when we're spawning lots of containers we a lot of churn in our",
    "start": "715300",
    "end": "720340"
  },
  {
    "text": "system the docker Damon would ended up taking 45 seconds to a minute to create a container or do anything and that's",
    "start": "720340",
    "end": "726580"
  },
  {
    "text": "something that typically takes a hundred milliseconds or less something like that and all that time was just file system",
    "start": "726580",
    "end": "732940"
  },
  {
    "text": "sink times or way too high so one way to work around that is if you mount empty",
    "start": "732940",
    "end": "739030"
  },
  {
    "text": "EBS volumes they don't need actually be fetched or pre-warmed or anything like that so you can mount empty eve EBS",
    "start": "739030",
    "end": "744880"
  },
  {
    "text": "volumes and do a bunch of work on those and they'll function way better than if you have something that needs to be",
    "start": "744880",
    "end": "749950"
  },
  {
    "text": "pre-warmed this only really matters for situations we are doing lots of auto",
    "start": "749950",
    "end": "755290"
  },
  {
    "text": "scaling if you just bring up a host and you expect it to be around for a long time you might not actually need to",
    "start": "755290",
    "end": "760480"
  },
  {
    "text": "worry about this very much um but oh I",
    "start": "760480",
    "end": "765700"
  },
  {
    "start": "763000",
    "end": "780000"
  },
  {
    "text": "have questions at the end sorry thank you but overall you know we able to move",
    "start": "765700",
    "end": "771670"
  },
  {
    "text": "everything to docker it works great but it doesn't do everything for us so the next question is kind of why do",
    "start": "771670",
    "end": "777730"
  },
  {
    "text": "we move on to kubernetes so docker you know we kind of abstracted around our",
    "start": "777730",
    "end": "783970"
  },
  {
    "start": "780000",
    "end": "822000"
  },
  {
    "text": "operating system when we have a nice way to bundle our applications but there's still a whole bunch of other problems so",
    "start": "783970",
    "end": "790660"
  },
  {
    "text": "kubernetes is going to help us get rid of more of our third-party software by being an Orchestrator becomes very easy",
    "start": "790660",
    "end": "795850"
  },
  {
    "text": "to plug in new applications that we might need into the platform and also we want to get better server",
    "start": "795850",
    "end": "801880"
  },
  {
    "text": "utilization before we had kubernetes we needed about twelve machines in order to",
    "start": "801880",
    "end": "806890"
  },
  {
    "text": "run our base system and that's for a highly a bit high availability and the",
    "start": "806890",
    "end": "811960"
  },
  {
    "text": "fact that you know you can really run one copy of a service for a machine and there's this whole orchestrating problem",
    "start": "811960",
    "end": "818350"
  },
  {
    "text": "there so how does kubernetes get us that since this is the end of the week of",
    "start": "818350",
    "end": "824410"
  },
  {
    "start": "822000",
    "end": "891000"
  },
  {
    "text": "kubernetes talk i don't think i'm gonna run through all of this but the most important things I found our services",
    "start": "824410",
    "end": "831340"
  },
  {
    "text": "which give you ability to do load balancing service discovery DNS entries",
    "start": "831340",
    "end": "837120"
  },
  {
    "text": "deployments slash replica sets they allow you to have n copies of thing running you can do rolling updates it's",
    "start": "837120",
    "end": "844030"
  },
  {
    "text": "all native you can configure how the updates going to go say you know do one at a time two at a time",
    "start": "844030",
    "end": "850090"
  },
  {
    "text": "wait a minute in between instances it's highly configurable lots of things you can do there and thirdly the cni plugin",
    "start": "850090",
    "end": "857530"
  },
  {
    "text": "which is the networking plugin which allows you to get the actual giving IP",
    "start": "857530",
    "end": "862600"
  },
  {
    "text": "addresses to every container and infrastructure and it also establishes",
    "start": "862600",
    "end": "867970"
  },
  {
    "text": "routing between all the containers and it allows you to create network policy so you can say I want my API servers to",
    "start": "867970",
    "end": "874510"
  },
  {
    "text": "talk to Redis but I don't want the web servers to talk to Redis or whatever situations you may have you can lock",
    "start": "874510",
    "end": "880210"
  },
  {
    "text": "down the way this works and on top of that there's also a service mesh which you can do even more fine-grain",
    "start": "880210",
    "end": "887020"
  },
  {
    "text": "firewalling but we'll come back to that in a little bit an interesting question",
    "start": "887020",
    "end": "892750"
  },
  {
    "start": "891000",
    "end": "995000"
  },
  {
    "text": "I think is what don't we use kubernetes for it's a highly powerful tool we could probably do everything we wanted to with",
    "start": "892750",
    "end": "899470"
  },
  {
    "text": "sort of native kubernetes ideas and like operators and stuff like that but we kind of chose not to and in part because",
    "start": "899470",
    "end": "906430"
  },
  {
    "text": "we're a small team it's a huge project there's a lot of things going on and we didn't want to throw a ton of",
    "start": "906430",
    "end": "911920"
  },
  {
    "text": "variability into everything else that we were already doing but on top of that we had spent a lot of time optimizing our",
    "start": "911920",
    "end": "918280"
  },
  {
    "text": "system for the way that we are scheduling algorithm containers people",
    "start": "918280",
    "end": "924310"
  },
  {
    "text": "we would look at things like whether or not images were there which kubernetes would also do but we have higher level",
    "start": "924310",
    "end": "929530"
  },
  {
    "text": "data about what will this algorithm need in terms of the data files that and what are the likely files it might",
    "start": "929530",
    "end": "935600"
  },
  {
    "text": "be using based on characteristics we've seen in the past things like machine utilization is something in kubernetes",
    "start": "935600",
    "end": "941569"
  },
  {
    "text": "is also very likable natively track for you but pending requests and queue sizes of something a little bit more",
    "start": "941569",
    "end": "947600"
  },
  {
    "text": "abstracted might not know about unless we added that on top we also have some",
    "start": "947600",
    "end": "953060"
  },
  {
    "text": "special security concerns that go as part of our scheduling we will proxy requests for users maybe put them on an",
    "start": "953060",
    "end": "959089"
  },
  {
    "text": "isolated network if we don't trust it or the user wants it to be on an isolated network and all of this we were still",
    "start": "959089",
    "end": "964759"
  },
  {
    "text": "getting within single-digit millisecond performance it's one of those things where it's like well you spent all this time optimizing and investing in this",
    "start": "964759",
    "end": "971300"
  },
  {
    "text": "codebase we didn't feel there was a need to move all of that inside of to be a little bit more like kubernetes native",
    "start": "971300",
    "end": "976939"
  },
  {
    "text": "instead of running in our application something we may change in the future especially as we want to extend those",
    "start": "976939",
    "end": "982639"
  },
  {
    "text": "capabilities further but for now it's something you know you should consider do we need to move every single aspect",
    "start": "982639",
    "end": "988759"
  },
  {
    "text": "of our system to be kubernetes native or can we leave some of it within the application also GPU and device",
    "start": "988759",
    "end": "996680"
  },
  {
    "start": "995000",
    "end": "1096000"
  },
  {
    "text": "management kubernetes will allow you to expose the devices to a particular container or a particular pod you can",
    "start": "996680",
    "end": "1002290"
  },
  {
    "text": "say I need one two four whatever it may be but there's a lot of metrics that you can't get out of a GPU",
    "start": "1002290",
    "end": "1008589"
  },
  {
    "text": "so in particular GPUs will track track memory usage etc by the process ID so",
    "start": "1008589",
    "end": "1015130"
  },
  {
    "text": "when you're looking at a GPU if you're just exporting okay process Ida you've one-two-three is using this much memory",
    "start": "1015130",
    "end": "1020470"
  },
  {
    "text": "that doesn't tell you anything well you need to know what was that process who was it what's the sort of meta",
    "start": "1020470",
    "end": "1025959"
  },
  {
    "text": "information about it which would be like the user of the algorithm that kind of information or application knows so we",
    "start": "1025959",
    "end": "1031900"
  },
  {
    "text": "had to find our own way to expose that information that wasn't sort of natural with the advisor and other metrics that",
    "start": "1031900",
    "end": "1038319"
  },
  {
    "text": "are already generated for you secondly with ingress controllers",
    "start": "1038319",
    "end": "1044140"
  },
  {
    "text": "ingress controllers are great but they have some certain limitations and different ingress controllers have",
    "start": "1044140",
    "end": "1050980"
  },
  {
    "text": "different limitations in terms of the amounts that you can do with SSL termination and things like that and for",
    "start": "1050980",
    "end": "1056289"
  },
  {
    "text": "a lot of our enterprise customers they would have particular certificate authorities certificates different they",
    "start": "1056289",
    "end": "1063730"
  },
  {
    "text": "might need to validate x.509 certificates from the clients who are coming in and all of these configuration options",
    "start": "1063730",
    "end": "1069220"
  },
  {
    "text": "didn't exist in any current ingress format so rather than go through and",
    "start": "1069220",
    "end": "1075160"
  },
  {
    "text": "figure out how to get all these annotations and other things we found a way to template our own sort of hot",
    "start": "1075160",
    "end": "1080380"
  },
  {
    "text": "proxy config and just kind of rolled with that it's very close it is almost",
    "start": "1080380",
    "end": "1085660"
  },
  {
    "text": "like the ingress controller spec and so I'd like to get it back in there but it's something that we kind of have to",
    "start": "1085660",
    "end": "1090850"
  },
  {
    "text": "work with the other projects and it's hard to figure out whether or not they would accept our changes but overall you",
    "start": "1090850",
    "end": "1099280"
  },
  {
    "text": "know we completed a migration there's in production in January this year everything was running on kubernetes",
    "start": "1099280",
    "end": "1104950"
  },
  {
    "text": "fully we were able to drop the minimum number of servers we needed from that twelve to six and six is because there's",
    "start": "1104950",
    "end": "1112570"
  },
  {
    "text": "three kubernetes masters for highly available set up and we'd have three kubernetes nodes for highly available",
    "start": "1112570",
    "end": "1118600"
  },
  {
    "text": "Redis API servers all those other things that we run in our system we also only",
    "start": "1118600",
    "end": "1123640"
  },
  {
    "text": "need a single load balancer now and that's part of like that ingress mindset where you have a single nginx hop proxy",
    "start": "1123640",
    "end": "1130179"
  },
  {
    "text": "whatever sitting in front of all of your services and then it routes based off of what the incoming you know address was",
    "start": "1130179",
    "end": "1137160"
  },
  {
    "text": "now we're able to replace our third-party search with using elastic",
    "start": "1137160",
    "end": "1142179"
  },
  {
    "text": "search is very easy to get elastic search configured and deployed within kubernetes and kubernetes again we have",
    "start": "1142179",
    "end": "1147820"
  },
  {
    "text": "native deployment and rolling upgrades so we could get rid of dis daily entirely overall is very easy to add",
    "start": "1147820",
    "end": "1154420"
  },
  {
    "text": "services to the stack as well such as Prometheus turn Cortana if you hadn't hear about them the rest",
    "start": "1154420",
    "end": "1160870"
  },
  {
    "start": "1157000",
    "end": "1189000"
  },
  {
    "text": "of this week there are amazing tools for graphing monitoring etc and only took about a day for us to integrate coober",
    "start": "1160870",
    "end": "1167050"
  },
  {
    "text": "integrate Prometheus with our stack because there's so many deployment templates already out there it was just",
    "start": "1167050",
    "end": "1172420"
  },
  {
    "text": "easy to take this plug into little bits that we needed specific after that has probably spent about a week or two weeks",
    "start": "1172420",
    "end": "1179020"
  },
  {
    "text": "adding metrics everywhere I could and creating graphs and templates because it's amazing how much detail you can get",
    "start": "1179020",
    "end": "1184809"
  },
  {
    "text": "about your cluster from these two tools so what went wrong in this process",
    "start": "1184809",
    "end": "1191500"
  },
  {
    "start": "1189000",
    "end": "1218000"
  },
  {
    "text": "there's a lot that can go wrong if you have an application that currently exists not in the context of kubernetes",
    "start": "1191500",
    "end": "1197919"
  },
  {
    "text": "and you want to migrate it piece wise it's always possible to duplicate your entire stack inside of kubernetes and sort of just point your",
    "start": "1197919",
    "end": "1204710"
  },
  {
    "text": "dns to the kubernetes one but that may or may not work depending on your situation and you might not even be able",
    "start": "1204710",
    "end": "1210710"
  },
  {
    "text": "to migrate everything to kubernetes if say there's some other team you depend on and they aren't willing to do so",
    "start": "1210710",
    "end": "1215929"
  },
  {
    "text": "something like that and so the projects are moving fast like really fast you",
    "start": "1215929",
    "end": "1221809"
  },
  {
    "text": "might have seen that from this whole week all the features people have talked about in the last year the things that are upcoming but they aren't always",
    "start": "1221809",
    "end": "1227990"
  },
  {
    "text": "compatible with each other so for example docker 1.13 came out earlier in this year and at that time kubernetes",
    "start": "1227990",
    "end": "1234650"
  },
  {
    "text": "1.5 was out however it was not compatible and also docker 1.13 wasn't",
    "start": "1234650",
    "end": "1240410"
  },
  {
    "text": "targeted for kubernetes 1.6 and it wasn't until 1.7 that it was targeted and things were fixed and it resulted in",
    "start": "1240410",
    "end": "1247040"
  },
  {
    "text": "docker was making some changes to the way that they did iptables rules so they added a default deny or default drop",
    "start": "1247040",
    "end": "1252679"
  },
  {
    "text": "policy or something to the top of your chain and that broke everything so people try and upgrade and didn't work",
    "start": "1252679",
    "end": "1259809"
  },
  {
    "text": "secondly there's a ton of different versions that are in development and use at the same time just in November I",
    "start": "1259809",
    "end": "1266720"
  },
  {
    "text": "think there was 1.6 12 117 183 and 1.9 Oh alpha all released in kubernetes in",
    "start": "1266720",
    "end": "1274010"
  },
  {
    "text": "addition to other versions so if you're gonna be taking time to migrate your service what kind of version are you going to",
    "start": "1274010",
    "end": "1280190"
  },
  {
    "text": "target it's really a project management decision but it does become difficult when the docs are continually going to",
    "start": "1280190",
    "end": "1286280"
  },
  {
    "text": "be getting updated and you can't find the information you need or you try and go with the bleeding edge and then you end up hitting bugs and you kind of get",
    "start": "1286280",
    "end": "1292580"
  },
  {
    "text": "stuck in your migration process that's just something worth considering and probably looking at what the chart is",
    "start": "1292580",
    "end": "1298700"
  },
  {
    "text": "and estimate how much time you might need and then try and target something appropriately there's also a lot of new",
    "start": "1298700",
    "end": "1305630"
  },
  {
    "start": "1304000",
    "end": "1344000"
  },
  {
    "text": "concepts which can break sort of the way your deployments may work so readiness checks with the deployment or are very",
    "start": "1305630",
    "end": "1312500"
  },
  {
    "text": "similar to a health check from a load balancer but there's lots of things in the application and pod life cycle that",
    "start": "1312500",
    "end": "1317630"
  },
  {
    "text": "are different you might have to change the way that your application gets stopped you have to change your rolling",
    "start": "1317630",
    "end": "1322760"
  },
  {
    "text": "update strategy you may be used to doing certain things that are initialization which now you have to create another container to be in a NIC container and",
    "start": "1322760",
    "end": "1329630"
  },
  {
    "text": "none of this is particularly difficult but it is something where every service you migrate you",
    "start": "1329630",
    "end": "1334790"
  },
  {
    "text": "we'll have to look at all of these things and figure out what do I need what don't I need possibly creating five",
    "start": "1334790",
    "end": "1340100"
  },
  {
    "text": "or six containers if you have a lot of initialization that needs to get done but the real interesting thing is the",
    "start": "1340100",
    "end": "1347030"
  },
  {
    "start": "1344000",
    "end": "1395000"
  },
  {
    "text": "technical challenges so as we said the CNI plugin will generate IP addresses",
    "start": "1347030",
    "end": "1352850"
  },
  {
    "text": "for every single pot in your cluster but those are not routable from things not within the kubernetes cluster and this",
    "start": "1352850",
    "end": "1359150"
  },
  {
    "text": "can become really awkward if your services are trying to talk to each other via IP address for example we have",
    "start": "1359150",
    "end": "1364790"
  },
  {
    "text": "API servers and we have Redis and Redis needs to talk to itself to you know replicate data and if the master goes",
    "start": "1364790",
    "end": "1370820"
  },
  {
    "text": "down one of the replicas needs to take over so if you migrate Redis before migrating the API servers well your API",
    "start": "1370820",
    "end": "1377480"
  },
  {
    "text": "servers won't be able to talk to Redis unless you put some sort of proxy or something in front of it that they can address and then that would be able to",
    "start": "1377480",
    "end": "1384350"
  },
  {
    "text": "address the things inside of Redis something you just need to understand your own application what's talking to",
    "start": "1384350",
    "end": "1390560"
  },
  {
    "text": "what before you can really migrate anything safely secondly classic load",
    "start": "1390560",
    "end": "1398240"
  },
  {
    "start": "1395000",
    "end": "1485000"
  },
  {
    "text": "balancers are not really meant to address containers again the IP address isn't real so what may happen is is that",
    "start": "1398240",
    "end": "1404060"
  },
  {
    "text": "you're exposing your containers or your pods on say port 9000 and the traffic is",
    "start": "1404060",
    "end": "1409430"
  },
  {
    "text": "getting proxy to that container from the host but it's just as likely that the traffic is getting forwarded to a",
    "start": "1409430",
    "end": "1415850"
  },
  {
    "text": "container on a different host because that's the way that QB proxies works it will forward to something that is active",
    "start": "1415850",
    "end": "1421160"
  },
  {
    "text": "across the fleet and then what happens if host B goes down or deployment comes",
    "start": "1421160",
    "end": "1426440"
  },
  {
    "text": "along and updates that pot or whatever well if that connection that it was going to host B gets reused for some",
    "start": "1426440",
    "end": "1432920"
  },
  {
    "text": "reason it's possible that LBO will mistakenly thank host a is down because from elby's point of view it's talking",
    "start": "1432920",
    "end": "1439550"
  },
  {
    "text": "to host a but in reality the track is getting forwarded this can happen with any sort of HTTP client and any other",
    "start": "1439550",
    "end": "1446360"
  },
  {
    "text": "application you're using as well the main reason this comes about is because of connection pooling so the connection",
    "start": "1446360",
    "end": "1454760"
  },
  {
    "text": "from ELB that was going to host B if L B were to create a new connection it would cou be proxy would only forward it to an",
    "start": "1454760",
    "end": "1461270"
  },
  {
    "text": "active pod you don't have to worry about that the problem is is that these connections may or may not get reused",
    "start": "1461270",
    "end": "1466490"
  },
  {
    "text": "based on your application can and if you're not careful about setting headers such as their connection clothes",
    "start": "1466490",
    "end": "1472640"
  },
  {
    "text": "you may end up stuck with this problem additionally Yale B allows you to do a",
    "start": "1472640",
    "end": "1478340"
  },
  {
    "text": "TCP check which would not use any pooling ever so it depends on what sort of situation you have secondly when you",
    "start": "1478340",
    "end": "1487280"
  },
  {
    "text": "have the cni plugin you have a whole bunch of hosts and they get networked together somehow for example in this",
    "start": "1487280",
    "end": "1493550"
  },
  {
    "text": "diagram so we use the weave net plugin which works great and has lots of configuration options but there was one",
    "start": "1493550",
    "end": "1500210"
  },
  {
    "text": "we weren't aware of until it sort of bit us since we down scaler cluster it's",
    "start": "1500210",
    "end": "1505580"
  },
  {
    "text": "possible you can end up with disconnected nodes so say like we have this diagram here and host BNE we turn",
    "start": "1505580",
    "end": "1512330"
  },
  {
    "start": "1510000",
    "end": "1526000"
  },
  {
    "text": "off because we don't have traffic or something like that well now you can see your cluster is divided into two different groups and",
    "start": "1512330",
    "end": "1518390"
  },
  {
    "text": "they can't talk to each other across that divide that's something where you can actually configure the number of",
    "start": "1518390",
    "end": "1524000"
  },
  {
    "text": "connections a given host is going to have and you know you can easily prevent if I expected downscale and nodes and",
    "start": "1524000",
    "end": "1530960"
  },
  {
    "start": "1526000",
    "end": "1570000"
  },
  {
    "text": "then make at least n plus 1 connections simple heuristic but it works it gets the job done but you do need to be aware",
    "start": "1530960",
    "end": "1537020"
  },
  {
    "text": "of your sort of topology there for us we highly monitor the number of connections",
    "start": "1537020",
    "end": "1543350"
  },
  {
    "text": "that each host has how many are failed having or whatever instead alerts on that all very easy using Prometheus and",
    "start": "1543350",
    "end": "1549830"
  },
  {
    "text": "graph on a-- and you could consider your network topology while scaling these",
    "start": "1549830",
    "end": "1554870"
  },
  {
    "text": "nodes will not refresh their node peers list so if a node goes down it may not actually try and to find a new node you",
    "start": "1554870",
    "end": "1561980"
  },
  {
    "text": "could you know make all the nodes aware of that we thought that was a little too complicated and haven't had a need based",
    "start": "1561980",
    "end": "1567320"
  },
  {
    "text": "on our heuristics and monitoring yet so how do we go from here to multiple",
    "start": "1567320",
    "end": "1572990"
  },
  {
    "start": "1570000",
    "end": "1588000"
  },
  {
    "text": "clouds there's still a lot of moving pieces of infrastructure that you need to create VP sees networks you know",
    "start": "1572990",
    "end": "1580460"
  },
  {
    "text": "virtual machines etc and we also have to get it to work on a whole bunch of different cloud providers so the first",
    "start": "1580460",
    "end": "1586910"
  },
  {
    "text": "step is that oh sorry but there's a bunch of considerations with your application you might want to think of",
    "start": "1586910",
    "end": "1592760"
  },
  {
    "start": "1588000",
    "end": "1626000"
  },
  {
    "text": "first vendors should like to try and lock you into certain to using them for example AWS has services like DynamoDB",
    "start": "1592760",
    "end": "1599630"
  },
  {
    "text": "Kinesis ECS all great services however they don't all necessarily have an analogue in every cloud provider and if you want",
    "start": "1599630",
    "end": "1606570"
  },
  {
    "text": "to target an on-premises environment certainly there's no dynamodb on-premise",
    "start": "1606570",
    "end": "1612020"
  },
  {
    "text": "additionally you can use kubernetes to replace a lot of things you might otherwise use infrastructure for for",
    "start": "1612020",
    "end": "1618240"
  },
  {
    "text": "example you can use a service to replace having physical load balancers as we've discussed before but we still have to",
    "start": "1618240",
    "end": "1625020"
  },
  {
    "text": "figure out how to provision and this is where it believed this was mentioned in the last talk as well where hashey Korps",
    "start": "1625020",
    "end": "1630600"
  },
  {
    "start": "1626000",
    "end": "1645000"
  },
  {
    "text": "terraform is great it allows you to create dot TF files you can commit infrastructure as code you can version",
    "start": "1630600",
    "end": "1636450"
  },
  {
    "text": "it you can plug in a whole bunch of different variables for say region and AWS account or whatever else and then we",
    "start": "1636450",
    "end": "1642240"
  },
  {
    "text": "can bring up a cluster with those parameters but once you have a whole",
    "start": "1642240",
    "end": "1648120"
  },
  {
    "start": "1645000",
    "end": "1658000"
  },
  {
    "text": "bunch of TF files for editable us you can't actually just change those to the azure equivalents it's not just a find",
    "start": "1648120",
    "end": "1654000"
  },
  {
    "text": "and replace virtual machine with the virtual machine for a matcher and there's a number of reasons for that",
    "start": "1654000",
    "end": "1659730"
  },
  {
    "text": "don't try and go quickly through these so AWS has an internal DNS to your V PC",
    "start": "1659730",
    "end": "1666060"
  },
  {
    "text": "but other cloud providers do not so you might have to reconsider your architectures a little bit there again I",
    "start": "1666060",
    "end": "1671640"
  },
  {
    "text": "would rely on kubernetes to do in services to use dns and not actually have to use of DNS within the cluster",
    "start": "1671640",
    "end": "1678720"
  },
  {
    "text": "but you can use a kubernetes level DNS and that solves the problem for you right there",
    "start": "1678720",
    "end": "1684210"
  },
  {
    "text": "in Rackspace if you have a public and a private IP address those come as two different network cards and there's this",
    "start": "1684210",
    "end": "1691200"
  },
  {
    "text": "issue with weave and the Kubla where it end up sending traffic to the wrong network card if it's basically if you're",
    "start": "1691200",
    "end": "1699570"
  },
  {
    "text": "trying to send traffic within the cluster but a route didn't exist then it would actually send it to the through the public network interface instead of",
    "start": "1699570",
    "end": "1706020"
  },
  {
    "text": "the private network interface and there are ways to work around that it's on that issue there not a workaround it",
    "start": "1706020",
    "end": "1712430"
  },
  {
    "text": "lasts API versions are never simple for",
    "start": "1712430",
    "end": "1717780"
  },
  {
    "text": "Rackspace is not exactly the same as OpenStack within a different OpenStack clusters you can have multiple versions",
    "start": "1717780",
    "end": "1724170"
  },
  {
    "text": "of authentication even and the types of and api's that are available may or may not change between different customers",
    "start": "1724170",
    "end": "1730680"
  },
  {
    "text": "who have an openstack cluster so it's one of those things like you can just got all the services available within",
    "start": "1730680",
    "end": "1736530"
  },
  {
    "text": "OpenStack for a particular environment these sort of end up with this giant matrix of what's there and what's not",
    "start": "1736530",
    "end": "1741630"
  },
  {
    "text": "there and you really just want this to be simple as possible and it will be a little complicated for OpenStack load",
    "start": "1741630",
    "end": "1751260"
  },
  {
    "start": "1749000",
    "end": "1824000"
  },
  {
    "text": "balancers you know if you're doing a lot of copy and paste between terraform files you might see Oh AWS HTTP load",
    "start": "1751260",
    "end": "1758130"
  },
  {
    "text": "balancer cool copy that's at OpenStack but it turns out that with AWS if you have HTTP that means terminate SSL but",
    "start": "1758130",
    "end": "1765990"
  },
  {
    "text": "an OpenStack HTTP means do not terminate SSL so there's just these like really small things that can get lost in this",
    "start": "1765990",
    "end": "1772740"
  },
  {
    "text": "mix of so much going on that you end up not noticing very fine details like that",
    "start": "1772740",
    "end": "1779190"
  },
  {
    "text": "additionally there are differences where you can only bind to a single internal",
    "start": "1779190",
    "end": "1784230"
  },
  {
    "text": "load balancer for a virtual machine and again it's because it's meant to be a classical one application per virtual",
    "start": "1784230",
    "end": "1791610"
  },
  {
    "text": "machine AWS as you're in Google compute I believe they all have concepts of like",
    "start": "1791610",
    "end": "1796950"
  },
  {
    "text": "an api gateway or something along those lines which is a Appler application load balancer I think is what it's called an",
    "start": "1796950",
    "end": "1803760"
  },
  {
    "text": "AWS which can route a little more natively to containers instead of",
    "start": "1803760",
    "end": "1809310"
  },
  {
    "text": "virtual machines it's something that it again doesn't exist in every environment certainly want to exist in an on-premise",
    "start": "1809310",
    "end": "1815550"
  },
  {
    "text": "environment but if you only want to target a particular cloud provider you could use those types of load balancers",
    "start": "1815550",
    "end": "1822210"
  },
  {
    "text": "instead but with Azure traffic if it goes from a you know say like you have",
    "start": "1822210",
    "end": "1829170"
  },
  {
    "start": "1824000",
    "end": "1864000"
  },
  {
    "text": "to containers a web and an API server on the same machine and you want to talk to the web wants to talk to the API server",
    "start": "1829170",
    "end": "1835290"
  },
  {
    "text": "through a load balancer from Adger and actually can't do that traffic can't go from a machine to a",
    "start": "1835290",
    "end": "1841050"
  },
  {
    "text": "load balancer back to the same machine in Azure which is just a part of their design because it's not meant for this",
    "start": "1841050",
    "end": "1847050"
  },
  {
    "text": "sort of environment again you can use kubernetes services and you can just avoid load balancers entirely avoid dns",
    "start": "1847050",
    "end": "1853590"
  },
  {
    "text": "entirely and that's sort of the lesson is that kubernetes will allow you to become more cloud agnostic just by",
    "start": "1853590",
    "end": "1860010"
  },
  {
    "text": "having all these primitives that exist everywhere and so at the end of all of",
    "start": "1860010",
    "end": "1866460"
  },
  {
    "start": "1864000",
    "end": "1888000"
  },
  {
    "text": "that we have a whole bunch of different tariffs files with some parameters and then a whole bunch of kubernetes and files for",
    "start": "1866460",
    "end": "1873749"
  },
  {
    "text": "deployments and stuff and we can go from absolutely nothing to bringing up our entire system running GPU algorithms and",
    "start": "1873749",
    "end": "1880139"
  },
  {
    "text": "all of that and less than an hour which i think is pretty good when it takes 25 minutes to provision an RDS instance in",
    "start": "1880139",
    "end": "1886049"
  },
  {
    "text": "AWS so not much we can do about that one so looking forward there's a lot of",
    "start": "1886049",
    "end": "1893580"
  },
  {
    "start": "1888000",
    "end": "1998000"
  },
  {
    "text": "performance improvements coming up just the last year the GPU exposing GPUs to a",
    "start": "1893580",
    "end": "1898769"
  },
  {
    "text": "container was just added to kubernetes I believe in 1/4 I think and since then",
    "start": "1898769",
    "end": "1904470"
  },
  {
    "text": "it's become easier to expose multiple devices but then you have AWS coming out with FPGA instances and I I do think",
    "start": "1904470",
    "end": "1912450"
  },
  {
    "text": "that FPGAs might be useful for some machine learning algorithms maybe they could be faster but I have no idea how",
    "start": "1912450",
    "end": "1918330"
  },
  {
    "text": "to get kubernetes to run and expose an FPGA I don't know but it'd be really interesting to see what can happen in",
    "start": "1918330",
    "end": "1923429"
  },
  {
    "text": "the future there next there's a sort of when we're building a whole bunch of",
    "start": "1923429",
    "end": "1929279"
  },
  {
    "text": "images and shipping them around our cluster a lot we found that the biggest the whole Nick ends up being just how",
    "start": "1929279",
    "end": "1934799"
  },
  {
    "text": "big some of the images are and being able to compress them better would be an ideal situation that I'm not sure this",
    "start": "1934799",
    "end": "1940559"
  },
  {
    "text": "is likely to change multi-cloud is sort of becoming a little more of a native",
    "start": "1940559",
    "end": "1946889"
  },
  {
    "text": "thing within kubernetes so what we have is a system we can deploy inside of a particular data center but you can",
    "start": "1946889",
    "end": "1952859"
  },
  {
    "text": "imagine it would be ideal to be able to run across multiple data centers which has problems of security concerns and",
    "start": "1952859",
    "end": "1959820"
  },
  {
    "text": "data traveling over the internet you know you can also get a lot of latency and it gets confusing and hard to track",
    "start": "1959820",
    "end": "1965970"
  },
  {
    "text": "all of that and that's where things like with the service mesh would enable you to track better how your how your",
    "start": "1965970",
    "end": "1971879"
  },
  {
    "text": "services are talking to each other across all of these regions there's the concept of a federated uber Nettie's",
    "start": "1971879",
    "end": "1977999"
  },
  {
    "text": "where if you're trying to run you know it's sort of the difference between running kubernetes that's has masters in",
    "start": "1977999",
    "end": "1983970"
  },
  {
    "text": "one data center and all the nodes are part of one cluster versus do you have you know kubernetes running and five",
    "start": "1983970",
    "end": "1990480"
  },
  {
    "text": "different clusters in some way to federate access to each one of those in a common format so I think that's about",
    "start": "1990480",
    "end": "1999570"
  },
  {
    "text": "all I have right now so you can contact me for any other questions at Patrick at algorithm eucom",
    "start": "1999570",
    "end": "2005710"
  },
  {
    "text": "if you're interested in infrastructure how artificial intelligence machine learning surrealist computing kubernetes",
    "start": "2005710",
    "end": "2012160"
  },
  {
    "text": "how all that sort of matches together we're hiring for pretty much everything and you know we have a coupon code we",
    "start": "2012160",
    "end": "2019150"
  },
  {
    "text": "give people free credits every month but there's also a free coupon code Thanks",
    "start": "2019150",
    "end": "2026850"
  },
  {
    "text": "questions thank you",
    "start": "2028530",
    "end": "2032640"
  },
  {
    "text": "it is kind of scary so there's there are a couple ways to expose it and that's something where when you're using kubernetes it doesn't matter anymore",
    "start": "2045560",
    "end": "2051710"
  },
  {
    "text": "anyway yeah because one of those things like as a temporary step you can also expose it",
    "start": "2051710",
    "end": "2057379"
  },
  {
    "text": "via TCP and then you want to enable TLS and then you have self-signed certificates and it's a little bit",
    "start": "2057380",
    "end": "2062570"
  },
  {
    "text": "annoying to deal with one of those things where when we are controlling the",
    "start": "2062570",
    "end": "2068870"
  },
  {
    "text": "stuff that's running you've mounted the docker socket to only a single thing and again it only has limited permissions",
    "start": "2068870",
    "end": "2074360"
  },
  {
    "text": "and you can't do anything we don't run any privilege containers or anything like that but again it's something you",
    "start": "2074360",
    "end": "2080179"
  },
  {
    "text": "want to get away from and use kubernetes and then you can have role based access control and allow certain users to",
    "start": "2080180",
    "end": "2085940"
  },
  {
    "text": "create certain API or you know only access certain things only be able to create destroy certain resources yeah",
    "start": "2085940",
    "end": "2099640"
  },
  {
    "text": "it's an AWS thing it's in their documentation it's basically if you have an EBS volume that has data on it the",
    "start": "2105960",
    "end": "2113369"
  },
  {
    "text": "way that the volumes are stored is that they're backed up in s3 so when you're create a new volume and you attach it to",
    "start": "2113369",
    "end": "2119550"
  },
  {
    "text": "an instance from a snapshot that volume gets attached immediately but it doesn't actually fetch all the data so if you",
    "start": "2119550",
    "end": "2125970"
  },
  {
    "text": "have a whole bunch of blocks it basically every time you access a block for the first time it has to get downloaded from s3 which can be slow and",
    "start": "2125970",
    "end": "2133440"
  },
  {
    "text": "if you have say you know 250 gigabyte Drive then it's got to download all of",
    "start": "2133440",
    "end": "2138480"
  },
  {
    "text": "that data from s3 even if there's nothing there if it's an empty block it doesn't know it's an empty block until",
    "start": "2138480",
    "end": "2143640"
  },
  {
    "text": "it fetches it yeah",
    "start": "2143640",
    "end": "2147529"
  }
]