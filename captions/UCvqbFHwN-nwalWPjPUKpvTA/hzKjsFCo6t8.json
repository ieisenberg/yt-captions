[
  {
    "text": "hi everyone uh I'm Lily uh I'm a student",
    "start": "280",
    "end": "3560"
  },
  {
    "text": "from UC Berkeley and I'm also part of",
    "start": "3560",
    "end": "5759"
  },
  {
    "text": "any scale so today I'm happy to give you",
    "start": "5759",
    "end": "8679"
  },
  {
    "text": "the talk about uh efficient LM",
    "start": "8679",
    "end": "10920"
  },
  {
    "text": "deployment a unified approach with Ray",
    "start": "10920",
    "end": "13719"
  },
  {
    "text": "VM and",
    "start": "13719",
    "end": "16198"
  },
  {
    "text": "kubernetes so for today's talk uh I will",
    "start": "16199",
    "end": "19119"
  },
  {
    "text": "Begin by discussing like various uh",
    "start": "19119",
    "end": "21160"
  },
  {
    "text": "infrastructure challenges associated",
    "start": "21160",
    "end": "23039"
  },
  {
    "text": "with generative AI workloads then I will",
    "start": "23039",
    "end": "25640"
  },
  {
    "text": "cover like kuber and kuber Ray like how",
    "start": "25640",
    "end": "28119"
  },
  {
    "text": "to leverage those two techniques to man",
    "start": "28119",
    "end": "30240"
  },
  {
    "text": "the um clusters for those demanding",
    "start": "30240",
    "end": "32800"
  },
  {
    "text": "applications then we'll talk about those",
    "start": "32800",
    "end": "35000"
  },
  {
    "text": "Ray AI libraries including Ray data",
    "start": "35000",
    "end": "37600"
  },
  {
    "text": "training serving uh and rank two and",
    "start": "37600",
    "end": "40760"
  },
  {
    "text": "lastly I will also introduce VM a",
    "start": "40760",
    "end": "43520"
  },
  {
    "text": "efficient like inference engine for LM",
    "start": "43520",
    "end": "47280"
  },
  {
    "text": "serving okay so let's get",
    "start": "47280",
    "end": "50920"
  },
  {
    "text": "started um so I will outline so I outla",
    "start": "50920",
    "end": "55000"
  },
  {
    "text": "three major challenges in generative AI",
    "start": "55000",
    "end": "57239"
  },
  {
    "text": "workflows nowadays so the first one is a",
    "start": "57239",
    "end": "60039"
  },
  {
    "text": "scale um the scale of the models is",
    "start": "60039",
    "end": "63000"
  },
  {
    "text": "numerous and the second is um um the the",
    "start": "63000",
    "end": "66920"
  },
  {
    "text": "pipeline of those LM work cloths are",
    "start": "66920",
    "end": "69240"
  },
  {
    "text": "right are quite complicated and lastly",
    "start": "69240",
    "end": "72200"
  },
  {
    "text": "um is is the efficiency of those like um",
    "start": "72200",
    "end": "74799"
  },
  {
    "text": "LM serving um is is very low like we",
    "start": "74799",
    "end": "78159"
  },
  {
    "text": "don't have high efficiency so I will",
    "start": "78159",
    "end": "80119"
  },
  {
    "text": "dive them those challenges in one by one",
    "start": "80119",
    "end": "82720"
  },
  {
    "text": "and then tell like how those like",
    "start": "82720",
    "end": "84759"
  },
  {
    "text": "techniques can be used to solve those",
    "start": "84759",
    "end": "88240"
  },
  {
    "text": "challenges so first",
    "start": "88240",
    "end": "90479"
  },
  {
    "text": "um as we all know the past few years",
    "start": "90479",
    "end": "92880"
  },
  {
    "text": "have witnessed a rapid growth of model",
    "start": "92880",
    "end": "95320"
  },
  {
    "text": "size like large language models so if",
    "start": "95320",
    "end": "97560"
  },
  {
    "text": "you look at the Timeline here that we",
    "start": "97560",
    "end": "99840"
  },
  {
    "text": "can see notable uh model size like uh",
    "start": "99840",
    "end": "102640"
  },
  {
    "text": "notable models like gpt3 um Bloom each",
    "start": "102640",
    "end": "106040"
  },
  {
    "text": "progressively larger than its U previous",
    "start": "106040",
    "end": "108640"
  },
  {
    "text": "generations and as we move forward in",
    "start": "108640",
    "end": "110960"
  },
  {
    "text": "time the model size increased",
    "start": "110960",
    "end": "113280"
  },
  {
    "text": "dramatically uh with models reaching up",
    "start": "113280",
    "end": "115759"
  },
  {
    "text": "to like hundreds of billions of",
    "start": "115759",
    "end": "117880"
  },
  {
    "text": "parameters and the trend is very ible",
    "start": "117880",
    "end": "120640"
  },
  {
    "text": "with examples such as like llama 3.1 and",
    "start": "120640",
    "end": "123920"
  },
  {
    "text": "45 billion parameters or like Gro model",
    "start": "123920",
    "end": "127399"
  },
  {
    "text": "with over 300 billion 300 billion",
    "start": "127399",
    "end": "130160"
  },
  {
    "text": "parameters or like GP GP uh open AI GPT",
    "start": "130160",
    "end": "135200"
  },
  {
    "text": "Series so why are the generative AI",
    "start": "135200",
    "end": "138239"
  },
  {
    "text": "models get bigger and bigger so the",
    "start": "138239",
    "end": "140760"
  },
  {
    "text": "reason is that as we increase the number",
    "start": "140760",
    "end": "143239"
  },
  {
    "text": "of parameters those models also tend to",
    "start": "143239",
    "end": "146040"
  },
  {
    "text": "demonstrate better accuracy so larger",
    "start": "146040",
    "end": "148959"
  },
  {
    "text": "models are generally more capable of",
    "start": "148959",
    "end": "151440"
  },
  {
    "text": "understanding complex language patterns",
    "start": "151440",
    "end": "154040"
  },
  {
    "text": "which improve their effectiveness across",
    "start": "154040",
    "end": "156360"
  },
  {
    "text": "a wider range of",
    "start": "156360",
    "end": "158519"
  },
  {
    "text": "tasks so as we can see here small models",
    "start": "158519",
    "end": "162000"
  },
  {
    "text": "have limited",
    "start": "162000",
    "end": "163400"
  },
  {
    "text": "capabilities uh like uh in areas like",
    "start": "163400",
    "end": "166080"
  },
  {
    "text": "question answering or",
    "start": "166080",
    "end": "167879"
  },
  {
    "text": "arithmatic however as model grow larger",
    "start": "167879",
    "end": "171200"
  },
  {
    "text": "not only do their question answering",
    "start": "171200",
    "end": "173159"
  },
  {
    "text": "arithmatic uh skills improve but they",
    "start": "173159",
    "end": "176120"
  },
  {
    "text": "also begin to demonstrate additional",
    "start": "176120",
    "end": "177959"
  },
  {
    "text": "abilities such as like summarization",
    "start": "177959",
    "end": "180239"
  },
  {
    "text": "code code",
    "start": "180239",
    "end": "183360"
  },
  {
    "text": "completion but at the same time as the",
    "start": "184319",
    "end": "186959"
  },
  {
    "text": "model grows larger the compute required",
    "start": "186959",
    "end": "189599"
  },
  {
    "text": "to like to train those models increase",
    "start": "189599",
    "end": "192440"
  },
  {
    "text": "at uh increase at an extraordinary rate",
    "start": "192440",
    "end": "195720"
  },
  {
    "text": "so in fact as Illustrated here training",
    "start": "195720",
    "end": "198799"
  },
  {
    "text": "compute has been growing 10 times um",
    "start": "198799",
    "end": "202560"
  },
  {
    "text": "every 18",
    "start": "202560",
    "end": "205319"
  },
  {
    "text": "months but in contrast the M law shows",
    "start": "205959",
    "end": "209480"
  },
  {
    "text": "that general purpose CPU advancement",
    "start": "209480",
    "end": "211799"
  },
  {
    "text": "grow at a much lower rate so it doubl",
    "start": "211799",
    "end": "214720"
  },
  {
    "text": "Lings about every uh 18 months so there",
    "start": "214720",
    "end": "217799"
  },
  {
    "text": "is a gap here between the uh between the",
    "start": "217799",
    "end": "220560"
  },
  {
    "text": "demand of the generate AI work clothes",
    "start": "220560",
    "end": "222920"
  },
  {
    "text": "and and the and the supply we can",
    "start": "222920",
    "end": "224920"
  },
  {
    "text": "provide by the the current",
    "start": "224920",
    "end": "227120"
  },
  {
    "text": "Hardware so as this Gap grows like",
    "start": "227120",
    "end": "229680"
  },
  {
    "text": "specialized Hardware like gpus tpus um I",
    "start": "229680",
    "end": "234040"
  },
  {
    "text": "developed um to support like like to",
    "start": "234040",
    "end": "237680"
  },
  {
    "text": "support those um those work CL like we",
    "start": "237680",
    "end": "240439"
  },
  {
    "text": "have Nvidia gpus like AMD gpus tpus",
    "start": "240439",
    "end": "244640"
  },
  {
    "text": "different Hardware now question is that",
    "start": "244640",
    "end": "247760"
  },
  {
    "text": "with those Advanced Hardware specialized",
    "start": "247760",
    "end": "250000"
  },
  {
    "text": "Hardware can we solve the",
    "start": "250000",
    "end": "252959"
  },
  {
    "text": "problem and the answer here is it's not",
    "start": "252959",
    "end": "255560"
  },
  {
    "text": "enough so similar story even with those",
    "start": "255560",
    "end": "258720"
  },
  {
    "text": "specialized Hardware the gap between the",
    "start": "258720",
    "end": "260959"
  },
  {
    "text": "demand and Supply increases 256 times",
    "start": "260959",
    "end": "264800"
  },
  {
    "text": "every 18 months so that being said a",
    "start": "264800",
    "end": "268240"
  },
  {
    "text": "single machine or a single Hardware is",
    "start": "268240",
    "end": "270800"
  },
  {
    "text": "definitely not enough to train those",
    "start": "270800",
    "end": "272720"
  },
  {
    "text": "large language models actually nowadays",
    "start": "272720",
    "end": "275880"
  },
  {
    "text": "as model grows larger and larger um a",
    "start": "275880",
    "end": "279759"
  },
  {
    "text": "single node may not even be enough for",
    "start": "279759",
    "end": "282080"
  },
  {
    "text": "serving the",
    "start": "282080",
    "end": "284680"
  },
  {
    "text": "model so let's talk about the",
    "start": "285919",
    "end": "288880"
  },
  {
    "text": "scalability uh requirements for the",
    "start": "288880",
    "end": "291160"
  },
  {
    "text": "current gener AI workloads today so with",
    "start": "291160",
    "end": "294080"
  },
  {
    "text": "a single node uh a single machine maybe",
    "start": "294080",
    "end": "296960"
  },
  {
    "text": "you have like multiple gpus on a single",
    "start": "296960",
    "end": "299000"
  },
  {
    "text": "machine you can f tune a small model",
    "start": "299000",
    "end": "301400"
  },
  {
    "text": "locally on limited data set this is",
    "start": "301400",
    "end": "304320"
  },
  {
    "text": "feasible for model uh for like for small",
    "start": "304320",
    "end": "307400"
  },
  {
    "text": "tasks but quickly become insufficient as",
    "start": "307400",
    "end": "310120"
  },
  {
    "text": "models and data",
    "start": "310120",
    "end": "311840"
  },
  {
    "text": "grow to F to a middle scale model you",
    "start": "311840",
    "end": "315240"
  },
  {
    "text": "need a larger setup from one to 100",
    "start": "315240",
    "end": "318000"
  },
  {
    "text": "notes this allow you to distribute the",
    "start": "318000",
    "end": "320520"
  },
  {
    "text": "model across multiple machines and",
    "start": "320520",
    "end": "322639"
  },
  {
    "text": "leverage data parm to process larger",
    "start": "322639",
    "end": "324840"
  },
  {
    "text": "data set efficiently um batch inference",
    "start": "324840",
    "end": "327759"
  },
  {
    "text": "is also possible on larger scale like on",
    "start": "327759",
    "end": "330479"
  },
  {
    "text": "structure data at this",
    "start": "330479",
    "end": "332039"
  },
  {
    "text": "level however if you want to train a",
    "start": "332039",
    "end": "334520"
  },
  {
    "text": "foundation model from scratch like a",
    "start": "334520",
    "end": "336639"
  },
  {
    "text": "model with billions of parameters trains",
    "start": "336639",
    "end": "338639"
  },
  {
    "text": "on trillions of tokens you need 100 or",
    "start": "338639",
    "end": "341360"
  },
  {
    "text": "even thousands of nodes so this is a",
    "start": "341360",
    "end": "343639"
  },
  {
    "text": "scale at which major AI companies uh",
    "start": "343639",
    "end": "346720"
  },
  {
    "text": "operate on um to develop state of art",
    "start": "346720",
    "end": "350240"
  },
  {
    "text": "models that being said uh with such",
    "start": "350240",
    "end": "353720"
  },
  {
    "text": "large scaleability of current generate",
    "start": "353720",
    "end": "355919"
  },
  {
    "text": "generative AI workloads you really need",
    "start": "355919",
    "end": "358199"
  },
  {
    "text": "a AI infrastructure then can manage",
    "start": "358199",
    "end": "361039"
  },
  {
    "text": "absorve such a large amount of data in",
    "start": "361039",
    "end": "363560"
  },
  {
    "text": "computer",
    "start": "363560",
    "end": "366000"
  },
  {
    "text": "sources and the Second Challenge uh in",
    "start": "367360",
    "end": "370240"
  },
  {
    "text": "generative AI workloads nowadays is a",
    "start": "370240",
    "end": "372919"
  },
  {
    "text": "complexity so let me give you a very",
    "start": "372919",
    "end": "375120"
  },
  {
    "text": "concrete example I HF so what is I it's",
    "start": "375120",
    "end": "378919"
  },
  {
    "text": "reinforcement learning with human",
    "start": "378919",
    "end": "380639"
  },
  {
    "text": "feedback so uh it is a technique noways",
    "start": "380639",
    "end": "383919"
  },
  {
    "text": "people use to improve the quality like",
    "start": "383919",
    "end": "386639"
  },
  {
    "text": "make the model safer like uh have a",
    "start": "386639",
    "end": "389280"
  },
  {
    "text": "better quality generate a better um",
    "start": "389280",
    "end": "392160"
  },
  {
    "text": "answers so uh what is so like GPT uh",
    "start": "392160",
    "end": "396199"
  },
  {
    "text": "like GPT models uh most of the models",
    "start": "396199",
    "end": "398560"
  },
  {
    "text": "nowadays we use uh they are they are",
    "start": "398560",
    "end": "400560"
  },
  {
    "text": "trained uh with",
    "start": "400560",
    "end": "402360"
  },
  {
    "text": "ilf so what what is IHF really doing",
    "start": "402360",
    "end": "405400"
  },
  {
    "text": "here let's look at those um pipelines in",
    "start": "405400",
    "end": "407520"
  },
  {
    "text": "more",
    "start": "407520",
    "end": "408280"
  },
  {
    "text": "detail so the first step step of IHF um",
    "start": "408280",
    "end": "412880"
  },
  {
    "text": "is is do you need to do some like fine",
    "start": "412880",
    "end": "415039"
  },
  {
    "text": "tuning so basically you will have some",
    "start": "415039",
    "end": "417199"
  },
  {
    "text": "input data and you send to some label",
    "start": "417199",
    "end": "419840"
  },
  {
    "text": "you hire some laer labeler you pay them",
    "start": "419840",
    "end": "422440"
  },
  {
    "text": "the money to uh to to let them generate",
    "start": "422440",
    "end": "425039"
  },
  {
    "text": "the correct answer like the good answer",
    "start": "425039",
    "end": "426639"
  },
  {
    "text": "for those prompts and then you find to",
    "start": "426639",
    "end": "428720"
  },
  {
    "text": "the model all those",
    "start": "428720",
    "end": "431319"
  },
  {
    "text": "data the second step is so the first",
    "start": "431319",
    "end": "434080"
  },
  {
    "text": "step is just to make the model generate",
    "start": "434080",
    "end": "436199"
  },
  {
    "text": "some um generate some reasonable output",
    "start": "436199",
    "end": "439039"
  },
  {
    "text": "so instead of repeating the",
    "start": "439039",
    "end": "441199"
  },
  {
    "text": "question the second step is you are uh",
    "start": "441199",
    "end": "444080"
  },
  {
    "text": "use human feedback to improve the model",
    "start": "444080",
    "end": "447280"
  },
  {
    "text": "you will still hire some labelers and",
    "start": "447280",
    "end": "449360"
  },
  {
    "text": "you you are give the labeler some like",
    "start": "449360",
    "end": "451520"
  },
  {
    "text": "example answers and you are let the",
    "start": "451520",
    "end": "453840"
  },
  {
    "text": "labelers to decide which answer is",
    "start": "453840",
    "end": "456120"
  },
  {
    "text": "better and you use those information to",
    "start": "456120",
    "end": "458840"
  },
  {
    "text": "train a reward model so what is a reward",
    "start": "458840",
    "end": "461400"
  },
  {
    "text": "model a reward model is saying that oh",
    "start": "461400",
    "end": "463680"
  },
  {
    "text": "given the prompt the model generate two",
    "start": "463680",
    "end": "465879"
  },
  {
    "text": "answer answer a and answer B which",
    "start": "465879",
    "end": "468240"
  },
  {
    "text": "answer is better right so the reward",
    "start": "468240",
    "end": "470039"
  },
  {
    "text": "model tells us oh answer a is better",
    "start": "470039",
    "end": "472280"
  },
  {
    "text": "than answer B so this is a like a",
    "start": "472280",
    "end": "475120"
  },
  {
    "text": "judgment right so this a reward",
    "start": "475120",
    "end": "477520"
  },
  {
    "text": "model and the last step you train a",
    "start": "477520",
    "end": "480599"
  },
  {
    "text": "policy model so what is a policy model",
    "start": "480599",
    "end": "482960"
  },
  {
    "text": "so in the ideal case you want the um you",
    "start": "482960",
    "end": "485840"
  },
  {
    "text": "want the large language model to",
    "start": "485840",
    "end": "488240"
  },
  {
    "text": "generate a good answer automatically so",
    "start": "488240",
    "end": "490840"
  },
  {
    "text": "what you will do is you will let the LM",
    "start": "490840",
    "end": "493039"
  },
  {
    "text": "to generate some answers and you will",
    "start": "493039",
    "end": "494960"
  },
  {
    "text": "reward the model to score those answers",
    "start": "494960",
    "end": "497840"
  },
  {
    "text": "and you will give higher score to those",
    "start": "497840",
    "end": "500039"
  },
  {
    "text": "good answer give low score to those bad",
    "start": "500039",
    "end": "501879"
  },
  {
    "text": "answers and you will use those",
    "start": "501879",
    "end": "503520"
  },
  {
    "text": "information to update the LM model",
    "start": "503520",
    "end": "506159"
  },
  {
    "text": "parameters so that the LM can learn that",
    "start": "506159",
    "end": "509000"
  },
  {
    "text": "oh I should always generate a good",
    "start": "509000",
    "end": "510400"
  },
  {
    "text": "answer instead of saying some like uh",
    "start": "510400",
    "end": "512479"
  },
  {
    "text": "bad out instead of generating some bad",
    "start": "512479",
    "end": "515279"
  },
  {
    "text": "outputs so those are the three steps uh",
    "start": "515279",
    "end": "517839"
  },
  {
    "text": "of I and I think this is um this is very",
    "start": "517839",
    "end": "520839"
  },
  {
    "text": "important for now uh for L to have a a",
    "start": "520839",
    "end": "523959"
  },
  {
    "text": "good",
    "start": "523959",
    "end": "524880"
  },
  {
    "text": "performance but if you look at this",
    "start": "524880",
    "end": "526760"
  },
  {
    "text": "pipeline it's quite",
    "start": "526760",
    "end": "528519"
  },
  {
    "text": "complex so you have like data some data",
    "start": "528519",
    "end": "531680"
  },
  {
    "text": "cleaning data labeling right so you you",
    "start": "531680",
    "end": "534000"
  },
  {
    "text": "need to uh prepare the data for the",
    "start": "534000",
    "end": "536920"
  },
  {
    "text": "labeler you also have some model",
    "start": "536920",
    "end": "539000"
  },
  {
    "text": "inference right because you ask a model",
    "start": "539000",
    "end": "541360"
  },
  {
    "text": "to generate some answer first and then",
    "start": "541360",
    "end": "543959"
  },
  {
    "text": "ask a labeler to score which answer is",
    "start": "543959",
    "end": "545959"
  },
  {
    "text": "bad which answer is good which answer is",
    "start": "545959",
    "end": "548000"
  },
  {
    "text": "bad you also need to uh sample some",
    "start": "548000",
    "end": "550440"
  },
  {
    "text": "output output to train the policy model",
    "start": "550440",
    "end": "553120"
  },
  {
    "text": "so you need to do the model",
    "start": "553120",
    "end": "555640"
  },
  {
    "text": "inference and also you need to do model",
    "start": "555640",
    "end": "558040"
  },
  {
    "text": "training and you need to train three",
    "start": "558040",
    "end": "559959"
  },
  {
    "text": "models right so first you need to find",
    "start": "559959",
    "end": "562040"
  },
  {
    "text": "through the model to make the model",
    "start": "562040",
    "end": "563720"
  },
  {
    "text": "generate some reasonable output second",
    "start": "563720",
    "end": "566480"
  },
  {
    "text": "you need to train the reward model so",
    "start": "566480",
    "end": "568320"
  },
  {
    "text": "basically you need to know which answer",
    "start": "568320",
    "end": "569880"
  },
  {
    "text": "is good which answer is bad so you need",
    "start": "569880",
    "end": "571680"
  },
  {
    "text": "to train the reward model and lastly you",
    "start": "571680",
    "end": "573920"
  },
  {
    "text": "need to train the policy model to update",
    "start": "573920",
    "end": "576000"
  },
  {
    "text": "the large language model to make it",
    "start": "576000",
    "end": "577800"
  },
  {
    "text": "generate the final output so we need to",
    "start": "577800",
    "end": "579880"
  },
  {
    "text": "train three",
    "start": "579880",
    "end": "582519"
  },
  {
    "text": "models as you can see here that a single",
    "start": "582560",
    "end": "585440"
  },
  {
    "text": "I HF pipeline it involves uh data",
    "start": "585440",
    "end": "588360"
  },
  {
    "text": "processing model inference model suring",
    "start": "588360",
    "end": "591079"
  },
  {
    "text": "and model training so what this mean it",
    "start": "591079",
    "end": "593839"
  },
  {
    "text": "means that um we kind of need a very uh",
    "start": "593839",
    "end": "597160"
  },
  {
    "text": "we need a layered like infrastructure",
    "start": "597160",
    "end": "599959"
  },
  {
    "text": "so that those ai ai scientists machine",
    "start": "599959",
    "end": "602320"
  },
  {
    "text": "learning scientists they can focus on",
    "start": "602320",
    "end": "604279"
  },
  {
    "text": "like data processing model training or",
    "start": "604279",
    "end": "606839"
  },
  {
    "text": "model inference and other people like U",
    "start": "606839",
    "end": "609760"
  },
  {
    "text": "infrastruct infrastructure engineer they",
    "start": "609760",
    "end": "611880"
  },
  {
    "text": "can handle those um infrastructure stuff",
    "start": "611880",
    "end": "615000"
  },
  {
    "text": "like of like how to handle the machines",
    "start": "615000",
    "end": "617279"
  },
  {
    "text": "how to handle the Clusters basically",
    "start": "617279",
    "end": "619360"
  },
  {
    "text": "want to separate the separate the task",
    "start": "619360",
    "end": "621200"
  },
  {
    "text": "because this pipel is quite",
    "start": "621200",
    "end": "624880"
  },
  {
    "text": "complicated and the last challenge I",
    "start": "626279",
    "end": "628680"
  },
  {
    "text": "want to um mentioned today is about the",
    "start": "628680",
    "end": "631160"
  },
  {
    "text": "LM serving efficiency so serving large",
    "start": "631160",
    "end": "634279"
  },
  {
    "text": "language models is surprisingly slow and",
    "start": "634279",
    "end": "637040"
  },
  {
    "text": "expensive um before vrm U let me give",
    "start": "637040",
    "end": "640160"
  },
  {
    "text": "you a concrete example a single a100 GPU",
    "start": "640160",
    "end": "643519"
  },
  {
    "text": "which is kind of advanced GPU can serve",
    "start": "643519",
    "end": "645920"
  },
  {
    "text": "only less than one request per second U",
    "start": "645920",
    "end": "649279"
  },
  {
    "text": "for a moderate size of model like 13",
    "start": "649279",
    "end": "651200"
  },
  {
    "text": "billion parameter model it's a it's a",
    "start": "651200",
    "end": "653639"
  },
  {
    "text": "moderate size it's not a very big it's",
    "start": "653639",
    "end": "655839"
  },
  {
    "text": "yeah but even with a moderate size model",
    "start": "655839",
    "end": "658480"
  },
  {
    "text": "you can only ser less than one request",
    "start": "658480",
    "end": "660279"
  },
  {
    "text": "per second which it means like if you",
    "start": "660279",
    "end": "662800"
  },
  {
    "text": "have hundreds of user or thousands of",
    "start": "662800",
    "end": "664519"
  },
  {
    "text": "users you need a tons of gpus uh to to",
    "start": "664519",
    "end": "668360"
  },
  {
    "text": "to achieve a good performance and we can",
    "start": "668360",
    "end": "670959"
  },
  {
    "text": "see like questions like how do we uh how",
    "start": "670959",
    "end": "674880"
  },
  {
    "text": "do we serve L more",
    "start": "674880",
    "end": "678160"
  },
  {
    "text": "efficiently so looking back on those uh",
    "start": "679160",
    "end": "681839"
  },
  {
    "text": "looking back on those challenges so what",
    "start": "681839",
    "end": "683839"
  },
  {
    "text": "are the possible solutions so today so I",
    "start": "683839",
    "end": "687279"
  },
  {
    "text": "will dive into three directions to solve",
    "start": "687279",
    "end": "689720"
  },
  {
    "text": "the challenges the first one is",
    "start": "689720",
    "end": "691519"
  },
  {
    "text": "kubernetes plus kuber ring um they uh",
    "start": "691519",
    "end": "695800"
  },
  {
    "text": "the second is a r data R train R Ser R",
    "start": "695800",
    "end": "698440"
  },
  {
    "text": "to which are those like R AI libraries",
    "start": "698440",
    "end": "700880"
  },
  {
    "text": "to facilitate uh the users to De develop",
    "start": "700880",
    "end": "703920"
  },
  {
    "text": "AI applications and lastly I will",
    "start": "703920",
    "end": "706560"
  },
  {
    "text": "introduce vrm so let's dive into",
    "start": "706560",
    "end": "708800"
  },
  {
    "text": "kubernetes plus kuber ring um so this is",
    "start": "708800",
    "end": "712680"
  },
  {
    "text": "uh this is a overview of how Ray",
    "start": "712680",
    "end": "715480"
  },
  {
    "text": "integrates with kubernetes to create a",
    "start": "715480",
    "end": "717959"
  },
  {
    "text": "scalable and flexible infrastructure for",
    "start": "717959",
    "end": "720279"
  },
  {
    "text": "AI workloads um on the top level are",
    "start": "720279",
    "end": "723880"
  },
  {
    "text": "different rate AI library right you have",
    "start": "723880",
    "end": "726279"
  },
  {
    "text": "Ray data for uh data for man for for",
    "start": "726279",
    "end": "729720"
  },
  {
    "text": "manage large scale data you have R train",
    "start": "729720",
    "end": "732440"
  },
  {
    "text": "to uh to do dist distributed model",
    "start": "732440",
    "end": "735320"
  },
  {
    "text": "training like you have tune rain tun to",
    "start": "735320",
    "end": "737800"
  },
  {
    "text": "do hyperparameter tuning race serve for",
    "start": "737800",
    "end": "740160"
  },
  {
    "text": "to do model",
    "start": "740160",
    "end": "741480"
  },
  {
    "text": "sering and below the AI libraries you",
    "start": "741480",
    "end": "744720"
  },
  {
    "text": "have the",
    "start": "744720",
    "end": "745680"
  },
  {
    "text": "recall so the recall layers they",
    "start": "745680",
    "end": "748680"
  },
  {
    "text": "includes some like core functionalities",
    "start": "748680",
    "end": "750800"
  },
  {
    "text": "and Integrations that can power the",
    "start": "750800",
    "end": "753040"
  },
  {
    "text": "distributed infrastructure so this layer",
    "start": "753040",
    "end": "755560"
  },
  {
    "text": "ensures that Ray can work across various",
    "start": "755560",
    "end": "758240"
  },
  {
    "text": "environments from unpromised clusters to",
    "start": "758240",
    "end": "761279"
  },
  {
    "text": "cloud-based",
    "start": "761279",
    "end": "763240"
  },
  {
    "text": "Solutions and lastly you have the uh QB",
    "start": "763240",
    "end": "766839"
  },
  {
    "text": "Ray so QB Ray is highlight in this",
    "start": "766839",
    "end": "769360"
  },
  {
    "text": "slides um as a critical uh technique",
    "start": "769360",
    "end": "772639"
  },
  {
    "text": "it's a critical integration point",
    "start": "772639",
    "end": "774440"
  },
  {
    "text": "between Ray and kuber NES so kuber Ray",
    "start": "774440",
    "end": "777360"
  },
  {
    "text": "Manch the life cycle of Ray cluster and",
    "start": "777360",
    "end": "779560"
  },
  {
    "text": "their applications within kuber netti",
    "start": "779560",
    "end": "781760"
  },
  {
    "text": "environments it enable Ray workloads to",
    "start": "781760",
    "end": "784320"
  },
  {
    "text": "be orchestrated on",
    "start": "784320",
    "end": "787360"
  },
  {
    "text": "kubernetes and the last layer and the",
    "start": "787360",
    "end": "789560"
  },
  {
    "text": "bottom is a kuber netti and Cloud",
    "start": "789560",
    "end": "792040"
  },
  {
    "text": "VMS so kubernetes provides a container o",
    "start": "792040",
    "end": "795000"
  },
  {
    "text": "stration layer which manages the develop",
    "start": "795000",
    "end": "797279"
  },
  {
    "text": "uh deployment scaling and operation of",
    "start": "797279",
    "end": "799920"
  },
  {
    "text": "Ray clusters and Cloud VMS uh allow the",
    "start": "799920",
    "end": "803040"
  },
  {
    "text": "infrastructure to be scaled on demand",
    "start": "803040",
    "end": "805120"
  },
  {
    "text": "ensure that resource can be uh can match",
    "start": "805120",
    "end": "807519"
  },
  {
    "text": "the work hold requirements",
    "start": "807519",
    "end": "811120"
  },
  {
    "text": "so what's the benefit of uh of such a",
    "start": "811839",
    "end": "814320"
  },
  {
    "text": "layer structure so this layer structure",
    "start": "814320",
    "end": "817120"
  },
  {
    "text": "with KU Ray provides a clear separation",
    "start": "817120",
    "end": "820639"
  },
  {
    "text": "uh between the respons uh separation of",
    "start": "820639",
    "end": "823600"
  },
  {
    "text": "the responsibilities between the um data",
    "start": "823600",
    "end": "826720"
  },
  {
    "text": "and data ml scientist and the",
    "start": "826720",
    "end": "828440"
  },
  {
    "text": "infrastructure Engineers so the data and",
    "start": "828440",
    "end": "831120"
  },
  {
    "text": "M ml scientist they only write some",
    "start": "831120",
    "end": "833480"
  },
  {
    "text": "Python scripts um within the ray",
    "start": "833480",
    "end": "836160"
  },
  {
    "text": "environment to create different task to",
    "start": "836160",
    "end": "839079"
  },
  {
    "text": "to check the house to handle like those",
    "start": "839079",
    "end": "840800"
  },
  {
    "text": "scaling requests for distributed Ai",
    "start": "840800",
    "end": "843199"
  },
  {
    "text": "workloads and for those INF",
    "start": "843199",
    "end": "845560"
  },
  {
    "text": "infrastructure Engineers they manage the",
    "start": "845560",
    "end": "847680"
  },
  {
    "text": "kuber uh kuber environment the integrate",
    "start": "847680",
    "end": "851519"
  },
  {
    "text": "kuber Ray with kuber netcosy can use",
    "start": "851519",
    "end": "854120"
  },
  {
    "text": "different tools we have in",
    "start": "854120",
    "end": "857399"
  },
  {
    "text": "kues so why do we want to use uh Ray on",
    "start": "859320",
    "end": "863079"
  },
  {
    "text": "kubernetes I think the the answer is um",
    "start": "863079",
    "end": "866120"
  },
  {
    "text": "is kind of um obvious so uh we want to",
    "start": "866120",
    "end": "869279"
  },
  {
    "text": "leverage those benefits of uh kubernetes",
    "start": "869279",
    "end": "871920"
  },
  {
    "text": "right like it have some infrastructure",
    "start": "871920",
    "end": "873920"
  },
  {
    "text": "automations kubernetes automates a",
    "start": "873920",
    "end": "876199"
  },
  {
    "text": "deployment scaling of those um",
    "start": "876199",
    "end": "879120"
  },
  {
    "text": "containerized application reduce the",
    "start": "879120",
    "end": "881360"
  },
  {
    "text": "manual effort required to set up and",
    "start": "881360",
    "end": "883880"
  },
  {
    "text": "maintain those",
    "start": "883880",
    "end": "885079"
  },
  {
    "text": "infrastructure um and also the",
    "start": "885079",
    "end": "886800"
  },
  {
    "text": "scalability so kubernetes uh has good",
    "start": "886800",
    "end": "889800"
  },
  {
    "text": "design for horizontal scalability making",
    "start": "889800",
    "end": "892399"
  },
  {
    "text": "an idea to scale those rate clusters and",
    "start": "892399",
    "end": "895440"
  },
  {
    "text": "also like high availability uh",
    "start": "895440",
    "end": "897800"
  },
  {
    "text": "reliability um resource efficiency and",
    "start": "897800",
    "end": "900680"
  },
  {
    "text": "the multic cloud deployment so all those",
    "start": "900680",
    "end": "903120"
  },
  {
    "text": "benefits uh make us want to leverage it",
    "start": "903120",
    "end": "906079"
  },
  {
    "text": "use the kubernetes uh within the uh with",
    "start": "906079",
    "end": "908519"
  },
  {
    "text": "the",
    "start": "908519",
    "end": "910639"
  },
  {
    "text": "ring so uh next let's shift our focus on",
    "start": "911839",
    "end": "916040"
  },
  {
    "text": "to like Ray data Ray train and Ray surve",
    "start": "916040",
    "end": "919240"
  },
  {
    "text": "those Ray libraries to enable the users",
    "start": "919240",
    "end": "921959"
  },
  {
    "text": "to build AI applications without the",
    "start": "921959",
    "end": "924279"
  },
  {
    "text": "need to manage online infrastructure",
    "start": "924279",
    "end": "928560"
  },
  {
    "text": "so the first is Ray data so what is Ray",
    "start": "929360",
    "end": "932040"
  },
  {
    "text": "data so you can think of Ray data as a",
    "start": "932040",
    "end": "934399"
  },
  {
    "text": "last mile bridge it connects the storage",
    "start": "934399",
    "end": "937759"
  },
  {
    "text": "or the ETL pipeline outputs uh to the",
    "start": "937759",
    "end": "940639"
  },
  {
    "text": "distributed applications uh within the",
    "start": "940639",
    "end": "943279"
  },
  {
    "text": "race ecosystem so it enables the users",
    "start": "943279",
    "end": "946880"
  },
  {
    "text": "to load and pre-process data for",
    "start": "946880",
    "end": "949279"
  },
  {
    "text": "distributed machine learning training",
    "start": "949279",
    "end": "950839"
  },
  {
    "text": "pipelines and it offers some key",
    "start": "950839",
    "end": "953079"
  },
  {
    "text": "features for example it can provide fast",
    "start": "953079",
    "end": "955920"
  },
  {
    "text": "recovery from like out of memory arrow",
    "start": "955920",
    "end": "958600"
  },
  {
    "text": "and can support for hetrogeneous",
    "start": "958600",
    "end": "960160"
  },
  {
    "text": "clusters and it can generate inte uh it",
    "start": "960160",
    "end": "963079"
  },
  {
    "text": "can guarantee data Integrity right so we",
    "start": "963079",
    "end": "966160"
  },
  {
    "text": "it will not have any drob R during the",
    "start": "966160",
    "end": "968440"
  },
  {
    "text": "distributed data set",
    "start": "968440",
    "end": "971480"
  },
  {
    "text": "iteration so after you get the data from",
    "start": "972360",
    "end": "974839"
  },
  {
    "text": "the ray data you will uh you will go to",
    "start": "974839",
    "end": "977440"
  },
  {
    "text": "ring train right so what is rain train",
    "start": "977440",
    "end": "979920"
  },
  {
    "text": "so ring train is just uh an easier an",
    "start": "979920",
    "end": "983399"
  },
  {
    "text": "easier way like for users to focus",
    "start": "983399",
    "end": "986319"
  },
  {
    "text": "solely on the training logic without",
    "start": "986319",
    "end": "988519"
  },
  {
    "text": "having to to manage the infrastructure",
    "start": "988519",
    "end": "990560"
  },
  {
    "text": "to be more concrete so what the user",
    "start": "990560",
    "end": "992639"
  },
  {
    "text": "need to do so user need to first uh",
    "start": "992639",
    "end": "995399"
  },
  {
    "text": "specify the desired setting right like",
    "start": "995399",
    "end": "997639"
  },
  {
    "text": "how many gpus how many CPUs to use what",
    "start": "997639",
    "end": "1000360"
  },
  {
    "text": "are the resources I need uh and the",
    "start": "1000360",
    "end": "1003079"
  },
  {
    "text": "second what is the training function",
    "start": "1003079",
    "end": "1005120"
  },
  {
    "text": "right so they can use py toch tensor",
    "start": "1005120",
    "end": "1007360"
  },
  {
    "text": "flow and all other um deep learning",
    "start": "1007360",
    "end": "1009560"
  },
  {
    "text": "framework uh to to write their own",
    "start": "1009560",
    "end": "1012360"
  },
  {
    "text": "training logic right customized training",
    "start": "1012360",
    "end": "1015600"
  },
  {
    "text": "logic and then and next the The Rain",
    "start": "1015600",
    "end": "1018600"
  },
  {
    "text": "train",
    "start": "1018600",
    "end": "1019360"
  },
  {
    "text": "uh where we we distributed the training",
    "start": "1019360",
    "end": "1022560"
  },
  {
    "text": "process uh across multiple nodes without",
    "start": "1022560",
    "end": "1025280"
  },
  {
    "text": "requiring the users to manually handle",
    "start": "1025280",
    "end": "1027360"
  },
  {
    "text": "the details of like resource allocation",
    "start": "1027360",
    "end": "1029600"
  },
  {
    "text": "for tolerance or parallel",
    "start": "1029600",
    "end": "1033438"
  },
  {
    "text": "processing so after after you train the",
    "start": "1036199",
    "end": "1038880"
  },
  {
    "text": "model you want to do the serving so when",
    "start": "1038880",
    "end": "1041798"
  },
  {
    "text": "you want to do the serving you will do",
    "start": "1041799",
    "end": "1043520"
  },
  {
    "text": "rain serve so what is ring serve ring",
    "start": "1043520",
    "end": "1046079"
  },
  {
    "text": "serve is a serving Library so um",
    "start": "1046079",
    "end": "1049679"
  },
  {
    "text": "it can it can uh handle multiple like um",
    "start": "1049679",
    "end": "1053520"
  },
  {
    "text": "it can have uh it can like provide the",
    "start": "1053520",
    "end": "1056160"
  },
  {
    "text": "infrastructure to deploy and manage like",
    "start": "1056160",
    "end": "1059280"
  },
  {
    "text": "ml models in production so most",
    "start": "1059280",
    "end": "1061360"
  },
  {
    "text": "specifically it provide some like uh",
    "start": "1061360",
    "end": "1063600"
  },
  {
    "text": "important features for example it can do",
    "start": "1063600",
    "end": "1065840"
  },
  {
    "text": "multi server model deployment so this is",
    "start": "1065840",
    "end": "1068200"
  },
  {
    "text": "very important for large models right",
    "start": "1068200",
    "end": "1069919"
  },
  {
    "text": "like if the model is like hundreds of",
    "start": "1069919",
    "end": "1072320"
  },
  {
    "text": "billions parameter sometimes it will",
    "start": "1072320",
    "end": "1074799"
  },
  {
    "text": "scale multiple nodes to serve a single",
    "start": "1074799",
    "end": "1076799"
  },
  {
    "text": "model so you want to do multi server",
    "start": "1076799",
    "end": "1079159"
  },
  {
    "text": "model deployment uh and also if the",
    "start": "1079159",
    "end": "1081640"
  },
  {
    "text": "request rate is very high you want to",
    "start": "1081640",
    "end": "1083640"
  },
  {
    "text": "replicate the nodes so uh in that case",
    "start": "1083640",
    "end": "1086880"
  },
  {
    "text": "you might also want to have multiple uh",
    "start": "1086880",
    "end": "1088799"
  },
  {
    "text": "multiple servers to serve the same model",
    "start": "1088799",
    "end": "1091960"
  },
  {
    "text": "and second it can do auto scaling so",
    "start": "1091960",
    "end": "1093919"
  },
  {
    "text": "race serve automatically adjust a number",
    "start": "1093919",
    "end": "1095760"
  },
  {
    "text": "of instance based on the traffic it will",
    "start": "1095760",
    "end": "1098240"
  },
  {
    "text": "scaling up during the high demand and",
    "start": "1098240",
    "end": "1100159"
  },
  {
    "text": "scaling down during the low",
    "start": "1100159",
    "end": "1102039"
  },
  {
    "text": "demand it also do load balancing right",
    "start": "1102039",
    "end": "1105159"
  },
  {
    "text": "so it will evenly distribute the request",
    "start": "1105159",
    "end": "1107960"
  },
  {
    "text": "across available servers maximizing",
    "start": "1107960",
    "end": "1110200"
  },
  {
    "text": "performance and lastly it can do like",
    "start": "1110200",
    "end": "1112880"
  },
  {
    "text": "failure recovery uh it itcl some build",
    "start": "1112880",
    "end": "1115799"
  },
  {
    "text": "in Failure recovery mechanism to to",
    "start": "1115799",
    "end": "1118400"
  },
  {
    "text": "ensure the model availability and",
    "start": "1118400",
    "end": "1122080"
  },
  {
    "text": "resilience and below the below the race",
    "start": "1122080",
    "end": "1125400"
  },
  {
    "text": "is a vrm so race only handle like uh",
    "start": "1125400",
    "end": "1128880"
  },
  {
    "text": "multi server development deployment Auto",
    "start": "1128880",
    "end": "1131039"
  },
  {
    "text": "scaling load balancing of failure",
    "start": "1131039",
    "end": "1133520"
  },
  {
    "text": "recovery but what uh when you actually",
    "start": "1133520",
    "end": "1136039"
  },
  {
    "text": "run the Transformer models you will use",
    "start": "1136039",
    "end": "1138360"
  },
  {
    "text": "the engine VM which is also developed um",
    "start": "1138360",
    "end": "1142640"
  },
  {
    "text": "uh from UC Berkeley U and I will dive",
    "start": "1142640",
    "end": "1145120"
  },
  {
    "text": "into",
    "start": "1145120",
    "end": "1147280"
  },
  {
    "text": "later okay so let's dive into like U",
    "start": "1147280",
    "end": "1151640"
  },
  {
    "text": "vlm so what is VM uh so VM is a fast and",
    "start": "1151640",
    "end": "1156760"
  },
  {
    "text": "easy to use open source LM inference and",
    "start": "1156760",
    "end": "1159720"
  },
  {
    "text": "serving engine so it is a serving engine",
    "start": "1159720",
    "end": "1162159"
  },
  {
    "text": "used by uh rer uh and actually many",
    "start": "1162159",
    "end": "1165240"
  },
  {
    "text": "other",
    "start": "1165240",
    "end": "1167120"
  },
  {
    "text": "companies so we launch the project the",
    "start": "1167120",
    "end": "1169840"
  },
  {
    "text": "VM project in June 2023 where there were",
    "start": "1169840",
    "end": "1173640"
  },
  {
    "text": "only few open source projects for LM",
    "start": "1173640",
    "end": "1176880"
  },
  {
    "text": "serving unfortunately VM got a lot of",
    "start": "1176880",
    "end": "1180200"
  },
  {
    "text": "Attraction from the beginning and has",
    "start": "1180200",
    "end": "1182320"
  },
  {
    "text": "become one of the most popular projects",
    "start": "1182320",
    "end": "1184520"
  },
  {
    "text": "in this field we also get uh many users",
    "start": "1184520",
    "end": "1187440"
  },
  {
    "text": "across industry including uh major Cloud",
    "start": "1187440",
    "end": "1190679"
  },
  {
    "text": "providers like AWS um and Google",
    "start": "1190679",
    "end": "1195519"
  },
  {
    "text": "Cloud so so why is U vrm fast so today I",
    "start": "1196280",
    "end": "1201480"
  },
  {
    "text": "will only point out two features but",
    "start": "1201480",
    "end": "1204240"
  },
  {
    "text": "there are actually a list of features",
    "start": "1204240",
    "end": "1206280"
  },
  {
    "text": "that can enable the fast um serving of",
    "start": "1206280",
    "end": "1209280"
  },
  {
    "text": "VM so the one of the most important",
    "start": "1209280",
    "end": "1212159"
  },
  {
    "text": "feature of VM is continuous batching so",
    "start": "1212159",
    "end": "1215520"
  },
  {
    "text": "what is continuous batching so when you",
    "start": "1215520",
    "end": "1217840"
  },
  {
    "text": "do when you do um model inference when",
    "start": "1217840",
    "end": "1220799"
  },
  {
    "text": "you do like LM text generation uh it's",
    "start": "1220799",
    "end": "1224039"
  },
  {
    "text": "actually very hard to fully utilize a",
    "start": "1224039",
    "end": "1226840"
  },
  {
    "text": "GPU why is the case because when you",
    "start": "1226840",
    "end": "1229440"
  },
  {
    "text": "generate the answer for a given prompt",
    "start": "1229440",
    "end": "1231520"
  },
  {
    "text": "you don't know the output lens right",
    "start": "1231520",
    "end": "1233400"
  },
  {
    "text": "like for example given the prompt like",
    "start": "1233400",
    "end": "1235039"
  },
  {
    "text": "what is your name the answer can be my",
    "start": "1235039",
    "end": "1237000"
  },
  {
    "text": "name is Lily or my name is a very long",
    "start": "1237000",
    "end": "1239440"
  },
  {
    "text": "word or or the the model might have some",
    "start": "1239440",
    "end": "1241880"
  },
  {
    "text": "chat oh thanks for asking the question",
    "start": "1241880",
    "end": "1243600"
  },
  {
    "text": "my name is blah blah blah blah so you",
    "start": "1243600",
    "end": "1245320"
  },
  {
    "text": "don't know the model output lens be in",
    "start": "1245320",
    "end": "1247640"
  },
  {
    "text": "advance so in that case how do you do",
    "start": "1247640",
    "end": "1250039"
  },
  {
    "text": "the uh man memory",
    "start": "1250039",
    "end": "1252559"
  },
  {
    "text": "management so so especially when you do",
    "start": "1252559",
    "end": "1255760"
  },
  {
    "text": "the batching when you batch different",
    "start": "1255760",
    "end": "1257400"
  },
  {
    "text": "prompts in the same batch and all of",
    "start": "1257400",
    "end": "1259480"
  },
  {
    "text": "them might have different generation",
    "start": "1259480",
    "end": "1260960"
  },
  {
    "text": "lens how to batch them together so the",
    "start": "1260960",
    "end": "1264120"
  },
  {
    "text": "left picture is a example of static",
    "start": "1264120",
    "end": "1266720"
  },
  {
    "text": "batch so basically you will assume that",
    "start": "1266720",
    "end": "1269159"
  },
  {
    "text": "each request have a have a fixed",
    "start": "1269159",
    "end": "1271360"
  },
  {
    "text": "generation lens they will all generate",
    "start": "1271360",
    "end": "1273240"
  },
  {
    "text": "200 tokens and you preallocate memory",
    "start": "1273240",
    "end": "1275400"
  },
  {
    "text": "for those 200 tokens but the problem is",
    "start": "1275400",
    "end": "1278200"
  },
  {
    "text": "that in that case you are wasting a lot",
    "start": "1278200",
    "end": "1279679"
  },
  {
    "text": "of memory you don't put enough request",
    "start": "1279679",
    "end": "1281760"
  },
  {
    "text": "in the same batch and the GPU is",
    "start": "1281760",
    "end": "1284559"
  },
  {
    "text": "underutilized and what is continuous",
    "start": "1284559",
    "end": "1286520"
  },
  {
    "text": "batching so continuous batching is",
    "start": "1286520",
    "end": "1288159"
  },
  {
    "text": "saying that uh I know that we uh I",
    "start": "1288159",
    "end": "1291640"
  },
  {
    "text": "understand that we don't know the output",
    "start": "1291640",
    "end": "1293039"
  },
  {
    "text": "lens of the of the request but let's",
    "start": "1293039",
    "end": "1295240"
  },
  {
    "text": "still put them in the same batch and we",
    "start": "1295240",
    "end": "1297440"
  },
  {
    "text": "have some mechanism to separate them out",
    "start": "1297440",
    "end": "1299760"
  },
  {
    "text": "when we finish the generation so in that",
    "start": "1299760",
    "end": "1302200"
  },
  {
    "text": "case you can squeeze a different request",
    "start": "1302200",
    "end": "1304440"
  },
  {
    "text": "in the same batch and make sure that um",
    "start": "1304440",
    "end": "1307640"
  },
  {
    "text": "all requests U Can can be run in a",
    "start": "1307640",
    "end": "1309799"
  },
  {
    "text": "single forward PA so that you can",
    "start": "1309799",
    "end": "1311799"
  },
  {
    "text": "increase uh the GPU",
    "start": "1311799",
    "end": "1314000"
  },
  {
    "text": "utilization so that is continuous",
    "start": "1314000",
    "end": "1316000"
  },
  {
    "text": "batching so basically batch requir of",
    "start": "1316000",
    "end": "1318200"
  },
  {
    "text": "different lens into the same batch to",
    "start": "1318200",
    "end": "1320279"
  },
  {
    "text": "increase your batch size to reduce the",
    "start": "1320279",
    "end": "1322440"
  },
  {
    "text": "waste of the um the",
    "start": "1322440",
    "end": "1325759"
  },
  {
    "text": "memory and the second reason is uh vrm",
    "start": "1325799",
    "end": "1329600"
  },
  {
    "text": "utilize efficient kernel and memory",
    "start": "1329600",
    "end": "1331919"
  },
  {
    "text": "management for attention operators so we",
    "start": "1331919",
    "end": "1334559"
  },
  {
    "text": "call it P attention um and and and it",
    "start": "1334559",
    "end": "1338240"
  },
  {
    "text": "can manage the KV cach like operating",
    "start": "1338240",
    "end": "1340120"
  },
  {
    "text": "system virtual memory um so instead of",
    "start": "1340120",
    "end": "1343200"
  },
  {
    "text": "allocating the the the memory for the",
    "start": "1343200",
    "end": "1345679"
  },
  {
    "text": "whole request it manage the memory like",
    "start": "1345679",
    "end": "1348760"
  },
  {
    "text": "block by block so we have an idea of",
    "start": "1348760",
    "end": "1350720"
  },
  {
    "text": "block so basically the memory for each",
    "start": "1350720",
    "end": "1352559"
  },
  {
    "text": "token so it's a little bit complicated",
    "start": "1352559",
    "end": "1355240"
  },
  {
    "text": "so I I I encourage to check out the page",
    "start": "1355240",
    "end": "1357559"
  },
  {
    "text": "attention paper for",
    "start": "1357559",
    "end": "1360919"
  },
  {
    "text": "details and and also the same questions",
    "start": "1362360",
    "end": "1365520"
  },
  {
    "text": "uh why do people choose VM the reason is",
    "start": "1365520",
    "end": "1368400"
  },
  {
    "text": "that the API is pretty easy to use so if",
    "start": "1368400",
    "end": "1371760"
  },
  {
    "text": "you have a um a GPU uh like you can you",
    "start": "1371760",
    "end": "1376120"
  },
  {
    "text": "can just try this like pip pepall V ping",
    "start": "1376120",
    "end": "1379279"
  },
  {
    "text": "store vrm and and use the four lines",
    "start": "1379279",
    "end": "1382440"
  },
  {
    "text": "here to try to run the like large large",
    "start": "1382440",
    "end": "1385080"
  },
  {
    "text": "language model don't use like Lama 38b",
    "start": "1385080",
    "end": "1387320"
  },
  {
    "text": "is too big try a small model and maybe",
    "start": "1387320",
    "end": "1389600"
  },
  {
    "text": "you can uh you can run",
    "start": "1389600",
    "end": "1392720"
  },
  {
    "text": "it and also VM have some um other",
    "start": "1394200",
    "end": "1398240"
  },
  {
    "text": "important features to make it widely um",
    "start": "1398240",
    "end": "1401799"
  },
  {
    "text": "adopted so the first one uh is has broad",
    "start": "1401799",
    "end": "1405080"
  },
  {
    "text": "model support so like it's a official",
    "start": "1405080",
    "end": "1407880"
  },
  {
    "text": "launch p of llama right like well",
    "start": "1407880",
    "end": "1410960"
  },
  {
    "text": "support like llama 400b 70b 8B and it",
    "start": "1410960",
    "end": "1415200"
  },
  {
    "text": "also um we also accept contribution from",
    "start": "1415200",
    "end": "1418440"
  },
  {
    "text": "those um state of art U Vision language",
    "start": "1418440",
    "end": "1421720"
  },
  {
    "text": "models uh training companies like",
    "start": "1421720",
    "end": "1423799"
  },
  {
    "text": "picture or q v uh",
    "start": "1423799",
    "end": "1427799"
  },
  {
    "text": "VL and the last features uh I I just",
    "start": "1428440",
    "end": "1431760"
  },
  {
    "text": "mentioned today is uh for VM it does",
    "start": "1431760",
    "end": "1434400"
  },
  {
    "text": "multi Hardware support so uh because VM",
    "start": "1434400",
    "end": "1438640"
  },
  {
    "text": "uh it depends on py Torch um and instead",
    "start": "1438640",
    "end": "1441799"
  },
  {
    "text": "of relying on some external comp",
    "start": "1441799",
    "end": "1443960"
  },
  {
    "text": "compilers or forax like onx VM directly",
    "start": "1443960",
    "end": "1447919"
  },
  {
    "text": "use py torch as a narrow waist a middle",
    "start": "1447919",
    "end": "1450840"
  },
  {
    "text": "ground to connect different back ends",
    "start": "1450840",
    "end": "1453080"
  },
  {
    "text": "with Hardware agnostic models and",
    "start": "1453080",
    "end": "1456960"
  },
  {
    "text": "utilities okay so with VM race of kuber",
    "start": "1460039",
    "end": "1464559"
  },
  {
    "text": "Ray kubernetes how will the end to LM",
    "start": "1464559",
    "end": "1467720"
  },
  {
    "text": "serving ACC ecture look like so first",
    "start": "1467720",
    "end": "1471679"
  },
  {
    "text": "you will use vom as a inference engine",
    "start": "1471679",
    "end": "1474840"
  },
  {
    "text": "and second you will use reserve and",
    "start": "1474840",
    "end": "1476880"
  },
  {
    "text": "Reser will call the VM right like it",
    "start": "1476880",
    "end": "1479080"
  },
  {
    "text": "will uh but Reser will handle the model",
    "start": "1479080",
    "end": "1481440"
  },
  {
    "text": "de deployment scaling request routing",
    "start": "1481440",
    "end": "1484200"
  },
  {
    "text": "load balancing and for",
    "start": "1484200",
    "end": "1485880"
  },
  {
    "text": "tolerance and on top of that you will",
    "start": "1485880",
    "end": "1488480"
  },
  {
    "text": "use KU Ray to bridge the gap between the",
    "start": "1488480",
    "end": "1490960"
  },
  {
    "text": "Rive and kubernetes and lastly you would",
    "start": "1490960",
    "end": "1494399"
  },
  {
    "text": "just use kues to do the container",
    "start": "1494399",
    "end": "1496440"
  },
  {
    "text": "orchestration so those four layers",
    "start": "1496440",
    "end": "1498840"
  },
  {
    "text": "together uh form a a pipeline end to end",
    "start": "1498840",
    "end": "1502399"
  },
  {
    "text": "architecture for the LM",
    "start": "1502399",
    "end": "1505880"
  },
  {
    "text": "serving okay um so in conclusion so",
    "start": "1507720",
    "end": "1511159"
  },
  {
    "text": "today I talk about U the like different",
    "start": "1511159",
    "end": "1514360"
  },
  {
    "text": "challenges we have in generative AI",
    "start": "1514360",
    "end": "1516520"
  },
  {
    "text": "workflows uh and I mentioned like the",
    "start": "1516520",
    "end": "1519000"
  },
  {
    "text": "scalable infrastructure with kubernetes",
    "start": "1519000",
    "end": "1520960"
  },
  {
    "text": "and kuber Ray um and also I introduced",
    "start": "1520960",
    "end": "1524120"
  },
  {
    "text": "different R like AI libraries um to",
    "start": "1524120",
    "end": "1527720"
  },
  {
    "text": "improve the development efficiency of uh",
    "start": "1527720",
    "end": "1530000"
  },
  {
    "text": "end users and lastly I introduce VM um",
    "start": "1530000",
    "end": "1533880"
  },
  {
    "text": "that's the end of this talk and I'm",
    "start": "1533880",
    "end": "1536120"
  },
  {
    "text": "happy to take questions yeah thank",
    "start": "1536120",
    "end": "1539940"
  },
  {
    "text": "[Applause]",
    "start": "1539940",
    "end": "1548209"
  },
  {
    "text": "you with VMS um when you're batching I'm",
    "start": "1551440",
    "end": "1555919"
  },
  {
    "text": "pretty old but do the new A1 100s have",
    "start": "1555919",
    "end": "1558000"
  },
  {
    "text": "some form of memory protection because",
    "start": "1558000",
    "end": "1560159"
  },
  {
    "text": "when you're doing multi- tendency on GPU",
    "start": "1560159",
    "end": "1562960"
  },
  {
    "text": "memory there's no memory protection",
    "start": "1562960",
    "end": "1564799"
  },
  {
    "text": "inside there's no tlb there's no nothing",
    "start": "1564799",
    "end": "1566600"
  },
  {
    "text": "to protect memory mm okay thanks for the",
    "start": "1566600",
    "end": "1569360"
  },
  {
    "text": "question so no the answer the short",
    "start": "1569360",
    "end": "1571159"
  },
  {
    "text": "answer is no um So currently U for vrm",
    "start": "1571159",
    "end": "1574640"
  },
  {
    "text": "is um we don't do multi- tendency so",
    "start": "1574640",
    "end": "1577360"
  },
  {
    "text": "basically a single GPU only host a",
    "start": "1577360",
    "end": "1579559"
  },
  {
    "text": "single a single",
    "start": "1579559",
    "end": "1581600"
  },
  {
    "text": "model well if you have multiple prompts",
    "start": "1581600",
    "end": "1584000"
  },
  {
    "text": "coming in from different locations even",
    "start": "1584000",
    "end": "1585320"
  },
  {
    "text": "though it's a single model that's the",
    "start": "1585320",
    "end": "1586440"
  },
  {
    "text": "question I was having so you're serving",
    "start": "1586440",
    "end": "1588360"
  },
  {
    "text": "multiple users coming in with different",
    "start": "1588360",
    "end": "1590840"
  },
  {
    "text": "prompts coming in and you're batching",
    "start": "1590840",
    "end": "1592640"
  },
  {
    "text": "across those different prompts then they",
    "start": "1592640",
    "end": "1594559"
  },
  {
    "text": "will be put in the same badge but yeah",
    "start": "1594559",
    "end": "1596679"
  },
  {
    "text": "is that the reason why sometimes you can",
    "start": "1596679",
    "end": "1598399"
  },
  {
    "text": "get random responses from other",
    "start": "1598399",
    "end": "1601399"
  },
  {
    "text": "people uh not no no um not not really",
    "start": "1601399",
    "end": "1606880"
  },
  {
    "text": "like if the the the the inference engine",
    "start": "1606880",
    "end": "1608760"
  },
  {
    "text": "is implemented correctly it should not",
    "start": "1608760",
    "end": "1610640"
  },
  {
    "text": "get the answer from the from other from",
    "start": "1610640",
    "end": "1613360"
  },
  {
    "text": "other user but uh uh but the output from",
    "start": "1613360",
    "end": "1616440"
  },
  {
    "text": "the other user might affect the um the",
    "start": "1616440",
    "end": "1619200"
  },
  {
    "text": "numerical stability of the current user",
    "start": "1619200",
    "end": "1621320"
  },
  {
    "text": "they will affect each other a little bit",
    "start": "1621320",
    "end": "1623200"
  },
  {
    "text": "but you should not get the answer from",
    "start": "1623200",
    "end": "1624960"
  },
  {
    "text": "from the other user yeah",
    "start": "1624960",
    "end": "1629278"
  }
]