[
  {
    "text": "thank you and hi everyone and today I'm",
    "start": "60",
    "end": "3000"
  },
  {
    "text": "gonna use English to present my slides",
    "start": "3000",
    "end": "6000"
  },
  {
    "text": "hi everyone I'm fog Dom from Ben 10 Mao",
    "start": "6000",
    "end": "9059"
  },
  {
    "text": "and today I want to talk about from",
    "start": "9059",
    "end": "11519"
  },
  {
    "text": "model to Market what's the missing link",
    "start": "11519",
    "end": "13860"
  },
  {
    "text": "in scaling open source models on cloud",
    "start": "13860",
    "end": "17520"
  },
  {
    "text": "so there's no doubt that AI is once",
    "start": "17520",
    "end": "20220"
  },
  {
    "text": "again catching everyone's eyes this",
    "start": "20220",
    "end": "22439"
  },
  {
    "text": "picture illustrates the newly founded AI",
    "start": "22439",
    "end": "25080"
  },
  {
    "text": "companies but even without this picture",
    "start": "25080",
    "end": "27480"
  },
  {
    "text": "I believe we all acknowledge that the",
    "start": "27480",
    "end": "30240"
  },
  {
    "text": "rights of the AI and also the ensuing",
    "start": "30240",
    "end": "32880"
  },
  {
    "text": "opportunities in fact the AI related",
    "start": "32880",
    "end": "36120"
  },
  {
    "text": "talk at this year's kubecon is more than",
    "start": "36120",
    "end": "38340"
  },
  {
    "text": "passed",
    "start": "38340",
    "end": "40820"
  },
  {
    "text": "however with the emergency",
    "start": "41120",
    "end": "43820"
  },
  {
    "text": "advancement of AI and also the emergence",
    "start": "43820",
    "end": "46680"
  },
  {
    "text": "of New Opportunities not every company",
    "start": "46680",
    "end": "49320"
  },
  {
    "text": "can afford or need to build their own",
    "start": "49320",
    "end": "51780"
  },
  {
    "text": "models",
    "start": "51780",
    "end": "52860"
  },
  {
    "text": "thankfully the community and the open",
    "start": "52860",
    "end": "55320"
  },
  {
    "text": "source had already provided numerous",
    "start": "55320",
    "end": "58260"
  },
  {
    "text": "three new models for us to utilize",
    "start": "58260",
    "end": "61379"
  },
  {
    "text": "compared to close models open source",
    "start": "61379",
    "end": "65338"
  },
  {
    "text": "models is winner in customizability data",
    "start": "65339",
    "end": "68580"
  },
  {
    "text": "privacy and also cost efficiency",
    "start": "68580",
    "end": "71280"
  },
  {
    "text": "if I want to build an AI product I can",
    "start": "71280",
    "end": "74580"
  },
  {
    "text": "Leverage The Power of Open Source model",
    "start": "74580",
    "end": "76939"
  },
  {
    "text": "fine-tuning them with my own data set",
    "start": "76939",
    "end": "79380"
  },
  {
    "text": "and to deploy it in my own environment",
    "start": "79380",
    "end": "82020"
  },
  {
    "text": "to make sure the data privacy and also",
    "start": "82020",
    "end": "84659"
  },
  {
    "text": "only pay for the cost that the model",
    "start": "84659",
    "end": "86520"
  },
  {
    "text": "need",
    "start": "86520",
    "end": "88259"
  },
  {
    "text": "however from model to application the",
    "start": "88259",
    "end": "91979"
  },
  {
    "text": "way is not always straightforward as a",
    "start": "91979",
    "end": "95280"
  },
  {
    "text": "developer if I want to Leverage The",
    "start": "95280",
    "end": "97680"
  },
  {
    "text": "Power of Open Source model for example",
    "start": "97680",
    "end": "99840"
  },
  {
    "text": "llama to build an application that can",
    "start": "99840",
    "end": "103320"
  },
  {
    "text": "generate advertising proposals for my",
    "start": "103320",
    "end": "106619"
  },
  {
    "text": "customers how long is the road from my",
    "start": "106619",
    "end": "110220"
  },
  {
    "text": "open source model that I just pulled to",
    "start": "110220",
    "end": "112799"
  },
  {
    "text": "an application that ready for use on",
    "start": "112799",
    "end": "115140"
  },
  {
    "text": "cloud",
    "start": "115140",
    "end": "116280"
  },
  {
    "text": "so if we delve deeper into this we can",
    "start": "116280",
    "end": "118860"
  },
  {
    "text": "consider model as ml codes but for an",
    "start": "118860",
    "end": "122280"
  },
  {
    "text": "application an application is just way",
    "start": "122280",
    "end": "125219"
  },
  {
    "text": "more complex than just ml codes it also",
    "start": "125219",
    "end": "128399"
  },
  {
    "text": "needs configuration data collection",
    "start": "128399",
    "end": "131039"
  },
  {
    "text": "serving infrastructure and more and ml",
    "start": "131039",
    "end": "134459"
  },
  {
    "text": "code is just a part of it",
    "start": "134459",
    "end": "136620"
  },
  {
    "text": "so in order to bridge the gap between",
    "start": "136620",
    "end": "139500"
  },
  {
    "text": "the model and the application we need",
    "start": "139500",
    "end": "142319"
  },
  {
    "text": "first find the intermediate station and",
    "start": "142319",
    "end": "145140"
  },
  {
    "text": "artifact",
    "start": "145140",
    "end": "146640"
  },
  {
    "text": "so that we can divide the whole process",
    "start": "146640",
    "end": "150060"
  },
  {
    "text": "into two parts the first one is build",
    "start": "150060",
    "end": "152700"
  },
  {
    "text": "and the second one is deploy",
    "start": "152700",
    "end": "155340"
  },
  {
    "text": "so for the build challenges we need to",
    "start": "155340",
    "end": "158040"
  },
  {
    "text": "handle the challenges like model",
    "start": "158040",
    "end": "159840"
  },
  {
    "text": "packaging environment management model",
    "start": "159840",
    "end": "162540"
  },
  {
    "text": "versioning and more and for the",
    "start": "162540",
    "end": "165120"
  },
  {
    "text": "deployment we need to handle the",
    "start": "165120",
    "end": "167160"
  },
  {
    "text": "challenges like environment consistency",
    "start": "167160",
    "end": "169580"
  },
  {
    "text": "scalability observability and and more",
    "start": "169580",
    "end": "172860"
  },
  {
    "text": "and the deployed changes might look",
    "start": "172860",
    "end": "175920"
  },
  {
    "text": "familiar to us so since those are the",
    "start": "175920",
    "end": "177959"
  },
  {
    "text": "challenges that the kubernetes and Cloud",
    "start": "177959",
    "end": "180000"
  },
  {
    "text": "native ecosystems try to resolve",
    "start": "180000",
    "end": "183300"
  },
  {
    "text": "right so now that we know the challenges",
    "start": "183300",
    "end": "185580"
  },
  {
    "text": "let's try to resolve them so when you",
    "start": "185580",
    "end": "188280"
  },
  {
    "text": "first handle the building problem we",
    "start": "188280",
    "end": "191519"
  },
  {
    "text": "need to somehow pack our model our",
    "start": "191519",
    "end": "193980"
  },
  {
    "text": "dependencies everything we need into",
    "start": "193980",
    "end": "196080"
  },
  {
    "text": "something that can be actually deployed",
    "start": "196080",
    "end": "198480"
  },
  {
    "text": "so this will eventually now comes to the",
    "start": "198480",
    "end": "201360"
  },
  {
    "text": "play Benton amount is an open source",
    "start": "201360",
    "end": "204120"
  },
  {
    "text": "python framework that can help you to",
    "start": "204120",
    "end": "206700"
  },
  {
    "text": "build your application",
    "start": "206700",
    "end": "209340"
  },
  {
    "text": "so what mental means is like uh it's a",
    "start": "209340",
    "end": "213239"
  },
  {
    "text": "typical Bento is a traditional Asian",
    "start": "213239",
    "end": "215640"
  },
  {
    "text": "food that contains rice vegetable meat",
    "start": "215640",
    "end": "219300"
  },
  {
    "text": "everything you need for your for your",
    "start": "219300",
    "end": "222120"
  },
  {
    "text": "meal and that's also bento's position",
    "start": "222120",
    "end": "224340"
  },
  {
    "text": "for your AI application will help you to",
    "start": "224340",
    "end": "227879"
  },
  {
    "text": "bet to bundle your API your dependency",
    "start": "227879",
    "end": "230700"
  },
  {
    "text": "your model and every other file you need",
    "start": "230700",
    "end": "233640"
  },
  {
    "text": "for your AI application into this one",
    "start": "233640",
    "end": "236459"
  },
  {
    "text": "Deployable unit a Bento",
    "start": "236459",
    "end": "239760"
  },
  {
    "text": "so uh we all know that basically all the",
    "start": "239760",
    "end": "243120"
  },
  {
    "text": "models are more like a compute intensive",
    "start": "243120",
    "end": "245700"
  },
  {
    "text": "workflow a workload but when a model",
    "start": "245700",
    "end": "248760"
  },
  {
    "text": "needs to be transformed into an",
    "start": "248760",
    "end": "250860"
  },
  {
    "text": "application we might need to handle like",
    "start": "250860",
    "end": "253260"
  },
  {
    "text": "a concurrent request scenario so that's",
    "start": "253260",
    "end": "256079"
  },
  {
    "text": "when it converts into an i o intensive",
    "start": "256079",
    "end": "259260"
  },
  {
    "text": "and that's also the reason why we",
    "start": "259260",
    "end": "261840"
  },
  {
    "text": "separate the API server and the runner",
    "start": "261840",
    "end": "264600"
  },
  {
    "text": "in your Bento so a typical Bento is",
    "start": "264600",
    "end": "267660"
  },
  {
    "text": "basically contains three parts API",
    "start": "267660",
    "end": "270360"
  },
  {
    "text": "server model Runner and also the",
    "start": "270360",
    "end": "272639"
  },
  {
    "text": "environment and in the API server we can",
    "start": "272639",
    "end": "275880"
  },
  {
    "text": "do the work like",
    "start": "275880",
    "end": "277340"
  },
  {
    "text": "processing as your business code expose",
    "start": "277340",
    "end": "280740"
  },
  {
    "text": "your Matrix Define your API basically",
    "start": "280740",
    "end": "283620"
  },
  {
    "text": "all the i o intensive work here",
    "start": "283620",
    "end": "286860"
  },
  {
    "text": "and for the model Runner that's where",
    "start": "286860",
    "end": "289560"
  },
  {
    "text": "you can load your model that built with",
    "start": "289560",
    "end": "291900"
  },
  {
    "text": "a different framework like pie torch or",
    "start": "291900",
    "end": "294540"
  },
  {
    "text": "tensorflow and make sure they're using",
    "start": "294540",
    "end": "297240"
  },
  {
    "text": "the right resources",
    "start": "297240",
    "end": "298919"
  },
  {
    "text": "another thing is important in this",
    "start": "298919",
    "end": "301080"
  },
  {
    "text": "architecture is how we're gonna organize",
    "start": "301080",
    "end": "303600"
  },
  {
    "text": "your Bento so if you want to build your",
    "start": "303600",
    "end": "306300"
  },
  {
    "text": "Bento the first thing you need to do is",
    "start": "306300",
    "end": "308759"
  },
  {
    "text": "to write a Bento file and a Bento file",
    "start": "308759",
    "end": "311639"
  },
  {
    "text": "is like a Docker file but slightly",
    "start": "311639",
    "end": "313919"
  },
  {
    "text": "different in your Bento file you can",
    "start": "313919",
    "end": "317100"
  },
  {
    "text": "specify the version of the dependencies",
    "start": "317100",
    "end": "319440"
  },
  {
    "text": "the entry point of your API server and",
    "start": "319440",
    "end": "322199"
  },
  {
    "text": "Runner and basically all the",
    "start": "322199",
    "end": "324120"
  },
  {
    "text": "configurations you cares about",
    "start": "324120",
    "end": "326639"
  },
  {
    "text": "so once a Bento is built we will by",
    "start": "326639",
    "end": "329220"
  },
  {
    "text": "default started in your local registry",
    "start": "329220",
    "end": "332280"
  },
  {
    "text": "but you can also use command like",
    "start": "332280",
    "end": "334800"
  },
  {
    "text": "Benzema push to push push your Bento to",
    "start": "334800",
    "end": "337800"
  },
  {
    "text": "S3 or other central registry",
    "start": "337800",
    "end": "341580"
  },
  {
    "text": "so these are the solution that will",
    "start": "341580",
    "end": "344160"
  },
  {
    "text": "provide in the building process we",
    "start": "344160",
    "end": "346860"
  },
  {
    "text": "divide everything you need for your AI",
    "start": "346860",
    "end": "349020"
  },
  {
    "text": "and application into API server and",
    "start": "349020",
    "end": "351600"
  },
  {
    "text": "model runner in the same time your",
    "start": "351600",
    "end": "354300"
  },
  {
    "text": "environment and configurations is",
    "start": "354300",
    "end": "356580"
  },
  {
    "text": "managed by the Bento file and we also",
    "start": "356580",
    "end": "359520"
  },
  {
    "text": "provide ecological command like bentama",
    "start": "359520",
    "end": "362160"
  },
  {
    "text": "build or push to help you easily build",
    "start": "362160",
    "end": "365100"
  },
  {
    "text": "your Bento and manage the version of the",
    "start": "365100",
    "end": "367800"
  },
  {
    "text": "Bento as well as the models",
    "start": "367800",
    "end": "370500"
  },
  {
    "text": "right so after the Bento is built we now",
    "start": "370500",
    "end": "373259"
  },
  {
    "text": "need to deploy it in your production and",
    "start": "373259",
    "end": "376620"
  },
  {
    "text": "we do provide the command like pentoma",
    "start": "376620",
    "end": "379199"
  },
  {
    "text": "observe to serve the bentle locally for",
    "start": "379199",
    "end": "381900"
  },
  {
    "text": "tests but when it comes to the",
    "start": "381900",
    "end": "383940"
  },
  {
    "text": "production we need to Leverage The Power",
    "start": "383940",
    "end": "386639"
  },
  {
    "text": "of cloud native and kubernetes for more",
    "start": "386639",
    "end": "389220"
  },
  {
    "text": "stability deploy so we can now deploy",
    "start": "389220",
    "end": "392520"
  },
  {
    "text": "the Bento as a microservice",
    "start": "392520",
    "end": "395400"
  },
  {
    "text": "so in this picture the left side of the",
    "start": "395400",
    "end": "398580"
  },
  {
    "text": "picture depicts how a developer build a",
    "start": "398580",
    "end": "401940"
  },
  {
    "text": "Bento so the simplest bentle just",
    "start": "401940",
    "end": "404400"
  },
  {
    "text": "consists bentofile.aml service.py which",
    "start": "404400",
    "end": "408120"
  },
  {
    "text": "contains your API server and Runner and",
    "start": "408120",
    "end": "410580"
  },
  {
    "text": "also the models you'll need and we can",
    "start": "410580",
    "end": "412860"
  },
  {
    "text": "now build it into a Bento and push it to",
    "start": "412860",
    "end": "415319"
  },
  {
    "text": "the registry and to want to deploy it in",
    "start": "415319",
    "end": "418680"
  },
  {
    "text": "the kubernetes we need two more",
    "start": "418680",
    "end": "421199"
  },
  {
    "text": "controllers work within the cluster the",
    "start": "421199",
    "end": "424080"
  },
  {
    "text": "first one is a image Builder controller",
    "start": "424080",
    "end": "426180"
  },
  {
    "text": "it will watch a customer resource called",
    "start": "426180",
    "end": "429000"
  },
  {
    "text": "Dental request and this controller will",
    "start": "429000",
    "end": "431940"
  },
  {
    "text": "help you automatically to build your",
    "start": "431940",
    "end": "434100"
  },
  {
    "text": "Bento into the image by default we will",
    "start": "434100",
    "end": "437220"
  },
  {
    "text": "generate a Docker file for you which",
    "start": "437220",
    "end": "439080"
  },
  {
    "text": "contains all the dependencies you need",
    "start": "439080",
    "end": "441060"
  },
  {
    "text": "in your in your Bento but you can also",
    "start": "441060",
    "end": "443880"
  },
  {
    "text": "specify and customize it in your pencil",
    "start": "443880",
    "end": "446759"
  },
  {
    "text": "file",
    "start": "446759",
    "end": "447780"
  },
  {
    "text": "so after Bento is built we can now",
    "start": "447780",
    "end": "450300"
  },
  {
    "text": "deploy it so another controller here the",
    "start": "450300",
    "end": "454199"
  },
  {
    "text": "the Bento deployment controller will use",
    "start": "454199",
    "end": "456479"
  },
  {
    "text": "the image that just built and it will",
    "start": "456479",
    "end": "459780"
  },
  {
    "text": "reconcile the resource called rental",
    "start": "459780",
    "end": "461880"
  },
  {
    "text": "deployment and this controller will",
    "start": "461880",
    "end": "464160"
  },
  {
    "text": "create all the resources for your AI",
    "start": "464160",
    "end": "467039"
  },
  {
    "text": "application for example the service the",
    "start": "467039",
    "end": "470039"
  },
  {
    "text": "HPA and also the deployment of AI API",
    "start": "470039",
    "end": "473520"
  },
  {
    "text": "server and Runner which can be scaled",
    "start": "473520",
    "end": "475919"
  },
  {
    "text": "independently",
    "start": "475919",
    "end": "477300"
  },
  {
    "text": "so all the tags here are open source",
    "start": "477300",
    "end": "479819"
  },
  {
    "text": "analysts here and yatai is the dashboard",
    "start": "479819",
    "end": "482759"
  },
  {
    "text": "that we provided for our users so that",
    "start": "482759",
    "end": "485340"
  },
  {
    "text": "we can easily uh serve the Bento in the",
    "start": "485340",
    "end": "488699"
  },
  {
    "text": "cluster so that by the way the yeah what",
    "start": "488699",
    "end": "491220"
  },
  {
    "text": "yatai means is like it's a place",
    "start": "491220",
    "end": "493560"
  },
  {
    "text": "somewhere you can sell your bento box",
    "start": "493560",
    "end": "497280"
  },
  {
    "text": "right so knowing the challenges and also",
    "start": "497280",
    "end": "500220"
  },
  {
    "text": "the solutions doesn't make this way less",
    "start": "500220",
    "end": "502500"
  },
  {
    "text": "daunting it's easy to get stuck",
    "start": "502500",
    "end": "504660"
  },
  {
    "text": "somewhere along the way so this year we",
    "start": "504660",
    "end": "507660"
  },
  {
    "text": "saw the trends of Open Source LL models",
    "start": "507660",
    "end": "510539"
  },
  {
    "text": "and the question suddenly occurs to us",
    "start": "510539",
    "end": "513560"
  },
  {
    "text": "are all the AI developers can easily",
    "start": "513560",
    "end": "516959"
  },
  {
    "text": "serve the open source language models on",
    "start": "516959",
    "end": "519659"
  },
  {
    "text": "cloud",
    "start": "519659",
    "end": "520620"
  },
  {
    "text": "so that with all the discussion that we",
    "start": "520620",
    "end": "523500"
  },
  {
    "text": "had earlier we not only need the",
    "start": "523500",
    "end": "525779"
  },
  {
    "text": "knowledge of ml but also the text of",
    "start": "525779",
    "end": "528660"
  },
  {
    "text": "cloud native so that's the initial idea",
    "start": "528660",
    "end": "531480"
  },
  {
    "text": "that's why we're gonna open source this",
    "start": "531480",
    "end": "533760"
  },
  {
    "text": "project called open llm in this year the",
    "start": "533760",
    "end": "537420"
  },
  {
    "text": "initial idea of this project is we're",
    "start": "537420",
    "end": "540120"
  },
  {
    "text": "gonna bring all the best practice in the",
    "start": "540120",
    "end": "542640"
  },
  {
    "text": "industry and to help our users to easily",
    "start": "542640",
    "end": "545700"
  },
  {
    "text": "start the popular llms with one single",
    "start": "545700",
    "end": "548880"
  },
  {
    "text": "command just like this",
    "start": "548880",
    "end": "551279"
  },
  {
    "text": "so but today is not like a deep dive",
    "start": "551279",
    "end": "554339"
  },
  {
    "text": "into this project since openload them is",
    "start": "554339",
    "end": "557700"
  },
  {
    "text": "exactly the project that starts with",
    "start": "557700",
    "end": "560279"
  },
  {
    "text": "open source model and can be deployed in",
    "start": "560279",
    "end": "563040"
  },
  {
    "text": "production in the end so today we'll use",
    "start": "563040",
    "end": "565620"
  },
  {
    "text": "it as a case study so that we can learn",
    "start": "565620",
    "end": "568620"
  },
  {
    "text": "from the other challenges with that we",
    "start": "568620",
    "end": "570839"
  },
  {
    "text": "learn from it",
    "start": "570839",
    "end": "572940"
  },
  {
    "text": "right so when it comes to products",
    "start": "572940",
    "end": "576260"
  },
  {
    "text": "productionizing llms little llama has so",
    "start": "576260",
    "end": "579600"
  },
  {
    "text": "many concerns like the scalability the",
    "start": "579600",
    "end": "582959"
  },
  {
    "text": "throughput the the operability the",
    "start": "582959",
    "end": "586680"
  },
  {
    "text": "latency and also the cost those are the",
    "start": "586680",
    "end": "589200"
  },
  {
    "text": "challenges that we must resolve during",
    "start": "589200",
    "end": "591899"
  },
  {
    "text": "the way we produce productionizing llms",
    "start": "591899",
    "end": "597180"
  },
  {
    "text": "so in the development of open llm",
    "start": "597180",
    "end": "600600"
  },
  {
    "text": "project if we want to make this project",
    "start": "600600",
    "end": "603420"
  },
  {
    "text": "as the best practice of language models",
    "start": "603420",
    "end": "606060"
  },
  {
    "text": "we need first prepacked and and also",
    "start": "606060",
    "end": "609240"
  },
  {
    "text": "pre-optimize the llms for our users in",
    "start": "609240",
    "end": "612839"
  },
  {
    "text": "other words we need first overcome some",
    "start": "612839",
    "end": "615240"
  },
  {
    "text": "unique challenges of language models and",
    "start": "615240",
    "end": "618600"
  },
  {
    "text": "pack it as a artifact a Bento so a Bento",
    "start": "618600",
    "end": "623100"
  },
  {
    "text": "for llm is slightly different from a",
    "start": "623100",
    "end": "625680"
  },
  {
    "text": "normal pencil first we incorporate SSE",
    "start": "625680",
    "end": "628980"
  },
  {
    "text": "support in your API server so that we",
    "start": "628980",
    "end": "632040"
  },
  {
    "text": "our language model can respond more",
    "start": "632040",
    "end": "634380"
  },
  {
    "text": "quickly",
    "start": "634380",
    "end": "635279"
  },
  {
    "text": "and also we want to Leverage The Power",
    "start": "635279",
    "end": "637740"
  },
  {
    "text": "of the community and open source so for",
    "start": "637740",
    "end": "641279"
  },
  {
    "text": "example vllm is the project that's doing",
    "start": "641279",
    "end": "643980"
  },
  {
    "text": "amazing work with a page attention and",
    "start": "643980",
    "end": "647399"
  },
  {
    "text": "other optimization to make our language",
    "start": "647399",
    "end": "650399"
  },
  {
    "text": "models improve its throughput and also",
    "start": "650399",
    "end": "653579"
  },
  {
    "text": "latency",
    "start": "653579",
    "end": "654839"
  },
  {
    "text": "uh and in addition like a conversation",
    "start": "654839",
    "end": "657899"
  },
  {
    "text": "text like gpdq also improves the uh also",
    "start": "657899",
    "end": "662640"
  },
  {
    "text": "help our outer lamps to work with less",
    "start": "662640",
    "end": "665880"
  },
  {
    "text": "resources so if you are using open llm",
    "start": "665880",
    "end": "669180"
  },
  {
    "text": "you can just use the command that list",
    "start": "669180",
    "end": "671700"
  },
  {
    "text": "here and those command will actually",
    "start": "671700",
    "end": "674399"
  },
  {
    "text": "apply apply this option optimizations in",
    "start": "674399",
    "end": "678120"
  },
  {
    "text": "your model Runner layer for example we",
    "start": "678120",
    "end": "680880"
  },
  {
    "text": "can just switch the different backend",
    "start": "680880",
    "end": "683100"
  },
  {
    "text": "from vlm to trt for your model Runner",
    "start": "683100",
    "end": "688380"
  },
  {
    "text": "and to improve more we also want to",
    "start": "688380",
    "end": "690660"
  },
  {
    "text": "introduce a continuous batching so for a",
    "start": "690660",
    "end": "693839"
  },
  {
    "text": "typical pencil when multiple requests is",
    "start": "693839",
    "end": "697200"
  },
  {
    "text": "coming through the API server will scale",
    "start": "697200",
    "end": "699720"
  },
  {
    "text": "first and then redirect the request to",
    "start": "699720",
    "end": "702420"
  },
  {
    "text": "our runner and the the runner actually",
    "start": "702420",
    "end": "705540"
  },
  {
    "text": "batches all the requests to model but",
    "start": "705540",
    "end": "708300"
  },
  {
    "text": "that's not sufficient enough for our",
    "start": "708300",
    "end": "710279"
  },
  {
    "text": "language models so that's why we need",
    "start": "710279",
    "end": "713339"
  },
  {
    "text": "continuous batching basically we just",
    "start": "713339",
    "end": "715200"
  },
  {
    "text": "have one giant batch that can Cycles",
    "start": "715200",
    "end": "717779"
  },
  {
    "text": "other requests",
    "start": "717779",
    "end": "719519"
  },
  {
    "text": "so those other challenges were resolved",
    "start": "719519",
    "end": "722940"
  },
  {
    "text": "one way uh build our language models",
    "start": "722940",
    "end": "726000"
  },
  {
    "text": "into a Bento and the optimization is",
    "start": "726000",
    "end": "729660"
  },
  {
    "text": "mostly applied in the runner layer and",
    "start": "729660",
    "end": "732300"
  },
  {
    "text": "this is like the real case when you want",
    "start": "732300",
    "end": "734040"
  },
  {
    "text": "to build your model make it sufficient",
    "start": "734040",
    "end": "737880"
  },
  {
    "text": "for production the work you need to be",
    "start": "737880",
    "end": "739860"
  },
  {
    "text": "done during the building process",
    "start": "739860",
    "end": "743540"
  },
  {
    "text": "and after the Bento is built and we want",
    "start": "743779",
    "end": "747779"
  },
  {
    "text": "to deploy it in the production",
    "start": "747779",
    "end": "750860"
  },
  {
    "text": "it is not worthy that 95 of the model",
    "start": "750860",
    "end": "755279"
  },
  {
    "text": "are idle most of the time yet the",
    "start": "755279",
    "end": "757860"
  },
  {
    "text": "reserved instance allocated for them are",
    "start": "757860",
    "end": "760620"
  },
  {
    "text": "generating costs every moment so that's",
    "start": "760620",
    "end": "763500"
  },
  {
    "text": "why when it comes to the production we",
    "start": "763500",
    "end": "765959"
  },
  {
    "text": "need to Leverage The Power of serverless",
    "start": "765959",
    "end": "769440"
  },
  {
    "text": "so if there's no request we want our",
    "start": "769440",
    "end": "772019"
  },
  {
    "text": "replicas to be zero so that we can",
    "start": "772019",
    "end": "774600"
  },
  {
    "text": "prevent the GPU waste",
    "start": "774600",
    "end": "776880"
  },
  {
    "text": "so in order to do this when you first",
    "start": "776880",
    "end": "779339"
  },
  {
    "text": "introduce Keda into this picture Keda is",
    "start": "779339",
    "end": "782639"
  },
  {
    "text": "the kubernetes event-based auto scaler",
    "start": "782639",
    "end": "785279"
  },
  {
    "text": "with the help of Canada and also HPA we",
    "start": "785279",
    "end": "788940"
  },
  {
    "text": "can now easily scale our replicas from",
    "start": "788940",
    "end": "791700"
  },
  {
    "text": "or down to zero so to make this word we",
    "start": "791700",
    "end": "794880"
  },
  {
    "text": "also need three more new components here",
    "start": "794880",
    "end": "797040"
  },
  {
    "text": "the Interceptor the scalar and also the",
    "start": "797040",
    "end": "800459"
  },
  {
    "text": "proxy container in your API server and",
    "start": "800459",
    "end": "802860"
  },
  {
    "text": "Runner",
    "start": "802860",
    "end": "804180"
  },
  {
    "text": "so if there's no request and all the",
    "start": "804180",
    "end": "806700"
  },
  {
    "text": "replicas is zero and the new request is",
    "start": "806700",
    "end": "809100"
  },
  {
    "text": "coming through so first this request",
    "start": "809100",
    "end": "811440"
  },
  {
    "text": "will redirect to our Interceptor and the",
    "start": "811440",
    "end": "814860"
  },
  {
    "text": "Interceptor will catch the request into",
    "start": "814860",
    "end": "817200"
  },
  {
    "text": "a queue and now the scalar will work as",
    "start": "817200",
    "end": "820380"
  },
  {
    "text": "an external scaler to tell Keda that",
    "start": "820380",
    "end": "823019"
  },
  {
    "text": "it's time to change the replica",
    "start": "823019",
    "end": "825000"
  },
  {
    "text": "and when Kayla is done his work the",
    "start": "825000",
    "end": "828060"
  },
  {
    "text": "Interceptor will the proxy container in",
    "start": "828060",
    "end": "830820"
  },
  {
    "text": "API server will consume the request from",
    "start": "830820",
    "end": "834240"
  },
  {
    "text": "the queue and redirect to the API server",
    "start": "834240",
    "end": "836760"
  },
  {
    "text": "and once again API server will trigger",
    "start": "836760",
    "end": "839519"
  },
  {
    "text": "the scalar of the runner",
    "start": "839519",
    "end": "842339"
  },
  {
    "text": "so in this case we Leverage The Power of",
    "start": "842339",
    "end": "844920"
  },
  {
    "text": "serverless to improve the scalability",
    "start": "844920",
    "end": "847320"
  },
  {
    "text": "and also save the cost",
    "start": "847320",
    "end": "850500"
  },
  {
    "text": "right so now that our journey has",
    "start": "850500",
    "end": "853320"
  },
  {
    "text": "reached to its destination we start with",
    "start": "853320",
    "end": "856200"
  },
  {
    "text": "using Benton Mount to build and pack",
    "start": "856200",
    "end": "858779"
  },
  {
    "text": "your model into bentos we manage the mer",
    "start": "858779",
    "end": "862019"
  },
  {
    "text": "version of your Bento and your model and",
    "start": "862019",
    "end": "865139"
  },
  {
    "text": "when it comes to the deployment",
    "start": "865139",
    "end": "866700"
  },
  {
    "text": "deployment will containerize your Bento",
    "start": "866700",
    "end": "869760"
  },
  {
    "text": "into an OCR image and when it needs to",
    "start": "869760",
    "end": "872700"
  },
  {
    "text": "be deployed in the production we use",
    "start": "872700",
    "end": "875820"
  },
  {
    "text": "serverless to make sure that your",
    "start": "875820",
    "end": "877500"
  },
  {
    "text": "deployment is not only scalable but also",
    "start": "877500",
    "end": "880320"
  },
  {
    "text": "cost efficiency",
    "start": "880320",
    "end": "882120"
  },
  {
    "text": "so from model to app is not easy today I",
    "start": "882120",
    "end": "885839"
  },
  {
    "text": "just want to share with you our approach",
    "start": "885839",
    "end": "887820"
  },
  {
    "text": "to doing to do this but I believe",
    "start": "887820",
    "end": "890459"
  },
  {
    "text": "there's a tons of other ways out there",
    "start": "890459",
    "end": "892620"
  },
  {
    "text": "in the community our approach is just",
    "start": "892620",
    "end": "895260"
  },
  {
    "text": "one of them so if you are interested in",
    "start": "895260",
    "end": "897899"
  },
  {
    "text": "this please join our community and we",
    "start": "897899",
    "end": "900120"
  },
  {
    "text": "can have further discussion yeah so",
    "start": "900120",
    "end": "902760"
  },
  {
    "text": "that's all of my slides today thank you",
    "start": "902760",
    "end": "904560"
  },
  {
    "text": "everyone for the listening",
    "start": "904560",
    "end": "906660"
  },
  {
    "text": "[Applause]",
    "start": "906660",
    "end": "910889"
  }
]