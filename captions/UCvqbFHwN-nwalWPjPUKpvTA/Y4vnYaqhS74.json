[
  {
    "start": "0",
    "end": "49000"
  },
  {
    "text": "okay hello everyone my name is bharata Skiba and i am a software engineer at",
    "start": "589",
    "end": "8370"
  },
  {
    "text": "Google were in Warsaw in Poland and I have spent my last one and a half years",
    "start": "8370",
    "end": "14490"
  },
  {
    "text": "working on after scaling in kubernetes and I have worked on multiple parts of",
    "start": "14490",
    "end": "20460"
  },
  {
    "text": "outer scaling and mainly cluster to scaling and also horizontal pallava",
    "start": "20460",
    "end": "26400"
  },
  {
    "text": "scaling but currently I've shifted my focus or to bring another dimension about the scaling to the kubernetes",
    "start": "26400",
    "end": "33149"
  },
  {
    "text": "world namely vertical without the scaling and this is what I would like to talk to you about today and in this talk",
    "start": "33149",
    "end": "40950"
  },
  {
    "text": "you will learn how ver Tecopa autoscaler can take the burden of choosing just the",
    "start": "40950",
    "end": "46200"
  },
  {
    "text": "right size for your pots so in kubernetes the size of the pot is",
    "start": "46200",
    "end": "52350"
  },
  {
    "start": "49000",
    "end": "165000"
  },
  {
    "text": "expressed as a resource requests so let's first take a look at what it means and how it impacts your applications you",
    "start": "52350",
    "end": "60930"
  },
  {
    "text": "have probably all seen something like this this is the simple deployment actually it's a part of a simple",
    "start": "60930",
    "end": "67170"
  },
  {
    "text": "deployment but for the sake of simply simplifications so that shouldn't be too hard right you just want to deploy a",
    "start": "67170",
    "end": "73710"
  },
  {
    "text": "simple thing to your kubernetes cluster i would like to focus specifically on the part that is shown at the bottom so",
    "start": "73710",
    "end": "80820"
  },
  {
    "text": "the resource request part here we say that deployment needs 100 megabytes of",
    "start": "80820",
    "end": "86250"
  },
  {
    "text": "memory and 250 milli suppose ok that's fantastic but how do we know that",
    "start": "86250",
    "end": "92960"
  },
  {
    "text": "so the reality looks more often like this we don't know we have no idea how",
    "start": "92960",
    "end": "98220"
  },
  {
    "text": "much resources our applications will actually need but why is that even",
    "start": "98220",
    "end": "103320"
  },
  {
    "text": "important why is the fact that we often don't know how to see set those resource requests harmful for our applications so",
    "start": "103320",
    "end": "111659"
  },
  {
    "text": "to start with the resource request is actually a contract it's a contract between your workload and the kubernetes",
    "start": "111659",
    "end": "119100"
  },
  {
    "text": "scheduler the resources that you requests are guaranteed to be reserved",
    "start": "119100",
    "end": "125280"
  },
  {
    "text": "for your workloads provided that they are available in the cluster so this",
    "start": "125280",
    "end": "130649"
  },
  {
    "text": "means for example that if you require a certain amount of CPU or memory kubernetes scheduler promises never to",
    "start": "130649",
    "end": "137070"
  },
  {
    "text": "put your workloads on a note that can't support that that doesn't have that resources available so on the other side of this bergen if",
    "start": "137070",
    "end": "144450"
  },
  {
    "text": "there is no such place for your workload currently available in the cluster the scheduler will not schedule your pod",
    "start": "144450",
    "end": "152670"
  },
  {
    "text": "anywhere so a short disclaimer this is not entirely true because now we have priority and preemption in kubernetes",
    "start": "152670",
    "end": "158490"
  },
  {
    "text": "but assuming all your pods have the same priority at the moment this is what will",
    "start": "158490",
    "end": "163560"
  },
  {
    "text": "happen so we know now about resource",
    "start": "163560",
    "end": "168630"
  },
  {
    "start": "165000",
    "end": "290000"
  },
  {
    "text": "requests is the contract but that's a big word and what does it really mean in",
    "start": "168630",
    "end": "173940"
  },
  {
    "text": "practice so we'll analyze two example situation to see how resource request",
    "start": "173940",
    "end": "180150"
  },
  {
    "text": "impacts the situation of your workloads in your cluster so this is the first one",
    "start": "180150",
    "end": "185730"
  },
  {
    "text": "and we have two notes in the cluster they are both free and they have 9990",
    "start": "185730",
    "end": "191640"
  },
  {
    "text": "use available space and we have two pods just waiting to be scheduled one is",
    "start": "191640",
    "end": "197160"
  },
  {
    "text": "requesting 200 mils another six 600 million views okay this one is easy",
    "start": "197160",
    "end": "203340"
  },
  {
    "text": "right one pod guts goes into OneNote another part goes into another node but",
    "start": "203340",
    "end": "209310"
  },
  {
    "text": "now imagine that we have mistakenly set the resource requests of the first spot",
    "start": "209310",
    "end": "215910"
  },
  {
    "text": "the red one so we said it actually needs 200 new CPUs but it actually uses 500",
    "start": "215910",
    "end": "222000"
  },
  {
    "text": "miss abuse so at the moment it also okay",
    "start": "222000",
    "end": "227160"
  },
  {
    "text": "right there is spare capacity on the note where you're scheduled and",
    "start": "227160",
    "end": "232700"
  },
  {
    "text": "kubernetes is so specifically cubelet will give you those spare resources because it makes no sense to have that",
    "start": "232700",
    "end": "240000"
  },
  {
    "text": "CPU sitting idle but now we have another pod there's another part it requests 600",
    "start": "240000",
    "end": "246690"
  },
  {
    "text": "and a CPU and it's a good citizen it actually uses the exact amount of resources that it requests of course",
    "start": "246690",
    "end": "253950"
  },
  {
    "text": "that never happens but let's assume and according to your contract of the scheduler you only need 200 million",
    "start": "253950",
    "end": "261120"
  },
  {
    "text": "views and that's what's guaranteed for you so what scheduler will do it will go ahead and",
    "start": "261120",
    "end": "266310"
  },
  {
    "text": "schedule the pot in that free space so now we have the situation your workload is are happy it doesn't have this free",
    "start": "266310",
    "end": "273419"
  },
  {
    "text": "space that it previously depended on so well it's actually not going to also",
    "start": "273419",
    "end": "280290"
  },
  {
    "text": "it's not going to get better with time because the scheduler only use it looks",
    "start": "280290",
    "end": "285720"
  },
  {
    "text": "at the resource requests so it doesn't actually know that your workload is unhappy so now we'll take a look at",
    "start": "285720",
    "end": "292320"
  },
  {
    "text": "another example and this one also includes cluster of the scalar to see",
    "start": "292320",
    "end": "299850"
  },
  {
    "text": "how it also depends on your resource requests set color correctly so this is",
    "start": "299850",
    "end": "307020"
  },
  {
    "text": "situation is similar but the first part is a big bigger so it requests almost a",
    "start": "307020",
    "end": "312720"
  },
  {
    "text": "whole note 800 meters the other one is the same as before so far so good",
    "start": "312720",
    "end": "319350"
  },
  {
    "text": "scheduling is easy one part guards on one note another on the second one but",
    "start": "319350",
    "end": "325850"
  },
  {
    "text": "what if the situation is reversed from what we had in the first place what if we said that we need more than",
    "start": "325850",
    "end": "332850"
  },
  {
    "text": "we're actually using at this moment this is not such a big deal because well",
    "start": "332850",
    "end": "338490"
  },
  {
    "text": "those resources would sit idle anyway right we're not harming anyone else in",
    "start": "338490",
    "end": "344130"
  },
  {
    "text": "this cluster but again we have a new pod and what happens now is scheduler again",
    "start": "344130",
    "end": "352830"
  },
  {
    "text": "takes a look at all the contracts that it has and it sees that there is no",
    "start": "352830",
    "end": "358680"
  },
  {
    "text": "space to schedule the new pod because otherwise it would have to violate one",
    "start": "358680",
    "end": "364680"
  },
  {
    "text": "of the contracts that it has so basically your pod is pending and it",
    "start": "364680",
    "end": "373590"
  },
  {
    "text": "stays spending so this is not good you have the workload that you want to run and it's not running even though the",
    "start": "373590",
    "end": "379470"
  },
  {
    "text": "resources are there like in theory they are there so things get a tad better if",
    "start": "379470",
    "end": "386970"
  },
  {
    "text": "you have cluster two scalar configured in your cluster because the pending pod",
    "start": "386970",
    "end": "393389"
  },
  {
    "text": "the permanently pending pod is just a signal that cluster of the scalar waits for in order to trigger",
    "start": "393389",
    "end": "400260"
  },
  {
    "text": "it's Kayla so it's seize your pot it sees that it's in a pending state it kicks in and it adds a new machine for",
    "start": "400260",
    "end": "407970"
  },
  {
    "text": "you okay so that's a little bit better right because the schedulers notices the",
    "start": "407970",
    "end": "414240"
  },
  {
    "text": "new note schedules the pot and now everyone is happy right well maybe all",
    "start": "414240",
    "end": "422730"
  },
  {
    "text": "your across all your workloads run that's good they have enough resources they actually have more than enough",
    "start": "422730",
    "end": "428850"
  },
  {
    "text": "resources but if the resource requests were set correctly in the first place we",
    "start": "428850",
    "end": "434640"
  },
  {
    "text": "wouldn't need that fir'd note so we're actually using more resources than we need so summing up there is danger in",
    "start": "434640",
    "end": "444510"
  },
  {
    "start": "441000",
    "end": "575000"
  },
  {
    "text": "both too small and too big boxes for your workloads if your requests are too",
    "start": "444510",
    "end": "451380"
  },
  {
    "text": "low as we've seen the scheduler will pack your notes tightly then the pots",
    "start": "451380",
    "end": "457200"
  },
  {
    "text": "don't have enough resources root to run and this results in several unhappy endings you can have out of memory error",
    "start": "457200",
    "end": "464460"
  },
  {
    "text": "errors or workloads evicted due to memory pressure and these both cause",
    "start": "464460",
    "end": "469710"
  },
  {
    "text": "disruption to your workflows they cause your your pots to be evicted and rescheduled on they can also become",
    "start": "469710",
    "end": "475800"
  },
  {
    "text": "pending if you have CPU starvation then",
    "start": "475800",
    "end": "482070"
  },
  {
    "text": "your workers will be underperforming because they actually need more CPU than they're given and what is more this will",
    "start": "482070",
    "end": "489060"
  },
  {
    "text": "be the worst when you need it the most because when your cluster is under high load and you needed to perform you need",
    "start": "489060",
    "end": "496410"
  },
  {
    "text": "your workloads to perform on their best they will actually be frugal and they can actually become evicted because of",
    "start": "496410",
    "end": "504450"
  },
  {
    "text": "not enough memory so lastly as I mentioned before this will not get",
    "start": "504450",
    "end": "510630"
  },
  {
    "text": "better over time nobody used looks at the actual usage of your applications",
    "start": "510630",
    "end": "516510"
  },
  {
    "text": "only on the requests so until you come and change them yourself you're in D and",
    "start": "516510",
    "end": "523380"
  },
  {
    "text": "happy state so it ends what is more this",
    "start": "523380",
    "end": "530490"
  },
  {
    "text": "will not get better even if you cluster autoscaler configured because as we've seen it depends on seeing a",
    "start": "530490",
    "end": "537579"
  },
  {
    "text": "pending pod that has no space to run but that this in turn depends on the scheduler to see that this part should",
    "start": "537579",
    "end": "544630"
  },
  {
    "text": "be pending which in turn again depends on correct correctly said resource requests on the other hand if we specify",
    "start": "544630",
    "end": "553870"
  },
  {
    "text": "too high resource requests then this may cause us to waste resources if we",
    "start": "553870",
    "end": "560769"
  },
  {
    "text": "overshoot them the scheduler will spread the pods on more nodes that we actually need and the cluster the scaler with",
    "start": "560769",
    "end": "567670"
  },
  {
    "text": "will kick in earlier because it was depending pods that don't have space to",
    "start": "567670",
    "end": "572920"
  },
  {
    "text": "schedule in your cluster so that's not a",
    "start": "572920",
    "end": "577930"
  },
  {
    "start": "575000",
    "end": "679000"
  },
  {
    "text": "pretty picture that we painted what can we do about it how can we set resource",
    "start": "577930",
    "end": "582940"
  },
  {
    "text": "requests right there's a couple ways that you can do you can try out your",
    "start": "582940",
    "end": "588670"
  },
  {
    "text": "applications by creating cannery deployments you can try simulating production Road and then set requests for your",
    "start": "588670",
    "end": "596410"
  },
  {
    "text": "production workloads based on that you can also go with trial and error sort of",
    "start": "596410",
    "end": "602769"
  },
  {
    "text": "a trial and error procedure you deploy your workloads with some requests then you see if they're happy they are happy",
    "start": "602769",
    "end": "609010"
  },
  {
    "text": "you look at their actual usage do they need more do they need less and you sort",
    "start": "609010",
    "end": "614470"
  },
  {
    "text": "of figure it out on the way you can also just try to guess and it's actually a",
    "start": "614470",
    "end": "620050"
  },
  {
    "text": "bit frightening and but sort of understandable looking at how hard it is there is a lot of people doing it this",
    "start": "620050",
    "end": "626800"
  },
  {
    "text": "way it's hard setting resource request is hard so all of those bad fights have",
    "start": "626800",
    "end": "634209"
  },
  {
    "text": "downsides and testing and caring they require additional resources right they",
    "start": "634209",
    "end": "639250"
  },
  {
    "text": "require require time effort they require additional computational resources what is more your results will they make",
    "start": "639250",
    "end": "648459"
  },
  {
    "text": "and they most probably will differ from what you later see in production and the",
    "start": "648459",
    "end": "655839"
  },
  {
    "text": "last one the last one is the like the most infuriating one because even if you",
    "start": "655839",
    "end": "662339"
  },
  {
    "text": "tailor your research like perfectly for for this moment",
    "start": "662339",
    "end": "668970"
  },
  {
    "text": "actually the resource needs of your applications will change over time just",
    "start": "668970",
    "end": "674860"
  },
  {
    "text": "because the load is changing the traffic is shifting so let me present the hero",
    "start": "674860",
    "end": "681760"
  },
  {
    "start": "679000",
    "end": "1050000"
  },
  {
    "text": "and all this unhappiness it's vertical productive scalar and it essentially",
    "start": "681760",
    "end": "689350"
  },
  {
    "text": "does three main things it does the thing that I said kubernetes scheduler never",
    "start": "689350",
    "end": "694899"
  },
  {
    "text": "does it goes ahead and looks at the actual usage of your applications while",
    "start": "694899",
    "end": "700870"
  },
  {
    "text": "they run then based on these observations it makes recommendations",
    "start": "700870",
    "end": "707769"
  },
  {
    "text": "for what your workloads should actually be using and then if you configure it to",
    "start": "707769",
    "end": "714639"
  },
  {
    "text": "do so it will update the resource requests of your running workloads and that last part",
    "start": "714639",
    "end": "720970"
  },
  {
    "text": "is experimental and I will comment more on why that is a little bit later and",
    "start": "720970",
    "end": "729190"
  },
  {
    "text": "all the information about vertical scaling is contained in a vpa object",
    "start": "729190",
    "end": "735130"
  },
  {
    "text": "it's defined as a customer services customer search definition and it is",
    "start": "735130",
    "end": "740170"
  },
  {
    "text": "divided as most kubernetes objects into spec and status so the spec controls",
    "start": "740170",
    "end": "746890"
  },
  {
    "text": "what and how should be scale and the status is where your recommendations are actually provided so since VP a object",
    "start": "746890",
    "end": "755860"
  },
  {
    "text": "is the central point of everything we're talking about onwards let's take a closer look at what it contains and what",
    "start": "755860",
    "end": "761290"
  },
  {
    "text": "it like what does it let us do so VP a spec is the way for you to provide the",
    "start": "761290",
    "end": "768760"
  },
  {
    "text": "information on what and how you want to scale so this is an example spec that",
    "start": "768760",
    "end": "774670"
  },
  {
    "text": "vertically scale of a worker deployment now where do I don't that from so the",
    "start": "774670",
    "end": "781440"
  },
  {
    "text": "selector is the what to scale part and this is expressed as a regular",
    "start": "781440",
    "end": "788110"
  },
  {
    "text": "kubernetes label selector and this gives us quite a bit of flexibility because this example shows how to outer scale a",
    "start": "788110",
    "end": "796240"
  },
  {
    "text": "single deployment but you can actually say that only part of your deployments should be vertically",
    "start": "796240",
    "end": "801320"
  },
  {
    "text": "scale or I have this two deployments and they're actually pretty similar please scale them together for me the how to",
    "start": "801320",
    "end": "811279"
  },
  {
    "text": "scale part consists of update policy in resource policy the a bleep I see lets",
    "start": "811279",
    "end": "817519"
  },
  {
    "text": "you control the actuation part so how are the equipment recommendations applied to your workloads and there are",
    "start": "817519",
    "end": "826449"
  },
  {
    "text": "free update modes available currently the first is off and it's sort of a dry",
    "start": "826449",
    "end": "832490"
  },
  {
    "text": "run it lets you get the feel of how EPA works for you because it provides the",
    "start": "832490",
    "end": "837680"
  },
  {
    "text": "recommendations but it will never never actually modify your pod requests you",
    "start": "837680",
    "end": "843529"
  },
  {
    "text": "can as I said you can use it to try it out but you can also use it for manual actuation sort of so you use vpa to let",
    "start": "843529",
    "end": "851510"
  },
  {
    "text": "you know what sort of showed up recommendations see what sort of",
    "start": "851510",
    "end": "856519"
  },
  {
    "text": "recommendation it spits out and then take it and update your deployment to request the amount that was recommended",
    "start": "856519",
    "end": "863589"
  },
  {
    "text": "the second mod is an initial and this will only change parts requests during",
    "start": "863589",
    "end": "871730"
  },
  {
    "text": "creation but it will never forcefully restart your parts to change their",
    "start": "871730",
    "end": "877220"
  },
  {
    "text": "requests so this will tap into any pod creations that are caused by something else than vpa system so evictions caused",
    "start": "877220",
    "end": "887029"
  },
  {
    "text": "by other systems adding new parts to deployments was killing it horizontally then rolling updates but if your",
    "start": "887029",
    "end": "895910"
  },
  {
    "text": "workloads are very stable you're not adding pods they're not crashing and restarting then it will not apply your",
    "start": "895910",
    "end": "902089"
  },
  {
    "text": "recommendations to the running pods and the last one is outer and this is a full thing so this will take a look and the",
    "start": "902089",
    "end": "909980"
  },
  {
    "text": "pods running in your cluster and restart them once their requests are too far off from the recommended resources and you",
    "start": "909980",
    "end": "919940"
  },
  {
    "text": "can also control the scaling of containers inside the pods that are",
    "start": "919940",
    "end": "925730"
  },
  {
    "text": "controlled by by your VP a specifying per container policies so",
    "start": "925730",
    "end": "931670"
  },
  {
    "text": "this specific policies policy says that your container your containers and it",
    "start": "931670",
    "end": "937730"
  },
  {
    "text": "says all containers this is the asterisk special container name they will never",
    "start": "937730",
    "end": "942890"
  },
  {
    "text": "get more than five gigs of memory recommended and but this can be done per",
    "start": "942890",
    "end": "949880"
  },
  {
    "text": "container and it can also like you can specify maximum minimum resources and",
    "start": "949880",
    "end": "955720"
  },
  {
    "text": "this goes both for memory and for CPU and you can also turn up scaling for a",
    "start": "955720",
    "end": "962240"
  },
  {
    "text": "specific container inside the pot and this is useful if you for example have a sidecar container that you know always",
    "start": "962240",
    "end": "968450"
  },
  {
    "text": "does the constant constant amount of work and will not actually need scaling",
    "start": "968450",
    "end": "974140"
  },
  {
    "text": "this is the status it's the output of vertical part of the scaler this is",
    "start": "974140",
    "end": "980180"
  },
  {
    "text": "where recommendations are provided and the recommendations are provided on a container level so we recommend",
    "start": "980180",
    "end": "987160"
  },
  {
    "text": "resources for all the containers that were configured to be vertically scaled",
    "start": "987160",
    "end": "992770"
  },
  {
    "text": "the recommendations contains three things it contains the target value a lower and",
    "start": "992770",
    "end": "999260"
  },
  {
    "text": "upper bound so the target is it's highlighted because it's the most",
    "start": "999260",
    "end": "1004510"
  },
  {
    "text": "important thing and this is the actual recommendation this is what we currently think that your workload should be using",
    "start": "1004510",
    "end": "1012040"
  },
  {
    "text": "at the moment the lower and upper bounds are sort of indicators of confidence of",
    "start": "1012040",
    "end": "1019660"
  },
  {
    "text": "our recommendation what this what they actually say is anything below lower",
    "start": "1019660",
    "end": "1025150"
  },
  {
    "text": "bound is known to be not enough for the workload and anything above upper bound",
    "start": "1025150",
    "end": "1031930"
  },
  {
    "text": "we know is wasteful and the bounds get closer to the target the more usage",
    "start": "1031930",
    "end": "1038438"
  },
  {
    "text": "samples we collect so the more confident we are that the recommendation that we're providing is based on a lot of",
    "start": "1038439",
    "end": "1044500"
  },
  {
    "text": "data so it's more we have more confidence that it's right so this",
    "start": "1044500",
    "end": "1051970"
  },
  {
    "start": "1050000",
    "end": "1459000"
  },
  {
    "text": "finishes the part that you need to know as the user and now for those interested",
    "start": "1051970",
    "end": "1058630"
  },
  {
    "text": "how basil looks under the hood let's take a look at how vpa provides actually provides recommendations for you so this",
    "start": "1058630",
    "end": "1066220"
  },
  {
    "text": "is a slight I'm not very proud of because there's a lot of things in it but what I would like to focus on is",
    "start": "1066220",
    "end": "1074860"
  },
  {
    "text": "that vpa is actually divided into three independent parts and these are actually",
    "start": "1074860",
    "end": "1080650"
  },
  {
    "text": "free independent binaries and they all interact only through the VPA object",
    "start": "1080650",
    "end": "1085840"
  },
  {
    "text": "never directly so we have VP a recommender and this is the like the",
    "start": "1085840",
    "end": "1091480"
  },
  {
    "text": "brain this is the thing that provides recommendations for you we have a VP admission plug-in that plugs into the",
    "start": "1091480",
    "end": "1098620"
  },
  {
    "text": "pod creation process in kubernetes to actually update the resource requests of",
    "start": "1098620",
    "end": "1104410"
  },
  {
    "text": "your pods when needed and the third part is the VP updater and this watches the",
    "start": "1104410",
    "end": "1110950"
  },
  {
    "text": "running pods and restarts those that have their requests too far away from",
    "start": "1110950",
    "end": "1116740"
  },
  {
    "text": "recommendation again if this is configured to be done so so the fact",
    "start": "1116740",
    "end": "1121840"
  },
  {
    "text": "that these three are separate and that the only communicate via VP a object--",
    "start": "1121840",
    "end": "1128980"
  },
  {
    "text": "is important from extensibility and plug ability part because if you have it's",
    "start": "1128980",
    "end": "1134890"
  },
  {
    "text": "very easy to provide an environment specific recommender that will calculate the recommendations completely",
    "start": "1134890",
    "end": "1141610"
  },
  {
    "text": "differently and there's a whole things the works you can also switch out updater if you want and this whole thing",
    "start": "1141610",
    "end": "1148180"
  },
  {
    "text": "still works so let's take a look at all those errors and boxes that were there",
    "start": "1148180",
    "end": "1154480"
  },
  {
    "text": "in more detail so first I want to talk about DP recommender so this is the part",
    "start": "1154480",
    "end": "1161620"
  },
  {
    "text": "it's the most important part it watches what's happening with your pods the positives you configure it to be",
    "start": "1161620",
    "end": "1166900"
  },
  {
    "text": "vertically scaled and it gets actual resource usage from metric server and it",
    "start": "1166900",
    "end": "1173740"
  },
  {
    "text": "also gets events that are connected with memory usage so out of memory events and",
    "start": "1173740",
    "end": "1181840"
  },
  {
    "text": "evictions due to memory pressure and it takes a look at VP a expect to see which",
    "start": "1181840",
    "end": "1189220"
  },
  {
    "text": "parts it should actually calculate calculate recommendations for and based",
    "start": "1189220",
    "end": "1194380"
  },
  {
    "text": "on the observations it's revised the recommendation so what's the",
    "start": "1194380",
    "end": "1200680"
  },
  {
    "text": "math behind this you know waving of a hands in the magic VPN kubernetes",
    "start": "1200680",
    "end": "1205750"
  },
  {
    "text": "currently uses a recommendation model that has been used successfully in google borg infrastructure it collects",
    "start": "1205750",
    "end": "1212680"
  },
  {
    "text": "usage samples history in a histogram and then takes a very high percentile of",
    "start": "1212680",
    "end": "1219910"
  },
  {
    "text": "that as target recommendation it currently does this for the last eight",
    "start": "1219910",
    "end": "1225430"
  },
  {
    "text": "days so the history is accumulated for the last eight days if it's if it's available and it's important to note",
    "start": "1225430",
    "end": "1232870"
  },
  {
    "text": "that this is not part of the API as I said before the recommender is pluggable",
    "start": "1232870",
    "end": "1238600"
  },
  {
    "text": "this is this is the way the recommendations are provided is not part",
    "start": "1238600",
    "end": "1243700"
  },
  {
    "text": "of the API and it's not guaranteed to always be the same so if we find a better algorithm if we find that it's",
    "start": "1243700",
    "end": "1249490"
  },
  {
    "text": "not suitable to some workloads we may and we probably will change it in the future so now we have our accommodation",
    "start": "1249490",
    "end": "1258880"
  },
  {
    "text": "ready now we would like to make use of it so when a part is created it goes",
    "start": "1258880",
    "end": "1264310"
  },
  {
    "text": "through in kubernetes it goes through a series of admission plugins and one of those is a mutating weapon at mission",
    "start": "1264310",
    "end": "1271180"
  },
  {
    "text": "plugin with which can be configured to call external web hooks to do some",
    "start": "1271180",
    "end": "1279010"
  },
  {
    "text": "things to the pod that is currently being created and one of those plugins is the VP admission plugin and what it",
    "start": "1279010",
    "end": "1286390"
  },
  {
    "text": "does it's it's aware of all the active vertical part out of scaling objects in the cluster it sees the pod that is",
    "start": "1286390",
    "end": "1293680"
  },
  {
    "text": "currently being created it's it's watch it takes a look at the VP a objects and verifies if this this pod actually",
    "start": "1293680",
    "end": "1302020"
  },
  {
    "text": "matches one of those and if you have configured your pod to be actually",
    "start": "1302020",
    "end": "1307480"
  },
  {
    "text": "actuated upon so the initial and outer moat that I said that I talked about before it will modify the resource",
    "start": "1307480",
    "end": "1314320"
  },
  {
    "text": "request to the target of the recommendation in the last part the",
    "start": "1314320",
    "end": "1322090"
  },
  {
    "text": "experimental part is actually updating pods running pods during their lifetime",
    "start": "1322090",
    "end": "1329890"
  },
  {
    "text": "so this is experimental and there's several reasons for that firstly in",
    "start": "1329890",
    "end": "1335500"
  },
  {
    "text": "kubernetes the pot spec is immutable so what this means it's actually impossible",
    "start": "1335500",
    "end": "1343420"
  },
  {
    "text": "in the kubernetes system to change resource requests in place you cannot",
    "start": "1343420",
    "end": "1349299"
  },
  {
    "text": "modify your search request of a pub during its lifetime so what do you have to do is you have to evict the pot and",
    "start": "1349299",
    "end": "1356559"
  },
  {
    "text": "let it be recreated and only then you can assign the correct resource requests",
    "start": "1356559",
    "end": "1362910"
  },
  {
    "text": "so this will cause actual disruption to your workload and there's a second",
    "start": "1362910",
    "end": "1370299"
  },
  {
    "text": "reason currently vpa is not aware it's not very aware of the infrastructure",
    "start": "1370299",
    "end": "1375820"
  },
  {
    "text": "that you're running in a way that it can recommend pot sizes that don't fit your",
    "start": "1375820",
    "end": "1381700"
  },
  {
    "text": "cluster for example too big for all your notes when it happens it will stop",
    "start": "1381700",
    "end": "1389799"
  },
  {
    "text": "updating your pods but still this will cause this can make cause actual",
    "start": "1389799",
    "end": "1394900"
  },
  {
    "text": "disruption so the way this works now now we have this disclaimer of experimental",
    "start": "1394900",
    "end": "1400600"
  },
  {
    "text": "out of the way the way this works is updater constantly analyzes the list of pods and",
    "start": "1400600",
    "end": "1406510"
  },
  {
    "text": "when it notices that there is a pod that is too much out of sync with the",
    "start": "1406510",
    "end": "1412840"
  },
  {
    "text": "recommendation and this translates actually to if the requests is outside",
    "start": "1412840",
    "end": "1419770"
  },
  {
    "text": "of the lower and upper bound that we talked about before this means that it",
    "start": "1419770",
    "end": "1424900"
  },
  {
    "text": "needs to be updated so the updater goes ahead and evicts the pod and this goes through the kubernetes eviction api so",
    "start": "1424900",
    "end": "1431980"
  },
  {
    "text": "it will observe any pod disruption budgets that you have and then it",
    "start": "1431980",
    "end": "1437080"
  },
  {
    "text": "depends on a controller so the pod has to be created by a replica set or",
    "start": "1437080",
    "end": "1442990"
  },
  {
    "text": "stateful said it would not so we rely on the controller to recreate a pod and",
    "start": "1442990",
    "end": "1449740"
  },
  {
    "text": "then go through all those submission plugins again and get correct resource requests assigned",
    "start": "1449740",
    "end": "1457440"
  },
  {
    "start": "1459000",
    "end": "1658000"
  },
  {
    "text": "all right so let's see where we currently are and what's planned for the",
    "start": "1459160",
    "end": "1464990"
  },
  {
    "text": "project so we had an alpha release in",
    "start": "1464990",
    "end": "1470090"
  },
  {
    "text": "April and we got feedback from community early adopters for which we are",
    "start": "1470090",
    "end": "1476840"
  },
  {
    "text": "extremely thankful for it helped us a lot to iterate to fix some bugs fix some things to verify that the API that we",
    "start": "1476840",
    "end": "1484850"
  },
  {
    "text": "have actually enables people to express the things that they want and again",
    "start": "1484850",
    "end": "1490760"
  },
  {
    "text": "again we were very very grateful and the we have come up with the beta quality",
    "start": "1490760",
    "end": "1497270"
  },
  {
    "text": "product and this is being in the process of being graduated at the moment so like",
    "start": "1497270",
    "end": "1503300"
  },
  {
    "text": "literally I mean we were just about to open a final PR with updates to documentation and we were ready to go so",
    "start": "1503300",
    "end": "1509390"
  },
  {
    "text": "I expect if when you come back from this cube colony we'll find it that it is indeed in beta and then what are the",
    "start": "1509390",
    "end": "1518060"
  },
  {
    "text": "plans what we believe is we're still missing the most are two key areas and",
    "start": "1518060",
    "end": "1524290"
  },
  {
    "text": "the first one is setting application limits because as maybe some of you",
    "start": "1524290",
    "end": "1530000"
  },
  {
    "text": "notice through all of the talk I've been talking about requests this request that but there's also limits and limits are",
    "start": "1530000",
    "end": "1536420"
  },
  {
    "text": "actually important especially for memory for your workload stability and",
    "start": "1536420",
    "end": "1541910"
  },
  {
    "text": "predictability and we recognize that and we're working on incorporating this into vertical pod autoscaler as well and",
    "start": "1541910",
    "end": "1550690"
  },
  {
    "text": "secondly I've been talking about that fact that pod spec is immutable but",
    "start": "1550690",
    "end": "1556670"
  },
  {
    "text": "there's actually an effort in the kubernetes community to change that and to make it possible to update resource",
    "start": "1556670",
    "end": "1564170"
  },
  {
    "text": "requests on the fly so if that happens and we will be able to take advantage of",
    "start": "1564170",
    "end": "1573050"
  },
  {
    "text": "that and to also be able to do non-disruptive updates to your workloads whenever that's possible because imagine",
    "start": "1573050",
    "end": "1579950"
  },
  {
    "text": "you just want to shrink your pod it's not going to use more requests it's still going to fit on the note why we",
    "start": "1579950",
    "end": "1586970"
  },
  {
    "text": "started right so there is like we recognize however that",
    "start": "1586970",
    "end": "1592300"
  },
  {
    "text": "this effort will require significant amount of work it touches a lot of",
    "start": "1592300",
    "end": "1598090"
  },
  {
    "text": "things at the curve kubernetes so kubernetes the board scheduler works the way crew cubelet works and also this has",
    "start": "1598090",
    "end": "1605830"
  },
  {
    "text": "been in grading assumption in kubernetes for a long time like everybody expects",
    "start": "1605830",
    "end": "1611050"
  },
  {
    "text": "these things to just stay immutable so we are watching this we're trying to",
    "start": "1611050",
    "end": "1616840"
  },
  {
    "text": "help with the with the community movement to to make those resource",
    "start": "1616840",
    "end": "1622720"
  },
  {
    "text": "requests mutable and also make that this work actually for for VP a so that we",
    "start": "1622720",
    "end": "1628270"
  },
  {
    "text": "can take use of it to provide not don't not don't not disruptive updates I'm not",
    "start": "1628270",
    "end": "1637690"
  },
  {
    "text": "sure that's very visible but we're actually part of stick out of scaling",
    "start": "1637690",
    "end": "1643020"
  },
  {
    "text": "you can find us at the kubernetes autoscaler repo on github please go",
    "start": "1643020",
    "end": "1649870"
  },
  {
    "text": "check it out check vpa out let us know how it works for you and i guess that's",
    "start": "1649870",
    "end": "1659020"
  },
  {
    "text": "it thank you very much [Applause]",
    "start": "1659020",
    "end": "1666559"
  },
  {
    "text": "so if they're sending questions yes yes",
    "start": "1666559",
    "end": "1673679"
  },
  {
    "text": "so the question was our BPA's namespace specific and the answer is yes the BPA",
    "start": "1673679",
    "end": "1679620"
  },
  {
    "text": "is a namespace it's a namespace CRD so",
    "start": "1679620",
    "end": "1687179"
  },
  {
    "start": "1686000",
    "end": "1748000"
  },
  {
    "text": "the question was will be ever will the updater epoch pods gradually it's very",
    "start": "1687179",
    "end": "1694290"
  },
  {
    "text": "much depends on your so it uses the eviction API so if you have a pod",
    "start": "1694290",
    "end": "1700440"
  },
  {
    "text": "disruption budget that only says you can also only have one put down then it will",
    "start": "1700440",
    "end": "1707010"
  },
  {
    "text": "just update one by one it also has some internal rate limiting so we have I",
    "start": "1707010",
    "end": "1713940"
  },
  {
    "text": "don't remember the exact numbers but it's probably something like not more than 10% of your workload can be down at",
    "start": "1713940",
    "end": "1724110"
  },
  {
    "text": "one go but you can control it very well you should be able to control it very well by setting very restrictive for",
    "start": "1724110",
    "end": "1730500"
  },
  {
    "text": "description budgets",
    "start": "1730500",
    "end": "1733250"
  },
  {
    "start": "1748000",
    "end": "1798000"
  },
  {
    "text": "so the question was if you have a new version of your deployment and it's actually different because it's a new",
    "start": "1748710",
    "end": "1754870"
  },
  {
    "text": "version and has different memory needs it actually needs more memory will the same history be used so the answer at",
    "start": "1754870",
    "end": "1762580"
  },
  {
    "text": "the moment is yes the same history B will be used so we're in early stages",
    "start": "1762580",
    "end": "1767770"
  },
  {
    "text": "and we will be looking at how we can also be smarter with the deployment",
    "start": "1767770",
    "end": "1778630"
  },
  {
    "text": "lifecycle right we can be more we're not",
    "start": "1778630",
    "end": "1783700"
  },
  {
    "text": "looking at this activity at the moment but we are aware that this may be a problem we'll be looking at the ADI",
    "start": "1783700",
    "end": "1788710"
  },
  {
    "text": "feedback so the question was it",
    "start": "1788710",
    "end": "1800440"
  },
  {
    "text": "integrates with metrics server but does it also have a way to plug in other",
    "start": "1800440",
    "end": "1805450"
  },
  {
    "text": "metric metric providers or does it work with other metric providers I believe we",
    "start": "1805450",
    "end": "1811539"
  },
  {
    "text": "have some integration of Prometheus I'm not sure how well that works actually we",
    "start": "1811539",
    "end": "1820000"
  },
  {
    "text": "don't have it we definitely don't have it extensively tested but there is a way",
    "start": "1820000",
    "end": "1825460"
  },
  {
    "text": "to provide different to have a different metrics provider in a system oh yes yes",
    "start": "1825460",
    "end": "1837970"
  },
  {
    "text": "that's so the question was can you explain how this works with HBA so at",
    "start": "1837970",
    "end": "1843490"
  },
  {
    "text": "the moment the answer is with CPU and memory it doesn't unfortunately so they",
    "start": "1843490",
    "end": "1853539"
  },
  {
    "text": "will erase each other they were not aware of each other so they will race each other with it's actually we haven't",
    "start": "1853539",
    "end": "1860140"
  },
  {
    "text": "tested it extensively but the intuition is it won't work very very well on the same metrics but what you can do is HPA",
    "start": "1860140",
    "end": "1868870"
  },
  {
    "text": "now supports custom metrics and external metrics and we actually have people successfully using VP a with H",
    "start": "1868870",
    "end": "1876900"
  },
  {
    "text": "configured on custom metrics so this is this is one way to solve this",
    "start": "1876900",
    "end": "1884840"
  },
  {
    "start": "1893000",
    "end": "1922000"
  },
  {
    "text": "so question is there a way to make it aware to make it be aware of the old",
    "start": "1894149",
    "end": "1899879"
  },
  {
    "text": "dear sources and if it's scarce on telly to up scale it down vertically I guess",
    "start": "1899879",
    "end": "1909389"
  },
  {
    "text": "we probably will be looking at how these two work together we haven't done it",
    "start": "1909389",
    "end": "1914519"
  },
  {
    "text": "looked into it yet",
    "start": "1914519",
    "end": "1917449"
  },
  {
    "text": "the question is do we have do you have plans to support extra extender",
    "start": "1923489",
    "end": "1928739"
  },
  {
    "text": "resources on the roadmap currently not but probably if we see that this is",
    "start": "1928739",
    "end": "1935129"
  },
  {
    "text": "something that the community needs we will take a look all right you see a lot of question",
    "start": "1935129",
    "end": "1941749"
  },
  {
    "start": "1949000",
    "end": "2015000"
  },
  {
    "text": "so the question is if there is no CPU requests or memory quest specified for",
    "start": "1949810",
    "end": "1955880"
  },
  {
    "text": "the deployment will it still work yes it will still work we don't depend on the",
    "start": "1955880",
    "end": "1961550"
  },
  {
    "text": "request initial requests to to work we will just look at your usage the only",
    "start": "1961550",
    "end": "1968600"
  },
  {
    "text": "problem is if you actually so if you have no resource request set at the beginning if you don't use the outer",
    "start": "1968600",
    "end": "1976760"
  },
  {
    "text": "mode if you just deploy your deployment it takes some time for DPA to generate",
    "start": "1976760",
    "end": "1983870"
  },
  {
    "text": "the recommendation if the pods are already running and they're not you know being restarted for whatever other",
    "start": "1983870",
    "end": "1989930"
  },
  {
    "text": "reason if you don't have automatic updates in VPI they will just run without the requests but if you have",
    "start": "1989930",
    "end": "1996500"
  },
  {
    "text": "auto mode on or if something happens to your application we have new pods coming",
    "start": "1996500",
    "end": "2001780"
  },
  {
    "text": "then this they will get the new recommended resources I think I've seen",
    "start": "2001780",
    "end": "2007780"
  },
  {
    "text": "here questioned for some time",
    "start": "2007780",
    "end": "2011310"
  },
  {
    "start": "2015000",
    "end": "2030000"
  },
  {
    "text": "so for the eviction process can it handle custom shutdown so custom by",
    "start": "2016560",
    "end": "2023220"
  },
  {
    "text": "customs shutdown do you mean I don't know like you have graceful termination and stuff like that yeah so I think this",
    "start": "2023220",
    "end": "2032160"
  },
  {
    "start": "2030000",
    "end": "2103000"
  },
  {
    "text": "is handled by the affectionately I wouldn't the only thing we do we ask kindly ask kubernetes a big de pot for",
    "start": "2032160",
    "end": "2038910"
  },
  {
    "text": "us and however long it takes it will it will accept that so we're we were actually thinking of limiting the",
    "start": "2038910",
    "end": "2045720"
  },
  {
    "text": "graceful termination period period because if you have a very long graceful termination period that will make your",
    "start": "2045720",
    "end": "2051750"
  },
  {
    "text": "VP a very very slow in updating your pods but at the moment this is your",
    "start": "2051750",
    "end": "2057240"
  },
  {
    "text": "choice if you're apart have a very long graceful termination period we will definitely observe that",
    "start": "2057240",
    "end": "2064700"
  },
  {
    "text": "so two questions first was how long does",
    "start": "2102970",
    "end": "2108020"
  },
  {
    "start": "2103000",
    "end": "2141000"
  },
  {
    "text": "it take for the lower and upper bounds who actually calls for evictions and so",
    "start": "2108020",
    "end": "2113270"
  },
  {
    "text": "let me first answer that so this is heavily dependent on how your workload",
    "start": "2113270",
    "end": "2118369"
  },
  {
    "text": "is dependent there is we're behaving I",
    "start": "2118369",
    "end": "2123650"
  },
  {
    "text": "can if you come later I can show you specifically what's how it's done but I",
    "start": "2123650",
    "end": "2130520"
  },
  {
    "text": "think basically it is conservative so at the moment I think after 12 hours you",
    "start": "2130520",
    "end": "2138740"
  },
  {
    "text": "need to be if you're overshooting you",
    "start": "2138740",
    "end": "2143990"
  },
  {
    "start": "2141000",
    "end": "2204000"
  },
  {
    "text": "need to overshoot by I think at least three times something like that and then it goes down after a day it goes much",
    "start": "2143990",
    "end": "2151640"
  },
  {
    "text": "closer so once you have actually accumulated full history it can react very fast it will also react faster if",
    "start": "2151640",
    "end": "2159349"
  },
  {
    "text": "you have actual out of memory errors so it will see that and will actually bump",
    "start": "2159349",
    "end": "2165020"
  },
  {
    "text": "it up very much so but it's very it's very work on specific so there's no like",
    "start": "2165020",
    "end": "2171410"
  },
  {
    "text": "you know after this time the update will happen as I said I can come and then I",
    "start": "2171410",
    "end": "2177200"
  },
  {
    "text": "can show you the actual piece of the code that the comments in the code that describe how it's done and there is the second question the",
    "start": "2177200",
    "end": "2186020"
  },
  {
    "text": "domino effect so is it so whenever one",
    "start": "2186020",
    "end": "2191270"
  },
  {
    "text": "part needs updating probably the other and it gets killed the other Pods will",
    "start": "2191270",
    "end": "2196579"
  },
  {
    "text": "get more load and then they need to be killed too because they start like they're even worse so the way this works",
    "start": "2196579",
    "end": "2206599"
  },
  {
    "start": "2204000",
    "end": "2222000"
  },
  {
    "text": "the recommendation is provided for the",
    "start": "2206599",
    "end": "2212059"
  },
  {
    "text": "whole workload so all of your parts so at the moment we decide we need to",
    "start": "2212059",
    "end": "2218390"
  },
  {
    "text": "update your pods we decide we app we need to update all your parts",
    "start": "2218390",
    "end": "2223480"
  },
  {
    "start": "2222000",
    "end": "2339000"
  },
  {
    "text": "if in the meantime your pod gets shut down the recommendation may vary",
    "start": "2223579",
    "end": "2229880"
  },
  {
    "text": "slightly because of the domino effect that you mentioned but I would actually",
    "start": "2229880",
    "end": "2235069"
  },
  {
    "text": "have to see it in the world to see that this actually causes domino effect",
    "start": "2235069",
    "end": "2240200"
  },
  {
    "text": "because I think",
    "start": "2240200",
    "end": "2242890"
  },
  {
    "text": "yeah I think they couldn't",
    "start": "2296860",
    "end": "2300760"
  },
  {
    "text": "okay yeah I guess I can see in our question can you can can come a little",
    "start": "2307480",
    "end": "2318470"
  },
  {
    "text": "closer because I'm sorry I can't hear so",
    "start": "2318470",
    "end": "2340310"
  },
  {
    "start": "2339000",
    "end": "2384000"
  },
  {
    "text": "the suggestion was only update on surface upgrade and I think this is this",
    "start": "2340310",
    "end": "2347420"
  },
  {
    "text": "is handled if I understand collect correctly what you mean this is handled by the initial mode so this is this",
    "start": "2347420",
    "end": "2355010"
  },
  {
    "text": "takes this never forcefully automatically restarts your pods it will only apply the recommendations",
    "start": "2355010",
    "end": "2361640"
  },
  {
    "text": "if for some other reason for example an upgrade your pods are being recreated",
    "start": "2361640",
    "end": "2367400"
  },
  {
    "text": "and we see them created so I think this is this is what covers this use case and it should be it should be useful",
    "start": "2367400",
    "end": "2375280"
  },
  {
    "start": "2384000",
    "end": "2455000"
  },
  {
    "text": "so the question was is there any overhead running this permanently as opposed to just learning and then using",
    "start": "2384050",
    "end": "2392790"
  },
  {
    "text": "that so this is actually running in your cluster if you deploy it it's it's actually adding some loud to your",
    "start": "2392790",
    "end": "2399300"
  },
  {
    "text": "cluster because this is free free binaries additional binaries running in your cluster it will also cause some",
    "start": "2399300",
    "end": "2406800"
  },
  {
    "text": "additional load on your API server because it needs to fetch it'll push all the data and the metric server or",
    "start": "2406800",
    "end": "2412410"
  },
  {
    "text": "another metric provider so there's an overhead if you're if your workloads are",
    "start": "2412410",
    "end": "2418470"
  },
  {
    "text": "very stable then I guess just just generating this and using it later it's",
    "start": "2418470",
    "end": "2425280"
  },
  {
    "text": "fine but if your workloads will change over time which i think is true for most",
    "start": "2425280",
    "end": "2430950"
  },
  {
    "text": "of them then I'd suggest just run it",
    "start": "2430950",
    "end": "2435980"
  },
  {
    "start": "2455000",
    "end": "2494000"
  },
  {
    "text": "so I only heard the first part the first part was is there a lower like hard low",
    "start": "2456120",
    "end": "2462470"
  },
  {
    "text": "lower bound so the answer is yes and I",
    "start": "2462470",
    "end": "2467850"
  },
  {
    "text": "don't remember this I think it's I don't remember the specific numbers because we changed it but I can go and check it but",
    "start": "2467850",
    "end": "2476430"
  },
  {
    "text": "there is actually we there is a lower bound under which we don't recommend",
    "start": "2476430",
    "end": "2481530"
  },
  {
    "text": "resources",
    "start": "2481530",
    "end": "2484340"
  },
  {
    "start": "2494000",
    "end": "2556000"
  },
  {
    "text": "so this so the question is is it limit range aware I mean so the answer is I",
    "start": "2494850",
    "end": "2503120"
  },
  {
    "text": "think limit range also works as an admission plugin am I correct",
    "start": "2503120",
    "end": "2508350"
  },
  {
    "text": "so it's sort of with admission plug-in like VP is also an admission plugin so this this depends on how on the on the",
    "start": "2508350",
    "end": "2517230"
  },
  {
    "text": "order so I think I'm not sure how this works with the web hook plugins because",
    "start": "2517230",
    "end": "2523800"
  },
  {
    "text": "it's possible that the web park admission plug-in runs at the very end and then this will not work I will have",
    "start": "2523800",
    "end": "2531000"
  },
  {
    "text": "to I would have to check how its configured but okay any last questions",
    "start": "2531000",
    "end": "2538820"
  },
  {
    "start": "2556000",
    "end": "2580000"
  },
  {
    "text": "sir can so I heard what happens if you have expire at the beginning and you're",
    "start": "2557320",
    "end": "2562750"
  },
  {
    "text": "a business cpu-bound okay I think I",
    "start": "2562750",
    "end": "2582460"
  },
  {
    "text": "think I'm wearing out at this time I didn't get get all this but if you can come and I guess we can discuss okay",
    "start": "2582460",
    "end": "2589180"
  },
  {
    "text": "thank you very much",
    "start": "2589180",
    "end": "2591960"
  }
]