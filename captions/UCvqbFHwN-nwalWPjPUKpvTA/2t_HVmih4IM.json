[
  {
    "start": "0",
    "end": "25000"
  },
  {
    "text": "hello hello everyone could afternoon this session we were going to talk about",
    "start": "60",
    "end": "6359"
  },
  {
    "text": "the eBay Co distributed database on culinary's we come from eBay first let",
    "start": "6359",
    "end": "12450"
  },
  {
    "text": "me introduce myself my name is the single I worked in the eBay data platform team actually I'm a kubernetes",
    "start": "12450",
    "end": "19350"
  },
  {
    "text": "user and another community developer and this Autopia included two parts in the",
    "start": "19350",
    "end": "24630"
  },
  {
    "text": "history today's agenda so first ever introduces a eBay geo disability database where I will share the",
    "start": "24630",
    "end": "31529"
  },
  {
    "start": "25000",
    "end": "25000"
  },
  {
    "text": "experience of how we deploy such a database on the kubernetes the second part of my colleague of Chania please",
    "start": "31529",
    "end": "38969"
  },
  {
    "text": "from ebay community we will do a deeper dive as a storage management in",
    "start": "38969",
    "end": "44070"
  },
  {
    "text": "kubernetes at the ebay we did a few extensions of the storage apart on top",
    "start": "44070",
    "end": "50340"
  },
  {
    "text": "of the kubernetes and this is the core causes to support us and running the database on top of the kubernetes so",
    "start": "50340",
    "end": "58920"
  },
  {
    "text": "first let me start from the lesion of the EP IOT trillion to the base as you know ebay our vision is we want to",
    "start": "58920",
    "end": "66900"
  },
  {
    "start": "59000",
    "end": "59000"
  },
  {
    "text": "people to shopper any item from ebay from any place anytime while any device",
    "start": "66900",
    "end": "71909"
  },
  {
    "text": "so we have the requirement that the one to the data close to the user so that's",
    "start": "71909",
    "end": "77729"
  },
  {
    "text": "why we develop as a Tod database and the vision of the these databases we want to",
    "start": "77729",
    "end": "83400"
  },
  {
    "text": "provide a resilient color the native and a cost-effective data platform to supply eBay close",
    "start": "83400",
    "end": "89250"
  },
  {
    "text": "global business so we want to eat are always available and also is can be",
    "start": "89250",
    "end": "95159"
  },
  {
    "text": "highly scalable and to support as a business guru and also attention to be high performance since we deployed our",
    "start": "95159",
    "end": "103020"
  },
  {
    "text": "cloud the native environment so we also want our operation agility we want to",
    "start": "103020",
    "end": "108960"
  },
  {
    "text": "everything can be fully automated otherwise it cannot survive if we don't",
    "start": "108960",
    "end": "115229"
  },
  {
    "text": "have this automation our cotton native environment so this is our vision of our",
    "start": "115229",
    "end": "120570"
  },
  {
    "text": "CEO to achieve a database in eBay so now let's look at how it looks like so",
    "start": "120570",
    "end": "126840"
  },
  {
    "start": "125000",
    "end": "125000"
  },
  {
    "text": "actually our distribution tott database idea is quite a similar like other",
    "start": "126840",
    "end": "133000"
  },
  {
    "text": "but this really the base so why we want to dispute it in the base is just that",
    "start": "133000",
    "end": "138070"
  },
  {
    "text": "the dealer said is too large what kind of fitting you one node so distributor actually have two meanings one we want",
    "start": "138070",
    "end": "144580"
  },
  {
    "text": "to split it the logic data set to a small chunk of data set in this case we",
    "start": "144580",
    "end": "150520"
  },
  {
    "text": "call it a shot actually it's a shot or ruin G of a Kieran Xia and physically we call this",
    "start": "150520",
    "end": "158050"
  },
  {
    "text": "replica set this is a duplicate in our database not the same as the because",
    "start": "158050",
    "end": "163060"
  },
  {
    "text": "there the counselor in the Canaries it's quite a similar so each shraddha",
    "start": "163060",
    "end": "168130"
  },
  {
    "text": "actually contain a small range is just a small data set of the global data set",
    "start": "168130",
    "end": "173170"
  },
  {
    "text": "and we was the second of the differences we want to this data have a few copies",
    "start": "173170",
    "end": "178300"
  },
  {
    "text": "and these copies will be distributed our multiple machines different rooms so",
    "start": "178300",
    "end": "184450"
  },
  {
    "text": "that's the the logical view of a dispute database Knology I have a graph to show",
    "start": "184450",
    "end": "192790"
  },
  {
    "text": "how physically deployed in a real environment so the database were",
    "start": "192790",
    "end": "199269"
  },
  {
    "text": "deployed other multiple zones unimportant layer record is the storage layer the storage layer just many a",
    "start": "199269",
    "end": "207100"
  },
  {
    "text": "storage node you should notice our mental machines it's a physical Hardware in each storage node we spawn a few",
    "start": "207100",
    "end": "215760"
  },
  {
    "text": "database engine process on this storage node and in the Kaurava same color",
    "start": "215760",
    "end": "221500"
  },
  {
    "text": "actually means the data copy is the same under the number i putting in here it just means a shadow ID of the of the",
    "start": "221500",
    "end": "229750"
  },
  {
    "text": "data so logically Arshad for example the",
    "start": "229750",
    "end": "235200"
  },
  {
    "text": "number to shut it will be in the first and second under the force node actually",
    "start": "235200",
    "end": "242950"
  },
  {
    "text": "this storage layer is quite a large the scale at Oak Valley pH will be assaulted",
    "start": "242950",
    "end": "247989"
  },
  {
    "text": "so these database engines so how our application to access this database data",
    "start": "247989",
    "end": "254769"
  },
  {
    "text": "to the reader right since we have thousands of the storage engines on the storage layer we cannot a letter the",
    "start": "254769",
    "end": "261489"
  },
  {
    "text": "application to connect to the other director so we introduce our service layer this",
    "start": "261489",
    "end": "266600"
  },
  {
    "text": "service layer to rot in the application requests to the storage layer database",
    "start": "266600",
    "end": "273230"
  },
  {
    "text": "engines so this service layer activated sister Denise but how it can't do the",
    "start": "273230",
    "end": "279140"
  },
  {
    "text": "routing actually we have in the service layer you have a publishing map to know",
    "start": "279140",
    "end": "285710"
  },
  {
    "text": "the layout layout of the storage layer but how do you need to know this layout we in the left side we call our",
    "start": "285710",
    "end": "292550"
  },
  {
    "text": "situation layer we have a coordinator this coordinated surface layer and a storage layer the coordinator active",
    "start": "292550",
    "end": "299870"
  },
  {
    "text": "layer monitor all of the actually the main control all of the storage engines on the storage layer in a man Jesus",
    "start": "299870",
    "end": "306770"
  },
  {
    "text": "state and main since eternity and Intel has his own when the datastore distorted they stayed meanwhile the coordinator",
    "start": "306770",
    "end": "313669"
  },
  {
    "text": "also push this change to the service layer so that will make the service",
    "start": "313669",
    "end": "319310"
  },
  {
    "text": "layer can knowing our top notch layout the physical layout of a storage layer the answer in a kind wrote in the",
    "start": "319310",
    "end": "325640"
  },
  {
    "text": "request based on this topology map meanwhile we also have the modular",
    "start": "325640",
    "end": "331850"
  },
  {
    "text": "system because sometimes for the distorted ending it may have some slowdown has some other problems so this",
    "start": "331850",
    "end": "339200"
  },
  {
    "text": "modular system also a monitor actively monitored its database engines in a storage layer and it just send some",
    "start": "339200",
    "end": "345710"
  },
  {
    "text": "signal in case something is a failure to the coordinator and the coordinating take some action so now what I have",
    "start": "345710",
    "end": "351380"
  },
  {
    "text": "talking about it says gesture for TE aute III database and how it relates to",
    "start": "351380",
    "end": "356540"
  },
  {
    "text": "the kubernetes so actually we deploy our whole things include the storage layer",
    "start": "356540",
    "end": "363200"
  },
  {
    "text": "and also our insufficient layer on the kubernetes so on the orchestration layer and a service layer actually it's a",
    "start": "363200",
    "end": "369500"
  },
  {
    "text": "template and also as Diddley's so it's not a big program when we deploy on kubernetes the cool part is how we",
    "start": "369500",
    "end": "376880"
  },
  {
    "text": "mention these storage things on the kubernetes we know cognates is great but is it easy to deploy such a complex",
    "start": "376880",
    "end": "384470"
  },
  {
    "text": "little cloud our communities so answer is is not easy so now i'm going to talk about when we",
    "start": "384470",
    "end": "391970"
  },
  {
    "start": "389000",
    "end": "389000"
  },
  {
    "text": "implemented such a database we found our challenges in channeling",
    "start": "391970",
    "end": "397090"
  },
  {
    "text": "when we deploy our kubernetes so the first one is that we are talk about a",
    "start": "397090",
    "end": "402300"
  },
  {
    "text": "distributed database we have a geo distribution so typically we would deploy these Saints on multiple clusters",
    "start": "402300",
    "end": "409560"
  },
  {
    "text": "typically kubernetes suggests our local availability zoom or local data center",
    "start": "409560",
    "end": "414820"
  },
  {
    "text": "it's just one cluster so first we press each problems we have to manage multiple",
    "start": "414820",
    "end": "420010"
  },
  {
    "text": "cogniser clusters we need to distribute our database engines are multiple clusters so once we deploy on the",
    "start": "420010",
    "end": "426820"
  },
  {
    "text": "multiple classes we know kubernetes raises their own internal network inside it you can have a good connectivity but",
    "start": "426820",
    "end": "434940"
  },
  {
    "text": "if you curse multiple clusters like I have a database that has a three copies",
    "start": "434940",
    "end": "441220"
  },
  {
    "text": "on two data centers you know won't want to do some real time reputation so that will have a second issue is how we",
    "start": "441220",
    "end": "448200"
  },
  {
    "text": "enable the network connectivity between the database engines about different classes so their second problem of the",
    "start": "448200",
    "end": "457270"
  },
  {
    "text": "network aside since we are talking about what I want to do a high performance a cost-effective data platform so but",
    "start": "457270",
    "end": "465250"
  },
  {
    "text": "kubernetes by default if we want to persist in the data we use the persister",
    "start": "465250",
    "end": "471130"
  },
  {
    "text": "volume a possible volume typically is shared remotest origin but if we want to Intel",
    "start": "471130",
    "end": "478120"
  },
  {
    "text": "high performance cost-effective listen we cannot has fulfill our requirement because the",
    "start": "478120",
    "end": "484270"
  },
  {
    "text": "remoter stipulates that means to slow it and maybe low IOPS and the share that means",
    "start": "484270",
    "end": "490150"
  },
  {
    "text": "that sometimes people can sum the process running our same storage it will",
    "start": "490150",
    "end": "495370"
  },
  {
    "text": "impact these hazard and the next one is we purchase the data and we want to view",
    "start": "495370",
    "end": "502270"
  },
  {
    "text": "data we want to control the data placement because we are running our database and we don't want to seem copy",
    "start": "502270",
    "end": "508720"
  },
  {
    "text": "running our same machines so well we are risk so actually we want to know that data",
    "start": "508720",
    "end": "514690"
  },
  {
    "text": "location we want to control the data placement in this analysis we want to do some of my optimization for example when",
    "start": "514690",
    "end": "521800"
  },
  {
    "text": "I wrote in the the request from our service layer though to our storage layer we want to",
    "start": "521800",
    "end": "527499"
  },
  {
    "text": "choose some closer data node then can minimize the latency so the database is",
    "start": "527499",
    "end": "534129"
  },
  {
    "text": "more than like a pet it's a very complex pet we need a more control of all things",
    "start": "534129",
    "end": "539610"
  },
  {
    "text": "next why is that in the database we mention the data it are not just likely",
    "start": "539610",
    "end": "545199"
  },
  {
    "text": "or deployment you have a police car you create another new product to edit with",
    "start": "545199",
    "end": "550600"
  },
  {
    "text": "the deployment we are we not only need a creator port we also need a move the",
    "start": "550600",
    "end": "557619"
  },
  {
    "text": "data from somewhere to restore the data so the changes we need some data lifecycle management either include",
    "start": "557619",
    "end": "563980"
  },
  {
    "text": "letter dated back harbour date a movement that between few database engines so this is the main challenges",
    "start": "563980",
    "end": "570910"
  },
  {
    "text": "we faced when we try to deploy such a workload other communities so now let's",
    "start": "570910",
    "end": "578170"
  },
  {
    "text": "look at how we reserved this issue so ie",
    "start": "578170",
    "end": "584920"
  },
  {
    "start": "580000",
    "end": "580000"
  },
  {
    "text": "base actually we have a little bit other words that would render community so I",
    "start": "584920",
    "end": "592139"
  },
  {
    "text": "just saw the seeing light the container network interface so this maybe can also",
    "start": "592139",
    "end": "599399"
  },
  {
    "text": "support the community we were dealing in eBay so you eBay we assign it supporter",
    "start": "599399",
    "end": "605619"
  },
  {
    "text": "as a global IP so that will makers can different that cognates cluster the",
    "start": "605619",
    "end": "613660"
  },
  {
    "text": "product of the network activity so we have a controller who my schedule pod it",
    "start": "613660",
    "end": "619839"
  },
  {
    "text": "will assign a globe ID and this color by P is global wrote a book between the",
    "start": "619839",
    "end": "625600"
  },
  {
    "text": "different clusters so so this will reserve the network connectivity issues",
    "start": "625600",
    "end": "633069"
  },
  {
    "text": "and the next we found if we want to use a very high performance rather high IOPS",
    "start": "633069",
    "end": "640989"
  },
  {
    "text": "and also very dedicated volumes we found",
    "start": "640989",
    "end": "646779"
  },
  {
    "text": "that only the local disk or the disk adjuster may be on same rep can be",
    "start": "646779",
    "end": "652509"
  },
  {
    "text": "mounted like icicle see these local scenes we can achieve this high our peers and",
    "start": "652509",
    "end": "658670"
  },
  {
    "text": "are also dedicated without any impact for other neighbors so this by",
    "start": "658670",
    "end": "666950"
  },
  {
    "text": "collaboration with our ebay kubernetes to the teams we utilize the local theater as a positional volume that",
    "start": "666950",
    "end": "673940"
  },
  {
    "text": "means we can request the low position volume and but actually it had returned",
    "start": "673940",
    "end": "680000"
  },
  {
    "text": "a local district to this port and the meanwhile our when we registers is",
    "start": "680000",
    "end": "685880"
  },
  {
    "text": "persistent volume as local disk we put our physical location info like the rec",
    "start": "685880",
    "end": "692240"
  },
  {
    "text": "and a host inside as the labels for the local position volumes then can allow us",
    "start": "692240",
    "end": "698209"
  },
  {
    "text": "in the application layer we know the location of this disk and to know which",
    "start": "698209",
    "end": "705290"
  },
  {
    "text": "a record is and which hosts Livi's mean why we enhance the actually is also done",
    "start": "705290",
    "end": "710750"
  },
  {
    "text": "by our kubernetes team the enhance to the just part of any affinity features",
    "start": "710750",
    "end": "717139"
  },
  {
    "text": "from the water level to the volume level",
    "start": "717139",
    "end": "722240"
  },
  {
    "text": "because when we schedule some things we were maybe first I need a system volume then I schedule pod so we have some",
    "start": "722240",
    "end": "729560"
  },
  {
    "text": "special implementation can also support a volume level any affinity that means I",
    "start": "729560",
    "end": "735170"
  },
  {
    "text": "for force in shadow data I don't want to this shard the two positional volumes",
    "start": "735170",
    "end": "741589"
  },
  {
    "text": "not eating as simple or similar so this will be cos can get to the high FPS with",
    "start": "741589",
    "end": "750140"
  },
  {
    "text": "mega or database high performance also we can support the entry affinity this is location in sense a next one is we",
    "start": "750140",
    "end": "758660"
  },
  {
    "text": "found over to the database database always need as a backup if we don't have a bad habit you know somebody's broken",
    "start": "758660",
    "end": "764930"
  },
  {
    "text": "even we have a multiple copy but we don't have like the time machine capability to know the history of the",
    "start": "764930",
    "end": "771050"
  },
  {
    "text": "data so that it back up is so they're quite important for us we have a",
    "start": "771050",
    "end": "776240"
  },
  {
    "text": "solution to to utilize the local disk this just like a device on the device",
    "start": "776240",
    "end": "783260"
  },
  {
    "text": "level would do the snapshot and the keeper sending the snap Delta to to sing credit to a network of volume",
    "start": "783260",
    "end": "790560"
  },
  {
    "text": "and use this you can listen to for our backup they said look at ISKCON castilla",
    "start": "790560",
    "end": "796230"
  },
  {
    "text": "walk with the higher-ups and then you just keep it doing the regular deltas",
    "start": "796230",
    "end": "803040"
  },
  {
    "text": "and the remote volume because if we just use the remote shared storage it you can maybe do the snapshot and the remotest",
    "start": "803040",
    "end": "809100"
  },
  {
    "text": "share story but since we now we use a look at this so we have a addition these features to enable this backup scenes",
    "start": "809100",
    "end": "817100"
  },
  {
    "text": "and now this is just a infrastructure level we reserved the networking issue",
    "start": "817100",
    "end": "822390"
  },
  {
    "text": "and also reserved the discrete issues but we found it after we utilize this",
    "start": "822390",
    "end": "828000"
  },
  {
    "text": "feature we are not only we cannot use like a deployment or silver set because",
    "start": "828000",
    "end": "833700"
  },
  {
    "text": "this just means that kubernetes can control your total lifecycle now I cannot utilize because we have",
    "start": "833700",
    "end": "841050"
  },
  {
    "text": "rare special things like whiny the controls and look at the disk and the code must be together you cannot utilize these features so we",
    "start": "841050",
    "end": "848460"
  },
  {
    "text": "so then we choose a solution we could just use the pod we user communities to",
    "start": "848460",
    "end": "854130"
  },
  {
    "text": "just imagine these containers and the other a plain we created our own which is adjacent layer to maintain the wispy",
    "start": "854130",
    "end": "860940"
  },
  {
    "text": "state so that is our institution layer I mentioned in previous slides just we",
    "start": "860940",
    "end": "866490"
  },
  {
    "text": "maintain our whiskey state so but how we can maintain the whiskey's data so first",
    "start": "866490",
    "end": "871950"
  },
  {
    "text": "we have our own configure store to store our desired state we also actively",
    "start": "871950",
    "end": "878430"
  },
  {
    "text": "monitor all of the physical resources in the kubernetes then we compare the difference to to",
    "start": "878430",
    "end": "886080"
  },
  {
    "text": "take some action built like a loop let's we watch is a cop needs change and they take some action to fix to make the",
    "start": "886080",
    "end": "892710"
  },
  {
    "text": "match our desired state but this is different like cognitivist always have",
    "start": "892710",
    "end": "898380"
  },
  {
    "text": "even more like a first multiple data center because of multiple calmness clusters and the mechanism is being our",
    "start": "898380",
    "end": "904560"
  },
  {
    "text": "own why our own data store also we utilize us but since we call the",
    "start": "904560",
    "end": "909930"
  },
  {
    "text": "workflow to maintain the wispy because like I said before we have of you like a",
    "start": "909930",
    "end": "916440"
  },
  {
    "text": "database data move plate operation are also cognizant oppression these are together sometimes I I do a create a new pod I",
    "start": "916440",
    "end": "924550"
  },
  {
    "text": "need to move some data so not just like a real simple operation it has a few",
    "start": "924550",
    "end": "930520"
  },
  {
    "text": "complex steps so we utilize the workflow we can listen to implement these complex",
    "start": "930520",
    "end": "936070"
  },
  {
    "text": "things and the methane is a Swiss B state now here is a example for how I do",
    "start": "936070",
    "end": "941470"
  },
  {
    "start": "940000",
    "end": "940000"
  },
  {
    "text": "the self remediation for this example I have a replica this is maybe the host",
    "start": "941470",
    "end": "950920"
  },
  {
    "text": "Daystar kubernetes deleted this replica this is in red and we are watching we're",
    "start": "950920",
    "end": "957490"
  },
  {
    "text": "watching this event who appear to be deleted at this event will be sent to our coordinator and our coordinator will",
    "start": "957490",
    "end": "963430"
  },
  {
    "text": "actually will have to level called coordinators each do have its own local one this will avoid that one didn't die",
    "start": "963430",
    "end": "970960"
  },
  {
    "text": "yet others but the we have a logically global one actually it also contributed our multiple data centers so this local",
    "start": "970960",
    "end": "977050"
  },
  {
    "text": "before the event to the caliber one and the global one will make us a position and if your triggers are work triggers",
    "start": "977050",
    "end": "982870"
  },
  {
    "text": "of whiskey workflow the whiskey of workflow we found okay I have one replication missing I just send us a",
    "start": "982870",
    "end": "988810"
  },
  {
    "text": "mess even center of code to the local community to create a new replica editor this sharp so this is quite similar like",
    "start": "988810",
    "end": "997510"
  },
  {
    "text": "a hug networks but we just expanded around multiple communities of clusters",
    "start": "997510",
    "end": "1003590"
  },
  {
    "text": "so here's all my talk actually I just to share our base to the geo database and",
    "start": "1003590",
    "end": "1011430"
  },
  {
    "text": "how we the changed were facing when we move on communities and how we serve these challenges and I know the core",
    "start": "1011430",
    "end": "1020640"
  },
  {
    "text": "part of our solution actually is based on eBay kubernetes 30 management solution and I also think about this",
    "start": "1020640",
    "end": "1027360"
  },
  {
    "text": "pattern is quite a typical maybe it can be extended to as a higher appears G ot",
    "start": "1027360",
    "end": "1032699"
  },
  {
    "text": "printers that were workload so that's all my heart ever forwarded to my colleague agenda for next part",
    "start": "1032700",
    "end": "1040819"
  },
  {
    "text": "okay thank Seema my name is changa i'm from ebay commnity",
    "start": "1045020",
    "end": "1050700"
  },
  {
    "text": "my walking tour me is the own OS management and water management on the",
    "start": "1050700",
    "end": "1055950"
  },
  {
    "text": "ebay kubernetes cluster next I'll give introduction of storage management on",
    "start": "1055950",
    "end": "1062820"
  },
  {
    "text": "eBay kubernetes cluster it's a support to geo distributed database it can also",
    "start": "1062820",
    "end": "1068669"
  },
  {
    "text": "support or at the eBay application as well and here is the agenda of my topics",
    "start": "1068669",
    "end": "1075450"
  },
  {
    "text": "here first I will introduce our storage classes next I will introduce our local",
    "start": "1075450",
    "end": "1082620"
  },
  {
    "text": "volume solution network Blackwell morning routine backup solution lastly I'll give a simple summary of our",
    "start": "1082620",
    "end": "1090270"
  },
  {
    "text": "future work okay storage classes in our",
    "start": "1090270",
    "end": "1096299"
  },
  {
    "start": "1093000",
    "end": "1093000"
  },
  {
    "text": "cube next cluster which you find to start class first day is the performance it can provide 50k LPS and also maximum",
    "start": "1096299",
    "end": "1105299"
  },
  {
    "text": "500 megabyte per second throughput the pic of typical user case is an Oracle",
    "start": "1105299",
    "end": "1111360"
  },
  {
    "text": "database or some other eBay in-house application the storage back-end is local SSD on kubernetes it's our local",
    "start": "1111360",
    "end": "1120059"
  },
  {
    "text": "SSD volume ii starch class is a standard it can provide the 300 PS or maximum 300",
    "start": "1120059",
    "end": "1127880"
  },
  {
    "text": "megabytes per second throughput and the user keys like backup service or EBC ICD",
    "start": "1127880",
    "end": "1135900"
  },
  {
    "text": "that Cassie can show gonna introduce the yesterday's session the storage back-end",
    "start": "1135900",
    "end": "1141210"
  },
  {
    "text": "is cf HDD based trust income dentist it's ours seen the volume okay then go",
    "start": "1141210",
    "end": "1152549"
  },
  {
    "start": "1151000",
    "end": "1151000"
  },
  {
    "text": "to the local volume we started the our local worm work last year at that time",
    "start": "1152549",
    "end": "1159320"
  },
  {
    "text": "there's a quite many network volumes Roshan income net yes but no local volume we compare the network valium",
    "start": "1159320",
    "end": "1167040"
  },
  {
    "text": "we found that we still have quite many strong reason that we must increment a local volume first days the performers",
    "start": "1167040",
    "end": "1174710"
  },
  {
    "text": "actually I think is a quite common sense that local SSD performance must be better than network morning I think",
    "start": "1174710",
    "end": "1182240"
  },
  {
    "text": "especially for the i/o latency we ever get attached to that if we write a 4 KB",
    "start": "1182240",
    "end": "1187670"
  },
  {
    "text": "block on local SSD it's take about 200 nanoseconds why if we write it on the",
    "start": "1187670",
    "end": "1195430"
  },
  {
    "text": "cinder as Casa volume it's take about one thousand eight hundred nine nanoseconds so so performers local SSD",
    "start": "1195430",
    "end": "1205070"
  },
  {
    "text": "is much better second is of course we all know that commercial hypotenuse",
    "start": "1205070",
    "end": "1211220"
  },
  {
    "text": "metro network storage is very costly so the reason is the availability I think",
    "start": "1211220",
    "end": "1218390"
  },
  {
    "text": "we most of us will think that one local SSD reliability is not good and compared",
    "start": "1218390",
    "end": "1225650"
  },
  {
    "text": "with network volume its availability is not good as well but mmm you know but",
    "start": "1225650",
    "end": "1233120"
  },
  {
    "text": "actually I think nowadays most distributed database actually management and the data ability by their own they",
    "start": "1233120",
    "end": "1240680"
  },
  {
    "text": "usually have several copies on different nodes on different disk axle that means the data availability is guaranteed by",
    "start": "1240680",
    "end": "1248840"
  },
  {
    "text": "the application level so single SSDs reliability is not a big issue now also",
    "start": "1248840",
    "end": "1257930"
  },
  {
    "text": "at that time kinetise only provide the empty dia and the host pass as the wait",
    "start": "1257930",
    "end": "1263990"
  },
  {
    "text": "for pod to access local disk but this to",
    "start": "1263990",
    "end": "1269600"
  },
  {
    "text": "Varner also has quite obvious limitation so first they don't support TB and PVC",
    "start": "1269600",
    "end": "1276040"
  },
  {
    "text": "second they cannot guarantee they are your size or IOPS for instance if a host",
    "start": "1276040",
    "end": "1282950"
  },
  {
    "text": "paths share the same disk with the root OFS the pod maybe can use all the disk",
    "start": "1282950",
    "end": "1288770"
  },
  {
    "text": "space on the root disk root root as FS so as at this reason we start to",
    "start": "1288770",
    "end": "1297770"
  },
  {
    "text": "implement our own local volume measuring we'd do these three changes in",
    "start": "1297770",
    "end": "1304040"
  },
  {
    "text": "kubernetes code to implement our local volume the first party is the local bottom plug in we",
    "start": "1304040",
    "end": "1311049"
  },
  {
    "text": "implemented this program to support hard disk and hard disk partition this local",
    "start": "1311049",
    "end": "1318070"
  },
  {
    "text": "volume plugging is similar as all are the kubernetes volume providing we",
    "start": "1318070",
    "end": "1324460"
  },
  {
    "text": "implemented their major local major volume in interfaces like a mount or a",
    "start": "1324460",
    "end": "1330309"
  },
  {
    "text": "mount or provisional and deleter and so we readjusted this worm plugging in the",
    "start": "1330309",
    "end": "1336879"
  },
  {
    "text": "cubelet and the cube controller so that it works as similar as there are other",
    "start": "1336879",
    "end": "1342029"
  },
  {
    "text": "volume plugging in the kubernetes second thing we implement pv and PVC so user",
    "start": "1342029",
    "end": "1351220"
  },
  {
    "text": "can just create one PVC for the local volume and using this PVC in their poor",
    "start": "1351220",
    "end": "1357669"
  },
  {
    "text": "spec the same as the all other PVC for the TV crew issuing country with sabonis",
    "start": "1357669",
    "end": "1365139"
  },
  {
    "text": "approaches that static of people creation so during the node provision we were generator can take a fire in this",
    "start": "1365139",
    "end": "1372399"
  },
  {
    "text": "configure fire we will indicate which partition or which disc can be used as",
    "start": "1372399",
    "end": "1378100"
  },
  {
    "text": "the local volume so when cube litter starts it will read this contradiction file and it will create the PV instance",
    "start": "1378100",
    "end": "1387129"
  },
  {
    "text": "in the EC CD and also mounted ideas from the local on the host node just like",
    "start": "1387129",
    "end": "1393429"
  },
  {
    "text": "this is to disk so we can mount them",
    "start": "1393429",
    "end": "1398679"
  },
  {
    "text": "first on the host and when the products start this to disk can be mounted in the",
    "start": "1398679",
    "end": "1404980"
  },
  {
    "text": "Pradas namespace so that this part can access the disk directory the last part",
    "start": "1404980",
    "end": "1412240"
  },
  {
    "text": "is changes in the scheduler the reason",
    "start": "1412240",
    "end": "1418149"
  },
  {
    "text": "is that local volume PV and PVC bonding is special it's not seen as other",
    "start": "1418149",
    "end": "1425919"
  },
  {
    "text": "Network one on TV and PVC bounding the reason is that when whenever local",
    "start": "1425919",
    "end": "1432639"
  },
  {
    "text": "volunteer and PVC abounded it means that the node is also selected this will",
    "start": "1432639",
    "end": "1437649"
  },
  {
    "text": "break the later pod scheduling policies so so the in this reason we postpone the",
    "start": "1437649",
    "end": "1444980"
  },
  {
    "text": "PV and PVC bonding in the scheduler instead of in the PV controller with",
    "start": "1444980",
    "end": "1451520"
  },
  {
    "text": "your two changes first day is that we add a local volume predict function so",
    "start": "1451520",
    "end": "1457010"
  },
  {
    "text": "in this function we would check her every node it's if it can fulfill the requirements of Pradas local volume",
    "start": "1457010",
    "end": "1463880"
  },
  {
    "text": "requests and secondly is that the way in the kubernetes scheduler one function way to the final a PV and PVC bounding",
    "start": "1463880",
    "end": "1471500"
  },
  {
    "text": "just behind the port 1 into the node so all this reach and just incoming edges",
    "start": "1471500",
    "end": "1479660"
  },
  {
    "text": "for us to implement a local volume here is an example that how can we use in the",
    "start": "1479660",
    "end": "1487790"
  },
  {
    "text": "local volume PVC in our production so we justify our local bottom PVC I think at",
    "start": "1487790",
    "end": "1494180"
  },
  {
    "text": "the storage cross as local SSD it's similar as the other PVC and in the",
    "start": "1494180",
    "end": "1500120"
  },
  {
    "text": "parts back we just putted a PV name a PVC name there so that's it here several",
    "start": "1500120",
    "end": "1510220"
  },
  {
    "text": "parts that we ever considered during our design and the implementation of local",
    "start": "1510220",
    "end": "1515330"
  },
  {
    "text": "volume first things is still wear true to the TV Bundy I've mentioned before if",
    "start": "1515330",
    "end": "1521540"
  },
  {
    "text": "we need to do the PV and PVC bounding in the pot scheduler or - in the PV",
    "start": "1521540",
    "end": "1527600"
  },
  {
    "text": "controller as mentioned before that if we do it in the PV controller there and",
    "start": "1527600",
    "end": "1534800"
  },
  {
    "text": "we will predict a lot of pods in scheduling policy like our customer geo",
    "start": "1534800",
    "end": "1540640"
  },
  {
    "text": "distributed base they require the anti affinity feature and also they define",
    "start": "1540640",
    "end": "1546320"
  },
  {
    "text": "the pod memory and the CPU resources this kind of scheduling requests so we",
    "start": "1546320",
    "end": "1552920"
  },
  {
    "text": "must fulfill these requirements so this make us postpone the PV and PVC to the",
    "start": "1552920",
    "end": "1559070"
  },
  {
    "text": "scheduler another thing is that some features some PD features will be",
    "start": "1559070",
    "end": "1564350"
  },
  {
    "text": "implementation will be more complex if we put this PV and PVC in the PV controller and just like if one power",
    "start": "1564350",
    "end": "1571610"
  },
  {
    "text": "requires two local volume PVC so if we do it in the TV controller now",
    "start": "1571610",
    "end": "1577840"
  },
  {
    "text": "it's possible that one PVC bun - no day another PVC bun - no to be there how can",
    "start": "1577840",
    "end": "1584769"
  },
  {
    "text": "this part of be scheduled so as this kind of reason so we decide to put this PV bonding in the scheduler",
    "start": "1584769",
    "end": "1593230"
  },
  {
    "text": "and the second point is that how do we recycle the local one so when the PVC is",
    "start": "1593230",
    "end": "1600539"
  },
  {
    "text": "dedicated we also must delete the data on the TV this actually wait we at the",
    "start": "1600539",
    "end": "1611080"
  },
  {
    "text": "very beginning we just start using the recycle pathway so whenever we delete the PVC we start a local startup odd and",
    "start": "1611080",
    "end": "1618580"
  },
  {
    "text": "schedule this part to this node so using this part to remove all the data on the",
    "start": "1618580",
    "end": "1624809"
  },
  {
    "text": "specified PV at the very beginning we are worried about its scalability but we",
    "start": "1624809",
    "end": "1631809"
  },
  {
    "text": "tested to 2 liters 3000 PVC at one time and we found that this solution can",
    "start": "1631809",
    "end": "1638679"
  },
  {
    "text": "still work well so we keep it in our production still and the third point is",
    "start": "1638679",
    "end": "1644769"
  },
  {
    "text": "about disco fader monetary and the remediation we know that disc actually",
    "start": "1644769",
    "end": "1651429"
  },
  {
    "text": "is the most unreliable device in in the machine compared with the memory asipi",
    "start": "1651429",
    "end": "1657700"
  },
  {
    "text": "own network interface so it's failure rate is high so currently our solutions",
    "start": "1657700",
    "end": "1663519"
  },
  {
    "text": "that we start a team instead pot and called the reader can yes some real",
    "start": "1663519",
    "end": "1669100"
  },
  {
    "text": "controllers CTL like maxi eye or smart city L these tools to detect a disco",
    "start": "1669100",
    "end": "1675429"
  },
  {
    "text": "failure and whenever we detect the face disco vidya and we send send out alerts",
    "start": "1675429",
    "end": "1681779"
  },
  {
    "text": "for the remediation part country we we can still manual we can only still to the manual remediation and this may be",
    "start": "1681779",
    "end": "1688659"
  },
  {
    "text": "will be improved later ok then we will",
    "start": "1688659",
    "end": "1694960"
  },
  {
    "start": "1693000",
    "end": "1693000"
  },
  {
    "text": "go to the network volume I think networking room still plays a very important role in our communities",
    "start": "1694960",
    "end": "1702220"
  },
  {
    "text": "cluster one of the most important feature for the net one is that it",
    "start": "1702220",
    "end": "1708190"
  },
  {
    "text": "attached to a note so but whenever the porter is the delineator or crushed so",
    "start": "1708190",
    "end": "1716889"
  },
  {
    "text": "if he also want to start a new pod or another note the distant one can still be reattach to another node and the",
    "start": "1716889",
    "end": "1723610"
  },
  {
    "text": "podcast still using this same network volume this is a one of the most",
    "start": "1723610",
    "end": "1729929"
  },
  {
    "text": "important feature of network in kubernetes second thing is that I think when when users are using the network",
    "start": "1729929",
    "end": "1736809"
  },
  {
    "text": "volume the the suppose the auto data availability has been guaranteed by the",
    "start": "1736809",
    "end": "1742269"
  },
  {
    "text": "back end of net one oh right like safe so they don't need a care too much about",
    "start": "1742269",
    "end": "1748259"
  },
  {
    "text": "their application how to guarantee the data availability and we'll make use a",
    "start": "1748259",
    "end": "1754690"
  },
  {
    "text": "much thing much easier so this is a reason that we we still need to keep the",
    "start": "1754690",
    "end": "1760629"
  },
  {
    "text": "network volume in our production environment we using the single volume",
    "start": "1760629",
    "end": "1765970"
  },
  {
    "text": "as our neck that will warn a solution at we're beginning we're using the standard open stacks in the volume but after one",
    "start": "1765970",
    "end": "1774759"
  },
  {
    "text": "year's use experience we found that it has some problem first is that it has",
    "start": "1774759",
    "end": "1782259"
  },
  {
    "text": "too many depend depends on third party components second is that it cannot",
    "start": "1782259",
    "end": "1788679"
  },
  {
    "text": "support a pair mental so as this reason we switch to using stand-alones in the",
    "start": "1788679",
    "end": "1794529"
  },
  {
    "text": "provisional from from kubernetes community okay so this figure shows two",
    "start": "1794529",
    "end": "1804250"
  },
  {
    "text": "different resolutions the left side we can see that this is the standard cinder",
    "start": "1804250",
    "end": "1809769"
  },
  {
    "text": "price Nova solution this is a kubernetes node it's a VM we can see how many",
    "start": "1809769",
    "end": "1817779"
  },
  {
    "text": "components depends why it's one to do the volume attached attached you can see",
    "start": "1817779",
    "end": "1822820"
  },
  {
    "text": "he has a queue novel computer rabbitmq novel and cinder",
    "start": "1822820",
    "end": "1828250"
  },
  {
    "text": "so in furnishing so every two of these components if they have some data",
    "start": "1828250",
    "end": "1834370"
  },
  {
    "text": "drift then will cause our bottom attach detach failure its occurs quite offering",
    "start": "1834370",
    "end": "1840940"
  },
  {
    "text": "our promotion that's the reason we switch to stand-alones in the provisioner so by",
    "start": "1840940",
    "end": "1847480"
  },
  {
    "text": "Odin cinders stand up along standalone cinder provisioner we can still that",
    "start": "1847480",
    "end": "1854830"
  },
  {
    "text": "user to use in the sink the PVC interface but when user create a standard PVC interface actually the",
    "start": "1854830",
    "end": "1863590"
  },
  {
    "text": "standalone parishioner can help you create a PV or ask a C TV in the",
    "start": "1863590",
    "end": "1869580"
  },
  {
    "text": "commonest side so it has much less third-party dependency and it can",
    "start": "1869580",
    "end": "1876790"
  },
  {
    "text": "improve network volumes attach detach reliability okay then we go to the",
    "start": "1876790",
    "end": "1886240"
  },
  {
    "start": "1884000",
    "end": "1884000"
  },
  {
    "text": "backup solution currently our perhaps ruching is based on scene the cinder",
    "start": "1886240",
    "end": "1893950"
  },
  {
    "text": "volume our storage team safety has already implemented a snapshot",
    "start": "1893950",
    "end": "1900190"
  },
  {
    "text": "controller so like here they have running a snapshot controller for each",
    "start": "1900190",
    "end": "1906360"
  },
  {
    "text": "single cluster so so why use a crater a port and they create a PVC and if this",
    "start": "1906360",
    "end": "1914200"
  },
  {
    "text": "PVC is net volume and then this net one is a cinder cinder volume so the",
    "start": "1914200",
    "end": "1920559"
  },
  {
    "text": "snapshot so if they want to do the snapshot we can send a request to the snapshot controller this measure",
    "start": "1920559",
    "end": "1927190"
  },
  {
    "text": "controller can trigger the cinder to do the snapshot and later when a client if",
    "start": "1927190",
    "end": "1932830"
  },
  {
    "text": "want to recover the data from the snapshot they can send us requests to",
    "start": "1932830",
    "end": "1938620"
  },
  {
    "text": "the snapshot controller to and the snare shops controller can make warf snapshot",
    "start": "1938620",
    "end": "1944740"
  },
  {
    "text": "via volume in the volume and return the volume ID to the controller the",
    "start": "1944740",
    "end": "1949870"
  },
  {
    "text": "controller then returns the user ID to the snapshot right so when user gets",
    "start": "1949870",
    "end": "1955420"
  },
  {
    "text": "this volume your ID it can create another pod by",
    "start": "1955420",
    "end": "1960940"
  },
  {
    "text": "specify these volumes your ID in the PVC and create another cinder volume PB and this new pod can",
    "start": "1960940",
    "end": "1968920"
  },
  {
    "text": "use this recovery date this is our backup solution",
    "start": "1968920",
    "end": "1974750"
  },
  {
    "text": "in kubernetes okay",
    "start": "1974750",
    "end": "1980030"
  },
  {
    "start": "1979000",
    "end": "1979000"
  },
  {
    "text": "lastly I will give a simple introduction of our future work first days that we",
    "start": "1980030",
    "end": "1988700"
  },
  {
    "text": "need to align our code with our stream of local volume especially tourist cargo",
    "start": "1988700",
    "end": "1994370"
  },
  {
    "text": "up stream has merged departure aware bottom schedule of Pia so I think most",
    "start": "1994370",
    "end": "2001000"
  },
  {
    "text": "of our required feature of local volume has been upstream so later we need to do",
    "start": "2001000",
    "end": "2006100"
  },
  {
    "text": "some work to align our implementation with up streams to implementation and the second part that we needed to do",
    "start": "2006100",
    "end": "2013810"
  },
  {
    "text": "some disk and partition modeling during our not provision so so that the purpose",
    "start": "2013810",
    "end": "2020200"
  },
  {
    "text": "is that we can support multiple hardware SKU in our production during the node",
    "start": "2020200",
    "end": "2029020"
  },
  {
    "text": "provision so that we can during the provision we can using this module to specify which disk partition can be used",
    "start": "2029020",
    "end": "2036130"
  },
  {
    "text": "in the local volume it will be more generic a solution the third point is",
    "start": "2036130",
    "end": "2041950"
  },
  {
    "text": "that we will consider the volume for the local disk we want to take advantage of",
    "start": "2041950",
    "end": "2049780"
  },
  {
    "text": "the powerful PI system ZFS and some of the feature of DFS maybe will help for",
    "start": "2049780",
    "end": "2055510"
  },
  {
    "text": "us to enhance that for local volume something like snapshot this kind of",
    "start": "2055510",
    "end": "2061389"
  },
  {
    "text": "feature ok so this is all for for my introduction thank you",
    "start": "2061390",
    "end": "2068158"
  },
  {
    "text": "[Applause]",
    "start": "2068159",
    "end": "2073878"
  }
]