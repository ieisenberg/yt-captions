[
  {
    "start": "0",
    "end": "52000"
  },
  {
    "text": "okay good afternoon and welcome everybody uh in person and online um thank you for joining us for trimaran",
    "start": "399",
    "end": "6480"
  },
  {
    "text": "real load we're scheduling in kubernetes um would you all join me in welcoming um abdul kadir from paypal and chen wang",
    "start": "6480",
    "end": "13599"
  },
  {
    "text": "from ibm [Applause]",
    "start": "13599",
    "end": "20720"
  },
  {
    "text": "uh good afternoon everyone those present here and greetings to my uh my greetings to those virtual present virtually",
    "start": "20720",
    "end": "27760"
  },
  {
    "text": "so i'm abdul khadir i i'm a senior software engineer at paypal i'll be presenting on load aware",
    "start": "27760",
    "end": "33440"
  },
  {
    "text": "scheduling with my collaborator chen wang who is a research scientist at ibm",
    "start": "33440",
    "end": "39280"
  },
  {
    "text": "so the image you see here is of trimaran so what it stands for is like a three hulled boat which provides more",
    "start": "40000",
    "end": "46719"
  },
  {
    "text": "stability and safety in the ocean all right let's get started",
    "start": "46719",
    "end": "53120"
  },
  {
    "start": "52000",
    "end": "94000"
  },
  {
    "text": "so this is the agenda for today uh we'll go over the motivation the background",
    "start": "53120",
    "end": "59120"
  },
  {
    "text": "and the problem definition followed by the primer and architecture",
    "start": "59120",
    "end": "64320"
  },
  {
    "text": "the design and the plugins we contributed to the open source community the first one is the target load packing",
    "start": "64320",
    "end": "71200"
  },
  {
    "text": "and the second one the load variation risk balancing plug-in then we have a demo",
    "start": "71200",
    "end": "77119"
  },
  {
    "text": "and then some of the challenges timer and phases how we overcome them",
    "start": "77119",
    "end": "83040"
  },
  {
    "text": "followed by good practices in using primary in production",
    "start": "83040",
    "end": "88240"
  },
  {
    "text": "and some future work that we have to do so let's get started with further review",
    "start": "88240",
    "end": "95040"
  },
  {
    "start": "94000",
    "end": "193000"
  },
  {
    "text": "so as many of you may know kubernetes provides a declarative resource model",
    "start": "95200",
    "end": "100400"
  },
  {
    "text": "for its pods what i mean by declarative model is that the",
    "start": "100400",
    "end": "105759"
  },
  {
    "text": "resource usage that you define for your pod or the container needs to be",
    "start": "105759",
    "end": "111280"
  },
  {
    "text": "defined in a spec before you're able to run it and run your workload and the core components in kubernetes",
    "start": "111280",
    "end": "117280"
  },
  {
    "text": "namely the cubelet the scheduler uh they honor honor it to behave consistently",
    "start": "117280",
    "end": "122399"
  },
  {
    "text": "with respect to the quality of service guarantees now given this the developers they tend",
    "start": "122399",
    "end": "128479"
  },
  {
    "text": "to over provision the resources with this model uh which you know",
    "start": "128479",
    "end": "133840"
  },
  {
    "text": "one of the reasons is that they want to avoid the penalties of evictions and cpu throttling that the kubernetes would do",
    "start": "133840",
    "end": "140480"
  },
  {
    "text": "now one way to solve this problem of declarative uh model is to benchmark",
    "start": "140480",
    "end": "146480"
  },
  {
    "text": "your applications but we we know that like it could be comparison to do for all your production",
    "start": "146480",
    "end": "153360"
  },
  {
    "text": "applications and in general estimating real load is hard",
    "start": "153360",
    "end": "158560"
  },
  {
    "text": "now with that background the main problem we are trying to solve is that the default scheduler in kubernetes it",
    "start": "158560",
    "end": "164080"
  },
  {
    "text": "uses allocation based scheduling model and it can lead to",
    "start": "164080",
    "end": "169440"
  },
  {
    "text": "under provision nodes in terms of resources and fragmentation of course across the cluster",
    "start": "169440",
    "end": "175519"
  },
  {
    "text": "what you see on the right bottom right is a graph from 29 day google trace in their",
    "start": "175519",
    "end": "180959"
  },
  {
    "text": "cluster and you can observe here that the usage is about 40 of the requests",
    "start": "180959",
    "end": "188000"
  },
  {
    "text": "which implies a lot of it is over provisioned okay",
    "start": "188000",
    "end": "194319"
  },
  {
    "start": "193000",
    "end": "312000"
  },
  {
    "text": "now moving on to the primer and design and architecture so around the time we started working on",
    "start": "194319",
    "end": "200560"
  },
  {
    "text": "this problem the cube the scheduler provides kubernetes",
    "start": "200560",
    "end": "206640"
  },
  {
    "text": "scheduler framework it was in beta api and it was moving towards stable so we",
    "start": "206640",
    "end": "212560"
  },
  {
    "text": "decided to leverage it to contribute our plugins in now the framework it provides",
    "start": "212560",
    "end": "218720"
  },
  {
    "text": "flexibility to extend the scheduler in different apis",
    "start": "218720",
    "end": "224159"
  },
  {
    "text": "and uh trimaran plugins they do so in an extension point called scoring extension",
    "start": "224159",
    "end": "229360"
  },
  {
    "text": "point if there's an upcoming talk by uh six scheduling folks uh and you're more than welcome to",
    "start": "229360",
    "end": "236000"
  },
  {
    "text": "attend that if you want if you're interested in a scheduling deep life",
    "start": "236000",
    "end": "241200"
  },
  {
    "text": "now for a given scoring plugin uh what you have is like you get an input as a set of nodes and what you",
    "start": "241200",
    "end": "248000"
  },
  {
    "text": "output are different node scores for that for those nodes so that depends on the algorithm you",
    "start": "248000",
    "end": "255760"
  },
  {
    "text": "define in your scoring plugin so in the diagram you can see that the",
    "start": "255760",
    "end": "261919"
  },
  {
    "text": "timer and plugins are part of the kubernetes scheduler so they run in the same binary",
    "start": "261919",
    "end": "268080"
  },
  {
    "text": "now the next major component in our design is the load watcher so load washer can be defined as a cluster-wide",
    "start": "268960",
    "end": "275520"
  },
  {
    "text": "aggregator of metrics which is backed by a third-party metrics provider",
    "start": "275520",
    "end": "282479"
  },
  {
    "text": "we'll i'll explain in the next slide what all metric providers we support",
    "start": "282479",
    "end": "287600"
  },
  {
    "text": "the load watcher also maintains the cache in memory for fast lookups",
    "start": "287600",
    "end": "292720"
  },
  {
    "text": "whenever the plugins need to talk to the load watcher to get the live metrics",
    "start": "292720",
    "end": "300080"
  },
  {
    "text": "it also maintains its cache in in a db to provide some fault tolerance if there is a failure",
    "start": "300560",
    "end": "306880"
  },
  {
    "text": "and fast recovery during restarts uh more in the next slide about load",
    "start": "306880",
    "end": "313039"
  },
  {
    "start": "312000",
    "end": "458000"
  },
  {
    "text": "watcher so another way to think about load watcher is that it is sort of a wrapper which uh",
    "start": "313039",
    "end": "319520"
  },
  {
    "text": "unifies metrics from different providers into a common format that the tramadon plugins understand",
    "start": "319520",
    "end": "326000"
  },
  {
    "text": "and the currently supported ones which we added are prometheus signalfx by splunk",
    "start": "326000",
    "end": "332080"
  },
  {
    "text": "and the native kubernetes metrics provider it can be extended by the users to",
    "start": "332080",
    "end": "338160"
  },
  {
    "text": "support any other metrics provider if needed now from the diagrams you can see that there",
    "start": "338160",
    "end": "344400"
  },
  {
    "text": "are two options of using the load watcher so first one is that you can use it as a service",
    "start": "344400",
    "end": "350560"
  },
  {
    "text": "and then the second one is that as a library so each one of them has their pros and cons it's up to the users to decide what",
    "start": "350560",
    "end": "357440"
  },
  {
    "text": "works best for them for example in the case of using it as a",
    "start": "357440",
    "end": "362479"
  },
  {
    "text": "service you have a separation of failure component from the scheduler process",
    "start": "362479",
    "end": "368720"
  },
  {
    "text": "and the ap additional api call latency from contacting over the network",
    "start": "368720",
    "end": "374479"
  },
  {
    "text": "it can be minimized if you use like a local container through the scheduler pod",
    "start": "374479",
    "end": "379520"
  },
  {
    "text": "and the other point is that it scales separately from the kubernetes scheduler",
    "start": "379520",
    "end": "384720"
  },
  {
    "text": "so this is important for us because in the future we want to make it a bit more complex by adding machine",
    "start": "384720",
    "end": "390319"
  },
  {
    "text": "learning models which could make the washer process more cpu intensive and we",
    "start": "390319",
    "end": "395840"
  },
  {
    "text": "don't want it to compete with the scheduler for resources",
    "start": "395840",
    "end": "400000"
  },
  {
    "text": "what you see on the other hand if you use it as a library from the plugins inside",
    "start": "401120",
    "end": "407039"
  },
  {
    "text": "you have of course no api call over the network which is important when you have fast",
    "start": "407039",
    "end": "412560"
  },
  {
    "text": "scheduling cycles then you have a much simpler deployment",
    "start": "412560",
    "end": "417840"
  },
  {
    "text": "we at paypal use it as a service and red hat folks they they integrated uh trimaran in their product offering on",
    "start": "418720",
    "end": "425520"
  },
  {
    "text": "kubernetes openshift and they use library as in in their offering",
    "start": "425520",
    "end": "430720"
  },
  {
    "text": "what you see in the rightmost column is a sample data that the road watcher process maintains in the cache",
    "start": "430720",
    "end": "438080"
  },
  {
    "text": "so it maintains windows of different metrics that are needed by the plugins",
    "start": "438080",
    "end": "443280"
  },
  {
    "text": "and the window durations range from 15 minutes to five minutes and we use each one of them as needed",
    "start": "443280",
    "end": "449360"
  },
  {
    "text": "depending on the use case it also adds some more metadata to enrich the",
    "start": "449360",
    "end": "455360"
  },
  {
    "text": "water cache",
    "start": "455360",
    "end": "458000"
  },
  {
    "start": "458000",
    "end": "520000"
  },
  {
    "text": "coming to the meat of the design uh so for the first plugin that we contributed",
    "start": "462800",
    "end": "468479"
  },
  {
    "text": "uh there are two main objectives we had in mind when we designed the target load packing plugin the first one is that we",
    "start": "468479",
    "end": "474560"
  },
  {
    "text": "want to achieve high utilization across all the running nodes in the cluster and",
    "start": "474560",
    "end": "480000"
  },
  {
    "text": "basically we want it to be around a given threshold that we can set before we run the plug-in",
    "start": "480000",
    "end": "486720"
  },
  {
    "text": "and we do so by packing the pods as long as",
    "start": "486720",
    "end": "492800"
  },
  {
    "text": "the cpu utilization is under the given threshold the other objective is to maintain",
    "start": "492800",
    "end": "499599"
  },
  {
    "text": "some sort of safe margin for cpu usage spikes which can happen due to unpredictable loads",
    "start": "499599",
    "end": "506479"
  },
  {
    "text": "so we like to do that by spreading the the incoming pods onto the",
    "start": "506479",
    "end": "512159"
  },
  {
    "text": "nodes which can result in higher safety higher safety here implies that there's more uh leftover cpu cores left",
    "start": "512159",
    "end": "520360"
  },
  {
    "start": "520000",
    "end": "656000"
  },
  {
    "text": "uh here's some here's the algorithm for the target load packing plugin",
    "start": "522080",
    "end": "527839"
  },
  {
    "text": "i'll try to explain that in similar words so what you see on the right is a graph",
    "start": "527839",
    "end": "536320"
  },
  {
    "text": "of the different scores that that the plugins",
    "start": "536320",
    "end": "541440"
  },
  {
    "text": "provides versus utilization now the kubernetes framework uh it",
    "start": "541440",
    "end": "546560"
  },
  {
    "text": "supports this it was it wants a scores in the range 0 to 100 so a",
    "start": "546560",
    "end": "551760"
  },
  {
    "text": "node if it is scored higher it means that it has more chances to be selected for scheduling",
    "start": "551760",
    "end": "558080"
  },
  {
    "text": "so the first line you see is an increasing function so as the utilization increases",
    "start": "558080",
    "end": "563440"
  },
  {
    "text": "from 0 to 50 we favor the nodes by scoring them high this implies the bin packing i talked",
    "start": "563440",
    "end": "569600"
  },
  {
    "text": "about that we keep tracking the ports on the nodes now as you reach 50",
    "start": "569600",
    "end": "574880"
  },
  {
    "text": "that is our threshold utilization the graph shown here is like for the utilization",
    "start": "574880",
    "end": "580720"
  },
  {
    "text": "of 50 by the way so there is a drop there is a discontinuous function so what that implies is that we would",
    "start": "580720",
    "end": "587040"
  },
  {
    "text": "like to penalize nodes which are trying to exceed the utilization that we set",
    "start": "587040",
    "end": "592800"
  },
  {
    "text": "for as a threshold so we try to favor the nodes lesser as the",
    "start": "592800",
    "end": "598160"
  },
  {
    "text": "utilization goes beyond 50 percent now in point number two and three just",
    "start": "598160",
    "end": "603760"
  },
  {
    "text": "explains uh how we calculate the incoming parts uh usage because we we don't have the usage",
    "start": "603760",
    "end": "610480"
  },
  {
    "text": "metrics for it yet so we predicted predicted based on uh it's different based on the quality of service of that",
    "start": "610480",
    "end": "617360"
  },
  {
    "text": "specific port if we have both requests and limits then we we specify the limits as the usage",
    "start": "617360",
    "end": "625200"
  },
  {
    "text": "or more like the prediction and then if there are no limits then we have a multiplier for the requests",
    "start": "625200",
    "end": "630800"
  },
  {
    "text": "and if there are if it's the best referred pod there is no request or limits then we assume",
    "start": "630800",
    "end": "636000"
  },
  {
    "text": "some number which is a minimum utilization so that said another way to look at the",
    "start": "636000",
    "end": "642000"
  },
  {
    "text": "algorithm is that it switches from a best straight variant of bin packing",
    "start": "642000",
    "end": "648000"
  },
  {
    "text": "to a least straight variant as the utilization changes from zero to hundred",
    "start": "648000",
    "end": "653920"
  },
  {
    "text": "okay so here's some results from an experiment we did",
    "start": "655200",
    "end": "660959"
  },
  {
    "start": "656000",
    "end": "764000"
  },
  {
    "text": "uh with about 100 nodes and 400 pods uh we did this using an open source tool",
    "start": "660959",
    "end": "667839"
  },
  {
    "text": "called kds cluster simulator with some changes to incorporate out of three out of three plugins which is what we",
    "start": "667839",
    "end": "674399"
  },
  {
    "text": "contributed so the pods here have birth stable qos so the limits are larger and different",
    "start": "674399",
    "end": "680880"
  },
  {
    "text": "than the requests and they have a normal distribution of utilizations which remain constant",
    "start": "680880",
    "end": "687680"
  },
  {
    "text": "throat through the time they they run now the graph on the left you see is for",
    "start": "687680",
    "end": "693839"
  },
  {
    "text": "the native humanities scheduler and the graph on the right is for primal",
    "start": "693839",
    "end": "699920"
  },
  {
    "text": "now there are multiple observations you can make from these two graphs by analyzing them the first one",
    "start": "699920",
    "end": "706959"
  },
  {
    "text": "is that the area that you see under the threshold mark the red line that you're",
    "start": "706959",
    "end": "712000"
  },
  {
    "text": "seeing is much higher for target load packing which implies better capacity utilization across the cluster",
    "start": "712000",
    "end": "720399"
  },
  {
    "text": "the second one is that there are lesser number of hot nodes uh in the case of target load packing by",
    "start": "720399",
    "end": "726959"
  },
  {
    "text": "hot nodes i mean that any nodes which exceed the of threshold utilization that we had as an objective",
    "start": "726959",
    "end": "733839"
  },
  {
    "text": "so that they are minimized in the case of a targeted packing plugin",
    "start": "733839",
    "end": "738959"
  },
  {
    "text": "the last one is that there are lesser number of fragmented cores so the way you can tell that is so",
    "start": "738959",
    "end": "745200"
  },
  {
    "text": "each bar here respond corresponds to a specific node so the variance in the height of the nodes is much lesser in",
    "start": "745200",
    "end": "751680"
  },
  {
    "text": "the case of target load packing so that implies there are lesser number of fragmented",
    "start": "751680",
    "end": "756880"
  },
  {
    "text": "courses compared to the default scheduling",
    "start": "756880",
    "end": "761600"
  },
  {
    "text": "okay moving on to the next important question",
    "start": "762320",
    "end": "767360"
  },
  {
    "start": "764000",
    "end": "878000"
  },
  {
    "text": "for production how well does trimaran perform so these tests were done",
    "start": "767360",
    "end": "773920"
  },
  {
    "text": "in scheduler perf tests from the kubernetes main ripple so just for the background it is what",
    "start": "773920",
    "end": "779600"
  },
  {
    "text": "kubernetes uses to publish their uh scheduler metrics so that they know that",
    "start": "779600",
    "end": "786639"
  },
  {
    "text": "the metrics for example the latencies the algorithm latencies and the end-to-end port scheduling duration is within the slos or not",
    "start": "786639",
    "end": "792800"
  },
  {
    "text": "so we modified that to include our own plugins and uh",
    "start": "792800",
    "end": "798959"
  },
  {
    "text": "so we did two experiments here the first one is that we used 5000 nodes and 1000 calls to schedule",
    "start": "798959",
    "end": "805680"
  },
  {
    "text": "the other one is with 500 nodes and thousand pods there are same 500 number of init pods",
    "start": "805680",
    "end": "811519"
  },
  {
    "text": "in each of them these are like a set of standard tests which you can use to compare",
    "start": "811519",
    "end": "817600"
  },
  {
    "text": "now before and we can see that the trimaran beats the default scheduler in both of the",
    "start": "817600",
    "end": "823440"
  },
  {
    "text": "experiments now before i explain why this happens i like to mention that the",
    "start": "823440",
    "end": "830160"
  },
  {
    "text": "default scheduler it has a set of scoring plugins that are configured by default",
    "start": "830160",
    "end": "836079"
  },
  {
    "text": "now framaran uses two lesser plugins than what is present that default so essentially you have like one less",
    "start": "836079",
    "end": "842240"
  },
  {
    "text": "plugin compared to the default so first reason for the performance benefit is that",
    "start": "842240",
    "end": "849199"
  },
  {
    "text": "there's like one less plugin who's doing scoring so it is faster the other one is that we optimized the",
    "start": "849199",
    "end": "855839"
  },
  {
    "text": "plugin to use fast json encoding and decoding libraries and we have a background",
    "start": "855839",
    "end": "862639"
  },
  {
    "text": "thread that fetches such as metrics from the load watcher cache so that it doesn't it prefetches",
    "start": "862639",
    "end": "870399"
  },
  {
    "text": "before each scheduling cycle okay",
    "start": "870399",
    "end": "877519"
  },
  {
    "start": "878000",
    "end": "954000"
  },
  {
    "text": "so moving on to my last slide this is about the strategies of primary primary at",
    "start": "878959",
    "end": "885120"
  },
  {
    "text": "paypal as i explained uh so the main use case for us",
    "start": "885120",
    "end": "890399"
  },
  {
    "text": "uh working in infrastructure team is to maintain and ensure efficiency of the fleet",
    "start": "890399",
    "end": "896320"
  },
  {
    "text": "so paypal relying on the cloud we would like to also minimize the costs",
    "start": "896320",
    "end": "902240"
  },
  {
    "text": "now this translates to the uh technical requirements i mentioned for target load packing plugin the",
    "start": "902240",
    "end": "907279"
  },
  {
    "text": "objectives one thing i didn't mention is that we also have a requirement for the replicas",
    "start": "907279",
    "end": "913199"
  },
  {
    "text": "of an application to be spread across topologies by topologies here i mean it could be nodes",
    "start": "913199",
    "end": "918959"
  },
  {
    "text": "or node pools or different zones for example so",
    "start": "918959",
    "end": "925199"
  },
  {
    "text": "that said we combine our plug-in the target load packing and port topology spread",
    "start": "925199",
    "end": "932959"
  },
  {
    "text": "to achieve these requirements the current status is that it is actively tested",
    "start": "932959",
    "end": "938320"
  },
  {
    "text": "in for for deployment in qa followed by production so this will be",
    "start": "938320",
    "end": "944959"
  },
  {
    "text": "uh enabling efficient fleet for a majority of the people applications",
    "start": "944959",
    "end": "950399"
  },
  {
    "text": "over to my collaboration okay thank you so um next i will introduce the load",
    "start": "950399",
    "end": "957360"
  },
  {
    "start": "954000",
    "end": "1006000"
  },
  {
    "text": "variation risk balancing plugin uh that is designed to solve and you should be observing uh one of our production",
    "start": "957360",
    "end": "964240"
  },
  {
    "text": "clusters so we observe that um even the nodes will have different",
    "start": "964240",
    "end": "971120"
  },
  {
    "text": "will have the same average utilization when you consider the preference to schedule the power they may have",
    "start": "971360",
    "end": "977440"
  },
  {
    "text": "significant different load variations and it's very important to account the",
    "start": "977440",
    "end": "983199"
  },
  {
    "text": "usage variations and notes when scheduling paths namely the higher vibrations of the workload you have on",
    "start": "983199",
    "end": "989759"
  },
  {
    "text": "the node the higher risk you will have to schedule that path and then if the power",
    "start": "989759",
    "end": "995120"
  },
  {
    "text": "runs there and there's some birthday workload you will easily end up with like pod evictions or some performance",
    "start": "995120",
    "end": "1001680"
  },
  {
    "text": "issues so this plugin is designed to balance such a risk",
    "start": "1001680",
    "end": "1006959"
  },
  {
    "start": "1006000",
    "end": "1171000"
  },
  {
    "text": "so let's take and take a look at an example suppose we have two nodes like each has",
    "start": "1006959",
    "end": "1013519"
  },
  {
    "text": "a capacitive eight cpus and only five are requested on both nodes",
    "start": "1013519",
    "end": "1018800"
  },
  {
    "text": "and in that case those two two nodes are deemed equivalent",
    "start": "1018800",
    "end": "1024400"
  },
  {
    "text": "uh when the default scheduler is trying to schedule paths and then it can fully fit in both nodes",
    "start": "1024400",
    "end": "1030558"
  },
  {
    "text": "and then if we can only consider the average utilization and you may have exactly the the same average utilization",
    "start": "1030559",
    "end": "1037600"
  },
  {
    "text": "on both nodes so considering the utilization average",
    "start": "1037600",
    "end": "1043438"
  },
  {
    "text": "utilization you will also regard these two are the same and then um so but if you look at the",
    "start": "1043439",
    "end": "1050840"
  },
  {
    "text": "variations on these two nodes apparently on node 2 at peak usage the maximum",
    "start": "1050840",
    "end": "1057280"
  },
  {
    "text": "utilization is much higher so the scoring plugin we want to propose",
    "start": "1057280",
    "end": "1062400"
  },
  {
    "text": "is to consider not only the average utilization but also the variation on that",
    "start": "1062400",
    "end": "1068160"
  },
  {
    "text": "on that node so if a path can fit in both nodes in this case in order to",
    "start": "1068160",
    "end": "1073360"
  },
  {
    "text": "minimize the risk of over committing at the peak hour it is much safer to choose",
    "start": "1073360",
    "end": "1080080"
  },
  {
    "text": "node one than that node two right so to better understand what we are",
    "start": "1080080",
    "end": "1085200"
  },
  {
    "text": "actually balancing uh based on what we can balance based on the average and standard deviation um let's just draw",
    "start": "1085200",
    "end": "1092799"
  },
  {
    "text": "all knows average and standard deviation in a new sigma plot where mu is",
    "start": "1092799",
    "end": "1099120"
  },
  {
    "text": "the average and sigma refers to the standard deviation",
    "start": "1099120",
    "end": "1104160"
  },
  {
    "text": "so when your scheduler is trying to balance the average utilization across",
    "start": "1104160",
    "end": "1109200"
  },
  {
    "text": "all nodes and their uh node utilization will look like the top left chart",
    "start": "1109200",
    "end": "1115840"
  },
  {
    "text": "where it is the vertical line and all nodes average utilization are the same",
    "start": "1115840",
    "end": "1121600"
  },
  {
    "text": "so if you want to balance the vibrations on on available notes it will look like the",
    "start": "1121600",
    "end": "1127679"
  },
  {
    "text": "horizontal line on the red part top left plot so if you want to balance something like",
    "start": "1127679",
    "end": "1135280"
  },
  {
    "text": "the your variation is proportional to your average it looks like the lower",
    "start": "1135280",
    "end": "1141600"
  },
  {
    "text": "left chart i don't know if there is a practical use case for that but it's a",
    "start": "1141600",
    "end": "1146880"
  },
  {
    "text": "very good illustrative example to explain the last one which is balancing",
    "start": "1146880",
    "end": "1152160"
  },
  {
    "text": "the risk so the higher average utilization you have you want lower uh variation on that node and the",
    "start": "1152160",
    "end": "1159120"
  },
  {
    "text": "higher variations you have you may want lower average utilization and it will look",
    "start": "1159120",
    "end": "1164320"
  },
  {
    "text": "like this line and which is basically mu plus sigma equals",
    "start": "1164320",
    "end": "1169520"
  },
  {
    "text": "to some constant so this is",
    "start": "1169520",
    "end": "1174559"
  },
  {
    "start": "1171000",
    "end": "1255000"
  },
  {
    "text": "exactly how we design this uh scoring plugin so assuming you have a",
    "start": "1174559",
    "end": "1181360"
  },
  {
    "text": "path coming and it's requested resource let's denote it as r",
    "start": "1181360",
    "end": "1187200"
  },
  {
    "text": "and then specifically what the album is doing is it is going throughout the node",
    "start": "1187200",
    "end": "1192320"
  },
  {
    "text": "for each node it's going to get the sliding window average and the standard deviation of resource utilization and",
    "start": "1192320",
    "end": "1199120"
  },
  {
    "text": "then it used the average utilization plus the arrival paths request as the",
    "start": "1199120",
    "end": "1204159"
  },
  {
    "text": "prediction of the future average utilization on that node and then",
    "start": "1204159",
    "end": "1209440"
  },
  {
    "text": "so we also use the standard deviation we observed in the past time window to assume uh",
    "start": "1209440",
    "end": "1216159"
  },
  {
    "text": "assume it will be similar in the future time window and then if we place them",
    "start": "1216159",
    "end": "1221200"
  },
  {
    "text": "together just as what i have shown in the uh graph in the previous chart eight",
    "start": "1221200",
    "end": "1226640"
  },
  {
    "text": "will be our risk so we want to balance this risk of course we want to bound it by the risk in the range like zero to",
    "start": "1226640",
    "end": "1233600"
  },
  {
    "text": "one and then so we can scale it up to the priority score in the because it's",
    "start": "1233600",
    "end": "1239280"
  },
  {
    "text": "lower risk you have the higher score you will prefer to choose that node so it's one",
    "start": "1239280",
    "end": "1245039"
  },
  {
    "text": "minus row multiplied by the maximum priority score and then finally the scheduler will just",
    "start": "1245039",
    "end": "1251840"
  },
  {
    "text": "choose whatever is a high score so next i will show a simple uh two",
    "start": "1251840",
    "end": "1258640"
  },
  {
    "start": "1255000",
    "end": "1682000"
  },
  {
    "text": "simple demos on how our two plugins can be deployed and",
    "start": "1258640",
    "end": "1264799"
  },
  {
    "text": "how it behaves in a real cluster so here we are in the cluster of three",
    "start": "1264799",
    "end": "1271200"
  },
  {
    "text": "nodes and we are going to first deploy the target",
    "start": "1271200",
    "end": "1277600"
  },
  {
    "text": "load packing load packing plugin",
    "start": "1277600",
    "end": "1283120"
  },
  {
    "text": "so let's take a look at details what you want to create and then including a service account you want to",
    "start": "1284480",
    "end": "1290880"
  },
  {
    "text": "use for this scheduler uh all the arbitrals that are needed by the scheduler plugins you want to bond bind",
    "start": "1290880",
    "end": "1297840"
  },
  {
    "text": "this these rows with your service account including uh also the",
    "start": "1297840",
    "end": "1303760"
  },
  {
    "text": "cube scheduler cluster row and then you want to create a config map",
    "start": "1303760",
    "end": "1310400"
  },
  {
    "text": "to wrap all the scheduler configurations there uh and within that config map you",
    "start": "1310400",
    "end": "1317200"
  },
  {
    "text": "want to disable the complete complicated plugins and enable our target load",
    "start": "1317200",
    "end": "1323200"
  },
  {
    "text": "packing plugin so here the important parameter to configure is for the target load packing",
    "start": "1323200",
    "end": "1328640"
  },
  {
    "text": "is like the percentage of utilization you want and the endpoint of your",
    "start": "1328640",
    "end": "1333760"
  },
  {
    "text": "measure provider and then we can mount it to the deployment of the scheduler where",
    "start": "1333760",
    "end": "1340400"
  },
  {
    "text": "we just use the upstream scheduler plug-in image",
    "start": "1340400",
    "end": "1346480"
  },
  {
    "text": "so we go ahead creating namespace trimmer run and within the traman namespace we",
    "start": "1346480",
    "end": "1353360"
  },
  {
    "text": "deploy this scheduler and we can also see if it's running now",
    "start": "1353360",
    "end": "1360720"
  },
  {
    "text": "so i record all the demo because the whole demo will run for like an hour and",
    "start": "1360720",
    "end": "1365919"
  },
  {
    "text": "we can speed up a little bit in deploying those workloads so now we log into the scheduler um",
    "start": "1365919",
    "end": "1373039"
  },
  {
    "text": "part see all the detailed messages and then we go ahead to take a look at",
    "start": "1373039",
    "end": "1379760"
  },
  {
    "text": "the workload we are going to deploy in the cluster so this is a simple hamster",
    "start": "1379760",
    "end": "1386840"
  },
  {
    "text": "application it's used around 400 millicourse but it's requesting only 200 millicourse",
    "start": "1386840",
    "end": "1394320"
  },
  {
    "text": "and we are going to see the target load packing will pack nodes up to the",
    "start": "1394320",
    "end": "1399760"
  },
  {
    "text": "predefined utilization percentage",
    "start": "1399760",
    "end": "1403840"
  },
  {
    "text": "so here we prepare a simple script to generate the workload and then it will",
    "start": "1406960",
    "end": "1412720"
  },
  {
    "text": "create a part every once a while and totally we can define how many parts we",
    "start": "1412720",
    "end": "1419600"
  },
  {
    "text": "want to create in the cluster [Music]",
    "start": "1419600",
    "end": "1424869"
  },
  {
    "text": "and what it does is just replacing the id in the part template",
    "start": "1427919",
    "end": "1434080"
  },
  {
    "text": "so totally we are going to create this part 36 times",
    "start": "1434080",
    "end": "1441360"
  },
  {
    "text": "and here i speed up a little bit uh because it will run for longer and uh on",
    "start": "1442880",
    "end": "1448480"
  },
  {
    "text": "the lower bottom window you can see the target load packing uh plugins logging",
    "start": "1448480",
    "end": "1454320"
  },
  {
    "text": "some messages on binding paths and and on the right hand side the first one is",
    "start": "1454320",
    "end": "1460080"
  },
  {
    "text": "the actual part usage versus the request the second one is the",
    "start": "1460080",
    "end": "1465279"
  },
  {
    "text": "the utilization on each node so what you are seeing is it go ahead to pack on one",
    "start": "1465279",
    "end": "1471840"
  },
  {
    "text": "node onto reaching the target utilization percentage and then it starts",
    "start": "1471840",
    "end": "1477760"
  },
  {
    "text": "on the other node and the third one is the number of paths testing paths scheduled on that node so",
    "start": "1477760",
    "end": "1485279"
  },
  {
    "text": "similarly it starts packing on one and then spreading to so when all of those",
    "start": "1485279",
    "end": "1491120"
  },
  {
    "text": "reach to like the target utilization the scheduler will try to spread the paths",
    "start": "1491120",
    "end": "1497120"
  },
  {
    "text": "across nodes to balance the risk",
    "start": "1497120",
    "end": "1501840"
  },
  {
    "text": "okay next let's take a look at the load variation risk balancing plugging",
    "start": "1505440",
    "end": "1512880"
  },
  {
    "text": "so similarly we will create all those service accounts uh our back views and",
    "start": "1513520",
    "end": "1518880"
  },
  {
    "text": "deployment it's all in our documentation you can find it in the tremoran plugin",
    "start": "1518880",
    "end": "1526799"
  },
  {
    "text": "ripple",
    "start": "1526799",
    "end": "1529799"
  },
  {
    "text": "so i will explain a little bit on the right hand side on the right hand side we will have a testing part",
    "start": "1537360",
    "end": "1544159"
  },
  {
    "text": "request versus the usage and also the",
    "start": "1544159",
    "end": "1549279"
  },
  {
    "text": "the cpu utilization on all nodes sorry this is",
    "start": "1549279",
    "end": "1557240"
  },
  {
    "text": "okay so it was replaying the previous uh demo yeah here we have the the behavior of",
    "start": "1567039",
    "end": "1574400"
  },
  {
    "text": "the node version risk balancing plugin in the cluster we have three nodes on the left showing their uh allocable",
    "start": "1574400",
    "end": "1582240"
  },
  {
    "text": "resources and the variation of the usage on each node in the middle we see we show the statistics of the average and",
    "start": "1582240",
    "end": "1589360"
  },
  {
    "text": "standard deviation and then we go ahead to create a testing",
    "start": "1589360",
    "end": "1594720"
  },
  {
    "text": "workload this workload is just a simple stress",
    "start": "1594720",
    "end": "1599919"
  },
  {
    "text": "workload with some variations",
    "start": "1599919",
    "end": "1603720"
  },
  {
    "text": "so now we are creating a testing work namespace for the workload and",
    "start": "1618799",
    "end": "1624080"
  },
  {
    "text": "created the path and on the right hand side we have all the logs from the load",
    "start": "1624080",
    "end": "1629279"
  },
  {
    "text": "version risk balancing plugin you can see the details about like it is computing the cpu and memory",
    "start": "1629279",
    "end": "1637840"
  },
  {
    "text": "average utilization and standard deviation and it adds them together to compute the",
    "start": "1637840",
    "end": "1643120"
  },
  {
    "text": "risk score and then based on the risk it's coming up with the score",
    "start": "1643120",
    "end": "1649760"
  },
  {
    "text": "and it actually also combined the resources so if you have a higher score on cpu but",
    "start": "1651600",
    "end": "1658240"
  },
  {
    "text": "lower score on memory it will always choose the memory uh the bottleneck resource score",
    "start": "1658240",
    "end": "1664399"
  },
  {
    "text": "so now you can see um there's bigger variations on the first",
    "start": "1664399",
    "end": "1669760"
  },
  {
    "text": "node and the third node and that's why even uh their average utilization are similar",
    "start": "1669760",
    "end": "1676159"
  },
  {
    "text": "the part will eventually be scheduled to the middle one",
    "start": "1676159",
    "end": "1681880"
  },
  {
    "start": "1682000",
    "end": "1776000"
  },
  {
    "text": "okay so just a summary if what if we have a birthday arrival pass and during that",
    "start": "1683120",
    "end": "1690000"
  },
  {
    "text": "time we may not have enough metrics changing and then we look like the",
    "start": "1690000",
    "end": "1695840"
  },
  {
    "text": "matrix like every five minutes but when you have like a hundred parts arriving in one minute and then it's not",
    "start": "1695840",
    "end": "1702480"
  },
  {
    "text": "reflected to the nose what we do so in that case the matrix would be missing but we we still have the previous",
    "start": "1702480",
    "end": "1709440"
  },
  {
    "text": "utilization window and also the arrivals of path so we can use the arrival supply",
    "start": "1709440",
    "end": "1715360"
  },
  {
    "text": "to predict what would be happening in the next time window so for those parts we request and limits we always come",
    "start": "1715360",
    "end": "1723120"
  },
  {
    "text": "with the limit so if you have 10 parts arrived and each one is requesting um",
    "start": "1723120",
    "end": "1728480"
  },
  {
    "text": "one cpu we add up to 10 cpus and then for those parts with only request values",
    "start": "1728480",
    "end": "1734320"
  },
  {
    "text": "we can configure a multiplier to leave some safe margin of your inaccurate",
    "start": "1734320",
    "end": "1740080"
  },
  {
    "text": "prediction and then for those parts of that best effort you can also configure",
    "start": "1740080",
    "end": "1745200"
  },
  {
    "text": "what might be the threshold for your uh for your power request pod usage",
    "start": "1745200",
    "end": "1751440"
  },
  {
    "text": "and in the case that one provider is not accessible one metric provider is not",
    "start": "1751440",
    "end": "1756799"
  },
  {
    "text": "accessible we can always fall back to the metric server solutions and uh you can also like in the future",
    "start": "1756799",
    "end": "1765039"
  },
  {
    "text": "we plan to use multiple metric sources and do cross validation in the load",
    "start": "1765039",
    "end": "1770320"
  },
  {
    "text": "water so if one metric provider fails we can switch to the other",
    "start": "1770320",
    "end": "1776480"
  },
  {
    "start": "1776000",
    "end": "1851000"
  },
  {
    "text": "so from development to test and testing of these plugins we learned several",
    "start": "1777039",
    "end": "1782080"
  },
  {
    "text": "licenses throughout the way first we should be very careful about how frequently we retrieve metric data",
    "start": "1782080",
    "end": "1789679"
  },
  {
    "text": "so the load on metric providers are not acceptable you are not overwhelming uh",
    "start": "1789679",
    "end": "1795039"
  },
  {
    "text": "like a promises and but the interval should not be too large",
    "start": "1795039",
    "end": "1800159"
  },
  {
    "text": "then we have like outdated metric data um so and this is exactly why we introduced",
    "start": "1800159",
    "end": "1806399"
  },
  {
    "text": "load virtual library which only fetch data from providers periodically and cache it in the scheduler plugin and",
    "start": "1806399",
    "end": "1814320"
  },
  {
    "text": "so it's ready to be used whenever the pod arrives and second we should always have a",
    "start": "1814320",
    "end": "1820000"
  },
  {
    "text": "fallback solution especially for plugins that use a lot of metric data because metric",
    "start": "1820000",
    "end": "1826640"
  },
  {
    "text": "provider can be inaccessible third it's important to not use",
    "start": "1826640",
    "end": "1832880"
  },
  {
    "text": "complicated plugins together for example target load packing is targeting the average utilization and then the load",
    "start": "1832880",
    "end": "1840399"
  },
  {
    "text": "variation risk balancing is targeting both average equalization and standard deviation so they have some complicated",
    "start": "1840399",
    "end": "1847760"
  },
  {
    "text": "objectives you should not use them together so but even with tremor and plugins uh",
    "start": "1847760",
    "end": "1854640"
  },
  {
    "start": "1851000",
    "end": "1920000"
  },
  {
    "text": "we can never predict the load variation or load usage a hundred percent",
    "start": "1854640",
    "end": "1859760"
  },
  {
    "text": "accurately so you can always end up with some over provisioning uh node uh if the",
    "start": "1859760",
    "end": "1865600"
  },
  {
    "text": "pod usage climbs up so here are some future work we want to do first we want to integrate load washer",
    "start": "1865600",
    "end": "1872559"
  },
  {
    "text": "with for example this scatterer to the schedule paths based on their actual usage to solve the reported issue i show",
    "start": "1872559",
    "end": "1880720"
  },
  {
    "text": "here and second we plan to include some additional resources such as io and natural",
    "start": "1880720",
    "end": "1887760"
  },
  {
    "text": "boundaries and here we propose a new proposal new cab proposal in the",
    "start": "1887760",
    "end": "1893039"
  },
  {
    "text": "community called network award scheduler proposal and it doesn't only",
    "start": "1893039",
    "end": "1898799"
  },
  {
    "text": "consider the bandwidth allocation but also the latencies requirements you may have",
    "start": "1898799",
    "end": "1905360"
  },
  {
    "text": "in either a distributed cloud or in a data center cloud and in the future we also want to try",
    "start": "1905360",
    "end": "1911840"
  },
  {
    "text": "some machine learning models to have a more accurate uh prediction of utilization of our load schedulers",
    "start": "1911840",
    "end": "1920559"
  },
  {
    "start": "1920000",
    "end": "1960000"
  },
  {
    "text": "so here are some references if you want more details",
    "start": "1920559",
    "end": "1926480"
  },
  {
    "text": "and lastly we would like to express our appreciation to those who",
    "start": "1926480",
    "end": "1932240"
  },
  {
    "text": "cannot attend the conference in person including tan and drinan from paypal and",
    "start": "1932240",
    "end": "1937519"
  },
  {
    "text": "other from ibm research that all contributors to this project and we",
    "start": "1937519",
    "end": "1943679"
  },
  {
    "text": "really appreciate help from uh all members in the sixth scheduling team",
    "start": "1943679",
    "end": "1950559"
  },
  {
    "text": "and here is questions [Applause]",
    "start": "1950559",
    "end": "1962250"
  }
]