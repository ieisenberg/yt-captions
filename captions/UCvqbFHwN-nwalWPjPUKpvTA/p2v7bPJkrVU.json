[
  {
    "text": "so great to be here with you today in Salt Lake City for cucon North America I'm really excited to be here today",
    "start": "280",
    "end": "6160"
  },
  {
    "text": "talking about kubernetes operators and some of my own experience at crunchy data engineering and operator",
    "start": "6160",
    "end": "12480"
  },
  {
    "text": "for postres I've got quite a lot of content to cover today so let's get",
    "start": "12480",
    "end": "18439"
  },
  {
    "text": "started first a quick note about myself my name is Andre lir and I am the senior director of kubernetes engineering at",
    "start": "18439",
    "end": "25439"
  },
  {
    "text": "crunchy data at crunchy data our motto is postes",
    "start": "25439",
    "end": "31679"
  },
  {
    "text": "anywhere we have a fully managed Cloud we support bare metal and VMS and of",
    "start": "31679",
    "end": "37399"
  },
  {
    "text": "course we have crunchy postgres for kubernetes which I'll be talking about",
    "start": "37399",
    "end": "43038"
  },
  {
    "text": "today in this talk I'll be covering Lessons Learned From crunchy data's experience building five versions of a",
    "start": "45480",
    "end": "51559"
  },
  {
    "text": "postgres operator for kubernetes and to do this I will highlight three key areas of the",
    "start": "51559",
    "end": "57879"
  },
  {
    "text": "operator architecture High availability upgrades and disaster",
    "start": "57879",
    "end": "64839"
  },
  {
    "text": "recovery so why do we need operators operators not only help to",
    "start": "68600",
    "end": "73720"
  },
  {
    "text": "manage the complexity of deploying and managing an application but they also reduce toil and invite new communities",
    "start": "73720",
    "end": "81200"
  },
  {
    "text": "to take part in the kubernetes",
    "start": "81200",
    "end": "84560"
  },
  {
    "text": "ecosystem and in 17 crunchy data recognized that operators could reduce the operational complexity of running",
    "start": "88400",
    "end": "96439"
  },
  {
    "text": "posts at first it was simply a matter of containerizing post but once we were containerized we needed to",
    "start": "96439",
    "end": "103840"
  },
  {
    "text": "orchestrate and to properly orchestrate especially in the areas of high availability upgrades and Disaster",
    "start": "103840",
    "end": "110159"
  },
  {
    "text": "Recovery we wanted an",
    "start": "110159",
    "end": "113840"
  },
  {
    "text": "operator so let's quickly rewind back to 2017 and the first pigo release pigo",
    "start": "116360",
    "end": "122399"
  },
  {
    "text": "version one was released on March 27th 2017 and kubernetes version 1.6 was",
    "start": "122399",
    "end": "128840"
  },
  {
    "text": "released just one day later as we all know we're now up to kubernetes version",
    "start": "128840",
    "end": "134440"
  },
  {
    "text": "1.31 with version 1.32 do out next month and safe to say quite a lot has",
    "start": "134440",
    "end": "140599"
  },
  {
    "text": "changed over 7even years and 25 releases of kubernetes Frameworks for operators were",
    "start": "140599",
    "end": "146959"
  },
  {
    "text": "limited as was support for stateful application within",
    "start": "146959",
    "end": "152360"
  },
  {
    "text": "kubernetes so let's dive into the P architecture and talk about high availability or ha starting with the",
    "start": "152360",
    "end": "160920"
  },
  {
    "text": "definition any application will experience failures or crashes during its operation by making an application",
    "start": "164239",
    "end": "171400"
  },
  {
    "text": "highly available we ensure it remains available for use which means quickly recovering from issues when they do",
    "start": "171400",
    "end": "178000"
  },
  {
    "text": "occur or even better prev Ving those issues from occurring in the first place so from a postgres perspective",
    "start": "178000",
    "end": "185599"
  },
  {
    "text": "this means ensuring the database and therefore the user's data remains",
    "start": "185599",
    "end": "191879"
  },
  {
    "text": "accessible in the early days of the operator we realized there are two types of high availability to consider",
    "start": "192360",
    "end": "199040"
  },
  {
    "text": "availability of the operator and availability of the database and we also determined that",
    "start": "199040",
    "end": "206000"
  },
  {
    "text": "availability of the database was the top priority while it might be frustrating to be un",
    "start": "206000",
    "end": "212080"
  },
  {
    "text": "unable to deploy a new postgres cluster if the operator crashes it's even worse if users are unable to access their",
    "start": "212080",
    "end": "220080"
  },
  {
    "text": "data and when it came to the high availability of postres we had to decide whether we take",
    "start": "220080",
    "end": "225840"
  },
  {
    "text": "a mature postgres ha solution from within the postgres ecosystem and get it to run in",
    "start": "225840",
    "end": "231840"
  },
  {
    "text": "kubernetes or we could build our own custom solution using the",
    "start": "231840",
    "end": "237360"
  },
  {
    "text": "operator and inversions 1 through three of pigo we add a custom solution for postgres high availability built into",
    "start": "238000",
    "end": "244920"
  },
  {
    "text": "the operator so as you can see here the operator is responsible for ensuring the",
    "start": "244920",
    "end": "250840"
  },
  {
    "text": "database remains available by leveraging kubernetes capabilities such as Readiness probes",
    "start": "250840",
    "end": "258160"
  },
  {
    "text": "Pico could detect failures within a postgres cluster and react accordingly for instance promoting a",
    "start": "258160",
    "end": "264440"
  },
  {
    "text": "replica when the primary database crashes however by simply looking at",
    "start": "264440",
    "end": "270160"
  },
  {
    "text": "this diagram I'm sure many of you are also sensing some issues first there is only a single P which means we have a",
    "start": "270160",
    "end": "276919"
  },
  {
    "text": "single point of failure so if pigo goes down we lose ha capabilities for all of",
    "start": "276919",
    "end": "282840"
  },
  {
    "text": "our postgres databases and this isn't the only issue",
    "start": "282840",
    "end": "288680"
  },
  {
    "text": "all operators also use a queuing mechanism to capture and respond to events in the kubernetes",
    "start": "288680",
    "end": "294280"
  },
  {
    "text": "cluster as you can imagine this could be problematic if many databases crash at the same time",
    "start": "294280",
    "end": "300520"
  },
  {
    "text": "because the last thing you want if your database crashes is to have to wait behind 10 other databases before the ha",
    "start": "300520",
    "end": "306120"
  },
  {
    "text": "system takes action and while these issues didn't manifest in major ways in the early days",
    "start": "306120",
    "end": "312840"
  },
  {
    "text": "of pigo where postgres deployments were typically smaller in scale as users made",
    "start": "312840",
    "end": "318080"
  },
  {
    "text": "their first steps into kubernetes for stateful deployments these cracks grew as users began to deploy postgres at",
    "start": "318080",
    "end": "325360"
  },
  {
    "text": "scale so in these early versions of pigo we learned another solution was needed",
    "start": "325360",
    "end": "330560"
  },
  {
    "text": "to support postgres deployments at",
    "start": "330560",
    "end": "334520"
  },
  {
    "text": "scale so in P V4 we pivoted to a battle tested solution within the",
    "start": "335720",
    "end": "341000"
  },
  {
    "text": "postgres ecosystem called Petron this provided the decentralized",
    "start": "341000",
    "end": "347000"
  },
  {
    "text": "architecture needed to mitigate the issues seen operating at scale with our original",
    "start": "347000",
    "end": "352039"
  },
  {
    "text": "architecture and fortunately by this time the amazing patroni team was already hard at work adding kubernetes",
    "start": "352039",
    "end": "357680"
  },
  {
    "text": "native capabilities to Patron itself and with Patron handling postgres High",
    "start": "357680",
    "end": "364479"
  },
  {
    "text": "availability we could address High availability of the operator Evolutions within the operator",
    "start": "364479",
    "end": "370800"
  },
  {
    "text": "ecosystem made this easy by switching to the controller runtime project in recent",
    "start": "370800",
    "end": "376000"
  },
  {
    "text": "versions of pigo a set of libraries for building kubernetes operators adding",
    "start": "376000",
    "end": "381120"
  },
  {
    "text": "High availability was reduced to a configuration change so we learned that Evolutions",
    "start": "381120",
    "end": "387400"
  },
  {
    "text": "within the postgres and kubernetes ecosystem specifically with Patron and the controller runtime projects were key to",
    "start": "387400",
    "end": "394800"
  },
  {
    "text": "enabling the ha solution we needed for both the operator and the",
    "start": "394800",
    "end": "400439"
  },
  {
    "text": "database all right so let's see what this looks like in action starting with postgres high",
    "start": "400840",
    "end": "406319"
  },
  {
    "text": "availability in this demo I'll introduce chaos into a running postgres cluster while highlighting how Patron ensures",
    "start": "406319",
    "end": "412759"
  },
  {
    "text": "our database remains available this includes scaling down the operator deleting the current primary pod and",
    "start": "412759",
    "end": "419560"
  },
  {
    "text": "leading the entire data directory for the primary postgres instance on the left hand side of the",
    "start": "419560",
    "end": "425840"
  },
  {
    "text": "screen here is a running postgres cluster which is comprised of a primary and a single replica you can also see",
    "start": "425840",
    "end": "432160"
  },
  {
    "text": "the Pod for the pigo deployment at the bottom below the postgres cluster a row",
    "start": "432160",
    "end": "437639"
  },
  {
    "text": "count is displayed and once I kick off this video a client will start inserting data into a table causing the row count",
    "start": "437639",
    "end": "443840"
  },
  {
    "text": "to increase this will simulate the minimal impact to a postgres client as chaos is introduced uced and the ha",
    "start": "443840",
    "end": "450479"
  },
  {
    "text": "system takes action over on the right hand side of the screen you'll see Patron managing",
    "start": "450479",
    "end": "457560"
  },
  {
    "text": "leader election for the postgres cluster via an endpoints resource Patron uses annotations in the",
    "start": "457560",
    "end": "464159"
  },
  {
    "text": "endpoints resource to track leader election details which are also displayed and what is really cool about",
    "start": "464159",
    "end": "471039"
  },
  {
    "text": "this solution is that by using the endpoints resource Patron can update both the leader lock as well as the IP",
    "start": "471039",
    "end": "478400"
  },
  {
    "text": "address pointing to the Post sc's primary database all at the same time so not only will you want to keep",
    "start": "478400",
    "end": "484599"
  },
  {
    "text": "an eye out for the various leader election details but also keep on the an eye out for the various Network details",
    "start": "484599",
    "end": "490120"
  },
  {
    "text": "within the endpoints Resource as well so let's kick off the",
    "start": "490120",
    "end": "497159"
  },
  {
    "text": "video so the first thing you will see me do here is scale down and remove the operator which due to our decentralized",
    "start": "497520",
    "end": "504280"
  },
  {
    "text": "architecture with Patron will have no impact on the client so there you can see we just got rid of our p pod and our",
    "start": "504280",
    "end": "510319"
  },
  {
    "text": "client is happily inserting so next I will delete the current primary pod which will result in a promotion of our",
    "start": "510319",
    "end": "516719"
  },
  {
    "text": "replica so when this occurs you will see the various leader election details as well as the IP address for the endpoints",
    "start": "516719",
    "end": "523518"
  },
  {
    "text": "updated to reflect the new primary and finally I will delete the entire postgres data directory for the",
    "start": "523519",
    "end": "530920"
  },
  {
    "text": "newly promoted primary this will cause Petron once again to promote the current replica and looking at the ongoing",
    "start": "530920",
    "end": "537399"
  },
  {
    "text": "insert you can see the minimal impact of the CLI L inserting data so we're going to wipe out our entire data directory",
    "start": "537399",
    "end": "542640"
  },
  {
    "text": "here we can see our client is paused here for a moment before Patron is going to quickly take action promote our",
    "start": "542640",
    "end": "549480"
  },
  {
    "text": "replica there we go and our inserts",
    "start": "549480",
    "end": "553920"
  },
  {
    "text": "continue okay so this next demo is going to is going to follow a similar script to the last one only this time we'll be",
    "start": "557839",
    "end": "564160"
  },
  {
    "text": "demonstrating High availability of the operator this demo starts with no operator pods as you can see here and",
    "start": "564160",
    "end": "571279"
  },
  {
    "text": "attempts to initiate a restore and because the operator isn't running the restore will not be",
    "start": "571279",
    "end": "576839"
  },
  {
    "text": "initiated even though Patron ensures postgres remains available we also want pigo to be highly available to ensure",
    "start": "576839",
    "end": "583360"
  },
  {
    "text": "other critical functionality in this case an inplace restore is still available in the event that the operator",
    "start": "583360",
    "end": "590440"
  },
  {
    "text": "crashes so from there I'll scale P go back up only this time adding two replicas and as I do that one of the P",
    "start": "590440",
    "end": "597600"
  },
  {
    "text": "replicas will grab the leader lock using this lease object shown on the top right hand side of the screen additionally",
    "start": "597600",
    "end": "603760"
  },
  {
    "text": "you'll see the restore initiate within our cluster and from there once again I'll",
    "start": "603760",
    "end": "608959"
  },
  {
    "text": "add some chaos only this time focusing on the operator by deleting the pigo replica that currently holds the leader",
    "start": "608959",
    "end": "615040"
  },
  {
    "text": "lock this will initiate a new leader election process for the operator so once again keep an eye on those details",
    "start": "615040",
    "end": "621880"
  },
  {
    "text": "within the lease object and like the previous video you can also see the impact of the client inserting data into",
    "start": "621880",
    "end": "628200"
  },
  {
    "text": "the database for instance we expect the inserts to fail during the restore but they will be uninterrupted by the chaos",
    "start": "628200",
    "end": "634600"
  },
  {
    "text": "introduced to pigo itself the restore and reinitialization of the cluster will also see minimal",
    "start": "634600",
    "end": "641079"
  },
  {
    "text": "impacts finally the demo will end with pigo detecting that the database is running out of storage space and autog",
    "start": "641079",
    "end": "647800"
  },
  {
    "text": "growing the database persist persistent volume to prevent that from occurring so",
    "start": "647800",
    "end": "653160"
  },
  {
    "text": "we'll end with an example of preventing a failure before it occurs so be sure to keep an eye on the PVC details shown on",
    "start": "653160",
    "end": "659560"
  },
  {
    "text": "the right all right so now let's kick off the video like I said the first thing",
    "start": "659560",
    "end": "665800"
  },
  {
    "text": "you'll see me attempt to do here is initiate a restore which is done by annotating the custom resource for the",
    "start": "665800",
    "end": "671800"
  },
  {
    "text": "postgres cluster however because the operator isn't running the restore isn't initiated so we're going to add our",
    "start": "671800",
    "end": "677560"
  },
  {
    "text": "annotation here but no restore will be initiated so next I will turn the",
    "start": "677560",
    "end": "683760"
  },
  {
    "text": "operator back on by scaling the operator deployment back up only this time I'll add two replicas",
    "start": "683760",
    "end": "689839"
  },
  {
    "text": "and when I do that you'll see the first pigo pod grab the leader lock using the lease object while the in place restore",
    "start": "689839",
    "end": "696240"
  },
  {
    "text": "then initiates next I'll delete the P pod that currently holds a leader lock triggering a new leader leader election",
    "start": "696240",
    "end": "703240"
  },
  {
    "text": "process for the operator at this point the second pigo pod here will grab the leader",
    "start": "703240",
    "end": "710160"
  },
  {
    "text": "lock and finally looking at the PVC details over here on the right you can see that once the client reconnected and",
    "start": "710160",
    "end": "717040"
  },
  {
    "text": "started inserting data following the restore pigo resized the PVC from 1 GB to Du",
    "start": "717040",
    "end": "722959"
  },
  {
    "text": "gabes to prevent the database from running out of",
    "start": "722959",
    "end": "727760"
  },
  {
    "text": "space so here is a diagram depicting what you just saw at the end of the previous demo Pig's ability to",
    "start": "735399",
    "end": "742040"
  },
  {
    "text": "automatically increase the size of a volume in the event that it is running out of space this is a great example of",
    "start": "742040",
    "end": "748560"
  },
  {
    "text": "another lesson learned while implementing ha within pigo which is emphasizing prevention over preparedness",
    "start": "748560",
    "end": "755600"
  },
  {
    "text": "in other words while we want to be prepared for an eventual failure it's even better if we can prevent that failure in the first place and by",
    "start": "755600",
    "end": "762959"
  },
  {
    "text": "leveraging kubernetes volume expansion capabilities we can prevent one of the most common errors we see managing",
    "start": "762959",
    "end": "768760"
  },
  {
    "text": "databases which is running out of storage",
    "start": "768760",
    "end": "772920"
  },
  {
    "text": "space all right so next up is up upgrades this includes major and minor",
    "start": "776959",
    "end": "782440"
  },
  {
    "text": "upgrades of postgres as well as major and minor upgrades of the operator in P versions 1 through three",
    "start": "782440",
    "end": "789800"
  },
  {
    "text": "the operator did not tackle upgrades directly it was a manual process and",
    "start": "789800",
    "end": "794880"
  },
  {
    "text": "with Patron handling database High availability we had freed up development time to focus on new",
    "start": "794880",
    "end": "800480"
  },
  {
    "text": "features one of those features was a fully automated rolling update strategy that could roll out postgres minor",
    "start": "800480",
    "end": "806399"
  },
  {
    "text": "version upgrades and for p by avoiding breaking changes within our own apis pigo",
    "start": "806399",
    "end": "812800"
  },
  {
    "text": "upgrades could also be rolled out via the same strategy post's major version upgrades",
    "start": "812800",
    "end": "819000"
  },
  {
    "text": "however were different since the use of invalid update upgrade configurations or",
    "start": "819000",
    "end": "824199"
  },
  {
    "text": "images could could result in serious downtime while compatibility issues could also break existing database",
    "start": "824199",
    "end": "832480"
  },
  {
    "text": "clients so when looking at these various types of upgrades we we had to determine where it made sense to to either fully",
    "start": "835279",
    "end": "841839"
  },
  {
    "text": "automate an upgrade or where it made sense for the operator to navigate the user through an",
    "start": "841839",
    "end": "848959"
  },
  {
    "text": "upgrade so on the left here is P's rolling update strategy and on the right",
    "start": "851199",
    "end": "856600"
  },
  {
    "text": "is the postgres major version upgrade strategy P's fully automated rolling",
    "start": "856600",
    "end": "862199"
  },
  {
    "text": "update process handles the majority of changes a user would need to apply on a day-to-day basis pigo can be upgraded",
    "start": "862199",
    "end": "870000"
  },
  {
    "text": "postgres can be reconfigured and updated to new minor versions or the podspec for",
    "start": "870000",
    "end": "875160"
  },
  {
    "text": "the postgres database can be modified for instance to add a custom side car all through a single consistent rolling",
    "start": "875160",
    "end": "881720"
  },
  {
    "text": "update process on the right is the postgres major version upgrade process the use of",
    "start": "881720",
    "end": "889240"
  },
  {
    "text": "status and conditions within the postgres major version upgrade API navigates the user through these steps",
    "start": "889240",
    "end": "895240"
  },
  {
    "text": "with sufficient transparency to empower the user to fully automate the process",
    "start": "895240",
    "end": "901040"
  },
  {
    "text": "process so this video will demonstrate the P rolling update process this will be done by making a",
    "start": "908160",
    "end": "914680"
  },
  {
    "text": "configuration change to postgres that requires a database restart mounting a key tab file for Kerberos authentication",
    "start": "914680",
    "end": "922199"
  },
  {
    "text": "as well as Prov performing an upgrade of pigo and during the upgrade of pigo not",
    "start": "922199",
    "end": "927680"
  },
  {
    "text": "only will the operator be UPG upgraded but a minor version upgrade will be rolled out to postgres as",
    "start": "927680",
    "end": "933000"
  },
  {
    "text": "well on the top left you can see the postgres cluster that will be rolling changes out to and once again a client",
    "start": "933000",
    "end": "940040"
  },
  {
    "text": "will continuously insert data as changes are rolled out and the version of postgres is also shown here to",
    "start": "940040",
    "end": "946079"
  },
  {
    "text": "demonstrate the successful roll out of a minor postgres version",
    "start": "946079",
    "end": "951199"
  },
  {
    "text": "upgrade on the top right you will see when a restart is needed for each instance following the configuration",
    "start": "951279",
    "end": "957639"
  },
  {
    "text": "change and the panel underneath shows a live view of the postgres configuration",
    "start": "957639",
    "end": "962720"
  },
  {
    "text": "allowing you to see that configuration change directly within a postgres pod finally the panel at the bottom",
    "start": "962720",
    "end": "969959"
  },
  {
    "text": "right hand side of the screen shows a a live view of the Etsy postgres directory within a postgres pod a key tab file",
    "start": "969959",
    "end": "976920"
  },
  {
    "text": "will appear here once I reconfigure the cluster for Kerberos authentication so let's kick off this",
    "start": "976920",
    "end": "983600"
  },
  {
    "text": "video and the first thing you'll see me do here is change the max connection setting for the postgres database face",
    "start": "983600",
    "end": "989639"
  },
  {
    "text": "from 100 as you see here to 200 so we're going to patch in that configuration",
    "start": "989639",
    "end": "996160"
  },
  {
    "text": "change and as I do that you'll see we'll get some pending restarts here for each postgres instance which are quickly",
    "start": "996160",
    "end": "1002040"
  },
  {
    "text": "cleared by pigo there's our first one pigo just rolled out that restart and",
    "start": "1002040",
    "end": "1007160"
  },
  {
    "text": "we'll see another one here in a second for our second instance now P's rolled that out so next a secret will be",
    "start": "1007160",
    "end": "1013000"
  },
  {
    "text": "mounted containing a file needed to enable Kerberos authentication and once this change is rolled out you'll see a",
    "start": "1013000",
    "end": "1019160"
  },
  {
    "text": "Kerberos key tab file appear in the Etsy postgres directory so there's our key",
    "start": "1019160",
    "end": "1024240"
  },
  {
    "text": "tab file and finally I will upgrade pigo from version 5.6 to version",
    "start": "1024240",
    "end": "1030120"
  },
  {
    "text": "5.7 and once I do this a minor version upgrade of postgres will be rolled out as well so you will see that postgres",
    "start": "1030120",
    "end": "1037120"
  },
  {
    "text": "has changed from version 16.3 to",
    "start": "1037120",
    "end": "1041640"
  },
  {
    "text": "16.4 so this next video demonstrates a postgres major version upgrade of the",
    "start": "1047199",
    "end": "1052280"
  },
  {
    "text": "same cluster shown in the previous video it will demonstrate a major version upgrade from postgres 16 to",
    "start": "1052280",
    "end": "1059039"
  },
  {
    "text": "postgres 17 using P's PG upgrade API you'll want to pay attention to the",
    "start": "1059039",
    "end": "1064880"
  },
  {
    "text": "status and conditions displayed for the PG upgrade Resource as will be shown on the right hand side of the screen as you",
    "start": "1064880",
    "end": "1071480"
  },
  {
    "text": "will see the status and conditions will safely navigate me through a successful upgrade",
    "start": "1071480",
    "end": "1078200"
  },
  {
    "text": "so let's start this video and the first thing I will do here is create the PG upgrade resource to",
    "start": "1079240",
    "end": "1085240"
  },
  {
    "text": "upgrade our hippo cluster here from postgres 16 to postgres 17 and once I do that the first",
    "start": "1085240",
    "end": "1092440"
  },
  {
    "text": "condition is displayed that shows that the upgrade cannot proceed until the postgres cluster is shut down so next",
    "start": "1092440",
    "end": "1098520"
  },
  {
    "text": "we'll shut it down and once I do this a new condition will be displayed indicating that the postgres cluster",
    "start": "1098520",
    "end": "1104200"
  },
  {
    "text": "needs to be annotated for the upgrade to proceed this is how P allows those man in the postgres database to indicate",
    "start": "1104200",
    "end": "1110679"
  },
  {
    "text": "that they're ready for a major upgrade so now I will annotate the postgres cluster and as I do that some upgrade",
    "start": "1110679",
    "end": "1117880"
  },
  {
    "text": "jobs are run and two new and and we can see that our upgrade is progressing by",
    "start": "1117880",
    "end": "1123320"
  },
  {
    "text": "looking at our condition here and next the upgrade will complete at which point two conditions are displayed one show",
    "start": "1123320",
    "end": "1129679"
  },
  {
    "text": "showing that the the upgrade is no longer progressing and the other showing that the upgrade completed",
    "start": "1129679",
    "end": "1135600"
  },
  {
    "text": "successfully and finally we'll turn the postgres cluster back gone at which point our client will reconnect and we",
    "start": "1135600",
    "end": "1141520"
  },
  {
    "text": "can see that we are now on postgres version 17 and our inserts",
    "start": "1141520",
    "end": "1147919"
  },
  {
    "text": "continue all right so now on to Disaster Recovery or Dr any system has the potential to",
    "start": "1157520",
    "end": "1164520"
  },
  {
    "text": "experience a disaster at some point this could be anything from a natural disaster that takes out your data center",
    "start": "1164520",
    "end": "1169919"
  },
  {
    "text": "to data corruption and when this occurs you want to have a process in place that allows you to quickly",
    "start": "1169919",
    "end": "1176159"
  },
  {
    "text": "recover this is what Disaster Recovery is all about establishing and exercising",
    "start": "1176159",
    "end": "1181559"
  },
  {
    "text": "processes and Technical Solutions to ensure your critical applications continue to operate for postgress this",
    "start": "1181559",
    "end": "1188760"
  },
  {
    "text": "means ensuring users and applications can access critical data according to the recovery time objectives or",
    "start": "1188760",
    "end": "1196000"
  },
  {
    "text": "RTO and by RTO I'm referring to the the amount of time the database can be offline before it is",
    "start": "1196000",
    "end": "1203480"
  },
  {
    "text": "recovered crunchy data's experience in Disaster Recovery told us that an effective Dr strategy was more than",
    "start": "1204320",
    "end": "1210840"
  },
  {
    "text": "simply creating backups it was really the recovery scenarios that mattered in",
    "start": "1210840",
    "end": "1216440"
  },
  {
    "text": "other words backups hold little value if we can't use them to meet our recovery objectives and because our recovery",
    "start": "1216440",
    "end": "1223400"
  },
  {
    "text": "scenarios might involve crossing the boundary of a single kubernetes cluster or Cloud environment",
    "start": "1223400",
    "end": "1229200"
  },
  {
    "text": "we also realized that our Dr strategy would be critical to enabling another important capability within pigo which",
    "start": "1229200",
    "end": "1235200"
  },
  {
    "text": "is data Mobility this means providing users with quick and easy access to their data where they need",
    "start": "1235200",
    "end": "1242960"
  },
  {
    "text": "it so this left us with an important decision use a tried and true Disaster",
    "start": "1243280",
    "end": "1248720"
  },
  {
    "text": "Recovery Solution within the postgres ecosystem that can meet our recovery needs or do we build a solution around",
    "start": "1248720",
    "end": "1255120"
  },
  {
    "text": "kubernetes kubernetes native capabilities for inance since the ability to create volume snapshots via",
    "start": "1255120",
    "end": "1260960"
  },
  {
    "text": "the kubernetes API to meet this need additionally we didn't want our",
    "start": "1260960",
    "end": "1266760"
  },
  {
    "text": "decision for instance if we did select a postr native solution to Blind us from embracing solutions from the kubernetes",
    "start": "1266760",
    "end": "1274960"
  },
  {
    "text": "community and if we did adopt a kubernetes solution we needed to use it in a way that recognized the specific",
    "start": "1274960",
    "end": "1281159"
  },
  {
    "text": "needs of our application in this case postest and finally while we might be",
    "start": "1281159",
    "end": "1286400"
  },
  {
    "text": "able to build custom layers on top of EX existing solutions to give us the Dr features we needed it would be even",
    "start": "1286400",
    "end": "1292840"
  },
  {
    "text": "better to work with the maintainers of those solutions to build in those missing",
    "start": "1292840",
    "end": "1298360"
  },
  {
    "text": "features so this diagram depicts the current architecture for Disaster Recovery within PEB in the middle of the",
    "start": "1300440",
    "end": "1307279"
  },
  {
    "text": "screen is the heart of our Dr solution a Dr solution within the postgres ecosystem called PG back rest this is",
    "start": "1307279",
    "end": "1314840"
  },
  {
    "text": "where we can attach different types of storage for managing our backups but local persistent volumes within the",
    "start": "1314840",
    "end": "1320320"
  },
  {
    "text": "kubernetes cluster or various types of cloud storage and in the demo on the next",
    "start": "1320320",
    "end": "1326320"
  },
  {
    "text": "slide I'll highlight various aspects of this architecture this includes using multiple types of storage for backups",
    "start": "1326320",
    "end": "1333400"
  },
  {
    "text": "allowing easy access to data to create things like standby clusters and clones additionally I'll show how kubernetes",
    "start": "1333400",
    "end": "1340000"
  },
  {
    "text": "volume snapshots can be used with PG back rest to further reduce the amount of time to create those clones",
    "start": "1340000",
    "end": "1348279"
  },
  {
    "text": "okay so this demo starts with an nonrem cluster as shown on the top left hand side of the screen that has two backup",
    "start": "1350679",
    "end": "1356760"
  },
  {
    "text": "repositories defined this includes a local PVC based repo as well as a repo",
    "start": "1356760",
    "end": "1361880"
  },
  {
    "text": "within a GCS bucket as shown here on the right hand side of the screen and the first thing I will do is",
    "start": "1361880",
    "end": "1368600"
  },
  {
    "text": "create a backup of the on-prem cluster in the GCS bucket after that I will create what",
    "start": "1368600",
    "end": "1375760"
  },
  {
    "text": "pigo calls a standby cluster in the GK environment shown up up in the top right",
    "start": "1375760",
    "end": "1381679"
  },
  {
    "text": "here this means I will now have a postgres cluster in gke that is replicating from my on-prem cluster all",
    "start": "1381679",
    "end": "1388640"
  },
  {
    "text": "using PG back rest from there I will shut down the on-prem cluster and promote the gke",
    "start": "1388640",
    "end": "1395480"
  },
  {
    "text": "cluster this means the gke cluster will now accept rights PVC details on the",
    "start": "1395480",
    "end": "1401400"
  },
  {
    "text": "right hand side of the screen will switch to the gke cluster and a new panel will be displayed on the bottom right hand side of the screen this pan",
    "start": "1401400",
    "end": "1408679"
  },
  {
    "text": "will show the volume snapshot that is safely created for the gke cluster using PG backrest and kubernetes volume",
    "start": "1408679",
    "end": "1416440"
  },
  {
    "text": "snapshots and finally I will create a clone of that gke cluster using the volume",
    "start": "1416440",
    "end": "1423158"
  },
  {
    "text": "snapshot so let's start the video and like I said we'll start with a",
    "start": "1423480",
    "end": "1429320"
  },
  {
    "text": "an on-prem backup to our GCS bucket which will quickly quickly compete complete sorry",
    "start": "1429320",
    "end": "1437720"
  },
  {
    "text": "and next I will create a standby cluster in the gke environment which is quickly bootstrapped using the backup to GCS",
    "start": "1439120",
    "end": "1445640"
  },
  {
    "text": "that just completed once that comes online here",
    "start": "1445640",
    "end": "1450720"
  },
  {
    "text": "we'll then shut down the on-prem cluster followed by promoting the gke",
    "start": "1450720",
    "end": "1457000"
  },
  {
    "text": "cluster and once the GK cluster is promoted you'll see some additional activity as it becomes the new primary",
    "start": "1457200",
    "end": "1463520"
  },
  {
    "text": "this includes the creation of a local backup as well as a volume snapshot which will be shown at the bottom right",
    "start": "1463520",
    "end": "1468799"
  },
  {
    "text": "hand side of the screen so we can see that we have our volume snapshot here which is about to",
    "start": "1468799",
    "end": "1474840"
  },
  {
    "text": "be ready to use and finally I create a clone which is quickly bootstrapped due",
    "start": "1474840",
    "end": "1480039"
  },
  {
    "text": "to the use of that volume snapshot so there we just submitted our Command to",
    "start": "1480039",
    "end": "1485799"
  },
  {
    "text": "create the Clone and we'll see once kubernetes schedules that job um everything will quickly bootstrap and",
    "start": "1485799",
    "end": "1491960"
  },
  {
    "text": "that is thanks to the use of that volume snapshot and this last command here is just looking at some of the PVC details",
    "start": "1491960",
    "end": "1499279"
  },
  {
    "text": "for this clone we just created to show that it is indeed using that snapshot that was just created as shown on the",
    "start": "1499279",
    "end": "1505480"
  },
  {
    "text": "bottom right hand side of the screen of course we saw that that cluster did come up very quickly um once it was scheduled",
    "start": "1505480",
    "end": "1512559"
  },
  {
    "text": "by",
    "start": "1512559",
    "end": "1514720"
  },
  {
    "text": "kubernetes so now that we've covered High availability upgrades and Disaster Recovery in",
    "start": "1520880",
    "end": "1526960"
  },
  {
    "text": "detail let's rview and summarize both the current solution within pigo for each as well as the important Lessons",
    "start": "1526960",
    "end": "1533360"
  },
  {
    "text": "Learned implementing these Solutions starting with high availability what was the solution we",
    "start": "1533360",
    "end": "1539960"
  },
  {
    "text": "settled on within pigo we have Petron being used for postgres high availability and the",
    "start": "1539960",
    "end": "1546600"
  },
  {
    "text": "controller runtime project is being used for operator High availability and kubernetes volume expansion is being",
    "start": "1546600",
    "end": "1553360"
  },
  {
    "text": "used to autog grow discs and the lessons we learned are as followed",
    "start": "1553360",
    "end": "1559279"
  },
  {
    "text": "first a decentralized architecture allows us to scale by using Petron we now have a decentralized architecture",
    "start": "1559279",
    "end": "1566120"
  },
  {
    "text": "that allows us to operate and manage highly available postgres clusters at",
    "start": "1566120",
    "end": "1572039"
  },
  {
    "text": "scale next we need to fight the not invented he syndrome and embrace existing Solutions within the community",
    "start": "1572039",
    "end": "1578799"
  },
  {
    "text": "well as operator developers it was a great exercise to see how kuber dti's features could be utilized to build our",
    "start": "1578799",
    "end": "1584039"
  },
  {
    "text": "own ha Solutions we ultimately determined that existing battle tested solutions for ha would give us the",
    "start": "1584039",
    "end": "1590360"
  },
  {
    "text": "robust ha capabilities we're looking for and finally we learned that",
    "start": "1590360",
    "end": "1596279"
  },
  {
    "text": "prevention is better than preparedness while having ha solutions to protect us in the event of a failure",
    "start": "1596279",
    "end": "1602159"
  },
  {
    "text": "is great the ability to leverage kubernetes volume expansion capabilities to prevent a common issue in the",
    "start": "1602159",
    "end": "1608360"
  },
  {
    "text": "database World running out of storage space is even",
    "start": "1608360",
    "end": "1613520"
  },
  {
    "text": "better now on to upgrades starting with the current solution implement mented within pigo to enable simple and",
    "start": "1614720",
    "end": "1620440"
  },
  {
    "text": "seamless upgrades a safe rolling update strategy is utilized for configuration changes",
    "start": "1620440",
    "end": "1627120"
  },
  {
    "text": "minor postgres version upgrades and pigo upgrades and a new API has been created",
    "start": "1627120",
    "end": "1632919"
  },
  {
    "text": "for postgres major version upgrades that uses status and conditions to empower users to successfully navigate the",
    "start": "1632919",
    "end": "1639600"
  },
  {
    "text": "upgrade process and these are the lessons we learned while implementing this",
    "start": "1639600",
    "end": "1644919"
  },
  {
    "text": "solution first manage risk associated with upgrades Automation and only update",
    "start": "1644919",
    "end": "1650279"
  },
  {
    "text": "or it only automate when risks can be mitigated and when we can't automate empower the user to orchestrate and",
    "start": "1650279",
    "end": "1657080"
  },
  {
    "text": "automate using status and",
    "start": "1657080",
    "end": "1660720"
  },
  {
    "text": "conditions and finally our solution and Lessons Learned for a comprehensive Disaster Recovery Solution within Pig",
    "start": "1663440",
    "end": "1670159"
  },
  {
    "text": "starting with the solution we use PG backrest for multicloud backup and restore",
    "start": "1670159",
    "end": "1675840"
  },
  {
    "text": "functionality as well as to provide data Mobil additionally kubernetes volume snapshots",
    "start": "1675840",
    "end": "1681679"
  },
  {
    "text": "are used to support recovery time objectives and the lessons we learned are as",
    "start": "1681679",
    "end": "1687240"
  },
  {
    "text": "follows first focus on recovery rather than backups while backups are important",
    "start": "1687240",
    "end": "1693159"
  },
  {
    "text": "they only matter if they support your recovery objectives next a robust ER solution can",
    "start": "1693159",
    "end": "1699440"
  },
  {
    "text": "enable data Mobility by having backups in the places you need them including",
    "start": "1699440",
    "end": "1704559"
  },
  {
    "text": "across cloud and kubernetes cluster boundaries not only only do you add redundancy to your backups but you can",
    "start": "1704559",
    "end": "1710760"
  },
  {
    "text": "provision the databases you need where you need them and finally use postgres native",
    "start": "1710760",
    "end": "1717440"
  },
  {
    "text": "solutions to to safely utilize kubernetes native solutions for us the combination of PG",
    "start": "1717440",
    "end": "1723760"
  },
  {
    "text": "backr and kubernetes volume snapshots allows us to create snapshots that are free of corruption and other issues so",
    "start": "1723760",
    "end": "1731279"
  },
  {
    "text": "we learned to pay attention to all the communities in which we operate when establishing a Dr solution for pigo",
    "start": "1731279",
    "end": "1739158"
  },
  {
    "text": "okay so as I wrap things up this morning I'd like to conclude with just a few final thoughts around operator",
    "start": "1742919",
    "end": "1750399"
  },
  {
    "text": "development many of you might have contemplated creating an operator for your own application while there isn't an easy",
    "start": "1751200",
    "end": "1757799"
  },
  {
    "text": "answer as to whether an operator is right for you I can say that not that an operator is a great fit for",
    "start": "1757799",
    "end": "1764440"
  },
  {
    "text": "postgres if you're struggling with managing the complexity of your appli in some of the ways I discuss today or",
    "start": "1764440",
    "end": "1771440"
  },
  {
    "text": "if that complexity is resulting in a frustrating experience for your users then an operator is worth",
    "start": "1771440",
    "end": "1778399"
  },
  {
    "text": "considering and since operators are comprised of the same building blocks as the core kubernetes apis they also",
    "start": "1778399",
    "end": "1785320"
  },
  {
    "text": "provide practical experience and knowledge that can be applied to the development of kubernetes",
    "start": "1785320",
    "end": "1791080"
  },
  {
    "text": "itself to provide an example this knowledge helped crunchy data participate in a community solution for",
    "start": "1791080",
    "end": "1797080"
  },
  {
    "text": "using huge pages is when running Post cres in",
    "start": "1797080",
    "end": "1801398"
  },
  {
    "text": "kubernetes so as I conclude today I will end with a final timeline showing just",
    "start": "1804600",
    "end": "1809720"
  },
  {
    "text": "how far things have come within kubernetes since 2017 especially as pertains to operator",
    "start": "1809720",
    "end": "1815880"
  },
  {
    "text": "development kubernetes is now a mature and stable platform for all types of applications stateful applications",
    "start": "1815880",
    "end": "1822640"
  },
  {
    "text": "included and I truly believe there's never been a time a better time to build an operator",
    "start": "1822640",
    "end": "1829720"
  },
  {
    "text": "all right so thank you so much for joining me this morning and please come find me at the crunchy data Booth to continue the conversation regarding any",
    "start": "1831159",
    "end": "1837679"
  },
  {
    "text": "of the topics covered today I'd love to hear about your own operator experiences and the lessons you've learned building",
    "start": "1837679",
    "end": "1843799"
  },
  {
    "text": "your own operator solution so thanks again and I hope everyone enjoys the rest of your time here in Salt Lake City",
    "start": "1843799",
    "end": "1850240"
  },
  {
    "text": "and with the time we do have left I'm I'm happy to field um some questions if there are any",
    "start": "1850240",
    "end": "1857440"
  },
  {
    "text": "one two okay thank you for the talk that was a very interesting pogress talk um",
    "start": "1863120",
    "end": "1870120"
  },
  {
    "text": "do you have anything to show about the operator which SDK you used uh what",
    "start": "1870120",
    "end": "1875519"
  },
  {
    "text": "challenges you faced between when you started the project a long time ago and nowadays what Chang in the operator",
    "start": "1875519",
    "end": "1884000"
  },
  {
    "text": "Frameworks yeah I mean a lot has evolved in that sense since the those early days I mean I can still remember coming",
    "start": "1884000",
    "end": "1890760"
  },
  {
    "text": "aboard crunchy data to work on our operator back in 2017 with just a blog post or two I think a section of a book",
    "start": "1890760",
    "end": "1897559"
  },
  {
    "text": "that helped me get started you know again a lot of the Frameworks I think most of us are very familiar with for",
    "start": "1897559",
    "end": "1902840"
  },
  {
    "text": "operator development these days you know Q Builder operator SD the operator SDK",
    "start": "1902840",
    "end": "1909279"
  },
  {
    "text": "um were still in their infancy or didn't even exist so um a lot has changed in that regard you know I mean today there",
    "start": "1909279",
    "end": "1916200"
  },
  {
    "text": "is just such a wealth of knowledge around operator development in general um including within the kubernetes",
    "start": "1916200",
    "end": "1922120"
  },
  {
    "text": "documentation itself countless blog posts there's mature Frameworks it's just so easy to get started today comp",
    "start": "1922120",
    "end": "1929720"
  },
  {
    "text": "excuse me compared to how it was you know back in 2017 so um so yeah not to",
    "start": "1929720",
    "end": "1935159"
  },
  {
    "text": "mention just how many operators are out there in the wild these days so if you're looking for a reference you know",
    "start": "1935159",
    "end": "1940760"
  },
  {
    "text": "operator for some sort of application you're creating whether it's stateless stateful whatever it might be chances",
    "start": "1940760",
    "end": "1946120"
  },
  {
    "text": "are there's another operator out there that's similar that you can reference so um it's it's like night and day as far",
    "start": "1946120",
    "end": "1953240"
  },
  {
    "text": "as I'm concerned you know kind of having seen those early days of operator development to where we're at today and",
    "start": "1953240",
    "end": "1958840"
  },
  {
    "text": "again that's why I think it's such a great time now to get involved in operator development because there is",
    "start": "1958840",
    "end": "1964240"
  },
  {
    "text": "just such a a wealth of knowledge out there it's such an healthy ecosystem great Frameworks great knowledge great",
    "start": "1964240",
    "end": "1970760"
  },
  {
    "text": "projects out there um so yeah just some great Evolutions in that area for sure since those early days yeah",
    "start": "1970760",
    "end": "1979840"
  },
  {
    "text": "anything else",
    "start": "1983120",
    "end": "1989080"
  },
  {
    "text": "sure I'm just going to add that the crunchy data booths at the pH at the right hand side of the um Expo area if",
    "start": "1989080",
    "end": "1997840"
  },
  {
    "text": "you you're walking in you go to the right and go to the back so come talk to us but I've talking about post",
    "start": "1997840",
    "end": "2004960"
  },
  {
    "text": "chis thank you for your talk was really really interesting in the first demo you",
    "start": "2006320",
    "end": "2011639"
  },
  {
    "text": "showed us that you are able to do an automatic fail over even when the operator is down that's a great",
    "start": "2011639",
    "end": "2017519"
  },
  {
    "text": "technical achievement in my opinion and later you said that operator has to",
    "start": "2017519",
    "end": "2023039"
  },
  {
    "text": "be intended as just as part as the cord kubernetes API so my question is which",
    "start": "2023039",
    "end": "2030880"
  },
  {
    "text": "scenario are you thinking about why the operator should be down at the same time",
    "start": "2030880",
    "end": "2036840"
  },
  {
    "text": "where an automatic fail over will happen I'm sorry I had a little trouble",
    "start": "2036840",
    "end": "2042080"
  },
  {
    "text": "catching that oh sorry I was saying why the operator should be down at the same",
    "start": "2042080",
    "end": "2047599"
  },
  {
    "text": "time when an automatic failover should happen which is the scenario you are",
    "start": "2047599",
    "end": "2053118"
  },
  {
    "text": "thinking about did you catch that I apologize",
    "start": "2053119",
    "end": "2060040"
  },
  {
    "text": "have yeah",
    "start": "2060040",
    "end": "2065679"
  },
  {
    "text": "dater yeah Master op yeah both at the same time oh if like",
    "start": "2066560",
    "end": "2074720"
  },
  {
    "text": "what if what if both are lost at the same time so so with Petron you know",
    "start": "2074720",
    "end": "2079800"
  },
  {
    "text": "Patron can actually continue to operate even if you lose access to the kubernetes API temporarily so Petron has",
    "start": "2079800",
    "end": "2087320"
  },
  {
    "text": "a a very cool feature in it called fail safe mode um it's something we do see",
    "start": "2087320",
    "end": "2092878"
  },
  {
    "text": "you know as you know um we work with a lot of different customers a lot of different kubernetes distri",
    "start": "2092879",
    "end": "2098920"
  },
  {
    "text": "distributions and you know you did see that kubernetes is using that endpoints resource right to manage leader election",
    "start": "2098920",
    "end": "2105359"
  },
  {
    "text": "of course if etcd is unhealthy or the kubernetes API is unhealthy that could be problematic for for uh for even",
    "start": "2105359",
    "end": "2112040"
  },
  {
    "text": "Petron to do what it needs to do um but there is another feature within Patron they call it fail safe mode because you",
    "start": "2112040",
    "end": "2119119"
  },
  {
    "text": "didn't see it in these demos directly but the different patronies running in all your postres instance pods they also",
    "start": "2119119",
    "end": "2125720"
  },
  {
    "text": "have their own rest apis where they're talking to each other over the network so even if there is temporary loss of",
    "start": "2125720",
    "end": "2131839"
  },
  {
    "text": "access to the kubernetes API it has the ability to reach out talk to the other",
    "start": "2131839",
    "end": "2136880"
  },
  {
    "text": "patronies and continue to operate You know despite that temporary loss to the kubernetes API so um yeah it's one of",
    "start": "2136880",
    "end": "2144480"
  },
  {
    "text": "those things we're always you know you know working with clusters or our customers to try to make sure you know their kubernetes environments are",
    "start": "2144480",
    "end": "2150920"
  },
  {
    "text": "healthy so we don't have to rely on that but there is kind of a again a fail safe mechanism in there to prevent that even",
    "start": "2150920",
    "end": "2157640"
  },
  {
    "text": "even in the event that you do lose your your kubernetes API temporarily yeah okay see thank you sure sure",
    "start": "2157640",
    "end": "2164319"
  },
  {
    "text": "thing yeah I have a question about upgrades so I saw basically you showed",
    "start": "2164319",
    "end": "2169920"
  },
  {
    "text": "how upgrade major upgrades are done and I see it primarily as an imperative way",
    "start": "2169920",
    "end": "2176280"
  },
  {
    "text": "of doing upgrades is there a declarative way of doing major upgrades and uh in",
    "start": "2176280",
    "end": "2182839"
  },
  {
    "text": "that case how roll back for example can be handled I'm so I didn't",
    "start": "2182839",
    "end": "2188480"
  },
  {
    "text": "quite I was having a little trouble hearing I apologize no probably it's because of my",
    "start": "2188480",
    "end": "2194079"
  },
  {
    "text": "language is I was saying that you show um measor upgrades but I saw primarily",
    "start": "2194079",
    "end": "2200720"
  },
  {
    "text": "there were um you show an imperative way of major upgrades for postgis so I'm",
    "start": "2200720",
    "end": "2208040"
  },
  {
    "text": "asking if there's a declarative way uh because I we develop a another operator",
    "start": "2208040",
    "end": "2213680"
  },
  {
    "text": "and the challenge is to do it in a declarative way if you have addressed that and in how do you see for example",
    "start": "2213680",
    "end": "2221119"
  },
  {
    "text": "roll back to be handled in case the major upgrade fails for example I'm",
    "start": "2221119",
    "end": "2226720"
  },
  {
    "text": "thinking of extensions like postgis or similar we're relying on the the r drr",
    "start": "2226720",
    "end": "2233200"
  },
  {
    "text": "solution in that scenario so we do recommend the user creates a backup before they ever um initiate the major",
    "start": "2233200",
    "end": "2240319"
  },
  {
    "text": "upgrade process um those status and conditions will tell you as you're navigating through the major upgrade if",
    "start": "2240319",
    "end": "2246800"
  },
  {
    "text": "something does go go wrong uh so if you do end up in a bad State our our",
    "start": "2246800",
    "end": "2251839"
  },
  {
    "text": "solution there is to use your disaster recovery process use that backup that we",
    "start": "2251839",
    "end": "2256960"
  },
  {
    "text": "did tell you to I didn't show that in the demo specifically but you might have noticed that that was the first step",
    "start": "2256960",
    "end": "2262160"
  },
  {
    "text": "when I was highlighting our major upgrade process was to create a backup and that is the solution in that",
    "start": "2262160",
    "end": "2267960"
  },
  {
    "text": "scenario so if something does go wrong we want to make sure you can revert back to where you were um in that in that",
    "start": "2267960",
    "end": "2274240"
  },
  {
    "text": "scenario yeah but what's your experience for example in case of thousand of postest clusters to be",
    "start": "2274240",
    "end": "2280800"
  },
  {
    "text": "upgraded in terms of major version how you know how do you cope",
    "start": "2280800",
    "end": "2286520"
  },
  {
    "text": "with that scale for example I mean based on your experience gotcha you know I mean most of our experience with",
    "start": "2286520",
    "end": "2292920"
  },
  {
    "text": "customers you know I think we're not typically seeing that many major major upgrades occurring concurrently at the",
    "start": "2292920",
    "end": "2299400"
  },
  {
    "text": "same time you know it's it's a lot less often that these things occur so it is something that's usually a little more",
    "start": "2299400",
    "end": "2306040"
  },
  {
    "text": "tightly controlled schedule manage that sort of thing um but I will say we are seeing customers who are starting to",
    "start": "2306040",
    "end": "2312599"
  },
  {
    "text": "leverage uh our API to do things in in terms of major upgrades at larger scale",
    "start": "2312599",
    "end": "2318760"
  },
  {
    "text": "um because what those status and conditions allow you to do if you are comfortable you know with performing a",
    "start": "2318760",
    "end": "2324560"
  },
  {
    "text": "major upgrade you're comfortable with managing mitigating the risks you know what extensions you have there you know",
    "start": "2324560",
    "end": "2330160"
  },
  {
    "text": "it's going to work you can fully orchestrate um that process because you know you're basically just waiting for",
    "start": "2330160",
    "end": "2336480"
  },
  {
    "text": "certain status and conditions to occur to tell you what to do next so what we're seeing a lot of our customers",
    "start": "2336480",
    "end": "2341880"
  },
  {
    "text": "starting to do now is they get more comfortable with major upgrades and honestly I think the major upgrade API",
    "start": "2341880",
    "end": "2348599"
  },
  {
    "text": "in general is allowing our our customers and users to get more comfortable with the process in general so as they go",
    "start": "2348599",
    "end": "2354960"
  },
  {
    "text": "through this with a few clusters and realize you know that they're comfortable mitigating those risks um",
    "start": "2354960",
    "end": "2360720"
  },
  {
    "text": "they are starting to automate more and more of that so we are see seeing customers who are writing um kind of",
    "start": "2360720",
    "end": "2366000"
  },
  {
    "text": "their you could call it a script whatever you want around our API um that basically is watching as the API you",
    "start": "2366000",
    "end": "2372440"
  },
  {
    "text": "know gives you those different status and conditions and then triggering the next phase of the upgrade process and",
    "start": "2372440",
    "end": "2378200"
  },
  {
    "text": "because of that we are now seeing more and more customers starting to do more more major upgrades more regularly at a",
    "start": "2378200",
    "end": "2385000"
  },
  {
    "text": "larger scale so um you know I think a lot of what this API is providing users",
    "start": "2385000",
    "end": "2390359"
  },
  {
    "text": "I think I mentioned the word empowering right it gives them visibility into the upgrade process it allows them to",
    "start": "2390359",
    "end": "2396720"
  },
  {
    "text": "understand the risk associated with it how to come back from something if something doesn't go wrong but as they",
    "start": "2396720",
    "end": "2402720"
  },
  {
    "text": "get more comfortable with it you know they can automate more and more um and do it at a larger scale yeah yeah okay",
    "start": "2402720",
    "end": "2410160"
  },
  {
    "text": "thank you thanks for the talk yeah sure thing thank",
    "start": "2410160",
    "end": "2414480"
  },
  {
    "text": "you hey um I just want to say uh I thought the demos were a really great way to demonstrate like um in a small",
    "start": "2415480",
    "end": "2422960"
  },
  {
    "text": "package exactly what could go wrong and how it's uh recovering from that um I'm just wondering if you have like",
    "start": "2422960",
    "end": "2428240"
  },
  {
    "text": "particular strategies used for automated testing to make sure that like the operator is working um not only for like",
    "start": "2428240",
    "end": "2434200"
  },
  {
    "text": "the parts implemented but also the fact that it's like interoperating with um a bunch of other Solutions gotcha you're just asking",
    "start": "2434200",
    "end": "2441319"
  },
  {
    "text": "about test aut automating testing around yeah just like what test strategies are you using what I'm sorry what what test",
    "start": "2441319",
    "end": "2448560"
  },
  {
    "text": "strategies are you using oh just to test our own operator itself you know like in",
    "start": "2448560",
    "end": "2453720"
  },
  {
    "text": "terms of development work we're doing and things like that right we're using a few different Frameworks uh to do",
    "start": "2453720",
    "end": "2459119"
  },
  {
    "text": "automated testing um one of the big projects we're using is something called cuddle which you might be familiar with",
    "start": "2459119",
    "end": "2465800"
  },
  {
    "text": "um which is a um it's a framework that allows you to declaratively you know def",
    "start": "2465800",
    "end": "2471079"
  },
  {
    "text": "Define what you want the API to produce and then kind of assert that what what is created um represents that um so that",
    "start": "2471079",
    "end": "2479240"
  },
  {
    "text": "is a big part of our testing strategy these days so I I highly encourage everyone to check out that project if",
    "start": "2479240",
    "end": "2484760"
  },
  {
    "text": "you're interested in in testing your operator um we do use the controller runtime project and within that we're",
    "start": "2484760",
    "end": "2491000"
  },
  {
    "text": "using some of the the testing capabilities baked into that as well firing up fake API servers and things",
    "start": "2491000",
    "end": "2497680"
  },
  {
    "text": "like that just to you know make sure hey when someone spits you know gives us a spec for a postgres cluster we're",
    "start": "2497680",
    "end": "2504000"
  },
  {
    "text": "generating all the resources that are expected to be generated as a result of",
    "start": "2504000",
    "end": "2509440"
  },
  {
    "text": "that you know you can imagine one postgres cluster spec that gets generated um could result in multiple",
    "start": "2509440",
    "end": "2515440"
  },
  {
    "text": "stateful sets config Maps Secrets Services you know a whole host of different resources so I'd say between",
    "start": "2515440",
    "end": "2522119"
  },
  {
    "text": "those two strategies you know with the controller runtime and and then cuddle um that's where we're capturing you know",
    "start": "2522119",
    "end": "2528599"
  },
  {
    "text": "that's that's what allows us to ex exercise a lot of that functionality um",
    "start": "2528599",
    "end": "2533640"
  },
  {
    "text": "yeah to verify you know the operator is you know reconciling what it should yeah",
    "start": "2533640",
    "end": "2539200"
  },
  {
    "text": "awesome thank you sure thing all right",
    "start": "2539200",
    "end": "2547880"
  },
  {
    "text": "I think that's it all right thank you so much I appreciate it thank you",
    "start": "2547880",
    "end": "2554599"
  }
]