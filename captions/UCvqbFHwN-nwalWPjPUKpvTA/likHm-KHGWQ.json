[
  {
    "start": "0",
    "end": "118000"
  },
  {
    "text": "good afternoon thanks for joining my name is Suresh and era of engineering",
    "start": "30",
    "end": "6359"
  },
  {
    "text": "at what and this is under Kumar he's Michael a part of my team",
    "start": "6359",
    "end": "12330"
  },
  {
    "text": "how many of you know about what what is what good someone is facing my hand",
    "start": "12330",
    "end": "19109"
  },
  {
    "text": "happy about that so what is a entity which was formed after Verizon what a",
    "start": "19109",
    "end": "25350"
  },
  {
    "text": "while Yahoo and bunch of other companies and merged into one unit called oath so",
    "start": "25350",
    "end": "30720"
  },
  {
    "text": "technically I'm an exe hago now today",
    "start": "30720",
    "end": "37760"
  },
  {
    "text": "we'll go through our experience we have been running kubernetes for almost two years now we will share our experience",
    "start": "37760",
    "end": "44040"
  },
  {
    "text": "what are the outages issue which we run into and then what's a lesson we learned",
    "start": "44040",
    "end": "49200"
  },
  {
    "text": "out of it so I'm part of the core platform team I lead the kubernetes effort at what most of media products",
    "start": "49200",
    "end": "57180"
  },
  {
    "text": "such as sports finance and you can talk homepage all these stacks are currently sold out of kubernetes okay if you",
    "start": "57180",
    "end": "64290"
  },
  {
    "text": "access yahoo.com page or sports page it's completely sold out of kubernetes tack today so my team manages the",
    "start": "64290",
    "end": "71790"
  },
  {
    "text": "kubernetes cluster and the tools are on kubernetes cluster not only this we do we do have other tools which kind of",
    "start": "71790",
    "end": "79500"
  },
  {
    "text": "helps improve the developer productivity and velocity of release we do bunch of things which we call the platform team",
    "start": "79500",
    "end": "85259"
  },
  {
    "text": "not just kubernetes this is my team you",
    "start": "85259",
    "end": "91079"
  },
  {
    "text": "guys see the flag kubernetes flag we really love kubernetes I cannot explain",
    "start": "91079",
    "end": "96780"
  },
  {
    "text": "more than this flag right bunch of guys are missing here but pretty much anti team is here and you can see in here we",
    "start": "96780",
    "end": "104130"
  },
  {
    "text": "have a fan a dashboard update so we extensively use graph and primitives for monitoring tomorrow we have a talk about",
    "start": "104130",
    "end": "112140"
  },
  {
    "text": "how we leverage the primitives for monitoring or 2:45 please to join the session okay where are the Chile right",
    "start": "112140",
    "end": "121560"
  },
  {
    "start": "118000",
    "end": "187000"
  },
  {
    "text": "we are in good production like most of our applications are running in",
    "start": "121560",
    "end": "127560"
  },
  {
    "text": "production with communities today we have 12 clusters production grade cluster distributed across six",
    "start": "127560",
    "end": "133440"
  },
  {
    "text": "regions and we have any caste routing which routes a road user to the process",
    "start": "133440",
    "end": "139230"
  },
  {
    "text": "data center based on where he comes from and we have 12 to k plus welcome nodes",
    "start": "139230",
    "end": "144390"
  },
  {
    "text": "in a cluster and 20k parts 50k containers running out there and if we",
    "start": "144390",
    "end": "149640"
  },
  {
    "text": "have 200 place application running here and at the peak pretty much we do is folder decay RPS made this particular",
    "start": "149640",
    "end": "156690"
  },
  {
    "text": "stack does and we do believe most of the application which deployed today is faithless when I mean stateless it's",
    "start": "156690",
    "end": "162930"
  },
  {
    "text": "it's not storing any state 7 its access yahoo.com page there's no state stored",
    "start": "162930",
    "end": "168420"
  },
  {
    "text": "there right and we are exploring stateful applications such as Redis cluster ring and Cassandra as we speak",
    "start": "168420",
    "end": "174810"
  },
  {
    "text": "and we will definitely share more in upcoming cube con conference we are building an operator where this operator",
    "start": "174810",
    "end": "180750"
  },
  {
    "text": "which tries to manage the register for you for all stateful workloads ok let's",
    "start": "180750",
    "end": "188010"
  },
  {
    "start": "187000",
    "end": "403000"
  },
  {
    "text": "go through a little architecture how this looks like at work today if you look at your right hand side how your",
    "start": "188010",
    "end": "195120"
  },
  {
    "text": "right hand sign right we have something but the orange boxes are the one which the component which we built for",
    "start": "195120",
    "end": "202980"
  },
  {
    "text": "bringing in kubernetes to it and the one the green are there existing both services which we live reached and the",
    "start": "202980",
    "end": "209760"
  },
  {
    "text": "other thing you pretty much know this is from kubernetes stack kublai the api",
    "start": "209760",
    "end": "214800"
  },
  {
    "text": "edulearn and things let's go through to our right we have custom ingress controller right so you pretty much know",
    "start": "214800",
    "end": "221459"
  },
  {
    "text": "in genetics and enjoy and things here we use Apache traffic server so apache",
    "start": "221459",
    "end": "226920"
  },
  {
    "text": "trail mix over is we have so much dependency on apache traffic server it used to be a go traffic server when it's",
    "start": "226920",
    "end": "232950"
  },
  {
    "text": "open sourced and apache license it become apache traffic circles we have lots of plugin which we have dependency",
    "start": "232950",
    "end": "238140"
  },
  {
    "text": "on traffic server so we have to build our color controller english controller",
    "start": "238140",
    "end": "243209"
  },
  {
    "text": "based on opposite effects allah over here so you can see this there's an agent running in the controller what is",
    "start": "243209",
    "end": "251190"
  },
  {
    "text": "there to do is it like to look for the kubernetes api it hooks to kubernetes api looks for the endpoints gets those",
    "start": "251190",
    "end": "257370"
  },
  {
    "text": "IPS for the origin for the where you are throughout the wreckage - and pretty much dynamically loaded into that ATS it",
    "start": "257370",
    "end": "263430"
  },
  {
    "text": "does all in memory so you can think of here like we have further decay RPS coming in and",
    "start": "263430",
    "end": "269360"
  },
  {
    "text": "there are parts getting had that removed deployments are happening that IPS are getting changed we need to seamlessly lower this",
    "start": "269360",
    "end": "275539"
  },
  {
    "text": "information to the ingress layer we cannot reload harvest at the ingress was",
    "start": "275539",
    "end": "280940"
  },
  {
    "text": "pretty much it will it will kind of impact your life traffic energy a long",
    "start": "280940",
    "end": "286009"
  },
  {
    "text": "tail latency will go up so we pretty much build this ingress such a way that we do in memory so there's no impact to",
    "start": "286009",
    "end": "291680"
  },
  {
    "text": "the live traffic so that's on the ingress controller which you have built and if you look at your left over here",
    "start": "291680",
    "end": "298250"
  },
  {
    "text": "custom tool let's talk about screwdriver screwdriver is an open source project from Earth this is our CI CD platform so",
    "start": "298250",
    "end": "305719"
  },
  {
    "text": "we build custom tools on top of screwdriver what this custom tool does right when we want to bring kubernetes",
    "start": "305719",
    "end": "311120"
  },
  {
    "text": "to earth right we don't want every engineer to completely understand in their kubernetes spec so we provided a",
    "start": "311120",
    "end": "317569"
  },
  {
    "text": "simple interface a single UML file for end-users so in that AML file he pretty",
    "start": "317569",
    "end": "322639"
  },
  {
    "text": "much says that I want to apply this particular image that's it right and then some same value in terms of CPU and",
    "start": "322639",
    "end": "327740"
  },
  {
    "text": "memory and what we do we have our own templating engine we take that file a",
    "start": "327740",
    "end": "333740"
  },
  {
    "text": "simple interface and we convert into a bunch of common additional files that",
    "start": "333740",
    "end": "338810"
  },
  {
    "text": "much of kubernetes KML files like in service deployment and then we natively apply using cube cutter to any cluster",
    "start": "338810",
    "end": "345169"
  },
  {
    "text": "which we built that is the customs tool which we built this enabled us to rapid onboarding to kubernetes because the",
    "start": "345169",
    "end": "351740"
  },
  {
    "text": "interface was very simple the engineer could understand the simple interface rather than forcing an engineer to understand every aspect of kubernetes",
    "start": "351740",
    "end": "358580"
  },
  {
    "text": "and all the resource of communities and while we're here we build an earth hook this is our authentication operation",
    "start": "358580",
    "end": "365599"
  },
  {
    "text": "hook it is integrated with Athens is pretty much open sourced as well and you can find it out",
    "start": "365599",
    "end": "371840"
  },
  {
    "text": "Nathan's is similar to I am an item Leah's fault so this who pretty much when you exude any command against the",
    "start": "371840",
    "end": "378620"
  },
  {
    "text": "namespace in use or that means it checks whether you are authenticated have you seen a valid we have our own attenuation",
    "start": "378620",
    "end": "385069"
  },
  {
    "text": "mechanism and it's checks again whether you are authorized to do that action on that particular resource on that",
    "start": "385069",
    "end": "390710"
  },
  {
    "text": "particular namespace this level of ISIL a chronology of policy which you can define in Athens so these are pretty",
    "start": "390710",
    "end": "398360"
  },
  {
    "text": "much common which we built and leverage to get kubernetes into what so let's go",
    "start": "398360",
    "end": "405230"
  },
  {
    "start": "403000",
    "end": "447000"
  },
  {
    "text": "through how do we build pipeline right so our base kubernetes cluster itself right it's built using Jeff so I was",
    "start": "405230",
    "end": "412820"
  },
  {
    "text": "remaining everything if I can add ons and application developer who's deploying their application they use",
    "start": "412820",
    "end": "418190"
  },
  {
    "text": "screwdriver and the custom tools but the base kubernetes cluster is used build using Jeff so if we pretty much run the",
    "start": "418190",
    "end": "425240"
  },
  {
    "text": "writer Jeff Wenberg and we've pushed the changes to stage be baked in there and",
    "start": "425240",
    "end": "430700"
  },
  {
    "text": "then we push the changes to Kennedy when Kennedy we push the change to production it's like if you need anything to",
    "start": "430700",
    "end": "436910"
  },
  {
    "text": "override Pacific environment you can go to an update in that particular environment the override files so this",
    "start": "436910",
    "end": "442040"
  },
  {
    "text": "is how we change the kubernetes changes the criminals based community changes okay we are operating on prompt right",
    "start": "442040",
    "end": "451790"
  },
  {
    "text": "Javier anak in decals are aks so this pretty much my team managing this covenant is awesome we all love it right",
    "start": "451790",
    "end": "458900"
  },
  {
    "text": "I on fact I always said I really kissed kubernetes all the day right so but",
    "start": "458900",
    "end": "465530"
  },
  {
    "text": "there are many interdependent components it's which is very hard right now when you try to operate kubernetes on your",
    "start": "465530",
    "end": "470720"
  },
  {
    "text": "own at the scale right you can talk about etcd controllers edge Euler API cube blade Tucker runtimes",
    "start": "470720",
    "end": "477050"
  },
  {
    "text": "what are not now there are so many control loops which you need to understand when you tries to operate that okay then when we try to manage and",
    "start": "477050",
    "end": "484160"
  },
  {
    "text": "we try to scale we ran into many issues it could be human error governor this bug then how did we fail and how did we",
    "start": "484160",
    "end": "490880"
  },
  {
    "text": "recover we will go through that now one of the first human error right namespace",
    "start": "490880",
    "end": "499820"
  },
  {
    "start": "495000",
    "end": "531000"
  },
  {
    "text": "right I told this is a single cluster right this is soft multi tennis if you say when I say soft multi-tenancy today",
    "start": "499820",
    "end": "506030"
  },
  {
    "text": "it's like there's not very this is a team of bigger team right every team",
    "start": "506030",
    "end": "511100"
  },
  {
    "text": "gets their own name so if they don't get a separate cluster they get their own namespace for them to operate right so",
    "start": "511100",
    "end": "516950"
  },
  {
    "text": "only it is sensitive workloads then we have a separate cluster for them if not it's pretty much they share the",
    "start": "516950",
    "end": "522320"
  },
  {
    "text": "underlying resource with the namespace isolation right so in a given name space you have 100 hundred plus deployments",
    "start": "522320",
    "end": "529040"
  },
  {
    "text": "going on so what is this command say Gibson give CTL",
    "start": "529040",
    "end": "535230"
  },
  {
    "start": "531000",
    "end": "586000"
  },
  {
    "text": "delete as an F directory you are deleting pretty much whatever there in that particular directory all the resource so one of the cluster I'd mean",
    "start": "535230",
    "end": "542759"
  },
  {
    "text": "he was trying to operate against non production environment his context was set to non production ok and then he he",
    "start": "542759",
    "end": "550829"
  },
  {
    "text": "he was thinking that it was said to be non production and he tried to delete this directory right and sorry that he",
    "start": "550829",
    "end": "558240"
  },
  {
    "text": "was pointing to production and he executed this comment against our production system right in this case one",
    "start": "558240",
    "end": "564480"
  },
  {
    "text": "of the file in that directory was name space so you pretty much deleted a namespace which had 100 plus deployment",
    "start": "564480",
    "end": "571560"
  },
  {
    "text": "when you deploy when you delete a namespace what happens you guys know",
    "start": "571560",
    "end": "579690"
  },
  {
    "text": "that is pretty much it wipes out what it's trying to do it's like ok yeah here",
    "start": "579690",
    "end": "588029"
  },
  {
    "start": "586000",
    "end": "627000"
  },
  {
    "text": "you go it has a cascading impact like what happened was like all the deployment and part everything within",
    "start": "588029",
    "end": "594029"
  },
  {
    "text": "that namespace came down like everything was deleted right and there is no way",
    "start": "594029",
    "end": "599190"
  },
  {
    "text": "for us to stop that because the command has been executed right and now like hey",
    "start": "599190",
    "end": "605639"
  },
  {
    "text": "how do we recover that is the impact has been done this is like you executed the command the deletion is happening for",
    "start": "605639",
    "end": "611130"
  },
  {
    "text": "the pause and deployment and all the resource within that namespace and then the namespace finalize our weekly in",
    "start": "611130",
    "end": "616139"
  },
  {
    "text": "that namespace right no nothing we can be done it's kind of complete outage what that particular name says which was",
    "start": "616139",
    "end": "622230"
  },
  {
    "text": "serving a critical workload around that particular data center it was complete out a shortage right how did we recover",
    "start": "622230",
    "end": "627959"
  },
  {
    "start": "627000",
    "end": "731000"
  },
  {
    "text": "ok we do as I was mentioning we failed out that particular data center so the",
    "start": "627959",
    "end": "633029"
  },
  {
    "text": "user can go to the next data center available it's easy quick way to do that and then we worked with our tenant",
    "start": "633029",
    "end": "638699"
  },
  {
    "text": "saying that hey we apologize how we made this mistake we deleted your name space and your deployment can you please read",
    "start": "638699",
    "end": "644970"
  },
  {
    "text": "trigger your deployment and they went and retrigger their deployment and the things came in right but if the IRB's",
    "start": "644970",
    "end": "650970"
  },
  {
    "text": "are like hey this accident could happen how do we prevent it from happening right so we have admission control in",
    "start": "650970",
    "end": "659100"
  },
  {
    "text": "kubernetes we have the admission controller right it's pretty much you can validate there are two types of",
    "start": "659100",
    "end": "664290"
  },
  {
    "text": "mutating and validating right you pretty much validate an object before you take any action on that particular resource so in",
    "start": "664290",
    "end": "672270"
  },
  {
    "text": "this case we built this an admission control and what this admission controller does is pretty much if you",
    "start": "672270",
    "end": "677340"
  },
  {
    "text": "run an operation delete on the resource namespace that object call is passed through this admission controller what",
    "start": "677340",
    "end": "685890"
  },
  {
    "text": "this guy's do primarily is it will check if you have any other resources within that particular name space if you have",
    "start": "685890",
    "end": "692970"
  },
  {
    "text": "part or if you have deployment if you have stateful set ingress it'll try to prevent you from deleting the namespace",
    "start": "692970",
    "end": "698910"
  },
  {
    "text": "it will say you please go ahead and delete all those resource and then come back and delete your namespace then we know that this action has been",
    "start": "698910",
    "end": "705180"
  },
  {
    "text": "deliberately taken by the users it's not an accident okay this is one way we prevented this kind",
    "start": "705180",
    "end": "711060"
  },
  {
    "text": "of accident happening and again we also provide administrator yeah key only they",
    "start": "711060",
    "end": "717360"
  },
  {
    "text": "can force delete it so that we're not forcing all the tenants so if you have a special key which we know of if we",
    "start": "717360",
    "end": "722910"
  },
  {
    "text": "passed that generally this hook will not be executed this for our convenience in case we run into any kind of corner",
    "start": "722910",
    "end": "728190"
  },
  {
    "text": "cases the next issue which we run into okay I said we had a single ingress",
    "start": "728190",
    "end": "736860"
  },
  {
    "start": "731000",
    "end": "766000"
  },
  {
    "text": "controller right today so we made everything self-serving right everyone just comes in creates that single",
    "start": "736860",
    "end": "742920"
  },
  {
    "text": "interface EML file we convert that into kubernetes file and we apply the change",
    "start": "742920",
    "end": "748620"
  },
  {
    "text": "machine cube cydia right give cutter so ingress resource is used for you to say that hey onboarding to kubernetes saying",
    "start": "748620",
    "end": "755370"
  },
  {
    "text": "i with this domain name or hostname you can call that sports.com right and then this is the back end I need to route to",
    "start": "755370",
    "end": "761370"
  },
  {
    "text": "right this is how you on board to kubernetes and create a ingress resource you can think of this one",
    "start": "761370",
    "end": "767430"
  },
  {
    "start": "766000",
    "end": "829000"
  },
  {
    "text": "what's wrong in this particular things right so one of the effective tool in",
    "start": "767430",
    "end": "772589"
  },
  {
    "text": "engineering world is copy paste right and clone right in this case what",
    "start": "772589",
    "end": "778980"
  },
  {
    "text": "happened was there's two ingress was created by two different tenants okay and they were pointing to different",
    "start": "778980",
    "end": "785010"
  },
  {
    "text": "back-end this is app 2 and that is app 1 but they claim the same alias which is",
    "start": "785010",
    "end": "790470"
  },
  {
    "text": "the same host name right so now what happens is you pretty much took someone else domain name right and pointing to",
    "start": "790470",
    "end": "797370"
  },
  {
    "text": "two different back-end so what happens during this case if you of the ingress controller right now when",
    "start": "797370",
    "end": "802540"
  },
  {
    "text": "he tries to get their requests and then it has to back end for the same domain right it will try to round-robin between",
    "start": "802540",
    "end": "807910"
  },
  {
    "text": "those two back ends right so you can think that I'm accessing sports.com I'm",
    "start": "807910",
    "end": "813190"
  },
  {
    "text": "getting Finance content because it's pointed to the finance back-end and if i refresh it next time you get a sports",
    "start": "813190",
    "end": "818380"
  },
  {
    "text": "content right so this is because of accidentally copy pasting or ruining someone's get repo and then trying to",
    "start": "818380",
    "end": "825250"
  },
  {
    "text": "extend API out of it right we would like to avoid this from happening and it's also very bad user experience so yeah",
    "start": "825250",
    "end": "833350"
  },
  {
    "start": "829000",
    "end": "894000"
  },
  {
    "text": "most of the investors round-robin so we build an admission controller which we called ingress claim right what what",
    "start": "833350",
    "end": "839589"
  },
  {
    "text": "this guys do is like if you domain name are a host name is being claimed by some other name space on a mother deployment",
    "start": "839589",
    "end": "846610"
  },
  {
    "text": "it will not allow you to claim it back it validates it and say that hey this particular domain name for examples for",
    "start": "846610",
    "end": "852640"
  },
  {
    "text": "Santiago that counts is being used by sports team okay you cannot claim that we try to avoid this we put a prevention",
    "start": "852640",
    "end": "860529"
  },
  {
    "text": "out of it right this is nice way for our wedding anyone from hijacking each other's domain okay scary moment so we do have",
    "start": "860529",
    "end": "867460"
  },
  {
    "text": "an experience they're like hey I'm accessing sports I'm singing all finance content right and then sick that's bad",
    "start": "867460",
    "end": "874920"
  },
  {
    "text": "so again the same thing whenever you create an ingress or source create our update and we pass that call to that to",
    "start": "874920",
    "end": "882160"
  },
  {
    "text": "the Ingram's claim admission controller very validate this and you're not copy-pasting are you're not duplicating",
    "start": "882160",
    "end": "888010"
  },
  {
    "text": "the same domain name and it denies in case you are claiming a domain which does not belong to you okay this is next",
    "start": "888010",
    "end": "895990"
  },
  {
    "text": "we kind of major things which we solved next thing this coconut is node right so",
    "start": "895990",
    "end": "901480"
  },
  {
    "text": "we are making changes we upgrade we started with 1.6 and then now we are in 1.9 1.5 we started pretty much now in",
    "start": "901480",
    "end": "908620"
  },
  {
    "text": "1.9 and so we did lots of upgrades so one of those changes are we were trying to make a change to cubelet and we're",
    "start": "908620",
    "end": "914920"
  },
  {
    "text": "trying to roll it out right and we're trying to roll that out and then we ran into this kubernetes bug so what has",
    "start": "914920",
    "end": "921520"
  },
  {
    "text": "happened is when the cubelet when is trying to restart it so it case a transient",
    "start": "921520",
    "end": "926620"
  },
  {
    "text": "C group file and it cleans up before it starts up right so this our clean way of studying for cubelet so because of this",
    "start": "926620",
    "end": "933430"
  },
  {
    "text": "particular bug in kubernetes the cube Latinas stack right and our rolling update of cubelet change",
    "start": "933430",
    "end": "940029"
  },
  {
    "text": "went across the fleet in one particular cluster so what happens when the cubelet is not coming up what's kubernetes api",
    "start": "940029",
    "end": "946870"
  },
  {
    "text": "thinks the notes pad the notes went down so in this case hunter pushed off the",
    "start": "946870",
    "end": "953290"
  },
  {
    "text": "node we're in a not ready state hi cool all the nodes are not ready that can't a",
    "start": "953290",
    "end": "960730"
  },
  {
    "text": "processing nodes are not ready yes but let's see what happened right you can",
    "start": "960730",
    "end": "965860"
  },
  {
    "text": "see this red line like it's create four things happening and this red line see that this number of nodes which are",
    "start": "965860",
    "end": "973180"
  },
  {
    "text": "getting into the eviction queue and getting being evicted this line grows up right and then you can see that everything keeps down and this guy shows",
    "start": "973180",
    "end": "979839"
  },
  {
    "text": "up it is scary very very scary right when a node when a node mr. Matt Lee",
    "start": "979839",
    "end": "986769"
  },
  {
    "text": "himself is not ready right what is the controller does the controller puts that",
    "start": "986769",
    "end": "991990"
  },
  {
    "text": "node into a eviction queue and it will tie to a weak start part from that particular node so that it can be",
    "start": "991990",
    "end": "997389"
  },
  {
    "text": "scheduled in other available nodes right so it started doing that we need to",
    "start": "997389",
    "end": "1002790"
  },
  {
    "text": "observe the node within that happen right this X that'll happen so this is",
    "start": "1002790",
    "end": "1008820"
  },
  {
    "text": "very cool feature in kubernetes I'm never sure how many of you know this particular feature it's not that much well documented - right so when the",
    "start": "1008820",
    "end": "1015990"
  },
  {
    "text": "controller observes it's 50% of the node in a cluster is not ready state it it",
    "start": "1015990",
    "end": "1022529"
  },
  {
    "text": "makes a smart decision it's just that it's something wrong with the cluster so I let me back off",
    "start": "1022529",
    "end": "1027780"
  },
  {
    "text": "right it backs it off and then it says that in the throttling rate in terms of eviction rate instead of doing every",
    "start": "1027780",
    "end": "1033298"
  },
  {
    "text": "seconds it will make every 15 seconds every 20 seconds it will try to slow down the eviction of",
    "start": "1033299",
    "end": "1038339"
  },
  {
    "text": "the part from the node right this prevents right we cannot evict everything right if we evict everything",
    "start": "1038339",
    "end": "1043530"
  },
  {
    "text": "across the cluster where will this go because all the nodes are not ready state everything will go in the pending",
    "start": "1043530",
    "end": "1048750"
  },
  {
    "text": "states right and fine we still had an impact like 50 percent of the north part",
    "start": "1048750",
    "end": "1055140"
  },
  {
    "text": "I could be a victor they're still getting scheduled in the other remaining set of course it's not scheduling it's",
    "start": "1055140",
    "end": "1060780"
  },
  {
    "text": "just codes on the pending state because there is no node is ready same time when",
    "start": "1060780",
    "end": "1065790"
  },
  {
    "text": "the controller observes that of the notes are down it enters into something called full disruption mode",
    "start": "1065790",
    "end": "1072000"
  },
  {
    "text": "right in this particular mode what is steady it will stop everything this is great this is what we want right the the",
    "start": "1072000",
    "end": "1079500"
  },
  {
    "text": "thing is the communication between cubelet and api is broken it's not the communication between the ingress to the",
    "start": "1079500",
    "end": "1085830"
  },
  {
    "text": "part right we still had that route working the part was up and running just think cubelet the process was down right",
    "start": "1085830",
    "end": "1092460"
  },
  {
    "text": "in this case we had a degraded experience because 50% of the pod cut the victor remaining 50% we were able to",
    "start": "1092460",
    "end": "1099230"
  },
  {
    "text": "sustain because kubernetes entered into this particular mode okay it's a pretty cool awesome feature in kubernetes it's",
    "start": "1099230",
    "end": "1106200"
  },
  {
    "text": "not well documented you can take a look at that ok what we did okay two quick",
    "start": "1106200",
    "end": "1114240"
  },
  {
    "start": "1112000",
    "end": "1242000"
  },
  {
    "text": "leave it to Rico because we had a degrade experience another completely down right because we have many data centers and many clusters we failed over",
    "start": "1114240",
    "end": "1120510"
  },
  {
    "text": "that particular data center and every user went to the next data center that's pretty cool it did it and we do update a",
    "start": "1120510",
    "end": "1127790"
  },
  {
    "text": "30% concurrence if you don't want to go across the fleet right we do that but what is the one of the lesson which we",
    "start": "1127790",
    "end": "1133170"
  },
  {
    "text": "learned out here is please do yeah test after upgrading cubelet right yes simple",
    "start": "1133170",
    "end": "1139830"
  },
  {
    "text": "curl command against cubelets api is sufficient okay we did not have that and",
    "start": "1139830",
    "end": "1145980"
  },
  {
    "text": "then what happened was like there's no functional test post upgrade it all came down right it's really good experience",
    "start": "1145980",
    "end": "1151890"
  },
  {
    "text": "we learned a lot during this experience and now we pretty much when the cubelets get apply updated you have a framework",
    "start": "1151890",
    "end": "1157620"
  },
  {
    "text": "testing framework to make sure that that cubelet is certified before the next 30% of the batch is being picked up okay",
    "start": "1157620",
    "end": "1167370"
  },
  {
    "text": "this is a very cool experience ATC the upgrade how many of you guys turned etcd upgrade BOM a lot right",
    "start": "1167370",
    "end": "1175610"
  },
  {
    "text": "scary really scary right and this is something which we did in oven kennedy environment so we want our LED",
    "start": "1175610",
    "end": "1182940"
  },
  {
    "text": "CD from one version to the next portion and one of the things which we did was we left the kubernetes api up and",
    "start": "1182940",
    "end": "1189900"
  },
  {
    "text": "running during this process okay fine we will take the etcd down the communication between the API and the",
    "start": "1189900",
    "end": "1195750"
  },
  {
    "text": "etcd will be failing and it's not going to take any action right okay we did we ran a script",
    "start": "1195750",
    "end": "1201480"
  },
  {
    "text": "we'll go and upgrade the etcd and puts back into rotation right so that script",
    "start": "1201480",
    "end": "1206490"
  },
  {
    "text": "had a bug okay we need state to upgrade it and then when it came up in the production it's",
    "start": "1206490",
    "end": "1212040"
  },
  {
    "text": "supposed to point to a production directly it some of the script had a bug it affected that as a non production",
    "start": "1212040",
    "end": "1217830"
  },
  {
    "text": "environment and pointer to a directory which was empty because that's not a production environment so the cubes that",
    "start": "1217830",
    "end": "1224340"
  },
  {
    "text": "the etcd started with an empty directory what happens now the cluster was up and running with hundred plus deployments",
    "start": "1224340",
    "end": "1229740"
  },
  {
    "text": "right now the etcd we would start with an empty data set okay what's the behavior will happen sorry",
    "start": "1229740",
    "end": "1239390"
  },
  {
    "text": "complete right but great right yes what happened here",
    "start": "1239390",
    "end": "1244500"
  },
  {
    "start": "1242000",
    "end": "1287000"
  },
  {
    "text": "now that API was able to connect to a TC d okay and the cubelet came up and then he started to connect with the API and a",
    "start": "1244500",
    "end": "1250320"
  },
  {
    "text": "PA say that hey I have 100 parts then you say that no you're not supposed to have under parts per my data source okay",
    "start": "1250320",
    "end": "1256590"
  },
  {
    "text": "you have supposed to not to have anybody what should be clean so what happens across the fleet all the cubelet it have",
    "start": "1256590",
    "end": "1263090"
  },
  {
    "text": "leave it all the part itself a fix all the parts clean slate the clusters complete down right this is awesome",
    "start": "1263090",
    "end": "1272750"
  },
  {
    "text": "right okay fine this is a mistake this is script bug right it's supposed to find to",
    "start": "1273049",
    "end": "1279179"
  },
  {
    "text": "production directly it pointed to a non production detective eachother empty in production cluster I mean it came out",
    "start": "1279179",
    "end": "1284669"
  },
  {
    "text": "empty this is expected what I really expect her like what how did we recover let's go through what how we could",
    "start": "1284669",
    "end": "1290160"
  },
  {
    "start": "1287000",
    "end": "1343000"
  },
  {
    "text": "recover right okay yeah we pointed to the right directory there's no rocket signs here and also we periodically take",
    "start": "1290160",
    "end": "1296940"
  },
  {
    "text": "a snapshot of etcd store and we took that snapshot and stored into the director directly and recovered that",
    "start": "1296940",
    "end": "1303000"
  },
  {
    "text": "once you put the data back now the cube let's synchronize with the API and the",
    "start": "1303000",
    "end": "1308190"
  },
  {
    "text": "AP say that they are not supposed to have this part but the source that a source which is etcd now everyone suffers shadowing this one",
    "start": "1308190",
    "end": "1314580"
  },
  {
    "text": "right I really really like the features like hey we can we should we want",
    "start": "1314580",
    "end": "1319740"
  },
  {
    "text": "kubernetes api to know that we are doing some kind of maintenance there should be a flag in kubernetes api which you are",
    "start": "1319740",
    "end": "1326940"
  },
  {
    "text": "also looking exploring which says that i am right now in maintenance mode do not sync right so that's I opened a bug",
    "start": "1326940",
    "end": "1334740"
  },
  {
    "text": "against coming and we are discussing their way which they I mean a maintenance do not sink so this kind of accidents could be avoided",
    "start": "1334740",
    "end": "1342080"
  },
  {
    "text": "cool I'll hand it over to none that to go over a few more scenarios there are so many as I said we had run for two years",
    "start": "1342080",
    "end": "1349560"
  },
  {
    "start": "1343000",
    "end": "1491000"
  },
  {
    "text": "there are so many bugs which we run into I cannot cover most of them I'll try to cover two major things okay great Ananda",
    "start": "1349560",
    "end": "1356120"
  },
  {
    "text": "thank you today's deal a certificate in reference how many of you have done till",
    "start": "1356120",
    "end": "1363120"
  },
  {
    "text": "a certificate rotation of your existing kubernetes cluster some quite brave",
    "start": "1363120",
    "end": "1368490"
  },
  {
    "text": "hands there right so so the reason why we do a periodic TLS certificate",
    "start": "1368490",
    "end": "1375090"
  },
  {
    "text": "rotation for the kubernetes cluster is the primary reason being is that it's a self-signed certificate and it's also",
    "start": "1375090",
    "end": "1381000"
  },
  {
    "text": "short-lived close to one year so what happens when we do the TLS rotation",
    "start": "1381000",
    "end": "1387590"
  },
  {
    "text": "pretty much the whole kubernetes control plane components needs a restart they",
    "start": "1387590",
    "end": "1393690"
  },
  {
    "text": "cubelet HCD a control manager the the APS over all these component needs to be",
    "start": "1393690",
    "end": "1399720"
  },
  {
    "text": "restarted to avoid communication between these components and there's a one",
    "start": "1399720",
    "end": "1405840"
  },
  {
    "text": "another component called service account which is a special service account tokens are created by controller manager",
    "start": "1405840",
    "end": "1412800"
  },
  {
    "text": "by using this private key service account private key and the current is API server authenticates that service",
    "start": "1412800",
    "end": "1419190"
  },
  {
    "text": "account token using the public corresponding public key the reason I am saying special is that almost every",
    "start": "1419190",
    "end": "1426900"
  },
  {
    "text": "controller in the kubernetes a ecosystem uses the service account to have indicated and we wanted to rotate that",
    "start": "1426900",
    "end": "1433590"
  },
  {
    "text": "also frequently the reason being the service account once you created it and it's an ever expiry token so to address",
    "start": "1433590",
    "end": "1441210"
  },
  {
    "text": "those security situations we often rotate the service account private key as well",
    "start": "1441210",
    "end": "1446910"
  },
  {
    "text": "so one thing we observed when we actually rotated this certificate this",
    "start": "1446910",
    "end": "1452250"
  },
  {
    "text": "private key that all the controllers stopped working like horizontal part autoscaler stopped working daemon sets",
    "start": "1452250",
    "end": "1458520"
  },
  {
    "text": "controller stopped working deployment controller everything and including all the add-ons like a hipster cube dns and",
    "start": "1458520",
    "end": "1465000"
  },
  {
    "text": "everything you know it also had some impact to our existing clock applications where we had an an identity",
    "start": "1465000",
    "end": "1472050"
  },
  {
    "text": "D component which watches the kubernetes api that particular communication is also broken so the way we resolved was",
    "start": "1472050",
    "end": "1478650"
  },
  {
    "text": "that we restarted or kind of recreated the service tokens and restarted these",
    "start": "1478650",
    "end": "1485850"
  },
  {
    "text": "affected components and they started working perfectly and when we were",
    "start": "1485850",
    "end": "1492660"
  },
  {
    "start": "1491000",
    "end": "1529000"
  },
  {
    "text": "looked at the API server a service account key file had an option that it",
    "start": "1492660",
    "end": "1498180"
  },
  {
    "text": "could support multiple public keys so if you if you are in a situation that you have to rotate the service account the",
    "start": "1498180",
    "end": "1504720"
  },
  {
    "text": "public key and so you make sure you add both the certificates to this component and restart the API server so you could",
    "start": "1504720",
    "end": "1511770"
  },
  {
    "text": "potentially avoid the catastrophic situation happens that at them but you know you really want it to face out the",
    "start": "1511770",
    "end": "1518340"
  },
  {
    "text": "world service our con tokens because you know it's it's it's a non expiry one so that you can do actually in a controlled",
    "start": "1518340",
    "end": "1525240"
  },
  {
    "text": "manner instead of rushing to it so this",
    "start": "1525240",
    "end": "1530910"
  },
  {
    "start": "1529000",
    "end": "1552000"
  },
  {
    "text": "is one of my favorite component when it comes to equivalent is ecosystem because it has a huge integration between the",
    "start": "1530910",
    "end": "1537690"
  },
  {
    "text": "applications running and the equivalent is clustered as well almost every part majority of the inducer personally in",
    "start": "1537690",
    "end": "1544170"
  },
  {
    "text": "the kubernetes cluster uses the cube DNS as a resolver there are a bunch of cases",
    "start": "1544170",
    "end": "1549390"
  },
  {
    "text": "we ran into problems with the cube DNS naming Lee so oftentimes not all the",
    "start": "1549390",
    "end": "1558180"
  },
  {
    "start": "1552000",
    "end": "1675000"
  },
  {
    "text": "applications part have a DNS cache built-in so they make for every DNS lookup they make a call to cube DNS if",
    "start": "1558180",
    "end": "1565080"
  },
  {
    "text": "you have it if you have an application that has thousand or 2,000 parts and when they have a deployment going on",
    "start": "1565080",
    "end": "1570540"
  },
  {
    "text": "this potentially can bring down the Q Tina's part and there's another cool another interesting thing the second",
    "start": "1570540",
    "end": "1576720"
  },
  {
    "text": "thing to notice that whenever there is a cube DNS pod goes down cube DNS runs in",
    "start": "1576720",
    "end": "1582390"
  },
  {
    "text": "a deployment right so pod can go down so when the part goes down q proxy updates",
    "start": "1582390",
    "end": "1588330"
  },
  {
    "text": "the endpoint controller saying hey just remove this particular endpoint and so every mission in the cluster gets the",
    "start": "1588330",
    "end": "1594990"
  },
  {
    "text": "new update of final set of kubernetes cube DNS endpoints so even",
    "start": "1594990",
    "end": "1601770"
  },
  {
    "text": "during this fraction of yet you would probably see a DNS latency in your in your applications and there's",
    "start": "1601770",
    "end": "1608419"
  },
  {
    "text": "an one another critical thing is because we run in our on-prem so we don't throw the host away when the host goes down we",
    "start": "1608419",
    "end": "1615409"
  },
  {
    "text": "wait for the host to come back right so oftentimes a node goes unresponsible during that time if that node B if that",
    "start": "1615409",
    "end": "1623690"
  },
  {
    "text": "node being the QB in a spar running there then we have another interesting",
    "start": "1623690",
    "end": "1629059"
  },
  {
    "text": "problem there that there is a flag in a controller say spot eviction time or",
    "start": "1629059",
    "end": "1634669"
  },
  {
    "text": "it's default to five minutes so for the first five minutes is not going to remove that the failed node or the fail",
    "start": "1634669",
    "end": "1640639"
  },
  {
    "text": "depart from the endpoint so for the for two to five minutes you will see the all",
    "start": "1640639",
    "end": "1646279"
  },
  {
    "text": "the application routing to the failed cube DNS part just having some outage on the leaners how many of you know dot",
    "start": "1646279",
    "end": "1654590"
  },
  {
    "text": "file lookups here yeah it's pretty interesting so that's one of the cause",
    "start": "1654590",
    "end": "1661369"
  },
  {
    "text": "in kubernetes that it adds a latency for external DNS lookups it has to traverse",
    "start": "1661369",
    "end": "1666559"
  },
  {
    "text": "through all five lookups before you make a DNS lookup for a external endpoint when I say external which is not in your",
    "start": "1666559",
    "end": "1672559"
  },
  {
    "text": "cluster that local so something all these problem we wanted to avoid so one",
    "start": "1672559",
    "end": "1677629"
  },
  {
    "start": "1675000",
    "end": "1745000"
  },
  {
    "text": "way we thought we could resolve this issue is we try to narrow down the pod addiction time odd how much we can go",
    "start": "1677629",
    "end": "1685789"
  },
  {
    "text": "thinner right so because we now on Prem we don't want to be getting a false alert so so we try it an alternative",
    "start": "1685789",
    "end": "1692090"
  },
  {
    "text": "approach running a daemon set for the DNS mask so that every node has a DNS",
    "start": "1692090",
    "end": "1698929"
  },
  {
    "text": "mask so all the external calls will be routed we have a ruling that the ants mask so",
    "start": "1698929",
    "end": "1704389"
  },
  {
    "text": "external calls will go directly from the node and it it will forward the DNS query to the cluster DNS only for those",
    "start": "1704389",
    "end": "1712309"
  },
  {
    "text": "clusters dot local DNS lookups so that way we brought down the impact to narrow to a specific node instead of the",
    "start": "1712309",
    "end": "1719419"
  },
  {
    "text": "cluster wide impact on the other case and also we make use of you know the",
    "start": "1719419",
    "end": "1724850"
  },
  {
    "text": "cluster proportional autoscaler so in case like we keep adding a machines right so for the applications we pretty",
    "start": "1724850",
    "end": "1730580"
  },
  {
    "text": "much make use of horizon a lot of our scalar so for for the cube DNS the more work",
    "start": "1730580",
    "end": "1735860"
  },
  {
    "text": "loads we added to the system so we make use of cluster Auto proportional scaler so hey we got ten more nodes probably",
    "start": "1735860",
    "end": "1742730"
  },
  {
    "text": "add one more D in a spot into the ecosystem so running an on-prem is",
    "start": "1742730",
    "end": "1748910"
  },
  {
    "start": "1745000",
    "end": "1817000"
  },
  {
    "text": "definitely a challenging thing because as soon as mentioned we had the ship pipeline we have a robust release",
    "start": "1748910",
    "end": "1754580"
  },
  {
    "text": "guidance and everything but on the other hand and senior partner mention in the keynote today kubernetes releases almost",
    "start": "1754580",
    "end": "1761390"
  },
  {
    "text": "two to three times major releases in a year right keeping up to date with those releases it's often a challenging task",
    "start": "1761390",
    "end": "1768320"
  },
  {
    "text": "and and often we ran into an issue where hey there are some flags depreciated this is a breaking change you may have",
    "start": "1768320",
    "end": "1775100"
  },
  {
    "text": "to make things so I just wanted to make sure that you know whenever you're doing don't file up those releases keep",
    "start": "1775100",
    "end": "1781090"
  },
  {
    "text": "whenever you get a major release keep a practice of upgrading to the major releases and and we have also bitten by",
    "start": "1781090",
    "end": "1790309"
  },
  {
    "text": "a couple of operating system packages so we tried what I recommend is just try to keep those kernel and operating system",
    "start": "1790309",
    "end": "1797030"
  },
  {
    "text": "packages up-to-date like there were a couple of scenarios where diversity man was you know doing a file descriptor",
    "start": "1797030",
    "end": "1803390"
  },
  {
    "text": "leak and the darker hole darker communication was broken we have we are getting a slack communication say hey",
    "start": "1803390",
    "end": "1808820"
  },
  {
    "text": "the sporadically no bunch of nodes on the cluster was throwing out for a darker issue so having a blue date on",
    "start": "1808820",
    "end": "1816530"
  },
  {
    "text": "these packages would help so it's a pretty interesting problem in a",
    "start": "1816530",
    "end": "1822530"
  },
  {
    "start": "1817000",
    "end": "1866000"
  },
  {
    "text": "multi-tenant environment as soon as mentioned we have a namespace isolation right so whenever there is an oem",
    "start": "1822530",
    "end": "1828950"
  },
  {
    "text": "happens c group is going to notice that and it is going to kill that process",
    "start": "1828950",
    "end": "1834049"
  },
  {
    "text": "right when that happens it's going to trace dump a lot of information into the",
    "start": "1834049",
    "end": "1839240"
  },
  {
    "text": "console and occasional unfortunately in our setup you know console is a serial port and",
    "start": "1839240",
    "end": "1844640"
  },
  {
    "text": "it's a 9600 baud rate you know what setup so during that the log is written",
    "start": "1844640",
    "end": "1850490"
  },
  {
    "text": "into the console the cpu had totally choked off it's effectively impacting all other applications running on that",
    "start": "1850490",
    "end": "1856970"
  },
  {
    "text": "particular node so we took an approach of step nasarah of you know reducing the last bus paucity on those on the kernel",
    "start": "1856970",
    "end": "1864710"
  },
  {
    "text": "verbose output it's pretty interesting a future in the",
    "start": "1864710",
    "end": "1870379"
  },
  {
    "start": "1866000",
    "end": "1943000"
  },
  {
    "text": "kubernetes like suresh mentioned node controller has the nice feature about the eviction throttling and and the",
    "start": "1870379",
    "end": "1876320"
  },
  {
    "text": "couplet side in a multi-tenant environment right as we mentioned we have 2,000 nodes we do get only three to",
    "start": "1876320",
    "end": "1883909"
  },
  {
    "text": "four pages a week the way we achieve it we silence some of the pages through you know some of the cool features available",
    "start": "1883909",
    "end": "1890029"
  },
  {
    "text": "in the kubernetes one feature as I mentioned just this out of resource handling the way it works is if you have",
    "start": "1890029",
    "end": "1898879"
  },
  {
    "text": "a tenant and if you have a user causing a disc face to fill up me could be on",
    "start": "1898879",
    "end": "1904250"
  },
  {
    "text": "the node FS on empty directory volume it could be even in the darker image you know image directory itself you don't",
    "start": "1904250",
    "end": "1910850"
  },
  {
    "text": "want to be woken up in the 2:00 a.m. and just forgetting a disk full the same thing could happen you know you know you have a more nodes available there is a",
    "start": "1910850",
    "end": "1918110"
  },
  {
    "text": "memory pressure you wanted to make sure you know you don't want to get a lot for you know one one node alert so this is a",
    "start": "1918110",
    "end": "1925610"
  },
  {
    "text": "cool feature what we can make use of it basically configure the eviction threshold so when when this particular",
    "start": "1925610",
    "end": "1931279"
  },
  {
    "text": "limit hits Kuebler trance the pod and it automatically fix the bad bad part so",
    "start": "1931279",
    "end": "1937399"
  },
  {
    "text": "that you don't want to be you know woken up in the middle of the night to solve this one single node issues so so it's a",
    "start": "1937399",
    "end": "1946519"
  },
  {
    "start": "1943000",
    "end": "2024000"
  },
  {
    "text": "game day testing so we often conduct a game day testing for our kubernetes environment so what I mean by game day",
    "start": "1946519",
    "end": "1953450"
  },
  {
    "text": "testing is basically we break every component in a controlled environment so we we break it and we'll record those",
    "start": "1953450",
    "end": "1960259"
  },
  {
    "text": "output and so we pretty much do that for all the control planes like API server bring down an API server and observe the",
    "start": "1960259",
    "end": "1966409"
  },
  {
    "text": "metrics on the node side we randomly reboot a node that's how we identify if the DNS is good or if the customer also",
    "start": "1966409",
    "end": "1973820"
  },
  {
    "text": "occurs is there any noise from the customer or or their pod sorry you gracefully shifted to another load so",
    "start": "1973820",
    "end": "1980240"
  },
  {
    "text": "having this practice helps us to react in a situation when there is a production instant like we can quickly",
    "start": "1980240",
    "end": "1987139"
  },
  {
    "text": "jump on it and I know kind of address those problems so these are some of the you know activities we do to avoid major",
    "start": "1987139",
    "end": "1995419"
  },
  {
    "text": "outages hand over to stress thanks Rhonda",
    "start": "1995419",
    "end": "2000670"
  },
  {
    "text": "okay this is like a scripted environment where we run the game-day testing like it's pretty much chaos engineering you",
    "start": "2000670",
    "end": "2006580"
  },
  {
    "text": "can think of alright we would like to if we are planning to write a product out",
    "start": "2006580",
    "end": "2011590"
  },
  {
    "text": "of this there it can be used across everyone really you can just run this it will it'll produce this environment with",
    "start": "2011590",
    "end": "2019240"
  },
  {
    "text": "scenarios and then say the body's lackluster behavior cool before I go here I there are many",
    "start": "2019240",
    "end": "2027400"
  },
  {
    "text": "scenarios which we run into there a few please talk to us I'll be here for whole day um we",
    "start": "2027400",
    "end": "2033460"
  },
  {
    "text": "discussed about what sadaqa or SUV ran into what's the English you should be ran into the network is should be ran into",
    "start": "2033460",
    "end": "2038770"
  },
  {
    "text": "right so pretty much all those things which I discussed about the complement which we built internal it's pretty much",
    "start": "2038770",
    "end": "2043810"
  },
  {
    "text": "out here it's kind of open sourced it is built so that hey we can contribute back",
    "start": "2043810",
    "end": "2049240"
  },
  {
    "text": "to the community rather than just always not observing it so please do use it",
    "start": "2049240",
    "end": "2054460"
  },
  {
    "text": "it's pretty much and then we are in the process of see if we can open source the ingress controller as well as in the 80s",
    "start": "2054460",
    "end": "2060908"
  },
  {
    "text": "Apache traffic server and kicking out",
    "start": "2060909",
    "end": "2066010"
  },
  {
    "start": "2064000",
    "end": "2132000"
  },
  {
    "text": "cater support yeah I just talked about the other experience right but we have done templating engine how we templated",
    "start": "2066010",
    "end": "2072760"
  },
  {
    "text": "arms and you have our one networking model of how did we do the networking for kubernetes and we have our own Top",
    "start": "2072760",
    "end": "2080200"
  },
  {
    "text": "Ramen ingress survey of how do we manage clusters what are the tools we built out of it how is our identity provided right",
    "start": "2080200",
    "end": "2086320"
  },
  {
    "text": "we provide a very unique identity short-lived identify the workloads right so our engineers just deploy their",
    "start": "2086320",
    "end": "2092110"
  },
  {
    "text": "application we provide an identity for that we call identity other courts are out there open sourced so that if you",
    "start": "2092110",
    "end": "2098950"
  },
  {
    "text": "want to get the complete view how we are doing into in communities please click on this particular link we had a detailed discussion about how each",
    "start": "2098950",
    "end": "2105730"
  },
  {
    "text": "components we have been doing and this is my main lady please reach out to me I'm very much happy to collaborate",
    "start": "2105730",
    "end": "2111880"
  },
  {
    "text": "I pretty much join all the slack channel I'm always active out there in case I want to talk to me please reach out to",
    "start": "2111880",
    "end": "2117460"
  },
  {
    "text": "me and important I'm wearing right you want to solve the challenge with through",
    "start": "2117460",
    "end": "2123580"
  },
  {
    "text": "automation please do contact me and we can talk about your possible option at least a thank you",
    "start": "2123580",
    "end": "2129240"
  },
  {
    "text": "[Applause]",
    "start": "2129240",
    "end": "2134340"
  }
]