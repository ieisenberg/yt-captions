[
  {
    "text": "thank you everybody my name is Arun Gupta I run the developer programs team at Intel and I'm super excited to give",
    "start": "80",
    "end": "6720"
  },
  {
    "text": "this talk about gen recipes for cloud native development and today I have with me",
    "start": "6720",
    "end": "13120"
  },
  {
    "text": "kosel yeah uh hi everyone I'm kosal I'm a distinguished technologist with infosis my core area seems to be AI gen",
    "start": "13120",
    "end": "21760"
  },
  {
    "text": "AI platforms and happy to be here uh we'll talk more all right let's dig into",
    "start": "21760",
    "end": "28800"
  },
  {
    "text": "it why why do we think cloud native is the ideal platform for AI I mean if you",
    "start": "28800",
    "end": "34120"
  },
  {
    "text": "think about Cloud native you know over the last 10 years a lot of companies",
    "start": "34120",
    "end": "40000"
  },
  {
    "text": "have built a developer platform on top of that and in terms of you know when you're are differing offering different",
    "start": "40000",
    "end": "46199"
  },
  {
    "text": "workloads on it you want scalability what that means is easy scaling of your",
    "start": "46199",
    "end": "51879"
  },
  {
    "text": "compute resources up and down because your loads are bursty you know at some time you need much more resources",
    "start": "51879",
    "end": "58879"
  },
  {
    "text": "sometimes you don't need much more resources and that is particularly true when you are looking at AI workloads you",
    "start": "58879",
    "end": "65158"
  },
  {
    "text": "know sometimes they are taking days sometimes they're taking weeks months if you're training your models which is",
    "start": "65159",
    "end": "71720"
  },
  {
    "text": "much more longer task but if you're doing inferencing that is a bursty workload so you need that sort of a",
    "start": "71720",
    "end": "77479"
  },
  {
    "text": "scalability you need cost efficiency you know you want to start with the llm large language model but then you may",
    "start": "77479",
    "end": "83880"
  },
  {
    "text": "want to go to a small language model so that cost efficiency that you're only",
    "start": "83880",
    "end": "88960"
  },
  {
    "text": "paying for what you you use really containerization is a key element of it",
    "start": "88960",
    "end": "94000"
  },
  {
    "text": "because what it gives you is a workload across different compute environments whether you're running on cloud whether",
    "start": "94000",
    "end": "100119"
  },
  {
    "text": "you're running on edge whether you're running on data center or on laptop you don't want that impedance mismatch and",
    "start": "100119",
    "end": "107000"
  },
  {
    "text": "that's exactly what Harmony is about your test Dev staging prod all of these",
    "start": "107000",
    "end": "112200"
  },
  {
    "text": "different environments they have exact same environment you just scale up from there high availability is critical",
    "start": "112200",
    "end": "119920"
  },
  {
    "text": "particularly for AI workloads if your model is getting trained you know in the middle of a training which has been done",
    "start": "119920",
    "end": "125880"
  },
  {
    "text": "for like 6 weeks you don't want your cluster to go down because if your cluster goes down that retraining starts",
    "start": "125880",
    "end": "131680"
  },
  {
    "text": "all over again and last but not the least you know as you're building your gen applications you need that",
    "start": "131680",
    "end": "138239"
  },
  {
    "text": "microservices architecture so that you can scale each and every different component",
    "start": "138239",
    "end": "144760"
  },
  {
    "text": "accordingly now if you're building a gen application a lot of the customers are really looking at rag or retrieval",
    "start": "144920",
    "end": "151560"
  },
  {
    "text": "augmented generation this is a typical architecture of a rag so don't get bogged down but if I walk you through",
    "start": "151560",
    "end": "157160"
  },
  {
    "text": "this on the left side what you see is your Enterprise data that could be sitting in Salesforce service now slack",
    "start": "157160",
    "end": "164319"
  },
  {
    "text": "email messages PDF documents databases wherever it is you do some ingestion of",
    "start": "164319",
    "end": "169720"
  },
  {
    "text": "that data you put that into a data store could be a vector could be a graph database could be elastic search or",
    "start": "169720",
    "end": "176720"
  },
  {
    "text": "wherever it is with a vector index so that it allows similarity search now your query comes in from a user from",
    "start": "176720",
    "end": "183200"
  },
  {
    "text": "the top you extract the relevant information out of the query you",
    "start": "183200",
    "end": "188319"
  },
  {
    "text": "retrieve the information from the vector database you augment it you send it to the llm and then you generate the",
    "start": "188319",
    "end": "195640"
  },
  {
    "text": "response retrieval augmented generation so that's a typical rag architecture now",
    "start": "195640",
    "end": "201280"
  },
  {
    "text": "you could use a llm as it is or you can fine tune the llm and then play around",
    "start": "201280",
    "end": "207000"
  },
  {
    "text": "with that or you can use rag so these are different components an architecture so essentially when you are building",
    "start": "207000",
    "end": "212480"
  },
  {
    "text": "your gen application this is an architecture that you're looking at it",
    "start": "212480",
    "end": "217599"
  },
  {
    "text": "but now if you start kind of breaking it down what are the different components of a gen application well you need a",
    "start": "217599",
    "end": "224120"
  },
  {
    "text": "data prep micro service which essentially takes your data ingest it",
    "start": "224120",
    "end": "230480"
  },
  {
    "text": "chunk it create the embeddings then you need a vector database could be an open",
    "start": "230480",
    "end": "235760"
  },
  {
    "text": "source database could be a closed database could be a service it doesn't matter or an existing database with a",
    "start": "235760",
    "end": "241760"
  },
  {
    "text": "vector index onto it then you need a retriever then you need a ranker which",
    "start": "241760",
    "end": "247040"
  },
  {
    "text": "kind of ranks your responses based upon what you're looking for and of course you need an llm or a slm or whatever",
    "start": "247040",
    "end": "254959"
  },
  {
    "text": "language model that you are using and sometimes llm are generic sometimes they are very tuned to a specific use case",
    "start": "254959",
    "end": "261759"
  },
  {
    "text": "but these are the different components that you need when you're really trying to build a gen",
    "start": "261759",
    "end": "267880"
  },
  {
    "text": "application as you're starting to compose your gen application when you look at embedding there's a wide range",
    "start": "267880",
    "end": "274360"
  },
  {
    "text": "of embedding I just want to build I'm writing a spring boot application or a go application or a python application",
    "start": "274360",
    "end": "281759"
  },
  {
    "text": "how do I take that and convert it into a gen application I don't know what embedding to choose data with Vector",
    "start": "281759",
    "end": "288440"
  },
  {
    "text": "databases my source of Truth lives in a post grass or in a my SQL or you know",
    "start": "288440",
    "end": "294680"
  },
  {
    "text": "somewhere else in a PDF document what Vector database should I choose I don't know which how to choose it and D with",
    "start": "294680",
    "end": "300960"
  },
  {
    "text": "llm there's a wide range of llms you know sometimes the llms require a GPU",
    "start": "300960",
    "end": "307120"
  },
  {
    "text": "but more often than not it just works very well on a CPU so how do I what is my decision Matrix if I'm choosing all",
    "start": "307120",
    "end": "314320"
  },
  {
    "text": "of that so effectively if you want to build a rag chatbot now you want to build a",
    "start": "314320",
    "end": "320639"
  },
  {
    "text": "chatbot that takes care of your Enterprise data bring it in and leverage that for a simple chat Q&A then this is",
    "start": "320639",
    "end": "328280"
  },
  {
    "text": "what your gen recipe would look like this is one gen",
    "start": "328280",
    "end": "333960"
  },
  {
    "text": "recipe meet Opia Opia is open platform for Enterprise AI this is a Linux",
    "start": "334280",
    "end": "342039"
  },
  {
    "text": "Foundation project belongs to LF aai and data and it gives you gen recipes that",
    "start": "342039",
    "end": "349479"
  },
  {
    "text": "can be deployed in Enterprises and that's the whole premise of it you know it has pre-built recipes across a wide",
    "start": "349479",
    "end": "356680"
  },
  {
    "text": "range of use cases that you think about and is fully open sourced and let's dig",
    "start": "356680",
    "end": "363080"
  },
  {
    "text": "a little bit more into it so what is Opia I mean if you think about it we talked about those different",
    "start": "363080",
    "end": "369759"
  },
  {
    "text": "component level microservices right so the first set is it's a framework of composable building blocks embedding",
    "start": "369759",
    "end": "377720"
  },
  {
    "text": "retriever ranker llm Vector database those are your component level",
    "start": "377720",
    "end": "385240"
  },
  {
    "text": "microservices using those composable microservices you can build endtoend",
    "start": "385759",
    "end": "391240"
  },
  {
    "text": "workflows so chat Q&A is your end to end workflow where you can compose these",
    "start": "391240",
    "end": "396759"
  },
  {
    "text": "components together agentic systems AI Avatar so you can build all of those end",
    "start": "396759",
    "end": "402400"
  },
  {
    "text": "to end blueprints recipes workflows whatever you want to call them as and last but not the least you need a",
    "start": "402400",
    "end": "410560"
  },
  {
    "text": "assessment system for grading these gen systems oh I build this blueprint recipe",
    "start": "410560",
    "end": "417080"
  },
  {
    "text": "that's going to run on AWS that's leveraging this particular llm this particular Vector database and so on so",
    "start": "417080",
    "end": "423800"
  },
  {
    "text": "forth and what is my throughput what is my time for first token my second token",
    "start": "423800",
    "end": "429639"
  },
  {
    "text": "so on so forth so if I uplevel from this description what we are saying is it",
    "start": "429639",
    "end": "436360"
  },
  {
    "text": "consists Opia consists of two components one is a construction component which is essentially your framework and",
    "start": "436360",
    "end": "442800"
  },
  {
    "text": "blueprints and on the right what you see is a evaluation component so construction and evaluation now if we",
    "start": "442800",
    "end": "450400"
  },
  {
    "text": "were to translate that into the GitHub repo github.com",
    "start": "450400",
    "end": "456680"
  },
  {
    "text": "Opia project is a GitHub REO is a GitHub organization again it's a LF a in data",
    "start": "456680",
    "end": "463280"
  },
  {
    "text": "project a fully open source in that sense on the top right what you see is a",
    "start": "463280",
    "end": "468440"
  },
  {
    "text": "gen comps that is your component level microservices each of those component",
    "start": "468440",
    "end": "474400"
  },
  {
    "text": "level microservice whether it's a embedding or a retriever or a llm or a vector database",
    "start": "474400",
    "end": "480280"
  },
  {
    "text": "is a cloud native component essentially it has a doer container that is sitting in the doer Hub gen examples on the",
    "start": "480280",
    "end": "488280"
  },
  {
    "text": "other side are your readymade blueprint recipes and those are available either",
    "start": "488280",
    "end": "493919"
  },
  {
    "text": "as a Docker compost so you could say show me a Docker compost file for chat Q&A and voila you have it there so",
    "start": "493919",
    "end": "501039"
  },
  {
    "text": "that's the way it works and chat Q&A is just one example and I'll talk through some of more and",
    "start": "501039",
    "end": "507319"
  },
  {
    "text": "on the bottom left what you see is a gen eval this is the evaluation framework that I was talking about earlier it",
    "start": "507319",
    "end": "514039"
  },
  {
    "text": "gives you tools by which you can evaluate how your gen recipe look like in a particular combination or a",
    "start": "514039",
    "end": "520159"
  },
  {
    "text": "configuration whatever works for you so let me show you a little bit you know on the GitHub repo how these look",
    "start": "520159",
    "end": "527320"
  },
  {
    "text": "like so here I am on the GitHub um repo",
    "start": "534560",
    "end": "540240"
  },
  {
    "text": "let me make the phone slightly bigger so when I look at gen comps",
    "start": "540240",
    "end": "546600"
  },
  {
    "text": "here when I look at the comps component here this is where all my components are sitting so this is where you can see you",
    "start": "549000",
    "end": "556200"
  },
  {
    "text": "know my Corde my data prep my embedding my fine-tuning my guardrails llms lvms",
    "start": "556200",
    "end": "563680"
  },
  {
    "text": "all of these component level microservices are sitting if I go to llms",
    "start": "563680",
    "end": "570360"
  },
  {
    "text": "then it says okay what kind of an llm do you want you know for a particular use case let's say I'm looking for text",
    "start": "570360",
    "end": "575519"
  },
  {
    "text": "generation then it says what kind of text generation llm you're looking for whether you're looking for Ama or TGI or",
    "start": "575519",
    "end": "581959"
  },
  {
    "text": "VM and multiple other Integrations are getting planned so that's",
    "start": "581959",
    "end": "587920"
  },
  {
    "text": "that if I go back to components again actually and comps and I look at down",
    "start": "587920",
    "end": "597839"
  },
  {
    "text": "here vectors stes you'll see a wide variety of vector",
    "start": "597839",
    "end": "603839"
  },
  {
    "text": "stores that are already available as a component level microservice okay now if I go back to Opia",
    "start": "603839",
    "end": "611120"
  },
  {
    "text": "project I look at say gen",
    "start": "611720",
    "end": "615959"
  },
  {
    "text": "examples these are about 20 plus readymade gen blueprints that are available to you so let's say if I look",
    "start": "618320",
    "end": "625160"
  },
  {
    "text": "at chat Q&A",
    "start": "625160",
    "end": "628600"
  },
  {
    "text": "it kind of gives you your Docker compos your kubernetes Helm charts single click",
    "start": "631360",
    "end": "637240"
  },
  {
    "text": "know you can fire up a Docker compost and say run it in my local environment Docker compost is really good for",
    "start": "637240",
    "end": "642800"
  },
  {
    "text": "experimenting and playing with how your Docker images are really behaving with each other so if I go in",
    "start": "642800",
    "end": "649440"
  },
  {
    "text": "here and I say um Intel no because it needs to be specific to a hardware I",
    "start": "649440",
    "end": "656040"
  },
  {
    "text": "want to deploy it on Intel CPU on Zeon which is the most prevalent CPU across",
    "start": "656040",
    "end": "661480"
  },
  {
    "text": "different hyperscalers so now I have a compost. yaml",
    "start": "661480",
    "end": "667160"
  },
  {
    "text": "here and in here these are all my services so you can see red is Vector database my data prep",
    "start": "668000",
    "end": "675279"
  },
  {
    "text": "service my embedding service so on so forth and the beauty of all of this",
    "start": "675279",
    "end": "682160"
  },
  {
    "text": "is I have a compost. yaml I can just swap out my llm my default llm is TGI",
    "start": "682360",
    "end": "692320"
  },
  {
    "text": "but now I want to run that exact same thing with compose V llm so everything",
    "start": "692320",
    "end": "697800"
  },
  {
    "text": "in my compos yaml stays exactly the same I just replace the TGI service with VM",
    "start": "697800",
    "end": "704320"
  },
  {
    "text": "and there we are similarly if I want to run it with pine cone which is a manage",
    "start": "704320",
    "end": "709519"
  },
  {
    "text": "service now this is an open source project but doesn't mean it only integrates with open source Vector",
    "start": "709519",
    "end": "714839"
  },
  {
    "text": "databases so now instead of data prep redish service I have a data prep Pine con service so it it's very streamlined",
    "start": "714839",
    "end": "722200"
  },
  {
    "text": "in that sense of what your choice is but it gives you an opin stack as",
    "start": "722200",
    "end": "728600"
  },
  {
    "text": "well so as I just explained we have 20 plus recipes that are available in gen",
    "start": "735920",
    "end": "741800"
  },
  {
    "text": "examples I just showed you one of chat Q&A but there is a code generator code",
    "start": "741800",
    "end": "747480"
  },
  {
    "text": "translator there is a gen gentic Q&A there is a graph rag where you want to use graph as a backend database uh there",
    "start": "747480",
    "end": "754639"
  },
  {
    "text": "is a video Q&A there is a visual Q&A FAQ generator give it a bunch of documents",
    "start": "754639",
    "end": "760639"
  },
  {
    "text": "and say generate a FAQ for me so these are simple use cases that we have seen customers deploying in practice and the",
    "start": "760639",
    "end": "767800"
  },
  {
    "text": "beauty of this is because gen components and gen examples are open source so you",
    "start": "767800",
    "end": "774000"
  },
  {
    "text": "should be able to take a look at gen components and create your own gen examples so I would love to see frankly",
    "start": "774000",
    "end": "781199"
  },
  {
    "text": "what gen examples that you are seeing and create your own recipe and contribute to gen",
    "start": "781199",
    "end": "788440"
  },
  {
    "text": "examples now Opia open platform for Enterprise AI is a 100% open source",
    "start": "788519",
    "end": "794639"
  },
  {
    "text": "project and so it is run by a technical steering committee and Intel is the",
    "start": "794639",
    "end": "800040"
  },
  {
    "text": "primary maintainer right now um and I'll tell about some really useful stories here so we have two people from Intel on",
    "start": "800040",
    "end": "807480"
  },
  {
    "text": "the technical steering committee but as you can see the diversity over here there are lots of good end users and",
    "start": "807480",
    "end": "813399"
  },
  {
    "text": "vendor companies that are part of it and the one that I'm particularly excited about is AMD now when we build these gen",
    "start": "813399",
    "end": "822079"
  },
  {
    "text": "components when we build these darker images they're of course created for Zeon and optimized and validated on Zeon",
    "start": "822079",
    "end": "830279"
  },
  {
    "text": "but AMD becoming as part of the technical steering committee have been contributing their Hardware to the cicd",
    "start": "830279",
    "end": "837040"
  },
  {
    "text": "pipelines and what that means is they are validating these samples on the AMD",
    "start": "837040",
    "end": "842160"
  },
  {
    "text": "Hardware as well so that's pretty awesome actually let's take a look at the rich",
    "start": "842160",
    "end": "848959"
  },
  {
    "text": "ecosystem of partners that we have built over the last 8 months or so when we",
    "start": "848959",
    "end": "855240"
  },
  {
    "text": "started the project there were about 14 um launch Partners over the last 8",
    "start": "855240",
    "end": "861440"
  },
  {
    "text": "months I've said I'm very proud to say that we have grown up to 45 plus partners and by the way this chart is",
    "start": "861440",
    "end": "867880"
  },
  {
    "text": "already outdated I give U I built the slides about two days ago and two more Partners have joined since then let me",
    "start": "867880",
    "end": "875240"
  },
  {
    "text": "talk about some of the examples here so AMD I already talked about how they are contributing their Hardware to the cicd",
    "start": "875240",
    "end": "881920"
  },
  {
    "text": "pipeline in your company if you are building gen blueprints you have to",
    "start": "881920",
    "end": "887600"
  },
  {
    "text": "figure out you know where are you running them which hyperscaler you're running on which Hardware you validating",
    "start": "887600",
    "end": "893240"
  },
  {
    "text": "on with this the Opia ecosystem is doing the validation on Zeon and AMD and there",
    "start": "893240",
    "end": "900279"
  },
  {
    "text": "is a wonderful effort that is going on within cncf there's a AIML play there",
    "start": "900279",
    "end": "905720"
  },
  {
    "text": "AIML working group within that we are also working on validating the samples",
    "start": "905720",
    "end": "911240"
  },
  {
    "text": "on arm architecture so do you want to run all of that validation by yourself",
    "start": "911240",
    "end": "917160"
  },
  {
    "text": "or do you want to leverage the work that is already happening in the Opia project",
    "start": "917160",
    "end": "922440"
  },
  {
    "text": "of taking this gen examples that are getting validated on different Hardware architecture and different hyperscalers",
    "start": "922440",
    "end": "928040"
  },
  {
    "text": "as well arangodb is a graph database and we've been working very actively with",
    "start": "928040",
    "end": "933399"
  },
  {
    "text": "them to do that data prep ingestion and the back end for a chat Q&A example here",
    "start": "933399",
    "end": "939240"
  },
  {
    "text": "canonical we all know is a company behind Ubuntu they we have been working with them to do the integration in their",
    "start": "939240",
    "end": "944959"
  },
  {
    "text": "micro Kates um which is a full full-blown micro uh kubernetes cluster",
    "start": "944959",
    "end": "950199"
  },
  {
    "text": "that they create integration over there uh we have been working with a Docker you know because all the images are",
    "start": "950199",
    "end": "956560"
  },
  {
    "text": "published on Docker Hub so how do we make it more more visible more you know streamlined workflow over there we have",
    "start": "956560",
    "end": "962920"
  },
  {
    "text": "been working with uh infosis and know koser is going to talk about some wonderful work that we've been doing with them we've been working with Maria",
    "start": "962920",
    "end": "969480"
  },
  {
    "text": "DB you know how we could use Maria DB as a backend Vector database we've been working with minio same thing Mino is uh",
    "start": "969480",
    "end": "976759"
  },
  {
    "text": "your storage platform through any unstructured data on a terabytes petabytes of data and how we can",
    "start": "976759",
    "end": "983240"
  },
  {
    "text": "leverage Min iio as a backend database manga DB again you don't have to have a",
    "start": "983240",
    "end": "988319"
  },
  {
    "text": "vector database mongodb would work fine as a backend store neo4j is a graph database working",
    "start": "988319",
    "end": "994839"
  },
  {
    "text": "with them on a very similar element as well prediction guard is a partner that have actually contributed uh God rails",
    "start": "994839",
    "end": "1002480"
  },
  {
    "text": "to the project you are leveraging a llm but before you send information to the llm",
    "start": "1002480",
    "end": "1008920"
  },
  {
    "text": "you want to make sure that there is no pii that is leaked out so that's the kind of guard rails that production",
    "start": "1008920",
    "end": "1015279"
  },
  {
    "text": "guard has contributed over there and the list goes on and on on all the beautiful work that we've been doing a wide range",
    "start": "1015279",
    "end": "1021079"
  },
  {
    "text": "of Partners over here with that I'm going to bring kosel to the stage thank",
    "start": "1021079",
    "end": "1027038"
  },
  {
    "text": "you thanks every so uh I'm kosel I'm from infosis uh and one of the key",
    "start": "1027039",
    "end": "1034360"
  },
  {
    "text": "things is we have the most demanding customers uh and I'm not talking about our customers but our own employees uh",
    "start": "1034360",
    "end": "1041280"
  },
  {
    "text": "so we have got about 300 odd thousand people looking for various information HR policies travel policies uh past uh",
    "start": "1041280",
    "end": "1050120"
  },
  {
    "text": "rfps sales documentation lot of knowledge article so there's a lot of knowledge that has been created inside",
    "start": "1050120",
    "end": "1056559"
  },
  {
    "text": "infosis and also there are a lot of people looking for it so uh we started our journey on AI around uh 2016 onwards",
    "start": "1056559",
    "end": "1065039"
  },
  {
    "text": "when the sparkl all of those things were coming up from there on J I mean J just",
    "start": "1065039",
    "end": "1070440"
  },
  {
    "text": "made it worse so uh one of our one of our key things on AI is to be an AI",
    "start": "1070440",
    "end": "1075919"
  },
  {
    "text": "first organization what do I mean by that is basically eating our own dog food that means whatever we do for our",
    "start": "1075919",
    "end": "1083039"
  },
  {
    "text": "customers we do it first for ourselves so we have a uh an award willing platform we call it infyme which is like",
    "start": "1083039",
    "end": "1089880"
  },
  {
    "text": "a sort of an app uh or a website or any internal experience is managed by one",
    "start": "1089880",
    "end": "1097120"
  },
  {
    "text": "big live live platform right and there are about 300,000 users using it any end point of day so what we did is we",
    "start": "1097120",
    "end": "1105000"
  },
  {
    "text": "created a co-pilot sort of experiences so for example if you look at Word document so today you would have seen",
    "start": "1105000",
    "end": "1110440"
  },
  {
    "text": "Microsoft co-pilot now M Microsoft co-pilot can tell you everything that is out there on the Google but it doesn't",
    "start": "1110440",
    "end": "1116039"
  },
  {
    "text": "tell you about what's inside your organization what is your knowledge Etc right uh so whether it's a Word document",
    "start": "1116039",
    "end": "1123360"
  },
  {
    "text": "it's BPT or it's just a website or a chatbot on your app right so what we did",
    "start": "1123360",
    "end": "1129960"
  },
  {
    "text": "is we build all this knowledge sources index those documents lot of as you can see right",
    "start": "1129960",
    "end": "1136720"
  },
  {
    "text": "the indexing those document Etc is a lot of work so for example lot of our proposals or rfis responses Etc we would",
    "start": "1136720",
    "end": "1143720"
  },
  {
    "text": "like to save now it has a lot of sensitive information it could be a pii information it could be a financial",
    "start": "1143720",
    "end": "1148960"
  },
  {
    "text": "information then you have different departments as any organization would have so how do you manage the data",
    "start": "1148960",
    "end": "1155320"
  },
  {
    "text": "security Etc all of those challenges that we do uh one of the key things uh",
    "start": "1155320",
    "end": "1160960"
  },
  {
    "text": "you might wonder why Opia so for us uh we are a cloud native organization so we",
    "start": "1160960",
    "end": "1166720"
  },
  {
    "text": "build uh our own stack uh lot of it is on Nvidia h00 clusters as well as we",
    "start": "1166720",
    "end": "1173440"
  },
  {
    "text": "also use internally Intel xon CPUs and and and lot of other uh Hardware right",
    "start": "1173440",
    "end": "1179200"
  },
  {
    "text": "so we wanted something which was a cloud native something we can put some of our things are on cloud some of things are",
    "start": "1179200",
    "end": "1185320"
  },
  {
    "text": "on on premise so we wanted a good balance between a hardware and a uh where we can deploy stuff right so",
    "start": "1185320",
    "end": "1191200"
  },
  {
    "text": "kubernetes is a right example microservices the right architecture and then we wanted something that we can",
    "start": "1191200",
    "end": "1197480"
  },
  {
    "text": "extend we don't want to buy product we want to extend it we can want to build it and we want to create our own",
    "start": "1197480",
    "end": "1202720"
  },
  {
    "text": "offerings on top of it right so with that now we are in a pilot phase where",
    "start": "1202720",
    "end": "1209400"
  },
  {
    "text": "we have about 100,000 users using this uh now the challenges is to how we can",
    "start": "1209400",
    "end": "1215960"
  },
  {
    "text": "optimize some of the experiences uh including multimodel capabilities could be a speech could be a images could be a",
    "start": "1215960",
    "end": "1222760"
  },
  {
    "text": "video Etc right so that's where the next challenge is interesting part is we are using elastic search ins side as our",
    "start": "1222760",
    "end": "1229720"
  },
  {
    "text": "Vector database uh what I like to say I'll just take a",
    "start": "1229720",
    "end": "1235720"
  },
  {
    "text": "slightly a uh tangential issue as to why elastic right so with Gen with Vector",
    "start": "1235720",
    "end": "1243039"
  },
  {
    "text": "DBS are a new rage what we realize at the end of the day is we needed a",
    "start": "1243039",
    "end": "1248679"
  },
  {
    "text": "cluster big enough for our own information index it performance and something that we are comfortable with",
    "start": "1248679",
    "end": "1255640"
  },
  {
    "text": "right and with elastic we really got lucky because we were upgrading elastic around 8 8.7 8.9 version where they",
    "start": "1255640",
    "end": "1261919"
  },
  {
    "text": "started introducing a lot of vector related things right uh key updates on there was uh hybrid search which is",
    "start": "1261919",
    "end": "1269720"
  },
  {
    "text": "really important I I'm not sure whether you guys are able to see the numbers but we ran some experiment inside where we",
    "start": "1269720",
    "end": "1275880"
  },
  {
    "text": "created some synthetic data like what kind of questions answers people would like to see on our application right so",
    "start": "1275880",
    "end": "1281919"
  },
  {
    "text": "we ran multiple benchmarks evaluated and typically what Arun",
    "start": "1281919",
    "end": "1287159"
  },
  {
    "text": "explained right that is a retrieval there is a reranking and then there is a generation so you are calling three apis",
    "start": "1287159",
    "end": "1292360"
  },
  {
    "text": "at least right don't forget the call to the vector database Etc so when we did",
    "start": "1292360",
    "end": "1297400"
  },
  {
    "text": "that we we actually realized that even though you can run a ranker and get a",
    "start": "1297400",
    "end": "1302640"
  },
  {
    "text": "better accuracy but with hybrid search right so something like bm25 what elastic uses internally as well as uh",
    "start": "1302640",
    "end": "1309880"
  },
  {
    "text": "with a dense search which is your embedding Vector it really gives you a good accuracy right so we could sa shave",
    "start": "1309880",
    "end": "1316400"
  },
  {
    "text": "off probably a 2C call of ranker model internally right on top of it elastic",
    "start": "1316400",
    "end": "1322200"
  },
  {
    "text": "definitely provides a lot of Enterprise functions in terms of the data security the ACLS for your user group management",
    "start": "1322200",
    "end": "1330000"
  },
  {
    "text": "Etc and one of the I think I think the last point is is we got really lucky",
    "start": "1330000",
    "end": "1335960"
  },
  {
    "text": "because elastic introduced uh Vector apis in in the Java stack right in",
    "start": "1335960",
    "end": "1341840"
  },
  {
    "text": "around 2 21 and that AVX instruction set which is can really work on Zeon emerald",
    "start": "1341840",
    "end": "1348400"
  },
  {
    "text": "and and Zeon rapid Sapphire so those ve using those instruction your searches",
    "start": "1348400",
    "end": "1354080"
  },
  {
    "text": "becomes really faster Bas you can imagine the embedding vectors can size from anywhere from 60 Dimension to th000",
    "start": "1354080",
    "end": "1360120"
  },
  {
    "text": "Dimension so it requires a special hardware and a special instruction set in order to make it more",
    "start": "1360120",
    "end": "1365640"
  },
  {
    "text": "performant so uh we did those changes all of that is merg as a PR in the",
    "start": "1365640",
    "end": "1372279"
  },
  {
    "text": "Opia um and finally we lot of our customer loves Microsoft Azure so what",
    "start": "1372279",
    "end": "1377840"
  },
  {
    "text": "we are putting is a similar offering on Microsoft Azure basically uh it's a set of terraform template or armm templates",
    "start": "1377840",
    "end": "1384760"
  },
  {
    "text": "as you would like to call it come in deploy it uh we are adding things like Cosmos DB and the Telemetry integration",
    "start": "1384760",
    "end": "1392000"
  },
  {
    "text": "with Azure monitor so it really becomes Microsoft specific offering uh that will be listed on Microsoft in few days time",
    "start": "1392000",
    "end": "1399559"
  },
  {
    "text": "and I hope you guys can see it use it uh",
    "start": "1399559",
    "end": "1404640"
  },
  {
    "text": "with that I think uh we good and we'll take any questions if you uh we",
    "start": "1404640",
    "end": "1410919"
  },
  {
    "text": "have few more slides yeah oh sorry if we take that whole Paradigm",
    "start": "1410919",
    "end": "1417000"
  },
  {
    "text": "forward right this would be eventually a oneclick deployment on Azure so click",
    "start": "1417000",
    "end": "1423480"
  },
  {
    "text": "and you get a chat Q&A up and running now we have also done similar work with",
    "start": "1423480",
    "end": "1428840"
  },
  {
    "text": "AWS Marketplace so there a QR code if you scan that QR code that takes you to a page which has a Opia Q&A chat Q&A",
    "start": "1428840",
    "end": "1437320"
  },
  {
    "text": "available on Azure Market place and what does that offer it's basically a terraform based Opia recipe that is for",
    "start": "1437320",
    "end": "1443840"
  },
  {
    "text": "a quick deployment on ews and what it is you know I'm kind of showing the stack here so we have llm we are using TGI as",
    "start": "1443840",
    "end": "1452080"
  },
  {
    "text": "embedding model we are using Tei again optimized and then as a vector database we're using open search open source",
    "start": "1452080",
    "end": "1459400"
  },
  {
    "text": "project and it's a straight up deployment on Amazon eks which is their managed service there is no tie in",
    "start": "1459400",
    "end": "1465279"
  },
  {
    "text": "because even Amazon eks is 100% Upstream compliance in that sense so there is really even though it's a service even",
    "start": "1465279",
    "end": "1471679"
  },
  {
    "text": "though it's a Marketplace offering but there is no tie in to any of the manag",
    "start": "1471679",
    "end": "1476880"
  },
  {
    "text": "services because you could run this on any kubernetes cluster and that would work very well however there are",
    "start": "1476880",
    "end": "1482000"
  },
  {
    "text": "elements by which it is configurable so for example it is also integrated with bedrock which is their managed llm",
    "start": "1482000",
    "end": "1487960"
  },
  {
    "text": "service by default you can use anthropic Cloud 3 Haiku but there are other models",
    "start": "1487960",
    "end": "1493440"
  },
  {
    "text": "that are supported by Bedrock that can be easily integrated there are guardrails there is redis Vector",
    "start": "1493440",
    "end": "1499000"
  },
  {
    "text": "database if you want to choose and reconfigure your open search all of that fun integration is already",
    "start": "1499000",
    "end": "1505360"
  },
  {
    "text": "available in terms of the road map what we did is we started the project back in",
    "start": "1505360",
    "end": "1511919"
  },
  {
    "text": "April of 2024 of this year we released 1.0 a couple of months ago we released",
    "start": "1511919",
    "end": "1519080"
  },
  {
    "text": "1.1 a couple of weeks ago and now we continuing to work on 1.2 1.3 and so for",
    "start": "1519080",
    "end": "1524640"
  },
  {
    "text": "and so forth and the thinking really is One release we're going to do a feature",
    "start": "1524640",
    "end": "1531279"
  },
  {
    "text": "the second one is going to be more of a stability and you know performance kind of a release and then we go to a Tik to",
    "start": "1531279",
    "end": "1536440"
  },
  {
    "text": "model so then feature and then stability and so on so forth now the important part is this is a road map that we have",
    "start": "1536440",
    "end": "1544039"
  },
  {
    "text": "created but being an open-source project it is required to be approved by the",
    "start": "1544039",
    "end": "1549520"
  },
  {
    "text": "technical steering committee so we propos the road map but the technical steering committee is going to give a thumbs up and that's when this road map",
    "start": "1549520",
    "end": "1555919"
  },
  {
    "text": "actually becomes committed so you the star on the top something else that I want to highlight",
    "start": "1555919",
    "end": "1561480"
  },
  {
    "text": "is you know AIS software catalog uh intel.com and if that's the URL that you",
    "start": "1561480",
    "end": "1566720"
  },
  {
    "text": "go to you can see a lot of these samples available for you to play around with and you can say hey I don't want to get",
    "start": "1566720",
    "end": "1573240"
  },
  {
    "text": "into GitHub I don't I don't know how to play with it give me instructions and guide me through those steps so this is",
    "start": "1573240",
    "end": "1579520"
  },
  {
    "text": "where you can see some of those samples start appearing up the intention really here is this is something that we are",
    "start": "1579520",
    "end": "1585320"
  },
  {
    "text": "playing with right now but eventually we want to contribute this to the cncf as an open source project and CNC within",
    "start": "1585320",
    "end": "1592520"
  },
  {
    "text": "the AIML working group there's been discussions going on to create a AI playground so this could potentially be",
    "start": "1592520",
    "end": "1597960"
  },
  {
    "text": "the basis for creating that AI playground",
    "start": "1597960",
    "end": "1602278"
  },
  {
    "text": "essentially if you want to get started with Opia now Opia - project it's a GitHub page essentially and we're",
    "start": "1603080",
    "end": "1609480"
  },
  {
    "text": "creating a simpler redirect as well but this is a best page for you to get started with and smack on the top and in",
    "start": "1609480",
    "end": "1615919"
  },
  {
    "text": "the middle you see a getting started page so I'm giving you a QR code here",
    "start": "1615919",
    "end": "1621159"
  },
  {
    "text": "from that QR code you can deploy your at least a chat Q&A example across five",
    "start": "1621159",
    "end": "1628320"
  },
  {
    "text": "different Cloud providers you know Amazon sorry AWS Azure Google Cloud IBM",
    "start": "1628320",
    "end": "1633640"
  },
  {
    "text": "cloud and oral cloud and essentially if you look at these instructions what they give you is hey spin up a uban 2 VM",
    "start": "1633640",
    "end": "1641440"
  },
  {
    "text": "2404 and on that once you got the VM get up and running then the steps are very",
    "start": "1641440",
    "end": "1647320"
  },
  {
    "text": "similar B install Docker set up Docker compos and fire it up and if you want to",
    "start": "1647320",
    "end": "1653720"
  },
  {
    "text": "run this I know there are accordingly Helm charts available as well which can easily get you started on kubernetes so",
    "start": "1653720",
    "end": "1659799"
  },
  {
    "text": "that's the whole premise here and the other work that I'm super excited about is that is happening in the AIML working",
    "start": "1659799",
    "end": "1666360"
  },
  {
    "text": "group within cncf so there is a strong opportunity for you to contribute and",
    "start": "1666360",
    "end": "1671559"
  },
  {
    "text": "collaborate so I would highly recommend you know joining the AIML working group there's lots of good stuff that is",
    "start": "1671559",
    "end": "1676840"
  },
  {
    "text": "happening over there particularly validating these samples on the arm architecture and the best part over",
    "start": "1676840",
    "end": "1683760"
  },
  {
    "text": "there is you don't have to have an account you know if you join that working group we will give you an",
    "start": "1683760",
    "end": "1689080"
  },
  {
    "text": "account we will give you the arm credits and then we will give you the ability to play all we need is you to validate and",
    "start": "1689080",
    "end": "1694679"
  },
  {
    "text": "test those uh on the architecture take the sample run it over there file the bugs contribute it so if a lot of people",
    "start": "1694679",
    "end": "1702200"
  },
  {
    "text": "have been asking me how do I start on my open source Journey this could be your open source Journey so with that I think",
    "start": "1702200",
    "end": "1708600"
  },
  {
    "text": "that's a wrap of our presentation and we'll take some",
    "start": "1708600",
    "end": "1713000"
  },
  {
    "text": "questions much dat do you",
    "start": "1734679",
    "end": "1739278"
  },
  {
    "text": "okay so let me just uh repeat the question uh what you're asking is a how",
    "start": "1746640",
    "end": "1752559"
  },
  {
    "text": "do you select the right size model for your use case b what is the uh what do",
    "start": "1752559",
    "end": "1758919"
  },
  {
    "text": "you do with hallucinations right so I think look hallucinations are not bugs",
    "start": "1758919",
    "end": "1765360"
  },
  {
    "text": "they are features right I mean it's it's just you producing next token you're predicting right so there is no right",
    "start": "1765360",
    "end": "1771440"
  },
  {
    "text": "answer can I reduce hallucination but I'll tell you what what we do right so we start with the Benchmark first I",
    "start": "1771440",
    "end": "1778240"
  },
  {
    "text": "don't start with the model first right uh when we say data first what do you mean by data first is we create the",
    "start": "1778240",
    "end": "1784240"
  },
  {
    "text": "Benchmark in terms of Q&A as you saw on the slide right I kind of we gone through about eight or nine different",
    "start": "1784240",
    "end": "1790480"
  },
  {
    "text": "models just to see what is that model without even fine-tuning what level of accuracy that gives me right now in",
    "start": "1790480",
    "end": "1797360"
  },
  {
    "text": "terms of your specific question in terms of what is the size of the model or parameters right so uh when you talk",
    "start": "1797360",
    "end": "1804039"
  },
  {
    "text": "about embedding model right there are some really great embedding model from BG M3 are very small uh they can be",
    "start": "1804039",
    "end": "1810880"
  },
  {
    "text": "deployed on CPU so you really don't need an expensive Hardware uh they can range",
    "start": "1810880",
    "end": "1816399"
  },
  {
    "text": "anywhere from a million parameter to about 1.5 billion parameters and in terms of generational model right so",
    "start": "1816399",
    "end": "1822600"
  },
  {
    "text": "generational model you have to figure out what is your use case so typically when you need an agentic kind of a",
    "start": "1822600",
    "end": "1827679"
  },
  {
    "text": "behavior where where you are creating a workflow where you are doing some level of reasoning Etc that's where the model",
    "start": "1827679",
    "end": "1833600"
  },
  {
    "text": "tend to get bigger right so for a typical basic fundamental rag use cases",
    "start": "1833600",
    "end": "1839120"
  },
  {
    "text": "anywhere from 3 billion to 7 billion parameter model should be good enough I think that that should be the thumb rule",
    "start": "1839120",
    "end": "1845679"
  },
  {
    "text": "you should go with",
    "start": "1845679",
    "end": "1849158"
  },
  {
    "text": "size that you would that this should be enough for or 4% accuracy after on top",
    "start": "1858360",
    "end": "1866120"
  },
  {
    "text": "of so okay um so again uh your question is",
    "start": "1866120",
    "end": "1873320"
  },
  {
    "text": "what is the size of the data that you need to fine tune right so uh that's a most asked question actually in AI but",
    "start": "1873320",
    "end": "1880240"
  },
  {
    "text": "uh if your if your domain is not very specific I suggest not to fine tune with",
    "start": "1880240",
    "end": "1885880"
  },
  {
    "text": "uh first first THM rule second is uh whatever base model that you have selected to F tune right uh look at what",
    "start": "1885880",
    "end": "1893399"
  },
  {
    "text": "are the scaling laws that model is using so for example how many tokens per parameter sort of a thing right so",
    "start": "1893399",
    "end": "1899600"
  },
  {
    "text": "probably I think we have broken the chinchila laws long back so uh typically",
    "start": "1899600",
    "end": "1904720"
  },
  {
    "text": "look at that as a hint uh and then over a period of time I think you need to",
    "start": "1904720",
    "end": "1911360"
  },
  {
    "text": "understand one thing that all gen models and don't quote me but all gen models are overfitted so more data better it is",
    "start": "1911360",
    "end": "1919440"
  },
  {
    "text": "uh and if you use something like PFT you can really play with your parameters so you can play with metric rank right",
    "start": "1919440",
    "end": "1927080"
  },
  {
    "text": "eight is recommended if you don't have much data I suggest don't find tune because basically if the domain is",
    "start": "1927080",
    "end": "1932559"
  },
  {
    "text": "understood by the base model rag is there for not for you to fine tune so",
    "start": "1932559",
    "end": "1937919"
  },
  {
    "text": "unless until you have a absolute need to fine tune don't do that I I think the two recommendations that we give to our",
    "start": "1937919",
    "end": "1943360"
  },
  {
    "text": "customers is first is you know exactly on those lines you got to think about what is",
    "start": "1943360",
    "end": "1948720"
  },
  {
    "text": "your end case you know rag usually works a lot better and data is the oil right",
    "start": "1948720",
    "end": "1954279"
  },
  {
    "text": "data is one that keeps it going so the more data you provide the more context it has the more it can create the",
    "start": "1954279",
    "end": "1959559"
  },
  {
    "text": "connections I think the other part that kosel briefly mentioned is you don't have to go for like a 405 billion",
    "start": "1959559",
    "end": "1965600"
  },
  {
    "text": "parameter model right 1 to 7 billion is a 1 to 5 is actually a pretty sweet range if you have enough data that is",
    "start": "1965600",
    "end": "1972240"
  },
  {
    "text": "generating the context for you and usually those models really very well work on a CPU and if you look at the",
    "start": "1972240",
    "end": "1978600"
  },
  {
    "text": "Opia story that we were talking about this is all about inferencing right it's not about training CU flow is more about",
    "start": "1978600",
    "end": "1984519"
  },
  {
    "text": "training and kind of that stuff but once your model is ready then you bring it in here then it's a pure inferencing story",
    "start": "1984519",
    "end": "1990720"
  },
  {
    "text": "and that's why Opia runs very well on just a Shar CPU itself but if you have",
    "start": "1990720",
    "end": "1996639"
  },
  {
    "text": "GPU and you want to leverage that that capability is there too",
    "start": "1996639",
    "end": "2002440"
  },
  {
    "text": "yeah absolutely so as a matter of fact some of our customers are actually working with open AI as the backend and",
    "start": "2018760",
    "end": "2023840"
  },
  {
    "text": "one of the Integrations that we have done in the AWS bedrock with using Amazon Bedrock is how you can use",
    "start": "2023840",
    "end": "2029320"
  },
  {
    "text": "anthropic clae 3 as the backend model so it's not restricted to by any means just",
    "start": "2029320",
    "end": "2036000"
  },
  {
    "text": "to um open source model model what we are seeing is for example customers",
    "start": "2036000",
    "end": "2041760"
  },
  {
    "text": "start with Azure open AI is very good for them very quick for them to get started as the app becomes more popular",
    "start": "2041760",
    "end": "2049520"
  },
  {
    "text": "the cost significantly Rises up so what we seeing is migration away from Azure",
    "start": "2049520",
    "end": "2054720"
  },
  {
    "text": "open AI because end of the day if you see open AI is a wrapper around a vector",
    "start": "2054720",
    "end": "2060599"
  },
  {
    "text": "and an llm and an embedding and a retriever all of that is hiding behind a single API so we have been working with",
    "start": "2060599",
    "end": "2066878"
  },
  {
    "text": "customers Where We Are migrating them from Azure open aai to straight up Azure",
    "start": "2066879",
    "end": "2072839"
  },
  {
    "text": "compute and so they have compute the cost is a bit more under control and then all of those components are",
    "start": "2072839",
    "end": "2079079"
  },
  {
    "text": "existing in Opia already and then most importantly you have the data in your",
    "start": "2079079",
    "end": "2084839"
  },
  {
    "text": "control in your jurisdiction they don't want to send it to openai at all because certain countries for example if you",
    "start": "2084839",
    "end": "2091440"
  },
  {
    "text": "look at subsaharan continent they have requirement that the data cannot leave the continent so those are some of the",
    "start": "2091440",
    "end": "2097000"
  },
  {
    "text": "advantages that we seeing moving away from closed models to straight up",
    "start": "2097000",
    "end": "2103040"
  },
  {
    "text": "Opia uh you mean General challenges I think uh adoption and the behavior so",
    "start": "2119800",
    "end": "2127320"
  },
  {
    "text": "human behavior is very different from uh you can use a chatbot for having fun",
    "start": "2127320",
    "end": "2133160"
  },
  {
    "text": "with a model and all of that right but when it comes to business how do you believe it what chatbot is saying right",
    "start": "2133160",
    "end": "2140119"
  },
  {
    "text": "so uh how do you adopt what is your adoption patterns what is the right way to do it right we always talk about",
    "start": "2140119",
    "end": "2145880"
  },
  {
    "text": "inflow applications right so that is something that we are doing in infosis where a lot of plugins are build on",
    "start": "2145880",
    "end": "2152160"
  },
  {
    "text": "existing app rather than we like for example lot of our code generation",
    "start": "2152160",
    "end": "2157240"
  },
  {
    "text": "related stuff right because we are a big tech company so we have open- Source model with our own plugins uh develop",
    "start": "2157240",
    "end": "2166000"
  },
  {
    "text": "for Eclipse intell Etc where we can enable this experiences the other things are uh in terms of compute uh at one",
    "start": "2166000",
    "end": "2174880"
  },
  {
    "text": "point in time we were limited by compute and thank God Intel and AMD have come up so that's one of the things was",
    "start": "2174880",
    "end": "2181079"
  },
  {
    "text": "restricting factor for a lot of customers to adopt things faster move on Etc and uh I think uh the industry per",
    "start": "2181079",
    "end": "2189720"
  },
  {
    "text": "se is growing very fast so for example in within rag if you see there are tons and tons of patterns and that's where",
    "start": "2189720",
    "end": "2195359"
  },
  {
    "text": "you see the so many examples that we have in Opia right so there are so many patterns so for example one of the",
    "start": "2195359",
    "end": "2201280"
  },
  {
    "text": "things that we in infosis trying to do like somebody asked a question what was the operating margin difference between",
    "start": "2201280",
    "end": "2207400"
  },
  {
    "text": "2024 and 2023 right in that case you are actually doing two queries from that one single",
    "start": "2207400",
    "end": "2213400"
  },
  {
    "text": "question and then combining the answers of two queries and then finding out the differ refences so there are so many",
    "start": "2213400",
    "end": "2219079"
  },
  {
    "text": "patterns that exist today some were there in Lang chain some is there in llama index somebody has written their",
    "start": "2219079",
    "end": "2224640"
  },
  {
    "text": "own papers Etc right so how do you how do you create a blueprint for Enterprises that you are not repeating",
    "start": "2224640",
    "end": "2231720"
  },
  {
    "text": "the same thing over and over right so and that's where Opia makes a great impression all right that's a time up",
    "start": "2231720",
    "end": "2238800"
  },
  {
    "text": "and we're going to hang out by the stage if you have any more questions thank you",
    "start": "2238800",
    "end": "2244720"
  }
]