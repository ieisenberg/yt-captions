[
  {
    "text": "let's get started um hello everyone I'm Jin Chen I'm CS professor at un",
    "start": "80",
    "end": "5400"
  },
  {
    "text": "University of Chicago uh and today I'm happy to join with my student IA here and Joel from uh moon cake labs to share",
    "start": "5400",
    "end": "13599"
  },
  {
    "text": "with you what we believe is the key to unlock the future of armm applications",
    "start": "13599",
    "end": "19520"
  },
  {
    "text": "and this is based on u a decade long research in building large scale AI",
    "start": "19520",
    "end": "25000"
  },
  {
    "text": "systems first of all generally umms have two challenges first of all",
    "start": "25000",
    "end": "32200"
  },
  {
    "text": "uhm training hundreds of millions of dollars have been spent on this and uh only a small number of companies are",
    "start": "32200",
    "end": "38960"
  },
  {
    "text": "really into training new armm models but in contrast millions of applications are",
    "start": "38960",
    "end": "44600"
  },
  {
    "text": "using armm to run armm inference okay um",
    "start": "44600",
    "end": "49640"
  },
  {
    "text": "uh Microsoft for example estimate that uh there will be 750 million applications running armm influencing",
    "start": "49640",
    "end": "56000"
  },
  {
    "text": "the next year and many industry leaders believe that every single apption service will be running armm very soon",
    "start": "56000",
    "end": "63800"
  },
  {
    "text": "so that's why our primary goal in this line of research and development is to",
    "start": "63800",
    "end": "69640"
  },
  {
    "text": "create a better service for armm inference in particular long contacts",
    "start": "69640",
    "end": "74680"
  },
  {
    "text": "armm inference now why long contacts first of all uh long contacts presents",
    "start": "74680",
    "end": "80600"
  },
  {
    "text": "the the the biggest opportunities uh in the next few years um for example uh",
    "start": "80600",
    "end": "87880"
  },
  {
    "text": "there have been a lot of talking about long Contex and even infinite contact Windows these days um with long contacts",
    "start": "87880",
    "end": "94759"
  },
  {
    "text": "for example you can uh feed hundreds of books to your armm and immediately start",
    "start": "94759",
    "end": "99880"
  },
  {
    "text": "asking a detailed questions about these books with long contacts armm inference",
    "start": "99880",
    "end": "105159"
  },
  {
    "text": "you can also dump um a big Cod repository into your armm and immediately starting asking it to create",
    "start": "105159",
    "end": "112320"
  },
  {
    "text": "new test cases uh or even uh add new features to the code base with long",
    "start": "112320",
    "end": "117920"
  },
  {
    "text": "context errm inference you can also Al upload hours of videos from your smartphone to your armm and asking it to",
    "start": "117920",
    "end": "125600"
  },
  {
    "text": "summarize uh memorable moments from this video now the list of exciting",
    "start": "125600",
    "end": "131360"
  },
  {
    "text": "applications enabled by long context ERM inference goes on and on so so that's why uh long context inference capability",
    "start": "131360",
    "end": "139000"
  },
  {
    "text": "is believed to be the key by many people as the key enabler for the next wave ofm",
    "start": "139000",
    "end": "144599"
  },
  {
    "text": "Innovations and applications but there's always a but",
    "start": "144599",
    "end": "150200"
  },
  {
    "text": "long context inference also presents the biggest challenges in terms of the uh",
    "start": "150200",
    "end": "156160"
  },
  {
    "text": "the delay to serve this context in terms of the re the amount of resource you need to process these contacts and also",
    "start": "156160",
    "end": "162840"
  },
  {
    "text": "the quality will suffer because if you let the model only read the context once the model may not be able to create to",
    "start": "162840",
    "end": "169800"
  },
  {
    "text": "find all the useful information in it okay so here I use the uh the two",
    "start": "169800",
    "end": "175599"
  },
  {
    "text": "figures to show you the the increase of delay and also the increase of amount of resources you need to process long",
    "start": "175599",
    "end": "182120"
  },
  {
    "text": "context with longer context the model need longer time to process context and",
    "start": "182120",
    "end": "187200"
  },
  {
    "text": "also the amount of resource you need to process the long context grow super linearly with the with the length of",
    "start": "187200",
    "end": "193480"
  },
  {
    "text": "context and this is true even with all these spity based optimizations okay so",
    "start": "193480",
    "end": "198959"
  },
  {
    "text": "just to give you a context uh just give you some perspective um a large Cod",
    "start": "198959",
    "end": "204040"
  },
  {
    "text": "repository uh the length of the larg code repository will be uh something like a kubernetes codebase will be 2",
    "start": "204040",
    "end": "210599"
  },
  {
    "text": "million tokens um a long book a 200 Page book",
    "start": "210599",
    "end": "215760"
  },
  {
    "text": "will be about 250k tokens and about and in a one hour video will contain at",
    "start": "215760",
    "end": "222080"
  },
  {
    "text": "least 360 uh thousand tokens and all these token numbers are way towards the",
    "start": "222080",
    "end": "228799"
  },
  {
    "text": "right of these figures okay so clearly this a big problem now to unleash the potential of",
    "start": "228799",
    "end": "237640"
  },
  {
    "text": "long contact inference what we need need is a better system a better system that can serve long contact inference with",
    "start": "237640",
    "end": "244920"
  },
  {
    "text": "lower delay and lower cost and add very high quality as well okay and we believe",
    "start": "244920",
    "end": "250720"
  },
  {
    "text": "we have a a answer to that and that's driven by our decad long research in large scale AI system uh building large",
    "start": "250720",
    "end": "257280"
  },
  {
    "text": "scale AI systems the key Insight here is you need a better abstraction for the",
    "start": "257280",
    "end": "263479"
  },
  {
    "text": "long contact data traditionally long contact data will be used as part of the",
    "start": "263479",
    "end": "268680"
  },
  {
    "text": "input so called in contact learning the the contact will be uh viewed as part of",
    "start": "268680",
    "end": "274360"
  },
  {
    "text": "the input tokens now that's why the model needs a lot of time to process it",
    "start": "274360",
    "end": "279400"
  },
  {
    "text": "now that's very slow and the on the other hand soal fine-tuning models AR and fine tuning embeds those new",
    "start": "279400",
    "end": "286600"
  },
  {
    "text": "contacts or new knowledge in the models weights but it takes a lot of time to to",
    "start": "286600",
    "end": "291759"
  },
  {
    "text": "fine tune the models in the first place and these fine tunings usually takes hours to tens of hours to even several",
    "start": "291759",
    "end": "298240"
  },
  {
    "text": "days and sometimes you don't get it right in first first time our Insight is that the abstraction for long contact",
    "start": "298240",
    "end": "305120"
  },
  {
    "text": "data should not be the tokens also should not be the model weights it should be the KV cache so what is KV",
    "start": "305120",
    "end": "310840"
  },
  {
    "text": "cach KV cache is the internal understanding of the long content of the large language model on the long contact",
    "start": "310840",
    "end": "317680"
  },
  {
    "text": "data okay so you feed the long contact data to the model once and the model can create internal understanding which is",
    "start": "317680",
    "end": "324440"
  },
  {
    "text": "the KB cache so that's why if you if you use KB cache uh as a representation of",
    "start": "324440",
    "end": "330319"
  },
  {
    "text": "the long context the model can use it and understand it directly and that's the high level reason why we focused on",
    "start": "330319",
    "end": "336720"
  },
  {
    "text": "Long context why we focus on KV cach as the abstraction for long contact service",
    "start": "336720",
    "end": "342560"
  },
  {
    "text": "and LM cache uh is basically this uh this open source project we have that's inspired by this new obstruction called",
    "start": "342560",
    "end": "349800"
  },
  {
    "text": "KV cach and it's it's highly optimized implementation for managing and serving",
    "start": "349800",
    "end": "355080"
  },
  {
    "text": "long contact data in the form of KB caches because long because KB cach",
    "start": "355080",
    "end": "360199"
  },
  {
    "text": "already captures the model's internal understanding of the long contact data um serving the long contact inference",
    "start": "360199",
    "end": "366680"
  },
  {
    "text": "with KB cach can massively speed up the inference and also it avoids a lot of repeated computation because if the",
    "start": "366680",
    "end": "373400"
  },
  {
    "text": "contact is used again and again and uh look at by the model itself and also",
    "start": "373400",
    "end": "378680"
  },
  {
    "text": "because you can can change the content in the KV cache you have a good chance to improve the quality uh which is not",
    "start": "378680",
    "end": "384800"
  },
  {
    "text": "covered in this slide and all these benefit can happen without changing the",
    "start": "384800",
    "end": "391160"
  },
  {
    "text": "model and the context okay or all the all the prompt so it's totally transparent to the applications running",
    "start": "391160",
    "end": "397560"
  },
  {
    "text": "on top of it now enough talking about abstraction ideas of a arum cach so",
    "start": "397560",
    "end": "403680"
  },
  {
    "text": "let's just look at quickly look at the the the speed up okay so compared to vrm",
    "start": "403680",
    "end": "409319"
  },
  {
    "text": "vrm probably a lot of you guys have heard about it's a widely used open source project in industry for its",
    "start": "409319",
    "end": "415120"
  },
  {
    "text": "Superior performance over Alternatives such as TGI or AMA but compared to vrm",
    "start": "415120",
    "end": "421639"
  },
  {
    "text": "uhm cach can still reduce the response delay the called token time to First",
    "start": "421639",
    "end": "427160"
  },
  {
    "text": "token by 10x in popular applications such as multiun QA uh retrieval",
    "start": "427160",
    "end": "432280"
  },
  {
    "text": "augmented generation or rack okay now again I want to emphasize that Aram cache is a a system on top of existing",
    "start": "432280",
    "end": "440280"
  },
  {
    "text": "uh serving engine so uh it's actually built on top of vrm so if you use vrm you can use armm cache uh directly and",
    "start": "440280",
    "end": "448639"
  },
  {
    "text": "uh not just serving in uhm inference faster it can also reduce the cost",
    "start": "448639",
    "end": "453759"
  },
  {
    "text": "compared to I mean if if we want to achieve the same uh serving throughput",
    "start": "453759",
    "end": "458919"
  },
  {
    "text": "on the same model um we here we compare to uh our serving cost with AWS or AWS",
    "start": "458919",
    "end": "465560"
  },
  {
    "text": "on demand service or any scale these popular Services the cost can can be 5 to 10x cheaper depending on the work the",
    "start": "465560",
    "end": "471840"
  },
  {
    "text": "characteristics of about the workload so before I hand over to the student and collaborators to show you the demos I",
    "start": "471840",
    "end": "478680"
  },
  {
    "text": "want to quickly go over the architecture of this system there is a request router",
    "start": "478680",
    "end": "484759"
  },
  {
    "text": "at the top that handles all the request and and map that to uh serving parts and",
    "start": "484759",
    "end": "489919"
  },
  {
    "text": "each serving Parts runs a state-of art vrm serving systems uh we collaborate with vrm so that this the serving Parts",
    "start": "489919",
    "end": "497039"
  },
  {
    "text": "can run the latest vrm version and behind each vrm there's a frontend",
    "start": "497039",
    "end": "502440"
  },
  {
    "text": "server of armm cash that handles the interaction with vrn and also Maps the",
    "start": "502440",
    "end": "507520"
  },
  {
    "text": "KB cach back to the backand and back end will be a efficient storing storage system of KB cach on top of hetrogeneous",
    "start": "507520",
    "end": "514360"
  },
  {
    "text": "storage Hardware um so a a very quick walk walk",
    "start": "514360",
    "end": "519640"
  },
  {
    "text": "through of the of the process of storing KV cache looks like this a request comes in the router map that to one of the",
    "start": "519640",
    "end": "526120"
  },
  {
    "text": "serving Parts the sing Parts runs vrm to create a KB cach now this is the first time you you see this contact you have",
    "start": "526120",
    "end": "532320"
  },
  {
    "text": "to run inference once but when the the vrm create the KB cach internally the KB",
    "start": "532320",
    "end": "537800"
  },
  {
    "text": "uh the V vrm will tell our back end but sorry our front end uh here is a KB cach",
    "start": "537800",
    "end": "542959"
  },
  {
    "text": "a new KB cach the first first time I see a new context where to store the KB cach and and frontend we ask a KV cach",
    "start": "542959",
    "end": "549959"
  },
  {
    "text": "manager uh to decide which backend server or where uh which backend servers uh to store the KV cach in this case it",
    "start": "549959",
    "end": "556920"
  },
  {
    "text": "can be distributed uh stored in different in different locations so that's storing KV cach what about",
    "start": "556920",
    "end": "563240"
  },
  {
    "text": "retrieving KB cach now if the next time a request with a very mean similar uh",
    "start": "563240",
    "end": "568519"
  },
  {
    "text": "contacts com in again uh the vrm will ask our frontend server have you seen this contacts have you do we have the KB",
    "start": "568519",
    "end": "575720"
  },
  {
    "text": "cach of this contacts in the first place uh the frontend server will ask the KV cach manager to get the location of the",
    "start": "575720",
    "end": "581640"
  },
  {
    "text": "store KV cach and they can be merged together so this all sounds quite familiar with you uh if if you know a",
    "start": "581640",
    "end": "588399"
  },
  {
    "text": "bit a little bit about KB cach related research but what's new here is that the KB cash is not stored as uh its original",
    "start": "588399",
    "end": "596640"
  },
  {
    "text": "big tensor format we speed it up by compressing KV cache into bitstream so",
    "start": "596640",
    "end": "602079"
  },
  {
    "text": "that KB cach can be flown easily and quickly across different nodes across different parts and more uh more than",
    "start": "602079",
    "end": "609720"
  },
  {
    "text": "just uh compressing KV cache we also allow KV cach to be composed dynamically",
    "start": "609720",
    "end": "614880"
  },
  {
    "text": "uh in a way that if you have input with multiple contacts in it at the same time the KB cache me let's say if you have",
    "start": "614880",
    "end": "621200"
  },
  {
    "text": "seen this these contact documents separately before you can combine the KB",
    "start": "621200",
    "end": "626240"
  },
  {
    "text": "cach uh in a dynamic way so that you don't have to to to reread the whole documents over and over again okay so",
    "start": "626240",
    "end": "633200"
  },
  {
    "text": "these are two uh mean I I just highlight these two optimizations there are a lot of engineering optimizations behind Aram",
    "start": "633200",
    "end": "638880"
  },
  {
    "text": "catch system uh but but I think I've talked my highight two high two optimizations and there are many more uh",
    "start": "638880",
    "end": "645360"
  },
  {
    "text": "feel free to check out the the code repository and now I'm handing the stage to Iha to uh give you guys a a quick um",
    "start": "645360",
    "end": "653519"
  },
  {
    "text": "demo of the whole thing the real thing let me try to",
    "start": "653519",
    "end": "660120"
  },
  {
    "text": "okay oh I think I need to maybe stop",
    "start": "660120",
    "end": "667480"
  },
  {
    "text": "uh oh I see cool Okay cool so yeah uh so",
    "start": "667480",
    "end": "673959"
  },
  {
    "text": "what we what we have here what we have here is uh we are uh we having some",
    "start": "673959",
    "end": "679639"
  },
  {
    "text": "Lambda servers running with uh a6000 and deploying some VM and V with cach uh so",
    "start": "679639",
    "end": "686360"
  },
  {
    "text": "basically what what we are currently doing is we have a uh M cach with VM deployed on kubernetes",
    "start": "686360",
    "end": "693959"
  },
  {
    "text": "uh environment so if you do this Cube CDL uh get PS you will see the here is",
    "start": "693959",
    "end": "700440"
  },
  {
    "text": "the MC VM deployment and also as here as well if you if we do the c c get parts",
    "start": "700440",
    "end": "707160"
  },
  {
    "text": "uh we we are seeing there's a stand of V deployed here and based on this",
    "start": "707160",
    "end": "713000"
  },
  {
    "text": "deployment based on the suring engine deployment uh we're running some uh very simple rag application uh on the front",
    "start": "713000",
    "end": "720760"
  },
  {
    "text": "end so let me open up the this one so on the left hand side here is a VM engine",
    "start": "720760",
    "end": "727320"
  },
  {
    "text": "plus uh M cach with KV blending feature and on the right hand side there's a center v m engine and what we're running",
    "start": "727320",
    "end": "734320"
  },
  {
    "text": "here is basically there's a very simple rag application which is configured to retrieve five Trunks and in the total",
    "start": "734320",
    "end": "741120"
  },
  {
    "text": "there is 25k uh 25,000 tokens so this is a a long context uh example of a long",
    "start": "741120",
    "end": "748040"
  },
  {
    "text": "context although it's not that long uh and the documents the database is more",
    "start": "748040",
    "end": "753880"
  },
  {
    "text": "is it's about a it contains a list of documents and M page about a command",
    "start": "753880",
    "end": "759680"
  },
  {
    "text": "line tool called FM pack and right now let let me go to ask it a few questions and so for example we can ask like what",
    "start": "759680",
    "end": "769560"
  },
  {
    "text": "FM uh so describe",
    "start": "769560",
    "end": "774880"
  },
  {
    "text": "what okay well FFM m inton",
    "start": "774880",
    "end": "781279"
  },
  {
    "text": "words let me copy it to here and okay",
    "start": "781279",
    "end": "787760"
  },
  {
    "text": "let's let's feel the the speed okay on the left hand side so the",
    "start": "787760",
    "end": "794320"
  },
  {
    "text": "m cach is uh generating the answer with just uh time to First token of 1.71",
    "start": "794320",
    "end": "801320"
  },
  {
    "text": "seconds and on right hand side the original V takes uh 5.4 seconds to",
    "start": "801320",
    "end": "807240"
  },
  {
    "text": "previe to process this uh 25k long contact and generate results and let me",
    "start": "807240",
    "end": "813440"
  },
  {
    "text": "maybe just ask another question so say for example uh how how",
    "start": "813440",
    "end": "819880"
  },
  {
    "text": "to specify input uh how to specify input file using FFM",
    "start": "819880",
    "end": "827519"
  },
  {
    "text": "pack pasting here okay it's running like you can see",
    "start": "827519",
    "end": "834920"
  },
  {
    "text": "the the left hand side it almost immediately start start generating the response but on the right hand side the",
    "start": "834920",
    "end": "840959"
  },
  {
    "text": "VM original VM still need a couple of times uh need some time to preprocess",
    "start": "840959",
    "end": "847560"
  },
  {
    "text": "the uh the the retrieved trunks so to conclude like m cach not only like uh",
    "start": "847560",
    "end": "854360"
  },
  {
    "text": "canot only uh reduce the time to First token redu this response delay also it",
    "start": "854360",
    "end": "859480"
  },
  {
    "text": "saves the GPU Cycles when pre-processing those long contexts and yeah that's",
    "start": "859480",
    "end": "864759"
  },
  {
    "text": "basically a quick demo of the mcache and let me hand the flow to to",
    "start": "864759",
    "end": "869920"
  },
  {
    "text": "Joe",
    "start": "869920",
    "end": "872320"
  },
  {
    "text": "here okay cool uh okay cool yeah um thanks for all the",
    "start": "876480",
    "end": "883480"
  },
  {
    "text": "work and thanks for all the great demo and let's actually see like we talk about a lot about the drags already but",
    "start": "883480",
    "end": "890639"
  },
  {
    "text": "let's actually feel a drag in production guess like many of you probably already like seeing Rags but like let's feel a",
    "start": "890639",
    "end": "897360"
  },
  {
    "text": "actual raging production so this is R is it playing",
    "start": "897360",
    "end": "905360"
  },
  {
    "text": "it how to play it",
    "start": "909680",
    "end": "913399"
  },
  {
    "text": "from okay yeah so this is like a chat app we took from one like the great post",
    "start": "922880",
    "end": "930560"
  },
  {
    "text": "grass companies and then we ask a question like how to do analytics in post grass we didn't slow it down at all",
    "start": "930560",
    "end": "937519"
  },
  {
    "text": "but you can see like just like a heart breaking like 20 second wait until like it actually start generating question",
    "start": "937519",
    "end": "943959"
  },
  {
    "text": "like the answer so actual Genera is pretty good like we know the companies we work with like the r application",
    "start": "943959",
    "end": "951079"
  },
  {
    "text": "Builders to make it like try different retrieval techniques or improve the quality but like there heting award like",
    "start": "951079",
    "end": "958759"
  },
  {
    "text": "there no way to get the time to first first token down and I'm myself a database person so",
    "start": "958759",
    "end": "966399"
  },
  {
    "text": "the first thing like we try always in this kind of application like Okay so let's make database better so we",
    "start": "966399",
    "end": "973360"
  },
  {
    "text": "actually spend time to build like a state of our Vector database and then when we start handing them into like the",
    "start": "973360",
    "end": "979880"
  },
  {
    "text": "application developers hands we start to realize it's actually don't matter at all like when looking at the whole like",
    "start": "979880",
    "end": "986319"
  },
  {
    "text": "time uh span across the rag application the retrieval comparing",
    "start": "986319",
    "end": "992480"
  },
  {
    "text": "to like the reranking and generation using RM it does not matter at all so",
    "start": "992480",
    "end": "997920"
  },
  {
    "text": "we're not actually optimizing anything here then we'll start looking at okay what's the actual solution well that's",
    "start": "997920",
    "end": "1005000"
  },
  {
    "text": "when I uh found the RM cach project I think okay uh maybe it's not a data",
    "start": "1005000",
    "end": "1011720"
  },
  {
    "text": "problem at all so RM cach will actually shrink the time to First token by shrink",
    "start": "1011720",
    "end": "1017959"
  },
  {
    "text": "the prefilling time of RM great that's actual",
    "start": "1017959",
    "end": "1023079"
  },
  {
    "text": "solution well but wait a second I soon start to learn like each",
    "start": "1023079",
    "end": "1029360"
  },
  {
    "text": "RM cach is actually on the magnitude of like hundreds of megabytes or to",
    "start": "1029360",
    "end": "1034480"
  },
  {
    "text": "gigabytes so we are actually still having another data problem and the actual bigger data problem since like",
    "start": "1034480",
    "end": "1041400"
  },
  {
    "text": "the what we store actually bigger we need to manage the KV",
    "start": "1041400",
    "end": "1047120"
  },
  {
    "text": "cache cool so let's actually try to rebuild the batter rag or better gener",
    "start": "1047120",
    "end": "1053160"
  },
  {
    "text": "application uh hope hopefully most of you should be familiar with like this the classic rag if you're not like just",
    "start": "1053160",
    "end": "1059080"
  },
  {
    "text": "please read it so uh the classic rag works by you have documents and you have",
    "start": "1059080",
    "end": "1064880"
  },
  {
    "text": "data you throw them in into like a vector database uh embedding them and",
    "start": "1064880",
    "end": "1069919"
  },
  {
    "text": "then at uh at quiry time people will quy the vector database to retrieve the",
    "start": "1069919",
    "end": "1075440"
  },
  {
    "text": "relevant information and then like you all throw the contacts and prompts as",
    "start": "1075440",
    "end": "1081679"
  },
  {
    "text": "like natureal language into like RM and then the RM is will be generating the",
    "start": "1081679",
    "end": "1087600"
  },
  {
    "text": "response okay let's see like how can we uh actually utilize LM cache and then",
    "start": "1087600",
    "end": "1093280"
  },
  {
    "text": "put that into this whole diagram the hope is like apps will be able to manage",
    "start": "1093280",
    "end": "1099080"
  },
  {
    "text": "the KV cache just as it's like managing all the vectors all the embedding in the vector database so instead of just",
    "start": "1099080",
    "end": "1106360"
  },
  {
    "text": "storing the data in Vector database we start to pre-process them so we can like",
    "start": "1106360",
    "end": "1111720"
  },
  {
    "text": "generate a lot of like for the probably the more useful longer contacts pre",
    "start": "1111720",
    "end": "1117640"
  },
  {
    "text": "process them with like RM and then generate and store those stays in RM cache so in query time like we don't if",
    "start": "1117640",
    "end": "1125720"
  },
  {
    "text": "you find something already in the RM cach you no longer need to feel feed the whole like uh contact and prompt in",
    "start": "1125720",
    "end": "1134000"
  },
  {
    "text": "nature language you directly serve the cach content which will be much faster",
    "start": "1134000",
    "end": "1140320"
  },
  {
    "text": "but while do you actually expect application Builder to actually manage in addition to managing a vector",
    "start": "1140320",
    "end": "1146000"
  },
  {
    "text": "database also start to manage a KV cache what what if we can provide an end",
    "start": "1146000",
    "end": "1152720"
  },
  {
    "text": "to endend solution so this is like what Monon is trying to do it's we are trying to build a mod data L guess for people",
    "start": "1152720",
    "end": "1160200"
  },
  {
    "text": "like don't in the data Community essentially data L is like you have Object Store people start to throw like",
    "start": "1160200",
    "end": "1166919"
  },
  {
    "text": "all kinds of different like arbitrary data into it and then build things on top of that to like get value from it so",
    "start": "1166919",
    "end": "1174440"
  },
  {
    "text": "what would that look like in this scenario so people would just like store data as like data L",
    "start": "1174440",
    "end": "1180360"
  },
  {
    "text": "tables uh those will just uh store on object stores and then on top of that",
    "start": "1180360",
    "end": "1186720"
  },
  {
    "text": "instead of like just moving data into a vector database and index them we directly build some Vector indexes on",
    "start": "1186720",
    "end": "1193240"
  },
  {
    "text": "the object store directly and then in addition seem like the data Lake already",
    "start": "1193240",
    "end": "1200400"
  },
  {
    "text": "have a lot of the background processes to do optimizations to do compaction",
    "start": "1200400",
    "end": "1205840"
  },
  {
    "text": "it's very nature to also add something to just pre-process the documents and then to generate those LM State and the",
    "start": "1205840",
    "end": "1213799"
  },
  {
    "text": "LM State can also be just stored in the object store uh although they're pretty",
    "start": "1213799",
    "end": "1219200"
  },
  {
    "text": "big but like storing an object store is cheap enough and then for like the user application at query time you just query",
    "start": "1219200",
    "end": "1226919"
  },
  {
    "text": "the whole data L and do the retrieval and it will automatically like either retrieve the context or retrieve the",
    "start": "1226919",
    "end": "1233400"
  },
  {
    "text": "actual RM cach and serve the op in the optimal format to RM to make it",
    "start": "1233400",
    "end": "1239919"
  },
  {
    "text": "faster yeah this is essentially I see what we saw like the r Evolution over",
    "start": "1239919",
    "end": "1245679"
  },
  {
    "text": "well it's kind hard to believe it's already two years so at the start everyone's having like in memory Vector",
    "start": "1245679",
    "end": "1252280"
  },
  {
    "text": "databases and it's pretty fast especially the vector search itself it's like extremely fast",
    "start": "1252280",
    "end": "1259280"
  },
  {
    "text": "but the cost at scale was like crazy people would spend like 10 times or 100",
    "start": "1259280",
    "end": "1264320"
  },
  {
    "text": "times their what they spend on regular databases then they spend on like in memory Vector",
    "start": "1264320",
    "end": "1270919"
  },
  {
    "text": "databases then people start to like when they have like bigger workloads or they",
    "start": "1270919",
    "end": "1276200"
  },
  {
    "text": "realize like retrieval does not matter as much they start to use like Object Store based Vector databases that's like",
    "start": "1276200",
    "end": "1283360"
  },
  {
    "text": "Lance DB or Proto Proto buffer so the cost was much much better since like you",
    "start": "1283360",
    "end": "1290559"
  },
  {
    "text": "no longer pay the in memory it's like state state less everything stores on Object Store but the performance like",
    "start": "1290559",
    "end": "1297760"
  },
  {
    "text": "even the retrieval part is getting slower and then now we see like let's",
    "start": "1297760",
    "end": "1303679"
  },
  {
    "text": "what if we actually put like vector DB and RM cach both in Object Store the",
    "start": "1303679",
    "end": "1309159"
  },
  {
    "text": "cost was still great you store a little more uh documents but it's on Object Store so cost is not the highest",
    "start": "1309159",
    "end": "1316880"
  },
  {
    "text": "priority and then the performance is actually better and even better than the previous in memory",
    "start": "1316880",
    "end": "1323720"
  },
  {
    "text": "implementations okay let's actually try to fill it in",
    "start": "1324279",
    "end": "1329960"
  },
  {
    "text": "uh video demo so we are extending the KV cat to actually like we we store the KV",
    "start": "1329960",
    "end": "1337360"
  },
  {
    "text": "cat inside object stores and then on the other side like it's just feeding from local in memory",
    "start": "1337360",
    "end": "1344360"
  },
  {
    "text": "databases let's see the performance",
    "start": "1344360",
    "end": "1349278"
  },
  {
    "text": "so you can see that like we can serve from Object Store even faster than",
    "start": "1350080",
    "end": "1355240"
  },
  {
    "text": "serving from local in memory state by storing the LM cach together with your",
    "start": "1355240",
    "end": "1363120"
  },
  {
    "text": "data",
    "start": "1363120",
    "end": "1366120"
  },
  {
    "text": "cool let me just uh quickly summarize um first of all uh Aram cach this project",
    "start": "1370919",
    "end": "1377440"
  },
  {
    "text": "in collaboration with lab uh we strive to make this thing uh as as uh easy to",
    "start": "1377440",
    "end": "1383760"
  },
  {
    "text": "to use and deploy as possible for our users uh including container images including Library based uh deployment",
    "start": "1383760",
    "end": "1390120"
  },
  {
    "text": "and also on cloud and uh um this project is not I mean doesn't exist in a vacuum",
    "start": "1390120",
    "end": "1396919"
  },
  {
    "text": "uh we collaborate with both open source communities and also industry uh design Partners um and we CL closely work with",
    "start": "1396919",
    "end": "1405080"
  },
  {
    "text": "open source communities to leverage their latest serving engines to improve our basic serving performance and also",
    "start": "1405080",
    "end": "1411440"
  },
  {
    "text": "to be able to run different models on different Hardware okay so that's the power of Open Source community and as we",
    "start": "1411440",
    "end": "1417880"
  },
  {
    "text": "get more industry adopters uh we always looking for more use cases from you guys if you if you feel like you want to",
    "start": "1417880",
    "end": "1423360"
  },
  {
    "text": "contribute or you you have uh a good use case for long contact armm serving uh please reach out to us okay or scan the",
    "start": "1423360",
    "end": "1430600"
  },
  {
    "text": "uh the QR code down there so just want to quickly summarize the the whole thing first of all like I said airm inference",
    "start": "1430600",
    "end": "1437400"
  },
  {
    "text": "in particular long context inference is going to be very important uh present the biggest opportunity but also it",
    "start": "1437400",
    "end": "1443120"
  },
  {
    "text": "creates the biggest system challenges and to unleash the potential of long",
    "start": "1443120",
    "end": "1448880"
  },
  {
    "text": "contact inference we need efficient system and in our belief the best system to serve long contact inference will be",
    "start": "1448880",
    "end": "1454960"
  },
  {
    "text": "the system that can manage KB cach efficiently okay that's the key to the success and Aram cach is this one",
    "start": "1454960",
    "end": "1461159"
  },
  {
    "text": "instance uh we build this to combine by combining latest vrm uh serving engine",
    "start": "1461159",
    "end": "1466440"
  },
  {
    "text": "and the latest KV cache research uh and Technologies and moon cake uh Labs um uh",
    "start": "1466440",
    "end": "1472520"
  },
  {
    "text": "create this this this enabler for us to make this trade-off so what is trade-off here the trade-off is you need to store",
    "start": "1472520",
    "end": "1478320"
  },
  {
    "text": "a lot of KB cach in um um in Big Data format and those KB cach uh doesn't have",
    "start": "1478320",
    "end": "1484279"
  },
  {
    "text": "to to to exist in your CPU memory or local dis it can store on S3 and moon",
    "start": "1484279",
    "end": "1490039"
  },
  {
    "text": "mooni lab enables that I think that that create the whole ecosystem of KB I mean creat the the sort of the initial",
    "start": "1490039",
    "end": "1496120"
  },
  {
    "text": "version of the whole ecosystem of KB cas optimization that's the that's the vision okay so thanks for uh for tuning",
    "start": "1496120",
    "end": "1502679"
  },
  {
    "text": "in and uh we're happy to take any questions",
    "start": "1502679",
    "end": "1507000"
  },
  {
    "text": "now hi uh great presentation um quick question uh can you not use an object",
    "start": "1511399",
    "end": "1517080"
  },
  {
    "text": "store in cluster like for example Min iio or or a redis uh blob",
    "start": "1517080",
    "end": "1524559"
  },
  {
    "text": "store oh good good good question yeah absolutely like like there's many ways to store this KV Cates but so you can",
    "start": "1524840",
    "end": "1531720"
  },
  {
    "text": "store it locally or even just store it in memory or uh even in GPU memory but",
    "start": "1531720",
    "end": "1538039"
  },
  {
    "text": "like uh we uh but they are very large so in",
    "start": "1538039",
    "end": "1543159"
  },
  {
    "text": "order to like efficiently like store them you want to store store it on something like cheap enough and ideally",
    "start": "1543159",
    "end": "1549279"
  },
  {
    "text": "like more Cloud native you want to like actually separate separate the storage and the compu so you kind like serveice",
    "start": "1549279",
    "end": "1555520"
  },
  {
    "text": "on any Compu in instance without need to like worry about if do you actually have",
    "start": "1555520",
    "end": "1561679"
  },
  {
    "text": "the cat locally thanks hello um I have a question in",
    "start": "1561679",
    "end": "1570320"
  },
  {
    "text": "terms of the infrastructure you put so you have the m cach to catch a vctor",
    "start": "1570320",
    "end": "1575799"
  },
  {
    "text": "database is am I understand correctly uh so yes you have to store K cat somewhere",
    "start": "1575799",
    "end": "1582000"
  },
  {
    "text": "and database will be Natural Choice to store them yes to index them so if I catch the slide correctly you saw the",
    "start": "1582000",
    "end": "1587799"
  },
  {
    "text": "pre LM State on the LM cast so what exactly is the pre-processed LM State",
    "start": "1587799",
    "end": "1594559"
  },
  {
    "text": "the pre oh so so so I asking how to get I mean how to um how to RM create the Fe",
    "start": "1594559",
    "end": "1600120"
  },
  {
    "text": "the KB cat in the first place is that a question no um so you get the pre so in",
    "start": "1600120",
    "end": "1607080"
  },
  {
    "text": "the diagram you put a pre-processed llm state on the to store in the LM cach so",
    "start": "1607080",
    "end": "1613559"
  },
  {
    "text": "I'm not sure what SE the pre-process LM cach is yeah oh State yeah uh so",
    "start": "1613559",
    "end": "1622080"
  },
  {
    "text": "initially you just th like throwing data into this like tables and then you need to somehow like generate RM States then",
    "start": "1622080",
    "end": "1629960"
  },
  {
    "text": "later you can just serve them so here this is the the pre process essentially",
    "start": "1629960",
    "end": "1635080"
  },
  {
    "text": "in the background you want to start like use RM is ask it like what would be the",
    "start": "1635080",
    "end": "1640279"
  },
  {
    "text": "RM what what would be your state if I previe with this document and then you",
    "start": "1640279",
    "end": "1645840"
  },
  {
    "text": "save them then later when you actually need to use it you can directly use the pre-process RM",
    "start": "1645840",
    "end": "1652799"
  },
  {
    "text": "State thank you hi um you mentioned earlier around",
    "start": "1652799",
    "end": "1658600"
  },
  {
    "text": "if you've already pre processed some documents you got them in there you can add new documents without pre-processing the old documents again um but how do",
    "start": "1658600",
    "end": "1665960"
  },
  {
    "text": "you handle things like if you want to handle multiple versions of a document that have like for example the Cub a",
    "start": "1665960",
    "end": "1671840"
  },
  {
    "text": "documents for version 1.2 and 1.3 like just having different",
    "start": "1671840",
    "end": "1678240"
  },
  {
    "text": "versions of the similar document in addition how do you subtract documents back out of there they stored as",
    "start": "1678240",
    "end": "1683480"
  },
  {
    "text": "separate kind of bodies in there so you can say remove that document um I'm so glad you asked that because uh so",
    "start": "1683480",
    "end": "1689679"
  },
  {
    "text": "basically um the KB cach is a function of the text and Al also the model right so you ask about what if the tax changes",
    "start": "1689679",
    "end": "1696799"
  },
  {
    "text": "slightly incrementally right um if you change the tax incrementally the part of the tax that's not changed by that that",
    "start": "1696799",
    "end": "1703120"
  },
  {
    "text": "that that incremental change uh can still hold that KV cat now again but the next time you use the KV cach you have",
    "start": "1703120",
    "end": "1709200"
  },
  {
    "text": "to to update its attention Matrix slightly um now you don't have to to to",
    "start": "1709200",
    "end": "1714880"
  },
  {
    "text": "change the attention Matrix uh completely because only um our key",
    "start": "1714880",
    "end": "1720279"
  },
  {
    "text": "observation in this L of research is um not every pair of tokens have important attentions between them okay so if you",
    "start": "1720279",
    "end": "1727279"
  },
  {
    "text": "change things incrementally uh the the KB cach of those change tokens will",
    "start": "1727279",
    "end": "1732360"
  },
  {
    "text": "definitely be changed but the remaining tokens don't have to be changed a lot uh at all because only a few tokens need to",
    "start": "1732360",
    "end": "1738679"
  },
  {
    "text": "be updated so that's the idea um so just slightly extend your question a little bit like I said KB Cas is depends on",
    "start": "1738679",
    "end": "1744760"
  },
  {
    "text": "both tax and model the model can change as well right maybe you fine tuna model what if the KV cash uh what about the KV",
    "start": "1744760",
    "end": "1751640"
  },
  {
    "text": "cash can it be still reused with uh mean by the new model that can also be reused that's is part of things that the AR",
    "start": "1751640",
    "end": "1758039"
  },
  {
    "text": "cach can support as well uh but again if the the the model is completely changed that all the architectures change as",
    "start": "1758039",
    "end": "1764320"
  },
  {
    "text": "well then the kcat shape will be different so that's the different story then thank you thank",
    "start": "1764320",
    "end": "1769679"
  },
  {
    "text": "you hi um great talk I had a question about like um VM has this prefix caching",
    "start": "1769679",
    "end": "1777039"
  },
  {
    "text": "so is that does that use something like this under the hood or what is the difference so uh short answer to you is",
    "start": "1777039",
    "end": "1784159"
  },
  {
    "text": "vrm itself Only Store KB cach in in GPU memory it does use CPU memory for offloading but it doesn't hold CP across",
    "start": "1784159",
    "end": "1792679"
  },
  {
    "text": "in sort of reuse KV cache across inference uh using CPU memory um so",
    "start": "1792679",
    "end": "1798480"
  },
  {
    "text": "early on we we collaborate with vrm Team closely on this because they also realized that KV cash is something you",
    "start": "1798480",
    "end": "1805039"
  },
  {
    "text": "want to hold for a long time so you definitely need a backhand system to store KV cach not in GPU memory not just",
    "start": "1805039",
    "end": "1811559"
  },
  {
    "text": "in CPU memory but also distributed across multiple nodes so that needs a separate system andm andm cach basically",
    "start": "1811559",
    "end": "1818159"
  },
  {
    "text": "as the open source projects to support that sounds good thanks one more question I had like u in this rag setup",
    "start": "1818159",
    "end": "1824840"
  },
  {
    "text": "you would have a lot of documents that you would create llm uh cash for so when you retrieve it I'm assuming you have",
    "start": "1824840",
    "end": "1831840"
  },
  {
    "text": "some sort of indexing to retrieve only specific cash for that uh Prix in some",
    "start": "1831840",
    "end": "1837279"
  },
  {
    "text": "way so how does that work do you store them how do you store them in S3 bucket",
    "start": "1837279",
    "end": "1843960"
  },
  {
    "text": "and how do you re so um is is the question how to look up the KV cach",
    "start": "1843960",
    "end": "1850039"
  },
  {
    "text": "store to find the KV cach of the right get specific uh portions of the KV cach",
    "start": "1850039",
    "end": "1855760"
  },
  {
    "text": "from yeah uh essentially like KV cach like you kind of stored like next",
    "start": "1855760",
    "end": "1860919"
  },
  {
    "text": "similar to your store your documents essentially like previously you use Vector search to retrieve a document now",
    "start": "1860919",
    "end": "1867120"
  },
  {
    "text": "you use the vector search to retrieve the corresponding key like RM cat entry",
    "start": "1867120",
    "end": "1872600"
  },
  {
    "text": "so it's kind of the indexing and retrieval part is kind of similar okay very similar to Vector DB but instead of",
    "start": "1872600",
    "end": "1879919"
  },
  {
    "text": "yeah yeah but everything is on Object Store sounds cool thank",
    "start": "1879919",
    "end": "1885200"
  },
  {
    "text": "you yeah hi uh how does this work for for structured data like Delta Leake or",
    "start": "1885200",
    "end": "1890639"
  },
  {
    "text": "Iceberg can you use this for structure data structur data data",
    "start": "1890639",
    "end": "1896279"
  },
  {
    "text": "like in of documents can I use it against uh some historical data that we",
    "start": "1896279",
    "end": "1901840"
  },
  {
    "text": "have depending what's your use case for like feeding into the uh large language",
    "start": "1901840",
    "end": "1907880"
  },
  {
    "text": "model at then to generation like for example you if you actually throw in the all the tables as well like the contest",
    "start": "1907880",
    "end": "1914679"
  },
  {
    "text": "itself into RM then definitely but if you actually like for example you run a query first and then throw it into LM",
    "start": "1914679",
    "end": "1922639"
  },
  {
    "text": "then probably you directly build your transitional rack against your database right what would you recommend you would",
    "start": "1922639",
    "end": "1928039"
  },
  {
    "text": "recommend llm to generate a query and then query the database or would you feed the structure data oh yeah I guess",
    "start": "1928039",
    "end": "1935399"
  },
  {
    "text": "it totally depends on your use case uh if you like if the use case is actually like for example bi you actually want",
    "start": "1935399",
    "end": "1942399"
  },
  {
    "text": "like inside to if you actually want to wrong SQL then like wrong SQL against",
    "start": "1942399",
    "end": "1947480"
  },
  {
    "text": "the traditional database is probably better but if your like question are more like ambiguous or more like uh you",
    "start": "1947480",
    "end": "1955320"
  },
  {
    "text": "want to ask some like more human like questions than just directly feding",
    "start": "1955320",
    "end": "1960519"
  },
  {
    "text": "everything to our might be a good choice yeah hypothetical scenario a stock price chart you have and then you want to",
    "start": "1960519",
    "end": "1966600"
  },
  {
    "text": "block a chart but a user wants to talk in a natural language and be able to generate that against some historical",
    "start": "1966600",
    "end": "1972919"
  },
  {
    "text": "data that we download from external yeah so like if it's plot it's us better to like how have oh like there are several",
    "start": "1972919",
    "end": "1979639"
  },
  {
    "text": "ways one is like you can provide the API for function calling to the RM then this",
    "start": "1979639",
    "end": "1985399"
  },
  {
    "text": "will probably do the best job and the next best will be like generate SQL",
    "start": "1985399",
    "end": "1990960"
  },
  {
    "text": "thank you great hey uh last I think I had a quick question um great presentation by",
    "start": "1990960",
    "end": "1996919"
  },
  {
    "text": "the way I love the demo um so the um the the cach that you have right like I was",
    "start": "1996919",
    "end": "2002559"
  },
  {
    "text": "I was concerned about like what is the hit rate of the cach like how often does do you have these this repetition uh for",
    "start": "2002559",
    "end": "2010120"
  },
  {
    "text": "context how often do you update the KB cach for the company yeah yeah I mean or how often do you when you when you have",
    "start": "2010120",
    "end": "2016200"
  },
  {
    "text": "when you look up the vectors to to retrieve your cash like what do you think is um uh like is the repetition",
    "start": "2016200",
    "end": "2023000"
  },
  {
    "text": "like how how often does your cash get a hit rather than having to compute it all over again like what's the hit rate",
    "start": "2023000",
    "end": "2028919"
  },
  {
    "text": "that's a good question so um it really depend on the the application uh the",
    "start": "2028919",
    "end": "2034120"
  },
  {
    "text": "issues the query in the first place chat applications will have pretty frequent uh reuse of KV cach although the KV cach",
    "start": "2034120",
    "end": "2040880"
  },
  {
    "text": "will be about um I mean it's chat history is not going to be very very long context but of course if you have",
    "start": "2040880",
    "end": "2046919"
  },
  {
    "text": "very long conversations the context can grow in indefinitely um for rag Services",
    "start": "2046919",
    "end": "2052240"
  },
  {
    "text": "uh it it also depends on whether there are hot items in the in the database or not right so if you have content I mean",
    "start": "2052240",
    "end": "2058040"
  },
  {
    "text": "contacts that sorry rack sorry chunks in the database that's query but that's",
    "start": "2058040",
    "end": "2063520"
  },
  {
    "text": "been used as a context for many queries those chunks will be uh will will will will be uh hit very frequently much more",
    "start": "2063520",
    "end": "2070560"
  },
  {
    "text": "frequently than others so if you talk about uh exact time duration between two",
    "start": "2070560",
    "end": "2075679"
  },
  {
    "text": "hits of the same chunks uh our I mean so we run some analysis uh for our own research papers so so so so we use uh",
    "start": "2075679",
    "end": "2082800"
  },
  {
    "text": "traces from moonot AI from Microsoft and and and also um I forgot the other",
    "start": "2082800",
    "end": "2088919"
  },
  {
    "text": "company but uh they open source traces they don't necessarily represent real um",
    "start": "2088919",
    "end": "2094118"
  },
  {
    "text": "the the the most representative workload in real world but uh the the the impression I got was about tens of",
    "start": "2094119",
    "end": "2100720"
  },
  {
    "text": "seconds to several minutes that that's sort of the medium now of course there are are situations you reuse the KV cat",
    "start": "2100720",
    "end": "2107720"
  },
  {
    "text": "just within a several seconds to hours um what I want to emphasize is uh",
    "start": "2107720",
    "end": "2112920"
  },
  {
    "text": "traditionally people think about KV cash to be re me to be stored in in GPU uh",
    "start": "2112920",
    "end": "2118720"
  },
  {
    "text": "that only applies when the when the ReUse had uh happens within several seconds which is not a common case in",
    "start": "2118720",
    "end": "2125240"
  },
  {
    "text": "real world so so that's why you definitely need a exension of the KV Cas storage like AR cach got it for longer",
    "start": "2125240",
    "end": "2131160"
  },
  {
    "text": "durations of retention exactly exactly so I think we're we're running out of time uh thanks for everyone for tuning",
    "start": "2131160",
    "end": "2136800"
  },
  {
    "text": "in um",
    "start": "2136800",
    "end": "2140560"
  }
]