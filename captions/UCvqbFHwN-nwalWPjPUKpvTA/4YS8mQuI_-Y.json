[
  {
    "text": "uh hello everyone um Welcome to our session uh sharing is caring uh GPU sharing and CDI and device plugins uh my",
    "start": "120",
    "end": "7560"
  },
  {
    "text": "name is David Porter uh I'm from Google I work on the Google kubernetes engine team uh where I work on uh node and kind",
    "start": "7560",
    "end": "14920"
  },
  {
    "text": "of our integration with accelerators and uh this is Chris yeah I'm Chris desis um",
    "start": "14920",
    "end": "20480"
  },
  {
    "text": "I'm a software engineer at Nvidia I work on our Cloud native engineering team that works on enabling gpus in the",
    "start": "20480",
    "end": "27119"
  },
  {
    "text": "container runtime ecosystem and in kubernetes cool so to get started let me just kind",
    "start": "27119",
    "end": "33600"
  },
  {
    "text": "of set the landscape uh so the landscape as everyone is well aware during this coupon uh devices are becoming",
    "start": "33600",
    "end": "39640"
  },
  {
    "text": "increasingly important in kubernetes right so we have all these new workloads uh things like inference training uh",
    "start": "39640",
    "end": "46360"
  },
  {
    "text": "fine-tuning Etc and all of them are requiring devices uh so it's important that devices and accelerators are kind",
    "start": "46360",
    "end": "52760"
  },
  {
    "text": "of well integrated in kubernetes uh just like uh other resources are today so uh",
    "start": "52760",
    "end": "58840"
  },
  {
    "text": "what we're going to cover today um we're going to start by talking a little bit about uh how devices are integrated in",
    "start": "58840",
    "end": "64360"
  },
  {
    "text": "kubernetes today uh then we're going to kind of go one level lower at the container runtime level and take a look",
    "start": "64360",
    "end": "70759"
  },
  {
    "text": "at how container runtimes integrate with devices uh and how CDI is a new technology uh that's going to help",
    "start": "70759",
    "end": "76360"
  },
  {
    "text": "enable that uh then we'll we're going to go up the stack a little bit and look at kind of resource management uh with some",
    "start": "76360",
    "end": "82520"
  },
  {
    "text": "of these devices and we're going to focus on gpus and how uh GPU sharing can be enabled and lastly we'll kind of end",
    "start": "82520",
    "end": "88640"
  },
  {
    "text": "with a little bit about uh the future of devices and device sharing uh in kubernetes so I think uh when we think",
    "start": "88640",
    "end": "96280"
  },
  {
    "text": "about it kind of from the kubernetes perspective right I think one of the big reasons kubernetes is so successful and",
    "start": "96280",
    "end": "101759"
  },
  {
    "text": "everybody uh you know really loves running kubernetes is because it's really good at Resource Management uh",
    "start": "101759",
    "end": "107200"
  },
  {
    "text": "what kubernetes allows you to do is take you know your whole Fleet of nodes right with all different types of resources",
    "start": "107200",
    "end": "112640"
  },
  {
    "text": "like CPU memory dis space and really allow you uh to be able to manage them under one control plane with one API and",
    "start": "112640",
    "end": "120119"
  },
  {
    "text": "devices are now becoming increasingly important in that Resource Management space and we want to enable you to",
    "start": "120119",
    "end": "125320"
  },
  {
    "text": "manage them just as effectively as you do with CPU memory and disk today so what we're trying to do from the",
    "start": "125320",
    "end": "131200"
  },
  {
    "text": "community perspective is we're trying to build uh Open Standards here and common resource models both to actually consume",
    "start": "131200",
    "end": "137280"
  },
  {
    "text": "and interact with these devices and uh to share their resources so we're going to be talking about what exists today",
    "start": "137280",
    "end": "143040"
  },
  {
    "text": "and what exists today is the device plugin framework in kubernetes and CDI those are kind of the Open Standards and",
    "start": "143040",
    "end": "148920"
  },
  {
    "text": "Frameworks that allow you to interact with devices today uh in the future and uh something that's been discussed",
    "start": "148920",
    "end": "154519"
  },
  {
    "text": "during this coupon has been a new feature called Dr and Dr is going to be a new API a new way to manage these",
    "start": "154519",
    "end": "160319"
  },
  {
    "text": "resources and share them more natively so let's talk a little bit about devices in kubernetes what what is",
    "start": "160319",
    "end": "166920"
  },
  {
    "text": "a device when we talk about uh devices here so a device we'd like to think about it as kind of this abstract",
    "start": "166920",
    "end": "172560"
  },
  {
    "text": "concept an abstract resource that the user wants to use for some specific resource uh some specific purpose and it",
    "start": "172560",
    "end": "179080"
  },
  {
    "text": "may be uh that it's a physical Hardware device but it may not be and so usually",
    "start": "179080",
    "end": "184200"
  },
  {
    "text": "these devices uh they don't just come in isolation there's kind of a whole ecosystem around them so usually if it's",
    "start": "184200",
    "end": "189920"
  },
  {
    "text": "a hardware device you'll actually need some kind of Kernel uh you know kernel drivers you'll need to access some",
    "start": "189920",
    "end": "195879"
  },
  {
    "text": "device nodes to actually talk to that device and probably you'll need some libraries and utilities as well to",
    "start": "195879",
    "end": "201120"
  },
  {
    "text": "actually be able to use that device in your application so this whole bundle uh of things is usually what we call the",
    "start": "201120",
    "end": "206799"
  },
  {
    "text": "device and so in kubernetes today we have this AP called the extended resources API and the extended resources",
    "start": "206799",
    "end": "213560"
  },
  {
    "text": "API allows uh kuber in kubernetes to basically advertise resources uh in the",
    "start": "213560",
    "end": "219879"
  },
  {
    "text": "in under the node uh just like CPU and memory and we can advertise these devices and account so the devices are",
    "start": "219879",
    "end": "227040"
  },
  {
    "text": "countable and have an integer count associated with them the way the devices are integrated in kubernetes is with",
    "start": "227040",
    "end": "233040"
  },
  {
    "text": "something called the device plug-in framework and the device plugin framework uh is a kind of a uh plug",
    "start": "233040",
    "end": "239959"
  },
  {
    "text": "framework that the vendor of the kind of device usually writes and so the device plugin framework usually has three calls",
    "start": "239959",
    "end": "246400"
  },
  {
    "text": "here it has a register call an allocate call and kind of Health checking so",
    "start": "246400",
    "end": "251680"
  },
  {
    "text": "usually when uh the device uh starts up it actually does some registration and what this means is that the device will",
    "start": "251680",
    "end": "258440"
  },
  {
    "text": "start up and it'll actually talk to uh kuet and it'll advertise a name so for example for uh gpus the name is",
    "start": "258440",
    "end": "264880"
  },
  {
    "text": "nvidia.com GPU and it'll advertise account how many of that resource we have then when a workload comes in",
    "start": "264880",
    "end": "271000"
  },
  {
    "text": "there's the allocation step the allocation step is when we actually figure out what device to allocate for",
    "start": "271000",
    "end": "276520"
  },
  {
    "text": "that workload and how to modify the container so that the workload can actually access the device so things are",
    "start": "276520",
    "end": "281680"
  },
  {
    "text": "like mounting the device nodes the libraries Etc and lastly the device plugins responsible for health checking",
    "start": "281680",
    "end": "287320"
  },
  {
    "text": "right so if the device plug if the device goes unhealthy uh the device plugin will react to that and update the",
    "start": "287320",
    "end": "293120"
  },
  {
    "text": "node uh about that so let's walk through uh kind of an example of how device",
    "start": "293120",
    "end": "298639"
  },
  {
    "text": "plugins actually work in kubernetes today so let's start off here with a node uh let's imagine we provisioned a",
    "start": "298639",
    "end": "304280"
  },
  {
    "text": "node uh like either on Prem or in a cloud provider you already have the gpus attached to it uh the first thing you",
    "start": "304280",
    "end": "309400"
  },
  {
    "text": "would want to do is actually set up uh the kernel GPU drivers here so on a cloud provider this is manage for you",
    "start": "309400",
    "end": "315440"
  },
  {
    "text": "there's also the Nvidia GPU operator uh that installs these drivers for you so you set that up you install it and then",
    "start": "315440",
    "end": "321479"
  },
  {
    "text": "you install the device plugin which again can be managed by C provider the operator and the device plug-in starts",
    "start": "321479",
    "end": "326960"
  },
  {
    "text": "up and the device plugin is usually just a pod that's running there so the first thing the device plugin",
    "start": "326960",
    "end": "332080"
  },
  {
    "text": "does is actually communicate with the GPU uh communicate with the GPU drivers and figure out how many of those gpus",
    "start": "332080",
    "end": "337360"
  },
  {
    "text": "there are uh from then uh the ku's going to go ahead and uh basically see that",
    "start": "337360",
    "end": "343039"
  },
  {
    "text": "there's a new device plugin and ask to do uh the registration so during the registration uh the device plugin is",
    "start": "343039",
    "end": "348720"
  },
  {
    "text": "basically going to say I have a resource that resource name is nvidia.com GPU like that's the name and I have two of",
    "start": "348720",
    "end": "355360"
  },
  {
    "text": "them and uh that's going to be communicated back to kubl and when kuet does that uh sees that it's going to go",
    "start": "355360",
    "end": "362080"
  },
  {
    "text": "actually up to the API server on the control plane and already the ku's responsible to update the node object",
    "start": "362080",
    "end": "368800"
  },
  {
    "text": "capacity field and it'll update the capacity field and just in addition to your CPU and memory you'll have a new",
    "start": "368800",
    "end": "373960"
  },
  {
    "text": "entry there Nvidia com GPU to so at this point all the components in the control",
    "start": "373960",
    "end": "379080"
  },
  {
    "text": "plane can see that this uh that this uh node has this resource attached so at",
    "start": "379080",
    "end": "384400"
  },
  {
    "text": "some later time you have a workload that comes in right and this is on the left side of the diagram here you have a pod come in and and under the container",
    "start": "384400",
    "end": "390880"
  },
  {
    "text": "request there uh it'll request some number of that device so the in this example it's requesting nvidia.com gp1",
    "start": "390880",
    "end": "398479"
  },
  {
    "text": "it'll come in and then uh it'll go up to the scheduler and the Schuler is going to see hey this node has that resource",
    "start": "398479",
    "end": "404319"
  },
  {
    "text": "available it's going to go ahead and do that scheduling and so uh after it does that scheduling the ku's going to see",
    "start": "404319",
    "end": "409880"
  },
  {
    "text": "that hey it needs to start a new POD at this point ku's going to communicate to the device plugin and actually do that",
    "start": "409880",
    "end": "415599"
  },
  {
    "text": "allocation step I mentioned earlier so it's going to figure out hey What GPU do I give it and how do I make that GPU",
    "start": "415599",
    "end": "421080"
  },
  {
    "text": "accessible in the container what modifications need to be made to the container to access that GPU once that's",
    "start": "421080",
    "end": "426560"
  },
  {
    "text": "done Kua goes ahead and goes to the container runtime so this is like uh container D or cryo and actually send",
    "start": "426560",
    "end": "433000"
  },
  {
    "text": "that uh container spec there uh for it to start container d uh or cryo just",
    "start": "433000",
    "end": "438440"
  },
  {
    "text": "pass it along to the low-level container runtime like run C run C goes ahead and actually starts the container does all",
    "start": "438440",
    "end": "444080"
  },
  {
    "text": "the mounting and so forth and um at that point the workload can start and now the",
    "start": "444080",
    "end": "449240"
  },
  {
    "text": "work can actually access the device like the GPU and at this point the workload is directly talking to the GPU it's not",
    "start": "449240",
    "end": "455199"
  },
  {
    "text": "talking to any of these kubernetes components like the device plug-in when it's actually running uh however with some devices the story is not so",
    "start": "455199",
    "end": "461879"
  },
  {
    "text": "straightforward and so simple and so uh Chris here is going to talk a little bit around some of the extra complexities",
    "start": "461879",
    "end": "467120"
  },
  {
    "text": "and how CDI is going to help address them sure thanks David So yeah so for",
    "start": "467120",
    "end": "472720"
  },
  {
    "text": "more complex devices there's actually more that comes into the picture than what was shown on the previous slide so",
    "start": "472720",
    "end": "478520"
  },
  {
    "text": "for NVIDIA gpus you not only need to install a driver and a device plugin but",
    "start": "478520",
    "end": "483560"
  },
  {
    "text": "you also need something called Nidia container toolkit which is a set of tools that ensure that GPU containers",
    "start": "483560",
    "end": "490080"
  },
  {
    "text": "can be set up with all the right things they need to to access to GPU so this diagram on the slide is just like a",
    "start": "490080",
    "end": "495840"
  },
  {
    "text": "sequence diagram from the point when cuet allocates a GPU one or more gpus for your container to all the way down",
    "start": "495840",
    "end": "503000"
  },
  {
    "text": "to when your actual containers is run um this is sort of what it looks like today",
    "start": "503000",
    "end": "508440"
  },
  {
    "text": "and historically right there's um some Nvidia specific components that under the hood make sure that your container",
    "start": "508440",
    "end": "514719"
  },
  {
    "text": "has access to all the right um device nodes uh driver libraries and so forth",
    "start": "514719",
    "end": "520039"
  },
  {
    "text": "so that your Kuda application can run transparently um the the the major sort",
    "start": "520039",
    "end": "525200"
  },
  {
    "text": "of um component that's doing all the heavy lifting is H this step nine that's pre-art hook so it's this Library that's",
    "start": "525200",
    "end": "532240"
  },
  {
    "text": "invoked as a pre-art hook before your container application the main process starts it has a lot of injecting of",
    "start": "532240",
    "end": "538880"
  },
  {
    "text": "device no mounting all the driver libraries and everything you need and it's doing this sort of behind the knowledge of uh a a container runtime",
    "start": "538880",
    "end": "546440"
  },
  {
    "text": "like run C so there's some downsides here um one is this is not declarative",
    "start": "546440",
    "end": "552800"
  },
  {
    "text": "like all of the edits to the containers environment are being happen under the hood and they're not encapsulated in the",
    "start": "552800",
    "end": "558680"
  },
  {
    "text": "container spec so the OCA runtime spec is the standard for that runc uses to",
    "start": "558680",
    "end": "563839"
  },
  {
    "text": "set up a container's environment reny has no idea that some of these things are actually being included in the",
    "start": "563839",
    "end": "568920"
  },
  {
    "text": "container so historically that's led to some hard to debug issues and some inconsistencies when interacting with",
    "start": "568920",
    "end": "575120"
  },
  {
    "text": "GPU containers so ideally we we want to provide a standard um to sort of",
    "start": "575120",
    "end": "582399"
  },
  {
    "text": "overcome this and and do things in a declarative way um and so there's a project called CDI which aims to solve",
    "start": "582399",
    "end": "588600"
  },
  {
    "text": "these problems and and standardize how we access gpus and and other accelerators can and Hardware in",
    "start": "588600",
    "end": "595519"
  },
  {
    "text": "containers so what is CDI it's a container device interface it's a cncf",
    "start": "595519",
    "end": "600800"
  },
  {
    "text": "sponsored project under tag runtime and like I said previously it it",
    "start": "600800",
    "end": "606000"
  },
  {
    "text": "it aims to standardize how containers or how uh thirdparty devices are made",
    "start": "606000",
    "end": "611279"
  },
  {
    "text": "available to Containers the way it does this is It's it uses a declarative specification and",
    "start": "611279",
    "end": "617279"
  },
  {
    "text": "and it defines in that specification you define as a vendor you define what your device actually means what access in the",
    "start": "617279",
    "end": "623279"
  },
  {
    "text": "device means that can be a list of device nodes mounts environment variables and even container runtime or",
    "start": "623279",
    "end": "630120"
  },
  {
    "text": "evener life cycle hooks can be included right and each one of these maps to a",
    "start": "630120",
    "end": "636800"
  },
  {
    "text": "set of modifications that need be made to a container spec um so everything is sort of",
    "start": "636800",
    "end": "642120"
  },
  {
    "text": "encapsulated declaratively and um right it's it's it's encoded in the container",
    "start": "642120",
    "end": "647360"
  },
  {
    "text": "spec um what happens is you request a CDI device and a container runtime which",
    "start": "647360",
    "end": "652720"
  },
  {
    "text": "actually understands CD CDI can take that device read the spec for it and",
    "start": "652720",
    "end": "658600"
  },
  {
    "text": "modify the container spec the OCA runtime spec so that your container has access to the device so it's all done uh",
    "start": "658600",
    "end": "664839"
  },
  {
    "text": "declaratively the way that we name devices there's a te taxonomy that CDI uses for naming devices it's a vendor",
    "start": "664839",
    "end": "672120"
  },
  {
    "text": "slcl class equals name so for an example for NVIDIA gpus it's a nvidia.com GPU",
    "start": "672120",
    "end": "678760"
  },
  {
    "text": "equals and then some ID we can use arbitrary naming so like I said before",
    "start": "678760",
    "end": "684120"
  },
  {
    "text": "um this is a declarative approach and some of the main benefits are that low-level run times like runc are doing",
    "start": "684120",
    "end": "690120"
  },
  {
    "text": "the heavy lifting for us right they runc is is specialized in setting up a",
    "start": "690120",
    "end": "696880"
  },
  {
    "text": "container runtime environment so we are leveraging it to actually provide access to accelerators and",
    "start": "696880",
    "end": "702360"
  },
  {
    "text": "devices um and having support in container D and cryo means that we no longer need some of the vendor specific",
    "start": "702360",
    "end": "709760"
  },
  {
    "text": "tooling to make this all work under the surface so here's an example of what a",
    "start": "709760",
    "end": "716680"
  },
  {
    "text": "CDI spec looks like for an Nvidia GPU um so this is a system where you have only a single GPU so under the devices",
    "start": "716680",
    "end": "724000"
  },
  {
    "text": "section we just have one one um entry called gpu0 and we have a series of",
    "start": "724000",
    "end": "729760"
  },
  {
    "text": "container edits so these are edits that need to be made to your container spec so that you can get access to gpu0 this",
    "start": "729760",
    "end": "736040"
  },
  {
    "text": "is device nodes like the Nvidia character device um mounts so like",
    "start": "736040",
    "end": "741279"
  },
  {
    "text": "driver libraries like libuda right it specifies where in the host that library is and where it should be mounted in the",
    "start": "741279",
    "end": "747240"
  },
  {
    "text": "container at what path um as well as container life cycle hooks so a simple",
    "start": "747240",
    "end": "752399"
  },
  {
    "text": "One is updating the LD cache so that this is a a standard hook that we like",
    "start": "752399",
    "end": "757440"
  },
  {
    "text": "to use so that your main process automatically can can discover the the driver libraries when it",
    "start": "757440",
    "end": "764480"
  },
  {
    "text": "runs so that's a brief introduction to what CDI is and what the the goals of the project are and some of the benefits",
    "start": "765120",
    "end": "772120"
  },
  {
    "text": "um where can use it today so um it's actually supported uh in in kubernetes",
    "start": "772120",
    "end": "778160"
  },
  {
    "text": "so there were uh a CDI devices field was added to the device plug-in API so that",
    "start": "778160",
    "end": "784120"
  },
  {
    "text": "um device plugins when they um provide an allocate response instead of providing other information they can",
    "start": "784120",
    "end": "790199"
  },
  {
    "text": "provide CDI device names in that uh response field in that response struct",
    "start": "790199",
    "end": "796519"
  },
  {
    "text": "uh the CRI was also extended to contain a CDI device field uh in the container config message starting in v",
    "start": "796519",
    "end": "804399"
  },
  {
    "text": "027.0 um and like I mentioned I think earlier container D and cryo already support CDI so they can understand the",
    "start": "804399",
    "end": "811839"
  },
  {
    "text": "specification and modify a container spec um for us um so this has all been discussed in",
    "start": "811839",
    "end": "819399"
  },
  {
    "text": "the context of kubernetes device plugins but CDI is actually a standard that's used outside and and is Meaningful",
    "start": "819399",
    "end": "825120"
  },
  {
    "text": "outside of uh the device plug-in use case so for Dr CDI is sort of the basis of defining and requesting",
    "start": "825120",
    "end": "832320"
  },
  {
    "text": "resources um it's also useful outside of kubernetes so if you're if you're launching containers interactively with",
    "start": "832320",
    "end": "838320"
  },
  {
    "text": "Docker or pod you can use CDI there uh and actually for NVIDIA gpus uh with",
    "start": "838320",
    "end": "843560"
  },
  {
    "text": "podman CDI is actually the recommended way today to to access",
    "start": "843560",
    "end": "849000"
  },
  {
    "text": "gpus um for HPC containers so we already there's been some support for CDI and",
    "start": "849000",
    "end": "854440"
  },
  {
    "text": "Singularity which is a popular um HPC runtime and we can think of even other use cases so even for networking devices",
    "start": "854440",
    "end": "860160"
  },
  {
    "text": "we can imagine a future of using CDI for that as well so going back to my sequence",
    "start": "860160",
    "end": "866519"
  },
  {
    "text": "diagram from a few slides this is how it would look with CDI if we if if our device plugin was leveraging CDI um",
    "start": "866519",
    "end": "873639"
  },
  {
    "text": "you'll notice that at the container runtime steps there are no like Nvidia specific components um the Assumption",
    "start": "873639",
    "end": "881519"
  },
  {
    "text": "here is that before running applications we have generated a CDI spec for all of",
    "start": "881519",
    "end": "888120"
  },
  {
    "text": "the gpus on the system so either some tooling is invoked or um uh the device",
    "start": "888120",
    "end": "894839"
  },
  {
    "text": "plugin itself can generate these specs that describe all the GPS in the system and what it means to access those so",
    "start": "894839",
    "end": "901600"
  },
  {
    "text": "those are stored somewhere as a file on the system at a standard path that run times no to look for so that's done",
    "start": "901600",
    "end": "908600"
  },
  {
    "text": "beforehand but at runtime what happens is container to your cryo right steps five through seven it will read the CGI",
    "start": "908600",
    "end": "914800"
  },
  {
    "text": "spec file for all the requested gpus for the container it will update the",
    "start": "914800",
    "end": "920120"
  },
  {
    "text": "container spec so that it has the right um bit so that it can up access that GPU",
    "start": "920120",
    "end": "925680"
  },
  {
    "text": "it forwards that container spec to run C runc creates your container with that specification and your container can run",
    "start": "925680",
    "end": "932079"
  },
  {
    "text": "and has access to the devices that were requested so this is all declarative there's no vendor specific so actually",
    "start": "932079",
    "end": "937279"
  },
  {
    "text": "this is a diagram that can be app applicable for not just Nvidia gpus but any other accelerator or device um",
    "start": "937279",
    "end": "944199"
  },
  {
    "text": "portable across container run timeses and right we're not using these vendor specific hooks that have a lot of",
    "start": "944199",
    "end": "951680"
  },
  {
    "text": "drawbacks okay so we went into detail about device plugins and S of the the lower level details about container",
    "start": "952680",
    "end": "959120"
  },
  {
    "text": "times and some standards that are being developed I think we're going to switch gears I'm going to go back to David we're going to talk about how can we",
    "start": "959120",
    "end": "965000"
  },
  {
    "text": "leverage device plugins to more effectively share gpus thanks Chris uh so let's jump gears",
    "start": "965000",
    "end": "972560"
  },
  {
    "text": "a little bit more uh and go one level up and so you know with these devices it's",
    "start": "972560",
    "end": "978279"
  },
  {
    "text": "becoming increasingly important to find ways to kind of maximize their utilization right so especially with",
    "start": "978279",
    "end": "983720"
  },
  {
    "text": "gpus they're very expensive they're limited and everybody right now is trying to figure out how we can squeeze",
    "start": "983720",
    "end": "989680"
  },
  {
    "text": "kind of the most from them right so to do that uh what we're really trying to do is to find ways that we can take",
    "start": "989680",
    "end": "995880"
  },
  {
    "text": "these physical devices that be already have and be able to break them up and partition them so we can actually run multiple workloads simultaneously on",
    "start": "995880",
    "end": "1002839"
  },
  {
    "text": "them and so we're going to be looking specifically a GPU and the ways that we can share gpus today and so there's",
    "start": "1002839",
    "end": "1009639"
  },
  {
    "text": "three main ways that you can do that uh the first one is called time slicing uh the second one is called multi- instance",
    "start": "1009639",
    "end": "1015199"
  },
  {
    "text": "GPU Mig and the third one uh which is uh kind of a newer one is called Cuda multiprocess service MPS so we'll go",
    "start": "1015199",
    "end": "1021920"
  },
  {
    "text": "into each detail into each one how it works and kind of the trade-offs of when you might want to use which",
    "start": "1021920",
    "end": "1028079"
  },
  {
    "text": "approach so we'll start with the simplest one which is time slicing so normally right we just have a single",
    "start": "1028079",
    "end": "1034438"
  },
  {
    "text": "workload that's cons that's running on the GPU just a onetoone mapping really simple with time slicing what we can do",
    "start": "1034439",
    "end": "1040280"
  },
  {
    "text": "is we can first of all Define how many workloads we want to actually be able to run on the GPU at a time so you can see",
    "start": "1040280",
    "end": "1045760"
  },
  {
    "text": "at the bottom of that command line you can see uh there's a parameter there Max shared clients per GPU is 10 so in this",
    "start": "1045760",
    "end": "1051400"
  },
  {
    "text": "case we're basically saying we're allowing 10 workloads to run on the GPU at that time and so with time slicing",
    "start": "1051400",
    "end": "1057559"
  },
  {
    "text": "how it works is we basically have context switching so a single workload runs then there's a context switch and",
    "start": "1057559",
    "end": "1063120"
  },
  {
    "text": "then the second workload runs and then it switches back and so that contact switch is a little bit expensive but it allows us to actually run multiple",
    "start": "1063120",
    "end": "1069320"
  },
  {
    "text": "workloads at a time and so uh the way it actually works from the device plugin perspective is instead of actually",
    "start": "1069320",
    "end": "1075679"
  },
  {
    "text": "advertising before the physical number of devices which was before what Nvidia . GPU represented now we're kind of advertising the virtual number of",
    "start": "1075679",
    "end": "1082080"
  },
  {
    "text": "devices so the clients that can be used uh the other kind of thing about time slicing that's worth to mention each",
    "start": "1082080",
    "end": "1087919"
  },
  {
    "text": "workload gets its own uh address space but there's no memory limits enforced so a workload can consume all the memory",
    "start": "1087919",
    "end": "1093760"
  },
  {
    "text": "and that can disrupt other workloads the other thing work to mention here is that in time slicing each workload actually",
    "start": "1093760",
    "end": "1100039"
  },
  {
    "text": "only runs uh at a single time right there's only one workload that's running so there's concurrency here but no",
    "start": "1100039",
    "end": "1106080"
  },
  {
    "text": "parallelism at a single time there's only one workload that's running on the GPU so that's available today and you",
    "start": "1106080",
    "end": "1111960"
  },
  {
    "text": "can use time slicing um and then also you know with the Nvidia GPU device plugin the way to enable time slicing",
    "start": "1111960",
    "end": "1117640"
  },
  {
    "text": "it's pretty similar uh you specify how many clients you want to use the GPU here it's 10 and now uh under the",
    "start": "1117640",
    "end": "1124320"
  },
  {
    "text": "nvidia.com GPU uh capacity that's reported instead of reporting one GPU now reporting 10 gpus so kind of the",
    "start": "1124320",
    "end": "1130440"
  },
  {
    "text": "meaning here has changed uh from the physical number of gpus to the clients uh that can use the GPU so uh when is",
    "start": "1130440",
    "end": "1137799"
  },
  {
    "text": "time slicing use so one uh scenario where I think it can be very useful is kind of for interactive workloads kind",
    "start": "1137799",
    "end": "1144360"
  },
  {
    "text": "of more exploratory workloads and so uh sometimes uh these workloads like jupyter notebooks if you're familiar",
    "start": "1144360",
    "end": "1150720"
  },
  {
    "text": "where you can kind of do data science experiments and kind of machine learning kind of Explorations are great candidates because they're kind of",
    "start": "1150720",
    "end": "1157000"
  },
  {
    "text": "interactive sometimes they need a lot of resources sometimes they don't you probably they're mostly for exploration",
    "start": "1157000",
    "end": "1162039"
  },
  {
    "text": "so we don't have super strong kind of guarantees that we need to provide so let's take a quick look at uh actually",
    "start": "1162039",
    "end": "1168159"
  },
  {
    "text": "using time slicing an action so um what we're going to do here uh is we're first",
    "start": "1168159",
    "end": "1174720"
  },
  {
    "text": "going to provision a uh cluster so we provisioned a cluster here on uh GK on",
    "start": "1174720",
    "end": "1182520"
  },
  {
    "text": "129 uh after that I'm going to set up a node pool uh which is a set of nodes and",
    "start": "1182520",
    "end": "1188600"
  },
  {
    "text": "I'm going to configure those nodes to be in time sharing mode so I'm going to set specify the GPU uh time sharing strategy",
    "start": "1188600",
    "end": "1195080"
  },
  {
    "text": "and I'm going to say that I have three gpus that I want to do time sharing with so I ahead and create that node pool uh",
    "start": "1195080",
    "end": "1200799"
  },
  {
    "text": "it's going to create the capacity in the node I'm just creating one node here kind of for demonstration purposes uh after that's there I'm going",
    "start": "1200799",
    "end": "1207799"
  },
  {
    "text": "to use coup cuddle and actually see like how many nodes I have so I'm running coup cutle get nodes I have uh two nodes",
    "start": "1207799",
    "end": "1213440"
  },
  {
    "text": "available here uh one of them is just a CPU node and then one is actually the one that we just provisioned with the",
    "start": "1213440",
    "end": "1218720"
  },
  {
    "text": "gpus so I'm just going to set an environment variable GPU node just so we can kind of play around with it and the",
    "start": "1218720",
    "end": "1224240"
  },
  {
    "text": "first thing I'm going to do is I'm going to look at the allocatable uh that's being reported uh from the API server and you can see here uh we have",
    "start": "1224240",
    "end": "1230520"
  },
  {
    "text": "nvidia.com GPU reported and we have three gpus uh that are reported so this node only has one GPU I provision an",
    "start": "1230520",
    "end": "1236919"
  },
  {
    "text": "a100 GPU on it so only has one physical GPU but due to the time sharing configuration it's actually reporting",
    "start": "1236919",
    "end": "1242240"
  },
  {
    "text": "that we have three gpus here so uh let's go ahead and uh we're going to try",
    "start": "1242240",
    "end": "1247559"
  },
  {
    "text": "running a workload here so the first thing I'm going to do is I'm going to label the node with notebook node true and I'm going to just use that as the",
    "start": "1247559",
    "end": "1252880"
  },
  {
    "text": "node selector for the pods that I'm going to deploy so uh for the purposes of the demo you can imagine here we're",
    "start": "1252880",
    "end": "1259000"
  },
  {
    "text": "going to deploy two uh Jupiter notebooks and you can imagine maybe there like two different uh data scientists or",
    "start": "1259000",
    "end": "1264320"
  },
  {
    "text": "researchers who are wanting to play around and use uh these notebooks so I created two pods here notebook pod one",
    "start": "1264320",
    "end": "1270520"
  },
  {
    "text": "uh and in the node selector here uh you can see I'm specifying I want an a100 GPU um you want to use time sharing uh",
    "start": "1270520",
    "end": "1277200"
  },
  {
    "text": "this is the one where I set up the the three clients and uh from the resource perspective I'm just specifying I need",
    "start": "1277200",
    "end": "1283279"
  },
  {
    "text": "one GPU here and then uh for the actual workload I'm just specifying like the",
    "start": "1283279",
    "end": "1288640"
  },
  {
    "text": "The jupyter Notebook to start up and so that's my first pod and then I have a second pod here which is basically the",
    "start": "1288640",
    "end": "1295200"
  },
  {
    "text": "same thing just a different pod that we can play around with um the only thing I'm running under a different port so we",
    "start": "1295200",
    "end": "1301000"
  },
  {
    "text": "can kind of uh explore it differently so uh what we're going to do we're going to deploy both of those pods here notebook",
    "start": "1301000",
    "end": "1307600"
  },
  {
    "text": "pod one and notebook pod two cool and then we're going to look if",
    "start": "1307600",
    "end": "1312760"
  },
  {
    "text": "those pods started up they're running awesome uh so now let's try to access those notebooks so those notebooks uh",
    "start": "1312760",
    "end": "1319600"
  },
  {
    "text": "I'm going to use uh CU cutle port forward and I'm going to port forward them on two different ports and so I'm",
    "start": "1319600",
    "end": "1324640"
  },
  {
    "text": "going to start up the first uh I'm going to go to that port forward and look at the jupyter notebook you can see it",
    "start": "1324640",
    "end": "1330080"
  },
  {
    "text": "started here on Port 888 and the other one I Port forwarded on 889 and you can",
    "start": "1330080",
    "end": "1336039"
  },
  {
    "text": "see it's also running here so awesome they're both running uh but uh they're using time sharing but we're not",
    "start": "1336039",
    "end": "1341600"
  },
  {
    "text": "actually using anything on the GPU yet so let's actually run some GPU workload uh to try out the time sharing so what",
    "start": "1341600",
    "end": "1347480"
  },
  {
    "text": "I'm going to do here I'm going to install some kind of libraries and then I'm going to first of all use pytorch and I'm going to access the gpus from",
    "start": "1347480",
    "end": "1353840"
  },
  {
    "text": "pytorch and the pytorch is able to access the gpus I'm just printing the the GPU kind of model here so now let's",
    "start": "1353840",
    "end": "1359840"
  },
  {
    "text": "try to actually run a workload so I'm going to use hugging face uh so I'm just going to log in with the hugging face credentials so I can download a model",
    "start": "1359840",
    "end": "1367000"
  },
  {
    "text": "and I'm going to actually try doing a uh the the Gemma kind of llm model so I'm using the open source uh Gemma model",
    "start": "1367000",
    "end": "1373240"
  },
  {
    "text": "which is like the small two billion model here and I'm going to try to do inference on it so I'm going to type in kubernetes is the best",
    "start": "1373240",
    "end": "1379200"
  },
  {
    "text": "um and try to run inference on that model so I'm going to run it now it's actually loading the model to the GPU um",
    "start": "1379200",
    "end": "1385840"
  },
  {
    "text": "and you can see here it's going to load it here and we can see the llm output at the bottom kubernetes is the best way to deploy and manage containerized",
    "start": "1385840",
    "end": "1391760"
  },
  {
    "text": "applications cool so makes sense uh so now you can imagine some later time like some other user wants to use that other",
    "start": "1391760",
    "end": "1397360"
  },
  {
    "text": "notebook right so we're going to switch over to the other notebook different user and let's say they want to run a different workload maybe they want to",
    "start": "1397360",
    "end": "1403039"
  },
  {
    "text": "try a different model here so very similar I'm just going to try the same thing but uh this user they're going to",
    "start": "1403039",
    "end": "1408200"
  },
  {
    "text": "try different model they're going to try the larger Gemma 7 billion parameter model so same type of thing uh different",
    "start": "1408200",
    "end": "1414360"
  },
  {
    "text": "model though and we're going to try a different input text type in kubernetes is great and see what the response is it",
    "start": "1414360",
    "end": "1419600"
  },
  {
    "text": "says kubernetes is great uh and it's hard it's hard to get started so hopefully uh this talk helps a little bit with that um so there we go we can",
    "start": "1419600",
    "end": "1426559"
  },
  {
    "text": "see uh we're running both of those things if we look at the metrics here that are reported we can look at actually the uh metrics that are",
    "start": "1426559",
    "end": "1432799"
  },
  {
    "text": "reported for those gpus um we can see the the vram usage here you can see it spiked to around 20 gigs and then it",
    "start": "1432799",
    "end": "1438799"
  },
  {
    "text": "jumped to 40 so uh when the first notebook user started uh playing around with the first uh model it allocated all",
    "start": "1438799",
    "end": "1444840"
  },
  {
    "text": "that vram and then the second one uh started allocating more memory for the second model and it jumped up to 40 and",
    "start": "1444840",
    "end": "1451000"
  },
  {
    "text": "uh just to verify that we can actually log into uh the node and I'm going to just SSH into the node I'm going to run",
    "start": "1451000",
    "end": "1457720"
  },
  {
    "text": "Nvidia SMI and uh you can actually see here from Nvidia SMI it shows like both",
    "start": "1457720",
    "end": "1462760"
  },
  {
    "text": "python processes running there and both uh allocated that amount of GPU memory which corresponds kind of to the metrics",
    "start": "1462760",
    "end": "1468080"
  },
  {
    "text": "we saw earlier uh so there you go that's kind of like a demo of how you might use time slicing so you can try that out uh today",
    "start": "1468080",
    "end": "1475960"
  },
  {
    "text": "so I'm going to hand it back to Chris to talk a little bit about some of the other GPU sharing strategies cool yeah",
    "start": "1475960",
    "end": "1481760"
  },
  {
    "text": "thanks for the demo David that was really great um there we go so there are two",
    "start": "1481760",
    "end": "1488799"
  },
  {
    "text": "other sharing strategies we want to cover in this talk so the next one is um Cuda MPS um this allows this takes a",
    "start": "1488799",
    "end": "1495919"
  },
  {
    "text": "Next Step Above time slicing in the sense that allows you to logically logically partition um the GPU in terms",
    "start": "1495919",
    "end": "1503480"
  },
  {
    "text": "of memory and compute so uh it's all done in software just like time slicing",
    "start": "1503480",
    "end": "1509039"
  },
  {
    "text": "so you can have multiple clients of the GPU using at the same time uh concurrently but also it sort of allows",
    "start": "1509039",
    "end": "1514840"
  },
  {
    "text": "you to do some level of parallelism so you can have uh clients a b and c running GPU kernels on the same GPU at",
    "start": "1514840",
    "end": "1522000"
  },
  {
    "text": "the same time and it's all sort of facilitated by MPS this MPS service or",
    "start": "1522000",
    "end": "1527120"
  },
  {
    "text": "server process that's running and so it sort of sits in between uh your actual clients and instructions running the GPU",
    "start": "1527120",
    "end": "1533960"
  },
  {
    "text": "so it it in software it can um sort of um enforce some sort of memory limits or",
    "start": "1533960",
    "end": "1540320"
  },
  {
    "text": "enforce memory limits with for each GPU client and um compute resources in terms of active thread percentage um so it can",
    "start": "1540320",
    "end": "1547360"
  },
  {
    "text": "lead to better utilization of your GPU in terms when compared with time slicing",
    "start": "1547360",
    "end": "1552640"
  },
  {
    "text": "um and the the one implementation detail is here is that we have one server that",
    "start": "1552640",
    "end": "1559279"
  },
  {
    "text": "um facilitates the the the GPU clients and and running instructions of the GPU so there's one shared Cuda context um",
    "start": "1559279",
    "end": "1567080"
  },
  {
    "text": "that's good in terms of context switching because you're it's not as much of an overhead between different clients running the same GPU as one",
    "start": "1567080",
    "end": "1573640"
  },
  {
    "text": "compared with time slicing where you are doing a full context switch between two different Cuda contexts um but at the",
    "start": "1573640",
    "end": "1580120"
  },
  {
    "text": "same time in terms of like fault domains like this is a shared fault domain so if a client a triggers some sort of fatal",
    "start": "1580120",
    "end": "1586360"
  },
  {
    "text": "error on the GPU it will affect client B and",
    "start": "1586360",
    "end": "1590799"
  },
  {
    "text": "C uh in the Nvidia GPU device plugin you can configure um MPS like this it's",
    "start": "1592159",
    "end": "1597520"
  },
  {
    "text": "almost very similar to the time slicing configuration uh you just specify MPS",
    "start": "1597520",
    "end": "1602760"
  },
  {
    "text": "instead of time slicing as the sharing strategy and you configure how many replicas so how many um concurrent",
    "start": "1602760",
    "end": "1609399"
  },
  {
    "text": "processes you want to to support running at the same time on the GPU and so if we",
    "start": "1609399",
    "end": "1614520"
  },
  {
    "text": "describe the node we'll see similar to with time sharing your one phys physical GPU is now um advertised as as 10 as",
    "start": "1614520",
    "end": "1622039"
  },
  {
    "text": "being 10 um in terms of support so we have a a new release of the device plugin coming",
    "start": "1622039",
    "end": "1627840"
  },
  {
    "text": "out very soon the 0.15.0 release and that will have official support for MPS",
    "start": "1627840",
    "end": "1632960"
  },
  {
    "text": "we have some release candidates out already that people are trying out trying MPS out",
    "start": "1632960",
    "end": "1639080"
  },
  {
    "text": "with the other uh sharing strategy is Mig so um multi-instance GPU right it's",
    "start": "1639720",
    "end": "1646080"
  },
  {
    "text": "a hardware feature of some of the later generation Nvidia gpus that allows you to take a",
    "start": "1646080",
    "end": "1651720"
  },
  {
    "text": "full GPU and subdivide it into multiple GP what we call GPU instances right and",
    "start": "1651720",
    "end": "1657480"
  },
  {
    "text": "each of these has dedicated compute and memory resources um so you can have up to seven",
    "start": "1657480",
    "end": "1663320"
  },
  {
    "text": "of these slices um on your GPU and there's some uh standard like profile",
    "start": "1663320",
    "end": "1668840"
  },
  {
    "text": "names uh that if you look at the uh Mig documentation you can sort of partition",
    "start": "1668840",
    "end": "1673960"
  },
  {
    "text": "your GPU into those fixed size slices um so on the bottom we have an example of",
    "start": "1673960",
    "end": "1680320"
  },
  {
    "text": "uh enabling Mig on gke so on your Noe pool with A1 100s you can specify the",
    "start": "1680320",
    "end": "1686600"
  },
  {
    "text": "partition size of your Mig instances in this case 1G 5gb which is the smallest slice so we can have seven of these each",
    "start": "1686600",
    "end": "1693399"
  },
  {
    "text": "with one sort of dedicated compute um GPU uh compute instance and um 5",
    "start": "1693399",
    "end": "1699480"
  },
  {
    "text": "gigabytes of of GPU",
    "start": "1699480",
    "end": "1703559"
  },
  {
    "text": "memory um so how do we enable Mig in the device plugin so the the Assumption here",
    "start": "1705840",
    "end": "1711080"
  },
  {
    "text": "is that you an admin or some some automation has gone in and already enabled Mig on your GPU on the Node and",
    "start": "1711080",
    "end": "1718080"
  },
  {
    "text": "has already sliced um the GPU into Mig instances so that has to be done",
    "start": "1718080",
    "end": "1723159"
  },
  {
    "text": "beforehand so there is some static sort of configuration that has to happen you have to sort of know what size profiles",
    "start": "1723159",
    "end": "1728799"
  },
  {
    "text": "you want um and configure the GPU in that way um the configuration with the",
    "start": "1728799",
    "end": "1734080"
  },
  {
    "text": "device plugin is really simple the device plugin will actually enumerate all of your migs es and so if you have",
    "start": "1734080",
    "end": "1739600"
  },
  {
    "text": "seven of the the small instance instance types then your node will have seven",
    "start": "1739600",
    "end": "1745120"
  },
  {
    "text": "mv.com GPU resources allocatable um instead of statically or",
    "start": "1745120",
    "end": "1750880"
  },
  {
    "text": "instead of uh configuring Mig yourself you can use a GPU operator we have a component called a big manager that",
    "start": "1750880",
    "end": "1756360"
  },
  {
    "text": "helps automate the the the enabling of MiG and configuring of uh Mig",
    "start": "1756360",
    "end": "1763600"
  },
  {
    "text": "instances so I'll hand it back to David who's going to do a little comparison and summarize our",
    "start": "1763600",
    "end": "1768640"
  },
  {
    "text": "talk cool so now that you've kind of heard around those uh different uh",
    "start": "1768640",
    "end": "1774200"
  },
  {
    "text": "resource sharing strategies might be asking yourself when do I know which one to use and what are kind of the trade-offs between them right so I guess",
    "start": "1774200",
    "end": "1781279"
  },
  {
    "text": "to start off the simplest one is the one we started with which is time slicing time slicing the big benefit of it it's",
    "start": "1781279",
    "end": "1786519"
  },
  {
    "text": "very Dynamic so you can add workloads on the Fly you can remove workloads on the Fly uh you know you can start out with",
    "start": "1786519",
    "end": "1792320"
  },
  {
    "text": "just one workload and then it's identical to just not not using research sharing and as you add more workloads",
    "start": "1792320",
    "end": "1797480"
  },
  {
    "text": "you can kind of uh get more workloads to share that GPU the negative with time slicing is you can start to introduce",
    "start": "1797480",
    "end": "1803880"
  },
  {
    "text": "some kind of Jitter Etc because of that Contex switching overhead and you don't have strong kind of resource management",
    "start": "1803880",
    "end": "1809399"
  },
  {
    "text": "guarantees because there are no memory limits that are available so that's kind of what brings us to MPS and MPS can be",
    "start": "1809399",
    "end": "1815760"
  },
  {
    "text": "thought of as kind of an improvement over time slicing because uh you actually do get stronger guarantees there right you do get uh performance",
    "start": "1815760",
    "end": "1822360"
  },
  {
    "text": "isolation you can specify how much compute each uh each application gets and you can specify memory limits so you",
    "start": "1822360",
    "end": "1828559"
  },
  {
    "text": "can actually enforce uh memory limits per application uh and then Mig is actually",
    "start": "1828559",
    "end": "1834039"
  },
  {
    "text": "the one that gives you kind of the strongest guarantees because it actually partitions things at the hardware level right so that means that at the hardware",
    "start": "1834039",
    "end": "1841279"
  },
  {
    "text": "level uh you have separate kind of memory that you can provide and you have separate kind of performance guarantees because it's actually split at Hardware",
    "start": "1841279",
    "end": "1848240"
  },
  {
    "text": "um the kind of the negative with Mig and kind of the downside is that you actually need to figure out how to partition your GPU kind of ahead of time",
    "start": "1848240",
    "end": "1854200"
  },
  {
    "text": "right and it's not Dynamic so when when there's no workloads running you have to kind of go in and understand okay",
    "start": "1854200",
    "end": "1859440"
  },
  {
    "text": "depending on my workload size which M partition sizes make sense for my application uh it works well however for",
    "start": "1859440",
    "end": "1866519"
  },
  {
    "text": "for example for multi-tenant scenarios right where you really do need strong kind of security and other types of",
    "start": "1866519",
    "end": "1871840"
  },
  {
    "text": "isolation guarantees between workloads so uh in summary I think what",
    "start": "1871840",
    "end": "1877720"
  },
  {
    "text": "we talked about today right is we started at the kubernetes device plug-in framework and that's the existing uh framework that exists in the ecosystem",
    "start": "1877720",
    "end": "1884679"
  },
  {
    "text": "uh to expose and uh expose devices like gpus tpus and fpga we also Dove one",
    "start": "1884679",
    "end": "1890159"
  },
  {
    "text": "level kind of lower at the container runtime level and gave you uh some information about CDI which is the new",
    "start": "1890159",
    "end": "1895399"
  },
  {
    "text": "standard of how uh devices will be integrated at the at the container runtime level uh then we looked at GPU",
    "start": "1895399",
    "end": "1901120"
  },
  {
    "text": "sharing and GPU sharing is a great way to improve utilization of your resources and we have different strategies there with their different trade-offs and I",
    "start": "1901120",
    "end": "1907840"
  },
  {
    "text": "think the big message we're trying to send is like as a community what we're trying to do is extend this resource model um make it really vendor agnostic",
    "start": "1907840",
    "end": "1915039"
  },
  {
    "text": "make it standard and make it kind of natively supported by kubernetes with these devices and so uh what we're",
    "start": "1915039",
    "end": "1921039"
  },
  {
    "text": "doing in the future is uh we're trying to extend this model here right so this resource sharing it's not natively",
    "start": "1921039",
    "end": "1926639"
  },
  {
    "text": "supported and it's not very declarative because we had to kind of overload the meaning of what does the device mean",
    "start": "1926639",
    "end": "1932039"
  },
  {
    "text": "depending on uh the device the the the resource sharing strategy so we're trying to come up with new apis to make",
    "start": "1932039",
    "end": "1937639"
  },
  {
    "text": "it more declarative and so in Dr we're hoping that we can be able to express kind of some of these resource uh",
    "start": "1937639",
    "end": "1942799"
  },
  {
    "text": "sharing mechanisms kind of more natively and so it' be really great to get your feedback around kind of some of what are",
    "start": "1942799",
    "end": "1948279"
  },
  {
    "text": "kind of the challenges today with the device plugin and and CDI and uh what what you would like uh in the future in",
    "start": "1948279",
    "end": "1953880"
  },
  {
    "text": "kubernetes your feedback is is really appreciated there and so a couple things I want to shout out is there's some",
    "start": "1953880",
    "end": "1959760"
  },
  {
    "text": "related talks both of this coupon that have already happened and in past coupons as well uh that go in more detail around devices in kubernetes as",
    "start": "1959760",
    "end": "1966840"
  },
  {
    "text": "well as uh GPU sharing which are uh some great resources there uh so thank you so",
    "start": "1966840",
    "end": "1971960"
  },
  {
    "text": "much [Applause]",
    "start": "1971960",
    "end": "1980799"
  },
  {
    "text": "any questions cool I think there's mics",
    "start": "1980799",
    "end": "1989638"
  },
  {
    "text": "yeah uh can hello can we expect in some future that you provide a memory limit",
    "start": "1990720",
    "end": "1997399"
  },
  {
    "text": "or so for for time slicing it's technically possible but it's not",
    "start": "1997399",
    "end": "2003360"
  },
  {
    "text": "implemented in in the driver yeah I don't think that's currently planned I think what some folks do is they",
    "start": "2003360",
    "end": "2009120"
  },
  {
    "text": "actually enforce those memory limits at the application Level so a lot of applications you know you can actually enforce memory limits at the application",
    "start": "2009120",
    "end": "2015639"
  },
  {
    "text": "Level uh but I don't think there's currently plans to actually support that natively with time slicing and the",
    "start": "2015639",
    "end": "2021120"
  },
  {
    "text": "recommendation would be to use MPS yeah yeah thank you um thank you for the talk um so my",
    "start": "2021120",
    "end": "2029320"
  },
  {
    "text": "question would be can you combine MPS and time slicing either today or tomorrow so saying that uh I want to use",
    "start": "2029320",
    "end": "2037000"
  },
  {
    "text": "MPS until the GPU is full basically and then start time slicing for additional",
    "start": "2037000",
    "end": "2044360"
  },
  {
    "text": "workloads I guess yeah theoretically you could but in practice I don't think we've investigated or documented how to",
    "start": "2047200",
    "end": "2053520"
  },
  {
    "text": "do that for our Cloud native users um okay so it's theoretically possible",
    "start": "2053520",
    "end": "2058679"
  },
  {
    "text": "today or I think so okay thank you you can also combine for example",
    "start": "2058679",
    "end": "2064720"
  },
  {
    "text": "like M and time slicing as well right so you can do time slicing on make partitions as well for example yeah you would need like multiple MPS server",
    "start": "2064720",
    "end": "2071878"
  },
  {
    "text": "processes right to and then they would time slice but all yeah I I think it's",
    "start": "2071879",
    "end": "2076960"
  },
  {
    "text": "theoretically possible",
    "start": "2076960",
    "end": "2079878"
  },
  {
    "text": "yeah so the example you gave um I guess it's now fractionalization is becoming a",
    "start": "2082359",
    "end": "2089158"
  },
  {
    "text": "standard um can I use those features in the new operator without Dr I mean this",
    "start": "2089159",
    "end": "2096280"
  },
  {
    "text": "is enabling Dr but I can all do these things now right yeah so everything we showed is",
    "start": "2096280",
    "end": "2101880"
  },
  {
    "text": "not using Dr this is using the device plugin API so all of the sharing strategies we showed you are available",
    "start": "2101880",
    "end": "2108240"
  },
  {
    "text": "and supported in the operator and the components that we deploy yeah and the new operator is about to be released",
    "start": "2108240",
    "end": "2113920"
  },
  {
    "text": "with support for I think you mentioned earlier there was a new version of that operator yeah so we're we're releasing a",
    "start": "2113920",
    "end": "2120280"
  },
  {
    "text": "new version of the device plugin and then also the operator will pick that up next month sometime hopefully to support",
    "start": "2120280",
    "end": "2126160"
  },
  {
    "text": "MPS Okay so we all start learning to use this way of allocating",
    "start": "2126160",
    "end": "2131720"
  },
  {
    "text": "fractions then in in a year's time or so the Dr will be the new way instead so Dr",
    "start": "2131720",
    "end": "2139320"
  },
  {
    "text": "will provide potentially like a new API to actually consume these in your workload so your pod spec may change but",
    "start": "2139320",
    "end": "2145079"
  },
  {
    "text": "the underlying actually technology of the you know those different research strategies that's not specific to Dr Dr",
    "start": "2145079",
    "end": "2151280"
  },
  {
    "text": "is just kind of the the declarative API representation of how we want to consume those okay yeah thanks",
    "start": "2151280",
    "end": "2159640"
  },
  {
    "text": "thanks for the presentation so my question is is this possible to share some saying about how GK supports the",
    "start": "2161680",
    "end": "2168280"
  },
  {
    "text": "time slicing is a mechanism is this possible how how it's implemented you're",
    "start": "2168280",
    "end": "2173839"
  },
  {
    "text": "ask how GK supports time slicing yeah so the way GK supports time slicing is kind",
    "start": "2173839",
    "end": "2179119"
  },
  {
    "text": "of the regular way it's same as with the Nvidia GPU device plugin so you just specify how many clients uh can use the",
    "start": "2179119",
    "end": "2185359"
  },
  {
    "text": "the GPU at a time right and then it'll basically just schedule those uh those workloads there and the device plugin",
    "start": "2185359",
    "end": "2191119"
  },
  {
    "text": "will advertise the number of clients that you specify so there's nothing kind of special done uh on GK versus the",
    "start": "2191119",
    "end": "2196480"
  },
  {
    "text": "Nvidia device plugin the underlying mechanism is is kind of the same okay thank",
    "start": "2196480",
    "end": "2202240"
  },
  {
    "text": "you hi um great talk um at the moment uh",
    "start": "2203400",
    "end": "2208640"
  },
  {
    "text": "we use uh time slicing for for a combination of GL workloads and Cuda workloads is that what we can also",
    "start": "2208640",
    "end": "2214880"
  },
  {
    "text": "expect from from MPS to uh",
    "start": "2214880",
    "end": "2219519"
  },
  {
    "text": "uh I think it should work transparently yeah it's just a different way of sharing the GPU I don't think the type of application you run necessarily",
    "start": "2220200",
    "end": "2226359"
  },
  {
    "text": "matters okay yeah because you mentioned the context and that sounded very Cuda specific",
    "start": "2226359",
    "end": "2231880"
  },
  {
    "text": "or I actually don't know uh if there's implications there of using the same Cuda context actually don't know okay my",
    "start": "2231880",
    "end": "2239119"
  },
  {
    "text": "understanding the goal is for it to be compatible like for most workloads out of the box raid so I think the expectation you shouldn't need to change",
    "start": "2239119",
    "end": "2244240"
  },
  {
    "text": "your your workload significantly to to pick up MPS from from time licing yeah and and the the GL workloads will they",
    "start": "2244240",
    "end": "2251280"
  },
  {
    "text": "remain to be supported because we're we're noticing a bit that the support for the container base images for for",
    "start": "2251280",
    "end": "2256960"
  },
  {
    "text": "example for the for the G has been lacking for for the last few years so can we expect that to remain supported",
    "start": "2256960",
    "end": "2263240"
  },
  {
    "text": "by your uh by the cloud uh run times are you asking about like the the",
    "start": "2263240",
    "end": "2269560"
  },
  {
    "text": "docker like what what specifically exactly yeah yeah I'm not fully aware honestly around",
    "start": "2269560",
    "end": "2275599"
  },
  {
    "text": "the to check thanks hello thank you very much for the",
    "start": "2275599",
    "end": "2282800"
  },
  {
    "text": "great talk uh one question would be for time slicing is there uh let's say a",
    "start": "2282800",
    "end": "2287839"
  },
  {
    "text": "soft limit depending on the cluster size and how many compute nodes you have uh",
    "start": "2287839",
    "end": "2293200"
  },
  {
    "text": "where like thanks to Performance in in a way yeah so for for time slicing it's",
    "start": "2293200",
    "end": "2300200"
  },
  {
    "text": "all kind of based on a single node right so actually the number of nodes in your cluster and so forth I I don't think are actually important factors it's the",
    "start": "2300200",
    "end": "2306640"
  },
  {
    "text": "number of workloads that running concurrently right on on on a single node so I think the main factor is how",
    "start": "2306640",
    "end": "2312160"
  },
  {
    "text": "many gpus do you actually physically have on that node and how many clients do you have connected to it right the more the more clients are going to have",
    "start": "2312160",
    "end": "2317839"
  },
  {
    "text": "the less runtime each work so you can't sh so you can't use time slicing",
    "start": "2317839",
    "end": "2322880"
  },
  {
    "text": "together with RDMA between like more nodes or connected nodes to like have a",
    "start": "2322880",
    "end": "2329640"
  },
  {
    "text": "a bigger a bigger pool of time slice gpus yeah I mean I I don't see why you couldn't use time slicing with other",
    "start": "2329640",
    "end": "2336000"
  },
  {
    "text": "technology right when the workload is actually running on the GPU it's the only thing running right so you could use RDMA or any any other techniques",
    "start": "2336000",
    "end": "2342240"
  },
  {
    "text": "you're using today thank you very",
    "start": "2342240",
    "end": "2346040"
  },
  {
    "text": "much yeah uh thanks for the for the talk um I have one question for uh for time",
    "start": "2347800",
    "end": "2353680"
  },
  {
    "text": "slicing it's currently the only solution that might have problems with workers fighting for",
    "start": "2353680",
    "end": "2358839"
  },
  {
    "text": "GPU um for GPU consumption so it is more a Jal than all the others is there a way",
    "start": "2358839",
    "end": "2364920"
  },
  {
    "text": "currently to natively achieve process prioritization so you can say okay these processes here are coming first and then",
    "start": "2364920",
    "end": "2371280"
  },
  {
    "text": "these are going to wait or is this not currently possible yeah I I don't think that's",
    "start": "2371280",
    "end": "2377680"
  },
  {
    "text": "possible today I think you can customize like the the time quota that's given for each application when it's time sliced",
    "start": "2377680",
    "end": "2383760"
  },
  {
    "text": "so I think there's a configuration option in N videoi you can say like short uh for how you know the context",
    "start": "2383760",
    "end": "2389160"
  },
  {
    "text": "window but I don't think there's prioritization built in there because at a at a single moment right there's only",
    "start": "2389160",
    "end": "2394280"
  },
  {
    "text": "one application that's running and they all get the the same share okay thank",
    "start": "2394280",
    "end": "2400319"
  },
  {
    "text": "you all right if there no other questions thank you everyone yeah thank you",
    "start": "2405520",
    "end": "2410860"
  },
  {
    "text": "[Applause]",
    "start": "2410860",
    "end": "2413760"
  }
]