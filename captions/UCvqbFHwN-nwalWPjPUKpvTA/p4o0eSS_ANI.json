[
  {
    "text": "let's get started so our talk is called Tales from On Call",
    "start": "299",
    "end": "5819"
  },
  {
    "text": "fun with operating at CD at scale my name is Gita and I will be joined over",
    "start": "5819",
    "end": "11219"
  },
  {
    "text": "video by ciao Chen both of us work at Amazon web services",
    "start": "11219",
    "end": "17360"
  },
  {
    "text": "so let me introduce a little bit about us we work for eks eks is a managed",
    "start": "18180",
    "end": "25859"
  },
  {
    "text": "kubernetes service what that means is eks manages the control plane for you",
    "start": "25859",
    "end": "31800"
  },
  {
    "text": "all aspects of it performance scalability and availability so all the",
    "start": "31800",
    "end": "38100"
  },
  {
    "text": "components here in the blue rectangle are owned and managed by eks customers",
    "start": "38100",
    "end": "45120"
  },
  {
    "text": "don't have access to those they typically just manage their workloads and sometimes the worker nodes",
    "start": "45120",
    "end": "51360"
  },
  {
    "text": "So within the control plane the team I am from is the hcd team which focuses on",
    "start": "51360",
    "end": "59219"
  },
  {
    "text": "operations and contributions to etcd",
    "start": "59219",
    "end": "64940"
  },
  {
    "text": "now let's hear from ciao about our hcd environment",
    "start": "66600",
    "end": "72560"
  },
  {
    "text": "uh thank you Akita good afternoon everyone my name is ciao a software",
    "start": "73740",
    "end": "79140"
  },
  {
    "text": "engineer at Amazon I've been activated working uh",
    "start": "79140",
    "end": "84540"
  },
  {
    "text": "in the operation of SCD the distributed key value store that is used by kubernetes as its primary data store in",
    "start": "84540",
    "end": "92159"
  },
  {
    "text": "my virtual talk today I'll be sharing some of some of my experiences and",
    "start": "92159",
    "end": "97560"
  },
  {
    "text": "insights of operating SCD in kubernetes clusters now SCD at eks",
    "start": "97560",
    "end": "106259"
  },
  {
    "text": "eks SCD each SCD cluster is a three node cluster evenly distributed across three",
    "start": "106259",
    "end": "113280"
  },
  {
    "text": "availability zones in a region availability results are isolated data",
    "start": "113280",
    "end": "119640"
  },
  {
    "text": "center located with specific regions in which public cloud services originate and",
    "start": "119640",
    "end": "125820"
  },
  {
    "text": "operate now the",
    "start": "125820",
    "end": "131220"
  },
  {
    "text": "second uh eks ACD uses static IP to advertise",
    "start": "131220",
    "end": "136260"
  },
  {
    "text": "SD client what endpoint it should connect to and for peer communication we",
    "start": "136260",
    "end": "141840"
  },
  {
    "text": "also use static volumes to store wall and DB files well static here",
    "start": "141840",
    "end": "147920"
  },
  {
    "text": "means every SED node where we use the same IP and volume after the previous",
    "start": "147920",
    "end": "153540"
  },
  {
    "text": "node is terminated it guarantees data durability even if SCD Chrome is lost permanently it also",
    "start": "153540",
    "end": "160860"
  },
  {
    "text": "indicates the sad membership is static there's no membership or reconfiguration",
    "start": "160860",
    "end": "169040"
  },
  {
    "text": "simplifies our the operation eks SCD supports version downgrade from",
    "start": "169040",
    "end": "175500"
  },
  {
    "text": "three five to three four it's a e case private patch and there is",
    "start": "175500",
    "end": "180720"
  },
  {
    "text": "a reference published in Upstream um eks",
    "start": "180720",
    "end": "186319"
  },
  {
    "text": "scd3435 and 33 does not have star region API layer schema incompatible issues",
    "start": "186319",
    "end": "193080"
  },
  {
    "text": "for example the difference between the three three and a three four rafter internal protocol buffer schema change",
    "start": "193080",
    "end": "199319"
  },
  {
    "text": "is a list checkpoint request uh if Venus 3-4 liter the experimental",
    "start": "199319",
    "end": "206580"
  },
  {
    "text": "release checkpoint feature is enabled once it starts to replicate the entry to the 3-3 follower the follower is it",
    "start": "206580",
    "end": "213300"
  },
  {
    "text": "server cannot understand the entry schema and and we're just Panic during",
    "start": "213300",
    "end": "219000"
  },
  {
    "text": "deserialization yeah",
    "start": "219000",
    "end": "225080"
  },
  {
    "text": "of course uh eks LCD are runs as a systemdm service unlike runs acid in a",
    "start": "225379",
    "end": "233340"
  },
  {
    "text": "container uh we run HD as a systemd service because it's a simpler approach",
    "start": "233340",
    "end": "239400"
  },
  {
    "text": "with less overhead involved like Network isolation and container Orchestra",
    "start": "239400",
    "end": "245580"
  },
  {
    "text": "orchestration",
    "start": "245580",
    "end": "248580"
  },
  {
    "text": "fifth eks SCD operator agent around in the sandbox as sat in eks we",
    "start": "252379",
    "end": "261540"
  },
  {
    "text": "run acid operator agent in the same box as SCD to manage provisioning Health",
    "start": "261540",
    "end": "267479"
  },
  {
    "text": "checking taking periodic backup and a story in persistent storage",
    "start": "267479",
    "end": "273240"
  },
  {
    "text": "automatic defrag and monitoring Etc a no space alarm self-service",
    "start": "273240",
    "end": "279180"
  },
  {
    "text": "however touched upon later by Gita",
    "start": "279180",
    "end": "284360"
  },
  {
    "text": "yep that's a introduction of acdfdks",
    "start": "284520",
    "end": "290220"
  },
  {
    "text": "um so now that you guys know who we are let's get to the agenda so today we want",
    "start": "294720",
    "end": "300060"
  },
  {
    "text": "to talk about five operational issues that we see while we are while operating",
    "start": "300060",
    "end": "305220"
  },
  {
    "text": "at CD the first one is where the storage quota provided by hcd is not enough for",
    "start": "305220",
    "end": "311040"
  },
  {
    "text": "the workload second one is the revision Divergence issue where the nodes don't",
    "start": "311040",
    "end": "316139"
  },
  {
    "text": "agree with each other a city can run out of memory that's the third one we sometimes see timeouts uh mostly related",
    "start": "316139",
    "end": "324180"
  },
  {
    "text": "to the maintenance workflows of hcd ciao will talk about that and sometimes we",
    "start": "324180",
    "end": "329220"
  },
  {
    "text": "see requests that are too large then the limit at CD runs with",
    "start": "329220",
    "end": "334919"
  },
  {
    "text": "let's get started so the first one is the database size quota exceeded",
    "start": "334919",
    "end": "340380"
  },
  {
    "text": "before I get to the production issue let's recap a few Concepts",
    "start": "340380",
    "end": "346320"
  },
  {
    "text": "so this is a simplified view of hcd layers we have raft for consensus and",
    "start": "346320",
    "end": "353039"
  },
  {
    "text": "then we have the backend storage which is bold TV or the DB file which is a big",
    "start": "353039",
    "end": "358320"
  },
  {
    "text": "memory mapped file the concept of quota applies to the back end",
    "start": "358320",
    "end": "364520"
  },
  {
    "text": "this is a toy example where we show key a which is shown in pink and key B which",
    "start": "364520",
    "end": "371759"
  },
  {
    "text": "is shown in green and those two keys have filled up the file A1 A2 A3 they",
    "start": "371759",
    "end": "377520"
  },
  {
    "text": "represent updates to that key and a Del represents the key getting deleted same",
    "start": "377520",
    "end": "384060"
  },
  {
    "text": "for B1 b2b3 there updates to that key so things to note here is that when the",
    "start": "384060",
    "end": "389759"
  },
  {
    "text": "file fills up there are multiple revisions per key occupying space in that file the other thing to notice is",
    "start": "389759",
    "end": "397199"
  },
  {
    "text": "that it's copy on right semantics so every update to the key takes up a new",
    "start": "397199",
    "end": "402720"
  },
  {
    "text": "page in the file deletes work the same way so a deletion will also add to the",
    "start": "402720",
    "end": "408419"
  },
  {
    "text": "file when the file hits the quota an alarm is raised and that needs to be explicitly",
    "start": "408419",
    "end": "415500"
  },
  {
    "text": "disarmed today we run with 8GB quota which is the maximum supported limit",
    "start": "415500",
    "end": "420539"
  },
  {
    "text": "Upstream when the alarm hits the cluster becomes read-only any modify operation put",
    "start": "420539",
    "end": "427259"
  },
  {
    "text": "operation cannot get in the next concept is that of compaction",
    "start": "427259",
    "end": "433940"
  },
  {
    "text": "so compaction cleans up the old history so in our toy example here if we",
    "start": "433940",
    "end": "439979"
  },
  {
    "text": "compacted everything that was in that file only B4 will stay everything else",
    "start": "439979",
    "end": "445380"
  },
  {
    "text": "will get cleaned up and the files will have free Pages or holes if you will under normal circumstances when there is",
    "start": "445380",
    "end": "452940"
  },
  {
    "text": "no alarm these holes are usable by hcd but when the alarm has already triggered",
    "start": "452940",
    "end": "459300"
  },
  {
    "text": "the put request will be declined even if there is there are holes in the file",
    "start": "459300",
    "end": "466560"
  },
  {
    "text": "in eks environment compaction is run by API server every five minutes",
    "start": "466560",
    "end": "473418"
  },
  {
    "text": "the next concept is that of the fragmentation and ciao will talk about this more when we visit the timeouts but",
    "start": "473699",
    "end": "480180"
  },
  {
    "text": "defragmentation basically removes all the holes and packs up the live data into a brand new file so again in our",
    "start": "480180",
    "end": "486780"
  },
  {
    "text": "toy example B4 will sit by itself in a brand new file at this point the size of",
    "start": "486780",
    "end": "492300"
  },
  {
    "text": "the file will drop however like I said the alarm will not clear by itself unless we call a specific disarm API",
    "start": "492300",
    "end": "502039"
  },
  {
    "text": "all right so in production multiple times we see workloads exceeding our 8GB",
    "start": "503280",
    "end": "508500"
  },
  {
    "text": "quota many times unintentionally when this limit is reached cluster becomes read-only and your on-call gets paged",
    "start": "508500",
    "end": "516240"
  },
  {
    "text": "the interesting thing is that the compaction as run by kubernetes API",
    "start": "516240",
    "end": "521640"
  },
  {
    "text": "server stops working when this alarm is raised this is because the compaction",
    "start": "521640",
    "end": "526860"
  },
  {
    "text": "workflow from kubernetes API server needs to do a put request before calling",
    "start": "526860",
    "end": "532380"
  },
  {
    "text": "the compact API and since that put request cannot get in it never calls the compact API the operator wakes up and",
    "start": "532380",
    "end": "540779"
  },
  {
    "text": "then we typically increase the quota and then it is a coordinated activity we request the customer to do the deletion",
    "start": "540779",
    "end": "547260"
  },
  {
    "text": "of objects before returning the quota back to 8GB",
    "start": "547260",
    "end": "552260"
  },
  {
    "text": "so why do we see this issue there are three main factors object size object",
    "start": "552480",
    "end": "557820"
  },
  {
    "text": "count and then multiple revisions due to fast updates so this is an example from production",
    "start": "557820",
    "end": "564839"
  },
  {
    "text": "where we see this last key here the admission reports there are two million",
    "start": "564839",
    "end": "569940"
  },
  {
    "text": "objects so even if the object size is just few KBS this will rack up GBS of",
    "start": "569940",
    "end": "576000"
  },
  {
    "text": "space quickly we have a Blog article out about this which you can find online",
    "start": "576000",
    "end": "582540"
  },
  {
    "text": "the second example object size this is when the workload typically has a big",
    "start": "582540",
    "end": "589560"
  },
  {
    "text": "key or a big binary blob in part specs such as SSH Keys typically because it's",
    "start": "589560",
    "end": "596339"
  },
  {
    "text": "part of the parts spec it gets replicated and the power spec becomes big like 500k 800k those are the bigger",
    "start": "596339",
    "end": "603180"
  },
  {
    "text": "ones we have seen while this is supported this is not optimal and there is an easy way to optimize this by",
    "start": "603180",
    "end": "609660"
  },
  {
    "text": "referencing the Big Blob instead of embedding it in the power spec so we talk about that in the in the blog",
    "start": "609660",
    "end": "616980"
  },
  {
    "text": "and the last one is these repeated updates we often get questions like I have just thousand Parts how come they",
    "start": "616980",
    "end": "623820"
  },
  {
    "text": "are taking up 8GB of space so remember that when the quota hits there are",
    "start": "623820",
    "end": "629940"
  },
  {
    "text": "multiple revisions per key so if something goes through fast updates it's taking up much more than the size of",
    "start": "629940",
    "end": "637440"
  },
  {
    "text": "that object consider a 800k pod spec and it goes through 10 updates it's now",
    "start": "637440",
    "end": "643740"
  },
  {
    "text": "taking up 8 mb per pod so thousands of those is gonna consume the AGB typically",
    "start": "643740",
    "end": "651000"
  },
  {
    "text": "we see this together with the large object size audit logs can help identify what's",
    "start": "651000",
    "end": "657839"
  },
  {
    "text": "changing fast and we have our Cloud watch query example in our blog other providers can also have similar query",
    "start": "657839",
    "end": "665399"
  },
  {
    "text": "um or on-prem also you can find the query for audit logs here is an example",
    "start": "665399",
    "end": "670860"
  },
  {
    "text": "of a fast changing objects here the scheduler is updating apart repeatedly just to record the fact that it's not",
    "start": "670860",
    "end": "678779"
  },
  {
    "text": "possible to schedule that part on any node so these are unintentional side",
    "start": "678779",
    "end": "684240"
  },
  {
    "text": "effects but they do eat up Dakota",
    "start": "684240",
    "end": "688220"
  },
  {
    "text": "this can be monitored for so for monitoring this proactively API server",
    "start": "689459",
    "end": "694560"
  },
  {
    "text": "has a metric for this so there could be a monitor on that detecting it reactively it has already",
    "start": "694560",
    "end": "700620"
  },
  {
    "text": "happened then there is this specific log message that you can look for which will",
    "start": "700620",
    "end": "705720"
  },
  {
    "text": "tell you that database quota is exceeded mitigation so today as I said when the",
    "start": "705720",
    "end": "713519"
  },
  {
    "text": "DB size approaches the quota it pages one of our team members who are on call and they wake up and they do this",
    "start": "713519",
    "end": "720000"
  },
  {
    "text": "coordination to get the cluster back into operational mode we have some work",
    "start": "720000",
    "end": "725700"
  },
  {
    "text": "in progress to automate some of the workflows we do manually today in long",
    "start": "725700",
    "end": "730980"
  },
  {
    "text": "run we would like to work with the community to enable a self-service experience for this such as a file",
    "start": "730980",
    "end": "737339"
  },
  {
    "text": "system stuff fills up you go and delete maybe in case of hcd you wait a little bit but then your cluster comes back to",
    "start": "737339",
    "end": "744060"
  },
  {
    "text": "rewrite we have thought of increasing the quota we used to run with 4GB now we run with",
    "start": "744060",
    "end": "749880"
  },
  {
    "text": "8GB uh increasing it any further we'll need more testing and validation for performance we are at the limit",
    "start": "749880",
    "end": "757140"
  },
  {
    "text": "supported by Upstream all right so that's the first issue now",
    "start": "757140",
    "end": "763980"
  },
  {
    "text": "let's hear from ciao about the revision Divergence",
    "start": "763980",
    "end": "769040"
  },
  {
    "text": "thanks Kita Let's uh go to the next topic acid",
    "start": "772860",
    "end": "778800"
  },
  {
    "text": "revision Divergence in a cluster um here is a animation of the revision",
    "start": "778800",
    "end": "786420"
  },
  {
    "text": "progress in SCD we can tell we are watching the the key",
    "start": "786420",
    "end": "792899"
  },
  {
    "text": "fool to get notification when the key value",
    "start": "792899",
    "end": "798660"
  },
  {
    "text": "pair is changed or updated or deleted you can see uh whenever we up",
    "start": "798660",
    "end": "809180"
  },
  {
    "text": "update a new version of the full key value the revision is increment is",
    "start": "809180",
    "end": "815880"
  },
  {
    "text": "incrementing and as long as we did it that revision is also increasing",
    "start": "815880",
    "end": "821899"
  },
  {
    "text": "so NCD uses This Global monotonically growing revision number to keep track of",
    "start": "821899",
    "end": "828839"
  },
  {
    "text": "changes to the data story in key Value Store where multiple nodes in the NCD",
    "start": "828839",
    "end": "834180"
  },
  {
    "text": "cluster are making changes to the data simultaneously it's possible for the revision numbers to strictly diverge and",
    "start": "834180",
    "end": "841200"
  },
  {
    "text": "converge to the same revision eventually however diverging revisions could also mean that",
    "start": "841200",
    "end": "847019"
  },
  {
    "text": "two or more nodes in the sad cluster have made conflicting changes to the",
    "start": "847019",
    "end": "852360"
  },
  {
    "text": "data for example OneNote successfully complete the modification or the other",
    "start": "852360",
    "end": "857579"
  },
  {
    "text": "node discards or partially commit the modification",
    "start": "857579",
    "end": "862820"
  },
  {
    "text": "this will cause sustained growing revision Divergence across SC nodes",
    "start": "863480",
    "end": "869459"
  },
  {
    "text": "so let's take a look at the real world example",
    "start": "869459",
    "end": "875600"
  },
  {
    "text": "uh recap each STD cluster is just remote uh cluster",
    "start": "875600",
    "end": "882680"
  },
  {
    "text": "the Orange Line represents the maximum of the revision across three nodes and",
    "start": "882680",
    "end": "888600"
  },
  {
    "text": "the green line represents the minimum of the revision across three nodes the Blue Line represents the gap or or the",
    "start": "888600",
    "end": "896760"
  },
  {
    "text": "Divergence of of the above two lines so you can",
    "start": "896760",
    "end": "902339"
  },
  {
    "text": "purely see the direction Scrolls uh starting from 10 o'clock",
    "start": "902339",
    "end": "908760"
  },
  {
    "text": "uh like and then indefinitely growing",
    "start": "908760",
    "end": "913800"
  },
  {
    "text": "so we observed this Fair symptom and configuring in our alarm systems and to",
    "start": "913800",
    "end": "921420"
  },
  {
    "text": "alert the on-call if the Divergence grows for an hour it helps our team",
    "start": "921420",
    "end": "927779"
  },
  {
    "text": "proactively mitigates the data in consistency problems uh before customer",
    "start": "927779",
    "end": "934860"
  },
  {
    "text": "noted uh notice the impact we also opened a HD feature request to",
    "start": "934860",
    "end": "942959"
  },
  {
    "text": "demonstrate an example to set up the alert role currently uh correctly in",
    "start": "942959",
    "end": "948060"
  },
  {
    "text": "permissions and grafana the link is shared below and it's on my",
    "start": "948060",
    "end": "954420"
  },
  {
    "text": "to-do list to complete it you may Wonder uh how come the revision",
    "start": "954420",
    "end": "961560"
  },
  {
    "text": "diverges Divergence grows so",
    "start": "961560",
    "end": "968000"
  },
  {
    "text": "this is the code snippet or from API server updates the objects so it's using",
    "start": "968000",
    "end": "975240"
  },
  {
    "text": "a SCD transaction API inside that transaction uh",
    "start": "975240",
    "end": "981720"
  },
  {
    "text": "if the SCT servers are stored prepared key modification is",
    "start": "981720",
    "end": "988320"
  },
  {
    "text": "is the same as the requested API servers kept revision number then it will result",
    "start": "988320",
    "end": "996779"
  },
  {
    "text": "into a put request otherwise",
    "start": "996779",
    "end": "1003339"
  },
  {
    "text": "it resulting get request so as you can see if initially uh the prepared Keys",
    "start": "1003339",
    "end": "1011720"
  },
  {
    "text": "modified revision is different with this core pattern from Klein",
    "start": "1011720",
    "end": "1017899"
  },
  {
    "text": "the revision Divergence will grow",
    "start": "1017899",
    "end": "1022660"
  },
  {
    "text": "impacted to kubernetes the core kubernetes components were",
    "start": "1026839",
    "end": "1033319"
  },
  {
    "text": "failed to acquire these and stop functioning for example Cube scheduler",
    "start": "1033319",
    "end": "1039980"
  },
  {
    "text": "and acute control manager and also many other",
    "start": "1039980",
    "end": "1045400"
  },
  {
    "text": "leadership based controllers will fail to acquire lease and stop functioning as",
    "start": "1045400",
    "end": "1050540"
  },
  {
    "text": "well um in other words a deployment scaling and",
    "start": "1050540",
    "end": "1057140"
  },
  {
    "text": "post-scale and post scheduling will fail so uh the following a screenshot is a",
    "start": "1057140",
    "end": "1063880"
  },
  {
    "text": "error message from the cube control manager",
    "start": "1063880",
    "end": "1069860"
  },
  {
    "text": "and and keep schedulers",
    "start": "1069860",
    "end": "1073600"
  },
  {
    "text": "next uh mitigation under medication can be simple once we detected the the the",
    "start": "1077600",
    "end": "1083559"
  },
  {
    "text": "failure mode we just removed the revision lagging",
    "start": "1083559",
    "end": "1089539"
  },
  {
    "text": "behind member and join a fresh new member to be in sync with other members so it's",
    "start": "1089539",
    "end": "1097039"
  },
  {
    "text": "exception to use static volume we were basically replace",
    "start": "1097039",
    "end": "1102620"
  },
  {
    "text": "um the DB so this is followed by the uh Upstream",
    "start": "1102620",
    "end": "1110419"
  },
  {
    "text": "operation guidance of data description next root cause the root cause",
    "start": "1110419",
    "end": "1118520"
  },
  {
    "text": "now it can be classified as two category or honest SCD mvcc partial commits",
    "start": "1118520",
    "end": "1126020"
  },
  {
    "text": "and followed by ACD process panics or um like secure the process",
    "start": "1126020",
    "end": "1135039"
  },
  {
    "text": "it's well summarized in the Upstream data in Constitution inconsistency",
    "start": "1135100",
    "end": "1141320"
  },
  {
    "text": "summary and the maintainers did a tremendous great",
    "start": "1141320",
    "end": "1147620"
  },
  {
    "text": "job to preventing this in current 35 release so um",
    "start": "1147620",
    "end": "1153740"
  },
  {
    "text": "if you are interested we should now you should you can take a look and",
    "start": "1153740",
    "end": "1159200"
  },
  {
    "text": "contributing that uh robustness test framework the second is a little bit unknown uh",
    "start": "1159200",
    "end": "1166640"
  },
  {
    "text": "about a bodb or it's called SED back-end",
    "start": "1166640",
    "end": "1171679"
  },
  {
    "text": "uh corruption",
    "start": "1171679",
    "end": "1174580"
  },
  {
    "text": "the revision Divergence root cause he mentioned there was a talk by maintainer Marek",
    "start": "1180559",
    "end": "1187940"
  },
  {
    "text": "before me if you missed it please check it out he goes he does a deep dive on",
    "start": "1187940",
    "end": "1193220"
  },
  {
    "text": "how they found the inconsistencies and now have a robustness framework for",
    "start": "1193220",
    "end": "1198860"
  },
  {
    "text": "testing all right the third issue hcd can run out of memory under overload",
    "start": "1198860",
    "end": "1205760"
  },
  {
    "text": "this issue Cascades because if one node goes boom the same workload that brought",
    "start": "1205760",
    "end": "1212059"
  },
  {
    "text": "that one down goes to the next one brings that one down as well and now we have Quorum loss",
    "start": "1212059",
    "end": "1217280"
  },
  {
    "text": "typically our case study suggests that the workload that causes this are large",
    "start": "1217280",
    "end": "1222679"
  },
  {
    "text": "unpaginated range requests typically the same one get pods repeatedly",
    "start": "1222679",
    "end": "1228700"
  },
  {
    "text": "why do the why do we see this issue so these are typically the unpaginated list requests like I said it's basically",
    "start": "1228980",
    "end": "1236240"
  },
  {
    "text": "spiky workload it spikes too much too fast uh every new request is a new",
    "start": "1236240",
    "end": "1241460"
  },
  {
    "text": "allocation in hcd LCD will never return any cached answers and the mechanism to",
    "start": "1241460",
    "end": "1247880"
  },
  {
    "text": "free up that memory the garbage collection is asynchronous so there is a window where hcd can run into memory",
    "start": "1247880",
    "end": "1254780"
  },
  {
    "text": "pressure and can own so we have two mitigations for this the",
    "start": "1254780",
    "end": "1260360"
  },
  {
    "text": "first one is a change on the API server side this change when it sees an",
    "start": "1260360",
    "end": "1266059"
  },
  {
    "text": "unpaginated list request it paginates it and it before sending to its CD so it's reclient and server will always see a",
    "start": "1266059",
    "end": "1273799"
  },
  {
    "text": "paginated request the pagination limit is configurable the link to the cap is",
    "start": "1273799",
    "end": "1279140"
  },
  {
    "text": "on the slide even if we do this pagination we could",
    "start": "1279140",
    "end": "1284179"
  },
  {
    "text": "land up with a page that the request can still be spiky and can still cause the",
    "start": "1284179",
    "end": "1289640"
  },
  {
    "text": "issue depending on workload so we have a second layer of Defense on the server side this is a server side throttler",
    "start": "1289640",
    "end": "1295880"
  },
  {
    "text": "which is implemented as a grpc Interceptor so it's watching every incoming and outgoing request if it is",
    "start": "1295880",
    "end": "1303500"
  },
  {
    "text": "not a range request it doesn't do anything to it it just simply goes through if it is a range request then",
    "start": "1303500",
    "end": "1309559"
  },
  {
    "text": "the throttler will check if the box is under memory pressure by just Consulting the resident set size of the process as",
    "start": "1309559",
    "end": "1317480"
  },
  {
    "text": "a percentage of the total box memory if the memory is under pressure then the",
    "start": "1317480",
    "end": "1323659"
  },
  {
    "text": "request will be admitted after delay it will throttle it",
    "start": "1323659",
    "end": "1328960"
  },
  {
    "text": "so this graph shows the test where you can see the throttling in action the",
    "start": "1330860",
    "end": "1335900"
  },
  {
    "text": "blue line here is the memory pressure when it goes past 65 percent which is the threshold it's running with the",
    "start": "1335900",
    "end": "1342380"
  },
  {
    "text": "throttler which is the orange line that activates and it starts throttling the requests",
    "start": "1342380",
    "end": "1348200"
  },
  {
    "text": "this is the same test except it's trying to show that the throttler and the memory pressure feeds into our",
    "start": "1348200",
    "end": "1354620"
  },
  {
    "text": "scalability system so in this test the orange line is still the throttler metric but the green line is the total",
    "start": "1354620",
    "end": "1361280"
  },
  {
    "text": "system memory so this box got scaled up from 4GB to 16 GB because of the",
    "start": "1361280",
    "end": "1367100"
  },
  {
    "text": "throttle signal and the memory pressure uh someone might want to use the same",
    "start": "1367100",
    "end": "1374659"
  },
  {
    "text": "changes we are using uh you may not need it in case you have",
    "start": "1374659",
    "end": "1379940"
  },
  {
    "text": "total predictable workload you can always provide for the peak workload and",
    "start": "1379940",
    "end": "1385940"
  },
  {
    "text": "use a bigger box but if you are interested in trying out the changes please do reach out to us",
    "start": "1385940",
    "end": "1394059"
  },
  {
    "text": "right that brings us to the fourth issue let's hear from ciao about the timeouts",
    "start": "1395000",
    "end": "1403000"
  },
  {
    "text": "oh thanks Kita let's go to the first topic timeout and causing 5xx",
    "start": "1405620",
    "end": "1413419"
  },
  {
    "text": "one of the top contributors of kubernetes Apex server 5xx is is the",
    "start": "1413419",
    "end": "1418460"
  },
  {
    "text": "online defrag stop the word so as you can see from the diagram when",
    "start": "1418460",
    "end": "1424460"
  },
  {
    "text": "API server trying to get an object using key one uh",
    "start": "1424460",
    "end": "1429980"
  },
  {
    "text": "if the request hits to a CD server that is defragging then the request will",
    "start": "1429980",
    "end": "1436400"
  },
  {
    "text": "enhanced until timeout",
    "start": "1436400",
    "end": "1439780"
  },
  {
    "text": "what is SCD frac um",
    "start": "1443419",
    "end": "1448640"
  },
  {
    "text": "as a following statement were recorded from SCD document website uh the key",
    "start": "1448640",
    "end": "1454159"
  },
  {
    "text": "value store is a factory immutable its operation do not update the structure in place but instead always generate a new",
    "start": "1454159",
    "end": "1461600"
  },
  {
    "text": "updated structure or pass the versions of keys are still accessible and watchable after modification to prevent",
    "start": "1461600",
    "end": "1469220"
  },
  {
    "text": "the data store from going indefinitely over time the store may be compacted to",
    "start": "1469220",
    "end": "1474500"
  },
  {
    "text": "discard the oldest versions of data so you can see from the DB file there",
    "start": "1474500",
    "end": "1480860"
  },
  {
    "text": "are multiple revisions uh for for a key value payer or just a",
    "start": "1480860",
    "end": "1486500"
  },
  {
    "text": "single version a single version of key value pair like K2",
    "start": "1486500",
    "end": "1493820"
  },
  {
    "text": "so after we compact revision seven um",
    "start": "1493820",
    "end": "1500480"
  },
  {
    "text": "you can see any key value pair that is deleted it",
    "start": "1500480",
    "end": "1506840"
  },
  {
    "text": "will be compacted or any uh a previous version of of key value pair that is",
    "start": "1506840",
    "end": "1513740"
  },
  {
    "text": "superseded with a new version that will also be compacted and leaves of three pages in the in the DB",
    "start": "1513740",
    "end": "1521059"
  },
  {
    "text": "file compacting the key uh the key space key",
    "start": "1521059",
    "end": "1526159"
  },
  {
    "text": "street drops or information about Keys super City prior to a given key space",
    "start": "1526159",
    "end": "1531679"
  },
  {
    "text": "revision a space used by these keys can then becomes available for additional",
    "start": "1531679",
    "end": "1537260"
  },
  {
    "text": "rights to the key space so uh at this point after compaction the",
    "start": "1537260",
    "end": "1544580"
  },
  {
    "text": "DPA file will not shrinks but the free Pages can be reused later",
    "start": "1544580",
    "end": "1552039"
  },
  {
    "text": "like if you insert a new key value pair then the free page can be a reused",
    "start": "1552039",
    "end": "1560179"
  },
  {
    "text": "after compacting the case the key space the backend database",
    "start": "1560179",
    "end": "1566539"
  },
  {
    "text": "make easy and like internal fragmentation any Eternal fragmentation in space that",
    "start": "1566539",
    "end": "1572600"
  },
  {
    "text": "is free to use by the back end but still consumes storage space compacting old revisions internally in",
    "start": "1572600",
    "end": "1580039"
  },
  {
    "text": "fragments STD by leaving gaps in the database uh back in database for",
    "start": "1580039",
    "end": "1585260"
  },
  {
    "text": "argumentative spaces available for use by SED but all available to the host of",
    "start": "1585260",
    "end": "1590480"
  },
  {
    "text": "our system deleting application data does not reclaim the space on disk so that's when",
    "start": "1590480",
    "end": "1598279"
  },
  {
    "text": "the defract uh comment come into the play after defrag DB file size shrinks",
    "start": "1598279",
    "end": "1607120"
  },
  {
    "text": "so what's the mitigation it has adopted um is",
    "start": "1611360",
    "end": "1617480"
  },
  {
    "text": "reduce the frequency of online defrag to round ones every 84 for uh 48 hours for",
    "start": "1617480",
    "end": "1625039"
  },
  {
    "text": "each member and um and to reduce the latency of defrag",
    "start": "1625039",
    "end": "1630760"
  },
  {
    "text": "we are advise customers to keep the number of keys in STD small and increase this throughput",
    "start": "1630760",
    "end": "1640240"
  },
  {
    "text": "um on demand if there are a lot of objects or key values",
    "start": "1640400",
    "end": "1645940"
  },
  {
    "text": "other mitigation it could be a defrag offline if the uh what different offline",
    "start": "1646120",
    "end": "1653419"
  },
  {
    "text": "means is take down the ACD server",
    "start": "1653419",
    "end": "1658360"
  },
  {
    "text": "a defrag the DP file a shrinks the file size and remove all of the unused pages",
    "start": "1659360",
    "end": "1667120"
  },
  {
    "text": "uh and then bring back the AC server online so you could uh mitigate this if the",
    "start": "1667120",
    "end": "1675260"
  },
  {
    "text": "availability risk is accepted some notices remember to diffract one",
    "start": "1675260",
    "end": "1682700"
  },
  {
    "text": "member at a time [Music]",
    "start": "1682700",
    "end": "1687450"
  },
  {
    "text": "Future Works uh SE Upstream has some great proposal to",
    "start": "1688580",
    "end": "1694039"
  },
  {
    "text": "graceful uh defrag LCD um",
    "start": "1694039",
    "end": "1699919"
  },
  {
    "text": "well either from client or from server first is from server to make defragon",
    "start": "1699919",
    "end": "1705799"
  },
  {
    "text": "current internally and not blocking other",
    "start": "1705799",
    "end": "1711080"
  },
  {
    "text": "from client if it's a multi-node cluster this SCD server can notify the client",
    "start": "1711400",
    "end": "1719539"
  },
  {
    "text": "I'm going to defrag please fail over to other endpoints that",
    "start": "1719539",
    "end": "1726140"
  },
  {
    "text": "is not defragging",
    "start": "1726140",
    "end": "1729100"
  },
  {
    "text": "right um that brings us to the last issue which is request size to large so SCD",
    "start": "1733100",
    "end": "1740840"
  },
  {
    "text": "has a limit on the request size the default is 1.5 MB that's what we run with but sometimes we see workloads",
    "start": "1740840",
    "end": "1747860"
  },
  {
    "text": "which want to push more data through a request uh why do we see this uh typically it's",
    "start": "1747860",
    "end": "1754700"
  },
  {
    "text": "a workload issue so here is an example where a customer was using endpoint",
    "start": "1754700",
    "end": "1760159"
  },
  {
    "text": "objects and they were updating the service and it got bigger than 1.5 MB it",
    "start": "1760159",
    "end": "1765559"
  },
  {
    "text": "turned out to be a known Upstream issue and the solution was to use endpoint slices instead of using endpoints",
    "start": "1765559",
    "end": "1773059"
  },
  {
    "text": "so mitigation typically we prefer to stay with the Upstream limit for this issue and we have helped customers",
    "start": "1773059",
    "end": "1779620"
  },
  {
    "text": "change the workloads to match best practices to make this issue go away",
    "start": "1779620",
    "end": "1787360"
  },
  {
    "text": "all right so to summarize we talked about five operational issues that we",
    "start": "1788299",
    "end": "1793580"
  },
  {
    "text": "see first one was the database size exceeding second one was revision",
    "start": "1793580",
    "end": "1799039"
  },
  {
    "text": "Divergence we talked about the Panic due to out of memory we talked about",
    "start": "1799039",
    "end": "1804260"
  },
  {
    "text": "timeouts due to defrag and we talked about the oversized request",
    "start": "1804260",
    "end": "1811360"
  },
  {
    "text": "that's it from us if you want to share your experiences operating at CD please",
    "start": "1811580",
    "end": "1816740"
  },
  {
    "text": "reach out to us we would love to learn if you want to contribute to hcd please check out the contributing guide at City",
    "start": "1816740",
    "end": "1822980"
  },
  {
    "text": "maintainers are here so please check out the booth and feel free to reach out to us on slack any questions",
    "start": "1822980",
    "end": "1829399"
  },
  {
    "text": "uh thank you uh maybe you can use the microphone over there",
    "start": "1829399",
    "end": "1834700"
  },
  {
    "text": "how much are the github's controllers causing all these issues sorry can you please repeat how much are",
    "start": "1841120",
    "end": "1848779"
  },
  {
    "text": "github's controllers causing cold decisions because they can hit a lot on etcd right so uh can't put a number on",
    "start": "1848779",
    "end": "1857000"
  },
  {
    "text": "it but we do see list requests from Argo time to time",
    "start": "1857000",
    "end": "1863620"
  },
  {
    "text": "any other questions all right thank you have a have a great conference thank you",
    "start": "1866899",
    "end": "1872960"
  },
  {
    "text": "[Applause]",
    "start": "1872960",
    "end": "1880319"
  }
]