[
  {
    "start": "0",
    "end": "150000"
  },
  {
    "text": "hi welcome to jaeger project deep dive uh we have three speakers in this talk uh and we'll go",
    "start": "1120",
    "end": "6960"
  },
  {
    "text": "uh over very many aspects of jager projects but not all of them because they we have a short session and we need",
    "start": "6960",
    "end": "12400"
  },
  {
    "text": "to pack a lot of content my name is yurish crowe i'm an engineer at facebook",
    "start": "12400",
    "end": "17520"
  },
  {
    "text": "i'm a maintainer of jaeger and as well as open tracing and open telemetry projects all three of them are members of cloud",
    "start": "17520",
    "end": "23600"
  },
  {
    "text": "native computing foundation and i published a book last year mastering distributed tracing",
    "start": "23600",
    "end": "29279"
  },
  {
    "text": "where you can find more information about the history of jaeger history of open tracing and as well as uh kind of introduction",
    "start": "29279",
    "end": "35760"
  },
  {
    "text": "into the distributed tracing as a discipline first and i will give an introduction",
    "start": "35760",
    "end": "41360"
  },
  {
    "text": "and then we will talk about um jaeger features in jaeger architecture",
    "start": "41360",
    "end": "46559"
  },
  {
    "text": "uh i will talk about sampling because this is one area where people have often uh questions uh and pavel at the",
    "start": "46559",
    "end": "52719"
  },
  {
    "text": "end will talk about jaeger integration with open telemetry and deploying jaegers on kubernetes",
    "start": "52719",
    "end": "58000"
  },
  {
    "text": "and so first anae take away with the intro to tracing",
    "start": "58000",
    "end": "62879"
  },
  {
    "text": "thank you yuri hi everyone my name is ananya garwal i'm a software developer at grafana labs",
    "start": "66000",
    "end": "71520"
  },
  {
    "text": "and i'm a contributor to the jaeger and open telemetry projects today we're going to take a look at what",
    "start": "71520",
    "end": "76960"
  },
  {
    "text": "distributed tracing is and how it fits into our debugging workflow we're also going to learn some concepts and terminology",
    "start": "76960",
    "end": "83680"
  },
  {
    "text": "this is a photo of uber's internal architecture which is generated by jaeger and we can see that in a mature",
    "start": "83680",
    "end": "89680"
  },
  {
    "text": "environment the number of microservices can run very well into hundreds or even thousands each of the green nodes present in this",
    "start": "89680",
    "end": "96400"
  },
  {
    "text": "graph represents a microservice and the gray lines represent the communications between these micro",
    "start": "96400",
    "end": "101680"
  },
  {
    "text": "services when we interact with the uber app a single request to the uber infrastructure may look something like",
    "start": "101680",
    "end": "108079"
  },
  {
    "text": "this and this typically happens billions of times a day so what are some of the monitoring tools that we use",
    "start": "108079",
    "end": "114640"
  },
  {
    "text": "to monitor such a complex architecture typically we use a combination of metrics and logs",
    "start": "114640",
    "end": "120560"
  },
  {
    "text": "metrics are great because they're aggregatable they can be used to alert upon and they're a great way to get an",
    "start": "120560",
    "end": "126399"
  },
  {
    "text": "overall picture under the performance of the system this is a sample metric that i'm exposing from my application it's a",
    "start": "126399",
    "end": "132480"
  },
  {
    "text": "standard prometheus metric which gives me the duration or the latency of a request to the",
    "start": "132480",
    "end": "138239"
  },
  {
    "text": "system and here i can see that this gives me a very high level picture saying that my app of the ice cream shop took 10",
    "start": "138239",
    "end": "144959"
  },
  {
    "text": "seconds of latency for a given request however if i want more fine-grained information about the system",
    "start": "144959",
    "end": "151040"
  },
  {
    "start": "150000",
    "end": "150000"
  },
  {
    "text": "then i can add more and more tags to it but very quickly i run into this problem of cardinality explosion",
    "start": "151040",
    "end": "157440"
  },
  {
    "text": "cardinality refers to the number of items present in a set and so in the metrics world this means",
    "start": "157440",
    "end": "162480"
  },
  {
    "text": "the total number of values that a given label set can take as i add more and more labels in order",
    "start": "162480",
    "end": "168879"
  },
  {
    "text": "to get more information about my application i increase cardinality and this can lead to degraded",
    "start": "168879",
    "end": "174800"
  },
  {
    "text": "performance and is also not cost effective logs are also a great way to check the",
    "start": "174800",
    "end": "180560"
  },
  {
    "text": "health of a given service but under concurrent requests it's really difficult to get the stack trace",
    "start": "180560",
    "end": "185760"
  },
  {
    "text": "of a given of one particular request that pass through the service and so really we need tracing because",
    "start": "185760",
    "end": "193680"
  },
  {
    "text": "traces are like stack trace debugging for distributed systems and it also tells us",
    "start": "193680",
    "end": "199920"
  },
  {
    "text": "a story about the system or or it tells us a story of the life cycle of a request passing",
    "start": "199920",
    "end": "205280"
  },
  {
    "text": "through the system um distributed tracing works on the concept of context propagation",
    "start": "205280",
    "end": "212400"
  },
  {
    "start": "207000",
    "end": "207000"
  },
  {
    "text": "on the left we can see that we have a very simple micro service architecture where an edge service a creates a unique",
    "start": "212400",
    "end": "220080"
  },
  {
    "text": "id for every inbound request to it and every time it makes enough it makes a downstream",
    "start": "220080",
    "end": "225120"
  },
  {
    "text": "request it passes along this unique id as part of the context as these services do some quantum of",
    "start": "225120",
    "end": "231760"
  },
  {
    "text": "work and generate spans they attach this unique id to the span once they're emitted",
    "start": "231760",
    "end": "237439"
  },
  {
    "text": "and stitched together in the back end we can see the trace as shown on the right this is",
    "start": "237439",
    "end": "242640"
  },
  {
    "text": "formed with the help of the unique id and so now let's look at some traces so",
    "start": "242640",
    "end": "248640"
  },
  {
    "start": "248000",
    "end": "248000"
  },
  {
    "text": "these traces are generated from the sample hot rod application that shipped as part of the jaeger repository",
    "start": "248640",
    "end": "255920"
  },
  {
    "text": "when we click on the system dependencies system architecture diagram or the dependencies",
    "start": "255920",
    "end": "261120"
  },
  {
    "text": "diagram we see that as a developer this already gives me a very intuitive picture",
    "start": "261120",
    "end": "266639"
  },
  {
    "text": "of the architecture of the system and the request flowing through it so this shows me the different microservices that are involved and also",
    "start": "266639",
    "end": "273440"
  },
  {
    "text": "shows me how many requests were made between these microservices",
    "start": "273440",
    "end": "278400"
  },
  {
    "text": "next in jager we also have deep dependency graphs which may look similar to the system",
    "start": "279360",
    "end": "284639"
  },
  {
    "text": "diagram at first but the difference is that now they're filtered by the service which is highlighted",
    "start": "284639",
    "end": "290639"
  },
  {
    "text": "so in the system architecture we could have we could have calls to redis from we",
    "start": "290639",
    "end": "297759"
  },
  {
    "text": "could have calls from the driver to the redis which may not have originated from the front end but in the transit of service graphs we",
    "start": "297759",
    "end": "304880"
  },
  {
    "start": "302000",
    "end": "302000"
  },
  {
    "text": "ensure that the service that's highlighted only those only the requests originating from that",
    "start": "304880",
    "end": "311440"
  },
  {
    "text": "service are highlighted we can also switch to an operation view in in the layout mode over here which is",
    "start": "311440",
    "end": "318000"
  },
  {
    "text": "what is shown here",
    "start": "318000",
    "end": "320800"
  },
  {
    "text": "next when we select a trace this is the typical gantt chart view of the trace that we see",
    "start": "323120",
    "end": "328400"
  },
  {
    "text": "on the left we see that the services are arranged in a hierarchical manner which shows the transit of nature of",
    "start": "328400",
    "end": "334400"
  },
  {
    "text": "requests as generated at the top we have a minimap which is really useful if the trace has",
    "start": "334400",
    "end": "341120"
  },
  {
    "text": "like a couple of thousand spans because we can highlight a given section and it will only show the spans for that",
    "start": "341120",
    "end": "346800"
  },
  {
    "text": "duration next we see just uh with a simple look",
    "start": "346800",
    "end": "352560"
  },
  {
    "start": "348000",
    "end": "348000"
  },
  {
    "text": "of this we can see that some of these operations were blocking for instance the front-end server",
    "start": "352560",
    "end": "357680"
  },
  {
    "text": "service which called the customer is called the mysql service probably was trying to retrieve customer",
    "start": "357680",
    "end": "362960"
  },
  {
    "text": "information and all other operations were blocked on this service on this request",
    "start": "362960",
    "end": "370800"
  },
  {
    "start": "371000",
    "end": "371000"
  },
  {
    "text": "next we can see that the waterfall diagram over here clearly represents a sequential order of operations",
    "start": "371360",
    "end": "381039"
  },
  {
    "text": "this is also really useful for an application developer and they can look at they can look at the",
    "start": "381039",
    "end": "387440"
  },
  {
    "text": "operations and say hey this should have been parallelized why is this running in sequence",
    "start": "387440",
    "end": "395039"
  },
  {
    "text": "we can also see that the parent child relationships are you know encoded into this view we can",
    "start": "395039",
    "end": "400880"
  },
  {
    "text": "see that the parent always encompasses the descendants",
    "start": "400880",
    "end": "405440"
  },
  {
    "text": "next when i click on a given span it generates some extra information in the form of tags and logs",
    "start": "406639",
    "end": "413199"
  },
  {
    "text": "logs can be optionally indexed we at grafana labs do not index the logs",
    "start": "413199",
    "end": "418560"
  },
  {
    "text": "that are ingested into jaeger but here you can see that as part of the tags i can add",
    "start": "418560",
    "end": "423840"
  },
  {
    "text": "higher cardinality data for instance like the sql query itself it would be very difficult to view the",
    "start": "423840",
    "end": "429680"
  },
  {
    "text": "query if it was part of the service and operation itself but here i can add higher cardinality data",
    "start": "429680",
    "end": "437840"
  },
  {
    "start": "438000",
    "end": "438000"
  },
  {
    "text": "this is a new view that's arrived in jager which is the trace diff we can see that the two",
    "start": "439840",
    "end": "445599"
  },
  {
    "text": "traces being differed are give shown at the top one of them took 2.7 seconds while the other took",
    "start": "445599",
    "end": "452479"
  },
  {
    "text": "1.4 seconds so clearly there must have been something very different about this because",
    "start": "452479",
    "end": "457599"
  },
  {
    "text": "even though they're hitting the same end point which is shown at the top and the difference is all of these nodes",
    "start": "457599",
    "end": "463840"
  },
  {
    "text": "so we can see that they had a common parent uh which is the common gateway endpoint",
    "start": "463840",
    "end": "469120"
  },
  {
    "text": "that they all hit but from here trace b all the nodes",
    "start": "469120",
    "end": "474560"
  },
  {
    "text": "highlighted in red were not present in trace b so this is sort of like a visual diff that we see in systems like git",
    "start": "474560",
    "end": "481520"
  },
  {
    "text": "where the red nodes show what was absent uh what is absent in trace b compared to",
    "start": "481520",
    "end": "487520"
  },
  {
    "text": "trace a and the green node show what is present in race b compare to trace a",
    "start": "487520",
    "end": "493759"
  },
  {
    "text": "so the operations right at the top over here were more or less common",
    "start": "493759",
    "end": "499120"
  },
  {
    "text": "and we can see that there's like a substantial diff and this is probably because one of these was a success and the other",
    "start": "499120",
    "end": "504639"
  },
  {
    "text": "was a failure the other view that we have is to",
    "start": "504639",
    "end": "510400"
  },
  {
    "text": "actually compare span durations so in the first view in the previous diagram over here we could see",
    "start": "510400",
    "end": "516560"
  },
  {
    "text": "this was more of a node wise view which showed which nodes were part of a given trace",
    "start": "516560",
    "end": "522959"
  },
  {
    "text": "compared to another whereas this view shows more of the latency differences between the two traces and the darker",
    "start": "522959",
    "end": "530160"
  },
  {
    "text": "the node the the starker or the more the more contrast between the latencies in trace",
    "start": "530160",
    "end": "535920"
  },
  {
    "text": "a and trace b so for instance if i was a developer looking at these trace views for",
    "start": "535920",
    "end": "542080"
  },
  {
    "text": "comparison i would see that in trace b this particular node might have been a",
    "start": "542080",
    "end": "549519"
  },
  {
    "text": "problem because this seems to have elevated latencies com in trace b compared to trace a",
    "start": "549519",
    "end": "555279"
  },
  {
    "text": "and transitively all the other upstream services have sort of had a",
    "start": "555279",
    "end": "561600"
  },
  {
    "text": "higher latency if i hover on top of these it also shows me the difference in latency so this is",
    "start": "561600",
    "end": "568160"
  },
  {
    "text": "really useful for debugging purposes next we're going to quickly browse through the",
    "start": "568160",
    "end": "574080"
  },
  {
    "text": "jaeger architecture so jaeger is not just a single",
    "start": "574080",
    "end": "580959"
  },
  {
    "start": "576000",
    "end": "576000"
  },
  {
    "text": "binary it's a collection of services which help in trace data collection storage as well",
    "start": "580959",
    "end": "586959"
  },
  {
    "text": "as querying and visualization so on this broad spectrum on the left we have",
    "start": "586959",
    "end": "592000"
  },
  {
    "text": "client libraries which are used to instrument the application and they're typically written in the same language as the",
    "start": "592000",
    "end": "598000"
  },
  {
    "text": "application and so the officially supported libraries are in golang java python node",
    "start": "598000",
    "end": "604880"
  },
  {
    "text": "c plus plus and c sharp while php and ruby are community maintained libraries",
    "start": "604880",
    "end": "610640"
  },
  {
    "text": "and on the right side we have the visualization front end which is written in react.js it's beautiful",
    "start": "610640",
    "end": "616480"
  },
  {
    "text": "and this and something that we already discussed",
    "start": "616480",
    "end": "620720"
  },
  {
    "start": "621000",
    "end": "621000"
  },
  {
    "text": "one important point to note is that jaeger does not provide instrumentation",
    "start": "621600",
    "end": "626880"
  },
  {
    "text": "jaeger provides an sdk but not the instrumentation api for this we can use something like open",
    "start": "626880",
    "end": "632720"
  },
  {
    "text": "tracing or open telemetry so a little history about jaeger jaeger",
    "start": "632720",
    "end": "640000"
  },
  {
    "text": "was inspired by google's dapper and open zipkin it was created by uber",
    "start": "640000",
    "end": "645120"
  },
  {
    "text": "in august of 2015 and finally open source in april of 2017.",
    "start": "645120",
    "end": "650800"
  },
  {
    "text": "the same year jaeger joined cncf as an incubating project and it graduated to a top level cncf",
    "start": "650800",
    "end": "657279"
  },
  {
    "text": "project in 2019 so this workflow shows what requests",
    "start": "657279",
    "end": "664240"
  },
  {
    "start": "660000",
    "end": "660000"
  },
  {
    "text": "were part of what information is propagated as part of a regular http call between services",
    "start": "664240",
    "end": "672000"
  },
  {
    "text": "and how the trace data reaches the jaeger backend when service a which is an upstream",
    "start": "672000",
    "end": "678720"
  },
  {
    "text": "service here calls it downstream service b it adds instrumentation from its end to",
    "start": "678720",
    "end": "684959"
  },
  {
    "text": "add the con to add the unique id into the http context headers and then passes it along",
    "start": "684959",
    "end": "691200"
  },
  {
    "text": "to service b at service b's and it receives the http request passes out the context information and",
    "start": "691200",
    "end": "697839"
  },
  {
    "text": "uses the same trace id in the spans that it creates finally the",
    "start": "697839",
    "end": "703440"
  },
  {
    "text": "span data that is emitted from service a and service b reaches out of band to the jaeger back end where it is stitched",
    "start": "703440",
    "end": "709040"
  },
  {
    "text": "together to form a common trace in 2017 when jager was open sourced the",
    "start": "709040",
    "end": "716720"
  },
  {
    "start": "712000",
    "end": "712000"
  },
  {
    "text": "architecture looked something like this on the left left we have the host or container that has been instrumented",
    "start": "716720",
    "end": "723120"
  },
  {
    "text": "it has the application and the jaeger client which was used for instrumentation the jaeger client sends spans locally to",
    "start": "723120",
    "end": "731040"
  },
  {
    "text": "the jager agent sends spans to the jaeger agent which may be running locally either on the same machine",
    "start": "731040",
    "end": "736560"
  },
  {
    "text": "or as a kubernetes sidecar and from here the jaeger agent sends pants to a jaeger",
    "start": "736560",
    "end": "744079"
  },
  {
    "text": "collector the jaeger collector is more of a central component whereas a jager agent may be deployed in multiple clusters this is",
    "start": "744079",
    "end": "751200"
  },
  {
    "text": "done for several reasons because the link between the jaeger agent and yeager collectors",
    "start": "751200",
    "end": "756399"
  },
  {
    "text": "cross dc and might break so the jaeger agent can do stuff like buffering and so on the jaeger collector is more",
    "start": "756399",
    "end": "763279"
  },
  {
    "text": "of a central component that sort of receives the spans denormalizes them can",
    "start": "763279",
    "end": "768639"
  },
  {
    "text": "perform some additional cleansing of the data for example removal of sensitive",
    "start": "768639",
    "end": "774320"
  },
  {
    "text": "information and so on and then ingest this into a database",
    "start": "774320",
    "end": "780399"
  },
  {
    "text": "from here we have spark jobs that run on the span data ingested that can compute",
    "start": "780399",
    "end": "786320"
  },
  {
    "text": "the dependencies tab that we discussed earlier and finally the jaeger query which queries the database",
    "start": "786320",
    "end": "791920"
  },
  {
    "text": "to help visualizing the spans in the ui another important point to note are the",
    "start": "791920",
    "end": "798560"
  },
  {
    "text": "red lines in this graph which shows the flow of sampling information the jaeger collector is a central store",
    "start": "798560",
    "end": "805519"
  },
  {
    "text": "for all sampling configuration and this can be used to define for service sampling per",
    "start": "805519",
    "end": "811360"
  },
  {
    "text": "operation sampling and so on the jaeger client can poll the jager agent which in turn",
    "start": "811360",
    "end": "816560"
  },
  {
    "text": "can pull the jager collector and receive sampling information without ever having to rotate config maps or something like",
    "start": "816560",
    "end": "822959"
  },
  {
    "text": "that so this is really useful",
    "start": "822959",
    "end": "828320"
  },
  {
    "text": "another important change we made to the architecture was the introduction of kafka what this means is that we've been able",
    "start": "828320",
    "end": "835120"
  },
  {
    "text": "to decouple the ingestion of spans from from the client to the ingestion of spans",
    "start": "835120",
    "end": "840639"
  },
  {
    "text": "into the database the jaeger collector can enqueues fans into kafka and the jaeger ingester can asynchronously",
    "start": "840639",
    "end": "847680"
  },
  {
    "text": "consume them and insert them into the database so if ever we receive a high volume of",
    "start": "847680",
    "end": "853279"
  },
  {
    "text": "spans because of increased traffic they can be buffered in kafka without overwhelming the database",
    "start": "853279",
    "end": "859360"
  },
  {
    "text": "the flim streaming jobs can also now enqueue from kafka and write dependency information into",
    "start": "859360",
    "end": "864720"
  },
  {
    "text": "the database the rest of the architecture remains the same",
    "start": "864720",
    "end": "869839"
  },
  {
    "start": "869000",
    "end": "869000"
  },
  {
    "text": "speaking of technology stack jaeger is written in go it's a go back in for tracing data it",
    "start": "869839",
    "end": "876000"
  },
  {
    "text": "has a plugable storage with support for cassandra elasticsearch and badger databases it also has an in-memory store and it",
    "start": "876000",
    "end": "883199"
  },
  {
    "text": "has a plugable storage uh and uses hashicorp go plugin",
    "start": "883199",
    "end": "888399"
  },
  {
    "text": "so if there are experts in other databases you can write a plug-in which can be plugged into the jaeger",
    "start": "888399",
    "end": "894320"
  },
  {
    "text": "collector jaeger uses a react.js front-end which is really",
    "start": "894320",
    "end": "900240"
  },
  {
    "text": "feature-rich and beautiful jaeger has open tracing instrumentation libraries",
    "start": "900240",
    "end": "907040"
  },
  {
    "text": "it's compatible with all open tracing instrumentation libraries it also has strong integration with",
    "start": "907040",
    "end": "912320"
  },
  {
    "text": "kafka and apache flink for tracing analytics and with that",
    "start": "912320",
    "end": "918079"
  },
  {
    "text": "i pass over to yuri to talk about sampling in jaeger thank you",
    "start": "918079",
    "end": "927360"
  },
  {
    "text": "okay let's talk about sampling in the distributed tracing we use the term sampling in the classical statistical",
    "start": "927360",
    "end": "933920"
  },
  {
    "text": "sense meaning that we try to select a subset of all individuals or traces from a",
    "start": "933920",
    "end": "940880"
  },
  {
    "text": "population of all possible traces in order to estimate certain characteristics of that",
    "start": "940880",
    "end": "946240"
  },
  {
    "text": "population or more specifically to reason about application performance based on those samples that we've selected",
    "start": "946240",
    "end": "952639"
  },
  {
    "start": "952000",
    "end": "952000"
  },
  {
    "text": "the question is why do we need to sample there are several reasons um the first reason is that uh tracing",
    "start": "952639",
    "end": "959839"
  },
  {
    "text": "generates a lot of information and storing all of it incurs significant storage costs",
    "start": "959839",
    "end": "965120"
  },
  {
    "text": "here's some napkin math assume that we have a tracing span of about two kilobytes on average and we have a",
    "start": "965120",
    "end": "971759"
  },
  {
    "text": "server doing 10 000 query per seconds right so that means we're already generating 20 megabytes per second of data",
    "start": "971759",
    "end": "978560"
  },
  {
    "text": "uh now assume that we have 100 instances of that service so that's two gigabytes per second or",
    "start": "978560",
    "end": "983680"
  },
  {
    "text": "170 petabytes per day and that's just for one service if your architecture is complex it may have",
    "start": "983680",
    "end": "990399"
  },
  {
    "text": "hundreds or even thousands of services you can imagine how much data we could generate if we actually were",
    "start": "990399",
    "end": "995519"
  },
  {
    "text": "sampling every single request the other reason is that if we're not sampling",
    "start": "995519",
    "end": "1000800"
  },
  {
    "text": "then the instrumentation that collects the data from the application by itself introduces performance",
    "start": "1000800",
    "end": "1006800"
  },
  {
    "text": "overhead here's an example again if we have a service doing 10 000 qps",
    "start": "1006800",
    "end": "1011839"
  },
  {
    "text": "then it's we have roughly 100 microseconds uh per request to work with",
    "start": "1011839",
    "end": "1017199"
  },
  {
    "text": "right and so if the instrumentation takes like five microseconds then that's already five percent",
    "start": "1017199",
    "end": "1022240"
  },
  {
    "text": "overhead on your uh compute costs and um that like if you run in a very large",
    "start": "1022240",
    "end": "1028319"
  },
  {
    "text": "fleet that that's a significant amount uh and finally the third reason is that uh when we collect traces that data is",
    "start": "1028319",
    "end": "1035280"
  },
  {
    "text": "actually very repetitive a majority of traces look very same they have the same shape",
    "start": "1035280",
    "end": "1040558"
  },
  {
    "text": "roughly similar latency measurements and so storing all of them is kind of useless",
    "start": "1040559",
    "end": "1047520"
  },
  {
    "text": "we don't get any more insights if or store all of them and so that's why we sample however",
    "start": "1047520",
    "end": "1054000"
  },
  {
    "text": "in distributed tracing specifically sampling has a slightly interesting aspect uh",
    "start": "1054000",
    "end": "1060640"
  },
  {
    "text": "what is called what we need to do so-called consistent sampling and what we mean by that is that if we",
    "start": "1060640",
    "end": "1068160"
  },
  {
    "text": "collect uh spans for trace then we should either collect all of them across the whole architecture for that",
    "start": "1068160",
    "end": "1074080"
  },
  {
    "text": "given request or we should collect none of them right because the alternative to that is shown in this diagram",
    "start": "1074080",
    "end": "1080080"
  },
  {
    "text": "here let's assume we had a system on the left and then we started randomly making",
    "start": "1080080",
    "end": "1085280"
  },
  {
    "text": "sampling decisions as part of the request and the three nodes happened to sample and the other two",
    "start": "1085280",
    "end": "1090720"
  },
  {
    "text": "didn't and so we got a trace which looks like on the right but um that trace is kind of broken we have",
    "start": "1090720",
    "end": "1097679"
  },
  {
    "text": "the sum of the notes that came without a parent and so it's hard to reason about these traces and so",
    "start": "1097679",
    "end": "1103600"
  },
  {
    "text": "they're not as useful as if we sample consistently and we get the whole trace every time or",
    "start": "1103600",
    "end": "1108799"
  },
  {
    "text": "we don't get the trace at all which is also acceptable um and as far as specific sampling",
    "start": "1108799",
    "end": "1116799"
  },
  {
    "start": "1113000",
    "end": "1113000"
  },
  {
    "text": "techniques uh there are two primary uh consistent sampling techniques that",
    "start": "1116799",
    "end": "1122160"
  },
  {
    "text": "are used in the industry uh head based is like uh most most popular it",
    "start": "1122160",
    "end": "1128480"
  },
  {
    "text": "traditionally has been used from the days of google's dapper paper uh all the modern tracing systems",
    "start": "1128480",
    "end": "1134480"
  },
  {
    "text": "support that and recently in the last few years tail-based sampling",
    "start": "1134480",
    "end": "1139679"
  },
  {
    "text": "started appearing as another popular techniques and i'll talk about both of them",
    "start": "1139679",
    "end": "1145120"
  },
  {
    "text": "so let's talk about head-based sampling or also called upfront sampling the approach there is very simple when we",
    "start": "1145120",
    "end": "1151679"
  },
  {
    "text": "start a new trace let's say when we generate a brand new trace id because the incoming request didn't have any trace id",
    "start": "1151679",
    "end": "1158160"
  },
  {
    "text": "then we make a sampling decision at that same time and we capture that sampling decision in the trace context which is",
    "start": "1158160",
    "end": "1164240"
  },
  {
    "text": "propagated throughout the request right this way uh we guarantee that the trace is",
    "start": "1164240",
    "end": "1170160"
  },
  {
    "text": "consistent to sample as long as all the sdks on the on the path of the call graph respect the sampling decision and",
    "start": "1170160",
    "end": "1176080"
  },
  {
    "text": "capture the data accordingly that implementation has minimal overhead",
    "start": "1176080",
    "end": "1182160"
  },
  {
    "text": "when the trace is not sampled because again we propagate the flag saying don't collect anything and so all",
    "start": "1182160",
    "end": "1187840"
  },
  {
    "text": "the calls to the tracing sdk become no op and they're very cheap and so we don't affect performance by",
    "start": "1187840",
    "end": "1194160"
  },
  {
    "text": "that and they don't collect any data it's also fairly easy to implement because if you think about it the the code is really",
    "start": "1194160",
    "end": "1199919"
  },
  {
    "text": "you just make a probabilistic decision or some other algorithm to decide when to sample and",
    "start": "1199919",
    "end": "1206000"
  },
  {
    "text": "then you just pass it around and all the other downstream sdks they just respect that decision um however that",
    "start": "1206000",
    "end": "1212559"
  },
  {
    "text": "approach also has a couple of drawbacks uh one is it's it's not as good as at capturing uh various anomalies uh for",
    "start": "1212559",
    "end": "1220159"
  },
  {
    "text": "example let's say you're looking at your metrics and you see your p99 latency spiking suddenly so you want to see okay can i",
    "start": "1220159",
    "end": "1226320"
  },
  {
    "text": "find some traces that represent that spike well p99 already means 100 and if we're",
    "start": "1226320",
    "end": "1232080"
  },
  {
    "text": "sampling with a rate of one percent that means that our total probability of actually catching",
    "start": "1232080",
    "end": "1237520"
  },
  {
    "text": "uh your p99 latency trades is one in ten thousand right now if your traffic is very high",
    "start": "1237520",
    "end": "1243280"
  },
  {
    "text": "uh to service uh that you have sufficient number of uh traces captured",
    "start": "1243280",
    "end": "1248400"
  },
  {
    "text": "even with that probability then you probably can get some example the more rare the outliers are the the less",
    "start": "1248400",
    "end": "1254799"
  },
  {
    "text": "chance you actually capture them with the like uniform probabilistic sampling approach um and the second big drawback of",
    "start": "1254799",
    "end": "1261919"
  },
  {
    "text": "upfront sampling is that it cannot be reactive to how the requests behave in the architecture",
    "start": "1261919",
    "end": "1268159"
  },
  {
    "text": "because the sampling decision needs to be made at the very beginning when we know nothing about that request",
    "start": "1268159",
    "end": "1274080"
  },
  {
    "text": "maybe we know like which end point was hit right at best uh but but nothing else and we",
    "start": "1274080",
    "end": "1279360"
  },
  {
    "text": "definitely don't know what's going to happen uh to the request in a life cycle but we already made a sampling decision and",
    "start": "1279360",
    "end": "1285600"
  },
  {
    "text": "every downstream service has to respect it and so it's very hard to sort of react",
    "start": "1285600",
    "end": "1291200"
  },
  {
    "text": "to errors in upfront sampling what about sampling head-based sampling in jaeger so jager",
    "start": "1291200",
    "end": "1298080"
  },
  {
    "start": "1294000",
    "end": "1294000"
  },
  {
    "text": "is the case out of the box support that and they come with a assortment of different samplers uh such",
    "start": "1298080",
    "end": "1305360"
  },
  {
    "text": "as like always on always off or probabilistic the common mostly commonly used or rate limiting",
    "start": "1305360",
    "end": "1310640"
  },
  {
    "text": "meaning let's sample like 10 traces per second no more things like that right and the benefit",
    "start": "1310640",
    "end": "1315760"
  },
  {
    "text": "of that is those samplers are very easy to implement however the the downside of sort of",
    "start": "1315760",
    "end": "1321760"
  },
  {
    "text": "configuring sampling in this way is that uh when when you have a service",
    "start": "1321760",
    "end": "1327200"
  },
  {
    "text": "and you instantiate a tracer you have to give it a sampler with a specific configuration",
    "start": "1327200",
    "end": "1332720"
  },
  {
    "text": "which means that uh if you have thousands or hundreds of services in the organization",
    "start": "1332720",
    "end": "1339039"
  },
  {
    "text": "then all those decisions are made by individual developers they're kind of sticky because once you deploy it it",
    "start": "1339039",
    "end": "1344240"
  },
  {
    "text": "stays in production with that whatever probability or rate that was assigned uh and the developers",
    "start": "1344240",
    "end": "1350880"
  },
  {
    "text": "um they usually don't know what effect individual sampling rates may have on your tracing back and can you trace them",
    "start": "1350880",
    "end": "1357039"
  },
  {
    "text": "back and actually support that level of sampling right so it's there's like a disconnect between uh the interest of the back and",
    "start": "1357039",
    "end": "1363360"
  },
  {
    "text": "then the capacity of it and how the sampling is configured um in the sdks and so there for that reason jaeger uh",
    "start": "1363360",
    "end": "1371200"
  },
  {
    "text": "sdk is actually default to different type of sampler which is called remote sampler and what that means is that",
    "start": "1371200",
    "end": "1376960"
  },
  {
    "text": "it actually reads the configuration from the central uh tier from from the collectors in",
    "start": "1376960",
    "end": "1382640"
  },
  {
    "text": "jaeger backend such that that configuration can be controlled in the central place",
    "start": "1382640",
    "end": "1387679"
  },
  {
    "text": "and then it's the team that runs the tracing back and can actually determine uh how much sampling for which service",
    "start": "1387679",
    "end": "1393280"
  },
  {
    "text": "for which endpoint should be happening right and you can change it on the fly if you want to uh but the point is that",
    "start": "1393280",
    "end": "1398880"
  },
  {
    "text": "you centralize the configuration rather than having it all done at the edges of your",
    "start": "1398880",
    "end": "1404320"
  },
  {
    "text": "traffic um here's an example of a configuration for for that type of sampling so um on the",
    "start": "1404320",
    "end": "1411200"
  },
  {
    "start": "1406000",
    "end": "1406000"
  },
  {
    "text": "top left we have a default sampling strategy which would apply to any service unless otherwise",
    "start": "1411200",
    "end": "1416320"
  },
  {
    "text": "configured with something else right so here we see it says like everyone should use probabilistic sampling with fifty",
    "start": "1416320",
    "end": "1422240"
  },
  {
    "text": "percent uh probability right uh except uh you can also provide some overrides for very",
    "start": "1422240",
    "end": "1428000"
  },
  {
    "text": "specific operations so let's say we don't want to sample anything on the health or on the metrics end point right and so",
    "start": "1428000",
    "end": "1433840"
  },
  {
    "text": "here we give them probabilities of zero on the right side there is a specific",
    "start": "1433840",
    "end": "1438880"
  },
  {
    "text": "overrides that we can do per service so if we know some services uh that",
    "start": "1438880",
    "end": "1444799"
  },
  {
    "text": "let's say maybe our foo service here is very like low qps and so",
    "start": "1444799",
    "end": "1449840"
  },
  {
    "text": "we give it a higher probability of sampling uh but on the other hand we can also override individual operations on",
    "start": "1449840",
    "end": "1455919"
  },
  {
    "text": "that service now let's talk about tail-based sampling so in tail-based sampling",
    "start": "1455919",
    "end": "1462159"
  },
  {
    "start": "1458000",
    "end": "1458000"
  },
  {
    "text": "as as the name applies the decision is made at the end of the trace rather at the beginning",
    "start": "1462159",
    "end": "1468000"
  },
  {
    "text": "what that means is that when we make a decision when we actually",
    "start": "1468000",
    "end": "1473279"
  },
  {
    "text": "already observed the whole trace and that decision may make can be a lot more intelligent because",
    "start": "1473279",
    "end": "1479520"
  },
  {
    "text": "uh we can look at latencies that we've seen on the trace we can see have there been any errors maybe there's",
    "start": "1479520",
    "end": "1484799"
  },
  {
    "text": "unusual cold uh graph shape etc so we can be very advanced with that decision",
    "start": "1484799",
    "end": "1490640"
  },
  {
    "text": "and only and basically either we instead of capturing samples uniformly we can",
    "start": "1490640",
    "end": "1495760"
  },
  {
    "text": "say let's steer them towards anomalies like if we have like a long latency or if we have an error let's always capture that",
    "start": "1495760",
    "end": "1501600"
  },
  {
    "text": "example right so that gives us control over what kind of data we can get into the back end um that means that",
    "start": "1501600",
    "end": "1508640"
  },
  {
    "text": "we can also catch anomalies much easier than with the upfront or head-based sampling um and another",
    "start": "1508640",
    "end": "1515600"
  },
  {
    "text": "benefit is that because we are getting all this data into collectors before we make a sampling decision",
    "start": "1515600",
    "end": "1520720"
  },
  {
    "text": "we're actually dealing with a lot more data that on which we can run various aggregations let's say we want to compute some statistical uh",
    "start": "1520720",
    "end": "1528240"
  },
  {
    "text": "aggregations of like what latency we're observing what histograms we're seeing in the services right if we do it before samples then we",
    "start": "1528240",
    "end": "1535200"
  },
  {
    "text": "just have more data to work with and those aggregates will be more accurate than if we were doing them",
    "start": "1535200",
    "end": "1540400"
  },
  {
    "text": "after the sampling especially if we're doing after sampling which is not uniform but like skewed towards",
    "start": "1540400",
    "end": "1545919"
  },
  {
    "text": "anomalies um two drawbacks though that approach is that because we need to collect the whole",
    "start": "1545919",
    "end": "1552720"
  },
  {
    "text": "trace uh before we can make a sampling decision it means we need to store it somewhere right because traces are distributed and",
    "start": "1552720",
    "end": "1559440"
  },
  {
    "text": "they're produced uh in the individual pieces from multiple services we kind of have to collect them all in one place",
    "start": "1559440",
    "end": "1565360"
  },
  {
    "text": "uh you have to hold on to it and until you receive all the data for that trace which is also uh and like somewhat indeterminate time",
    "start": "1565360",
    "end": "1573360"
  },
  {
    "text": "potentially and typically in in modern",
    "start": "1573360",
    "end": "1578880"
  },
  {
    "text": "systems that support tailway sampling this is done in memory like you store traces in memory uh then you make a sampling decision and",
    "start": "1578880",
    "end": "1585120"
  },
  {
    "text": "if the decision is no you just throw them away expire from the memory right and because most of the requests are",
    "start": "1585120",
    "end": "1590240"
  },
  {
    "text": "very short-lived you can actually scale that system fairly well to very large traffic of inbound traces um",
    "start": "1590240",
    "end": "1598400"
  },
  {
    "text": "and the other downside of tail-based sampling is that if if the goal is actually to collect all",
    "start": "1598400",
    "end": "1604400"
  },
  {
    "text": "the data up front and then sample only so much so that our storage can support",
    "start": "1604400",
    "end": "1609840"
  },
  {
    "text": "uh then all this collection up front introduces an additional overhead on the application itself",
    "start": "1609840",
    "end": "1616000"
  },
  {
    "text": "right as i mentioned in a napkin mask before uh you can have up to five to like eight",
    "start": "1616000",
    "end": "1621919"
  },
  {
    "text": "percent overhead uh on very high qps services if you essentially",
    "start": "1621919",
    "end": "1627039"
  },
  {
    "text": "collect data on every single request and export it into collector tier before you make the sampling decision um",
    "start": "1627039",
    "end": "1634159"
  },
  {
    "text": "so it's a bit expensive in terms of jager support so jaeger",
    "start": "1634159",
    "end": "1640240"
  },
  {
    "start": "1635000",
    "end": "1635000"
  },
  {
    "text": "as we'll talk later um is moving towards uh building most of the jaegerbacons on top",
    "start": "1640240",
    "end": "1646000"
  },
  {
    "text": "of open telemetry collector and open telemetry collector does uh have a logic for tailway sampling all right you can",
    "start": "1646000",
    "end": "1652720"
  },
  {
    "text": "configure various sampling rules already based on like latency or certain tags like the error flags",
    "start": "1652720",
    "end": "1658000"
  },
  {
    "text": "uh unfortunately at this time it's only in a single node mode so um sort of you you can run a single",
    "start": "1658000",
    "end": "1664799"
  },
  {
    "text": "service and send all the traces there then it will work but if if your traffic is such so large that",
    "start": "1664799",
    "end": "1671679"
  },
  {
    "text": "that service cannot scale to it and you need to run multiple collectors normally they're status so it's not a problem but",
    "start": "1671679",
    "end": "1677840"
  },
  {
    "text": "in case of a tail-based sampling they are become stateful and there needs to be some sort of a sharding solution which is right now not",
    "start": "1677840",
    "end": "1684559"
  },
  {
    "text": "available but it will be available in the future there are already prototypes in open telemetry",
    "start": "1684559",
    "end": "1690000"
  },
  {
    "text": "so that is uh all about something uh and now pavwell will talk about um",
    "start": "1690000",
    "end": "1698320"
  },
  {
    "text": "uh jaeger and open telemeter integration hello everybody my name is paulo phi i'm",
    "start": "1698320",
    "end": "1704640"
  },
  {
    "text": "software engineer at traceable ai and i'm core maintainer of jager project",
    "start": "1704640",
    "end": "1710000"
  },
  {
    "text": "and contributor to open telemetry and open tracing projects in this section i will talk about eager",
    "start": "1710000",
    "end": "1716159"
  },
  {
    "text": "and open telemetry integration before we deep dive into open telemetry",
    "start": "1716159",
    "end": "1721760"
  },
  {
    "start": "1719000",
    "end": "1719000"
  },
  {
    "text": "let me quickly talk about open tracing so we better understand open telemetry as the next evolution of open tracing",
    "start": "1721760",
    "end": "1729520"
  },
  {
    "text": "and data collection libraries in general so on this slide we see basically two",
    "start": "1729520",
    "end": "1735600"
  },
  {
    "text": "parts on the bottom there is our tracing infrastructure collecting distributed",
    "start": "1735600",
    "end": "1740880"
  },
  {
    "text": "traces from the user application and on the top there is the user",
    "start": "1740880",
    "end": "1746559"
  },
  {
    "text": "application process that is instrumented with open tracing open tracing is uh is a specification",
    "start": "1746559",
    "end": "1754480"
  },
  {
    "text": "that tells what kind of data should be collected from the rpc frameworks databases and so",
    "start": "1754480",
    "end": "1760960"
  },
  {
    "text": "on but it is also an instrumentation api that sits in between user application",
    "start": "1760960",
    "end": "1768480"
  },
  {
    "text": "code and the tracing library implementation the tracing library implementation is",
    "start": "1768480",
    "end": "1774640"
  },
  {
    "text": "basically the implementation of the open tracing api so this",
    "start": "1774640",
    "end": "1780960"
  },
  {
    "text": "architecture allows us to change tracing system without changing all the instrumentation",
    "start": "1780960",
    "end": "1787679"
  },
  {
    "text": "points that are embedded into rpc frameworks but also in our application",
    "start": "1787679",
    "end": "1793840"
  },
  {
    "text": "however there is one downside that we want to do that we still have to recompile and redeploy our application",
    "start": "1793840",
    "end": "1802720"
  },
  {
    "text": "and this might be a problem if we have you know dozens maybe hundreds or even thousands of",
    "start": "1802720",
    "end": "1807760"
  },
  {
    "text": "microservices it can be very costly thing to do the problem is open tracing",
    "start": "1807760",
    "end": "1813279"
  },
  {
    "text": "doesn't define any data format so all the tracing implementations they use",
    "start": "1813279",
    "end": "1819600"
  },
  {
    "text": "different data formats so let's have a look at the open telemetry",
    "start": "1819600",
    "end": "1828159"
  },
  {
    "start": "1827000",
    "end": "1827000"
  },
  {
    "text": "so bbc basically the same architecture as from the as on the",
    "start": "1828159",
    "end": "1833520"
  },
  {
    "text": "previous slide but open telemetry is now now substitutes the",
    "start": "1833520",
    "end": "1839520"
  },
  {
    "text": "instrumentation abr api but also the implementation of that api",
    "start": "1839520",
    "end": "1844960"
  },
  {
    "text": "but we see also open telemetry uh in the agent and collector",
    "start": "1844960",
    "end": "1851760"
  },
  {
    "text": "so the difference between open tracing and open telemetry is that open telemetry defines the api but also defines the sdk",
    "start": "1851760",
    "end": "1859919"
  },
  {
    "text": "the implementation of that api and it also defines a data format that is exported from the sdk",
    "start": "1859919",
    "end": "1867919"
  },
  {
    "text": "and so this allows us to have an open telemetry collector which accepts you know this data format and then can",
    "start": "1867919",
    "end": "1876159"
  },
  {
    "text": "translate it to different data formats for different tracing systems",
    "start": "1876159",
    "end": "1881600"
  },
  {
    "text": "and this pattern allows us to change tracing system without",
    "start": "1881600",
    "end": "1888240"
  },
  {
    "text": "recompiling and redeploying our applications",
    "start": "1888240",
    "end": "1893840"
  },
  {
    "text": "so maybe a little bit confusing for jaeger users is that we see open telemetry logo in the agent and",
    "start": "1894559",
    "end": "1901440"
  },
  {
    "text": "collector and this is purely jager's decision because",
    "start": "1901440",
    "end": "1907200"
  },
  {
    "text": "in the jaeger project we have decided to base our agent and collector in gesture basically",
    "start": "1907200",
    "end": "1914399"
  },
  {
    "text": "all our beckon components on top of open telemetry collector so this way all the jagerbeckon",
    "start": "1914399",
    "end": "1920880"
  },
  {
    "text": "components will provide the same functionality that is available in the open telemetry collector and we",
    "start": "1920880",
    "end": "1927360"
  },
  {
    "text": "will just add jager specific functionality to it for example storage implementation",
    "start": "1927360",
    "end": "1935360"
  },
  {
    "text": "so let's talk more about open thermometer collector the collector itself is written in",
    "start": "1935360",
    "end": "1940399"
  },
  {
    "text": "goblin as jaeger beckon components and in terms of jaeger integration",
    "start": "1940399",
    "end": "1948559"
  },
  {
    "text": "we basically rebased our backend components on top of",
    "start": "1948559",
    "end": "1954720"
  },
  {
    "text": "open terminatory collector and we have added jaeger specific functionality to it",
    "start": "1954720",
    "end": "1960480"
  },
  {
    "text": "so now jager users will benefit uh from all the functionality that is",
    "start": "1960480",
    "end": "1966960"
  },
  {
    "text": "available in the collector but they will still be able to use all the current functionality of jager",
    "start": "1966960",
    "end": "1973760"
  },
  {
    "text": "components we want to we also want to make it very easy for users to migrate to these",
    "start": "1973760",
    "end": "1979519"
  },
  {
    "text": "components so we will keep the current you know architecture with agent collector ingester and all in",
    "start": "1979519",
    "end": "1986880"
  },
  {
    "text": "one and also probably the same configuration options",
    "start": "1986880",
    "end": "1992399"
  },
  {
    "text": "if you are interested on our website there is already a section for open telemetry where you can read",
    "start": "1992399",
    "end": "1999039"
  },
  {
    "text": "what kind of configuration options are provided but also there are some guidelines",
    "start": "1999039",
    "end": "2004480"
  },
  {
    "text": "and you can start using these new components right now so let's talk about jaeger and open",
    "start": "2004480",
    "end": "2011360"
  },
  {
    "text": "telemetry sdks relationship so open telemetry sdks they usually",
    "start": "2011360",
    "end": "2018880"
  },
  {
    "text": "support eager grpc exporter and jager propagation format so this basically allows you to use or",
    "start": "2018880",
    "end": "2025600"
  },
  {
    "text": "to deploy services instrumented with open telemetry into an ecosystem where you are using jaeger",
    "start": "2025600",
    "end": "2033440"
  },
  {
    "text": "clients then there is open tracing shin which is",
    "start": "2033440",
    "end": "2038720"
  },
  {
    "text": "which is basically an open tracing implementation that uses open telemetry sdks and this allows you",
    "start": "2038720",
    "end": "2046159"
  },
  {
    "text": "to use all existing open tracing instrumentation libraries with open trace with open telemetry sdk",
    "start": "2046159",
    "end": "2054878"
  },
  {
    "text": "and last but not least jaeger clients they support trace context",
    "start": "2054879",
    "end": "2061280"
  },
  {
    "text": "which is the default propagation format in open telemetry as the case so you will be able to use jaeger",
    "start": "2061280",
    "end": "2067760"
  },
  {
    "text": "clients in a new ecosystem with where open telemetry sdks are used",
    "start": "2067760",
    "end": "2075838"
  },
  {
    "text": "okay let's move to different topic uh which is eager and kubernetes",
    "start": "2076720",
    "end": "2082079"
  },
  {
    "start": "2082000",
    "end": "2082000"
  },
  {
    "text": "yiga provides an excellent integration with kubernetes and you can deploy jaeger into",
    "start": "2082079",
    "end": "2087440"
  },
  {
    "text": "kubernetes by using helm charts plain kubernetes manifest",
    "start": "2087440",
    "end": "2092638"
  },
  {
    "text": "files and also jager operator operator is probably the most advanced",
    "start": "2092639",
    "end": "2097839"
  },
  {
    "text": "method how you can deploy jaeger into kubernetes uh so it follows",
    "start": "2097839",
    "end": "2104720"
  },
  {
    "text": "the standard operator pattern where where first you have to deploy eager operator",
    "start": "2104720",
    "end": "2110320"
  },
  {
    "text": "create custom resource definition for it and then you will be able to create a",
    "start": "2110320",
    "end": "2116079"
  },
  {
    "text": "custom resource where you define what kind of parts of the jager deployment or how the jager deployment",
    "start": "2116079",
    "end": "2123760"
  },
  {
    "text": "should look like so for example in the custom resource you can define that you just want all in",
    "start": "2123760",
    "end": "2130160"
  },
  {
    "text": "one deployment or you want the production deployment with the storage back end",
    "start": "2130160",
    "end": "2136079"
  },
  {
    "text": "jager operator can also provision storage begins under some conditions so",
    "start": "2136079",
    "end": "2142560"
  },
  {
    "text": "for example if the cluster if you have deployed in",
    "start": "2142560",
    "end": "2147839"
  },
  {
    "text": "the cluster kafka or streams the operator jaeger operator will be able to auto provision the kafka",
    "start": "2147839",
    "end": "2154560"
  },
  {
    "text": "cluster for you okay this is everything from my side and",
    "start": "2154560",
    "end": "2160400"
  },
  {
    "text": "thank you very much for your attention thank you paul",
    "start": "2160400",
    "end": "2165520"
  },
  {
    "text": "this is the end of our talk uh these are the different ways you can get in touch with the jaeger maintainers and community",
    "start": "2165520",
    "end": "2171520"
  },
  {
    "text": "we have a biblically meetings where you can dial in and participate in discussions and make sure to start the projects on",
    "start": "2171520",
    "end": "2178400"
  },
  {
    "text": "github developers like those stars thank you very much for joining",
    "start": "2178400",
    "end": "2186560"
  }
]