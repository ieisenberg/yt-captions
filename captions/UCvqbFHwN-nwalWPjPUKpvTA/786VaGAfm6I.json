[
  {
    "text": "happy Friday everyone I hope you all have been enjoying the kubecon so far and also the rest of time so today we",
    "start": "0",
    "end": "7319"
  },
  {
    "text": "are going to focus on topic of model serving on kubernetes so my name is Dan",
    "start": "7319",
    "end": "13200"
  },
  {
    "text": "I run the influence platform at Bloomberg um working for AWS",
    "start": "13200",
    "end": "21420"
  },
  {
    "text": "okay so before I get started like I want to know like um so today we talk about the really the",
    "start": "21420",
    "end": "27720"
  },
  {
    "text": "last mileage after the Intel machine learning lifecycle um to deploy your model to production so",
    "start": "27720",
    "end": "34260"
  },
  {
    "text": "I wonder like how many people here like I are able to get your AI model service",
    "start": "34260",
    "end": "39420"
  },
  {
    "text": "operating on your laptop anyone able to have ever done that before",
    "start": "39420",
    "end": "45020"
  },
  {
    "text": "it would take minutes hours",
    "start": "45020",
    "end": "50360"
  },
  {
    "text": "um how many of you are able to get your model to production cool Fair bit",
    "start": "51000",
    "end": "58020"
  },
  {
    "text": "um and how many of you are able to get over running on kubernetes oh wow so more than what I expected",
    "start": "58020",
    "end": "66979"
  },
  {
    "text": "how many of you are able to deploy a large language model",
    "start": "66979",
    "end": "72060"
  },
  {
    "text": "okay one two three cool um so yeah",
    "start": "72060",
    "end": "78000"
  },
  {
    "text": "um so let's talk about that so how um about like a case server is able to help you like deploy your actually large",
    "start": "78000",
    "end": "84479"
  },
  {
    "text": "language inverse for example like chai GPT which is a kind of hyper so we'll cover that later",
    "start": "84479",
    "end": "90479"
  },
  {
    "text": "um so yes as you think like the launching AI application Pilots uh relatively easy and deceptive easy so",
    "start": "90479",
    "end": "98880"
  },
  {
    "text": "you can probably uh load up a motor and laptop and then put it behind a fast API server endpoint and you can standard",
    "start": "98880",
    "end": "106079"
  },
  {
    "text": "inference requests and pretty easily like get back a response back right so um but when you think about moving this",
    "start": "106079",
    "end": "112500"
  },
  {
    "text": "application to production uh it actually gets a lot more complex and there's a",
    "start": "112500",
    "end": "117720"
  },
  {
    "text": "lot of things you need to consider so it's not just a model and often time you will need to do some feature processing",
    "start": "117720",
    "end": "124219"
  },
  {
    "text": "for to process your image or like a tokenize your text inputs and you need",
    "start": "124219",
    "end": "131459"
  },
  {
    "text": "to like uh deploy the virtualization code like it was as a microservice and",
    "start": "131459",
    "end": "137459"
  },
  {
    "text": "to look up a feature store and your uh you may have your model on cloud storage and you may wonder how you can get that",
    "start": "137459",
    "end": "144480"
  },
  {
    "text": "model download onto your like a server process or a container and there how do",
    "start": "144480",
    "end": "150599"
  },
  {
    "text": "you communicate between your like a feature Transformer and the and the model service",
    "start": "150599",
    "end": "155959"
  },
  {
    "text": "and on production you um oftentimes you need to handle a large amount of data",
    "start": "155959",
    "end": "161540"
  },
  {
    "text": "and simultaneously so it's critical you uh you should be able to scale up your",
    "start": "161540",
    "end": "168920"
  },
  {
    "text": "imprint service in a cost effective way and another",
    "start": "168920",
    "end": "175580"
  },
  {
    "text": "top priority like considerations that like you often times your model may be like a contains sensitive data so you",
    "start": "175580",
    "end": "182160"
  },
  {
    "text": "need to secure your service endpoints and make sure your uh the service to service communication between",
    "start": "182160",
    "end": "188220"
  },
  {
    "text": "Transformer and a predictor is secured and the data is encrypted and you also",
    "start": "188220",
    "end": "194400"
  },
  {
    "text": "need to make sure your model deployment process is repeatable reproducible so in case there are some outage and you",
    "start": "194400",
    "end": "201480"
  },
  {
    "text": "should be able to replicate the problem pretty quickly in a few minutes or and",
    "start": "201480",
    "end": "206580"
  },
  {
    "text": "make sure it's reproducible and it is also easy to roll back to your previous",
    "start": "206580",
    "end": "212940"
  },
  {
    "text": "deployments so last not the least after your model is the problem is that the",
    "start": "212940",
    "end": "219300"
  },
  {
    "text": "product production you need to make sure you have all the available data like a tracing metrics and logs and to analyze",
    "start": "219300",
    "end": "226680"
  },
  {
    "text": "the your Model Behavior and to make sure it meets your production sras or",
    "start": "226680",
    "end": "234360"
  },
  {
    "text": "uh analyze the uh the uh the problem like pretty quickly with all the uh",
    "start": "234360",
    "end": "239519"
  },
  {
    "text": "available observated data um yeah so uh kubernetes actually is a",
    "start": "239519",
    "end": "246780"
  },
  {
    "text": "great platform which provides as a container uh orchestration uh platform where uh it's really specifically",
    "start": "246780",
    "end": "254280"
  },
  {
    "text": "designed for deploying microservices to raise all the capability of like a resource management load balancing and",
    "start": "254280",
    "end": "261239"
  },
  {
    "text": "accumulates provide a really nice Cloud native way like a yaml spec which you can like uh deploy your models with the",
    "start": "261239",
    "end": "268560"
  },
  {
    "text": "uh decorative yamos so this really makes sure your Department's reputable and reproducible it can run either on cloud",
    "start": "268560",
    "end": "275880"
  },
  {
    "text": "or on Prime um in any like a cloud environment and it also provides the horizontal scaling",
    "start": "275880",
    "end": "283199"
  },
  {
    "text": "features both works on like a CPU and a GPU device and cumulates is also four",
    "start": "283199",
    "end": "289380"
  },
  {
    "text": "tolerance by it can detect container failures um it's really resilient to all the",
    "start": "289380",
    "end": "296340"
  },
  {
    "text": "outages and can minimize the down times so kubernetes is great right",
    "start": "296340",
    "end": "302340"
  },
  {
    "text": "um and but it's a more designed for developers and AI Engineers usually are",
    "start": "302340",
    "end": "307380"
  },
  {
    "text": "not equipped with uh necessary knowledge for",
    "start": "307380",
    "end": "312419"
  },
  {
    "text": "for the for the necessary kubernetes knowledge so to bridge this Gap",
    "start": "312419",
    "end": "317759"
  },
  {
    "text": "um case server is a is designed for like a highly scalable and standardized based uh Cloud native mere inference platform",
    "start": "317759",
    "end": "324900"
  },
  {
    "text": "on kubernetes so which in case through all the complexity for the complex deployments to simplify the production",
    "start": "324900",
    "end": "331740"
  },
  {
    "text": "deployment so case surf currently can be deployed as a standalone so you can like",
    "start": "331740",
    "end": "337199"
  },
  {
    "text": "have a serving component running on production environment in an isolated environment uh sometimes because you may",
    "start": "337199",
    "end": "343320"
  },
  {
    "text": "not want to like a mix the training and serving money in the uh in the same",
    "start": "343320",
    "end": "348419"
  },
  {
    "text": "cluster so um and but you can also do like a for experimental or testing",
    "start": "348419",
    "end": "353580"
  },
  {
    "text": "purpose you can also deploy as the add-on components within Q flow as it",
    "start": "353580",
    "end": "360120"
  },
  {
    "text": "switches cncf actually is currently moving to cncf as well in both like",
    "start": "360120",
    "end": "365940"
  },
  {
    "text": "cloud and on-prem on-premises environment uh so a little history for the caser",
    "start": "365940",
    "end": "373020"
  },
  {
    "text": "project it was originally introduced back in 2019 kubecon in San Diego not",
    "start": "373020",
    "end": "379800"
  },
  {
    "text": "sure if anyone here like what's happens to be in that talk um but it has been a while like so as",
    "start": "379800",
    "end": "385860"
  },
  {
    "text": "you see like after in the coverage suddenly 2021 so in 2020 like Nvidia contributed the uh the influence",
    "start": "385860",
    "end": "392280"
  },
  {
    "text": "protocol to the project and in 2021 IBM contributed the model match to the case",
    "start": "392280",
    "end": "397620"
  },
  {
    "text": "of project uh and in 2022 so as the project scope continues to expand so um",
    "start": "397620",
    "end": "405600"
  },
  {
    "text": "because it was original like as a sub project started with a sub project on the cube floor umbrella and uh um and in",
    "start": "405600",
    "end": "412740"
  },
  {
    "text": "late like 2022 we uh it becomes an independent project which is currently hosted on the",
    "start": "412740",
    "end": "419340"
  },
  {
    "text": "air for AI foundation so as you can see now there are like 193 contributors and 20 call maintenance and",
    "start": "419340",
    "end": "428039"
  },
  {
    "text": "there are 26 known adopting companies which is listed but there's a lot of we",
    "start": "428039",
    "end": "434940"
  },
  {
    "text": "may not know which they might be using and then according to the survey there",
    "start": "434940",
    "end": "440280"
  },
  {
    "text": "are 63 percent of kubeflow users which is using ksurf currently and you can see",
    "start": "440280",
    "end": "447300"
  },
  {
    "text": "all the logos like uh it's the company currently using case serve",
    "start": "447300",
    "end": "452960"
  },
  {
    "text": "um so yeah um next I'm going to talk about the state of the project so uh as of case of",
    "start": "453060",
    "end": "459300"
  },
  {
    "text": "zero uh dot 10 release data features we kind of support so it mainly falls into three pillars call inference Advanced",
    "start": "459300",
    "end": "466319"
  },
  {
    "text": "inference and the model explainability and the monitoring so uh on the call inference side",
    "start": "466319",
    "end": "472020"
  },
  {
    "text": "um we uh it offers the feature Transformations and uh uh inference with",
    "start": "472020",
    "end": "478680"
  },
  {
    "text": "uh uh with a set of like a pre-built serving runtimes which is shipped out of standard okay server installation it",
    "start": "478680",
    "end": "485520"
  },
  {
    "text": "also allows you to build your own serving runtime with the custom python",
    "start": "485520",
    "end": "490680"
  },
  {
    "text": "SDK you can build your custom serving runtimes which are can follow all the in",
    "start": "490680",
    "end": "496380"
  },
  {
    "text": "same like standardized inference protocol um and uh on the uh Advanced inference",
    "start": "496380",
    "end": "502919"
  },
  {
    "text": "we currently support like a model mesh which allows you to pause thousand supporters at scale and inference graph",
    "start": "502919",
    "end": "510000"
  },
  {
    "text": "is used to for the use case where you want to build a more sophisticated ml",
    "start": "510000",
    "end": "515339"
  },
  {
    "text": "pipelines with multiple models changed together um and uh for model experimentability we K",
    "start": "515339",
    "end": "522599"
  },
  {
    "text": "server offers text image and tabular explainers and for more returning it",
    "start": "522599",
    "end": "528540"
  },
  {
    "text": "supports buyers adversial outlier and GIF detectors to make sure your model is",
    "start": "528540",
    "end": "535500"
  },
  {
    "text": "able to produce reliable predictions on production",
    "start": "535500",
    "end": "540440"
  },
  {
    "text": "uh next let's take a look at the Core Concepts in the care k-serv which is a",
    "start": "540740",
    "end": "546480"
  },
  {
    "text": "semi runtime and inference service so um imprint service allows to specify the",
    "start": "546480",
    "end": "551940"
  },
  {
    "text": "model format and Divergence View for the given chain model uh and the service runtime is really defines the the",
    "start": "551940",
    "end": "558120"
  },
  {
    "text": "templates for the parts which is to serve the one the particular model formats and so",
    "start": "558120",
    "end": "566160"
  },
  {
    "text": "um and the survey runtime is really uh for like a case of admins so it's a you",
    "start": "566160",
    "end": "572760"
  },
  {
    "text": "may need a little bit like a comments knowledge to define the the Passback how to serve your models and specify the",
    "start": "572760",
    "end": "579120"
  },
  {
    "text": "container images and all the arguments environment variables which is necessary to launch the imprint service",
    "start": "579120",
    "end": "585440"
  },
  {
    "text": "and so but on the user side it's the almost a lot simpler so you're already",
    "start": "585440",
    "end": "592019"
  },
  {
    "text": "only the need to specify is the model format and the versions of the model standard case of will be able to automatically select based on the model",
    "start": "592019",
    "end": "598800"
  },
  {
    "text": "from any specified on the internet service and the two use the the right server runtime to launch your model",
    "start": "598800",
    "end": "607700"
  },
  {
    "text": "um so as you can see the from The Matrix with a case of that's the product a wide range of like Serbian runtimes and also",
    "start": "608640",
    "end": "615959"
  },
  {
    "text": "which covers the various like different uh uh Emir from Frameworks like cycle",
    "start": "615959",
    "end": "621899"
  },
  {
    "text": "and SG boost tensorflow pytouch uh uh Onyx and you can also bring build your",
    "start": "621899",
    "end": "628920"
  },
  {
    "text": "own custom model format um um so um so we do as case of support a",
    "start": "628920",
    "end": "637500"
  },
  {
    "text": "variety of like serving runtimes so it's really important to give user a cohesive",
    "start": "637500",
    "end": "643160"
  },
  {
    "text": "influence experience so case of developers inference protocol which",
    "start": "643160",
    "end": "649279"
  },
  {
    "text": "enables the standardized high performance like a data plan and so we recently um s0 we see a lot more",
    "start": "649279",
    "end": "657000"
  },
  {
    "text": "adoptions for this protocol which is currently already implemented by a celldon ml server uh try to implement",
    "start": "657000",
    "end": "663959"
  },
  {
    "text": "server touch serve openvino and AMD inference server so we recently renamed the influence protocol to be in opening",
    "start": "663959",
    "end": "671640"
  },
  {
    "text": "physical protocol so the idea is to we have the core inference protocol and then like we encourage people to like",
    "start": "671640",
    "end": "677760"
  },
  {
    "text": "have a forum to like keep improving this protocol and have everyone or the",
    "start": "677760",
    "end": "683100"
  },
  {
    "text": "industry becomes more like a industry standard influence protocol for everyone and so this really allows the",
    "start": "683100",
    "end": "690540"
  },
  {
    "text": "interoperability between all these serving runtimes and it also and more",
    "start": "690540",
    "end": "695579"
  },
  {
    "text": "importantly it enables the uh the capability to building all this clients and Benchmark tours which can works with",
    "start": "695579",
    "end": "702779"
  },
  {
    "text": "all these serving runtimes without changing your client or like uh to tooling as to uh what um for like if you",
    "start": "702779",
    "end": "710220"
  },
  {
    "text": "want to experiment with different server runtimes or models um so the report is just up as of",
    "start": "710220",
    "end": "716399"
  },
  {
    "text": "yesterday so check out so",
    "start": "716399",
    "end": "721440"
  },
  {
    "text": "and uh so here is like an example where like uh",
    "start": "722160",
    "end": "727260"
  },
  {
    "text": "you can see like uh oh where's like a a Triton inference server a runtime and a",
    "start": "727260",
    "end": "732300"
  },
  {
    "text": "case server it can cover almost the most of the emerald popular Emerald Frameworks so in this case you can send",
    "start": "732300",
    "end": "739200"
  },
  {
    "text": "the same inputs it can work with tensorflow Triton tensorflow pytorch or",
    "start": "739200",
    "end": "745860"
  },
  {
    "text": "or like SG boost SQL it can use the same input and then it gives the same output",
    "start": "745860",
    "end": "751500"
  },
  {
    "text": "back to user so in this case you can easily like uh when you do the experimentation you can easily switch",
    "start": "751500",
    "end": "756959"
  },
  {
    "text": "between different serving runtimes or different like even like a model model formats",
    "start": "756959",
    "end": "763139"
  },
  {
    "text": "uh so the the open inference protocol defines the uh mainly the arrest and",
    "start": "763139",
    "end": "768180"
  },
  {
    "text": "grpc protocol which is a mainly for the trade-off like easy of use versus like high performance and so the it defines",
    "start": "768180",
    "end": "775500"
  },
  {
    "text": "the model uh servers server Health uh and the model metadata and the core",
    "start": "775500",
    "end": "780660"
  },
  {
    "text": "inference endpoint um so let's take a look at the most",
    "start": "780660",
    "end": "785760"
  },
  {
    "text": "important like uh endpoint for for inference and so um it defines the input and output you",
    "start": "785760",
    "end": "793380"
  },
  {
    "text": "can see like a a tensor a schemer where you can Define the shapes of your tensor",
    "start": "793380",
    "end": "798480"
  },
  {
    "text": "and the data the data usually needs to be flattened into the one-dimensional array when you send over the wire and uh",
    "start": "798480",
    "end": "806160"
  },
  {
    "text": "um so data science might be familiar with the common data types like a numpy",
    "start": "806160",
    "end": "812579"
  },
  {
    "text": "array or like a pandas data frame so for non-pyri you can easily like a code",
    "start": "812579",
    "end": "817820"
  },
  {
    "text": "translate into a tensor inputs with the consistent data type if the shapes are the same but if you're",
    "start": "817820",
    "end": "825660"
  },
  {
    "text": "like in a pandas data frame you may have the mixed input types with integers flow types or like strings so",
    "start": "825660",
    "end": "834060"
  },
  {
    "text": "in that case you can Define multiple inputs each with different can be different shape or different uh",
    "start": "834060",
    "end": "840300"
  },
  {
    "text": "uh data types so we do provide the different codecs",
    "start": "840300",
    "end": "845639"
  },
  {
    "text": "which can easily allow you to translate between the the numpy array and",
    "start": "845639",
    "end": "850800"
  },
  {
    "text": "appenders to the input tensor which can be like a theorized on The Wire",
    "start": "850800",
    "end": "857600"
  },
  {
    "text": "okay next I'm going to hand over my course gear to talk about the more exciting stuff about k-serv thanks Don",
    "start": "857880",
    "end": "864240"
  },
  {
    "text": "okay okay can you okay so here on the left hand",
    "start": "864240",
    "end": "870720"
  },
  {
    "text": "side we have the yaml definition where we have we can see the Transformer and the predictor where we Define for",
    "start": "870720",
    "end": "878279"
  },
  {
    "text": "example this Transformer is connecting to a feature store to fetch a feature vector and send the request to the predictor to do what it's supposed to do",
    "start": "878279",
    "end": "884899"
  },
  {
    "text": "nicely to do the matrix multiplication between the features and the weights that we have loaded in the model server",
    "start": "884899",
    "end": "890459"
  },
  {
    "text": "so let's see what is the role of K serve here the k-serv controller is reconciling by by looking at the",
    "start": "890459",
    "end": "896399"
  },
  {
    "text": "inference service is reconciling a k-native deployment an AK native serving deployment which is our core service",
    "start": "896399",
    "end": "902880"
  },
  {
    "text": "that we are using to to deploy our containers our pods so on the left hand",
    "start": "902880",
    "end": "908820"
  },
  {
    "text": "side we have the pods for the Transformer and on the right hand side we have the pods for the predictor service",
    "start": "908820",
    "end": "914480"
  },
  {
    "text": "so the the predictor service when it's instantiating when it's initializing it's using what we call a storage",
    "start": "914480",
    "end": "921240"
  },
  {
    "text": "initializer that is loading from the model storage the model file that has been saved after the training process so",
    "start": "921240",
    "end": "927959"
  },
  {
    "text": "the model serving process is loading this artifact in the memory of the CPU and then transfer it to the memory of",
    "start": "927959",
    "end": "934680"
  },
  {
    "text": "the GPU to do the matrix multiplication on the top we see the AI application that will send the inference request to",
    "start": "934680",
    "end": "941579"
  },
  {
    "text": "the Transformer service and the Transformer service in turn will transform it to a numpy array of",
    "start": "941579",
    "end": "947100"
  },
  {
    "text": "integers or floats to do the and invoke the predictor so this is the flow the AI",
    "start": "947100",
    "end": "952500"
  },
  {
    "text": "application is calling the Transformer service and then the predictor service gives the prediction back to the",
    "start": "952500",
    "end": "957660"
  },
  {
    "text": "Transformer to send it with processing by and adding some extra things in in",
    "start": "957660",
    "end": "963060"
  },
  {
    "text": "the request um now let's look at the model storage",
    "start": "963060",
    "end": "969360"
  },
  {
    "text": "pattern we have the storage initializer on the left where if we have a directory",
    "start": "969360",
    "end": "974760"
  },
  {
    "text": "structure in our cloud storage we can save different versions of models or different types of models in in such a",
    "start": "974760",
    "end": "982320"
  },
  {
    "text": "directory structure and then you can load these files in the mode from the storage initializer to the storage of",
    "start": "982320",
    "end": "989040"
  },
  {
    "text": "the Pod and since it's loaded in the storage of the Pod the other container which is in the pot the predictor will",
    "start": "989040",
    "end": "995519"
  },
  {
    "text": "load it in the CPU memory and in the gpg memory the same with the model puller container this is the pattern that we",
    "start": "995519",
    "end": "1001820"
  },
  {
    "text": "are using in the model message we will see later there are cases that there are use cases",
    "start": "1001820",
    "end": "1009259"
  },
  {
    "text": "where we see customers or companies or users that have a lot of models and it",
    "start": "1009259",
    "end": "1015079"
  },
  {
    "text": "doesn't make sense to use one model server per model it for example some model files can be one megabyte file uh",
    "start": "1015079",
    "end": "1021860"
  },
  {
    "text": "in size why should I bother to use a whole pod for that one megabyte model so",
    "start": "1021860",
    "end": "1027199"
  },
  {
    "text": "we have the concept of of multi-model server where we can host many models in the same model server yeah",
    "start": "1027199",
    "end": "1034640"
  },
  {
    "text": "um this use case is where maybe some data scientists are building models per user yeah imagine or models per customer",
    "start": "1034640",
    "end": "1042860"
  },
  {
    "text": "if this is a pattern for some some use cases and if we take the model server the",
    "start": "1042860",
    "end": "1050240"
  },
  {
    "text": "multi-model server in steroids we do have the model mesh that is making an agnostic to the model",
    "start": "1050240",
    "end": "1058000"
  },
  {
    "text": "infrastructure where many model servers can run in parallel and receive the",
    "start": "1058000",
    "end": "1063020"
  },
  {
    "text": "models in different versions different architectures different formats to host them the same way we are using the the",
    "start": "1063020",
    "end": "1069919"
  },
  {
    "text": "service mesh we are we can also consider the model message the the advanced",
    "start": "1069919",
    "end": "1075340"
  },
  {
    "text": "multi-model serving now last year we were excited to launch",
    "start": "1075340",
    "end": "1080600"
  },
  {
    "text": "the inference graph it's very popular among data scientists who want to not only have one model to do the inference",
    "start": "1080600",
    "end": "1086240"
  },
  {
    "text": "but use a series of models to perform a a sequence of tasks let's say an example",
    "start": "1086240",
    "end": "1093020"
  },
  {
    "text": "a model may be a typical model let's say an example model would be that we have a",
    "start": "1093020",
    "end": "1099320"
  },
  {
    "text": "classifier for animals and the first model is checking if this image that it's receiving is a cat or a dog yeah",
    "start": "1099320",
    "end": "1106400"
  },
  {
    "text": "then if the outcome of the prediction is that this is a dog maybe we have a downstream model that is doing",
    "start": "1106400",
    "end": "1113440"
  },
  {
    "text": "classification of what dog breed is this so conditionally based on the output of",
    "start": "1113440",
    "end": "1119120"
  },
  {
    "text": "the first model we can route the traffic to the next model to do the classification or of a more specialized",
    "start": "1119120",
    "end": "1125059"
  },
  {
    "text": "model so this is the inference graph and it's very helpful imagine like a like a dug",
    "start": "1125059",
    "end": "1130280"
  },
  {
    "text": "that you define the the route or the conditions that it can navigate in a",
    "start": "1130280",
    "end": "1135799"
  },
  {
    "text": "sequence or in a Ensemble pattern",
    "start": "1135799",
    "end": "1140559"
  },
  {
    "text": "a case of plays nicely with the rest of the K native CNC with the rest of the",
    "start": "1142640",
    "end": "1147799"
  },
  {
    "text": "cncf projects we already mentioned K native which is the core project that we are utilizing but we are also utilizing",
    "start": "1147799",
    "end": "1154640"
  },
  {
    "text": "steel for for having the Ingress Gateway and many other projects that help us",
    "start": "1154640",
    "end": "1161539"
  },
  {
    "text": "with the security with a tracing with a logging with a matrix and",
    "start": "1161539",
    "end": "1166940"
  },
  {
    "text": "with a faster transportation across the inference requests a case service is not part of cncf but is part of Linux",
    "start": "1166940",
    "end": "1173960"
  },
  {
    "text": "Foundation LFA of Link Foundation Ai and data but we would do we would like to",
    "start": "1173960",
    "end": "1179660"
  },
  {
    "text": "see caser being part of cncf because it plays nicely with the rest of the ecosystem",
    "start": "1179660",
    "end": "1186399"
  },
  {
    "text": "let's see how we are using the K native example so after the inference request is coming to the Ingress controller",
    "start": "1186440",
    "end": "1192559"
  },
  {
    "text": "sorry to the Ingress Gateway the Ingress Gateway will route the traffic to the K native service and the K native service",
    "start": "1192559",
    "end": "1198679"
  },
  {
    "text": "in turn will send the request to the Q proxy which is the sidecar of the envoy that is running next in the Pod together",
    "start": "1198679",
    "end": "1205039"
  },
  {
    "text": "with a Transformer and do the prediction and give back the response there are",
    "start": "1205039",
    "end": "1211760"
  },
  {
    "text": "cases that you might have many models that are different versions or different formats that you would like to try and",
    "start": "1211760",
    "end": "1219140"
  },
  {
    "text": "you might want to have serverless capabilities for model serving K native is empowering us of of using such",
    "start": "1219140",
    "end": "1225500"
  },
  {
    "text": "capabilities because it supports scale down to zero this is not something that you can get out of the box from",
    "start": "1225500",
    "end": "1230960"
  },
  {
    "text": "kubernetes with a horizontal scaler and the K native Auto scaler is powerful tool that we love using because we can",
    "start": "1230960",
    "end": "1238039"
  },
  {
    "text": "have our models deployed in our cluster but turned off and when there is a request coming the auto scaler of K",
    "start": "1238039",
    "end": "1245360"
  },
  {
    "text": "native will will hold it we'll receive it but it will hold it it will instantiate the model version that needs",
    "start": "1245360",
    "end": "1251900"
  },
  {
    "text": "to serve this particular request and send the request to the transformer in the predictor and give back the response",
    "start": "1251900",
    "end": "1257780"
  },
  {
    "text": "this is powerful and huge very very important capability when you're doing development and testing of different",
    "start": "1257780",
    "end": "1264799"
  },
  {
    "text": "versions or when you want to do a b testing across the next version or the",
    "start": "1264799",
    "end": "1270500"
  },
  {
    "text": "previous version that you had and the the service Mass capability that",
    "start": "1270500",
    "end": "1276080"
  },
  {
    "text": "we inherit from istio is also powerful and if you are using it you have also",
    "start": "1276080",
    "end": "1282140"
  },
  {
    "text": "most probably seen the benefits because with a sidecar of envoy deploying the PODS of our Transformers and our",
    "start": "1282140",
    "end": "1288440"
  },
  {
    "text": "predictors we are able to observe this traffic with the monitoring capabilities that he still is giving us out of the",
    "start": "1288440",
    "end": "1294799"
  },
  {
    "text": "box with a Jager for example to see how is the inference graph traversing across",
    "start": "1294799",
    "end": "1300320"
  },
  {
    "text": "the predictor and the Transformer and the request is coming back this is powerful for troubleshooting also not",
    "start": "1300320",
    "end": "1306740"
  },
  {
    "text": "only all for tracing but also for logging to have the ability to send the",
    "start": "1306740",
    "end": "1312039"
  },
  {
    "text": "requests that are coming to the envoy proxy to a to a third party a logging",
    "start": "1312039",
    "end": "1317600"
  },
  {
    "text": "provider or some for metrics istio is giving us out of the box the capability",
    "start": "1317600",
    "end": "1323120"
  },
  {
    "text": "with the Prometheus metrics that can be stored in another storage Ingress kit we",
    "start": "1323120",
    "end": "1329299"
  },
  {
    "text": "also handles the authentication mechanism so when the JWT token is is coming from the SSO or the the mechanism",
    "start": "1329299",
    "end": "1337520"
  },
  {
    "text": "that provides the authentication and the authorization is routing it to the appropriate services and and voice is",
    "start": "1337520",
    "end": "1344299"
  },
  {
    "text": "also making sure that the communication between the Transformer and the predictor is is authenticated with",
    "start": "1344299",
    "end": "1350720"
  },
  {
    "text": "mutual DLS further insecurity with use of spiffy",
    "start": "1350720",
    "end": "1356960"
  },
  {
    "text": "and the implementation of spire we have this secure identity framework that the",
    "start": "1356960",
    "end": "1363260"
  },
  {
    "text": "the identity providers providing this this token that is required by an application that might run on a VM to",
    "start": "1363260",
    "end": "1370880"
  },
  {
    "text": "perform this inference request and through the attestation process the the credentials are being issued for this",
    "start": "1370880",
    "end": "1376400"
  },
  {
    "text": "particular application finally on the observability stack we",
    "start": "1376400",
    "end": "1384140"
  },
  {
    "text": "are using all these tools already mentioned with HD on the K native that are giving us the capability to do the",
    "start": "1384140",
    "end": "1391220"
  },
  {
    "text": "tracing with open with hotel to use the cloud event so we have a cloud agnostic capability to store the logs and",
    "start": "1391220",
    "end": "1398360"
  },
  {
    "text": "transfer transfer the mass Events maybe to to a bus or or a broker that can be",
    "start": "1398360",
    "end": "1403400"
  },
  {
    "text": "further consumed by Downstream models like an explainer or a bias detector or",
    "start": "1403400",
    "end": "1409940"
  },
  {
    "text": "some simple model monitoring techniques but Cloud events is the core component here that helps us transfer this",
    "start": "1409940",
    "end": "1416620"
  },
  {
    "text": "events this inference requests and especially the responses which is the the the the the new treasure that you",
    "start": "1416620",
    "end": "1422960"
  },
  {
    "text": "are generating by by serving models we want to make sure that we are storing them properly and also doing more things",
    "start": "1422960",
    "end": "1428659"
  },
  {
    "text": "with it like seeing we're having class imbalance and do I need to retrain my model because I see that the the over",
    "start": "1428659",
    "end": "1435260"
  },
  {
    "text": "time this class has more predictions than the other Etc with Jager and grafana of course these",
    "start": "1435260",
    "end": "1442340"
  },
  {
    "text": "tools out of the box are giving us the capabilities to observe the information that is stored through these mechanisms",
    "start": "1442340",
    "end": "1450020"
  },
  {
    "text": "um build packs of course is also helping us is helping the developers forget about the docker combo the the docker",
    "start": "1450020",
    "end": "1457220"
  },
  {
    "text": "file they don't have to to to build it they don't have to build it and the security Departments of the organizations uh can apply their",
    "start": "1457220",
    "end": "1464659"
  },
  {
    "text": "policies by uh letting the developers of the Transformers only focus on doing their work on building the python code",
    "start": "1464659",
    "end": "1471260"
  },
  {
    "text": "for the Transformer rather than having to define the libraries and the requirements for the Transformer",
    "start": "1471260",
    "end": "1477980"
  },
  {
    "text": "now these are the things that we have already uh that are already available",
    "start": "1477980",
    "end": "1483020"
  },
  {
    "text": "and we need to start looking at the future the first thing that we need to uh to share with you is that we want to",
    "start": "1483020",
    "end": "1490520"
  },
  {
    "text": "in the roadmap of going to the version 1.0 we want to graduate all these capabilities that we currently have into",
    "start": "1490520",
    "end": "1496820"
  },
  {
    "text": "the 1.0 version this is the goal for this year for the community for of the project and hopefully will be",
    "start": "1496820",
    "end": "1505580"
  },
  {
    "text": "um realized at some point now let's take a step back in 2019 when we launched",
    "start": "1505580",
    "end": "1512179"
  },
  {
    "text": "k-serv we had the birth model this is how this is so see here here in history",
    "start": "1512179",
    "end": "1519320"
  },
  {
    "text": "how there is a logarithmic scale of the number of parameters that we see in the",
    "start": "1519320",
    "end": "1524360"
  },
  {
    "text": "model sizes we have more and more large models and K Serv when it was built it",
    "start": "1524360",
    "end": "1530659"
  },
  {
    "text": "was there to host models of I don't know 300 million parameters which is like one",
    "start": "1530659",
    "end": "1536840"
  },
  {
    "text": "gigabyte file fair enough you can load it in the CPU memory you can load it in the cool gpus",
    "start": "1536840",
    "end": "1542779"
  },
  {
    "text": "but now we have one trillion parameters from gpt4 how do we even Host this in a",
    "start": "1542779",
    "end": "1548840"
  },
  {
    "text": "in in a server that is that requires 400 gigabytes of parameter of parameter",
    "start": "1548840",
    "end": "1554539"
  },
  {
    "text": "values how do you serve it and how do you make sure that is fast enough so we",
    "start": "1554539",
    "end": "1559880"
  },
  {
    "text": "do believe that the future for the project is to support these language models and we will discuss with you some",
    "start": "1559880",
    "end": "1566659"
  },
  {
    "text": "of the capabilities and some of the challenges and some of the of the tools that we have to to help you get there",
    "start": "1566659",
    "end": "1573440"
  },
  {
    "text": "so first of all uh because we have hundreds of billions of parameters we have large size yes and this means large",
    "start": "1573440",
    "end": "1580820"
  },
  {
    "text": "transform transports transport cost a large time to complete",
    "start": "1580820",
    "end": "1587059"
  },
  {
    "text": "this transfer but also how to to break it down different gpus is a challenge so",
    "start": "1587059",
    "end": "1592820"
  },
  {
    "text": "then video faster Transformer can allows us to use the accelerator Sun engine and",
    "start": "1592820",
    "end": "1597980"
  },
  {
    "text": "support many gpus and nodes in a distributed fashion the hiking phase",
    "start": "1597980",
    "end": "1603020"
  },
  {
    "text": "accelerate is a mechanism that is splitting the file let's say the the four Giga the 400 gigabyte file of the",
    "start": "1603020",
    "end": "1609559"
  },
  {
    "text": "GPT into smaller chunks that can be loaded in the charge of the GPU to to to",
    "start": "1609559",
    "end": "1615919"
  },
  {
    "text": "to to calculate it in the distributed fashion the tensor parallelism is allowing us to do that across multiple",
    "start": "1615919",
    "end": "1622880"
  },
  {
    "text": "gpus in the same node and finally the pipeline parallelism is what is allowing us to do that across multiple gpus and",
    "start": "1622880",
    "end": "1629960"
  },
  {
    "text": "multiple nodes that are running gpus so this way we can stack many nodes the one after the other",
    "start": "1629960",
    "end": "1635980"
  },
  {
    "text": "that and all their memory is loading them the weights with the chance that we",
    "start": "1635980",
    "end": "1642020"
  },
  {
    "text": "have mentioned now some of the challenges that we see by deploying such models are the",
    "start": "1642020",
    "end": "1648860"
  },
  {
    "text": "following since the cost of the transfer is large we need to make sure that somehow maybe we are doing casting we",
    "start": "1648860",
    "end": "1655460"
  },
  {
    "text": "need to take into account also the latency how do we make sure that the inference request that will do the Matrix duplication across all gpus of",
    "start": "1655460",
    "end": "1662480"
  },
  {
    "text": "all nodes will be that fast that will you the user will not wait when you when you use that GPT you don't wait a lot",
    "start": "1662480",
    "end": "1668600"
  },
  {
    "text": "for the response it gives you the response within a couple of seconds so an example model file is the Bloom model",
    "start": "1668600",
    "end": "1675860"
  },
  {
    "text": "yeah this is a large language model that is available through hugging phase this is a almost 400 gigabyte size it has 176",
    "start": "1675860",
    "end": "1684559"
  },
  {
    "text": "billion parameters this file with the help of the accelerate can be split into 72 chunks of or splits of five gigabyte",
    "start": "1684559",
    "end": "1693020"
  },
  {
    "text": "file and then the normal gpus that we have in our normal nodes can handle this can load this file and then this the",
    "start": "1693020",
    "end": "1700700"
  },
  {
    "text": "distribution is is happening on the inference request other challenges are the transfer cost or the transfer time",
    "start": "1700700",
    "end": "1706580"
  },
  {
    "text": "between the cloud provider storage that you have like S3 where you load the",
    "start": "1706580",
    "end": "1711799"
  },
  {
    "text": "model in the Pod of the container that is running in the node that takes time and we have seen implementations and",
    "start": "1711799",
    "end": "1718159"
  },
  {
    "text": "contributions that people are sharing are casting this model into nvme storage",
    "start": "1718159",
    "end": "1724640"
  },
  {
    "text": "for example and the same challenges are with transfers now the faster Transformer or Triton is",
    "start": "1724640",
    "end": "1731360"
  },
  {
    "text": "a pod itself is an image that is 32 gigabytes of size this is another",
    "start": "1731360",
    "end": "1736400"
  },
  {
    "text": "challenge that we see how do you even load such a container we are struggling to make the containers a few megabytes not to have them in the size of 32",
    "start": "1736400",
    "end": "1743179"
  },
  {
    "text": "gigabytes so maybe as a demo here we have time",
    "start": "1743179",
    "end": "1749380"
  },
  {
    "text": "okay so this is an inference request to send a",
    "start": "1749840",
    "end": "1755120"
  },
  {
    "text": "if it's working let's try so we have an infinite service here with a predictor",
    "start": "1755120",
    "end": "1760220"
  },
  {
    "text": "in the spec that defines that is the model format is Triton and we have the",
    "start": "1760220",
    "end": "1765679"
  },
  {
    "text": "storage URI where we have saved our GPT model and the Transformer",
    "start": "1765679",
    "end": "1771140"
  },
  {
    "text": "is defining the path for the tokenizer that will transform the the text into",
    "start": "1771140",
    "end": "1777279"
  },
  {
    "text": "integers or tokens that will be in the input of the model",
    "start": "1777279",
    "end": "1782740"
  },
  {
    "text": "and this is a custom container that you can also find the code in the samples directory of k-serv if I will send the",
    "start": "1782740",
    "end": "1789620"
  },
  {
    "text": "inference request I already got back the response the inference request is opposed to to an",
    "start": "1789620",
    "end": "1795559"
  },
  {
    "text": "end point that is running the sdo Ingress and the input says kubernetes is the best platform to serve your models",
    "start": "1795559",
    "end": "1801679"
  },
  {
    "text": "because and I want 15 tokens back or 15 words back as an answer and the model is",
    "start": "1801679",
    "end": "1807440"
  },
  {
    "text": "telling me is doing some reasoning and telling me that because it's a cloud-based service that is built etc",
    "start": "1807440",
    "end": "1813140"
  },
  {
    "text": "etc so this step up and of course we can also see the the log files if we want on",
    "start": "1813140",
    "end": "1818480"
  },
  {
    "text": "the board to see that the request has been received these are the pods that are running with the 32 gigabytes of",
    "start": "1818480",
    "end": "1824360"
  },
  {
    "text": "image and if we look at the log of the the K7 container for example we can see",
    "start": "1824360",
    "end": "1830240"
  },
  {
    "text": "the challenges that that we have into loading the model file within a matter",
    "start": "1830240",
    "end": "1836059"
  },
  {
    "text": "of a few minutes so from 31 to 33 last night and let me go back to the slides",
    "start": "1836059",
    "end": "1842960"
  },
  {
    "text": "so these are these challenges I don't know if this is visible but",
    "start": "1842960",
    "end": "1848899"
  },
  {
    "text": "I have a sample here of a three gig oops a three gigabyte file that is downloaded",
    "start": "1848899",
    "end": "1854600"
  },
  {
    "text": "from uh from hudging phase and then through the use of the faster Transformer we are splitting this file",
    "start": "1854600",
    "end": "1860299"
  },
  {
    "text": "into smaller chunks and its dense layer is saved in a in a separate file of a 16 megabyte file that will appropriately be",
    "start": "1860299",
    "end": "1867559"
  },
  {
    "text": "stored in the GPU specific chart that we have got from our model server other",
    "start": "1867559",
    "end": "1874159"
  },
  {
    "text": "challenges oops other challenges the storage initializer takes let's say one minute to load a two gigabyte file the",
    "start": "1874159",
    "end": "1881600"
  },
  {
    "text": "predictor takes 10 seconds to copy the model file from the Pod Storage to the CPU and the GPU memory and the Sardine",
    "start": "1881600",
    "end": "1889220"
  },
  {
    "text": "technique is visualized like that so done would you like to continue",
    "start": "1889220",
    "end": "1897460"
  },
  {
    "text": "uh use case here for the GP model so as some of me you know so recently",
    "start": "1903320",
    "end": "1909140"
  },
  {
    "text": "Bloomberg published the uh the Bloomberg GPT which is uh purposely built like with the 50 billion parameters large",
    "start": "1909140",
    "end": "1916039"
  },
  {
    "text": "language model uh which was built like a trained from scratch for finance domain and so uh it uh actually outperforms the",
    "start": "1916039",
    "end": "1924500"
  },
  {
    "text": "general model uh in the uh in the financial Financial tasks but without a",
    "start": "1924500",
    "end": "1929659"
  },
  {
    "text": "sacrificing sacrificing the accuracy on the general for the general task as well so the model is trained up approximately",
    "start": "1929659",
    "end": "1937460"
  },
  {
    "text": "is 53 days on 64 servers each with eight",
    "start": "1937460",
    "end": "1942740"
  },
  {
    "text": "a 100 gpus on AWS Sage maker however the model is actually served internally on",
    "start": "1942740",
    "end": "1950240"
  },
  {
    "text": "Prime because of the data privacy issues as well as the uh the cost so",
    "start": "1950240",
    "end": "1957799"
  },
  {
    "text": "um so the the model is deployed on the Enviro Triton faster server as the zero",
    "start": "1957799",
    "end": "1963919"
  },
  {
    "text": "just a demoed pretty similarly so the serving the model is on two a100 gpus",
    "start": "1963919",
    "end": "1969919"
  },
  {
    "text": "for each replica each GPU is where it's like 80 gig GPU memory so the so the",
    "start": "1969919",
    "end": "1977299"
  },
  {
    "text": "model is uh converted to faster Transformer formats from the hugging phase checkpoints which is we set the",
    "start": "1977299",
    "end": "1984200"
  },
  {
    "text": "target the Precision to BF 16 which gives us a better accuracy on the tensor",
    "start": "1984200",
    "end": "1989419"
  },
  {
    "text": "parallel is statute 2 and pipeline parallel to one which means it's a shouted the models are shouted on across",
    "start": "1989419",
    "end": "1995899"
  },
  {
    "text": "two a100 gpus um and uh so you can check out the more details about the Bloomberg GPT on the",
    "start": "1995899",
    "end": "2002140"
  },
  {
    "text": "press and the paper linked here and so this is uh uh like how the",
    "start": "2002140",
    "end": "2008799"
  },
  {
    "text": "problem looks like which is like a I think CR ratio the yamos is like from the and we have the pre-processing with",
    "start": "2008799",
    "end": "2016299"
  },
  {
    "text": "the tokenizer which it takes the text import and then converts to import tokens sent over by like grpc to the try",
    "start": "2016299",
    "end": "2023799"
  },
  {
    "text": "to faster Transformer predictor and gets back the generator tokens so another",
    "start": "2023799",
    "end": "2028840"
  },
  {
    "text": "thing we are kind of working around is to streaming the token back is because sometimes it may take a long time for a",
    "start": "2028840",
    "end": "2034179"
  },
  {
    "text": "larger number of tokens so it will make the user a little bit more interactive to get the tokens back while running",
    "start": "2034179",
    "end": "2041500"
  },
  {
    "text": "linkers and so as you can see the models are shouted along to a100 gpus and uh",
    "start": "2041500",
    "end": "2047740"
  },
  {
    "text": "and you can see some metrics we got collected from the models so we used about like a 65 of the GPU memory and",
    "start": "2047740",
    "end": "2055358"
  },
  {
    "text": "sometimes the the GPU utilization can really get a hundred percent on a100 GPU",
    "start": "2055359",
    "end": "2061378"
  },
  {
    "text": "so the latency is around like a few seconds on average",
    "start": "2061379",
    "end": "2067419"
  },
  {
    "text": "um so um yeah that's a conclude our talk and uh we really around like uh having new",
    "start": "2067419",
    "end": "2075520"
  },
  {
    "text": "contributors during our project and then pave our way to the 1.0 like a version uh roadmap so here are the links and uh",
    "start": "2075520",
    "end": "2082960"
  },
  {
    "text": "if you have any questions and then feel free to reach out to the community to serial or me",
    "start": "2082960",
    "end": "2088599"
  },
  {
    "text": "um yeah I'll open up the questions",
    "start": "2088599",
    "end": "2092820"
  },
  {
    "text": "thanks everyone [Applause]",
    "start": "2095679",
    "end": "2101510"
  },
  {
    "text": "it's booking yep um I have a question about the model",
    "start": "2112240",
    "end": "2117579"
  },
  {
    "text": "mesh currently we're using we trying to use a model mesh and then",
    "start": "2117579",
    "end": "2123760"
  },
  {
    "text": "we struggle with the scaling of the runtime because as I know currently",
    "start": "2123760",
    "end": "2131200"
  },
  {
    "text": "there is no Auto scaling it's a I believe the model mesh maintenance are",
    "start": "2131200",
    "end": "2136960"
  },
  {
    "text": "currently working on the horizon scaling for the you're talking about scaling the seven runtime right yeah yeah they are",
    "start": "2136960",
    "end": "2143320"
  },
  {
    "text": "currently working on that so check out the I believe there's a period already up we thought about geta",
    "start": "2143320",
    "end": "2149380"
  },
  {
    "text": "just to scare it based the metrics maybe it's good way maybe it's not maybe it's",
    "start": "2149380",
    "end": "2154599"
  },
  {
    "text": "your way but this is what we tell we told and another question about the",
    "start": "2154599",
    "end": "2160260"
  },
  {
    "text": "we're using Triton so we have Triton adapter that download the S3 and",
    "start": "2160260",
    "end": "2166480"
  },
  {
    "text": "currency we can't use irsa it's your side",
    "start": "2166480",
    "end": "2172540"
  },
  {
    "text": "um I know that storage initializer I think it's really really new using right now",
    "start": "2172540",
    "end": "2179079"
  },
  {
    "text": "with irsa to download from S3 when it come to model mesh and Triton",
    "start": "2179079",
    "end": "2185800"
  },
  {
    "text": "adapter there is a roadmap for that because it's",
    "start": "2185800",
    "end": "2190900"
  },
  {
    "text": "really it's make me fight with my security guys because I'm using a IMEI user currently",
    "start": "2190900",
    "end": "2198760"
  },
  {
    "text": "right so it's bad for me okay okay yeah maybe we can catch up like after okay",
    "start": "2198760",
    "end": "2203859"
  },
  {
    "text": "yeah thank you sorry can you do",
    "start": "2203859",
    "end": "2211020"
  },
  {
    "text": "um I think that I saw some commenting issues that currently is",
    "start": "2211960",
    "end": "2218500"
  },
  {
    "text": "there is no option and in the roadmap it will be it will come",
    "start": "2218500",
    "end": "2224140"
  },
  {
    "text": "maybe you know something that I don't know yeah we'll catch up later yeah we can",
    "start": "2224140",
    "end": "2230200"
  },
  {
    "text": "catch up here thank you thank you",
    "start": "2230200",
    "end": "2234420"
  },
  {
    "text": "is there any other question yes",
    "start": "2238060",
    "end": "2243060"
  },
  {
    "text": "um thank you for this presentation my question is well don't you support a",
    "start": "2243579",
    "end": "2248800"
  },
  {
    "text": "simple model like cat boost in the table you presented uh yeah as another flavor",
    "start": "2248800",
    "end": "2255640"
  },
  {
    "text": "of like a kind of boost kind of model right so yeah so as uh you can it's",
    "start": "2255640",
    "end": "2261280"
  },
  {
    "text": "pretty straightforter to implement that I think they're probably open you should have seen that before um but yeah I feel free to contribute if",
    "start": "2261280",
    "end": "2268060"
  },
  {
    "text": "you think this is like a widely used store like uh but you can always bring your like a you can Implement an ad and",
    "start": "2268060",
    "end": "2273160"
  },
  {
    "text": "uh because like uh you can add additional like a serving runtime it's just like in addition to like the pre-ship like the serving runtime so you",
    "start": "2273160",
    "end": "2279520"
  },
  {
    "text": "can always add your own seven run times they're pretty flexible and you can like bring your own like around times but you",
    "start": "2279520",
    "end": "2285099"
  },
  {
    "text": "do think it's useful for the community so feel free to contribute that",
    "start": "2285099",
    "end": "2289859"
  },
  {
    "text": "I have a second question is uh does uh the multimodal serving support",
    "start": "2290859",
    "end": "2296619"
  },
  {
    "text": "confidential containers have you can do confidential containers with multi-model serving",
    "start": "2296619",
    "end": "2305200"
  },
  {
    "text": "uh sorry the I asking the model mesh for the",
    "start": "2305200",
    "end": "2310619"
  },
  {
    "text": "can you uh does does it support confidential containers",
    "start": "2310619",
    "end": "2317800"
  },
  {
    "text": "no sorry what the question is sorry I didn't get that in gloves containers in",
    "start": "2317800",
    "end": "2324160"
  },
  {
    "text": "class type of container what is that",
    "start": "2324160",
    "end": "2331180"
  },
  {
    "text": "I'm looking like a photographer sorry I didn't get a question yeah",
    "start": "2331180",
    "end": "2336940"
  },
  {
    "text": "by the way there is a ml server from seldom supports cut boost and you can load it in case of there is a PR there",
    "start": "2336940",
    "end": "2343060"
  },
  {
    "text": "that you can find the extra I feel like a Ms server already supports that then we should be able to run it too",
    "start": "2343060",
    "end": "2350579"
  },
  {
    "text": "is there any other question all right thank you for coming here today thanks everyone",
    "start": "2351339",
    "end": "2357750"
  },
  {
    "text": "[Applause]",
    "start": "2357750",
    "end": "2361219"
  }
]