[
  {
    "start": "0",
    "end": "28000"
  },
  {
    "text": "hello and welcome to auto scaling and cost optimization on kubernetes um from zero to 100 i'm guy templeton",
    "start": "80",
    "end": "7279"
  },
  {
    "text": "i'm a senior software engineer at skyscanner working on skyscanner's shared container platforms",
    "start": "7279",
    "end": "13759"
  },
  {
    "text": "hey this is joshi i'm i'm working on aws uh ecas team and i'm mainly",
    "start": "13759",
    "end": "21199"
  },
  {
    "text": "focused on the other scouting projects and machine learning areas",
    "start": "21199",
    "end": "26800"
  },
  {
    "text": "so you've deployed kits you've deployed your clusters into your cloud or data center of choice you've packaged up",
    "start": "29519",
    "end": "35920"
  },
  {
    "text": "your um either shiny new apps or old legacy apps into containers deployed",
    "start": "35920",
    "end": "41200"
  },
  {
    "text": "them onto your new case clusters and that's when the person responsible for the compute building your organization appears",
    "start": "41200",
    "end": "47280"
  },
  {
    "text": "asking you questions awkward questions i thought this cloud native thing was supposed to save us money so why are we",
    "start": "47280",
    "end": "52960"
  },
  {
    "text": "spending more than we did previously that's when you realize that one of the key pieces that kubernetes enables is the",
    "start": "52960",
    "end": "60239"
  },
  {
    "text": "ability to elastically scale your workloads and your clusters as you need to",
    "start": "60239",
    "end": "66720"
  },
  {
    "start": "67000",
    "end": "110000"
  },
  {
    "text": "so ejection will take you over what we're planning to cover",
    "start": "68320",
    "end": "73840"
  },
  {
    "text": "yeah in today's session what we're planning to cover roughly",
    "start": "79520",
    "end": "85840"
  },
  {
    "text": "enforced into four different use cases horizontal part other scaling",
    "start": "85840",
    "end": "91439"
  },
  {
    "text": "vertical auto scouting plaster auto scaling and a few other scouting related tips",
    "start": "91439",
    "end": "99600"
  },
  {
    "text": "and tricks in each of these areas i'll take you through the basic concept",
    "start": "99600",
    "end": "105040"
  },
  {
    "text": "first and guy will share more tips on those real-world use cases",
    "start": "105040",
    "end": "111280"
  },
  {
    "start": "110000",
    "end": "169000"
  },
  {
    "text": "so let's look at the overview how kubernetes auto scholar interact",
    "start": "111759",
    "end": "116960"
  },
  {
    "text": "together let's look at the left part first the hpa and vpa they both check the metrics to",
    "start": "116960",
    "end": "124159"
  },
  {
    "text": "help make decisions the different part is hpa update the pub graphic cards",
    "start": "124159",
    "end": "129840"
  },
  {
    "text": "and vpa update the resources allocated to the existing part hpa and vpa are currently",
    "start": "129840",
    "end": "137040"
  },
  {
    "text": "incompatible and the best practice is to avoid using both together",
    "start": "137040",
    "end": "142319"
  },
  {
    "text": "for the same set of the paws on the right side if there is not enough capacity to run",
    "start": "142319",
    "end": "148000"
  },
  {
    "text": "pause the ca pick up those past impending state the ca will do some calculation to find",
    "start": "148000",
    "end": "154319"
  },
  {
    "text": "the right node groups and desired number of the nodes and then request the resources",
    "start": "154319",
    "end": "159360"
  },
  {
    "text": "from cloud providers the part can be scheduled later on provision node the ca can work with",
    "start": "159360",
    "end": "166560"
  },
  {
    "text": "hpa and vpa pretty well now let's come into the horizontal about",
    "start": "166560",
    "end": "172319"
  },
  {
    "start": "169000",
    "end": "240000"
  },
  {
    "text": "auto scaling",
    "start": "172319",
    "end": "175120"
  },
  {
    "text": "hpa codes live in coop controller manager which is a little bit different from c",
    "start": "178239",
    "end": "184720"
  },
  {
    "text": "and vpa so they are located in kubernetes auto scholar project the hpa automatically scale the number",
    "start": "184720",
    "end": "191920"
  },
  {
    "text": "of the replicas in deployment based on the target metrics and there's uh",
    "start": "191920",
    "end": "197200"
  },
  {
    "text": "three different metrics the building metrics uh custom matrix and external",
    "start": "197200",
    "end": "203840"
  },
  {
    "text": "metrics we'll cover this later each of this uh metrics can be served by one of a number",
    "start": "203840",
    "end": "210879"
  },
  {
    "text": "of components however a single matrix like the custom metrics cannot currently be served by multiple",
    "start": "210879",
    "end": "217519"
  },
  {
    "text": "items only one component can register itself to the api server as serving a given api so in kubernetes",
    "start": "217519",
    "end": "225599"
  },
  {
    "text": "uh 118 so there are more flexibilities to customize hpa a number of settings",
    "start": "225599",
    "end": "232080"
  },
  {
    "text": "which were previously cluster-wide can now between on per hpa basis",
    "start": "232080",
    "end": "240879"
  },
  {
    "start": "240000",
    "end": "720000"
  },
  {
    "text": "guy will talk about the matrix types here",
    "start": "240879",
    "end": "245519"
  },
  {
    "text": "yeah so the first metric type is resource metrics these are the simplest of three metrics",
    "start": "246640",
    "end": "253280"
  },
  {
    "text": "cpu and memory based auto scale so this is the the sort of traditional uh auto scaling you may be used to with",
    "start": "253280",
    "end": "259919"
  },
  {
    "text": "your cloud provider of choice scaling and auto scaling group or equivalent based on the cpu",
    "start": "259919",
    "end": "265440"
  },
  {
    "text": "target cpu utilization or memory utilization and it's provided by an api metrics that",
    "start": "265440",
    "end": "271360"
  },
  {
    "text": "case files so this is the same uh api path that's providing metrics that's when running cube call top",
    "start": "271360",
    "end": "278160"
  },
  {
    "text": "and this was this api was originally served by a component called heapster um however this was deprecated",
    "start": "278160",
    "end": "284400"
  },
  {
    "text": "in case 111 if you're running is still running this these days you really shouldn't be and should be moving on to the more",
    "start": "284400",
    "end": "291120"
  },
  {
    "text": "modern ways of serving this app and it's now usually provided by the metric server um",
    "start": "291120",
    "end": "296960"
  },
  {
    "text": "this scrapes their resource metrics from cubelets apis and serves them up by the cube api server via a",
    "start": "296960",
    "end": "304880"
  },
  {
    "text": "via of api aggregation uh one thing to be aware of though is",
    "start": "304880",
    "end": "310880"
  },
  {
    "text": "it's currently based on the usage of the entire pods so this can be an issue if you're injecting site cars",
    "start": "310880",
    "end": "316479"
  },
  {
    "text": "for example you're running an sd mesh um and the code you care about scaling on that",
    "start": "316479",
    "end": "322160"
  },
  {
    "text": "serving traffic to your users is uh using a high amount of utilization but if other other containers inside the pod",
    "start": "322160",
    "end": "329199"
  },
  {
    "text": "are using a low enough it can drag the average utilization down and therefore you may not trigger auto scaling when",
    "start": "329199",
    "end": "335120"
  },
  {
    "text": "you think you should be uh the next uh metro type is custom",
    "start": "335120",
    "end": "341440"
  },
  {
    "text": "matrix so this is served on the api custom.entrans.com and there's no currently no official",
    "start": "341440",
    "end": "348400"
  },
  {
    "text": "implementation though the most widely adopted is the prometheus adapter there's a number of other implementations from",
    "start": "348400",
    "end": "354639"
  },
  {
    "text": "companies like zolando uh google and others um however currently the",
    "start": "354639",
    "end": "361280"
  },
  {
    "text": "prometheus adapter is in the process of believing being adopted by sick instrumentation um so by the time",
    "start": "361280",
    "end": "367759"
  },
  {
    "text": "this talk gets out that may well have changed and these metrics still have to correspond",
    "start": "367759",
    "end": "373520"
  },
  {
    "text": "to kubernetes objects and so you can have two types here pod metrics things like requests and flight per pod",
    "start": "373520",
    "end": "380880"
  },
  {
    "text": "or object metrics so these are um slightly more complicated they still describe the kubernetes",
    "start": "380880",
    "end": "386880"
  },
  {
    "text": "object it has to be in the same name space as the pods being scaled um so things like requests",
    "start": "386880",
    "end": "393360"
  },
  {
    "text": "per ingress and in the same name space as the pods serving those requests",
    "start": "393360",
    "end": "399120"
  },
  {
    "text": "um so this is this is useful say where you have a service where you know for instance you know how many requests",
    "start": "399120",
    "end": "405600"
  },
  {
    "text": "a given pod can serve at any one time um but the cpu or memory may not be a good",
    "start": "405600",
    "end": "411120"
  },
  {
    "text": "indicator of that given that uh it takes an indeterminate amount of",
    "start": "411120",
    "end": "416720"
  },
  {
    "text": "cpu per request so some requests may be extremely low in cpu but are still blocking an entire request therefore you",
    "start": "416720",
    "end": "423039"
  },
  {
    "text": "want to actually scale on your number of you whiskey processes that are currently",
    "start": "423039",
    "end": "428319"
  },
  {
    "text": "running rather than cpu so in that case either scaling on cpu or",
    "start": "428319",
    "end": "434080"
  },
  {
    "text": "memory is going to waste your money or result in decreased performance because you put it too low so the better option is",
    "start": "434080",
    "end": "441120"
  },
  {
    "text": "to go for your known bottleneck",
    "start": "441120",
    "end": "445039"
  },
  {
    "text": "and finally external metrics so this is served on the sternum dot metrics.case.io api path again",
    "start": "446240",
    "end": "453840"
  },
  {
    "text": "you have components registering um to serve this metrics path under api aggregation as",
    "start": "453840",
    "end": "460160"
  },
  {
    "text": "jackson said this this can only have one component serving this metrics path at any one time",
    "start": "460160",
    "end": "466160"
  },
  {
    "text": "and there's again a number of implementations there's your gcp aws all provide ones for their metric system so",
    "start": "466160",
    "end": "472639"
  },
  {
    "text": "you can scale your case services based on metrics from your cloud provider um",
    "start": "472639",
    "end": "478240"
  },
  {
    "text": "some of the already mentioned custom metrics implementations also uh are capable of serving up external",
    "start": "478240",
    "end": "484240"
  },
  {
    "text": "documents like outside um so it's intended for metrics entirely external to kubernetes objects",
    "start": "484240",
    "end": "490560"
  },
  {
    "text": "so things like kafka q laying for zero surface plus q length aws a lb active request so things that are",
    "start": "490560",
    "end": "496319"
  },
  {
    "text": "entirely decoupled from your kubernetes cluster um and they support two different target types",
    "start": "496319",
    "end": "502560"
  },
  {
    "text": "value and average value average value and the hpa takes the um",
    "start": "502560",
    "end": "509039"
  },
  {
    "text": "value that is being evaluated and divided by the number of pods in the target that it is scaling and",
    "start": "509039",
    "end": "514959"
  },
  {
    "text": "before comparing it to the target value value it just compares directly to the target without dividing it over the",
    "start": "514959",
    "end": "521120"
  },
  {
    "text": "number of pods um so let's go into a bit more detail on",
    "start": "521120",
    "end": "528800"
  },
  {
    "text": "hp's algorithm and uh course of its behavior so a lot of users might want to scale",
    "start": "528800",
    "end": "535120"
  },
  {
    "text": "multiple metrics and so if you've got something where you can either become cpu saturated or",
    "start": "535120",
    "end": "542640"
  },
  {
    "text": "exhaust the number of connections and you may want to set up those multiple metrics supposed to get this um",
    "start": "542640",
    "end": "548160"
  },
  {
    "text": "acid case 115 the hpa handles this well it will always make the safest choice so even in the case where",
    "start": "548160",
    "end": "553440"
  },
  {
    "text": "one or more of your metrics is unavailable it will still um if one or more of the metrics says",
    "start": "553440",
    "end": "559600"
  },
  {
    "text": "that it should scale up it will take the highest choice out of all of those metrics and and",
    "start": "559600",
    "end": "564800"
  },
  {
    "text": "increase the size uh conversely if that metric if one or more metrics is unavailable",
    "start": "564800",
    "end": "570000"
  },
  {
    "text": "it won't scale down because obviously it can't can't figure out whether that metric",
    "start": "570000",
    "end": "575360"
  },
  {
    "text": "would say that it is safe to scale down and you can also scale down to zero if",
    "start": "575360",
    "end": "581279"
  },
  {
    "text": "you really want to optimize your cost saving um there's a couple things required for this though you have to",
    "start": "581279",
    "end": "586800"
  },
  {
    "text": "um enable alpha feature date called hp skill zero and you also have to set the result",
    "start": "586800",
    "end": "592399"
  },
  {
    "text": "associated hp up with at least one object or external metric because obviously",
    "start": "592399",
    "end": "597440"
  },
  {
    "text": "in the case of pods metrics it can't make a reasoning when the pods are scaled to zero about what",
    "start": "597440",
    "end": "603279"
  },
  {
    "text": "the behavior should use um and if you want to fine-tune the behavior of a given hpa",
    "start": "603279",
    "end": "609839"
  },
  {
    "text": "so if you have certain services which are uh you want to scale up really fast um but",
    "start": "609839",
    "end": "616320"
  },
  {
    "text": "scaled out slowly and you can now uh tune that out of one eighth uh",
    "start": "616320",
    "end": "621680"
  },
  {
    "text": "kubernetes 118 onwards with uh this new part of the um horizontal",
    "start": "621680",
    "end": "627040"
  },
  {
    "text": "autoscaler spec um where you can see the behavior snippet here so in this case",
    "start": "627040",
    "end": "632160"
  },
  {
    "text": "it's saying at most scale it down by five percent every five minutes and scale it down to a minimum of zero",
    "start": "632160",
    "end": "640959"
  },
  {
    "text": "and however if that's not much use to you um if you're running kubernetes 117 or",
    "start": "644079",
    "end": "649839"
  },
  {
    "text": "earlier um and all of those flags at that point were still cluster-wide so if you had",
    "start": "649839",
    "end": "655360"
  },
  {
    "text": "two different apps with two entirely different uh scaling behaviors you would you would have to make a",
    "start": "655360",
    "end": "660720"
  },
  {
    "text": "compromise um across the entire cluster you can still achieve that same sort of behavior and",
    "start": "660720",
    "end": "666720"
  },
  {
    "text": "here you can see that we are in skyscanner we've got this service that's being a deployment that's",
    "start": "666720",
    "end": "673440"
  },
  {
    "text": "being scaled uh with a number of different metrics so we've got a jetty util thread good thread pool um",
    "start": "673440",
    "end": "680880"
  },
  {
    "text": "resource that we're a pod uh metric that we're scaling on we've also got a cpu utilization metric that we're scaling on",
    "start": "680880",
    "end": "687279"
  },
  {
    "text": "and finally we've got this object metric called downscale limit and on the right hand side you can see",
    "start": "687279",
    "end": "692880"
  },
  {
    "text": "the prometheus adapter config for what that metric actually maps to inside",
    "start": "692880",
    "end": "698399"
  },
  {
    "text": "prometheus adapter and that basically achieves on for us the ability to only downscale at most um",
    "start": "698399",
    "end": "706320"
  },
  {
    "text": "10 at a time um without needing the kubernetes 118 um",
    "start": "706320",
    "end": "714240"
  },
  {
    "text": "tunable behavior now jackson's going to take you over they",
    "start": "714240",
    "end": "719760"
  },
  {
    "text": "introduce you to the basics of vertical autoscaling cool",
    "start": "719760",
    "end": "726320"
  },
  {
    "text": "um yeah for vpa uh many of us may have questions when we define new",
    "start": "727040",
    "end": "732240"
  },
  {
    "text": "applications like uh how many resources should we give to the deployment uh",
    "start": "732240",
    "end": "737279"
  },
  {
    "text": "initially so most of the time we want to give a higher limit to handle the peak load",
    "start": "737279",
    "end": "742480"
  },
  {
    "text": "otherwise there's a risk our applications are not able to serve the big traffic so even if we like",
    "start": "742480",
    "end": "749440"
  },
  {
    "text": "to get the highest limit as we can we need to spend some time on the benchmark to get those",
    "start": "749440",
    "end": "754959"
  },
  {
    "text": "values uh besides that another challenge is application is changing over time the older settings",
    "start": "754959",
    "end": "761680"
  },
  {
    "text": "is no longer efficient as the like the daily traffic padding change or your users exploring",
    "start": "761680",
    "end": "768000"
  },
  {
    "text": "et cetera the vertical uh auto scaling uh aims to solve these problems",
    "start": "768000",
    "end": "774480"
  },
  {
    "text": "automatically adjust the cpu and memory resolutions for your parts",
    "start": "774480",
    "end": "779760"
  },
  {
    "text": "to help right size your applications up and down to match the demand and reduce the waste",
    "start": "779760",
    "end": "787839"
  },
  {
    "start": "786000",
    "end": "1043000"
  },
  {
    "text": "there are three components in vpa recommender like updater and emission plugin",
    "start": "788639",
    "end": "797440"
  },
  {
    "text": "the recommended look at the metrics history om events and the suggest the fitting",
    "start": "798160",
    "end": "803680"
  },
  {
    "text": "values for request and install it in vpn object the updater",
    "start": "803680",
    "end": "809200"
  },
  {
    "text": "will decide which parts should be restarted based on the recommendations importantly the updater respects the pod",
    "start": "809200",
    "end": "816639"
  },
  {
    "text": "disruption budget so you can apply pdb sphere to limit the number of concurrent disruptions the mutation",
    "start": "816639",
    "end": "824560"
  },
  {
    "text": "admission workbook is used to apply the recommendations on part submissions",
    "start": "824560",
    "end": "830639"
  },
  {
    "text": "and there's also four kinds of modes that you can use in vpa so the difference is",
    "start": "830639",
    "end": "837279"
  },
  {
    "text": "between them is that when you want to adjust the recommended popular sources if you are not confident enough we",
    "start": "837279",
    "end": "844399"
  },
  {
    "text": "suggest you to start from off which only recommend the pod size that doesn't actually change anything",
    "start": "844399",
    "end": "850560"
  },
  {
    "text": "the initial results reside the part only when pods are created",
    "start": "850560",
    "end": "855680"
  },
  {
    "text": "well the recreated and auto resides the pause by restarting the",
    "start": "855680",
    "end": "860720"
  },
  {
    "text": "existing ones a guy will talk about the vpa tips",
    "start": "860720",
    "end": "870480"
  },
  {
    "text": "yeah uh so vertical pod auto scaling um is really useful for singletons so",
    "start": "870480",
    "end": "876639"
  },
  {
    "text": "things like the big prometheus instance you're using to monitor your cluster um and also services used by internal",
    "start": "876639",
    "end": "882880"
  },
  {
    "text": "teams if these aren't uh distributed around will you'll likely have traffic patterns which um scale up and",
    "start": "882880",
    "end": "890240"
  },
  {
    "text": "down in a predictable way and and it's no use giving them peak resource usage and burning money during",
    "start": "890240",
    "end": "895920"
  },
  {
    "text": "those quiet periods where no one's in the office so in this case you can see uh graft over a week",
    "start": "895920",
    "end": "901440"
  },
  {
    "text": "the cpu utilization of a prometheus instance inside skyscanner and you can see there's very obvious uh",
    "start": "901440",
    "end": "907839"
  },
  {
    "text": "traffic peaks when people when engineers are in the office and then it drops down significantly to roughly a third",
    "start": "907839",
    "end": "915519"
  },
  {
    "text": "at the lowest points of its peak usage these sorts of things are the prime candidates for uh vertical pod auto scaling where you",
    "start": "915519",
    "end": "922399"
  },
  {
    "text": "can easily achieve a huge uh improvements in your resource usage",
    "start": "922399",
    "end": "928720"
  },
  {
    "text": "um so there are a few limitations with uh vertical auto scaling um so",
    "start": "928720",
    "end": "934800"
  },
  {
    "text": "as jackson already mentioned uh you shouldn't really use it in",
    "start": "934800",
    "end": "939920"
  },
  {
    "text": "conjunction with resource-based hpas and currently as the two will conflict as obviously the vpa will be",
    "start": "939920",
    "end": "946000"
  },
  {
    "text": "trying to change the resource uh limits for requests at the same time as the",
    "start": "946000",
    "end": "952240"
  },
  {
    "text": "hpa is trying to scale on based on those things um you can however use them where you're",
    "start": "952240",
    "end": "958240"
  },
  {
    "text": "your hpa is set up using a customer external metrics and your vpa is then",
    "start": "958240",
    "end": "963519"
  },
  {
    "text": "modifying the resource requests and currently modifying the resource requests requires",
    "start": "963519",
    "end": "970079"
  },
  {
    "text": "recreating the entire pod meaning a pod restart so that will result in disruption that's why us you",
    "start": "970079",
    "end": "976160"
  },
  {
    "text": "should make use of pdbs as jackson mentioned um there is work on going to um",
    "start": "976160",
    "end": "983920"
  },
  {
    "text": "try and mitigate this and that at that point uh the auto um mode will become",
    "start": "983920",
    "end": "990560"
  },
  {
    "text": "much more useful where it should be able to modify the request resource requests and limits and",
    "start": "990560",
    "end": "996320"
  },
  {
    "text": "as long as the node that's currently running on has those extra resources then the pod should um",
    "start": "996320",
    "end": "1001759"
  },
  {
    "text": "not require a restart to be modified um and it can also be uh tricky to be to",
    "start": "1001759",
    "end": "1008000"
  },
  {
    "text": "use with jvm based workloads on the member's height and this is unsurprising but here we've got a graph",
    "start": "1008000",
    "end": "1013279"
  },
  {
    "text": "of a java based service inside skyscanner and as you can see um from the viewpoint of kubernetes most",
    "start": "1013279",
    "end": "1020959"
  },
  {
    "text": "of the time these pods are running along 100 memory utilization and in this case the vpa would likely",
    "start": "1020959",
    "end": "1027600"
  },
  {
    "text": "continue and giving it more giving these pods more and more memory um until we hit the maximum size of the",
    "start": "1027600",
    "end": "1034240"
  },
  {
    "text": "nodes we're running on um so just something to be aware of uh jackson will now take you through the uh",
    "start": "1034240",
    "end": "1041438"
  },
  {
    "text": "basics of cluster autoscale",
    "start": "1041439",
    "end": "1045120"
  },
  {
    "start": "1043000",
    "end": "1137000"
  },
  {
    "text": "class autoscaler scale your cluster based on the pending cost you periodically check whether there's a",
    "start": "1048400",
    "end": "1055039"
  },
  {
    "text": "pending cost and increase the size of the cluster if more resources are needed",
    "start": "1055039",
    "end": "1060559"
  },
  {
    "text": "if nodes are underutilized and all parts could be scheduled even on fewer nodes",
    "start": "1060559",
    "end": "1067440"
  },
  {
    "text": "in the cluster so plus the autoscaler will try to remove android utilize node",
    "start": "1067440",
    "end": "1073840"
  },
  {
    "text": "down to the minimum size of the cluster and the guide will talk about the utilization",
    "start": "1073840",
    "end": "1079760"
  },
  {
    "text": "graph here",
    "start": "1079760",
    "end": "1082480"
  },
  {
    "text": "yep so we have a graph here which shows that so we can see here a",
    "start": "1084880",
    "end": "1092000"
  },
  {
    "text": "bold red line and the amber sort of filled in section is the um minimum utilization that we",
    "start": "1092000",
    "end": "1099360"
  },
  {
    "text": "have on a set on our cluster of 90 and and you can see as soon as the blur red line",
    "start": "1099360",
    "end": "1105120"
  },
  {
    "text": "and sits underneath it for a while it becomes considered as a candidate for scale down by the cluster auto scaler at which point it",
    "start": "1105120",
    "end": "1111760"
  },
  {
    "text": "cordons out drains it and starts uh no pods start moving off it so you can see the utilization coming down",
    "start": "1111760",
    "end": "1118320"
  },
  {
    "text": "um in steps as these pods are rescheduled onto other nodes in the cluster eventually the uh nodes utilization",
    "start": "1118320",
    "end": "1125840"
  },
  {
    "text": "drops are low that it's only demonstrate pods left on it and at which point the cholesterol to scalar",
    "start": "1125840",
    "end": "1131520"
  },
  {
    "text": "terminates the node and we stop paying for it",
    "start": "1131520",
    "end": "1137840"
  },
  {
    "start": "1137000",
    "end": "1175000"
  },
  {
    "text": "cool here's a detail uh scaled down process you uh in each scan loop c will mark",
    "start": "1138640",
    "end": "1145840"
  },
  {
    "text": "under utilized nodes and and native nodes at the same time ca uses kubernetes",
    "start": "1145840",
    "end": "1151039"
  },
  {
    "text": "scheduler code to simulate if the past on a needed node can be moved to other nodes so after",
    "start": "1151039",
    "end": "1158080"
  },
  {
    "text": "like a well empty node without the path will be recycled first in this case",
    "start": "1158080",
    "end": "1164080"
  },
  {
    "text": "if those ender utilize the nodes as filled and needed after 10 minutes ca",
    "start": "1164080",
    "end": "1169919"
  },
  {
    "text": "will drain the parts on them and remove the remove the node",
    "start": "1169919",
    "end": "1175520"
  },
  {
    "start": "1175000",
    "end": "1399000"
  },
  {
    "text": "to uh let's talk about the expanders in your clusters you may have multiple",
    "start": "1176799",
    "end": "1182080"
  },
  {
    "text": "nodes groups and when ca try to bring uh bring more capacity to the cluster you need to",
    "start": "1182080",
    "end": "1189039"
  },
  {
    "text": "decide which node group to scale up you see it brings a concept",
    "start": "1189039",
    "end": "1194160"
  },
  {
    "text": "here called expanders so there are four different uh extenders here like the random expander priority",
    "start": "1194160",
    "end": "1201200"
  },
  {
    "text": "expander price expander and the list always expander it's very important to understand these",
    "start": "1201200",
    "end": "1207440"
  },
  {
    "text": "expanders to optimize your cost and performance you can choose the right one based on your needs",
    "start": "1207440",
    "end": "1213520"
  },
  {
    "text": "or customize your own expanders so these expanders are very easy to understand",
    "start": "1213520",
    "end": "1219039"
  },
  {
    "text": "from their names we will talk about a few cases using priority expanded data",
    "start": "1219039",
    "end": "1226400"
  },
  {
    "text": "i will talk about the things to consider in ca",
    "start": "1226640",
    "end": "1231840"
  },
  {
    "text": "uh so there's a number of things to consider when enabling cluster auto scaling um so which of your pods and workloads",
    "start": "1233360",
    "end": "1239760"
  },
  {
    "text": "can tolerate interruptions um whether your pods being scaled down need to do any cleanup so for instance if you have in-flight",
    "start": "1239760",
    "end": "1246480"
  },
  {
    "text": "requests and and if you can't tolerate those requests effectively being uh cancelled while",
    "start": "1246480",
    "end": "1251840"
  },
  {
    "text": "they're in flight or it's going to cause a degradation to your customers um then you can use container lifecycle",
    "start": "1251840",
    "end": "1258080"
  },
  {
    "text": "pre-stock hooks to basically ensure that your containers finish their own flight requests but",
    "start": "1258080",
    "end": "1263919"
  },
  {
    "text": "stop accepting any new requests before they're terminated whether pods using disk space that can",
    "start": "1263919",
    "end": "1271200"
  },
  {
    "text": "resume safely whether they should be made stateful sets to speed up start times so that they get their disk",
    "start": "1271200",
    "end": "1277919"
  },
  {
    "text": "usage back um and pod priorities which pods are most important which you can",
    "start": "1277919",
    "end": "1283039"
  },
  {
    "text": "tolerate not running for periods of time and so things that aren't customer impacting you may be okay with",
    "start": "1283039",
    "end": "1290960"
  },
  {
    "text": "not having scheduled at a given time or having lower priority when customer impacting workloads need to",
    "start": "1291120",
    "end": "1297760"
  },
  {
    "text": "scale up so what happens if you have batch jobs",
    "start": "1297760",
    "end": "1304880"
  },
  {
    "text": "or jobs which you don't need to run immediately so you can use uh combining the use of pod priorities with the",
    "start": "1304880",
    "end": "1310880"
  },
  {
    "text": "uh cf flag expendable pods priority cut off so you can use this to avoid the ca",
    "start": "1310880",
    "end": "1317200"
  },
  {
    "text": "scaling up and purely for ultra low priority jobs so if you have jobs which are um not important enough that you",
    "start": "1317200",
    "end": "1324720"
  },
  {
    "text": "want to spend money on nodes to scale up the cluster but just want to bin pack around",
    "start": "1324720",
    "end": "1330720"
  },
  {
    "text": "and the rest of the workload find their spare capacity in the cluster this is ideal for that um",
    "start": "1330720",
    "end": "1337280"
  },
  {
    "text": "and if you're using the uh priority expander as jackson mentioned um how do you fall back to on-demand",
    "start": "1337280",
    "end": "1343919"
  },
  {
    "text": "instances when spots or preemptable instances or your cloud providers equivalent",
    "start": "1343919",
    "end": "1349440"
  },
  {
    "text": "are out of capacity and so you can create on-demand node groups with lower expansion priorities and spot instance node groups",
    "start": "1349440",
    "end": "1356000"
  },
  {
    "text": "with higher priorities this will cause the cholesterol to scaler to preferentially try to scale up the um",
    "start": "1356000",
    "end": "1363440"
  },
  {
    "text": "spot or preemptable node groups and then fall back to the um on demand instances if it",
    "start": "1363440",
    "end": "1369440"
  },
  {
    "text": "can't get them so here you can see um an example of the config map showing that",
    "start": "1369440",
    "end": "1374960"
  },
  {
    "text": "so in this case if there's no it will first attempt to uh increase the low cost um",
    "start": "1374960",
    "end": "1381600"
  },
  {
    "text": "option but then if there's no spot instances available within the max node provision time which is a tunable flag",
    "start": "1381600",
    "end": "1387360"
  },
  {
    "text": "on the cluster auto scale it'd be on it'll fall back to using the on-demand group and",
    "start": "1387360",
    "end": "1393520"
  },
  {
    "text": "jackson will now take you through some more bits of cost optimization with the ca",
    "start": "1393520",
    "end": "1400320"
  },
  {
    "start": "1399000",
    "end": "1498000"
  },
  {
    "text": "so there are some common problems uh like these uh uh like you you have multiple spot note",
    "start": "1401440",
    "end": "1407440"
  },
  {
    "text": "groups and some of them are uh like shout out capacity so ca will try to wait for the no calming",
    "start": "1407440",
    "end": "1414400"
  },
  {
    "text": "it takes up to the next uh no probation time to move to the next note group",
    "start": "1414400",
    "end": "1419919"
  },
  {
    "text": "uh if the curing notebook is not available however if your next note group is the spot group again",
    "start": "1419919",
    "end": "1426320"
  },
  {
    "text": "and it's very likely the capacity is limited as well so there's two things you can do here",
    "start": "1426320",
    "end": "1433039"
  },
  {
    "text": "the first thing is you can turn the max node provision time reduce the time",
    "start": "1433039",
    "end": "1439440"
  },
  {
    "text": "for ca to move to the next node group so there's there's another",
    "start": "1439440",
    "end": "1445200"
  },
  {
    "text": "choice use the mixed instance policy this is the aws asd feature and maximum policy allows",
    "start": "1445200",
    "end": "1453200"
  },
  {
    "text": "you to uh have a different spot instance in one asg the good thing is that even some of the",
    "start": "1453200",
    "end": "1460080"
  },
  {
    "text": "instances are short of capacity it's very likely you get other instance types uh",
    "start": "1460080",
    "end": "1465520"
  },
  {
    "text": "instead and the using uh instance the mixed things policy has another",
    "start": "1465520",
    "end": "1471120"
  },
  {
    "text": "advantage so you can only have like one or two uh like uh asgs",
    "start": "1471120",
    "end": "1476559"
  },
  {
    "text": "and help reduce the scan loop uh interval uh latencies uh remember it will be better",
    "start": "1476559",
    "end": "1483520"
  },
  {
    "text": "to use the similar instance with the same amount of memory and cpus in one mixed",
    "start": "1483520",
    "end": "1490000"
  },
  {
    "text": "instance policy because it helps ca on the simulations to calculate the desired number of the node accurately",
    "start": "1490000",
    "end": "1500799"
  },
  {
    "start": "1498000",
    "end": "1790000"
  },
  {
    "text": "uh next we'll talk about the uh gpu optimizations um",
    "start": "1500799",
    "end": "1506000"
  },
  {
    "text": "so gpu node is a little bit different from cpu nodes because the gpu resources need to be advertised",
    "start": "1506000",
    "end": "1511520"
  },
  {
    "text": "by device plugins uh which takes longer time the common problem using gpu node is c",
    "start": "1511520",
    "end": "1518320"
  },
  {
    "text": "bring up the node gpu is not ready then casing pog cannot be scheduled on the newcoming node",
    "start": "1518320",
    "end": "1525200"
  },
  {
    "text": "since a lack of the gpu resources so if you apply the label the ca will wait for gpu to become ready",
    "start": "1525200",
    "end": "1532799"
  },
  {
    "text": "and these problems can be resolved so there is no analysis with scale ups",
    "start": "1532799",
    "end": "1538159"
  },
  {
    "text": "another case in scale down is when you apply the label gpu metrics will be honored in scale",
    "start": "1538159",
    "end": "1544080"
  },
  {
    "text": "down one case is if gpu is underutilized even if the cpu and memory utilization",
    "start": "1544080",
    "end": "1550559"
  },
  {
    "text": "is high this node still becomes a skilled scaled down candidate",
    "start": "1550559",
    "end": "1555679"
  },
  {
    "text": "and a lot of users need problems scaling up from zero ca actually doesn't have any uh like",
    "start": "1555679",
    "end": "1563120"
  },
  {
    "text": "template in this case it needs to build its template from the cloud provider for simulations",
    "start": "1563120",
    "end": "1568799"
  },
  {
    "text": "however cloud provider has limited information in order for ca to build an accurate",
    "start": "1568799",
    "end": "1574480"
  },
  {
    "text": "template user need to tag the resources in auto scaling group so the common cases here is a custom",
    "start": "1574480",
    "end": "1582840"
  },
  {
    "text": "metrics and custom metrics",
    "start": "1582840",
    "end": "1588159"
  },
  {
    "text": "and labels for node affinities stands for toleration so this guarantees ca makes the right",
    "start": "1588159",
    "end": "1594720"
  },
  {
    "text": "schedule scheduling decision even without existing node and guy will continue on the gauchos",
    "start": "1594720",
    "end": "1601840"
  },
  {
    "text": "with the ca next yep so there's a",
    "start": "1601840",
    "end": "1609360"
  },
  {
    "text": "few more uh gotchas to be aware of with the ca so how do you protect critical workloads um",
    "start": "1609360",
    "end": "1615039"
  },
  {
    "text": "so if for instance you've got machine learning workloads or something similar where you can't tolerate it being",
    "start": "1615039",
    "end": "1620080"
  },
  {
    "text": "interrupted in the midst of a run um without losing all of the work it's done up to that point you can apply an annotation",
    "start": "1620080",
    "end": "1627279"
  },
  {
    "text": "uh safety effect equals false that prevents the ca terminating the node with your critical job even if the node",
    "start": "1627279",
    "end": "1632720"
  },
  {
    "text": "utilization is lower than default threshold um and there's a two sort of related",
    "start": "1632720",
    "end": "1638480"
  },
  {
    "text": "problems so it can take uh the ca while to notice upon being marked on schedule all",
    "start": "1638480",
    "end": "1645039"
  },
  {
    "text": "to the time it requests the cloud provider scale up as well as the time the cloud provider takes uh to bring up that new",
    "start": "1645039",
    "end": "1651039"
  },
  {
    "text": "node um and potentially over scaling the cluster to avoid that time",
    "start": "1651039",
    "end": "1656559"
  },
  {
    "text": "so you can use the uh over provisioning feature so you put dummy pods with low priority on your cluster to reserve",
    "start": "1656559",
    "end": "1662640"
  },
  {
    "text": "space um and then allow the cake scheduler to remove them to make space for unscheduled pods with higher priority",
    "start": "1662640",
    "end": "1668880"
  },
  {
    "text": "when they are scaled up that means you don't have to wait for new nodes to be provisioned",
    "start": "1668880",
    "end": "1675200"
  },
  {
    "text": "and you can even use a low priority workloads as already mentioned if you have a suitable workflow for that",
    "start": "1675200",
    "end": "1681919"
  },
  {
    "text": "rather than dummy pods and slightly wasting money um so what if all your services start",
    "start": "1681919",
    "end": "1689200"
  },
  {
    "text": "scaling and don't stop scaling though and so resource quotas are invaluable here figure out the maximum resources are",
    "start": "1689200",
    "end": "1694960"
  },
  {
    "text": "given name space you'd use at peak load even with failover and set resource quotas to limit this",
    "start": "1694960",
    "end": "1700399"
  },
  {
    "text": "this prevents runway scaling controls your costs and allows you to have honest conversations with your users about",
    "start": "1700399",
    "end": "1706080"
  },
  {
    "text": "how much resources they actually need for their workloads and in addition you can set the maximum",
    "start": "1706080",
    "end": "1711440"
  },
  {
    "text": "size with the node groups and to limit the scale of the cluster on the cluster autos get other side to prevent runaway scaling there",
    "start": "1711440",
    "end": "1718399"
  },
  {
    "text": "um and make use of pod disruption budgets cluster autoscaler respects these fine draining nodes and this is a way to ensure that you",
    "start": "1718399",
    "end": "1726000"
  },
  {
    "text": "don't end up with accidental outages whilst the cluster auto scaler is draining uh nodes and the other thing to be aware",
    "start": "1726000",
    "end": "1734080"
  },
  {
    "text": "of is the cluster of scale that doesn't yet support all cloud providers all the big ones are covered um however there's work on going to",
    "start": "1734080",
    "end": "1740000"
  },
  {
    "text": "decouple the cloud providers and support a plugable cloud provider over grpc implementation to make it easier to",
    "start": "1740000",
    "end": "1745919"
  },
  {
    "text": "implement new ones um this is a quick list of the",
    "start": "1745919",
    "end": "1751120"
  },
  {
    "text": "flags that we think are most valuable to look at tuning away potentially away from their default values depending on your workloads",
    "start": "1751120",
    "end": "1757279"
  },
  {
    "text": "um within the cluster auto scaler so things like uh scale down uh delay after add or scale",
    "start": "1757279",
    "end": "1763919"
  },
  {
    "text": "down utilization threshold for instance that defaults to fifty percent uh we think some guy's going to rerun",
    "start": "1763919",
    "end": "1769679"
  },
  {
    "text": "that at um 80 or 90 and depending on the cluster so the cholesterol scaler is far more likely to",
    "start": "1769679",
    "end": "1776399"
  },
  {
    "text": "consider um no just candidates for scale day um and now jackson will take you through",
    "start": "1776399",
    "end": "1783520"
  },
  {
    "text": "a few other bits and pieces related to auto scaling and plus and kubernetes",
    "start": "1783520",
    "end": "1789520"
  },
  {
    "start": "1790000",
    "end": "2145000"
  },
  {
    "text": "um there's some other community projects in other scaling uh like areas uh like add-on resizer and",
    "start": "1792000",
    "end": "1800320"
  },
  {
    "text": "the cluster professional auto scholar they both scale the resources based on the cluster size",
    "start": "1800320",
    "end": "1805760"
  },
  {
    "text": "as the clusters scale up and down so you can think uh add-on resizer a much less",
    "start": "1805760",
    "end": "1811200"
  },
  {
    "text": "sophisticated vpa we use the linear formula to calculate the",
    "start": "1811200",
    "end": "1816399"
  },
  {
    "text": "request limit values of resources based on the cluster size normally it is used to scale metrics",
    "start": "1816399",
    "end": "1822880"
  },
  {
    "text": "related add-ons for example when your class will grow you probably want a large",
    "start": "1822880",
    "end": "1828080"
  },
  {
    "text": "matrix server cluster proportional auto scaler is normally used for",
    "start": "1828080",
    "end": "1833279"
  },
  {
    "text": "scaling dns replicas horizontally to ensure enough pulse to meet the dns query needs of the",
    "start": "1833279",
    "end": "1840720"
  },
  {
    "text": "cluster but nothing prevents you from putting these q autoscaler or other type of workflows beside the matrix and dns",
    "start": "1840720",
    "end": "1849679"
  },
  {
    "text": "and the kubernetes event uh event driver auto scouting was released last group account it",
    "start": "1849679",
    "end": "1856799"
  },
  {
    "text": "allows the uh like the large",
    "start": "1856799",
    "end": "1862320"
  },
  {
    "text": "auto scouting workflows from a wider variety of sources like uh kafka rugby and cube radius and",
    "start": "1862320",
    "end": "1869200"
  },
  {
    "text": "some cloud provider queries yeah guy will talk uh the summaries uh at the end",
    "start": "1869200",
    "end": "1879840"
  },
  {
    "text": "yep so what we've covered um as with anything cost saving in kubernetes is",
    "start": "1880720",
    "end": "1885760"
  },
  {
    "text": "about analyzing the trade-offs you can make based on your workloads in your environment uh which pods you can afford to be",
    "start": "1885760",
    "end": "1891519"
  },
  {
    "text": "interrupted how quickly you need services to scale up and down and what scaling behavior you want in your classroom services",
    "start": "1891519",
    "end": "1897519"
  },
  {
    "text": "um those strategies can vary depending on your clusters environments cloud provider the thing",
    "start": "1897519",
    "end": "1904480"
  },
  {
    "text": "that's most likely to yield the best return on investment is your horizontal pod auto scaling enabling",
    "start": "1904480",
    "end": "1910640"
  },
  {
    "text": "your service owners to figure out what are the bottlenecks for them and what metrics they want to scale",
    "start": "1910640",
    "end": "1916159"
  },
  {
    "text": "on and enabling them to do that whether it's through resource metrics um through custom metrics or through",
    "start": "1916159",
    "end": "1923120"
  },
  {
    "text": "external metrics that analysis is likely to give you the most improvement the quickest",
    "start": "1923120",
    "end": "1930640"
  },
  {
    "text": "thank you very much any questions",
    "start": "1931039",
    "end": "1935840"
  },
  {
    "text": "hello yes um so there are a few questions i will i've",
    "start": "1939440",
    "end": "1945519"
  },
  {
    "text": "not got a huge amount of time and i'll try and get through as many live as i can and i'll be in slack and",
    "start": "1945519",
    "end": "1950960"
  },
  {
    "text": "answering us any i don't get through afterwards um so there",
    "start": "1950960",
    "end": "1956000"
  },
  {
    "text": "is a question how do you handle auto scaling during deployment uh that is um a good question in sky",
    "start": "1956000",
    "end": "1964240"
  },
  {
    "text": "scanner we have our own um blue green deployment tooling we use that um and we set the minimum",
    "start": "1964240",
    "end": "1972080"
  },
  {
    "text": "um to be the current uh desired count when we started deployment so we still",
    "start": "1972080",
    "end": "1978640"
  },
  {
    "text": "allow it to scale up but we limit how much it can scale down and during a given deployment and that's",
    "start": "1978640",
    "end": "1984240"
  },
  {
    "text": "that's largely due to the behavior of our our deployment tooling so you may want to vary that and depending on how you do",
    "start": "1984240",
    "end": "1991519"
  },
  {
    "text": "that um for a long time the vpa was not recommended",
    "start": "1991519",
    "end": "1997200"
  },
  {
    "text": "for production workloads and this was written in the official documentation is it still the case and no so the recommender um a few",
    "start": "1997200",
    "end": "2005360"
  },
  {
    "text": "months back went um ga um if you have any worries about it as uh jackson",
    "start": "2005360",
    "end": "2012240"
  },
  {
    "text": "mentioned and we'd still recommend using it effectively in the off setting to begin with so that you can",
    "start": "2012240",
    "end": "2018640"
  },
  {
    "text": "see what recommendations the vpa would make at any given point in time and use that to build your confidence",
    "start": "2018640",
    "end": "2025360"
  },
  {
    "text": "with it and before actually enabling it and modifying pods",
    "start": "2025360",
    "end": "2030720"
  },
  {
    "text": "um do you have any suggestions on how to effectively auto scale when not using",
    "start": "2030840",
    "end": "2036480"
  },
  {
    "text": "a cloud provider offered cluster at uksgk um that's that will",
    "start": "2036480",
    "end": "2043840"
  },
  {
    "text": "uh depend a lot on the setup of your cluster and what sort of auto scaling you're you're",
    "start": "2043840",
    "end": "2050079"
  },
  {
    "text": "asking about um if if you want to put more details into slack channel i can give you a better answer than that",
    "start": "2050079",
    "end": "2056240"
  },
  {
    "text": "uh which is the better approach hpa or vpa um hpa is the the more mature approach i",
    "start": "2056240",
    "end": "2064240"
  },
  {
    "text": "would say and however there are situations where as mentioned there are situations where the vpa is the the better option",
    "start": "2064240",
    "end": "2070638"
  },
  {
    "text": "especially sort of singleton's things where you cannot replicate those those um pods horizontally so um",
    "start": "2070639",
    "end": "2079679"
  },
  {
    "text": "staple workloads for instance etc um i think i've got just enough time to",
    "start": "2079679",
    "end": "2085280"
  },
  {
    "text": "answer one more and if i use githubs and specify resources this might like lead to the vpa operator conflict which",
    "start": "2085280",
    "end": "2091919"
  },
  {
    "text": "might result in constant pod restarts am i right uh i believe that is the case",
    "start": "2091919",
    "end": "2097920"
  },
  {
    "text": "though currently um skyscanner don't use a github space workflow so i'm not actually certain but i believe yes if it",
    "start": "2097920",
    "end": "2104640"
  },
  {
    "text": "was trying to reconcile the pods uh running in the cluster with uh their resources against um",
    "start": "2104640",
    "end": "2113359"
  },
  {
    "text": "what you were desiring yeah i think your get ups operator would effectively start fighting with the vpa operator",
    "start": "2113359",
    "end": "2119760"
  },
  {
    "text": "around the pods being created with different resources uh thank you very much i think that's me",
    "start": "2119760",
    "end": "2126480"
  },
  {
    "text": "out of time um if you have any any of the other",
    "start": "2126480",
    "end": "2131599"
  },
  {
    "text": "questions that i wasn't able to answer um i'm happy to enter in um the slack channel to",
    "start": "2131599",
    "end": "2137040"
  },
  {
    "text": "dash cubecon dash operations and i will try and answer all of them and to the best of my ability so thank",
    "start": "2137040",
    "end": "2143599"
  },
  {
    "text": "you very much",
    "start": "2143599",
    "end": "2147200"
  }
]