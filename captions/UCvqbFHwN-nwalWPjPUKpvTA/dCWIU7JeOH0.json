[
  {
    "start": "0",
    "end": "62000"
  },
  {
    "text": "hello and welcome to our talk uh how to migrate 100 clusters between",
    "start": "1280",
    "end": "6720"
  },
  {
    "text": "clouds without uh downtime my name is manuel struss i'm a systems architect and tech lead at",
    "start": "6720",
    "end": "13759"
  },
  {
    "text": "guru matic we're doing a whole bunch of kubernetes and cloud native",
    "start": "13759",
    "end": "19119"
  },
  {
    "text": "consulting and we're developing the kubernetes kubernetes platform as well",
    "start": "19119",
    "end": "24400"
  },
  {
    "text": "as coupon our kubernetes management tools with me today is to be a schneck um head",
    "start": "24400",
    "end": "31119"
  },
  {
    "text": "of professional services at problematic and he's gonna tell you a few words about himself now",
    "start": "31119",
    "end": "36160"
  },
  {
    "text": "yes thanks manuel um yeah my name is tobias i'm already working two and a half years",
    "start": "36160",
    "end": "41760"
  },
  {
    "text": "for lutze and mostly responsible for professional professional service where we discovered",
    "start": "41760",
    "end": "47680"
  },
  {
    "text": "a way how to migrate clusters so uh this was a fancy idea and now we want to present you",
    "start": "47680",
    "end": "54079"
  },
  {
    "text": "how far we came to the journey and um so heading back to manuel",
    "start": "54079",
    "end": "59520"
  },
  {
    "text": "all right thanks um right why would you actually want to migrate",
    "start": "59520",
    "end": "64960"
  },
  {
    "start": "62000",
    "end": "172000"
  },
  {
    "text": "uh classes between cloud providers and there are actually a couple of reasons for that um",
    "start": "64960",
    "end": "71439"
  },
  {
    "text": "on the more businessy side of things um you might have better contract conditions at another cloud provider",
    "start": "71439",
    "end": "78400"
  },
  {
    "text": "so you would be able to save cost um there could be the need to migrate data",
    "start": "78400",
    "end": "84799"
  },
  {
    "text": "centers to a different hosting provider or cloud provider",
    "start": "84799",
    "end": "89920"
  },
  {
    "text": "from a logistic point of view to like a legal point of view or you're",
    "start": "89920",
    "end": "95680"
  },
  {
    "text": "driven by a multi-cloud strategy and you want to decrease your dependency on one",
    "start": "95680",
    "end": "102079"
  },
  {
    "text": "single uh existing cloud provider and and expand out to other providers there are also",
    "start": "102079",
    "end": "108560"
  },
  {
    "text": "some technical reasons for that um so again more logistical kind of reason might be that you",
    "start": "108560",
    "end": "115200"
  },
  {
    "text": "have a location migration of a data center or you might want to migrate to another",
    "start": "115200",
    "end": "121280"
  },
  {
    "text": "network segment for separation of concerns or other reasons um",
    "start": "121280",
    "end": "129520"
  },
  {
    "text": "you might be um adapting um improvements in your on-prem and",
    "start": "129520",
    "end": "135440"
  },
  {
    "text": "cloud environments um add a new provider that you want to use like new features",
    "start": "135440",
    "end": "140640"
  },
  {
    "text": "new different infrastructure technologies you want to use at a new",
    "start": "140640",
    "end": "146959"
  },
  {
    "text": "provider or you're bound to some constraints when it comes to data location",
    "start": "146959",
    "end": "152640"
  },
  {
    "text": "uh of a certain um services you're running maybe on some",
    "start": "152640",
    "end": "157680"
  },
  {
    "text": "cloud offered services for example where you run your machine learning and where you have your machine",
    "start": "157680",
    "end": "163440"
  },
  {
    "text": "learning data or some gdpr compliance",
    "start": "163440",
    "end": "168560"
  },
  {
    "text": "needs that you need to fulfill so what are the main challenging",
    "start": "168560",
    "end": "175280"
  },
  {
    "start": "172000",
    "end": "423000"
  },
  {
    "text": "challenges around moving to another cloud provider kubernetes itself abstracts",
    "start": "175280",
    "end": "181440"
  },
  {
    "text": "infrastructure but it does have several kind of dependencies nonetheless",
    "start": "181440",
    "end": "186959"
  },
  {
    "text": "right um so it does consume infrastructure resources for example the virtual machines where the cluster",
    "start": "186959",
    "end": "193120"
  },
  {
    "text": "itself runs on it uses and consumes the network provided right the ip address space",
    "start": "193120",
    "end": "200560"
  },
  {
    "text": "routing and firewalling rules management of ingress and egress traffic and also",
    "start": "200560",
    "end": "205920"
  },
  {
    "text": "dns as always and external storage systems",
    "start": "205920",
    "end": "211200"
  },
  {
    "text": "then there are kubernetes components that are actually dependent on a certain cloud provider um in",
    "start": "211360",
    "end": "218799"
  },
  {
    "text": "most of that is the cloud controller manager um that contains the node controller for",
    "start": "218799",
    "end": "224000"
  },
  {
    "text": "updating kubernetes nodes the service controller which translates a service type load balancer",
    "start": "224000",
    "end": "230560"
  },
  {
    "text": "within kubernetes to an actual cloud load balancer in the cloud provider's environment the route controller which is",
    "start": "230560",
    "end": "237360"
  },
  {
    "text": "responsible for setting up network routes in the cloud provider's network but there are also things like storage",
    "start": "237360",
    "end": "242799"
  },
  {
    "text": "classes that map to cloud provider specific storage offerings",
    "start": "242799",
    "end": "248400"
  },
  {
    "text": "and sometimes the overlay network you use has some dependency on a cloud provider as well",
    "start": "248400",
    "end": "256480"
  },
  {
    "text": "and just to remind ourselves a quick overview of the components of kubernetes the",
    "start": "256720",
    "end": "264160"
  },
  {
    "text": "central one being the api server which kind of handles all the changes in state of the cluster",
    "start": "264160",
    "end": "271040"
  },
  {
    "text": "itself and the worker nodes with the cubelet which runs the actual workload that that users",
    "start": "271040",
    "end": "278400"
  },
  {
    "text": "run on top of that cluster and between the api server and the cubelet there has to be a two-way",
    "start": "278400",
    "end": "283680"
  },
  {
    "text": "um communication happening um and um between the cubelets as well um i",
    "start": "283680",
    "end": "289520"
  },
  {
    "text": "mean between the worker nodes as well and that is one main challenges where we're kind of solving today",
    "start": "289520",
    "end": "296720"
  },
  {
    "text": "to to enable this ways of communication and that directly leads us",
    "start": "296720",
    "end": "304960"
  },
  {
    "text": "to our actual dependencies when we migrate so for us the application workload has",
    "start": "304960",
    "end": "311360"
  },
  {
    "text": "the highest priority but we need to ensure for that to be the case we need to ensure",
    "start": "311360",
    "end": "317120"
  },
  {
    "text": "fundamental networking rules that kubernetes expect to be in place those rules are that all containers",
    "start": "317120",
    "end": "323440"
  },
  {
    "text": "within a pod can communicate unimpededly on layer 4 so tcp udp",
    "start": "323440",
    "end": "330639"
  },
  {
    "text": "all pods can communicate with all other pods within the cluster without netting and all nodes can communicate with all",
    "start": "330639",
    "end": "337680"
  },
  {
    "text": "parts and vice versa also without netting and the final one the ip that the pod",
    "start": "337680",
    "end": "343440"
  },
  {
    "text": "sees itself is the same ip that others see the part as and this is really",
    "start": "343440",
    "end": "350160"
  },
  {
    "text": "important to keep up so that the actual networking between parts and applications works",
    "start": "350160",
    "end": "355759"
  },
  {
    "text": "um we have to have some external dependencies that need to be reachable right like",
    "start": "355759",
    "end": "361759"
  },
  {
    "text": "externally routed ips for load balancers and notepad services within the cluster",
    "start": "361759",
    "end": "366880"
  },
  {
    "text": "and dns names need to be reachable resolvable right and storage",
    "start": "366880",
    "end": "373600"
  },
  {
    "text": "um some applications might have to have state that needs to be migrated without data loss",
    "start": "373600",
    "end": "380400"
  },
  {
    "text": "so when we look at that now um at a level of like scale",
    "start": "380400",
    "end": "387360"
  },
  {
    "text": "of hundreds of clusters maybe we look at large organizations running a whole",
    "start": "387360",
    "end": "392960"
  },
  {
    "text": "bunch of classes in different location for different organizational units in different time zones right and",
    "start": "392960",
    "end": "400880"
  },
  {
    "text": "for those users cluster users the cluster itself is just the service",
    "start": "400880",
    "end": "406720"
  },
  {
    "text": "that they consume right um and that means that the cluster connection and secrets so the actual",
    "start": "406720",
    "end": "412560"
  },
  {
    "text": "interface that the user has to the cluster doesn't uh is not allowed to change right otherwise",
    "start": "412560",
    "end": "418319"
  },
  {
    "text": "it would impede the actual service that those users consume",
    "start": "418319",
    "end": "423840"
  },
  {
    "start": "423000",
    "end": "590000"
  },
  {
    "text": "um so how do we solve for that um the status quo is uh will have a multi-cloud setup with",
    "start": "424000",
    "end": "430000"
  },
  {
    "text": "the kubernetes kubernetes platform our open source uh kubernetes management platform",
    "start": "430000",
    "end": "435360"
  },
  {
    "text": "and it has a concept of a seed cluster that holds the containerized control plane of the user clusters the user",
    "start": "435360",
    "end": "441199"
  },
  {
    "text": "clusters being the class kubernetes clusters that are managed by kkp the worker nodes itself",
    "start": "441199",
    "end": "448479"
  },
  {
    "text": "are uh via the kubomatic machine controller which is a cluster api conform operator that",
    "start": "448479",
    "end": "456639"
  },
  {
    "text": "translates a machine deployment object into actual machines vms on cloud",
    "start": "456639",
    "end": "463840"
  },
  {
    "text": "providers and we'll use canal as our default overlay networking which is effectively",
    "start": "463840",
    "end": "469520"
  },
  {
    "text": "flannel um the exlan overlay networking with a calico network policy plugin and the target is",
    "start": "469520",
    "end": "477120"
  },
  {
    "text": "that we migrate the user and c cluster control planes and worker nodes to a different cloud provider",
    "start": "477120",
    "end": "483120"
  },
  {
    "text": "uh we'll keep all the external clustering points stable that means the control plane kubernetes api server",
    "start": "483120",
    "end": "489039"
  },
  {
    "text": "endpoints and the actual application endpoints being the dns and ingress routine",
    "start": "489039",
    "end": "495840"
  },
  {
    "text": "out of scope for now is the storage replication um the assumption is that the actual",
    "start": "495840",
    "end": "500879"
  },
  {
    "text": "application layer manages the storage replication like at cd which is a feature that we will use",
    "start": "500879",
    "end": "506560"
  },
  {
    "text": "to migrate the user cluster control plane so how does it look um we have a",
    "start": "506560",
    "end": "513440"
  },
  {
    "text": "kubomatic installation that has a seat cluster which is also just a kubernetes cluster",
    "start": "513440",
    "end": "518719"
  },
  {
    "text": "running in the google cloud and it hosts a couple of user clusters and we'll have a look at the vsphere",
    "start": "518719",
    "end": "526320"
  },
  {
    "text": "user cluster that runs worker nodes on a vsphere cluster and we want to move all of that over to aws because",
    "start": "526320",
    "end": "533040"
  },
  {
    "text": "uh for whatever reason we want to run on aws um and just some recommended",
    "start": "533040",
    "end": "539519"
  },
  {
    "text": "prerequisites uh uh doing that uh in production uh you'll have to announce a maintenance",
    "start": "539519",
    "end": "545360"
  },
  {
    "text": "window and block cluster updates so that those doesn't don't interfere with the",
    "start": "545360",
    "end": "550959"
  },
  {
    "text": "actual migration process we'll have to ensure that our backup and recovery procedure for the seat and user",
    "start": "550959",
    "end": "557279"
  },
  {
    "text": "clusters but also for the application workloads works is tested and and proven to be",
    "start": "557279",
    "end": "563120"
  },
  {
    "text": "working right um we'll should create a target cloud cluster as a reference in",
    "start": "563120",
    "end": "570880"
  },
  {
    "text": "our case an aws cluster just so that we can uh copy and paste some stuff over",
    "start": "570880",
    "end": "576880"
  },
  {
    "text": "and we'll have to ensure that we actually control the dns entries and be",
    "start": "576880",
    "end": "582320"
  },
  {
    "text": "able to to switch over dns entries to the new cloud endpoints once we migrated the workload over to",
    "start": "582320",
    "end": "588320"
  },
  {
    "text": "the new cloud provider yeah and now we'll look at the actual",
    "start": "588320",
    "end": "594160"
  },
  {
    "start": "590000",
    "end": "610000"
  },
  {
    "text": "solution approach and toby is going to give us a quick demo on how that works",
    "start": "594160",
    "end": "600320"
  },
  {
    "text": "okay down over to me segs manually um share my screen hopefully you can see it",
    "start": "600320",
    "end": "608800"
  },
  {
    "text": "now um and yes so what is the solution approach um so",
    "start": "608800",
    "end": "616959"
  },
  {
    "start": "610000",
    "end": "893000"
  },
  {
    "text": "first of all we want to migrate the user cluster workers so in this case um",
    "start": "616959",
    "end": "623360"
  },
  {
    "text": "we we want to migrate it and we want to have uh new workers in the target cloud how",
    "start": "623360",
    "end": "629440"
  },
  {
    "text": "we can reach that so we're using the so-called machine controller in kubernetes and keyboard and access",
    "start": "629440",
    "end": "636480"
  },
  {
    "text": "controller can create workers based on crds what's called machine deployment",
    "start": "636480",
    "end": "642000"
  },
  {
    "text": "so machine deployment is similar like the deployment of pots it's a deployment of",
    "start": "642000",
    "end": "648240"
  },
  {
    "text": "machines and if we like change that machine controller and give them a new",
    "start": "648240",
    "end": "653839"
  },
  {
    "text": "specification we can create the machines in the new cloud what is needed to ensure the traffic is",
    "start": "653839",
    "end": "660800"
  },
  {
    "text": "reachable we need to somehow need a way how to communicate",
    "start": "660800",
    "end": "666160"
  },
  {
    "text": "between ports and ports and nodes to nodes for that we create a vpn overlay by a daemon set and",
    "start": "666160",
    "end": "672480"
  },
  {
    "text": "route and traffic of the cni uh in our case kennel through the vpn network",
    "start": "672480",
    "end": "678640"
  },
  {
    "text": "and let's ensure that we have maybe different network segmentations but still can talk to each other if we",
    "start": "678640",
    "end": "685440"
  },
  {
    "text": "have this client to client vpn traffic enabled and uh at least we should ensure the",
    "start": "685440",
    "end": "691839"
  },
  {
    "text": "reachability that means that we try as long as possible to keep the increase endpoint stable to trans and",
    "start": "691839",
    "end": "699279"
  },
  {
    "text": "then transfer the workload to the new cloud and after the workload is the new cloud",
    "start": "699279",
    "end": "705519"
  },
  {
    "text": "we delete the connectivity okay how sus uh scan looks like so we have the c",
    "start": "705519",
    "end": "713200"
  },
  {
    "text": "cluster what host in the containerize control plane here we have some um controllers and we have the",
    "start": "713200",
    "end": "720639"
  },
  {
    "text": "kubernetes containerized control plane we have their default vpn server this vpn server",
    "start": "720639",
    "end": "726639"
  },
  {
    "text": "is used for vpn traffic between this control plane and the workers attributic",
    "start": "726639",
    "end": "732560"
  },
  {
    "text": "so that workers can connect with the control plane and your control plane can back uh",
    "start": "732560",
    "end": "738160"
  },
  {
    "text": "tunnel the cube control locks and exec cores over this vpn tunnel",
    "start": "738160",
    "end": "745040"
  },
  {
    "text": "also we have a machine controller which is configured to place on machines on the vsphere cloud and we",
    "start": "745120",
    "end": "750959"
  },
  {
    "text": "have between the v3 workers we have an overlay what's based on kennel and we",
    "start": "750959",
    "end": "756079"
  },
  {
    "text": "have a metal ap service what uh creates then the inbound",
    "start": "756079",
    "end": "762399"
  },
  {
    "text": "traffic load balancer to the dedicated application ports",
    "start": "762399",
    "end": "767600"
  },
  {
    "text": "first step how to migrate is now to deploy a vpn daemon said this vpn daemon set ensures that we have",
    "start": "767600",
    "end": "774399"
  },
  {
    "text": "an uh vpn client at every worker node this opens in the working out the new interface and we",
    "start": "774399",
    "end": "780959"
  },
  {
    "text": "route the traffic from the vpn interface through the vpn interface uh our kennel",
    "start": "780959",
    "end": "788160"
  },
  {
    "text": "to our dedicated overlay so and for that we need to pause the cluster because",
    "start": "788160",
    "end": "793519"
  },
  {
    "text": "our cluster is controlled by a cluster controller and this cluster controller would then reconcile uh the machine",
    "start": "793519",
    "end": "801279"
  },
  {
    "text": "deployments and uh the vpn servers so to ensure that this does not happen we",
    "start": "801279",
    "end": "806399"
  },
  {
    "text": "pause the cluster to make some patches there next step is um",
    "start": "806399",
    "end": "811440"
  },
  {
    "text": "after we have the credentials for the new cloud",
    "start": "811440",
    "end": "817519"
  },
  {
    "text": "uh adopted we can update the cluster spec and specificate the new aws cloud then the machine",
    "start": "817519",
    "end": "825120"
  },
  {
    "text": "controller gets updated and we get a new machine controller instance with co and now talk to aws and the",
    "start": "825120",
    "end": "831920"
  },
  {
    "text": "new nodes get created and also joins our vpn network and the join so also the kennel overlay",
    "start": "831920",
    "end": "838880"
  },
  {
    "text": "routing the cloud controller now ensures that we also have a new aws lb",
    "start": "838880",
    "end": "845199"
  },
  {
    "text": "what get created chronolysis llb is not routed but anyway traffic goes from here to over the metal",
    "start": "845199",
    "end": "852320"
  },
  {
    "text": "ap service to the dedicated workload after this this happens we can",
    "start": "852320",
    "end": "857839"
  },
  {
    "text": "remove the workload from the old uh workers and move the workload to the new",
    "start": "857839",
    "end": "863360"
  },
  {
    "text": "applications after this is done we can also re-name the dns names to the new cloud load balancer",
    "start": "863360",
    "end": "871680"
  },
  {
    "text": "and ensure that we have now may create traffic to the new cloud",
    "start": "871680",
    "end": "877120"
  },
  {
    "text": "at least then clean up the old resources and we removed we not needed any more vpn",
    "start": "877120",
    "end": "883360"
  },
  {
    "text": "overlay because this two workers can now talk to each other with eth",
    "start": "883360",
    "end": "888480"
  },
  {
    "text": "interface and that's how we migrated it so to give you a short in a",
    "start": "888480",
    "end": "895199"
  },
  {
    "start": "893000",
    "end": "980000"
  },
  {
    "text": "short look what already is happening we create some demo be aware that uh currently this project",
    "start": "895199",
    "end": "901360"
  },
  {
    "text": "is like not fully finished so it's more a proof of concept state so as we see here we have here",
    "start": "901360",
    "end": "908880"
  },
  {
    "text": "a app or what is our reference echo service and this is deployed on the cluster",
    "start": "908880",
    "end": "914000"
  },
  {
    "text": "let's take a look in our chiromatic control plane and in this control plane",
    "start": "914000",
    "end": "919279"
  },
  {
    "text": "we have um the dedicated clusters here so first we have here this",
    "start": "919279",
    "end": "926720"
  },
  {
    "text": "cube called migrate cluster what we want to migrate here we see this cluster contains the containerized",
    "start": "926720",
    "end": "935120"
  },
  {
    "text": "control planes a machine deployment of two nodes and a so-called echo service engine x",
    "start": "935120",
    "end": "941680"
  },
  {
    "text": "and uh metal ap this metal ap points to the peer queries of the vsphere and",
    "start": "941680",
    "end": "948880"
  },
  {
    "text": "deploys our echo services so if we go to the vsphere we see here",
    "start": "948880",
    "end": "954880"
  },
  {
    "text": "under the cluster id what is here kbhc",
    "start": "954880",
    "end": "963040"
  },
  {
    "text": "we see here the little machines running and this one we want now to move to",
    "start": "963680",
    "end": "970560"
  },
  {
    "text": "aws first step what we already did is that we",
    "start": "970560",
    "end": "976800"
  },
  {
    "text": "deployed a vpn so we patched our vpn server and for that we see that we have a",
    "start": "976800",
    "end": "983920"
  },
  {
    "start": "980000",
    "end": "1095000"
  },
  {
    "text": "cluster spec and we have in so we see here also in our c cluster that",
    "start": "983920",
    "end": "990000"
  },
  {
    "text": "everything what you see in cubic is also represented as a cluster crt in the in the cluster namespace",
    "start": "990000",
    "end": "997440"
  },
  {
    "text": "here sysnamespace we see the control planes and here you see that we have like the vpn",
    "start": "997440",
    "end": "1004160"
  },
  {
    "text": "server running and we have api server and all other components running",
    "start": "1004160",
    "end": "1010240"
  },
  {
    "text": "if we connect here to the user cluster so let's take this shortcut yes i want to",
    "start": "1010240",
    "end": "1016320"
  },
  {
    "text": "go to the cubicle cluster um we see a bunch of containers here",
    "start": "1016320",
    "end": "1021759"
  },
  {
    "text": "and we see here that we have our echo service we have our ancient x we have our kennel",
    "start": "1021759",
    "end": "1027600"
  },
  {
    "text": "we have our vpn client and that's now we want what we want to",
    "start": "1027600",
    "end": "1033678"
  },
  {
    "text": "migrate so yeah first step is the vpn is already there so we can now deploy um",
    "start": "1033679",
    "end": "1041280"
  },
  {
    "text": "our control plane and migrate it what is i think the most interesting part of the whole thing",
    "start": "1041280",
    "end": "1046558"
  },
  {
    "text": "so we can then say okay here our update target cloud script what does it",
    "start": "1046559",
    "end": "1052160"
  },
  {
    "text": "do so we have here um yeah we first we need somehow",
    "start": "1052160",
    "end": "1059360"
  },
  {
    "text": "cluster id and we need a project id the project id we",
    "start": "1060480",
    "end": "1066880"
  },
  {
    "text": "find here in our hubometic um url here to approach it actually we can",
    "start": "1066880",
    "end": "1073840"
  },
  {
    "text": "copy it and we can here place it so what is the first step we",
    "start": "1073840",
    "end": "1081120"
  },
  {
    "text": "create in backup that's the backup of the specification of this cluster",
    "start": "1081120",
    "end": "1086559"
  },
  {
    "text": "so yeah i want to create it and as next step i want to pause the cluster",
    "start": "1086559",
    "end": "1093840"
  },
  {
    "text": "and yes after i pause the cluster the controller does not care anymore so",
    "start": "1093840",
    "end": "1099600"
  },
  {
    "start": "1095000",
    "end": "1340000"
  },
  {
    "text": "now i can safely update my cloud provider so yes i want to patch the cloud provider so",
    "start": "1099600",
    "end": "1105280"
  },
  {
    "text": "i create a new cluster the ammo what we now can take a look on so if we go no to my",
    "start": "1105280",
    "end": "1111840"
  },
  {
    "text": "files i see here uh that i have here under control plane somehow a new file",
    "start": "1111840",
    "end": "1120559"
  },
  {
    "text": "uh where um is should be here let's see",
    "start": "1120960",
    "end": "1128550"
  },
  {
    "text": "[Music]",
    "start": "1128550",
    "end": "1131599"
  },
  {
    "text": "um okay um yeah here we are now we have",
    "start": "1134840",
    "end": "1140640"
  },
  {
    "text": "um this one backup clusters yamo and uh the petchamu so let's see what's",
    "start": "1140640",
    "end": "1147280"
  },
  {
    "text": "the difference is so yeah so currently that's our cluster crd on the left we have the backup",
    "start": "1147280",
    "end": "1153760"
  },
  {
    "text": "cluster spec and we have here our api server token and so on and we have to finalize us",
    "start": "1153760",
    "end": "1160559"
  },
  {
    "text": "what clean up our cloud when we delete the clusters and that's now the important stuff we have here the",
    "start": "1160559",
    "end": "1167840"
  },
  {
    "text": "vsphere configuration what reference the credential and",
    "start": "1167840",
    "end": "1173120"
  },
  {
    "text": "the folder and so on and that's something what we now remove as well as status fields",
    "start": "1173120",
    "end": "1179440"
  },
  {
    "text": "and we apply this change now to our cluster the first step to remove the credentials",
    "start": "1179440",
    "end": "1188480"
  },
  {
    "text": "this is needed so let's apply that and see what happens so we see now we have configured",
    "start": "1188480",
    "end": "1194799"
  },
  {
    "text": "and now to start reconciling we need to compose the cluster that the cluster controller can take care about",
    "start": "1194799",
    "end": "1201039"
  },
  {
    "text": "the change and yeah let's see what now is happening so we have now the",
    "start": "1201039",
    "end": "1206480"
  },
  {
    "text": "cloud spec what get recreated by the cloud controller so hopefully everything",
    "start": "1206480",
    "end": "1212559"
  },
  {
    "text": "works well and we see that the cluster can now reconcile yeah we see now we're getting the new",
    "start": "1212559",
    "end": "1219360"
  },
  {
    "text": "object we have the vsphere is empty okay this looks good and we can now watch",
    "start": "1219360",
    "end": "1225280"
  },
  {
    "text": "uh that hopefully yeah we see the api server is reconciling to",
    "start": "1225280",
    "end": "1230640"
  },
  {
    "text": "now an empty cloud provider okay cool that was the first step so now as next",
    "start": "1230640",
    "end": "1236320"
  },
  {
    "text": "step we want to change to to our aws so let's go out of this view",
    "start": "1236320",
    "end": "1242000"
  },
  {
    "text": "and let's pause the cluster again like currently uh depending on the chromatic controller",
    "start": "1242000",
    "end": "1247600"
  },
  {
    "text": "we need this two-step upgrades uh because we are just a cube control client and not",
    "start": "1247600",
    "end": "1252720"
  },
  {
    "text": "operator and and now we can patch the cloud provider so what happens now um yes",
    "start": "1252720",
    "end": "1259840"
  },
  {
    "text": "i want to patch it yes i'd want a new secret so there we create a new secret reference for the",
    "start": "1259840",
    "end": "1265840"
  },
  {
    "text": "aws credentials and then we created a new cluster patch file again",
    "start": "1265840",
    "end": "1272880"
  },
  {
    "text": "so let's also see here what's different now so we see now we have uh changed",
    "start": "1272880",
    "end": "1278799"
  },
  {
    "text": "basically removed the finalizers because this are not valid anymore we added a annotation here",
    "start": "1278799",
    "end": "1286559"
  },
  {
    "text": "to the chromatic avws region we patched cloud spec aws with the credentials and we",
    "start": "1286559",
    "end": "1294159"
  },
  {
    "text": "have a migration a new data center so that's the new migration data center where we want to",
    "start": "1294159",
    "end": "1299760"
  },
  {
    "text": "migrate it after we now make that pause to",
    "start": "1299760",
    "end": "1305200"
  },
  {
    "text": "force uh the controller try to reconcile where uh new aws",
    "start": "1305200",
    "end": "1311760"
  },
  {
    "text": "should place there and the nice thing is therefore cubematic creates now a new uh group a new",
    "start": "1311760",
    "end": "1320080"
  },
  {
    "text": "roles and that's hopefully what's now happening so let's see so let's patch that cluster",
    "start": "1320080",
    "end": "1328080"
  },
  {
    "text": "and um configured and yeah now let's on pause cluster and",
    "start": "1328080",
    "end": "1334240"
  },
  {
    "text": "see what's happening happening good so okay so what we see now here we can",
    "start": "1334240",
    "end": "1342080"
  },
  {
    "start": "1340000",
    "end": "1540000"
  },
  {
    "text": "reconcile you see we get now here also aws data back from the chromatic controller",
    "start": "1342080",
    "end": "1349280"
  },
  {
    "text": "and see what we created new security group and the instance profile um",
    "start": "1349280",
    "end": "1354799"
  },
  {
    "text": "cool so let's see what happens with my components we're now seeing yeah we have a",
    "start": "1354799",
    "end": "1360159"
  },
  {
    "text": "restarting api server again uh it started now with the new club prevent credentials",
    "start": "1360159",
    "end": "1366240"
  },
  {
    "text": "so let's try to find a little bit out what's happening here so we have here um deployment",
    "start": "1366240",
    "end": "1374480"
  },
  {
    "text": "so and let's take a look into oh it's the wrong side let's see what we are",
    "start": "1374480",
    "end": "1382400"
  },
  {
    "text": "have placed there so can you get explain deployment",
    "start": "1382400",
    "end": "1389840"
  },
  {
    "text": "for sure i need to write cube config and now we should go to the api",
    "start": "1390880",
    "end": "1397760"
  },
  {
    "text": "server of our cluster and see that here we have specified hopefully now the",
    "start": "1397760",
    "end": "1405039"
  },
  {
    "text": "uh cloud provider uh this here um our new",
    "start": "1405039",
    "end": "1412559"
  },
  {
    "text": "club provider aws and now that's reconciling take place and we also have a new machine",
    "start": "1412559",
    "end": "1418400"
  },
  {
    "text": "controller that now is able to talk with aws so what we can now create is the new aws",
    "start": "1418400",
    "end": "1424799"
  },
  {
    "text": "workers so let's go back into the user cluster so um",
    "start": "1424799",
    "end": "1430240"
  },
  {
    "text": "yeah here go to this qr migration cluster and first um yeah pause",
    "start": "1430240",
    "end": "1438240"
  },
  {
    "text": "all sheep deployments to be sure so uh worker",
    "start": "1438240",
    "end": "1444720"
  },
  {
    "text": "machine deployment pause",
    "start": "1444720",
    "end": "1448158"
  },
  {
    "text": "um that we don't upgrade this all machines and yes pause",
    "start": "1450480",
    "end": "1458400"
  },
  {
    "text": "done and then we can now create our new aws workers therefore we need uh",
    "start": "1458400",
    "end": "1466000"
  },
  {
    "text": "first a few inputs so here we need to specify the cluster id",
    "start": "1466000",
    "end": "1472720"
  },
  {
    "text": "the instance profile in the security group but chromatic had created automatically so here yeah",
    "start": "1472720",
    "end": "1479360"
  },
  {
    "text": "i want to see the metadatas we have here the cluster id so",
    "start": "1479360",
    "end": "1486320"
  },
  {
    "text": "let's change that one here we have the aws instance profile",
    "start": "1486320",
    "end": "1493520"
  },
  {
    "text": "what have we created what should be that one and we have the aws security group",
    "start": "1493520",
    "end": "1500880"
  },
  {
    "text": "okay so let's create that one save it and then running the script",
    "start": "1500880",
    "end": "1509840"
  },
  {
    "text": "deploy so now hopefully fingers crossed we grab new aws workers so",
    "start": "1509840",
    "end": "1517760"
  },
  {
    "text": "yes i want to create one here we see that's now rendered in in the machine deployment",
    "start": "1517760",
    "end": "1523039"
  },
  {
    "text": "we see our security group we see that we want to have a t3 medium and",
    "start": "1523039",
    "end": "1529679"
  },
  {
    "text": "yeah that in the usb best one see so okay then let's deploy it",
    "start": "1529679",
    "end": "1537919"
  },
  {
    "text": "so fingers cross yes as has been created so i want to watch the",
    "start": "1538880",
    "end": "1544880"
  },
  {
    "start": "1540000",
    "end": "1673000"
  },
  {
    "text": "creation yes i want to see that and you now see what the machine deployment we have a new machine",
    "start": "1544880",
    "end": "1550880"
  },
  {
    "text": "deployment with a boot target aws what creates two new machines and uh",
    "start": "1550880",
    "end": "1556240"
  },
  {
    "text": "what get provision now in atl yes um so let's see uh our workload is still",
    "start": "1556240",
    "end": "1563120"
  },
  {
    "text": "running as we see uh we have here our target cluster we see also here we have the new ubit",
    "start": "1563120",
    "end": "1570080"
  },
  {
    "text": "would retarget aws node group let's now get muted and yeah let's go to aws and see what's",
    "start": "1570080",
    "end": "1578159"
  },
  {
    "text": "have been created so here [Music]",
    "start": "1578159",
    "end": "1583200"
  },
  {
    "text": "hopefully the aws console is fast enough uh we can now go to the",
    "start": "1583200",
    "end": "1591760"
  },
  {
    "text": "ec2 in sensors and should see that we can have booted now new to instances",
    "start": "1592240",
    "end": "1601440"
  },
  {
    "text": "and let's see",
    "start": "1602480",
    "end": "1612240"
  },
  {
    "text": "yeah we see here initializing so that's the new two machines what we created if we take a look here we also see the",
    "start": "1612240",
    "end": "1618640"
  },
  {
    "text": "tag that's the cluster for what we have and for that so",
    "start": "1618640",
    "end": "1623760"
  },
  {
    "text": "we have created new machines so let's wait until the points uh say i get booted",
    "start": "1623760",
    "end": "1631360"
  },
  {
    "text": "in the meantime we can take a look in the load pencil so hopefully um we also have a load balancer created",
    "start": "1631360",
    "end": "1638240"
  },
  {
    "text": "that's what aws uh uh cloud controller manager will create because we already",
    "start": "1638240",
    "end": "1643279"
  },
  {
    "text": "have a service type load balancer in the cluster and that aws takes over and creates also low bands on",
    "start": "1643279",
    "end": "1650559"
  },
  {
    "text": "their side so let's see that's not the right one that's the right one",
    "start": "1650559",
    "end": "1658480"
  },
  {
    "text": "um here we see there's a class created but we don't have any instances here because",
    "start": "1658480",
    "end": "1664559"
  },
  {
    "text": "yeah the instances get now booted so let's hopefully let's go back and see",
    "start": "1664559",
    "end": "1669760"
  },
  {
    "text": "how fast they are coming up okay we now see that we get a new node that's",
    "start": "1669760",
    "end": "1676880"
  },
  {
    "text": "not ready but we can already connect to him so let's try to ssh into one adapter aws node",
    "start": "1676880",
    "end": "1685520"
  },
  {
    "text": "and let's go here for that kind and see what's happening there yes i",
    "start": "1685520",
    "end": "1692559"
  },
  {
    "text": "want to connect and yeah we see here that we have",
    "start": "1692559",
    "end": "1699279"
  },
  {
    "text": "here uh a flannel route and we will soon have a cube",
    "start": "1699279",
    "end": "1705760"
  },
  {
    "text": "or a interface for the vpn server as soon as that's started here we go here we see that",
    "start": "1705760",
    "end": "1712000"
  },
  {
    "text": "interface and we can now try to get the interconnection between one cloud to another so we i",
    "start": "1712000",
    "end": "1718399"
  },
  {
    "text": "then connect here to the on-premise node on the down and um",
    "start": "1718399",
    "end": "1725120"
  },
  {
    "text": "let's connect to that one ubuntu and test if we are now can talk",
    "start": "1725120",
    "end": "1732399"
  },
  {
    "text": "between clouds and here as well i have ip address",
    "start": "1732399",
    "end": "1739039"
  },
  {
    "text": "for the cube interface what is 10 20 42 and let's try",
    "start": "1739039",
    "end": "1747200"
  },
  {
    "text": "if we can bring it from our aws node",
    "start": "1747200",
    "end": "1753840"
  },
  {
    "text": "okay now we see that we get a connection here from the club node to the on-premise node",
    "start": "1760960",
    "end": "1769039"
  },
  {
    "text": "good cool so next step what we now need to do is to to migrate our workload",
    "start": "1769039",
    "end": "1777919"
  },
  {
    "text": "okay cool then let's go back here and",
    "start": "1777919",
    "end": "1784480"
  },
  {
    "text": "here as well and try to migrate to the workload so we have now",
    "start": "1789039",
    "end": "1796000"
  },
  {
    "text": "switched here to the user cluster to see what's happening and",
    "start": "1796000",
    "end": "1803039"
  },
  {
    "text": "see take a look in the echo server namespace",
    "start": "1804720",
    "end": "1810880"
  },
  {
    "text": "sorry um we see now here okay the echo cell is",
    "start": "1811279",
    "end": "1818080"
  },
  {
    "text": "deployed on the migration vsphere node we want to now to roll out the new",
    "start": "1818080",
    "end": "1823679"
  },
  {
    "text": "workload to the new cloud so let's try to quarten the notes",
    "start": "1823679",
    "end": "1830080"
  },
  {
    "text": "according the notes because that new workload should not anymore go",
    "start": "1830080",
    "end": "1836720"
  },
  {
    "text": "to this old nodes good that's corton and",
    "start": "1836720",
    "end": "1844240"
  },
  {
    "text": "we have the another node to curtain so that means that the node should be now marked as not schedule",
    "start": "1844240",
    "end": "1851840"
  },
  {
    "text": "able yes so scheduling disabled perfect so we can now uh use the cubecontrol",
    "start": "1851840",
    "end": "1858720"
  },
  {
    "text": "rollout restart feature to now restart our deployment of the echo server in the",
    "start": "1858720",
    "end": "1864640"
  },
  {
    "text": "namespace ico server to trigger like a row in the release of the echo server without any",
    "start": "1864640",
    "end": "1869919"
  },
  {
    "text": "change so uh yeah let's trigger that one and see what's happening in the down we see okay that's still we",
    "start": "1869919",
    "end": "1877519"
  },
  {
    "text": "have uh the application up and running and yes so now we see the container get created",
    "start": "1877519",
    "end": "1883279"
  },
  {
    "text": "in the new cloud so first success so hopefully this will now work um we see that now a new container is",
    "start": "1883279",
    "end": "1891039"
  },
  {
    "text": "running on the new aws workers and we can now terminate the old one so that's now going step by",
    "start": "1891039",
    "end": "1898720"
  },
  {
    "text": "step and we have three new workers and you see the service is still reachable and we",
    "start": "1898720",
    "end": "1905919"
  },
  {
    "text": "can't get now hopefully also if we go to the browser the",
    "start": "1905919",
    "end": "1912880"
  },
  {
    "text": "we can see here detail that we get back",
    "start": "1912880",
    "end": "1918159"
  },
  {
    "text": "from the aws node to cause so we have uh where is the host name",
    "start": "1918240",
    "end": "1925360"
  },
  {
    "text": "no it's just the host name um we see that the yeah the workers are",
    "start": "1925360",
    "end": "1930640"
  },
  {
    "text": "running in a new cloud and uh is still reachable through our old endpoint",
    "start": "1930640",
    "end": "1936960"
  },
  {
    "text": "cool so that seems to work um like now that the next step would be to migrate",
    "start": "1936960",
    "end": "1943120"
  },
  {
    "text": "all other workload to the new cloud remove the load balancer and then using",
    "start": "1943120",
    "end": "1948399"
  },
  {
    "text": "the new dns name so um we can quickly try if the new low",
    "start": "1948399",
    "end": "1953440"
  },
  {
    "text": "balance is now listening so yeah we have here the new instances and maybe the dns name is already",
    "start": "1953440",
    "end": "1960320"
  },
  {
    "text": "propagated to see if this is working we can see okay let's change that to the new dns",
    "start": "1960320",
    "end": "1968640"
  },
  {
    "text": "name and no so dns is not",
    "start": "1968640",
    "end": "1974880"
  },
  {
    "text": "there so yeah good then finally migrated to the new cloud",
    "start": "1974880",
    "end": "1982720"
  },
  {
    "text": "so how are the next step looks like so um that's uh",
    "start": "1982720",
    "end": "1988880"
  },
  {
    "text": "how we can create the migration uh the worker user clusters and to move completely to the another",
    "start": "1988880",
    "end": "1994720"
  },
  {
    "text": "cloud we need now to migrate also the c cluster for that how we can achieve that is the same way",
    "start": "1994720",
    "end": "2000720"
  },
  {
    "text": "we're reusing the same principle so here we we have the workers now in the new cloud",
    "start": "2000720",
    "end": "2006159"
  },
  {
    "text": "and now we need to make migrate control plane the good thing is on that",
    "start": "2006159",
    "end": "2011360"
  },
  {
    "text": "in any case the workers workload is still safe because it's already in the new cloud so we can",
    "start": "2011360",
    "end": "2018000"
  },
  {
    "text": "migrate it even if we may break it and the only thing what would maybe not work is an",
    "start": "2018000",
    "end": "2024640"
  },
  {
    "text": "upgrade of the kubernetes but still the workload is safe so how we migrate the c clusters first",
    "start": "2024640",
    "end": "2031440"
  },
  {
    "text": "we create new master nodes for the seat masters so that means we creating new",
    "start": "2031440",
    "end": "2037919"
  },
  {
    "text": "nodes putting a new capabilities apis load balance on it update api endpoints because this",
    "start": "2037919",
    "end": "2045039"
  },
  {
    "text": "api endpoints are stable in the qubit in this cluster and then we um block uh",
    "start": "2045039",
    "end": "2051440"
  },
  {
    "text": "for sure the c cluster for upgrades so that we don't anything yeah uh",
    "start": "2051440",
    "end": "2057679"
  },
  {
    "text": "trigger unexpected characteristics during this migration and then um yeah we migrating",
    "start": "2057679",
    "end": "2063839"
  },
  {
    "text": "the user control planes uh in the same way as migrated the workload now at the",
    "start": "2063839",
    "end": "2068878"
  },
  {
    "text": "user clusters so we moved the scds to the new cloud and using sct quorum for the data",
    "start": "2068879",
    "end": "2076079"
  },
  {
    "text": "migration and um yeah for sure we should have uh backups and recovery for all the",
    "start": "2076079",
    "end": "2082240"
  },
  {
    "text": "components how does this look like for example here we have our seed",
    "start": "2082240",
    "end": "2087520"
  },
  {
    "text": "jns for our own api server what's projecting these remasters there we have our now",
    "start": "2087520",
    "end": "2093280"
  },
  {
    "text": "our aws clusters hosted the control plane and we have the workers who are hosting",
    "start": "2093280",
    "end": "2098560"
  },
  {
    "text": "security voice control plane every single gcp then as fast first step",
    "start": "2098560",
    "end": "2103599"
  },
  {
    "text": "we would again place the vpn that time in the c cluster",
    "start": "2103599",
    "end": "2108880"
  },
  {
    "text": "connecting a reworker node and masternodes then uh create a quorum with five",
    "start": "2108880",
    "end": "2116560"
  },
  {
    "text": "hcd for the masternodes that ensures that the new two lcds get the data replicated from",
    "start": "2116560",
    "end": "2123599"
  },
  {
    "text": "gcp nodes to the aws nodes then as next step if we have achieved that we",
    "start": "2123599",
    "end": "2129599"
  },
  {
    "text": "can remove two gcp workers and still have the quorum of three so three or five if",
    "start": "2129599",
    "end": "2135520"
  },
  {
    "text": "enough is enough for the quorum and we can therefore also change now the uh the the dns name",
    "start": "2135520",
    "end": "2142560"
  },
  {
    "text": "to the new aws club provider load balancer so there now we can then remove the",
    "start": "2142560",
    "end": "2151119"
  },
  {
    "text": "quorum also from five to three and we have a stable control plane",
    "start": "2151119",
    "end": "2158880"
  },
  {
    "text": "um for sure also in this project procedure uh we need then to clean up",
    "start": "2158880",
    "end": "2165599"
  },
  {
    "text": "remove the old gsp node again create a new idea by s master node and then we have three lcds uh healthy",
    "start": "2165599",
    "end": "2173440"
  },
  {
    "text": "in the new cloud migrated in the same way as we migrated to user clusters workers",
    "start": "2173440",
    "end": "2180720"
  },
  {
    "text": "we now need to migrate the c-cluster workers for that um we",
    "start": "2180720",
    "end": "2186160"
  },
  {
    "text": "here can change the um vpn again create a new worker nodes and then in",
    "start": "2186160",
    "end": "2193760"
  },
  {
    "text": "this kind also changing here at the cloud provider here that it's pointing now to aws",
    "start": "2193760",
    "end": "2201040"
  },
  {
    "text": "after this is happened we can increase here uh in chromatic settings the",
    "start": "2201040",
    "end": "2206880"
  },
  {
    "text": "default value of entity replicas to five that means that the programmatic user",
    "start": "2206880",
    "end": "2211920"
  },
  {
    "text": "cluster controller manager creates two new entities for every user cluster",
    "start": "2211920",
    "end": "2217040"
  },
  {
    "text": "that's insurers in the same ways as me migrated the lcd on the top it's migrating also for the for the",
    "start": "2217040",
    "end": "2223680"
  },
  {
    "text": "users clusters so i have now a quorum of five",
    "start": "2223680",
    "end": "2228800"
  },
  {
    "text": "it is running but still pointing to the old dns but between clouds as next step um",
    "start": "2228800",
    "end": "2237040"
  },
  {
    "text": "we now creating a new cloud balance load balancer and rename the white card",
    "start": "2237040",
    "end": "2242800"
  },
  {
    "text": "so that's the connection from the user cluster workers what we already migrated to the new",
    "start": "2242800",
    "end": "2248560"
  },
  {
    "text": "cloud load balancer and yes then we switch over and we",
    "start": "2248560",
    "end": "2255599"
  },
  {
    "text": "adding one new aws node replacing all gcp nodes so we have then",
    "start": "2255599",
    "end": "2260720"
  },
  {
    "text": "a quorum of three entities per user cluster on the new cloud that is the most reasonable target so",
    "start": "2260720",
    "end": "2267520"
  },
  {
    "text": "now we are safe and we can remove the old missingno working roads",
    "start": "2267520",
    "end": "2273520"
  },
  {
    "text": "uh clean up the old cloud resources that means we scale down to lcd level",
    "start": "2273520",
    "end": "2279359"
  },
  {
    "text": "three we um remove then the vpn overlay and um",
    "start": "2279359",
    "end": "2287119"
  },
  {
    "text": "we now have the kernel away what's routing between the workers again between eth0",
    "start": "2287119",
    "end": "2294880"
  },
  {
    "text": "so then we have successfully migrated every user cluster into seed cluster to the new cloud so that is",
    "start": "2294880",
    "end": "2302240"
  },
  {
    "text": "the approach what we think are is possible to migrate really 100 clusters without downtime",
    "start": "2302240",
    "end": "2308800"
  },
  {
    "text": "and in a scalable way so for sure we are not right now that we just press a button",
    "start": "2308800",
    "end": "2314240"
  },
  {
    "text": "but that's our future target um so we want to automate also",
    "start": "2314240",
    "end": "2319839"
  },
  {
    "text": "um the clean approach procedure and for sure if you want to really make it scalable we need to write",
    "start": "2319839",
    "end": "2325440"
  },
  {
    "text": "an operator currently we did everything by a yeah hacky help uh um bash script",
    "start": "2325440",
    "end": "2332240"
  },
  {
    "text": "um and to not like yeah doing that by hand the health checks we need to have an operator who check the house check",
    "start": "2332240",
    "end": "2338880"
  },
  {
    "text": "the conditions and have also some repair options and retry options but therefore the do we need this",
    "start": "2338880",
    "end": "2345200"
  },
  {
    "text": "reconsidering pattern match in the same way as is match for our cloud migration where the",
    "start": "2345200",
    "end": "2350480"
  },
  {
    "text": "chromatic controller creates the new cloud provider and so on we can use the",
    "start": "2350480",
    "end": "2356720"
  },
  {
    "text": "reconciling also for the migration and using okay we have a new target cloud",
    "start": "2356720",
    "end": "2362400"
  },
  {
    "text": "operator take care about that matching the old state to the new state",
    "start": "2362400",
    "end": "2367520"
  },
  {
    "text": "um technical we also need to stabilize the vpn connection so um right now we have only one vpn server",
    "start": "2367520",
    "end": "2375200"
  },
  {
    "text": "this could be maybe on a bottleneck on bigger clusters maybe we should deploy multiple vpn",
    "start": "2375200",
    "end": "2381359"
  },
  {
    "text": "clusters and we have uh like maybe uh have also more soft switch",
    "start": "2381359",
    "end": "2388400"
  },
  {
    "text": "between vpn and host networking overlay that's something what we need to explore more um",
    "start": "2388400",
    "end": "2394079"
  },
  {
    "text": "and maybe their wire card can be an alternative as a vpn connection",
    "start": "2394079",
    "end": "2399280"
  },
  {
    "text": "what could help maybe on the setup okay um one more detail here currently",
    "start": "2399280",
    "end": "2405280"
  },
  {
    "text": "we tested it in a 1.17 seat cluster where the manage fields",
    "start": "2405280",
    "end": "2410319"
  },
  {
    "text": "feature is not included somehow this makes a little bit of trouble between okay you have an like patch by cube",
    "start": "2410319",
    "end": "2417440"
  },
  {
    "text": "control and you have an operator what reconsidering the fields so that's why currently i'm a little bit limited",
    "start": "2417440",
    "end": "2423680"
  },
  {
    "text": "on the 117 seed clusters as you saw in the demo uh the user cluster can be on 118.",
    "start": "2423680",
    "end": "2431359"
  },
  {
    "text": "cool then um yeah thanks for your attention uh we are happy to answer any questions",
    "start": "2431359",
    "end": "2438079"
  },
  {
    "text": "so feel free to reach out now and yeah thanks to listening and yeah manuel a few last words from",
    "start": "2438079",
    "end": "2445520"
  },
  {
    "text": "your side uh no that was amazing thank you so much and we're open to questions now",
    "start": "2445520",
    "end": "2453200"
  },
  {
    "text": "okay thanks a lot and looking forward to your questions",
    "start": "2453200",
    "end": "2460318"
  }
]