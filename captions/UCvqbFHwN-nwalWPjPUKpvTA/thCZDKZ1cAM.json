[
  {
    "text": "and with that I think we're going to get",
    "start": "960",
    "end": "2720"
  },
  {
    "text": "started we're going to take like a",
    "start": "2720",
    "end": "3840"
  },
  {
    "text": "10-second lead on this whole situation",
    "start": "3840",
    "end": "5520"
  },
  {
    "text": "and we're going to get started and the",
    "start": "5520",
    "end": "6960"
  },
  {
    "text": "talk of the title of the talk is yes you",
    "start": "6960",
    "end": "9120"
  },
  {
    "text": "can run LLMs on Kubernetes who here was",
    "start": "9120",
    "end": "11920"
  },
  {
    "text": "in this room for the last talk anybody",
    "start": "11920",
    "end": "13759"
  },
  {
    "text": "stayed around for the last talk and uh",
    "start": "13759",
    "end": "16000"
  },
  {
    "text": "in general who here knows that you can",
    "start": "16000",
    "end": "18240"
  },
  {
    "text": "run LMS on",
    "start": "18240",
    "end": "19880"
  },
  {
    "text": "Kubernetes all right so yeah then then",
    "start": "19880",
    "end": "22320"
  },
  {
    "text": "you don't need the talk it's good you",
    "start": "22320",
    "end": "24000"
  },
  {
    "text": "all know that's great you can just tell",
    "start": "24000",
    "end": "25519"
  },
  {
    "text": "other folks and I didn't need to join a",
    "start": "25519",
    "end": "26880"
  },
  {
    "text": "call now that's good",
    "start": "26880",
    "end": "28720"
  },
  {
    "text": "jump jump back in yeah so all of you",
    "start": "28720",
    "end": "30720"
  },
  {
    "text": "know already but let's do the talk my",
    "start": "30720",
    "end": "33120"
  },
  {
    "text": "name is Mi and I have my friend here",
    "start": "33120",
    "end": "35440"
  },
  {
    "text": "Abdel and here today we're talking about",
    "start": "35440",
    "end": "37840"
  },
  {
    "text": "it so for those of you who already knew",
    "start": "37840",
    "end": "39520"
  },
  {
    "text": "that you could run LLMs on uh Kubernetes",
    "start": "39520",
    "end": "43200"
  },
  {
    "text": "thank you you can now leave the room",
    "start": "43200",
    "end": "45200"
  },
  {
    "text": "yeah that's that's good but for the rest",
    "start": "45200",
    "end": "48160"
  },
  {
    "text": "of you um I mean we're here to talk",
    "start": "48160",
    "end": "51120"
  },
  {
    "text": "about LLMs because Jai have been",
    "start": "51120",
    "end": "52879"
  },
  {
    "text": "evolving over the last few years right",
    "start": "52879",
    "end": "55280"
  },
  {
    "text": "and you all know about all these like",
    "start": "55280",
    "end": "57760"
  },
  {
    "text": "very cool big models hosted by cloud",
    "start": "57760",
    "end": "59920"
  },
  {
    "text": "providers that can do thousands and",
    "start": "59920",
    "end": "61520"
  },
  {
    "text": "thousands and millions of windows",
    "start": "61520",
    "end": "63120"
  },
  {
    "text": "context windows but we're here",
    "start": "63120",
    "end": "64720"
  },
  {
    "text": "Kubernetes nerds and we don't care about",
    "start": "64720",
    "end": "66720"
  },
  {
    "text": "cloud hosted stuff we want to host stuff",
    "start": "66720",
    "end": "68600"
  },
  {
    "text": "ourself and most of you will probably be",
    "start": "68600",
    "end": "71119"
  },
  {
    "text": "hosting open LMS right which are also",
    "start": "71119",
    "end": "73840"
  },
  {
    "text": "getting big um the open the deepseek",
    "start": "73840",
    "end": "76880"
  },
  {
    "text": "model has uh the R1 has 671 billion",
    "start": "76880",
    "end": "80400"
  },
  {
    "text": "parameters it's a 800 gigabyte model in",
    "start": "80400",
    "end": "83360"
  },
  {
    "text": "a single file um and then all the other",
    "start": "83360",
    "end": "86159"
  },
  {
    "text": "models that exist in the open world we",
    "start": "86159",
    "end": "88720"
  },
  {
    "text": "actually were debating whether we should",
    "start": "88720",
    "end": "90240"
  },
  {
    "text": "keep open source because most of them",
    "start": "90240",
    "end": "91439"
  },
  {
    "text": "are not they're open weight but most of",
    "start": "91439",
    "end": "93439"
  },
  {
    "text": "these openweight models are getting big",
    "start": "93439",
    "end": "95360"
  },
  {
    "text": "they're getting complex they're huge",
    "start": "95360",
    "end": "97600"
  },
  {
    "text": "putting them in production is not",
    "start": "97600",
    "end": "100200"
  },
  {
    "text": "easy um a very wise person Clayton",
    "start": "100200",
    "end": "103040"
  },
  {
    "text": "Coleman had this statement saying \"LM is",
    "start": "103040",
    "end": "105920"
  },
  {
    "text": "the new web application.\" Yeah and if",
    "start": "105920",
    "end": "108640"
  },
  {
    "text": "you are in the conference until Friday",
    "start": "108640",
    "end": "110479"
  },
  {
    "text": "Clayton actually has a keynote on Friday",
    "start": "110479",
    "end": "113600"
  },
  {
    "text": "and I have gotten a sneak peek that he",
    "start": "113600",
    "end": "115280"
  },
  {
    "text": "might actually update this statement so",
    "start": "115280",
    "end": "116640"
  },
  {
    "text": "if you want to be the first to know what",
    "start": "116640",
    "end": "118079"
  },
  {
    "text": "the new statement what LLM is you should",
    "start": "118079",
    "end": "120560"
  },
  {
    "text": "check out the uh keynote from Clayton on",
    "start": "120560",
    "end": "122719"
  },
  {
    "text": "Friday but let's continue on uh when you",
    "start": "122719",
    "end": "124960"
  },
  {
    "text": "want to run LLMs on the cloud you",
    "start": "124960",
    "end": "126719"
  },
  {
    "text": "definitely have a lot of options right",
    "start": "126719",
    "end": "128080"
  },
  {
    "text": "so you could do the old school bare",
    "start": "128080",
    "end": "129759"
  },
  {
    "text": "metal your server or VMs manage it all",
    "start": "129759",
    "end": "131840"
  },
  {
    "text": "yourself and all the way on the other",
    "start": "131840",
    "end": "133520"
  },
  {
    "text": "side automated versions like Vertex AI",
    "start": "133520",
    "end": "135920"
  },
  {
    "text": "you have Bedrock you have Asure Managed",
    "start": "135920",
    "end": "138000"
  },
  {
    "text": "Services or many other managed services",
    "start": "138000",
    "end": "140400"
  },
  {
    "text": "i believe Kubernetes sits somewhere in",
    "start": "140400",
    "end": "142080"
  },
  {
    "text": "the middle where it gives you kind of",
    "start": "142080",
    "end": "143440"
  },
  {
    "text": "the control of managing your platform",
    "start": "143440",
    "end": "145360"
  },
  {
    "text": "and your infrastructure but also gives",
    "start": "145360",
    "end": "147520"
  },
  {
    "text": "you the flexibility of autoscaling and",
    "start": "147520",
    "end": "149360"
  },
  {
    "text": "scaling to maximum and scaling down",
    "start": "149360",
    "end": "151280"
  },
  {
    "text": "getting access to a lot of resources",
    "start": "151280",
    "end": "152720"
  },
  {
    "text": "different resource types uh so I think",
    "start": "152720",
    "end": "155040"
  },
  {
    "text": "Kubernetes kind of does the best of both",
    "start": "155040",
    "end": "156959"
  },
  {
    "text": "worlds in many ways",
    "start": "156959",
    "end": "159280"
  },
  {
    "text": "and the reason is because we've been",
    "start": "159280",
    "end": "161280"
  },
  {
    "text": "using Kubernetes for web applications so",
    "start": "161280",
    "end": "163360"
  },
  {
    "text": "far right and it's great at doing that",
    "start": "163360",
    "end": "165040"
  },
  {
    "text": "all the stuff that MPI talked about the",
    "start": "165040",
    "end": "166800"
  },
  {
    "text": "automation the scalability but also the",
    "start": "166800",
    "end": "168959"
  },
  {
    "text": "device management the working group",
    "start": "168959",
    "end": "170319"
  },
  {
    "text": "device management have been doing a lot",
    "start": "170319",
    "end": "171680"
  },
  {
    "text": "of great job and the talk that was",
    "start": "171680",
    "end": "173040"
  },
  {
    "text": "before us was actually explaining in",
    "start": "173040",
    "end": "174560"
  },
  {
    "text": "details how the device plug-in can help",
    "start": "174560",
    "end": "176640"
  },
  {
    "text": "you plug a GPU into a node and make sure",
    "start": "176640",
    "end": "179360"
  },
  {
    "text": "that your pod gets scheduled on the",
    "start": "179360",
    "end": "180879"
  },
  {
    "text": "right node the right GPU one of In my",
    "start": "180879",
    "end": "184000"
  },
  {
    "text": "opinion one of the most important things",
    "start": "184000",
    "end": "185040"
  },
  {
    "text": "that Kubernetes brings is the multi",
    "start": "185040",
    "end": "186800"
  },
  {
    "text": "cloud capabilities being able to deploy",
    "start": "186800",
    "end": "188879"
  },
  {
    "text": "it in any cloud provider you want and",
    "start": "188879",
    "end": "190800"
  },
  {
    "text": "also having the actually the this the",
    "start": "190800",
    "end": "193519"
  },
  {
    "text": "kind of slide that uh Miy showed where",
    "start": "193519",
    "end": "195680"
  },
  {
    "text": "you have all the way manage yourself",
    "start": "195680",
    "end": "197680"
  },
  {
    "text": "kind of bare metal to all the way using",
    "start": "197680",
    "end": "199440"
  },
  {
    "text": "a managed service if we take Kubernetes",
    "start": "199440",
    "end": "201360"
  },
  {
    "text": "itself we can also expand it because you",
    "start": "201360",
    "end": "203599"
  },
  {
    "text": "can also do all the way manage",
    "start": "203599",
    "end": "204879"
  },
  {
    "text": "Kubernetes by a cloud provider to all",
    "start": "204879",
    "end": "206400"
  },
  {
    "text": "the way manage it yourself um the the",
    "start": "206400",
    "end": "209120"
  },
  {
    "text": "the value of Kubernetes is the standard",
    "start": "209120",
    "end": "211280"
  },
  {
    "text": "single API for doing everything all",
    "start": "211280",
    "end": "214080"
  },
  {
    "text": "right so let's see some of this in",
    "start": "214080",
    "end": "216400"
  },
  {
    "text": "action right so uh conference internet",
    "start": "216400",
    "end": "219120"
  },
  {
    "text": "willing we'll get to see some of these",
    "start": "219120",
    "end": "220480"
  },
  {
    "text": "things in live if it doesn't we'll just",
    "start": "220480",
    "end": "222000"
  },
  {
    "text": "fall back onto and act like it that was",
    "start": "222000",
    "end": "223920"
  },
  {
    "text": "the plan all along yeah we rely on you",
    "start": "223920",
    "end": "226000"
  },
  {
    "text": "to to just Yeah just like act like it",
    "start": "226000",
    "end": "227920"
  },
  {
    "text": "was the plan so the first things first",
    "start": "227920",
    "end": "229840"
  },
  {
    "text": "I'm going to try to write an YAML uh",
    "start": "229840",
    "end": "231440"
  },
  {
    "text": "that's my to-do write fail writing this",
    "start": "231440",
    "end": "233120"
  },
  {
    "text": "YAML from scratch and see how that goes",
    "start": "233120",
    "end": "234959"
  },
  {
    "text": "uh so I so we're going to create a",
    "start": "234959",
    "end": "237840"
  },
  {
    "text": "Kubernetes deployment that is going to",
    "start": "237840",
    "end": "239360"
  },
  {
    "text": "deploy a large language model on a",
    "start": "239360",
    "end": "241120"
  },
  {
    "text": "cluster with GPU uh we work for Google",
    "start": "241120",
    "end": "244400"
  },
  {
    "text": "so we have access to GKE Google's",
    "start": "244400",
    "end": "246080"
  },
  {
    "text": "managed Kubernetes services we're also",
    "start": "246080",
    "end": "247840"
  },
  {
    "text": "going to use something called GK",
    "start": "247840",
    "end": "249120"
  },
  {
    "text": "autopilot that can automatically",
    "start": "249120",
    "end": "250640"
  },
  {
    "text": "provision a new node with the GPU on",
    "start": "250640",
    "end": "252879"
  },
  {
    "text": "demand instead of us having to",
    "start": "252879",
    "end": "254080"
  },
  {
    "text": "pre-provision the node so we're going to",
    "start": "254080",
    "end": "255519"
  },
  {
    "text": "do all of that real time so create a",
    "start": "255519",
    "end": "257280"
  },
  {
    "text": "deployment i'm going to let the",
    "start": "257280",
    "end": "258880"
  },
  {
    "text": "Kubernetes uh uh it's called ID",
    "start": "258880",
    "end": "262000"
  },
  {
    "text": "extension on VS code that lets me just",
    "start": "262000",
    "end": "264320"
  },
  {
    "text": "create a scaffold deployment so let's",
    "start": "264320",
    "end": "266160"
  },
  {
    "text": "give it a name the model I'm deploying",
    "start": "266160",
    "end": "267680"
  },
  {
    "text": "is the Gemma uh three 1 billion",
    "start": "267680",
    "end": "270479"
  },
  {
    "text": "parameter model so I'm going to give",
    "start": "270479",
    "end": "271919"
  },
  {
    "text": "that name JMA 3 1B it instruction tune",
    "start": "271919",
    "end": "275360"
  },
  {
    "text": "model and then I'm going to name the",
    "start": "275360",
    "end": "277360"
  },
  {
    "text": "deployment deployment because I have too",
    "start": "277360",
    "end": "279919"
  },
  {
    "text": "many of these things and I forget which",
    "start": "279919",
    "end": "281120"
  },
  {
    "text": "one is what and then I'm going to need",
    "start": "281120",
    "end": "283360"
  },
  {
    "text": "an image and I'm going to not going to",
    "start": "283360",
    "end": "284800"
  },
  {
    "text": "memorize this image i'm going to just",
    "start": "284800",
    "end": "285919"
  },
  {
    "text": "copy it from one of the other",
    "start": "285919",
    "end": "286800"
  },
  {
    "text": "deployments that I have here uh so these",
    "start": "286800",
    "end": "288720"
  },
  {
    "text": "are images that we provide as part of",
    "start": "288720",
    "end": "290320"
  },
  {
    "text": "our Vertex you could also grab any open",
    "start": "290320",
    "end": "292720"
  },
  {
    "text": "image so we're using VLM as the serving",
    "start": "292720",
    "end": "294479"
  },
  {
    "text": "engine right now what that is we're",
    "start": "294479",
    "end": "296080"
  },
  {
    "text": "going to talk about in a second but I'm",
    "start": "296080",
    "end": "297440"
  },
  {
    "text": "going to use that image to get us",
    "start": "297440",
    "end": "298960"
  },
  {
    "text": "started next up I need resources so",
    "start": "298960",
    "end": "301759"
  },
  {
    "text": "we're talking about uh deploying a large",
    "start": "301759",
    "end": "303680"
  },
  {
    "text": "language model so the type of resource",
    "start": "303680",
    "end": "305520"
  },
  {
    "text": "we need are GPUs so let's give it some",
    "start": "305520",
    "end": "308320"
  },
  {
    "text": "memory so let's say about 20 GB of",
    "start": "308320",
    "end": "310880"
  },
  {
    "text": "memory uh capital G don't forget that",
    "start": "310880",
    "end": "313280"
  },
  {
    "text": "one uh I'm going to need about four CPU",
    "start": "313280",
    "end": "316320"
  },
  {
    "text": "and",
    "start": "316320",
    "end": "317720"
  },
  {
    "text": "Nvidia.comGPU1 and I'm also going to",
    "start": "317720",
    "end": "319360"
  },
  {
    "text": "need some ephemeral storage because I'm",
    "start": "319360",
    "end": "320800"
  },
  {
    "text": "downloading lots of data i want to store",
    "start": "320800",
    "end": "322160"
  },
  {
    "text": "them on my device somewhere so I'm going",
    "start": "322160",
    "end": "323520"
  },
  {
    "text": "to give some ephemeral storage about 20",
    "start": "323520",
    "end": "324960"
  },
  {
    "text": "GB that's my limit i'm going to copy the",
    "start": "324960",
    "end": "327120"
  },
  {
    "text": "same thing for",
    "start": "327120",
    "end": "328600"
  },
  {
    "text": "request and should help me out there a",
    "start": "328600",
    "end": "331600"
  },
  {
    "text": "little bit okay here we go autocomplete",
    "start": "331600",
    "end": "333120"
  },
  {
    "text": "is so great these days so I don't have",
    "start": "333120",
    "end": "334479"
  },
  {
    "text": "to it's pretty good and I'm going to",
    "start": "334479",
    "end": "336720"
  },
  {
    "text": "copy a bunch of some a couple other",
    "start": "336720",
    "end": "338320"
  },
  {
    "text": "things from my other file that I have",
    "start": "338320",
    "end": "340240"
  },
  {
    "text": "because I don't want to watch you all uh",
    "start": "340240",
    "end": "343199"
  },
  {
    "text": "fail uh watch you all watch me fail",
    "start": "343199",
    "end": "346320"
  },
  {
    "text": "typing that's a long sentence i'm going",
    "start": "346320",
    "end": "348560"
  },
  {
    "text": "to just copy this command right",
    "start": "348560",
    "end": "350440"
  },
  {
    "text": "here all right so the command basically",
    "start": "350440",
    "end": "353199"
  },
  {
    "text": "is I have this VLM based image which",
    "start": "353199",
    "end": "355360"
  },
  {
    "text": "happens to have have a Python file i'm",
    "start": "355360",
    "end": "357280"
  },
  {
    "text": "just running that with some arguments so",
    "start": "357280",
    "end": "358800"
  },
  {
    "text": "the arguments are my model ID tensor",
    "start": "358800",
    "end": "361440"
  },
  {
    "text": "parallel size is how many shard I'm",
    "start": "361440",
    "end": "362800"
  },
  {
    "text": "doing the model it's a small enough",
    "start": "362800",
    "end": "364000"
  },
  {
    "text": "model that fits in a single GPU i only",
    "start": "364000",
    "end": "366000"
  },
  {
    "text": "need parall size one host is just where",
    "start": "366000",
    "end": "368560"
  },
  {
    "text": "I'm running which uh host I'm running it",
    "start": "368560",
    "end": "370400"
  },
  {
    "text": "at and the port is 8,000 and I max",
    "start": "370400",
    "end": "373280"
  },
  {
    "text": "length is 32,000 uh tokens so uh Gemma",
    "start": "373280",
    "end": "377360"
  },
  {
    "text": "31B has a maxed um context window of",
    "start": "377360",
    "end": "380560"
  },
  {
    "text": "32,000 tokens so let's do that you also",
    "start": "380560",
    "end": "384000"
  },
  {
    "text": "see I have I'm u referencing a",
    "start": "384000",
    "end": "386400"
  },
  {
    "text": "environment variable here which I have",
    "start": "386400",
    "end": "387759"
  },
  {
    "text": "to create now which I'll also just go",
    "start": "387759",
    "end": "389600"
  },
  {
    "text": "ahead and copy from here which I'm going",
    "start": "389600",
    "end": "391759"
  },
  {
    "text": "to",
    "start": "391759",
    "end": "393319"
  },
  {
    "text": "need and okay so I have this environment",
    "start": "393319",
    "end": "396039"
  },
  {
    "text": "variable called model ID and I'm going",
    "start": "396039",
    "end": "398400"
  },
  {
    "text": "to name it 1d bit and this is the same",
    "start": "398400",
    "end": "401360"
  },
  {
    "text": "name you're going to need we're getting",
    "start": "401360",
    "end": "402720"
  },
  {
    "text": "this from hugging face so if you go to",
    "start": "402720",
    "end": "404000"
  },
  {
    "text": "hugging face you will see like different",
    "start": "404000",
    "end": "405199"
  },
  {
    "text": "model ID names we're going to copy that",
    "start": "405199",
    "end": "406880"
  },
  {
    "text": "one and one last thing I need here is",
    "start": "406880",
    "end": "408319"
  },
  {
    "text": "the hugging face hub token because Gemma",
    "start": "408319",
    "end": "410800"
  },
  {
    "text": "like many other models like mistrol or",
    "start": "410800",
    "end": "412720"
  },
  {
    "text": "llama is a gated model so you're going",
    "start": "412720",
    "end": "414240"
  },
  {
    "text": "to hear this term gated model a lot is",
    "start": "414240",
    "end": "416080"
  },
  {
    "text": "that these models are stored in hugging",
    "start": "416080",
    "end": "418000"
  },
  {
    "text": "face or Kaggle and you have to sign some",
    "start": "418000",
    "end": "419919"
  },
  {
    "text": "sort of consent to say I am willing to",
    "start": "419919",
    "end": "422000"
  },
  {
    "text": "like sign this consent so that I can use",
    "start": "422000",
    "end": "423440"
  },
  {
    "text": "this model so I'm going to have my API",
    "start": "423440",
    "end": "425280"
  },
  {
    "text": "token which I already create the secret",
    "start": "425280",
    "end": "426639"
  },
  {
    "text": "for so I'm going to use that and the",
    "start": "426639",
    "end": "428240"
  },
  {
    "text": "last two things I need are volume mounts",
    "start": "428240",
    "end": "430240"
  },
  {
    "text": "and volumes because I want to store this",
    "start": "430240",
    "end": "432560"
  },
  {
    "text": "model in a local storage as I download",
    "start": "432560",
    "end": "434560"
  },
  {
    "text": "that so I'm going to create that volume",
    "start": "434560",
    "end": "435919"
  },
  {
    "text": "mount right",
    "start": "435919",
    "end": "437000"
  },
  {
    "text": "here",
    "start": "437000",
    "end": "439240"
  },
  {
    "text": "token all right and the last couple of",
    "start": "439240",
    "end": "441520"
  },
  {
    "text": "things is I need to have a port and as",
    "start": "441520",
    "end": "444400"
  },
  {
    "text": "you saw before I was uh setting the port",
    "start": "444400",
    "end": "446560"
  },
  {
    "text": "as 8,000 so I'm going to do the same",
    "start": "446560",
    "end": "448080"
  },
  {
    "text": "thing as my port",
    "start": "448080",
    "end": "450120"
  },
  {
    "text": "here and this is the last part I need to",
    "start": "450120",
    "end": "452880"
  },
  {
    "text": "be able to say this workload should uh",
    "start": "452880",
    "end": "455599"
  },
  {
    "text": "run on a specific type of nodes in",
    "start": "455599",
    "end": "457440"
  },
  {
    "text": "autopilot what we do in GK autopilot is",
    "start": "457440",
    "end": "460000"
  },
  {
    "text": "depending on the node selector we can",
    "start": "460000",
    "end": "461680"
  },
  {
    "text": "create the specific type of node for you",
    "start": "461680",
    "end": "463840"
  },
  {
    "text": "so we will go ahead and set that node",
    "start": "463840",
    "end": "465520"
  },
  {
    "text": "selector in regular Kubernetes you can",
    "start": "465520",
    "end": "468080"
  },
  {
    "text": "do the same thing because again this is",
    "start": "468080",
    "end": "469520"
  },
  {
    "text": "a special type of device we're using",
    "start": "469520",
    "end": "470880"
  },
  {
    "text": "which is GPU we want to make sure this",
    "start": "470880",
    "end": "472560"
  },
  {
    "text": "workload run against a specific type of",
    "start": "472560",
    "end": "474639"
  },
  {
    "text": "node so we'll copy that node selector",
    "start": "474639",
    "end": "477599"
  },
  {
    "text": "copy that in there so that's my",
    "start": "477599",
    "end": "479440"
  },
  {
    "text": "deployment done hopefully and then I'm",
    "start": "479440",
    "end": "481120"
  },
  {
    "text": "going to need a service so that I can",
    "start": "481120",
    "end": "482479"
  },
  {
    "text": "expose this deployment and so that I can",
    "start": "482479",
    "end": "484800"
  },
  {
    "text": "talk to this model and I'm going to",
    "start": "484800",
    "end": "486879"
  },
  {
    "text": "create a service and it's going to need",
    "start": "486879",
    "end": "489199"
  },
  {
    "text": "a name so I'm going to call it Gemma",
    "start": "489199",
    "end": "492759"
  },
  {
    "text": "31BT service and the selector is going",
    "start": "492759",
    "end": "495599"
  },
  {
    "text": "to be Gemma 3 1B oh I called it IB no",
    "start": "495599",
    "end": "499919"
  },
  {
    "text": "one caught it yet change this one to Yep",
    "start": "499919",
    "end": "503360"
  },
  {
    "text": "yep one uh one here you go and the port",
    "start": "503360",
    "end": "506400"
  },
  {
    "text": "is I'm going to expose the port 8000 and",
    "start": "506400",
    "end": "508639"
  },
  {
    "text": "I'm going to target port 8000 as well on",
    "start": "508639",
    "end": "512320"
  },
  {
    "text": "my uh deployment so with everything",
    "start": "512320",
    "end": "514479"
  },
  {
    "text": "there hopefully I did everything right",
    "start": "514479",
    "end": "517039"
  },
  {
    "text": "if not we'll find out together",
    "start": "517039",
    "end": "519680"
  },
  {
    "text": "so cube control apply -f k8s the file is",
    "start": "519680",
    "end": "526000"
  },
  {
    "text": "gemma",
    "start": "526000",
    "end": "528040"
  },
  {
    "text": "31b 1b it uh yeah it",
    "start": "528040",
    "end": "532440"
  },
  {
    "text": "vlm.ymol send this",
    "start": "532440",
    "end": "535080"
  },
  {
    "text": "in what happened oh",
    "start": "535080",
    "end": "538680"
  },
  {
    "text": "boy oh server doesn't have a v1 so I",
    "start": "538680",
    "end": "541519"
  },
  {
    "text": "maybe copied the server wrong or not",
    "start": "541519",
    "end": "547040"
  },
  {
    "text": "uh it's a port problem it's a port",
    "start": "547040",
    "end": "549760"
  },
  {
    "text": "problem",
    "start": "549760",
    "end": "551600"
  },
  {
    "text": "oh volume zero ports you know what cut",
    "start": "551600",
    "end": "554640"
  },
  {
    "text": "my losses copy this in and change things",
    "start": "554640",
    "end": "559519"
  },
  {
    "text": "yeah let's just make it simple yeah and",
    "start": "559519",
    "end": "562880"
  },
  {
    "text": "change all the names to uh",
    "start": "562880",
    "end": "566279"
  },
  {
    "text": "1B here you go one B here one here one B",
    "start": "566279",
    "end": "570160"
  },
  {
    "text": "here one B here one B here one B here",
    "start": "570160",
    "end": "575279"
  },
  {
    "text": "the joy of debugging YAML oh yeah i",
    "start": "575279",
    "end": "578080"
  },
  {
    "text": "don't have my YAML engineer hat the",
    "start": "578080",
    "end": "579680"
  },
  {
    "text": "service change the service no service is",
    "start": "579680",
    "end": "581519"
  },
  {
    "text": "the same thing all right cool okay test",
    "start": "581519",
    "end": "584160"
  },
  {
    "text": "it again that should work maybe hey",
    "start": "584160",
    "end": "587839"
  },
  {
    "text": "there we go good so I just ran this",
    "start": "587839",
    "end": "590240"
  },
  {
    "text": "workload let's check what is happening",
    "start": "590240",
    "end": "591760"
  },
  {
    "text": "with this i'm going to do a quick watch",
    "start": "591760",
    "end": "593360"
  },
  {
    "text": "cube",
    "start": "593360",
    "end": "594600"
  },
  {
    "text": "control get PL and we're just pending so",
    "start": "594600",
    "end": "598320"
  },
  {
    "text": "what is happening right now we are",
    "start": "598320",
    "end": "599519"
  },
  {
    "text": "sending off this workload and our GKA",
    "start": "599519",
    "end": "602000"
  },
  {
    "text": "controller is going to go ahead and",
    "start": "602000",
    "end": "603200"
  },
  {
    "text": "create a new node schedule my workload",
    "start": "603200",
    "end": "605279"
  },
  {
    "text": "and run this so while that happens we're",
    "start": "605279",
    "end": "606880"
  },
  {
    "text": "going to continue on we're going to come",
    "start": "606880",
    "end": "607920"
  },
  {
    "text": "back to see what has happened here and",
    "start": "607920",
    "end": "609440"
  },
  {
    "text": "some of the other fun stuff we could do",
    "start": "609440",
    "end": "610640"
  },
  {
    "text": "with this yeah so one thing you might",
    "start": "610640",
    "end": "614240"
  },
  {
    "text": "have noticed through this kind of",
    "start": "614240",
    "end": "616079"
  },
  {
    "text": "example of very simple deployment is we",
    "start": "616079",
    "end": "618880"
  },
  {
    "text": "said initially it's like LLMs are the",
    "start": "618880",
    "end": "621920"
  },
  {
    "text": "new web app but it's not a typical web",
    "start": "621920",
    "end": "623519"
  },
  {
    "text": "app this requires 20 GB of memory and",
    "start": "623519",
    "end": "626320"
  },
  {
    "text": "four CPUs and the GPU you don't need",
    "start": "626320",
    "end": "628560"
  },
  {
    "text": "this for web application right and also",
    "start": "628560",
    "end": "630720"
  },
  {
    "text": "requires storage requires access to some",
    "start": "630720",
    "end": "632720"
  },
  {
    "text": "sort of remote endpoint to download",
    "start": "632720",
    "end": "634240"
  },
  {
    "text": "weights so they are web applications are",
    "start": "634240",
    "end": "637360"
  },
  {
    "text": "in the sense that Kubernetes doesn't",
    "start": "637360",
    "end": "638800"
  },
  {
    "text": "really care it just here is a an a",
    "start": "638800",
    "end": "641440"
  },
  {
    "text": "workload give me some resources and",
    "start": "641440",
    "end": "643040"
  },
  {
    "text": "Kubernetes will allocate the resources",
    "start": "643040",
    "end": "644880"
  },
  {
    "text": "but they require some extra",
    "start": "644880",
    "end": "647399"
  },
  {
    "text": "stuff before we move on we figured we'd",
    "start": "647399",
    "end": "650320"
  },
  {
    "text": "spend a little bit of time because",
    "start": "650320",
    "end": "651519"
  },
  {
    "text": "you're going to be hearing LLMs and a",
    "start": "651519",
    "end": "653200"
  },
  {
    "text": "bunch of these words through in this",
    "start": "653200",
    "end": "654480"
  },
  {
    "text": "entire conference so we spend a little",
    "start": "654480",
    "end": "656640"
  },
  {
    "text": "bit of time kind of equipping you with",
    "start": "656640",
    "end": "658160"
  },
  {
    "text": "some basics right so LLM I think",
    "start": "658160",
    "end": "660720"
  },
  {
    "text": "everybody knows what that's a large",
    "start": "660720",
    "end": "661920"
  },
  {
    "text": "language model it's a thing that can",
    "start": "661920",
    "end": "663760"
  },
  {
    "text": "speak words basically right very simple",
    "start": "663760",
    "end": "667120"
  },
  {
    "text": "inference or serving i have a very",
    "start": "667120",
    "end": "668800"
  },
  {
    "text": "stupid example of explaining inference",
    "start": "668800",
    "end": "670320"
  },
  {
    "text": "of serving if you take a web application",
    "start": "670320",
    "end": "672800"
  },
  {
    "text": "a web application is a combination of",
    "start": "672800",
    "end": "674480"
  },
  {
    "text": "CSS HTML and JavaScript right these",
    "start": "674480",
    "end": "677519"
  },
  {
    "text": "three things are useless without web",
    "start": "677519",
    "end": "679600"
  },
  {
    "text": "server if you want to render a web page",
    "start": "679600",
    "end": "682959"
  },
  {
    "text": "you need a web server that you can talk",
    "start": "682959",
    "end": "684800"
  },
  {
    "text": "to and it will serve you your web server",
    "start": "684800",
    "end": "688079"
  },
  {
    "text": "your website right so serving or",
    "start": "688079",
    "end": "690880"
  },
  {
    "text": "inference or sometime we call it a model",
    "start": "690880",
    "end": "693120"
  },
  {
    "text": "server is essentially a piece of",
    "start": "693120",
    "end": "695120"
  },
  {
    "text": "software that can render an LLM or",
    "start": "695120",
    "end": "698079"
  },
  {
    "text": "render a machine learning model it",
    "start": "698079",
    "end": "699760"
  },
  {
    "text": "doesn't render it because there is no",
    "start": "699760",
    "end": "701040"
  },
  {
    "text": "web page but it's able to talk to the",
    "start": "701040",
    "end": "703440"
  },
  {
    "text": "model using an interface that the model",
    "start": "703440",
    "end": "705279"
  },
  {
    "text": "understand and exposes an endpoint that",
    "start": "705279",
    "end": "708240"
  },
  {
    "text": "you as an application can integrate with",
    "start": "708240",
    "end": "710480"
  },
  {
    "text": "that endpoint is usually rest and then",
    "start": "710480",
    "end": "712720"
  },
  {
    "text": "the model and talks something that",
    "start": "712720",
    "end": "714480"
  },
  {
    "text": "depends on the architecture of the model",
    "start": "714480",
    "end": "716160"
  },
  {
    "text": "so whenever people say serve inference",
    "start": "716160",
    "end": "718240"
  },
  {
    "text": "or model server they usually refer to",
    "start": "718240",
    "end": "720560"
  },
  {
    "text": "VLM that's what MPHI used but there are",
    "start": "720560",
    "end": "723120"
  },
  {
    "text": "tons of open source ones in the market",
    "start": "723120",
    "end": "724720"
  },
  {
    "text": "and we're going to talk about them",
    "start": "724720",
    "end": "726800"
  },
  {
    "text": "accelerators pretty straightforward gpu",
    "start": "726800",
    "end": "729279"
  },
  {
    "text": "TPU or this new thing called Grock",
    "start": "729279",
    "end": "731920"
  },
  {
    "text": "essentially any any sort of special",
    "start": "731920",
    "end": "734560"
  },
  {
    "text": "hardware that's that is really good at",
    "start": "734560",
    "end": "736720"
  },
  {
    "text": "doing matrix calculation or matrix",
    "start": "736720",
    "end": "739360"
  },
  {
    "text": "multiplication yeah and some of the",
    "start": "739360",
    "end": "741920"
  },
  {
    "text": "other things you're going to hear quite",
    "start": "741920",
    "end": "742959"
  },
  {
    "text": "a lot is quantization so when I said a",
    "start": "742959",
    "end": "744880"
  },
  {
    "text": "model is like a 1 billion parameter or",
    "start": "744880",
    "end": "747040"
  },
  {
    "text": "10 billion parameter what we were saying",
    "start": "747040",
    "end": "748399"
  },
  {
    "text": "is there's 1 billion numbers in there",
    "start": "748399",
    "end": "750160"
  },
  {
    "text": "and that number could be on the full",
    "start": "750160",
    "end": "751839"
  },
  {
    "text": "32-bit precision or it could have less",
    "start": "751839",
    "end": "754160"
  },
  {
    "text": "precision and have less memory footprint",
    "start": "754160",
    "end": "756560"
  },
  {
    "text": "so you could fit a large much larger",
    "start": "756560",
    "end": "758160"
  },
  {
    "text": "model in a smaller GPU if you use a",
    "start": "758160",
    "end": "760079"
  },
  {
    "text": "lower uh precision of the using",
    "start": "760079",
    "end": "762079"
  },
  {
    "text": "quantization with quantization you kind",
    "start": "762079",
    "end": "764240"
  },
  {
    "text": "of give up some of the like the quality",
    "start": "764240",
    "end": "765920"
  },
  {
    "text": "of the model to be able to fit a larger",
    "start": "765920",
    "end": "767600"
  },
  {
    "text": "model so there is like a trade-off you",
    "start": "767600",
    "end": "769279"
  },
  {
    "text": "have but you get to fit bigger models uh",
    "start": "769279",
    "end": "771440"
  },
  {
    "text": "weights are just the what I said the the",
    "start": "771440",
    "end": "773279"
  },
  {
    "text": "numbers that I have the model how the",
    "start": "773279",
    "end": "775600"
  },
  {
    "text": "model represents the world around it are",
    "start": "775600",
    "end": "777120"
  },
  {
    "text": "the weights context window is how many",
    "start": "777120",
    "end": "779120"
  },
  {
    "text": "token you can fit to a model in a given",
    "start": "779120",
    "end": "780560"
  },
  {
    "text": "time for example something like Gemini",
    "start": "780560",
    "end": "782720"
  },
  {
    "text": "can have up to like two million context",
    "start": "782720",
    "end": "784240"
  },
  {
    "text": "window our open models like Llama Gemma",
    "start": "784240",
    "end": "786880"
  },
  {
    "text": "has up to 128k context window which is",
    "start": "786880",
    "end": "789519"
  },
  {
    "text": "quite a lot but you know if you're",
    "start": "789519",
    "end": "791040"
  },
  {
    "text": "trying to process videos and such it's",
    "start": "791040",
    "end": "792399"
  },
  {
    "text": "probably not going to be enough and",
    "start": "792399",
    "end": "793839"
  },
  {
    "text": "finally multimodal so the models we're",
    "start": "793839",
    "end": "795519"
  },
  {
    "text": "dealing with here are all text to text",
    "start": "795519",
    "end": "797519"
  },
  {
    "text": "you send text you get text back but",
    "start": "797519",
    "end": "799519"
  },
  {
    "text": "something like stable diffusion you can",
    "start": "799519",
    "end": "801120"
  },
  {
    "text": "send text and get image back or send",
    "start": "801120",
    "end": "803360"
  },
  {
    "text": "text get video back or send an image and",
    "start": "803360",
    "end": "805440"
  },
  {
    "text": "get text back so you can have a lot of",
    "start": "805440",
    "end": "806800"
  },
  {
    "text": "this A2B situation going on",
    "start": "806800",
    "end": "810560"
  },
  {
    "text": "and when you talk about model servers",
    "start": "810560",
    "end": "812160"
  },
  {
    "text": "there we just looked at VLM but that's",
    "start": "812160",
    "end": "814399"
  },
  {
    "text": "not the only one there is many of them",
    "start": "814399",
    "end": "815680"
  },
  {
    "text": "in the market right now uh TGI comes",
    "start": "815680",
    "end": "817760"
  },
  {
    "text": "from Hugging Face so Hugging Face is",
    "start": "817760",
    "end": "819200"
  },
  {
    "text": "where we're getting our model data from",
    "start": "819200",
    "end": "821040"
  },
  {
    "text": "they also provide you some software to",
    "start": "821040",
    "end": "823120"
  },
  {
    "text": "run these models yourself nvidia again",
    "start": "823120",
    "end": "825440"
  },
  {
    "text": "they're the biggest provider of GPU at",
    "start": "825440",
    "end": "827120"
  },
  {
    "text": "this day and so they also have like a",
    "start": "827120",
    "end": "829440"
  },
  {
    "text": "software in the market called NIM which",
    "start": "829440",
    "end": "831120"
  },
  {
    "text": "is I think proprietary they don't have",
    "start": "831120",
    "end": "833040"
  },
  {
    "text": "an open source component all that much",
    "start": "833040",
    "end": "834399"
  },
  {
    "text": "yet but they is built on top of open",
    "start": "834399",
    "end": "836399"
  },
  {
    "text": "source standards you have Ray Serve ray",
    "start": "836399",
    "end": "838959"
  },
  {
    "text": "you can probably hear about it or look",
    "start": "838959",
    "end": "840320"
  },
  {
    "text": "it up they are bu purpose-built for",
    "start": "840320",
    "end": "842639"
  },
  {
    "text": "distributed workload so model serving is",
    "start": "842639",
    "end": "845360"
  },
  {
    "text": "kind of a distributed workload they're",
    "start": "845360",
    "end": "846639"
  },
  {
    "text": "really good at uh in the world of TPU",
    "start": "846639",
    "end": "848959"
  },
  {
    "text": "and also GPU you also have Jax which is",
    "start": "848959",
    "end": "851360"
  },
  {
    "text": "Jetstream jax is an open source project",
    "start": "851360",
    "end": "853440"
  },
  {
    "text": "uh coming out of Google and a lot of",
    "start": "853440",
    "end": "855600"
  },
  {
    "text": "people can run models locally in Olama",
    "start": "855600",
    "end": "857920"
  },
  {
    "text": "but you can also run as a container on",
    "start": "857920",
    "end": "860160"
  },
  {
    "text": "the cloud so you can kind of bring your",
    "start": "860160",
    "end": "862399"
  },
  {
    "text": "own home experience into the cloud very",
    "start": "862399",
    "end": "864160"
  },
  {
    "text": "easily with",
    "start": "864160",
    "end": "865560"
  },
  {
    "text": "Olama yep and as you're doing this one",
    "start": "865560",
    "end": "868800"
  },
  {
    "text": "of the thing one of the reason this like",
    "start": "868800",
    "end": "871120"
  },
  {
    "text": "large language models are not like your",
    "start": "871120",
    "end": "872720"
  },
  {
    "text": "typical web app is the size of the",
    "start": "872720",
    "end": "874800"
  },
  {
    "text": "container image over the last 10 years",
    "start": "874800",
    "end": "876880"
  },
  {
    "text": "we kind of preached to the choir and",
    "start": "876880",
    "end": "878959"
  },
  {
    "text": "told people to let's make container",
    "start": "878959",
    "end": "880800"
  },
  {
    "text": "smaller let's bring like small",
    "start": "880800",
    "end": "882800"
  },
  {
    "text": "containers that are very portable all of",
    "start": "882800",
    "end": "884639"
  },
  {
    "text": "a sudden 2022 happens and BLM image is",
    "start": "884639",
    "end": "887440"
  },
  {
    "text": "like 10 GB and all the learnings we had",
    "start": "887440",
    "end": "890000"
  },
  {
    "text": "is goes out the window like how do you",
    "start": "890000",
    "end": "891519"
  },
  {
    "text": "now run this giant model into our",
    "start": "891519",
    "end": "893199"
  },
  {
    "text": "container and on our cluster if you're",
    "start": "893199",
    "end": "895120"
  },
  {
    "text": "trying to download a model of that size",
    "start": "895120",
    "end": "896800"
  },
  {
    "text": "with the on the public internet you're",
    "start": "896800",
    "end": "898399"
  },
  {
    "text": "looking at times anywhere from 2 minutes",
    "start": "898399",
    "end": "900399"
  },
  {
    "text": "up to like 10 minutes because again",
    "start": "900399",
    "end": "902240"
  },
  {
    "text": "you're going off to the public internet",
    "start": "902240",
    "end": "903680"
  },
  {
    "text": "to download all that image",
    "start": "903680",
    "end": "906600"
  },
  {
    "text": "data uh you also have the size of the",
    "start": "906600",
    "end": "910160"
  },
  {
    "text": "model to how much GPU memory you need",
    "start": "910160",
    "end": "912480"
  },
  {
    "text": "comparison so easy way to run the math",
    "start": "912480",
    "end": "914639"
  },
  {
    "text": "is that if you run it on full precision",
    "start": "914639",
    "end": "916720"
  },
  {
    "text": "you are going to need twice uh the",
    "start": "916720",
    "end": "918959"
  },
  {
    "text": "memory if you run it in half precision",
    "start": "918959",
    "end": "921600"
  },
  {
    "text": "it could run uh in less memory right",
    "start": "921600",
    "end": "923680"
  },
  {
    "text": "sorry if you run it in full precision",
    "start": "923680",
    "end": "925680"
  },
  {
    "text": "one bit becomes four byt right one",
    "start": "925680",
    "end": "928560"
  },
  {
    "text": "number becomes four bytes so it requires",
    "start": "928560",
    "end": "930079"
  },
  {
    "text": "four times the GPU memory so if you have",
    "start": "930079",
    "end": "931920"
  },
  {
    "text": "a four billion parameter model in full",
    "start": "931920",
    "end": "933920"
  },
  {
    "text": "32-bit precision it's going to require",
    "start": "933920",
    "end": "935440"
  },
  {
    "text": "you to have 32 GB of video memory most",
    "start": "935440",
    "end": "937920"
  },
  {
    "text": "of the models you're going to see run on",
    "start": "937920",
    "end": "939360"
  },
  {
    "text": "the internet is going to be something",
    "start": "939360",
    "end": "940560"
  },
  {
    "text": "like BF16 or FP16 which is like 16 bit",
    "start": "940560",
    "end": "943920"
  },
  {
    "text": "precision which only requires double so",
    "start": "943920",
    "end": "946320"
  },
  {
    "text": "if I have a 27 billion parame parameter",
    "start": "946320",
    "end": "948959"
  },
  {
    "text": "model I'm going to need roughly around",
    "start": "948959",
    "end": "950720"
  },
  {
    "text": "54 GB of GPU memory so with the same",
    "start": "950720",
    "end": "953519"
  },
  {
    "text": "math you can see deepseek yeah so you",
    "start": "953519",
    "end": "956480"
  },
  {
    "text": "can see here deepseek at full precision",
    "start": "956480",
    "end": "958880"
  },
  {
    "text": "is",
    "start": "958880",
    "end": "959880"
  },
  {
    "text": "1.37 terabytes that's how much memory",
    "start": "959880",
    "end": "962959"
  },
  {
    "text": "GPU memory you will need if you need to",
    "start": "962959",
    "end": "964720"
  },
  {
    "text": "run deepseeek today on the markets there",
    "start": "964720",
    "end": "966959"
  },
  {
    "text": "is no GPU that has this amount of memory",
    "start": "966959",
    "end": "968959"
  },
  {
    "text": "the biggest GPU you can find on the",
    "start": "968959",
    "end": "970320"
  },
  {
    "text": "market is an H200 which has 80 GB of",
    "start": "970320",
    "end": "973279"
  },
  {
    "text": "memory h200 has 141 h100 has 80 thank",
    "start": "973279",
    "end": "976480"
  },
  {
    "text": "you h100 is 80 h200 is 140 right yeah",
    "start": "976480",
    "end": "980320"
  },
  {
    "text": "even with that a single node of H100 or",
    "start": "980320",
    "end": "982600"
  },
  {
    "text": "H200 does not have like you can have a",
    "start": "982600",
    "end": "985040"
  },
  {
    "text": "single node with eight H100 GPU still",
    "start": "985040",
    "end": "987360"
  },
  {
    "text": "doesn't have enough memory to fit deep",
    "start": "987360",
    "end": "988639"
  },
  {
    "text": "cigar one because there is also",
    "start": "988639",
    "end": "990399"
  },
  {
    "text": "something to keep in mind is there is",
    "start": "990399",
    "end": "991759"
  },
  {
    "text": "sometimes a a physical limitation into",
    "start": "991759",
    "end": "994000"
  },
  {
    "text": "how many GPUs you can fit on a single",
    "start": "994000",
    "end": "995759"
  },
  {
    "text": "node and that typically depends how many",
    "start": "995759",
    "end": "997519"
  },
  {
    "text": "PCI Express ports are available on that",
    "start": "997519",
    "end": "999120"
  },
  {
    "text": "node which typically is eight so let's",
    "start": "999120",
    "end": "1001360"
  },
  {
    "text": "say we take an example of an Nvidia A100",
    "start": "1001360",
    "end": "1003600"
  },
  {
    "text": "with 40 GB of memory if we take a node",
    "start": "1003600",
    "end": "1006240"
  },
  {
    "text": "and we fill it with a 100 GPUs we can",
    "start": "1006240",
    "end": "1009360"
  },
  {
    "text": "only do 40 * 8 so that's 200 320 320 GB",
    "start": "1009360",
    "end": "1014320"
  },
  {
    "text": "right so if we want to run something",
    "start": "1014320",
    "end": "1016000"
  },
  {
    "text": "like DeepS which is 1,370 GB we need",
    "start": "1016000",
    "end": "1019759"
  },
  {
    "text": "more nodes so the kind of variations",
    "start": "1019759",
    "end": "1022800"
  },
  {
    "text": "you're going to see in this space are",
    "start": "1022800",
    "end": "1024959"
  },
  {
    "text": "either what Miy did single host single",
    "start": "1024959",
    "end": "1027199"
  },
  {
    "text": "GPU or single host single accelerator or",
    "start": "1027199",
    "end": "1030240"
  },
  {
    "text": "single host multi accelerator you",
    "start": "1030240",
    "end": "1032079"
  },
  {
    "text": "remember earlier when uh Muffy was",
    "start": "1032079",
    "end": "1034240"
  },
  {
    "text": "deploying there was this sharding",
    "start": "1034240",
    "end": "1035438"
  },
  {
    "text": "parameter which he set to one you can",
    "start": "1035439",
    "end": "1037600"
  },
  {
    "text": "also set it to two three four five and",
    "start": "1037600",
    "end": "1039839"
  },
  {
    "text": "that's essentially the sharding the",
    "start": "1039839",
    "end": "1041280"
  },
  {
    "text": "splitting of the model across multiple",
    "start": "1041280",
    "end": "1042959"
  },
  {
    "text": "GPUs but then you might also be faced",
    "start": "1042959",
    "end": "1045199"
  },
  {
    "text": "with situation where you cannot fit the",
    "start": "1045199",
    "end": "1046640"
  },
  {
    "text": "model on a single node and you need to",
    "start": "1046640",
    "end": "1048000"
  },
  {
    "text": "split it across multiple nodes and",
    "start": "1048000",
    "end": "1049840"
  },
  {
    "text": "that's the third situation which is",
    "start": "1049840",
    "end": "1051280"
  },
  {
    "text": "multi-host multi accelerators now the",
    "start": "1051280",
    "end": "1054240"
  },
  {
    "text": "two first ones Kubernetes can support",
    "start": "1054240",
    "end": "1056160"
  },
  {
    "text": "them out of the box the third one can",
    "start": "1056160",
    "end": "1059440"
  },
  {
    "text": "only be supported through this new API",
    "start": "1059440",
    "end": "1061360"
  },
  {
    "text": "called LWS or leader worker set so the",
    "start": "1061360",
    "end": "1064320"
  },
  {
    "text": "leader worker set is new API Kubernetes",
    "start": "1064320",
    "end": "1066320"
  },
  {
    "text": "open source that allows you to define a",
    "start": "1066320",
    "end": "1068559"
  },
  {
    "text": "leader and the set of workers where the",
    "start": "1068559",
    "end": "1071440"
  },
  {
    "text": "leader distributes the shards and the",
    "start": "1071440",
    "end": "1073520"
  },
  {
    "text": "workers run the actual model so the",
    "start": "1073520",
    "end": "1076240"
  },
  {
    "text": "sharding is done by the leader and then",
    "start": "1076240",
    "end": "1077840"
  },
  {
    "text": "you have multiple nodes and that's how",
    "start": "1077840",
    "end": "1079520"
  },
  {
    "text": "you're able to do multi-host multi",
    "start": "1079520",
    "end": "1081360"
  },
  {
    "text": "accelerators yeah so uh if you want to",
    "start": "1081360",
    "end": "1085360"
  },
  {
    "text": "learn more about this there's there was",
    "start": "1085360",
    "end": "1086960"
  },
  {
    "text": "a talk by one of the maintainers of",
    "start": "1086960",
    "end": "1088559"
  },
  {
    "text": "litter worker set in the last cubecon in",
    "start": "1088559",
    "end": "1090799"
  },
  {
    "text": "North America you can go find that talk",
    "start": "1090799",
    "end": "1092160"
  },
  {
    "text": "on YouTube do not watch that now we're",
    "start": "1092160",
    "end": "1094000"
  },
  {
    "text": "still talking um yes uh so when you're",
    "start": "1094000",
    "end": "1097280"
  },
  {
    "text": "trying to run large language models on a",
    "start": "1097280",
    "end": "1098880"
  },
  {
    "text": "Kubernetes cluster it is like as we said",
    "start": "1098880",
    "end": "1101120"
  },
  {
    "text": "there are multiple dimensions of",
    "start": "1101120",
    "end": "1102480"
  },
  {
    "text": "optimization we could do right number",
    "start": "1102480",
    "end": "1103919"
  },
  {
    "text": "one is image size has gotten really big",
    "start": "1103919",
    "end": "1106000"
  },
  {
    "text": "so we could do some optimization to make",
    "start": "1106000",
    "end": "1108000"
  },
  {
    "text": "sure the images are cached on the nodes",
    "start": "1108000",
    "end": "1109919"
  },
  {
    "text": "before uh you could do some optimization",
    "start": "1109919",
    "end": "1111840"
  },
  {
    "text": "but on the data layer downloading the",
    "start": "1111840",
    "end": "1113520"
  },
  {
    "text": "model every time from hugging face is",
    "start": "1113520",
    "end": "1115440"
  },
  {
    "text": "expensive operation can we do something",
    "start": "1115440",
    "end": "1117440"
  },
  {
    "text": "to pre-cache those model data somehow or",
    "start": "1117440",
    "end": "1120000"
  },
  {
    "text": "download it one time and for every other",
    "start": "1120000",
    "end": "1121679"
  },
  {
    "text": "consecutive runs we can just use the",
    "start": "1121679",
    "end": "1123120"
  },
  {
    "text": "same downloaded model and there's also",
    "start": "1123120",
    "end": "1125039"
  },
  {
    "text": "like workload scaling we can do some",
    "start": "1125039",
    "end": "1126799"
  },
  {
    "text": "work on the cloud level or your",
    "start": "1126799",
    "end": "1128240"
  },
  {
    "text": "infrastructure level to scale that",
    "start": "1128240",
    "end": "1129840"
  },
  {
    "text": "workload more intelligently rather than",
    "start": "1129840",
    "end": "1131280"
  },
  {
    "text": "just like having bunch of GPU just",
    "start": "1131280",
    "end": "1132720"
  },
  {
    "text": "sitting around just in case you need it",
    "start": "1132720",
    "end": "1134799"
  },
  {
    "text": "so there are a lot of optimizations that",
    "start": "1134799",
    "end": "1136160"
  },
  {
    "text": "are happening uh this talk is probably",
    "start": "1136160",
    "end": "1138240"
  },
  {
    "text": "not the right place to talk about all of",
    "start": "1138240",
    "end": "1139679"
  },
  {
    "text": "this would be more than happy to talk",
    "start": "1139679",
    "end": "1141200"
  },
  {
    "text": "about like if you have a specific need",
    "start": "1141200",
    "end": "1142480"
  },
  {
    "text": "of that kind or what are we doing in",
    "start": "1142480",
    "end": "1144400"
  },
  {
    "text": "open source to kind of handle that",
    "start": "1144400",
    "end": "1146039"
  },
  {
    "text": "situation but um let's move on to check",
    "start": "1146039",
    "end": "1150320"
  },
  {
    "text": "out what what is happening with my demo",
    "start": "1150320",
    "end": "1151919"
  },
  {
    "text": "if it's still if it's still uh figuring",
    "start": "1151919",
    "end": "1153440"
  },
  {
    "text": "things out here we go we we I think we",
    "start": "1153440",
    "end": "1155520"
  },
  {
    "text": "stalled enough for the GPU to like uh",
    "start": "1155520",
    "end": "1157200"
  },
  {
    "text": "figure it out and run everything um I'm",
    "start": "1157200",
    "end": "1159360"
  },
  {
    "text": "going to quickly go ahead and give a",
    "start": "1159360",
    "end": "1161760"
  },
  {
    "text": "quick",
    "start": "1161760",
    "end": "1162840"
  },
  {
    "text": "logs uh what is the name let's go that's",
    "start": "1162840",
    "end": "1166080"
  },
  {
    "text": "that oh no",
    "start": "1166080",
    "end": "1168720"
  },
  {
    "text": "okay i'm going to have to do sports k",
    "start": "1168720",
    "end": "1171200"
  },
  {
    "text": "get PL i have to copy the name oh I",
    "start": "1171200",
    "end": "1173440"
  },
  {
    "text": "started with",
    "start": "1173440",
    "end": "1175840"
  },
  {
    "text": "VLM uh okay what did I do oh K",
    "start": "1176120",
    "end": "1180440"
  },
  {
    "text": "logs get me on stage and I forget",
    "start": "1180440",
    "end": "1184200"
  },
  {
    "text": "everything there you go i run this and",
    "start": "1184200",
    "end": "1187120"
  },
  {
    "text": "at the end of everything we can see that",
    "start": "1187120",
    "end": "1188640"
  },
  {
    "text": "about few minutes ago about it took",
    "start": "1188640",
    "end": "1190400"
  },
  {
    "text": "about four minutes for it to download",
    "start": "1190400",
    "end": "1192000"
  },
  {
    "text": "everything and start the VLM server",
    "start": "1192000",
    "end": "1194240"
  },
  {
    "text": "right now I have a server ready here i",
    "start": "1194240",
    "end": "1196320"
  },
  {
    "text": "could also do k get service and I would",
    "start": "1196320",
    "end": "1198640"
  },
  {
    "text": "see that I have a service ready to go i",
    "start": "1198640",
    "end": "1200640"
  },
  {
    "text": "could like set up a load balancer on top",
    "start": "1200640",
    "end": "1202160"
  },
  {
    "text": "of it to talk to it but I thought what",
    "start": "1202160",
    "end": "1204160"
  },
  {
    "text": "would be fun for all of us today",
    "start": "1204160",
    "end": "1205840"
  },
  {
    "text": "actually instead of just talking to one",
    "start": "1205840",
    "end": "1207679"
  },
  {
    "text": "model what if we talk to a bunch of",
    "start": "1207679",
    "end": "1209039"
  },
  {
    "text": "model i have a different name space in",
    "start": "1209039",
    "end": "1212400"
  },
  {
    "text": "default and I over the last couple of",
    "start": "1212400",
    "end": "1214720"
  },
  {
    "text": "days deployed a bunch of these large",
    "start": "1214720",
    "end": "1216160"
  },
  {
    "text": "language models so if I do k get service",
    "start": "1216160",
    "end": "1218799"
  },
  {
    "text": "you would see bunch of these large",
    "start": "1218799",
    "end": "1220480"
  },
  {
    "text": "language models that are currently",
    "start": "1220480",
    "end": "1221600"
  },
  {
    "text": "running and then what I thought would be",
    "start": "1221600",
    "end": "1223760"
  },
  {
    "text": "very nice for us to be able to chat with",
    "start": "1223760",
    "end": "1225200"
  },
  {
    "text": "all this model at the same time so I",
    "start": "1225200",
    "end": "1226720"
  },
  {
    "text": "built a small little UI tool and I can",
    "start": "1226720",
    "end": "1229360"
  },
  {
    "text": "go here and I can show you the URL for",
    "start": "1229360",
    "end": "1231280"
  },
  {
    "text": "this actually I will show you the this",
    "start": "1231280",
    "end": "1235559"
  },
  {
    "text": "uh one on the right is going to take you",
    "start": "1235559",
    "end": "1238400"
  },
  {
    "text": "to a UI where it's going to have a text",
    "start": "1238400",
    "end": "1240320"
  },
  {
    "text": "in box where you can type in your prompt",
    "start": "1240320",
    "end": "1242559"
  },
  {
    "text": "one on the left is the all the code that",
    "start": "1242559",
    "end": "1244400"
  },
  {
    "text": "builds this so if you want to go check",
    "start": "1244400",
    "end": "1245520"
  },
  {
    "text": "out the code you could do that later but",
    "start": "1245520",
    "end": "1247200"
  },
  {
    "text": "one on the right is where you want to go",
    "start": "1247200",
    "end": "1248480"
  },
  {
    "text": "so together we're going to do uh one",
    "start": "1248480",
    "end": "1250720"
  },
  {
    "text": "simple prompt here okay everybody got",
    "start": "1250720",
    "end": "1252080"
  },
  {
    "text": "the phone out everybody got it don't all",
    "start": "1252080",
    "end": "1254640"
  },
  {
    "text": "yell out at once okay cool we'll see if",
    "start": "1254640",
    "end": "1257360"
  },
  {
    "text": "it scales all right yeah probably not",
    "start": "1257360",
    "end": "1259440"
  },
  {
    "text": "because I didn't set up an autoscaler i",
    "start": "1259440",
    "end": "1261039"
  },
  {
    "text": "could have i didn't i'm going to quickly",
    "start": "1261039",
    "end": "1262559"
  },
  {
    "text": "run this one prompt which worked really",
    "start": "1262559",
    "end": "1264559"
  },
  {
    "text": "well last time so let's see how that",
    "start": "1264559",
    "end": "1265840"
  },
  {
    "text": "goes uh tell me uh knockk",
    "start": "1265840",
    "end": "1269400"
  },
  {
    "text": "knockock",
    "start": "1269400",
    "end": "1272039"
  },
  {
    "text": "joke use because we're in uh London use",
    "start": "1272039",
    "end": "1276320"
  },
  {
    "text": "elements of British humor i thought like",
    "start": "1276320",
    "end": "1279679"
  },
  {
    "text": "you know we're we're in the right we're",
    "start": "1279679",
    "end": "1280880"
  },
  {
    "text": "in London right now we should use that",
    "start": "1280880",
    "end": "1282559"
  },
  {
    "text": "and send this prompt across all this",
    "start": "1282559",
    "end": "1284320"
  },
  {
    "text": "model at the same time you can all try",
    "start": "1284320",
    "end": "1285600"
  },
  {
    "text": "that in your own phones if you scan the",
    "start": "1285600",
    "end": "1287280"
  },
  {
    "text": "token and this came out one time before",
    "start": "1287280",
    "end": "1290480"
  },
  {
    "text": "too it's one one of my favorite jokes",
    "start": "1290480",
    "end": "1292000"
  },
  {
    "text": "now I'm going to read out for all of you",
    "start": "1292000",
    "end": "1293600"
  },
  {
    "text": "okay uh tell me a knock-knock joke use",
    "start": "1293600",
    "end": "1296159"
  },
  {
    "text": "elements of British humor okay so I'm",
    "start": "1296159",
    "end": "1298000"
  },
  {
    "text": "going to say the knock-knock part you",
    "start": "1298000",
    "end": "1299280"
  },
  {
    "text": "can say the other part okay let's go",
    "start": "1299280",
    "end": "1300799"
  },
  {
    "text": "knock-knock who's there",
    "start": "1300799",
    "end": "1304960"
  },
  {
    "text": "arthur are there any biscuits",
    "start": "1305159",
    "end": "1309799"
  },
  {
    "text": "all right so again all of you have this",
    "start": "1309919",
    "end": "1312320"
  },
  {
    "text": "like a tool right now i'm going to keep",
    "start": "1312320",
    "end": "1313600"
  },
  {
    "text": "it running for the for the rest of the",
    "start": "1313600",
    "end": "1315120"
  },
  {
    "text": "week so if you want to like keep testing",
    "start": "1315120",
    "end": "1316400"
  },
  {
    "text": "bunch of different LLM models at the",
    "start": "1316400",
    "end": "1318080"
  },
  {
    "text": "same time you can but let's take a look",
    "start": "1318080",
    "end": "1319840"
  },
  {
    "text": "at how all of these models are being",
    "start": "1319840",
    "end": "1321280"
  },
  {
    "text": "served we talked about VLM but we we",
    "start": "1321280",
    "end": "1323360"
  },
  {
    "text": "actually can use a bunch of different",
    "start": "1323360",
    "end": "1324960"
  },
  {
    "text": "serving engine so we have VLM for one of",
    "start": "1324960",
    "end": "1328159"
  },
  {
    "text": "them uh for the Gemma 12B that you are",
    "start": "1328159",
    "end": "1331200"
  },
  {
    "text": "seeing on your screen right now if you",
    "start": "1331200",
    "end": "1332400"
  },
  {
    "text": "have the phone up and running we're",
    "start": "1332400",
    "end": "1333840"
  },
  {
    "text": "using Olama so we're just basically",
    "start": "1333840",
    "end": "1335520"
  },
  {
    "text": "packaging Olama with the Gemma 12B on it",
    "start": "1335520",
    "end": "1338960"
  },
  {
    "text": "we're running using Olama for the 3B",
    "start": "1338960",
    "end": "1341679"
  },
  {
    "text": "version of Llama 3 we're using TGI so",
    "start": "1341679",
    "end": "1345360"
  },
  {
    "text": "you can just package everything in a TGI",
    "start": "1345360",
    "end": "1347200"
  },
  {
    "text": "uh container use that model and have",
    "start": "1347200",
    "end": "1349120"
  },
  {
    "text": "that running that way uh if I you might",
    "start": "1349120",
    "end": "1351840"
  },
  {
    "text": "be wondering hey you work for Google you",
    "start": "1351840",
    "end": "1353520"
  },
  {
    "text": "I heard you guys have TPUs are you doing",
    "start": "1353520",
    "end": "1355200"
  },
  {
    "text": "anything with TPUs yes we are uh we",
    "start": "1355200",
    "end": "1357840"
  },
  {
    "text": "deploying the Llama 38 billion version",
    "start": "1357840",
    "end": "1359919"
  },
  {
    "text": "on a TPU and one funny thing you have",
    "start": "1359919",
    "end": "1361919"
  },
  {
    "text": "might have noticed that you are talking",
    "start": "1361919",
    "end": "1363200"
  },
  {
    "text": "to on a single interface you're talking",
    "start": "1363200",
    "end": "1364799"
  },
  {
    "text": "to all these different models is fully",
    "start": "1364799",
    "end": "1366400"
  },
  {
    "text": "transparent to you the user that you are",
    "start": "1366400",
    "end": "1368720"
  },
  {
    "text": "talking to a TPU or a GPU because",
    "start": "1368720",
    "end": "1370320"
  },
  {
    "text": "underneath we're using VLM with TPU and",
    "start": "1370320",
    "end": "1372880"
  },
  {
    "text": "it gives the same exact API interface",
    "start": "1372880",
    "end": "1374799"
  },
  {
    "text": "that's the big big point of using these",
    "start": "1374799",
    "end": "1376799"
  },
  {
    "text": "models we're also deploying Mistral um",
    "start": "1376799",
    "end": "1380240"
  },
  {
    "text": "uh again one thing I think Abdul",
    "start": "1380240",
    "end": "1382159"
  },
  {
    "text": "mentioned about like a deepse being a",
    "start": "1382159",
    "end": "1383760"
  },
  {
    "text": "really large model how do I go about",
    "start": "1383760",
    "end": "1385120"
  },
  {
    "text": "doing this all of you are probably",
    "start": "1385120",
    "end": "1386159"
  },
  {
    "text": "itching to learn about LWS i have you",
    "start": "1386159",
    "end": "1388240"
  },
  {
    "text": "covered you can go to DeepSc R1 folder",
    "start": "1388240",
    "end": "1391039"
  },
  {
    "text": "again this is a bigger model it requires",
    "start": "1391039",
    "end": "1392720"
  },
  {
    "text": "a little bit more things to be able to",
    "start": "1392720",
    "end": "1394159"
  },
  {
    "text": "handle things a little bit better so you",
    "start": "1394159",
    "end": "1396159"
  },
  {
    "text": "have this DeepSc R1 using Ray and VLM",
    "start": "1396159",
    "end": "1399280"
  },
  {
    "text": "together we're setting up this thing",
    "start": "1399280",
    "end": "1401280"
  },
  {
    "text": "called the leader which is starting as a",
    "start": "1401280",
    "end": "1403679"
  },
  {
    "text": "single worker has eight and H100 GPU on",
    "start": "1403679",
    "end": "1406240"
  },
  {
    "text": "it and setting up this VLM worker using",
    "start": "1406240",
    "end": "1409280"
  },
  {
    "text": "Ray so I'm setting up a ray worker that",
    "start": "1409280",
    "end": "1411440"
  },
  {
    "text": "can now say okay everybody else under",
    "start": "1411440",
    "end": "1413360"
  },
  {
    "text": "under my control send me your work and",
    "start": "1413360",
    "end": "1416480"
  },
  {
    "text": "then I have a worker uh template worker",
    "start": "1416480",
    "end": "1418640"
  },
  {
    "text": "pool which is also a single H100 node",
    "start": "1418640",
    "end": "1421200"
  },
  {
    "text": "with eight GPU so I have two node total",
    "start": "1421200",
    "end": "1423200"
  },
  {
    "text": "that is doing the work i have 16 H100",
    "start": "1423200",
    "end": "1424960"
  },
  {
    "text": "GPU do all the work and that is",
    "start": "1424960",
    "end": "1427120"
  },
  {
    "text": "basically sending its work back onto uh",
    "start": "1427120",
    "end": "1430000"
  },
  {
    "text": "the the leader worker right so together",
    "start": "1430000",
    "end": "1432640"
  },
  {
    "text": "they're basically computing my every",
    "start": "1432640",
    "end": "1434080"
  },
  {
    "text": "time you send a request the R1 model is",
    "start": "1434080",
    "end": "1435760"
  },
  {
    "text": "sending so in the list I think the last",
    "start": "1435760",
    "end": "1437280"
  },
  {
    "text": "model is the R1 so you can see that",
    "start": "1437280",
    "end": "1439159"
  },
  {
    "text": "working um couple of other things we",
    "start": "1439159",
    "end": "1441760"
  },
  {
    "text": "talked about like optimization is that",
    "start": "1441760",
    "end": "1443919"
  },
  {
    "text": "when you're doing like such a big model",
    "start": "1443919",
    "end": "1445600"
  },
  {
    "text": "which is about 800 GB of download you",
    "start": "1445600",
    "end": "1447280"
  },
  {
    "text": "don't want to go and download it every",
    "start": "1447280",
    "end": "1448480"
  },
  {
    "text": "time back and forth so we're using",
    "start": "1448480",
    "end": "1450320"
  },
  {
    "text": "something called HDML which is a Google",
    "start": "1450320",
    "end": "1452480"
  },
  {
    "text": "cloud specific thing but you could do a",
    "start": "1452480",
    "end": "1453919"
  },
  {
    "text": "specific thing like this in pretty much",
    "start": "1453919",
    "end": "1455679"
  },
  {
    "text": "any cloud where you use extra storage or",
    "start": "1455679",
    "end": "1457919"
  },
  {
    "text": "PVC to store the model data so we're",
    "start": "1457919",
    "end": "1460320"
  },
  {
    "text": "doing that here instead of actually uh",
    "start": "1460320",
    "end": "1462159"
  },
  {
    "text": "downloading the model every time we're",
    "start": "1462159",
    "end": "1463520"
  },
  {
    "text": "just mounting this volume that we can",
    "start": "1463520",
    "end": "1465679"
  },
  {
    "text": "talk to it so yeah we have all of this",
    "start": "1465679",
    "end": "1467679"
  },
  {
    "text": "example if you uh got the GitHub repo",
    "start": "1467679",
    "end": "1470480"
  },
  {
    "text": "you should be able to actually the one",
    "start": "1470480",
    "end": "1472000"
  },
  {
    "text": "on the left if you understand that real",
    "start": "1472000",
    "end": "1473440"
  },
  {
    "text": "quick all the YAML and all the things we",
    "start": "1473440",
    "end": "1475440"
  },
  {
    "text": "use including the UI is actually open",
    "start": "1475440",
    "end": "1477200"
  },
  {
    "text": "source you can go try and play around",
    "start": "1477200",
    "end": "1478559"
  },
  {
    "text": "with it um so yeah unfortunately we",
    "start": "1478559",
    "end": "1482640"
  },
  {
    "text": "don't have enough time to go through all",
    "start": "1482640",
    "end": "1484000"
  },
  {
    "text": "the details one thing that we we didn't",
    "start": "1484000",
    "end": "1485840"
  },
  {
    "text": "really show is actually with Ola you can",
    "start": "1485840",
    "end": "1487840"
  },
  {
    "text": "see and Ola is one of the ones that you",
    "start": "1487840",
    "end": "1490159"
  },
  {
    "text": "can try this very simple do I download",
    "start": "1490159",
    "end": "1492400"
  },
  {
    "text": "the model right when you do app the",
    "start": "1492400",
    "end": "1494799"
  },
  {
    "text": "model get downloaded or you can pre-bake",
    "start": "1494799",
    "end": "1496799"
  },
  {
    "text": "the model inside the image and then",
    "start": "1496799",
    "end": "1498320"
  },
  {
    "text": "produce a new image that already",
    "start": "1498320",
    "end": "1499520"
  },
  {
    "text": "contains the model that's one thing you",
    "start": "1499520",
    "end": "1501120"
  },
  {
    "text": "can play with to see kind of like the",
    "start": "1501120",
    "end": "1502400"
  },
  {
    "text": "alternatives and kind of see which one",
    "start": "1502400",
    "end": "1504240"
  },
  {
    "text": "starts faster uh but basically",
    "start": "1504240",
    "end": "1507760"
  },
  {
    "text": "um we we thought we would wrap it up",
    "start": "1507760",
    "end": "1509279"
  },
  {
    "text": "with showing this reference architecting",
    "start": "1509279",
    "end": "1510720"
  },
  {
    "text": "for AI platforms and the whole point of",
    "start": "1510720",
    "end": "1512559"
  },
  {
    "text": "this talk was to say yes you can run lm",
    "start": "1512559",
    "end": "1514559"
  },
  {
    "text": "on Kubernetes kubernetes is the bottom",
    "start": "1514559",
    "end": "1517039"
  },
  {
    "text": "layer where you can run pretty much",
    "start": "1517039",
    "end": "1518640"
  },
  {
    "text": "everything and the project is doing a",
    "start": "1518640",
    "end": "1520480"
  },
  {
    "text": "lot of work to actually make it possible",
    "start": "1520480",
    "end": "1522320"
  },
  {
    "text": "to do more things with LLMs so you have",
    "start": "1522320",
    "end": "1524640"
  },
  {
    "text": "the typical stuff like the autoscaling",
    "start": "1524640",
    "end": "1526960"
  },
  {
    "text": "you have this open source project called",
    "start": "1526960",
    "end": "1528480"
  },
  {
    "text": "Q um which we open source but it's you",
    "start": "1528480",
    "end": "1531200"
  },
  {
    "text": "can use it anywhere you want it's a job",
    "start": "1531200",
    "end": "1533520"
  },
  {
    "text": "uh Kubernetes native job queuing system",
    "start": "1533520",
    "end": "1535679"
  },
  {
    "text": "um you have all your multi-tenency",
    "start": "1535679",
    "end": "1537679"
  },
  {
    "text": "infrastructure as code githops like your",
    "start": "1537679",
    "end": "1539200"
  },
  {
    "text": "DevOpsy way of doing things so if you're",
    "start": "1539200",
    "end": "1541279"
  },
  {
    "text": "doing DevOps for web applications you",
    "start": "1541279",
    "end": "1542640"
  },
  {
    "text": "can also do the DevOps for LMS and then",
    "start": "1542640",
    "end": "1545039"
  },
  {
    "text": "at the bottom layer you have your multi-",
    "start": "1545039",
    "end": "1547600"
  },
  {
    "text": "instances like the way you share",
    "start": "1547600",
    "end": "1548960"
  },
  {
    "text": "instances on the same nodes or across",
    "start": "1548960",
    "end": "1550480"
  },
  {
    "text": "nodes um you have access to your um hard",
    "start": "1550480",
    "end": "1553120"
  },
  {
    "text": "drives volumes as uh as Muffy showed you",
    "start": "1553120",
    "end": "1556159"
  },
  {
    "text": "have this thing called fuse where you",
    "start": "1556159",
    "end": "1557520"
  },
  {
    "text": "can mount a buckets it doesn't have to",
    "start": "1557520",
    "end": "1558720"
  },
  {
    "text": "be a Google cloud bucket it can be any",
    "start": "1558720",
    "end": "1560080"
  },
  {
    "text": "bucket the fuse driver supports multiple",
    "start": "1560080",
    "end": "1561679"
  },
  {
    "text": "types of buckets um and then yeah and",
    "start": "1561679",
    "end": "1565360"
  },
  {
    "text": "finally like the blue colored box i",
    "start": "1565360",
    "end": "1567279"
  },
  {
    "text": "don't know if the color shows it does",
    "start": "1567279",
    "end": "1568480"
  },
  {
    "text": "fantastic the blue colored box is kind",
    "start": "1568480",
    "end": "1570400"
  },
  {
    "text": "of like what Kubernetes cares about",
    "start": "1570400",
    "end": "1572000"
  },
  {
    "text": "anything outside as a software you could",
    "start": "1572000",
    "end": "1573840"
  },
  {
    "text": "run pretty much in your control it could",
    "start": "1573840",
    "end": "1575600"
  },
  {
    "text": "be your own custom like machine learning",
    "start": "1575600",
    "end": "1577840"
  },
  {
    "text": "application it could be any of the open",
    "start": "1577840",
    "end": "1579440"
  },
  {
    "text": "source tool you could just grab from the",
    "start": "1579440",
    "end": "1580720"
  },
  {
    "text": "internet anywhere so for that part a lot",
    "start": "1580720",
    "end": "1583520"
  },
  {
    "text": "of decisions like how those apps are",
    "start": "1583520",
    "end": "1585440"
  },
  {
    "text": "built is outside of Kubernetes control",
    "start": "1585440",
    "end": "1588000"
  },
  {
    "text": "so we are actually working with the",
    "start": "1588000",
    "end": "1589360"
  },
  {
    "text": "community quite a lot to make sure that",
    "start": "1589360",
    "end": "1591440"
  },
  {
    "text": "any type of app that you want to run on",
    "start": "1591440",
    "end": "1593200"
  },
  {
    "text": "Kubernetes works and we've been doing",
    "start": "1593200",
    "end": "1594400"
  },
  {
    "text": "this for the last 10 plus years at this",
    "start": "1594400",
    "end": "1595919"
  },
  {
    "text": "point so again the community is evolving",
    "start": "1595919",
    "end": "1598640"
  },
  {
    "text": "we're seeing a lot of changes from data",
    "start": "1598640",
    "end": "1600400"
  },
  {
    "text": "scientists to like machine learning",
    "start": "1600400",
    "end": "1601840"
  },
  {
    "text": "engineers and Kubernetes we want to make",
    "start": "1601840",
    "end": "1603840"
  },
  {
    "text": "it the best place to do this kind of",
    "start": "1603840",
    "end": "1605440"
  },
  {
    "text": "workload with that we're going to give",
    "start": "1605440",
    "end": "1607600"
  },
  {
    "text": "you this QR code uh this is to send us",
    "start": "1607600",
    "end": "1609840"
  },
  {
    "text": "any feedback feedback is a gift anything",
    "start": "1609840",
    "end": "1611679"
  },
  {
    "text": "you liked and this also lets CNCF know",
    "start": "1611679",
    "end": "1613360"
  },
  {
    "text": "if this type of a talk is useful for you",
    "start": "1613360",
    "end": "1615440"
  },
  {
    "text": "to learn and I don't think we're going",
    "start": "1615440",
    "end": "1617120"
  },
  {
    "text": "to have much time for question answers i",
    "start": "1617120",
    "end": "1618880"
  },
  {
    "text": "don't think we have a mic running around",
    "start": "1618880",
    "end": "1619840"
  },
  {
    "text": "or anything but we're going to be here",
    "start": "1619840",
    "end": "1621360"
  },
  {
    "text": "around if you don't find us here we're",
    "start": "1621360",
    "end": "1622640"
  },
  {
    "text": "going to be in the Google booth so if",
    "start": "1622640",
    "end": "1623840"
  },
  {
    "text": "you have any question there is actually",
    "start": "1623840",
    "end": "1624640"
  },
  {
    "text": "a mic oh there is a mic fantastic but we",
    "start": "1624640",
    "end": "1626640"
  },
  {
    "text": "have only four minutes i don't know we",
    "start": "1626640",
    "end": "1627760"
  },
  {
    "text": "have much time maybe like one or two",
    "start": "1627760",
    "end": "1629120"
  },
  {
    "text": "questions or just find us after the talk",
    "start": "1629120",
    "end": "1631360"
  },
  {
    "text": "outside here anywhere if you have any",
    "start": "1631360",
    "end": "1633039"
  },
  {
    "text": "type of questions about learning uh",
    "start": "1633039",
    "end": "1635679"
  },
  {
    "text": "running machine learning models for",
    "start": "1635679",
    "end": "1636880"
  },
  {
    "text": "serving use cases or training use cases",
    "start": "1636880",
    "end": "1639760"
  },
  {
    "text": "yeah thank you so much for joining us",
    "start": "1639760",
    "end": "1641279"
  },
  {
    "text": "thank you",
    "start": "1641279",
    "end": "1644440"
  }
]