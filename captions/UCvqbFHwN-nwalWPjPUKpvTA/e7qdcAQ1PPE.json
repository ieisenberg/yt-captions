[
  {
    "text": "hi everyone and Welcome to our presentation on simplifying AI infrastructure with kubernetes operators",
    "start": "399",
    "end": "5960"
  },
  {
    "text": "we're so excited to be here today I'm Ganesh Kumar and I'm a software engineer on the Azure",
    "start": "5960",
    "end": "12040"
  },
  {
    "text": "kubernetes service team AKs is a managed kubernetes platform to make it easier to",
    "start": "12040",
    "end": "17199"
  },
  {
    "text": "run kubernetes workloads on Azure I particularly work on GPU Management in",
    "start": "17199",
    "end": "22240"
  },
  {
    "text": "kubernetes and also improving pod startup times with me is",
    "start": "22240",
    "end": "27720"
  },
  {
    "text": "TK hey everyone I'm t and I work at Nvidia and I work at the cloud native",
    "start": "27720",
    "end": "33440"
  },
  {
    "text": "Technologies team uh here at the cloud native Technologies we work on tooling",
    "start": "33440",
    "end": "38480"
  },
  {
    "text": "and software that enable users to use gpus on cloud native environments like kubernetes and",
    "start": "38480",
    "end": "45680"
  },
  {
    "text": "containers and I'm also the maintainer of the GPU operator GitHub um",
    "start": "45680",
    "end": "52399"
  },
  {
    "text": "project in today's talk we'll go over why kubernetes is useful for AIML",
    "start": "56079",
    "end": "61399"
  },
  {
    "text": "workloads give a high level overview of the ml deployment stack then we'll talk",
    "start": "61399",
    "end": "66920"
  },
  {
    "text": "in detail about GPU Management in kubernetes how it's typically done today we will share about kubernetes operators",
    "start": "66920",
    "end": "74479"
  },
  {
    "text": "and why you should use operators for GPU management and then talk about many of",
    "start": "74479",
    "end": "79799"
  },
  {
    "text": "the details and features of GPU operators then we'll share about the ml",
    "start": "79799",
    "end": "84880"
  },
  {
    "text": "application layer and how you could fine-tune uh workloads and run various I",
    "start": "84880",
    "end": "90119"
  },
  {
    "text": "kinds of ml workloads with operators and we'll show a demo which combines these two operators",
    "start": "90119",
    "end": "97560"
  },
  {
    "text": "together so why would you use kubernetes for AIML you start off with the support",
    "start": "97560",
    "end": "103000"
  },
  {
    "text": "for scalability kubernetes has many features to be able to scale up and down your workloads based on demand uh for",
    "start": "103000",
    "end": "110399"
  },
  {
    "text": "instance if your inference workload increases significantly then kubernetes has features to improve uh scalability",
    "start": "110399",
    "end": "117799"
  },
  {
    "text": "there then in terms of fall kubernetes has uh many built-in components for",
    "start": "117799",
    "end": "123520"
  },
  {
    "text": "self-healing capabilities which are particularly useful for GPU",
    "start": "123520",
    "end": "129239"
  },
  {
    "text": "workloads one of the cool features of kubernetes is that it's very extensible you can extend it through the custom",
    "start": "129239",
    "end": "135200"
  },
  {
    "text": "resource definition and that enables a broad ecosystem of tools that have been developed to make it easier to run AIML",
    "start": "135200",
    "end": "144640"
  },
  {
    "text": "workloads at a high level you have the ml application uh that can be written in",
    "start": "144640",
    "end": "150840"
  },
  {
    "text": "py toch or tensorflow then you have a deployment orchestration layer which is almost subdivided into two layers where",
    "start": "150840",
    "end": "157360"
  },
  {
    "text": "you have tools like cubra operator or Kido which run on top of kubernetes and",
    "start": "157360",
    "end": "163400"
  },
  {
    "text": "then you have the GPU configuration layer below that uh and then you finally have nodes and the operating system",
    "start": "163400",
    "end": "170400"
  },
  {
    "text": "which are either managed by the cloud provider or your on Prem infra",
    "start": "170400",
    "end": "176879"
  },
  {
    "text": "team in this section we'll focus on GPU management ment and to start off with we",
    "start": "176920",
    "end": "182360"
  },
  {
    "text": "will need the device drivers to be installed then we need to have runtime handlers on the nodes for",
    "start": "182360",
    "end": "189360"
  },
  {
    "text": "gpus and then finally we need to advertise the gpus to the cuet so that",
    "start": "189360",
    "end": "195959"
  },
  {
    "text": "uh the pods with that request GPU resources can be scheduled",
    "start": "195959",
    "end": "201040"
  },
  {
    "text": "appropriately and once all of these components are configured you can uh see",
    "start": "201040",
    "end": "206400"
  },
  {
    "text": "when you describe a node that there are GPU resources available when",
    "start": "206400",
    "end": "212280"
  },
  {
    "text": "applicable so how's this currently done today typically teams would create",
    "start": "212280",
    "end": "218200"
  },
  {
    "text": "machine images of provisioning scripts to install host level components that includes the driver and the container",
    "start": "218200",
    "end": "225080"
  },
  {
    "text": "toolkit which is the runtime Handler for gpus then users would end up deploying",
    "start": "225080",
    "end": "231519"
  },
  {
    "text": "Helm charts or yaml manifests for the kubernetes level components like the device Plugin or the dcgm exporter and",
    "start": "231519",
    "end": "239200"
  },
  {
    "text": "other component as needed the advantage of managing it in",
    "start": "239200",
    "end": "245400"
  },
  {
    "text": "this way is that machine images are a very standardized way to support uh",
    "start": "245400",
    "end": "250680"
  },
  {
    "text": "development of of drivers and provisioning of drivers and then even if",
    "start": "250680",
    "end": "257320"
  },
  {
    "text": "you're on Prem and manage these images you can use tools like Anil or Packer to easily build these",
    "start": "257320",
    "end": "263440"
  },
  {
    "text": "images Cloud providers also have their own scripts to do this and this AKs GPU",
    "start": "263440",
    "end": "268960"
  },
  {
    "text": "repo is is one of the repos that I work on where we manage this driver",
    "start": "268960",
    "end": "274240"
  },
  {
    "text": "configuration and there's extensive testing done by the cloud provider or the on-prem teams usually before these",
    "start": "274240",
    "end": "281360"
  },
  {
    "text": "certified machine images are deployed but the challenge for this",
    "start": "281360",
    "end": "287680"
  },
  {
    "text": "approach is that from the end user perspective changing the default driver version can be",
    "start": "287680",
    "end": "293720"
  },
  {
    "text": "hard the upgrades also typically need upgrades to your uh machine images",
    "start": "293720",
    "end": "300560"
  },
  {
    "text": "themselves and that can be quite complex and timec consuming you also need to maintain",
    "start": "300560",
    "end": "307039"
  },
  {
    "text": "separate images for GPU nodes in certain cases and that adds complexity for smaller",
    "start": "307039",
    "end": "313240"
  },
  {
    "text": "teams now Tark will share more about some of the day challenges with",
    "start": "313240",
    "end": "319600"
  },
  {
    "text": "this thanks so yep I'll be going into detail on the day two uh challenges when",
    "start": "320039",
    "end": "326319"
  },
  {
    "text": "managing gpus on Kates now just to clarify day operations are um operations",
    "start": "326319",
    "end": "332479"
  },
  {
    "text": "that you perform typically after deployment so that can include like reconfiguration upgrades updates",
    "start": "332479",
    "end": "339160"
  },
  {
    "text": "migrations Etc now with the machine image approach something that inevitably ends up happening is that you have nodes",
    "start": "339160",
    "end": "348319"
  },
  {
    "text": "with different versions bootstraps with different versions of the machine images and as a result you have different sets",
    "start": "348319",
    "end": "354600"
  },
  {
    "text": "of drivers and toolkits uh spread across um various nodes in your clust",
    "start": "354600",
    "end": "360600"
  },
  {
    "text": "so what happens is cluster admins when they have to perform maintenance and",
    "start": "360600",
    "end": "365960"
  },
  {
    "text": "Driver upgrades they have to handle each and every node individually and make sure all the driver um the nodes are up",
    "start": "365960",
    "end": "373479"
  },
  {
    "text": "to date so this becomes harder to scale and it's also requires a lot of manual intervention and extremely error",
    "start": "373479",
    "end": "381560"
  },
  {
    "text": "prone secondly let's look at GPU driver upgrades now in the case where you want",
    "start": "381560",
    "end": "387479"
  },
  {
    "text": "to go um in uh upgrade drivers of nodes in place these are um the steps that you",
    "start": "387479",
    "end": "394400"
  },
  {
    "text": "have to keep in mind when executing a GPU driver upgrade that is clean and uh",
    "start": "394400",
    "end": "399680"
  },
  {
    "text": "free of Errors so as you can see in the flowchart you first ensure that there",
    "start": "399680",
    "end": "405639"
  },
  {
    "text": "are no running workloads on the the node so that the GPU driver isn't is not in",
    "start": "405639",
    "end": "411479"
  },
  {
    "text": "use then after that you deploy any of the you disable any of the GPU case",
    "start": "411479",
    "end": "416680"
  },
  {
    "text": "components like the device plugin because they depend on the driver and the toolkit then after that you unload",
    "start": "416680",
    "end": "423240"
  },
  {
    "text": "the driver modules then you C you CAU in the node so that you don't have any um",
    "start": "423240",
    "end": "429160"
  },
  {
    "text": "ml workloads being scheduled on that node where you intend to run the driver upgrades now you install the new GPU",
    "start": "429160",
    "end": "435840"
  },
  {
    "text": "driver you reenable the Kates components so that they're redeployed on top of the",
    "start": "435840",
    "end": "441120"
  },
  {
    "text": "new driver setup then you un cord in the node and then you have your new GPU node",
    "start": "441120",
    "end": "446520"
  },
  {
    "text": "with the driver and it's ready to accept new workloads so as you can see it's very it's a very involved process you",
    "start": "446520",
    "end": "452919"
  },
  {
    "text": "have to make sure it's done in the right order and then it's um the driver upgrade successful now this this becomes",
    "start": "452919",
    "end": "460560"
  },
  {
    "text": "harder to manage when you do this at scale another common challenge that",
    "start": "460560",
    "end": "467159"
  },
  {
    "text": "we've run into is node level configuration and what I mean by this is",
    "start": "467159",
    "end": "472479"
  },
  {
    "text": "uh persisting no per node configurations and one common area where this is",
    "start": "472479",
    "end": "478759"
  },
  {
    "text": "applied is is um multi- instance GPU and time sharing configurations now let's",
    "start": "478759",
    "end": "484599"
  },
  {
    "text": "say uh you have a huge cluster and on some of the Clusters you want to enable Mig mode or time slicing or you want to",
    "start": "484599",
    "end": "491360"
  },
  {
    "text": "enable vgpus this is to ensure that your gpus are shared well enough it's",
    "start": "491360",
    "end": "497759"
  },
  {
    "text": "utilized and it it's there's proper utilization in the multi-end environment",
    "start": "497759",
    "end": "503720"
  },
  {
    "text": "so one one thing that cluster admins have to keep keep in mind is that when they uh perform driver up upgrades or",
    "start": "503720",
    "end": "510400"
  },
  {
    "text": "node reboots or node reimaging you have to make sure um the node level configuration also has to persist across",
    "start": "510400",
    "end": "516680"
  },
  {
    "text": "these day2 operations so again this is another added over operational overhead for",
    "start": "516680",
    "end": "522919"
  },
  {
    "text": "devops and um cluster admins so I've gone over all of these",
    "start": "522919",
    "end": "528640"
  },
  {
    "text": "operational uh challenges and we can see that there is like a need for automation a consistent approach so that all of",
    "start": "528640",
    "end": "536320"
  },
  {
    "text": "these operational knowledge can be codified and um we can automate this and",
    "start": "536320",
    "end": "541640"
  },
  {
    "text": "so that this is uh executable in in a large",
    "start": "541640",
    "end": "546920"
  },
  {
    "text": "scale and um oh yeah and let's all let's Al not forget the um dcgm exporter so",
    "start": "546920",
    "end": "553920"
  },
  {
    "text": "where when it comes to large clusters monitoring observability in is another",
    "start": "553920",
    "end": "559600"
  },
  {
    "text": "important aspect for operational success so you want uh the ability to collect Health Diagnostics and usage data you",
    "start": "559600",
    "end": "567160"
  },
  {
    "text": "want to be able to integrate like the GPU met the Telemetry into Cloud native systems like promethus you wanten",
    "start": "567160",
    "end": "573519"
  },
  {
    "text": "alerting to on unhealthy gpus to facilitate um",
    "start": "573519",
    "end": "578640"
  },
  {
    "text": "remediation so for all of these um operational challenges you you want",
    "start": "578640",
    "end": "584120"
  },
  {
    "text": "Automation and what better way than to use the strengths of kubernetes to",
    "start": "584120",
    "end": "590680"
  },
  {
    "text": "achieve this um automation at large scales so this is where um kubernetes",
    "start": "590680",
    "end": "596760"
  },
  {
    "text": "operators come in because kubernetes operators were created for this very purpose you you can leverage the",
    "start": "596760",
    "end": "602160"
  },
  {
    "text": "kubernetes strengths to solve um operational challenges so is",
    "start": "602160",
    "end": "607640"
  },
  {
    "text": "automatable and you follow a consistent approach that the apid driven approach that kubernetes uh",
    "start": "607640",
    "end": "615320"
  },
  {
    "text": "promotes so what are operators operators simply put are nothing but software extensions they use",
    "start": "615320",
    "end": "622519"
  },
  {
    "text": "custom resource uh to manage um the life cycle of applications the they're used",
    "start": "622519",
    "end": "628920"
  },
  {
    "text": "to autom made day one and day two operations uh it t it takes to manage an",
    "start": "628920",
    "end": "636200"
  },
  {
    "text": "application so this is a diagram that goes over like provides a high level overview of what an operator is so there",
    "start": "636200",
    "end": "642680"
  },
  {
    "text": "are two critical components for an operator you have the custom resource and then you have the the controller so",
    "start": "642680",
    "end": "648600"
  },
  {
    "text": "the custom resource is um the kubernetes feature where you can extend um",
    "start": "648600",
    "end": "655360"
  },
  {
    "text": "kubernetes API to represent uh a design State and configuration in um in a",
    "start": "655360",
    "end": "662680"
  },
  {
    "text": "domain that is specific to the problem you're trying to solve the controller is something that will watch changes to the",
    "start": "662680",
    "end": "669920"
  },
  {
    "text": "custom resources the controller will have the operational Logic the operational knowledge codified in it as",
    "start": "669920",
    "end": "675639"
  },
  {
    "text": "business logic and then the the controller will ensure that the desired",
    "start": "675639",
    "end": "681279"
  },
  {
    "text": "State Express in the custom resource is reflected in the",
    "start": "681279",
    "end": "686680"
  },
  {
    "text": "cluster so yeah so to why use operators so we extend the kubernetes functionality to solve domain specific",
    "start": "687000",
    "end": "694440"
  },
  {
    "text": "problems so just to put it in perspective like when it comes to deploying applications you know that you",
    "start": "694440",
    "end": "700440"
  },
  {
    "text": "use um deployments um Damon sets cluster roles and all the other kubernetes DMO",
    "start": "700440",
    "end": "706880"
  },
  {
    "text": "manifest it takes to deploy that out set application on kubernetes with operators",
    "start": "706880",
    "end": "712320"
  },
  {
    "text": "you can abtract all of that detail away so so that you only expose the adjustments and configuration that are",
    "start": "712320",
    "end": "718720"
  },
  {
    "text": "specific to the problem that you're trying to solve so um effective operators Pro obstructions so that it",
    "start": "718720",
    "end": "726399"
  },
  {
    "text": "enables the users of the operator solve the problems at hand another advantage",
    "start": "726399",
    "end": "731959"
  },
  {
    "text": "of operators that now we have a broad ecosystem of operator Frameworks and cataloges if one wants to develop their",
    "start": "731959",
    "end": "738360"
  },
  {
    "text": "own operator you can use um software like Cube Builder operator",
    "start": "738360",
    "end": "743720"
  },
  {
    "text": "SDK and they will generate the scaffolding for you and all you need to do is um develop your um business logic",
    "start": "743720",
    "end": "751519"
  },
  {
    "text": "and then your operators up and running and you you also have Rich cataloges",
    "start": "751519",
    "end": "756880"
  },
  {
    "text": "where there are already operators built to solve problems that you're trying to um find sorry the problems that you're",
    "start": "756880",
    "end": "764199"
  },
  {
    "text": "trying to solve and you can already reuse operators that are out there so let's look at um how kubernetes",
    "start": "764199",
    "end": "772279"
  },
  {
    "text": "operators can help in the AI ml space so the GPU operator is one such",
    "start": "772279",
    "end": "778680"
  },
  {
    "text": "example where we built this operator to um codify like all the operational",
    "start": "778680",
    "end": "785160"
  },
  {
    "text": "knowledge it takes to efficiently maintain uh software components for um",
    "start": "785160",
    "end": "792000"
  },
  {
    "text": "enabling GPU usage on kubernetes so as you can see here this",
    "start": "792000",
    "end": "797680"
  },
  {
    "text": "diagram ex exemplifies uh how um an operator will simplify the installation",
    "start": "797680",
    "end": "804800"
  },
  {
    "text": "and the life cycle management of all the software components it takes for um to",
    "start": "804800",
    "end": "810079"
  },
  {
    "text": "get gpus um usable on kubernetes so on the left side you see that without the GPU operator users was",
    "start": "810079",
    "end": "817440"
  },
  {
    "text": "typically use like the machine images which packages in the toolkit and the driver and then to layer the the other",
    "start": "817440",
    "end": "823720"
  },
  {
    "text": "components you use in a standalone Helm charts or yl manifests and then they you",
    "start": "823720",
    "end": "829079"
  },
  {
    "text": "use multiple tools to make sure like your your GPU or G kubernetes cluster is GPU ready whereas for the GPU operator",
    "start": "829079",
    "end": "838199"
  },
  {
    "text": "you have uh one tool one application that is managing it all for you it does",
    "start": "838199",
    "end": "843639"
  },
  {
    "text": "it all for you from deployment to um upgrade and migration",
    "start": "843639",
    "end": "850480"
  },
  {
    "text": "updates so let's look at the anatomy of um the GPU operator so as I said before",
    "start": "851000",
    "end": "857399"
  },
  {
    "text": "and operators have the custom resource and the controllers so in the case of GPU operator you have two Uh custom",
    "start": "857399",
    "end": "864120"
  },
  {
    "text": "resources the cost of policy and the Nvidia driver and you have the controllers the co for the cluster",
    "start": "864120",
    "end": "869800"
  },
  {
    "text": "policy and the Nvidia driver custom resources and then you have the upgrade controller so let's look at the cluster",
    "start": "869800",
    "end": "876079"
  },
  {
    "text": "policy API so the cluster policy custom resource is basically a representation",
    "start": "876079",
    "end": "881320"
  },
  {
    "text": "of the GPU software stack that you want to um Deploy on your cluster so the advantage is that you can focus on the",
    "start": "881320",
    "end": "888680"
  },
  {
    "text": "components that you want to deploy the component versions and the configs",
    "start": "888680",
    "end": "893759"
  },
  {
    "text": "pertaining to that component and then the GPU operator will recognize that as the desired state that you want to apply",
    "start": "893759",
    "end": "900600"
  },
  {
    "text": "cluster wide another aspect of the custom resource is the status field the status",
    "start": "900600",
    "end": "906480"
  },
  {
    "text": "field helps you keep track of how um you know how far along it has gone when it",
    "start": "906480",
    "end": "912720"
  },
  {
    "text": "comes to applying the desired state so see you can see over here the status said that it's it's ready that the the",
    "start": "912720",
    "end": "920120"
  },
  {
    "text": "expressed configuration the cost of policy has been applied the other custom resource is the",
    "start": "920120",
    "end": "926320"
  },
  {
    "text": "Nvidia driver API so this is a relatively new API that we've introduced with the Nvidia driver we have the",
    "start": "926320",
    "end": "932279"
  },
  {
    "text": "ability to deploy multiple driver versions now while it is possible to deploy uh the GPU driver with the cost",
    "start": "932279",
    "end": "939319"
  },
  {
    "text": "of policy there's a limitation in that you can only deploy One driver version or one driver configuration cluster wide",
    "start": "939319",
    "end": "946880"
  },
  {
    "text": "there are cases where you want to deploy multiple driver versions or multiple driver configuration sets and the Nvidia",
    "start": "946880",
    "end": "952759"
  },
  {
    "text": "driver API makes this possible so if you can see here you have",
    "start": "952759",
    "end": "958199"
  },
  {
    "text": "two Nvidia d ders which makes use of the node selector to Target a certain note",
    "start": "958199",
    "end": "963440"
  },
  {
    "text": "pool and then you can apply a desired Drive configuration on that note pool so on the left side you see that you have",
    "start": "963440",
    "end": "969759"
  },
  {
    "text": "the Nvidia driver which targets the Azure T4 instances and it deploys the r550 drivers on there on the right hand",
    "start": "969759",
    "end": "977720"
  },
  {
    "text": "side you have a node selector the Nvidia driver with a node selector pointing to the V100 instance instances and then",
    "start": "977720",
    "end": "983800"
  },
  {
    "text": "you're deploying the 535 drivers now let's go into into some of",
    "start": "983800",
    "end": "989319"
  },
  {
    "text": "the uh core components that are packaged into the GPU operator so one of the more",
    "start": "989319",
    "end": "994440"
  },
  {
    "text": "um one of the most popular uses of GPU operator is for the GPU driver container with the GPU driver container you have",
    "start": "994440",
    "end": "999880"
  },
  {
    "text": "the install logic of the driver packaged into the Container so basically you get you leverage the benefits of the",
    "start": "999880",
    "end": "1005560"
  },
  {
    "text": "container when deploying the GPU operator so this makes it um easy to deploy gives you",
    "start": "1005560",
    "end": "1012440"
  },
  {
    "text": "reproducibility and it also makes it conducive to perform um automated upgrades like all the operations that I",
    "start": "1012440",
    "end": "1019680"
  },
  {
    "text": "went over when it comes to in D updating GPU drivers in place in a node that can",
    "start": "1019680",
    "end": "1024918"
  },
  {
    "text": "be done with the help of the driver container driver container and the upgrade",
    "start": "1024919",
    "end": "1029959"
  },
  {
    "text": "controller the next is the Mig manager this is the component that enables you",
    "start": "1029959",
    "end": "1035079"
  },
  {
    "text": "to um to enable multi-instance gpus on your node so multi- instance gpus is a",
    "start": "1035079",
    "end": "1041480"
  },
  {
    "text": "feature on some of the um Nvidia gpus where you can divide your GPU into",
    "start": "1041480",
    "end": "1047640"
  },
  {
    "text": "smaller physical slices so so that you can enable GPU sharing where the isolation happens at the",
    "start": "1047640",
    "end": "1053679"
  },
  {
    "text": "hardware level so what the Mig manager is very good at is you can apply um by",
    "start": "1053679",
    "end": "1060640"
  },
  {
    "text": "way of node label uh a desired M configuration on one of your nodes and",
    "start": "1060640",
    "end": "1065799"
  },
  {
    "text": "what the m m manager will then do is it will possess that node level configuration the next time you perform",
    "start": "1065799",
    "end": "1072000"
  },
  {
    "text": "a driver upgrade and you need that Mig mode and Mig configuration restored on that node",
    "start": "1072000",
    "end": "1079600"
  },
  {
    "text": "the next is a GPU feature Discovery so this is another Damon set that basically",
    "start": "1079600",
    "end": "1086320"
  },
  {
    "text": "uh probes the GPU that is deployed on your um node and then exposes the GPU",
    "start": "1086320",
    "end": "1091720"
  },
  {
    "text": "details as lab node labels so let's see how a GPU feature Discovery could be",
    "start": "1091720",
    "end": "1096760"
  },
  {
    "text": "useful so let's say you have a cluster with multiple gpus and you know you um",
    "start": "1096760",
    "end": "1102520"
  },
  {
    "text": "you have a training job and you want to make sure you want to Target the training job to a powerful GPU like the",
    "start": "1102520",
    "end": "1107679"
  },
  {
    "text": "a100 now how do how do you make this possible because with the Nvidia device plug-in you're only exposing the GPU",
    "start": "1107679",
    "end": "1114440"
  },
  {
    "text": "count but with the GPU features Discovery however you can use node labels wherein you can uh you can have",
    "start": "1114440",
    "end": "1121880"
  },
  {
    "text": "your ml um training job use node selectors to Target the specific a100",
    "start": "1121880",
    "end": "1128799"
  },
  {
    "text": "instance and then it'll make sure that your ml training jobs will be only deployed on the a100",
    "start": "1128799",
    "end": "1136360"
  },
  {
    "text": "gpus so now let's go with the GPU operator like end to end like what it does this is what happens when you um",
    "start": "1136360",
    "end": "1144400"
  },
  {
    "text": "Helm install the GPU operator so the first component that runs is the node",
    "start": "1144400",
    "end": "1149480"
  },
  {
    "text": "feature Discovery so node feature Discovery is another component like an open source project that we import and",
    "start": "1149480",
    "end": "1156000"
  },
  {
    "text": "the GP uh the GPU operator runs this as a Damon set and the node feature Discovery probes the the node and",
    "start": "1156000",
    "end": "1163039"
  },
  {
    "text": "exposes the relevant features as labels in the case of GPU operator what we want",
    "start": "1163039",
    "end": "1168799"
  },
  {
    "text": "is the PCI label that uh confirms that the Nvidia gpus is present so once the",
    "start": "1168799",
    "end": "1175280"
  },
  {
    "text": "GPU detects this label generated by node feature Discovery GPU operator will know",
    "start": "1175280",
    "end": "1180960"
  },
  {
    "text": "to deploy the GPU um the GPU driver and the other components on that GPU node so",
    "start": "1180960",
    "end": "1188000"
  },
  {
    "text": "once it does that it deploys to um the Nvidia driver and then it validates a",
    "start": "1188000",
    "end": "1193240"
  },
  {
    "text": "driver install and that's when you know you can proceed to deploying the toolkit",
    "start": "1193240",
    "end": "1198760"
  },
  {
    "text": "and then once the toolkit is deployed then you run another round of validation then once the toolkit is validated",
    "start": "1198760",
    "end": "1204080"
  },
  {
    "text": "you're clear to deploy the rest of the components like the Kates device plugin the M manager the GBU feature Discovery",
    "start": "1204080",
    "end": "1210200"
  },
  {
    "text": "and the dcgm exporter and then once all of these components are deployed you have another",
    "start": "1210200",
    "end": "1215760"
  },
  {
    "text": "final round of validation that um makes sure that all the components are up and running and that's when you know your",
    "start": "1215760",
    "end": "1222280"
  },
  {
    "text": "node is ready to accept GPU workloads as you can see here the GPU operator has m",
    "start": "1222280",
    "end": "1228760"
  },
  {
    "text": "is automated a lot of tasks what would normally be done by cluster admins like they would deploy these components run",
    "start": "1228760",
    "end": "1235240"
  },
  {
    "text": "the validations and then ensure and then um open up the uh node to for GPU",
    "start": "1235240",
    "end": "1242280"
  },
  {
    "text": "workloads so you can see here that the GPU operator does it all for you and this is a model that will",
    "start": "1242280",
    "end": "1249320"
  },
  {
    "text": "scale so over to ganes for fing LOL operators",
    "start": "1249679",
    "end": "1256400"
  },
  {
    "text": "thanks s now we figured out how to get gpus up and running and configured with the GPU operator it's time to look at",
    "start": "1259440",
    "end": "1266640"
  },
  {
    "text": "the application side here we're looking at fine-tuning as the application uh for this",
    "start": "1266640",
    "end": "1274799"
  },
  {
    "text": "demo for fine tuning there's four main components that you can think of from a high level uh first is the pre-trained",
    "start": "1274799",
    "end": "1282120"
  },
  {
    "text": "model it could be something like a llama model or fi models and then you'd want",
    "start": "1282120",
    "end": "1288039"
  },
  {
    "text": "to know the data set that you're going to fine tune with for instance You' want to fine tune a language model to have",
    "start": "1288039",
    "end": "1294440"
  },
  {
    "text": "knowledge about medical data sets and that would you'd pick the appropriate",
    "start": "1294440",
    "end": "1300080"
  },
  {
    "text": "data set for that then you'd have to write the code for the training Loop so this would involve figuring out the",
    "start": "1300080",
    "end": "1306320"
  },
  {
    "text": "hyperparameters and picking the right algorithms for training or fine-tuning and then finally You' need",
    "start": "1306320",
    "end": "1312919"
  },
  {
    "text": "to think about What GPU nodes you need what type of resources are needed per GPU node and how many GPU nodes are",
    "start": "1312919",
    "end": "1320039"
  },
  {
    "text": "needed and then also write some code for parallelization if",
    "start": "1320039",
    "end": "1325320"
  },
  {
    "text": "needed this different ways of doing this but today operators have almost become a",
    "start": "1325320",
    "end": "1330760"
  },
  {
    "text": "standardized way for deploying machine learning applications on kubernetes cub and kerve are operators",
    "start": "1330760",
    "end": "1339080"
  },
  {
    "text": "where you could use uh distributed computing for inference and fine tuning",
    "start": "1339080",
    "end": "1344120"
  },
  {
    "text": "and even other types of distributed compute uh with the operator framework and and it has Integrations with many uh",
    "start": "1344120",
    "end": "1351080"
  },
  {
    "text": "ml Frameworks in the ecosystem the kubernetes AI tool chain operator is a very easy to use operator",
    "start": "1351080",
    "end": "1358440"
  },
  {
    "text": "for inference and fine-tuning workloads uh This Is An Open Source operator uh which is open source by Azure and we'll",
    "start": "1358440",
    "end": "1365200"
  },
  {
    "text": "be using that today to see how to deploy a fine-tuning workload this is a highle architecture",
    "start": "1365200",
    "end": "1372039"
  },
  {
    "text": "diagram of Kito uh you have two crds the",
    "start": "1372039",
    "end": "1377240"
  },
  {
    "text": "workspace crd and the provisioner crd and both the node provisioner crd is",
    "start": "1377240",
    "end": "1382440"
  },
  {
    "text": "optional and today uh we'll be replacing that part with uh the nodes which are",
    "start": "1382440",
    "end": "1387919"
  },
  {
    "text": "managed by AKs node pools and the GPU operator the workspace operator a",
    "start": "1387919",
    "end": "1393039"
  },
  {
    "text": "workspace crd is mainly looking at uh making sure that your inference and",
    "start": "1393039",
    "end": "1398640"
  },
  {
    "text": "fine-tuning workloads are configured properly and shows the various statuses for them and Kido also has a set of",
    "start": "1398640",
    "end": "1406000"
  },
  {
    "text": "predefined llms which are L used like the Llama models which and which are",
    "start": "1406000",
    "end": "1412200"
  },
  {
    "text": "pushed to the registry and those will be fetched during runtime as needed and",
    "start": "1412200",
    "end": "1417760"
  },
  {
    "text": "it's also configurable to add other models so for the demo we're going to",
    "start": "1417760",
    "end": "1425600"
  },
  {
    "text": "have four layers of the stack uh first we'll start off with the draining code which is written in py then that's",
    "start": "1425600",
    "end": "1433080"
  },
  {
    "text": "packaged as part of Kao as well and that has code for uh picking up the",
    "start": "1433080",
    "end": "1439840"
  },
  {
    "text": "fine-tuning config and deploying it on kubernetes we'll also have the gpus completely managed by the GPU operator",
    "start": "1439840",
    "end": "1446720"
  },
  {
    "text": "and then that's going to be running on Azure kuet service these steps can be changed but",
    "start": "1446720",
    "end": "1453880"
  },
  {
    "text": "the main goal we want to show is how operators come together to make it",
    "start": "1453880",
    "end": "1459240"
  },
  {
    "text": "simpler so in this demo I'm starting by creating a kubernetes cluster on AKs we",
    "start": "1463960",
    "end": "1470000"
  },
  {
    "text": "are going to see um nodes come up with just CPU nodes these are for running",
    "start": "1470000",
    "end": "1475880"
  },
  {
    "text": "system pods then I'm I'm creating a node pool uh with just one a100 GPU and I'm",
    "start": "1475880",
    "end": "1483640"
  },
  {
    "text": "skipping the driver installation so that the GPU operator manages uh the driver",
    "start": "1483640",
    "end": "1491440"
  },
  {
    "text": "configuration then I'm going to be describing the a100 node and we'll see",
    "start": "1493919",
    "end": "1499080"
  },
  {
    "text": "that there is nothing in this which says that there is a GPU configured um so",
    "start": "1499080",
    "end": "1505240"
  },
  {
    "text": "there's no GPU allocable there and then when you also grap for the GPU you see",
    "start": "1505240",
    "end": "1511919"
  },
  {
    "text": "that it's not present in in the spec now we are installing the GPU",
    "start": "1511919",
    "end": "1517080"
  },
  {
    "text": "operator to configure the GPU nodes we I'm using the default uh",
    "start": "1517080",
    "end": "1523919"
  },
  {
    "text": "installation parameters I'm not passing in anything so it'll pick the latest driver for instance and you see the crds",
    "start": "1523919",
    "end": "1529960"
  },
  {
    "text": "corresponding to the GP operator here and the state of that custom resource for cluster",
    "start": "1529960",
    "end": "1536120"
  },
  {
    "text": "policy here are the GP operator pods being deployed through by the operator",
    "start": "1536120",
    "end": "1542600"
  },
  {
    "text": "you will see the pods for the driver installation for the container toolkit",
    "start": "1542600",
    "end": "1548720"
  },
  {
    "text": "also see validators to make sure the components are configured properly and also the dcgm exporter and it's going to",
    "start": "1548720",
    "end": "1555799"
  },
  {
    "text": "be run in the order that Tark had mentioned earlier where you start off with the driver then the container toolkit and then the other the rest of",
    "start": "1555799",
    "end": "1563640"
  },
  {
    "text": "the components are installed and validated so it's going from in it state",
    "start": "1563640",
    "end": "1569360"
  },
  {
    "text": "to to running and completed and then we can check the",
    "start": "1569360",
    "end": "1576279"
  },
  {
    "text": "state of the custom Resource as well and we' see once all of this is",
    "start": "1576279",
    "end": "1583880"
  },
  {
    "text": "complete that the custom resource shows up as as",
    "start": "1583880",
    "end": "1590480"
  },
  {
    "text": "ready this is uh just a describe of the custom resource for cluster policy it",
    "start": "1592559",
    "end": "1598360"
  },
  {
    "text": "shows things like the driver versions also shows uh other states of the uh GP",
    "start": "1598360",
    "end": "1608960"
  },
  {
    "text": "operator and then here I'm doing a describe on the node and we can see that now the GPU resources are being shown",
    "start": "1615480",
    "end": "1622360"
  },
  {
    "text": "shown there and when you uh you can also port",
    "start": "1622360",
    "end": "1627640"
  },
  {
    "text": "forward the dcgm exporter and look at the metrics you can see it on a browser or",
    "start": "1627640",
    "end": "1634559"
  },
  {
    "text": "you could get it with M much better UI through uh grafana dashboards for",
    "start": "1634559",
    "end": "1640000"
  },
  {
    "text": "instance and that's especially useful when you have a large Fleet of GPU nodes to monitor this displays a lot of",
    "start": "1640000",
    "end": "1646080"
  },
  {
    "text": "information on GPU health and status and you can do the classic Nvidia SMI command through the driver pod now we're",
    "start": "1646080",
    "end": "1654960"
  },
  {
    "text": "going to the application state where we're deploying the Kido workspace",
    "start": "1654960",
    "end": "1661320"
  },
  {
    "text": "crd that's that's deployed now and",
    "start": "1662000",
    "end": "1667279"
  },
  {
    "text": "then we can see the workspace crd corresponding to",
    "start": "1667519",
    "end": "1674000"
  },
  {
    "text": "kto and then there's additional conf here for the workspace resource where we",
    "start": "1675039",
    "end": "1680600"
  },
  {
    "text": "specify the pre-train model which is five then we also have the data set that we're going to be using along with the",
    "start": "1680600",
    "end": "1687640"
  },
  {
    "text": "output of the container registry where we want the finetune model to be pushed",
    "start": "1687640",
    "end": "1693240"
  },
  {
    "text": "to we're going to be running this on the a100 Node apply the config it's going to then",
    "start": "1696519",
    "end": "1705039"
  },
  {
    "text": "create a pod to uh run the workload now it's just pulling the container",
    "start": "1705039",
    "end": "1711880"
  },
  {
    "text": "image and we can also just see the Pod to look at more details so it's pulling the uh the image",
    "start": "1711880",
    "end": "1721080"
  },
  {
    "text": "for the FI model and you can also see more stus",
    "start": "1721080",
    "end": "1726200"
  },
  {
    "text": "fields for the C",
    "start": "1726200",
    "end": "1730480"
  },
  {
    "text": "workspace and here are um some logs corresponding to the the fine tune model",
    "start": "1732279",
    "end": "1739480"
  },
  {
    "text": "fine tune fine tuning steps so here you see that it's just started doing fine",
    "start": "1739480",
    "end": "1744720"
  },
  {
    "text": "tuning and then later on after a while you see that the entire fine tuning is completed and you can see the loss uh",
    "start": "1744720",
    "end": "1751960"
  },
  {
    "text": "values also being printed for it and then this can be uploaded to the",
    "start": "1751960",
    "end": "1757120"
  },
  {
    "text": "registry and then you can use Kido again for inference uh this demo is showcasing two",
    "start": "1757120",
    "end": "1764279"
  },
  {
    "text": "of these operators together but you can leverage many other operators for similar workflows as well and it's quite",
    "start": "1764279",
    "end": "1771120"
  },
  {
    "text": "customizable as",
    "start": "1771120",
    "end": "1774240"
  },
  {
    "text": "needed so the key takeaways for a presentation are that from an infr provider and cluster admin perspective",
    "start": "1777880",
    "end": "1784919"
  },
  {
    "text": "it's very complex to manage GPU nodes while providing flexibility to users then kubernetes operators are a",
    "start": "1784919",
    "end": "1793200"
  },
  {
    "text": "great way to simplify and automate processes related to uh related to your",
    "start": "1793200",
    "end": "1799600"
  },
  {
    "text": "workloads and then uh you can automate GPU Resource Management through the GPU",
    "start": "1799600",
    "end": "1804720"
  },
  {
    "text": "operator and we also show how operators can be used at the application layer to simplify ml workload management and then",
    "start": "1804720",
    "end": "1813159"
  },
  {
    "text": "uh there's multiple layers of the ml deployment stack which can be simplified through the use of corresponding",
    "start": "1813159",
    "end": "1819360"
  },
  {
    "text": "operators and with this we hope that you've learned few new techniques and are encouraged to use operators to",
    "start": "1819360",
    "end": "1825679"
  },
  {
    "text": "simplify your AI infrastructure thank",
    "start": "1825679",
    "end": "1830600"
  },
  {
    "text": "you now open to",
    "start": "1835000",
    "end": "1838600"
  },
  {
    "text": "questions okay yeah",
    "start": "1846480",
    "end": "1850919"
  },
  {
    "text": "mic is",
    "start": "1858080",
    "end": "1860720"
  },
  {
    "text": "that thank",
    "start": "1870480",
    "end": "1873720"
  },
  {
    "text": "you thank you for your presentation I have one question about GPU operator uh",
    "start": "1876600",
    "end": "1883080"
  },
  {
    "text": "the GP operator will cover the driver in installation and does the upgrade but if",
    "start": "1883080",
    "end": "1890399"
  },
  {
    "text": "the uh driver P restart uh so everything will",
    "start": "1890399",
    "end": "1897360"
  },
  {
    "text": "reset uh the the driver will uninstall and reinstall uh and the application will be",
    "start": "1897360",
    "end": "1905880"
  },
  {
    "text": "crash uh how about this this thing is very very important for un",
    "start": "1905880",
    "end": "1912720"
  },
  {
    "text": "user okay got it so you mean to say that uh",
    "start": "1912720",
    "end": "1918039"
  },
  {
    "text": "um you have unintended driver upgrades which uh kind of cause your whole uh",
    "start": "1918039",
    "end": "1924200"
  },
  {
    "text": "software stack to get reset and then that kind of crashes your workloads yes",
    "start": "1924200",
    "end": "1929480"
  },
  {
    "text": "the the workload will crash yeah yeah yeah that's fair so we have a a field",
    "start": "1929480",
    "end": "1937480"
  },
  {
    "text": "called uh wait for completion and the driver upgrade and what wait for",
    "start": "1937480",
    "end": "1943120"
  },
  {
    "text": "completion does is you can um use a specific pod selector so what that does",
    "start": "1943120",
    "end": "1949159"
  },
  {
    "text": "is um we have the katees driver manager which runs as an in container that will",
    "start": "1949159",
    "end": "1954880"
  },
  {
    "text": "wait till those workloads are complete and only then proceed to um unload the",
    "start": "1954880",
    "end": "1961760"
  },
  {
    "text": "driver modules and you know continue on with the driver upgrade so you can definitely look into using the weight for completion I'm happy to share more",
    "start": "1961760",
    "end": "1968360"
  },
  {
    "text": "details offline oh okay offline",
    "start": "1968360",
    "end": "1973880"
  },
  {
    "text": "hello uh thank you for this uh exciting presentation uh I have a question about also mention uh related to the driver",
    "start": "1978720",
    "end": "1986679"
  },
  {
    "text": "installation uh as we know that the uh Nvidia uh operator has a uh crd uh about",
    "start": "1986679",
    "end": "1994039"
  },
  {
    "text": "the uh driver version right so uh we must have a uh driver installer the component on the running on the Node so",
    "start": "1994039",
    "end": "2002279"
  },
  {
    "text": "um but it's running in the container right so um if there are many versions",
    "start": "2002279",
    "end": "2008120"
  },
  {
    "text": "of the OS or kernel versions in the in the cluster then uh how can this uh the",
    "start": "2008120",
    "end": "2013919"
  },
  {
    "text": "driver installer running in the container knows the version of the uh Os",
    "start": "2013919",
    "end": "2019200"
  },
  {
    "text": "or Cent versions and because uh as we know that during the driver installation",
    "start": "2019200",
    "end": "2024600"
  },
  {
    "text": "they have to compile right that's great yes yeah that's a good question um so I",
    "start": "2024600",
    "end": "2030720"
  },
  {
    "text": "think one thing to note is that the driver container is a privileged container so it has visibility into the",
    "start": "2030720",
    "end": "2036960"
  },
  {
    "text": "host so and as you mentioned like one of the installation one of the steps in the driver installation is building like",
    "start": "2036960",
    "end": "2044039"
  },
  {
    "text": "compiling and building the kernel modules so during that step the GPU container is able to fetch the",
    "start": "2044039",
    "end": "2049960"
  },
  {
    "text": "underlying hosts kernel version and then it downloads the kernel headers accordingly builds the kernel modules um",
    "start": "2049960",
    "end": "2057398"
  },
  {
    "text": "compiles and builds the kernel modules does that answer your question yep it definitely give the driver container a",
    "start": "2057399",
    "end": "2063280"
  },
  {
    "text": "try because as as you run the driver container you'll you'll see in the logs the driver container discerns what uh OS",
    "start": "2063280",
    "end": "2070839"
  },
  {
    "text": "it's running on like what uh the kernel version is it fetches",
    "start": "2070839",
    "end": "2076280"
  },
  {
    "text": "it thanks for your presentations and I have a question here like uh why are you",
    "start": "2086079",
    "end": "2091280"
  },
  {
    "text": "doing the uh fine tuning job on the kubernetes uh platforms uh I uh noticed",
    "start": "2091280",
    "end": "2097359"
  },
  {
    "text": "that you have running the image directly so I'm not sure or very curious about that that the model and the data side",
    "start": "2097359",
    "end": "2104400"
  },
  {
    "text": "you do the container rest manually for these images and another questions is",
    "start": "2104400",
    "end": "2110400"
  },
  {
    "text": "about we all know that machine learning models have to do some like uh metadata",
    "start": "2110400",
    "end": "2115800"
  },
  {
    "text": "or version controlling so how do you uh handle this or figure this machine",
    "start": "2115800",
    "end": "2120960"
  },
  {
    "text": "learning metad datas thanks great questions so for the the",
    "start": "2120960",
    "end": "2127119"
  },
  {
    "text": "container images themselves so those are containing the foundational models so in this case it's the FI model that's been",
    "start": "2127119",
    "end": "2133960"
  },
  {
    "text": "containerized beforehand and there's a set of common or popular foundational",
    "start": "2133960",
    "end": "2139800"
  },
  {
    "text": "models that have been containerized already but then it's also customizable to add your own own ones the data set is",
    "start": "2139800",
    "end": "2147400"
  },
  {
    "text": "specified by the user in the config so there is a spec in the finetuning spec",
    "start": "2147400",
    "end": "2152480"
  },
  {
    "text": "there's a field to add your data set which can either just be the URL of the data dat set but it has to be formatted",
    "start": "2152480",
    "end": "2159599"
  },
  {
    "text": "in a particular way which is uh consumable or it could be containerized",
    "start": "2159599",
    "end": "2165440"
  },
  {
    "text": "and the Kido documentation also talks about like how to do it so you could containerize the data set and then so",
    "start": "2165440",
    "end": "2171440"
  },
  {
    "text": "that's the first question I think the second question is around management of",
    "start": "2171440",
    "end": "2177760"
  },
  {
    "text": "the data sets and models right for different versions I think that part is",
    "start": "2177760",
    "end": "2184280"
  },
  {
    "text": "uh there's different ways to do it right you could have it as container images you could have it as oras like artifacts",
    "start": "2184280",
    "end": "2189680"
  },
  {
    "text": "for instance I think here it's more focused on a common set of foundation",
    "start": "2189680",
    "end": "2196920"
  },
  {
    "text": "models so we haven't looked into the um the cases where you need to manage a ton",
    "start": "2196920",
    "end": "2203640"
  },
  {
    "text": "of different models but the container registry itself is like one option to do it and even uh the registry here",
    "start": "2203640",
    "end": "2211079"
  },
  {
    "text": "typically and most Cloud providers are supporting oras artifacts and that's one good way to manage multiple versions and",
    "start": "2211079",
    "end": "2218040"
  },
  {
    "text": "you can kind of combine that with uh Kido and the other operators to to make it work yeah thanks thanks for your",
    "start": "2218040",
    "end": "2224200"
  },
  {
    "text": "answers and I have a following uh think here like uh we have a recent kubernetes",
    "start": "2224200",
    "end": "2229599"
  },
  {
    "text": "of new features that we could mounted the uh uh Mount the images as volume uh",
    "start": "2229599",
    "end": "2236160"
  },
  {
    "text": "directly into the uh containers already so maybe in the Futures we could mounted",
    "start": "2236160",
    "end": "2241599"
  },
  {
    "text": "both the data set and the model itself as the images and uh as well you know that the for the model Registries we",
    "start": "2241599",
    "end": "2249400"
  },
  {
    "text": "have uh many capabilities to do uh the uh version versioning or the security",
    "start": "2249400",
    "end": "2255480"
  },
  {
    "text": "checkings for yeah yeah uh thanks and linking yeah that's that's actually",
    "start": "2255480",
    "end": "2260720"
  },
  {
    "text": "something we're looking into as well uh so one part of that is also related to",
    "start": "2260720",
    "end": "2266079"
  },
  {
    "text": "pod start times which is um we want to be able to make sure the pods start uh",
    "start": "2266079",
    "end": "2272440"
  },
  {
    "text": "fast and sometimes that can be a helpful approach um the other approach for that",
    "start": "2272440",
    "end": "2277760"
  },
  {
    "text": "a bit of a Sidetrack but is to use lazy image loading for that and you know",
    "start": "2277760",
    "end": "2282920"
  },
  {
    "text": "that's something that we support in um in AKs but that's uh one way to address",
    "start": "2282920",
    "end": "2288359"
  },
  {
    "text": "like one of the underlying problems for for using it but we're looking into supporting uh more of this native mounts",
    "start": "2288359",
    "end": "2295599"
  },
  {
    "text": "thanks for an thanks for your time thank you there's one more question",
    "start": "2295599",
    "end": "2301838"
  },
  {
    "text": "there okay uh thank you my question is about the RDMA part because RDMA is very",
    "start": "2305880",
    "end": "2312440"
  },
  {
    "text": "important in AI workload so does Invidia has any plan about providing an RDMA",
    "start": "2312440",
    "end": "2321119"
  },
  {
    "text": "operator which can manage a massive RDMA",
    "start": "2321119",
    "end": "2326480"
  },
  {
    "text": "n sure yeah that there does exist an operator called a network operator um so you can go to github.com melanox network",
    "start": "2326480",
    "end": "2334760"
  },
  {
    "text": "operator and you'll find yeah yeah you have people using um the network",
    "start": "2334760",
    "end": "2340160"
  },
  {
    "text": "operator uh to to achieve the the very Pro you know uh solution that you mentioned uh how about the MV link part",
    "start": "2340160",
    "end": "2348560"
  },
  {
    "text": "about the GPU that they provide in the GPU operator so",
    "start": "2348560",
    "end": "2353960"
  },
  {
    "text": "um one thing to note is that there is a setting in the clust of policy where you",
    "start": "2353960",
    "end": "2360319"
  },
  {
    "text": "can enable RDMA and what that does is it loads the NV PRM module",
    "start": "2360319",
    "end": "2368119"
  },
  {
    "text": "that is packaged into the driver container that is in turn you leveraged",
    "start": "2368119",
    "end": "2374440"
  },
  {
    "text": "by the network network operator you and so",
    "start": "2374440",
    "end": "2380079"
  },
  {
    "text": "when you want to run your nickel tests you know you can have your network operator and a GPU operator running together and you have uh pods that are",
    "start": "2380079",
    "end": "2388119"
  },
  {
    "text": "able to request G um gpus and melanox NEX resources okay thanks so yeah so the",
    "start": "2388119",
    "end": "2395200"
  },
  {
    "text": "network operator basically run is created in the same way as and runs the same much like the GPU operator",
    "start": "2395200",
    "end": "2404359"
  },
  {
    "text": "okay I think we're done with time so thank you all for your questions and for",
    "start": "2404560",
    "end": "2410160"
  },
  {
    "text": "your time [Applause]",
    "start": "2410160",
    "end": "2414889"
  }
]