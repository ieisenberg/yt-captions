[
  {
    "text": "welcome everyone and thank you uh for coming to my talk I'm super excited that",
    "start": "240",
    "end": "6839"
  },
  {
    "text": "all of you all are here and I cannot wait to take you on a journey with large language",
    "start": "6839",
    "end": "12320"
  },
  {
    "text": "models um you've all you've all noticed that in recent times there's been an",
    "start": "12320",
    "end": "18720"
  },
  {
    "text": "undeniable Buzz surrounding large language models or as you call them",
    "start": "18720",
    "end": "24320"
  },
  {
    "text": "llms perhaps you've encountered discussions articles or even first and",
    "start": "24320",
    "end": "30400"
  },
  {
    "text": "experience the remarkable capabilities these models have to offer yet despite",
    "start": "30400",
    "end": "36440"
  },
  {
    "text": "all this excitement many of us may find ourselves hesitant to dive in the world",
    "start": "36440",
    "end": "42280"
  },
  {
    "text": "of llms held back by the perceived complexity of deploying and utilizing",
    "start": "42280",
    "end": "49920"
  },
  {
    "text": "these powerful tools so let's get into the REM of large language models and hosting your own",
    "start": "49920",
    "end": "57199"
  },
  {
    "text": "models using kubernetes to harness the full potential of llms within your own",
    "start": "57199",
    "end": "62840"
  },
  {
    "text": "projects and workflows so before we get into the world of llms I'll take a quick moment",
    "start": "62840",
    "end": "69840"
  },
  {
    "text": "to introduce myself and my co-speaker my name is akanga Dugal I am a senior data",
    "start": "69840",
    "end": "75840"
  },
  {
    "text": "scientist in the office of the CTO at Red Hat I work in the emerging Technologies data science team my",
    "start": "75840",
    "end": "82880"
  },
  {
    "text": "co-speaker HMA veradi is also a senior data scientist in my team but unfortunately couldn't make it here",
    "start": "82880",
    "end": "88920"
  },
  {
    "text": "today so in this talk we are going to talk about what is llms where does llm come",
    "start": "88920",
    "end": "96799"
  },
  {
    "text": "from we'll also go over what are some open source llms what are the commercial",
    "start": "96799",
    "end": "103079"
  },
  {
    "text": "llms that you can get started with what are some close source llms that you should be careful when you're using them",
    "start": "103079",
    "end": "110079"
  },
  {
    "text": "then we'll go over what are the various steps when you're trying to build a large language model application then",
    "start": "110079",
    "end": "115840"
  },
  {
    "text": "we'll quickly go over the concept of self-hosting your large language model models and Then followed by a demo we",
    "start": "115840",
    "end": "122880"
  },
  {
    "text": "also help you set up your own llm using kubernetes before diving into llms let's",
    "start": "122880",
    "end": "131039"
  },
  {
    "text": "take a step back let's address a language model first a language model is",
    "start": "131039",
    "end": "136879"
  },
  {
    "text": "a type of machine learning model that is trained to conduct a probability distribution over words simply put it",
    "start": "136879",
    "end": "144519"
  },
  {
    "text": "tries to predict the next most appropriate word to fill in a blank space in a sentence",
    "start": "144519",
    "end": "150599"
  },
  {
    "text": "or a phrase based on the context of the previously given text just like when you're trying to write an email or a",
    "start": "150599",
    "end": "157480"
  },
  {
    "text": "text message the language model tries to predict the next set of words so the job",
    "start": "157480",
    "end": "163840"
  },
  {
    "text": "of a language model is to approximate a function that would fit your input data",
    "start": "163840",
    "end": "169319"
  },
  {
    "text": "so if the input data is onedimensional like on the left um you can have a",
    "start": "169319",
    "end": "175360"
  },
  {
    "text": "linear Trend that linear function that basically fits the input data but if",
    "start": "175360",
    "end": "180640"
  },
  {
    "text": "your input data is natural language or an image we need Advanced architectures",
    "start": "180640",
    "end": "186519"
  },
  {
    "text": "to approximate the function there are various kind of language models that can be put to use statistical language",
    "start": "186519",
    "end": "193760"
  },
  {
    "text": "models are type of models that use statistical patterns in the data to make predictions about the likelihood of",
    "start": "193760",
    "end": "200640"
  },
  {
    "text": "specific con sequences of words just like the probabilistic language models use engram",
    "start": "200640",
    "end": "208200"
  },
  {
    "text": "probabilities then comes the neural language models they are similar to neural networks in the way that they",
    "start": "208200",
    "end": "214920"
  },
  {
    "text": "predict the likelihood of sequence of words but the only difference is that these models are trained on a larger",
    "start": "214920",
    "end": "220640"
  },
  {
    "text": "carpus of words and images then comes the large language",
    "start": "220640",
    "end": "226120"
  },
  {
    "text": "models llm is just a larger version of a language model a language model is more",
    "start": "226120",
    "end": "233640"
  },
  {
    "text": "generic than a large language model just like all squares are rectangles but not",
    "start": "233640",
    "end": "239360"
  },
  {
    "text": "all rectangles are squares all llms are language models but not all language",
    "start": "239360",
    "end": "244480"
  },
  {
    "text": "models are llms so what makes llm special as compared to the other",
    "start": "244480",
    "end": "250239"
  },
  {
    "text": "language models so there are two key differ uh differentiators when it comes",
    "start": "250239",
    "end": "255439"
  },
  {
    "text": "to large language models the first quantitatively uh what distinguishes llm",
    "start": "255439",
    "end": "261400"
  },
  {
    "text": "from a language model is the amount of data that has been put in the model current large language models have the",
    "start": "261400",
    "end": "268080"
  },
  {
    "text": "order of 10 to 100 100 billion parameters qualitatively something",
    "start": "268080",
    "end": "273639"
  },
  {
    "text": "remarkable happens when the mod model gets to this size it exhibits properties",
    "start": "273639",
    "end": "279000"
  },
  {
    "text": "like zero shot learning and these kind of properties only happen when the model",
    "start": "279000",
    "end": "284240"
  },
  {
    "text": "has reached a significantly large size back in the day a model needed to be",
    "start": "284240",
    "end": "289720"
  },
  {
    "text": "explicitly trained on a task it had to be aim uh it had to aim for a good performance and it required",
    "start": "289720",
    "end": "296240"
  },
  {
    "text": "approximately around 1,000 to 1 million pre-label train examples so for example if you were to",
    "start": "296240",
    "end": "302639"
  },
  {
    "text": "do translation or sentiment analysis each of these tasks would require",
    "start": "302639",
    "end": "308520"
  },
  {
    "text": "separate labeled examples but however llms today can do all of this without",
    "start": "308520",
    "end": "315160"
  },
  {
    "text": "any explicit training technically a large language model is a type of AI that is trained on",
    "start": "315160",
    "end": "321720"
  },
  {
    "text": "a massive data set of text and code this allows the model to learn the statistical relationships between words",
    "start": "321720",
    "end": "329199"
  },
  {
    "text": "and PHR phrases which in turn allows it to generate text translate languages",
    "start": "329199",
    "end": "334720"
  },
  {
    "text": "write creative content answer your questions and so much more to",
    "start": "334720",
    "end": "340120"
  },
  {
    "text": "that so let's talk a little about the open source and the close- source models that are available for usage some of the",
    "start": "340120",
    "end": "347360"
  },
  {
    "text": "commonly used open-source models are llama 2 by meta Falcon Mistral then",
    "start": "347360",
    "end": "354120"
  },
  {
    "text": "there are famous close Source models as well like gp4 Gemini and Claude",
    "start": "354120",
    "end": "360759"
  },
  {
    "text": "but in a recent evaluation conducted by rbond University several instruction",
    "start": "360759",
    "end": "367240"
  },
  {
    "text": "tuned text generators including Lama 2 underwent scrutiny regarding their open source claims so the study",
    "start": "367240",
    "end": "373960"
  },
  {
    "text": "comprehensively assessed availability the doc documentation quality the access",
    "start": "373960",
    "end": "379479"
  },
  {
    "text": "methods and they also tried to rank some of these models on the basis of their openness So Meta quite did not fit into",
    "start": "379479",
    "end": "388560"
  },
  {
    "text": "the category of openness so I've also put a link here which from from which",
    "start": "388560",
    "end": "393800"
  },
  {
    "text": "you can take a look at all the open source models that are available for commercial use in order to make sure",
    "start": "393800",
    "end": "399680"
  },
  {
    "text": "that you're not using somebody else's proprietary model and their proprietary data associated with it and as these",
    "start": "399680",
    "end": "406160"
  },
  {
    "text": "large language models are becoming increasingly sophisticated there's a growing emphasis on democratizing the",
    "start": "406160",
    "end": "413759"
  },
  {
    "text": "access to these models as well open- Source models in particular are playing",
    "start": "413759",
    "end": "418960"
  },
  {
    "text": "a pivotal role in this democratization and offering researchers developers and enthusiasts the",
    "start": "418960",
    "end": "426120"
  },
  {
    "text": "opportunity to delve into the intricacies fine-tune these amazing models for specific tasks and even build",
    "start": "426120",
    "end": "433199"
  },
  {
    "text": "upon the foundation of this model so on the basis of the technical",
    "start": "433199",
    "end": "439800"
  },
  {
    "text": "uh technical knowledge and the computational resources that are available we have three levels of",
    "start": "439800",
    "end": "446240"
  },
  {
    "text": "utilizing these large language models so the first one is prompt",
    "start": "446240",
    "end": "451520"
  },
  {
    "text": "engineering using a large language model out of the box without changing any",
    "start": "451520",
    "end": "456560"
  },
  {
    "text": "model parameters is the most accessible way to get started with llms both",
    "start": "456560",
    "end": "461840"
  },
  {
    "text": "technically and economically it requires very little knowledge to get started with prompt",
    "start": "461840",
    "end": "468159"
  },
  {
    "text": "engineering and even when you're trying to do this there are two ways to approach prompt engineering the first",
    "start": "468159",
    "end": "474479"
  },
  {
    "text": "one the easy way the most common and I'm pretty sure everybody in this room has used it chat GPT or any llm UI is the",
    "start": "474479",
    "end": "483800"
  },
  {
    "text": "con most convenient tool to get started with large language models tools like chat GPT have U no cost no code",
    "start": "483800",
    "end": "492599"
  },
  {
    "text": "requirement and it's pretty intuitive to get started with that and it doesn't get any easier than that but it does come",
    "start": "492599",
    "end": "499560"
  },
  {
    "text": "with a lack of functionality and the customers cannot customize the model um",
    "start": "499560",
    "end": "505400"
  },
  {
    "text": "by any model parameters or having a separate data set you cannot change the temperature or the",
    "start": "505400",
    "end": "511759"
  },
  {
    "text": "maximum token length of these models which are the values that are required to modulate the llm",
    "start": "511759",
    "end": "517279"
  },
  {
    "text": "outputs the second way is to interact with the large language model directly in a programmatic fashion so since we've",
    "start": "517279",
    "end": "525360"
  },
  {
    "text": "seen that there are drawbacks that we cannot address using the UI so we can also access these models using their",
    "start": "525360",
    "end": "532080"
  },
  {
    "text": "apis in a Jupiter notebook or a python script as well we will also go over an",
    "start": "532080",
    "end": "537959"
  },
  {
    "text": "example where you can sell Host this model and also do PR prompt tuning later",
    "start": "537959",
    "end": "543120"
  },
  {
    "text": "but for now I'm going to just talk about how you can access the public apis so",
    "start": "543120",
    "end": "548480"
  },
  {
    "text": "again I will continue with the example of open AI which also has an API key associated with it however that may have",
    "start": "548480",
    "end": "555760"
  },
  {
    "text": "some cost associated with it it's still a good beginner friendly way to get started with that",
    "start": "555760",
    "end": "560880"
  },
  {
    "text": "model while all of this is easy and less convenient also at the same time because",
    "start": "560880",
    "end": "567320"
  },
  {
    "text": "of some cost Associated it this it does provide you with a customizable and a flexible way to get started with",
    "start": "567320",
    "end": "576040"
  },
  {
    "text": "llms so have you all noticed that when you tune your prompts a little bit uh on",
    "start": "576040",
    "end": "582000"
  },
  {
    "text": "chat GPT or when you're trying to interact with an llm using programs have you thought that when you give prompts",
    "start": "582000",
    "end": "588839"
  },
  {
    "text": "like let's think step by step or prompts like let's take a deep breath and work on this problem slowly then the third",
    "start": "588839",
    "end": "597079"
  },
  {
    "text": "one has provided some amazing results which says this is very important to my",
    "start": "597079",
    "end": "602480"
  },
  {
    "text": "career this has had 5 to 20% increase in the uh efficiency of the model so this",
    "start": "602480",
    "end": "610279"
  },
  {
    "text": "is the art of ROM tuning where you specify how important a particular task is to your llm and the output gets",
    "start": "610279",
    "end": "616839"
  },
  {
    "text": "significantly better the second level of using a large language model is fine tuning",
    "start": "616839",
    "end": "623720"
  },
  {
    "text": "fine-tuning is a process of continuing the training on a pre-trained large language model on a specific data set",
    "start": "623720",
    "end": "631440"
  },
  {
    "text": "which I'll Define as taking an existing pre-train large language model and then",
    "start": "631440",
    "end": "636959"
  },
  {
    "text": "tweaking at least one model parameter in the context of large language models and",
    "start": "636959",
    "end": "642959"
  },
  {
    "text": "neural networks in general the model parameters refer to the trainable variables that the model learns during",
    "start": "642959",
    "end": "649399"
  },
  {
    "text": "the training process like weights and biases these parameters are also adjusted iteratively through",
    "start": "649399",
    "end": "655839"
  },
  {
    "text": "optimization algorithms like stochastic gradient descent and many more in order",
    "start": "655839",
    "end": "661360"
  },
  {
    "text": "to minimize the difference between the model's predictions and the ground truth of this data",
    "start": "661360",
    "end": "666680"
  },
  {
    "text": "set this is a powerful approach to the model development because it is",
    "start": "666680",
    "end": "672200"
  },
  {
    "text": "relatively it requires a relatively smaller numbers of examples and the computational resources that are",
    "start": "672200",
    "end": "678440"
  },
  {
    "text": "required here are also much smaller than than building your own large language",
    "start": "678440",
    "end": "683920"
  },
  {
    "text": "model and this can also produce exceptional model performance for a small company or a small small project",
    "start": "683920",
    "end": "689959"
  },
  {
    "text": "as well and then third and the final way is to build your own L large language",
    "start": "689959",
    "end": "695600"
  },
  {
    "text": "model in terms of model parameters this is where you come up with all the model",
    "start": "695600",
    "end": "700920"
  },
  {
    "text": "parameters from scratch so as I mentioned earlier the llm is primarily a",
    "start": "700920",
    "end": "706639"
  },
  {
    "text": "product of its training data thus for some applications it may be necessary to",
    "start": "706639",
    "end": "712880"
  },
  {
    "text": "curate a custom high quality text corpora for model training for example",
    "start": "712880",
    "end": "719040"
  },
  {
    "text": "if if you were to build a medical research Corpus for the development of a",
    "start": "719040",
    "end": "724120"
  },
  {
    "text": "clinical application you would need specific medical data for that the biggest upside to this approach",
    "start": "724120",
    "end": "730480"
  },
  {
    "text": "is that you can fully customize this llm for your particular use case this provides you the ultimate flexibility",
    "start": "730480",
    "end": "737360"
  },
  {
    "text": "and however as often the case the flexibility does come at the cost of convenience since L lm's",
    "start": "737360",
    "end": "745839"
  },
  {
    "text": "performance uh is mostly dependent on scale and building the llm from scratch",
    "start": "745839",
    "end": "751160"
  },
  {
    "text": "would require not only a lot of computational resources but it does require a certain amount of technical",
    "start": "751160",
    "end": "757560"
  },
  {
    "text": "expertise and it's not a project that can be put together over a weekend so it requires months and months of effort to",
    "start": "757560",
    "end": "763800"
  },
  {
    "text": "build your own large language model from scratch so now um having said all the",
    "start": "763800",
    "end": "769880"
  },
  {
    "text": "levels of uh large language models I think for today's talk I will focus",
    "start": "769880",
    "end": "775320"
  },
  {
    "text": "mainly on the first two parts where we deal with promp tuning and and fine-tuning and in order to build your",
    "start": "775320",
    "end": "782920"
  },
  {
    "text": "own large language model these are the few steps that we will have to follow the initial step is very crucial as this",
    "start": "782920",
    "end": "791040"
  },
  {
    "text": "sets the foundation for your llm application so we have to take the time clearly to define the kind of problem",
    "start": "791040",
    "end": "796720"
  },
  {
    "text": "you want to address we have to answer questions like what are the tasks that we would want our llm to do what is the",
    "start": "796720",
    "end": "803120"
  },
  {
    "text": "kind of target audience what are the needs and the pain points and then we",
    "start": "803120",
    "end": "808199"
  },
  {
    "text": "also have to conduct a the thorough research and gather insights to ensure that the llm application also aligns",
    "start": "808199",
    "end": "814040"
  },
  {
    "text": "with the real world challenges and opportunities once we've identified the problem and outlined the goals it's time",
    "start": "814040",
    "end": "820600"
  },
  {
    "text": "to start building the application first begin by selecting the appropriate llm for the task we there",
    "start": "820600",
    "end": "827199"
  },
  {
    "text": "are tons of platforms where you can get uh models or large language models particularly for the task I will go over",
    "start": "827199",
    "end": "834199"
  },
  {
    "text": "that shortly but for example hugging face has tons of uh open source models that you can get started with and after",
    "start": "834199",
    "end": "841639"
  },
  {
    "text": "you have an appropriate llm that you have for your task consider factors like model size the performance compatib",
    "start": "841639",
    "end": "849639"
  },
  {
    "text": "compatibility with your problem domain and so much more next customize this",
    "start": "849639",
    "end": "855480"
  },
  {
    "text": "large language model to suit your needs this may involve promp tuning and fine-tuning of the model on specific",
    "start": "855480",
    "end": "862759"
  },
  {
    "text": "data set or adjusting the parameters to optimizing the performance Additionally",
    "start": "862759",
    "end": "868079"
  },
  {
    "text": "you have to also set up an architecture for your application which ensures that it has appropriate integration with the",
    "start": "868079",
    "end": "875199"
  },
  {
    "text": "other systems as well and once the model is deployed it's time to put it to test",
    "start": "875199",
    "end": "880880"
  },
  {
    "text": "Implement evaluation mechanisms to assess the performance and effectiveness of your application this may involve uh",
    "start": "880880",
    "end": "888240"
  },
  {
    "text": "metrics such as uh accuracy speed and user satisfaction you can also use Lang",
    "start": "888240",
    "end": "893959"
  },
  {
    "text": "chain and so much so many other scoring uh metrics for testing your large",
    "start": "893959",
    "end": "899240"
  },
  {
    "text": "language model you can also gather feedback from users and stakeholders to identify the areas for improvement and",
    "start": "899240",
    "end": "907560"
  },
  {
    "text": "iteration continuous inte Integra uh continuous iteration based on feedback",
    "start": "907560",
    "end": "913279"
  },
  {
    "text": "is the key to refining and enhancing the capabilities of your large language",
    "start": "913279",
    "end": "919160"
  },
  {
    "text": "model so answering the question to self host or to not to in this rapidly",
    "start": "919160",
    "end": "926199"
  },
  {
    "text": "evolving landscape of large language models the question to whether to sell for or opt for a proprietary solution",
    "start": "926199",
    "end": "934160"
  },
  {
    "text": "that has gained considerable significance over time this decision requires a comprehensive evaluation of",
    "start": "934160",
    "end": "941519"
  },
  {
    "text": "factors such as customization needs scalability requirements Regulatory",
    "start": "941519",
    "end": "946720"
  },
  {
    "text": "Compliance and most importantly technical expertise self-hosting offers",
    "start": "946720",
    "end": "952600"
  },
  {
    "text": "unparal unparalleled customization options and greater control over the",
    "start": "952600",
    "end": "957920"
  },
  {
    "text": "data providing organizations the flexibility to tailor their solutions to",
    "start": "957920",
    "end": "963279"
  },
  {
    "text": "the specific requirements on the other hand the proprietary of the Shelf options offer convenience but may",
    "start": "963279",
    "end": "970759"
  },
  {
    "text": "present limitations in customization and long-term flexibility moreover self-hosting ensures that Optimal",
    "start": "970759",
    "end": "979880"
  },
  {
    "text": "Performance",
    "start": "987920",
    "end": "990920"
  },
  {
    "text": "okay okay I'm going to start again so moreover self-hosting ensures Optimal",
    "start": "1002360",
    "end": "1008399"
  },
  {
    "text": "Performance and reduced latency crucial for applications that require realtime",
    "start": "1008399",
    "end": "1013480"
  },
  {
    "text": "responses in contrast the open API introduces dependencies on external",
    "start": "1013480",
    "end": "1019360"
  },
  {
    "text": "servers potentially leading to service disruptions and operational challenges",
    "start": "1019360",
    "end": "1024760"
  },
  {
    "text": "long-term cost considerations also favor self hosting as it eliminates the recuring subscription costs associated",
    "start": "1024760",
    "end": "1031880"
  },
  {
    "text": "with the third party providers however it's also essential to recognize that",
    "start": "1031880",
    "end": "1037000"
  },
  {
    "text": "self-hosting requires upfront investment in infrastructure and expertise unless",
    "start": "1037000",
    "end": "1042199"
  },
  {
    "text": "you are doing a small project or in a small team then you would not want that",
    "start": "1042199",
    "end": "1047720"
  },
  {
    "text": "many resources additionally self-hosting promotes flexibility and adaptability enabling",
    "start": "1047720",
    "end": "1054280"
  },
  {
    "text": "seamless integration of updates and advancements on the other hand changes",
    "start": "1054280",
    "end": "1059720"
  },
  {
    "text": "to open apis May disrupt workflows and may require adjustments creating uncertainity and eff",
    "start": "1059720",
    "end": "1067200"
  },
  {
    "text": "inefficiencies so in short if you were to build a quick prototype and test a hypothesis of a large language model",
    "start": "1067200",
    "end": "1074600"
  },
  {
    "text": "sure the best solution is using open AI API but if privacy is one of the major",
    "start": "1074600",
    "end": "1081039"
  },
  {
    "text": "concerns then you must opt for self-hosted llms even open AIS terms of",
    "start": "1081039",
    "end": "1086559"
  },
  {
    "text": "use mention and I quote may use content from Services other than our API to help",
    "start": "1086559",
    "end": "1093240"
  },
  {
    "text": "develop and improve our services this means that anything that you send to chat GPT or sent to their uh API key may",
    "start": "1093240",
    "end": "1101600"
  },
  {
    "text": "or may not be included in their training data so despite the anonymizing of the data efforts it still contributes to the",
    "start": "1101600",
    "end": "1108960"
  },
  {
    "text": "knowledge of model in conclusion if you deal with sensitive data or you privacy",
    "start": "1108960",
    "end": "1115880"
  },
  {
    "text": "is the most important thing for you I would suggest that you definitely use self-hosted models you are going to be",
    "start": "1115880",
    "end": "1122679"
  },
  {
    "text": "in 100% control of your model and you are the only party which is responsible for the system up time and there there",
    "start": "1122679",
    "end": "1129640"
  },
  {
    "text": "would be no failures on external services or any API quota limits so you have 100% control of your",
    "start": "1129640",
    "end": "1137720"
  },
  {
    "text": "model so now let's talk about self-hosting containerized large language models so first of all given",
    "start": "1137720",
    "end": "1145919"
  },
  {
    "text": "the kind of problem you're trying to solve you will have to select a large language model from hugging",
    "start": "1145919",
    "end": "1152080"
  },
  {
    "text": "face hugging face is a platform with over 350k models 75,000 data sets and",
    "start": "1152080",
    "end": "1159000"
  },
  {
    "text": "150,000 demo applications all of which is open source and publicly available to",
    "start": "1159000",
    "end": "1164039"
  },
  {
    "text": "use in an online platform where U machine learning folks can collabor orate easily and once we have chosen the",
    "start": "1164039",
    "end": "1171880"
  },
  {
    "text": "model for our application we will containerize this large language model locally on our laptop using container",
    "start": "1171880",
    "end": "1179320"
  },
  {
    "text": "engines like podman or Docker and once this model is containerized we can use",
    "start": "1179320",
    "end": "1185799"
  },
  {
    "text": "this container image along with fast API to serve the model additionally this container image",
    "start": "1185799",
    "end": "1193600"
  },
  {
    "text": "can be pulled into self hosted clusters or in any Cube environment and if you",
    "start": "1193600",
    "end": "1199240"
  },
  {
    "text": "want to run this locally you can continue using container engines like podman or Docker now let's take a look",
    "start": "1199240",
    "end": "1205960"
  },
  {
    "text": "at a demo that we've put together uh that Pro helps you with a use case which is going to convert speech to text by",
    "start": "1205960",
    "end": "1213240"
  },
  {
    "text": "self-hosting your large language model so this is a demo from my",
    "start": "1213240",
    "end": "1219960"
  },
  {
    "text": "colleague himma who couldn't be here today so I'm going to go ahead and play that hello everyone my name is HMA vadi",
    "start": "1219960",
    "end": "1228400"
  },
  {
    "text": "and I'm I'm also working as a senior data scientist along with akanga in the emerging technology data science team at",
    "start": "1228400",
    "end": "1235240"
  },
  {
    "text": "Red Hat unfortunately I couldn't be here today in person at cubec con however I",
    "start": "1235240",
    "end": "1240360"
  },
  {
    "text": "do have a small demo for you all and I hope you enjoy it and also give it a try",
    "start": "1240360",
    "end": "1245480"
  },
  {
    "text": "once you're free after the conference so as AA already kind of walked us through",
    "start": "1245480",
    "end": "1251640"
  },
  {
    "text": "what an llm is how do you kind of set up an llm and how do we actually have all",
    "start": "1251640",
    "end": "1257280"
  },
  {
    "text": "this running on our local local machines and laptops as most of us are aware the",
    "start": "1257280",
    "end": "1262760"
  },
  {
    "text": "large language models that we have all seen or interacted with are quite large",
    "start": "1262760",
    "end": "1268120"
  },
  {
    "text": "and compute intensive so it's nearly impossible to kind of spin it up and try",
    "start": "1268120",
    "end": "1273720"
  },
  {
    "text": "it out for ourselves on our laptop because of the resource constraints that we have however if you are completely",
    "start": "1273720",
    "end": "1281760"
  },
  {
    "text": "new to this entire generative AI space and if you're a developer or if you're",
    "start": "1281760",
    "end": "1287159"
  },
  {
    "text": "just a new user who trying to get familiar with large language models and you just want to test out some of these",
    "start": "1287159",
    "end": "1293520"
  },
  {
    "text": "models to kind of see how they run how they work how you can interact with it",
    "start": "1293520",
    "end": "1298960"
  },
  {
    "text": "how do we see how well it's doing and just kind of doing some development",
    "start": "1298960",
    "end": "1304159"
  },
  {
    "text": "locally then an easy solution is to kind of set it up yourself through a",
    "start": "1304159",
    "end": "1309600"
  },
  {
    "text": "containerized fashion or through kind of um um having it run based on the",
    "start": "1309600",
    "end": "1316120"
  },
  {
    "text": "resources that you have on your laptop so that's precisely what this demo is",
    "start": "1316120",
    "end": "1321159"
  },
  {
    "text": "going to do we are going to use the concept of containerization and kubernetes and as most of you all are",
    "start": "1321159",
    "end": "1328480"
  },
  {
    "text": "here today at cucon I'm taking it that all of you have interacted or have used",
    "start": "1328480",
    "end": "1334360"
  },
  {
    "text": "containerization techniques at some point or are at least familiar with the concept and that's what uh this demo is",
    "start": "1334360",
    "end": "1342120"
  },
  {
    "text": "all about is how can we use containerization techniques to kind of wrap up all of these llm mod mods and",
    "start": "1342120",
    "end": "1349480"
  },
  {
    "text": "have it running within our local machines so the example llm use case",
    "start": "1349480",
    "end": "1354679"
  },
  {
    "text": "that we're looking at today is that of speech recognition so what we're doing",
    "start": "1354679",
    "end": "1359840"
  },
  {
    "text": "is we want to basically translate a non-english speech and give the",
    "start": "1359840",
    "end": "1365480"
  },
  {
    "text": "translation in a text format generated in English so in order to do this task",
    "start": "1365480",
    "end": "1371279"
  },
  {
    "text": "luckily for us there are already a bunch of llms who do a pretty good job at",
    "start": "1371279",
    "end": "1376440"
  },
  {
    "text": "speech recognition and language translation so for today's demo we're going to be using the open AI whisper",
    "start": "1376440",
    "end": "1384120"
  },
  {
    "text": "model this whisper model has been developed by open Ai and it has also been licensed under Apache which is a",
    "start": "1384120",
    "end": "1391720"
  },
  {
    "text": "good sign that you can kind of use it um for your open- Source projects and",
    "start": "1391720",
    "end": "1396880"
  },
  {
    "text": "contributions that you would like to do on top of it so um The Whisper model itself has a bunch of different flavors",
    "start": "1396880",
    "end": "1404080"
  },
  {
    "text": "to it in the sense that it has different sizes or variations of models that you",
    "start": "1404080",
    "end": "1409559"
  },
  {
    "text": "can use and uh the nice thing about this is that since we are testing it locally",
    "start": "1409559",
    "end": "1414880"
  },
  {
    "text": "on our laptop we can kind of choose a small model or a more compressed model",
    "start": "1414880",
    "end": "1420279"
  },
  {
    "text": "rather than using the entire base uh model which requires like um tons of",
    "start": "1420279",
    "end": "1426120"
  },
  {
    "text": "compute resources in order to run them so uh these models which have been",
    "start": "1426120",
    "end": "1431200"
  },
  {
    "text": "compressed and optimized are just model binaries which are about 500 MB to about",
    "start": "1431200",
    "end": "1437200"
  },
  {
    "text": "2 to 4 gigs um in size and that's easily able to uh be installed on your local",
    "start": "1437200",
    "end": "1443400"
  },
  {
    "text": "machines and laptops so with that let's get started and to begin like I said we",
    "start": "1443400",
    "end": "1449559"
  },
  {
    "text": "need to wrap this model and have it served through a container so in order",
    "start": "1449559",
    "end": "1454760"
  },
  {
    "text": "to do that we need to first come up with a container file and the container file is nothing but specifying all the",
    "start": "1454760",
    "end": "1461480"
  },
  {
    "text": "requirements that we need to safely download and all the dependencies and",
    "start": "1461480",
    "end": "1467720"
  },
  {
    "text": "all the packages that's needed to kind of render this whisper model that we are",
    "start": "1467720",
    "end": "1473279"
  },
  {
    "text": "working with so here the base image that I'm going to be using is the Ubi image",
    "start": "1473279",
    "end": "1479480"
  },
  {
    "text": "and then we kind of um have this particular GitHub repository where all of the whisper code has been developed",
    "start": "1479480",
    "end": "1486880"
  },
  {
    "text": "and that's the repo that we're going to clone inside our container and since we're dealing with audio files over here",
    "start": "1486880",
    "end": "1493960"
  },
  {
    "text": "we do need to have this particular ffmc uh package in installed inside the",
    "start": "1493960",
    "end": "1499240"
  },
  {
    "text": "container FFM is just um an audio formatting and audio processing uh",
    "start": "1499240",
    "end": "1505240"
  },
  {
    "text": "package that's available and uh do note that whisper requires your audio files",
    "start": "1505240",
    "end": "1511200"
  },
  {
    "text": "to be in a 16bit wave format so it does not accept MP3 files and other such",
    "start": "1511200",
    "end": "1518000"
  },
  {
    "text": "formats it requires you to kind of convert it to that uh specified uh wave",
    "start": "1518000",
    "end": "1523279"
  },
  {
    "text": "format that you needed to process so um ffmpeg is a tool that that you can use",
    "start": "1523279",
    "end": "1529039"
  },
  {
    "text": "to actually convert your audio files into that suitable format and lastly",
    "start": "1529039",
    "end": "1535200"
  },
  {
    "text": "this is how we want to run the container itself so we have a bash script over",
    "start": "1535200",
    "end": "1540600"
  },
  {
    "text": "here where we specify that we want to serve this particular whisper model and",
    "start": "1540600",
    "end": "1546399"
  },
  {
    "text": "here we're using the small whisper model that I mentioned and I'm basically downloading the binary of this inside",
    "start": "1546399",
    "end": "1553159"
  },
  {
    "text": "this model directory and the binaries of these whisper models can in fact be",
    "start": "1553159",
    "end": "1558320"
  },
  {
    "text": "found on hugging face so in the repository that akaka will share with you all later there is a link to where",
    "start": "1558320",
    "end": "1565880"
  },
  {
    "text": "you can go and fetch those binaries so using that particular URL I've just gone",
    "start": "1565880",
    "end": "1571559"
  },
  {
    "text": "and downloaded that binary and uh copied it over into my models directory inside",
    "start": "1571559",
    "end": "1577200"
  },
  {
    "text": "of my repository and then finally this is the host and the port that I'm specifying where I want my model to be",
    "start": "1577200",
    "end": "1585080"
  },
  {
    "text": "serving and the inference end point with which I want to interact um once we have",
    "start": "1585080",
    "end": "1590559"
  },
  {
    "text": "kind of uploaded all our audio files so that's what this entry point is doing so",
    "start": "1590559",
    "end": "1596440"
  },
  {
    "text": "now that we know what the container file looks like let's go ahead and run this so before that I also want to see what",
    "start": "1596440",
    "end": "1603840"
  },
  {
    "text": "all are the other uh images that I've already built in the past so here are some uh similar container images that",
    "start": "1603840",
    "end": "1611200"
  },
  {
    "text": "I've built just a couple of hour ago to kind of test out and see if this is working so we're just going to build a",
    "start": "1611200",
    "end": "1617720"
  },
  {
    "text": "complet new one here so I'm going to say podman build and um I'm going to give it",
    "start": "1617720",
    "end": "1624760"
  },
  {
    "text": "a name let's say whisper cuon demo and I'm going to ask it to build it from",
    "start": "1624760",
    "end": "1631159"
  },
  {
    "text": "this particular directory now I'm using podman here but you're free to use any",
    "start": "1631159",
    "end": "1636240"
  },
  {
    "text": "other uh container tool like Docker uh it's just uh a preference that you can",
    "start": "1636240",
    "end": "1642240"
  },
  {
    "text": "kind of play around with so now that we have the image and we have it under this particular name this cucon demo we",
    "start": "1642240",
    "end": "1649559"
  },
  {
    "text": "actually need to go ahead and run this image now so to run it we have this",
    "start": "1649559",
    "end": "1654720"
  },
  {
    "text": "particular command over here so in this command I'm basically",
    "start": "1654720",
    "end": "1660720"
  },
  {
    "text": "exposing the port which I need for the model server to be uh up and running and",
    "start": "1660720",
    "end": "1666880"
  },
  {
    "text": "then here I'm going to pass the image name so we called it thiser Cube con demo and along with that the other",
    "start": "1666880",
    "end": "1674200"
  },
  {
    "text": "argument that we need to pass is where is the location of of your model binary",
    "start": "1674200",
    "end": "1679440"
  },
  {
    "text": "file so that's the part that I have specified over here and once all of that",
    "start": "1679440",
    "end": "1685320"
  },
  {
    "text": "is correct you can see that the whisper model server is running and it's",
    "start": "1685320",
    "end": "1690720"
  },
  {
    "text": "listening at this particular address and this particular Port awesome so now that",
    "start": "1690720",
    "end": "1696000"
  },
  {
    "text": "we have it up and running how do we kind of pass the audio files now there are two ways you can do this one you can",
    "start": "1696000",
    "end": "1702640"
  },
  {
    "text": "send a simple curl request which does a post request to this endpoint and it",
    "start": "1702640",
    "end": "1708440"
  },
  {
    "text": "gives you back the output as a logged text in a Json format in your uh console",
    "start": "1708440",
    "end": "1714320"
  },
  {
    "text": "and within the terminal itself but if you want a more easier way and you want it to have a more user interactive",
    "start": "1714320",
    "end": "1720760"
  },
  {
    "text": "format you can actually spin up a very uh simple UI application so to do this",
    "start": "1720760",
    "end": "1726760"
  },
  {
    "text": "there are a couple of tools that are popularly being used for many of these llm use cases one of them is streamlit",
    "start": "1726760",
    "end": "1733679"
  },
  {
    "text": "which is what we're using today so streamlit just requires you to kind of come up with the code of how you want",
    "start": "1733679",
    "end": "1741200"
  },
  {
    "text": "the UI to look like using simple Python and it has a streamlet package uh that's",
    "start": "1741200",
    "end": "1747159"
  },
  {
    "text": "uh supported by python so it's kind of easy for you to come up with that python script so this is what my streamlet",
    "start": "1747159",
    "end": "1753960"
  },
  {
    "text": "python script would look like um I'm importing the required stream packages and here I'm just kind of giving the",
    "start": "1753960",
    "end": "1760640"
  },
  {
    "text": "structure as a very barebone simple UI application and then I'm uploading the",
    "start": "1760640",
    "end": "1766200"
  },
  {
    "text": "audio file over here so I've given an option for users to kind of drag and drop whatever audio files they would",
    "start": "1766200",
    "end": "1772840"
  },
  {
    "text": "like to have translated and then finally I'm sending that audio file as a post",
    "start": "1772840",
    "end": "1778679"
  },
  {
    "text": "request to the model serving endpoint that is up and running over here so now",
    "start": "1778679",
    "end": "1784640"
  },
  {
    "text": "let's go ahead and actually run this particular python script so to do that",
    "start": "1784640",
    "end": "1790279"
  },
  {
    "text": "we're going to say streamlit run and I'm going to give the python file here and",
    "start": "1790279",
    "end": "1797159"
  },
  {
    "text": "now we can see see that streamlet is up and running and you can see your UI at this particular Local Host URL so let's",
    "start": "1797159",
    "end": "1804679"
  },
  {
    "text": "go ahead and see what that looks like this is what the streamlet UI looks like",
    "start": "1804679",
    "end": "1810159"
  },
  {
    "text": "very simple and this is the URL at which it's getting rendered at the moment so",
    "start": "1810159",
    "end": "1815880"
  },
  {
    "text": "here you can see there's a drag and drop option here to upload your audio file and one thing to note that whisper does",
    "start": "1815880",
    "end": "1823120"
  },
  {
    "text": "require your audio file to be in a 16bit wave format so we do need convert some",
    "start": "1823120",
    "end": "1829039"
  },
  {
    "text": "of the MP3 and other formats into this required um wave format for misper to",
    "start": "1829039",
    "end": "1835600"
  },
  {
    "text": "process it so once we have it in that particular format we can go ahead and upload so here I'm going to upload a",
    "start": "1835600",
    "end": "1842919"
  },
  {
    "text": "French audio file since we are here at cucon in Paris so let's go ahead and",
    "start": "1842919",
    "end": "1848200"
  },
  {
    "text": "give it a very simple uh French audio which is just about 7 seconds so if I",
    "start": "1848200",
    "end": "1854440"
  },
  {
    "text": "play this audio M professor of the mathematics",
    "start": "1854440",
    "end": "1859919"
  },
  {
    "text": "so that was the audio that we had I do not know French but I'm relying on the whisper model to kind of give me the",
    "start": "1864440",
    "end": "1870919"
  },
  {
    "text": "accurate translation so this is the translation that it gave and there you",
    "start": "1870919",
    "end": "1876039"
  },
  {
    "text": "go that's pretty much how this application works you can kind of play",
    "start": "1876039",
    "end": "1881159"
  },
  {
    "text": "around with it by uploading other languages like uh which are supported by Whisper like Japanese uh Italian Spanish",
    "start": "1881159",
    "end": "1889639"
  },
  {
    "text": "there are a couple of languages that it supports at the moment and yeah I hope you enjoyed this demo and I hope you all",
    "start": "1889639",
    "end": "1896720"
  },
  {
    "text": "enjoy the rest of the conference as well thank [Applause]",
    "start": "1896720",
    "end": "1908240"
  },
  {
    "text": "you um I do have couple more slides left uh well thank you uh and I'm glad that",
    "start": "1908240",
    "end": "1914760"
  },
  {
    "text": "you all like the demo uh so I will quickly go ahead and tell you like what are our future steps in the field of",
    "start": "1914760",
    "end": "1921600"
  },
  {
    "text": "large language models so the first and the most important is to have an",
    "start": "1921600",
    "end": "1927080"
  },
  {
    "text": "enhanced developer experience for also enabling the non- DAT scientist to",
    "start": "1927080",
    "end": "1932159"
  },
  {
    "text": "follow a simple workflow for setting up and also interacting with their own large language models via microservices",
    "start": "1932159",
    "end": "1939399"
  },
  {
    "text": "also implementing a seamless workflow from trans transitioning from a local Dev to a production grade environment",
    "start": "1939399",
    "end": "1946240"
  },
  {
    "text": "and also finally providing end to end tooling and framework for setting up large language models locally for",
    "start": "1946240",
    "end": "1953000"
  },
  {
    "text": "various applications such as text generation code generation document search Rag applications and so much more",
    "start": "1953000",
    "end": "1961039"
  },
  {
    "text": "and these are the resources that we went over in the presentation the first one is our GitHub repository all the code is",
    "start": "1961039",
    "end": "1968080"
  },
  {
    "text": "updated and running on the repository you just have to go follow along the readme and you should be able to create",
    "start": "1968080",
    "end": "1974600"
  },
  {
    "text": "the same demo as HMA just demonstrated we also have our slides updated on sked",
    "start": "1974600",
    "end": "1980440"
  },
  {
    "text": "and also on our GitHub repository we've also mentioned the link for the hugging face model that was required for the",
    "start": "1980440",
    "end": "1986880"
  },
  {
    "text": "demo and some of our colleagues have also worked on an interesting project",
    "start": "1986880",
    "end": "1991919"
  },
  {
    "text": "with whisper at Edge using microshift so if you are more expert in these kind of",
    "start": "1991919",
    "end": "1997720"
  },
  {
    "text": "things you can definitely explore their demo as well and thank you so much",
    "start": "1997720",
    "end": "2003080"
  },
  {
    "text": "everyone for sitting through coming for my talk and I do appreciate feedback and",
    "start": "2003080",
    "end": "2009000"
  },
  {
    "text": "questions do reach out uh to us make issues on the repository if you facing",
    "start": "2009000",
    "end": "2014039"
  },
  {
    "text": "any troubles we would be happy to help you out with that thank you so [Applause]",
    "start": "2014039",
    "end": "2019090"
  },
  {
    "text": "[Music] [Applause]",
    "start": "2019090",
    "end": "2028028"
  },
  {
    "text": "much no questions uh please feel free to meet me",
    "start": "2031320",
    "end": "2036880"
  },
  {
    "text": "at the redhe Hat booth if you have any questions or would want to talk more about",
    "start": "2036880",
    "end": "2042760"
  },
  {
    "text": "this",
    "start": "2042760",
    "end": "2045760"
  }
]