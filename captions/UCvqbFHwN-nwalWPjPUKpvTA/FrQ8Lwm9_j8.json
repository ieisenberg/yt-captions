[
  {
    "start": "0",
    "end": "46000"
  },
  {
    "text": "cool so this is ten weird ways to blow up your kubernetes who are we I'm",
    "start": "30",
    "end": "5190"
  },
  {
    "text": "Melanie this is a great photo of you Melanie I didn't have as good a photo so this is me I'm Bruce and yeah so we're",
    "start": "5190",
    "end": "15389"
  },
  {
    "text": "gonna talk about kubernetes at Airbnb very briefly in one slide started from the bottom and now we're here but it's",
    "start": "15389",
    "end": "23250"
  },
  {
    "text": "not all successes so like we're here to talk about the spectacular pitfalls mistakes and challenges we've run into",
    "start": "23250",
    "end": "29939"
  },
  {
    "text": "in the past few years Airbnb and by the way this is 10 weird ways to blow up your kubernetes not 10 ways to fix your",
    "start": "29939",
    "end": "36870"
  },
  {
    "text": "kubernetes so please no judgement these are not all perfect solutions some of these are like ugly hacks and other",
    "start": "36870",
    "end": "43260"
  },
  {
    "text": "things or just stuff that didn't work so enjoy first up zombie jobs so",
    "start": "43260",
    "end": "50520"
  },
  {
    "start": "46000",
    "end": "46000"
  },
  {
    "text": "community's jobs and cron jobs are really great we and we use them a lot at Airbnb they're a nice distributed highly",
    "start": "50520",
    "end": "56250"
  },
  {
    "text": "available cron job solution we use them for a lot of things but we have this",
    "start": "56250",
    "end": "61320"
  },
  {
    "text": "problem and many of you have probably seen this when does a cron job end or community's job end you have a typical",
    "start": "61320",
    "end": "66960"
  },
  {
    "text": "job that might have some side cars that do support services like a network proxy or logging agent those run forever and",
    "start": "66960",
    "end": "74549"
  },
  {
    "text": "kubernetes jobs don't end until all of the containers exit so this is really a",
    "start": "74549",
    "end": "80100"
  },
  {
    "text": "struggle kubernetes provides you this feature to try to deal with this you can set active deadline seconds in order to",
    "start": "80100",
    "end": "86490"
  },
  {
    "text": "get that to work properly you need to also set the concurrency policy and the restart policy but suppose you do this",
    "start": "86490",
    "end": "92220"
  },
  {
    "text": "you have you still have a few problems first is it will appear as if your cron jobs are always running because",
    "start": "92220",
    "end": "98880"
  },
  {
    "text": "kubernetes hasn't killed it yet until the act of deadlines seconds expires so you don't really know how long they're actually running and secondly when",
    "start": "98880",
    "end": "106259"
  },
  {
    "text": "kubernetes finally kills a job based on it reaching it reaching acti active deadlines seconds it marks the job as",
    "start": "106259",
    "end": "113040"
  },
  {
    "text": "failed so if you go to say cube CTL get and take a look at what's going on your cluster and looking at one of your jobs",
    "start": "113040",
    "end": "119189"
  },
  {
    "text": "you'll see the current one always running and all of the previous ones always failed you don't really have good insight into what's going on with your",
    "start": "119189",
    "end": "125369"
  },
  {
    "text": "cron jobs so there's a simple solution to this right just use touch files have",
    "start": "125369",
    "end": "131760"
  },
  {
    "text": "your main container file to indicate that it's finished and have the sidecars exit when that file appears so I suggested this gave it to",
    "start": "131760",
    "end": "140810"
  },
  {
    "text": "Melanie as a project to work on on it so I'm a lazy engineer and the first thing",
    "start": "140810",
    "end": "146659"
  },
  {
    "text": "I do is Google to see if anyone is running the same problem so that I can steal their solution so I found a gap",
    "start": "146659",
    "end": "152689"
  },
  {
    "text": "issue better support for sidecar caters in batch jobs same thing jobs not exiting not really a great solution here",
    "start": "152689",
    "end": "160519"
  },
  {
    "text": "great scroll down the github issue looks like Jamie Milliken - stripe is the",
    "start": "160519",
    "end": "166549"
  },
  {
    "text": "winner pasting - code directly into the issue seems like I can just introduce some of that and see if it works so the",
    "start": "166549",
    "end": "174199"
  },
  {
    "text": "way this kind of the way the solutions outlined is that you have a main container in a sidecar container the",
    "start": "174199",
    "end": "179719"
  },
  {
    "text": "main container rights touch file when it exits and to the sidecar container runs a wrapper script that looks for that",
    "start": "179719",
    "end": "185959"
  },
  {
    "text": "file to be written and then Alex exits itself when it sees that it has been written so I actually just took the bash",
    "start": "185959",
    "end": "192319"
  },
  {
    "text": "script from the Sacramento post and put it into the basket to see if it worked hmmm well it didn't quite work which I",
    "start": "192319",
    "end": "199909"
  },
  {
    "text": "kind of expected so take them or - we needed to support dumb init have it exit properly with the correct exit code well",
    "start": "199909",
    "end": "208340"
  },
  {
    "text": "that didn't quite work either so there was a few other PRS needed to try to tweak this behavior and actually got",
    "start": "208340",
    "end": "215209"
  },
  {
    "text": "into like the space of complicated signal handling each of these changes were actually quite scary because we're",
    "start": "215209",
    "end": "220970"
  },
  {
    "text": "not running production batch jobs this way and so at least one of these cause the production incident by terminating",
    "start": "220970",
    "end": "226009"
  },
  {
    "text": "the wrong PID yeah so if you find",
    "start": "226009",
    "end": "231169"
  },
  {
    "text": "yourself writing a batch script with over 100 lines of code asking yourself whose PID is it anyway",
    "start": "231169",
    "end": "237409"
  },
  {
    "text": "and you have hundreds of lines of complicated signal handling logic maybe you're working in the wrong abstraction",
    "start": "237409",
    "end": "242629"
  },
  {
    "text": "layer and so let's just try solving in kubernetes fine so again I like look",
    "start": "242629",
    "end": "250159"
  },
  {
    "text": "this up because that's what I do and OH sidecar containers that seems legit there's like a ton of people that want",
    "start": "250159",
    "end": "256039"
  },
  {
    "text": "to support this and it's going to be in communities but it's gonna be in 117 and it's an API change",
    "start": "256039",
    "end": "262750"
  },
  {
    "text": "I can't really back port it safely to previous versions so I guess I'm gonna",
    "start": "262750",
    "end": "268060"
  },
  {
    "text": "try to for kubernetes and I asked the internet what they thought I got the",
    "start": "268060",
    "end": "273520"
  },
  {
    "text": "most horrified response possible but I was like okay okay so I'm not 14",
    "start": "273520",
    "end": "279640"
  },
  {
    "text": "kubernetes I'm just patching it yeah very different and actually I didn't",
    "start": "279640",
    "end": "284860"
  },
  {
    "text": "have to set up this whole thing where we pull like the latest coming stable version that we're using and then we run",
    "start": "284860",
    "end": "290290"
  },
  {
    "text": "it as a CI CD pipeline that like applies github patches to this type of release so not forking okay it's like a very",
    "start": "290290",
    "end": "297340"
  },
  {
    "text": "sophisticated process but you will find that the kate's codebase evolves quickly",
    "start": "297340",
    "end": "302880"
  },
  {
    "text": "there's major cluster upgrades that need to be carefully perform didn't like pulling in all of these changes for like",
    "start": "302880",
    "end": "308380"
  },
  {
    "text": "one key use case that you need to support may not make as much sense and you may just have use cases that end up",
    "start": "308380",
    "end": "313480"
  },
  {
    "text": "being ahead of the curve and that's fine so of course I looked this up has anyone",
    "start": "313480",
    "end": "319180"
  },
  {
    "text": "else had this problem and it turns out lyft has a problem so they actually had their patch just hanging around in their",
    "start": "319180",
    "end": "324880"
  },
  {
    "text": "open-source Cabrini's fork and so yeah let's see how their patch works",
    "start": "324880",
    "end": "330220"
  },
  {
    "text": "basically you have a pod with a bunch of containers you have your main service container a bunch of sidecar helper",
    "start": "330220",
    "end": "335950"
  },
  {
    "text": "containers and the patch is in cubelet so if you annotate the containers as standard or sidecar then the logic in",
    "start": "335950",
    "end": "343690"
  },
  {
    "text": "cubelet has been modified to enforce that sidecar containers start before the standard container and shutdown after",
    "start": "343690",
    "end": "350290"
  },
  {
    "text": "them which have exactly the behavior we want here and it supports many other use cases some one other thing we use this",
    "start": "350290",
    "end": "356229"
  },
  {
    "text": "for was just service discovery containers so it works really well there actually and so the takeaway here is to",
    "start": "356229",
    "end": "363790"
  },
  {
    "text": "just solve your palm at the appropriate appropriate abstraction level and that may even be patching it behavior into",
    "start": "363790",
    "end": "369490"
  },
  {
    "text": "communities itself right next problem I want to talk about we'll call service",
    "start": "369490",
    "end": "375040"
  },
  {
    "start": "372000",
    "end": "372000"
  },
  {
    "text": "mesh speeding accidents so at Airbnb we use a service discovery system called",
    "start": "375040",
    "end": "381100"
  },
  {
    "text": "smart stack which we open sourced in 2013 and smart stack has a number of components and I'm not going to cover it",
    "start": "381100",
    "end": "387130"
  },
  {
    "text": "in much detail but at its core it involves every pod having an H a proxy",
    "start": "387130",
    "end": "392620"
  },
  {
    "text": "sidecar container that is its outbound or versus proxy and the smart sex system",
    "start": "392620",
    "end": "398610"
  },
  {
    "text": "will periodically update the H a proxy config and restart it to keep keep things up to date",
    "start": "398610",
    "end": "403910"
  },
  {
    "text": "unfortunately restarting H a proxy is memory intensive the way that works is agent proxy Forks it starts a new copy",
    "start": "403910",
    "end": "410580"
  },
  {
    "text": "of itself taking up all intent of memory is first copy the original copy will",
    "start": "410580",
    "end": "416250"
  },
  {
    "text": "finish any outstanding connections that are running and then terminate so your memory usage can double when HR proxy is",
    "start": "416250",
    "end": "423090"
  },
  {
    "text": "running or if your connections are long-lived and your service mesh is changing frequently you might have a lot of a",
    "start": "423090",
    "end": "429810"
  },
  {
    "text": "cheap proxies running and run into you and out of memory issue so we have this problem had this problem saw some",
    "start": "429810",
    "end": "437340"
  },
  {
    "text": "outages related to this we came up with a simple fix just wait a little bit so we have our so we slowed down our to",
    "start": "437340",
    "end": "444720"
  },
  {
    "text": "service smash our service discovery so that we only restart 88 proxy no more often than once every 30 seconds so you",
    "start": "444720",
    "end": "452910"
  },
  {
    "text": "can see where this is going right kubernetes has this great feature where",
    "start": "452910",
    "end": "458100"
  },
  {
    "text": "it can deploy really really fast and developers don't like waiting so they cranked up max surge for the deployment",
    "start": "458100",
    "end": "464850"
  },
  {
    "text": "strategy and you can actually deploy your whole service in less than 30 seconds and the service mesh can't keep",
    "start": "464850",
    "end": "473100"
  },
  {
    "text": "up so each time a service would deploy we'd see the spike of errors calling",
    "start": "473100",
    "end": "479220"
  },
  {
    "text": "into that service and so you see this error spike with every deploy our fix",
    "start": "479220",
    "end": "485250"
  },
  {
    "text": "for that so far is this kind of hacky solution which is basically to just tell kubernetes to slow down the termination",
    "start": "485250",
    "end": "492270"
  },
  {
    "text": "of pods on deploy you add a pre stop hook to sleep and wait long enough for",
    "start": "492270",
    "end": "497310"
  },
  {
    "text": "your service mesh to catch up and figure out what happened kubernetes will this",
    "start": "497310",
    "end": "503100"
  },
  {
    "text": "will put the pot of determinating state the kubernetes will actually delete it in that state by default after 30",
    "start": "503100",
    "end": "509220"
  },
  {
    "text": "seconds so you also need to set termination grace period seconds and once you have those two settings and you",
    "start": "509220",
    "end": "514860"
  },
  {
    "text": "kind of tweak it a little bit you can get the pods to survive long enough to keep your to stay around until your",
    "start": "514860",
    "end": "520500"
  },
  {
    "text": "service mesh catches up with what happened so our take away kubernetes deploys can cycle pods superfast weather",
    "start": "520500",
    "end": "527760"
  },
  {
    "text": "the rest of your infrastructure and keep up or not next up monster",
    "start": "527760",
    "end": "533230"
  },
  {
    "start": "532000",
    "end": "532000"
  },
  {
    "text": "daemon sets so why do you want a daemon set anyway suppose you have a pod that needs to download a large amount of data",
    "start": "533230",
    "end": "539620"
  },
  {
    "text": "every time it starts it'd be convenient if you could use a daemon set to just make sure the data is there every time",
    "start": "539620",
    "end": "545500"
  },
  {
    "text": "your products schedule on to that node for example we have gigs of translation data or search index data and many pods",
    "start": "545500",
    "end": "551530"
  },
  {
    "text": "that need that data so Damien said sure I'll solve that for you the reality is",
    "start": "551530",
    "end": "557650"
  },
  {
    "text": "that the daemon starts kind of flaky so he kind of just steps outside when he's deploying and the pods like but wait I",
    "start": "557650",
    "end": "564280"
  },
  {
    "text": "kind of am still running I still need you to be around also oops I died the daemon psycho is unhealthy or",
    "start": "564280",
    "end": "571150"
  },
  {
    "text": "it just dies and doesn't come back that pod again it's still running and expecting the daemon set to also be",
    "start": "571150",
    "end": "576490"
  },
  {
    "text": "running so one thing we tried okay well maybe a daemon set isn't the right",
    "start": "576490",
    "end": "581560"
  },
  {
    "text": "solution here what if we try a deployment and use this neat feature called pond affinity to try to schedule",
    "start": "581560",
    "end": "586840"
  },
  {
    "text": "the service pods and the translation data pods on the same node but there's some doctors with pod affinity for",
    "start": "586840",
    "end": "593110"
  },
  {
    "text": "example if you want like preferred during execution or required during execution you're making a trade-off of",
    "start": "593110",
    "end": "598210"
  },
  {
    "text": "well they're not always scheduled together or they are always scheduled together and if your cluster can't keep",
    "start": "598210",
    "end": "603220"
  },
  {
    "text": "up you're like full of scheduler errors now so we didn't only like that okay so",
    "start": "603220",
    "end": "608380"
  },
  {
    "text": "we thought of another idea which is let's use the daemon set and then there's another kubernetes features called paints and Toleration so tanks is",
    "start": "608380",
    "end": "614830"
  },
  {
    "text": "a way to say you know don't schedule on this node during these conditions so maybe we can paint on pod readiness but",
    "start": "614830",
    "end": "620920"
  },
  {
    "text": "one thing we were thinking about was you know supporting a very complex cluster we didn't really want to have David sets",
    "start": "620920",
    "end": "627100"
  },
  {
    "text": "being scheduled on and off on different nodes so the complex new approach was just kind of not worth it and even",
    "start": "627100",
    "end": "632950"
  },
  {
    "text": "setting all that aside there was another really big problem with daemon sets so we have our legacy our first cluster",
    "start": "632950",
    "end": "638260"
  },
  {
    "text": "which has over 2000 notes and if you want to have a daemon set and not runs on every single node you're looking at",
    "start": "638260",
    "end": "643720"
  },
  {
    "text": "over 2000 pods and so it's quite easy for to take up a huge amount of space on your cluster so I'll go through like a",
    "start": "643720",
    "end": "650620"
  },
  {
    "text": "real example of how that can go quite badly so I'm going to create and deploy a daemon set workload this is actually",
    "start": "650620",
    "end": "657490"
  },
  {
    "text": "me Melanie I did this I just wanted to test something out and what happens I'm going to adjust some",
    "start": "657490",
    "end": "663790"
  },
  {
    "text": "behavior and I give it like a typical like the typical resource instead of job as service uses cool and then like ten",
    "start": "663790",
    "end": "671080"
  },
  {
    "text": "minutes later someone on our site reliability team message me saying hey I noticed you deployed a daemon set",
    "start": "671080",
    "end": "677580"
  },
  {
    "text": "there's now thousands of pods that are getting you killed and SVD is filling up",
    "start": "677580",
    "end": "682600"
  },
  {
    "text": "with all of these events and the whole cluster is going down and so I was like",
    "start": "682600",
    "end": "687670"
  },
  {
    "text": "wow that was actually really easy I guess I didn't think that through that much so the whole cluster was like you",
    "start": "687670",
    "end": "692800"
  },
  {
    "text": "know I mean we saved it but it was kind of dicey so for this particular use case we're like maybe let's just try the",
    "start": "692800",
    "end": "699070"
  },
  {
    "text": "sidecar model again that seemed to work okay and in fact maybe we should use a mission controller to introduce some",
    "start": "699070",
    "end": "705670"
  },
  {
    "text": "friction around creating daemon sets because it seems pretty easy to just take everything down and of course there",
    "start": "705670",
    "end": "710890"
  },
  {
    "text": "are use cases for daemon sets another special node behavior so we'll just create dedicated smaller special",
    "start": "710890",
    "end": "716080"
  },
  {
    "text": "clusters for services that need that special behavior and actually work pretty well for us so the main thing",
    "start": "716080",
    "end": "722590"
  },
  {
    "text": "right here was just that David sets you know they can actually just take down your cluster in a way that's harder for other workloads - next topic where's my",
    "start": "722590",
    "end": "731920"
  },
  {
    "start": "730000",
    "end": "730000"
  },
  {
    "text": "docker image so it Airbnb we make a lot of docker images we're generating more",
    "start": "731920",
    "end": "738820"
  },
  {
    "text": "than 2 million docker images a day and we mainly use ECR for this Amazon's",
    "start": "738820",
    "end": "743980"
  },
  {
    "text": "elastic container registry but ECR is great but it has a limit of 10,000",
    "start": "743980",
    "end": "749710"
  },
  {
    "text": "images per repository so we need some way of making sure that we don't overflow our ECR repos no ECR does",
    "start": "749710",
    "end": "755560"
  },
  {
    "text": "provide a feature called lifecycle policy that allows you to manage how how",
    "start": "755560",
    "end": "761650"
  },
  {
    "text": "many images you have in each repository but life cycle policy only lets you limit the number of images and apply",
    "start": "761650",
    "end": "767590"
  },
  {
    "text": "deletion policy based on least recently used or total number of images it doesn't have any way of accounting for",
    "start": "767590",
    "end": "773380"
  },
  {
    "text": "the images that you're currently using so if you have a fast growing repo but",
    "start": "773380",
    "end": "778480"
  },
  {
    "text": "you're currently using a single image say the one you're using in production there's no way of telling it don't",
    "start": "778480",
    "end": "784390"
  },
  {
    "text": "delete that one and so ECR life cycle policy is not enough we came up with a simple",
    "start": "784390",
    "end": "792070"
  },
  {
    "text": "relatively simple idea we'll run a write a program we call it ECR cleaner it works",
    "start": "792070",
    "end": "797499"
  },
  {
    "text": "basically like this it runs once a day as a cron job it finds all the images that are in use by calling something",
    "start": "797499",
    "end": "804069"
  },
  {
    "text": "like cube CT I'll get deployments and looking for all the image IDs in use and then for each ECR repository delete and",
    "start": "804069",
    "end": "811720"
  },
  {
    "text": "delete images down to some threshold but but skipping the ones that are in use so",
    "start": "811720",
    "end": "817240"
  },
  {
    "text": "that worked pretty well for a while till a couple things went wrong almost every line of this went wrong first one was CI",
    "start": "817240",
    "end": "826360"
  },
  {
    "text": "failures due to some sort of docker image error we dig into this and discover ultimately the root cause is",
    "start": "826360",
    "end": "832749"
  },
  {
    "text": "that first part of this ECR cleaner it only runs once a day and amazingly we",
    "start": "832749",
    "end": "840220"
  },
  {
    "text": "were generating more than 10,000 images a day in CI now that I say it out loud",
    "start": "840220",
    "end": "846339"
  },
  {
    "text": "it doesn't actually sound like that much but it was pretty surprising when we crossed that threshold so even re CR",
    "start": "846339",
    "end": "853420"
  },
  {
    "text": "cleaner wasn't sufficient we came up with this great solution",
    "start": "853420",
    "end": "858209"
  },
  {
    "text": "Romney's er cleaner even more frequently and eventually we're gonna probably",
    "start": "858870",
    "end": "863970"
  },
  {
    "text": "generate more than 10,000 images per hour and then we're gonna have to come up with something else but this is holding for right now the next problem",
    "start": "863970",
    "end": "870960"
  },
  {
    "text": "we ran into manifested like this this is how these things manifest they're kind of mysterious and takes some tracking",
    "start": "870960",
    "end": "877200"
  },
  {
    "text": "down you rotate a kubernetes Minya node and suddenly the service is down and the",
    "start": "877200",
    "end": "882750"
  },
  {
    "text": "reason why is because the last remaining pods of that service that had workable images were on that one node and the",
    "start": "882750",
    "end": "888420"
  },
  {
    "text": "images are gone from ECR why did that happen since we have this cleaner well",
    "start": "888420",
    "end": "893720"
  },
  {
    "text": "find all images in use was only checking the cube deployments in the cluster",
    "start": "893720",
    "end": "899160"
  },
  {
    "text": "where the ECR cleaner was running but now we're a multi we have a multi",
    "start": "899160",
    "end": "904440"
  },
  {
    "text": "cluster solution in fact we have over 30 clusters and it wasn't checking all the other clusters so that so we need to",
    "start": "904440",
    "end": "913370"
  },
  {
    "text": "experience and use to actually find all images in all clusters that are in use but that presents a problem to where do",
    "start": "913370",
    "end": "920670"
  },
  {
    "text": "you run the ECR cleaner because you want to have isolation between your various",
    "start": "920670",
    "end": "925950"
  },
  {
    "text": "kubernetes clusters VCR cleaner really needs to be a global singleton it needs to know about all images in use in all",
    "start": "925950",
    "end": "932880"
  },
  {
    "text": "clusters so where does it run we solve this by adding a new cluster",
    "start": "932880",
    "end": "941089"
  },
  {
    "text": "so we create a cluster we call the management cluster which has permission to access the API servers of every other",
    "start": "941089",
    "end": "948079"
  },
  {
    "text": "cluster and then we have a more strict review process for services that run in the management cluster so the takeaway",
    "start": "948079",
    "end": "955040"
  },
  {
    "text": "why can't I hold all these docker images make sure to keep track of your docker images next up to an it or not to in it",
    "start": "955040",
    "end": "963980"
  },
  {
    "text": "that is the question so we have a runtime configuration service that",
    "start": "963980",
    "end": "969170"
  },
  {
    "text": "propagates value changes to applications within seconds think of like turning on or off features or adjusting experiments",
    "start": "969170",
    "end": "975879"
  },
  {
    "text": "and we kind of need a way to support that in coburn at ease so we have a",
    "start": "975879",
    "end": "983059"
  },
  {
    "text": "runtime config agent container which sits in our pod and it pulls the runtime config service for configuration updates",
    "start": "983059",
    "end": "989449"
  },
  {
    "text": "but our pod actually needs that data that data before it starts otherwise the service will crash so",
    "start": "989449",
    "end": "996379"
  },
  {
    "text": "let's split the logic and do an initial update in an init container and then have the runtime configuration pole",
    "start": "996379",
    "end": "1002019"
  },
  {
    "text": "container silly for more updates oh but to pull in the latest dynamic",
    "start": "1002019",
    "end": "1009279"
  },
  {
    "text": "configuration we actually need to communicate with runtime config service which requires service discovery which",
    "start": "1009279",
    "end": "1014769"
  },
  {
    "text": "definitely doesn't exist during the initialization phase and so therefore you know the first approach of this is",
    "start": "1014769",
    "end": "1021600"
  },
  {
    "text": "let's have the init container set up its own service discovery that seems like great idea yeah it's not so it was like",
    "start": "1021600",
    "end": "1031120"
  },
  {
    "text": "yeah so the way that air worked is it started up the network proxy and then ran its logic to pull the data and then",
    "start": "1031120",
    "end": "1037870"
  },
  {
    "text": "like shut down the network proxy this was complex pretty brittle slow and then obviously had a significant footprint on",
    "start": "1037870",
    "end": "1044918"
  },
  {
    "text": "our service discovery info so not ideal but we have sidecar ordering now so",
    "start": "1044919",
    "end": "1050470"
  },
  {
    "text": "let's just slow this at that problem surely this will solve it let's just make runtime configuration and network proxy sidecars",
    "start": "1050470",
    "end": "1056770"
  },
  {
    "text": "but then you know we still aren't guaranteeing that the network proxy is up before the runtime config is it so",
    "start": "1056770",
    "end": "1063490"
  },
  {
    "text": "it's possible that the runtime config in itself can still not connect to the network proxy if it's ready first okay",
    "start": "1063490",
    "end": "1071110"
  },
  {
    "text": "so what do we do we can add logic to the runtime configure like our to just wait for the network",
    "start": "1071110",
    "end": "1076320"
  },
  {
    "text": "proxy site card to be ready with the rapper script so some kind of logic it says you know continuously a pole for",
    "start": "1076320",
    "end": "1082110"
  },
  {
    "text": "runtime config service if it can now successfully connect to runtime config service then let's get that initial update and then begin polling so",
    "start": "1082110",
    "end": "1091520"
  },
  {
    "text": "takeaway you might need ordering between sidecars to and you might say hey Melanie you didn't solve that directly",
    "start": "1091520",
    "end": "1097680"
  },
  {
    "text": "communities you used in other batch scripts to which I challenge you to please write a topological sort patch",
    "start": "1097680",
    "end": "1103380"
  },
  {
    "text": "for sidecar ordering thank you please let us know when you've got and you've got that next up where's my",
    "start": "1103380",
    "end": "1111300"
  },
  {
    "start": "1107000",
    "end": "1107000"
  },
  {
    "text": "custom resource so custom resources are great and at Airbnb we use them a lot",
    "start": "1111300",
    "end": "1117030"
  },
  {
    "text": "especially to keep track of resources external to the communities cluster but that are associated with a service such",
    "start": "1117030",
    "end": "1122940"
  },
  {
    "text": "as its storage its dashboard its Alerts its AWS resources like I am roles and so",
    "start": "1122940",
    "end": "1128309"
  },
  {
    "text": "forth one challenge with using custom resources is knowing when the deploy has completed we want to be able to",
    "start": "1128309",
    "end": "1134850"
  },
  {
    "text": "determine whether a deploy is completed or not and whether the custom resource changes were applied successfully or not",
    "start": "1134850",
    "end": "1140720"
  },
  {
    "text": "so we came up with this kind of simple idea we'll make use of the status field in the resource right simple keeps ETL",
    "start": "1140720",
    "end": "1148740"
  },
  {
    "text": "cube CTL apply we'll set the status depending the the controller will wake up when it receives the new custom",
    "start": "1148740",
    "end": "1156090"
  },
  {
    "text": "resource it will apply the changes and then it'll change the status to either ready or error and this is really",
    "start": "1156090",
    "end": "1161880"
  },
  {
    "text": "convenient it's easy for the custom for the controller to know when it needs to run it just checks to see if the status",
    "start": "1161880",
    "end": "1167100"
  },
  {
    "text": "is pending and it's easy for an operator manually to check and see what happened you can just do keep CTL get and the",
    "start": "1167100",
    "end": "1172679"
  },
  {
    "text": "status is at the bottom and is kind of human readable so that's great except that it doesn't work or it didn't work",
    "start": "1172679",
    "end": "1179610"
  },
  {
    "text": "it worked up until kubernetes version 112 after 1.12 only controllers can",
    "start": "1179610",
    "end": "1185309"
  },
  {
    "text": "modify the status field of resource objects keep CTL apply can no longer make those changes so this was great it",
    "start": "1185309",
    "end": "1192330"
  },
  {
    "text": "worked until it didn't it and it actually was pretty difficult to determine that this was going on it was",
    "start": "1192330",
    "end": "1197429"
  },
  {
    "text": "a kind of it was an insidious nasty outage where what happened is that new resources would apply and work fine",
    "start": "1197429",
    "end": "1203460"
  },
  {
    "text": "right but any changes to existing custom resources silently succeed because they would",
    "start": "1203460",
    "end": "1209550"
  },
  {
    "text": "still look like they were in ready state but the controllers would never run on them because they never got reset back",
    "start": "1209550",
    "end": "1214950"
  },
  {
    "text": "to pending state by cube see tail apply which incidentally succeeds but doesn't make the change so this was a pretty",
    "start": "1214950",
    "end": "1221880"
  },
  {
    "text": "gnarly outage caused by this change in that we determined as part of the",
    "start": "1221880",
    "end": "1227640"
  },
  {
    "text": "kubernetes upgrade when they upgraded past 12 so here's a solution now what",
    "start": "1227640",
    "end": "1234480"
  },
  {
    "text": "what's different in 1.12 kubernetes now provides you a field spec meta data generation which is an integer auto",
    "start": "1234480",
    "end": "1241320"
  },
  {
    "text": "incrementing provided by communities itself so when you update a resource",
    "start": "1241320",
    "end": "1248460"
  },
  {
    "text": "kubernetes will change that generation from say one to two when you do your cube CTL apply the status still stays",
    "start": "1248460",
    "end": "1254730"
  },
  {
    "text": "the same you can't change that part and this at least gives you some facility to figure",
    "start": "1254730",
    "end": "1260400"
  },
  {
    "text": "out what's going on with your custom resources but it's it's not enough right how does your controller know whether",
    "start": "1260400",
    "end": "1266910"
  },
  {
    "text": "it's operated on generation 2 or not so here's our Airbnb solution we add in the",
    "start": "1266910",
    "end": "1274860"
  },
  {
    "text": "status field and observe generation which is the last generation that the controller has run on now the controller",
    "start": "1274860",
    "end": "1281310"
  },
  {
    "text": "updates when it when it sees a new version of the resource it'll if the generation doesn't match",
    "start": "1281310",
    "end": "1287100"
  },
  {
    "text": "the observe generation it'll reapply the changes and update the status field and this works for now which is good but the",
    "start": "1287100",
    "end": "1295110"
  },
  {
    "text": "user interface for manual inspection is still quite a bit worse when you do keep CTL get well on the slide I've put these",
    "start": "1295110",
    "end": "1302460"
  },
  {
    "text": "two lines pretty much right next to each other but there might be hundreds of lines of spec in between and you kinda have to manually eyeball to see that",
    "start": "1302460",
    "end": "1308730"
  },
  {
    "text": "those things are are updated so not as good as it used to be but still works that's our that's our stopgap so our",
    "start": "1308730",
    "end": "1316020"
  },
  {
    "text": "takeaway knowing when it deploy is complete and if it succeeded or not is tricky cool next up I can't believe I",
    "start": "1316020",
    "end": "1324840"
  },
  {
    "text": "have all the notes resources you probably see where this one is going so before kubernetes there was a simple",
    "start": "1324840",
    "end": "1330990"
  },
  {
    "text": "time simple place I got one service on one instance instance is pretty big it's got 72 gigs",
    "start": "1330990",
    "end": "1337190"
  },
  {
    "text": "and the JVM allocates made 72 gigs so that works out pretty well in kubernetes",
    "start": "1337190",
    "end": "1342470"
  },
  {
    "text": "you have multiple co-located services running in pods and the JVM gives you something to gigs and use 72 gigs and",
    "start": "1342470",
    "end": "1348440"
  },
  {
    "text": "use 72 gigs and now suddenly you're out of memory and it's the same thing for",
    "start": "1348440",
    "end": "1354139"
  },
  {
    "text": "CPU so all kinds of problems and the",
    "start": "1354139",
    "end": "1359210"
  },
  {
    "text": "problem is that all the versions of the JDK are not aware of cgroups and so okay",
    "start": "1359210",
    "end": "1364940"
  },
  {
    "text": "well I guess we're running a really old JDK we should probably upgrade it it's solved in newer versions and it knows",
    "start": "1364940",
    "end": "1371629"
  },
  {
    "text": "how to allocate resources based on the container or not the node so we kind of",
    "start": "1371629",
    "end": "1377299"
  },
  {
    "text": "like did investigation just to make sure this was the case so here's a real graph of a service tracking it's p95 latency",
    "start": "1377299",
    "end": "1383320"
  },
  {
    "text": "after the upgrade the latency went down and for its worth we tried a few other flags and things that didn't really seem",
    "start": "1383320",
    "end": "1389179"
  },
  {
    "text": "to help us so that seemed to work pretty well so now a GVM gives your pod the",
    "start": "1389179",
    "end": "1395539"
  },
  {
    "text": "correct amount of CPU but this service has a concurrency bug that gets worse",
    "start": "1395539",
    "end": "1401000"
  },
  {
    "text": "the less CPU that was allocated to you based on the third Pole oh okay so",
    "start": "1401000",
    "end": "1408529"
  },
  {
    "text": "basically downgrade to the correct amount of CPUs actually expose an application race condition which led to",
    "start": "1408529",
    "end": "1414919"
  },
  {
    "text": "an outage so yeah and here's the here's the real evidence of the outage",
    "start": "1414919",
    "end": "1420590"
  },
  {
    "text": "basically it was like really slow requests as I started to hit the race condition more and therefore when",
    "start": "1420590",
    "end": "1427700"
  },
  {
    "text": "upgrading you know the JDK or any other language framework you know you have to just pay attention to these sorts of problems it's actually another major",
    "start": "1427700",
    "end": "1433549"
  },
  {
    "text": "upgrade that you're taking on so cool application owner please use canary and",
    "start": "1433549",
    "end": "1439039"
  },
  {
    "text": "test it in 24 hours before you know rolling out to the rest of production and so this is not specific to Java",
    "start": "1439039",
    "end": "1446659"
  },
  {
    "text": "services so you're like yeah Melanie yeah this is like cube Con 2016 everyone already knows about that but I guess the",
    "start": "1446659",
    "end": "1453889"
  },
  {
    "text": "reason I kept it in here is kind of it just it just keeps happening so like with other Lego train works and side",
    "start": "1453889",
    "end": "1459910"
  },
  {
    "text": "cars and stuff and so I think it's just something that you should just always having your back pocket of maybe maybe that's what's happening here",
    "start": "1459910",
    "end": "1465760"
  },
  {
    "text": "so like another example is when we first started experimenting with envoy we noticed that it's that concurrency",
    "start": "1465760",
    "end": "1471460"
  },
  {
    "text": "didn't have CPUs on the underlying host by default that sounds familiar like probably that's gonna call its contention on the host so like we had to",
    "start": "1471460",
    "end": "1477790"
  },
  {
    "text": "we had to like set the concurrency to be way lower so the takeaway there it's just it like be aware of language",
    "start": "1477790",
    "end": "1483460"
  },
  {
    "text": "framework since I cars they're not aware of these container abstractions and you know that you might be signing yourself",
    "start": "1483460",
    "end": "1489250"
  },
  {
    "text": "up for more upgrades you know when you when you upgrade to containers we're nice alright next autoscale akka limbs",
    "start": "1489250",
    "end": "1498660"
  },
  {
    "start": "1494000",
    "end": "1494000"
  },
  {
    "text": "so auto scaling is great probably hearing me using this a lot I love all these Cuban ease features till they",
    "start": "1498660",
    "end": "1504550"
  },
  {
    "text": "don't work so auto scaling for those of you that aren't familiar the way auto scaling works is it creates a new",
    "start": "1504550",
    "end": "1511180"
  },
  {
    "text": "resource called a horizontal pot autoscaler which monitors your deployment and whenever the average CPU",
    "start": "1511180",
    "end": "1517180"
  },
  {
    "text": "of your deployment deviates from a set value it'll add or remove pods so keep your",
    "start": "1517180",
    "end": "1522250"
  },
  {
    "text": "utilization approximately flat now suppose you have a service like this you",
    "start": "1522250",
    "end": "1529990"
  },
  {
    "text": "have a service that consumes a lot of CPU on start say for initialization in",
    "start": "1529990",
    "end": "1535420"
  },
  {
    "text": "our case we have some services that go through a startup phase that we call JVM warm-up where they replay synthetic",
    "start": "1535420",
    "end": "1541540"
  },
  {
    "text": "traffic in order to populate their in-memory cache to establish connections and their connection pools to fill up",
    "start": "1541540",
    "end": "1547720"
  },
  {
    "text": "thread pools etc this process takes a few minutes and uses essentially a hundred percent CPU while it's happening",
    "start": "1547720",
    "end": "1554020"
  },
  {
    "text": "so what happens with the autoscaler so here's an example of a deploy of a",
    "start": "1554020",
    "end": "1560590"
  },
  {
    "text": "service just what is the autoscaler doing this service typically uses about",
    "start": "1560590",
    "end": "1565630"
  },
  {
    "text": "30 pods it's autoscaler ranges from 20 to 200 and you deploy which surges and",
    "start": "1565630",
    "end": "1573280"
  },
  {
    "text": "adds a few extra pods they come in at a hundred percent CPU and raise the CPU average which causes the autoscaler to",
    "start": "1573280",
    "end": "1579340"
  },
  {
    "text": "add some pods which raises the cpu average which causes the autoscaler to add some more pods and in fact will",
    "start": "1579340",
    "end": "1586090"
  },
  {
    "text": "spike the number of pods almost all the way to the maximum allowed in this auto",
    "start": "1586090",
    "end": "1591670"
  },
  {
    "text": "skill group in this in this autoscaler it's the trajectory is that the",
    "start": "1591670",
    "end": "1597190"
  },
  {
    "text": "autoscaler is adding pods so fast that that all the ones that are coming in are still in their warm-up bays and still",
    "start": "1597190",
    "end": "1602380"
  },
  {
    "text": "burning more cpu so a service that typically runs in 30 pods will spike to",
    "start": "1602380",
    "end": "1607560"
  },
  {
    "text": "190 pods on every deploy that's not great how do you fix this it's not",
    "start": "1607560",
    "end": "1614260"
  },
  {
    "text": "obvious like surely there must be a setting from it for this I hear you cry",
    "start": "1614260",
    "end": "1620370"
  },
  {
    "text": "well there used to be a setting for this but it was removed with this cryptic",
    "start": "1620550",
    "end": "1627310"
  },
  {
    "text": "comment in the documentation starting from 1.12 a new algorithmic update removes the need for upscale",
    "start": "1627310",
    "end": "1633640"
  },
  {
    "text": "delay so here's our embarrassing hack how we work around this incidentally",
    "start": "1633640",
    "end": "1640060"
  },
  {
    "text": "there is a setting called horizontal pod autoscaler initial readiness delay which sure seems",
    "start": "1640060",
    "end": "1645910"
  },
  {
    "text": "like it's meant to address this problem but it did not work for us it did not help this problem at all and we tried",
    "start": "1645910",
    "end": "1651250"
  },
  {
    "text": "even setting it as long as high as 15 minutes so our really dumb hack is to set the pod autoscaler sync period this",
    "start": "1651250",
    "end": "1658450"
  },
  {
    "text": "essentially means we run the autoscaler only once every 5 minutes and you really",
    "start": "1658450",
    "end": "1663700"
  },
  {
    "text": "can't run it any less frequently than that or you really risk the autoscaler not reacting to real traffic spikes and",
    "start": "1663700",
    "end": "1670740"
  },
  {
    "text": "even with that setting it doesn't fix the problem it only mitigates the problem so here's",
    "start": "1670740",
    "end": "1676150"
  },
  {
    "text": "the same service deploying now typically runs at 30 pods still spikes to around",
    "start": "1676150",
    "end": "1681610"
  },
  {
    "text": "85 or 90 pods due to runaway auto scaling on every deploy so our takeaway",
    "start": "1681610",
    "end": "1688510"
  },
  {
    "text": "auto scaling does not work well for services that burn a lot of CPU on start okay next up hey",
    "start": "1688510",
    "end": "1697580"
  },
  {
    "text": "my scheduled operation took down all services so here's an excerpt from an actual operation fYI we are running an",
    "start": "1697580",
    "end": "1705140"
  },
  {
    "text": "operation to upgrade stats the agent in Cabrini's in a few minutes there may be a few",
    "start": "1705140",
    "end": "1710690"
  },
  {
    "text": "minutes we're symmetric SAR not available to services and then a few minutes later I think all of my",
    "start": "1710690",
    "end": "1716930"
  },
  {
    "text": "kubernetes sources are down right now and they also have metrics but mostly they're down and so to explain this one",
    "start": "1716930",
    "end": "1724940"
  },
  {
    "text": "just a brief overview of kubernetes health checks so health tech synchronize there's readiness probes and liveness",
    "start": "1724940",
    "end": "1730370"
  },
  {
    "text": "probes vinius probes don't send traffic to me if it fails life is probes replace",
    "start": "1730370",
    "end": "1736100"
  },
  {
    "text": "me if it fails and so you know when you think about a service container for us",
    "start": "1736100",
    "end": "1742070"
  },
  {
    "text": "at least most of the services just said both of these the flash health so if it responds with flash health it's probably healthy and you can set you know more",
    "start": "1742070",
    "end": "1749000"
  },
  {
    "text": "complex ones if you want but in Cabana health checking is per container so you'd be there to think about all of",
    "start": "1749000",
    "end": "1754220"
  },
  {
    "text": "these containers and how they contribute to your health check and so what should their health text be and like how should",
    "start": "1754220",
    "end": "1761000"
  },
  {
    "text": "they affect you know whether your whole pod is receiving traffic or not so you know a while ago before this incident we",
    "start": "1761000",
    "end": "1767000"
  },
  {
    "text": "had already decided that we didn't want stats see agent failing to take down our service so our initial try was okay just",
    "start": "1767000",
    "end": "1775100"
  },
  {
    "text": "don't set probes for non-critical containers containers like logging tracing metrics should not affect",
    "start": "1775100",
    "end": "1781310"
  },
  {
    "text": "service health so pretty easy right just some set the probes well we didn't know that a crashing container fails its",
    "start": "1781310",
    "end": "1788030"
  },
  {
    "text": "readiness check regardless of whether you delete all the probes or not it will still fail the readiness check and so",
    "start": "1788030",
    "end": "1793820"
  },
  {
    "text": "with sexy agent crashed it failed its readiness check and at the time our",
    "start": "1793820",
    "end": "1800120"
  },
  {
    "text": "service much logic basically had a loop like this so it's pause pot ready true",
    "start": "1800120",
    "end": "1806030"
  },
  {
    "text": "for each container size in the pod if it's not ready the pod is not ready and then at the end of this whole loop with",
    "start": "1806030",
    "end": "1812000"
  },
  {
    "text": "the pods ready publisher that's ready so that it can receive traffic so if any one of the containers or",
    "start": "1812000",
    "end": "1817060"
  },
  {
    "text": "ready the whole pot isn't ready just receive traffic so in this case our crashing starts the agent container",
    "start": "1817060",
    "end": "1822910"
  },
  {
    "text": "which became unready caused all of the pods in our cluster to become on ready not great so the next solution was",
    "start": "1822910",
    "end": "1832690"
  },
  {
    "text": "basically well let's just like hack this logic into our service mesh so we mark containers that we don't want to be",
    "start": "1832690",
    "end": "1838720"
  },
  {
    "text": "considered for pod readiness as non critical and here's the example again logger tracing set ste there's more and",
    "start": "1838720",
    "end": "1846400"
  },
  {
    "text": "so these ones are just not considered and then we just add one line to our sort of scary logic that basically says",
    "start": "1846400",
    "end": "1851830"
  },
  {
    "text": "don't consider containers that are not critical we've also considered other solutions here and this has just been like kind of",
    "start": "1851830",
    "end": "1857650"
  },
  {
    "text": "one of the stopgap solutions but it's like definitely evolving and so regardless of what your solution is I",
    "start": "1857650",
    "end": "1863680"
  },
  {
    "text": "think the main takeaway here is just be careful when configuring health techs for pods especially keep in mind you know how containers affect your overall",
    "start": "1863680",
    "end": "1869620"
  },
  {
    "text": "pod health and how crashing sidecar containers are not ready next scheduling",
    "start": "1869620",
    "end": "1875500"
  },
  {
    "start": "1874000",
    "end": "1874000"
  },
  {
    "text": "is easy and fun so this is an example of",
    "start": "1875500",
    "end": "1881530"
  },
  {
    "text": "a deploy of a service that runs in three availability zones turns out the kubernetes scheduler is not very good at",
    "start": "1881530",
    "end": "1888250"
  },
  {
    "text": "a-z balancing in fact it's really bad at agency balancing it appears and this is",
    "start": "1888250",
    "end": "1894520"
  },
  {
    "text": "speculation I haven't really looked too closely the code but it appears that in each sort of round of scheduling it will",
    "start": "1894520",
    "end": "1901330"
  },
  {
    "text": "assign all of the pods it's trying to create to a single AZ so if you have a fairly large max urge you Serge a set of",
    "start": "1901330",
    "end": "1908620"
  },
  {
    "text": "pods at once they seem to all land in one AZ at once so deploys will tend to knock services completely out of balance",
    "start": "1908620",
    "end": "1914650"
  },
  {
    "text": "and we do a lot of AZ aware routing so in our services are out of balance like",
    "start": "1914650",
    "end": "1919780"
  },
  {
    "text": "this it can cause you know significant problems in our infrastructure so",
    "start": "1919780",
    "end": "1925260"
  },
  {
    "text": "scheduler doesn't do what we want so we'll write it can how we solve this will write a controller so we implement",
    "start": "1925260",
    "end": "1932140"
  },
  {
    "text": "what we call our deployment pruner controller the way this controller works in our first attempt to fix this we're",
    "start": "1932140",
    "end": "1937810"
  },
  {
    "text": "ready we have this controller it checks for deployments that are out of AZ balance and then it tries to cautiously",
    "start": "1937810",
    "end": "1944310"
  },
  {
    "text": "delete pods until the deployment gets back into balance now this has a couple of problems still",
    "start": "1944310",
    "end": "1950280"
  },
  {
    "text": "right first of all it's fighting with the scheduler so it's deleting pods the schedulers putting the pods back but the",
    "start": "1950280",
    "end": "1956700"
  },
  {
    "text": "schedule are still not putting them back in AZ balance necessarily you want this to run fairly quickly so that you can",
    "start": "1956700",
    "end": "1961830"
  },
  {
    "text": "get back into balance quickly but you don't want to kill pods too quickly because you don't want to you know jeopardize the service running and also",
    "start": "1961830",
    "end": "1968430"
  },
  {
    "text": "if you delete too many positive ones scheduler does the wrong thing you can see in this example this went from out",
    "start": "1968430",
    "end": "1973980"
  },
  {
    "text": "of balance and one AZ to out of balance and a different AZ as the scheduler kind of fought with the pruner and the whole",
    "start": "1973980",
    "end": "1980520"
  },
  {
    "text": "process is pretty slow in this example of a deploy took over four hours for the",
    "start": "1980520",
    "end": "1985530"
  },
  {
    "text": "service to get back into AZ balance within one cluster so this is kind of",
    "start": "1985530",
    "end": "1990990"
  },
  {
    "text": "this was a stopgap solution but really not a very good one so as Melanie pointed out earlier some of the time you",
    "start": "1990990",
    "end": "1997710"
  },
  {
    "text": "just need to fix this in kubernetes so we've created a couple of patches to fix this one's been merged one is still open",
    "start": "1997710",
    "end": "2004550"
  },
  {
    "text": "so if some of you wanna someone here can help us get that accept it that'd be really nice these fix the problems fix",
    "start": "2004550",
    "end": "2011510"
  },
  {
    "text": "the ultimately fix the bug in the scheduler itself so that it distributes pods correctly to maintain AZ balance",
    "start": "2011510",
    "end": "2019210"
  },
  {
    "text": "fortunately in the case of the scheduler you don't have to fork excuse me patch kubernetes there's a workaround you can",
    "start": "2019210",
    "end": "2026960"
  },
  {
    "text": "just change the docker image of the scheduler so there's a silver lining you don't have to run a completely patch",
    "start": "2026960",
    "end": "2032090"
  },
  {
    "text": "version of kubernetes you can just kind of poke in this change and get an upgraded scheduler so the takeaway you",
    "start": "2032090",
    "end": "2038360"
  },
  {
    "text": "may need to make fixes to the community scheduler itself but you can upload at least you can easily upload them as",
    "start": "2038360",
    "end": "2044240"
  },
  {
    "text": "custom image cool so if you bit up sleep asleep up to this point now is the time",
    "start": "2044240",
    "end": "2049970"
  },
  {
    "text": "to wake up and take a picture I'll wait here's ten takeaways from ten",
    "start": "2049970",
    "end": "2055340"
  },
  {
    "text": "weird ways to blow up your kubernetes",
    "start": "2055340",
    "end": "2058660"
  },
  {
    "text": "okay that's enough of you so thank you we're hiring you can learn more at",
    "start": "2065080",
    "end": "2071118"
  },
  {
    "text": "medium comm slash every engineering epic me comm slash careers you can contact us on Twitter are these various Twitter",
    "start": "2071119",
    "end": "2077510"
  },
  {
    "text": "handles and just a quick plug directly after this talk been huge we'll be in",
    "start": "2077510",
    "end": "2083540"
  },
  {
    "text": "the room next door talking about scaling kubernetes to thousands of nodes across multiple clusters calmly and if you want to hear",
    "start": "2083540",
    "end": "2091190"
  },
  {
    "text": "more about basically ways to blow up your kubernetes but specifically focusing on latency tomorrow our co-workers Jen and Steven will be",
    "start": "2091190",
    "end": "2097820"
  },
  {
    "text": "talking about the Kuban Eddie's make my p90 5s worse thank you everyone thank you",
    "start": "2097820",
    "end": "2104859"
  },
  {
    "text": "[Applause]",
    "start": "2106840",
    "end": "2109030"
  }
]