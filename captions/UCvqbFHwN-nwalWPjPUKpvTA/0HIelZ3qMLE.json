[
  {
    "text": "hello everyone welcome to uh the session about using kubernetes to build a data lake",
    "start": "80",
    "end": "6720"
  },
  {
    "text": "for our ai and machine learning workloads and also how we used it for the entire ai and ml infrastructure stack",
    "start": "6720",
    "end": "14799"
  },
  {
    "text": "i am jude bohanna a principal product manager at red hat responsible for ai machine learning and",
    "start": "14799",
    "end": "21359"
  },
  {
    "text": "big data in the storage uh business unit with me is peter mccannon",
    "start": "21359",
    "end": "27119"
  },
  {
    "text": "and he works in the red hat ai center of excellence as a principal software engineer working on ai and machine",
    "start": "27119",
    "end": "33760"
  },
  {
    "text": "learning technologies and we have collaborated on this project for close to a couple of years",
    "start": "33760",
    "end": "39280"
  },
  {
    "text": "so this talk is about our journey that we went through and the experiences we",
    "start": "39280",
    "end": "44399"
  },
  {
    "text": "had while we were trying to build an ai and machine learning platform at red hat",
    "start": "44399",
    "end": "49680"
  },
  {
    "text": "and also about the challenges we faced on how we solve them we also open source the platform now",
    "start": "49680",
    "end": "55520"
  },
  {
    "text": "it's available for everyone to download and get it up and running and try it out",
    "start": "55520",
    "end": "60879"
  },
  {
    "text": "and pete will talk to you about those details too so let's get started",
    "start": "60879",
    "end": "67360"
  },
  {
    "text": "as you know ai and machine learning are growing workloads within the enterprise today",
    "start": "67600",
    "end": "73360"
  },
  {
    "text": "and as we started more of these our workloads pop up within the enterprises and network had we saw a set of common",
    "start": "73360",
    "end": "80720"
  },
  {
    "text": "challenges that arise from pharaohs from each of this the biggest challenge we saw",
    "start": "80720",
    "end": "86000"
  },
  {
    "text": "was at the lack of really usable data all of the data in the company was",
    "start": "86000",
    "end": "91119"
  },
  {
    "text": "siloed in different locations by departments or by the owners of the data sets",
    "start": "91119",
    "end": "96320"
  },
  {
    "text": "and the authentication and authorization policies were different and preventing access this silo data set",
    "start": "96320",
    "end": "102880"
  },
  {
    "text": "also created a challenge for teams that wanted to collaborate and it prevented teams from",
    "start": "102880",
    "end": "108640"
  },
  {
    "text": "collaborating and working together we also saw that a data scientist",
    "start": "108640",
    "end": "114479"
  },
  {
    "text": "especially were not happy with the long turnaround times they had using the traditional i.t ticketing",
    "start": "114479",
    "end": "120159"
  },
  {
    "text": "processes to get infrastructure up and running they wanted something that's real time quick and ready",
    "start": "120159",
    "end": "125920"
  },
  {
    "text": "on the cloud the the final challenge was also lack of the right tools and",
    "start": "125920",
    "end": "131120"
  },
  {
    "text": "infrastructure for the data engineers and data scientists data engineers needed something that was easy to govern their data and",
    "start": "131120",
    "end": "137599"
  },
  {
    "text": "transform the data as they needed and data scientists wanted a cloud-like experience and flexibility in their",
    "start": "137599",
    "end": "143760"
  },
  {
    "text": "in their execution so once we listed a listen to them and figured out the kind of common challenges and high priority",
    "start": "143760",
    "end": "150480"
  },
  {
    "text": "issues we were facing we came up with a set of design points around what we needed to do in our",
    "start": "150480",
    "end": "157200"
  },
  {
    "text": "you know next generation architecture so we set up three goals first is we needed a centralized",
    "start": "157200",
    "end": "164000"
  },
  {
    "text": "shared infrastructure that all the teams could use this centralized shared infrastructure",
    "start": "164000",
    "end": "169599"
  },
  {
    "text": "broke down the silos for data and also enabled collaboration across the teams because",
    "start": "169599",
    "end": "175120"
  },
  {
    "text": "they were all working off of the common platform that they had we also wanted something that was more",
    "start": "175120",
    "end": "181200"
  },
  {
    "text": "self-service driven so the customers can get a more cloud-like experience",
    "start": "181200",
    "end": "186560"
  },
  {
    "text": "and also have a real-time a workflow like they are used to we also wanted to build an",
    "start": "186560",
    "end": "192480"
  },
  {
    "text": "infrastructure that was flexible in in the sense that it could allow bring your own",
    "start": "192480",
    "end": "198640"
  },
  {
    "text": "tools kind of a workflow but also provide with a set of standard libraries that can be used on standard tooling",
    "start": "198640",
    "end": "205440"
  },
  {
    "text": "that can be used so data scientists can get off the ground and get their work done faster",
    "start": "205440",
    "end": "212239"
  },
  {
    "text": "so once we set those goals uh for on the challenges we identified we also looked at the end-to-end ai and",
    "start": "212239",
    "end": "219519"
  },
  {
    "text": "machine learning workflow within our customers and internally within our company so we wanted to figure out the different",
    "start": "219519",
    "end": "225120"
  },
  {
    "text": "personas that were emerging in the air machine learning landscape and we figured out uh it's anywhere from",
    "start": "225120",
    "end": "232640"
  },
  {
    "text": "the business side of the world which is business leadership that has financial goals to your traditional",
    "start": "232640",
    "end": "238159"
  },
  {
    "text": "itu operations that has a broad swath of responsibilities across the entire area",
    "start": "238159",
    "end": "243200"
  },
  {
    "text": "and machine learning life cycle and your app developers who create the end user applications that are being",
    "start": "243200",
    "end": "249680"
  },
  {
    "text": "used that use the machine learning workflows and the ai models we also saw the emergence and prominence of",
    "start": "249680",
    "end": "257040"
  },
  {
    "text": "new roles like data engineers that work with a lot of the datasets coming in the ml engineers that are really worried",
    "start": "257040",
    "end": "263520"
  },
  {
    "text": "about and look at the tools to develop and the models for machine learning and also data scientists who are the",
    "start": "263520",
    "end": "269759"
  },
  {
    "text": "real end users of of ai and machine learning platforms to create models and predict predictions so we",
    "start": "269759",
    "end": "277120"
  },
  {
    "text": "took uh the personas that we developed and put it against the fabric of the ai and machine learning workflow to",
    "start": "277120",
    "end": "283600"
  },
  {
    "text": "figure out where the different touch points were and where each role interacts with the system and the things we needed to do",
    "start": "283600",
    "end": "290560"
  },
  {
    "text": "to optimize the system for each of those roles so with that in mind we we we set out",
    "start": "290560",
    "end": "298320"
  },
  {
    "text": "trying to solve the problem um now that we have a set of challenges uh the personas and user requirements",
    "start": "298320",
    "end": "304080"
  },
  {
    "text": "and our goals we set out to solve the problem and find the right technologies pretty early on in the game we figured",
    "start": "304080",
    "end": "310720"
  },
  {
    "text": "out that uh containerizing on kubernetes was the right solution for us going forward",
    "start": "310720",
    "end": "316160"
  },
  {
    "text": "and would help us to build the next generation ai and machine learning platform kubernetes gave us pretty much uh most",
    "start": "316160",
    "end": "323600"
  },
  {
    "text": "of the design points we wanted that we set as goals it gave us the agility to quickly",
    "start": "323600",
    "end": "328880"
  },
  {
    "text": "respond to end users and and also provide it in a self-service way so customers can compute manage compute",
    "start": "328880",
    "end": "336800"
  },
  {
    "text": "resources as they need it it also gave us the portability of a common runtime that can be used",
    "start": "336800",
    "end": "342639"
  },
  {
    "text": "to move containers between or between companies at the edge and just on the core and",
    "start": "342639",
    "end": "347919"
  },
  {
    "text": "across different departments or across the company it also gave us more importantly the flexibility we needed",
    "start": "347919",
    "end": "354240"
  },
  {
    "text": "in in today's area and ml environments to be agile and and flexible like for example we wanted",
    "start": "354240",
    "end": "361199"
  },
  {
    "text": "to run multiple versions of the same application or the same model in parallel to figure out the different",
    "start": "361199",
    "end": "367919"
  },
  {
    "text": "outcomes and what the impact would be and kubernetes and containers gave us an easy way to do it",
    "start": "367919",
    "end": "374319"
  },
  {
    "text": "and of course kubernetes is highly scalable and it's known for scalability so that was that was a really good thing to have too",
    "start": "374319",
    "end": "382400"
  },
  {
    "text": "so uh for this part of my presentation uh i'll i will focus on how we build the data lake and the data",
    "start": "382880",
    "end": "388479"
  },
  {
    "text": "management capabilities of our ai and ml uh infrastructure that we have and and then hand over to pete to talk",
    "start": "388479",
    "end": "395199"
  },
  {
    "text": "about the higher layers of uh the machine learning libraries and the capabilities that we built on top of this data lake from a",
    "start": "395199",
    "end": "402479"
  },
  {
    "text": "technology perspective like i said we chose kubernetes and for the storage layer we also chose to build it on ceph",
    "start": "402479",
    "end": "408560"
  },
  {
    "text": "and rook and we'll talk more about it",
    "start": "408560",
    "end": "412639"
  },
  {
    "text": "so when we started to set uh design the data lake and the data architecture for",
    "start": "414400",
    "end": "419520"
  },
  {
    "text": "our ai and machine learning infrastructure in red hat we set out to understand the workflows",
    "start": "419520",
    "end": "425680"
  },
  {
    "text": "and a couple of workflows stood out as the must support so we we learned early on that there is",
    "start": "425680",
    "end": "431759"
  },
  {
    "text": "a requirement for certain kubernetes clusters to have storage that was localized within the kubernetes",
    "start": "431759",
    "end": "438160"
  },
  {
    "text": "cluster so this would either be for reasons of uh security and complaints where certain",
    "start": "438160",
    "end": "443759"
  },
  {
    "text": "data sets need to be siloed off or for a data that was temporal or not of long term use",
    "start": "443759",
    "end": "450080"
  },
  {
    "text": "or also for things like persistent storage for time-based uh operations and such and",
    "start": "450080",
    "end": "456000"
  },
  {
    "text": "for such first these clusters where the storage has to run with the kubernetes cluster running the",
    "start": "456000",
    "end": "462160"
  },
  {
    "text": "storage inside kubernetes was the best way for us to go forward and that gave us the ease of use and",
    "start": "462160",
    "end": "468000"
  },
  {
    "text": "simplicity for the cluster admin uh to manage both the compute and storage as one entity but like i said in the",
    "start": "468000",
    "end": "475440"
  },
  {
    "text": "initial part we also had a goal to build a centralized data lake that could be used",
    "start": "475440",
    "end": "481120"
  },
  {
    "text": "across the organization a data lake that could be shared across multiple kubernetes clusters",
    "start": "481120",
    "end": "486879"
  },
  {
    "text": "and and that would be the single source of truth for all data sets being used in the organization",
    "start": "486879",
    "end": "492240"
  },
  {
    "text": "so what we really wanted uh in the long term is what you see sort of in the right most rectangle in",
    "start": "492240",
    "end": "497919"
  },
  {
    "text": "this picture we realize that both apps and users will need a combination of some local storage",
    "start": "497919",
    "end": "504879"
  },
  {
    "text": "within that kubernetes cluster that's running that it's easy to manage and easy to access",
    "start": "504879",
    "end": "510400"
  },
  {
    "text": "but also the need for a long term centralized data lake where all the data can be funneled into",
    "start": "510400",
    "end": "516159"
  },
  {
    "text": "and stored for things like machine learning and analysis",
    "start": "516159",
    "end": "521839"
  },
  {
    "text": "so with that in mind uh we set out to choose the right technology like we said we chose ceph for building the data lake",
    "start": "523919",
    "end": "530880"
  },
  {
    "text": "ceph as you know is an open source storage project that provides a uniform that provides multiple protocol support",
    "start": "530880",
    "end": "538000"
  },
  {
    "text": "for object file and block it also is software defined and is highly scalable",
    "start": "538000",
    "end": "543040"
  },
  {
    "text": "so safe fit our needs from that aspect we also chose to use the rook operator",
    "start": "543040",
    "end": "549040"
  },
  {
    "text": "which is uh the operator that will enable us to spin up and manage a ceph cluster within an existing kubernetes",
    "start": "549040",
    "end": "555839"
  },
  {
    "text": "cluster for those locations where they wanted a converge like offering where compute and storage were",
    "start": "555839",
    "end": "561600"
  },
  {
    "text": "running in the same kubernetes cluster ceph has been around for a couple of decades and is highly known in the",
    "start": "561600",
    "end": "567200"
  },
  {
    "text": "industry and is a very vibrant open source project so for for now i'll focus a little bit more on",
    "start": "567200",
    "end": "573360"
  },
  {
    "text": "rook and what it does because it's relatively new rook is is a kubernetes operator that",
    "start": "573360",
    "end": "580560"
  },
  {
    "text": "can enable uh customers to spin up entire saf clusters within the existing kubernetes cluster",
    "start": "580560",
    "end": "587600"
  },
  {
    "text": "so rook will take the right daemons required for ceph like mons mgrs osgs and managers and spin them",
    "start": "587600",
    "end": "594959"
  },
  {
    "text": "spin those containers up inside a kubernetes cluster and create a full-fledged ceph cluster",
    "start": "594959",
    "end": "601920"
  },
  {
    "text": "within an existing kubernetes cluster which means you have a full functional storage cluster to use",
    "start": "601920",
    "end": "608000"
  },
  {
    "text": "right away for all your applications running in that cube cluster rook also more excitingly does all the",
    "start": "608000",
    "end": "614959"
  },
  {
    "text": "required configuration and back-end operations required to support the native cube way of accessing storage",
    "start": "614959",
    "end": "622160"
  },
  {
    "text": "using rwx volume claims rw volume claims and the newly emerging pocket claims so",
    "start": "622160",
    "end": "628480"
  },
  {
    "text": "this combination of the ability to spin up an entire cluster and automatically configure it first the",
    "start": "628480",
    "end": "634320"
  },
  {
    "text": "kubernetes storage workflows made a loop the obvious choice for us to manage safe clusters within kubernetes",
    "start": "634320",
    "end": "647839"
  },
  {
    "text": "and of course like i said for we also needed a centralized data lake that would store all the long-term data assets of a",
    "start": "648480",
    "end": "654560"
  },
  {
    "text": "company and for that we chose to build it on ceph also running on linux and in",
    "start": "654560",
    "end": "660000"
  },
  {
    "text": "this case the majority of the data was stored in object storage as you know object storage is emerging",
    "start": "660000",
    "end": "666000"
  },
  {
    "text": "and standard that is becoming pervasive for storing large data sets especially unstructured data and ceph",
    "start": "666000",
    "end": "672560"
  },
  {
    "text": "has a rich set of object storage capabilities so that gave us a really good flexibility again to recap",
    "start": "672560",
    "end": "678959"
  },
  {
    "text": "we used rook to use to instantiate safe clusters inside kubernetes for localized",
    "start": "678959",
    "end": "684560"
  },
  {
    "text": "storage operations and clusters and we used object storage as the centralized data lake running across the",
    "start": "684560",
    "end": "691040"
  },
  {
    "text": "organization we also did something really exciting to ease a data engineer's",
    "start": "691040",
    "end": "696800"
  },
  {
    "text": "task when we built this infrastructure on data lake we provided a way to automatically process incoming data into",
    "start": "696800",
    "end": "703920"
  },
  {
    "text": "the cluster and place it in the right location within the data lay so this was built using a combination of",
    "start": "703920",
    "end": "710320"
  },
  {
    "text": "self-bucket notifications kafka and k-nato serverless so in this architecture multiple data",
    "start": "710320",
    "end": "717040"
  },
  {
    "text": "streams would bring in data from external sources into a cell bucket ceph would then trigger a bucket",
    "start": "717040",
    "end": "722880"
  },
  {
    "text": "notification alerting about the presence of new data or the changes in existing data sets and",
    "start": "722880",
    "end": "728480"
  },
  {
    "text": "these bucket notifications would be received by kafka which would process them and send them over to knight to",
    "start": "728480",
    "end": "734240"
  },
  {
    "text": "serverless knight to serverless would then spin up the right serverless function to process this data source also things",
    "start": "734240",
    "end": "741040"
  },
  {
    "text": "like removing pii information tagging any required metadata tags adding the right security policies",
    "start": "741040",
    "end": "747200"
  },
  {
    "text": "and then move this new data set into the right location in the data lake so any data scientists",
    "start": "747200",
    "end": "753360"
  },
  {
    "text": "and engineers can have access to it in real time so this operation freed up the data engineer from having to manually process",
    "start": "753360",
    "end": "760079"
  },
  {
    "text": "all the incoming data and almost created a real-time data in this pipeline that automatically processes the data",
    "start": "760079",
    "end": "766880"
  },
  {
    "text": "sanitizes it does the required metadata operations and puts it in the right location and",
    "start": "766880",
    "end": "772399"
  },
  {
    "text": "for example if you're in the medical field we prototype this with an x-ray engine where",
    "start": "772399",
    "end": "777680"
  },
  {
    "text": "there's multiple x-rays or ct scan vertical images coming into a cell bucket and as soon as the images are",
    "start": "777680",
    "end": "784079"
  },
  {
    "text": "stored on the bucket a notification goes out from ceph to kafka and then to k native and k native would spin up the",
    "start": "784079",
    "end": "791360"
  },
  {
    "text": "right function that looks at these images anonymizes them removes all the pa information",
    "start": "791360",
    "end": "796720"
  },
  {
    "text": "and then moves them to the right bucket that can be used for machine learning engineers that work on this anonymized",
    "start": "796720",
    "end": "801920"
  },
  {
    "text": "data so the combination of ceph and root and this data pipeline ingest solution that we",
    "start": "801920",
    "end": "807839"
  },
  {
    "text": "created gave us a pretty good benefit for creating the data lay and the data",
    "start": "807839",
    "end": "813279"
  },
  {
    "text": "infrastructure and also a good basic framework that we can build more workloads on",
    "start": "813279",
    "end": "819360"
  },
  {
    "text": "and at this point i'll transition to pete to go some of our die and machine learning workflows and tools",
    "start": "819360",
    "end": "825839"
  },
  {
    "text": "thanks or to you pete",
    "start": "825839",
    "end": "829839"
  },
  {
    "text": "thanks uday the open data hub project started internally within red hat as a data store",
    "start": "832959",
    "end": "838560"
  },
  {
    "text": "for our own data engineers and data scientists hence the name data hub early on we",
    "start": "838560",
    "end": "845360"
  },
  {
    "text": "realized that the data scientists and the data engineers requirements for tools and aiml components are different than",
    "start": "845360",
    "end": "852720"
  },
  {
    "text": "devops requirements they are mostly ui driven they avoid using terminal commands and expect the",
    "start": "852720",
    "end": "859199"
  },
  {
    "text": "tools to include all their favorite aiml libraries that they are accustomed to using",
    "start": "859199",
    "end": "866160"
  },
  {
    "text": "collaboration and sharing is also an important requirement for their workflows to successfully deliver models to",
    "start": "866160",
    "end": "872560"
  },
  {
    "text": "production the main pain points are sharing machine learning work initially done in",
    "start": "872560",
    "end": "878480"
  },
  {
    "text": "notebooks moving the model to production from those notebooks and of course managing",
    "start": "878480",
    "end": "883760"
  },
  {
    "text": "the model while in production this includes monitoring it making sure predictions are accurate",
    "start": "883760",
    "end": "890880"
  },
  {
    "text": "watching for data drift managing resource usage for cpu gpu",
    "start": "890880",
    "end": "896160"
  },
  {
    "text": "memory and lots more",
    "start": "896160",
    "end": "899839"
  },
  {
    "text": "open data hub has an upstream and downstream relationship with many open source projects",
    "start": "901680",
    "end": "907279"
  },
  {
    "text": "open data hub derives some functionality from the upstream kubeflow project which we'll talk about",
    "start": "907279",
    "end": "913519"
  },
  {
    "text": "more in a bit however open data hub also downstreams from",
    "start": "913519",
    "end": "918560"
  },
  {
    "text": "other open source projects such as selden kafka that uday talked about earlier",
    "start": "918560",
    "end": "926480"
  },
  {
    "text": "spark as well grafana and prometheus all this to provide a comprehensive",
    "start": "926480",
    "end": "932240"
  },
  {
    "text": "end-to-end aiml platform that runs on openshift",
    "start": "932240",
    "end": "937600"
  },
  {
    "text": "in many cases enhancements and changes made in the open data hub project are also offered back upstream to the",
    "start": "937600",
    "end": "944480"
  },
  {
    "text": "original open source communities such as a variety of changes that we sent upstream to kubeflow",
    "start": "944480",
    "end": "950560"
  },
  {
    "text": "for proper openshift platform support",
    "start": "950560",
    "end": "955839"
  },
  {
    "text": "talking about the personas that uday laid out for us in the context of open data hub for the",
    "start": "959759",
    "end": "966639"
  },
  {
    "text": "data analyst persona open data hub provides integration for data lakes",
    "start": "966639",
    "end": "971759"
  },
  {
    "text": "such as an s3 interface to ceph object storage there's sql databases such as postgresql",
    "start": "971759",
    "end": "979040"
  },
  {
    "text": "and mysql and data streaming using kafka strimzy",
    "start": "979040",
    "end": "984720"
  },
  {
    "text": "for data exploration open data hub includes the superset and hue projects superset",
    "start": "984720",
    "end": "991440"
  },
  {
    "text": "you can think of as an open source version of tableau for data processing open data hub",
    "start": "991440",
    "end": "998160"
  },
  {
    "text": "provides spark and spark sql thrift server finally there are metadata tools such as",
    "start": "998160",
    "end": "1004240"
  },
  {
    "text": "the hive metastore the data scientist accesses the data through the various",
    "start": "1004240",
    "end": "1010720"
  },
  {
    "text": "storage interfaces uh who they just described",
    "start": "1010720",
    "end": "1016000"
  },
  {
    "text": "jupiter hub is also provided for a notebook development environment integrated with openshift authentication",
    "start": "1016000",
    "end": "1022959"
  },
  {
    "text": "and can also make use of available gpu resources if they're enabled on the kubernetes",
    "start": "1022959",
    "end": "1030558"
  },
  {
    "text": "nodes multiple tools for model training verification are provided",
    "start": "1030559",
    "end": "1036240"
  },
  {
    "text": "such as for the frameworks tensorflow and pi torch and of course spark",
    "start": "1036240",
    "end": "1043199"
  },
  {
    "text": "open data hub also provides pre-trained models as part of its ai library component so",
    "start": "1043199",
    "end": "1050160"
  },
  {
    "text": "pre-trained models for things like fraud detection and various other types of machine",
    "start": "1050160",
    "end": "1056400"
  },
  {
    "text": "learning algorithms for creating machine learning pipelines data scientists can use",
    "start": "1056400",
    "end": "1062400"
  },
  {
    "text": "argo and we're going to talk about kubeflow a bit more but it can also make use of kubeflow pipelines",
    "start": "1062400",
    "end": "1068720"
  },
  {
    "text": "for the devops engineers they're provided with monitoring tools such as prometheus and graffana",
    "start": "1068720",
    "end": "1075200"
  },
  {
    "text": "for the observation of all the components in the aiml",
    "start": "1075200",
    "end": "1080960"
  },
  {
    "text": "end-to-end platform and also there's model serving tools such as tensorflow serving",
    "start": "1080960",
    "end": "1087600"
  },
  {
    "text": "and selden that are provided within open data hub",
    "start": "1087600",
    "end": "1095840"
  },
  {
    "text": "the open data hub project is a meta project to integrate all these open source tools and provide",
    "start": "1096000",
    "end": "1101200"
  },
  {
    "text": "an end to end aiml platform and openshift so the meta project integrates these",
    "start": "1101200",
    "end": "1106880"
  },
  {
    "text": "open source projects into one project that makes it easier to be consumed by",
    "start": "1106880",
    "end": "1112320"
  },
  {
    "text": "users going back to sort of where all these",
    "start": "1112320",
    "end": "1117520"
  },
  {
    "text": "projects come into play in the overall ml workflow it starts with",
    "start": "1117520",
    "end": "1122880"
  },
  {
    "text": "data prepping and etling that data into a data lake or storage of some kind and making it",
    "start": "1122880",
    "end": "1129679"
  },
  {
    "text": "accessible to the data scientists the next phase is the model development which includes feature selection model",
    "start": "1129679",
    "end": "1136559"
  },
  {
    "text": "creation training and validation finally the last phase is moving that model out and serving the",
    "start": "1136559",
    "end": "1143120"
  },
  {
    "text": "model in production and this is not sort of a",
    "start": "1143120",
    "end": "1148799"
  },
  {
    "text": "one-stop model serving thing but it's goes through a constant loop of",
    "start": "1148799",
    "end": "1154080"
  },
  {
    "text": "optimization so models in production are actually monitored",
    "start": "1154080",
    "end": "1159280"
  },
  {
    "text": "to see how the model is performing against live data so that's cycle",
    "start": "1159280",
    "end": "1166720"
  },
  {
    "text": "monitoring optimizing serving that's constant and happens for the lifetime of the model",
    "start": "1166720",
    "end": "1172000"
  },
  {
    "text": "and that encompasses a collaboration between devops data scientists data engineers",
    "start": "1172000",
    "end": "1178000"
  },
  {
    "text": "and business developers so open data hub sort of provides a unifying",
    "start": "1178000",
    "end": "1186080"
  },
  {
    "text": "umbrella if you will for the activities of all those different personas",
    "start": "1186640",
    "end": "1193840"
  },
  {
    "text": "as part of the data the open data hub project we see a great deal of value in the kubeflow project um so we dedicate our",
    "start": "1196160",
    "end": "1204320"
  },
  {
    "text": "efforts to enable kubeflow on openshift and integrate the open data hub",
    "start": "1204320",
    "end": "1209679"
  },
  {
    "text": "with kubeflow in an installation based on the operator lifecycle manager",
    "start": "1209679",
    "end": "1215120"
  },
  {
    "text": "which is a key component in openshift4 it is not it is now integrated to open",
    "start": "1215120",
    "end": "1222159"
  },
  {
    "text": "data hub and runs on openshift um kubeflow brings multiple new aiml",
    "start": "1222159",
    "end": "1228720"
  },
  {
    "text": "capabilities and features so for distributed model training we have tensorflow jobs and pi torch jobs uh",
    "start": "1228720",
    "end": "1235919"
  },
  {
    "text": "mpi jobs for model serving again they're seldon but there's also a new",
    "start": "1235919",
    "end": "1241120"
  },
  {
    "text": "project called kf serving which provides an abstraction layer over the various different types of surveying",
    "start": "1241120",
    "end": "1247600"
  },
  {
    "text": "frameworks for pipelines we have kubeflow pipelines based on argo",
    "start": "1247600",
    "end": "1253760"
  },
  {
    "text": "and as part of the goal of all this we upstream any enhancements back to the kubeflow project",
    "start": "1253760",
    "end": "1259440"
  },
  {
    "text": "so we also work with the kubeflow community to add openshift as one of the supported platforms for kubeflow",
    "start": "1259440",
    "end": "1266400"
  },
  {
    "text": "is we show with the red arrow in the website menu on the right you see a list",
    "start": "1266400",
    "end": "1271440"
  },
  {
    "text": "of um cloud platforms and openshift is one of those that is",
    "start": "1271440",
    "end": "1278400"
  },
  {
    "text": "provided as supported for uh the latest kubeflow versions",
    "start": "1278400",
    "end": "1285280"
  },
  {
    "text": "so we've released the open data hub 0.7 beta operator with kubeflow integrated",
    "start": "1285280",
    "end": "1292000"
  },
  {
    "text": "into that",
    "start": "1292000",
    "end": "1294559"
  },
  {
    "text": "open data hub is an operator currently available in the openshift operator hub community it's one of the",
    "start": "1297360",
    "end": "1304640"
  },
  {
    "text": "community operators you'll find it in the embedded openshift",
    "start": "1304640",
    "end": "1310000"
  },
  {
    "text": "operator hub that ships with openshift 4. we recently released open data hub",
    "start": "1310000",
    "end": "1316320"
  },
  {
    "text": "version 0.52 that was built using the ansible operator includes",
    "start": "1316320",
    "end": "1322480"
  },
  {
    "text": "many of the aiml tools we talked about previously",
    "start": "1322480",
    "end": "1327840"
  },
  {
    "text": "but we also support a a dually a new version of the operator that is",
    "start": "1327840",
    "end": "1333200"
  },
  {
    "text": "based on the kubeflow golang kf cuddle operator and that gives us this mechanism for better integration",
    "start": "1333200",
    "end": "1340000"
  },
  {
    "text": "with kubeflow so this dual mode these two",
    "start": "1340000",
    "end": "1345679"
  },
  {
    "text": "operators zero five two is to support so-called legacy deployments",
    "start": "1345679",
    "end": "1351120"
  },
  {
    "text": "that going forward will only have bug fixes um however with the open data hub 0.7",
    "start": "1351120",
    "end": "1357919"
  },
  {
    "text": "operator based on kubeflow it integrates with kubeflow includes kubeflow components",
    "start": "1357919",
    "end": "1363840"
  },
  {
    "text": "as well as some of the legacy open data hub tools that have been migrated to the kf cuddle",
    "start": "1363840",
    "end": "1370000"
  },
  {
    "text": "customer resource definition so that version will be the target for all new feature development",
    "start": "1370000",
    "end": "1378320"
  },
  {
    "text": "open data hub is an open source community and we welcome contributions and engagement our main site is",
    "start": "1380880",
    "end": "1389039"
  },
  {
    "text": "located at opendatahub.io and our code base can be found in gitlab and github",
    "start": "1389039",
    "end": "1396400"
  },
  {
    "text": "there's two areas for that gitlab sort of hosts some of the the legacy uh features of",
    "start": "1396400",
    "end": "1402960"
  },
  {
    "text": "um open data hub where the manifest development um for features",
    "start": "1402960",
    "end": "1409600"
  },
  {
    "text": "in open data hub 0.7 are hosted in github we have bi-weekly open community",
    "start": "1409600",
    "end": "1416559"
  },
  {
    "text": "meetings and the calendar is posted in the get lab community link you see there and we also have",
    "start": "1416559",
    "end": "1423039"
  },
  {
    "text": "an aiml youtube channel for all of the video tutorials we do as well as",
    "start": "1423039",
    "end": "1429360"
  },
  {
    "text": "mailing lists as listed here on the slide",
    "start": "1429360",
    "end": "1435039"
  },
  {
    "text": "thank you for joining our talk today and now we'll be happy to take any questions",
    "start": "1436640",
    "end": "1445840"
  },
  {
    "text": "so uh who today uh do you want to take that question",
    "start": "1455120",
    "end": "1460480"
  },
  {
    "text": "number two there yep pete yeah so so the question uh question number two",
    "start": "1460480",
    "end": "1466159"
  },
  {
    "text": "uh is there redundancy between the data stored on the data lake and the local storage",
    "start": "1466159",
    "end": "1471360"
  },
  {
    "text": "if yes how is synchronization performed yes there is a way to synchronize data between your local storage",
    "start": "1471360",
    "end": "1477840"
  },
  {
    "text": "and at the network data lake to using active active replication with sub supports",
    "start": "1477840",
    "end": "1483679"
  },
  {
    "text": "so you could set it up for data to go both the ways or to just go one way and then stay in the data lake",
    "start": "1483679",
    "end": "1489360"
  },
  {
    "text": "forever it's primarily driven by object and we are making more enhancements in ceph with the community for a block",
    "start": "1489360",
    "end": "1496559"
  },
  {
    "text": "and also for file but that capability does exist today",
    "start": "1496559",
    "end": "1501840"
  },
  {
    "text": "and i will take number three and the question is could you provide links to the product or download page um",
    "start": "1503919",
    "end": "1511279"
  },
  {
    "text": "it's important to emphasize the open data hub project and i'm assuming we're talking about",
    "start": "1511279",
    "end": "1516799"
  },
  {
    "text": "open data hub as opposed to the the ceph project um the information for",
    "start": "1516799",
    "end": "1522640"
  },
  {
    "text": "that product can be found or project i'm sorry can be found at open opendatahub.io",
    "start": "1522640",
    "end": "1529120"
  },
  {
    "text": "so we have a documentation there blog posts",
    "start": "1529120",
    "end": "1534320"
  },
  {
    "text": "and we host in gitlab the the source for the various",
    "start": "1534320",
    "end": "1540320"
  },
  {
    "text": "components there there are pieces that are hosted in github for for legacy reasons",
    "start": "1540320",
    "end": "1548480"
  },
  {
    "text": "also with openshift 4 it has an embedded operator lifecycle manager and in fact",
    "start": "1548480",
    "end": "1555840"
  },
  {
    "text": "the easiest way to install open data hub is just to make use of that so you would go to the openshift4",
    "start": "1555840",
    "end": "1561840"
  },
  {
    "text": "console and uh you would bring up the catalog",
    "start": "1561840",
    "end": "1567120"
  },
  {
    "text": "the first item there will be ai and machine learning and you can just type in open data hub",
    "start": "1567120",
    "end": "1572400"
  },
  {
    "text": "and it will come up as one of the community operators and you can click on that tile and it",
    "start": "1572400",
    "end": "1578480"
  },
  {
    "text": "will sort of work you through uh basically the steps for installing that into the cluster directly",
    "start": "1578480",
    "end": "1585679"
  },
  {
    "text": "hopefully that answers the question i can take question number four uh pete",
    "start": "1585679",
    "end": "1592159"
  },
  {
    "text": "uh the question is in the automated injection pipeline does the k native serverless function read",
    "start": "1592159",
    "end": "1597840"
  },
  {
    "text": "directly from sephora does the message consume from kafka contained within itself the data payload as well",
    "start": "1597840",
    "end": "1603679"
  },
  {
    "text": "so uh both the modes are are supported in fact so today uh ceph sent a notification to kafka and",
    "start": "1603679",
    "end": "1610640"
  },
  {
    "text": "kafka initiates the knee to interaction but that is all the metadata flow the",
    "start": "1610640",
    "end": "1615760"
  },
  {
    "text": "real data is still on ceph and so cephei is the one that starts the notification process with bucket",
    "start": "1615760",
    "end": "1621279"
  },
  {
    "text": "notifications where there is also a way that we prototyped upstream where",
    "start": "1621279",
    "end": "1626480"
  },
  {
    "text": "ceph can directly send a notification to k native and trigger the help to trigger the right function that is",
    "start": "1626480",
    "end": "1632799"
  },
  {
    "text": "being uh tested right now it's upstream and we will productize it in the next coming days so depending on",
    "start": "1632799",
    "end": "1638640"
  },
  {
    "text": "your scalability need and product needs you could either use a message broker like kafka in the middle",
    "start": "1638640",
    "end": "1643679"
  },
  {
    "text": "or you could directly hook up ceph buckets to k native",
    "start": "1643679",
    "end": "1648640"
  },
  {
    "text": "uh number five i guess uday we can both sort of have a stab at it says uh the question is do you back up your",
    "start": "1652240",
    "end": "1658799"
  },
  {
    "text": "data if yes how do you back up your data and probably you day that speaks to the",
    "start": "1658799",
    "end": "1665039"
  },
  {
    "text": "sapphire architecture i would i would say sure update so uh the",
    "start": "1665039",
    "end": "1670480"
  },
  {
    "text": "backing up data part today it's through uh the replication again uh",
    "start": "1670480",
    "end": "1675520"
  },
  {
    "text": "supports native replication for all the protocols uh like rbd for block and primarily for object",
    "start": "1675520",
    "end": "1681279"
  },
  {
    "text": "and since most of the data we have is an object the way we backup is using uh the multi-site feature where",
    "start": "1681279",
    "end": "1687600"
  },
  {
    "text": "we create a second site and keep a copy of all the data in and and pete can talk about some of the",
    "start": "1687600",
    "end": "1693600"
  },
  {
    "text": "next generation backup workflows that we're working on more around time-based backup and",
    "start": "1693600",
    "end": "1698880"
  },
  {
    "text": "restoration and auto restoration and stuff and we are making some progress there it's a work in progress for",
    "start": "1698880",
    "end": "1704880"
  },
  {
    "text": "more real-time backup and stuff",
    "start": "1704880",
    "end": "1711840"
  },
  {
    "text": "okay and number two yeah thank you uh the question is with",
    "start": "1712080",
    "end": "1719440"
  },
  {
    "text": "respect to the open data hub solution architecture is the hive metadata catalog used as the primary",
    "start": "1719440",
    "end": "1725360"
  },
  {
    "text": "data catalog for example for governance purposes or does it have more of an internal use",
    "start": "1725360",
    "end": "1730960"
  },
  {
    "text": "case it is provided generally there are other solutions that we're",
    "start": "1730960",
    "end": "1738000"
  },
  {
    "text": "looking to incorporate in open data hub with respect to governance and um",
    "start": "1738000",
    "end": "1745200"
  },
  {
    "text": "there's a value evaluation going on of some other candidate open source components that",
    "start": "1745200",
    "end": "1750880"
  },
  {
    "text": "could fulfill sort of a broader governance solution there so things like atlas uh ranger",
    "start": "1750880",
    "end": "1758960"
  },
  {
    "text": "uh algeria these are all components that we're actively looking at for providing sort of a broader",
    "start": "1758960",
    "end": "1767039"
  },
  {
    "text": "data governance and self-service",
    "start": "1767039",
    "end": "1772880"
  },
  {
    "text": "type of capability for open data hub with respect to data cataloging",
    "start": "1773440",
    "end": "1781679"
  },
  {
    "text": "do you want me to read that next one uh the next question is",
    "start": "1781679",
    "end": "1788559"
  },
  {
    "text": "uh looking to use this for a research product on large archive feeding data",
    "start": "1789440",
    "end": "1795200"
  },
  {
    "text": "into such design as ceph chosen for weather data so the use case is large volumes of weather",
    "start": "1795200",
    "end": "1802960"
  },
  {
    "text": "data the concern is pulling our access into",
    "start": "1802960",
    "end": "1808320"
  },
  {
    "text": "our archive using this design given that it's 40 petabytes or more",
    "start": "1808320",
    "end": "1814159"
  },
  {
    "text": "and there are security and perhaps restricted information there as well",
    "start": "1814159",
    "end": "1820799"
  },
  {
    "text": "looking for comparisons here has this been passed uh passed on to the new cncf",
    "start": "1820799",
    "end": "1827360"
  },
  {
    "text": "research group for review as working with them for reviews so",
    "start": "1827360",
    "end": "1832399"
  },
  {
    "text": "um open data hub is uh an open source project it is not currently affiliated with uh",
    "start": "1832399",
    "end": "1839520"
  },
  {
    "text": "cncf in terms of under any of the umbrella projects for that",
    "start": "1839520",
    "end": "1846640"
  },
  {
    "text": "we'd be probably interested in the community would be interesting in hearing more about this use case the",
    "start": "1847360",
    "end": "1853679"
  },
  {
    "text": "community meets bi-weekly on mondays and on",
    "start": "1853679",
    "end": "1858720"
  },
  {
    "text": "our open data hub io website we have the upcoming schedule i believe",
    "start": "1858720",
    "end": "1864320"
  },
  {
    "text": "for uh the community meeting so we welcome this this use case and other use cases like it to",
    "start": "1864320",
    "end": "1870880"
  },
  {
    "text": "to um to have for further discussion",
    "start": "1870880",
    "end": "1875679"
  },
  {
    "text": "uh number eight yeah number eight is a few questions same as number eight as you scroll through",
    "start": "1876880",
    "end": "1882559"
  },
  {
    "text": "it's almost the same question what be stroke who does this run on and stuff so it's all your speed yeah um",
    "start": "1882559",
    "end": "1890720"
  },
  {
    "text": "so one form of the question is that openshift only the other form is does this run on stock kubernetes",
    "start": "1890720",
    "end": "1897279"
  },
  {
    "text": "openshift is kubernetes however it does have extensions and some of those extensions",
    "start": "1897279",
    "end": "1903440"
  },
  {
    "text": "are uh used um natively in open data hub so it is",
    "start": "1903440",
    "end": "1909600"
  },
  {
    "text": "targeted for open shift a lot of the constructs will be obviously familiar in terms of",
    "start": "1909600",
    "end": "1917600"
  },
  {
    "text": "deployments and pods and services and things like that but we also make use of uh",
    "start": "1917600",
    "end": "1925039"
  },
  {
    "text": "resource objects and openshift called uh routes which are actually a higher level of abstraction of ingress",
    "start": "1925039",
    "end": "1932240"
  },
  {
    "text": "resources so there's various um sort of areas where we are specifically",
    "start": "1932240",
    "end": "1938640"
  },
  {
    "text": "providing um a tool a suite of tools basically",
    "start": "1938640",
    "end": "1943919"
  },
  {
    "text": "uh specifically designed for open um open shift",
    "start": "1943919",
    "end": "1949519"
  },
  {
    "text": "so question number nine is more of a comment uh so yes piloting now and using many of the same tools",
    "start": "1949519",
    "end": "1955360"
  },
  {
    "text": "we would love to have more feedback and even have you collaborate with the community like people is referring we do have",
    "start": "1955360",
    "end": "1961360"
  },
  {
    "text": "regular meetings for the community that are on the website so please feel free to join in",
    "start": "1961360",
    "end": "1966480"
  },
  {
    "text": "share feedback and we can learn from you and you can get more information from the community too",
    "start": "1966480",
    "end": "1972080"
  },
  {
    "text": "and question number 10 is the same the session in the slides yes we will follow the regular cncf path of making everything available",
    "start": "1972080",
    "end": "1979200"
  },
  {
    "text": "as soon as cncf makes all of the sessions available the session and the slides",
    "start": "1979200",
    "end": "1985840"
  },
  {
    "text": "i think question 11 we just answered what questions are this run on and 12 we just answered",
    "start": "1985919",
    "end": "1994159"
  },
  {
    "text": "13 so p to 13 is is again similar to what we're talking about can this run on native kubernetes and you talked about",
    "start": "1995120",
    "end": "2000720"
  },
  {
    "text": "the extensions and stuff so yep ah here's one sorry there's multiple",
    "start": "2000720",
    "end": "2008000"
  },
  {
    "text": "pages to the q a so uh the question does open data hub support",
    "start": "2008000",
    "end": "2013360"
  },
  {
    "text": "gpu resource sharing scheduling um it does provide gpu support in terms of",
    "start": "2013360",
    "end": "2020480"
  },
  {
    "text": "um automatically building um gpu-enabled notebooks that can be launched from",
    "start": "2020480",
    "end": "2027440"
  },
  {
    "text": "jupiter to hub which is one of the components in in open data hub so",
    "start": "2027440",
    "end": "2033200"
  },
  {
    "text": "the actual um the details of gpu enablement i've done talks about",
    "start": "2033200",
    "end": "2040000"
  },
  {
    "text": "this um in other venues like uh devonation and stuff like that um",
    "start": "2040000",
    "end": "2049360"
  },
  {
    "text": "it's enabled as an operator in openshift and kubernetes nvidia actually",
    "start": "2049760",
    "end": "2055760"
  },
  {
    "text": "provides the the gpu operator for that so that's how the enablement is done",
    "start": "2055760",
    "end": "2063839"
  },
  {
    "text": "um another question we're just closing up here running out of time we need",
    "start": "2063839",
    "end": "2068960"
  },
  {
    "text": "disconnected install for odh it is not supported yet uh yes we're aware of that it's uh it's",
    "start": "2068960",
    "end": "2075280"
  },
  {
    "text": "an issue that we're um still looking into and",
    "start": "2075280",
    "end": "2080560"
  },
  {
    "text": "also we have data sets in nfs large seismic data sets how do you picture we can consume these data",
    "start": "2080560",
    "end": "2086960"
  },
  {
    "text": "um maybe that's the last question for you uday sure yeah uh so you can consume them",
    "start": "2086960",
    "end": "2092480"
  },
  {
    "text": "using the native pv uh persistent volume construct that kubernetes provides",
    "start": "2092480",
    "end": "2098480"
  },
  {
    "text": "an openshift provide or you can also do out of band access but we recommend the persistent volume a",
    "start": "2098480",
    "end": "2104800"
  },
  {
    "text": "way of consuming the data sets already in nfs because that follows with the data on the control path that's emerging in the",
    "start": "2104800",
    "end": "2111040"
  },
  {
    "text": "kubernetes pa to read and write persistent data",
    "start": "2111040",
    "end": "2117839"
  },
  {
    "text": "and i believe that's all we have time for and we do appreciate all the",
    "start": "2117920",
    "end": "2125359"
  },
  {
    "text": "the attendees uh joining our talk today yep thanks folks it's been a good",
    "start": "2125359",
    "end": "2131839"
  },
  {
    "text": "interactive session so thanks for the questions and looking forward to interacting with most of you on one of the community meeting",
    "start": "2131839",
    "end": "2138079"
  },
  {
    "text": "days thank you thanks",
    "start": "2138079",
    "end": "2145440"
  }
]