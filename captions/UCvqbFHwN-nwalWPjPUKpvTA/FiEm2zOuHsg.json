[
  {
    "text": "it's working hello hello hello gosh there's a lot of you welcome thank you thank you for attending obviously this is the last one",
    "start": "0",
    "end": "5680"
  },
  {
    "text": "between between you and beer so we're trying to keep this as short as possible um",
    "start": "5680",
    "end": "12000"
  },
  {
    "text": "welcome to our talk so today uh guy and i we we were talking about",
    "start": "12000",
    "end": "17039"
  },
  {
    "text": "how we might uh pitch our talk about cube to kubecon um and we kind of agreed",
    "start": "17039",
    "end": "23680"
  },
  {
    "text": "that the vendor talks are great right they're they're good you learn a lot but",
    "start": "23680",
    "end": "28800"
  },
  {
    "text": "actually the thing that brings us to conferences is the war stories right we like to hear",
    "start": "28800",
    "end": "33920"
  },
  {
    "text": "from customers we like to hear from companies um who actually talk about the things that they do in production",
    "start": "33920",
    "end": "39920"
  },
  {
    "text": "and and the failure states that they've seen in production because we learn from it they learn from it it's great so that",
    "start": "39920",
    "end": "46000"
  },
  {
    "text": "that was our pitch we decided to take the second biggest outage revenue-wise",
    "start": "46000",
    "end": "51600"
  },
  {
    "text": "of skyscanner and present it to you all right bare bones and everything we're just going to",
    "start": "51600",
    "end": "57039"
  },
  {
    "text": "talk about it we're going to talk about how we got there uh we're going to talk about the actual technical failures the",
    "start": "57039",
    "end": "63280"
  },
  {
    "text": "the cultural failures all the big mock-ups um with the idea that this is essentially three free therapy for us",
    "start": "63280",
    "end": "70240"
  },
  {
    "text": "right we're yeah we've got 20 minutes of you guys just you guys got just sitting listening to us and uh we get to talk",
    "start": "70240",
    "end": "77520"
  },
  {
    "text": "about it and if you learn something from it then it means uh it's less of a failure right you've got",
    "start": "77520",
    "end": "84159"
  },
  {
    "text": "something from it we've got something from it it's a data point rather than a failure but before we go any further",
    "start": "84159",
    "end": "90640"
  },
  {
    "text": "ah i need this thanks okay um we should introduce ourselves i'm the top one my",
    "start": "90640",
    "end": "96079"
  },
  {
    "text": "name is stuart davidson i'm a director of engineering at skyscanner i run the production platform tripe",
    "start": "96079",
    "end": "101439"
  },
  {
    "text": "which is an entity a group of squads that run",
    "start": "101439",
    "end": "106640"
  },
  {
    "text": "almost like a platform as a service so we run all the interest the kind of compute traffic routing observability",
    "start": "106640",
    "end": "114399"
  },
  {
    "text": "um incident management everything everything that our product teams use to get their product across the globe and",
    "start": "114399",
    "end": "120799"
  },
  {
    "text": "we we do all the kind of heavy lifting underneath uh and i'm guy templeton can you can you",
    "start": "120799",
    "end": "126399"
  },
  {
    "text": "hear a guy no all right this is guy templeton he is the the lead kubernetes engineer for",
    "start": "126399",
    "end": "132720"
  },
  {
    "text": "skyscanner he's also the the co-lead of um the sig auto scaling um here so",
    "start": "132720",
    "end": "137840"
  },
  {
    "text": "that's him there oh no there you go all right this is sky",
    "start": "137840",
    "end": "143599"
  },
  {
    "text": "scanner now some of you might not know what sky scanner is skyscanner is a website that",
    "start": "143599",
    "end": "148959"
  },
  {
    "text": "puts together a bunch of flights hotels and car hire from about 1200 travel partners across",
    "start": "148959",
    "end": "155120"
  },
  {
    "text": "the globe and you want to go somewhere maybe multiple places at once or in a",
    "start": "155120",
    "end": "160319"
  },
  {
    "text": "kind of a holiday we put together them all together for the cheapest possible price",
    "start": "160319",
    "end": "166080"
  },
  {
    "text": "to give you a sense of the scale of skyscanner we serve at the moment i think it was last month we serve about 75 million unique customers a month okay",
    "start": "166080",
    "end": "174720"
  },
  {
    "text": "so that gives you a sense of the size and scale of the website that we're talking about",
    "start": "174720",
    "end": "180159"
  },
  {
    "text": "today we're going to go through three different different areas i'm going to talk about skyscanner's cloud native journey right so how did we get from the",
    "start": "180159",
    "end": "186560"
  },
  {
    "text": "data centers and as not a small amount of users but a far less amount of users",
    "start": "186560",
    "end": "192239"
  },
  {
    "text": "to where we are just before the incident guy's going to talk to you about the incident and actually the failure state",
    "start": "192239",
    "end": "198239"
  },
  {
    "text": "and how we recovered it and then we're going to go through it together and talk about some of the learnings because",
    "start": "198239",
    "end": "203360"
  },
  {
    "text": "that's ultimately what you're here for right we're going to give you some context then you're going to get the learnings hopefully we'll have lots of",
    "start": "203360",
    "end": "208560"
  },
  {
    "text": "time for questions as well so please think about these questions this is a chance for you to learn from us for us to learn from you so this is a",
    "start": "208560",
    "end": "215120"
  },
  {
    "text": "deliberate attempt for us to share some information it's maybe not the best forum for it but there's beers after this so we can do it there",
    "start": "215120",
    "end": "222720"
  },
  {
    "text": "sky scanners cloud native journey so i joined skyscanner in 2015 so it was about seven eight",
    "start": "222720",
    "end": "228480"
  },
  {
    "text": "years ago and at that time we had about one mil just less than one million users a month",
    "start": "228480",
    "end": "234400"
  },
  {
    "text": "then we got to 10 million users a month and just before covert hit we were over 100 million users a month",
    "start": "234400",
    "end": "240640"
  },
  {
    "text": "so that's 10x and then 10x again and it has been a wild journey right",
    "start": "240640",
    "end": "247040"
  },
  {
    "text": "the the the number of pieces of infrastructure the number of uh systems that you trusted in that had never",
    "start": "247040",
    "end": "254080"
  },
  {
    "text": "failed in years suddenly became bottlenecks suddenly became uh things",
    "start": "254080",
    "end": "259199"
  },
  {
    "text": "that wouldn't scale the way you needed it to or suddenly added latency or or some sort of uh i don't know problem",
    "start": "259199",
    "end": "265919"
  },
  {
    "text": "that you hadn't foreseen so it's been a really fun journey kind of iterating and iterating and",
    "start": "265919",
    "end": "271280"
  },
  {
    "text": "re-establishing what you're going to do and thinking about it from almost the ground up several times along",
    "start": "271280",
    "end": "277199"
  },
  {
    "text": "the way um we started off in data centers like everyone i guess",
    "start": "277199",
    "end": "284240"
  },
  {
    "text": "we're what about 15 years old as a company so we were in data centers we had five data centers across the globe",
    "start": "284240",
    "end": "291120"
  },
  {
    "text": "and uh yeah we made this shift to the public cloud we saw the potential of the public cloud you can see you know we",
    "start": "291120",
    "end": "297600"
  },
  {
    "text": "were scaling so quickly that actually buying infrastructure and getting it plugged in and things like it just wasn't sustainable and we saw the",
    "start": "297600",
    "end": "304240"
  },
  {
    "text": "opportunity of allowing our teams to decide on their own for infrastructure and all the benefits of is coding that",
    "start": "304240",
    "end": "309600"
  },
  {
    "text": "sort of thing so we made this shift to the cloud now i say that in one sentence one bullet point as if it was a simple",
    "start": "309600",
    "end": "316560"
  },
  {
    "text": "transition but um we are still paying the price of our migration from the data centers to the cloud even yet right so",
    "start": "316560",
    "end": "323840"
  },
  {
    "text": "at one point we just decided to not renew our data center contract that's how it took us such a long time to",
    "start": "323840",
    "end": "330880"
  },
  {
    "text": "transition the workloads from a data center native into a cloud native solution with all the the fun and games",
    "start": "330880",
    "end": "337600"
  },
  {
    "text": "of auto scaling and and and failure states and such so we lifted and shifted a ton of our workloads and we're still",
    "start": "337600",
    "end": "344400"
  },
  {
    "text": "paying that technical debt off at the moment but we're in aws fully now so that's that's our cloud provider we are",
    "start": "344400",
    "end": "350080"
  },
  {
    "text": "fully aws at the same time uh around sort of 2015 2016 this thing called docker was",
    "start": "350080",
    "end": "357199"
  },
  {
    "text": "starting to happen and it was at the time where people in conference talks like this would say who here's heard of",
    "start": "357199",
    "end": "362880"
  },
  {
    "text": "docker and people will go wow and they were like who's using it in production they go oh no no",
    "start": "362880",
    "end": "368560"
  },
  {
    "text": "um like it was just about that time where it's all quite exciting but we hadn't quite decided what we're going to do with it",
    "start": "368560",
    "end": "374639"
  },
  {
    "text": "skyscanner decided to try it out with our ci solution right so it's quite a",
    "start": "374639",
    "end": "381280"
  },
  {
    "text": "common kind of first step to try a container native ci solution it's actually the precursor to harness it was",
    "start": "381280",
    "end": "386800"
  },
  {
    "text": "a thing called drone io it's an open source solution container native and we really started to see the benefit",
    "start": "386800",
    "end": "393120"
  },
  {
    "text": "of ephemeral build agents that the squads could define right instead of release engineers mucking around with",
    "start": "393120",
    "end": "398639"
  },
  {
    "text": "team city agents and ansible scripts and things like that squads could define their own",
    "start": "398639",
    "end": "403919"
  },
  {
    "text": "ephemeral build agents in containers and do their own builds we thought this was great it was groundbreaking to",
    "start": "403919",
    "end": "409120"
  },
  {
    "text": "skyscanner and that was one of the the real drivers to us starting to talk about using containers really pretty",
    "start": "409120",
    "end": "416240"
  },
  {
    "text": "early on in production we had a look at a bunch of options at that time and there was loads of",
    "start": "416240",
    "end": "421919"
  },
  {
    "text": "container schedulers at that time and we decided because we're moving at jws we would go all in and we would use ecs",
    "start": "421919",
    "end": "429919"
  },
  {
    "text": "and this was the grand plan for our ecs right solution so we we asked squads to define",
    "start": "429919",
    "end": "436800"
  },
  {
    "text": "in yaml uh the configuration of their service and then we would point our our internal",
    "start": "436800",
    "end": "444000"
  },
  {
    "text": "tool called slingshot at um each git repo and from there we would deploy a route",
    "start": "444000",
    "end": "449759"
  },
  {
    "text": "53 entry um behind an elb in front of an elb sorry and then we",
    "start": "449759",
    "end": "455199"
  },
  {
    "text": "would deploy containers just by adding them and removing them from the elb nothing more complicated than that",
    "start": "455199",
    "end": "461360"
  },
  {
    "text": "again this was totally groundbreaking like it totally revolutionized how skyscanner was doing things because it",
    "start": "461360",
    "end": "467919"
  },
  {
    "text": "used to be that services and squads would have to talk to the release engineering team they would get their",
    "start": "467919",
    "end": "472960"
  },
  {
    "text": "ansible scripts updated they would they would actually have a release engineer who's like have you done your",
    "start": "472960",
    "end": "478560"
  },
  {
    "text": "tests etc etc with this it was all self-service right so squads could",
    "start": "478560",
    "end": "485440"
  },
  {
    "text": "just deploy the services without engaging with release engineering and it got to this crazy state where we didn't",
    "start": "485440",
    "end": "491280"
  },
  {
    "text": "actually know which services were being deployed it was kind of a bit scary for the release engineering team and our okr",
    "start": "491280",
    "end": "496879"
  },
  {
    "text": "was uh like get eight services deployed on slingshot and we ended up with like 62 by the end of the month and our okr",
    "start": "496879",
    "end": "503680"
  },
  {
    "text": "stats for the for the month and the quarter were like thousands of percent it was brilliant and but this really",
    "start": "503680",
    "end": "509440"
  },
  {
    "text": "kind of progressed our our path along using containers into production and it",
    "start": "509440",
    "end": "514560"
  },
  {
    "text": "it made a it was ah what was the main it was so important",
    "start": "514560",
    "end": "519839"
  },
  {
    "text": "because squads could deploy with confidence right that was that was the main",
    "start": "519839",
    "end": "525760"
  },
  {
    "text": "takeaway from it they could deploy with confidence and it moved it away from being this really",
    "start": "525760",
    "end": "531440"
  },
  {
    "text": "scary really a difficult process with lots of human interaction it became this",
    "start": "531440",
    "end": "537760"
  },
  {
    "text": "uh ubiquitous thing that just happened in the background like squads could deploy several times a day and it almost it",
    "start": "537760",
    "end": "544560"
  },
  {
    "text": "became a strategic enabler right people use deployments to get out of trouble rather than causing trouble and it just",
    "start": "544560",
    "end": "551200"
  },
  {
    "text": "it was it was such a big change for skyscanner ecs took us a long way and in fact we",
    "start": "551200",
    "end": "558399"
  },
  {
    "text": "still use ecs to this day almost almost were out of ecs but",
    "start": "558399",
    "end": "564000"
  },
  {
    "text": "we were finding that we were having to rebuild a lot of components that we were seeing happening in the open source",
    "start": "564000",
    "end": "569440"
  },
  {
    "text": "community and we decided to take on kubernetes we thought this is the way forward now again there was a lot of schedulers at",
    "start": "569440",
    "end": "575600"
  },
  {
    "text": "the time but kubernetes had quite a strong ecosystem as demonstrated here it's continued that journey um",
    "start": "575600",
    "end": "583519"
  },
  {
    "text": "and we went through many many iterations of trying to get kubernetes to work like i",
    "start": "583519",
    "end": "589519"
  },
  {
    "text": "say we haven't fully got all our services into kubernetes yet there are still some on ecs our first iteration we got a consultant",
    "start": "589519",
    "end": "596720"
  },
  {
    "text": "in and we spent a lot of money on it and it was at a time again where no one",
    "start": "596720",
    "end": "601839"
  },
  {
    "text": "really knew how to run kubernetes at scale so they learned a lot we learned a lot um and ultimately the solution that",
    "start": "601839",
    "end": "609120"
  },
  {
    "text": "we we ended up with at the end didn't last about it was about a month a month and a half before we started to",
    "start": "609120",
    "end": "615279"
  },
  {
    "text": "see problems as our workloads grew then we iterated and we started uh we",
    "start": "615279",
    "end": "621839"
  },
  {
    "text": "did this typical skyscanner thing of the time because we built so much in ecs we",
    "start": "621839",
    "end": "627519"
  },
  {
    "text": "started to rebuild and build custom solutions on top of kubernetes rather than leveraging the open source",
    "start": "627519",
    "end": "633839"
  },
  {
    "text": "community that was one of the main selling points for kubernetes and again we went through several",
    "start": "633839",
    "end": "640720"
  },
  {
    "text": "iterations of kubernetes as kind of i guess architectures at that time",
    "start": "640720",
    "end": "647839"
  },
  {
    "text": "it got to a point where we got some critical workloads onto kubernetes but we had enormous clusters like we had",
    "start": "650480",
    "end": "658240"
  },
  {
    "text": "one cluster per region we were in four regions we had one cluster per region and these clusters were enormous",
    "start": "658240",
    "end": "664560"
  },
  {
    "text": "um well what we thought was enormous i mean we had the keynote by cern right they're enormous we thought we were enormous at",
    "start": "664560",
    "end": "670880"
  },
  {
    "text": "the time and but because it was a single cluster it meant if any failure the failure mode",
    "start": "670880",
    "end": "677200"
  },
  {
    "text": "like any sort of slight failure the whole thing went down and we had one particular outage um",
    "start": "677200",
    "end": "684560"
  },
  {
    "text": "where the business asked us to slow down stop changing things in kubernetes right stop upgrading kubernetes well we can't",
    "start": "684560",
    "end": "691839"
  },
  {
    "text": "the the the upgrade path of kubernetes it's like a re-release every 12 to 14 weeks right so we couldn't slow down and",
    "start": "691839",
    "end": "698480"
  },
  {
    "text": "that's just the core version of kubernetes right you've got all the different components different parts of the kubernetes architecture",
    "start": "698480",
    "end": "705200"
  },
  {
    "text": "so we couldn't slow down and i remember i was about to go on stage a github satellite and i was about",
    "start": "705200",
    "end": "710959"
  },
  {
    "text": "to talk about our continuous deployment system and how rapid iteration reduces risk and like that's the way we wanted",
    "start": "710959",
    "end": "716959"
  },
  {
    "text": "to go forward in skyscanner um but paul gillespie who's our senior tech tech in uh in production platform",
    "start": "716959",
    "end": "723680"
  },
  {
    "text": "he phoned me and he said look we've just had this big outage not this one we're about to talk about we had another outage um and the business really wants",
    "start": "723680",
    "end": "730560"
  },
  {
    "text": "us to slow down but i couldn't advocate i couldn't say let's slow down and then literally 20 minutes later go on stage",
    "start": "730560",
    "end": "736160"
  },
  {
    "text": "and say we should go fast because it reduces risk so i said to paul there needs to be a different way we need to approach this",
    "start": "736160",
    "end": "742079"
  },
  {
    "text": "in a different fashion is that is there any other way of doing this",
    "start": "742079",
    "end": "747760"
  },
  {
    "text": "so all credit to paul and all credit to guy and all credit to guys team we went back to basics and we started again and",
    "start": "748160",
    "end": "754800"
  },
  {
    "text": "we started to reevaluate all the technologies that we were using and actually which ones were valuable and",
    "start": "754800",
    "end": "760079"
  },
  {
    "text": "try and de-clutter all the different things we'd installed in our kubernetes architecture",
    "start": "760079",
    "end": "767040"
  },
  {
    "text": "there's a really good book called good strategy bad strategy and i really recommend you read it if you've ever tried to do a strategy someone will ask",
    "start": "767040",
    "end": "773440"
  },
  {
    "text": "you to do a strategy and i really recommend you read it because it starts with the diagnosis so what's",
    "start": "773440",
    "end": "779519"
  },
  {
    "text": "wrong right let's start with what's wrong not the solutions but the diagnosis of the problems and one of our big problems was that we weren't using",
    "start": "779519",
    "end": "786399"
  },
  {
    "text": "industry standards i talked about that so we decided specifically to set a policy of moving to industry standards",
    "start": "786399",
    "end": "793519"
  },
  {
    "text": "um guy will talk a bit more about that about this bit but we started to adopt like i say more open standards more open",
    "start": "793519",
    "end": "799360"
  },
  {
    "text": "uh more open tooling technologies but the big fundamental shift was a move",
    "start": "799360",
    "end": "805360"
  },
  {
    "text": "to a cellular architecture so this is what our architecture looks like in terms of kubernetes at the",
    "start": "805360",
    "end": "812839"
  },
  {
    "text": "moment what we decided to do was to to look at the failure states um that we had to",
    "start": "812839",
    "end": "819680"
  },
  {
    "text": "work with and create an architecture around that so in this particular case this",
    "start": "819680",
    "end": "825920"
  },
  {
    "text": "this isn't actually well named that's a cluster not a cell okay sorry about that that's that's my fault so these are",
    "start": "825920",
    "end": "831680"
  },
  {
    "text": "different clusters there's a cell per region and within a cell there are many clusters we have an even",
    "start": "831680",
    "end": "838399"
  },
  {
    "text": "amount of uh clusters per az and we",
    "start": "838399",
    "end": "844639"
  },
  {
    "text": "each workload is n plus two clusters big to allow one",
    "start": "844639",
    "end": "850000"
  },
  {
    "text": "cluster to be down for maintenance or upgrade and one cluster to be down for a",
    "start": "850000",
    "end": "855040"
  },
  {
    "text": "failure state of any sort and even then a hundred percent of the the work the um requests coming through",
    "start": "855040",
    "end": "861920"
  },
  {
    "text": "should be served by by our architecture now this has an amazing benefit beyond",
    "start": "861920",
    "end": "868240"
  },
  {
    "text": "being a really resilient architecture one that's other than the one we're about to talk about uh hasn't cost us it caused us a",
    "start": "868240",
    "end": "875519"
  },
  {
    "text": "production incident and we use spot instances a hundred percent in production so our entire",
    "start": "875519",
    "end": "883680"
  },
  {
    "text": "infrastructure is spot and that has saved us literally millions of dollars",
    "start": "883680",
    "end": "888959"
  },
  {
    "text": "the company has been saved millions of dollars because of this resilience if our nodes are taken out because",
    "start": "888959",
    "end": "895199"
  },
  {
    "text": "spot termination we've got that resilience we can deal with the failure state really easily",
    "start": "895199",
    "end": "901279"
  },
  {
    "text": "it's part it's ingrained it's part of the actual default state um so this is this is the state where",
    "start": "901279",
    "end": "907600"
  },
  {
    "text": "we're in now what actually happened right so",
    "start": "907600",
    "end": "913199"
  },
  {
    "text": "pride cometh before a fall i've just told you that the cells architecture is the best",
    "start": "913199",
    "end": "918560"
  },
  {
    "text": "architecture ever ever ever and at 9 52 and 33 seconds on the 25th of august",
    "start": "918560",
    "end": "926079"
  },
  {
    "text": "there was an engineering all hands and it was a really good question because we moved about 70 of our workloads into the",
    "start": "926079",
    "end": "931519"
  },
  {
    "text": "sales architecture and i was asked the question why by by another team that hadn't moved yet why",
    "start": "931519",
    "end": "937759"
  },
  {
    "text": "was it a good idea to move the sales architecture and i said well it's the traditional theory of constraints bottlenecks but we have",
    "start": "937759",
    "end": "944560"
  },
  {
    "text": "solved the compute problem it is now the most available resilient thing we're going to make move to a different",
    "start": "944560",
    "end": "950560"
  },
  {
    "text": "bottleneck which at that point was traffic routing and the solution for traffic routing will be based on sales",
    "start": "950560",
    "end": "955600"
  },
  {
    "text": "so you should move but cells has fixed the availability problem at 1553",
    "start": "955600",
    "end": "963040"
  },
  {
    "text": "not one day no it was the same day argo cd",
    "start": "963040",
    "end": "969360"
  },
  {
    "text": "deleted every service in every cluster in every region across the globe",
    "start": "969360",
    "end": "975440"
  },
  {
    "text": "ultimately because we told it to guy",
    "start": "975440",
    "end": "981440"
  },
  {
    "text": "thank you so how did we go from can you hear me yet",
    "start": "981440",
    "end": "986560"
  },
  {
    "text": "no no no yeah that sounds good good job awesome so how did how did we go from",
    "start": "986560",
    "end": "994480"
  },
  {
    "text": "478 services running serving travelers to zero um and it",
    "start": "994480",
    "end": "1000480"
  },
  {
    "text": " good stories do it's a no-op change just merge this late on the day um and this is the change that killed",
    "start": "1000480",
    "end": "1008839"
  },
  {
    "text": "skyscanner uh yeah so um as you can see removed some ginger brackets and what",
    "start": "1008839",
    "end": "1015519"
  },
  {
    "text": "could possibly go wrong so very quickly soon after that rather than seeing our lovely homepage and",
    "start": "1015519",
    "end": "1021680"
  },
  {
    "text": "being able to search for flights accommodation carhar people instead",
    "start": "1021680",
    "end": "1027760"
  },
  {
    "text": "didn't even see this nice error page instead they saw this",
    "start": "1027760",
    "end": "1034160"
  },
  {
    "text": "which it's not the greatest traveler experience it turns out and people get very confused as to why they're not uh",
    "start": "1035679",
    "end": "1042720"
  },
  {
    "text": "getting access to night for their flights um and very quickly someone uh was able to",
    "start": "1042720",
    "end": "1048558"
  },
  {
    "text": "guess what had happened um and was smelling istio mesh because we use istio",
    "start": "1048559",
    "end": "1053840"
  },
  {
    "text": "we have mtls we use authorization policies and uh when we deleted",
    "start": "1053840",
    "end": "1059360"
  },
  {
    "text": "everything sdo started going no you're not allowed to call anything um",
    "start": "1059360",
    "end": "1064880"
  },
  {
    "text": "so in terms of what actually happened though and going back to what stewart mentioned we have a",
    "start": "1064880",
    "end": "1070799"
  },
  {
    "text": "slingshot or deployment tooling and for reasons when we moved to when we started",
    "start": "1070799",
    "end": "1076400"
  },
  {
    "text": "moving to kubernetes we went we don't want our developers to have to relearn tooling or",
    "start": "1076400",
    "end": "1082320"
  },
  {
    "text": "create completely change their uh application uh specification so we kept the same",
    "start": "1082320",
    "end": "1088760"
  },
  {
    "text": "slingshot.yaml format and just added on kubernetes support so they could add a few different fields and now instead of",
    "start": "1088760",
    "end": "1095600"
  },
  {
    "text": "being deployed to ecs they were getting deployed to kubernetes for free um however that meant we were in a",
    "start": "1095600",
    "end": "1103520"
  },
  {
    "text": "weird in-between state and this is where the trouble uh originated because we were we had some",
    "start": "1103520",
    "end": "1110559"
  },
  {
    "text": "things which would move to the industry standards of githubs and then we had all of the things that users deployed onto",
    "start": "1110559",
    "end": "1116559"
  },
  {
    "text": "the cluster that people care about things like deployments services hpas",
    "start": "1116559",
    "end": "1121600"
  },
  {
    "text": "and service monitors meanwhile gitops is deploying things like sd objects resource quotas but",
    "start": "1121600",
    "end": "1127440"
  },
  {
    "text": "critically namespaces which means that gitops is controlling something that",
    "start": "1127440",
    "end": "1133600"
  },
  {
    "text": "contains things which are not get ops controlled",
    "start": "1133600",
    "end": "1138400"
  },
  {
    "text": "so in terms of how that works um for those who aren't familiar with gitops um i'm",
    "start": "1138640",
    "end": "1144799"
  },
  {
    "text": "assuming most people will be but we have a repository where people declare like",
    "start": "1144799",
    "end": "1150240"
  },
  {
    "text": "their service and on board it and say which cells they want to deploy to uh what how many resources how many pods",
    "start": "1150240",
    "end": "1157919"
  },
  {
    "text": "they'll have what resources look like that so we can do some of the templating of like resource quotas to prevent",
    "start": "1157919",
    "end": "1164480"
  },
  {
    "text": "runaway scaling etc for users and then argo cd is is responsible for doing that",
    "start": "1164480",
    "end": "1170160"
  },
  {
    "text": "helm chart templating based on the values and then charts defined in the repo and then applies them to the",
    "start": "1170160",
    "end": "1176799"
  },
  {
    "text": "clusters so if we go back to this this one line",
    "start": "1176799",
    "end": "1182640"
  },
  {
    "text": "change what this is being used for by argo is telling it what clusters to apply all to",
    "start": "1182640",
    "end": "1190240"
  },
  {
    "text": "all of these objects to so multiple layers of templating this this becomes uh for argo the driver",
    "start": "1190240",
    "end": "1198720"
  },
  {
    "text": "of what applications do you need to diff against the cluster and apply to the cluster uh so when this",
    "start": "1198720",
    "end": "1204960"
  },
  {
    "text": "became a invalid thing it suddenly is going oh i don't need to apply any",
    "start": "1204960",
    "end": "1210799"
  },
  {
    "text": "objects to any cluster oh there's a lot of objects there that i don't need i'm gonna delete those now",
    "start": "1210799",
    "end": "1217840"
  },
  {
    "text": "um so in terms of our recovery that that meant that we had githubs nice easy",
    "start": "1217840",
    "end": "1225200"
  },
  {
    "text": "revert the pr all of our name spaces are back but all of those objects that people care about like deployments and",
    "start": "1225200",
    "end": "1232159"
  },
  {
    "text": "services are not back um which is a bit painful so in terms of the recovery uh",
    "start": "1232159",
    "end": "1238960"
  },
  {
    "text": "we had to first mitigate so we got people our nice error page so that they",
    "start": "1238960",
    "end": "1244320"
  },
  {
    "text": "could at least see you know a traveler with a surfboard instead of an our back error",
    "start": "1244320",
    "end": "1249440"
  },
  {
    "text": "then we had uh prioritizing getting a single region functional and then uh also shed non-critical load",
    "start": "1249440",
    "end": "1256640"
  },
  {
    "text": "so things like price alerts for travelers where we want to prioritize getting",
    "start": "1256640",
    "end": "1262720"
  },
  {
    "text": "users quotes for their travel or um information about the tickets they've already bought but price alerts we can",
    "start": "1262720",
    "end": "1269360"
  },
  {
    "text": "we can do without for a couple of days so this is a graph of the namespaces and",
    "start": "1269360",
    "end": "1274799"
  },
  {
    "text": "you can see very obviously where we deleted everything and how we reverted things",
    "start": "1274799",
    "end": "1280480"
  },
  {
    "text": "and classed argo cd slowly sort of started going oh wait i've got thousands of objects to reapply here",
    "start": "1280480",
    "end": "1286880"
  },
  {
    "text": "um but you'll see that's that's a very short gap like",
    "start": "1286880",
    "end": "1292320"
  },
  {
    "text": "and it's definitely under an hour for some clusters to recover",
    "start": "1292320",
    "end": "1298080"
  },
  {
    "text": "however this is the traffic graph for skyscanner effectively over that",
    "start": "1298080",
    "end": "1303520"
  },
  {
    "text": "time period and you can see that outage is far longer and is pretty choppy um to",
    "start": "1303520",
    "end": "1310080"
  },
  {
    "text": "recovering to basically normal levels afterwards so in terms of",
    "start": "1310080",
    "end": "1316559"
  },
  {
    "text": "why that took so long that's that's the other bits of restoring clusters um and",
    "start": "1316559",
    "end": "1322880"
  },
  {
    "text": "that's that's far tougher because it was manual recovery we had dusty run books that were designed for not a cluster",
    "start": "1322880",
    "end": "1330159"
  },
  {
    "text": "where we deleted all the services but kept the same cluster but instead a cluster where we'd we'd just wiped out",
    "start": "1330159",
    "end": "1336960"
  },
  {
    "text": "deleted the entire cluster spun up a new cluster and restored um and we also discovered that our",
    "start": "1336960",
    "end": "1342000"
  },
  {
    "text": "runbooks were not easy to follow when a stressed engineer is trying to copy and paste like these two blocks are very similar and",
    "start": "1342000",
    "end": "1349360"
  },
  {
    "text": "make it very easy to copy and paste the wrong thing when so you might be trying",
    "start": "1349360",
    "end": "1354559"
  },
  {
    "text": "to restore the wrong cl xcd cluster so this is this is what the recovery",
    "start": "1354559",
    "end": "1362320"
  },
  {
    "text": "looked like of the actual services so this is the number of services running on each cluster and you can see here",
    "start": "1362320",
    "end": "1368400"
  },
  {
    "text": "we've got this like stepped recovery as we did the manual restore in each each",
    "start": "1368400",
    "end": "1374400"
  },
  {
    "text": "cluster so eventually get back to a point where we left it overnight where actually",
    "start": "1374400",
    "end": "1379600"
  },
  {
    "text": "that the strength of that cell-based architecture we were able to serve all of skyscanner's critical traveler",
    "start": "1379600",
    "end": "1385440"
  },
  {
    "text": "traffic out of just four clusters in a single region and able to allow our",
    "start": "1385440",
    "end": "1390960"
  },
  {
    "text": "support engineers to get a proper night's sleep before they resumed recovering the next day",
    "start": "1390960",
    "end": "1397120"
  },
  {
    "text": "so what can we what have we learned from it what can you learn from it",
    "start": "1397120",
    "end": "1403840"
  },
  {
    "text": "the the the risk here came from us crossing this chasm between we want to get",
    "start": "1404159",
    "end": "1409919"
  },
  {
    "text": "from where we were to using all these standard tools we want to get all get ops we want to be fully based in that",
    "start": "1409919",
    "end": "1417120"
  },
  {
    "text": "world but we are in an in-between state where we've actually caused",
    "start": "1417120",
    "end": "1422559"
  },
  {
    "text": "potentially more risk for ourselves by still being in that state where get ops can cause a complete outage but",
    "start": "1422559",
    "end": "1429039"
  },
  {
    "text": "then gitops can't be used to resolve all of the outage uh",
    "start": "1429039",
    "end": "1435200"
  },
  {
    "text": "yeah i mean we we provided argo cd the power to apply all these helm charts um and",
    "start": "1435200",
    "end": "1441600"
  },
  {
    "text": "it used that power and deleted everything uh finally templates templates with",
    "start": "1441600",
    "end": "1447120"
  },
  {
    "text": "logic are actually good like we we made a single uh line change to a templating",
    "start": "1447120",
    "end": "1452480"
  },
  {
    "text": "file and managed to wipe out the entire uh cluster this wasn't caught um",
    "start": "1452480",
    "end": "1459039"
  },
  {
    "text": "pr time because we don't have tests on it because it's just a template why would you write tests for that",
    "start": "1459039",
    "end": "1465120"
  },
  {
    "text": "um especially when you've got multiple layers of templating i would recommend",
    "start": "1465120",
    "end": "1470480"
  },
  {
    "text": "trying to find a way to test that and make sure that what you're changing does what you think it does",
    "start": "1470480",
    "end": "1477360"
  },
  {
    "text": "um and this may seem obvious but don't do a global config deploys um since since we had this outage we have made a",
    "start": "1477600",
    "end": "1484159"
  },
  {
    "text": "change so that even changes to templating like that will only ever roll out to a small subset of clusters at a",
    "start": "1484159",
    "end": "1490080"
  },
  {
    "text": "time so that theoretically we can only still only take uh down a single cluster i mean i'm",
    "start": "1490080",
    "end": "1496480"
  },
  {
    "text": "now worried that i'm going to get paged and get told that all of the clusters have gone down again yeah yeah because you've just said yeah",
    "start": "1496480",
    "end": "1504000"
  },
  {
    "text": "okay it's my turn you switch my mic on yeah can you hear me excellent excellent right um yeah i just",
    "start": "1504000",
    "end": "1510880"
  },
  {
    "text": "wanted to quickly talk about um another part of the less technical side of this",
    "start": "1510880",
    "end": "1515919"
  },
  {
    "text": "right so guy talked about the technical solution the change that was made",
    "start": "1515919",
    "end": "1520960"
  },
  {
    "text": "to the file that was right at the root it was the root file of our templating",
    "start": "1520960",
    "end": "1526960"
  },
  {
    "text": "and the engineer that made the change even he was an experienced engineer",
    "start": "1526960",
    "end": "1533279"
  },
  {
    "text": "right he was an experienced engineer and he made a change that",
    "start": "1533279",
    "end": "1538559"
  },
  {
    "text": "i don't know looking at it it's a pretty it's a pretty easy one to miss",
    "start": "1538559",
    "end": "1544720"
  },
  {
    "text": "and it also got pr'd like sorry it got reviewed by a very senior engineer and they",
    "start": "1544720",
    "end": "1551279"
  },
  {
    "text": "missed it as well now the title of our talk was how a couple of characters brought down our",
    "start": "1551279",
    "end": "1557200"
  },
  {
    "text": "site that's a bit tongue-in-cheek it's a couple of characters in terms of the curly braces it's also a couple of characters as in people right there's a",
    "start": "1557200",
    "end": "1564880"
  },
  {
    "text": "bit of a joke yeah it's a bit okay it's not a great one sky scanner",
    "start": "1564880",
    "end": "1570559"
  },
  {
    "text": "really works hard at having a blameless attitude to incidents right humans are",
    "start": "1570559",
    "end": "1575919"
  },
  {
    "text": "fallible they make mistakes everyone does it and that's why the sales architecture is so important to us",
    "start": "1575919",
    "end": "1581760"
  },
  {
    "text": "because it allows people to make mistakes now we thought we'd got all the failure states and we thought we'd put",
    "start": "1581760",
    "end": "1587120"
  },
  {
    "text": "all the guard rails in place but like guy says we didn't apply testing to our to our templating and we did global",
    "start": "1587120",
    "end": "1593440"
  },
  {
    "text": "deploys in a way that we never thought possible that root file hadn't changed in like three years",
    "start": "1593440",
    "end": "1598559"
  },
  {
    "text": "um so it was a failure state that we hadn't considered but we know that humans fail",
    "start": "1598559",
    "end": "1603679"
  },
  {
    "text": "so when it actually came to the incident management side of things that's where i",
    "start": "1603679",
    "end": "1609279"
  },
  {
    "text": "genuinely believe skyscanner sean because everyone that was responsible needed to come to the response of what",
    "start": "1609279",
    "end": "1615919"
  },
  {
    "text": "had happened and that's not just engineers we're talking legal we're talking user satisfaction teams we're",
    "start": "1615919",
    "end": "1622559"
  },
  {
    "text": "like the ceo et cetera et cetera like everybody came into a room and other than the cxos that's not fair the ceo",
    "start": "1622559",
    "end": "1628400"
  },
  {
    "text": "wasn't the vp of avenge did pop in for a little while and then left to let the engineers get on with fixing the problem",
    "start": "1628400",
    "end": "1634320"
  },
  {
    "text": "rather than figuring out who made the mistake now we still know who made the mistake but we've not talked about it in any",
    "start": "1634320",
    "end": "1641120"
  },
  {
    "text": "ilds uh incident learning documents uh which is what we do after an incident and why we got some of these uh",
    "start": "1641120",
    "end": "1647279"
  },
  {
    "text": "reflections and retrospectives um but it really wasn't valuable because everybody moved in one direction to fix the",
    "start": "1647279",
    "end": "1653120"
  },
  {
    "text": "problem rather than trying to cover their asses and trying to cover up what went wrong which would ultimately delay",
    "start": "1653120",
    "end": "1659120"
  },
  {
    "text": "the resolving of this problem now i got a couple of quotes from engineers now the",
    "start": "1659120",
    "end": "1665440"
  },
  {
    "text": "everybody was tired and quite emotional so these quotes are a little bit um emotional but i think they're awesome",
    "start": "1665440",
    "end": "1671039"
  },
  {
    "text": "like guy says we actually because of the architecture we got to send everybody home really pretty early on about 11 o'clock",
    "start": "1671039",
    "end": "1677039"
  },
  {
    "text": "at night but people were tired because it happened four o'clock in the afternoon people had",
    "start": "1677039",
    "end": "1682320"
  },
  {
    "text": "had a full working day and then they had to resolve this problem full outage",
    "start": "1682320",
    "end": "1687919"
  },
  {
    "text": "but yeah like the the positivity and calmness to give us the space to triage and recover like these quotes are are to",
    "start": "1687919",
    "end": "1695760"
  },
  {
    "text": "me really inspirational and i i share them with you not to beg us up but to try and advocate to you",
    "start": "1695760",
    "end": "1702320"
  },
  {
    "text": "the sense of achievement of that blameless culture right so try it try it in your",
    "start": "1702320",
    "end": "1707760"
  },
  {
    "text": "organization next time you have a problem don't look at who just look at what just just leave it out don't even",
    "start": "1707760",
    "end": "1713120"
  },
  {
    "text": "talk about the person you don't need to it's not it's not important failures happen",
    "start": "1713120",
    "end": "1718799"
  },
  {
    "text": "um so yeah i know i'm starting to bang on about it a little bit but i thought it was important folks thank you for your attention and i",
    "start": "1718799",
    "end": "1725679"
  },
  {
    "text": "know it's maybe one of the shorter talks but we're really keen to hear some questions if you've got any any things you want to ask thank you",
    "start": "1725679",
    "end": "1734200"
  },
  {
    "text": "uh i mean we were kind of hoping someone there's a question down there but i don't know if",
    "start": "1743919",
    "end": "1749039"
  },
  {
    "text": "there's a microphone we're kind of hoping there was somebody with a mic yeah is there someone with a mic",
    "start": "1749039",
    "end": "1755600"
  },
  {
    "text": "yeah all right tell you about me you you you yeah thanks carlos",
    "start": "1756960",
    "end": "1763720"
  },
  {
    "text": "first of all thanks for sharing failure stories are always uh interesting so",
    "start": "1766880",
    "end": "1773760"
  },
  {
    "text": "actually i have two questions the first one is about you mentioned that",
    "start": "1773760",
    "end": "1779279"
  },
  {
    "text": "never you should never configure global configurations do we have uh stages",
    "start": "1779279",
    "end": "1785919"
  },
  {
    "text": "different stagings for githubs and the second one is about maybe do you have a",
    "start": "1785919",
    "end": "1791520"
  },
  {
    "text": "disaster recovery procedure to maybe create a new region at regular intervals",
    "start": "1791520",
    "end": "1799919"
  },
  {
    "text": "uh so in terms of the uh the the the rollouts yeah we have a",
    "start": "1799919",
    "end": "1805520"
  },
  {
    "text": "concept that we call channels um so we have different clusters in different channels",
    "start": "1805520",
    "end": "1810559"
  },
  {
    "text": "so we have effectively add dev alpha beta and main and and",
    "start": "1810559",
    "end": "1816080"
  },
  {
    "text": "effectively pr time we enforce the changes only roll out to a given channel a single channel at a time",
    "start": "1816080",
    "end": "1823279"
  },
  {
    "text": "um so that we effectively cannot roll out global changes um i i am sure there is some way",
    "start": "1823279",
    "end": "1831600"
  },
  {
    "text": "and somehow that we could could uh cause it to happen and we've tried to catch everything we we could um",
    "start": "1831600",
    "end": "1839120"
  },
  {
    "text": "so yeah we we basically progress changes through those channels we have different testing mechanisms as well um",
    "start": "1839120",
    "end": "1846080"
  },
  {
    "text": "yeah there was quite a an argument um quite a debate about do you want to separate each region and",
    "start": "1846080",
    "end": "1852640"
  },
  {
    "text": "have like a separate argo cd and a separate infrastructure for each region",
    "start": "1852640",
    "end": "1858880"
  },
  {
    "text": "and it's that balance of like efficiency against that blast radius and ultimately we have we have stuck with the single um",
    "start": "1858880",
    "end": "1865600"
  },
  {
    "text": "instance of argo cd to rule out the different regions but like guy says we've got the different um files um",
    "start": "1865600",
    "end": "1871279"
  },
  {
    "text": "in terms of your regional question in terms of rolling out a new region um because we're in four regions we we",
    "start": "1871279",
    "end": "1878159"
  },
  {
    "text": "haven't got a sort of hit this button and roll out a brand new region um",
    "start": "1878159",
    "end": "1883200"
  },
  {
    "text": "but because we're in four regions we're comfortable deploying to multiple regions so if that were to happen",
    "start": "1883200",
    "end": "1888320"
  },
  {
    "text": "obviously there's a couple of layers under the sales architecture in terms of aws and such um but all that's infrastructure's code so i think we",
    "start": "1888320",
    "end": "1894640"
  },
  {
    "text": "could do it pretty quickly if if forced um maybe we should run a war game on it",
    "start": "1894640",
    "end": "1901440"
  },
  {
    "text": "but uh yeah it's certainly something that we theoretically could but it probably wouldn't be our first option",
    "start": "1901440",
    "end": "1908559"
  },
  {
    "text": "cool sorry man we'll get to you next time hi um yeah great talk",
    "start": "1909600",
    "end": "1916240"
  },
  {
    "text": "do you ever do any dr like uh drills about like recovery from things like",
    "start": "1916240",
    "end": "1921600"
  },
  {
    "text": "this because that seems uh potentially like something that no one ever drills for like total outage",
    "start": "1921600",
    "end": "1927760"
  },
  {
    "text": "but you know what happens if suddenly everything was to get deleted how quickly can you get back up like is that",
    "start": "1927760",
    "end": "1933840"
  },
  {
    "text": "worth even drilling what do you guys think oh it's it's really challenging right how much",
    "start": "1933840",
    "end": "1939200"
  },
  {
    "text": "time do you spend on a disaster recovery but also a backup isn't a backup until you've restored it",
    "start": "1939200",
    "end": "1944799"
  },
  {
    "text": "um so there's definitely a kind of a uh i guess uh what we're trying to say um",
    "start": "1944799",
    "end": "1950720"
  },
  {
    "text": "challenge there friction there right um we have done subsequently we have done far more disaster recoveries than we",
    "start": "1950720",
    "end": "1956320"
  },
  {
    "text": "have in the past particularly security-based ones we've done we've done some really good stuff with our security tribe and gone okay a malicious",
    "start": "1956320",
    "end": "1962399"
  },
  {
    "text": "actor has come in and switched this off what do you do and some of those um",
    "start": "1962399",
    "end": "1967519"
  },
  {
    "text": "kind of theoretical scenarios you don't need infrastructure you don't need the",
    "start": "1967519",
    "end": "1972559"
  },
  {
    "text": "the um chaos engineering or anything like that to actually run through that all it was was a powerpoint deck and some you know uh sugared up actors that",
    "start": "1972559",
    "end": "1979440"
  },
  {
    "text": "kind of went ah no there's things around and that was actually a lot of fun and we spent an afternoon doing that so that was pretty cheap um but even during the",
    "start": "1979440",
    "end": "1986640"
  },
  {
    "text": "bat do you want to talk about the the backup issue that we had yeah yeah so as as i mentioned we we had a runbook for",
    "start": "1986640",
    "end": "1992480"
  },
  {
    "text": "doing uh restores of xcd um at the time we were using cops to manage our clusters so we we had the lcd clusters",
    "start": "1992480",
    "end": "2000320"
  },
  {
    "text": "um but like as i mentioned we we had practiced restoring it but only ever onto",
    "start": "2000320",
    "end": "2006240"
  },
  {
    "text": "completely fresh clusters where we imagined the entire cluster had been deleted we hadn't imagined this as the",
    "start": "2006240",
    "end": "2012080"
  },
  {
    "text": "potential scenario where we were trying to effectively roll xcd back in time um",
    "start": "2012080",
    "end": "2017279"
  },
  {
    "text": "which did show up some of the some flaws in the run book and some assumptions where we'd we made incorrect assumptions",
    "start": "2017279",
    "end": "2023760"
  },
  {
    "text": "so we had to do some on the fly uh stuff we have",
    "start": "2023760",
    "end": "2028880"
  },
  {
    "text": "we have since moved to eks we're using valero now is the backup tool for that and and we we are",
    "start": "2028880",
    "end": "2035039"
  },
  {
    "text": "validating those um more often like we covered these these scenarios and know how know how it would behave in both",
    "start": "2035039",
    "end": "2042159"
  },
  {
    "text": "scenarios of a fresh cluster and a a cluster that is in a",
    "start": "2042159",
    "end": "2047360"
  },
  {
    "text": "like bad state but there was also an instance to do with i am policies so we",
    "start": "2047360",
    "end": "2053520"
  },
  {
    "text": "we we had restored a bunch of stuff guy will go into what um but then security did an audit of ryan policies and locked",
    "start": "2053520",
    "end": "2059679"
  },
  {
    "text": "them all down so getting access to the backup suddenly became an issue right yeah yeah trying to on the fly discover why um",
    "start": "2059679",
    "end": "2068960"
  },
  {
    "text": "cops had uh created or had not cleaned up old i am old backups so we were stuck",
    "start": "2068960",
    "end": "2076240"
  },
  {
    "text": "listing thousands upon thousands upon thousands of objects trying to go no just give me that i don't i don't",
    "start": "2076240",
    "end": "2082800"
  },
  {
    "text": "want you to carry on listing i just want you to restore um the iam policy had removed the ability to delete the deltas so we had",
    "start": "2082800",
    "end": "2090158"
  },
  {
    "text": "like hundreds of deltas rather than six which was meant to be the policy and so there's definitely things like that that will always catch you",
    "start": "2090159",
    "end": "2097200"
  },
  {
    "text": "yes what extent do you go to right",
    "start": "2097200",
    "end": "2102240"
  },
  {
    "text": "carlos can this gentleman he put his hand up first so grab him",
    "start": "2102240",
    "end": "2107680"
  },
  {
    "text": "yeah so as far as recovery is concerned what is the first thing you do once you once you saw it you are running zero",
    "start": "2108079",
    "end": "2113839"
  },
  {
    "text": "services did you just revert the commit and rerun the cd so we",
    "start": "2113839",
    "end": "2119920"
  },
  {
    "text": "yeah we immediately reverted the pr um and one of the one of the",
    "start": "2119920",
    "end": "2125599"
  },
  {
    "text": "joys of the the blameless incident culture was we had someone who was not an engineer involved in like the actual",
    "start": "2125599",
    "end": "2132240"
  },
  {
    "text": "service who took on an instant commander role so that freed and they were doing the coordination between squad so that",
    "start": "2132240",
    "end": "2138320"
  },
  {
    "text": "freed my squad to do the the revert and start figuring out how do we restore things and whilst another a member of",
    "start": "2138320",
    "end": "2145920"
  },
  {
    "text": "another squad the traffic routine squad inside sky scanner was able to do the the the effects of like let's let's send",
    "start": "2145920",
    "end": "2152560"
  },
  {
    "text": "people to a static error page until we have services back yeah and that incident commander role is really",
    "start": "2152560",
    "end": "2158320"
  },
  {
    "text": "important and there's a lot there's some new incident management tools out there that are kind of slack-based and they",
    "start": "2158320",
    "end": "2163920"
  },
  {
    "text": "drive this attitude of having an incident commander who's not doing anything technical and is dealing with",
    "start": "2163920",
    "end": "2168960"
  },
  {
    "text": "the comms to the cxos and the legal department because the legal department had to be told and like twitter like our",
    "start": "2168960",
    "end": "2175359"
  },
  {
    "text": "um social media teams and things like that they were dealing with that so that the technical teams could get on with fixing the technical issues",
    "start": "2175359",
    "end": "2183280"
  },
  {
    "text": "why did it take years to restore traffic but because of those those issues that we we mentioned in terms of like cops",
    "start": "2185119",
    "end": "2191760"
  },
  {
    "text": "etc um it took us it took a it took us time to uh",
    "start": "2191760",
    "end": "2197200"
  },
  {
    "text": "we had to dust off the run but we had to figure out exactly like was a restore going to actually fix all",
    "start": "2197200",
    "end": "2204160"
  },
  {
    "text": "of the services we needed like we there was there was a bit of um",
    "start": "2204160",
    "end": "2209280"
  },
  {
    "text": "there had to be a discussion in terms of like what was the best way to approach all of that restore um so yeah that was",
    "start": "2209280",
    "end": "2215200"
  },
  {
    "text": "that was largely the the lag there and the restore because the clusters were",
    "start": "2215200",
    "end": "2220320"
  },
  {
    "text": "quite big and the restorers were not the fastest either well one of my one of the favorite",
    "start": "2220320",
    "end": "2226880"
  },
  {
    "text": "things that one of my engineers said shout out to caitlin she says before you make a big decision have a go make a cup",
    "start": "2226880",
    "end": "2232640"
  },
  {
    "text": "of tea right because if you make a quick decision in a situation like that you",
    "start": "2232640",
    "end": "2238079"
  },
  {
    "text": "can make it worse a lot worse and there was at one point we were like this is going to take days because like guy says git ops replaced all the name spaces but",
    "start": "2238079",
    "end": "2244960"
  },
  {
    "text": "then how do we deploy each service and that was actually going like do we have to talk to each squad and figure out so",
    "start": "2244960",
    "end": "2250320"
  },
  {
    "text": "there was definitely a moment of time where we stopped and went right what what is it we're trying to do and what's the quickest way of doing it",
    "start": "2250320",
    "end": "2256400"
  },
  {
    "text": "rather than going just do that so yeah it saved us days but it did cost us you know time",
    "start": "2256400",
    "end": "2265280"
  },
  {
    "text": "can you talk about your data persistence i'm here yeah hi um can you talk about data persistence um how much are you",
    "start": "2265280",
    "end": "2271920"
  },
  {
    "text": "doing in the cluster and how much outside you you've mentioned restore but that sounded like scd restored to me",
    "start": "2271920",
    "end": "2277520"
  },
  {
    "text": "but then other data persistence for your services so we currently only run um stateless",
    "start": "2277520",
    "end": "2285520"
  },
  {
    "text": "workloads in our kubernetes clusters um everything staple was pretty much an aws",
    "start": "2285520",
    "end": "2291200"
  },
  {
    "text": "managed service rds elastic and whatever it may be and the",
    "start": "2291200",
    "end": "2297119"
  },
  {
    "text": "the one thing we do is uh we have um prometheus but",
    "start": "2297119",
    "end": "2302880"
  },
  {
    "text": "in in those cases it was we we have those um [Music]",
    "start": "2302880",
    "end": "2308880"
  },
  {
    "text": "we had some ebs volume snapshotting i think as well um but again we i think we",
    "start": "2308880",
    "end": "2314880"
  },
  {
    "text": "just ended up discarding the data because we went as metric data like we have thanos for long-term retention it's",
    "start": "2314880",
    "end": "2320800"
  },
  {
    "text": "generally metrics about a time when we've not had any traffic on the cluster so it was it was again one of those",
    "start": "2320800",
    "end": "2326880"
  },
  {
    "text": "things where we we had the discussion and when is it worth the time to try and do that no let's let's move on and uh do",
    "start": "2326880",
    "end": "2333440"
  },
  {
    "text": "the restores of more regions yeah and you're right you know it's it's the third rail right how do you",
    "start": "2333440",
    "end": "2339280"
  },
  {
    "text": "deal with stateful services um but in our case such a large proportion of our services",
    "start": "2339280",
    "end": "2345359"
  },
  {
    "text": "are stateless so we're benefiting from spot instances on kubernetes and saving a lot of money doing it when it comes to state full services we take a different",
    "start": "2345359",
    "end": "2351920"
  },
  {
    "text": "approach and we we leverage aws far more",
    "start": "2351920",
    "end": "2356160"
  },
  {
    "text": "thank you to carlos for this by the way he's just taking this on we're not quite sure why there's not a track host or someone that's doing the questions but",
    "start": "2362720",
    "end": "2368720"
  },
  {
    "text": "thanks carlos hi um just wondering if you had any cluster rather scalars that just",
    "start": "2368720",
    "end": "2374320"
  },
  {
    "text": "annihilated your notes and you had to start from like five notes or something",
    "start": "2374320",
    "end": "2379760"
  },
  {
    "text": "uh no i i don't think the cluster auto scaler's ever done that to us",
    "start": "2379760",
    "end": "2385599"
  },
  {
    "text": "he says famous last words um we have i think we have had misbehaving hpas um",
    "start": "2385599",
    "end": "2392160"
  },
  {
    "text": "there used to be bugs in the hpa where it would happily scale some things down when one metric said and the other",
    "start": "2392160",
    "end": "2399040"
  },
  {
    "text": "metrics were unavailable um which led to like i think a prometheus outage",
    "start": "2399040",
    "end": "2405359"
  },
  {
    "text": "led to some services being scaled down which was was less than ideal shall we say",
    "start": "2405359",
    "end": "2410880"
  },
  {
    "text": "um once we realized we we managed to get a patch in upstream",
    "start": "2410880",
    "end": "2417440"
  },
  {
    "text": "but we also like as soon as we restored prometheus it scaled back up",
    "start": "2417440",
    "end": "2422880"
  },
  {
    "text": "so eventually how did you get to test a template on your cases um did",
    "start": "2422960",
    "end": "2429040"
  },
  {
    "text": "you do some another clusters uh so we",
    "start": "2429040",
    "end": "2434240"
  },
  {
    "text": "have um we effectively have test values that we",
    "start": "2434240",
    "end": "2439440"
  },
  {
    "text": "we now use for templating so effectively if people are um changing templating the ideas that we",
    "start": "2439440",
    "end": "2446720"
  },
  {
    "text": "we will drive that um the test values through that templating engine and check",
    "start": "2446720",
    "end": "2452800"
  },
  {
    "text": "against a uh unexpected output yeah unit testing yeah actual unit treat it like code so",
    "start": "2452800",
    "end": "2458960"
  },
  {
    "text": "it needs unit tests um",
    "start": "2458960",
    "end": "2463039"
  },
  {
    "text": "hey um so if you use purely spot instances have you got a backup plan in",
    "start": "2465520",
    "end": "2470800"
  },
  {
    "text": "the unlikely event that there's not enough spot instances to handle all of your traffic interesting you see that at",
    "start": "2470800",
    "end": "2477200"
  },
  {
    "text": "this moment in time with an aws person in the room as well",
    "start": "2477200",
    "end": "2482319"
  },
  {
    "text": "uh yes we well we we use we have a diverse range of instance",
    "start": "2482319",
    "end": "2489359"
  },
  {
    "text": "types like we we use the cluster autoscaler and auto scaling group still so we we do need to revisit those those",
    "start": "2489359",
    "end": "2495599"
  },
  {
    "text": "instance mixes as new instances are launched add them into the potential",
    "start": "2495599",
    "end": "2501200"
  },
  {
    "text": "um pool effectively if we needed to we could change all of our auto scaling groups to",
    "start": "2501200",
    "end": "2507440"
  },
  {
    "text": "to on demand instead of spot we've also moved traffic to different regions right so eu central one had a",
    "start": "2507440",
    "end": "2512800"
  },
  {
    "text": "problem last last year sort of october time where i i'm gonna i'm gonna raise my fist at cern again i think they were",
    "start": "2512800",
    "end": "2519040"
  },
  {
    "text": "well no it wasn't certain but every so often cern will take all the spot instances from eu central one and they're like where's ours but um yeah we",
    "start": "2519040",
    "end": "2525839"
  },
  {
    "text": "shifted traffic to u.s one and that that saves us a lot of problems i mean we've got a lot of savings plans and things",
    "start": "2525839",
    "end": "2531440"
  },
  {
    "text": "like that that we can set up load groups where we're using reserved instances and we have done that in the past but um",
    "start": "2531440",
    "end": "2537040"
  },
  {
    "text": "because we use such a diverse range of different spot instance types we can get away with it and we haven't really been",
    "start": "2537040",
    "end": "2543119"
  },
  {
    "text": "in a situation where we've needed to like 100 percent move to on demand and we have used on demand when when there's",
    "start": "2543119",
    "end": "2549520"
  },
  {
    "text": "big peaks as well",
    "start": "2549520",
    "end": "2552480"
  },
  {
    "text": "wow we fill our filibustered",
    "start": "2559440",
    "end": "2563480"
  },
  {
    "text": "um i only put my hand up because it's kind of related so it was more just a question to both of you because of sky scanner yeah um i know you guys have is",
    "start": "2565040",
    "end": "2572640"
  },
  {
    "text": "it turbo lift yes yep turbo lift help with any of these things",
    "start": "2572640",
    "end": "2578000"
  },
  {
    "text": "or was that because when i saw the title of the the presentation i was kind of going was it turbolift",
    "start": "2578000",
    "end": "2585599"
  },
  {
    "text": "those you don't know turbolift just mass pr to everyone's github repos but it does yeah well no yeah after you sir i i",
    "start": "2585680",
    "end": "2593280"
  },
  {
    "text": "um so it was one of the things we were considering when we thought are we going to have to redeploy everything um like",
    "start": "2593280",
    "end": "2601359"
  },
  {
    "text": "do do we need to use turbolift to like raise prs against all the repos in sky scanner and tell them to deploy to",
    "start": "2601359",
    "end": "2607040"
  },
  {
    "text": "different clusters etc um but like we we sat down and had that",
    "start": "2607040",
    "end": "2612880"
  },
  {
    "text": "conversation and when uh we reckon the restores will will fix it there's",
    "start": "2612880",
    "end": "2619440"
  },
  {
    "text": "it would definitely be useful in other kinds of outage um and we have used it for um [Music]",
    "start": "2619440",
    "end": "2625440"
  },
  {
    "text": "where we've changed infrastructure and just uh need to need service owners to update their their",
    "start": "2625440",
    "end": "2630480"
  },
  {
    "text": "specifications etc there's one one slight challenge with you using something like ecr so we use",
    "start": "2630480",
    "end": "2636079"
  },
  {
    "text": "ecr as a container registry we think it's amazing um i'm not getting paid for anybody else to say that we just it's",
    "start": "2636079",
    "end": "2642000"
  },
  {
    "text": "been so robust after using other container registry products that we have to run ourselves in the past but the one",
    "start": "2642000",
    "end": "2647760"
  },
  {
    "text": "problem with it is it's got the account id in it so if we were going through an issue where there was a regional failure",
    "start": "2647760",
    "end": "2653359"
  },
  {
    "text": "or an account problem and particularly with sales each region is a separate account we would maybe have to do a mass",
    "start": "2653359",
    "end": "2659599"
  },
  {
    "text": "change of the account id and we would love to see that getting changed in aws",
    "start": "2659599",
    "end": "2665280"
  },
  {
    "text": "i have two questions i think uh the first one is have you performed any change in argo after that because we",
    "start": "2674160",
    "end": "2681920"
  },
  {
    "text": "had a similar situation like one month and a half ago but it was only on the",
    "start": "2681920",
    "end": "2688000"
  },
  {
    "text": "environment and it was with the first service that we started our developers to deploy with",
    "start": "2688000",
    "end": "2693119"
  },
  {
    "text": "argo so during the first week of deploying argo they changed",
    "start": "2693119",
    "end": "2698160"
  },
  {
    "text": "the name of the namespace with same situation more or less",
    "start": "2698160",
    "end": "2704800"
  },
  {
    "text": "and and they destroy all the dev environment itself nice so we",
    "start": "2704800",
    "end": "2711520"
  },
  {
    "text": "did disable um auto prune um",
    "start": "2711520",
    "end": "2716839"
  },
  {
    "text": "and uh i can't remember the name of the uh cascading autoprint um so it",
    "start": "2716839",
    "end": "2724319"
  },
  {
    "text": "no um that does mean when we do when we make a change that means we have to delete things or clean up objects it",
    "start": "2724319",
    "end": "2731760"
  },
  {
    "text": "does mean we have to at that level we have to go in and manually manually trigger the print",
    "start": "2731760",
    "end": "2739680"
  },
  {
    "text": "question over there",
    "start": "2743839",
    "end": "2747400"
  },
  {
    "text": "yeah my question was how did you recover the stateful sets",
    "start": "2751040",
    "end": "2757280"
  },
  {
    "text": "so we didn't we well we we put the stateful sites back in place but we had that",
    "start": "2757280",
    "end": "2763040"
  },
  {
    "text": "conversation about the only stateful sets we had running are prometheus and thanos",
    "start": "2763040",
    "end": "2768480"
  },
  {
    "text": "and we went well thanos has shipped the longer term metrics test",
    "start": "2768480",
    "end": "2773599"
  },
  {
    "text": "three um and we we had that conversation of going is is metrics we don't",
    "start": "2773599",
    "end": "2780560"
  },
  {
    "text": "care enough to restore it for time where the site has been down we know the site",
    "start": "2780560",
    "end": "2786000"
  },
  {
    "text": "has been down we don't need prometheus metrics to tell us that um so we we just didn't",
    "start": "2786000",
    "end": "2792880"
  },
  {
    "text": "and what did you do with the databases i mean maybe you use a database to run all this",
    "start": "2792880",
    "end": "2799040"
  },
  {
    "text": "stuff at the databases are our rds like as an aws",
    "start": "2799040",
    "end": "2805599"
  },
  {
    "text": "say ws managed services we don't run databases on top of kubernetes so that that data was never",
    "start": "2805599",
    "end": "2811359"
  },
  {
    "text": "touched by the the fact that we'd wiped out all these services",
    "start": "2811359",
    "end": "2817480"
  },
  {
    "text": "hi hey have you had actually an outage from a malicious actions of someone and",
    "start": "2824720",
    "end": "2831599"
  },
  {
    "text": "if so how did you recover from that nope [Laughter]",
    "start": "2831599",
    "end": "2837680"
  },
  {
    "text": "no i mean we've we've war gamed it we've done it um at a high level with our cxos",
    "start": "2837680",
    "end": "2842720"
  },
  {
    "text": "and talked about all the the situations and scenarios put them under a bit of pressure about making decisions and then",
    "start": "2842720",
    "end": "2848559"
  },
  {
    "text": "from a technical perspective we've also talked about um walking through a scenario and then",
    "start": "2848559",
    "end": "2854559"
  },
  {
    "text": "discovering more and more problems than going actually this might be malicious rather than a failure um and and then",
    "start": "2854559",
    "end": "2862559"
  },
  {
    "text": "figuring out what's going on from that and with that there's a really good game day i honestly don't work for aws but",
    "start": "2862559",
    "end": "2868079"
  },
  {
    "text": "it's a really brand new new game day which the teams maliciously deal with one",
    "start": "2868079",
    "end": "2873520"
  },
  {
    "text": "another so that's quite a good one that's quite a fun fun one to consider other failure states but we've never had",
    "start": "2873520",
    "end": "2878720"
  },
  {
    "text": "that right folks it's roasting up here maybe",
    "start": "2878720",
    "end": "2884800"
  },
  {
    "text": "we'll meet you for beers thank you very much for your attention thank you thanks",
    "start": "2884800",
    "end": "2890920"
  }
]