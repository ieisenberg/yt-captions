[
  {
    "text": "this is the sig auto scaling update and we're going to tell you about a lot of exciting things that we've been doing",
    "start": "9920",
    "end": "16960"
  },
  {
    "text": "since the last time we all got together take my mask off okay um",
    "start": "16960",
    "end": "23279"
  },
  {
    "text": "why don't i guess we'll go through some introductions to begin with my name is michael mccune i work for red hat i'm an",
    "start": "23279",
    "end": "28400"
  },
  {
    "text": "engineer we're all engineers who work in the sig auto scaling community this is uh joachim bartosik from google guy",
    "start": "28400",
    "end": "35760"
  },
  {
    "text": "templeton from sky scanner and david morrison from airbnb and so",
    "start": "35760",
    "end": "41440"
  },
  {
    "text": "when when and if we can get the slides going we're going to talk to you about some changes that have been happening on the",
    "start": "41440",
    "end": "46719"
  },
  {
    "text": "horizontal pod auto scaler we've got an update about what's going on with the v2 api that's been released and kind of the",
    "start": "46719",
    "end": "53360"
  },
  {
    "text": "deprecation notices around v2 beta 1 and v2 beta2",
    "start": "53360",
    "end": "59440"
  },
  {
    "text": "david's going to talk a little bit about the grpc editions that have come into the cluster auto scaler recently",
    "start": "59440",
    "end": "66720"
  },
  {
    "text": "and in specific about the grpc expander that's been written and we also have some more grpc kind of",
    "start": "66720",
    "end": "73680"
  },
  {
    "text": "provider coming as well i mean it's already there but you know and then joaquim's going to talk about",
    "start": "73680",
    "end": "78880"
  },
  {
    "text": "the changes that have been happening in the vertical pod auto scaler and guy's going to bring it home and",
    "start": "78880",
    "end": "84320"
  },
  {
    "text": "talk a little bit about the community and how you all can get involved and give us bug reports or pull requests or",
    "start": "84320",
    "end": "91280"
  },
  {
    "text": "tell us our documentation stinks and we should make it better or we need more testing",
    "start": "91280",
    "end": "96880"
  },
  {
    "text": "so i don't know how we doing on the on the tech situation here",
    "start": "96880",
    "end": "103280"
  },
  {
    "text": "i guess okay [Music]",
    "start": "103280",
    "end": "108920"
  },
  {
    "text": "[Applause]",
    "start": "109720",
    "end": "115510"
  },
  {
    "text": "does anybody know any good jokes",
    "start": "126399",
    "end": "129840"
  },
  {
    "text": "[Laughter] we just can't win for losing here you",
    "start": "145070",
    "end": "150560"
  },
  {
    "text": "know",
    "start": "150560",
    "end": "153560"
  },
  {
    "text": "all right well um [Music] so as many of you may or may not know",
    "start": "157040",
    "end": "162959"
  },
  {
    "text": "uh for several releases now the horizontal pod auto scaler community",
    "start": "162959",
    "end": "168480"
  },
  {
    "text": "has been working to release a v2 api and this work has been ongoing i think",
    "start": "168480",
    "end": "174400"
  },
  {
    "text": "since before 1.22 it started off with v2 beta1 migrated to a v2 beta 2 status",
    "start": "174400",
    "end": "182560"
  },
  {
    "text": "and just recently we've merged the pr to make the v2 stable and we've put out the",
    "start": "182560",
    "end": "188480"
  },
  {
    "text": "deprecation notices for the previous versions and so if you're using v2 beta 1 and i",
    "start": "188480",
    "end": "196000"
  },
  {
    "text": "imagine probably most people are not or if you're using v2 beta 2 you should be aware that you know we're coming up on",
    "start": "196000",
    "end": "202440"
  },
  {
    "text": "1.25 which will be the end of life for v2 beta1",
    "start": "202440",
    "end": "208000"
  },
  {
    "text": "v2 beta2 is also in deprecation right now but i think it will exist until 1.26",
    "start": "208000",
    "end": "214080"
  },
  {
    "text": "and then it will be deprecated as well now for the most part if you're using",
    "start": "214080",
    "end": "219280"
  },
  {
    "text": "the horizontal pod auto scaler and you've been using the v2 beta 1 or v2 beta 2 apis",
    "start": "219280",
    "end": "226640"
  },
  {
    "text": "you won't have to change much the serialization format has not changed from v2 beta 1",
    "start": "226640",
    "end": "233360"
  },
  {
    "text": "to v2 there have been some changes to the programmatic api interface so if if",
    "start": "233360",
    "end": "240640"
  },
  {
    "text": "you're writing code that uses the hpa as a library",
    "start": "240640",
    "end": "245840"
  },
  {
    "text": "you'll probably want to look at the release notes on the pr and on the hpa so that you can know there are a couple functions i think the",
    "start": "245840",
    "end": "252000"
  },
  {
    "text": "change name that was all it looked like to me but for the most part it should be a seamless transition",
    "start": "252000",
    "end": "258799"
  },
  {
    "text": "from v2 beta 1 or v2 beta 2 into the v2 api",
    "start": "258799",
    "end": "265199"
  },
  {
    "text": "so let's just see if this is gonna",
    "start": "265440",
    "end": "270400"
  },
  {
    "text": "so um yeah so if you're using horizontal pod auto scaler you shouldn't really have to uh change too much oh thanks",
    "start": "272639",
    "end": "280080"
  },
  {
    "text": "yeah that actually helps let's go back to um yeah so no changes to the serialization format",
    "start": "280080",
    "end": "286720"
  },
  {
    "text": "and you know if for all of you who are taking notes out there if you're really curious about this",
    "start": "289680",
    "end": "295120"
  },
  {
    "text": "you want to go to the kubernetes kubernetes repo and look for pull request 102",
    "start": "295120",
    "end": "302080"
  },
  {
    "text": "534 and that's where the merge happened you can read the release notes there now that's pretty much the end of what i",
    "start": "302080",
    "end": "308240"
  },
  {
    "text": "had to say and unfortunately for you all i only had a couple slides with a little",
    "start": "308240",
    "end": "313280"
  },
  {
    "text": "bit of text on them david and jokim have a lot of slides with more text on them and graphics",
    "start": "313280",
    "end": "320720"
  },
  {
    "text": "and i'm going to hand it over to david soon but you're going to have to kind of imagine what he's talking about",
    "start": "320720",
    "end": "328000"
  },
  {
    "text": "until well maybe",
    "start": "328240",
    "end": "332638"
  },
  {
    "text": "just in time so with that i will hand it over to david to talk",
    "start": "335919",
    "end": "342800"
  },
  {
    "text": "about oh come on don't if we have to present it for you",
    "start": "342800",
    "end": "348880"
  },
  {
    "text": "we'll just keep going okay don't you don't it's right don't touch you know",
    "start": "348880",
    "end": "354880"
  },
  {
    "text": "so i'm going to hand it over david he's going to tell you all about cluster auto scaler and grpc and all that cool stuff so take it away dude",
    "start": "354880",
    "end": "361680"
  },
  {
    "text": "thanks um i was actually uh like you said my slides have a lot of text on them which is not my",
    "start": "361680",
    "end": "368080"
  },
  {
    "text": "normal deal so i was kind of excited to not have to show you walls of text but you know here we are um i'm david",
    "start": "368080",
    "end": "374560"
  },
  {
    "text": "morrison i'm a staff software engineer at airbnb um i work on the compute",
    "start": "374560",
    "end": "379919"
  },
  {
    "text": "infrastructure team um and it's specifically i do a lot with scheduling",
    "start": "379919",
    "end": "385199"
  },
  {
    "text": "and auto scaling and cluster efficiency for all of our kubernetes clusters um so today i want to talk to you about",
    "start": "385199",
    "end": "391919"
  },
  {
    "text": "the custom expander interface that we contributed to cluster autoscaler",
    "start": "391919",
    "end": "397600"
  },
  {
    "text": "a bunch of this work was actually done by one of my colleagues evan sheng who unfortunately wasn't able to be here but",
    "start": "397600",
    "end": "403759"
  },
  {
    "text": "you know joint effort all of us uh say auto scaling folks it's",
    "start": "403759",
    "end": "408800"
  },
  {
    "text": "all good um so yeah let's uh let's crack on um so just as a quick reminder i want to",
    "start": "408800",
    "end": "416800"
  },
  {
    "text": "talk about what is an expander um so this is a code snippet from the cluster autoscaler code and this is the scale up",
    "start": "416800",
    "end": "424479"
  },
  {
    "text": "function so first thing that cluster auto scaler does is it looks for all of the",
    "start": "424479",
    "end": "429680"
  },
  {
    "text": "unscheduleable pods things that can't be can't fit anywhere on the cluster",
    "start": "429680",
    "end": "435440"
  },
  {
    "text": "and then it looks at all of the different node groups that it has available so",
    "start": "435440",
    "end": "440639"
  },
  {
    "text": "a node group if you're using aws for example might be an auto scaling group",
    "start": "440639",
    "end": "446000"
  },
  {
    "text": "just as a reminder cluster auto scaler one of the requirements is that all of the nodes in a node group have to",
    "start": "446000",
    "end": "453440"
  },
  {
    "text": "be identical from an auto scaling perspective so they all have to have the same amount of cpu same resources uh if",
    "start": "453440",
    "end": "459440"
  },
  {
    "text": "you're using things like pod topology spread they often be in the same az etc",
    "start": "459440",
    "end": "465800"
  },
  {
    "text": "then what happens is it takes all of the node groups that can accommodate the",
    "start": "465840",
    "end": "472400"
  },
  {
    "text": "unscheduleable pods and it passes it off to the expander that's this line that's highlighted in",
    "start": "472400",
    "end": "478639"
  },
  {
    "text": "blue right here it calls this best option function the expander does its thing it computes",
    "start": "478639",
    "end": "485280"
  },
  {
    "text": "one or more node groups that it wants to scale up and then hands that back to cluster autoscaler to actually do the",
    "start": "485280",
    "end": "492240"
  },
  {
    "text": "hard work of adding new nodes so we're going to be focusing today on the best option function here",
    "start": "492240",
    "end": "500479"
  },
  {
    "text": "so again just as a reminder what are the types of expanders that cluster autoscaler has available the default is",
    "start": "500479",
    "end": "507120"
  },
  {
    "text": "random does what it says on the tin picks a node group at random scales it up",
    "start": "507120",
    "end": "512159"
  },
  {
    "text": "most pods in least waste are kind of complementary to each other most pods tries to pick a node group that uh will",
    "start": "512159",
    "end": "519919"
  },
  {
    "text": "schedule the most unschedulable pods um least waste kind of does the converse",
    "start": "519919",
    "end": "526000"
  },
  {
    "text": "it picks a node group that will once you scale it up we'll have the sort of least",
    "start": "526000",
    "end": "531440"
  },
  {
    "text": "unused resources cpu memory etc if you're using gk you can pick the",
    "start": "531440",
    "end": "537120"
  },
  {
    "text": "cheapest node group which is great if you're on gk and then the one that we've been using at airbnb up until now is the priority",
    "start": "537120",
    "end": "544160"
  },
  {
    "text": "expander i think this is the one that a lot of people use in practice",
    "start": "544160",
    "end": "549200"
  },
  {
    "text": "you can specify a prioritized list of your node groups and the expander will pick the node group that is the highest",
    "start": "549200",
    "end": "556720"
  },
  {
    "text": "priority that can accommodate your unscheduleable pods um so uh the one we're going to talk",
    "start": "556720",
    "end": "563760"
  },
  {
    "text": "about today the one that's new is the custom grpc expander um so let's talk a",
    "start": "563760",
    "end": "568800"
  },
  {
    "text": "little bit about why we wanted to write a new one of these things um",
    "start": "568800",
    "end": "574240"
  },
  {
    "text": "we were looking at what is available previously in clusterautoscaler and none of them quite did what we wanted to um",
    "start": "574240",
    "end": "582640"
  },
  {
    "text": "we wanted to be able to sort of dynamically change things on the fly and you like you can kind of do that with",
    "start": "582640",
    "end": "587839"
  },
  {
    "text": "the priority one you can update the priority config map and",
    "start": "587839",
    "end": "593600"
  },
  {
    "text": "it'll take that into account um but we really wanted to have more complicated logic here",
    "start": "593600",
    "end": "600399"
  },
  {
    "text": "so then we talked a little bit about well what if we just built a new expander and contributed upstream um we",
    "start": "600399",
    "end": "606800"
  },
  {
    "text": "really don't want to be running a fork of cluster autoscaler we'd like to stay with what's sort of available to",
    "start": "606800",
    "end": "613440"
  },
  {
    "text": "everyone else but we couldn't really come up with something that was both going to solve",
    "start": "613440",
    "end": "618880"
  },
  {
    "text": "our business needs and would also be appropriate to upstream and so we finally settled on this",
    "start": "618880",
    "end": "624240"
  },
  {
    "text": "grpc expander so this has a couple of benefits uh the first is that it allows us to",
    "start": "624240",
    "end": "630800"
  },
  {
    "text": "encode our business specific scaling logic so it allows us to do things like",
    "start": "630800",
    "end": "636000"
  },
  {
    "text": "take into account our aws contract which might change as we renegotiate things we're looking",
    "start": "636000",
    "end": "642480"
  },
  {
    "text": "at trying to run more spot instances and spot you know",
    "start": "642480",
    "end": "648079"
  },
  {
    "text": "depends a lot on the time of day it depends on the price depends on what else is running in our cluster and so",
    "start": "648079",
    "end": "654800"
  },
  {
    "text": "it's kind of hard to figure out how to generalize that and then the other thing that we were",
    "start": "654800",
    "end": "661200"
  },
  {
    "text": "concerned about is uh cluster auto scaler releases roughly in lockstep with",
    "start": "661200",
    "end": "666959"
  },
  {
    "text": "uh mainline kubernetes and we needed some more flexibility so if",
    "start": "666959",
    "end": "673200"
  },
  {
    "text": "you know our traffic patterns change or if our contract changes or like anything changes we wanted to have the",
    "start": "673200",
    "end": "680399"
  },
  {
    "text": "ability to up update our uh scaling logic and so what we settled on is let's",
    "start": "680399",
    "end": "686800"
  },
  {
    "text": "build a interface that allows us to sort of",
    "start": "686800",
    "end": "692000"
  },
  {
    "text": "encapsulate all of our business specific logic as a separate service and it'll just talk to cluster auto scaler over uh",
    "start": "692000",
    "end": "698560"
  },
  {
    "text": "grpc um so that's what we did uh this is the one and only diagram in",
    "start": "698560",
    "end": "705519"
  },
  {
    "text": "my section of the talk so i hope you like it uh on the left you can see you've got a",
    "start": "705519",
    "end": "711440"
  },
  {
    "text": "node in your cluster cluster autoscalers running on it somewhere and what we built is inside cluster",
    "start": "711440",
    "end": "718240"
  },
  {
    "text": "autoscaler there's a grpc client that conforms to the expander interface so",
    "start": "718240",
    "end": "725279"
  },
  {
    "text": "it's got that best options call and what best options does is it",
    "start": "725279",
    "end": "730480"
  },
  {
    "text": "translates the expander parameters into a protobuf uh passes that over the network to",
    "start": "730480",
    "end": "737440"
  },
  {
    "text": "uh some other service that's sitting somewhere else in your cluster that acts as a grpc server",
    "start": "737440",
    "end": "745040"
  },
  {
    "text": "that service will then take all of the options that cluster auto scaler did or",
    "start": "745040",
    "end": "752240"
  },
  {
    "text": "provided to it and it will you know do whatever business logic you want and then it returns the choice back",
    "start": "752240",
    "end": "759040"
  },
  {
    "text": "to cluster auto scaler where the grpc client will then translate it back into",
    "start": "759040",
    "end": "764880"
  },
  {
    "text": "cluster auto scaler lingo and it goes on from there",
    "start": "764880",
    "end": "770399"
  },
  {
    "text": "let's take a look at just briefly what the interface looks like here",
    "start": "770480",
    "end": "775519"
  },
  {
    "text": "um so this is the protobuf interface um",
    "start": "775519",
    "end": "781200"
  },
  {
    "text": "it's pretty straightforward so the function it has one rpc",
    "start": "781200",
    "end": "788079"
  },
  {
    "text": "it's best options it takes a best options request returns a best options response the request has all of the node group",
    "start": "788079",
    "end": "795920"
  },
  {
    "text": "options that are available and then it takes this map this node info map which",
    "start": "795920",
    "end": "802079"
  },
  {
    "text": "describes what the nodes look like in each one of those options and then the response just returns one",
    "start": "802079",
    "end": "808480"
  },
  {
    "text": "or more node groups that it would like to scale up",
    "start": "808480",
    "end": "813880"
  },
  {
    "text": "so here's some really simple expander code um the goal here is kind of like i want to show you what it looks like to",
    "start": "815360",
    "end": "821120"
  },
  {
    "text": "write one of these things it's not actually that hard so you use your standard boilerplate to",
    "start": "821120",
    "end": "826959"
  },
  {
    "text": "set up your grpc server you create a new expander interface you",
    "start": "826959",
    "end": "832079"
  },
  {
    "text": "register it you start listing for requests from cluster autoscaler",
    "start": "832079",
    "end": "837519"
  },
  {
    "text": "and then here is an example of what the best options function",
    "start": "837519",
    "end": "842639"
  },
  {
    "text": "inside your custom expander might look like this is a really dumb",
    "start": "842639",
    "end": "847839"
  },
  {
    "text": "best options function i don't really recommend that you actually use this this just picks the node group that has",
    "start": "847839",
    "end": "854320"
  },
  {
    "text": "the longest name so you know i guess that might be useful for somebody",
    "start": "854320",
    "end": "859920"
  },
  {
    "text": "um but you know it's pretty straightforward um takes in the response or it takes in the",
    "start": "859920",
    "end": "865279"
  },
  {
    "text": "request returns a response and you're done",
    "start": "865279",
    "end": "870680"
  },
  {
    "text": "the only other sort of tricky thing um it's not actually that tricky is configuring cluster auto scaler to talk",
    "start": "871120",
    "end": "878160"
  },
  {
    "text": "to your expander service um this is pretty straightforward you just have to pass in three command",
    "start": "878160",
    "end": "884800"
  },
  {
    "text": "line arguments to your cluster auto scale or invocation so the first one dash dash expander",
    "start": "884800",
    "end": "891760"
  },
  {
    "text": "we actually another feature that landed in ca fairly recently is you can specify",
    "start": "891760",
    "end": "896880"
  },
  {
    "text": "multiple expanders we recommend doing this when you're doing this custom expander so",
    "start": "896880",
    "end": "903360"
  },
  {
    "text": "here we're saying grpc is your first expander and then if something goes wrong if there's a",
    "start": "903360",
    "end": "909040"
  },
  {
    "text": "network partition you know your expander service crashes for who knows whatever reason then it'll fall back onto the",
    "start": "909040",
    "end": "914800"
  },
  {
    "text": "priority expander so definitely worth having a fallback there um and then the other two are just",
    "start": "914800",
    "end": "921519"
  },
  {
    "text": "telling cluster autoscaler how to talk to the expander so",
    "start": "921519",
    "end": "926560"
  },
  {
    "text": "grpc expander url says hey this is where you need to",
    "start": "926560",
    "end": "931680"
  },
  {
    "text": "send your requests and then the second one grpc expander cert says",
    "start": "931680",
    "end": "936720"
  },
  {
    "text": "these are the tls certificates that you need to use to encrypt that communication",
    "start": "936720",
    "end": "942800"
  },
  {
    "text": "so it's pretty straightforward if you've done anything else like there's the grpc provider",
    "start": "942800",
    "end": "949120"
  },
  {
    "text": "has a similar sort of pattern if you've done anything with like admission controllers and kubernetes",
    "start": "949120",
    "end": "954320"
  },
  {
    "text": "it's all the same sort of pattern um so nothing too exciting or complicated here",
    "start": "954320",
    "end": "963360"
  },
  {
    "text": "so i'm just going to finish up with a bunch of links you can download this from the sked site",
    "start": "964240",
    "end": "971759"
  },
  {
    "text": "first is a link to our design proposal the second is a link to the actual pull request that got upstreamed",
    "start": "971759",
    "end": "979920"
  },
  {
    "text": "inside there there's a readme that's just a sort of text version of what i just said",
    "start": "979920",
    "end": "986399"
  },
  {
    "text": "there's also some example code in there so if this is something that's interested that's a good jumping off",
    "start": "986399",
    "end": "991600"
  },
  {
    "text": "point so sorry if this is something you're interested in that example code is a good jumping off point for you",
    "start": "991600",
    "end": "998160"
  },
  {
    "text": "um i don't think this has gone live yet um but we have written a blog post about",
    "start": "998160",
    "end": "1005920"
  },
  {
    "text": "the expander work that we've done as well as a few of the other contributions we've made to cluster",
    "start": "1005920",
    "end": "1012480"
  },
  {
    "text": "autoscaler i think that's going to go live on our blog either today or tomorrow i'm not sure exactly when",
    "start": "1012480",
    "end": "1019839"
  },
  {
    "text": "i will update the slides once that blog post is live for right now that's just a",
    "start": "1019839",
    "end": "1024959"
  },
  {
    "text": "generic link to our engineering blog but hopefully you all can read that and get some",
    "start": "1024959",
    "end": "1031839"
  },
  {
    "text": "interesting information out of there as well so that's all i have to say about expanders i'm going to hand it off now",
    "start": "1031839",
    "end": "1038319"
  },
  {
    "text": "to joaquim to talk about vpa thanks so i'm going to give you updates",
    "start": "1038319",
    "end": "1044640"
  },
  {
    "text": "about vpa i have a few things to talk about so first i'll be talking about what vpa",
    "start": "1044640",
    "end": "1052559"
  },
  {
    "text": "does then i'll talk about enhancements which we introduced to vpa and finally",
    "start": "1052559",
    "end": "1057840"
  },
  {
    "text": "i'll talk a little bit about our releases so what does vpa do we have we do three",
    "start": "1057840",
    "end": "1066080"
  },
  {
    "text": "things and corresponding to that we have three components and we have three modes of operation",
    "start": "1066080",
    "end": "1072160"
  },
  {
    "text": "so first thing we always do when you create vpa object is to generate recommendations and record those",
    "start": "1072160",
    "end": "1078320"
  },
  {
    "text": "recommendations in the dvp object and a component responsible for that is",
    "start": "1078320",
    "end": "1084160"
  },
  {
    "text": "recommender second thing we do is that you might want to apply those recommendations for",
    "start": "1084160",
    "end": "1091280"
  },
  {
    "text": "that we have admission controller which applies recommendations to pods when they start",
    "start": "1091280",
    "end": "1097760"
  },
  {
    "text": "and it operates only in auto and initial modes so we can turn this off",
    "start": "1097760",
    "end": "1103200"
  },
  {
    "text": "and finally we have updater component which evicts your posts so that when",
    "start": "1103200",
    "end": "1108480"
  },
  {
    "text": "they are recreated by controller admission controller can apply recommendations if you want so if and",
    "start": "1108480",
    "end": "1115120"
  },
  {
    "text": "you can turn that on by choosing auto or recreate mode",
    "start": "1115120",
    "end": "1120960"
  },
  {
    "text": "if you want your recommendations applied automatically",
    "start": "1120960",
    "end": "1127360"
  },
  {
    "text": "okay now on to the first improvement we made",
    "start": "1127360",
    "end": "1132640"
  },
  {
    "text": "alternative recommender support so why did we introduce this enhancement",
    "start": "1132640",
    "end": "1139679"
  },
  {
    "text": "because it's very hard to write algorithm that will generate very good recommendations for",
    "start": "1139679",
    "end": "1145600"
  },
  {
    "text": "all possible workloads for example some many workloads have a weekly usage pattern and for those",
    "start": "1145600",
    "end": "1153039"
  },
  {
    "text": "workloads um uh eight with that default eight day window of data which we're looking at",
    "start": "1153039",
    "end": "1159280"
  },
  {
    "text": "will be enough but different workloads might have a longer for example monthly",
    "start": "1159280",
    "end": "1165919"
  },
  {
    "text": "and for that eight days so a date window will be too short but on the other hand we don't want to",
    "start": "1165919",
    "end": "1172080"
  },
  {
    "text": "extend the window to be very long because then our reaction would be slow",
    "start": "1172080",
    "end": "1177280"
  },
  {
    "text": "so it would would be good to choose different windows for different workloads similarly",
    "start": "1177280",
    "end": "1183360"
  },
  {
    "text": "workloads which answer user queries might want to be able to react very quickly even if there",
    "start": "1183360",
    "end": "1189200"
  },
  {
    "text": "is a load spike on the other hand some workloads don't need to react",
    "start": "1189200",
    "end": "1194720"
  },
  {
    "text": "very quickly to spikes and can spread out their work and conserve resources",
    "start": "1194720",
    "end": "1199919"
  },
  {
    "text": "so to allow that we allow you to choose from multiple recommenders now",
    "start": "1199919",
    "end": "1207360"
  },
  {
    "text": "so you start multiple recommenders and then you can set choose which one to use in your",
    "start": "1207360",
    "end": "1213280"
  },
  {
    "text": "vpa object like that by simply specifying name of the recommender which you want to use in for this vpa object",
    "start": "1213280",
    "end": "1220240"
  },
  {
    "text": "so like you can see here multiple recommenders would read the object but only one would actually write the",
    "start": "1220240",
    "end": "1225919"
  },
  {
    "text": "recommendation but to use multiple choose from multiple recommenders",
    "start": "1225919",
    "end": "1232159"
  },
  {
    "text": "you need to actually run multiple recommenders in your cluster and to do that right now you have to",
    "start": "1232159",
    "end": "1238640"
  },
  {
    "text": "implement your own recommender i hope that this will change but for now we have to write some code",
    "start": "1238640",
    "end": "1246159"
  },
  {
    "text": "and when you do it's very important to remember that only one recommender should recognize recognize any name",
    "start": "1246159",
    "end": "1254080"
  },
  {
    "text": "because if two recommenders try to write to the same vpa object then the recommendation will change very",
    "start": "1254080",
    "end": "1260720"
  },
  {
    "text": "quickly and it might not work very well",
    "start": "1260720",
    "end": "1265760"
  },
  {
    "text": "on the other hand one recommender might might recognize multiple names so for example",
    "start": "1265760",
    "end": "1271200"
  },
  {
    "text": "the default recommender writes recommendation either if there is no recommender name specified because",
    "start": "1271200",
    "end": "1277120"
  },
  {
    "text": "that was the behavior before or if you specified that the default recommender should write",
    "start": "1277120",
    "end": "1283280"
  },
  {
    "text": "the recommendation explicitly okay the other enhancement we introduced",
    "start": "1283280",
    "end": "1290240"
  },
  {
    "text": "is per vp object min replicas so by default vpa will evict your pods",
    "start": "1290240",
    "end": "1297840"
  },
  {
    "text": "only if there are there are at least two running pods in our contour controller we do that",
    "start": "1297840",
    "end": "1303360"
  },
  {
    "text": "because if there is only one pod running then it's pretty likely we will disrupt your workload if we evict them only pod",
    "start": "1303360",
    "end": "1310960"
  },
  {
    "text": "but it's not a good behavior for everyone because if you have only one running pod in your controller by design",
    "start": "1310960",
    "end": "1317600"
  },
  {
    "text": "then we will never apply recommendations and this might not be what you want before there was a",
    "start": "1317600",
    "end": "1324559"
  },
  {
    "text": "option you could set to change this behavior for all vpa objects in your cluster but again",
    "start": "1324559",
    "end": "1331679"
  },
  {
    "text": "this might not be what you want because you might be running different workloads for some workloads you might want to",
    "start": "1331679",
    "end": "1338080"
  },
  {
    "text": "wait for multiple pods to appear so that at least some will be working",
    "start": "1338080",
    "end": "1344000"
  },
  {
    "text": "for other workloads you might want to do every the only part to apply recommendations um",
    "start": "1344000",
    "end": "1349760"
  },
  {
    "text": "and uh the other problem was that uh sometimes you're not controlling your cluster somebody else's and then you",
    "start": "1349760",
    "end": "1355919"
  },
  {
    "text": "couldn't set the option um so this is changed to uh updater",
    "start": "1355919",
    "end": "1361600"
  },
  {
    "text": "component from before uh and again this is uh using this is",
    "start": "1361600",
    "end": "1366880"
  },
  {
    "text": "pretty simple this time you don't even have to write any code you just specify how many pods you want running",
    "start": "1366880",
    "end": "1373200"
  },
  {
    "text": "in your controller before we start evicting them and it will work",
    "start": "1373200",
    "end": "1380559"
  },
  {
    "text": "finally we used to do releases ad hoc and this resulted in some pretty long",
    "start": "1381120",
    "end": "1390400"
  },
  {
    "text": "time durations between our releases going forward we would like to do releases every time there is a",
    "start": "1391919",
    "end": "1398640"
  },
  {
    "text": "kubernetes release so three times a year yeah and if you would like to learn more",
    "start": "1398640",
    "end": "1405360"
  },
  {
    "text": "i linked to caps about the enhancements i talked about also",
    "start": "1405360",
    "end": "1410799"
  },
  {
    "text": "if you want to help we have some ideas we i would like to make it possible to configure",
    "start": "1410799",
    "end": "1417200"
  },
  {
    "text": "pass parameters to recommenders when you specify them it would be nice to",
    "start": "1417200",
    "end": "1424400"
  },
  {
    "text": "make it possible to expose multiple recommendations in one vpn object so that you can preview",
    "start": "1424400",
    "end": "1430080"
  },
  {
    "text": "whatever and different recommender recommends before you actually make the switch",
    "start": "1430080",
    "end": "1435360"
  },
  {
    "text": "and it will be good to make the default recommender more configurable so that you can use it as a",
    "start": "1435360",
    "end": "1442240"
  },
  {
    "text": "alternate multiple recommenders writing multiple instances is without",
    "start": "1442240",
    "end": "1447760"
  },
  {
    "text": "writing your own code and with that i'm hanging over",
    "start": "1447760",
    "end": "1453200"
  },
  {
    "text": "thank you um so you've heard a lot about the the new features new extensibility that we've",
    "start": "1456559",
    "end": "1463600"
  },
  {
    "text": "shipped over the last uh year and a bit um however this is where we need your help",
    "start": "1463600",
    "end": "1469840"
  },
  {
    "text": "um you've heard obviously ideas for future improvements to the vpa cluster",
    "start": "1469840",
    "end": "1475279"
  },
  {
    "text": "auto scalers improving its extensibility you know people can write their own um grpc expanders now um so we need your",
    "start": "1475279",
    "end": "1483039"
  },
  {
    "text": "help um we we own a lot of projects like you've heard about the hpa you've heard",
    "start": "1483039",
    "end": "1489120"
  },
  {
    "text": "about the cluster autoscaler vpa um we we only have a small number of maintainers we can't do all that we want",
    "start": "1489120",
    "end": "1495760"
  },
  {
    "text": "to do um just just with the number of engineers we currently have and contributors um so we want your feature",
    "start": "1495760",
    "end": "1502480"
  },
  {
    "text": "requests implementation uh bug bug reports what's not working for you",
    "start": "1502480",
    "end": "1508000"
  },
  {
    "text": "what you would like to see work better and and we also need your help with bug triage and response we don't have enough",
    "start": "1508000",
    "end": "1515120"
  },
  {
    "text": "contributors at the moment to triage all the bugs that we get reported um we'd",
    "start": "1515120",
    "end": "1520559"
  },
  {
    "text": "really love to improve that as well as infrastructure improvements to improve the the testing around all of our",
    "start": "1520559",
    "end": "1526320"
  },
  {
    "text": "components to make it easier for us end users to contribute um and we we definitely want to expand",
    "start": "1526320",
    "end": "1532960"
  },
  {
    "text": "our maintainers um as well at the moment we know that there's there's diff for instance in the",
    "start": "1532960",
    "end": "1539039"
  },
  {
    "text": "cluster autoscaler despite the extensibility now with grpc expanders and grpc um potentially cloud providers",
    "start": "1539039",
    "end": "1546960"
  },
  {
    "text": "um at the moment most of the big cloud providers are baked into cluster autoscaler so we need",
    "start": "1546960",
    "end": "1553120"
  },
  {
    "text": "maintainers for those cloud provider implementations a lot of the big cloud providers do do support",
    "start": "1553120",
    "end": "1559360"
  },
  {
    "text": "that but we need we need more to improve their responsiveness and prove how often",
    "start": "1559360",
    "end": "1564720"
  },
  {
    "text": "we're uh releasing cluster auto scholars as well and and as you already heard we want to improve the extendability so",
    "start": "1564720",
    "end": "1571279"
  },
  {
    "text": "that you're not dependent on us cutting releases but you can if you want to bake in business logic whether it be in a an",
    "start": "1571279",
    "end": "1578159"
  },
  {
    "text": "expander or a cloud provider implementation that you can do that out of band as well",
    "start": "1578159",
    "end": "1583520"
  },
  {
    "text": "um we've got links here to our community charter if you want to get involved and we've also got a mailing list",
    "start": "1583520",
    "end": "1589760"
  },
  {
    "text": "that's not massively active however we also have uh that also enables you to edit",
    "start": "1589760",
    "end": "1595760"
  },
  {
    "text": "our officers put things on the agenda and and we're also fairly active on the",
    "start": "1595760",
    "end": "1601360"
  },
  {
    "text": "segout scaling channel uh there's also a sega auto scaling api channel for discussion about uh things like the hpa",
    "start": "1601360",
    "end": "1608240"
  },
  {
    "text": "v2 api and potential few other future improvements to the api side of things",
    "start": "1608240",
    "end": "1614480"
  },
  {
    "text": "uh and we'd love to have you involved uh any questions",
    "start": "1614480",
    "end": "1619810"
  },
  {
    "text": "[Music] stunned silence",
    "start": "1619810",
    "end": "1626440"
  },
  {
    "text": "so yeah my question was uh you can use like vp and hpa but you have limitations due to the vpa so i was just",
    "start": "1641200",
    "end": "1648320"
  },
  {
    "text": "wondering if there's like some roadmap on that some some updates",
    "start": "1648320",
    "end": "1655480"
  },
  {
    "text": "so there is some work on making vpa and hbo",
    "start": "1657120",
    "end": "1662640"
  },
  {
    "text": "work together but it's very early as so i wouldn't wait hold breath for it",
    "start": "1662640",
    "end": "1671120"
  },
  {
    "text": "any other questions",
    "start": "1672080",
    "end": "1674880"
  },
  {
    "text": "thank you for a great presentation the cluster art scaler now has the",
    "start": "1677919",
    "end": "1683600"
  },
  {
    "text": "custom expanders it's that pattern you're looking into for the vpa as well",
    "start": "1683600",
    "end": "1691120"
  },
  {
    "text": "so i haven't thought about it yet if you have ideas you know feel free to come to seek and",
    "start": "1695120",
    "end": "1700640"
  },
  {
    "text": "talk about this who else",
    "start": "1700640",
    "end": "1707240"
  },
  {
    "text": "the main replica feature in the vba the new one is for disruption purposes",
    "start": "1712720",
    "end": "1718960"
  },
  {
    "text": "right and if it is can't we just use the bot",
    "start": "1718960",
    "end": "1725600"
  },
  {
    "text": "disruption budget for that value so",
    "start": "1725600",
    "end": "1729840"
  },
  {
    "text": "uh okay so vpa also respects spot disruption budget",
    "start": "1730960",
    "end": "1736960"
  },
  {
    "text": "but there are extra measures in vpa",
    "start": "1736960",
    "end": "1742159"
  },
  {
    "text": "so if there is only one pod running in your controller vpa wouldn't",
    "start": "1742159",
    "end": "1748080"
  },
  {
    "text": "evict this only pod even if power disruption budget would allow it okay does this answer your question",
    "start": "1748080",
    "end": "1755519"
  },
  {
    "text": "okay i mean if i have the butt disruption budget like a three",
    "start": "1762640",
    "end": "1767679"
  },
  {
    "text": "and uh so the only way where this at the benefit if it is only one",
    "start": "1767679",
    "end": "1775360"
  },
  {
    "text": "can we get a little community effort here uh yes so if you have pod disruption",
    "start": "1779360",
    "end": "1784640"
  },
  {
    "text": "budget and you want at the pod disruption budget says you want at least three pots running",
    "start": "1784640",
    "end": "1790720"
  },
  {
    "text": "then this lock will not change anything for you like this is extra check we do in vpa",
    "start": "1790720",
    "end": "1798720"
  },
  {
    "text": "uh in addition to re following whatever restriction spot disruption budget uh sets right so if so",
    "start": "1798720",
    "end": "1806240"
  },
  {
    "text": "we have both you but you could have pop disruption but",
    "start": "1806240",
    "end": "1811440"
  },
  {
    "text": "that allows to evict all the pods but even then before this vpa wouldn't",
    "start": "1811440",
    "end": "1816799"
  },
  {
    "text": "do that because we had an extra check inside vpa and we would say hey there is",
    "start": "1816799",
    "end": "1822880"
  },
  {
    "text": "only one pod and we wouldn't even try to evict okay",
    "start": "1822880",
    "end": "1828799"
  },
  {
    "text": "so we have a couple online questions as well and the most voted one right now says is",
    "start": "1830320",
    "end": "1836159"
  },
  {
    "text": "there a predictive auto scaling algorithm and in my opinion predictive auto scaling is is a very complex topic",
    "start": "1836159",
    "end": "1843039"
  },
  {
    "text": "and i don't we do not have anything like that currently but i don't does anyone else want to talk to predictive auto scaling",
    "start": "1843039",
    "end": "1849440"
  },
  {
    "text": "it involves a lot of machine learning and a lot of mistakes and you know those kind of things um so no we we do not",
    "start": "1849440",
    "end": "1856640"
  },
  {
    "text": "have a predictive auto scaler included in any of this stuff right now i've seen many attempts over the years for people",
    "start": "1856640",
    "end": "1862320"
  },
  {
    "text": "to try to use uh you know data science to analyze the usage patterns in their",
    "start": "1862320",
    "end": "1867519"
  },
  {
    "text": "clusters to create models that they could then predict like how they're gonna auto scale in my opinion you'd probably be better",
    "start": "1867519",
    "end": "1873919"
  },
  {
    "text": "off just looking at like the traffic logs for whatever you're doing and it's like well we know that friday at 9 00 pm",
    "start": "1873919",
    "end": "1880000"
  },
  {
    "text": "we get hit so we we already know the prediction friday at 9 00 pm add some more nodes um to do things in a",
    "start": "1880000",
    "end": "1887440"
  },
  {
    "text": "more dynamic way is a little more complicated did you want to speak that sure i've actually",
    "start": "1887440",
    "end": "1893279"
  },
  {
    "text": "tried to write one of those predictive auto scalers in a past life [Music]",
    "start": "1893279",
    "end": "1901059"
  },
  {
    "text": "i guess i'll say there's a reason that i've switched to using cluster autoscaler and just watching the number",
    "start": "1901120",
    "end": "1906640"
  },
  {
    "text": "of unscheduleable pods because it generally seems to work better in my opinion so",
    "start": "1906640",
    "end": "1912399"
  },
  {
    "text": "cool thanks um so there are a couple more questions here one question is where can we find",
    "start": "1912399",
    "end": "1918000"
  },
  {
    "text": "the slides i don't think they're on the sketch page yet but we will they are okay so look on the skedge.org or",
    "start": "1918000",
    "end": "1923919"
  },
  {
    "text": "sketch.com page for this presentation you can download it it's got all the links in there there's one other really",
    "start": "1923919",
    "end": "1930320"
  },
  {
    "text": "cryptic question and maybe this is a good one to end on because it it seems kind of maybe an existential question",
    "start": "1930320",
    "end": "1936960"
  },
  {
    "text": "it just says what about node auto scaling what what about it",
    "start": "1936960",
    "end": "1943440"
  },
  {
    "text": "yes i'm not sure how deep to go there there is a cluster auto scaler and it does",
    "start": "1943440",
    "end": "1950240"
  },
  {
    "text": "node auto scaling um maybe this person was asking about some sort of like vertical node scaling or",
    "start": "1950240",
    "end": "1956960"
  },
  {
    "text": "something like right now the the yeah right exactly something like carpenter where you've got different",
    "start": "1956960",
    "end": "1962960"
  },
  {
    "text": "auto scaling um you know primitives and whatnot where it's selecting machines in a different way than the kubernetes",
    "start": "1962960",
    "end": "1968960"
  },
  {
    "text": "cluster auto scaler does so what about node auto scaling i think as a sig we think node auto scaling is",
    "start": "1968960",
    "end": "1975360"
  },
  {
    "text": "pretty cool we like it we'd like to see more of it we'd like to see the community coming",
    "start": "1975360",
    "end": "1981279"
  },
  {
    "text": "and telling us what they want to see in it whoever made this question if you have",
    "start": "1981279",
    "end": "1986559"
  },
  {
    "text": "suggestions open an issue on the kubernetes auto scale or repo and",
    "start": "1986559",
    "end": "1992240"
  },
  {
    "text": "yeah maybe that's a good place to end it [Applause]",
    "start": "1992240",
    "end": "2003510"
  }
]