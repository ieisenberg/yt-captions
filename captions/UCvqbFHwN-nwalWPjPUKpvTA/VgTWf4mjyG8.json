[
  {
    "text": "hello everyone and welcome to the Sig Ultra scaling update for kubecon cloud nativecon Europe 2023 uh",
    "start": "0",
    "end": "9059"
  },
  {
    "text": "so what does segot what's going on um we now own a number of different",
    "start": "9059",
    "end": "14820"
  },
  {
    "text": "projects uh so the balancer which is new for",
    "start": "14820",
    "end": "20340"
  },
  {
    "text": "um the sort of past year uh Pastor will be giving you an update on that and the",
    "start": "20340",
    "end": "25380"
  },
  {
    "text": "implementation of that um we also in the cluster Auto scaler for",
    "start": "25380",
    "end": "31320"
  },
  {
    "text": "scaling your cluster in terms of nodes um horizontally uh and giant I'll be",
    "start": "31320",
    "end": "37980"
  },
  {
    "text": "giving you an update on one of the newer bits functionality uh for that to Aid cluster operators in debugging what the",
    "start": "37980",
    "end": "45540"
  },
  {
    "text": "cluster of tuscular is is doing and why um we also own horizontal and vertical",
    "start": "45540",
    "end": "52739"
  },
  {
    "text": "part of scaling horizontal product scaling which is implemented as part of the",
    "start": "52739",
    "end": "57780"
  },
  {
    "text": "um kubernetes kubernetes controller manager and but also vertical pod Auto scaling a newer",
    "start": "57780",
    "end": "64378"
  },
  {
    "text": "um Standalone component um and Chen and Kelly will be talking",
    "start": "64379",
    "end": "69479"
  },
  {
    "text": "you through a demo of a proposal for a new uh feature called the multi-dimensional product scaler and how",
    "start": "69479",
    "end": "76260"
  },
  {
    "text": "that will uh allow users to potentially overcome suddenly Uh current challenges",
    "start": "76260",
    "end": "82920"
  },
  {
    "text": "with using horizontal and vertical products or scaling together um and finally we also on the add-on",
    "start": "82920",
    "end": "89700"
  },
  {
    "text": "resizer however we won't really be covering that this time right um so I've mentioned some of the",
    "start": "89700",
    "end": "95700"
  },
  {
    "text": "in-depth demos um and uh sort of walkthroughs you're about to get however we also have a",
    "start": "95700",
    "end": "101520"
  },
  {
    "text": "number of other um updates that are uh might be of interest to you as a cluster operator or a user of a kubernetes offering",
    "start": "101520",
    "end": "109079"
  },
  {
    "text": "um so for the horizontal scalar container resource scaling has been available in Alpha um since 120",
    "start": "109079",
    "end": "117000"
  },
  {
    "text": "um a member of the community has finally picked up um the work to get that promoted to Beta",
    "start": "117000",
    "end": "123180"
  },
  {
    "text": "um in time for kubernetes 127 hopefully um and that will allow uh users uh",
    "start": "123180",
    "end": "130520"
  },
  {
    "text": "potentially have managed Services um provided by Cloud providers who might",
    "start": "130520",
    "end": "136200"
  },
  {
    "text": "block off um Alpha features uh potentially to make use of it and this this has the potential to improve the um",
    "start": "136200",
    "end": "143760"
  },
  {
    "text": "your your ability as a service owner to improve the granularity of your Skilling",
    "start": "143760",
    "end": "149040"
  },
  {
    "text": "behaviors um we also asked um owners of cluster",
    "start": "149040",
    "end": "154140"
  },
  {
    "text": "Auto Skiller have committed to an improved release process going forward and so current users may be aware that",
    "start": "154140",
    "end": "161400"
  },
  {
    "text": "they actually stem to this point has been a bit ad hoc and we are looking to provide a uh uh far more clarity for",
    "start": "161400",
    "end": "170220"
  },
  {
    "text": "users of the cluster autoscalers to win patch releases might come out and therefore when",
    "start": "170220",
    "end": "175500"
  },
  {
    "text": "um Cherry picks of uh bug fixes Etc might make it back to support releases",
    "start": "175500",
    "end": "182280"
  },
  {
    "text": "um and finally on the vertical board doctoral scaling side we're looking to we're beginning to gather the work to enable taking advantage of dynamic pod",
    "start": "182280",
    "end": "189000"
  },
  {
    "text": "resizing uh this has always been intended as a feature for the vertical doctoral scaler however uh we've been",
    "start": "189000",
    "end": "196920"
  },
  {
    "text": "dependent on uh work from other cigs and that's now progressing significantly and",
    "start": "196920",
    "end": "202440"
  },
  {
    "text": "looks like it's going to make it to kubernetes 127 release and so we want to be ready to do uh all of the work that's",
    "start": "202440",
    "end": "209640"
  },
  {
    "text": "required um for the vpa to take advantage of that as soon as possible",
    "start": "209640",
    "end": "215360"
  },
  {
    "text": "um if any of this interests you um we we as I said definitely need help as mentioned we have ownership of a",
    "start": "215640",
    "end": "222599"
  },
  {
    "text": "number of different subprojects and we need uh your help as the community in",
    "start": "222599",
    "end": "228420"
  },
  {
    "text": "all of these areas uh we we would love to get your feedback where issues where",
    "start": "228420",
    "end": "234000"
  },
  {
    "text": "um some of our projects aren't working the way you think they should um or progressing up that contributorial",
    "start": "234000",
    "end": "240360"
  },
  {
    "text": "ladder via issue triage reviewing PRS or even contributing features or bug fixes",
    "start": "240360",
    "end": "246780"
  },
  {
    "text": "that you are and think are important or excited about",
    "start": "246780",
    "end": "251879"
  },
  {
    "text": "um if any of that interests you um please get in touch with us uh seg uh we hang out on um the GitHub repo that",
    "start": "251879",
    "end": "258600"
  },
  {
    "text": "hosts most of our projects we also have weekly segmentings at 1600 CET FS time",
    "start": "258600",
    "end": "263699"
  },
  {
    "text": "zone doesn't work for you please reach out and we'll try to arrange something and however we're also reachable on",
    "start": "263699",
    "end": "269820"
  },
  {
    "text": "slack at Sig Auto scaling um and enjoy the demos",
    "start": "269820",
    "end": "276320"
  },
  {
    "text": "hi I'm going to cover balancer a new resource in the auto scaling space it",
    "start": "277680",
    "end": "283800"
  },
  {
    "text": "allows to control how pods are distributed across similar deployments and auto scale them together",
    "start": "283800",
    "end": "290160"
  },
  {
    "text": "in a second I will explain why it may be useful we've introduced the balancer",
    "start": "290160",
    "end": "295500"
  },
  {
    "text": "users and a controller for it to use it you need to install an optional component from kubernetes slash",
    "start": "295500",
    "end": "302160"
  },
  {
    "text": "autoscaler it's just been published so it's an early Alpha stage",
    "start": "302160",
    "end": "307680"
  },
  {
    "text": "so why would you want to distribute pods in a workload across multiple similar",
    "start": "307680",
    "end": "312900"
  },
  {
    "text": "deployments there are a couple of use cases perhaps you have a regional cluster and",
    "start": "312900",
    "end": "318600"
  },
  {
    "text": "want to ensure that the workload is spread evenly across zones for availability reasons you have multiple",
    "start": "318600",
    "end": "325199"
  },
  {
    "text": "deployments each responsible for a different Zone when pods are added or removed to the",
    "start": "325199",
    "end": "332820"
  },
  {
    "text": "workload you expect this even distribution to be maintained",
    "start": "332820",
    "end": "338220"
  },
  {
    "text": "or perhaps you want to run your workload on both standard VMS and spot or",
    "start": "338220",
    "end": "343500"
  },
  {
    "text": "preemptable VMS which are cheaper but less reliable you would like to always",
    "start": "343500",
    "end": "348539"
  },
  {
    "text": "have 25 percent of PODS running on this less reliable VMS",
    "start": "348539",
    "end": "353880"
  },
  {
    "text": "again you expect new pods to be distributed according to the defined ratio",
    "start": "353880",
    "end": "361039"
  },
  {
    "text": "or perhaps you run your workload on different machine families or there's any other slight difference in",
    "start": "361380",
    "end": "367560"
  },
  {
    "text": "configuration now that we know when the balancer feature may be useful let's explore how",
    "start": "367560",
    "end": "374580"
  },
  {
    "text": "to configure it let's say I want to balance my app between two deployments in a three to",
    "start": "374580",
    "end": "381180"
  },
  {
    "text": "one ratio the use case we described earlier for the 25 percent of pods on",
    "start": "381180",
    "end": "386699"
  },
  {
    "text": "spot VMS you can see the balancer level to the right API version",
    "start": "386699",
    "end": "393620"
  },
  {
    "text": "balancer.xkatesio slash V1 alpha 1 kind balancer this balancer defines two Targets which",
    "start": "393620",
    "end": "401520"
  },
  {
    "text": "are deployments called mayab a and my app B we can Define Min and Max replicas for",
    "start": "401520",
    "end": "409440"
  },
  {
    "text": "each Target and lastly we Define the policy in this",
    "start": "409440",
    "end": "414900"
  },
  {
    "text": "case this will be a proportional policy with ratios 3 and 1. with this config if the balancerate",
    "start": "414900",
    "end": "422100"
  },
  {
    "text": "controller detects that the number of replicas in deployments is no longer in proportions of three to one it will",
    "start": "422100",
    "end": "429180"
  },
  {
    "text": "redistribute pods to bring back the desired ratio also when scale on the balancer object",
    "start": "429180",
    "end": "436259"
  },
  {
    "text": "changes so pods are to be added or removed balancer controller will add or",
    "start": "436259",
    "end": "441840"
  },
  {
    "text": "remove them from the right deployments in order to maintain the three to one ratio",
    "start": "441840",
    "end": "448080"
  },
  {
    "text": "this combines well with horizontal Port Auto scaling so we can define an HPA",
    "start": "448080",
    "end": "453300"
  },
  {
    "text": "that has the balancer object as its scale Target this HPA covers the whole workload both",
    "start": "453300",
    "end": "459900"
  },
  {
    "text": "deployments combined and recommends by updating the scale on the balancer",
    "start": "459900",
    "end": "465180"
  },
  {
    "text": "object the balancer controller then distributes this new scale across its targets",
    "start": "465180",
    "end": "471300"
  },
  {
    "text": "deployment A and B",
    "start": "471300",
    "end": "474978"
  },
  {
    "text": "let's have a look at a different use case I want to run my app generally in deployment a maybe because it's a",
    "start": "476639",
    "end": "483660"
  },
  {
    "text": "particular Zone I prefer",
    "start": "483660",
    "end": "486860"
  },
  {
    "text": "however if there's no space there Max replicas are reached then overflow to",
    "start": "488819",
    "end": "495300"
  },
  {
    "text": "deployment b a different Zone perhaps as soon as there's space in deployment a",
    "start": "495300",
    "end": "503220"
  },
  {
    "text": "rebalance pods back there you can see that now the policy part of",
    "start": "503220",
    "end": "510539"
  },
  {
    "text": "the balancer yaml at the bottom is set to type priority and priority order is",
    "start": "510539",
    "end": "515760"
  },
  {
    "text": "defined as my up a first and my up B second",
    "start": "515760",
    "end": "521539"
  },
  {
    "text": "we can add a new requirement where if ports cannot start in deployment a even",
    "start": "522959",
    "end": "528420"
  },
  {
    "text": "if Max replicas is not reached there we will still start pods in deployment B",
    "start": "528420",
    "end": "534000"
  },
  {
    "text": "perhaps there are pending pods in deployment a because there is no space in the node pool",
    "start": "534000",
    "end": "542120"
  },
  {
    "text": "we can configure this fallback Behavior by adding a fallback section to the policy spec we enable fallback and",
    "start": "544380",
    "end": "551640"
  },
  {
    "text": "Define the startup timeout the time after which a pod is considered blocked it will trigger starting another pod in",
    "start": "551640",
    "end": "558660"
  },
  {
    "text": "deployment B once the original pods finally start the",
    "start": "558660",
    "end": "563820"
  },
  {
    "text": "excessive pods in deployment B are removed in summary the new balancer resource",
    "start": "563820",
    "end": "571680"
  },
  {
    "text": "allows to Define how pods should be distributed across similar deployments you can additionally Define horizontal",
    "start": "571680",
    "end": "578760"
  },
  {
    "text": "Port Auto scaling on the balancer object to Auto scale the pods together you can learn more at the GitHub",
    "start": "578760",
    "end": "585300"
  },
  {
    "text": "repository page kubernetes slash autoscaler and we're waiting for your feedback",
    "start": "585300",
    "end": "590519"
  },
  {
    "text": "thanks hi everyone how are you doing um today I'm going to be presenting",
    "start": "590519",
    "end": "596760"
  },
  {
    "text": "debugging snapshotter which is a tool that we have in cluster scaler before we",
    "start": "596760",
    "end": "601800"
  },
  {
    "text": "get into what the tool is let me unders let me try to explain what are we trying to solve here uh CA loves a lot of",
    "start": "601800",
    "end": "609120"
  },
  {
    "text": "information about the decisions taken uh for example if you scale up you scale down you don't scale up but we don't log",
    "start": "609120",
    "end": "615120"
  },
  {
    "text": "what data these decisions are based on as CA internally simulates the behavior of the impact cluster and that's just",
    "start": "615120",
    "end": "621480"
  },
  {
    "text": "too much to log uh when something goes wrong we usually need to understand how these decisions were taken and not just",
    "start": "621480",
    "end": "628980"
  },
  {
    "text": "what these decisions were RCA of an issue usually takes time and",
    "start": "628980",
    "end": "634380"
  },
  {
    "text": "the internal state of the cluster changes when we mitigate the issue uh thereby making it more difficult to",
    "start": "634380",
    "end": "640440"
  },
  {
    "text": "debug the issue debugging snapshotter is a tool to visualize the internal state of cluster",
    "start": "640440",
    "end": "646800"
  },
  {
    "text": "scalar at a point in time to help debug Auto scaling issues so before you go",
    "start": "646800",
    "end": "651959"
  },
  {
    "text": "about mitigating the cluster maybe it would be easier if you had a snapshot of",
    "start": "651959",
    "end": "657240"
  },
  {
    "text": "what was the state of cluster scaler at the time when the issue was happening",
    "start": "657240",
    "end": "662940"
  },
  {
    "text": "um so let me quickly take you through some common use cases that are available uh",
    "start": "662940",
    "end": "669120"
  },
  {
    "text": "you know scale up and scale down not working with special focus on skill from zero nodes uh maybe if you have an",
    "start": "669120",
    "end": "675360"
  },
  {
    "text": "attached instance group for example um there's a mismatch between scheduler and cluster Auto scalar decisions where",
    "start": "675360",
    "end": "681600"
  },
  {
    "text": "plus color scaler decides to not scale up but scheduler says it cannot schedule anything there's a mismatched resource",
    "start": "681600",
    "end": "689399"
  },
  {
    "text": "availability I don't know where there might be extra resource on the Node but",
    "start": "689399",
    "end": "695459"
  },
  {
    "text": "plus rscaler assumes there's a different resource available uh so what data is being captured you know",
    "start": "695459",
    "end": "703800"
  },
  {
    "text": "by the snapshotter uh that's available for you um first and foremost the node list uh",
    "start": "703800",
    "end": "711300"
  },
  {
    "text": "that's essentially all of the nodes in the cluster um secondly any unscheduable pods that",
    "start": "711300",
    "end": "718019"
  },
  {
    "text": "cluster Auto scale of things are scheduleable um there's also template nodes which are",
    "start": "718019",
    "end": "724860"
  },
  {
    "text": "simulated nodes or any attached instance Group which with no nodes on the cluster",
    "start": "724860",
    "end": "730820"
  },
  {
    "text": "there is an error field which which is filled in case the snapshot or fails and",
    "start": "730820",
    "end": "735899"
  },
  {
    "text": "there's an error generating the snapshot itself and there's a start and end timestamp uh this could help you with if",
    "start": "735899",
    "end": "742980"
  },
  {
    "text": "you decide to take multiple snapshots and also um how also because a snapshotter might",
    "start": "742980",
    "end": "749940"
  },
  {
    "text": "sometimes take a non-trivial amount of time so this this would encapsulate you",
    "start": "749940",
    "end": "755820"
  },
  {
    "text": "know the complete time frame when this particular snapshot was generated um how does it work",
    "start": "755820",
    "end": "763320"
  },
  {
    "text": "um it basically captures the internal fields that we talked about when we receive a request for a snapshot and",
    "start": "763320",
    "end": "770519"
  },
  {
    "text": "returns a well-formed Json after receiving the request in the",
    "start": "770519",
    "end": "775620"
  },
  {
    "text": "following cluster scale of processing Loop snapshot will snapshot the cluster State uh and limit it to a single Loop",
    "start": "775620",
    "end": "782940"
  },
  {
    "text": "no Crossing data so it will start to capture uh in the following loop at the",
    "start": "782940",
    "end": "788040"
  },
  {
    "text": "start of the loop and it will go through all of the steps that cluster scalar goes through and collect all of the data that is part of the snapshot and and",
    "start": "788040",
    "end": "796500"
  },
  {
    "text": "when that particular process Loop closes the HTTP request blocks for the duration",
    "start": "796500",
    "end": "803160"
  },
  {
    "text": "of the snapshot generation and returns a Json as the HTTP responds to the request",
    "start": "803160",
    "end": "809839"
  },
  {
    "text": "there's no files you make a request you wait for some time and you get the response as HTTP response",
    "start": "809839",
    "end": "818399"
  },
  {
    "text": "um how do you make this request um you SSH onto the server running the",
    "start": "818399",
    "end": "824160"
  },
  {
    "text": "leader cluster scanner in case you're running multiple plus five scalers and you curl to this following link it's a",
    "start": "824160",
    "end": "831420"
  },
  {
    "text": "local Port that uh that cluster scaler has a path available on",
    "start": "831420",
    "end": "838019"
  },
  {
    "text": "um let's quickly go through the demo it's super simple uh we have a cluster",
    "start": "838019",
    "end": "843959"
  },
  {
    "text": "Auto scaler running uh and we have a cluster running where we have three nodes in the cluster we have uh plus",
    "start": "843959",
    "end": "851700"
  },
  {
    "text": "Florida scalar logs here which which has an attached instance which is empty for",
    "start": "851700",
    "end": "857100"
  },
  {
    "text": "now and let's um let's apply a pot or a deployment with",
    "start": "857100",
    "end": "863639"
  },
  {
    "text": "some resource requests which cannot be accommodated on any of the existing modes and we should see a scale up at",
    "start": "863639",
    "end": "870660"
  },
  {
    "text": "that point so let me go ahead and apply uh the deployment uh that we have done",
    "start": "870660",
    "end": "875880"
  },
  {
    "text": "that um if you see here uh space we have a",
    "start": "875880",
    "end": "881699"
  },
  {
    "text": "pod uh that is unschedulable um and we see a scale up for an",
    "start": "881699",
    "end": "888360"
  },
  {
    "text": "incidence growth this is GC specific but this also simulates two different uh",
    "start": "888360",
    "end": "893459"
  },
  {
    "text": "Cloud providers and as soon as we see the whole operation being done let's go",
    "start": "893459",
    "end": "899519"
  },
  {
    "text": "ahead and make a snapshot request as we see here and if you see in the logs we",
    "start": "899519",
    "end": "905940"
  },
  {
    "text": "also have logs around the snapshotter itself um it's it marks when it has received",
    "start": "905940",
    "end": "912540"
  },
  {
    "text": "the request uh let me just take you through it really quickly and there's a",
    "start": "912540",
    "end": "918420"
  },
  {
    "text": "bunch of other things on you know when the data a collection has started what data has already been connected and you",
    "start": "918420",
    "end": "926579"
  },
  {
    "text": "know snapshotter uh snapshot being flushed back as response so here we have",
    "start": "926579",
    "end": "932820"
  },
  {
    "text": "we have already a we just made a color request when as soon as that happened and we see now there are four nodes but",
    "start": "932820",
    "end": "940860"
  },
  {
    "text": "we made this before the node the new node is registered so we should see the",
    "start": "940860",
    "end": "946800"
  },
  {
    "text": "new pod um as something that can be scheduled uh by and but there's no other uh yeah",
    "start": "946800",
    "end": "955560"
  },
  {
    "text": "so let's go through all of the different items and we're going to use a common JQ",
    "start": "955560",
    "end": "962519"
  },
  {
    "text": "tool that's used to parse jsons and if you see here uh we we have a bunch of",
    "start": "962519",
    "end": "968820"
  },
  {
    "text": "items that are available as keys so let's go ahead and",
    "start": "968820",
    "end": "974579"
  },
  {
    "text": "see how many nodes there are and there are four nodes um we can also see",
    "start": "974579",
    "end": "980100"
  },
  {
    "text": "template nodes um here let's go through that and there will be one templator because we have",
    "start": "980100",
    "end": "986160"
  },
  {
    "text": "one attached instance Group which has a simulated node available and we will also have",
    "start": "986160",
    "end": "993740"
  },
  {
    "text": "um would you say unscheduled pause that can be scheduled and this would be the node",
    "start": "995820",
    "end": "1000980"
  },
  {
    "text": "um that is available uh this is the part uh that could be scheduled on the",
    "start": "1000980",
    "end": "1006440"
  },
  {
    "text": "upcoming note but isn't so it's it's in that middle State and this could also",
    "start": "1006440",
    "end": "1012079"
  },
  {
    "text": "represent a a bad part uh or unscheduled part which uh you might want to debug so",
    "start": "1012079",
    "end": "1018019"
  },
  {
    "text": "if you see here nginx deployment one um yeah there is a bunch of information around it and this is how you can easily",
    "start": "1018019",
    "end": "1025579"
  },
  {
    "text": "navigate your Json um",
    "start": "1025579",
    "end": "1031220"
  },
  {
    "text": "um yeah that's it so let me give you a quick rundown uh a quick start on how",
    "start": "1031220",
    "end": "1037040"
  },
  {
    "text": "you can get yourself up and running with debugging stop shutter you just need to enable the snapshot or on your cluster",
    "start": "1037040",
    "end": "1042620"
  },
  {
    "text": "scale within the Manifest and by adding the following flag no manifest uh this is already available in production uh on",
    "start": "1042620",
    "end": "1050240"
  },
  {
    "text": "all cluster scale versions 124 and above so I I'm hoping even for all the",
    "start": "1050240",
    "end": "1055760"
  },
  {
    "text": "Clusters where people want more stability this should also be uh available for you",
    "start": "1055760",
    "end": "1061700"
  },
  {
    "text": "um also there are going to be detailed instructions um in cluster or scalar FAQs similar to what I have gone through",
    "start": "1061700",
    "end": "1068780"
  },
  {
    "text": "um that you can refer to and you know how to operate the snapshotter and we're",
    "start": "1068780",
    "end": "1074240"
  },
  {
    "text": "looking forward to hearing back on any feedback that you have and any possible extensions to the snapshotter that you",
    "start": "1074240",
    "end": "1080480"
  },
  {
    "text": "think will make everybody's life easier um yeah that's all for me uh thanks",
    "start": "1080480",
    "end": "1085520"
  },
  {
    "text": "everyone um and passing it on hello everyone I'm John Wang from IBM",
    "start": "1085520",
    "end": "1092480"
  },
  {
    "text": "research today my colleague michaeli Orlando and I will introduce our new in-house Moon",
    "start": "1092480",
    "end": "1098600"
  },
  {
    "text": "proposal for kubernetes auto scaler which is called multi-dimensional Part",
    "start": "1098600",
    "end": "1103820"
  },
  {
    "text": "auto scaler and my research staff member from IBM research and my daily work",
    "start": "1103820",
    "end": "1109460"
  },
  {
    "text": "involves enhancement in kubernetes including Auto scalers schedulers and",
    "start": "1109460",
    "end": "1115580"
  },
  {
    "text": "node resource plugins I'm also actively working on cloud native AI system platform and applying",
    "start": "1115580",
    "end": "1122900"
  },
  {
    "text": "AI techniques in Cloud platform management and then open source Advocate",
    "start": "1122900",
    "end": "1128480"
  },
  {
    "text": "a kubernetes contributor and the regular coupon speaker",
    "start": "1128480",
    "end": "1134240"
  },
  {
    "text": "so today I will briefly introduce our motivation why we propose multi-dimensional part autoscaler and",
    "start": "1134240",
    "end": "1141620"
  },
  {
    "text": "the design of the mpa and later micheli will",
    "start": "1141620",
    "end": "1147559"
  },
  {
    "text": "show us a demo on how to use the mpa",
    "start": "1147559",
    "end": "1152919"
  },
  {
    "text": "So currently there are two Auto scaling controllers available in the community",
    "start": "1153440",
    "end": "1158679"
  },
  {
    "text": "which is the horizontal Auto scalar and vertical part Auto scaler so kubernetes",
    "start": "1158679",
    "end": "1165860"
  },
  {
    "text": "horizontal pod Auto scaler later we refer to HPA is an auto scaler that",
    "start": "1165860",
    "end": "1171919"
  },
  {
    "text": "allows you to automatically scale the number of parts in a deployment our",
    "start": "1171919",
    "end": "1177440"
  },
  {
    "text": "replica Side based on either the CPU utilization metric or other Custom Performance metrics",
    "start": "1177440",
    "end": "1184340"
  },
  {
    "text": "so with HPA you can ensure that your application is running at an Optimal",
    "start": "1184340",
    "end": "1189919"
  },
  {
    "text": "Performance at optimal uh CPU utilization rate and uh by automatically",
    "start": "1189919",
    "end": "1198200"
  },
  {
    "text": "scaling out and in the number of Parts based on the magic and another controller is called",
    "start": "1198200",
    "end": "1205220"
  },
  {
    "text": "vertical part autoscaler we later refer to vpa so vpa automatically adjusts the",
    "start": "1205220",
    "end": "1211880"
  },
  {
    "text": "resource requests and limits of a container based on its actual usage rather than the initial values that by",
    "start": "1211880",
    "end": "1219320"
  },
  {
    "text": "the developer it analyzed the historical resource usage patterns of the Pod",
    "start": "1219320",
    "end": "1225200"
  },
  {
    "text": "periodically and then recommends an appropriate resource request on limit to",
    "start": "1225200",
    "end": "1230780"
  },
  {
    "text": "be set for the part this ensures that the part always has the amount of",
    "start": "1230780",
    "end": "1235880"
  },
  {
    "text": "resources allocated to it based on its usage and can help prevent over",
    "start": "1235880",
    "end": "1240919"
  },
  {
    "text": "provisioning of resources so the design of BPA actually consists",
    "start": "1240919",
    "end": "1246380"
  },
  {
    "text": "of three main controllers the recommended analyze the historical usage patterns of the Pod and recommends the",
    "start": "1246380",
    "end": "1253280"
  },
  {
    "text": "resource usage and limit to be said based on the histogram of the resource usage observed in the previous time",
    "start": "1253280",
    "end": "1259640"
  },
  {
    "text": "window uh you only 15 minutes and the update the updater is responsible for observing",
    "start": "1259640",
    "end": "1267679"
  },
  {
    "text": "the difference between the recommended requests and limits and the current set request and limit and will evict the",
    "start": "1267679",
    "end": "1274940"
  },
  {
    "text": "parts if the Gap is too big and the admission controller is",
    "start": "1274940",
    "end": "1280760"
  },
  {
    "text": "responsible for updating the pause request a limit when the past the victim",
    "start": "1280760",
    "end": "1286640"
  },
  {
    "text": "pass are restarting according to the recommended values provided by the",
    "start": "1286640",
    "end": "1292100"
  },
  {
    "text": "recommended so those three controllers work together to provide automatic",
    "start": "1292100",
    "end": "1297620"
  },
  {
    "text": "vertical scaling for kubernetes path allowing them to dynamically adjust their resource requests and limits based",
    "start": "1297620",
    "end": "1305419"
  },
  {
    "text": "on their actual usage so because right now HPA and vpa control",
    "start": "1305419",
    "end": "1311600"
  },
  {
    "text": "the scaling actions separately as independent controllers",
    "start": "1311600",
    "end": "1317059"
  },
  {
    "text": "um so when they are configured to optimize the certain targets for example",
    "start": "1317059",
    "end": "1322159"
  },
  {
    "text": "CPU usage they can lead to an upward situation where HPS tries to spend more",
    "start": "1322159",
    "end": "1328940"
  },
  {
    "text": "pause based on the higher than threshold CPU usage well vpa tries to squeeze the sides of each",
    "start": "1328940",
    "end": "1336080"
  },
  {
    "text": "part based on the uh lower CPU usage after scaling out by the HPA so the",
    "start": "1336080",
    "end": "1343700"
  },
  {
    "text": "final outcome could be conflicting meaning it will lead to a large number of small parts creates the broader",
    "start": "1343700",
    "end": "1350360"
  },
  {
    "text": "workload and none of the objectives uh were uh can be guaranteed",
    "start": "1350360",
    "end": "1356960"
  },
  {
    "text": "so there's some motivations for the um for the combined control of both",
    "start": "1356960",
    "end": "1362900"
  },
  {
    "text": "horizontal and vertical scaling because first we sometimes we sometimes want to",
    "start": "1362900",
    "end": "1368360"
  },
  {
    "text": "fine-tuning the timing to do the vertical and horizontal scaling and",
    "start": "1368360",
    "end": "1373760"
  },
  {
    "text": "prioritize a certain Dimension um Auto scalings than the other and",
    "start": "1373760",
    "end": "1381380"
  },
  {
    "text": "there might be some there might be some synchronizations between both actions",
    "start": "1381380",
    "end": "1387020"
  },
  {
    "text": "and sometimes controlling the vertical scaling based on the usage and controlling the horizontal scalings",
    "start": "1387020",
    "end": "1392480"
  },
  {
    "text": "based on the performance just doesn't guarantee either performance or resource efficiency",
    "start": "1392480",
    "end": "1398980"
  },
  {
    "text": "because the usage observed is statistics from a Time window and sometimes a",
    "start": "1398980",
    "end": "1405919"
  },
  {
    "text": "certain margin of resource uh over provisioning is needed to handle the",
    "start": "1405919",
    "end": "1411679"
  },
  {
    "text": "fluctuation of the workload to guarantee certain performance objective therefore",
    "start": "1411679",
    "end": "1416840"
  },
  {
    "text": "there needs an advanced combined algorithm to find an optimal combined control of uh vertical vertical",
    "start": "1416840",
    "end": "1425000"
  },
  {
    "text": "horizontal scanning actions for certain application under a certain load",
    "start": "1425000",
    "end": "1430940"
  },
  {
    "text": "so moreover in some cases one objective is prioritized than the other for",
    "start": "1430940",
    "end": "1436820"
  },
  {
    "text": "example when there is only one replica for deployment you may not only want to",
    "start": "1436820",
    "end": "1442100"
  },
  {
    "text": "scale down uh the vertical scaling down the resources if the resource",
    "start": "1442100",
    "end": "1448159"
  },
  {
    "text": "utilization is low so uh therefore here we propose a",
    "start": "1448159",
    "end": "1453260"
  },
  {
    "text": "multi-dimensional Part auto scaling firmware that combines the control of",
    "start": "1453260",
    "end": "1458539"
  },
  {
    "text": "vertical and horizontal scaling in a single action but separates the",
    "start": "1458539",
    "end": "1463640"
  },
  {
    "text": "actuation of actions completely from the controlling algorithm",
    "start": "1463640",
    "end": "1468740"
  },
  {
    "text": "uh just similarly as a design of vpa NPA consists of three controllers a",
    "start": "1468740",
    "end": "1475400"
  },
  {
    "text": "recommender an updater and an amazing controller and of course we have a new",
    "start": "1475400",
    "end": "1480620"
  },
  {
    "text": "API defined in the customer resources as NPA and that connects all the auto",
    "start": "1480620",
    "end": "1487460"
  },
  {
    "text": "scaling recommendations to its actuation so the multi-dimensional scaling",
    "start": "1487460",
    "end": "1493159"
  },
  {
    "text": "algorithm is implemented in the recommender the scaling decisions derived from the recommender are stored",
    "start": "1493159",
    "end": "1500360"
  },
  {
    "text": "in the mpa object and the updater animation controller retrieves those",
    "start": "1500360",
    "end": "1506659"
  },
  {
    "text": "decisions from the mpa object and actuate those vertical horizontal actions so our proposed MPA can also",
    "start": "1506659",
    "end": "1514940"
  },
  {
    "text": "support developers to replace the default recommender with their alternative customized recommender",
    "start": "1514940",
    "end": "1521679"
  },
  {
    "text": "algorithm so developers can provide their own recommendations and their own",
    "start": "1521679",
    "end": "1528200"
  },
  {
    "text": "algorithms implementing some Advanced control of both actions",
    "start": "1528200",
    "end": "1534460"
  },
  {
    "text": "so in details in MPA API developers can specify the auto scaling configurations",
    "start": "1536240",
    "end": "1542900"
  },
  {
    "text": "include whether they only want to know the recommendations from MPA or whether they want MPA to directly actuate the",
    "start": "1542900",
    "end": "1550760"
  },
  {
    "text": "auto scaling decision and it can specify",
    "start": "1550760",
    "end": "1555799"
  },
  {
    "text": "certain applications for performance targets such as latency or throughput",
    "start": "1555799",
    "end": "1561080"
  },
  {
    "text": "and it allows any custom metric to be used and it",
    "start": "1561080",
    "end": "1568100"
  },
  {
    "text": "um so other also scaling configurations such as",
    "start": "1568100",
    "end": "1574120"
  },
  {
    "text": "what has been available in HPA and vpa are also available in MPA so MPA API is",
    "start": "1574120",
    "end": "1582679"
  },
  {
    "text": "also responsible for connecting the auto scan actions generated from the recommender to the automation controller",
    "start": "1582679",
    "end": "1589580"
  },
  {
    "text": "and updater and it's created based on the mpa",
    "start": "1589580",
    "end": "1595240"
  },
  {
    "text": "actually the mpa custom resources uh",
    "start": "1595240",
    "end": "1600340"
  },
  {
    "text": "provided by the Upstream community and",
    "start": "1600340",
    "end": "1605778"
  },
  {
    "text": "eight um basically it is a CR and it keeps tracks uh what's recommended",
    "start": "1605919",
    "end": "1612919"
  },
  {
    "text": "um size and part and the number of replicas so the recommended just retrieves the time indexed measurement",
    "start": "1612919",
    "end": "1619340"
  },
  {
    "text": "data from The Matrix API about the usage and uh it generates the vertical and",
    "start": "1619340",
    "end": "1625220"
  },
  {
    "text": "horizontal scaling actions and the actions from the recommender are then updated in the object and the auto",
    "start": "1625220",
    "end": "1632659"
  },
  {
    "text": "scaling behavior is based on the user defined configuration as well and the updater will updated the number",
    "start": "1632659",
    "end": "1639559"
  },
  {
    "text": "of replicas of the deployment and debate eligible parts for vertical scaling and the admission controller just similar as",
    "start": "1639559",
    "end": "1646340"
  },
  {
    "text": "vpa admission controller I will just update the part request limit for those",
    "start": "1646340",
    "end": "1651860"
  },
  {
    "text": "effective part when they were starting foreign",
    "start": "1651860",
    "end": "1657640"
  },
  {
    "text": "object so it includes configurations such as update policy and namely whether",
    "start": "1659679",
    "end": "1665419"
  },
  {
    "text": "you want to do recommendation only or you want to actually actuate those Auto scaling actions and then uh direct",
    "start": "1665419",
    "end": "1672740"
  },
  {
    "text": "vendor it defines you can Define the default recommender of any customized",
    "start": "1672740",
    "end": "1677960"
  },
  {
    "text": "alternative recommender with the name you provided and there are also vertical scaling",
    "start": "1677960",
    "end": "1683840"
  },
  {
    "text": "related configurations such as the minimum maximum allowed resources the type of resources you want to control",
    "start": "1683840",
    "end": "1689720"
  },
  {
    "text": "and what containers to resize the horizontal scaling related configurations include the minimum",
    "start": "1689720",
    "end": "1696559"
  },
  {
    "text": "maximum number of replicas and the type of magic",
    "start": "1696559",
    "end": "1701919"
  },
  {
    "text": "H horizontal obviously is operating on so uh that's all of my introduction of",
    "start": "1702159",
    "end": "1710539"
  },
  {
    "text": "mpf framework and next I will handle it to Michaela to show a simple demo on how to",
    "start": "1710539",
    "end": "1717980"
  },
  {
    "text": "use it hello everybody my name is Michael I'm From Italy",
    "start": "1717980",
    "end": "1723279"
  },
  {
    "text": "today I'm going to show you uh the multi-dimensional but Auto scaler",
    "start": "1723279",
    "end": "1731200"
  },
  {
    "text": "we can see this as a mixture between the vertical and horizontal autoscaler",
    "start": "1731200",
    "end": "1738140"
  },
  {
    "text": "I'm running on a IBM Cloud IKS",
    "start": "1738140",
    "end": "1744260"
  },
  {
    "text": "kubernetes cluster so uh let me deploy the objects",
    "start": "1744260",
    "end": "1753279"
  },
  {
    "text": "emls that are going to uh make up my um",
    "start": "1753279",
    "end": "1759580"
  },
  {
    "text": "Auto scaler so as you can see it's creating a",
    "start": "1759860",
    "end": "1765620"
  },
  {
    "text": "some custom resource definitions cluster roll bindings Services I'm going to",
    "start": "1765620",
    "end": "1771140"
  },
  {
    "text": "mainly uh look at this particular [Music]",
    "start": "1771140",
    "end": "1776600"
  },
  {
    "text": "um deployment which is the the actual",
    "start": "1776600",
    "end": "1781700"
  },
  {
    "text": "recommender so let me just get the actual name of the Pod for this one",
    "start": "1781700",
    "end": "1788539"
  },
  {
    "text": "so that's the one okay sorry okay",
    "start": "1788539",
    "end": "1793580"
  },
  {
    "text": "so it has no actual MPA object to observe",
    "start": "1793580",
    "end": "1800059"
  },
  {
    "text": "so I'm going to create one right now this object is just a deployment is is",
    "start": "1800059",
    "end": "1807380"
  },
  {
    "text": "the actual Auto scaler and as a Target it has this deployment",
    "start": "1807380",
    "end": "1813320"
  },
  {
    "text": "here which I'm going to actually",
    "start": "1813320",
    "end": "1818419"
  },
  {
    "text": "apply here so as you can see uh I've created my",
    "start": "1818419",
    "end": "1824360"
  },
  {
    "text": "Apache uh application next we're going to wait until the actual recommender",
    "start": "1824360",
    "end": "1830419"
  },
  {
    "text": "actually detects the metrics from the application",
    "start": "1830419",
    "end": "1835520"
  },
  {
    "text": "at the moment no Matrix are available yet still not getting the metrics here as",
    "start": "1835520",
    "end": "1842120"
  },
  {
    "text": "you can see the recommended CPU is 100 Milli cores oh there there we go metrics have",
    "start": "1842120",
    "end": "1848960"
  },
  {
    "text": "arrived so we should get the recommender to detect them over here we have a limit",
    "start": "1848960",
    "end": "1855200"
  },
  {
    "text": "and requested CPU for the uh",
    "start": "1855200",
    "end": "1861020"
  },
  {
    "text": "the only about running okay so here we have the the vertical scaling it has updated the",
    "start": "1861020",
    "end": "1869419"
  },
  {
    "text": "CPU recommendation but he has decided not to scale the number of horizontally",
    "start": "1869419",
    "end": "1875360"
  },
  {
    "text": "scale the number of uh odds because as you can see the part is basically just",
    "start": "1875360",
    "end": "1880520"
  },
  {
    "text": "sitting there so I'm just gonna load add a load to this to this pod I'm",
    "start": "1880520",
    "end": "1887720"
  },
  {
    "text": "going to load it with the requests and let's see what the recommender says when does the",
    "start": "1887720",
    "end": "1894620"
  },
  {
    "text": "recommender detect the load as you can see the metrics have still not being",
    "start": "1894620",
    "end": "1899720"
  },
  {
    "text": "updated even though the parties is fulfilling all these requests and the",
    "start": "1899720",
    "end": "1906980"
  },
  {
    "text": "replicas have not been updated either now the Matrix have have",
    "start": "1906980",
    "end": "1912440"
  },
  {
    "text": "gone up we have a 157 Milli cores so the recommender should realize this on the",
    "start": "1912440",
    "end": "1919159"
  },
  {
    "text": "next uh run and should scale up both horizontally and vertically let's",
    "start": "1919159",
    "end": "1926899"
  },
  {
    "text": "see what happens okay there you go so this is the new horizontal scaling",
    "start": "1926899",
    "end": "1932179"
  },
  {
    "text": "sorry vertical and uh and this is the actual horizontal",
    "start": "1932179",
    "end": "1937520"
  },
  {
    "text": "one because we now we have four replicas for desired replicas",
    "start": "1937520",
    "end": "1944919"
  },
  {
    "text": "and in fact we already they already been updated here",
    "start": "1945500",
    "end": "1951020"
  },
  {
    "text": "you can see also there a replica set and then deployment has been modified and you can see also",
    "start": "1951020",
    "end": "1959779"
  },
  {
    "text": "the limits and the requests have been modified this is the recommended uh millicore value for the CPU we have 182",
    "start": "1959779",
    "end": "1968059"
  },
  {
    "text": "as the recommended value now let's see what happens in the next run okay 163 it",
    "start": "1968059",
    "end": "1975440"
  },
  {
    "text": "changes the value slightly but the replicas have now gone up to six which is the actual maximum value that we can",
    "start": "1975440",
    "end": "1982340"
  },
  {
    "text": "have as you can see so now let's see uh if it scales down",
    "start": "1982340",
    "end": "1987919"
  },
  {
    "text": "we're gonna stop the loading we're gonna interrupt it okay",
    "start": "1987919",
    "end": "1993440"
  },
  {
    "text": "Ctrl C there you go so now the load part is has disappeared",
    "start": "1993440",
    "end": "2000100"
  },
  {
    "text": "we're going to wait until the uh [Music] the mpa realizes the text a lower load",
    "start": "2000100",
    "end": "2008980"
  },
  {
    "text": "here coming from the Matrix and we expect to have an update on the",
    "start": "2008980",
    "end": "2015100"
  },
  {
    "text": "vertical and on the horizontal side so let's wait for for the recommender to detect this",
    "start": "2015100",
    "end": "2021880"
  },
  {
    "text": "change okay what do we have here a vertical",
    "start": "2021880",
    "end": "2027059"
  },
  {
    "text": "126 yeah it's definitely going down before I think it was 182.",
    "start": "2027059",
    "end": "2034720"
  },
  {
    "text": "but the actual the horizontal scaling is has still not gone down",
    "start": "2034720",
    "end": "2041039"
  },
  {
    "text": "as you can see average utilization is basically zero still not scaling vertical",
    "start": "2041080",
    "end": "2047099"
  },
  {
    "text": "recommendation is not changing either even though metrics are all below Target so it's actually uh watching metrics",
    "start": "2047099",
    "end": "2054520"
  },
  {
    "text": "from all pods a vertical recommendation has gone down but horizontal is still at",
    "start": "2054520",
    "end": "2060878"
  },
  {
    "text": "six okay I think we have something there",
    "start": "2060879",
    "end": "2066520"
  },
  {
    "text": "okay this is the new desired replicas okay it is already scaled down and we're",
    "start": "2066520",
    "end": "2073898"
  },
  {
    "text": "basically back to the initial situation yes it took a couple of minutes to",
    "start": "2073899",
    "end": "2081460"
  },
  {
    "text": "to actually scale down but uh in the end we did it okay uh I think that's it for now uh",
    "start": "2081460",
    "end": "2089679"
  },
  {
    "text": "thank you for watching this video and hope to see you soon bye",
    "start": "2089679",
    "end": "2097440"
  }
]