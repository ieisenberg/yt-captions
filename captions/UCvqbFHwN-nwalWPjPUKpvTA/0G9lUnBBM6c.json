[
  {
    "text": "hello um and my name is steven mcdonald i am a psych reliability engineer at uh",
    "start": "80",
    "end": "5839"
  },
  {
    "text": "usability by surveillancy as you can see a bit about me here um",
    "start": "5839",
    "end": "11280"
  },
  {
    "text": "so i initially joined usabilla in march of 2018 in order to help set up and migrate to",
    "start": "11280",
    "end": "17760"
  },
  {
    "text": "new cloud native infrastructure and around the start of 2019 i took on the logging component of that",
    "start": "17760",
    "end": "24000"
  },
  {
    "text": "architecture which is what led to the material being discussed here",
    "start": "24000",
    "end": "31039"
  },
  {
    "text": "so our infrastructure is completely running in ec2 we have a wide variety of different",
    "start": "31039",
    "end": "37600"
  },
  {
    "text": "instances both legacy infrastructure and cloud native infrastructure and we want to aggregate all of those",
    "start": "37600",
    "end": "43360"
  },
  {
    "text": "logs together into the one place so that's that's the goal that we're working towards here",
    "start": "43360",
    "end": "49840"
  },
  {
    "text": "um and so the first iteration that we built was a fairly standard fluency setup so",
    "start": "49840",
    "end": "55920"
  },
  {
    "text": "we had two uh redundant fluency aggregators with the fluentd forwarding protocol",
    "start": "55920",
    "end": "63440"
  },
  {
    "text": "being used to forward logs from ec2 instances and we used fluent bit which is a lightweight c",
    "start": "63440",
    "end": "69200"
  },
  {
    "text": "implementation of the fluently frauding protocol to do that from those instances and then those fluency aggregators would",
    "start": "69200",
    "end": "75840"
  },
  {
    "text": "of course do any filtering do any rewriting and end up writing the logs",
    "start": "75840",
    "end": "81360"
  },
  {
    "text": "to cloudwatch and elasticsearch which is so we were using cloudwatch for archival and elasticsearch for easy searching of",
    "start": "81360",
    "end": "87520"
  },
  {
    "text": "recent logs so that's all fairly standard and it worked well to a point but we ran",
    "start": "87520",
    "end": "95119"
  },
  {
    "text": "into a problem that highlighted quite a few problems throughout the entire",
    "start": "95119",
    "end": "100799"
  },
  {
    "text": "blog processing pipeline so what we ran into was the first thing",
    "start": "100799",
    "end": "106720"
  },
  {
    "text": "that went wrong was that our elasticsearch cluster that we were using turned out to be too small to",
    "start": "106720",
    "end": "111840"
  },
  {
    "text": "handle the logs that we were putting into it so that's a fairly innocuous misconfiguration it",
    "start": "111840",
    "end": "118079"
  },
  {
    "text": "means blocks can't be written to elasticsearch but you would expect that the problem would remain isolated",
    "start": "118079",
    "end": "123439"
  },
  {
    "text": "to writing the logs into elasticsearch unfortunately that led us into a",
    "start": "123439",
    "end": "128479"
  },
  {
    "text": "cascading failure where we had a misconfiguration",
    "start": "128479",
    "end": "133840"
  },
  {
    "text": "of the fluently elastic search plug-in um which caused log duplication so this requires",
    "start": "133840",
    "end": "138879"
  },
  {
    "text": "a bit more explanation um it is documented in the fluently elasticsearch plug-in",
    "start": "138879",
    "end": "144319"
  },
  {
    "text": "that uh in case of a partial failure from the elastic search api the failed",
    "start": "144319",
    "end": "151599"
  },
  {
    "text": "messages will be re-emitted into the login pipeline and the reason for this is there's no way in the fluency plugin api for the",
    "start": "151599",
    "end": "159120"
  },
  {
    "text": "plugin to tell fluentd some of these log events were successfully submitted but others weren't",
    "start": "159120",
    "end": "164560"
  },
  {
    "text": "so what so it just re-emits them from the beginning of the blog processing pipeline so that was how bad we hadn't read that",
    "start": "164560",
    "end": "171840"
  },
  {
    "text": "bit of the documentation so we hadn't configured it in that way um but that caused log duplication which",
    "start": "171840",
    "end": "178640"
  },
  {
    "text": "both meant logs were duplicated in cloudwatch but it also meant that um it also placed",
    "start": "178640",
    "end": "186800"
  },
  {
    "text": "a whole lot more low on the fluidity aggregators because they were processing the same messages hundreds and hundreds of times over",
    "start": "186800",
    "end": "192560"
  },
  {
    "text": "again uh eventually to the point that they were unable to accept new logs",
    "start": "192560",
    "end": "198000"
  },
  {
    "text": "from the fluent bid clients which is now a problem because those",
    "start": "198000",
    "end": "203280"
  },
  {
    "text": "start those logs um start backing up in log buffers on the individual instances and if",
    "start": "203280",
    "end": "209120"
  },
  {
    "text": "one of those instances dies we lose any logs that were not accepted by fluently",
    "start": "209120",
    "end": "215360"
  },
  {
    "text": "but that caused even more problems because we um we ran into a bug influence bit where",
    "start": "215360",
    "end": "222480"
  },
  {
    "text": "as the log buffer grows it creates more files to hold the additional logs",
    "start": "222480",
    "end": "227519"
  },
  {
    "text": "and fluid bit has a bug where it will hold all of those files open at the same time",
    "start": "227519",
    "end": "233439"
  },
  {
    "text": "and when it hits its open file limit it just crashes it doesn't gracefully recover",
    "start": "233439",
    "end": "238480"
  },
  {
    "text": "from that situation so that means that logs are not even",
    "start": "238480",
    "end": "243680"
  },
  {
    "text": "being collected by fluent bit once it's crashed but additionally fluid bit doesn't always recover from",
    "start": "243680",
    "end": "250720"
  },
  {
    "text": "that situation at all without manual intervention there seems to be another bug where if it crashes while the log buffer is in",
    "start": "250720",
    "end": "258079"
  },
  {
    "text": "the wrong state it will spam a whole lot of empty invalid log messages at fluently instead",
    "start": "258079",
    "end": "264479"
  },
  {
    "text": "of sending it the um the actual log messages that are in the buffer which means",
    "start": "264479",
    "end": "269919"
  },
  {
    "text": "there's now no way to recover those log messages without diving into the internal fluid bit um buffer format and manually recovering",
    "start": "269919",
    "end": "277840"
  },
  {
    "text": "that which the easiest thing to do is just consider those logs lost and delete the buffer so",
    "start": "277840",
    "end": "283280"
  },
  {
    "text": "that it just picks up new blocks so this turned out this one tiny misconfiguration in elasticsearch",
    "start": "283280",
    "end": "290160"
  },
  {
    "text": "turned out to cause problems through the entire pipeline even in cloud watch where the vlogs were",
    "start": "290160",
    "end": "296320"
  },
  {
    "text": "being duplicated so we learned a few lessons from this about the way we were doing things",
    "start": "296320",
    "end": "303440"
  },
  {
    "text": "the initial root cause and which it was an overload of elasticsearch",
    "start": "303440",
    "end": "308800"
  },
  {
    "text": "we can easily fix that so that's just a case of giving elasticsearch more resources but we didn't want a problem in one part",
    "start": "308800",
    "end": "316000"
  },
  {
    "text": "of the architecture to be able to cause a whole lot of problems elsewhere",
    "start": "316000",
    "end": "321360"
  },
  {
    "text": "secondly the the misconfiguration of the search fluency plugin um it is documented behavior but it",
    "start": "321360",
    "end": "328400"
  },
  {
    "text": "highlights that it it's difficult to configure fluency plugins for reliability because",
    "start": "328400",
    "end": "335039"
  },
  {
    "text": "um you can't assume that because this is an output plug-in it's only going to be able to do the function of outputting",
    "start": "335039",
    "end": "340639"
  },
  {
    "text": "locks it has full access to these internal apis and in this case it was using them to behave",
    "start": "340639",
    "end": "346000"
  },
  {
    "text": "sort of like an input plug-in by re-emitting uh those log events which is the an input plug-ins drop",
    "start": "346000",
    "end": "352320"
  },
  {
    "text": "um fluid bits crashing highlights that if fluent d does have",
    "start": "352320",
    "end": "358479"
  },
  {
    "text": "availability problems this can cause problems on every instance on the network that's",
    "start": "358479",
    "end": "363840"
  },
  {
    "text": "17 logs with fluid bit which is also undesirable",
    "start": "363840",
    "end": "368960"
  },
  {
    "text": "and so the upshot of all these things the fact that one component can have knock-on effects earlier on is we need",
    "start": "369680",
    "end": "375360"
  },
  {
    "text": "to decouple those somehow so if it's difficult to configure fluency for reliability and if fluently not",
    "start": "375360",
    "end": "381840"
  },
  {
    "text": "being reliable causes problems elsewhere then we must have something in between we need to have a buffer",
    "start": "381840",
    "end": "387280"
  },
  {
    "text": "that doesn't break when fluentd goes unavailable",
    "start": "387280",
    "end": "392960"
  },
  {
    "text": "so we came up with a new plan first of all we wrote a new talk uh sorry not yet uh first of all we",
    "start": "393600",
    "end": "400639"
  },
  {
    "text": "decided to use kafka as a central logging buffer so all of",
    "start": "400639",
    "end": "406080"
  },
  {
    "text": "whenever any logs went from one system to another they would always go via kafka they would never get written",
    "start": "406080",
    "end": "411280"
  },
  {
    "text": "directly from one um from one tool to another kafka if you're not familiar with it is",
    "start": "411280",
    "end": "417280"
  },
  {
    "text": "a distributed message broker it's it's used in production by a whole lot of companies it has",
    "start": "417280",
    "end": "423360"
  },
  {
    "text": "demonstrated fault tolerance we're also using it in our application so we have confidence in that",
    "start": "423360",
    "end": "428960"
  },
  {
    "text": "um there are a couple of added benefits to it as well consumers may be stopped entirely um",
    "start": "428960",
    "end": "436240"
  },
  {
    "text": "which means that if fluent d breaks entirely for some reason such as what i just described it doesn't",
    "start": "436240",
    "end": "444240"
  },
  {
    "text": "that any any log producers don't even see that they just keep writing their logs into kafka and when the consumer",
    "start": "444240",
    "end": "449840"
  },
  {
    "text": "comes back online it can reprocess word mist another added benefit is that you can",
    "start": "449840",
    "end": "455120"
  },
  {
    "text": "process the same logs twice by two different applications which makes testing a new configuration very easy",
    "start": "455120",
    "end": "461199"
  },
  {
    "text": "you don't need to roll out a new configuration and find out if it breaks you can have a testing fluency instance that",
    "start": "461199",
    "end": "468800"
  },
  {
    "text": "reprocesses the same logs on real data with a new configuration and just see what happens so given all of",
    "start": "468800",
    "end": "475919"
  },
  {
    "text": "these advantages it seemed like a good fit for logging buffer at least for our use case",
    "start": "475919",
    "end": "482879"
  },
  {
    "text": "um we also noticed that we only really need half of what fluent bit was doing for us",
    "start": "482879",
    "end": "489199"
  },
  {
    "text": "so what fluent bit does is it takes logs from a variety of sources puts them into a buffer that it manages",
    "start": "489199",
    "end": "494720"
  },
  {
    "text": "internally and then writes those logs out from that buffer to some destination in our case it was",
    "start": "494720",
    "end": "501360"
  },
  {
    "text": "a fluency instance but it can also be kafka or whatever else but we don't actually need that first",
    "start": "501360",
    "end": "506720"
  },
  {
    "text": "part reading logs from various sources because all of our machines um are running systemd and",
    "start": "506720",
    "end": "512640"
  },
  {
    "text": "systemd has a component called journal d that maintains a unified login buffer to",
    "start": "512640",
    "end": "518399"
  },
  {
    "text": "aggregate all of the logs on a particular host so we already have a blog blog buffer",
    "start": "518399",
    "end": "525279"
  },
  {
    "text": "there any log exporter that just reads that and writes it into another buffer is providing duplicate functionality so we",
    "start": "525279",
    "end": "530959"
  },
  {
    "text": "only need that second half we only need something to read from the systemd journal and write those logs",
    "start": "530959",
    "end": "536959"
  },
  {
    "text": "into kafka and then as a matter of architectural design we can say anything that logs must log to",
    "start": "536959",
    "end": "542959"
  },
  {
    "text": "the journal and from there it will be picked up and taken where it needs to go",
    "start": "542959",
    "end": "549839"
  },
  {
    "text": "so that's all well and good that's the general design that we're going for what code do",
    "start": "549920",
    "end": "555600"
  },
  {
    "text": "we then need to write to to make this this architecture happen so the first thing we developed is a",
    "start": "555600",
    "end": "561920"
  },
  {
    "text": "tool called peripera which is just a silly pun so uh fluent d is a japanese",
    "start": "561920",
    "end": "567120"
  },
  {
    "text": "project and peripera is japanese for fluent so it's just reversed um we looked into using the um",
    "start": "567120",
    "end": "575519"
  },
  {
    "text": "existing fluid bit kafka plug-in for this but what that does so it uses a",
    "start": "575519",
    "end": "582560"
  },
  {
    "text": "library called liberty kafka which is a c implementation of the kafka protocol",
    "start": "582560",
    "end": "587680"
  },
  {
    "text": "and the way liberty kafka works for performance reasons is when you write a message to it it puts it into an in-memory buffer and",
    "start": "587680",
    "end": "594560"
  },
  {
    "text": "then once it's aggregated some messages it'll write a batch to kafka the fluent bit kafka output plug-in",
    "start": "594560",
    "end": "600880"
  },
  {
    "text": "considers the right successful as soon as liberty kafka has it in its in-memory buffer",
    "start": "600880",
    "end": "606160"
  },
  {
    "text": "so if fluid bit crashes while there are messages in that buffer you just lose them and we didn't want to um we",
    "start": "606160",
    "end": "613519"
  },
  {
    "text": "didn't want that behavior but also all of the problems that we'd encountered with fluent bit",
    "start": "613519",
    "end": "618640"
  },
  {
    "text": "were related to the way that it does buffering and we as we already said we don't need",
    "start": "618640",
    "end": "623839"
  },
  {
    "text": "it to do any buffering because we have a buffer um so we developed pear apparel which is",
    "start": "623839",
    "end": "629440"
  },
  {
    "text": "a very simple tool um it doesn't support more than one log input it doesn't support more than one",
    "start": "629440",
    "end": "634640"
  },
  {
    "text": "log output all it does is read the systemd journal and write everything in it to kafka with no",
    "start": "634640",
    "end": "640399"
  },
  {
    "text": "processing minimal filtering the only thing it does is filter out its own messages if you",
    "start": "640399",
    "end": "645440"
  },
  {
    "text": "enable debug logging to avoid right amplification and we also use messagepack which is the",
    "start": "645440",
    "end": "651360"
  },
  {
    "text": "same serialization format used by fluentd's native forwarding protocol because what",
    "start": "651360",
    "end": "656640"
  },
  {
    "text": "my reasoning was that it would be most likely to give us the best compatibility with fluency fluently can",
    "start": "656640",
    "end": "662640"
  },
  {
    "text": "also read json messages from kafka but we won't message back um and it the only state that peer appear",
    "start": "662640",
    "end": "668959"
  },
  {
    "text": "itself keeps is the journal cursor which is just an",
    "start": "668959",
    "end": "674160"
  },
  {
    "text": "address within within the journal the the cursor that was last acknowledged by the kafka broker so",
    "start": "674160",
    "end": "680640"
  },
  {
    "text": "waits for the broker to acknowledge that something has actually been committed to the kafka cluster before it",
    "start": "680640",
    "end": "686240"
  },
  {
    "text": "says okay i've actually written that so if pero pera crashes next time it starts up it starts reading",
    "start": "686240",
    "end": "692240"
  },
  {
    "text": "from that last successful right so [Music] barring any bugs we're guaranteed that",
    "start": "692240",
    "end": "699440"
  },
  {
    "text": "no long messages will be lost the other component then is making",
    "start": "699440",
    "end": "706000"
  },
  {
    "text": "fluency work statelessly and what i mean by that is if we want to use kafka as a centralized log buffer",
    "start": "706000",
    "end": "713600"
  },
  {
    "text": "we want fluent d not to keep its own state because it should be using kafka to persistent state this is a bit",
    "start": "713600",
    "end": "721120"
  },
  {
    "text": "complicated because fluentd has um there are buffered and unbuffered output",
    "start": "721120",
    "end": "726800"
  },
  {
    "text": "plugins for fluency and the the choice of whether a plug-in supports",
    "start": "726800",
    "end": "732800"
  },
  {
    "text": "buffer unbuffered mode is up to the plug-in author and not the plug-in user if the plug-in author supports both then the plug-in user can",
    "start": "732800",
    "end": "738959"
  },
  {
    "text": "choose but most output plug-ins only choose to support buffered mode for performance reasons",
    "start": "738959",
    "end": "745040"
  },
  {
    "text": "because if you're reading one log message at a time you don't want to make one right call for every log message you",
    "start": "745040",
    "end": "752639"
  },
  {
    "text": "want to batch them together that's not a problem when we're using kafka as a log buffer because we can batch read from kafka but this isn't the",
    "start": "752639",
    "end": "759519"
  },
  {
    "text": "way that these plugins are designed to work so we would have to re-implement every output plug-in fluentd",
    "start": "759519",
    "end": "766160"
  },
  {
    "text": "as an unbuffered output plug-in in order to do this but we didn't want to do that so we",
    "start": "766160",
    "end": "772079"
  },
  {
    "text": "didn't so what we did instead was we um [Music] wrote one plug-in so there is an",
    "start": "772079",
    "end": "779839"
  },
  {
    "text": "existing output plug-in fluid called bufferize which is intended to give a buffer to an",
    "start": "779839",
    "end": "785680"
  },
  {
    "text": "output plug-in that doesn't support buffering so it basically wraps another plug-in and",
    "start": "785680",
    "end": "791040"
  },
  {
    "text": "adds a buffer to it and that's all it does we wrote the opposite of that plug-in so we wrote a plug-in called unbufferize",
    "start": "791040",
    "end": "797760"
  },
  {
    "text": "and what that does is it's it presents itself to fluenty as an unbuffered output plug-in and it",
    "start": "797760",
    "end": "805600"
  },
  {
    "text": "presents itself to a wrapped plug-in as the fluency buffer api",
    "start": "805600",
    "end": "810880"
  },
  {
    "text": "so that that plug-in is tricked into thinking that it's having messages written to it from a buffer when in fact they're coming directly from",
    "start": "810880",
    "end": "816560"
  },
  {
    "text": "the input plugin and it also um i'm not going to go into too",
    "start": "816560",
    "end": "822480"
  },
  {
    "text": "much detail here it performs chunking of each stream so that you can use chunk keys in strings in the configuration if",
    "start": "822480",
    "end": "828399"
  },
  {
    "text": "you've used fluentd you'll know why this is useful if you haven't it'll take too long to explain but um",
    "start": "828399",
    "end": "834959"
  },
  {
    "text": "and but this only works well when you can batch reads as i said which is perfect if you're using kafka because we can easily batch reads from conflict",
    "start": "834959",
    "end": "841279"
  },
  {
    "text": "this wouldn't work well with fluenty's forwarding input plug-in because that just reads",
    "start": "841279",
    "end": "846560"
  },
  {
    "text": "however many messages the the client sent at a time",
    "start": "846560",
    "end": "851760"
  },
  {
    "text": "so if we put that all together what actually happens within fluentd is the input plug-in um",
    "start": "853199",
    "end": "860079"
  },
  {
    "text": "the the kafka plug-in will not commit a read commit means inform kafka yes i've read",
    "start": "860079",
    "end": "866240"
  },
  {
    "text": "these messages don't don't send them to me again the input plugin won't actually do that commit",
    "start": "866240",
    "end": "872320"
  },
  {
    "text": "until the emit call which is the name of the function in fluency that sends a log event to be",
    "start": "872320",
    "end": "878480"
  },
  {
    "text": "processed returns successfully normally this will return when they've been committed to",
    "start": "878480",
    "end": "884079"
  },
  {
    "text": "the the buffer within fluentd within the output plug-in um but with the unbufferized plug-in we",
    "start": "884079",
    "end": "889920"
  },
  {
    "text": "made sure that uh it instead returns once the logs have been written out successfully to their final destination",
    "start": "889920",
    "end": "896000"
  },
  {
    "text": "so we're sure that they're actually unstable storage wherever they're supposed to go before we say to kafka yes i've read",
    "start": "896000",
    "end": "902320"
  },
  {
    "text": "these messages we also made sure that if an exception is raised in any of this code it does",
    "start": "902320",
    "end": "907760"
  },
  {
    "text": "get propagated back to the input plugin and the read is not committed so any errors will result in a retry of",
    "start": "907760",
    "end": "913680"
  },
  {
    "text": "reprocessing those locks that means that we have at least once rather than at most once delivery so we",
    "start": "913680",
    "end": "918800"
  },
  {
    "text": "can get duplicate logs but that's better than losing locks at least for our use case",
    "start": "918800",
    "end": "925680"
  },
  {
    "text": "so here's a diagram of how it all fits together instead of fluid bit on the ec2 instance we have peripera doing nothing",
    "start": "925680",
    "end": "931440"
  },
  {
    "text": "but reading from the journal and writing to kafka um the kafka topic is then an",
    "start": "931440",
    "end": "937759"
  },
  {
    "text": "intermediary buffer when kafka is um as i said highly available it can easily survive the loss of a node",
    "start": "937759",
    "end": "945199"
  },
  {
    "text": "so parapara should theoretically never have a problem writing that",
    "start": "945199",
    "end": "950480"
  },
  {
    "text": "now that i've said that it's probably probably going to happen out with it but um you get what i mean so it doesn't",
    "start": "950480",
    "end": "956880"
  },
  {
    "text": "break it fluently goes down is the point um fluently reads um from",
    "start": "956880",
    "end": "962639"
  },
  {
    "text": "that kafka topic and writes to another topic and here's the other thing that we can do with this is we can we can break up the fluency",
    "start": "962639",
    "end": "969920"
  },
  {
    "text": "logic into multiple different fluency processes so rather than having one fluency that does filtering",
    "start": "969920",
    "end": "976000"
  },
  {
    "text": "and also writes to cloudwatch and elasticsearch we have another kafka topic in between the filtering fluently",
    "start": "976000",
    "end": "982000"
  },
  {
    "text": "and the output and the benefit there is if we have another outage with elasticsearch",
    "start": "982000",
    "end": "987199"
  },
  {
    "text": "the only thing that that can affect is that one little fluency on the bottom right of the diagram it that cannot be propagated any further",
    "start": "987199",
    "end": "994480"
  },
  {
    "text": "back through the chain because all that fluenty is doing is reading from a kafka topic the filtering plugin will have no",
    "start": "994480",
    "end": "1000959"
  },
  {
    "text": "knowledge of this cloud log messages will not be duplicated in cloudwatch because it doesn't affect the",
    "start": "1000959",
    "end": "1007839"
  },
  {
    "text": "top right fluency and it definitely doesn't cause any problems at all with parapara so putting kafka in between these these",
    "start": "1007839",
    "end": "1015839"
  },
  {
    "text": "sort of roles makes everything a lot more reliable at least in theory",
    "start": "1015839",
    "end": "1021120"
  },
  {
    "text": "but how does it work in practice so we found it worked a lot better than the previous approach it actually made",
    "start": "1021120",
    "end": "1027839"
  },
  {
    "text": "things faster overall because without the buffer within fluentd we were just processing batches",
    "start": "1027839",
    "end": "1033360"
  },
  {
    "text": "from kafka as soon as they came in the main pain point for it is now the",
    "start": "1033360",
    "end": "1039280"
  },
  {
    "text": "the way that ruby kafka library works it only sends heartbeat messages to",
    "start": "1039280",
    "end": "1044720"
  },
  {
    "text": "kafka in between processing log batches which means we can either have nice big log batches and very infrequent heartbeats",
    "start": "1044720",
    "end": "1052799"
  },
  {
    "text": "so it takes a long time to recover from failure or we can have short heartbeats so that it recovers quickly from failure but we",
    "start": "1052799",
    "end": "1058559"
  },
  {
    "text": "have to process really tiny batches of logs and then run into stuff like cloudwatch api limits because",
    "start": "1058559",
    "end": "1064480"
  },
  {
    "text": "we're processing those logs too frequently so that's that's a bit annoying um",
    "start": "1064480",
    "end": "1072480"
  },
  {
    "text": "that was also difficult to debug because the um the uh sorry the kafka plug-in for",
    "start": "1072480",
    "end": "1078720"
  },
  {
    "text": "fluency doesn't log messages from the ruby kafka library by default so it took me took me a while to",
    "start": "1078720",
    "end": "1085360"
  },
  {
    "text": "figure out there are actually log messages that i was missing there but we eventually got to the bottom of that um and also fluentd",
    "start": "1085360",
    "end": "1091679"
  },
  {
    "text": "still uses a lot of memory on the order of several gigabytes per fluency process so if we go back to",
    "start": "1091679",
    "end": "1097360"
  },
  {
    "text": "the diagram that i had here each of these fluenties is using",
    "start": "1097360",
    "end": "1102400"
  },
  {
    "text": "several gigabytes of memory and of course if you wanted if you want them to be highly available then you're running multiple fluencies",
    "start": "1102400",
    "end": "1108960"
  },
  {
    "text": "for each of these fluent d boxes in this diagram so that's also we can afford it but it's also not",
    "start": "1108960",
    "end": "1117039"
  },
  {
    "text": "not ideal especially when we're not processing batches that big had to make various fixes so we've we",
    "start": "1117039",
    "end": "1124880"
  },
  {
    "text": "submitted a total of nine patches to various open source projects in the process of this work i won't go through all of them but",
    "start": "1124880",
    "end": "1130799"
  },
  {
    "text": "a couple of the things that we did were we added metrics to the prometheus plug-in",
    "start": "1130799",
    "end": "1136400"
  },
  {
    "text": "for fluid to report the oldest and newest time keys in the buffer um this was before we decided to go with",
    "start": "1136400",
    "end": "1142799"
  },
  {
    "text": "the new approach we wanted more more visibility into fluency's buffering one thing that fluentd lets you do is",
    "start": "1142799",
    "end": "1148240"
  },
  {
    "text": "chunk um buffers by time key so you say basically cut logs up into 10 second or",
    "start": "1148240",
    "end": "1155520"
  },
  {
    "text": "30 second or 60 second increments and write each one to a separate chunk in the buffer and i just wanted to have",
    "start": "1155520",
    "end": "1161840"
  },
  {
    "text": "metrics so we could see okay what's the oldest time key that we haven't yet processed and also what's the newest time key so we",
    "start": "1161840",
    "end": "1168480"
  },
  {
    "text": "know if there's some delay upstream before d with kafka we of course have better ways of finding these metrics using",
    "start": "1168480",
    "end": "1175039"
  },
  {
    "text": "the metrics on of the consumer group lag but um those metrics",
    "start": "1175039",
    "end": "1181600"
  },
  {
    "text": "are now on the open source project so if you're doing things the traditional way with fluentd you can make use of that",
    "start": "1181600",
    "end": "1187520"
  },
  {
    "text": "we also did some fixes for the way timestamps were handled so we could use uh kafka's timestamps as our canonical",
    "start": "1187520",
    "end": "1193600"
  },
  {
    "text": "source of um timekeeping rather than having a separate timestamp within the message given kafka already provides that um and",
    "start": "1193600",
    "end": "1200559"
  },
  {
    "text": "we also implemented correct consumer group rebalancing or previously so the way a consumer group in kafka works",
    "start": "1200559",
    "end": "1206320"
  },
  {
    "text": "is if you have a group of consumers that work collaboratively and when a group is",
    "start": "1206320",
    "end": "1214159"
  },
  {
    "text": "formed each group member sends a join group message to the kafka broker which will",
    "start": "1214159",
    "end": "1220080"
  },
  {
    "text": "wait until it receives well it'll wait for a timeout or until it receives",
    "start": "1220080",
    "end": "1225280"
  },
  {
    "text": "a joint group from all known consumers in the case of a consumer group rebalance",
    "start": "1225280",
    "end": "1231380"
  },
  {
    "text": "[Music] there was a bug in ruby kafka where it was using an old version of the join",
    "start": "1231380",
    "end": "1236640"
  },
  {
    "text": "group message that um that didn't that caused the broker not to wait for a timeout so every time",
    "start": "1236640",
    "end": "1243280"
  },
  {
    "text": "a new consumer tried to join it would end up in a consumer group of one and all of the other consumers would",
    "start": "1243280",
    "end": "1248799"
  },
  {
    "text": "be kicked out of the group so they were fighting each other over processing logs in serial instead of processing them in parallel",
    "start": "1248799",
    "end": "1254880"
  },
  {
    "text": "as they're supposed to so we also fixed that and so there are a bunch of improvements there that um",
    "start": "1254880",
    "end": "1260960"
  },
  {
    "text": "that other projects can also make use of and now there is some bad news",
    "start": "1260960",
    "end": "1267760"
  },
  {
    "text": "so i talked a bit about the fact that the the ruby kafka plug-in makes it difficult to balance",
    "start": "1267760",
    "end": "1273600"
  },
  {
    "text": "nice big log batches with reasonable failure recovery times",
    "start": "1273600",
    "end": "1278880"
  },
  {
    "text": "for that reason after this talk was submitted and it already had a title we decided to drop fluently for processing",
    "start": "1278880",
    "end": "1284799"
  },
  {
    "text": "log messages from kafka um we this approach in general is still",
    "start": "1284799",
    "end": "1291200"
  },
  {
    "text": "possibly useful for anyone else who might want to try a deployment like this with fluency",
    "start": "1291200",
    "end": "1296480"
  },
  {
    "text": "um but it's not all bad because we are still using fluency in one case so before the logs even get",
    "start": "1296480",
    "end": "1302720"
  },
  {
    "text": "to peripera on our kubernetes cluster we're using fluentd to write the kubernetes logs",
    "start": "1302720",
    "end": "1307760"
  },
  {
    "text": "into the journal so that's what i said before we've made an architectural decision that all logs on a node go to the journal and that is",
    "start": "1307760",
    "end": "1313840"
  },
  {
    "text": "our canonical buffer so we use fluentd as a way of getting logs into that buffer for kubernetes",
    "start": "1313840",
    "end": "1320720"
  },
  {
    "text": "um this is what the third iteration looks like and this also illustrates a nice property of the design that we went",
    "start": "1320720",
    "end": "1326480"
  },
  {
    "text": "with which is that components can be replaced so we could easily just take fluency out",
    "start": "1326480",
    "end": "1332080"
  },
  {
    "text": "of the the slots here in this diagram where it was and put in kafka streams and kafka",
    "start": "1332080",
    "end": "1337760"
  },
  {
    "text": "connect um we also switched to s3 to um for not",
    "start": "1337760",
    "end": "1343039"
  },
  {
    "text": "archival for other reasons but um the thing here is we could still use fluency if we wanted",
    "start": "1343039",
    "end": "1349760"
  },
  {
    "text": "to so we could still use fluency for filtering and use kafka connect for output or we could use we could go the other",
    "start": "1349760",
    "end": "1355760"
  },
  {
    "text": "way and use calculus streams for filtering and use fluency for output so breaking things up with kafka buffers in",
    "start": "1355760",
    "end": "1362080"
  },
  {
    "text": "between makes it possible to independently replace each component of the architecture without",
    "start": "1362080",
    "end": "1368080"
  },
  {
    "text": "needing to consider dependencies as long as as long as each component agrees on the",
    "start": "1368080",
    "end": "1374960"
  },
  {
    "text": "the protocol that gets written to the kafka topic um we peripera didn't need any",
    "start": "1374960",
    "end": "1380880"
  },
  {
    "text": "modification to deal with the fact that we were placed fluently with kafka streams so that's that's the real power of this",
    "start": "1380880",
    "end": "1386960"
  },
  {
    "text": "um this solution and we may bring fluency back into it at some point it's easy to add it back in because we can just slot it in",
    "start": "1386960",
    "end": "1393919"
  },
  {
    "text": "wherever we want it so and the future of this we um i",
    "start": "1393919",
    "end": "1400320"
  },
  {
    "text": "mentioned some of the open source contributions we've made we are looking at the possibility of open sourcing some of the new software we've",
    "start": "1400320",
    "end": "1406640"
  },
  {
    "text": "written if we find that it's not too specific to our setup currently i can't make any promises at",
    "start": "1406640",
    "end": "1412159"
  },
  {
    "text": "all there but we are looking at the possibility and for updates you can",
    "start": "1412159",
    "end": "1417280"
  },
  {
    "text": "look at a github blog or you can feel free to reach out to me personally if you if you want and yeah",
    "start": "1417280",
    "end": "1424799"
  },
  {
    "text": "so that's about it are there any questions",
    "start": "1424799",
    "end": "1431279"
  },
  {
    "text": "hello um so i'm going to go through these questions in the order that they came in",
    "start": "1431279",
    "end": "1437520"
  },
  {
    "text": "so the first one which should be",
    "start": "1438480",
    "end": "1445600"
  },
  {
    "text": "appearing now um how did you detect um the influence d",
    "start": "1445600",
    "end": "1452080"
  },
  {
    "text": "that the logs were being sent duplicated we actually detected that in cloudwatch because we were noticing",
    "start": "1452080",
    "end": "1458640"
  },
  {
    "text": "that cloudwatch had hundreds and hundreds of the same log over and over again um so that was um",
    "start": "1458640",
    "end": "1467279"
  },
  {
    "text": "actually tracking down where that came from did involve looking at the fluency logs the we in the logs it was just saying that",
    "start": "1467279",
    "end": "1472880"
  },
  {
    "text": "there were partial failures running to elasticsearch and i actually went and looked at the elasticsearch plug-in code and",
    "start": "1472880",
    "end": "1479039"
  },
  {
    "text": "eventually found that it was documented in the readme and we'd missed it um but the initial way we found it was",
    "start": "1479039",
    "end": "1484640"
  },
  {
    "text": "just seeing that there were duplicate clocks in cloudwatch um so how did you handle this",
    "start": "1484640",
    "end": "1491039"
  },
  {
    "text": "configuration that caused uh duplicates within fluentd due to partial success requests",
    "start": "1491039",
    "end": "1497440"
  },
  {
    "text": "um in the end we handled it by completely changing and changing the architecture as i've",
    "start": "1497440",
    "end": "1503600"
  },
  {
    "text": "described in the talk um but the short-term fix is um the elasticsearch plug-in will re-emit those",
    "start": "1503600",
    "end": "1510240"
  },
  {
    "text": "logs with a particular um label and you can match on that label and just throw them away if you the short",
    "start": "1510240",
    "end": "1517679"
  },
  {
    "text": "term fix would just just throw them away so that they stopped being duplicated uh but ideally you're supposed to handle",
    "start": "1517679",
    "end": "1524720"
  },
  {
    "text": "those three emissions properly um this one i'm not entirely sure what",
    "start": "1524720",
    "end": "1531039"
  },
  {
    "text": "it's um a difference between fluent d fluid but",
    "start": "1531039",
    "end": "1536400"
  },
  {
    "text": "with kafka integration um i assume that's asking about the difference between the two architectures",
    "start": "1536400",
    "end": "1542799"
  },
  {
    "text": "um with the fluency forwarding protocol fluent bit sends log messages directly",
    "start": "1542799",
    "end": "1548240"
  },
  {
    "text": "to fluency over a tcp connection whereas with kafka",
    "start": "1548240",
    "end": "1554000"
  },
  {
    "text": "well it's still going on for tcp connection but kafka isn't doing anything smart with it um so it's much less likely to break",
    "start": "1554559",
    "end": "1562480"
  },
  {
    "text": "um is is there a way to know maybe looking",
    "start": "1562480",
    "end": "1568080"
  },
  {
    "text": "into the code if an output plug-in does or does not support buffering yes there is",
    "start": "1568080",
    "end": "1573279"
  },
  {
    "text": "um at the very least you can look at the code there are probably other ways um but if you look at the code they will",
    "start": "1573279",
    "end": "1581039"
  },
  {
    "text": "there are two methods that can implement there's one called processor one called right",
    "start": "1581039",
    "end": "1586159"
  },
  {
    "text": "one of those is buffered and one of them is unbuffered and i don't remember which ones which but yes you can easily see",
    "start": "1586159",
    "end": "1591200"
  },
  {
    "text": "from looking at the code um which ones it implements",
    "start": "1591200",
    "end": "1596840"
  },
  {
    "text": "um this next one is a good question um how many logs can you ingest and consume um with this solution",
    "start": "1596840",
    "end": "1604400"
  },
  {
    "text": "so far we um so far it's handled all of the log",
    "start": "1604400",
    "end": "1610000"
  },
  {
    "text": "volume that we've thrown at it so i don't know what the upper limit is but um it's",
    "start": "1610000",
    "end": "1616480"
  },
  {
    "text": "higher than the most logs that we've had to try to use it i can get you",
    "start": "1616480",
    "end": "1623120"
  },
  {
    "text": "better numbers if you want to ask that one again in the slack channel after the talk i can take a look and",
    "start": "1623120",
    "end": "1628400"
  },
  {
    "text": "um give you a more concrete answer to that um this next one why don't we use",
    "start": "1628400",
    "end": "1635279"
  },
  {
    "text": "fluentd memory buffer instead of unbuffered mode because we want to make sure that uh if fluentd",
    "start": "1635279",
    "end": "1642000"
  },
  {
    "text": "crashes while something is in its buffer it gets properly flushed uh sorry it gets it it",
    "start": "1642000",
    "end": "1649039"
  },
  {
    "text": "doesn't we don't lose those log messages so um what we were doing previously was using disk buffering",
    "start": "1649039",
    "end": "1654880"
  },
  {
    "text": "and we um we actually um had that on an ebs volume attached to the instance so that",
    "start": "1654880",
    "end": "1661760"
  },
  {
    "text": "if the instance died completely we still had that ebs volume with the buffer so we could recover those locks",
    "start": "1661760",
    "end": "1666799"
  },
  {
    "text": "with a memory buffer you don't even have that option if if the fluency process dies while there are logs in the buffer",
    "start": "1666799",
    "end": "1672320"
  },
  {
    "text": "you've just lost those um lost those logs forever so",
    "start": "1672320",
    "end": "1680000"
  },
  {
    "text": "the advantage of using unbuffered mode is um that the input plug-in",
    "start": "1680000",
    "end": "1685360"
  },
  {
    "text": "the kafka input plug-in won't acknowledge that it's written sorry that's read those messages from kafka until the output plug-in has",
    "start": "1685360",
    "end": "1691679"
  },
  {
    "text": "actually written them um so it guarantees that we don't lose lots",
    "start": "1691679",
    "end": "1697279"
  },
  {
    "text": "um did we consider apache pulsar instead of",
    "start": "1697279",
    "end": "1702480"
  },
  {
    "text": "kafka uh no to be perfectly honest i've never heard of apache pulsar before",
    "start": "1702480",
    "end": "1707919"
  },
  {
    "text": "but one of the reasons we used kafka is that we're also using it in our application so it made sense to",
    "start": "1707919",
    "end": "1714080"
  },
  {
    "text": "use technology we were already familiar with um",
    "start": "1714080",
    "end": "1719039"
  },
  {
    "text": "is peripera open source uh no um no slash not yet as i said i can't",
    "start": "1723360",
    "end": "1731039"
  },
  {
    "text": "make any promises at the moment but it's one of the components that we're looking at the possibility of open",
    "start": "1731039",
    "end": "1736080"
  },
  {
    "text": "sourcing did",
    "start": "1736080",
    "end": "1741039"
  },
  {
    "text": "did you ever encounter missing logs in journal d um i'm not sure what",
    "start": "1742080",
    "end": "1749360"
  },
  {
    "text": "so we journal d is kind of our canonical that's what we treat as a",
    "start": "1749360",
    "end": "1755600"
  },
  {
    "text": "canonical source of logs to export so that's in a way the definition of a",
    "start": "1755600",
    "end": "1760720"
  },
  {
    "text": "log message existing is going into journal d um [Music]",
    "start": "1760720",
    "end": "1765760"
  },
  {
    "text": "i can't say that we've ever been encountered a situation where we've",
    "start": "1765760",
    "end": "1772320"
  },
  {
    "text": "um had a we found a lot message that we thought should be in journal d but isn't uh the only thing is if",
    "start": "1772320",
    "end": "1780399"
  },
  {
    "text": "journal d has limited retention so if pero pera falls behind it can miss log messages if the journal",
    "start": "1780399",
    "end": "1786159"
  },
  {
    "text": "gets rotated um but that's a matter of um configuration first of all power repair doesn't tend",
    "start": "1786159",
    "end": "1792320"
  },
  {
    "text": "to fall behind often and second of all you can adjust the the general rotation if you need to",
    "start": "1792320",
    "end": "1800398"
  },
  {
    "text": "so how many people do we have on this team um it's a good question",
    "start": "1801919",
    "end": "1809279"
  },
  {
    "text": "the team's been we've shuffled a bit in terms of people actually working on this",
    "start": "1809279",
    "end": "1814399"
  },
  {
    "text": "i'd say um two people who have worked on it in any significant",
    "start": "1814399",
    "end": "1820840"
  },
  {
    "text": "capacity",
    "start": "1820840",
    "end": "1823840"
  },
  {
    "text": "um did we evaluate using file b as log shipper no to be honest i",
    "start": "1827120",
    "end": "1833600"
  },
  {
    "text": "haven't that's something i haven't heard of either",
    "start": "1833600",
    "end": "1838880"
  },
  {
    "text": "so this next one is an interesting question what were the red flags before deciding to improve the solution during",
    "start": "1842720",
    "end": "1848720"
  },
  {
    "text": "the various iterations [Music]",
    "start": "1848720",
    "end": "1855440"
  },
  {
    "text": "that's kind of a really broad question [Music]",
    "start": "1855440",
    "end": "1862000"
  },
  {
    "text": "i would i'd say that the most important ones i described during the talks so um",
    "start": "1862000",
    "end": "1870080"
  },
  {
    "text": "clearly the the initial problem that led to this restructuring is um a problem with the problem that was",
    "start": "1870159",
    "end": "1878000"
  },
  {
    "text": "caused by um elasticsearch having problems and backing up throughout the",
    "start": "1878000",
    "end": "1883919"
  },
  {
    "text": "the stack um all of those issues were kind of red flags none of them by",
    "start": "1883919",
    "end": "1889840"
  },
  {
    "text": "themselves was very serious but all put together they kind of convinced us that we had to",
    "start": "1889840",
    "end": "1894880"
  },
  {
    "text": "um had to change something",
    "start": "1894880",
    "end": "1898640"
  },
  {
    "text": "next question is we show the k stream why not use a fluency source and domestic such sync",
    "start": "1903440",
    "end": "1910320"
  },
  {
    "text": "connector um we could do that and we we were doing that we",
    "start": "1910320",
    "end": "1915760"
  },
  {
    "text": "the reason why we stopped doing that was um as i said there are problems with the ruby kafka",
    "start": "1915760",
    "end": "1921360"
  },
  {
    "text": "library that um that make it difficult to consume batches of",
    "start": "1921360",
    "end": "1926799"
  },
  {
    "text": "messages effectively you can either have really big batches and very infrequent heartbeats so that",
    "start": "1926799",
    "end": "1933519"
  },
  {
    "text": "it takes a long time to recover when something goes wrong or really small batches and um really short heartbeats so you",
    "start": "1933519",
    "end": "1940640"
  },
  {
    "text": "recover quickly if things go wrong but you end up doing a lot of api requests such which to be honest in the case of",
    "start": "1940640",
    "end": "1946159"
  },
  {
    "text": "elasticsearch was not that big of an issue it was more cloud watch because of its api limits but we decided",
    "start": "1946159",
    "end": "1951840"
  },
  {
    "text": "to just go with tougher streams for everything",
    "start": "1951840",
    "end": "1955840"
  },
  {
    "text": "so this question is asking did we have a particular config for the topics in",
    "start": "1959120",
    "end": "1966480"
  },
  {
    "text": "kafka um we currently we have 24-hour retention which is more than enough we always process those",
    "start": "1966480",
    "end": "1973840"
  },
  {
    "text": "logs in a timely fashion replication factor i think is three or",
    "start": "1973840",
    "end": "1979279"
  },
  {
    "text": "so it's a fairly standard configuration",
    "start": "1979279",
    "end": "1986158"
  },
  {
    "text": "someone's asking uh can can i explain what kafka does um it's well as i said it's a",
    "start": "1988559",
    "end": "1995039"
  },
  {
    "text": "distributed um messaging broker so you it you essentially have one thing that writes messages to",
    "start": "1995039",
    "end": "2001519"
  },
  {
    "text": "it and another thing that reads messages from it and the whole point of kafka is to be very reliable and distributed",
    "start": "2001519",
    "end": "2007120"
  },
  {
    "text": "and fault-tolerant so there's obviously more to it than just reading and writing messages the idea is that",
    "start": "2007120",
    "end": "2012640"
  },
  {
    "text": "it should basically never go away because it can survive all kinds of failure modes",
    "start": "2012640",
    "end": "2017840"
  },
  {
    "text": "that's it in a nutshell any more than that i think it's out of scope but there is excellent documentation on the kafka website if",
    "start": "2017840",
    "end": "2024320"
  },
  {
    "text": "you want to read that",
    "start": "2024320",
    "end": "2033840"
  },
  {
    "text": "um next question is did we think of using flink over fluently when it comes to stream processing",
    "start": "2035200",
    "end": "2041440"
  },
  {
    "text": "uh no i also i haven't heard of flink to be honest um we did i did look briefly at um",
    "start": "2041440",
    "end": "2049839"
  },
  {
    "text": "i'm completely drawing a blank now the the um there is another tool that i looked at",
    "start": "2050399",
    "end": "2055839"
  },
  {
    "text": "that i can't remember the name of but um in the in the end it was um",
    "start": "2055839",
    "end": "2062320"
  },
  {
    "text": "fluently seemed to be the thing that um had all the plugins that we wanted for",
    "start": "2062320",
    "end": "2068240"
  },
  {
    "text": "filtering and and so forth um and i mean it would have worked well if",
    "start": "2068240",
    "end": "2075919"
  },
  {
    "text": "it hadn't been for the the problems that we had reading messages from kafka in a reasonable fashion but",
    "start": "2075919",
    "end": "2081760"
  },
  {
    "text": "um in the end the kafka connects and kafka streams turned out to be",
    "start": "2081760",
    "end": "2089200"
  },
  {
    "text": "i think the best fit partly because we're already using kafka we might as well use the um the methods kafka provides for",
    "start": "2089200",
    "end": "2096720"
  },
  {
    "text": "this stuff um i mean that's all the questions for now i think",
    "start": "2096720",
    "end": "2102880"
  },
  {
    "text": "well thanks very much for coming thanks very much for all the questions and yep please join in this lac channel",
    "start": "2102880",
    "end": "2109839"
  },
  {
    "text": "that's on your screen now bye",
    "start": "2109839",
    "end": "2115680"
  }
]