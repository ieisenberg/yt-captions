[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "text": "hi my name is Ken and in this webinar",
    "start": "2080",
    "end": "5160"
  },
  {
    "text": "we're going to talk about uh one of the",
    "start": "5160",
    "end": "7160"
  },
  {
    "text": "most popular topics right now is working",
    "start": "7160",
    "end": "9880"
  },
  {
    "text": "with ai ai models working with",
    "start": "9880",
    "end": "13519"
  },
  {
    "text": "gpts and I'm going to focus on the",
    "start": "13519",
    "end": "16920"
  },
  {
    "text": "second part of working with these which",
    "start": "16920",
    "end": "18600"
  },
  {
    "text": "is how do we get them up and running",
    "start": "18600",
    "end": "20800"
  },
  {
    "text": "part of our applications and rolled out",
    "start": "20800",
    "end": "23519"
  },
  {
    "text": "into production and there's a lot of",
    "start": "23519",
    "end": "25640"
  },
  {
    "text": "challenges in getting through this",
    "start": "25640",
    "end": "27320"
  },
  {
    "text": "process and I'm going to focus on uh",
    "start": "27320",
    "end": "30039"
  },
  {
    "text": "primarily the the end part of that",
    "start": "30039",
    "end": "32078"
  },
  {
    "text": "process I've got some models I want to",
    "start": "32079",
    "end": "34320"
  },
  {
    "text": "get them running in production how do I",
    "start": "34320",
    "end": "36120"
  },
  {
    "text": "do that and how can I figure out if",
    "start": "36120",
    "end": "38000"
  },
  {
    "text": "things are working well so you may have",
    "start": "38000",
    "end": "41640"
  },
  {
    "start": "40000",
    "end": "75000"
  },
  {
    "text": "noticed in March 2024 a great paper came",
    "start": "41640",
    "end": "45200"
  },
  {
    "text": "out from cncf about Cloud native Ai and",
    "start": "45200",
    "end": "49039"
  },
  {
    "text": "all of the capabilities that are part of",
    "start": "49039",
    "end": "51360"
  },
  {
    "text": "the cncf ecosystem I highly recommend",
    "start": "51360",
    "end": "54600"
  },
  {
    "text": "that you take a look at this paper",
    "start": "54600",
    "end": "56199"
  },
  {
    "text": "there's a ton of great resources in",
    "start": "56199",
    "end": "58559"
  },
  {
    "text": "there and I kind of want to Riff Off of",
    "start": "58559",
    "end": "60320"
  },
  {
    "text": "some of the things that are in there and",
    "start": "60320",
    "end": "63079"
  },
  {
    "text": "especially like I said at the end part",
    "start": "63079",
    "end": "65600"
  },
  {
    "text": "now I've got some ideas and I want to",
    "start": "65600",
    "end": "67720"
  },
  {
    "text": "get them up and running in production",
    "start": "67720",
    "end": "70280"
  },
  {
    "text": "and uh I'll share an open source example",
    "start": "70280",
    "end": "72960"
  },
  {
    "text": "of how I do this uh as part of the",
    "start": "72960",
    "end": "75600"
  },
  {
    "start": "75000",
    "end": "102000"
  },
  {
    "text": "process so for a little bit about me I'm",
    "start": "75600",
    "end": "77600"
  },
  {
    "text": "one of the co-founders and CEO of speed",
    "start": "77600",
    "end": "80280"
  },
  {
    "text": "scale we're a cncf member and I've",
    "start": "80280",
    "end": "83759"
  },
  {
    "text": "presented at cubec con and we've",
    "start": "83759",
    "end": "85439"
  },
  {
    "text": "attended cubec con for a number of years",
    "start": "85439",
    "end": "88280"
  },
  {
    "text": "and we primarily work with customers who",
    "start": "88280",
    "end": "90720"
  },
  {
    "text": "have workloads that are in kubernetes",
    "start": "90720",
    "end": "92560"
  },
  {
    "text": "environments and trying to figure out",
    "start": "92560",
    "end": "95360"
  },
  {
    "text": "how to improve the performance of their",
    "start": "95360",
    "end": "97600"
  },
  {
    "text": "own code and you might say hey Ken what",
    "start": "97600",
    "end": "100040"
  },
  {
    "text": "are your own personal qualifications",
    "start": "100040",
    "end": "102240"
  },
  {
    "start": "102000",
    "end": "160000"
  },
  {
    "text": "well I consider myself a prompt engineer",
    "start": "102240",
    "end": "105200"
  },
  {
    "text": "kind of like everyone nowadays I've",
    "start": "105200",
    "end": "107479"
  },
  {
    "text": "familiarized myself with working with a",
    "start": "107479",
    "end": "109479"
  },
  {
    "text": "number of the different AI tools you can",
    "start": "109479",
    "end": "111479"
  },
  {
    "text": "see here from perplexity where they",
    "start": "111479",
    "end": "114040"
  },
  {
    "text": "actually have a lab by the way you can",
    "start": "114040",
    "end": "115560"
  },
  {
    "text": "go and use and test out a bunch of the",
    "start": "115560",
    "end": "117880"
  },
  {
    "text": "different models chat GP T where it's",
    "start": "117880",
    "end": "121640"
  },
  {
    "text": "pretty well known from open AI as one of",
    "start": "121640",
    "end": "124320"
  },
  {
    "text": "the first really easy to ous models you",
    "start": "124320",
    "end": "126840"
  },
  {
    "text": "can go and start to ask it questions and",
    "start": "126840",
    "end": "130520"
  },
  {
    "text": "that kind of thing as well as tools like",
    "start": "130520",
    "end": "133360"
  },
  {
    "text": "anthropic and uh you know Google's",
    "start": "133360",
    "end": "135879"
  },
  {
    "text": "models there's a ton of different things",
    "start": "135879",
    "end": "138080"
  },
  {
    "text": "and uh a lot of these offer a free tier",
    "start": "138080",
    "end": "140519"
  },
  {
    "text": "you can sign up and easily start using",
    "start": "140519",
    "end": "143040"
  },
  {
    "text": "and working with them but in all",
    "start": "143040",
    "end": "144720"
  },
  {
    "text": "seriousness I've spent most of my career",
    "start": "144720",
    "end": "147440"
  },
  {
    "text": "in the application performance area",
    "start": "147440",
    "end": "149920"
  },
  {
    "text": "helping companies figure out why is this",
    "start": "149920",
    "end": "152800"
  },
  {
    "text": "running slow where are the slow Parts",
    "start": "152800",
    "end": "154680"
  },
  {
    "text": "how can we improve it how can we load",
    "start": "154680",
    "end": "156760"
  },
  {
    "text": "test and validate that things are",
    "start": "156760",
    "end": "158319"
  },
  {
    "text": "running better so uh one of the things I",
    "start": "158319",
    "end": "162720"
  },
  {
    "start": "160000",
    "end": "222000"
  },
  {
    "text": "definitely recommend you drill into and",
    "start": "162720",
    "end": "164879"
  },
  {
    "text": "do some research on is hugging face they",
    "start": "164879",
    "end": "167760"
  },
  {
    "text": "have a free accounts you can go and sign",
    "start": "167760",
    "end": "170200"
  },
  {
    "text": "up and you'll find really quickly",
    "start": "170200",
    "end": "172400"
  },
  {
    "text": "there's hundreds of thousands of models",
    "start": "172400",
    "end": "175000"
  },
  {
    "text": "that are already out there this is a",
    "start": "175000",
    "end": "176720"
  },
  {
    "text": "screenshot from the uh this the the",
    "start": "176720",
    "end": "180120"
  },
  {
    "text": "leaderboard where they're saying hey",
    "start": "180120",
    "end": "182519"
  },
  {
    "text": "these are some of the best models for a",
    "start": "182519",
    "end": "185599"
  },
  {
    "text": "variety of different tasks and you can",
    "start": "185599",
    "end": "187480"
  },
  {
    "text": "see how fast this space is moving so",
    "start": "187480",
    "end": "190440"
  },
  {
    "text": "just in the past uh six to eight months",
    "start": "190440",
    "end": "193879"
  },
  {
    "text": "things have improved from 50% to over",
    "start": "193879",
    "end": "196239"
  },
  {
    "text": "80% and getting close to human levels of",
    "start": "196239",
    "end": "199120"
  },
  {
    "text": "intelligence obviously you want to take",
    "start": "199120",
    "end": "201159"
  },
  {
    "text": "advantage of this and say what gen AI",
    "start": "201159",
    "end": "204200"
  },
  {
    "text": "capabilities can I take and add to my",
    "start": "204200",
    "end": "206760"
  },
  {
    "text": "own application and I I recommend and",
    "start": "206760",
    "end": "210480"
  },
  {
    "text": "take a look at some of these models",
    "start": "210480",
    "end": "212040"
  },
  {
    "text": "these were the trending models at this",
    "start": "212040",
    "end": "213720"
  },
  {
    "text": "time in kind of April 2024 when I'm uh",
    "start": "213720",
    "end": "216480"
  },
  {
    "text": "doing this webinar but it's always",
    "start": "216480",
    "end": "218200"
  },
  {
    "text": "changing over time so you want to have",
    "start": "218200",
    "end": "220480"
  },
  {
    "text": "uh flexibility in how you work with",
    "start": "220480",
    "end": "222439"
  },
  {
    "text": "these so when you start working with AI",
    "start": "222439",
    "end": "226920"
  },
  {
    "text": "models and you get some feedback and you",
    "start": "226920",
    "end": "229480"
  },
  {
    "text": "go okay I got some ideas I want to add a",
    "start": "229480",
    "end": "231680"
  },
  {
    "text": "recommendation system I want to add some",
    "start": "231680",
    "end": "233760"
  },
  {
    "text": "generative capabilities to my product",
    "start": "233760",
    "end": "236159"
  },
  {
    "text": "you're going to find there's a there's",
    "start": "236159",
    "end": "237680"
  },
  {
    "text": "actually a pretty long process for doing",
    "start": "237680",
    "end": "240120"
  },
  {
    "text": "this kind of thing from making sure the",
    "start": "240120",
    "end": "242239"
  },
  {
    "text": "data you give the models is really clean",
    "start": "242239",
    "end": "245040"
  },
  {
    "text": "and um doesn't have the wrong",
    "start": "245040",
    "end": "246959"
  },
  {
    "text": "information in it training the models is",
    "start": "246959",
    "end": "249560"
  },
  {
    "text": "where almost all of the information",
    "start": "249560",
    "end": "252000"
  },
  {
    "text": "you'll find about AI is about is about",
    "start": "252000",
    "end": "254519"
  },
  {
    "text": "training and I'm going to cover that",
    "start": "254519",
    "end": "256400"
  },
  {
    "text": "real briefly there's not as much about",
    "start": "256400",
    "end": "258880"
  },
  {
    "text": "model serving or you might hear it",
    "start": "258880",
    "end": "260440"
  },
  {
    "text": "called inference which just means",
    "start": "260440",
    "end": "262759"
  },
  {
    "text": "running it making sure that it's it",
    "start": "262759",
    "end": "265040"
  },
  {
    "text": "stays up and running and that kind of",
    "start": "265040",
    "end": "266560"
  },
  {
    "text": "thing I'm going to focus a lot in this",
    "start": "266560",
    "end": "268759"
  },
  {
    "text": "webinar about model serving and of",
    "start": "268759",
    "end": "271240"
  },
  {
    "text": "course once you've got something it's up",
    "start": "271240",
    "end": "273120"
  },
  {
    "text": "and running is it running properly is it",
    "start": "273120",
    "end": "275600"
  },
  {
    "text": "crashing is it erroring for people and",
    "start": "275600",
    "end": "277759"
  },
  {
    "text": "there's new ways to implement monitoring",
    "start": "277759",
    "end": "280520"
  },
  {
    "text": "observability for these kinds of uh AI",
    "start": "280520",
    "end": "284639"
  },
  {
    "start": "283000",
    "end": "348000"
  },
  {
    "text": "models so drilling into each one of",
    "start": "284639",
    "end": "287639"
  },
  {
    "text": "these data preparation is super critical",
    "start": "287639",
    "end": "290680"
  },
  {
    "text": "if you feed garbage data into your",
    "start": "290680",
    "end": "293120"
  },
  {
    "text": "models they're going to give you garbage",
    "start": "293120",
    "end": "295080"
  },
  {
    "text": "results this is in a couple different",
    "start": "295080",
    "end": "297600"
  },
  {
    "text": "areas so one is the mo the data you use",
    "start": "297600",
    "end": "300240"
  },
  {
    "text": "to train the model and you obviously",
    "start": "300240",
    "end": "302560"
  },
  {
    "text": "don't want to train it on bad data wrong",
    "start": "302560",
    "end": "304320"
  },
  {
    "text": "responses that kind of thing but also",
    "start": "304320",
    "end": "306360"
  },
  {
    "text": "the way you prompt the model so take",
    "start": "306360",
    "end": "309039"
  },
  {
    "text": "time to test out a bunch of different",
    "start": "309039",
    "end": "311240"
  },
  {
    "text": "prompts try different variations on how",
    "start": "311240",
    "end": "313880"
  },
  {
    "text": "you uh send data to it so that you can",
    "start": "313880",
    "end": "316039"
  },
  {
    "text": "get the best kind of results I highly",
    "start": "316039",
    "end": "318680"
  },
  {
    "text": "recommend researching a capability",
    "start": "318680",
    "end": "320360"
  },
  {
    "text": "called rag where you take an",
    "start": "320360",
    "end": "322240"
  },
  {
    "text": "off-the-shelf model that already is good",
    "start": "322240",
    "end": "325520"
  },
  {
    "text": "at a variety of different tasks and you",
    "start": "325520",
    "end": "327759"
  },
  {
    "text": "augment it with your own proprietary",
    "start": "327759",
    "end": "330880"
  },
  {
    "text": "data so this is kind of a sweet spot",
    "start": "330880",
    "end": "333479"
  },
  {
    "text": "where you don't have to take the huge",
    "start": "333479",
    "end": "335400"
  },
  {
    "text": "expense of training the model yourself",
    "start": "335400",
    "end": "337319"
  },
  {
    "text": "but you can take something that already",
    "start": "337319",
    "end": "338720"
  },
  {
    "text": "works really well and add your own",
    "start": "338720",
    "end": "341600"
  },
  {
    "text": "responses to it this can help you cut",
    "start": "341600",
    "end": "344039"
  },
  {
    "text": "down on hallucination and weird",
    "start": "344039",
    "end": "345600"
  },
  {
    "text": "responses that uh come out of the middle",
    "start": "345600",
    "end": "347479"
  },
  {
    "text": "of nowhere so model training this is",
    "start": "347479",
    "end": "351280"
  },
  {
    "start": "348000",
    "end": "390000"
  },
  {
    "text": "obviously really well known as a big",
    "start": "351280",
    "end": "353319"
  },
  {
    "text": "challenge nowadays everyone in the world",
    "start": "353319",
    "end": "355199"
  },
  {
    "text": "is fighting over the gpus everyone's",
    "start": "355199",
    "end": "357919"
  },
  {
    "text": "trying to get access to these kinds of",
    "start": "357919",
    "end": "360160"
  },
  {
    "text": "things personally I skipped some of this",
    "start": "360160",
    "end": "362840"
  },
  {
    "text": "if there's 500,000 models on hugging",
    "start": "362840",
    "end": "365479"
  },
  {
    "text": "face maybe I don't need to train my own",
    "start": "365479",
    "end": "367680"
  },
  {
    "text": "model from scratch maybe I can take",
    "start": "367680",
    "end": "369639"
  },
  {
    "text": "someone else's model and just rag my",
    "start": "369639",
    "end": "372680"
  },
  {
    "text": "responses into it so this lets you get",
    "start": "372680",
    "end": "375840"
  },
  {
    "text": "something up and running faster I'm all",
    "start": "375840",
    "end": "378120"
  },
  {
    "text": "about moving fast getting an MVP going",
    "start": "378120",
    "end": "380560"
  },
  {
    "text": "and so um I'll be honest I skipped this",
    "start": "380560",
    "end": "383120"
  },
  {
    "text": "whole stage and for most people you",
    "start": "383120",
    "end": "385440"
  },
  {
    "text": "don't need to train your own model from",
    "start": "385440",
    "end": "387440"
  },
  {
    "text": "scratch unless that's your business",
    "start": "387440",
    "end": "390120"
  },
  {
    "text": "uh model serving and getting this thing",
    "start": "390120",
    "end": "394160"
  },
  {
    "text": "up and running you've got to figure out",
    "start": "394160",
    "end": "395919"
  },
  {
    "text": "how do I package it how do I build a",
    "start": "395919",
    "end": "397720"
  },
  {
    "text": "container what kind of infrastructure do",
    "start": "397720",
    "end": "400000"
  },
  {
    "text": "I need to use so it turns out running",
    "start": "400000",
    "end": "402360"
  },
  {
    "text": "the models might require a GPU but way",
    "start": "402360",
    "end": "405400"
  },
  {
    "text": "less than training it and you'll quickly",
    "start": "405400",
    "end": "408560"
  },
  {
    "text": "find one of the challenges is you have a",
    "start": "408560",
    "end": "410720"
  },
  {
    "text": "lot of microservices that are calling",
    "start": "410720",
    "end": "412720"
  },
  {
    "text": "your model and everyone has this new",
    "start": "412720",
    "end": "415479"
  },
  {
    "text": "dependency so you can use a technique",
    "start": "415479",
    "end": "417520"
  },
  {
    "text": "called service mocking where",
    "start": "417520",
    "end": "420240"
  },
  {
    "text": "uh you record the responses that come",
    "start": "420240",
    "end": "422599"
  },
  {
    "text": "across this API and you create a mock",
    "start": "422599",
    "end": "426599"
  },
  {
    "text": "which will repeatedly send those same",
    "start": "426599",
    "end": "429199"
  },
  {
    "text": "kind of responses back this is a way you",
    "start": "429199",
    "end": "431639"
  },
  {
    "text": "can provide these mocks to the",
    "start": "431639",
    "end": "433319"
  },
  {
    "text": "development teams so that they get what",
    "start": "433319",
    "end": "436520"
  },
  {
    "text": "looks like realistic responses without",
    "start": "436520",
    "end": "439039"
  },
  {
    "text": "necessarily having to have everyone run",
    "start": "439039",
    "end": "441599"
  },
  {
    "text": "a giant model on their own machine",
    "start": "441599",
    "end": "443199"
  },
  {
    "text": "that's not always uh feasible so I",
    "start": "443199",
    "end": "446319"
  },
  {
    "text": "highly recommend uh that you take a look",
    "start": "446319",
    "end": "448639"
  },
  {
    "text": "at how to do Serv mocking I'll try to",
    "start": "448639",
    "end": "450319"
  },
  {
    "text": "show some examples as part of this as",
    "start": "450319",
    "end": "452319"
  },
  {
    "start": "452000",
    "end": "529000"
  },
  {
    "text": "well and then monitoring and",
    "start": "452319",
    "end": "454080"
  },
  {
    "text": "observability you spent all this time to",
    "start": "454080",
    "end": "456080"
  },
  {
    "text": "get the model into production is it",
    "start": "456080",
    "end": "458199"
  },
  {
    "text": "crashing is it is it running really slow",
    "start": "458199",
    "end": "461520"
  },
  {
    "text": "so um you may be familiar with the SR",
    "start": "461520",
    "end": "465800"
  },
  {
    "text": "golden signals that came out of Google's",
    "start": "465800",
    "end": "467680"
  },
  {
    "text": "SRE handbook they are what is the",
    "start": "467680",
    "end": "469599"
  },
  {
    "text": "latency of this service what is the",
    "start": "469599",
    "end": "471639"
  },
  {
    "text": "throughput that it can handle the the",
    "start": "471639",
    "end": "474360"
  },
  {
    "text": "saturation how much infrastructure is",
    "start": "474360",
    "end": "476360"
  },
  {
    "text": "required to run it and the fourth one",
    "start": "476360",
    "end": "478840"
  },
  {
    "text": "being the errors is it even responding",
    "start": "478840",
    "end": "481560"
  },
  {
    "text": "by the way errors sometimes have really",
    "start": "481560",
    "end": "483520"
  },
  {
    "text": "good performance that responds really",
    "start": "483520",
    "end": "485599"
  },
  {
    "text": "fast with an error message is not that",
    "start": "485599",
    "end": "487560"
  },
  {
    "text": "helpful in addition for AI models",
    "start": "487560",
    "end": "490199"
  },
  {
    "text": "there's a couple of different uh things",
    "start": "490199",
    "end": "492000"
  },
  {
    "text": "that you're going to want to include are",
    "start": "492000",
    "end": "493919"
  },
  {
    "text": "the answers accurate so uh you may",
    "start": "493919",
    "end": "497639"
  },
  {
    "text": "haveit returning really inaccurate",
    "start": "497639",
    "end": "499599"
  },
  {
    "text": "answers this is actually the most common",
    "start": "499599",
    "end": "501280"
  },
  {
    "text": "thing you see coming from the AI folks",
    "start": "501280",
    "end": "503159"
  },
  {
    "text": "when they talk about performance they're",
    "start": "503159",
    "end": "505120"
  },
  {
    "text": "actually talking about prediction",
    "start": "505120",
    "end": "506599"
  },
  {
    "text": "accuracy that's great and another one",
    "start": "506599",
    "end": "509039"
  },
  {
    "text": "you want include is how many tokens are",
    "start": "509039",
    "end": "511319"
  },
  {
    "text": "used there's a good correlation between",
    "start": "511319",
    "end": "513240"
  },
  {
    "text": "the more tokens the higher the latency",
    "start": "513240",
    "end": "516279"
  },
  {
    "text": "so you're going to want to work on that",
    "start": "516279",
    "end": "518518"
  },
  {
    "text": "and Tinker with it what's the smaller",
    "start": "518519",
    "end": "521080"
  },
  {
    "text": "number of tokens that you can use for",
    "start": "521080",
    "end": "523039"
  },
  {
    "text": "your query so that you can get good",
    "start": "523039",
    "end": "525440"
  },
  {
    "text": "latency uh you know without breaking the",
    "start": "525440",
    "end": "528080"
  },
  {
    "text": "bank so I'm all about applied",
    "start": "528080",
    "end": "531600"
  },
  {
    "start": "529000",
    "end": "588000"
  },
  {
    "text": "engineering actually building and",
    "start": "531600",
    "end": "533560"
  },
  {
    "text": "running these things so uh what I did",
    "start": "533560",
    "end": "536880"
  },
  {
    "text": "for myself is I designed an experiment",
    "start": "536880",
    "end": "539600"
  },
  {
    "text": "let me go and get a kubernetes cluster",
    "start": "539600",
    "end": "542519"
  },
  {
    "text": "stand it up and start to deploy some of",
    "start": "542519",
    "end": "544680"
  },
  {
    "text": "these things I selected from hugging",
    "start": "544680",
    "end": "547000"
  },
  {
    "text": "face a container called TGI which I'll",
    "start": "547000",
    "end": "549760"
  },
  {
    "text": "show you it it is a way that you can run",
    "start": "549760",
    "end": "552839"
  },
  {
    "text": "these models and uh it's just wrapped in",
    "start": "552839",
    "end": "555920"
  },
  {
    "text": "a Docker container so I put that in a",
    "start": "555920",
    "end": "557720"
  },
  {
    "text": "kubernetes",
    "start": "557720",
    "end": "559000"
  },
  {
    "text": "manifest you need a cluster that you can",
    "start": "559000",
    "end": "561320"
  },
  {
    "text": "run these things in that has a GPU",
    "start": "561320",
    "end": "564120"
  },
  {
    "text": "setting up your node groups takes a lot",
    "start": "564120",
    "end": "565800"
  },
  {
    "text": "of time uh for starters I used an",
    "start": "565800",
    "end": "568079"
  },
  {
    "text": "autopilot cluster that that way it says",
    "start": "568079",
    "end": "570240"
  },
  {
    "text": "hey this this workload needs a GPU it",
    "start": "570240",
    "end": "572519"
  },
  {
    "text": "spins up a GPU node the other workloads",
    "start": "572519",
    "end": "574880"
  },
  {
    "text": "may not and it spins up a regular uh arm",
    "start": "574880",
    "end": "578760"
  },
  {
    "text": "or x86 whatever kind of node that's",
    "start": "578760",
    "end": "582279"
  },
  {
    "text": "required again this helps you get things",
    "start": "582279",
    "end": "584640"
  },
  {
    "text": "up and running experiment testing out",
    "start": "584640",
    "end": "586519"
  },
  {
    "text": "the model so you can get feedback so I",
    "start": "586519",
    "end": "589480"
  },
  {
    "start": "588000",
    "end": "644000"
  },
  {
    "text": "have an open- source project that I'll",
    "start": "589480",
    "end": "592320"
  },
  {
    "text": "share here and uh it's got a couple of",
    "start": "592320",
    "end": "595480"
  },
  {
    "text": "different containers there is a react",
    "start": "595480",
    "end": "598560"
  },
  {
    "text": "user interface that's being served off",
    "start": "598560",
    "end": "600920"
  },
  {
    "text": "ofix there's an Ingress into the cluster",
    "start": "600920",
    "end": "604279"
  },
  {
    "text": "so you can see it from the browser uh",
    "start": "604279",
    "end": "606600"
  },
  {
    "text": "the backend API is written in node",
    "start": "606600",
    "end": "609600"
  },
  {
    "text": "nodejs a lot of the examples are either",
    "start": "609600",
    "end": "612040"
  },
  {
    "text": "in python or nodejs I like working with",
    "start": "612040",
    "end": "615120"
  },
  {
    "text": "uh node now I've got kind of the same",
    "start": "615120",
    "end": "617640"
  },
  {
    "text": "node code for my front end and for my",
    "start": "617640",
    "end": "620640"
  },
  {
    "text": "API and then like I said the",
    "start": "620640",
    "end": "624399"
  },
  {
    "text": "TGI is coming from hugging face so",
    "start": "624399",
    "end": "627760"
  },
  {
    "text": "there's an existing container that we",
    "start": "627760",
    "end": "629720"
  },
  {
    "text": "can take advantage of and I'll show you",
    "start": "629720",
    "end": "631880"
  },
  {
    "text": "how you flag the um manifest to say hey",
    "start": "631880",
    "end": "635440"
  },
  {
    "text": "this needs an Nvidia GPU so let's jump",
    "start": "635440",
    "end": "638800"
  },
  {
    "text": "in and take a look at this and U I'll",
    "start": "638800",
    "end": "641320"
  },
  {
    "text": "show you how you can run these in your",
    "start": "641320",
    "end": "642720"
  },
  {
    "text": "own",
    "start": "642720",
    "end": "644680"
  },
  {
    "start": "644000",
    "end": "698000"
  },
  {
    "text": "cluster okay so uh this is the",
    "start": "644680",
    "end": "647360"
  },
  {
    "text": "documentation page for the TGI project",
    "start": "647360",
    "end": "650639"
  },
  {
    "text": "from hugging face it's a pretty active",
    "start": "650639",
    "end": "653360"
  },
  {
    "text": "project you can see here thousands of",
    "start": "653360",
    "end": "655639"
  },
  {
    "text": "GitHub stars from hugging face and it's",
    "start": "655639",
    "end": "658839"
  },
  {
    "text": "a great great uh way that you can come",
    "start": "658839",
    "end": "661680"
  },
  {
    "text": "and run these open source llms I highly",
    "start": "661680",
    "end": "664920"
  },
  {
    "text": "recommend it as a way to get started and",
    "start": "664920",
    "end": "667519"
  },
  {
    "text": "get something running there's a uh in",
    "start": "667519",
    "end": "670800"
  },
  {
    "text": "addition to this by the way it it",
    "start": "670800",
    "end": "672959"
  },
  {
    "text": "already has uh capabilities like we",
    "start": "672959",
    "end": "675920"
  },
  {
    "text": "talked about monitoring and",
    "start": "675920",
    "end": "677279"
  },
  {
    "text": "observability and uh it also is set up",
    "start": "677279",
    "end": "680079"
  },
  {
    "text": "for uh server server sent events that's",
    "start": "680079",
    "end": "683040"
  },
  {
    "text": "where you see the responses get streamed",
    "start": "683040",
    "end": "685399"
  },
  {
    "text": "back to the user kind of one token at a",
    "start": "685399",
    "end": "687720"
  },
  {
    "text": "time and uh it can enable you to have",
    "start": "687720",
    "end": "691240"
  },
  {
    "text": "instant data the first part of the",
    "start": "691240",
    "end": "693079"
  },
  {
    "text": "response comes back fast and then the",
    "start": "693079",
    "end": "694920"
  },
  {
    "text": "entire response is sent over time so you",
    "start": "694920",
    "end": "698760"
  },
  {
    "start": "698000",
    "end": "719000"
  },
  {
    "text": "can see by the way uh the details on the",
    "start": "698760",
    "end": "701120"
  },
  {
    "text": "different models that are available from",
    "start": "701120",
    "end": "703399"
  },
  {
    "text": "this so for my own testing I was I've",
    "start": "703399",
    "end": "705360"
  },
  {
    "text": "been using mixol there are so many",
    "start": "705360",
    "end": "708120"
  },
  {
    "text": "models available here that U there's",
    "start": "708120",
    "end": "711600"
  },
  {
    "text": "there's plenty that you're going to be",
    "start": "711600",
    "end": "713480"
  },
  {
    "text": "able to work with and figure out and get",
    "start": "713480",
    "end": "715720"
  },
  {
    "text": "working for your",
    "start": "715720",
    "end": "718040"
  },
  {
    "text": "application the the API on how you call",
    "start": "718040",
    "end": "721120"
  },
  {
    "start": "719000",
    "end": "755000"
  },
  {
    "text": "it is in the current versions which is",
    "start": "721120",
    "end": "724200"
  },
  {
    "text": "the one I'm using it looks like the uh",
    "start": "724200",
    "end": "727440"
  },
  {
    "text": "V1 chat completions end point that has",
    "start": "727440",
    "end": "730720"
  },
  {
    "text": "the same shape as you see from open AI",
    "start": "730720",
    "end": "733720"
  },
  {
    "text": "so this makes it easy so that you can",
    "start": "733720",
    "end": "736519"
  },
  {
    "text": "switch the different uh backends out and",
    "start": "736519",
    "end": "739120"
  },
  {
    "text": "I highly recommend that you test",
    "start": "739120",
    "end": "740959"
  },
  {
    "text": "different back ends what does it happen",
    "start": "740959",
    "end": "742360"
  },
  {
    "text": "if I use chat GPT what happens if I use",
    "start": "742360",
    "end": "745760"
  },
  {
    "text": "um anthropic what if I want to call uh t",
    "start": "745760",
    "end": "749199"
  },
  {
    "text": "GI all of these uh can be done without",
    "start": "749199",
    "end": "752399"
  },
  {
    "text": "having to to rewrite your code So",
    "start": "752399",
    "end": "755040"
  },
  {
    "start": "755000",
    "end": "811000"
  },
  {
    "text": "speaking of the code let me show you my",
    "start": "755040",
    "end": "758399"
  },
  {
    "text": "uh GitHub repo here so this is a really",
    "start": "758399",
    "end": "761480"
  },
  {
    "text": "simple project uh just to show you how",
    "start": "761480",
    "end": "763760"
  },
  {
    "text": "to get things up and running here are",
    "start": "763760",
    "end": "766519"
  },
  {
    "text": "the components that are part of it so",
    "start": "766519",
    "end": "768839"
  },
  {
    "text": "the UI code in the in the UI directory",
    "start": "768839",
    "end": "772959"
  },
  {
    "text": "is a little react gooey okay and uh",
    "start": "772959",
    "end": "777279"
  },
  {
    "text": "it'll it'll get built into a container",
    "start": "777279",
    "end": "780120"
  },
  {
    "text": "the API tier is Express node.js app when",
    "start": "780120",
    "end": "785000"
  },
  {
    "text": "it gets an API request it goes and hits",
    "start": "785000",
    "end": "788120"
  },
  {
    "text": "the backend TGI system and because I",
    "start": "788120",
    "end": "792240"
  },
  {
    "text": "want to keep this information around I",
    "start": "792240",
    "end": "795160"
  },
  {
    "text": "hooked a little database up to it right",
    "start": "795160",
    "end": "797279"
  },
  {
    "text": "now the database is sort of",
    "start": "797279",
    "end": "798800"
  },
  {
    "text": "self-contained inside the container in a",
    "start": "798800",
    "end": "801120"
  },
  {
    "text": "future version I might break this out",
    "start": "801120",
    "end": "803360"
  },
  {
    "text": "but uh I can hold all of the responses",
    "start": "803360",
    "end": "805839"
  },
  {
    "text": "in details like the uh the latency of",
    "start": "805839",
    "end": "808440"
  },
  {
    "text": "the response and how many tokens were",
    "start": "808440",
    "end": "810760"
  },
  {
    "text": "used and then to deploy this I've got a",
    "start": "810760",
    "end": "814360"
  },
  {
    "start": "811000",
    "end": "950000"
  },
  {
    "text": "couple of kubernetes manifests you can",
    "start": "814360",
    "end": "816680"
  },
  {
    "text": "see this is pretty simple there's I'm",
    "start": "816680",
    "end": "818839"
  },
  {
    "text": "using customization and so there's just",
    "start": "818839",
    "end": "820760"
  },
  {
    "text": "three this is very vanilla stuff uh a",
    "start": "820760",
    "end": "824079"
  },
  {
    "text": "service and a deployment but uh this is",
    "start": "824079",
    "end": "826519"
  },
  {
    "text": "the one that's probably the more",
    "start": "826519",
    "end": "827680"
  },
  {
    "text": "interesting how do you run the TGI well",
    "start": "827680",
    "end": "831600"
  },
  {
    "text": "this is this is pretty simple here you",
    "start": "831600",
    "end": "834560"
  },
  {
    "text": "put in the image details I've because",
    "start": "834560",
    "end": "838279"
  },
  {
    "text": "I'm using an auto autopilot cluster I",
    "start": "838279",
    "end": "840519"
  },
  {
    "text": "need to uh Define everything the CPU",
    "start": "840519",
    "end": "843279"
  },
  {
    "text": "memory storage required and I just say",
    "start": "843279",
    "end": "845800"
  },
  {
    "text": "give me one GPU and then you can pass in",
    "start": "845800",
    "end": "849680"
  },
  {
    "text": "the model that you want to use so here",
    "start": "849680",
    "end": "852639"
  },
  {
    "text": "is the specific model uh that I'm",
    "start": "852639",
    "end": "855480"
  },
  {
    "text": "running you might need to put in your",
    "start": "855480",
    "end": "857560"
  },
  {
    "text": "hugging face token so uh that's you've",
    "start": "857560",
    "end": "861160"
  },
  {
    "text": "got to figure that out for yourself set",
    "start": "861160",
    "end": "862639"
  },
  {
    "text": "up a token by the way you can create a",
    "start": "862639",
    "end": "864639"
  },
  {
    "text": "free account and then add this node",
    "start": "864639",
    "end": "867279"
  },
  {
    "text": "selector down here uh if you're running",
    "start": "867279",
    "end": "869680"
  },
  {
    "text": "it in gke which I am so that you make",
    "start": "869680",
    "end": "873399"
  },
  {
    "text": "sure that you get a GPU that's created",
    "start": "873399",
    "end": "877199"
  },
  {
    "text": "as part of this and that's it and when",
    "start": "877199",
    "end": "881079"
  },
  {
    "text": "you go and deploy this it will spin up",
    "start": "881079",
    "end": "884000"
  },
  {
    "text": "in um in your cluster and one thing to",
    "start": "884000",
    "end": "888519"
  },
  {
    "text": "note is it does take a little bit of",
    "start": "888519",
    "end": "890800"
  },
  {
    "text": "time for it to download the model uh I",
    "start": "890800",
    "end": "893480"
  },
  {
    "text": "recommend when you productize this",
    "start": "893480",
    "end": "896279"
  },
  {
    "text": "you're going to want to put um you know",
    "start": "896279",
    "end": "898680"
  },
  {
    "text": "uh storage associated with it because",
    "start": "898680",
    "end": "900800"
  },
  {
    "text": "you can store the model so it doesn't",
    "start": "900800",
    "end": "902120"
  },
  {
    "text": "have to download it that kind of thing",
    "start": "902120",
    "end": "903480"
  },
  {
    "text": "again this is the quick and dirty",
    "start": "903480",
    "end": "905199"
  },
  {
    "text": "fastest way to get something up and",
    "start": "905199",
    "end": "906639"
  },
  {
    "text": "running for proof of concept and uh for",
    "start": "906639",
    "end": "910320"
  },
  {
    "text": "the other components you'll see that uh",
    "start": "910320",
    "end": "913440"
  },
  {
    "text": "I'm I'm building containers for the AI",
    "start": "913440",
    "end": "916120"
  },
  {
    "text": "uh sorry for the API and for the UI then",
    "start": "916120",
    "end": "920480"
  },
  {
    "text": "um these are I'm also building them with",
    "start": "920480",
    "end": "923800"
  },
  {
    "text": "h GitHub actions so again it's a you can",
    "start": "923800",
    "end": "928000"
  },
  {
    "text": "you do this on your free account and",
    "start": "928000",
    "end": "930759"
  },
  {
    "text": "then um put a service in front of it and",
    "start": "930759",
    "end": "934600"
  },
  {
    "text": "then I'm hosting this right now on uh",
    "start": "934600",
    "end": "937720"
  },
  {
    "text": "one of the domains I have traffic",
    "start": "937720",
    "end": "939920"
  },
  {
    "text": "replay. and uh so this this part setting",
    "start": "939920",
    "end": "943600"
  },
  {
    "text": "up the Ingress that's on you if uh how",
    "start": "943600",
    "end": "945800"
  },
  {
    "text": "you want to uh get the interface",
    "start": "945800",
    "end": "948800"
  },
  {
    "text": "deployed so here's what it looks like my",
    "start": "948800",
    "end": "951880"
  },
  {
    "start": "950000",
    "end": "1097000"
  },
  {
    "text": "little uh demo app if you hit my URL by",
    "start": "951880",
    "end": "955560"
  },
  {
    "text": "the way you need to log in as John do",
    "start": "955560",
    "end": "958120"
  },
  {
    "text": "the password is in my repo if you can't",
    "start": "958120",
    "end": "961000"
  },
  {
    "text": "find it just drop me a note or ask me a",
    "start": "961000",
    "end": "964160"
  },
  {
    "text": "question on the GitHub repo so you can",
    "start": "964160",
    "end": "967920"
  },
  {
    "text": "come in and say hey uh generate a brief",
    "start": "967920",
    "end": "971120"
  },
  {
    "text": "poem about um you know kubernetes okay",
    "start": "971120",
    "end": "974959"
  },
  {
    "text": "and give it a number of tokens like a",
    "start": "974959",
    "end": "976720"
  },
  {
    "text": "hint on how how many tokens to use a",
    "start": "976720",
    "end": "980480"
  },
  {
    "text": "small number of tokens like I was saying",
    "start": "980480",
    "end": "982440"
  },
  {
    "text": "earlier should respond a little faster",
    "start": "982440",
    "end": "985160"
  },
  {
    "text": "so uh okay it went and created my uh",
    "start": "985160",
    "end": "989120"
  },
  {
    "text": "poem for me and you can actually see",
    "start": "989120",
    "end": "991800"
  },
  {
    "text": "here the poem's getting cut off so uh 50",
    "start": "991800",
    "end": "995800"
  },
  {
    "text": "tokens is not enough it I I told it how",
    "start": "995800",
    "end": "998600"
  },
  {
    "text": "many tokens to use my prompts required",
    "start": "998600",
    "end": "1001600"
  },
  {
    "text": "16 and uh this took over 3 seconds so",
    "start": "1001600",
    "end": "1005800"
  },
  {
    "text": "let's try another poem and um you know",
    "start": "1005800",
    "end": "1009079"
  },
  {
    "text": "we'll do this again write a poem about",
    "start": "1009079",
    "end": "1011519"
  },
  {
    "text": "kubernetes but we'll give it more tokens",
    "start": "1011519",
    "end": "1013959"
  },
  {
    "text": "and see um how this one works so uh",
    "start": "1013959",
    "end": "1018199"
  },
  {
    "text": "obviously it's going to take a little",
    "start": "1018199",
    "end": "1019519"
  },
  {
    "text": "bit longer because I've given it more",
    "start": "1019519",
    "end": "1021560"
  },
  {
    "text": "tokens and by the way uh 3 seconds is a",
    "start": "1021560",
    "end": "1024918"
  },
  {
    "text": "pretty long response time here this one",
    "start": "1024919",
    "end": "1027160"
  },
  {
    "text": "is not um you know it's synchronous so",
    "start": "1027160",
    "end": "1030240"
  },
  {
    "text": "it is going",
    "start": "1030240",
    "end": "1031400"
  },
  {
    "text": "to uh wait for the entire response to",
    "start": "1031400",
    "end": "1034079"
  },
  {
    "text": "come back that took 12 seconds and uh it",
    "start": "1034079",
    "end": "1037600"
  },
  {
    "text": "actually still it still ran out uh the",
    "start": "1037600",
    "end": "1040720"
  },
  {
    "text": "quality of the poem I will leave up up",
    "start": "1040720",
    "end": "1043120"
  },
  {
    "text": "to you to decide but you can see a",
    "start": "1043120",
    "end": "1045600"
  },
  {
    "text": "really direct correlation here between",
    "start": "1045600",
    "end": "1048280"
  },
  {
    "text": "uh the number of tokens that it uses and",
    "start": "1048280",
    "end": "1052200"
  },
  {
    "text": "um you know the the response time this",
    "start": "1052200",
    "end": "1054520"
  },
  {
    "text": "has not been tuned for performance or",
    "start": "1054520",
    "end": "1057200"
  },
  {
    "text": "anything that is for a later stage we",
    "start": "1057200",
    "end": "1059799"
  },
  {
    "text": "got to start by at least getting",
    "start": "1059799",
    "end": "1061400"
  },
  {
    "text": "visibility into it so let me try you",
    "start": "1061400",
    "end": "1063600"
  },
  {
    "text": "know one more uh poem about kubernetes",
    "start": "1063600",
    "end": "1067720"
  },
  {
    "text": "and we're going to say like go crazy",
    "start": "1067720",
    "end": "1069960"
  },
  {
    "text": "give this a ton of token so",
    "start": "1069960",
    "end": "1072600"
  },
  {
    "text": "25 so now we can see we just got some",
    "start": "1072600",
    "end": "1075919"
  },
  {
    "text": "kind of",
    "start": "1075919",
    "end": "1076960"
  },
  {
    "text": "error and uh",
    "start": "1076960",
    "end": "1079360"
  },
  {
    "text": "this is the most common thing in the",
    "start": "1079360",
    "end": "1080880"
  },
  {
    "text": "world right what something happened in",
    "start": "1080880",
    "end": "1083400"
  },
  {
    "text": "my environment here and I don't know",
    "start": "1083400",
    "end": "1087080"
  },
  {
    "text": "what it is right so this is where um API",
    "start": "1087080",
    "end": "1091840"
  },
  {
    "text": "observability and understanding what's",
    "start": "1091840",
    "end": "1094120"
  },
  {
    "text": "going on uh comes in so I did go ahead",
    "start": "1094120",
    "end": "1097600"
  },
  {
    "start": "1097000",
    "end": "1325000"
  },
  {
    "text": "and hook this up to speed scale so I can",
    "start": "1097600",
    "end": "1100960"
  },
  {
    "text": "see the different calls that are",
    "start": "1100960",
    "end": "1103120"
  },
  {
    "text": "happening in the environment here so I",
    "start": "1103120",
    "end": "1104640"
  },
  {
    "text": "can see you know getting the list of",
    "start": "1104640",
    "end": "1107440"
  },
  {
    "text": "poems uh I got a a 304 response means it",
    "start": "1107440",
    "end": "1110919"
  },
  {
    "text": "hasn't changed here is that uh specific",
    "start": "1110919",
    "end": "1114400"
  },
  {
    "text": "poem that was sent this is exactly how",
    "start": "1114400",
    "end": "1118240"
  },
  {
    "text": "my um the the TGI model",
    "start": "1118240",
    "end": "1121880"
  },
  {
    "text": "responded uh and how long it took and",
    "start": "1121880",
    "end": "1126159"
  },
  {
    "text": "again the the tokens and that kind of",
    "start": "1126159",
    "end": "1128000"
  },
  {
    "text": "thing so um this is the this is the",
    "start": "1128000",
    "end": "1131840"
  },
  {
    "text": "inbound call that went to the API here I",
    "start": "1131840",
    "end": "1135720"
  },
  {
    "text": "can see the outbound call that uh was",
    "start": "1135720",
    "end": "1140640"
  },
  {
    "text": "where uh how TGI responded in the",
    "start": "1140640",
    "end": "1143720"
  },
  {
    "text": "specific details and I can actually see",
    "start": "1143720",
    "end": "1146080"
  },
  {
    "text": "also the prompt that was used very basic",
    "start": "1146080",
    "end": "1150039"
  },
  {
    "text": "uh you know for for this example and",
    "start": "1150039",
    "end": "1152880"
  },
  {
    "text": "then uh you know moving up we can see",
    "start": "1152880",
    "end": "1155080"
  },
  {
    "text": "our one that took 12 seconds and the",
    "start": "1155080",
    "end": "1158039"
  },
  {
    "text": "specific details about that and then um",
    "start": "1158039",
    "end": "1161280"
  },
  {
    "text": "you know interestingly this this other",
    "start": "1161280",
    "end": "1164120"
  },
  {
    "text": "one where it was trying",
    "start": "1164120",
    "end": "1166840"
  },
  {
    "text": "to um trying to to post a uh that that",
    "start": "1166840",
    "end": "1171919"
  },
  {
    "text": "other one we don't know what the error",
    "start": "1171919",
    "end": "1173880"
  },
  {
    "text": "was so we can see here",
    "start": "1173880",
    "end": "1176760"
  },
  {
    "text": "actually uh the backend replied with a",
    "start": "1176760",
    "end": "1180640"
  },
  {
    "text": "422 so this is that this is coming from",
    "start": "1180640",
    "end": "1184679"
  },
  {
    "text": "that hugging face TGI server input",
    "start": "1184679",
    "end": "1187440"
  },
  {
    "text": "validation error you're not allowed to",
    "start": "1187440",
    "end": "1190760"
  },
  {
    "text": "send uh 2500 tokens that's too many so",
    "start": "1190760",
    "end": "1195280"
  },
  {
    "text": "uh this is the kind of thing that is not",
    "start": "1195280",
    "end": "1197559"
  },
  {
    "text": "always obvious to a developer to someone",
    "start": "1197559",
    "end": "1199880"
  },
  {
    "text": "who's working with the system what's",
    "start": "1199880",
    "end": "1201360"
  },
  {
    "text": "going on in the environment so tracking",
    "start": "1201360",
    "end": "1203440"
  },
  {
    "text": "these kinds of details is really",
    "start": "1203440",
    "end": "1205559"
  },
  {
    "text": "important and if as a developer if I",
    "start": "1205559",
    "end": "1209840"
  },
  {
    "text": "want to try to work with these apis",
    "start": "1209840",
    "end": "1212799"
  },
  {
    "text": "again it's kind of hard to get this up",
    "start": "1212799",
    "end": "1214720"
  },
  {
    "text": "and running in my",
    "start": "1214720",
    "end": "1217200"
  },
  {
    "text": "environment so uh using a tool uh like",
    "start": "1217200",
    "end": "1221200"
  },
  {
    "text": "speed scale or another capability for",
    "start": "1221200",
    "end": "1223480"
  },
  {
    "text": "service mocking you can come and save",
    "start": "1223480",
    "end": "1225720"
  },
  {
    "text": "this data uh and simp easily build a",
    "start": "1225720",
    "end": "1229480"
  },
  {
    "text": "mock that will allow you to run this uh",
    "start": "1229480",
    "end": "1233520"
  },
  {
    "text": "um locally on your own machine and you",
    "start": "1233520",
    "end": "1235400"
  },
  {
    "text": "can see the mock responses include the",
    "start": "1235400",
    "end": "1239880"
  },
  {
    "text": "successful responses of you know when uh",
    "start": "1239880",
    "end": "1243679"
  },
  {
    "text": "when it was properly returned as well as",
    "start": "1243679",
    "end": "1245720"
  },
  {
    "text": "the error conditions and this lets",
    "start": "1245720",
    "end": "1250480"
  },
  {
    "text": "developers easily work locally without",
    "start": "1250480",
    "end": "1253960"
  },
  {
    "text": "having to uh deploy Giant gpus and spend",
    "start": "1253960",
    "end": "1257200"
  },
  {
    "text": "a ton of money on infrastructure because",
    "start": "1257200",
    "end": "1259640"
  },
  {
    "text": "these can be run uh just with a single",
    "start": "1259640",
    "end": "1262080"
  },
  {
    "text": "command line um let me show you uh you",
    "start": "1262080",
    "end": "1265120"
  },
  {
    "text": "know as as simple as this can be set up",
    "start": "1265120",
    "end": "1267679"
  },
  {
    "text": "is just run it on your desktop like this",
    "start": "1267679",
    "end": "1270200"
  },
  {
    "text": "with a with a single command go and run",
    "start": "1270200",
    "end": "1272840"
  },
  {
    "text": "this locally on my own machine let me",
    "start": "1272840",
    "end": "1274960"
  },
  {
    "text": "hook my node.js code up to that and",
    "start": "1274960",
    "end": "1278039"
  },
  {
    "text": "develop and iterate so that I don't get",
    "start": "1278039",
    "end": "1280480"
  },
  {
    "text": "that server communication error or",
    "start": "1280480",
    "end": "1282760"
  },
  {
    "text": "whatever kind of thing but I I handle it",
    "start": "1282760",
    "end": "1284799"
  },
  {
    "text": "in a cleaner way and show that response",
    "start": "1284799",
    "end": "1287360"
  },
  {
    "text": "back to the user",
    "start": "1287360",
    "end": "1289279"
  },
  {
    "text": "so this was just a quick overview of",
    "start": "1289279",
    "end": "1293000"
  },
  {
    "text": "some of the capabilities in the cloud",
    "start": "1293000",
    "end": "1295159"
  },
  {
    "text": "native space and um please take a look",
    "start": "1295159",
    "end": "1298320"
  },
  {
    "text": "at my GitHub repo open um you know",
    "start": "1298320",
    "end": "1302159"
  },
  {
    "text": "comments send me some issues and this",
    "start": "1302159",
    "end": "1304000"
  },
  {
    "text": "kind of thing so we can work together I",
    "start": "1304000",
    "end": "1306120"
  },
  {
    "text": "would love a chance to uh help out I'm",
    "start": "1306120",
    "end": "1308000"
  },
  {
    "text": "always interested in what folks are",
    "start": "1308000",
    "end": "1309679"
  },
  {
    "text": "doing around uh Cloud native AI models",
    "start": "1309679",
    "end": "1313200"
  },
  {
    "text": "nowadays and getting them up and running",
    "start": "1313200",
    "end": "1315120"
  },
  {
    "text": "in kubernetes is easy and fast and uh",
    "start": "1315120",
    "end": "1319360"
  },
  {
    "text": "this is definitely something you can",
    "start": "1319360",
    "end": "1320760"
  },
  {
    "text": "check out so if you have any questions",
    "start": "1320760",
    "end": "1322919"
  },
  {
    "text": "feel free to reach out thank you very",
    "start": "1322919",
    "end": "1325159"
  },
  {
    "text": "much",
    "start": "1325159",
    "end": "1327480"
  }
]