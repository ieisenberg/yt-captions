[
  {
    "start": "0",
    "end": "48000"
  },
  {
    "text": "hello and welcome to calico vpp all you can eat networking uh",
    "start": "1199",
    "end": "7359"
  },
  {
    "text": "my name is casey and i'm a software engineer at tigera where i work as a core developer on",
    "start": "7359",
    "end": "14080"
  },
  {
    "text": "calico i am really excited to be here today with luis",
    "start": "14080",
    "end": "19600"
  },
  {
    "text": "who has been doing some awesome work in the calico community bringing compatibility with vpp",
    "start": "19600",
    "end": "25039"
  },
  {
    "text": "networking hi everyone thanks casey my name is luis and i work as a software engineer for",
    "start": "25039",
    "end": "30960"
  },
  {
    "text": "cisco more specifically in the vvp team i'm super excited to talk about the work we've been doing with calico",
    "start": "30960",
    "end": "37600"
  },
  {
    "text": "but before we dive into that let's hear from kz how calico was designed and how it made this calico vpp",
    "start": "37600",
    "end": "44320"
  },
  {
    "text": "integration possible so i'd like to start by giving a quick",
    "start": "44320",
    "end": "51360"
  },
  {
    "start": "48000",
    "end": "124000"
  },
  {
    "text": "overview of calico how it works and some of the lower level design",
    "start": "51360",
    "end": "57280"
  },
  {
    "text": "decisions that we as the calico team have made to help",
    "start": "57280",
    "end": "63199"
  },
  {
    "text": "enable some of this really awesome work that eloise has been doing",
    "start": "63199",
    "end": "68080"
  },
  {
    "text": "so to start calico is an open source networking and network policy provider",
    "start": "68560",
    "end": "75600"
  },
  {
    "text": "it can provide networking and network policy for kubernetes pods but also kubernetes",
    "start": "75600",
    "end": "83119"
  },
  {
    "text": "nodes vms openstack and legacy workloads",
    "start": "83119",
    "end": "89600"
  },
  {
    "text": "uh calico supports the the native built-in kubernetes network policy apis as well",
    "start": "89600",
    "end": "95840"
  },
  {
    "text": "as um a rich extension set of apis that that are native to calico",
    "start": "95840",
    "end": "103438"
  },
  {
    "text": "and calico self itself is battle tested it's uh you know been deployed in",
    "start": "104000",
    "end": "110640"
  },
  {
    "text": "production for for years now um and is really the most common choice",
    "start": "110640",
    "end": "118479"
  },
  {
    "text": "in clusters that need to scale and where performance really matters",
    "start": "118479",
    "end": "124240"
  },
  {
    "start": "124000",
    "end": "173000"
  },
  {
    "text": "so um for those of you who might not be familiar with calico at a really high level it",
    "start": "124799",
    "end": "130879"
  },
  {
    "text": "works like this um users specify high level descriptions",
    "start": "130879",
    "end": "136400"
  },
  {
    "text": "of how they want their network to behave uh either via the kubernetes api",
    "start": "136400",
    "end": "141599"
  },
  {
    "text": "the calico apis or both and this is this is stuff like network",
    "start": "141599",
    "end": "147280"
  },
  {
    "text": "policies bgp configuration etc that configuration is in turn read",
    "start": "147280",
    "end": "156080"
  },
  {
    "text": "by the calico components which are running on every node in your cluster and calico then takes",
    "start": "156080",
    "end": "162400"
  },
  {
    "text": "that and combines it with the locally running pods that it knows about and uses the result to set up each node",
    "start": "162400",
    "end": "169120"
  },
  {
    "text": "with the correct network programming",
    "start": "169120",
    "end": "172640"
  },
  {
    "text": "drilling down a bit further and here's what's happening on each calico node",
    "start": "174879",
    "end": "180239"
  },
  {
    "text": "they're really two main components there's the cni plugin which is called by the container runtime",
    "start": "180239",
    "end": "186959"
  },
  {
    "text": "as part of setting up and tearing down networking for each pod and so this this",
    "start": "186959",
    "end": "194080"
  },
  {
    "text": "plugin gets called on pod add and pod delete and it's responsible for setting up",
    "start": "194080",
    "end": "200560"
  },
  {
    "text": "um you know the network name space uh programming routes virtual ethernet",
    "start": "200560",
    "end": "206480"
  },
  {
    "text": "devices um you know all the stuff that it a pod needs in order to be able to communicate",
    "start": "206480",
    "end": "212640"
  },
  {
    "text": "with its own local node calcu node is the the second main",
    "start": "212640",
    "end": "218560"
  },
  {
    "text": "component of calico and this is a long-lived container that",
    "start": "218560",
    "end": "224319"
  },
  {
    "text": "runs on every node typically as a daemon set and it makes routing and policy",
    "start": "224319",
    "end": "229519"
  },
  {
    "text": "decisions and so this is really what's interpreting that configuration",
    "start": "229519",
    "end": "234560"
  },
  {
    "text": "and making sure that uh you know the network data plane is is programmed correctly",
    "start": "234560",
    "end": "241760"
  },
  {
    "text": "and within calico node there are two main sub-components that are responsible for this there's",
    "start": "241760",
    "end": "247760"
  },
  {
    "text": "felix which is a component that the calico team wrote and felix is responsible for maintaining",
    "start": "247760",
    "end": "255519"
  },
  {
    "text": "network policy state uh and there's bird which is an open source",
    "start": "255519",
    "end": "262560"
  },
  {
    "text": "networking stack which is included in calico and is used when bgp is required to",
    "start": "262560",
    "end": "268479"
  },
  {
    "text": "distribute routes through the network uh now obviously each of these components needs to be",
    "start": "268479",
    "end": "274639"
  },
  {
    "text": "pretty tightly coupled with the underlying networking technology because they're each reading and writing",
    "start": "274639",
    "end": "280080"
  },
  {
    "text": "state you know and interacting with that data plane now with a bit of understanding about",
    "start": "280080",
    "end": "286880"
  },
  {
    "start": "284000",
    "end": "380000"
  },
  {
    "text": "what calico is and and roughly how it works i i wanted to talk about",
    "start": "286880",
    "end": "292240"
  },
  {
    "text": "one of the core principles be on the engineering team follow when developing calico",
    "start": "292240",
    "end": "299520"
  },
  {
    "text": "specifically that is to use the right tool for the job at hand",
    "start": "299520",
    "end": "305840"
  },
  {
    "text": "you know technology and implementations are important uh but they change over time as",
    "start": "307120",
    "end": "314400"
  },
  {
    "text": "engineers it's it's really easy to lose sight of this sometimes uh you know you lose lose the forest for",
    "start": "314400",
    "end": "321600"
  },
  {
    "text": "the trees um and and it's really easy to focus on the technology choices",
    "start": "321600",
    "end": "327600"
  },
  {
    "text": "rather than what's actually really important which is leveraging that technology to",
    "start": "327600",
    "end": "333600"
  },
  {
    "text": "actually solve someone's problems so in other words",
    "start": "333600",
    "end": "340720"
  },
  {
    "text": "we want to do with calico is to make sure that the design of the software enables using the right technology to",
    "start": "340720",
    "end": "347759"
  },
  {
    "text": "solve the various diverse problems that our users have",
    "start": "347759",
    "end": "354080"
  },
  {
    "text": "and there are a lot of ways that you know this mindset shows itself in calico",
    "start": "354960",
    "end": "361600"
  },
  {
    "text": "we've got multiple built-in networking techniques using ipip vxlan",
    "start": "361600",
    "end": "368960"
  },
  {
    "text": "unencapsulated bgp we've got compatibility with a wide",
    "start": "368960",
    "end": "374880"
  },
  {
    "text": "array of third-party cni plugins etc but",
    "start": "374880",
    "end": "381440"
  },
  {
    "start": "380000",
    "end": "429000"
  },
  {
    "text": "the main design decision that i wanted to talk about today",
    "start": "381440",
    "end": "386800"
  },
  {
    "text": "um is the separation of control plane functions and data plane functions",
    "start": "386800",
    "end": "391840"
  },
  {
    "text": "and specifically how this enables calico to meet a variety of use cases leveraging",
    "start": "391840",
    "end": "396880"
  },
  {
    "text": "different underlying data plane technologies conceptually this is a pretty common",
    "start": "396880",
    "end": "403360"
  },
  {
    "text": "pattern in networking software and you know the control plane",
    "start": "403360",
    "end": "408560"
  },
  {
    "text": "is you know complex software that performs routing calculations",
    "start": "408560",
    "end": "414000"
  },
  {
    "text": "and it needs to be kept separate from high performance packet processing code but",
    "start": "414000",
    "end": "422080"
  },
  {
    "text": "for calico i really see a more important consequence of this pattern than just resource",
    "start": "422080",
    "end": "427759"
  },
  {
    "text": "isolation when we first built calico it",
    "start": "427759",
    "end": "433120"
  },
  {
    "start": "429000",
    "end": "624000"
  },
  {
    "text": "it only supported a single data plane that is standard linux networking",
    "start": "433120",
    "end": "440319"
  },
  {
    "text": "this is routing using linux routes and filtering using iptables",
    "start": "440319",
    "end": "447680"
  },
  {
    "text": "still at the time we were pretty clear that we wanted to architect this code in such a way that",
    "start": "447680",
    "end": "453440"
  },
  {
    "text": "if we needed to we could extend it to support additional data planes in the future",
    "start": "453440",
    "end": "459198"
  },
  {
    "text": "you know this provides future proofing and gives us flexibility to choose the best tool for the job",
    "start": "460240",
    "end": "467759"
  },
  {
    "text": "and avoids us getting fixated on a single data plane technology",
    "start": "467759",
    "end": "472800"
  },
  {
    "text": "to do this we pulled all the intelligence into a subcomponent of felix called the",
    "start": "473680",
    "end": "479360"
  },
  {
    "text": "calculation graph and then we built an internal api within",
    "start": "479360",
    "end": "484639"
  },
  {
    "text": "calico between this calculation graph and a swappable",
    "start": "484639",
    "end": "489919"
  },
  {
    "text": "data plane driver component uh if you're familiar with it this is",
    "start": "489919",
    "end": "495599"
  },
  {
    "text": "parallel to how the container runtime interface works in kubernetes um it enables using a bunch of different",
    "start": "495599",
    "end": "503520"
  },
  {
    "text": "container runtimes like docker cryo container d while still providing a consistent",
    "start": "503520",
    "end": "510400"
  },
  {
    "text": "feature set and user experience to the end user so",
    "start": "510400",
    "end": "516719"
  },
  {
    "text": "the data plane driver in calico is designed to be as simple as possible it just translates",
    "start": "516719",
    "end": "523839"
  },
  {
    "text": "events emitted by the calculation graph into the right messages for the",
    "start": "523839",
    "end": "529360"
  },
  {
    "text": "underlying data plane implementation and at least the the hard work that we",
    "start": "529360",
    "end": "534640"
  },
  {
    "text": "don't want to duplicate um you know interpreting configuration making decisions",
    "start": "534640",
    "end": "540240"
  },
  {
    "text": "uh up to the calculation graph well we started with just a single implementation uh",
    "start": "540240",
    "end": "547360"
  },
  {
    "text": "you know we've since been able to take advantage of the advantage of this decision to extend support to a variety of technologies um",
    "start": "547360",
    "end": "555600"
  },
  {
    "text": "you know this includes an evpf based data plane a windows hns data plane and of course",
    "start": "555600",
    "end": "562000"
  },
  {
    "text": "what we're here to talk about today which is a data plane built on vpp",
    "start": "562000",
    "end": "568399"
  },
  {
    "text": "um and just for completeness you know while those slides were really focused on how this works in felix",
    "start": "568399",
    "end": "574959"
  },
  {
    "text": "and the calico cni plug-in also supports grpc-based data plane drivers",
    "start": "574959",
    "end": "580240"
  },
  {
    "text": "um in addition to the default compiled in implementation and this was a feature",
    "start": "580240",
    "end": "586399"
  },
  {
    "text": "that was added by eloise as part of the vpp integration effort",
    "start": "586399",
    "end": "592480"
  },
  {
    "text": "which leads me to really the the last and most important ingredient",
    "start": "592480",
    "end": "597760"
  },
  {
    "text": "of calico which is its collaborative community",
    "start": "597760",
    "end": "602839"
  },
  {
    "text": "um you know there's a lot of people who have made made calico happen and it's through this",
    "start": "602839",
    "end": "609680"
  },
  {
    "text": "community that uh we've been able to build a relationship with alois and his team",
    "start": "609680",
    "end": "615920"
  },
  {
    "text": "um who's now going to to share with you how he's built upon this foundation",
    "start": "615920",
    "end": "621680"
  },
  {
    "text": "to bring something really cool in the form of uh vpp support to calico",
    "start": "621680",
    "end": "628160"
  },
  {
    "text": "thanks casey so now that we know how calico works and supports multiple data",
    "start": "628160",
    "end": "633839"
  },
  {
    "text": "planes let's see how we leverage this to add vpp as a data plane option for kiriko",
    "start": "633839",
    "end": "641120"
  },
  {
    "start": "640000",
    "end": "692000"
  },
  {
    "text": "but first a few words about vpp vpp is an open source software router",
    "start": "641120",
    "end": "646640"
  },
  {
    "text": "under the linux foundation umbrella it has many features from layer 2 to layer 4.",
    "start": "646640",
    "end": "652000"
  },
  {
    "text": "it supports tunneling net acl but also transport protocols such as tcp tls and",
    "start": "652000",
    "end": "657760"
  },
  {
    "text": "quick it's built to be easily extensible thanks to a plugin architecture",
    "start": "657760",
    "end": "663680"
  },
  {
    "text": "it supports both virtual and physical interfaces and has a really fast api but if there is one thing you must",
    "start": "663680",
    "end": "669600"
  },
  {
    "text": "remember about vpp it is that it's really highly optimized it uses vector instructions in order to",
    "start": "669600",
    "end": "675760"
  },
  {
    "text": "process multiple packets in a single instruction it uses prefetches heavily in order to",
    "start": "675760",
    "end": "680959"
  },
  {
    "text": "improve the data cache efficiency and the packet processing is split into a graph of small elementary nodes",
    "start": "680959",
    "end": "688320"
  },
  {
    "text": "that ensure that the instruction cache is also very efficient you may wonder what user space",
    "start": "688320",
    "end": "694320"
  },
  {
    "start": "692000",
    "end": "765000"
  },
  {
    "text": "networking exactly is it's actually quite simple it's just a regular process that does",
    "start": "694320",
    "end": "699440"
  },
  {
    "text": "packet processing instead of for instance http request processing so it's packets in packets out",
    "start": "699440",
    "end": "705519"
  },
  {
    "text": "there are many examples of user space networking applications that you've already used they include vpn clients like openvpn or",
    "start": "705519",
    "end": "713519"
  },
  {
    "text": "appropriate vpn clients dpdk-based applications as well and of course vpp",
    "start": "713519",
    "end": "719760"
  },
  {
    "text": "there are many benefits to user space networking the most important one to us is performance",
    "start": "719760",
    "end": "725120"
  },
  {
    "text": "when you have a user space network stack you can tune it to do exactly what you need for your specific use case",
    "start": "725120",
    "end": "731279"
  },
  {
    "text": "you don't have to rely on a general purpose tags that have features that you may not need and that would be",
    "start": "731279",
    "end": "736320"
  },
  {
    "text": "detrimental to performance it's also simpler to develop and deploy because you don't have any dependency on",
    "start": "736320",
    "end": "742639"
  },
  {
    "text": "your kernel so you can make changes without rebooting your machine",
    "start": "742639",
    "end": "748079"
  },
  {
    "text": "and it allows you to manage your own network stack just like any other software component this is possible thanks to specific",
    "start": "748079",
    "end": "754639"
  },
  {
    "text": "interface types provided by the linux kernel that allowed to retrieve packets in user space and drivers that allowed to expose",
    "start": "754639",
    "end": "761920"
  },
  {
    "text": "physical interfaces in user space as well there has been a recent trend in the",
    "start": "761920",
    "end": "767920"
  },
  {
    "start": "765000",
    "end": "823000"
  },
  {
    "text": "linux kernel to increase its modularity most obvious example",
    "start": "767920",
    "end": "773200"
  },
  {
    "text": "is ebpf which allows to inject code in the kernel and bypass parts of its network stack for instance",
    "start": "773200",
    "end": "780000"
  },
  {
    "text": "but there are also other examples such as af xdp which allows to implement very fast and",
    "start": "780000",
    "end": "785680"
  },
  {
    "text": "generic user space networking functions and also tune and tap interfaces",
    "start": "785680",
    "end": "791680"
  },
  {
    "text": "which thanks to the implementation of your tire backends guilty queue and gso allow to very",
    "start": "791680",
    "end": "798000"
  },
  {
    "text": "efficiently exchange packets with the linux kernel so this modularization is very",
    "start": "798000",
    "end": "804000"
  },
  {
    "text": "beneficial to user space networking it allows to use the best tool for each job",
    "start": "804000",
    "end": "809360"
  },
  {
    "text": "and it really opens many new options to make things more efficient and thanks to",
    "start": "809360",
    "end": "814560"
  },
  {
    "text": "these recent improvements in the linux kernel it's now possible to leverage user space networking stacks to",
    "start": "814560",
    "end": "820399"
  },
  {
    "text": "accelerate regular linux applications vvp was initially designed for",
    "start": "820399",
    "end": "825839"
  },
  {
    "start": "823000",
    "end": "929000"
  },
  {
    "text": "environments where it had a fixed amount of resources and it could consume all of them of",
    "start": "825839",
    "end": "832320"
  },
  {
    "text": "course that's not really well suited to container environments ideally you'd want vpp's resource consumption to scale up and down with",
    "start": "832320",
    "end": "839440"
  },
  {
    "text": "the actual load so one thing we did in order to improve",
    "start": "839440",
    "end": "845120"
  },
  {
    "text": "vpp's behavior in container environments is to switch from pole mode where vvp is constantly busy looping",
    "start": "845120",
    "end": "851600"
  },
  {
    "text": "checking if new packets are available for processing to interrupt mode where vpp is actually notified by the",
    "start": "851600",
    "end": "857920"
  },
  {
    "text": "network interfaces when packets are available so this allows to reduce the cpu that is",
    "start": "857920",
    "end": "863279"
  },
  {
    "text": "actually consumed by vpp regarding memory vpp used to require huge pages to run but this make it more",
    "start": "863279",
    "end": "870880"
  },
  {
    "text": "complex to deploy because you need to change the configuration of your hosts so we'll remove this requirement as well",
    "start": "870880",
    "end": "877680"
  },
  {
    "text": "we also made some improvements in vpp that allow it to better integrate with the linux kernel",
    "start": "877680",
    "end": "883680"
  },
  {
    "text": "the most important one is implementation of gso and gro json jro actually allow vpp to exchange",
    "start": "883680",
    "end": "891360"
  },
  {
    "text": "64 kilobyte buffers with a linux kernel instead of packet-sized buffers",
    "start": "891360",
    "end": "896720"
  },
  {
    "text": "this greatly reduces the load on the linux tcp stack and gives a very significant speed up on",
    "start": "896720",
    "end": "903440"
  },
  {
    "text": "tcp connections so for instance when an application needs to send data on a tcp connection",
    "start": "903440",
    "end": "910240"
  },
  {
    "text": "linux will pass a 64 kilobyte buffer to vpp and vpp will segment it before sending",
    "start": "910240",
    "end": "916480"
  },
  {
    "text": "it on the network in the other direction when receiving packets vpp will try to reassemble them",
    "start": "916480",
    "end": "924639"
  },
  {
    "text": "and pass a buffer that is as big as possible to the linux kernel",
    "start": "924639",
    "end": "929839"
  },
  {
    "start": "929000",
    "end": "981000"
  },
  {
    "text": "in addition to these improvements that make vvp play well with the kernel we also had to make a few improvements",
    "start": "930639",
    "end": "936800"
  },
  {
    "text": "that were more specific to kubernetes kubernetes has some specific requirements for the network",
    "start": "936800",
    "end": "942880"
  },
  {
    "text": "in particular the services load balancing requires pretty specific net behavior calico also",
    "start": "942880",
    "end": "949199"
  },
  {
    "text": "supports source netting outgoing connections so that your containers can reach external networks even if they have",
    "start": "949199",
    "end": "954959"
  },
  {
    "text": "private ips so we developed a custom net plugin in",
    "start": "954959",
    "end": "960480"
  },
  {
    "text": "gpp that is really tailored to this use case another point that is specific to calico",
    "start": "960480",
    "end": "966880"
  },
  {
    "text": "is that it proposes very rich policies so again we implemented a dedicated plugin",
    "start": "966880",
    "end": "972480"
  },
  {
    "text": "for these policies in vpp this allows vpp to implement the data plane api",
    "start": "972480",
    "end": "978079"
  },
  {
    "text": "of felix that casey mentioned earlier using vpp as your data plane with calico",
    "start": "978079",
    "end": "986079"
  },
  {
    "start": "981000",
    "end": "1064000"
  },
  {
    "text": "also brings many benefits to your operations so since vpp is packaged as a regular",
    "start": "986079",
    "end": "992160"
  },
  {
    "text": "container that means you can update it just like you would update any other applications",
    "start": "992160",
    "end": "999360"
  },
  {
    "text": "we made it so vpp can be upgraded and restarted with really minimal descriptions to the pods",
    "start": "999360",
    "end": "1005920"
  },
  {
    "text": "so if you don't care about the traffic being lost for about maybe one or two seconds you can",
    "start": "1005920",
    "end": "1011440"
  },
  {
    "text": "just restart vpp and all the pods will be able to communicate on the network normally as soon as vpp",
    "start": "1011440",
    "end": "1017920"
  },
  {
    "text": "is back up of course you can also evict your pods from the host before doing that",
    "start": "1017920",
    "end": "1023279"
  },
  {
    "text": "if that's better for your applications so this is helpful for upgrading gpp if",
    "start": "1023279",
    "end": "1029280"
  },
  {
    "text": "for instance there are fixes that you need to deploy or new features that you want to try out",
    "start": "1029280",
    "end": "1035600"
  },
  {
    "text": "calico vvp also has very limited kernel dependencies it basically only needs two tap",
    "start": "1035600",
    "end": "1041360"
  },
  {
    "text": "interfaces which any kernel that supports containerization support",
    "start": "1041360",
    "end": "1046400"
  },
  {
    "text": "this is interesting in particular in environments where you do not control your kernel so for instance in public clouds",
    "start": "1046400",
    "end": "1053840"
  },
  {
    "text": "sometimes you don't have a choice of what kernel is used and that means even there you can just deploy the new version of calico vpp and",
    "start": "1053840",
    "end": "1061360"
  },
  {
    "text": "get the latest and greatest features of calico so let's see a bit more detail how this",
    "start": "1061360",
    "end": "1068640"
  },
  {
    "start": "1064000",
    "end": "1153000"
  },
  {
    "text": "calico vpp integration works so on the left hand side here we see the",
    "start": "1068640",
    "end": "1074240"
  },
  {
    "text": "regular calico network topology basically every pod is connected to the host by one vc interface",
    "start": "1074240",
    "end": "1081840"
  },
  {
    "text": "and the host is responsible for all the networking functions when we deploy calico vpp vpp inserts",
    "start": "1081840",
    "end": "1089360"
  },
  {
    "text": "itself between the host and the opening interface so it takes ownership of the sampling",
    "start": "1089360",
    "end": "1094559"
  },
  {
    "text": "interface it restores connectivity to the host by creating a tune interface in the host",
    "start": "1094559",
    "end": "1101200"
  },
  {
    "text": "and it also creates two interfaces for all the pods",
    "start": "1101200",
    "end": "1106400"
  },
  {
    "text": "so now the networking responsibilities are split between the control plane running on the",
    "start": "1106400",
    "end": "1111520"
  },
  {
    "text": "host kubelet for instance will be running on the host but also the calico specific components such as the bgp daemon",
    "start": "1111520",
    "end": "1118160"
  },
  {
    "text": "and felix vpp itself is responsible for all the data",
    "start": "1118160",
    "end": "1123760"
  },
  {
    "text": "plane and that means of course routing the container traffic but also doing the services load",
    "start": "1123760",
    "end": "1129120"
  },
  {
    "text": "balancing implementing the policies configured by felix and so on",
    "start": "1129120",
    "end": "1135200"
  },
  {
    "text": "one specificity of the vpp network model is that twin interfaces are layer 3 interfaces",
    "start": "1135360",
    "end": "1141760"
  },
  {
    "text": "whereas these interfaces are layer 2 interfaces what that means is that there is no arp",
    "start": "1141760",
    "end": "1147840"
  },
  {
    "text": "going on between the pods and vpp or between the host and gpp it's all pure layer 3.",
    "start": "1147840",
    "end": "1154559"
  },
  {
    "start": "1153000",
    "end": "1218000"
  },
  {
    "text": "so now let's take a closer look at what happens to the application traffic when calico vpp is running the",
    "start": "1154559",
    "end": "1161919"
  },
  {
    "text": "applications are completely unmodified they use socket apis to send",
    "start": "1161919",
    "end": "1167520"
  },
  {
    "text": "and receive traffic to and from the kernel the kernel in the pod network namespace is then configured to send",
    "start": "1167520",
    "end": "1173840"
  },
  {
    "text": "this traffic over the tune interface that is connected to vpp the drawback of this architecture is",
    "start": "1173840",
    "end": "1179840"
  },
  {
    "text": "that the data from the application is coming from user space then going to the kernel space and then back to user space in vpp",
    "start": "1179840",
    "end": "1187760"
  },
  {
    "text": "this is required in order to keep the applications unmodified but it's not the most efficient way",
    "start": "1187760",
    "end": "1193919"
  },
  {
    "text": "however another advantage of this architecture is that the kernel still provides the isolation",
    "start": "1193919",
    "end": "1200000"
  },
  {
    "text": "one other possibility would be to have the applications leverage the vpp transport stack and send that traffic",
    "start": "1200000",
    "end": "1206240"
  },
  {
    "text": "directly to vbp this would be more efficient but it",
    "start": "1206240",
    "end": "1211360"
  },
  {
    "text": "would also require modifying the application it's something that we want to look at but we will likely support only a",
    "start": "1211360",
    "end": "1217120"
  },
  {
    "text": "restricted set of applications and finally this is what you get when you deploy calico vpp",
    "start": "1217120",
    "end": "1223760"
  },
  {
    "start": "1218000",
    "end": "1259000"
  },
  {
    "text": "on a kubernetes node so in the daemon set that is running on",
    "start": "1223760",
    "end": "1228960"
  },
  {
    "text": "every node you will get an additional container for vpp and in the calico node container we add",
    "start": "1228960",
    "end": "1234640"
  },
  {
    "text": "a component that we call the calico vpp agent so this component is communicating with",
    "start": "1234640",
    "end": "1240720"
  },
  {
    "text": "all the regular calico and kubernetes components and programs vpp in order to do the",
    "start": "1240720",
    "end": "1247440"
  },
  {
    "text": "routing the service load balancing and the policies it also handles the cni function in",
    "start": "1247440",
    "end": "1254880"
  },
  {
    "text": "order to create and delete the tune interfaces for the pods",
    "start": "1254880",
    "end": "1260400"
  },
  {
    "start": "1259000",
    "end": "1300000"
  },
  {
    "text": "so this is how calico vpp works now if you are about the product status",
    "start": "1261919",
    "end": "1267120"
  },
  {
    "text": "so this is of course open source on github in the project calico organization",
    "start": "1267120",
    "end": "1272480"
  },
  {
    "text": "currently under alpha status and considered as a calico incubation",
    "start": "1272480",
    "end": "1277600"
  },
  {
    "text": "project as of today we support most calico features",
    "start": "1277600",
    "end": "1282880"
  },
  {
    "text": "the features that are not supported right now include host policies and some specific",
    "start": "1282880",
    "end": "1289679"
  },
  {
    "text": "configuration features related to bgb we have started running initial performance benchmarks on this data",
    "start": "1289679",
    "end": "1296000"
  },
  {
    "text": "plane and as you will see the results are quite promising without going into all the details we",
    "start": "1296000",
    "end": "1302880"
  },
  {
    "start": "1300000",
    "end": "1320000"
  },
  {
    "text": "run our benchmarks on the bare metal testbed with two scalex servers connected by 140 gignik",
    "start": "1302880",
    "end": "1310159"
  },
  {
    "text": "we used iperf in order to run throughput tests and nginx and wrk to simulate",
    "start": "1310159",
    "end": "1317039"
  },
  {
    "text": "api servers and clients connecting to each other the first test we run was an http",
    "start": "1317039",
    "end": "1324159"
  },
  {
    "start": "1320000",
    "end": "1353000"
  },
  {
    "text": "latency test so we basically have an nginx http server running on onenote",
    "start": "1324159",
    "end": "1329679"
  },
  {
    "text": "and the wrk client located outside the cluster the urk client is sending 4 kilobytes",
    "start": "1329679",
    "end": "1336400"
  },
  {
    "text": "http request as fast as it can to the server and we measure the number of requests",
    "start": "1336400",
    "end": "1342080"
  },
  {
    "text": "per second that we are able to perform with that and we also measured the cpu consumption",
    "start": "1342080",
    "end": "1347360"
  },
  {
    "text": "during the test this benchmark simulates an external client connecting to the service in the",
    "start": "1347360",
    "end": "1352480"
  },
  {
    "text": "cluster here are the results that we got for this test we see that",
    "start": "1352480",
    "end": "1358559"
  },
  {
    "start": "1353000",
    "end": "1397000"
  },
  {
    "text": "vpp linux and ebpf are really close in terms of performance",
    "start": "1358559",
    "end": "1363600"
  },
  {
    "text": "all around 350 000 requests per second vpp is slightly higher at 370. but",
    "start": "1363600",
    "end": "1370159"
  },
  {
    "text": "what's really noticeable is that the cpu consumption of the server is lower with vpp both linux and upfront",
    "start": "1370159",
    "end": "1377440"
  },
  {
    "text": "are running at around 80 cpu usage vpp is at 67.",
    "start": "1377440",
    "end": "1382640"
  },
  {
    "text": "so this is quite encouraging because it means that in order to perform roughly the same amount of work",
    "start": "1382640",
    "end": "1388159"
  },
  {
    "text": "we are saving quite a few cpu cycles and this means that there are more",
    "start": "1388159",
    "end": "1393520"
  },
  {
    "text": "cycles available for your application logic",
    "start": "1393520",
    "end": "1397760"
  },
  {
    "start": "1397000",
    "end": "1427000"
  },
  {
    "text": "now in addition to this http request per second test we also run some tcp throughput tests so",
    "start": "1399840",
    "end": "1406640"
  },
  {
    "text": "in this case we use the two node cluster with encapsulation between the node",
    "start": "1406640",
    "end": "1412159"
  },
  {
    "text": "in this test we have one cluster ip service pointing to an ipf server pod",
    "start": "1412159",
    "end": "1417200"
  },
  {
    "text": "and one ipf client third both parts being pinned to different node and we test the tcp throughput that we",
    "start": "1417200",
    "end": "1423279"
  },
  {
    "text": "can obtain with varying number of connections between the client and the server",
    "start": "1423279",
    "end": "1428639"
  },
  {
    "text": "so with one flow we can see already that ebpf and vpp are much faster than linux twice as fast",
    "start": "1429279",
    "end": "1436880"
  },
  {
    "text": "at almost 20 gigs per second well while linux is at nine",
    "start": "1436960",
    "end": "1442159"
  },
  {
    "text": "with two flows ebpf takes the edge at 36 gigabits per second",
    "start": "1442159",
    "end": "1447520"
  },
  {
    "text": "vpp is at 30 and linux is scaling to 17. with four flows vpp and linux are",
    "start": "1447520",
    "end": "1454080"
  },
  {
    "text": "catching up with epf and with eight flows basically all the data planes are saturating the link",
    "start": "1454080",
    "end": "1461200"
  },
  {
    "text": "the vpp has a really fast ipsec implementation so one thing we wanted to measure is how",
    "start": "1461200",
    "end": "1467279"
  },
  {
    "text": "the vpp encryption compared to the encryption provided by linux and ebpf so for this test we rerun the same tests",
    "start": "1467279",
    "end": "1474720"
  },
  {
    "text": "as before but this time both linux and ebpf were configured with wireguard encryption",
    "start": "1474720",
    "end": "1480159"
  },
  {
    "text": "and vpp was configured with ipsec so here are the results that we got this",
    "start": "1480159",
    "end": "1486880"
  },
  {
    "start": "1483000",
    "end": "1509000"
  },
  {
    "text": "test was really favorable to vpp linux and ebpf respectively reach 32 000",
    "start": "1486880",
    "end": "1492960"
  },
  {
    "text": "and 42000 requests per second and vpp is at 250 000.",
    "start": "1492960",
    "end": "1498240"
  },
  {
    "text": "vpp does consume more cpu in this test but when we compare the amount of work actually being done to the amount of cpu",
    "start": "1498240",
    "end": "1504640"
  },
  {
    "text": "consumed vpp actually consumes much less cpu per request",
    "start": "1504640",
    "end": "1510080"
  },
  {
    "start": "1509000",
    "end": "1534000"
  },
  {
    "text": "now for the ipaf tests the encrypted ipf performance is also really good with vpp",
    "start": "1510080",
    "end": "1515679"
  },
  {
    "text": "basically the linux and the bpf data planes seem to be bottlenecked by the wire guard implementation at around 2.5",
    "start": "1515679",
    "end": "1522080"
  },
  {
    "text": "gigabits per second while vvp is able to get 12 gigabits per",
    "start": "1522080",
    "end": "1527760"
  },
  {
    "text": "second on one connection and 36 gigabits per second on eight connections which is basically link",
    "start": "1527760",
    "end": "1533200"
  },
  {
    "text": "speed one other aspect that we wanted to benchmark was how vpp behaved when the",
    "start": "1533200",
    "end": "1540559"
  },
  {
    "text": "scale of the cluster increased in order to do so we designed the test where we configure many services in a",
    "start": "1540559",
    "end": "1546960"
  },
  {
    "text": "cluster and we measure the time it actually takes to establish",
    "start": "1546960",
    "end": "1552159"
  },
  {
    "text": "a tcp connection from one node to another this allows to measure the behavior of the net code in the",
    "start": "1552159",
    "end": "1558799"
  },
  {
    "text": "different data planes we did that with a custom test client written in go",
    "start": "1558799",
    "end": "1564240"
  },
  {
    "text": "that sends http request at the concentrate and measures the connect and the request latency",
    "start": "1564240",
    "end": "1569760"
  },
  {
    "text": "this client is available on github if you want to try it this was required because wrk does not",
    "start": "1569760",
    "end": "1575760"
  },
  {
    "text": "measure the connect latency it actually measures only the request latency and the net implementation mostly",
    "start": "1575760",
    "end": "1582159"
  },
  {
    "text": "impacts the connect time here is the median connect latency that we obtained for the different data",
    "start": "1582159",
    "end": "1587279"
  },
  {
    "start": "1584000",
    "end": "1617000"
  },
  {
    "text": "planes so we see that as the number of services increases",
    "start": "1587279",
    "end": "1593360"
  },
  {
    "text": "the ebpf and gdp data plane behave really well because the connect latency doesn't change a lot",
    "start": "1593360",
    "end": "1598480"
  },
  {
    "text": "it's re it remains close to 250 microseconds on the other hand the linux data plane",
    "start": "1598480",
    "end": "1604640"
  },
  {
    "text": "which in this case is caprocky with aptables backend doesn't scale that well",
    "start": "1604640",
    "end": "1610240"
  },
  {
    "text": "and with 100k services the latency is really huge at more than six milliseconds",
    "start": "1610240",
    "end": "1617200"
  },
  {
    "start": "1617000",
    "end": "1653000"
  },
  {
    "text": "so that's it for the benchmark that we have but there are many other things that we",
    "start": "1619279",
    "end": "1624799"
  },
  {
    "text": "need to measure for instance we would also need to measure how vpp behaves when the number of backends",
    "start": "1624799",
    "end": "1630320"
  },
  {
    "text": "of a service increases and in particular we want to see the behavior of the system when there is a lot of",
    "start": "1630320",
    "end": "1636960"
  },
  {
    "text": "pattern meaning that back-ends will be added and removed from the different services continuously",
    "start": "1636960",
    "end": "1643360"
  },
  {
    "text": "we think that it's more representative of a real-life large cluster and we're very curious to see how vpp",
    "start": "1643360",
    "end": "1649120"
  },
  {
    "text": "will compare to the other data planes in that case in terms of features there are also some",
    "start": "1649120",
    "end": "1656080"
  },
  {
    "start": "1653000",
    "end": "1721000"
  },
  {
    "text": "things you would like to add the first one is wireguard support we recently had a contribution for a",
    "start": "1656080",
    "end": "1662720"
  },
  {
    "text": "wireguard implementation in vpp and integrating this in calico vpp will",
    "start": "1662720",
    "end": "1668240"
  },
  {
    "text": "allow us to have better compatibility with regular calico nodes",
    "start": "1668240",
    "end": "1673760"
  },
  {
    "text": "we also want to leverage the vpp telemetry infrastructure in order to expose more metrics about what's going on in",
    "start": "1673760",
    "end": "1679360"
  },
  {
    "text": "the containers and as i mentioned earlier we also want",
    "start": "1679360",
    "end": "1685200"
  },
  {
    "text": "to explore different ways to bring connectivity to the containers",
    "start": "1685200",
    "end": "1690399"
  },
  {
    "text": "we expect this to be much more performance at the current tune interface but as i said they require application",
    "start": "1690399",
    "end": "1696880"
  },
  {
    "text": "modification however one application that's highly likely to benefit from this",
    "start": "1696880",
    "end": "1702559"
  },
  {
    "text": "is envoy android is becoming more and more common and if we can accelerate it with vpp we",
    "start": "1702559",
    "end": "1709600"
  },
  {
    "text": "think that we could get a really interesting story there and finally of course we would love to",
    "start": "1709600",
    "end": "1716480"
  },
  {
    "text": "graduate from our incubation status to ga in keliko that's it for this",
    "start": "1716480",
    "end": "1722240"
  },
  {
    "start": "1721000",
    "end": "1819000"
  },
  {
    "text": "presentation i would like to really thank both the fdio vpp and the calico communities for all the",
    "start": "1722240",
    "end": "1728799"
  },
  {
    "text": "support they brought to this project if you're interested in trying out calico vpp you'll find",
    "start": "1728799",
    "end": "1735279"
  },
  {
    "text": "a link to our docs in the pdf version of the slides we provide configurations that will",
    "start": "1735279",
    "end": "1740640"
  },
  {
    "text": "allow you to deploy it on any cluster but it can take a bit of tuning to get the best performance out of your system",
    "start": "1740640",
    "end": "1748080"
  },
  {
    "text": "so if that's something you're interested in definitely reach out to us we will very gladly help you with that",
    "start": "1748080",
    "end": "1754000"
  },
  {
    "text": "thanks elise i'm really excited to see this project progressing as it as it",
    "start": "1754000",
    "end": "1759840"
  },
  {
    "text": "makes its way towards ga uh the last thing i wanted to say it's",
    "start": "1759840",
    "end": "1765200"
  },
  {
    "text": "really emphasized how much i like that this project fits",
    "start": "1765200",
    "end": "1770799"
  },
  {
    "text": "with the calico design philosophy that i was talking about earlier um i really see this as another tool",
    "start": "1770799",
    "end": "1778880"
  },
  {
    "text": "that we can put in in calico's toolbox and i think it's going to help enable a",
    "start": "1778880",
    "end": "1784080"
  },
  {
    "text": "new segment of users to leverage what calico and kubernetes",
    "start": "1784080",
    "end": "1791200"
  },
  {
    "text": "brings to the table additionally as a maintainer of calico i'm really excited",
    "start": "1791200",
    "end": "1797039"
  },
  {
    "text": "to see the community picking up such a",
    "start": "1797039",
    "end": "1802320"
  },
  {
    "text": "you know big project and really driving this to comp completion um and really happy to see",
    "start": "1802320",
    "end": "1809520"
  },
  {
    "text": "you taking advantage of uh you know some of the ways that we've architected the code to drive",
    "start": "1809520",
    "end": "1815120"
  },
  {
    "text": "innovation in the way that you are",
    "start": "1815120",
    "end": "1821039"
  }
]