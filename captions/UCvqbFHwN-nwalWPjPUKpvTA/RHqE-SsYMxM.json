[
  {
    "text": "okay hello everyone welcome my name is martin van gogh's i am the leader of the",
    "start": "30",
    "end": "6960"
  },
  {
    "text": "auto-scaling special interest group in kubernetes and i would like to join the",
    "start": "6960",
    "end": "12540"
  },
  {
    "text": "keynote speakers and welcome you at cube con and in the beautiful city of barcelona barcelona is a very popular",
    "start": "12540",
    "end": "20880"
  },
  {
    "text": "tourist destination many people visit it because it's the gateway to beautiful beaches of Costa Brava and Costa Dorada",
    "start": "20880",
    "end": "28490"
  },
  {
    "text": "some people come here to visit famous monuments like sagrada familia / guell",
    "start": "28490",
    "end": "33690"
  },
  {
    "text": "gothic district other enjoyed the city atmosphere and love to walk for vivid",
    "start": "33690",
    "end": "38879"
  },
  {
    "text": "and bustling boulevards like redonda there among acrobats painters cartoonist",
    "start": "38879",
    "end": "44730"
  },
  {
    "text": "burgers and pickpocketers if they look carefully they can spot them fortune",
    "start": "44730",
    "end": "50730"
  },
  {
    "text": "tellers the force of ten years for handful of euros will tell you your future will tell you your good and bad",
    "start": "50730",
    "end": "58260"
  },
  {
    "text": "moments to come would be your prince and princess arming whether your current love will last for the lifetime and if",
    "start": "58260",
    "end": "66810"
  },
  {
    "text": "they are really really good they will tell you what will be the names of your grandchildren they will answer any of",
    "start": "66810",
    "end": "73680"
  },
  {
    "text": "your questions well almost any imagine that you are just about to launch a new",
    "start": "73680",
    "end": "80549"
  },
  {
    "text": "service and you are nervous anxious and you want to validate your assumptions and you come to a legitimate looking for",
    "start": "80549",
    "end": "87960"
  },
  {
    "text": "- tenor and ask this how much resources do I need in my cluster how many pots",
    "start": "87960",
    "end": "94770"
  },
  {
    "text": "should I have in my deployment how big my pots should be well they will react",
    "start": "94770",
    "end": "102689"
  },
  {
    "text": "with complete astonishment the set true is that currently there are no cloud",
    "start": "102689",
    "end": "108479"
  },
  {
    "text": "native certified fortune tellers who understand the internals of kubernetes not even in Barcelona not on la rambla",
    "start": "108479",
    "end": "116570"
  },
  {
    "text": "you can ask these questions to your developers devops admins product",
    "start": "116570",
    "end": "122909"
  },
  {
    "text": "managers they may have a really good grasp on kubernetes and your product domain they can do a lot of measurements",
    "start": "122909",
    "end": "130679"
  },
  {
    "text": "test checks however most of the technical people don't have strong clairvoyant skills and the life",
    "start": "130679",
    "end": "139000"
  },
  {
    "text": "will always find a way to surprise you proper pod count for today may be too",
    "start": "139000",
    "end": "144790"
  },
  {
    "text": "little for tomorrow when the traffic increase an absent-minded developer may forget to mention that he increased the",
    "start": "144790",
    "end": "152170"
  },
  {
    "text": "cash in your application ten times and now it starts rolling out of memory exception someone from another team may",
    "start": "152170",
    "end": "159610"
  },
  {
    "text": "start another workload in your cluster depleting its capacity and you were",
    "start": "159610",
    "end": "164739"
  },
  {
    "text": "called to fix it if lucky within your business hours you slightly less lucky",
    "start": "164739",
    "end": "170260"
  },
  {
    "text": "at 3:00 a.m. on Saturday and obviously",
    "start": "170260",
    "end": "175300"
  },
  {
    "text": "you don't want to be woken up and called with alerts you would like to have an",
    "start": "175300",
    "end": "181480"
  },
  {
    "text": "automation a robot who would observe your cluster and add or remove",
    "start": "181480",
    "end": "187500"
  },
  {
    "text": "computational capacity when needed to keep your workouts healthy and what is",
    "start": "187500",
    "end": "194680"
  },
  {
    "text": "equally important to keep your monthly bills as low as possible lucky for you",
    "start": "194680",
    "end": "201370"
  },
  {
    "text": "we have a couple of such robots in kubernetes and now I'm going to describe them to you the first robot is called",
    "start": "201370",
    "end": "210220"
  },
  {
    "text": "horizontal pot autoscaler horizontal pot autoscaler is based on metrics that",
    "start": "210220",
    "end": "216670"
  },
  {
    "text": "Express the load that your application gets it can be either real CPU usage or",
    "start": "216670",
    "end": "222130"
  },
  {
    "text": "something a little bit more custom like the number of queries per second that are coming to your application or other",
    "start": "222130",
    "end": "228610"
  },
  {
    "text": "gouts like value provided by the user HPA takes the value of this matrix",
    "start": "228610",
    "end": "235019"
  },
  {
    "text": "compares it with user-defined target and depending on the result of this",
    "start": "235019",
    "end": "240160"
  },
  {
    "text": "comparison add or remove replicas to move the metrics toward the desired",
    "start": "240160",
    "end": "246640"
  },
  {
    "text": "value let's take a look how does it work in practice but before that let me",
    "start": "246640",
    "end": "255400"
  },
  {
    "text": "clarify something while talking about horizontal pot autoscaler I will use a",
    "start": "255400",
    "end": "260560"
  },
  {
    "text": "word utilization I understand it as a ratio between the current load X",
    "start": "260560",
    "end": "267440"
  },
  {
    "text": "by some metrics and the maximum amount of load that your application can handle",
    "start": "267440",
    "end": "273290"
  },
  {
    "text": "for example if your pot requests one CPU and is currently using half of it then",
    "start": "273290",
    "end": "279530"
  },
  {
    "text": "your utilization is 50% if a pot can handle 1000 QPS but its processing 200",
    "start": "279530",
    "end": "286700"
  },
  {
    "text": "at this moment then the utilization is 20% okay now dimensional example on this",
    "start": "286700",
    "end": "293270"
  },
  {
    "text": "example we have four pots that are burning hot and trip are reporting very high utilization the reload is above the",
    "start": "293270",
    "end": "300560"
  },
  {
    "text": "target they use almost all of their capacity should the traffic increase",
    "start": "300560",
    "end": "305960"
  },
  {
    "text": "there might be not enough computing power to handle it but if we add another",
    "start": "305960",
    "end": "313190"
  },
  {
    "text": "pot and the same traffic is spread across five pots then the average per",
    "start": "313190",
    "end": "319850"
  },
  {
    "text": "pot load obviously will decrease leaving more space for spikes and coming closer",
    "start": "319850",
    "end": "325790"
  },
  {
    "text": "to the desired target in this way - if the situation is slightly different and",
    "start": "325790",
    "end": "333970"
  },
  {
    "text": "four and all the pots are below the target like significantly below the",
    "start": "333970",
    "end": "339890"
  },
  {
    "text": "target then it might be actually beneficial to delete one of the pots and",
    "start": "339890",
    "end": "344990"
  },
  {
    "text": "then the traffic on the remaining pots will increase the post will be better",
    "start": "344990",
    "end": "350900"
  },
  {
    "text": "utilized and you will have more resources in your cluster and this is",
    "start": "350900",
    "end": "356240"
  },
  {
    "text": "what horizontal pot autoscaler does it add and delete replicas when needed when",
    "start": "356240",
    "end": "363890"
  },
  {
    "text": "to use HP a horizontal pot autoscaler works best with stateless serving",
    "start": "363890",
    "end": "370940"
  },
  {
    "text": "workloads that can spin up new replicas reasonably fast you can also use horizontal pot autoscaler with Q based",
    "start": "370940",
    "end": "378200"
  },
  {
    "text": "processing how to enable it HP is a part",
    "start": "378200",
    "end": "383360"
  },
  {
    "text": "of core kubernetes api s-- it is controlled by HP object you can create this object either with the ml file or",
    "start": "383360",
    "end": "391430"
  },
  {
    "text": "by cube CTL auto scale command cpu usage is available out of the box custom",
    "start": "391430",
    "end": "398390"
  },
  {
    "text": "metrics require a little bit more work while setting up horizontal pot",
    "start": "398390",
    "end": "403910"
  },
  {
    "text": "autoscaler it is crucial to set the target value or more generally target utilization write",
    "start": "403910",
    "end": "410210"
  },
  {
    "text": "a simple formula 1 - utilization is the size of your spike buffer capacity and",
    "start": "410210",
    "end": "416930"
  },
  {
    "text": "the driver for scallops 2 small spike buffer may prevent scallops and may",
    "start": "416930",
    "end": "423740"
  },
  {
    "text": "overload the application too big obviously waste your money the value of",
    "start": "423740",
    "end": "431810"
  },
  {
    "text": "the target utilization should allow your application to run with the increased",
    "start": "431810",
    "end": "437000"
  },
  {
    "text": "traffic but with the old number of replicas for at least 2 minutes to allow HP or reaction possibly not provisioning",
    "start": "437000",
    "end": "444560"
  },
  {
    "text": "and starting new pots so if you expect that within any 2 minutes your traffic",
    "start": "444560",
    "end": "450560"
  },
  {
    "text": "may rise by let's say 50% your target utilization should be set to 65% or less",
    "start": "450560",
    "end": "456710"
  },
  {
    "text": "if it can double to 50% or less and so on and so on what are the other best",
    "start": "456710",
    "end": "465290"
  },
  {
    "text": "practices for horizontal potato scaler application under HP I should correctly",
    "start": "465290",
    "end": "471620"
  },
  {
    "text": "handle 6 terms this means that after receiving the signal the application",
    "start": "471620",
    "end": "477170"
  },
  {
    "text": "should not stop but finish all of the",
    "start": "477170",
    "end": "482390"
  },
  {
    "text": "requests that are currently in flight and still listen for the incoming connections that may arrive after the",
    "start": "482390",
    "end": "488810"
  },
  {
    "text": "pod termination begins it may take a while to update all q proxies in your",
    "start": "488810",
    "end": "494360"
  },
  {
    "text": "clusters and all components of the load balancers sometimes even one minute or",
    "start": "494360",
    "end": "499850"
  },
  {
    "text": "more depending on your cloud provider if your application dies before our Q",
    "start": "499850",
    "end": "505430"
  },
  {
    "text": "proxies and load balancers are updated some requests may end up in void causing",
    "start": "505430",
    "end": "510620"
  },
  {
    "text": "errors on the client side also your new",
    "start": "510620",
    "end": "515690"
  },
  {
    "text": "replicas should not receive a request before they are ready for that to ensure ensure that please PLEASE setup",
    "start": "515690",
    "end": "522770"
  },
  {
    "text": "readiness probe for your pots and by the way also liveness probes it is a good practice even without horizontal potato",
    "start": "522770",
    "end": "529730"
  },
  {
    "text": "scaler but it's particularly useful when your pods are constantly starting instead pink also please keep your metric server",
    "start": "529730",
    "end": "538190"
  },
  {
    "text": "healthy it is the source of CPU utilization metrics and without it HPA",
    "start": "538190",
    "end": "543860"
  },
  {
    "text": "doesn't work last but not least please enable a cluster autoscaler while adding",
    "start": "543860",
    "end": "550820"
  },
  {
    "text": "new replicas you may end up in this situation when there are no free resources in your cluster and someone",
    "start": "550820",
    "end": "556430"
  },
  {
    "text": "needs to fix it it will be either you will receive the call or and the",
    "start": "556430",
    "end": "562700"
  },
  {
    "text": "automation I will talk more about cluster autoscaler later in this presentation ok what is the next big thing for",
    "start": "562700",
    "end": "570400"
  },
  {
    "text": "horizontal potato scaler the barrack functionality has been there for quite a",
    "start": "570400",
    "end": "576740"
  },
  {
    "text": "while CPU base scaling is considered G a custom metric scaling is considered to",
    "start": "576740",
    "end": "583250"
  },
  {
    "text": "be like delayed beta but we are adding more knobs for advanced users we have",
    "start": "583250",
    "end": "588470"
  },
  {
    "text": "couple peers in flight that will add more controls and speed for scale up scale downs and their sensitiveness",
    "start": "588470",
    "end": "594670"
  },
  {
    "text": "moreover we want to allow scaling down from zero and to zero for HP is driven",
    "start": "594670",
    "end": "601580"
  },
  {
    "text": "by custom metrics the second robot from our happy auto scaling family is",
    "start": "601580",
    "end": "607940"
  },
  {
    "text": "vertical path autoscaler vertical path autoscaler monitors the real CPU and memory usage",
    "start": "607940",
    "end": "615110"
  },
  {
    "text": "to estimate what should be the best pot size in terms of requested CPU and",
    "start": "615110",
    "end": "621350"
  },
  {
    "text": "memory what request is used by scheduler to decide where to put the pot on which",
    "start": "621350",
    "end": "626600"
  },
  {
    "text": "machine only machine that has enough free resources can host the pot pot",
    "start": "626600",
    "end": "632530"
  },
  {
    "text": "request also defines how much request will be guaranteed for your application",
    "start": "632530",
    "end": "638270"
  },
  {
    "text": "if it's too small then the application might be either throttled or Metro out",
    "start": "638270",
    "end": "643700"
  },
  {
    "text": "of memory exception or both if it's too big then your resources are obviously",
    "start": "643700",
    "end": "648770"
  },
  {
    "text": "wasted and your money is wasted so it is quite important to set pato request right so what does vpa do",
    "start": "648770",
    "end": "657220"
  },
  {
    "text": "let's take a look at this example here we have a pot that uses 95% of the",
    "start": "657220",
    "end": "664250"
  },
  {
    "text": "requested capacity it's not the situation in this case vertical path",
    "start": "664250",
    "end": "672269"
  },
  {
    "text": "autoscaler will recommend to increase pot size so that the same load consumes only let's say 75% cup of capacity not",
    "start": "672269",
    "end": "680699"
  },
  {
    "text": "95 if the load on the pot is low and for",
    "start": "680699",
    "end": "686399"
  },
  {
    "text": "extended period of time VP or it will recommend to decrease pot size to free some space and allow better",
    "start": "686399",
    "end": "691949"
  },
  {
    "text": "utilization of your machines vertical pot autoscaler can work in free modes in",
    "start": "691949",
    "end": "699509"
  },
  {
    "text": "first it only provides recommendation but doesn't change anything the user is",
    "start": "699509",
    "end": "705360"
  },
  {
    "text": "responsible for setting the spot request manually in the second slightly more",
    "start": "705360",
    "end": "710759"
  },
  {
    "text": "automatic mode it only set spot request for new pots it doesn't touch the",
    "start": "710759",
    "end": "716639"
  },
  {
    "text": "existing pots even if they have sub optimal requests the last mode auto",
    "start": "716639",
    "end": "722879"
  },
  {
    "text": "allows vertical pot autoscaler to kill existing pots and recreate them",
    "start": "722879",
    "end": "728579"
  },
  {
    "text": "with the updated requests you can use",
    "start": "728579",
    "end": "734040"
  },
  {
    "text": "vertical pot autoscaler for both stateless and stifled workload especially if they cannot be handled",
    "start": "734040",
    "end": "740639"
  },
  {
    "text": "with horizontal path autoscaler and obviously vertical pot autoscaler is particularly useful when you have no",
    "start": "740639",
    "end": "747990"
  },
  {
    "text": "idea what should be your pot size and auto mode is quite handy if your",
    "start": "747990",
    "end": "753809"
  },
  {
    "text": "application can be started and you gained enough confidence with VP recommendations when running it in",
    "start": "753809",
    "end": "760319"
  },
  {
    "text": "recommendation on remote vertical pot",
    "start": "760319",
    "end": "765329"
  },
  {
    "text": "autoscaler is not a part of Cal Cooper Netta's you either need to install it manually as an",
    "start": "765329",
    "end": "771600"
  },
  {
    "text": "additional component on an able it on your kubernetes managed solution like",
    "start": "771600",
    "end": "776639"
  },
  {
    "text": "GAE with appropriate cloud provider specific commands then you create a",
    "start": "776639",
    "end": "781980"
  },
  {
    "text": "vertical pot autoscaler custom resource definition object pointing to your application in a",
    "start": "781980",
    "end": "788669"
  },
  {
    "text": "similar fashion as in HPA however this time you don't have a dedicated cube CTL",
    "start": "788669",
    "end": "794309"
  },
  {
    "text": "comment now a list of best practices for",
    "start": "794309",
    "end": "800069"
  },
  {
    "text": "vp8 mentioned earlier you should start your adventure with vertical pot autoscaler",
    "start": "800069",
    "end": "805889"
  },
  {
    "text": "in a recommendation only mode give it a while like a day or maybe more to learn",
    "start": "805889",
    "end": "811050"
  },
  {
    "text": "here what is your workload to see what are your daily patterns convince yourself that the recommendations are",
    "start": "811050",
    "end": "817709"
  },
  {
    "text": "good and then consider switching hit total vertical potty autoscaler is still",
    "start": "817709",
    "end": "822899"
  },
  {
    "text": "better soon being a little bit cautious may be addressed while running in the",
    "start": "822899",
    "end": "828300"
  },
  {
    "text": "outer mode please set pot disruption budgets actually you should set them and",
    "start": "828300",
    "end": "833939"
  },
  {
    "text": "way for many other reasons when configuring vertical pot autoscaler set",
    "start": "833939",
    "end": "840149"
  },
  {
    "text": "minimum and maximum container sizes in VP object just to be on the safe side in",
    "start": "840149",
    "end": "847019"
  },
  {
    "text": "outage like situation for example when your traffic is not coming over to your",
    "start": "847019",
    "end": "852540"
  },
  {
    "text": "application for hours or maybe days vpmi significantly reduce the pot size and",
    "start": "852540",
    "end": "859319"
  },
  {
    "text": "then when you suddenly enable the traffic you may have an unpleasant surprise vertical pot autoscaler does it",
    "start": "859319",
    "end": "868170"
  },
  {
    "text": "update your deployment or stateful set specification it only updates spots when",
    "start": "868170",
    "end": "873600"
  },
  {
    "text": "they are created so a good practice is to put reasonable pot requests for",
    "start": "873600",
    "end": "880350"
  },
  {
    "text": "example taken from VP recommendations in the deployment itself just in case you",
    "start": "880350",
    "end": "885839"
  },
  {
    "text": "have some problems with VP a or someone turns of VP accidently you don't want to",
    "start": "885839",
    "end": "891179"
  },
  {
    "text": "lose like this evaluated pot sizes",
    "start": "891179",
    "end": "897559"
  },
  {
    "text": "vertical pot autoscaler doesn't like huge abrupt changes for example if you",
    "start": "897559",
    "end": "902670"
  },
  {
    "text": "had 40 pots and you suddenly decide to turn your deployment from 30 to 5 pots",
    "start": "902670",
    "end": "908999"
  },
  {
    "text": "in one step it's not the best idea you",
    "start": "908999",
    "end": "914759"
  },
  {
    "text": "should do it gradually over a day a VPS truss history so very significant",
    "start": "914759",
    "end": "921350"
  },
  {
    "text": "changes like a completely new image complete redesign may require a new VP",
    "start": "921350",
    "end": "928259"
  },
  {
    "text": "object and maybe a new deployment with different labor set",
    "start": "928259",
    "end": "933580"
  },
  {
    "text": "VPI takes matrix from metric server so it is important to keep metrics server a",
    "start": "933580",
    "end": "939310"
  },
  {
    "text": "healthy and in the same way as horizontal pot autoscaler you should enable cluster autoscaler in",
    "start": "939310",
    "end": "947080"
  },
  {
    "text": "your cluster in case vp a enlarge your pot beyond your available capacity can",
    "start": "947080",
    "end": "956560"
  },
  {
    "text": "you mix a horizontal pot autoscaler and vertical path autoscaler on the same workload well the short answer is no the",
    "start": "956560",
    "end": "964540"
  },
  {
    "text": "more detailed answer starts with execute Gaussian and it depends technically you",
    "start": "964540",
    "end": "970930"
  },
  {
    "text": "can always use vertical path autoscaler in recommendation on any mode and see what it says it's safe nothing bad",
    "start": "970930",
    "end": "978190"
  },
  {
    "text": "should happen and you can always discard the recommendations however in order to get stable results you should either use",
    "start": "978190",
    "end": "985420"
  },
  {
    "text": "custom metrics in horizontal put out the scalar or absolute values for your HP targets when scaling on CPU and you",
    "start": "985420",
    "end": "993640"
  },
  {
    "text": "should make sure that the application is getting enough traffic horizontal",
    "start": "993640",
    "end": "998890"
  },
  {
    "text": "potahto scalar should not be running at the absolute minimum number of replicas and way beyond the target utilization",
    "start": "998890",
    "end": "1006829"
  },
  {
    "text": "switching vertical path autoscaler to auto when mixing to if HP is risky and",
    "start": "1006829",
    "end": "1012089"
  },
  {
    "text": "should shouldn't be done without the deeper understanding of HPV piant in your workload characteristic ok what",
    "start": "1012089",
    "end": "1020670"
  },
  {
    "text": "next with vpa we have 200 major things on the road not as I said previously VP",
    "start": "1020670",
    "end": "1027360"
  },
  {
    "text": "a is in beta and we hope to graduate it to GA soon and we are working on the",
    "start": "1027360",
    "end": "1034050"
  },
  {
    "text": "support for in place spot request updates currently in order to update pot",
    "start": "1034050",
    "end": "1039418"
  },
  {
    "text": "requests the pot has to be recreated and obviously your application has a small",
    "start": "1039419",
    "end": "1044459"
  },
  {
    "text": "here cap this is not an optimal situation and we are working with come in G particular with the note and",
    "start": "1044459",
    "end": "1050640"
  },
  {
    "text": "scheduling teams to fix it now the third",
    "start": "1050640",
    "end": "1056910"
  },
  {
    "text": "and last robot cluster autoscaler horizontal pot",
    "start": "1056910",
    "end": "1062790"
  },
  {
    "text": "autoscaler and vertical pot autoscaler are targeted at scaling your workloads cluster",
    "start": "1062790",
    "end": "1069180"
  },
  {
    "text": "autoscaler scale is the underlying computing infrastructure it provides",
    "start": "1069180",
    "end": "1074340"
  },
  {
    "text": "notes for your pods for pods that don't have a place to run it also compacts underutilized nodes it",
    "start": "1074340",
    "end": "1082380"
  },
  {
    "text": "is based on scheduling simulation and declared port requests it doesn't use",
    "start": "1082380",
    "end": "1087540"
  },
  {
    "text": "metrics like actual CPU use it or mine on QP ASIS so let's take a look on how does",
    "start": "1087540",
    "end": "1093870"
  },
  {
    "text": "it work in practice here we have four nodes and with four pots free ports are",
    "start": "1093870",
    "end": "1100260"
  },
  {
    "text": "big one pod is relatively small and on that note we have a little bit of free",
    "start": "1100260",
    "end": "1106350"
  },
  {
    "text": "capacity if I new pod comes then maybe it will fit into this third node and the",
    "start": "1106350",
    "end": "1113970"
  },
  {
    "text": "scheduler will place it there and it will start running there however if all",
    "start": "1113970",
    "end": "1119490"
  },
  {
    "text": "nodes are kind of full and scheduler has no way of placing the pot you need",
    "start": "1119490",
    "end": "1125700"
  },
  {
    "text": "another node and cluster autoscaler notices this situation it talks to your",
    "start": "1125700",
    "end": "1131360"
  },
  {
    "text": "cloud provider and spins up a new node the new node arrives scheduler notices",
    "start": "1131360",
    "end": "1138060"
  },
  {
    "text": "it and lays the pot on the new node and everyone is happy on some occasions some",
    "start": "1138060",
    "end": "1144030"
  },
  {
    "text": "nodes may not be utilized to their full potential let's take a look at this",
    "start": "1144030",
    "end": "1150060"
  },
  {
    "text": "picture small yellow and green pods have notes for themselves if we move the",
    "start": "1150060",
    "end": "1157620"
  },
  {
    "text": "green pod to the third node then the last node will be empty and can be",
    "start": "1157620",
    "end": "1164100"
  },
  {
    "text": "safely deleted like this reducing your monthly cloud bill and this is exactly",
    "start": "1164100",
    "end": "1171690"
  },
  {
    "text": "what cluster autoscaler does it adds and removes replicas as needed we want to",
    "start": "1171690",
    "end": "1178650"
  },
  {
    "text": "use castrato scalar this short answer is always especially if you have more than",
    "start": "1178650",
    "end": "1184740"
  },
  {
    "text": "two free nodes in your cluster and your workloads change all the time",
    "start": "1184740",
    "end": "1190430"
  },
  {
    "text": "cluster autoscaler works with multiple cloud provider it has support for",
    "start": "1190430",
    "end": "1195750"
  },
  {
    "text": "Alibaba AWS as your Baidu Google cloud platform and OpenStack running",
    "start": "1195750",
    "end": "1201859"
  },
  {
    "text": "with Magnum to enable cluster autoscaler follow your cloud provider specific",
    "start": "1201859",
    "end": "1209359"
  },
  {
    "text": "guide heads some environments like google kubernetes engine have built-in easy to use cluster autoscaler but many",
    "start": "1209359",
    "end": "1216710"
  },
  {
    "text": "require additional manual steps please check the appropriate documentation for details best practices use the hosted",
    "start": "1216710",
    "end": "1225799"
  },
  {
    "text": "version of trust autoscaler if your cloud provider supports it or has it in",
    "start": "1225799",
    "end": "1231229"
  },
  {
    "text": "managed solution pick the version if you're running it manually pick the version of cluster autoscaler",
    "start": "1231229",
    "end": "1237589"
  },
  {
    "text": "that matches your cluster version or more specifically scheduler version there may be some subtle differences",
    "start": "1237589",
    "end": "1245269"
  },
  {
    "text": "between scheduler versions and they may result in inconsistent cluster",
    "start": "1245269",
    "end": "1250969"
  },
  {
    "text": "autoscaler behavior and define the pod disruption budget for your application",
    "start": "1250969",
    "end": "1256909"
  },
  {
    "text": "to make sure that the critical workloads are not moved too frequently or in large",
    "start": "1256909",
    "end": "1263330"
  },
  {
    "text": "extent as I mentioned it with vertical path autoscaler but it is also useful",
    "start": "1263330",
    "end": "1268399"
  },
  {
    "text": "when running cluster autoscaler in your cluster what's next with cluster",
    "start": "1268399",
    "end": "1276200"
  },
  {
    "text": "autoscaler well the project has been in gie for quite a long time and no bigger",
    "start": "1276200",
    "end": "1283249"
  },
  {
    "text": "changes are expected they're probably the only bigger thing that is coming is the support for clustering pi2 support",
    "start": "1283249",
    "end": "1289909"
  },
  {
    "text": "more cloud providers and mark environments like on-premise as the",
    "start": "1289909",
    "end": "1297200"
  },
  {
    "text": "stock is slowly getting to the end I would like to give you a little bit more information about auto scaling special",
    "start": "1297200",
    "end": "1303679"
  },
  {
    "text": "interest group itself we have meetings every Monday at 4:00 p.m. Barcelona time",
    "start": "1303679",
    "end": "1309289"
  },
  {
    "text": "on zoom' we have quite active slack channel where you can ask questions and",
    "start": "1309289",
    "end": "1316330"
  },
  {
    "text": "propose your new ideas and most of our stuff like cluster autoscaler and",
    "start": "1316330",
    "end": "1322309"
  },
  {
    "text": "vertical path autoscaler sees in kubernetes slash autoscaler repository on github HP itself seen",
    "start": "1322309",
    "end": "1331130"
  },
  {
    "text": "kubernetes / kubernetes repo and last but not least I would like you to I",
    "start": "1331130",
    "end": "1337100"
  },
  {
    "text": "would like to invite you to other auto-scaling talks we have sick",
    "start": "1337100",
    "end": "1343040"
  },
  {
    "text": "auto-scaling deep dive where we will get into details of cluster autoscaler and vertical path autoscaler on Thursday at",
    "start": "1343040",
    "end": "1351370"
  },
  {
    "text": "3:55 the and there are two talks and focus on pod requests in-place upgrades",
    "start": "1351370",
    "end": "1360800"
  },
  {
    "text": "both of tomorrow the first one is at 11:05 and the second one immediately",
    "start": "1360800",
    "end": "1367340"
  },
  {
    "text": "after yet 11:55 and this is the end of",
    "start": "1367340",
    "end": "1372410"
  },
  {
    "text": "my talk thank you for being and training with me today and now we have a couple",
    "start": "1372410",
    "end": "1378320"
  },
  {
    "text": "minutes for questions [Applause]",
    "start": "1378320",
    "end": "1383269"
  },
  {
    "text": "we have someone with the mic so please raise your hand and you will get a mic",
    "start": "1384580",
    "end": "1390460"
  },
  {
    "text": "okay anyone no questions at all okay so okay",
    "start": "1390460",
    "end": "1396860"
  },
  {
    "text": "here here you are",
    "start": "1396860",
    "end": "1399940"
  },
  {
    "text": "okay hello the reason I'm here is that we're",
    "start": "1407480",
    "end": "1413580"
  },
  {
    "text": "sort of shifting an old application to communities and it fits fairly well and",
    "start": "1413580",
    "end": "1419270"
  },
  {
    "text": "one thing we want to do is to run optimization jobs run optimization jobs",
    "start": "1419270",
    "end": "1426840"
  },
  {
    "text": "okay that will take some time and we want to auto scale the cluster nodes based on the number of like you use",
    "start": "1426840",
    "end": "1433590"
  },
  {
    "text": "given at this as a cue more or less okay it's the one thing I've been thinking about this it can't",
    "start": "1433590",
    "end": "1443280"
  },
  {
    "text": "the scheduler or the wall to cluster scaler thing can't move things before",
    "start": "1443280",
    "end": "1448410"
  },
  {
    "text": "the node is completely empty you have any possibility to do it yes it can move",
    "start": "1448410",
    "end": "1454110"
  },
  {
    "text": "pods when before the node is completely empty so the default setting is 50",
    "start": "1454110",
    "end": "1459990"
  },
  {
    "text": "percent so if your pod if you're noticed utilized in less than 50 percent in",
    "start": "1459990",
    "end": "1466950"
  },
  {
    "text": "terms of pot requests and cluster autoscaler khan starts to consider this knob as not that much needed and if it",
    "start": "1466950",
    "end": "1473550"
  },
  {
    "text": "finds a place for all pots from this node to be moved also elsewhere and there are no system pots note what we",
    "start": "1473550",
    "end": "1480300"
  },
  {
    "text": "put with restrictive pot disruption budget and couple of other conditions",
    "start": "1480300",
    "end": "1485580"
  },
  {
    "text": "are met then these pots may be moved as well the node will become empty and then cluster autoscaler will remove it okay",
    "start": "1485580",
    "end": "1493260"
  },
  {
    "text": "so I think we can do what we want see them yeah okay",
    "start": "1493260",
    "end": "1500750"
  },
  {
    "text": "do I need to do something special about my lightness or readiness probe when",
    "start": "1504950",
    "end": "1510840"
  },
  {
    "text": "dealing with any of these auto scaling modes sorry do I need to do any special",
    "start": "1510840",
    "end": "1516270"
  },
  {
    "text": "anything specially my cold like typically we find that going like and really easy by ideas oh no no no you",
    "start": "1516270",
    "end": "1522990"
  },
  {
    "text": "shouldn't do like anything like super special your readiness probe should indicate whether your application is",
    "start": "1522990",
    "end": "1529650"
  },
  {
    "text": "ready to take traffic and let's draw for the cases where you have auto scaling in",
    "start": "1529650",
    "end": "1536520"
  },
  {
    "text": "the picture and as well as in two cases where you don't have auto scaling in the picture basically when your application",
    "start": "1536520",
    "end": "1542310"
  },
  {
    "text": "is ready the networking part of kubernetes adds the pods to the service and reports my receive traffic if your",
    "start": "1542310",
    "end": "1549150"
  },
  {
    "text": "application and declares that is not ready or it's not ready yet then the pods are not added to the",
    "start": "1549150",
    "end": "1556080"
  },
  {
    "text": "service and they will receive no traffic so the readiness probe is not done for",
    "start": "1556080",
    "end": "1561750"
  },
  {
    "text": "the auto scaling itself it's done for to make your networking aware of the state",
    "start": "1561750",
    "end": "1569040"
  },
  {
    "text": "of your application it wouldn't be that important if you had a very stable situation so you start your pods at",
    "start": "1569040",
    "end": "1577890"
  },
  {
    "text": "let's say January the first and they run on the same machine for for a year it wouldn't make any difference and if they",
    "start": "1577890",
    "end": "1583860"
  },
  {
    "text": "were I mean ready for all the time however if you start pods top pod start positive and stop pod it's important to",
    "start": "1583860",
    "end": "1590760"
  },
  {
    "text": "let your network Inc know what is the state of the application if you don't",
    "start": "1590760",
    "end": "1596220"
  },
  {
    "text": "specify anything then as soon as the pods started almost as soon as post arted the POTUS added to the service but",
    "start": "1596220",
    "end": "1604650"
  },
  {
    "text": "your application might still be initializing for the next five minutes and if the request arrives then it won't",
    "start": "1604650",
    "end": "1611610"
  },
  {
    "text": "be handled properly okay there",
    "start": "1611610",
    "end": "1619599"
  },
  {
    "text": "you mentioned that I shouldn't stop accepting new requests when I receive a sitcom because of as a movement event or",
    "start": "1621929",
    "end": "1631139"
  },
  {
    "text": "a scaled-down event so what actually do when I get sick Tom because my",
    "start": "1631139",
    "end": "1636360"
  },
  {
    "text": "understanding was that I would set readiness to false and continue",
    "start": "1636360",
    "end": "1641460"
  },
  {
    "text": "processing requests I have and when I'm done with that stop but if I am still supposed to get new requests when when",
    "start": "1641460",
    "end": "1647759"
  },
  {
    "text": "do I stop doing them okay so let me answer it that in more details so what",
    "start": "1647759",
    "end": "1653879"
  },
  {
    "text": "happens when your pod declares that is an unready or its start termination process it is removed from the service",
    "start": "1653879",
    "end": "1662159"
  },
  {
    "text": "endpoint list and all of the coup proxies in your cluster will be notified",
    "start": "1662159",
    "end": "1668549"
  },
  {
    "text": "about it however this cube proxies have a control look and control loop basically execute",
    "start": "1668549",
    "end": "1675419"
  },
  {
    "text": "every ten seconds so if you are unlucky it will take ten seconds from the time",
    "start": "1675419",
    "end": "1682559"
  },
  {
    "text": "that the pot is declared as unready to the time when it is removed from queue",
    "start": "1682559",
    "end": "1688619"
  },
  {
    "text": "proxy on a particular node if your cluster is big and there is a lot of",
    "start": "1688619",
    "end": "1693990"
  },
  {
    "text": "going on there it may take longer like 20 seconds on 30 seconds and during that",
    "start": "1693990",
    "end": "1699600"
  },
  {
    "text": "time all nodes or some nodes in your cluster will still think that the pot on",
    "start": "1699600",
    "end": "1706019"
  },
  {
    "text": "the other side is there and will receive will be able to accept new traffic so",
    "start": "1706019",
    "end": "1714629"
  },
  {
    "text": "it's a problem of letting all of the components in your cluster know what is going on and it takes time also on some",
    "start": "1714629",
    "end": "1723450"
  },
  {
    "text": "cloud providers updating load balancers take some time so even if you declare",
    "start": "1723450",
    "end": "1730799"
  },
  {
    "text": "the deposition already this information is not applied to all components",
    "start": "1730799",
    "end": "1736200"
  },
  {
    "text": "immediately it takes time so and during this time the rapist will be flowing so you either have to",
    "start": "1736200",
    "end": "1743600"
  },
  {
    "text": "accept them or your clients will get errors so that's how it works so what do",
    "start": "1743600",
    "end": "1752029"
  },
  {
    "text": "I do wait one minute and then minion yeah and then you will receive sick a kill signal",
    "start": "1752029",
    "end": "1759350"
  },
  {
    "text": "if what at this moment your application will deplete it's graceful termination",
    "start": "1759350",
    "end": "1766369"
  },
  {
    "text": "time and will receive kill signal or you can terminate after one minute or you can also measure how measure it",
    "start": "1766369",
    "end": "1775729"
  },
  {
    "text": "on your cloud provider in your deployment how fast it reacts and wait for that amount of time",
    "start": "1775729",
    "end": "1782200"
  },
  {
    "text": "everything depends on specific environment but it's important to pay attention to these little details to get",
    "start": "1782200",
    "end": "1788779"
  },
  {
    "text": "the best experience with auto-scaling okay so for this cluster autoscaler -",
    "start": "1788779",
    "end": "1801679"
  },
  {
    "text": "what do we need to use do we need to use a KS or ETS kind of services or sorry I",
    "start": "1801679",
    "end": "1811220"
  },
  {
    "text": "didn't understand the question could you please repeat it so my question is for cluster auto-scale to work do we need to",
    "start": "1811220",
    "end": "1818720"
  },
  {
    "text": "use kubernetes services like a KS or e KS or you know you don't have it however",
    "start": "1818720",
    "end": "1828279"
  },
  {
    "text": "services like hosted version of services may have cluster out the scaler built in",
    "start": "1828279",
    "end": "1833450"
  },
  {
    "text": "and it will be easier for you to deploy it on DK it's just a single command to",
    "start": "1833450",
    "end": "1840019"
  },
  {
    "text": "enable it I believe on Azure it is also like built into ETS so we can enables it",
    "start": "1840019",
    "end": "1848119"
  },
  {
    "text": "quicker but on other cloud providers or if you are running kubernetes in custom",
    "start": "1848119",
    "end": "1853700"
  },
  {
    "text": "mode on clock infrastructure you just need to execute a little bit more manual",
    "start": "1853700",
    "end": "1859279"
  },
  {
    "text": "step so start cluster autoscaler pod manually and configure it in proper way",
    "start": "1859279",
    "end": "1865729"
  },
  {
    "text": "but you can use it on all of the cloud infrastructure animation heat on some",
    "start": "1865729",
    "end": "1871730"
  },
  {
    "text": "cloud providers it's a little bit easier on some other it's a little bit more difficult but in general you should not",
    "start": "1871730",
    "end": "1881159"
  },
  {
    "text": "have like much troubles with with it and does it work with VMware also sorry",
    "start": "1881159",
    "end": "1887909"
  },
  {
    "text": "VMware does it work with VMware and not from the core repository I believe some",
    "start": "1887909",
    "end": "1895289"
  },
  {
    "text": "work to support VMware was I was done but I'm not exactly sure about the",
    "start": "1895289",
    "end": "1901440"
  },
  {
    "text": "details hi thank you for the talk a",
    "start": "1901440",
    "end": "1907559"
  },
  {
    "text": "quick question about the testing and benchmarking I'm quite new to the",
    "start": "1907559",
    "end": "1913620"
  },
  {
    "text": "community so this might be a silly question once I configure my cluster with the auto scaling do you recommend any tool",
    "start": "1913620",
    "end": "1920130"
  },
  {
    "text": "to test or benchmark D configuration yes the general advice is to always test",
    "start": "1920130",
    "end": "1927270"
  },
  {
    "text": "your deployments with something that closely resembles the traffic that you",
    "start": "1927270",
    "end": "1933510"
  },
  {
    "text": "will get in your application however as I mentioned with VP a please be careful",
    "start": "1933510",
    "end": "1943110"
  },
  {
    "text": "so VP a star C story and if you run a",
    "start": "1943110",
    "end": "1948120"
  },
  {
    "text": "benchmark and then don't run a benchmark for let's say a week and then you enable",
    "start": "1948120",
    "end": "1955080"
  },
  {
    "text": "full traffic at once and then VP a will be slow to responding to this traffic so",
    "start": "1955080",
    "end": "1961380"
  },
  {
    "text": "it will run basically that there was nothing there for a week it would recommend small percent it will take a",
    "start": "1961380",
    "end": "1967080"
  },
  {
    "text": "while before it starts recommending like bigger pod sizes right this is generation I'm sorry for that reason",
    "start": "1967080",
    "end": "1974250"
  },
  {
    "text": "usually after running a benchmark you should mmm kind of lock vertical path autoscaler",
    "start": "1974250",
    "end": "1981179"
  },
  {
    "text": "with specifying it's minimal a container size and ideally also put the",
    "start": "1981179",
    "end": "1986340"
  },
  {
    "text": "recommended pot sizes to your deployment specification right thank you I will get",
    "start": "1986340",
    "end": "1992090"
  },
  {
    "text": "more details on that data thank you",
    "start": "1992090",
    "end": "1996380"
  },
  {
    "text": "do you have any tips for scaling gpu-accelerated nodes that take a long time to provision",
    "start": "2002480",
    "end": "2009110"
  },
  {
    "text": "windows not know I said gpu-accelerated knows GPU nodes GPU knows",
    "start": "2009110",
    "end": "2017669"
  },
  {
    "text": "so her cluster autoscaler supports the GPUs it waits until they are properly",
    "start": "2017669",
    "end": "2025980"
  },
  {
    "text": "spinned but well kind of do nothing to make things faster so I don't have like",
    "start": "2025980",
    "end": "2034970"
  },
  {
    "text": "and you like meaning forget the advices for scaling GP you knows it also depends",
    "start": "2034970",
    "end": "2042149"
  },
  {
    "text": "what type of traffic you run it are are you running bad jobs or some serving workloads zoomerton's okay so the",
    "start": "2042149",
    "end": "2053099"
  },
  {
    "text": "general advice is to kind of try to fight with long pot or node and startup",
    "start": "2053099",
    "end": "2060898"
  },
  {
    "text": "time with over-provisioning so you need to over provision to cover all like all",
    "start": "2060899",
    "end": "2066030"
  },
  {
    "text": "the small spikes where we've enough capacity and then said for example HP a",
    "start": "2066030",
    "end": "2074658"
  },
  {
    "text": "target to let's say low but still reasonable value so as soon as it goes",
    "start": "2074659",
    "end": "2080638"
  },
  {
    "text": "above this like standard standard spice it starts adding the capacity but you",
    "start": "2080639",
    "end": "2086158"
  },
  {
    "text": "should not depend on HP a with handlink the regular specs that you frequently",
    "start": "2086159",
    "end": "2092700"
  },
  {
    "text": "get like final 5 10 20 % spikes because it will be like too slow to answer and",
    "start": "2092700",
    "end": "2100050"
  },
  {
    "text": "note our proposal with the appropriate timing",
    "start": "2100050",
    "end": "2105380"
  },
  {
    "text": "hello I want to set up a cloud bursting and I looked into Federation my problem",
    "start": "2109950",
    "end": "2116289"
  },
  {
    "text": "is that v1 is discontinued and v2 doesn't work with our vendor probably",
    "start": "2116289",
    "end": "2121930"
  },
  {
    "text": "stuff because it uses different kind of resources not the classic ones do you have any tips or pointers I'm not up to",
    "start": "2121930",
    "end": "2131109"
  },
  {
    "text": "date with the current status of Federation so I don't know what to",
    "start": "2131109",
    "end": "2136119"
  },
  {
    "text": "recommend to you okay sorry okay it looks like that we are ran out of time",
    "start": "2136119",
    "end": "2142240"
  },
  {
    "text": "so thank you again for being here today and for all the questions [Applause]",
    "start": "2142240",
    "end": "2149040"
  },
  {
    "text": "[Music]",
    "start": "2149040",
    "end": "2151860"
  }
]