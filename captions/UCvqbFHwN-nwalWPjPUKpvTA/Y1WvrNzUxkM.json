[
  {
    "text": "hello everybody nice steaks I'm Priyanka",
    "start": "390",
    "end": "6450"
  },
  {
    "text": "and I'm your track track show host I am director of technical evangelism at a",
    "start": "6450",
    "end": "11460"
  },
  {
    "text": "company called gitlab and I also serve on the cloud native computing foundation sport the cloud native computing",
    "start": "11460",
    "end": "18359"
  },
  {
    "text": "foundation same as CN CF the organizer of this show so thank you so much for",
    "start": "18359",
    "end": "23430"
  },
  {
    "text": "coming to this talk just a friendly reminder that please read it after the session on scat comm that will really",
    "start": "23430",
    "end": "30630"
  },
  {
    "text": "help us with programming as we go along I serve on the program committee as well and there's so many sessions and just",
    "start": "30630",
    "end": "36600"
  },
  {
    "text": "having this feedback from you really really helps so we're all here because",
    "start": "36600",
    "end": "42090"
  },
  {
    "text": "we care about kubernetes one way or another and when of this talk is very",
    "start": "42090",
    "end": "47340"
  },
  {
    "text": "interesting to me because it speaks to the operational side of the story where it's about costing resource allocation",
    "start": "47340",
    "end": "54210"
  },
  {
    "text": "how much money are we spending and the reality is that accounting with qu4",
    "start": "54210",
    "end": "60239"
  },
  {
    "text": "kubernetes pods was much harder until recently there are just gaps with the",
    "start": "60239",
    "end": "65400"
  },
  {
    "text": "release of pod overhead that's changed and things are much better now and",
    "start": "65400",
    "end": "70650"
  },
  {
    "text": "that's what our speaker today Erik Ernst senior software engineer at Intel is going to tell us about erik has been",
    "start": "70650",
    "end": "77850"
  },
  {
    "text": "working in open source for a while and he is a prolific contributor to various projects committees and he is of course",
    "start": "77850",
    "end": "85020"
  },
  {
    "text": "a kubernetes contributor so with that Eric please take it away thank you so",
    "start": "85020",
    "end": "91530"
  },
  {
    "text": "I'm gonna be talking about pod overhead it's your last chance to leave the room if this isn't what you're interested in",
    "start": "91530",
    "end": "96710"
  },
  {
    "text": "and a lot of this talk is gonna be based around you know how can we effectively",
    "start": "96710",
    "end": "102659"
  },
  {
    "text": "use sandbox runtimes in kubernetes but also you know if you're using the traditional runtime where the gaps and",
    "start": "102659",
    "end": "109530"
  },
  {
    "text": "how will this improve your situation as well so I am a software engineer I am a technical lead for a team working on the",
    "start": "109530",
    "end": "116700"
  },
  {
    "text": "cotta containers project a sandbox runtime which is kind of why I'm pretty interested in making sure this works and",
    "start": "116700",
    "end": "124350"
  },
  {
    "text": "you know I with the help from different folks on sig note put together the cap and then did the implementation for this",
    "start": "124350",
    "end": "129869"
  },
  {
    "text": "feature so let's get into it I wanted to take a bit of time to really",
    "start": "129869",
    "end": "138210"
  },
  {
    "text": "walk through what happens when you have um oh then you keep cuddle apply or control apply or however you want to do",
    "start": "138210",
    "end": "143400"
  },
  {
    "text": "it's fine and what happens between that point to when it's actually running on a",
    "start": "143400",
    "end": "149850"
  },
  {
    "text": "worker node somewhere the reason I want to do this is it's going to kind of highlight where some of the gaps are and what some of the changes were that I",
    "start": "149850",
    "end": "156600"
  },
  {
    "text": "needed to make for this feature and then you know through this I'm gonna be able to identify what are these overheads and",
    "start": "156600",
    "end": "162900"
  },
  {
    "text": "where are they and then say this is the feature itself and hopefully after that",
    "start": "162900",
    "end": "169470"
  },
  {
    "text": "I'll be able to show you and convince you why you should care about this or maybe not we'll see so at the very very",
    "start": "169470",
    "end": "177450"
  },
  {
    "text": "beginning I wish I could I'm just gonna kind of come over here a little bit",
    "start": "177450",
    "end": "183770"
  },
  {
    "text": "you're sitting there on your client node and you're gonna use gibb CTL cube cuddle anything else and you have your",
    "start": "183770",
    "end": "190620"
  },
  {
    "text": "llamo and you're gonna go through and submit it so basically cube cuddle is your first",
    "start": "190620",
    "end": "196830"
  },
  {
    "text": "chance of rejection in this whole process client-side is gonna go through and verify is this gamal formatted",
    "start": "196830",
    "end": "204000"
  },
  {
    "text": "correctly or is this pods back correct so you know it has a view of what the pod spec API is at this point so if it's",
    "start": "204000",
    "end": "210720"
  },
  {
    "text": "not I'm just gonna reject it but let's say it's okay it's gonna do go ahead and create a request which it will then",
    "start": "210720",
    "end": "216870"
  },
  {
    "text": "submit to your communities cluster so we're assuming you already have a control plane up and everything else so",
    "start": "216870",
    "end": "222480"
  },
  {
    "text": "the first hour entry point to the cluster is going to be the API server so the API server is on a control node and",
    "start": "222480",
    "end": "228840"
  },
  {
    "text": "what the API server is gonna do is first it's gonna do authentication it's gonna say who are you you sent me this request",
    "start": "228840",
    "end": "236160"
  },
  {
    "text": "do I do I know you and if it does ok let's go to the next step that's gonna do authorization of are you even allowed",
    "start": "236160",
    "end": "243180"
  },
  {
    "text": "to run you know submit a job to create a pod can you can you create this pod object in the cluster let's assume that",
    "start": "243180",
    "end": "249420"
  },
  {
    "text": "we have everything set up correctly we have the cube config and everything and it goes through the next thing we're",
    "start": "249420",
    "end": "255329"
  },
  {
    "text": "gonna hit our Domitian controllers and admission controller they so there's a whole bunch of them and they kind of",
    "start": "255329",
    "end": "260370"
  },
  {
    "text": "have two different phases that they can run in they can do a mutation which essentially is a high C what you're",
    "start": "260370",
    "end": "266010"
  },
  {
    "text": "doing but I to change it slightly an example of mutation would be to update what the policy is for pulling the container",
    "start": "266010",
    "end": "272129"
  },
  {
    "text": "image you could have in here and a mission control that just says always pull latest so what it'll do is it'll",
    "start": "272129",
    "end": "279479"
  },
  {
    "text": "look at your pods back say okay I'm gonna augment what you have and I'm gonna mutate it I'm gonna change it after that it's going to go ahead and do",
    "start": "279479",
    "end": "285479"
  },
  {
    "text": "a validation phases for different emission controllers one of the relevant",
    "start": "285479",
    "end": "290719"
  },
  {
    "text": "validation ones would be resource quota so you described your workload and you said that I have certain limits i I need",
    "start": "290719",
    "end": "297780"
  },
  {
    "text": "this much CPU this much memory and it's going to go through and say AHA yes you have this much available in your",
    "start": "297780",
    "end": "303060"
  },
  {
    "text": "namespace still yes you can go apply so there'd be an example of validation so let's say that everything is good the",
    "start": "303060",
    "end": "309419"
  },
  {
    "text": "next step is to set it all in stone we're gonna use persistent storage we're gonna write it into Etsy D so that city",
    "start": "309419",
    "end": "315150"
  },
  {
    "text": "is kind of the view of what is the state of the cluster and everything else everybody's going to kind of reference",
    "start": "315150",
    "end": "320460"
  },
  {
    "text": "this to say you know is this object exists in this case we write it in there",
    "start": "320460",
    "end": "326340"
  },
  {
    "text": "our pot is called awesome because I'm not creative and it doesn't have a no dam associated with it so we are at that",
    "start": "326340",
    "end": "333300"
  },
  {
    "text": "wonderful state of pending so if you've seen pending this is this is where you're at it's the objects there we know",
    "start": "333300",
    "end": "340020"
  },
  {
    "text": "it's there but we haven't taken any action yet so after that what we would want to do is scheduling and we do that",
    "start": "340020",
    "end": "345569"
  },
  {
    "text": "in Cube scheduler which is very well named and basically this is going to be",
    "start": "345569",
    "end": "350940"
  },
  {
    "text": "like a reconciliation controller and pretty much everything in kubernetes is almost like a reconciliation controller where it's going to go through it's",
    "start": "350940",
    "end": "356729"
  },
  {
    "text": "going to look at C D it's going to go through that pod list and see are there pods that don't have a pod binding they",
    "start": "356729",
    "end": "362550"
  },
  {
    "text": "don't have a node binding associated with it if there are start doing something let's let's assign it so it's",
    "start": "362550",
    "end": "367800"
  },
  {
    "text": "going to do that it's gonna see awesome and it's gonna go and schedule to it so",
    "start": "367800",
    "end": "373250"
  },
  {
    "text": "scheduling what it ends up doing is it doesn't really look at nodes and say you know how many what's the current",
    "start": "373250",
    "end": "379860"
  },
  {
    "text": "utilization on the node in a real time what it does is it looks at the requests of the pods that are running on those",
    "start": "379860",
    "end": "386550"
  },
  {
    "text": "nodes and it will you know statically look at it like hey you ran this workload and I said it need two CPUs and",
    "start": "386550",
    "end": "392909"
  },
  {
    "text": "you have ten CPU so of course you have room it's not actually checking in real-time that your two CPU workload isn't actually using anything",
    "start": "392909",
    "end": "399080"
  },
  {
    "text": "so it's a bit static in that way but it's going to go through two phases so essentially have a node list it's going",
    "start": "399080",
    "end": "405470"
  },
  {
    "text": "to go through and it's going to do first predication and then prioritization so predication is just binary it's does it",
    "start": "405470",
    "end": "411889"
  },
  {
    "text": "fit yes or no and there's some different characteristics little check but essentially do you have room to even",
    "start": "411889",
    "end": "418159"
  },
  {
    "text": "take this workload you filter through that now you have a remaining list of nodes which one is the best you know",
    "start": "418159",
    "end": "424699"
  },
  {
    "text": "kind of heuristic aliy what is it wasn't appropriate no it may be based on the amount of resources remaining or may be based on you know it already has an",
    "start": "424699",
    "end": "430849"
  },
  {
    "text": "image on it or different things like this so let's say that it found it it picked a node and it assigned it to",
    "start": "430849",
    "end": "436819"
  },
  {
    "text": "worker another creative name so we finally now move on from pending to pod",
    "start": "436819",
    "end": "442009"
  },
  {
    "text": "scheduled everything exciting so we had like a 10 line go program and we did all these steps and we're finally getting",
    "start": "442009",
    "end": "447199"
  },
  {
    "text": "closer to maybe starting to do work so we'll jump from the worker node over to",
    "start": "447199",
    "end": "452810"
  },
  {
    "text": "our from the control plane node over to a worker node and this is where we start interfacing with cubelet which also is",
    "start": "452810",
    "end": "459139"
  },
  {
    "text": "kind of doing some reconciliation it's looking at CD and saying what are the pods that belong to me out of these ones",
    "start": "459139",
    "end": "467169"
  },
  {
    "text": "what are the pods that are not scheduled and actually running at this point if there are any I need to take corrective",
    "start": "467169",
    "end": "473360"
  },
  {
    "text": "action so that's what this essentially what this giant four loop pod sync is doing and it's not gonna do it all by",
    "start": "473360",
    "end": "481759"
  },
  {
    "text": "itself what cubelet is going to do it's going to create a pod c group that it's gonna",
    "start": "481759",
    "end": "487520"
  },
  {
    "text": "hand off and say hey you over there can you create this pod for me and you over there is the other side of the CRI API",
    "start": "487520",
    "end": "494060"
  },
  {
    "text": "so CRI is the container runtime interface so classic implementers",
    "start": "494060",
    "end": "499569"
  },
  {
    "text": "default is docker shrimp but the ones that I really like our container D or",
    "start": "499569",
    "end": "504919"
  },
  {
    "text": "cryo and basically the CRI implementation is the one who's kind of",
    "start": "504919",
    "end": "509990"
  },
  {
    "text": "in charge of actually creating the pod pulling the image calling C and I to create a network device and everything",
    "start": "509990",
    "end": "515810"
  },
  {
    "text": "else but in actuality they're not really doing anything either well they are they're doing a lot of",
    "start": "515810",
    "end": "521089"
  },
  {
    "text": "things but we'll get into that basically we hit this pod secret sync and so stepping back",
    "start": "521089",
    "end": "528230"
  },
  {
    "text": "how do we do this okay so we create a pod c group and then its gonna call into CRI and say create a sandbox for me",
    "start": "528230",
    "end": "535190"
  },
  {
    "text": "which is essentially here's this pod infrastructure create this for me and that's asking CRI to do it and cRIO call",
    "start": "535190",
    "end": "541250"
  },
  {
    "text": "C and I created ve Thor or whatever kind of device you're using drop it in the network namespace for you and then it's",
    "start": "541250",
    "end": "546350"
  },
  {
    "text": "going to call CRI and say can you create the Samba or it's gonna call osei at this point and oh yeah it's going to do",
    "start": "546350",
    "end": "552590"
  },
  {
    "text": "it so osya is the open containers initiative and there's several different runtimes that kind of meet this runtime",
    "start": "552590",
    "end": "558980"
  },
  {
    "text": "spec the canonical like default solution you're always gonna use out of the box is gonna be run see and this is using",
    "start": "558980",
    "end": "565850"
  },
  {
    "text": "you know namespaces cgroups dropping capabilities all the typical Linux kernel primitives they kind of are the",
    "start": "565850",
    "end": "573200"
  },
  {
    "text": "goodness of what a container actually is so that's wonderful but then there are also other runtimes why do we have them",
    "start": "573200",
    "end": "578480"
  },
  {
    "text": "some people like sandboxing some people say that namespaces aren't enough for me I want to do a little bit extra so cata",
    "start": "578480",
    "end": "584870"
  },
  {
    "text": "is an example of one gee visor is another so I have those two on there they have different approaches we'll get",
    "start": "584870",
    "end": "590240"
  },
  {
    "text": "into it so basically CRI is gonna pass down and say OC I can you actually do this for me",
    "start": "590240",
    "end": "595400"
  },
  {
    "text": "it's the one who's going to deal with the actual lifecycle management of the containers and up the button so if we're",
    "start": "595400",
    "end": "603500"
  },
  {
    "text": "to go even further in the run C case what happens you had the pod C group run C's called",
    "start": "603500",
    "end": "610160"
  },
  {
    "text": "and says you know creates container for me everything else here's the image it's going to actually you know again use the namespaces create the container put it",
    "start": "610160",
    "end": "616340"
  },
  {
    "text": "in there but for the sandbox it actually puts an infra container in there which is kind of a placeholder that nobody is",
    "start": "616340",
    "end": "622040"
  },
  {
    "text": "actually gonna care about unless you're trying to do reaping of processes and things like this so this is something that's kind of necessary for cleanup",
    "start": "622040",
    "end": "628880"
  },
  {
    "text": "you know Ultra Hall but it's something running in there that nobody cares about so keep that in mind if you were using",
    "start": "628880",
    "end": "635360"
  },
  {
    "text": "Java what we do is we actually launch a virtual machine and then using the guest",
    "start": "635360",
    "end": "640970"
  },
  {
    "text": "Linux kernel inside that virtual machine will create a container so in this case you have a pod C group and there you're",
    "start": "640970",
    "end": "646730"
  },
  {
    "text": "gonna do your business which is you're gonna have a VM M could be firecracker it could be QM you could be cloud hypervisor I'm just several supportive",
    "start": "646730",
    "end": "653870"
  },
  {
    "text": "and then you have these vertigo back-end threads these are essentially hey you want to do networking somebody's got to take it from here and pass it here",
    "start": "653870",
    "end": "659960"
  },
  {
    "text": "that's come to the back end what you're going to do with your vmm those are gonna be running in the Pazzi group and then you actually have a guest",
    "start": "659960",
    "end": "665870"
  },
  {
    "text": "colonel and then you finally have your workloads so there's some things in there that maybe are not being accounted",
    "start": "665870",
    "end": "671570"
  },
  {
    "text": "for and then if you were to look at gee visor what they do is they have like a",
    "start": "671570",
    "end": "678050"
  },
  {
    "text": "century available which is kind of like a user space colonel that they provide and that's the interface that they're",
    "start": "678050",
    "end": "683540"
  },
  {
    "text": "gonna use for the actual container workload so instead of calling into the hosts Linux kernel they say that's not",
    "start": "683540",
    "end": "688580"
  },
  {
    "text": "ideal what we would like to do is use this kind of proxied one over here instead so you have that and you can",
    "start": "688580",
    "end": "694640"
  },
  {
    "text": "they have this emulated machine sentry run a/c sandbox go in there and then you",
    "start": "694640",
    "end": "700040"
  },
  {
    "text": "have a gopher on top of it which similar to the video backend you know you're gonna have to pass and proxy some stuff",
    "start": "700040",
    "end": "705170"
  },
  {
    "text": "in and out so all these things are running on your system and finally you",
    "start": "705170",
    "end": "711770"
  },
  {
    "text": "know we came from a client node up above and went all the way down you finally have your 10 line go program running with just a little bit of overhead and a",
    "start": "711770",
    "end": "718670"
  },
  {
    "text": "little couple hoops and everything else to go the problem is is is really this",
    "start": "718670",
    "end": "724580"
  },
  {
    "text": "isn't accounted for today kind of I'll get into it so obviously I hope you can",
    "start": "724580",
    "end": "731120"
  },
  {
    "text": "see that there are some overheads here and when it comes down to it this some of the pod resources so that pod see",
    "start": "731120",
    "end": "737600"
  },
  {
    "text": "group definitely does not equal to some of containers so if you look at the container and ask each one if you're in",
    "start": "737600",
    "end": "743480"
  },
  {
    "text": "the run C case it's easy you can just go into a subdirectory go into this container slice which is a child of the",
    "start": "743480",
    "end": "749000"
  },
  {
    "text": "parent which is the pod and you can just add those up and say that should be the total up here it's not so when you just",
    "start": "749000",
    "end": "757550"
  },
  {
    "text": "do the summation and containers that's not really gonna be reflective and the way it's kind of handled today is is a",
    "start": "757550",
    "end": "763459"
  },
  {
    "text": "heuristic kind of it's a minimal overhead don't worry about it it is per pod but it's so small we don't we just",
    "start": "763459",
    "end": "770750"
  },
  {
    "text": "were just not accounting for today so what we're gonna do instead is we kind of carve up the node so this is again on",
    "start": "770750",
    "end": "776810"
  },
  {
    "text": "a worker node and you have cube reserved you have system reserved and they may have allocatable so in your I think it's a cubelet",
    "start": "776810",
    "end": "784399"
  },
  {
    "text": "configuration you can define these and set mine where you said so you maybe you have a good 10 gig machine or something",
    "start": "784399",
    "end": "790900"
  },
  {
    "text": "you're just gonna say I'm gonna take a half a gig out of the pool don't schedule you like I'm telling cougar",
    "start": "790900",
    "end": "796970"
  },
  {
    "text": "Tiny's you only have this alligator bull pool just ten minus half a gig and so when you're doing scheduling and",
    "start": "796970",
    "end": "803270"
  },
  {
    "text": "everything else just don't pay attention to that only use this you know you're gonna be full because it's on the side this kind of works okay if you if using",
    "start": "803270",
    "end": "810740"
  },
  {
    "text": "just run see it works okay you're not directly billing some of these overheads but it's it's it's okay but as soon as",
    "start": "810740",
    "end": "817820"
  },
  {
    "text": "you do sandbox friend times that's not gonna work because the overheads are very much not negligible so what we did",
    "start": "817820",
    "end": "827240"
  },
  {
    "text": "was create a feature the feature is called pot overhead and it's available in 1.16 as an Elfa feature 117 it's",
    "start": "827240",
    "end": "834320"
  },
  {
    "text": "gonna be alpha feature didn't get to it which is going to account for these overheads associated with running a pod",
    "start": "834320",
    "end": "839980"
  },
  {
    "text": "so how it works first we had to modify",
    "start": "839980",
    "end": "845210"
  },
  {
    "text": "the pods back as well as the runtime class object as well to kind of add a",
    "start": "845210",
    "end": "850880"
  },
  {
    "text": "couple of fields to track pot overhead I'll show it in a second then after that",
    "start": "850880",
    "end": "856940"
  },
  {
    "text": "at admission time you know we don't want users have to do anything or know anything about this so they should not",
    "start": "856940",
    "end": "863000"
  },
  {
    "text": "ever write this in the pods back it'll get rejected if you do what ends up happening is we have an admission controller that does a mutation at the",
    "start": "863000",
    "end": "870560"
  },
  {
    "text": "beginning and it says if you're using a runtime class I'm gonna go ahead and if that runtime class has an overhead",
    "start": "870560",
    "end": "876200"
  },
  {
    "text": "defined I'm gonna copy it into the pods back and then from then on I'm going to account for it so what does this look",
    "start": "876200",
    "end": "883280"
  },
  {
    "text": "like we have a runtime class definition up above I hope you can see it runtime",
    "start": "883280",
    "end": "888950"
  },
  {
    "text": "class is pretty basic and it's your way in kubernetes I guess I meant to talk",
    "start": "888950",
    "end": "894110"
  },
  {
    "text": "about this a bit more before you I said that you can call an OC I runtime CRI right CRI calls an OS yeah that one say",
    "start": "894110",
    "end": "900110"
  },
  {
    "text": "actually does the work everyone just kind of passes the buck down what CRI does it's very important is it will look",
    "start": "900110",
    "end": "905720"
  },
  {
    "text": "at the pods back and in the pods back you can say I want to use this runtime",
    "start": "905720",
    "end": "910730"
  },
  {
    "text": "class so it's a way that on a single node you can support multiple OCI runtimes which is great because maybe",
    "start": "910730",
    "end": "916370"
  },
  {
    "text": "for most your workloads you just want to use run see but one of them super sketchy so you want to use the sandbox runtime that's the way it works",
    "start": "916370",
    "end": "922700"
  },
  {
    "text": "so runtime class is the way that we do this so this is the object form and if",
    "start": "922700",
    "end": "927860"
  },
  {
    "text": "you look at it we added the last four lines very exciting there's overhead pod fixed which I can",
    "start": "927860",
    "end": "933500"
  },
  {
    "text": "talk about later memory and CPU so in doing this I'm saying that this is using",
    "start": "933500",
    "end": "939470"
  },
  {
    "text": "katak containers with the fire cracker vmm I want to reserve for this pot and",
    "start": "939470",
    "end": "945260"
  },
  {
    "text": "extra 130 megabytes because if you run a full guest Linux kernel sorry it costs that much money",
    "start": "945260",
    "end": "950480"
  },
  {
    "text": "for the VM m+ the guest and everything else and from a CPU is kind of an arbitrary number that I put in there now",
    "start": "950480",
    "end": "958430"
  },
  {
    "text": "what happens is again from a user you shouldn't have to change anything I don't want you to have to worry about what is the pot overhead what runtime",
    "start": "958430",
    "end": "964640"
  },
  {
    "text": "class am I using and what is you know they just shouldn't be involved in this process so all that it has is requests",
    "start": "964640",
    "end": "971180"
  },
  {
    "text": "and limits for your actual workload itself so this is the traditional what you should be doing anyway just saying I'm running busy box it's gonna take 100",
    "start": "971180",
    "end": "979010"
  },
  {
    "text": "Meg's that's a lot and 100 millisievert that's all and you see that it has a",
    "start": "979010",
    "end": "984560"
  },
  {
    "text": "runtime class name that's defining the handler interesting hey I want this workload again don't use run see I want",
    "start": "984560",
    "end": "991130"
  },
  {
    "text": "to use kata instead what happens though is you apply it it goes through the admission controller and then if you go",
    "start": "991130",
    "end": "997940"
  },
  {
    "text": "ahead and pull out and say give me the description of this pod what's in it let me let me get some enamel you'll see",
    "start": "997940",
    "end": "1003940"
  },
  {
    "text": "that now the overhead is actually inside of there which is exactly what we expect there's also a validation phase where",
    "start": "1003940",
    "end": "1009430"
  },
  {
    "text": "it'll go ahead and check to see you know did someone else set it and everything",
    "start": "1009430",
    "end": "1014920"
  },
  {
    "text": "else and if they did set it doesn't match the original value but that's that's fine the important thing is if we",
    "start": "1014920",
    "end": "1020470"
  },
  {
    "text": "skip all the D the end you know we skipped it whole flow and and cubelet got it cubelet created the pod c group",
    "start": "1020470",
    "end": "1025810"
  },
  {
    "text": "for me which i'm now allowed to run in as a runtime you'll see that I requested 100 millisieverts and what I have",
    "start": "1025810",
    "end": "1033250"
  },
  {
    "text": "instead is 350 ml of CPUs and someone math for me 230 Meg's of overhead so",
    "start": "1033250",
    "end": "1040030"
  },
  {
    "text": "it's accounted for that's that's a good amount of space I know that you know you can't run the VM M in this amount of RAM",
    "start": "1040030",
    "end": "1046660"
  },
  {
    "text": "so stepping all the way back and look at the whole picture what parts did I have to change one it's",
    "start": "1046660",
    "end": "1052090"
  },
  {
    "text": "a new feature and if you do like a cap and go through and do a user visible feature you're gonna have to create a feature gate which essentially says this",
    "start": "1052090",
    "end": "1059140"
  },
  {
    "text": "seems like a good idea but you have to opt in to use it so it has to be out of the running path",
    "start": "1059140",
    "end": "1065140"
  },
  {
    "text": "of the rest of the cluster so a lot of checks of saying if feature gates enabled do this so they had to be added",
    "start": "1065140",
    "end": "1071520"
  },
  {
    "text": "had to modify the pods back which is part of the core API which is fun because then I had to go through an API review and everyone was really nice and",
    "start": "1071520",
    "end": "1078280"
  },
  {
    "text": "I it went well and then runtime class had to be updated to add this fields as well API server again we created a new",
    "start": "1078280",
    "end": "1085270"
  },
  {
    "text": "runtime class in mission controller and we also had to update the resource quota because one way or they're saying is",
    "start": "1085270",
    "end": "1092080"
  },
  {
    "text": "there room for this does this user or namespace have room to allocate for this new workload you've got to include the",
    "start": "1092080",
    "end": "1097960"
  },
  {
    "text": "patil red cube scheduler both the predicate as well as a prioritization algorithms need to take it into account",
    "start": "1097960",
    "end": "1104110"
  },
  {
    "text": "and then cubelet as well for both sizing but also how they do a fiction handling because if you're just checking their",
    "start": "1104110",
    "end": "1110680"
  },
  {
    "text": "usage against what they're requested it's not really fair unless you consider that it is a sandbox or maybe this fair",
    "start": "1110680",
    "end": "1117420"
  },
  {
    "text": "so the changes in the code itself they're pretty lame I was kind of",
    "start": "1117420",
    "end": "1122560"
  },
  {
    "text": "surprised how many times in kubernetes and different areas of the project they do the same exact thing which is that",
    "start": "1122560",
    "end": "1128260"
  },
  {
    "text": "for loop where they go through and sum up the requests of all the containers compared against any containers take the",
    "start": "1128260",
    "end": "1134380"
  },
  {
    "text": "bigger of the two and then return and say this is what a pot is so I had to go in there and see well technically it's",
    "start": "1134380",
    "end": "1140530"
  },
  {
    "text": "not exactly that we also want to take into account the overhead of the pot infrastructure itself if so defined in",
    "start": "1140530",
    "end": "1146740"
  },
  {
    "text": "the feature is enabled so really complex code if statement so now we kind of get",
    "start": "1146740",
    "end": "1152620"
  },
  {
    "text": "into the who cares aspect of it so as a user why do you care if you were",
    "start": "1152620",
    "end": "1159490"
  },
  {
    "text": "severely over constrained you just don't run a workload it you can't there's no",
    "start": "1159490",
    "end": "1165490"
  },
  {
    "text": "room at this point and I'll show an example of that if you're over constrained then it's I would say it's",
    "start": "1165490",
    "end": "1172030"
  },
  {
    "text": "even worse because at least in the first case you fail and you're like okay that didn't work what am I going to do in the over constrain case what you'll have is",
    "start": "1172030",
    "end": "1178290"
  },
  {
    "text": "inconsistent and kind of poor weird performance because essentially what you do is you have a workload running but at",
    "start": "1178290",
    "end": "1183940"
  },
  {
    "text": "the same time you're starving the vmm you're starving the V CPU threads you're starving the virtio backend you're",
    "start": "1183940",
    "end": "1190270"
  },
  {
    "text": "starving all these things so maybe it looks like it's chugging along happy but as far as actual host CPU cycles it's",
    "start": "1190270",
    "end": "1196479"
  },
  {
    "text": "it's kind of garbage and inconsistent and I think I won't do that let me see",
    "start": "1196479",
    "end": "1202899"
  },
  {
    "text": "if I can type in front of you with shaky hands so I have a cluster up now I'm",
    "start": "1202899",
    "end": "1213369"
  },
  {
    "text": "just kind of like what we were showing and if I look at a tiny deployment so I have a pod it's got two containers in it",
    "start": "1213369",
    "end": "1220090"
  },
  {
    "text": "and they only want 20 Meg's each so let's see what happens when we go ahead and apply it it's going to go through",
    "start": "1220090",
    "end": "1229299"
  },
  {
    "text": "and we're gonna sit there and we can wait for a while and we're gonna be this container creating phase which is an area of status and state that I live in",
    "start": "1229299",
    "end": "1236679"
  },
  {
    "text": "quite often and if we were to do a describe cube cut a little I'm not used",
    "start": "1236679",
    "end": "1247539"
  },
  {
    "text": "to the pod slash yet I still have to type it it failed kind of not really clear a typical I",
    "start": "1247539",
    "end": "1254259"
  },
  {
    "text": "kind of failure that you can see now if I were to do and edit oh well let's just",
    "start": "1254259",
    "end": "1259719"
  },
  {
    "text": "look at the actual cgroups itself yeah you'll see this is the 40 Meg if you can",
    "start": "1259719",
    "end": "1267879"
  },
  {
    "text": "I can do Express or whatever but that that's essentially invites how much room is in the pod see group so in that pod",
    "start": "1267879",
    "end": "1273580"
  },
  {
    "text": "see group of 40 megabytes I'm trying to run a be mmm it's gonna launch a kernel it's going to now create for work club that's not gonna happen",
    "start": "1273580",
    "end": "1279279"
  },
  {
    "text": "so if I do an edit and instead of untying class we can use a different",
    "start": "1279279",
    "end": "1288789"
  },
  {
    "text": "runtime this is just gonna come up I hope that's the goal of what I want you",
    "start": "1288789",
    "end": "1294309"
  },
  {
    "text": "to see it takes a while yeah okay and you check it this is now instead of 40",
    "start": "1294309",
    "end": "1300399"
  },
  {
    "text": "megabytes it's 200 megabytes because we reserved in that runtime class",
    "start": "1300399",
    "end": "1306660"
  },
  {
    "text": "get runtime classes describe runtime class either way it's in there please",
    "start": "1306660",
    "end": "1316050"
  },
  {
    "text": "trust me so that's the kind of this over current strained if you have very small ones things are just not going to work",
    "start": "1316050",
    "end": "1321690"
  },
  {
    "text": "as soon as you enter sandbox seen getting back to it who else should care",
    "start": "1321690",
    "end": "1327210"
  },
  {
    "text": "so that's you know a user but administrators should really care too because if you get into let's say just",
    "start": "1327210",
    "end": "1333570"
  },
  {
    "text": "like resource quota what you're gonna end up having is that a user is gonna",
    "start": "1333570",
    "end": "1338730"
  },
  {
    "text": "come in request a small workload and you're gonna say that's fine they're gonna ask you now they're going to say it's fine and then they have another namespace for their other use and you",
    "start": "1338730",
    "end": "1344580"
  },
  {
    "text": "say that's fine that's fine because I have this cube reserved that's totally fine but all the sudden your actual",
    "start": "1344580",
    "end": "1350190"
  },
  {
    "text": "system is severely over constrained on memory you have memory pressure and maybe your control planes is getting",
    "start": "1350190",
    "end": "1355290"
  },
  {
    "text": "requests and everything else so if you're not taking an account you're gonna get an in stable system but for me",
    "start": "1355290",
    "end": "1363030"
  },
  {
    "text": "yeah you can work through this and this is in an example so I talked about the",
    "start": "1363030",
    "end": "1368250"
  },
  {
    "text": "infer container I just ran a couple of pods using a few different runtimes and this is using run C so what it is is the",
    "start": "1368250",
    "end": "1376200"
  },
  {
    "text": "bottom one is container 1 container 2 is stacked on top of that and then the blue line that's the pod so I'm measuring the",
    "start": "1376200",
    "end": "1382830"
  },
  {
    "text": "C groups I'm comparing it against what the pod shows and this is the difference and it's about a megabyte that's not",
    "start": "1382830",
    "end": "1388140"
  },
  {
    "text": "really being accounted for necessarily directly and that's because of that in for a container people are working on",
    "start": "1388140",
    "end": "1393510"
  },
  {
    "text": "ways to kind of pull that back out so we don't need to do it so there's almost zero overhead for run C but well that's",
    "start": "1393510",
    "end": "1399690"
  },
  {
    "text": "the long going from a CPU utilization standpoint there's really not much overhead if we go over to look at qmu",
    "start": "1399690",
    "end": "1406140"
  },
  {
    "text": "there's a tiny bit which kind of makes sense it's like a fraction but you're still almost using just 200 mil CTU",
    "start": "1406140",
    "end": "1412890"
  },
  {
    "text": "which is what the workload is allowed to do if you look at memory this is a horrible picture and I thought about",
    "start": "1412890",
    "end": "1418470"
  },
  {
    "text": "like quickly running like elastic or something like that just so it didn't look like it was a major discrepancy but it is what it is this needs to be",
    "start": "1418470",
    "end": "1424860"
  },
  {
    "text": "optimized when you run a single one you don't get a lot of the benefits of sharing so you just run one pod sorry",
    "start": "1424860",
    "end": "1430440"
  },
  {
    "text": "it's gonna cost you almost a little bit over 150 megabytes and you know this is this is what you think you're doing",
    "start": "1430440",
    "end": "1436760"
  },
  {
    "text": "is for but it's actually doing a hundred-fifty and if you look at firecracker it saves about 30 megabytes",
    "start": "1436760",
    "end": "1442730"
  },
  {
    "text": "footprint so you'll see a little bit less but so obviously kind of too much to ignore from a provider standpoint you",
    "start": "1442730",
    "end": "1450800"
  },
  {
    "text": "can just say I'm going to be very aggressive and reserved a lot in Cube reserved and system reserved and",
    "start": "1450800",
    "end": "1457550"
  },
  {
    "text": "everything should be okay you can calculate maybe you're only running 38 100 pies how much overhead would this be",
    "start": "1457550",
    "end": "1463190"
  },
  {
    "text": "okay put it over there don't worry those nodes have so much memory I don't need to do with it anyway it's fine but the",
    "start": "1463190",
    "end": "1469610"
  },
  {
    "text": "thing that I think is a little bit missing is you can't really charge for it if you just lump sum of these overheads in to know where you're gonna",
    "start": "1469610",
    "end": "1474770"
  },
  {
    "text": "advertise that cost maybe two everybody has to eat it or you just eat it as a provider versus if you're actually",
    "start": "1474770",
    "end": "1479840"
  },
  {
    "text": "looking at pod C group and everything is sized appropriately so you can use a pod C group for what it is to host the",
    "start": "1479840",
    "end": "1485180"
  },
  {
    "text": "infrastructure for running a pod now you can say on a per workload basis this is the cost on the system so you don't have",
    "start": "1485180",
    "end": "1491870"
  },
  {
    "text": "to conservatively over block but you can",
    "start": "1491870",
    "end": "1497210"
  },
  {
    "text": "actually have it in there and start charging people for this which is better accounting as well as stability so the",
    "start": "1497210",
    "end": "1505190"
  },
  {
    "text": "status of the feature it's in 116 again it's an alpha feature overheads are static which is not realistic but it's a",
    "start": "1505190",
    "end": "1512930"
  },
  {
    "text": "really good standpoint I'd be happy to talk more after about that I think you use like vertical pod auto scaling and",
    "start": "1512930",
    "end": "1519110"
  },
  {
    "text": "things like this as long as it overheads enough to have a stable system vpa should take over and do the rest or horizontal or whatever you want to do",
    "start": "1519110",
    "end": "1525620"
  },
  {
    "text": "but basically if you have a lot of network i/o you're gonna see more V CPU overhead or CPU overhead on hosts but",
    "start": "1525620",
    "end": "1532880"
  },
  {
    "text": "anyway it I expect it to move to beta in the 118 release so because of that again",
    "start": "1532880",
    "end": "1539930"
  },
  {
    "text": "it's alpha you have to enable it so the know parts of it that we changed were the cube scheduler the API server as",
    "start": "1539930",
    "end": "1546830"
  },
  {
    "text": "well as cubelet itself so if you go through here like a human iam yeah Mille you would just have to enable this",
    "start": "1546830",
    "end": "1553070"
  },
  {
    "text": "feature gate for those particular paddock components and that's all the",
    "start": "1553070",
    "end": "1560060"
  },
  {
    "text": "content they have I'm happy to answer any questions folks have yeah",
    "start": "1560060",
    "end": "1566350"
  },
  {
    "text": "from what I gather the overhead makes sense when cattle containers in the",
    "start": "1572509",
    "end": "1577619"
  },
  {
    "text": "picture because that's where the resources outside the containers are significant in the ransie it's probably",
    "start": "1577619",
    "end": "1583529"
  },
  {
    "text": "not that relevant in Runcie it depends I I didn't really get into some of the CRI",
    "start": "1583529",
    "end": "1589259"
  },
  {
    "text": "I have like kind of different monitors you would have running as well which is not negligible you can still get in this",
    "start": "1589259",
    "end": "1594450"
  },
  {
    "text": "case where it's pretty dirty where essentially the pot is saying I get this much I'm using all of it absolutely all",
    "start": "1594450",
    "end": "1600480"
  },
  {
    "text": "of it so then you look at the parent C group and they're using all of it and you're trying to run anything else that's pretty sensitive then if now that",
    "start": "1600480",
    "end": "1606899"
  },
  {
    "text": "one's going to have to pick the workload or your control infrastructure so I think it's super relevant there but it's",
    "start": "1606899",
    "end": "1614129"
  },
  {
    "text": "way more relevant if you even sandbox not just comma if you use any of the other sandbox already like you've ug",
    "start": "1614129",
    "end": "1619679"
  },
  {
    "text": "buys your things like this as well",
    "start": "1619679",
    "end": "1623028"
  },
  {
    "text": "do you know what kind of work is being done to like encourage adoption of this with like the manage kubernetes services",
    "start": "1631000",
    "end": "1638530"
  },
  {
    "text": "such as eks and gke nobody's gonna touch it until it's at alpha so I would",
    "start": "1638530",
    "end": "1644560"
  },
  {
    "text": "imagine some of those people would like oh it's interesting maybe it'd come 118 it'd be utilized I imagine they probably",
    "start": "1644560",
    "end": "1652450"
  },
  {
    "text": "have as a service product they probably have a lot of extra plumbing underneath it to do this accounting side of it",
    "start": "1652450",
    "end": "1657910"
  },
  {
    "text": "already but I don't have a lot of visibility and I'm really curious to you though",
    "start": "1657910",
    "end": "1664030"
  },
  {
    "text": "- hopefully quick questions so one is do we do you have any any idea what the",
    "start": "1664030",
    "end": "1671290"
  },
  {
    "text": "best way is to calculate this overhead for the given ones is there like a place to start we could already reference or",
    "start": "1671290",
    "end": "1676420"
  },
  {
    "text": "do we have to do that ourselves I I would say the any kind of runtime class",
    "start": "1676420",
    "end": "1681760"
  },
  {
    "text": "configuration generally comes from a group of engineers they have an idea and they should be able to characterize it",
    "start": "1681760",
    "end": "1687610"
  },
  {
    "text": "as a provider there should be reference I think is there one note now not really you can easily go through I set up that",
    "start": "1687610",
    "end": "1694690"
  },
  {
    "text": "gharana where we're essentially scraping what the containers say they're doing versus what the pods the adviser is",
    "start": "1694690",
    "end": "1700120"
  },
  {
    "text": "scraping and you can compare and just calculate it but you really would have to look at what is a characteristic workload as well because it can be",
    "start": "1700120",
    "end": "1706270"
  },
  {
    "text": "dynamic but right so your mileage kind of will vary if you're gonna have a lot of i/o dependent you don't want to bump",
    "start": "1706270",
    "end": "1712630"
  },
  {
    "text": "up the CPU overhead and the other question is so right now this is very",
    "start": "1712630",
    "end": "1717970"
  },
  {
    "text": "static as you as you mentioned so we'll kind of have to it's the same problem all over again at a smaller scale of the",
    "start": "1717970",
    "end": "1724960"
  },
  {
    "text": "allocating extra memory or whatever do you have ideas on how to make that more",
    "start": "1724960",
    "end": "1730090"
  },
  {
    "text": "dynamic so it could maybe be more accurate not using pod overhead not really I haven't really thought about",
    "start": "1730090",
    "end": "1736930"
  },
  {
    "text": "how you do big if you're adding a lot more memory to the container that's gonna cost in a VM two megabytes per",
    "start": "1736930",
    "end": "1743530"
  },
  {
    "text": "gigabyte so you can kind of scale a little bit but that's negligible I think in the big picture I think that what",
    "start": "1743530",
    "end": "1749590"
  },
  {
    "text": "you're gonna see is it's gonna depend on the workload this running and in order to do that in my world set the overhead",
    "start": "1749590",
    "end": "1755830"
  },
  {
    "text": "for it's the right size and then use VP a and VP a is going to bounce the pod C group size up and down to meet your QP s",
    "start": "1755830",
    "end": "1762820"
  },
  {
    "text": "or what every metric are using to drive vertical pot on the scaling I think with those two it's it's a pretty cool story I",
    "start": "1762820",
    "end": "1768570"
  },
  {
    "text": "think that trying to just like have like a formula in there I'm not sure if you can put matheny a mole or not and I",
    "start": "1768570",
    "end": "1774600"
  },
  {
    "text": "guess you said that already VP a so that was the answer to my question thank you more questions so since you had to make",
    "start": "1774600",
    "end": "1787980"
  },
  {
    "text": "changes to the schedule special I'm curious if you write customs case I don't know I was like a lot of people do",
    "start": "1787980",
    "end": "1793860"
  },
  {
    "text": "then they will each account for this pot overhead when they do that I ain't",
    "start": "1793860",
    "end": "1799500"
  },
  {
    "text": "didn't I just went through and modified the existing just because it's pretty",
    "start": "1799500",
    "end": "1805200"
  },
  {
    "text": "easy in that way to just because it cuz it's all is static anyway it's just based on requests and maybe limits if",
    "start": "1805200",
    "end": "1811020"
  },
  {
    "text": "you're looking at prioritization I you know it's just and add this at the end",
    "start": "1811020",
    "end": "1817200"
  },
  {
    "text": "essentially so the existing schedule was clean enough to be able to just use this as is custom schedules are really",
    "start": "1817200",
    "end": "1825990"
  },
  {
    "text": "interesting to think and talk about anyway so yeah more questions",
    "start": "1825990",
    "end": "1836660"
  },
  {
    "text": "all right well thank you very much Eric and the audience for the great exchange",
    "start": "1837530",
    "end": "1843040"
  },
  {
    "text": "[Applause]",
    "start": "1843040",
    "end": "1847589"
  }
]