[
  {
    "text": "hello and welcome in this session we're going to talk about elasticsearch or open search we're",
    "start": "160",
    "end": "6080"
  },
  {
    "text": "going to use the terms interchangeably for the most part we're going to talk about them in the",
    "start": "6080",
    "end": "11280"
  },
  {
    "text": "context of logs and other time series data and we'll focus on using a kubernetes",
    "start": "11280",
    "end": "18800"
  },
  {
    "text": "operator to manage elasticsearch clusters in this context",
    "start": "18800",
    "end": "24560"
  },
  {
    "text": "but before we start let us introduce ourselves i'm radu i'm a search person at sematext",
    "start": "24560",
    "end": "32480"
  },
  {
    "text": "and most of my time goes into consulting production support and",
    "start": "32480",
    "end": "38399"
  },
  {
    "text": "training for elasticsearch open search and solar and sometimes i contribute to our",
    "start": "38399",
    "end": "45680"
  },
  {
    "text": "observability platform which is called semitex cloud okay hi my name is chiprian",
    "start": "45680",
    "end": "53120"
  },
  {
    "text": "i'm a consultant for kubernetes and automation and also work as a software engineer for",
    "start": "53120",
    "end": "60320"
  },
  {
    "text": "poly poly mostly on privacy related projects also in my spare time i'm an open source",
    "start": "60320",
    "end": "67920"
  },
  {
    "text": "maintainer contributing to many of the projects in the kubernetes ecosystem",
    "start": "67920",
    "end": "75200"
  },
  {
    "text": "so let's start with the agenda we'll talk about why we want to have",
    "start": "75280",
    "end": "82159"
  },
  {
    "text": "such such an operator so the use cases we will talk about how",
    "start": "82159",
    "end": "89600"
  },
  {
    "text": "how it should work when should it scale up and down and",
    "start": "89600",
    "end": "95360"
  },
  {
    "text": "what should we do when it does that and available options",
    "start": "95360",
    "end": "102240"
  },
  {
    "text": "and last a quick demo of our proof of concept",
    "start": "102240",
    "end": "108960"
  },
  {
    "text": "let's start with the why if you have a small cluster then you don't want to think about elastic search",
    "start": "109360",
    "end": "117119"
  },
  {
    "text": "to learn all about it the ins and outs and you would like to have",
    "start": "117119",
    "end": "123200"
  },
  {
    "text": "as less maintenance as possible so ideally zero maintenance this kind of operator would help you get",
    "start": "123200",
    "end": "130800"
  },
  {
    "text": "started with your logging without caring about",
    "start": "130800",
    "end": "135840"
  },
  {
    "text": "learning elasticsearch too much for bigger clusters",
    "start": "135840",
    "end": "141280"
  },
  {
    "text": "usually these clusters are multi-tenant and you you could split them easier because",
    "start": "141280",
    "end": "149040"
  },
  {
    "text": "it's not such a big maintenance to have multiple clusters if you have the operator",
    "start": "149040",
    "end": "156400"
  },
  {
    "text": "all right moving on to how so in other words what does the operator",
    "start": "156400",
    "end": "162000"
  },
  {
    "text": "need to do in order to auto scale um in order to get there i want to talk about",
    "start": "162000",
    "end": "167360"
  },
  {
    "text": "three things so one is using time-based indices so if you use for example logstash before you might have noticed",
    "start": "167360",
    "end": "174480"
  },
  {
    "text": "that it creates one index per day um we're going to talk about why that's a",
    "start": "174480",
    "end": "179920"
  },
  {
    "text": "good idea next i'm going to argue that for most use",
    "start": "179920",
    "end": "185280"
  },
  {
    "text": "cases rotating by size instead of by time will work better",
    "start": "185280",
    "end": "190879"
  },
  {
    "text": "and the third thing would be how would those indices behave when you scale your cluster",
    "start": "190879",
    "end": "197200"
  },
  {
    "text": "up and down so let's start with with time-based indices you know they don't have to be",
    "start": "197200",
    "end": "202959"
  },
  {
    "text": "daily you can have like one index per month or one index per hour but the idea is the same and the",
    "start": "202959",
    "end": "209280"
  },
  {
    "text": "advantages are pretty big so when it comes to indexing",
    "start": "209280",
    "end": "214319"
  },
  {
    "text": "typically the bottleneck is down to the",
    "start": "214319",
    "end": "220159"
  },
  {
    "text": "lucene segment merges which happen in the background and if you're indexing in a smaller index it's going to be much",
    "start": "220159",
    "end": "226480"
  },
  {
    "text": "much faster so we're basically comparing the time based indices with just indexing everything in in one index",
    "start": "226480",
    "end": "234080"
  },
  {
    "text": "when it comes to searches most of the searches tend to be in the latest data",
    "start": "234080",
    "end": "239120"
  },
  {
    "text": "so again if we have it broken down by time we can hit just a slice of our data",
    "start": "239120",
    "end": "245519"
  },
  {
    "text": "and that's going to be faster but even if we search in all the data even",
    "start": "245519",
    "end": "252319"
  },
  {
    "text": "because their indices are not written anymore they're much more easily cacheable by both",
    "start": "252319",
    "end": "258959"
  },
  {
    "text": "elasticsearch and the operating system underneath it so um in my experience",
    "start": "258959",
    "end": "265040"
  },
  {
    "text": "both indexing and searches will be orders of magnitude faster with this design compared to having just one index",
    "start": "265040",
    "end": "273520"
  },
  {
    "text": "and finally when you delete when you have to expire data um with time businesses you can simply delete whole",
    "start": "273520",
    "end": "279600"
  },
  {
    "text": "indices which by and large implies deleting some files on disk as opposed",
    "start": "279600",
    "end": "284800"
  },
  {
    "text": "to deleting documents from within an index which are only soft deletes and will trigger additional",
    "start": "284800",
    "end": "292000"
  },
  {
    "text": "lucine segment merges in practice you would have like multiple",
    "start": "292000",
    "end": "298880"
  },
  {
    "text": "series of such time-based indices um um normally um we would break them down",
    "start": "298880",
    "end": "307280"
  },
  {
    "text": "by uh how you'd search them so for example if we search nginx logs uh",
    "start": "307280",
    "end": "312479"
  },
  {
    "text": "separately then syslog in general we don't have to like we can always search through everything then it",
    "start": "312479",
    "end": "319440"
  },
  {
    "text": "makes sense to keep them in separate index series",
    "start": "319440",
    "end": "324479"
  },
  {
    "text": "but this design is not perfect and we may run into",
    "start": "324479",
    "end": "330479"
  },
  {
    "text": "what we call the black friday problem so let's say we are an e-commerce",
    "start": "330479",
    "end": "335780"
  },
  {
    "text": "[Music] website and we're logging access logs and so hopefully during black friday we",
    "start": "335780",
    "end": "341759"
  },
  {
    "text": "will have a lot more traffic and so that index will grow larger but much larger than the indices from the following",
    "start": "341759",
    "end": "349120"
  },
  {
    "text": "saturday or sunday and then you have cyber monday and then again it's a spike of traffic and then again it goes down",
    "start": "349120",
    "end": "354320"
  },
  {
    "text": "and so on and so forth and the problem is that um the big indices that will get generated",
    "start": "354320",
    "end": "361360"
  },
  {
    "text": "on friday and monday will behave much like that big index that we",
    "start": "361360",
    "end": "366639"
  },
  {
    "text": "talked about before so indexing will be slower searches will be slower exactly when we need them the most",
    "start": "366639",
    "end": "373440"
  },
  {
    "text": "so to fix this problem we can rotate indices by size instead of by time so",
    "start": "373440",
    "end": "380240"
  },
  {
    "text": "the way this works is at least typically we would have a",
    "start": "380240",
    "end": "385759"
  },
  {
    "text": "an alias that would point to an index and logstash will write to that alias or whatever",
    "start": "385759",
    "end": "391360"
  },
  {
    "text": "puts data into elasticsearch and then when that index gets to the target size uh typically 10 gigabytes",
    "start": "391360",
    "end": "399039"
  },
  {
    "text": "per shard is is like a good rule of thumb in in our tests um",
    "start": "399039",
    "end": "404319"
  },
  {
    "text": "we're going to create a new index we're going to flip the alias to the new empty index and then we can continue writing there and",
    "start": "404319",
    "end": "411759"
  },
  {
    "text": "we just keep on doing that thankfully there's some automation in",
    "start": "411759",
    "end": "416880"
  },
  {
    "text": "recent versions of both elasticsearch and open search in open search this is called index state management and in",
    "start": "416880",
    "end": "425120"
  },
  {
    "text": "elasticsearch it's called index lifecycle management and these can be used to automate this process of",
    "start": "425120",
    "end": "431919"
  },
  {
    "text": "okay let's create a new index let's flip the alias even let's remove old",
    "start": "431919",
    "end": "437280"
  },
  {
    "text": "indices which is going to be a bit more challenging in this context because you don't have",
    "start": "437280",
    "end": "442639"
  },
  {
    "text": "indices strictly divided by time um of course this design isn't perfect",
    "start": "442639",
    "end": "448800"
  },
  {
    "text": "either so for example if you need to backfill data right we're just onboarding some",
    "start": "448800",
    "end": "454720"
  },
  {
    "text": "new project that already has a lot lots of logs those will all go into the latest index and that's problematic um",
    "start": "454720",
    "end": "461840"
  },
  {
    "text": "and also when we're searching let's say you're searching the last 24 hours which indices contain uh those 24 hours this",
    "start": "461840",
    "end": "468319"
  },
  {
    "text": "is a bit harder to figure out though elasticsearch again will have this kind of stuff out of the box so shards that",
    "start": "468319",
    "end": "475199"
  },
  {
    "text": "don't have data matching your time frame will quickly kind of dismiss your query saying zero",
    "start": "475199",
    "end": "482879"
  },
  {
    "text": "okay so how does this work in the context of auto scaling so let's start with the simplest or one of the simplest",
    "start": "483440",
    "end": "488879"
  },
  {
    "text": "examples let's say we have uh two elastic search nodes and we have one index uh with two",
    "start": "488879",
    "end": "496080"
  },
  {
    "text": "shards so we have a spike in load let's say we want to auto scale we add a new node obviously we cannot take advantage",
    "start": "496080",
    "end": "502080"
  },
  {
    "text": "of it even if we had previous indices before some shards will migrate but the third node will not be able to",
    "start": "502080",
    "end": "507520"
  },
  {
    "text": "contribute to indexing which is our main load um so our suggestion is to say okay even",
    "start": "507520",
    "end": "514640"
  },
  {
    "text": "if we're not at 10 gigabytes per share or whatever our threshold is let's force rotate the index",
    "start": "514640",
    "end": "521120"
  },
  {
    "text": "and like create a new one that will be evenly spread out throughout our cluster and",
    "start": "521120",
    "end": "526720"
  },
  {
    "text": "this way all three nodes can can contribute to indexing this will typically imply three steps so",
    "start": "526720",
    "end": "533760"
  },
  {
    "text": "one we would change the index template so normally all your settings and mappings and all that stuff will leave",
    "start": "533760",
    "end": "539200"
  },
  {
    "text": "an index template so that when you create a new index it will inherit everything so we'll need to change that template to",
    "start": "539200",
    "end": "546000"
  },
  {
    "text": "say three shards instead of two and um we're going to do this uh forcing first",
    "start": "546000",
    "end": "553279"
  },
  {
    "text": "rollover like create the index and and flip the alias and finally we may need to adjust",
    "start": "553279",
    "end": "559600"
  },
  {
    "text": "the life cycle policy so if if the life cycle policy said uh 20 gigabytes before",
    "start": "559600",
    "end": "567279"
  },
  {
    "text": "because we have 10 gigabytes per shot times 2 shards now it has to say 30 gigabytes in order to um keep",
    "start": "567279",
    "end": "574839"
  },
  {
    "text": "consistency and we're going to do much of the same when we need to scale back down um the only difference would be",
    "start": "574839",
    "end": "581839"
  },
  {
    "text": "that we need to make sure that nodes are properly drained before we shut them down right so we would",
    "start": "581839",
    "end": "586959"
  },
  {
    "text": "exclude them from allocation and then elasticsearch would move the shards off of them before",
    "start": "586959",
    "end": "592399"
  },
  {
    "text": "we take them down but once we take them down we get into a similar problem as before",
    "start": "592399",
    "end": "597839"
  },
  {
    "text": "in the sense that the cluster is not balanced right so in order to make it balanced again we can do the same thing",
    "start": "597839",
    "end": "604480"
  },
  {
    "text": "change the template back to two shards force roll over and adjust the um",
    "start": "604480",
    "end": "611120"
  },
  {
    "text": "the policy again is that make sense cool so that's what we're trying to automate",
    "start": "611120",
    "end": "616800"
  },
  {
    "text": "but before we get there i want to talk about three more sort of best practices we can't be comprehensive here in terms",
    "start": "616800",
    "end": "622959"
  },
  {
    "text": "of best practice but i think these three are important especially in this context so one of them is that it's often",
    "start": "622959",
    "end": "629680"
  },
  {
    "text": "tempting to judge the size of a cluster based on indexing",
    "start": "629680",
    "end": "635440"
  },
  {
    "text": "throughput because that's our main workload we do much more indexing than searches typically",
    "start": "635440",
    "end": "641600"
  },
  {
    "text": "so let's say we index one million documents per second what kind of cluster do we need but",
    "start": "641600",
    "end": "647360"
  },
  {
    "text": "typically the unit of scale would be either search latency or just outright disk",
    "start": "647360",
    "end": "654720"
  },
  {
    "text": "usage because if we want to if we index 1 million documents per second but we want",
    "start": "654720",
    "end": "660320"
  },
  {
    "text": "to keep this data for like one month we'll need a big cluster and",
    "start": "660320",
    "end": "665519"
  },
  {
    "text": "that big cluster would be big enough to index um one million documents per second",
    "start": "665519",
    "end": "671600"
  },
  {
    "text": "right so this is typically what we will look for and since searches are our bottleneck",
    "start": "671600",
    "end": "678880"
  },
  {
    "text": "for scaling then i think it's worth mentioning that searches do lots of random i o",
    "start": "678880",
    "end": "684800"
  },
  {
    "text": "so i o latency tends to be more important than throughput or even iops",
    "start": "684800",
    "end": "690880"
  },
  {
    "text": "so with most cloud providers we would typically go for the local ssds the ephemeral storage rather than the",
    "start": "690880",
    "end": "697760"
  },
  {
    "text": "managed over the network disks because we have much much better latency and so",
    "start": "697760",
    "end": "703120"
  },
  {
    "text": "we can uh put a lot more data on the same node now obviously this has a downside in the",
    "start": "703120",
    "end": "709440"
  },
  {
    "text": "sense that if the node goes down it takes the storage with it um so we'll have to account for that either by",
    "start": "709440",
    "end": "715120"
  },
  {
    "text": "having more replicas obviously depends on how important the data is and regular backups",
    "start": "715120",
    "end": "722079"
  },
  {
    "text": "to sort of compensate and the last one is you may have heard",
    "start": "722240",
    "end": "727519"
  },
  {
    "text": "of hot warm cold colder kind of architectures the idea is um you have",
    "start": "727519",
    "end": "734720"
  },
  {
    "text": "like your let's say best hardware in the hot tier that's where indexing happens that's where most of your searches uh",
    "start": "734720",
    "end": "740560"
  },
  {
    "text": "recent searches will happen and as data becomes less relevant we move it",
    "start": "740560",
    "end": "746560"
  },
  {
    "text": "down to lesser hardware um frankly our proof of concept does not",
    "start": "746560",
    "end": "753040"
  },
  {
    "text": "support this but i'm not sure if it's needed because um i think there's a limp",
    "start": "753040",
    "end": "758399"
  },
  {
    "text": "there's limited use of this design because you cannot put like trash hardware on the call tier because",
    "start": "758399",
    "end": "765279"
  },
  {
    "text": "elasticsearch will still keep need to keep the data open it still needs to be able to monitor itself and stuff like",
    "start": "765279",
    "end": "770880"
  },
  {
    "text": "that and if the call tier is too slow it will just become unstable it's not the problem that you wait like",
    "start": "770880",
    "end": "777360"
  },
  {
    "text": "30 seconds more for a query it's just it's gonna the nodes will drop out and stuff like that um",
    "start": "777360",
    "end": "783120"
  },
  {
    "text": "but if the so if the cold tier is fast enough um at least this is what we notice like hey",
    "start": "783120",
    "end": "789519"
  },
  {
    "text": "these are these nodes are idling we might as well get them back to work and do indexing and then you end up back to",
    "start": "789519",
    "end": "796160"
  },
  {
    "text": "like a flat kind of design okay so um this idea of an elasticsearch",
    "start": "796160",
    "end": "803519"
  },
  {
    "text": "operator is not new at all so we thought okay let's see what existing operators",
    "start": "803519",
    "end": "809519"
  },
  {
    "text": "can help us with and unfortunately a lot of them are either unmaintained or they don't do auto scaling at all",
    "start": "809519",
    "end": "816959"
  },
  {
    "text": "and but there is one bioelastic that obviously supports elasticsearch very",
    "start": "816959",
    "end": "822320"
  },
  {
    "text": "well it's called elastic cloud in kubernetes trouble is it's not exactly open search we don't want to get into",
    "start": "822320",
    "end": "827839"
  },
  {
    "text": "that flameware but not open search not open source um",
    "start": "827839",
    "end": "832959"
  },
  {
    "text": "because it's on the elastic license but particularly for auto scaling",
    "start": "832959",
    "end": "838160"
  },
  {
    "text": "you need to pay you you need an enterprise license um there's i think it's worth mentioning uh",
    "start": "838160",
    "end": "844800"
  },
  {
    "text": "opster which is an operator um used by open search it's under active",
    "start": "844800",
    "end": "850560"
  },
  {
    "text": "development but right now it does not support auto scaling it's on the roadmap but it's not there yet so the one we",
    "start": "850560",
    "end": "856800"
  },
  {
    "text": "went for is called es operator it's from zalando it was presented at kubecon",
    "start": "856800",
    "end": "862480"
  },
  {
    "text": "cloud native con before and it is open source",
    "start": "862480",
    "end": "868160"
  },
  {
    "text": "it does support auto scaling it does support draining uh nodes like um what i",
    "start": "868160",
    "end": "873920"
  },
  {
    "text": "mentioned we need to move shards off before um uh shutting the node down when we scale",
    "start": "873920",
    "end": "880560"
  },
  {
    "text": "down and it works really well for let's say e-commerce type of use cases so when you",
    "start": "880560",
    "end": "886480"
  },
  {
    "text": "have one or more indices and you just want to increase your cluster capacity to to handle them even if that means",
    "start": "886480",
    "end": "893680"
  },
  {
    "text": "adding additional replicas so that works really well but we wanted to change it so that it",
    "start": "893680",
    "end": "899040"
  },
  {
    "text": "works for logs so that it does this template management and lifecycle policy management and the force rotate",
    "start": "899040",
    "end": "904959"
  },
  {
    "text": "everything that that i talked about before uh and this is what uh cpan will show you uh in a second okay",
    "start": "904959",
    "end": "914079"
  },
  {
    "text": "so uh we're going to do the demo right now uh i will start",
    "start": "914639",
    "end": "920560"
  },
  {
    "text": "with uh a little uh about the zalandu operator",
    "start": "920560",
    "end": "927199"
  },
  {
    "text": "inner workings it requires you first of all to have your own master",
    "start": "927199",
    "end": "934480"
  },
  {
    "text": "deployment before running it so it's not fully managed the cluster",
    "start": "934480",
    "end": "941360"
  },
  {
    "text": "but after that you have some simple options that control",
    "start": "941360",
    "end": "947839"
  },
  {
    "text": "scaling and other various aspects like excluding",
    "start": "947839",
    "end": "953120"
  },
  {
    "text": "system indices from any calculation that the cluster that the operator does",
    "start": "953120",
    "end": "960160"
  },
  {
    "text": "in our case the most important options are the minimum replicas maximum",
    "start": "960160",
    "end": "966720"
  },
  {
    "text": "replicas that we allow for it to to create and we will be using",
    "start": "966720",
    "end": "973199"
  },
  {
    "text": "this usage percent for scaling up and down the cluster",
    "start": "973199",
    "end": "981120"
  },
  {
    "text": "okay um so at the moment we have",
    "start": "981360",
    "end": "987040"
  },
  {
    "text": "already prepared the the demo in docker",
    "start": "987040",
    "end": "993680"
  },
  {
    "text": "in kubernetes docker so it's pretty easy to to do without networking",
    "start": "994240",
    "end": "1001279"
  },
  {
    "text": "um as you can see there is only the master we already applied this configuration",
    "start": "1001279",
    "end": "1009360"
  },
  {
    "text": "and once we will start the operator it will create the",
    "start": "1009360",
    "end": "1014800"
  },
  {
    "text": "data nodes and then we will show how it scales up how it",
    "start": "1014800",
    "end": "1021199"
  },
  {
    "text": "applies the templates um the ism configuration",
    "start": "1021199",
    "end": "1027120"
  },
  {
    "text": "and what it does at each scaling step so starting",
    "start": "1027120",
    "end": "1034959"
  },
  {
    "text": "the operator now okay",
    "start": "1034959",
    "end": "1040959"
  },
  {
    "text": "immediately it notices that it needs two replicas and it has none",
    "start": "1040959",
    "end": "1048480"
  },
  {
    "text": "um already started the pods and",
    "start": "1048480",
    "end": "1054720"
  },
  {
    "text": "looking here it should take very little to get the cluster",
    "start": "1054720",
    "end": "1061679"
  },
  {
    "text": "up and running until then let's",
    "start": "1061679",
    "end": "1067840"
  },
  {
    "text": "let's look at the cluster health so it's a green cluster",
    "start": "1068960",
    "end": "1076000"
  },
  {
    "text": "there's no index yet",
    "start": "1077840",
    "end": "1081840"
  },
  {
    "text": "there is no template there is pretty much nothing as you can see",
    "start": "1083039",
    "end": "1089280"
  },
  {
    "text": "it already found that the nodes are healthy it created",
    "start": "1089280",
    "end": "1095760"
  },
  {
    "text": "the component template for logstash and also a separate component template",
    "start": "1095760",
    "end": "1100880"
  },
  {
    "text": "for scaling and then it applied it to the index you",
    "start": "1100880",
    "end": "1106720"
  },
  {
    "text": "can see that it applied both logstash and the scaling components",
    "start": "1106720",
    "end": "1112720"
  },
  {
    "text": "and it also created the ism policy with the",
    "start": "1112720",
    "end": "1118480"
  },
  {
    "text": "minimum size 10 gigs okay let's see here",
    "start": "1118480",
    "end": "1123679"
  },
  {
    "text": "indices we have one index with one primary shard and one replica",
    "start": "1123679",
    "end": "1129360"
  },
  {
    "text": "um we have template with logsdash and scaling",
    "start": "1129360",
    "end": "1135919"
  },
  {
    "text": "and the cluster should be healthy so it's still green",
    "start": "1135919",
    "end": "1142880"
  },
  {
    "text": "if we go back to the configuration we can change this",
    "start": "1144160",
    "end": "1150640"
  },
  {
    "text": "scale up percent boundary to 21 this is something that works on my setup we",
    "start": "1150640",
    "end": "1156720"
  },
  {
    "text": "don't have the time to ingest that many logs or data to to do a real",
    "start": "1156720",
    "end": "1163200"
  },
  {
    "text": "demo so we just fake it with",
    "start": "1163200",
    "end": "1167360"
  },
  {
    "text": "decreasing the thresholds so um let's see",
    "start": "1168240",
    "end": "1173840"
  },
  {
    "text": "okay should be applied and",
    "start": "1175520",
    "end": "1181840"
  },
  {
    "text": "very soon it should detect that um the",
    "start": "1181840",
    "end": "1188799"
  },
  {
    "text": "bound the disk usage percent boundary has been reached and it should start",
    "start": "1189120",
    "end": "1194400"
  },
  {
    "text": "adding more replicas while it add more it adds more replicas",
    "start": "1194400",
    "end": "1199600"
  },
  {
    "text": "it will reconfigure the scaling template the ism",
    "start": "1199600",
    "end": "1206919"
  },
  {
    "text": "and roll the logstash index to",
    "start": "1207280",
    "end": "1212320"
  },
  {
    "text": "to match the new number of nodes with bet best practices",
    "start": "1212320",
    "end": "1218320"
  },
  {
    "text": "so let's see oh okay",
    "start": "1218320",
    "end": "1224000"
  },
  {
    "text": "still waiting for the third note to be ready i guess",
    "start": "1224000",
    "end": "1229120"
  },
  {
    "text": "um",
    "start": "1229120",
    "end": "1232120"
  },
  {
    "text": "so let's see here how it does",
    "start": "1235120",
    "end": "1239760"
  },
  {
    "text": "okay it should be up and running and we should see",
    "start": "1241840",
    "end": "1247760"
  },
  {
    "text": "how it sets the scaling template right now to",
    "start": "1247760",
    "end": "1253280"
  },
  {
    "text": "to match the number of nodes and it increases the ism policy to",
    "start": "1253280",
    "end": "1259280"
  },
  {
    "text": "minimum size to 30 gigs to match the three primary shards",
    "start": "1259280",
    "end": "1264320"
  },
  {
    "text": "because all the nodes that we have share the same storage it will not stop at three",
    "start": "1264320",
    "end": "1271840"
  },
  {
    "text": "replicas right now it will go down it will go up until",
    "start": "1271840",
    "end": "1277600"
  },
  {
    "text": "the maximum allowed by the configuration so for replicas",
    "start": "1277600",
    "end": "1283280"
  },
  {
    "text": "and once it reaches that [Music] threshold",
    "start": "1283280",
    "end": "1288720"
  },
  {
    "text": "we will change back the uh scale up disk percent",
    "start": "1288720",
    "end": "1295600"
  },
  {
    "text": "boundary and we will make it go back to",
    "start": "1295600",
    "end": "1300880"
  },
  {
    "text": "to the way it was to two nodes and at that time it will start recreating each",
    "start": "1300880",
    "end": "1309200"
  },
  {
    "text": "index um no not recreating sorry rolling the",
    "start": "1309200",
    "end": "1314320"
  },
  {
    "text": "indices and applying uh the correct uh",
    "start": "1314320",
    "end": "1319360"
  },
  {
    "text": "scaling template for them so right now if we will be looking here",
    "start": "1319360",
    "end": "1324640"
  },
  {
    "text": "in this series you can see that we no longer have one",
    "start": "1324640",
    "end": "1330159"
  },
  {
    "text": "we already have three uh created at each step so it started with",
    "start": "1330159",
    "end": "1337120"
  },
  {
    "text": "one uh primary and one replica then uh it went to",
    "start": "1337120",
    "end": "1342960"
  },
  {
    "text": "three primaries and one replica so that the allocation can",
    "start": "1342960",
    "end": "1349120"
  },
  {
    "text": "be uniform on that amount of nodes and lastly it went",
    "start": "1349120",
    "end": "1354640"
  },
  {
    "text": "to two primaries and one replica so i guess",
    "start": "1354640",
    "end": "1359919"
  },
  {
    "text": "right now we should have the cluster with four nodes it should be healthy",
    "start": "1359919",
    "end": "1368399"
  },
  {
    "text": "let's see so it's healthy we can try to see that the ism policy",
    "start": "1368720",
    "end": "1377760"
  },
  {
    "text": "was applied correctly so it's 20 gigs the minimum size for all over as we",
    "start": "1377760",
    "end": "1383760"
  },
  {
    "text": "expected and um",
    "start": "1383760",
    "end": "1388799"
  },
  {
    "text": "scaling and the scaling template matches what we see in the",
    "start": "1388799",
    "end": "1394960"
  },
  {
    "text": "in the logs okay uh reducing this uh",
    "start": "1394960",
    "end": "1400080"
  },
  {
    "text": "changing this back to 75 percent and increasing this to 20",
    "start": "1400080",
    "end": "1405200"
  },
  {
    "text": "25 percent the scale down disc usage percent boundary so that it starts uh",
    "start": "1405200",
    "end": "1412000"
  },
  {
    "text": "decreasing in size applying so right now it should",
    "start": "1412000",
    "end": "1418480"
  },
  {
    "text": "quickly scale down also drain the nodes at",
    "start": "1418480",
    "end": "1424880"
  },
  {
    "text": "each step and reapply the scaling templates",
    "start": "1424880",
    "end": "1432480"
  },
  {
    "text": "to two indices while rolling them okay",
    "start": "1433279",
    "end": "1440080"
  },
  {
    "text": "let's see in this is",
    "start": "1440080",
    "end": "1446240"
  },
  {
    "text": "you can see already have the fourth index that's matching the second",
    "start": "1446320",
    "end": "1451840"
  },
  {
    "text": "one created like three primaries and one replica",
    "start": "1451840",
    "end": "1457039"
  },
  {
    "text": "and pretty soon it will you see draining the pods",
    "start": "1457039",
    "end": "1462080"
  },
  {
    "text": "ensuring the cluster is green and",
    "start": "1462080",
    "end": "1467360"
  },
  {
    "text": "pretty soon it will get to [Music] uh two ready",
    "start": "1467600",
    "end": "1472960"
  },
  {
    "text": "nodes instead of four as it was in the past so in this is",
    "start": "1472960",
    "end": "1479360"
  },
  {
    "text": "it's already at five indices and",
    "start": "1479360",
    "end": "1484480"
  },
  {
    "text": "we have again two data nodes",
    "start": "1484480",
    "end": "1490320"
  },
  {
    "text": "in elasticsearch at the moment this is pretty much",
    "start": "1490320",
    "end": "1497679"
  },
  {
    "text": "hard-coded many of the things that we did here it was most it's mostly a proof of concept",
    "start": "1497679",
    "end": "1502799"
  },
  {
    "text": "but we want to improve it and publish it as a usable solution for people that",
    "start": "1502799",
    "end": "1509679"
  },
  {
    "text": "want to do logs um and",
    "start": "1509679",
    "end": "1515200"
  },
  {
    "text": "i think this concludes our demo so thanks everyone for",
    "start": "1515200",
    "end": "1520480"
  },
  {
    "text": "coming here and watching us thank you [Applause]",
    "start": "1520480",
    "end": "1530309"
  },
  {
    "text": "any questions",
    "start": "1534240",
    "end": "1537480"
  },
  {
    "text": "okay",
    "start": "1542400",
    "end": "1544720"
  },
  {
    "text": "for me should i keep it i don't know",
    "start": "1550880",
    "end": "1555799"
  },
  {
    "text": "so uh basically at the end you have a log stash o5 index with strip three primary shard",
    "start": "1556480",
    "end": "1563760"
  },
  {
    "text": "and one repeater for each but you have only two nodes in your cluster so is the cluster beginning to be yellow at that",
    "start": "1563760",
    "end": "1569919"
  },
  {
    "text": "point no no it wasn't it wasn't three it was one at the very end",
    "start": "1569919",
    "end": "1576000"
  },
  {
    "text": "you have an index with three screw primary at some point and you keep it so the cluster should be yellow right",
    "start": "1576000",
    "end": "1582240"
  },
  {
    "text": "yellow because we when we oh do you want to take this sure so we modified the shards per node",
    "start": "1582240",
    "end": "1588640"
  },
  {
    "text": "setting while scaling down so it allows the cluster to scale down properly we",
    "start": "1588640",
    "end": "1594799"
  },
  {
    "text": "don't want we don't want that part to scale to hold back the scaling okay",
    "start": "1594799",
    "end": "1600559"
  },
  {
    "text": "thank you no problem any other questions",
    "start": "1600559",
    "end": "1605559"
  },
  {
    "text": "yeah just uh more of a high level question on the the operator so um",
    "start": "1613919",
    "end": "1620000"
  },
  {
    "text": "the reason i'm interested in this is because i have some customer facing real data not necessarily for the",
    "start": "1620000",
    "end": "1625039"
  },
  {
    "text": "observability aspect it's i mean how confident would you guys be with this operator for",
    "start": "1625039",
    "end": "1631360"
  },
  {
    "text": "genuinely customer facing critical uh usage as opposed to a little bit less",
    "start": "1631360",
    "end": "1637840"
  },
  {
    "text": "worried if i lose it well this particular one that we showed",
    "start": "1637840",
    "end": "1643919"
  },
  {
    "text": "it's not production ready i can tell you that um the one that is sort of open",
    "start": "1643919",
    "end": "1649200"
  },
  {
    "text": "source already that we linked if you don't have logs if you have like let's say e-commerce it should work i",
    "start": "1649200",
    "end": "1655600"
  },
  {
    "text": "mean as far as we work with it it i think it's obviously up for sort of testing for your use case but i uh they",
    "start": "1655600",
    "end": "1661600"
  },
  {
    "text": "use it for quite a long time now okay at their shop so i guess it works",
    "start": "1661600",
    "end": "1668480"
  },
  {
    "text": "cheers thanks thanks",
    "start": "1668480",
    "end": "1674720"
  },
  {
    "text": "any other questions",
    "start": "1675360",
    "end": "1678080"
  },
  {
    "text": "what matrix are your monitoring in order to know when to add more nodes or scale more less nodes and stuff",
    "start": "1680640",
    "end": "1689120"
  },
  {
    "text": "yeah yeah where we initially the the the metrics were cpu usage",
    "start": "1690399",
    "end": "1696399"
  },
  {
    "text": "um and also um number of shards per node so you can say i want at most five",
    "start": "1696399",
    "end": "1701679"
  },
  {
    "text": "shares per or something like that that's what the original operator does we added the disk thresholds so if the one that",
    "start": "1701679",
    "end": "1707840"
  },
  {
    "text": "cheaper modified like if you go over a specific disk and we scale up if we go under we",
    "start": "1707840",
    "end": "1713120"
  },
  {
    "text": "scale down um and there's another one that has to do with disk which is like a safety net so when you scale",
    "start": "1713120",
    "end": "1720159"
  },
  {
    "text": "back down you don't want to get over a specific um disk usage because like you don't",
    "start": "1720159",
    "end": "1725520"
  },
  {
    "text": "want to run out of disk when you scale down or run into some sort of endless loop so those are the metrics the plan was to",
    "start": "1725520",
    "end": "1732159"
  },
  {
    "text": "add something with that has to do with search latency as well we can pretty much get any metric",
    "start": "1732159",
    "end": "1737600"
  },
  {
    "text": "that we want from elasticsearch because these this this metrics um are from elasticsearch itself",
    "start": "1737600",
    "end": "1744080"
  },
  {
    "text": "like from cat nodes or something like that we we'd want to use metrics from inside",
    "start": "1744080",
    "end": "1750480"
  },
  {
    "text": "elasticsearch because elasticsearch does quite a lot of stuff in the background",
    "start": "1750480",
    "end": "1755600"
  },
  {
    "text": "and if we use something from external sources like",
    "start": "1755600",
    "end": "1762240"
  },
  {
    "text": "some monitoring tool it may differ from",
    "start": "1762480",
    "end": "1767679"
  },
  {
    "text": "the same the data that elasticsearch uses for internal",
    "start": "1767679",
    "end": "1773760"
  },
  {
    "text": "uh thresholds so we cannot do for disk uh we cannot use for this external tools",
    "start": "1773760",
    "end": "1781360"
  },
  {
    "text": "um i think the zalando people are trying to extend their operator to use",
    "start": "1781360",
    "end": "1787120"
  },
  {
    "text": "horizontal podatto scalar so that would make it pretty easy for us to",
    "start": "1787120",
    "end": "1792399"
  },
  {
    "text": "gather more metrics and add the logs related ones",
    "start": "1792399",
    "end": "1797520"
  },
  {
    "text": "there so improve on that but at the moment it's uh",
    "start": "1797520",
    "end": "1803200"
  },
  {
    "text": "from what we read in their comments they are heavily developing",
    "start": "1803200",
    "end": "1808559"
  },
  {
    "text": "this feature so we cannot do a merge of our code there or even",
    "start": "1808559",
    "end": "1815279"
  },
  {
    "text": "discuss the possibility because we don't know how it will look it's kind of frozen",
    "start": "1815279",
    "end": "1821200"
  },
  {
    "text": "at the moment",
    "start": "1821200",
    "end": "1823840"
  },
  {
    "text": "any other questions yeah thank you",
    "start": "1826799",
    "end": "1831360"
  },
  {
    "text": "uh hello uh in in in a big cluster that uh has uh big charts like 50 gig charts",
    "start": "1835279",
    "end": "1843200"
  },
  {
    "text": "and i don't know 60 notes or something like that uh how does the scaling up and scaling down",
    "start": "1843200",
    "end": "1850000"
  },
  {
    "text": "performs we did not test it with that size",
    "start": "1850000",
    "end": "1856720"
  },
  {
    "text": "i think my recommendation would be if you can not get there and but rather",
    "start": "1856720",
    "end": "1861760"
  },
  {
    "text": "split it in multiple smaller clusters i think for a larger cluster what we",
    "start": "1861760",
    "end": "1867519"
  },
  {
    "text": "would want is to scale in larger increments like you don't want to scale up all the time because we force rotate",
    "start": "1867519",
    "end": "1873120"
  },
  {
    "text": "and then it would be kind of sub-optimal if we go 61 62 63 and then go back and",
    "start": "1873120",
    "end": "1878159"
  },
  {
    "text": "so on and so forth so i would add some sort of increments we the operator already has stuff like cool",
    "start": "1878159",
    "end": "1885200"
  },
  {
    "text": "down both when you scale up when you scale down so it's not that jittery and you can obviously configure that but i",
    "start": "1885200",
    "end": "1890799"
  },
  {
    "text": "would add some sort of steps so you don't so we kind of reduce the noise and for",
    "start": "1890799",
    "end": "1897360"
  },
  {
    "text": "large shards this is again something that you can have control of and if you use it for time series data i think 50",
    "start": "1897360",
    "end": "1903360"
  },
  {
    "text": "60 gigabytes is kind of large but of course you cannot have like a million shards so there's a trade-off",
    "start": "1903360",
    "end": "1908640"
  },
  {
    "text": "there you may be aware of that um because if you have a lot of shards then your cluster state becomes large and",
    "start": "1908640",
    "end": "1913919"
  },
  {
    "text": "then it's going to be difficult to coordinate that across the nodes and that's in my experience at least ultimately the the",
    "start": "1913919",
    "end": "1919919"
  },
  {
    "text": "scaling limit of how much data you can put in a in a single elasticsearch cluster",
    "start": "1919919",
    "end": "1925120"
  },
  {
    "text": "because at some point it becomes unstable because of the large cluster state that has to be coordinated",
    "start": "1925120",
    "end": "1930880"
  },
  {
    "text": "i hope i answered your question yeah okay any other questions",
    "start": "1930880",
    "end": "1937519"
  },
  {
    "text": "right if not thank you very much for attending um hope you have thank you for great rest of the conference",
    "start": "1940799",
    "end": "1948760"
  }
]