[
  {
    "text": "hi everybody it's so happy and i'm very glad that i'm",
    "start": "160",
    "end": "5279"
  },
  {
    "text": "able to meet you all in the kubernetes ai day europe 2022",
    "start": "5279",
    "end": "11920"
  },
  {
    "text": "so the title of my talk is a deep dive into cube flow pipelines",
    "start": "11920",
    "end": "18640"
  },
  {
    "text": "while you might have heard or you might have watched several talks about",
    "start": "18640",
    "end": "23680"
  },
  {
    "text": "kubeflow pipelines before i presume most of those talks would have",
    "start": "23680",
    "end": "29920"
  },
  {
    "text": "been highly data scientist or data engineer focused meaning",
    "start": "29920",
    "end": "36640"
  },
  {
    "text": "the more focus would have been to how to write pipelines in a more efficient manner how",
    "start": "36640",
    "end": "44079"
  },
  {
    "text": "to build components from scratch or how to convert a python",
    "start": "44079",
    "end": "49520"
  },
  {
    "text": "function into components and then eventually build a pipeline right",
    "start": "49520",
    "end": "55360"
  },
  {
    "text": "so that is fine but today i'm going to talk to you about q flow pipelines which",
    "start": "55360",
    "end": "62480"
  },
  {
    "text": "will be more from an let's say ml engineer point of view or an ml ops engineer point of view or",
    "start": "62480",
    "end": "70240"
  },
  {
    "text": "even a devops point of view so today i try to cover how cube flow pipelines",
    "start": "70240",
    "end": "77040"
  },
  {
    "text": "are composed what are the components that comes along with kubeflow pipelines",
    "start": "77040",
    "end": "82880"
  },
  {
    "text": "and how these components interact with each other and eventually how these",
    "start": "82880",
    "end": "89040"
  },
  {
    "text": "components are able to execute the pipeline that is submitted",
    "start": "89040",
    "end": "94640"
  },
  {
    "text": "to the kubeflow pipelines so let's enter into the talk i am central i am working as",
    "start": "94640",
    "end": "102640"
  },
  {
    "text": "principal software engineer in ericsson and my job in ericsson is to primarily",
    "start": "102640",
    "end": "110560"
  },
  {
    "text": "architect cloud native aiml platforms so these are platforms that are highly",
    "start": "110560",
    "end": "117040"
  },
  {
    "text": "distributed in nature and use kubernetes as the underlying",
    "start": "117040",
    "end": "122560"
  },
  {
    "text": "platform for compute and other resources and apart from work i take time to",
    "start": "122560",
    "end": "129360"
  },
  {
    "text": "participate in other aspirations of mine for instance i am the organizer of",
    "start": "129360",
    "end": "136400"
  },
  {
    "text": "kubernetes community days chennai so this is going to happen on 3rd and 4th",
    "start": "136400",
    "end": "143040"
  },
  {
    "text": "june of this year i am the maintainer of an open source project called as cubefletch",
    "start": "143040",
    "end": "149920"
  },
  {
    "text": "so this project is actually an operator which will help you to cache container",
    "start": "149920",
    "end": "156879"
  },
  {
    "text": "images directly on the worker nodes of a kubernetes cluster",
    "start": "156879",
    "end": "162400"
  },
  {
    "text": "and i am also an occasional speaker i would say i am not",
    "start": "162400",
    "end": "167519"
  },
  {
    "text": "you know very active in speaking but whenever i talk i",
    "start": "167519",
    "end": "172720"
  },
  {
    "text": "love to talk about kubernetes cloud native technologies and very recently i",
    "start": "172720",
    "end": "179120"
  },
  {
    "text": "have also picked up an interest in talking about mlaps and i am a tech blogger you can watch my",
    "start": "179120",
    "end": "186319"
  },
  {
    "text": "blogs in medium and nowadays i am a little bit not that active in tech blogging due to",
    "start": "186319",
    "end": "192720"
  },
  {
    "text": "my preoccupation with organizing kubernetes community days chennai",
    "start": "192720",
    "end": "198879"
  },
  {
    "text": "and i am fairly active with social media sites like twitter and linkedin so do",
    "start": "198879",
    "end": "206159"
  },
  {
    "text": "check out my profiles on the social media platforms so let's get into the talk",
    "start": "206159",
    "end": "213760"
  },
  {
    "text": "the agenda for us today is actually very simple i'm going to talk about ml workflows and",
    "start": "213760",
    "end": "222000"
  },
  {
    "text": "the various ml pipelining tools and i'm going to pick out cube flow",
    "start": "222000",
    "end": "227280"
  },
  {
    "text": "and i'm going to talk about cube flow what are the platform components that",
    "start": "227280",
    "end": "233040"
  },
  {
    "text": "comprise of kubeflow pipeline i'll be talking at length about the cube",
    "start": "233040",
    "end": "240159"
  },
  {
    "text": "flow pipeline architecture that is where i will talk about the various components that make up q flow pipeline and how",
    "start": "240159",
    "end": "246640"
  },
  {
    "text": "these components interact with each other and i'll try to dig more deeper into q",
    "start": "246640",
    "end": "252959"
  },
  {
    "text": "flow pipeline i'll also go and talk about what is an argo workflow executor",
    "start": "252959",
    "end": "260160"
  },
  {
    "text": "okay and i'll also talk about other notable features of kfb queue flow",
    "start": "260160",
    "end": "266320"
  },
  {
    "text": "pipelines and finally i'll finish it off with the cube flow pipeline demo a very simple demo which",
    "start": "266320",
    "end": "273440"
  },
  {
    "text": "will help us understand the theoretical part that we see during the",
    "start": "273440",
    "end": "279600"
  },
  {
    "text": "talk and by the way we know very well what an",
    "start": "279600",
    "end": "284639"
  },
  {
    "text": "ml workflow looks like right so there are distinct steps right and each and",
    "start": "284639",
    "end": "291680"
  },
  {
    "text": "every step performs a certain portion of the overall work that needs to be done",
    "start": "291680",
    "end": "298160"
  },
  {
    "text": "and each and every step is also self-contained it has its own distinct set of input and",
    "start": "298160",
    "end": "306960"
  },
  {
    "text": "it has its own distinct set of output and the input can be a",
    "start": "306960",
    "end": "312560"
  },
  {
    "text": "very simple parameter like a string or integer or float or the",
    "start": "312560",
    "end": "318639"
  },
  {
    "text": "input can be a huge data set which is stored somewhere in a data store okay",
    "start": "318639",
    "end": "324560"
  },
  {
    "text": "similarly the output can be a very simple file or the output can be a huge",
    "start": "324560",
    "end": "330000"
  },
  {
    "text": "data set that is for instance pushed into a kafka or that is for instance",
    "start": "330000",
    "end": "335120"
  },
  {
    "text": "stored into a mini object storage so whatever it may be",
    "start": "335120",
    "end": "340320"
  },
  {
    "text": "we all know that machine learning systems are typically",
    "start": "340320",
    "end": "346479"
  },
  {
    "text": "workflows so you need to build a machine learning system in the form of a workflow and as the",
    "start": "346479",
    "end": "355199"
  },
  {
    "text": "execution proceeds from each and every step of the workflow",
    "start": "355199",
    "end": "360560"
  },
  {
    "text": "there is a distinct work done and data gets processed in each and every",
    "start": "360560",
    "end": "366080"
  },
  {
    "text": "step and a model is being built in each and every step and eventually the model",
    "start": "366080",
    "end": "371600"
  },
  {
    "text": "is deployed into production and then the monitoring happens where the drift detection and all these things are",
    "start": "371600",
    "end": "378479"
  },
  {
    "text": "coming into play now whenever we talk about workflows not",
    "start": "378479",
    "end": "383680"
  },
  {
    "text": "necessarily about ml workflows any workflows in general",
    "start": "383680",
    "end": "388720"
  },
  {
    "text": "so we make use of pipelining tools so for instance if you are from a devops ci cd background you know that we need a",
    "start": "388720",
    "end": "396400"
  },
  {
    "text": "tool like jenkins in order to perform the cicd workflows",
    "start": "396400",
    "end": "401840"
  },
  {
    "text": "so similarly in the ml world for us to execute the ml workflows we",
    "start": "401840",
    "end": "407680"
  },
  {
    "text": "need ml pipeline tools so that is how we can be more productive right",
    "start": "407680",
    "end": "413199"
  },
  {
    "text": "and there are a plethora of tools available for us to build ml workflows and run",
    "start": "413199",
    "end": "419919"
  },
  {
    "text": "these workflows in production and today i am going to focus about",
    "start": "419919",
    "end": "425360"
  },
  {
    "text": "one single tool which is called as cube flow and cube flow is by the way an open",
    "start": "425360",
    "end": "432160"
  },
  {
    "text": "source project which also provides you not only with pipelining",
    "start": "432160",
    "end": "438479"
  },
  {
    "text": "capabilities and it also provides you with a game of features and",
    "start": "438479",
    "end": "444599"
  },
  {
    "text": "functionalities that you would expect from an end-to-end machine learning platform okay for instance there is a",
    "start": "444599",
    "end": "452319"
  },
  {
    "text": "k-serve which takes care of serving the models in production",
    "start": "452319",
    "end": "458800"
  },
  {
    "text": "at scale and it provides features like a b testing multi-armed bandits and things like that",
    "start": "458800",
    "end": "466560"
  },
  {
    "text": "and kubeflow also provides you with development capabilities where you can make use of jupiter notebooks in order",
    "start": "466560",
    "end": "473440"
  },
  {
    "text": "to make use of various machine learning frameworks to develop",
    "start": "473440",
    "end": "478479"
  },
  {
    "text": "your model it provides you with capabilities of training your model retraining your model and things",
    "start": "478479",
    "end": "485520"
  },
  {
    "text": "like that but for this talk i will be focusing only on cube flow",
    "start": "485520",
    "end": "492800"
  },
  {
    "text": "pipelines okay so now cube flow as i said earlier",
    "start": "492800",
    "end": "499360"
  },
  {
    "text": "it is actually being tutored or it is actually being branded as a machine learning toolkit for",
    "start": "499360",
    "end": "506720"
  },
  {
    "text": "kubernetes right and it is highly kubernetes native and it makes use of many of the features",
    "start": "506720",
    "end": "515599"
  },
  {
    "text": "that are available in native kubernetes and that's why i call it as kubernetes",
    "start": "515599",
    "end": "521839"
  },
  {
    "text": "native and cube flow by the way it started as an open sourcing of the way google",
    "start": "521839",
    "end": "529200"
  },
  {
    "text": "ran tensorflow models internally right so we know that tensorflow is a very",
    "start": "529200",
    "end": "535279"
  },
  {
    "text": "popular machine learning framework that is widely used and once a tensorflow",
    "start": "535279",
    "end": "541040"
  },
  {
    "text": "model is developed so you need to run this model so google was using some of the features that you find",
    "start": "541040",
    "end": "547680"
  },
  {
    "text": "today in kubeflow internally to run their tensorflow models",
    "start": "547680",
    "end": "553360"
  },
  {
    "text": "in fact it began as just a simpler way to run tenfold tensorflow jobs on kubernetes okay",
    "start": "553360",
    "end": "561680"
  },
  {
    "text": "it actually uh aimed for removing the complexities",
    "start": "561680",
    "end": "567600"
  },
  {
    "text": "associated with running tensorflow jobs on kubernetes and that is how it all started and ever",
    "start": "567600",
    "end": "575360"
  },
  {
    "text": "since uh that q flow has even expanded into a",
    "start": "575360",
    "end": "580640"
  },
  {
    "text": "multi-architecture multi-cloud framework for running end-to-end machine learning",
    "start": "580640",
    "end": "586640"
  },
  {
    "text": "workflows okay so what i mean by end to end is it caters to each and every step",
    "start": "586640",
    "end": "593519"
  },
  {
    "text": "of a typical machine learning life cycle starting from data exploration or even starting from",
    "start": "593519",
    "end": "600800"
  },
  {
    "text": "defining your model accuracy criteria and metrics criteria up till",
    "start": "600800",
    "end": "607040"
  },
  {
    "text": "deploying the model and monitoring the model in production so it offers an",
    "start": "607040",
    "end": "612880"
  },
  {
    "text": "end-to-end platform and kubeflow provides components as i said earlier for each and every stage in",
    "start": "612880",
    "end": "619360"
  },
  {
    "text": "the ml life cycle for exploration for training for deployment for monitoring for",
    "start": "619360",
    "end": "625760"
  },
  {
    "text": "retraining and things like that okay",
    "start": "625760",
    "end": "630320"
  },
  {
    "text": "so what are the installation options available for queue flow so either you",
    "start": "631519",
    "end": "637360"
  },
  {
    "text": "can install kubeflow pipelines as a standalone framework or a platform so that is",
    "start": "637360",
    "end": "643920"
  },
  {
    "text": "available or you can choose to install the complete kubeflow",
    "start": "643920",
    "end": "649680"
  },
  {
    "text": "platform and then use only the kubeflow pipelines part of it okay",
    "start": "649680",
    "end": "654959"
  },
  {
    "text": "or there is a third option you can consume kubeflow as a fully managed",
    "start": "654959",
    "end": "660399"
  },
  {
    "text": "service consume queue flow pipelines as a fully managed service this is offered",
    "start": "660399",
    "end": "665440"
  },
  {
    "text": "by google cloud ai platform pipelines or if you are you if you are trying to",
    "start": "665440",
    "end": "672800"
  },
  {
    "text": "use kubeflow pipelines just for testing purposes you can also install it on local",
    "start": "672800",
    "end": "678800"
  },
  {
    "text": "kubernetes distributions like a3s so that is also available okay",
    "start": "678800",
    "end": "685519"
  },
  {
    "text": "so when we talk about cube flow pipelines it is predominantly built",
    "start": "685519",
    "end": "691279"
  },
  {
    "text": "of these four components okay so the first and foremost you have an",
    "start": "691279",
    "end": "696640"
  },
  {
    "text": "user interface for managing and tracking the various machine learning experiments",
    "start": "696640",
    "end": "702560"
  },
  {
    "text": "jobs and runs and there is a very core workflow engine",
    "start": "702560",
    "end": "708399"
  },
  {
    "text": "that actually performs the hard work of executing the workflow okay we will talk",
    "start": "708399",
    "end": "714320"
  },
  {
    "text": "about what this engine is made up of and things like that later",
    "start": "714320",
    "end": "719839"
  },
  {
    "text": "and a third more important feature of q flow pipelines is it provides you with",
    "start": "719839",
    "end": "725279"
  },
  {
    "text": "an sdk for you to write your pipeline okay and for you to",
    "start": "725279",
    "end": "732000"
  },
  {
    "text": "even build components reusable components for pipeline so that these",
    "start": "732000",
    "end": "737360"
  },
  {
    "text": "components can then be used in different pipelines okay so it provides you with",
    "start": "737360",
    "end": "742560"
  },
  {
    "text": "sdk and there is also a rest api so if you want to re consume kfp by in the",
    "start": "742560",
    "end": "747760"
  },
  {
    "text": "form of rest apis that is also available and whereas if you want to do it in the",
    "start": "747760",
    "end": "754160"
  },
  {
    "text": "using the sdk that is also possible or if you want to just use the ui and then",
    "start": "754160",
    "end": "760320"
  },
  {
    "text": "submit submit the submit the jobs via the ui and then see",
    "start": "760320",
    "end": "765680"
  },
  {
    "text": "the artifacts and things like that that is also possible and kfb also provides you",
    "start": "765680",
    "end": "772160"
  },
  {
    "text": "with some inbuilt notebooks for you to easily interact with kfp using the sdk so that",
    "start": "772160",
    "end": "778959"
  },
  {
    "text": "is also available so",
    "start": "778959",
    "end": "784320"
  },
  {
    "text": "let's get uh or let's spend more time on the slide where this is where you see",
    "start": "784320",
    "end": "791680"
  },
  {
    "text": "the architecture of cube flow okay so at the top of it",
    "start": "791680",
    "end": "797680"
  },
  {
    "text": "at the top of it you have the ui and the ui is served by the pipeline web server",
    "start": "797680",
    "end": "805680"
  },
  {
    "text": "and the ui itself has several capabilities for instance you can actually submit a pipeline",
    "start": "805680",
    "end": "813680"
  },
  {
    "text": "in the ui and once the pipeline is submitted once you have you have run the pipeline",
    "start": "813680",
    "end": "820880"
  },
  {
    "text": "you can see the history of the runs in the ui and you can see several metadata you can",
    "start": "820880",
    "end": "826800"
  },
  {
    "text": "in fact even drill down more deeper into the job history and see what are the steps that were executed what up",
    "start": "826800",
    "end": "833760"
  },
  {
    "text": "what was the input for that step what was the output for that step and",
    "start": "833760",
    "end": "839440"
  },
  {
    "text": "in fact if you you can also see where that output is stored okay and you can also use it for debugging and things",
    "start": "839440",
    "end": "846959"
  },
  {
    "text": "like that and there is also a capability for you to visualize the run so if you get a",
    "start": "846959",
    "end": "853279"
  },
  {
    "text": "results out of training your machine learning model for instance if you are trying your machine learning model using",
    "start": "853279",
    "end": "860160"
  },
  {
    "text": "various hyper parameters right so you can visually see how the model is performing with these various hyper",
    "start": "860160",
    "end": "866880"
  },
  {
    "text": "parameters so so the ui is actually catering to [Music]",
    "start": "866880",
    "end": "872240"
  },
  {
    "text": "a wide wide set of features that is one good thing about q flow pipelines",
    "start": "872240",
    "end": "880480"
  },
  {
    "text": "and underneath you have the system which is the",
    "start": "880480",
    "end": "885680"
  },
  {
    "text": "primary orchestration engine which performs all all the hard work",
    "start": "885680",
    "end": "892079"
  },
  {
    "text": "necessary for executing a kubeflow pipeline so on top of everything you have the",
    "start": "892079",
    "end": "897360"
  },
  {
    "text": "pipeline service so the goal of pipeline service or the responsibility of pipeline services",
    "start": "897360",
    "end": "905680"
  },
  {
    "text": "whatever pipeline you submit to kfp it is the pipeline service that",
    "start": "905680",
    "end": "911199"
  },
  {
    "text": "interprets it it parses it so it understands the python dsl",
    "start": "911199",
    "end": "917360"
  },
  {
    "text": "that is actually defined for writing the pipeline it understands the dsl",
    "start": "917360",
    "end": "923440"
  },
  {
    "text": "it parses the dsl and then eventually it compiles it compiles the",
    "start": "923440",
    "end": "930320"
  },
  {
    "text": "pipeline code and then it prepares the pipeline yaml so that is the job of",
    "start": "930320",
    "end": "936240"
  },
  {
    "text": "pipeline service and whatever is done by the pipeline servers every at every point in time it",
    "start": "936240",
    "end": "942639"
  },
  {
    "text": "makes sure to store the metadata into the metadata",
    "start": "942639",
    "end": "948399"
  },
  {
    "text": "database and by the way it's a mysql database and it stores all the metadata",
    "start": "948399",
    "end": "953519"
  },
  {
    "text": "into this mysql database and once it has determined what",
    "start": "953519",
    "end": "959680"
  },
  {
    "text": "actions or what tasks have to be performed for a particular pipeline run",
    "start": "959680",
    "end": "966079"
  },
  {
    "text": "it goes ahead and creates the necessary kubernetes resources that are required",
    "start": "966079",
    "end": "972560"
  },
  {
    "text": "for executing the pipeline okay and for and and",
    "start": "972560",
    "end": "978000"
  },
  {
    "text": "in kfp each and every step of the pipeline is",
    "start": "978000",
    "end": "983519"
  },
  {
    "text": "executed as a kubernetes part okay so there is a container image",
    "start": "983519",
    "end": "988800"
  },
  {
    "text": "and each and every container is run within the kubernetes part okay so",
    "start": "988800",
    "end": "994959"
  },
  {
    "text": "essentially what happens is whatever kubernetes resources",
    "start": "994959",
    "end": "1000880"
  },
  {
    "text": "that are necessary to execute this pipeline are created by",
    "start": "1000880",
    "end": "1005920"
  },
  {
    "text": "the pipeline service and the pipeline persistence agent basically persists all",
    "start": "1005920",
    "end": "1012639"
  },
  {
    "text": "these kubernetes resources the state of these kubernetes resources",
    "start": "1012639",
    "end": "1018399"
  },
  {
    "text": "the output that these kubernetes resources create it is the job of the",
    "start": "1018399",
    "end": "1024160"
  },
  {
    "text": "pipeline persistence agent to persist all this into the metadata store or even",
    "start": "1024160",
    "end": "1029600"
  },
  {
    "text": "in the artifact storage okay let's move on now underneath the",
    "start": "1029600",
    "end": "1035918"
  },
  {
    "text": "orchestration system you will have a bunch of orchestration controllers",
    "start": "1035919",
    "end": "1042798"
  },
  {
    "text": "so q flow pipeline is built in such a way that it can support multiple",
    "start": "1042799",
    "end": "1048240"
  },
  {
    "text": "orchestration controllers so one primary controller that we use for",
    "start": "1048240",
    "end": "1053919"
  },
  {
    "text": "task driven workflows is the argo workflow and argo workflow is again a separate",
    "start": "1053919",
    "end": "1060799"
  },
  {
    "text": "cncf project for executing workflows so you will also see instances",
    "start": "1060799",
    "end": "1066320"
  },
  {
    "text": "where ml pipelines are written directly in argo workflow using",
    "start": "1066320",
    "end": "1072320"
  },
  {
    "text": "yaml constructs okay but whereas in kubeflow pipeline you have a pipeline servers you have an sdk there is a v2",
    "start": "1072320",
    "end": "1080640"
  },
  {
    "text": "version of the sdk you have a dsl you have a dsl compiler and you get everything on top of that",
    "start": "1080640",
    "end": "1087840"
  },
  {
    "text": "okay but for this presentation we will stick to the orgo workflow controller",
    "start": "1087840",
    "end": "1094880"
  },
  {
    "text": "and yes once the resources are created whatever output that are created by",
    "start": "1094880",
    "end": "1100640"
  },
  {
    "text": "these resources for instance by resources i mean the pods that",
    "start": "1100640",
    "end": "1105919"
  },
  {
    "text": "actually execute the step is eventually stored in the data artifact by default it is menio and",
    "start": "1105919",
    "end": "1114240"
  },
  {
    "text": "there is an option to use other data artifacts as well so let's get moving",
    "start": "1114240",
    "end": "1122080"
  },
  {
    "text": "choosing an argo workflow executor so as i said earlier",
    "start": "1122080",
    "end": "1127200"
  },
  {
    "text": "queue flow pipelines run on argo workflows so argo workflow",
    "start": "1127200",
    "end": "1132880"
  },
  {
    "text": "is the primary workflow engine okay that actually executes the ml workflow",
    "start": "1132880",
    "end": "1140160"
  },
  {
    "text": "and you can either use the docker executor for argo workflow or you can",
    "start": "1140160",
    "end": "1145600"
  },
  {
    "text": "use the very latest emissary executor and by the way emissary executor is the default",
    "start": "1145600",
    "end": "1151679"
  },
  {
    "text": "executor from version 1.8.0 onwards docker executor is actually",
    "start": "1151679",
    "end": "1160559"
  },
  {
    "text": "has some limitations for instance it supports only the docker container runtime and we know very well",
    "start": "1160559",
    "end": "1167600"
  },
  {
    "text": "that in version 1.24 of kubernetes uh the docker",
    "start": "1167600",
    "end": "1173120"
  },
  {
    "text": "shim is getting removed or the docker shim has already been removed because 1.24 is already out",
    "start": "1173120",
    "end": "1180000"
  },
  {
    "text": "and which means the docker executor can be used only if you are using an",
    "start": "1180000",
    "end": "1186720"
  },
  {
    "text": "older version of kubernetes right and from security perspective since docker needs",
    "start": "1186720",
    "end": "1193919"
  },
  {
    "text": "privileged access to the docker socket on the host it is not preferable",
    "start": "1193919",
    "end": "1199520"
  },
  {
    "text": "to use such a such a approach or such a solution in",
    "start": "1199520",
    "end": "1205200"
  },
  {
    "text": "production whereas emissary executor supports any container runtime and it is",
    "start": "1205200",
    "end": "1211440"
  },
  {
    "text": "also more secure okay so so moving forward uh it is going to",
    "start": "1211440",
    "end": "1216880"
  },
  {
    "text": "be by default uh emissary executor that is already and default",
    "start": "1216880",
    "end": "1222000"
  },
  {
    "text": "executor from version 1.8.0 onwards",
    "start": "1222000",
    "end": "1227520"
  },
  {
    "text": "and other notable features of kfp so i wanted to give you this",
    "start": "1228159",
    "end": "1234400"
  },
  {
    "text": "other features because it will help you to understand in a more deeper way about kfp",
    "start": "1234400",
    "end": "1242400"
  },
  {
    "text": "so it provides out of the box multi-user isolation for pipelines and by the way",
    "start": "1242400",
    "end": "1248480"
  },
  {
    "text": "this is available only in the full q flow deployment it is not yet available in the standalone kfb deployment",
    "start": "1248480",
    "end": "1256000"
  },
  {
    "text": "basically this feature allows you to separate the kubernetes resources for",
    "start": "1256000",
    "end": "1261039"
  },
  {
    "text": "multiple users so you can create multiple profiles and each profile is nothing but",
    "start": "1261039",
    "end": "1267600"
  },
  {
    "text": "each profile is actually get getting mapped into a kubernetes namespace so if you if you create a user",
    "start": "1267600",
    "end": "1274320"
  },
  {
    "text": "profile and that particular user when they run a queue flow",
    "start": "1274320",
    "end": "1279679"
  },
  {
    "text": "pipeline whatever resources that are created for that pipeline run will get created only in that particular",
    "start": "1279679",
    "end": "1285919"
  },
  {
    "text": "namespace okay so this provides you with isolation for instance when you are",
    "start": "1285919",
    "end": "1292159"
  },
  {
    "text": "sharing a queue flow instance with multiple users it provides you with very good isolation",
    "start": "1292159",
    "end": "1298400"
  },
  {
    "text": "and another good feature is step caching okay so we saw that there are the",
    "start": "1298400",
    "end": "1304159"
  },
  {
    "text": "pipeline is executed in multiple steps let's say you create a pipeline run",
    "start": "1304159",
    "end": "1309440"
  },
  {
    "text": "and let's say you once again recreate a pipeline pipeline run this",
    "start": "1309440",
    "end": "1315280"
  },
  {
    "text": "time just by modifying the hyper parameters alone okay the and this modification of hyper",
    "start": "1315280",
    "end": "1322400"
  },
  {
    "text": "parameters is specific to a particular step let's assume so step caching makes",
    "start": "1322400",
    "end": "1328080"
  },
  {
    "text": "use that whatever steps that were run previously do not get executed again and it will",
    "start": "1328080",
    "end": "1334240"
  },
  {
    "text": "elegantly use the output of the step that is already cashed and it will",
    "start": "1334240",
    "end": "1341919"
  },
  {
    "text": "skip the execution of the step so the speeds up the execution of the pipeline it also",
    "start": "1341919",
    "end": "1348080"
  },
  {
    "text": "efficiently uses the resource of the pipeline and you can also control uh when",
    "start": "1348080",
    "end": "1354640"
  },
  {
    "text": "you the cache invalidation should happen and when the caching should be disabled",
    "start": "1354640",
    "end": "1360480"
  },
  {
    "text": "and you can also altogether either enable or disable the caching feature",
    "start": "1360480",
    "end": "1366880"
  },
  {
    "text": "and another feature that was recently introduced in the sdk v2 is pipeline",
    "start": "1366880",
    "end": "1372480"
  },
  {
    "text": "root this essentially represents an artifact repository where the pipeline stores",
    "start": "1372480",
    "end": "1380000"
  },
  {
    "text": "artifacts okay so originally only minio was supported and that too the minio",
    "start": "1380000",
    "end": "1385760"
  },
  {
    "text": "that was packaged along with kubeflow pipelines that was the only way to store",
    "start": "1385760",
    "end": "1390799"
  },
  {
    "text": "your artifacts but whereas now you have three different options you can have minio you can bring your own menu",
    "start": "1390799",
    "end": "1398480"
  },
  {
    "text": "or you can use any s3 compatible object storage or you can use even gcs google",
    "start": "1398480",
    "end": "1403919"
  },
  {
    "text": "cloud storage all right now let's get into a quick demo",
    "start": "1403919",
    "end": "1411679"
  },
  {
    "text": "so for the demo this is how the pipeline is going to look like so the first step is training the",
    "start": "1411679",
    "end": "1418880"
  },
  {
    "text": "initial model and then receiving a candidate model and further on we retrain the model with",
    "start": "1418880",
    "end": "1426880"
  },
  {
    "text": "more data so that we increase the accuracy of the model",
    "start": "1426880",
    "end": "1431919"
  },
  {
    "text": "and once we get this retrained model we we just",
    "start": "1431919",
    "end": "1437200"
  },
  {
    "text": "run model prediction on this model and then we calculate the matrix out of the",
    "start": "1437200",
    "end": "1444240"
  },
  {
    "text": "data that was produced by the model prediction and if the matrix is within as within",
    "start": "1444240",
    "end": "1451840"
  },
  {
    "text": "the acceptable criteria then the retraining is stopped if or",
    "start": "1451840",
    "end": "1457919"
  },
  {
    "text": "else the training is again retriggered and then",
    "start": "1457919",
    "end": "1463279"
  },
  {
    "text": "the retraining happens and this happens in a loop until the",
    "start": "1463279",
    "end": "1469120"
  },
  {
    "text": "model accuracy is as per our expected criteria",
    "start": "1469120",
    "end": "1474799"
  },
  {
    "text": "so let me let us go into the demo",
    "start": "1474799",
    "end": "1479440"
  },
  {
    "text": "so let me end the slide show and before i open the ui let me show",
    "start": "1480400",
    "end": "1487840"
  },
  {
    "text": "the list of pods that are actually running for a queue flow pipeline",
    "start": "1487840",
    "end": "1494320"
  },
  {
    "text": "installation so here you can see the minio which is actually the artifact",
    "start": "1494320",
    "end": "1500000"
  },
  {
    "text": "repository the mysql database which is actually the metadata store",
    "start": "1500000",
    "end": "1506720"
  },
  {
    "text": "and workflow controller is basically the argo workflow controller because this installation has only",
    "start": "1506720",
    "end": "1513520"
  },
  {
    "text": "orgo workflow controller and this is the pipeline service which",
    "start": "1513520",
    "end": "1519600"
  },
  {
    "text": "basically accepts the pipeline and then creates the various kubernetes resources",
    "start": "1519600",
    "end": "1525600"
  },
  {
    "text": "this is the pipeline persistence agent which persists all the kubernetes",
    "start": "1525600",
    "end": "1531039"
  },
  {
    "text": "resources their input and output everything in the ml data store",
    "start": "1531039",
    "end": "1537360"
  },
  {
    "text": "and schedule workflow is used whenever we need to schedule workflows rather than",
    "start": "1537360",
    "end": "1543600"
  },
  {
    "text": "one-time workflows we can also have scheduled workflows and when we have scheduled workflows the scheduling is",
    "start": "1543600",
    "end": "1550960"
  },
  {
    "text": "actually taken care of this component and you have a bunch of other components",
    "start": "1550960",
    "end": "1556960"
  },
  {
    "text": "these are all ui related components the pipeline ui uh the pipeline viewer crd as well as",
    "start": "1556960",
    "end": "1564000"
  },
  {
    "text": "the pipeline visualization server so visualization server is basically",
    "start": "1564000",
    "end": "1569120"
  },
  {
    "text": "it crunches all the data from the metadata server and then it creates",
    "start": "1569120",
    "end": "1574960"
  },
  {
    "text": "the visualizations that are necessary to actually evaluate the performance of the model",
    "start": "1574960",
    "end": "1581360"
  },
  {
    "text": "okay now let's get into the kfp ui",
    "start": "1581360",
    "end": "1588320"
  },
  {
    "text": "so along with the installation of kfp there are some default pipelines that are",
    "start": "1588320",
    "end": "1595600"
  },
  {
    "text": "installed as part of the installation and today i am going to use one such pipeline",
    "start": "1595600",
    "end": "1602960"
  },
  {
    "text": "which is actually the pipeline that i explained using this light",
    "start": "1602960",
    "end": "1608240"
  },
  {
    "text": "so this is how it looks graphically and i am going to run this pipeline",
    "start": "1608240",
    "end": "1616000"
  },
  {
    "text": "by clicking on start so once i do that a run a new run has",
    "start": "1616080",
    "end": "1621520"
  },
  {
    "text": "been created so i can click on this run and it will show you show me a visual",
    "start": "1621520",
    "end": "1627919"
  },
  {
    "text": "graph explaining the various progressions of that particular graph so",
    "start": "1627919",
    "end": "1635440"
  },
  {
    "text": "as you can see this step has completed and you can see",
    "start": "1635440",
    "end": "1641120"
  },
  {
    "text": "that this step produced two output artifacts so oneness",
    "start": "1641120",
    "end": "1646240"
  },
  {
    "text": "it produced a table which was stored in the artifact repository",
    "start": "1646240",
    "end": "1653279"
  },
  {
    "text": "and then it produced the logs and we can also see the pod that was the kubernetes",
    "start": "1653279",
    "end": "1658640"
  },
  {
    "text": "pod that was created for executing the step what are the events that were generated",
    "start": "1658640",
    "end": "1665360"
  },
  {
    "text": "so if at all there are some failures we can look at the events and try to figure out what went wrong",
    "start": "1665360",
    "end": "1673279"
  },
  {
    "text": "so what is happening is the data transformation steps have",
    "start": "1673279",
    "end": "1678320"
  },
  {
    "text": "completed the initial model training has also been completed",
    "start": "1678320",
    "end": "1684720"
  },
  {
    "text": "and we are seeing in this step",
    "start": "1684720",
    "end": "1690159"
  },
  {
    "text": "that the initial model as well as",
    "start": "1690159",
    "end": "1696320"
  },
  {
    "text": "the data set are being sent as input for the step",
    "start": "1696320",
    "end": "1701679"
  },
  {
    "text": "and the output of the step is the trained model",
    "start": "1701679",
    "end": "1707679"
  },
  {
    "text": "along with the model config plus the logs okay that is what we see it as output",
    "start": "1707679",
    "end": "1715039"
  },
  {
    "text": "again we can see the pod that was created we can also see the",
    "start": "1715039",
    "end": "1720640"
  },
  {
    "text": "logs that was created by the container that ran this step",
    "start": "1720640",
    "end": "1726880"
  },
  {
    "text": "so there are some details it says succeeded volumes so there were no volume mounts",
    "start": "1726880",
    "end": "1734159"
  },
  {
    "text": "used for this step visualizations no visualizations for this particular step",
    "start": "1734159",
    "end": "1741679"
  },
  {
    "text": "let's see what's there in ml data ml metadata there is it says corresponding ml",
    "start": "1741679",
    "end": "1747279"
  },
  {
    "text": "metadata not found and meanwhile the pipeline has progressed",
    "start": "1747279",
    "end": "1753760"
  },
  {
    "text": "to the point that it has calculated the matrix and it has decided that the matrix is not",
    "start": "1753760",
    "end": "1760000"
  },
  {
    "text": "as per expectations and it is triggering a retraining",
    "start": "1760000",
    "end": "1765840"
  },
  {
    "text": "okay so the retraining has concluded and it is again predicting the retrained",
    "start": "1765840",
    "end": "1773200"
  },
  {
    "text": "model so for this prediction it uses the retrained model as well as",
    "start": "1773200",
    "end": "1778880"
  },
  {
    "text": "the data set data from the data set okay",
    "start": "1778880",
    "end": "1784159"
  },
  {
    "text": "let's see what is the output of this prediction yeah the prediction output is available",
    "start": "1786799",
    "end": "1793039"
  },
  {
    "text": "and there is a calculation that is going on let us see what is the result of this",
    "start": "1793039",
    "end": "1798080"
  },
  {
    "text": "calculation ok so the calculation has determined that the expected condition has reached",
    "start": "1798080",
    "end": "1805360"
  },
  {
    "text": "so the run has completed so you can see that there is a green tick mark and it says executed successfully the pipeline",
    "start": "1805360",
    "end": "1813520"
  },
  {
    "text": "has ran successfully and now if we come here we can see all the pods that were",
    "start": "1813520",
    "end": "1820640"
  },
  {
    "text": "created by argo workflow in order to execute the pipeline so for every step",
    "start": "1820640",
    "end": "1826399"
  },
  {
    "text": "in the pipeline you will see a corresponding pod so you can also",
    "start": "1826399",
    "end": "1831520"
  },
  {
    "text": "use cube ctl commands and then look at the pods look at the logs that were",
    "start": "1831520",
    "end": "1837200"
  },
  {
    "text": "produced by this pod events that were produced by the spots uh the same information that you saw in",
    "start": "1837200",
    "end": "1844799"
  },
  {
    "text": "the ui okay yeah this is a very very simple",
    "start": "1844799",
    "end": "1850080"
  },
  {
    "text": "simple pipeline that we typically will find during model",
    "start": "1850080",
    "end": "1855120"
  },
  {
    "text": "exploration and model development phase and we saw now that q flow pipeline was able",
    "start": "1855120",
    "end": "1862080"
  },
  {
    "text": "to execute this pipeline and execute it successfully okay",
    "start": "1862080",
    "end": "1868240"
  },
  {
    "text": "yeah and that is pretty much what i intended to talk and i really hope that",
    "start": "1868240",
    "end": "1874799"
  },
  {
    "text": "you enjoy the talk and i really hope that the content of this talk will be",
    "start": "1874799",
    "end": "1880799"
  },
  {
    "text": "useful and by the way if you have any questions about this talk feel free to post them as text questions",
    "start": "1880799",
    "end": "1889519"
  },
  {
    "text": "in the corresponding slack channel and i'll make sure that i provide an",
    "start": "1889519",
    "end": "1894720"
  },
  {
    "text": "appropriate reply to your questions so enjoy that enjoy the day and enjoy the",
    "start": "1894720",
    "end": "1900320"
  },
  {
    "text": "rest of the talks and see you soon thank you so much bye",
    "start": "1900320",
    "end": "1907559"
  }
]