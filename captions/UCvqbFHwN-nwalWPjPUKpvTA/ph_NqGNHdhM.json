[
  {
    "text": "hi everyone I'm Dave I'm the Tri Coast for this room for this afternoon and I just wanted to introduce muslin and",
    "start": "30",
    "end": "6600"
  },
  {
    "text": "remind everyone to leave feedback on the sched app after the talk so we can tell",
    "start": "6600",
    "end": "12360"
  },
  {
    "text": "most of how we all felt about it but with that most in talks about linker D we pay thank you David",
    "start": "12360",
    "end": "21020"
  },
  {
    "text": "all right thanks everyone for joining me today Moss and Rose I am the software engineer",
    "start": "21020",
    "end": "27480"
  },
  {
    "text": "at we pay now a chase company super excited to be here talking about what we",
    "start": "27480",
    "end": "33210"
  },
  {
    "text": "did last year to essentially integrate service much with our infrastructure and",
    "start": "33210",
    "end": "38930"
  },
  {
    "text": "migrate all of our production services and and traffic to link Rd last year so",
    "start": "38930",
    "end": "47489"
  },
  {
    "text": "to kind of get things started the who's here have learned have heard of linker D before and who's played with lincolni",
    "start": "47489",
    "end": "56430"
  },
  {
    "text": "before and who's in production all right",
    "start": "56430",
    "end": "62239"
  },
  {
    "text": "this is a good is a good crowd I hope I hope you learned a lot from you know",
    "start": "62239",
    "end": "69000"
  },
  {
    "text": "what we learned last year from migrating so to kind of you know since this is",
    "start": "69000",
    "end": "77130"
  },
  {
    "text": "about our journey and I thought it would be appropriate to take a look at you know where we came from pre mesh and",
    "start": "77130",
    "end": "83729"
  },
  {
    "text": "then take a look at you know how we",
    "start": "83729",
    "end": "89729"
  },
  {
    "text": "migrated with mesh I guess introduced service much to our infrastructure and then migrated our services to it it's",
    "start": "89729",
    "end": "95189"
  },
  {
    "text": "really a two-step process and at the end you know take a look at a couple of options or examples of how we",
    "start": "95189",
    "end": "103950"
  },
  {
    "text": "you know maintain the infrastructure going forward now that we have everything on the infrastructure so",
    "start": "103950",
    "end": "114710"
  },
  {
    "text": "generally as our you know as our business has evolved and your",
    "start": "114710",
    "end": "119939"
  },
  {
    "text": "infrastructure involved with the business you know we've kind of faced challenges that I want to cover in this",
    "start": "119939",
    "end": "125790"
  },
  {
    "text": "section and that sort of starts with sorry it starts with our business focus",
    "start": "125790",
    "end": "134310"
  },
  {
    "text": "so we pay what we do we empower businesses using world-class software to",
    "start": "134310",
    "end": "141840"
  },
  {
    "text": "be able to facilitate payments through the for their users and so how do we do",
    "start": "141840",
    "end": "150150"
  },
  {
    "text": "this the way we did add is we provide public API is public payment API is that",
    "start": "150150",
    "end": "155450"
  },
  {
    "text": "our payment partners use to be able to synchronously authorize charge of",
    "start": "155450",
    "end": "161700"
  },
  {
    "text": "payments and those are famously receive from their users and so any successful",
    "start": "161700",
    "end": "167940"
  },
  {
    "text": "payment that goes through the system and successful you are authorized then there",
    "start": "167940",
    "end": "173160"
  },
  {
    "text": "are captured in the background in a different time so as a to be able to",
    "start": "173160",
    "end": "180959"
  },
  {
    "text": "provide that world-class payment api's you know we the goal is to",
    "start": "180959",
    "end": "187769"
  },
  {
    "text": "provide a very high success rate for all the payments that are being authorized",
    "start": "187769",
    "end": "194129"
  },
  {
    "text": "through a through the system so we want to be able to you know minimize that",
    "start": "194129",
    "end": "199620"
  },
  {
    "text": "fraction a fraction of issues that happens and since these authorizations are happening synchronicity in the 3d",
    "start": "199620",
    "end": "206909"
  },
  {
    "text": "system we want to make sure that any internal infrastructure or service",
    "start": "206909",
    "end": "212190"
  },
  {
    "text": "problems I don't really affect the users of the api's so a few years ago this whole all",
    "start": "212190",
    "end": "223590"
  },
  {
    "text": "the payment api is were backed by a single monolithic application that all",
    "start": "223590",
    "end": "229590"
  },
  {
    "text": "the synchronous payment requests were going through it and it would validate all the payments and then so we wanted",
    "start": "229590",
    "end": "237420"
  },
  {
    "text": "to make sure us providing a highly available service one make sure we have we have internally and overall hae",
    "start": "237420",
    "end": "244049"
  },
  {
    "text": "system so we made use of a few monitoring services to be able to you",
    "start": "244049",
    "end": "249150"
  },
  {
    "text": "know look at custom metrics you know create custom checks and look at application performance through all",
    "start": "249150",
    "end": "256400"
  },
  {
    "text": "these different things so we can cover an overall high availability story and",
    "start": "256400",
    "end": "262490"
  },
  {
    "text": "if anything goes wrong then there's an on-call team that for anything that is not out of healed then that on-call team",
    "start": "262490",
    "end": "269750"
  },
  {
    "text": "kind of digs in and starts debugging so as our monolith became the bottleneck",
    "start": "269750",
    "end": "277280"
  },
  {
    "text": "for the whole infrastructure and our engineering teams started to grow larger",
    "start": "277280",
    "end": "283970"
  },
  {
    "text": "than you know being able to collaborate on a single service we started refactoring that monolithic application",
    "start": "283970",
    "end": "290600"
  },
  {
    "text": "into micro services and by by doing that you need to find a solution to be able",
    "start": "290600",
    "end": "295729"
  },
  {
    "text": "to host them so we chose the managed kubernetes clusters and running those",
    "start": "295729",
    "end": "304310"
  },
  {
    "text": "micro services as containers to be able to you know serve the same set of",
    "start": "304310",
    "end": "310220"
  },
  {
    "text": "features in the background and so these these micro services are all split into",
    "start": "310220",
    "end": "315560"
  },
  {
    "text": "different sub networks and in return in two different decayed clusters to be",
    "start": "315560",
    "end": "322340"
  },
  {
    "text": "able to kind of focus on a specific set of logics in the background so to kind",
    "start": "322340",
    "end": "331010"
  },
  {
    "text": "of continue that H a story we also use the same monitoring best practices and",
    "start": "331010",
    "end": "338150"
  },
  {
    "text": "applied it to the micro services so overall we can continue relying on the",
    "start": "338150",
    "end": "343280"
  },
  {
    "text": "same monitoring services that gave us the good h/h story all until until this",
    "start": "343280",
    "end": "350330"
  },
  {
    "text": "point so now if we kind of focus a little bit and zoom in on the micro",
    "start": "350330",
    "end": "356030"
  },
  {
    "text": "services the way these microsoft says communication layers set up is the",
    "start": "356030",
    "end": "362510"
  },
  {
    "text": "engine axis fronting the containers to be able to provide the TLS termination",
    "start": "362510",
    "end": "369650"
  },
  {
    "text": "and then if you take an example of service a calling service B from cluster",
    "start": "369650",
    "end": "375650"
  },
  {
    "text": "B to cluster a service a is configured to",
    "start": "375650",
    "end": "380719"
  },
  {
    "text": "a fully qualified name for service B so what it does it tries to resolve the",
    "start": "380719",
    "end": "386989"
  },
  {
    "text": "four that fully qualified name in its configuration once it has a once it's resolved that then it's sent census",
    "start": "386989",
    "end": "394849"
  },
  {
    "text": "requests over TLS to service B and genetics terminates a cell and then sends that request to service B and then",
    "start": "394849",
    "end": "402439"
  },
  {
    "text": "the same thing happens for the communication between B and C so it doesn't matter where these services are",
    "start": "402439",
    "end": "408529"
  },
  {
    "text": "existing what cluster they are they're all configured the same way to be able to find one another in the same way and",
    "start": "408529",
    "end": "415519"
  },
  {
    "text": "so since these micro services are the only eyes in the communication layer",
    "start": "415519",
    "end": "422149"
  },
  {
    "text": "they're also responsible for creating their own tracing and you know distributed tracing if you will in an",
    "start": "422149",
    "end": "428569"
  },
  {
    "text": "old-school way and they're also creating the metrics that's appropriate for",
    "start": "428569",
    "end": "435559"
  },
  {
    "text": "knowing what's going on in the infrastructure and that includes I need request numbers any latency so on and so",
    "start": "435559",
    "end": "442339"
  },
  {
    "text": "forth so we can kind of take that and it's an open-ended kind of a world for metrics so in this scenario Prometheus",
    "start": "442339",
    "end": "450019"
  },
  {
    "text": "collects the metrics from these micro services directly and sends who since",
    "start": "450019",
    "end": "457119"
  },
  {
    "text": "they need to monitor the health check of these services from the eyes of all these other micro services it calls you",
    "start": "457119",
    "end": "465559"
  },
  {
    "text": "know nginx to be able to test the health of these services so we've successfully",
    "start": "465559",
    "end": "474529"
  },
  {
    "text": "been able to kind of develop that infrastructure that gives us a few things you know end-to-end monitoring",
    "start": "474529",
    "end": "480979"
  },
  {
    "text": "we're able to monitor everything great we're able to tell what's going on if something goes wrong in the",
    "start": "480979",
    "end": "486919"
  },
  {
    "text": "infrastructure every single piece in the infrastructure is you know configurable upgradeable independently and it doesn't",
    "start": "486919",
    "end": "495319"
  },
  {
    "text": "matter where they live in it in that environment or data center so and certain things to certain point are",
    "start": "495319",
    "end": "501469"
  },
  {
    "text": "automated and it kind of gives us what we need to be able to run the",
    "start": "501469",
    "end": "506869"
  },
  {
    "text": "infrastructure the way we want it but",
    "start": "506869",
    "end": "512090"
  },
  {
    "text": "the setup is super complex and and complexity and being so closely",
    "start": "512090",
    "end": "518690"
  },
  {
    "text": "integrated with these micro services it doesn't really encourage big improvements on the infrastructure or",
    "start": "518690",
    "end": "525350"
  },
  {
    "text": "the micro services that are using the infrastructure so we want to kind of take this opportunity and start moving",
    "start": "525350",
    "end": "532460"
  },
  {
    "text": "toward an infrastructure 2.0 where we can start defining our infrastructure in",
    "start": "532460",
    "end": "538220"
  },
  {
    "text": "code with an idea to be doing that and through being able to build a",
    "start": "538220",
    "end": "544940"
  },
  {
    "text": "communication platform where you know communication protocol configuration or",
    "start": "544940",
    "end": "550070"
  },
  {
    "text": "changes are easier a lifecycle of that layer is easier independent of how many",
    "start": "550070",
    "end": "555380"
  },
  {
    "text": "services or where the services are running and be able to make use of more modern technologies like software load",
    "start": "555380",
    "end": "563330"
  },
  {
    "text": "balancing you know you might need to use specific algorithms in your software load",
    "start": "563330",
    "end": "568700"
  },
  {
    "text": "balancing then you know round-robin for example and then make it easier and take",
    "start": "568700",
    "end": "574070"
  },
  {
    "text": "some of the responsibility that's really the infrastructure is responsibility from the micro services so they can",
    "start": "574070",
    "end": "581030"
  },
  {
    "text": "focus on developing the product more than being concerned about the",
    "start": "581030",
    "end": "586130"
  },
  {
    "text": "infrastructure layers so and that's essentially the goal of sort of",
    "start": "586130",
    "end": "593870"
  },
  {
    "text": "integrating with service mesh and being able to simplify the infrastructure and make it more modern and that really",
    "start": "593870",
    "end": "600880"
  },
  {
    "text": "involves three major steps first is introducing service much to the",
    "start": "600880",
    "end": "605960"
  },
  {
    "text": "infrastructure that it's only a level of complexity the other is integrating all",
    "start": "605960",
    "end": "611480"
  },
  {
    "text": "the micro services that are not on service managed and being able to start moving their request to service mash and",
    "start": "611480",
    "end": "618830"
  },
  {
    "text": "that's opposed for receiving and sending requests and also doing all that while",
    "start": "618830",
    "end": "624500"
  },
  {
    "text": "we keep the high availability at the same level with the same set of",
    "start": "624500",
    "end": "629780"
  },
  {
    "text": "requirements and at the same time we want to make sure that this new infrastructure is also highly available",
    "start": "629780",
    "end": "637660"
  },
  {
    "text": "so kind of recap on what service measure is a typical you probably hear this",
    "start": "638800",
    "end": "644390"
  },
  {
    "text": "every and all the mesh tox service the typical service nation infrastructure includes",
    "start": "644390",
    "end": "650420"
  },
  {
    "text": "two major components one is the data plane which is backed by a linker D in",
    "start": "650420",
    "end": "656660"
  },
  {
    "text": "this scenario and this is linker d1 and I'll explain why and then one of the",
    "start": "656660",
    "end": "663440"
  },
  {
    "text": "reasons is because we're using named our D for service discovery and name where D gives us that backbone for the control",
    "start": "663440",
    "end": "669530"
  },
  {
    "text": "plane in linker d 1x world so what name or D does it watches the events that are",
    "start": "669530",
    "end": "676580"
  },
  {
    "text": "going through the API servers for the GK clusters and since we're using the",
    "start": "676580",
    "end": "682490"
  },
  {
    "text": "regional GK clusters here we also get a GK master cluster or API server cluster",
    "start": "682490",
    "end": "689210"
  },
  {
    "text": "where you are given a load balancer with set of backends for different zones so",
    "start": "689210",
    "end": "695510"
  },
  {
    "text": "you're available in the region with multiple depending on where your class which zones your clusters are set up",
    "start": "695510",
    "end": "701540"
  },
  {
    "text": "with so we also add that layer of high availability to be able to discover",
    "start": "701540",
    "end": "707840"
  },
  {
    "text": "services in in case zone C for example Goes Down and you're available in a B",
    "start": "707840",
    "end": "713060"
  },
  {
    "text": "and C you can still use a B to be able to watch the events that are going",
    "start": "713060",
    "end": "718550"
  },
  {
    "text": "through that cluster and the same kind of watch events story happens for all",
    "start": "718550",
    "end": "725600"
  },
  {
    "text": "the cluster that that are in that environment so and we call that a",
    "start": "725600",
    "end": "730910"
  },
  {
    "text": "discovery scope so you want to be able to discover all the things that are in the discovery scope no matter if you're",
    "start": "730910",
    "end": "736640"
  },
  {
    "text": "inside GK or outside GK which is another reason why we're using being pretty one",
    "start": "736640",
    "end": "745000"
  },
  {
    "text": "so layering this whole thing with the microservices story you might already",
    "start": "745000",
    "end": "752840"
  },
  {
    "text": "see that it's simplified you know services making communications to one another they're not they don't need to",
    "start": "752840",
    "end": "759920"
  },
  {
    "text": "resolve anymore you know names the linker DS already",
    "start": "759920",
    "end": "765350"
  },
  {
    "text": "have a mapping of all the service graph and so they can so when service a is sending a request a",
    "start": "765350",
    "end": "771410"
  },
  {
    "text": "service C all it does it sends a request to its link ready proxy and that proxy",
    "start": "771410",
    "end": "777260"
  },
  {
    "text": "takes it over fold it to the destination privacy in cluster a and enact forwards the request",
    "start": "777260",
    "end": "784000"
  },
  {
    "text": "to service C and the same thing happens from service to service B so so really",
    "start": "784000",
    "end": "791650"
  },
  {
    "text": "that data plane layer it provides the tracing so now that we've shifted the",
    "start": "791650",
    "end": "798630"
  },
  {
    "text": "visibility or observer observability from the services to that",
    "start": "798630",
    "end": "804010"
  },
  {
    "text": "data layer data plane layer so these the proxies can now do that distributed",
    "start": "804010",
    "end": "810070"
  },
  {
    "text": "tracing for the services it doesn't matter what service is doing what call all these prices are aware of what's",
    "start": "810070",
    "end": "816070"
  },
  {
    "text": "going on in that whole infrastructure in the scope that demand and they can also produce the right metrics and this is",
    "start": "816070",
    "end": "823030"
  },
  {
    "text": "this very important because now we're starting to refactor and take away the",
    "start": "823030",
    "end": "828280"
  },
  {
    "text": "infrastructure responsibilities from services and make it simpler for them and all we need to do is integrate",
    "start": "828280",
    "end": "835600"
  },
  {
    "text": "everything with their proxies so as far as integration for the micro services",
    "start": "835600",
    "end": "842710"
  },
  {
    "text": "with Lync Rd there are two options you can and each have their pros and cons",
    "start": "842710",
    "end": "849450"
  },
  {
    "text": "the local option which is using a sidecar pattern in kubernetes world it",
    "start": "849450",
    "end": "856270"
  },
  {
    "text": "uses you know we'll use a localhost with an outgoing port which means send my requests outside to the destination",
    "start": "856270",
    "end": "864670"
  },
  {
    "text": "service with the API requests because URI for their destination service the",
    "start": "864670",
    "end": "870610"
  },
  {
    "text": "other option is using a per node configuration where then you need to be",
    "start": "870610",
    "end": "876760"
  },
  {
    "text": "able to the D services at runtime need to be able to find their proxy that they",
    "start": "876760",
    "end": "882640"
  },
  {
    "text": "need to send a request to but the other important the API stays the same so",
    "start": "882640",
    "end": "889210"
  },
  {
    "text": "challenge here is for the services to actually be able to find the right place",
    "start": "889210",
    "end": "894520"
  },
  {
    "text": "where these practices are running in the D it's a pattern so so since we have",
    "start": "894520",
    "end": "902050"
  },
  {
    "text": "both kubernetes and non kubernetes services we need to you know we're using link 31",
    "start": "902050",
    "end": "908880"
  },
  {
    "text": "and that's the only way we can get an on kubernetes connection in this infrastructure within the same discovery",
    "start": "908880",
    "end": "917550"
  },
  {
    "text": "scope so now monolith needs to talk to you service B since it has a linker D",
    "start": "917550",
    "end": "924390"
  },
  {
    "text": "proxy it sends a request there and then service B receives it from the proxy",
    "start": "924390",
    "end": "930300"
  },
  {
    "text": "that's running in cluster a so how that happens is with a semi simplified detail",
    "start": "930300",
    "end": "936900"
  },
  {
    "text": "that you see next to name or D and so for rest it's super simple like there's",
    "start": "936900",
    "end": "942390"
  },
  {
    "text": "no the service discovery already handles what's going out for GRP see it's a",
    "start": "942390",
    "end": "948390"
  },
  {
    "text": "little bit simplified here because you need to kind of go through the G RPC header configuration to be able to kind",
    "start": "948390",
    "end": "957000"
  },
  {
    "text": "of map your G RPC packages to the actual service name that name ready knows as a service name I'm not going to go into",
    "start": "957000",
    "end": "964080"
  },
  {
    "text": "detail about that but I encourage you reading our latest blog post about this it goes into detail about how we",
    "start": "964080",
    "end": "969570"
  },
  {
    "text": "configured both name most well I guess name or D and linker D but at the same",
    "start": "969570",
    "end": "974940"
  },
  {
    "text": "time how we have our applications use this configuration to be able to send both rest and G RPC requests at the same",
    "start": "974940",
    "end": "983160"
  },
  {
    "text": "time so since both these clusters and non-kin kubernetes services are in the",
    "start": "983160",
    "end": "989490"
  },
  {
    "text": "same discovery scope then service they can also send request a service c by",
    "start": "989490",
    "end": "996690"
  },
  {
    "text": "using the same name or d configuration so i kind of mentioned this but linker",
    "start": "996690",
    "end": "1003470"
  },
  {
    "text": "DS and the core telemetry pieces of name or d exposed more than a thousand",
    "start": "1003470",
    "end": "1009350"
  },
  {
    "text": "metrics points so then you can start gathering information about you know on",
    "start": "1009350",
    "end": "1015260"
  },
  {
    "text": "the client side you can look at things like latency whether it's client latency or overall latency over the wire between",
    "start": "1015260",
    "end": "1022160"
  },
  {
    "text": "service you know and then you know request times and so",
    "start": "1022160",
    "end": "1027800"
  },
  {
    "text": "on and so forth and then on the server side you can take you can look at a lot of other things as internal resources",
    "start": "1027800",
    "end": "1035230"
  },
  {
    "text": "any you know internal connectivity and then anything that could be a bottom like for the for either link ID or name",
    "start": "1035230",
    "end": "1041750"
  },
  {
    "text": "ID and you can map it in Griffin I using parameters using that collected data and",
    "start": "1041750",
    "end": "1049910"
  },
  {
    "text": "aggregated data in this system then you can you know trigger things like success",
    "start": "1049910",
    "end": "1056570"
  },
  {
    "text": "rate drops below hundred percent for service X and we can trigger an alert",
    "start": "1056570",
    "end": "1062030"
  },
  {
    "text": "with our alert manager and be able to kind of if it's not Auto healing then",
    "start": "1062030",
    "end": "1067430"
  },
  {
    "text": "have people look at it and debug it live and that's where these dashboards come",
    "start": "1067430",
    "end": "1074870"
  },
  {
    "text": "into play and you know if you're looking at a sec set overall success rate that's",
    "start": "1074870",
    "end": "1080000"
  },
  {
    "text": "dropping then we can put that next to all these other dashboards where its showing internals and overall",
    "start": "1080000",
    "end": "1087130"
  },
  {
    "text": "health of the infrastructure and be able to live debug anything that's not really",
    "start": "1087130",
    "end": "1093290"
  },
  {
    "text": "visible from a service perspective or service logs and you know things that",
    "start": "1093290",
    "end": "1098450"
  },
  {
    "text": "are outside this dashboards so some examples that we've been able to easily",
    "start": "1098450",
    "end": "1103700"
  },
  {
    "text": "tell you know by migrating one of our services I had was running an old",
    "start": "1103700",
    "end": "1110450"
  },
  {
    "text": "version of an Eddie HTTP client and we're immediately when we migrated to service mesh we were able to tell that",
    "start": "1110450",
    "end": "1116810"
  },
  {
    "text": "you know latency of morning you know Delta of 50-plus millisecond was very",
    "start": "1116810",
    "end": "1124160"
  },
  {
    "text": "visible and we were able to debug that back to the service and be able to fix that immediately without causing any any",
    "start": "1124160",
    "end": "1130880"
  },
  {
    "text": "more latency to the overall infrastructure and the other more interesting problem that we were able to",
    "start": "1130880",
    "end": "1136370"
  },
  {
    "text": "solve with these was name or D was leaking a lot of",
    "start": "1136370",
    "end": "1141620"
  },
  {
    "text": "memory which was causing communities to force restart and then cause a lot of the grid director addiction as the",
    "start": "1141620",
    "end": "1148190"
  },
  {
    "text": "service was running so all these kind of puts us for high availability and and I",
    "start": "1148190",
    "end": "1156470"
  },
  {
    "text": "think this is this was the most important and most I guess challenging part of adopting this infrastructure and",
    "start": "1156470",
    "end": "1164980"
  },
  {
    "text": "so avoiding all the internal hell checking because he can you know since",
    "start": "1164980",
    "end": "1170450"
  },
  {
    "text": "these are all running in kubernetes you can do a lot of lightness probes and a lot of tricks with internals to be able",
    "start": "1170450",
    "end": "1176540"
  },
  {
    "text": "to have kubernetes watch all the pods as they're you know doing their thing and",
    "start": "1176540",
    "end": "1183050"
  },
  {
    "text": "restart them or you know the report back but externally because that's what",
    "start": "1183050",
    "end": "1188210"
  },
  {
    "text": "that's how link or DS are seeing service discovery and named or D we can then ask questions like you know if name or D at",
    "start": "1188210",
    "end": "1196160"
  },
  {
    "text": "all can respond to any discovery is it is it alive at all or if we can take an",
    "start": "1196160",
    "end": "1201800"
  },
  {
    "text": "example for discovering a service food we can ask question through name and the",
    "start": "1201800",
    "end": "1207320"
  },
  {
    "text": "API is that if I were to give you the service name can you discover it for me and so what we did internally we also",
    "start": "1207320",
    "end": "1214880"
  },
  {
    "text": "hooked up sensor because that's the one that's running the custom checks to a service registry that can automatically",
    "start": "1214880",
    "end": "1222470"
  },
  {
    "text": "then as services come alive based on that certain you know either deployment",
    "start": "1222470",
    "end": "1228320"
  },
  {
    "text": "or something that registers the service of the system be able to dynamically either discover say I want you to be",
    "start": "1228320",
    "end": "1236059"
  },
  {
    "text": "able to make sure that these are these such services are discoverable and as these services go offline we want to",
    "start": "1236059",
    "end": "1242120"
  },
  {
    "text": "kind of take him off that list and that all happens using a service registry so",
    "start": "1242120",
    "end": "1247760"
  },
  {
    "text": "the other part of H a is data plan and this becomes more challenging because I",
    "start": "1247760",
    "end": "1254840"
  },
  {
    "text": "mean compared to the Premesh because now we're looking at request only going",
    "start": "1254840",
    "end": "1260240"
  },
  {
    "text": "internally so we can't really rely on external custom checks from sensor so",
    "start": "1260240",
    "end": "1266840"
  },
  {
    "text": "what we did we implemented a probing service where it's idempotent you can you know live",
    "start": "1266840",
    "end": "1272490"
  },
  {
    "text": "anywhere inside outside kubernetes and it's able to handle both restful and g",
    "start": "1272490",
    "end": "1278100"
  },
  {
    "text": "RPC so what it does a you know you send a request to it and say I want you to",
    "start": "1278100",
    "end": "1284610"
  },
  {
    "text": "check the health of the service see for example here and you know you tell about",
    "start": "1284610",
    "end": "1289980"
  },
  {
    "text": "Oracle or data stream you want to use and it will be able to tell you whether that service is healthy or not and that",
    "start": "1289980",
    "end": "1295860"
  },
  {
    "text": "kind of uses the you know in the way we build our micro services they use a",
    "start": "1295860",
    "end": "1302240"
  },
  {
    "text": "framework player for monitoring that gives all of our micro services and this",
    "start": "1302240",
    "end": "1307590"
  },
  {
    "text": "probing service a layer where it can assume that any service that comes alive in the system it has that layer and we",
    "start": "1307590",
    "end": "1315600"
  },
  {
    "text": "can ask it for health check we can say now service X comes alive in this infrastructure can we route to it from",
    "start": "1315600",
    "end": "1323310"
  },
  {
    "text": "probe probing service to wherever it lives in this discovery scope so we're",
    "start": "1323310",
    "end": "1331680"
  },
  {
    "text": "really kind of exposing that same observability internally from instead of",
    "start": "1331680",
    "end": "1338730"
  },
  {
    "text": "doing that externally through sensor",
    "start": "1338730",
    "end": "1342770"
  },
  {
    "text": "and we can use all these the same channels as for learning so we can ask",
    "start": "1350970",
    "end": "1358410"
  },
  {
    "text": "if for example a gr PC health fails for service C we can trigger the right channels for the right people to kind of",
    "start": "1358410",
    "end": "1366420"
  },
  {
    "text": "jump on and take a look at of course that's again if nothing if it's something that doesn't auto heal so what",
    "start": "1366420",
    "end": "1376140"
  },
  {
    "text": "with this what we achieve basically we've been able to refactor a lot of things outside of micro-services and",
    "start": "1376140",
    "end": "1382830"
  },
  {
    "text": "being able to kind of simplify both at a configuration level and life cycling and",
    "start": "1382830",
    "end": "1389970"
  },
  {
    "text": "we're able to get the load balancing user using linker D to be able to kind of tweak all the algorithms for load",
    "start": "1389970",
    "end": "1396510"
  },
  {
    "text": "balancing for example that's a very specific case and be able to use the right algorithm the right places in the",
    "start": "1396510",
    "end": "1402780"
  },
  {
    "text": "infrastructure and since you know the services now have less responsibility a",
    "start": "1402780",
    "end": "1410010"
  },
  {
    "text": "lot of times it's zero code and zero config for the services so as they",
    "start": "1410010",
    "end": "1415110"
  },
  {
    "text": "become live it's easier for them to I guess as as the developers are building",
    "start": "1415110",
    "end": "1421290"
  },
  {
    "text": "these services it it becomes easier for them to kind of roll that out all the way to production without needing to",
    "start": "1421290",
    "end": "1426900"
  },
  {
    "text": "worry about extra pieces that's outside of the product specs so one thing that",
    "start": "1426900",
    "end": "1434340"
  },
  {
    "text": "we're still working on and was super challenging to deal with was distributed",
    "start": "1434340",
    "end": "1439350"
  },
  {
    "text": "tracing using linker D and the reason why that's challenging is because when you start introducing distributed",
    "start": "1439350",
    "end": "1445710"
  },
  {
    "text": "tracing to these proxies then they start",
    "start": "1445710",
    "end": "1450870"
  },
  {
    "text": "becoming bottom legs under pressure so what we had to do we had to do a workaround and be able and use other",
    "start": "1450870",
    "end": "1457500"
  },
  {
    "text": "services like in astana to be able to give us if we're using something more than something close to 100% sampling",
    "start": "1457500",
    "end": "1464880"
  },
  {
    "text": "rate then we can't really use the proxies to do the same work as they're",
    "start": "1464880",
    "end": "1470460"
  },
  {
    "text": "you know under load and do the same distributed tracing at the same time so",
    "start": "1470460",
    "end": "1475920"
  },
  {
    "text": "that's one thing that we're trying to kind of work with the you need to kind of fix so now that we",
    "start": "1475920",
    "end": "1482650"
  },
  {
    "text": "have everything on service mesh how do we you know maintain things while we",
    "start": "1482650",
    "end": "1488730"
  },
  {
    "text": "make sure that there's no downtime and high-availability stays the same to kind",
    "start": "1488730",
    "end": "1498010"
  },
  {
    "text": "of put that in picture it's it's really about you know if you have all these",
    "start": "1498010",
    "end": "1504730"
  },
  {
    "text": "this is a snapshot of you know a couple of weeks ago from our service graph and",
    "start": "1504730",
    "end": "1509820"
  },
  {
    "text": "each dot here is a service is not a pod so each service multiplied by the number",
    "start": "1509820",
    "end": "1515830"
  },
  {
    "text": "of pods they're running in the environment and you want to make sure as all these little dots go in between these bigger dots then we're not really",
    "start": "1515830",
    "end": "1524230"
  },
  {
    "text": "affecting any live traffic as we're migrating or upgrading things to newer versions or new configuration so what we",
    "start": "1524230",
    "end": "1534580"
  },
  {
    "text": "arrived at was our upgrade story also becomes the same thing as a chase story",
    "start": "1534580",
    "end": "1540160"
  },
  {
    "text": "we split it into the control plane and data plane so control plane and name or",
    "start": "1540160",
    "end": "1546040"
  },
  {
    "text": "D so we built named our D into its own you know horizontally scalable",
    "start": "1546040",
    "end": "1552300"
  },
  {
    "text": "kubernetes service where we're able to kind of give enough information",
    "start": "1552300",
    "end": "1558900"
  },
  {
    "text": "configuration to each of these parts to be able to discover all the things in",
    "start": "1558900",
    "end": "1564640"
  },
  {
    "text": "the discovery scope on their own so and these pods being behind the load",
    "start": "1564640",
    "end": "1570160"
  },
  {
    "text": "balancer then we can kind of play with all these different release strategies for name of D so whether it's blue green",
    "start": "1570160",
    "end": "1576370"
  },
  {
    "text": "that works or canary you want to test a new you know version that's you're not",
    "start": "1576370",
    "end": "1581770"
  },
  {
    "text": "sure your backward compatible or not then you can do canary and get it tested through that traffic that's going",
    "start": "1581770",
    "end": "1588040"
  },
  {
    "text": "through the load balancer so as far as",
    "start": "1588040",
    "end": "1593320"
  },
  {
    "text": "data plane it's the upgrades strategy becomes more interesting when you start using the",
    "start": "1593320",
    "end": "1599220"
  },
  {
    "text": "diamond set pattern and it's easier for the sidecar because really the lifecycle is tied to how the micro services are",
    "start": "1599220",
    "end": "1606630"
  },
  {
    "text": "deployed and like what what are their lifecycle because each part will get restarted on their own schedule based on",
    "start": "1606630",
    "end": "1614280"
  },
  {
    "text": "deployment or health check and whatnot but in this scenario if we take an",
    "start": "1614280",
    "end": "1619980"
  },
  {
    "text": "example and use this as the with the rolling upgrade pattern node a is the",
    "start": "1619980",
    "end": "1628140"
  },
  {
    "text": "first node that's triggered to be upgraded so when that's finished then node C is being upgraded",
    "start": "1628140",
    "end": "1635390"
  },
  {
    "text": "so one load C is being upgraded and service one is trying to send a request",
    "start": "1635390",
    "end": "1641070"
  },
  {
    "text": "to service two then that request will either fail or a wool and depending on",
    "start": "1641070",
    "end": "1648510"
  },
  {
    "text": "the race condition will either fail or it that node B proxy already knows that",
    "start": "1648510",
    "end": "1654180"
  },
  {
    "text": "note C is not available at the moment and it will send a request to node a and really that's that's a responsibility of",
    "start": "1654180",
    "end": "1661650"
  },
  {
    "text": "both the this rolling upgrade configuration and the services and the",
    "start": "1661650",
    "end": "1668370"
  },
  {
    "text": "reason is that because you need to make sure that if if the service needs to",
    "start": "1668370",
    "end": "1674010"
  },
  {
    "text": "retry things that are not reliable at the privacy level then you need to be able to handle that at the service level",
    "start": "1674010",
    "end": "1679590"
  },
  {
    "text": "but they're things that their methods or API is that he can retry at that proxy level and you can configure that in",
    "start": "1679590",
    "end": "1685230"
  },
  {
    "text": "linker D and that would just give that capability to all services with just one",
    "start": "1685230",
    "end": "1691440"
  },
  {
    "text": "configuration so I'm proud to say with this strategy that I described here we",
    "start": "1691440",
    "end": "1697800"
  },
  {
    "text": "were able to kind of migrate everything last year in production with zero downtime and it was huge for us so that",
    "start": "1697800",
    "end": "1707160"
  },
  {
    "text": "was it thanks for listening and I kind of scan through a lot of things and this",
    "start": "1707160",
    "end": "1712260"
  },
  {
    "text": "is on top is our engineering blog and I highly recommend you go there and kind",
    "start": "1712260",
    "end": "1717630"
  },
  {
    "text": "of take a look at all the details of the stuff I talked about it thank you",
    "start": "1717630",
    "end": "1722780"
  },
  {
    "text": "[Applause]",
    "start": "1722780",
    "end": "1730260"
  },
  {
    "text": "all right are there any questions",
    "start": "1730260",
    "end": "1734670"
  },
  {
    "text": "I just one question did you evaluate T steel did you ever wait e steel sorry I",
    "start": "1741320",
    "end": "1749690"
  },
  {
    "text": "can't did you evaluate steel a steel we did a",
    "start": "1749690",
    "end": "1754730"
  },
  {
    "text": "lot of kind of comparison between the two and at the time we were doing that",
    "start": "1754730",
    "end": "1760520"
  },
  {
    "text": "migration to service smash the future parody was way behind linker D so based",
    "start": "1760520",
    "end": "1766790"
  },
  {
    "text": "on what we needed we kind of decided to go with linker D at the time thank you",
    "start": "1766790",
    "end": "1772510"
  },
  {
    "text": "great talk thanks so much lot to think about there I'll kind of curious maybe it's a conversation for offline but",
    "start": "1777790",
    "end": "1783320"
  },
  {
    "text": "what's your thought with the whole link D 1 & 2 split now that you've rolled out one do you see yourself migrating to",
    "start": "1783320",
    "end": "1789320"
  },
  {
    "text": "linker db2 or not right so that's one of the other things were working with the community and really with the Boyan team",
    "start": "1789320",
    "end": "1796370"
  },
  {
    "text": "is trying to bring that non kubernetes side of the stories to link it to -",
    "start": "1796370",
    "end": "1802180"
  },
  {
    "text": "they're really great at getting all the features of one to two but since of",
    "start": "1802180",
    "end": "1808010"
  },
  {
    "text": "focus it's been laser focused on kubernetes we're trying to kind of get everything first and then so we're not",
    "start": "1808010",
    "end": "1813650"
  },
  {
    "text": "really losing functionality we're actually at least we're feature parity when we go through this link ready -",
    "start": "1813650",
    "end": "1820000"
  },
  {
    "text": "thank you any other questions",
    "start": "1820000",
    "end": "1828100"
  },
  {
    "text": "I nice talk Thanks how do you manage changes to your service mash because",
    "start": "1829050",
    "end": "1835950"
  },
  {
    "text": "it's a very critical central component and we had the cases where like version",
    "start": "1835950",
    "end": "1843000"
  },
  {
    "text": "change would introduce issues across the whole cluster it wouldn't use we're",
    "start": "1843000",
    "end": "1850260"
  },
  {
    "text": "using envoi but I guess the problem is probably similar mm-hmm so I guess it",
    "start": "1850260",
    "end": "1858300"
  },
  {
    "text": "goes through about like how do you handle backward incompatible changes is that is that a question more like it can",
    "start": "1858300",
    "end": "1866460"
  },
  {
    "text": "be a single coil point of failure I would say right and that's specific to the practices I'm assuming you're",
    "start": "1866460",
    "end": "1872850"
  },
  {
    "text": "wondering I mean there's a lot of testing that goes into migrations to or",
    "start": "1872850",
    "end": "1879510"
  },
  {
    "text": "upgrades to new versions that really depends on what we see in our testing environment and in our staging",
    "start": "1879510",
    "end": "1885720"
  },
  {
    "text": "environment and I guess through all that testing and making sure that all these environments are in sync with each other",
    "start": "1885720",
    "end": "1892350"
  },
  {
    "text": "we are able to tell all the problems in advance before we go to production do",
    "start": "1892350",
    "end": "1897420"
  },
  {
    "text": "you mirror traffic maybe to use staging environments because that is what we found what was the problem because we",
    "start": "1897420",
    "end": "1904230"
  },
  {
    "text": "couldn't simulate similar laws necessarily easily our testing",
    "start": "1904230",
    "end": "1913680"
  },
  {
    "text": "environments are much smaller and simpler than production environments in terms of sheer traffic and patterns yes",
    "start": "1913680",
    "end": "1923160"
  },
  {
    "text": "so we have a lot of data that goes in our testing environment that's copied",
    "start": "1923160",
    "end": "1928320"
  },
  {
    "text": "from production that we can kind of simulate what's happening in production in fact we found a lot of the like",
    "start": "1928320",
    "end": "1935940"
  },
  {
    "text": "problems with load testing in our testing environment then before we actually even go into production for",
    "start": "1935940",
    "end": "1941750"
  },
  {
    "text": "migration to all the services to it nice yeah sorry",
    "start": "1941750",
    "end": "1952820"
  },
  {
    "text": "you mentioned you develop the probe service that you attach to each pod I'm",
    "start": "1953320",
    "end": "1958420"
  },
  {
    "text": "curious have you considered console and well that does my first question console",
    "start": "1958420",
    "end": "1966100"
  },
  {
    "text": "for discovery well because console does the health checking that you've mentioned right",
    "start": "1966100",
    "end": "1972160"
  },
  {
    "text": "what the probe service does and it right actually serve the whole discovery process for you I wonder why you haven't",
    "start": "1972160",
    "end": "1979120"
  },
  {
    "text": "chose that said he developed your own right we did not use console for the",
    "start": "1979120",
    "end": "1987970"
  },
  {
    "text": "containerize applications but we've used it for the VM based applications which is more of an infrastructure services",
    "start": "1987970",
    "end": "1995040"
  },
  {
    "text": "the reason for that was because we needed to add that layer of you know agents I'm not probably the best person",
    "start": "1995040",
    "end": "2001980"
  },
  {
    "text": "to talk about this but it was really about the setup and like how much overhead it was for the micro services",
    "start": "2001980",
    "end": "2008549"
  },
  {
    "text": "in the containers to be able to set up that same health checking mechanism as the non kubernetes services",
    "start": "2008549",
    "end": "2016460"
  },
  {
    "text": "very nice stuck so as you asked the beginning there's a lot of people who are curious of our link ad our service",
    "start": "2025419",
    "end": "2031269"
  },
  {
    "text": "meshes in general but having not used them in production so from a company has",
    "start": "2031269",
    "end": "2036579"
  },
  {
    "text": "being using them in production apart from all the features which occurs are very nice what is the operational overhead the difficulty of introducing a",
    "start": "2036579",
    "end": "2043869"
  },
  {
    "text": "have you had any problems have you found yourself using liquidy so far the early",
    "start": "2043869",
    "end": "2049898"
  },
  {
    "text": "problems like I mean again I guess the",
    "start": "2049899",
    "end": "2057158"
  },
  {
    "text": "reason why it took a long time for us to kind of migrate all for micro services was really the upfront load testing and",
    "start": "2057159",
    "end": "2064089"
  },
  {
    "text": "you know figuring out exactly what could go wrong based on and that's how really why we iterated so much on the hae story",
    "start": "2064089",
    "end": "2072700"
  },
  {
    "text": "for the whole infrastructure and it was I guess he came down for us to like",
    "start": "2072700",
    "end": "2078970"
  },
  {
    "text": "whether these extra hops will add latency to the overall request of the service and and it didn't I mean even if",
    "start": "2078970",
    "end": "2086349"
  },
  {
    "text": "it's like up to five millisecond of you know latency you're getting so much out",
    "start": "2086349",
    "end": "2091779"
  },
  {
    "text": "of it that overall by adding gr PC and everything that's kind of speeding up",
    "start": "2091779",
    "end": "2097148"
  },
  {
    "text": "the whole communication they are you gaining something as opposed to losing",
    "start": "2097149",
    "end": "2102759"
  },
  {
    "text": "the kind of worrying about that five millisecond",
    "start": "2102759",
    "end": "2107880"
  },
  {
    "text": "are we out of time I guess take one more question you mentioned that you had a",
    "start": "2112400",
    "end": "2119730"
  },
  {
    "text": "problem with load on your proxies when you were doing distributed tracing mm-hm is there something you expect to be",
    "start": "2119730",
    "end": "2125730"
  },
  {
    "text": "different when you move to linker d2 so we're actually having a talk yesterday",
    "start": "2125730",
    "end": "2131280"
  },
  {
    "text": "in a workshop and apparently for distributed tracing it's not going to be",
    "start": "2131280",
    "end": "2136890"
  },
  {
    "text": "a problem that's gonna be solved using the practice because when you put these prices under load and they have just you",
    "start": "2136890",
    "end": "2144360"
  },
  {
    "text": "know so much resources left to dedicate to distribute tracing and and since they're their stands and you know you",
    "start": "2144360",
    "end": "2150270"
  },
  {
    "text": "need to do 100% sampling to be able to get the whole picture of that span then",
    "start": "2150270",
    "end": "2155670"
  },
  {
    "text": "you can't really rely on all these to be kind of shipping all the tracing",
    "start": "2155670",
    "end": "2161340"
  },
  {
    "text": "information at the same time on their heavy load so you can do lowered and you",
    "start": "2161340",
    "end": "2167250"
  },
  {
    "text": "know 5% sampling rate but that's really like debugging something live which is",
    "start": "2167250",
    "end": "2173160"
  },
  {
    "text": "you know linker d2 gives it a with tap and all the other great new features for",
    "start": "2173160",
    "end": "2179250"
  },
  {
    "text": "debugging all right that's pretty much it thanks everyone for listening it's",
    "start": "2179250",
    "end": "2185400"
  },
  {
    "text": "good to be here [Applause]",
    "start": "2185400",
    "end": "2190589"
  }
]