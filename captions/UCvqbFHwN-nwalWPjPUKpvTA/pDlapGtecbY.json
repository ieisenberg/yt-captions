[
  {
    "text": "okay so we should get started so let's go so I'm Clive I'm CEO of Selden we're",
    "start": "30",
    "end": "7020"
  },
  {
    "text": "based in London Berkeley is a wise fin tech hub and just a shout out we also run tensorflow in London beat up so any",
    "start": "7020",
    "end": "13860"
  },
  {
    "text": "of you guys here in London and want to come to learn more about tensorflow we have that each month have a couple of",
    "start": "13860",
    "end": "18930"
  },
  {
    "text": "talks both quite advanced in quite beginner so come along as company we do two things we have a product which is",
    "start": "18930",
    "end": "25439"
  },
  {
    "text": "based on machine learning deployment on kubernetes which i'm only going to be talking about today and we also do a",
    "start": "25439",
    "end": "31380"
  },
  {
    "text": "mild consultancy especially in the FinTech area would work with banks on doing predictions in various areas so",
    "start": "31380",
    "end": "38820"
  },
  {
    "text": "for today's talk what I'm going to talk about is we should landing on kubernetes why that's good hopefully as you've seen",
    "start": "38820",
    "end": "44460"
  },
  {
    "text": "many queue flow talks you should probably believe that by now then focus on machine learning deployments are just",
    "start": "44460",
    "end": "49860"
  },
  {
    "text": "the serving an inference layer and talk about what we're doing at sell and call and talk about how we integrate with cue flow and then I have a example of an",
    "start": "49860",
    "end": "57719"
  },
  {
    "text": "end-to-end machine learning example which if there's enough time I'll give a demo of we'll see what we get soon so one thing we're trying to Excel then",
    "start": "57719",
    "end": "64320"
  },
  {
    "text": "is really try to get these three types of people to work together well on a",
    "start": "64320",
    "end": "69479"
  },
  {
    "text": "machine learning project so you got the data scientist obvious use doing the core algorithms and data analysis but",
    "start": "69479",
    "end": "76950"
  },
  {
    "text": "then to get it into production they gotta work with like a DevOps engineer and these people sometimes don't have a good understanding of machine learning",
    "start": "76950",
    "end": "82860"
  },
  {
    "text": "and don't really know how that's going to work and also the data scientist doesn't want to really get too involved with maybe the DevOps side so how do you",
    "start": "82860",
    "end": "89159"
  },
  {
    "text": "get those two sides to work together and once you have the business manager who's defining the project it gets the RI and",
    "start": "89159",
    "end": "95549"
  },
  {
    "text": "wants to make sure that it's going to work well and these three people have to work together well for a machine",
    "start": "95549",
    "end": "101159"
  },
  {
    "text": "learning project to actually be successful and really that's the sort of goal of our companies to try and make these people work together with various",
    "start": "101159",
    "end": "108299"
  },
  {
    "text": "certain systems so machine learning on kubernetes and probably don't have to explain too much about that or why it's",
    "start": "108299",
    "end": "113490"
  },
  {
    "text": "a good thing but obviously you've got the GPU a sauce aspect so you can easily push your jobs to the proprietary that's",
    "start": "113490",
    "end": "120930"
  },
  {
    "text": "that's needed obviously the fact that you can move between cloud and premise is grateful especially our customers in",
    "start": "120930",
    "end": "127110"
  },
  {
    "text": "banks who who's who might do some proof of concepts on cloud but they probably want to do on-premise machine learning",
    "start": "127110",
    "end": "132300"
  },
  {
    "text": "adventure for various reasons then you get all the stuff up but offer up there in the top left which you guys probably know about",
    "start": "132300",
    "end": "138730"
  },
  {
    "text": "which is good for any software product was a project but it's also good for machine land it's all the are back you know making sure that people working on",
    "start": "138730",
    "end": "145659"
  },
  {
    "text": "machine lighting projects just have the quotas and resources they need they could scale up what they're doing health",
    "start": "145659",
    "end": "151480"
  },
  {
    "text": "checks what they're doing and all be separated out on a communities cluster",
    "start": "151480",
    "end": "157019"
  },
  {
    "text": "says many cute there's many projects out there that already machine many bodies running on top of kubernetes park and",
    "start": "157019",
    "end": "163360"
  },
  {
    "text": "Jupiter amongst many others and we're part of that ecosystem that's growing with machine learning on top of kubernetes as coop flow as well so now",
    "start": "163360",
    "end": "171940"
  },
  {
    "text": "I'm going to talk about machine learning deployment and southern court in particular so these are some of the",
    "start": "171940",
    "end": "176980"
  },
  {
    "text": "challenges or goals that were trying to solve first the core deployment aspect so a lot of this has really helped by",
    "start": "176980",
    "end": "183480"
  },
  {
    "text": "walking on top of kubernetes a lot of these things here actually solved by curb measures itself in terms of",
    "start": "183480",
    "end": "189340"
  },
  {
    "text": "launching a scaling up but also as many ways you want to update your running machine learning project in terms of",
    "start": "189340",
    "end": "196180"
  },
  {
    "text": "various ways you might want to update it in real time not just doing rolling updates and I'll discuss some of those and that's part of what we're trying to",
    "start": "196180",
    "end": "203290"
  },
  {
    "text": "do in our project as well and obviously healthchoice in recovery we want to do everything as a ssin so obviously",
    "start": "203290",
    "end": "209590"
  },
  {
    "text": "infrastructure optimizations that you're saving money on machine lane because me showing machine learning prediction covers you the infant site could be very",
    "start": "209590",
    "end": "215739"
  },
  {
    "text": "resource intensive and so you if you want to make a profit on on your machine early applications out there you need to",
    "start": "215739",
    "end": "221200"
  },
  {
    "text": "get that right I'm obviously latency especially in the FinTech world and various areas where as low latency is",
    "start": "221200",
    "end": "226569"
  },
  {
    "text": "required that's needs to be tried and optimized as much as possible and also the other side free put some projects you want to",
    "start": "226569",
    "end": "233019"
  },
  {
    "text": "get maximum throughput in terms of the predictions going for your system and so that's key and then every form of actual",
    "start": "233019",
    "end": "239200"
  },
  {
    "text": "model optimization as well so if you might have multiple models trying to work out which model is the best trying",
    "start": "239200",
    "end": "244720"
  },
  {
    "text": "to change them in real time in terms of pushing out traffic to the correct model and I go through some examples there and",
    "start": "244720",
    "end": "252370"
  },
  {
    "text": "then finally obviously you deploy your machine learning inference stack but you need to then tie it into the business",
    "start": "252370",
    "end": "257919"
  },
  {
    "text": "who's going to be using those predictions to do things so it's part of seldom we automatically expose rest and",
    "start": "257919",
    "end": "263409"
  },
  {
    "text": "gr PC in terms of what we're doing and in future we're going to look at more i'm a synchronous methods which is",
    "start": "263409",
    "end": "269600"
  },
  {
    "text": "a some of our customers need was that still early stages and we're not sure how that's going to go and it's probably interesting interactions with cue flow",
    "start": "269600",
    "end": "276590"
  },
  {
    "text": "there in terms of how that will work and then the whole sort of management side is key as well so all the aspects of all",
    "start": "276590",
    "end": "282350"
  },
  {
    "text": "the thing and versioning so how do you know that what's changed and from the previous model update maybe that needs",
    "start": "282350",
    "end": "288680"
  },
  {
    "text": "approval before you actually put it out into production what versions of the data provenance has great talks about",
    "start": "288680",
    "end": "294740"
  },
  {
    "text": "packing them tomorrow yesterday and i think they fit in key for that area and",
    "start": "294740",
    "end": "299870"
  },
  {
    "text": "we definitely want to use that and it's great well with them all forms of monitoring is my machine learning working well in terms of the accuracy",
    "start": "299870",
    "end": "305870"
  },
  {
    "text": "etc and see ICD and things are tired to get hops and i've explained about how that is viewed in terms of machine",
    "start": "305870",
    "end": "312770"
  },
  {
    "text": "learning deployment to do those aspects then two things i'm going to discuss him",
    "start": "312770",
    "end": "318800"
  },
  {
    "text": "slightly more detail one thing we definitely wanna do is allow data scientists to use any of the tools that",
    "start": "318800",
    "end": "323930"
  },
  {
    "text": "they're using now and not be restricted so we just want to focus on deployment the serving in front side and they can",
    "start": "323930",
    "end": "330770"
  },
  {
    "text": "use python to do tends flow sky can learn they might want to do their models in our spark h2o it's anything that",
    "start": "330770",
    "end": "337250"
  },
  {
    "text": "using that all commercial toolkit so just the training that can continue as they were doing now and we want to handle the serving side and then we want",
    "start": "337250",
    "end": "343220"
  },
  {
    "text": "to handle the actual service mesh of the one-time prediction aspect so not just",
    "start": "343220",
    "end": "348349"
  },
  {
    "text": "the model but also routing requests so doing a/b tests multi-armed bandit if you don't know that i'll explain about",
    "start": "348349",
    "end": "353720"
  },
  {
    "text": "that later and give an example transformations so feature normalization you might want to put that as a separate",
    "start": "353720",
    "end": "359750"
  },
  {
    "text": "docker image and dokdo Kermode you'll to do that maybe also you want to wash your",
    "start": "359750",
    "end": "365090"
  },
  {
    "text": "ensemble results from multiple models to come back together to get a single result and metrics so the things that",
    "start": "365090",
    "end": "370820"
  },
  {
    "text": "you want to do things like concept drift so is my model accuracy decreasing in time so maybe it's because there's",
    "start": "370820",
    "end": "376669"
  },
  {
    "text": "different data coming in in real time which was not the same as they try trained it on so maybe i need to retrain",
    "start": "376669",
    "end": "382160"
  },
  {
    "text": "my model and guess again get alerts there or stuff like out lie detection you know other strange data requests",
    "start": "382160",
    "end": "387949"
  },
  {
    "text": "coming in can i kind of have these flack your mobile to add modules into your one-time wofully that should provide these pieces of functionality and every",
    "start": "387949",
    "end": "395120"
  },
  {
    "text": "aspect of security obviously is key so this is a slide from cue flow just",
    "start": "395120",
    "end": "402509"
  },
  {
    "text": "this really make it clear where we're situated so I see the data science steps as you've seen puppy seed there's many",
    "start": "402509",
    "end": "408900"
  },
  {
    "text": "steps in this flow that you need to go through in a data science project and probably all of these have a feedback loops to previous steps as you iterate",
    "start": "408900",
    "end": "415889"
  },
  {
    "text": "around in terms of what we're doing we're really just doing the rollout serving monitoring of a machine lender",
    "start": "415889",
    "end": "422610"
  },
  {
    "text": "deployments and cue flow is obviously a whole ecosystem providing a set of easy packages you can choose to do all of",
    "start": "422610",
    "end": "428580"
  },
  {
    "text": "these steps and you can choose Selden Corps to do the serving you can choose intensive or serving to do that as well",
    "start": "428580",
    "end": "434400"
  },
  {
    "text": "as the two options in queue flow at the moment so it's all about applicability freedom to choose what you need to solve",
    "start": "434400",
    "end": "440160"
  },
  {
    "text": "your task as a data scientist so what you need to do to get running with Selden free steps so we have helm charts",
    "start": "440160",
    "end": "447229"
  },
  {
    "text": "to deploy once you've got your kubernetes cluster we also have K sonnet registries we are well in case you",
    "start": "447229",
    "end": "453630"
  },
  {
    "text": "aren't reading also one is part we also have a package as part of queue flow so you can easily install it as part of the",
    "start": "453630",
    "end": "459270"
  },
  {
    "text": "queue flow registry secondly you will have your own models your own one x and",
    "start": "459270",
    "end": "464550"
  },
  {
    "text": "you need to then package shows up as images so we can handle that you're not so micro service graph to actually",
    "start": "464550",
    "end": "470490"
  },
  {
    "text": "manage those at one time so we provide different methods I'm going over one method we provide to make that easy for",
    "start": "470490",
    "end": "475889"
  },
  {
    "text": "developers so that that's using openshift source 2 image to take a source and turn it into an image and",
    "start": "475889",
    "end": "480930"
  },
  {
    "text": "then finally we have a custom resource that defines your graph that you want to have at one time and then obvious you can just use cube control to the",
    "start": "480930",
    "end": "486690"
  },
  {
    "text": "actually deploy out to the graph and so deploy that to kubernetes so this is the",
    "start": "486690",
    "end": "492210"
  },
  {
    "text": "architecture of Selden Corps so you can discuss on the right you you push your custom resource into",
    "start": "492210",
    "end": "498720"
  },
  {
    "text": "the cluster we have operator that's running in the cluster the spots these things coming in and then crates the underlying kubernetes resources",
    "start": "498720",
    "end": "504539"
  },
  {
    "text": "deployment graphs deployment services pods etc to actually manage that graph and handle changes to",
    "start": "504539",
    "end": "510720"
  },
  {
    "text": "it as you do updates and as part of that we also have a surface all service Orchestrator which is handling the",
    "start": "510720",
    "end": "515729"
  },
  {
    "text": "request and response flow through the graph using great projects unlike Ambassador to do which is like a reverse",
    "start": "515729",
    "end": "523828"
  },
  {
    "text": "proxy to allow you to expose your geo PC in West End points outside the",
    "start": "523829",
    "end": "528839"
  },
  {
    "text": "kubernetes to tie in and that's great for us because we already want to get kristan things like that we'll just",
    "start": "528839",
    "end": "535150"
  },
  {
    "text": "purely focus on the machine learning parts and so that projects like a bastard great because you've got a",
    "start": "535150",
    "end": "540580"
  },
  {
    "text": "pluggable authentication so you can do oh or for whatever authentication you need to plug in there so it's pretty",
    "start": "540580",
    "end": "547240"
  },
  {
    "text": "simple things pretty easy how it works and to set up and get going so as I said",
    "start": "547240",
    "end": "553600"
  },
  {
    "text": "we want to have reasonably complex one-time graphs so not just model so obviously models is the thing you all",
    "start": "553600",
    "end": "558940"
  },
  {
    "text": "expect in terms of machine learning at one time secret your tensor flow model etc and we want to allow that we will",
    "start": "558940",
    "end": "564130"
  },
  {
    "text": "also want to allow Reuters which we call Z's these are the false or abstract types that we allow so Reuters basically",
    "start": "564130",
    "end": "569590"
  },
  {
    "text": "take a request and decide where it's going to go further down the graph so that would cover things like a be testing or multi-armed bandit swear",
    "start": "569590",
    "end": "575320"
  },
  {
    "text": "you're looking at which model is is working best and pushing resources to the best performing model we also have",
    "start": "575320",
    "end": "580570"
  },
  {
    "text": "combiners which take the responses coming back up from the models and join",
    "start": "580570",
    "end": "585760"
  },
  {
    "text": "them together so you can use that to obviously ensemble different models into different responses from models into a",
    "start": "585760",
    "end": "591640"
  },
  {
    "text": "single result and transformers which is sort of generic component where you might with to use that to feature that",
    "start": "591640",
    "end": "597940"
  },
  {
    "text": "feature normalization so it's taking the request and changing it into something else it will also contain the response and change it to something else as goes",
    "start": "597940",
    "end": "604120"
  },
  {
    "text": "as it goes back out and that's where you would handle things like concept drift or out lie detection so these modules",
    "start": "604120",
    "end": "609130"
  },
  {
    "text": "might add a little extra data actually some metadata to your response as it goes back up through the graph so to",
    "start": "609130",
    "end": "615070"
  },
  {
    "text": "clarify that I'm just going to show you some examples of how that might happen so you could be two parts is like client",
    "start": "615070",
    "end": "621640"
  },
  {
    "text": "components which is the which is the UM components that you build and then there's third-party components which might be a stuff you take from Seldon",
    "start": "621640",
    "end": "627220"
  },
  {
    "text": "core or third parties provide that so the simple thing okay you've got a model we can manage that obviously and it's",
    "start": "627220",
    "end": "632920"
  },
  {
    "text": "connected your API GRP cos you send a request gets back response but maybe your data scientist creates a new model",
    "start": "632920",
    "end": "640150"
  },
  {
    "text": "that they want to test out so it won't have an a/b test you want to do a rolling update of that model and to do",
    "start": "640150",
    "end": "645220"
  },
  {
    "text": "a/b test to test out the accuracy and success of each each model but then",
    "start": "645220",
    "end": "650680"
  },
  {
    "text": "maybe you want to do something more complex maybe we'll have four models and you want to run a multi-armed bandit and so you have to take the requests as they",
    "start": "650680",
    "end": "657400"
  },
  {
    "text": "come in and decide which of those models is performing best and push push all your requests to that model while at the",
    "start": "657400",
    "end": "662830"
  },
  {
    "text": "same time maybe monitoring every model so doing more complex things in that area maybe you want to separate",
    "start": "662830",
    "end": "668050"
  },
  {
    "text": "out the feature normalization just because all those models are using the same set of features you want to separate that into out into a separate",
    "start": "668050",
    "end": "673959"
  },
  {
    "text": "image and handle that as part of a graph so you can add that in maybe you wanted an add in out lie detection to add that",
    "start": "673959",
    "end": "680740"
  },
  {
    "text": "as part of your one-time graph so you can get those extra pieces of information and maybe want to do more complex things like I should explain",
    "start": "680740",
    "end": "685899"
  },
  {
    "text": "your results so this might be an area that we've worked on quite a lot where",
    "start": "685899",
    "end": "690940"
  },
  {
    "text": "you have maybe deep learning models that though it's hard to understand what are the features that they use to give their",
    "start": "690940",
    "end": "696519"
  },
  {
    "text": "response but I'm not be very good at have a higher accuracy but it's hard to explain those to stakeholders such human",
    "start": "696519",
    "end": "704920"
  },
  {
    "text": "beings or why that prediction was made for a particular input data so you can use various techniques like this there's",
    "start": "704920",
    "end": "710829"
  },
  {
    "text": "one called lime and various other techniques out there that you can use to try and give more human readable",
    "start": "710829",
    "end": "716410"
  },
  {
    "text": "understandable results I'm human understandable explanations as to why your models running so you might want to tie that in normal actually these things",
    "start": "716410",
    "end": "722860"
  },
  {
    "text": "are quite so resource intensive so you probably wouldn't put that in production and probably have a site cluster running",
    "start": "722860",
    "end": "728139"
  },
  {
    "text": "to where you could say okay for this particular request why did it give that response so this is the sort of thing and you want to sort of iterate between",
    "start": "728139",
    "end": "734410"
  },
  {
    "text": "these inbuilt time to update them and with no downtime so I know how readable",
    "start": "734410",
    "end": "741550"
  },
  {
    "text": "that is but I'm going to show the core blocks of our custom resource of how you define the graph so the first thing is",
    "start": "741550",
    "end": "747010"
  },
  {
    "text": "you give the graph definition so that would be what I showed you on the previous slide where you define how your",
    "start": "747010",
    "end": "752860"
  },
  {
    "text": "components fit together and your graph it's very simple it's just a single model but obviously more complex in",
    "start": "752860",
    "end": "758980"
  },
  {
    "text": "terms of the other things I showed there then you have a pod temple expert which is really just simply a kubernetes",
    "start": "758980",
    "end": "764170"
  },
  {
    "text": "report template expects you can put everything in there that you have in a normal combat communities port template spec so the image and all the stuff you",
    "start": "764170",
    "end": "770800"
  },
  {
    "text": "might want to connect to it like the volumes so resource requests like GPUs etc so that's just standard kubernetes",
    "start": "770800",
    "end": "777370"
  },
  {
    "text": "port templates but then you'd obviously set the number of replicas how many copies of this go after you want to run",
    "start": "777370",
    "end": "782860"
  },
  {
    "text": "and finally the fact that you can also allow a list of these which we call",
    "start": "782860",
    "end": "788050"
  },
  {
    "text": "predictors that those parts down below and so you cannot just have one but multiple sets of those running in terms",
    "start": "788050",
    "end": "794079"
  },
  {
    "text": "of your graph and I'll explain why in the next slide or things this is where it ties into how",
    "start": "794079",
    "end": "799760"
  },
  {
    "text": "you might want to do updates to keep the complete resource isolation between u1",
    "start": "799760",
    "end": "805130"
  },
  {
    "text": "and graphs one is obviously Canaries so here we assume you have a model one one",
    "start": "805130",
    "end": "810290"
  },
  {
    "text": "model running with ten replicas and we've deployed it and crated these components underneath but you want to",
    "start": "810290",
    "end": "815720"
  },
  {
    "text": "test a new model and check out whether it's going to blow up or it was going to be okay so you just have one replica of",
    "start": "815720",
    "end": "821779"
  },
  {
    "text": "that may be running handling production traffic until you're happy and then you do it a rolling update so we can handle",
    "start": "821779",
    "end": "827900"
  },
  {
    "text": "that and that's where you'd have these two grass grass completely isolated so it means you discontinue a connected to",
    "start": "827900",
    "end": "835400"
  },
  {
    "text": "the existing production graph and then maybe when you're happy you do a rolling update Bluegreen deployments that's not",
    "start": "835400",
    "end": "841100"
  },
  {
    "text": "part of the current open source but we plan to handle that so obviously you you have a say ten replicas running of a single model and then you want to",
    "start": "841100",
    "end": "847730"
  },
  {
    "text": "completely set up a separate model of ten replicas once that's all set up because sometimes machine learning a",
    "start": "847730",
    "end": "853360"
  },
  {
    "text": "predictors take a long time to load all the parameters etc once that setup you can then transfer traffic from the first",
    "start": "853360",
    "end": "860480"
  },
  {
    "text": "set to the second set and we also have customers who asking for more complex deployment aspects which is like I'm",
    "start": "860480",
    "end": "867320"
  },
  {
    "text": "shadow deployments where instead you just split the traffic so you might have two separate models and you just want to send all the requests that your main",
    "start": "867320",
    "end": "873980"
  },
  {
    "text": "production models are seeing and send them out to the other model but not care about the actual responses just throw",
    "start": "873980",
    "end": "879050"
  },
  {
    "text": "them away and some people like that just so they can see their model running with the same exact real production lower",
    "start": "879050",
    "end": "884839"
  },
  {
    "text": "that's happening now in the system and just to see if it's running okay before they decide it's whichever well they can",
    "start": "884839",
    "end": "890150"
  },
  {
    "text": "also use it for just understanding how the model is behaving so maybe run it for one day as a shell deployment and",
    "start": "890150",
    "end": "895220"
  },
  {
    "text": "then investigate what predictions they gave and how it was behaving during that day so that's one part that's the graph",
    "start": "895220",
    "end": "901490"
  },
  {
    "text": "that we deploy now the other part is how do you make it easy for developers to take their one time and images and",
    "start": "901490",
    "end": "906980"
  },
  {
    "text": "one-time source code for the machine learning code and put it into an image so they can do it these are the two",
    "start": "906980",
    "end": "913760"
  },
  {
    "text": "steps they have to do one is there to dock or is it to put it into a container and second they have to expose the API",
    "start": "913760",
    "end": "920060"
  },
  {
    "text": "so it follows our less than gr PCM API s-- so they can do that themselves it's not really not that difficult but we",
    "start": "920060",
    "end": "926450"
  },
  {
    "text": "want to make it really easy for them to do that so that they can integrate very easily what they're doing and",
    "start": "926450",
    "end": "931760"
  },
  {
    "text": "we're using open chef source to image as one way of doing that and we plan to have other ways of doing this as well and I'll show you how that works now so",
    "start": "931760",
    "end": "940100"
  },
  {
    "text": "for those of you don't know OpenShift source to image basically allows you to take source code and turn it into a",
    "start": "940100",
    "end": "945140"
  },
  {
    "text": "docker container a docker image basically so you have your source code that has some source code on the right for a and it's not always always",
    "start": "945140",
    "end": "953540"
  },
  {
    "text": "classifiers you've got a predicted method it's loading its model parameters from someplace on disk and you want to",
    "start": "953540",
    "end": "959060"
  },
  {
    "text": "turn that into image basically you choose one of our builder images this is a source to image term so we've got to Python once we go to our one and a Java",
    "start": "959060",
    "end": "965660"
  },
  {
    "text": "one it's you obviously choose one of the Python ones in this case and that includes all the actual dependencies needed to to actually put it into and",
    "start": "965660",
    "end": "972800"
  },
  {
    "text": "allow it to be one herbal by Selden and then we define a set of source to image",
    "start": "972800",
    "end": "978020"
  },
  {
    "text": "scripts symbol one and usage that basically this symbol tells you how to how we can combine your source code with",
    "start": "978020",
    "end": "985460"
  },
  {
    "text": "our dependencies to create the final image that can run and then the one script will say how about it's going to",
    "start": "985460",
    "end": "990980"
  },
  {
    "text": "run when it's actually one in production and and also there's a usage script which gives you some details so you use",
    "start": "990980",
    "end": "996680"
  },
  {
    "text": "the magical source to image to actually create it and I'll give a quick example here so this is for wrapping pie for",
    "start": "996680",
    "end": "1002380"
  },
  {
    "text": "models you can do that on one line so he gos - I build on the current directory you're saying I want to use the Selden",
    "start": "1002380",
    "end": "1008500"
  },
  {
    "text": "Python to build it image and I want to create an image called my repo iris PI classifier so you provide your source",
    "start": "1008500",
    "end": "1015460"
  },
  {
    "text": "code this is your prediction source code remember not the training just the prediction source code you put you can",
    "start": "1015460",
    "end": "1021010"
  },
  {
    "text": "provide a set of requirements which would then get some pipe installed in this case ask I can't learn inside PI",
    "start": "1021010",
    "end": "1026050"
  },
  {
    "text": "and then you just give us some information so that we understand how to join these bits together so this key can",
    "start": "1026050",
    "end": "1031420"
  },
  {
    "text": "even be provided on the command line or next to your source code and odorous to our environment location see you say",
    "start": "1031420",
    "end": "1037089"
  },
  {
    "text": "what is your model names that ties it into the source code number on the left and you say what API types you want rest",
    "start": "1037089",
    "end": "1043180"
  },
  {
    "text": "or G IPC and what is this this is a model because obviously we have everything as I said like Reuters and ensembl errs so Reuters and combiners",
    "start": "1043180",
    "end": "1049960"
  },
  {
    "text": "and transformers as well and I'll go for this very quickly but we also have our so you can do the same thing with our so",
    "start": "1049960",
    "end": "1056140"
  },
  {
    "text": "you have our code you can give a set of dependencies and you can say how to join together and you use our our build a",
    "start": "1056140",
    "end": "1061840"
  },
  {
    "text": "package and we do it the same for Java javis bit more of prescriptive at the",
    "start": "1061840",
    "end": "1067740"
  },
  {
    "text": "moment and you in to create a spring boot app and then you use our library southern call rapper to then have the",
    "start": "1067740",
    "end": "1074730"
  },
  {
    "text": "various parts that you can then implement to manage to add your code so",
    "start": "1074730",
    "end": "1082470"
  },
  {
    "text": "in this case implements the Zelda model Handler and you do the same thing and then it's just one line so this point is",
    "start": "1082470",
    "end": "1088800"
  },
  {
    "text": "this can be then put as part of your CI CD pipeline very easily so that as code gets changed and in your models they can",
    "start": "1088800",
    "end": "1094770"
  },
  {
    "text": "just go through it and package them up as images and push them so just to",
    "start": "1094770",
    "end": "1100020"
  },
  {
    "text": "clarify the steps so one is you want to package up your runtime graph you define your graph in your custom resource",
    "start": "1100020",
    "end": "1106830"
  },
  {
    "text": "deploy it onto kubernetes and then go through that loop of doing updates etc as you go along so we have two external",
    "start": "1106830",
    "end": "1114750"
  },
  {
    "text": "API so that we provide so this is how you tie into your business apps so one",
    "start": "1114750",
    "end": "1119880"
  },
  {
    "text": "is obviously predict so this takes various types of payload so you can just have a tensor which is just a shape set",
    "start": "1119880",
    "end": "1126360"
  },
  {
    "text": "of floats or you can have like an NDA which allows you to handle to a multi type data so if you're doing natural language processing you might have you",
    "start": "1126360",
    "end": "1132900"
  },
  {
    "text": "know words and words and sentences there but you can have mixed types basically for that well type if those don't fit",
    "start": "1132900",
    "end": "1138870"
  },
  {
    "text": "you can also have just a customer whale or a custom string or binary but the caveat with that is that if there's",
    "start": "1138870",
    "end": "1144690"
  },
  {
    "text": "components that need to understand what's flowing through the graph like out lie-detection stuff obviously that won't work but if the first tooth don't",
    "start": "1144690",
    "end": "1151080"
  },
  {
    "text": "fit you can do that so that would allow you then to send in your request you get back a response so in this case like",
    "start": "1151080",
    "end": "1156930"
  },
  {
    "text": "this is the EM NIST handwritten digit classification sending a five image and",
    "start": "1156930",
    "end": "1162270"
  },
  {
    "text": "it said it's a five we then also have a feedback API we can say whether the model got it wise for wrong so you can",
    "start": "1162270",
    "end": "1169260"
  },
  {
    "text": "send them this is a request this is the response I got back and then you can send in I'm a reward just say whether it",
    "start": "1169260",
    "end": "1175380"
  },
  {
    "text": "got it wired to a wall so you might send him one if it got it right zero if it got it wrong and that would then help you help modules that need that such as",
    "start": "1175380",
    "end": "1182190"
  },
  {
    "text": "concept drift or things like multi and bandaged to then do their work because they can see which models are getting",
    "start": "1182190",
    "end": "1188130"
  },
  {
    "text": "things right or wrong so in terms of our roadmap so we will definitely focus on",
    "start": "1188130",
    "end": "1195120"
  },
  {
    "text": "so low latency so obviously I've shown you lots of complex graphs there but definitely there's obviously a lot of use cases we just",
    "start": "1195120",
    "end": "1200940"
  },
  {
    "text": "have a single model and that's just it and there's a lot of work especially from people like in video unlike ten to",
    "start": "1200940",
    "end": "1207690"
  },
  {
    "text": "Artie and other things they're doing to really optimize use of GPUs and we want to make that easy for Dever so data",
    "start": "1207690",
    "end": "1214710"
  },
  {
    "text": "scientists to use stuff like predictive batching I would definitely wanna have a look at that that's in tensorflow",
    "start": "1214710",
    "end": "1220110"
  },
  {
    "text": "serving and that's great it's great and we want to have that as part of our car code as well and so everything around",
    "start": "1220110",
    "end": "1226200"
  },
  {
    "text": "the area of optimizing single model things so you can have the UM flexibility i've also data will encode",
    "start": "1226200",
    "end": "1231570"
  },
  {
    "text": "provenance so won't integrate in with people at pachyderm so we can get that aspect and then things like get ops to",
    "start": "1231570",
    "end": "1237600"
  },
  {
    "text": "allow your code to be clearly defined and then we definitely handle sort of things like distributed graphs so not",
    "start": "1237600",
    "end": "1244110"
  },
  {
    "text": "just at the moment everything served deployed is a single deployment but you might want to separate it out into separate deployments so if you have one",
    "start": "1244110",
    "end": "1250860"
  },
  {
    "text": "resource intensive GPU part of your graph I can be put one place on one node",
    "start": "1250860",
    "end": "1256110"
  },
  {
    "text": "and my you might have other parts to run on different notes so just a quick intro",
    "start": "1256110",
    "end": "1261270"
  },
  {
    "text": "you probably know queue flow by now obviously so keifa lower on how we have but I just want to show how we fit in so",
    "start": "1261270",
    "end": "1266520"
  },
  {
    "text": "obviously queue flow provides a whole set of components and width where the part the book that provides I'm serving along with tensorflow serving and",
    "start": "1266520",
    "end": "1273750"
  },
  {
    "text": "they've got a great road map which I'm sure you've heard about now and several talks in terms of extending that into different training components and we",
    "start": "1273750",
    "end": "1279300"
  },
  {
    "text": "won't be there to handle the serving part of that obviously they also use C",
    "start": "1279300",
    "end": "1284670"
  },
  {
    "text": "IDs for tensorflow and pi2 option etc so that there's a great so compatibility so",
    "start": "1284670",
    "end": "1290280"
  },
  {
    "text": "compatibility there in terms of how they are solving of things and the terms of using queue flow CS obvious you've seen",
    "start": "1290280",
    "end": "1296370"
  },
  {
    "text": "in the last talk if you were here and how you can use case on it to include the packages you need so here you",
    "start": "1296370",
    "end": "1302790"
  },
  {
    "text": "include tens flow serving but you just can switch it out and you sell them so it's very easy to set up and choose the",
    "start": "1302790",
    "end": "1307950"
  },
  {
    "text": "parts that you actually want to run in production um so yeah so how would you",
    "start": "1307950",
    "end": "1313320"
  },
  {
    "text": "do this end to end machine learning this is one way one view of doing it in terms of perhaps taken from the github idea so",
    "start": "1313320",
    "end": "1322140"
  },
  {
    "text": "here you would have you can view that as having two source source repos one for your code and one for your deployment",
    "start": "1322140",
    "end": "1329240"
  },
  {
    "text": "resources and then as code has changed you integrate that with commit Hertz via",
    "start": "1329240",
    "end": "1335830"
  },
  {
    "text": "Jenkins or Travis to fire off that your CI pipeline to actually wrap your models your one time parts also the also the in",
    "start": "1335830",
    "end": "1343030"
  },
  {
    "text": "terms of training parts quite images for them push them out onto a cluster with cue flow on to do the training just in",
    "start": "1343030",
    "end": "1349570"
  },
  {
    "text": "tests obviously and then as new images are created that could be pushed into your resource repos that would then be",
    "start": "1349570",
    "end": "1356650"
  },
  {
    "text": "updated with new image versions and then push that out onto a cluster for production so for that final stage",
    "start": "1356650",
    "end": "1362950"
  },
  {
    "text": "probably things like we've worked flux will investigating how to use that so that you can automatically push things",
    "start": "1362950",
    "end": "1369010"
  },
  {
    "text": "out as new images are created but you can also use other scene Helmand case on it and I've always are packaging up that",
    "start": "1369010",
    "end": "1374410"
  },
  {
    "text": "complete flow and obviously push out your motor weights onto the cluster as well so yes a quick end to an example of",
    "start": "1374410",
    "end": "1382030"
  },
  {
    "text": "how that would work in production so this is using the classic as m-miss classifier and so you've got handwritten",
    "start": "1382030",
    "end": "1392190"
  },
  {
    "text": "[Music] digits and you need to classify them into zero or night and you can find this",
    "start": "1392190",
    "end": "1397810"
  },
  {
    "text": "example on cue flow example Selden so it gives it end to end example you can work",
    "start": "1397810",
    "end": "1402880"
  },
  {
    "text": "for it takes about 20 minutes probably to set up and run all the whole stages someone's on GK at the moment so it's",
    "start": "1402880",
    "end": "1412000"
  },
  {
    "text": "quite easy so first you've got the training part of your model so training using our go-to we have there's various",
    "start": "1412000",
    "end": "1418930"
  },
  {
    "text": "Argo workflows to Train all the different types of models as free models in this case one tensorflow one one sky",
    "start": "1418930",
    "end": "1424360"
  },
  {
    "text": "kit learn one and one are one so you train your models quite images for them and then actually sorry crate images for",
    "start": "1424360",
    "end": "1430150"
  },
  {
    "text": "your models from the source code and then push them out onto the cluster to do the training and then same for the",
    "start": "1430150",
    "end": "1435700"
  },
  {
    "text": "runtime models using our go workflows to actually create your image source codes from your source code and then push them",
    "start": "1435700",
    "end": "1441940"
  },
  {
    "text": "out of the cluster and for that we're using open shift and as examples in the code of how to use OpenShift for these free models to wrap them so",
    "start": "1441940",
    "end": "1448300"
  },
  {
    "text": "they can be one in southern core and finally once you've done that you push it out and maybe you deploy a single model",
    "start": "1448300",
    "end": "1453490"
  },
  {
    "text": "hopefully I'll show one model to probably turn it into an a/b test with a loading update and then do a multi",
    "start": "1453490",
    "end": "1459850"
  },
  {
    "text": "unbanned it and then go through that ever this also uses all the model parameters to push to NFS for that so let's just go so yes",
    "start": "1459850",
    "end": "1469360"
  },
  {
    "text": "so here's the repo it's got three different models I won't go into the code now but there's three models a",
    "start": "1469360",
    "end": "1474670"
  },
  {
    "text": "tensor flow model skycat learn model and our model for classifying digits and",
    "start": "1474670",
    "end": "1481510"
  },
  {
    "text": "there's training which I won't do now obviously that takes some time there's a training notebook that goes through the Argo code for actually training each of",
    "start": "1481510",
    "end": "1489940"
  },
  {
    "text": "those models so there's a auga workflow for each of the models in this case that",
    "start": "1489940",
    "end": "1495360"
  },
  {
    "text": "creates the image form from this source",
    "start": "1495360",
    "end": "1501370"
  },
  {
    "text": "code turn to quite an image and then pushes it out and transits this case it's training the tensorflow model using it other TF job so that's all there for",
    "start": "1501370",
    "end": "1509140"
  },
  {
    "text": "each of the models and you can change those parameters to actually use your own get repo and push your own docker",
    "start": "1509140",
    "end": "1516010"
  },
  {
    "text": "hub repo so in terms of serving so we've already got a model running so as part",
    "start": "1516010",
    "end": "1522880"
  },
  {
    "text": "of the southern core we have a standard I'm gonna find a dashboard so here you've got the number of requests per",
    "start": "1522880",
    "end": "1527920"
  },
  {
    "text": "second running through them up this one model this is the tensor flow model this is actually the accuracy of the model",
    "start": "1527920",
    "end": "1533320"
  },
  {
    "text": "cuz we're actually the running code that some testing this is actually saying is to sending out random digits to the",
    "start": "1533320",
    "end": "1540429"
  },
  {
    "text": "model and then checking when it comes back whether the model got it right so it's running at about this is actually not that great for emiliÂ´s but I wasn't",
    "start": "1540429",
    "end": "1546490"
  },
  {
    "text": "spending much time in this example for to get high high-performing models they're just really example models this",
    "start": "1546490",
    "end": "1552370"
  },
  {
    "text": "is running at 90 percent accuracy and here there's natural various to request",
    "start": "1552370",
    "end": "1557980"
  },
  {
    "text": "latency to go through so in terms of serving so what we've got now what we've got there running is just that simple",
    "start": "1557980",
    "end": "1563770"
  },
  {
    "text": "example of a single single enter flow model with a rest endpoint and so you've",
    "start": "1563770",
    "end": "1569260"
  },
  {
    "text": "pretty much seen how it looks like you've got two here you've got the model image is actually taking its its data",
    "start": "1569260",
    "end": "1576670"
  },
  {
    "text": "from a NFS volume from the training that was the most honest part of the Eiger",
    "start": "1576670",
    "end": "1581950"
  },
  {
    "text": "workflow and then there's a very simple graph as I said to actually define that",
    "start": "1581950",
    "end": "1587260"
  },
  {
    "text": "so that's running and you can send in the GRP rest request so here's like a rest request for two got that one right",
    "start": "1587260",
    "end": "1594280"
  },
  {
    "text": "or you could send in GOP and that's all part of the code you can see how easy it is to integrating rest and gr PC into your",
    "start": "1594280",
    "end": "1601420"
  },
  {
    "text": "code and so the second part is the load test running and that's already running there so you see is pushing stuff",
    "start": "1601420",
    "end": "1606640"
  },
  {
    "text": "through onto the models and now what we can do is we can turn it into an a/b test",
    "start": "1606640",
    "end": "1612010"
  },
  {
    "text": "so I'll just apply that straightaway so that will be so it's changing the actual",
    "start": "1612010",
    "end": "1618090"
  },
  {
    "text": "resource definition for for this setup so this is basically what we're going to",
    "start": "1618090",
    "end": "1624100"
  },
  {
    "text": "turn it into we're going to turn it to a random a/b test with two models we have the tensor flow model and the sky Kettler model both with rest endpoints",
    "start": "1624100",
    "end": "1630780"
  },
  {
    "text": "so what does that look like in terms of the actual CRD customer we saw so now",
    "start": "1630780",
    "end": "1637060"
  },
  {
    "text": "we've changed it so we've got two sections here in the containers simply the other the original tensor flow model and we've got the new sky can't learn",
    "start": "1637060",
    "end": "1643960"
  },
  {
    "text": "model here and then the graph definition is a bit different so you've got the random a/b test with a few",
    "start": "1643960",
    "end": "1649920"
  },
  {
    "text": "configurations of how much traffic you want partial to each model tends flow model and the S scalar and model there",
    "start": "1649920",
    "end": "1656080"
  },
  {
    "text": "so let's just have a look to see if that's started up so yes the previous one was terminating now and it's got",
    "start": "1656080",
    "end": "1661300"
  },
  {
    "text": "Linux running the new one so we can send in some requests to that and see what that looks like or you can also check",
    "start": "1661300",
    "end": "1667870"
  },
  {
    "text": "from the status of the cell deployment whether it's ready or not and I'll give you say c1 replicas replicas available",
    "start": "1667870",
    "end": "1674770"
  },
  {
    "text": "so now we can send an another worse request and here all you've seen that's different is it's given as a extra piece",
    "start": "1674770",
    "end": "1680470"
  },
  {
    "text": "of metadata which of the which are the two routes is its it went this is one probably if we want it a few times we'd",
    "start": "1680470",
    "end": "1686860"
  },
  {
    "text": "get the other one if we're lucky 50% there were three times as a Laker",
    "start": "1686860",
    "end": "1691930"
  },
  {
    "text": "not yet and so just to prove that we'll just run a test to run a hundred",
    "start": "1691930",
    "end": "1697210"
  },
  {
    "text": "requests through that and we can also just update the graph on our dashboard and we should see now that we've got two",
    "start": "1697210",
    "end": "1704830"
  },
  {
    "text": "models going through so here we got the s Keller and model this just started and we got the new version of the the tensor",
    "start": "1704830",
    "end": "1713710"
  },
  {
    "text": "flow model running this so that the update you see this s kilo model actually accuracy is really low only 25%",
    "start": "1713710",
    "end": "1719320"
  },
  {
    "text": "here and you see the traffic is now split so what was going at 50 requests per second and now it's not till 25 on",
    "start": "1719320",
    "end": "1725980"
  },
  {
    "text": "each so yeah so here's an example we sent 100 requests and we just",
    "start": "1725980",
    "end": "1731800"
  },
  {
    "text": "that those have been split between the two these are the two end points 1 or 0 and so we've got our traffic split and",
    "start": "1731800",
    "end": "1737440"
  },
  {
    "text": "that was done a rolling update with a low touch running with no downtime so now I'm going to apply more complex",
    "start": "1737440",
    "end": "1742480"
  },
  {
    "text": "things a multi-armed bandit so I'll apply that as it will take a few seconds to actually start so what if we look at",
    "start": "1742480",
    "end": "1749740"
  },
  {
    "text": "straight through what the graph is now we're going to have three models so we've got the sky can't learn model it tends to flow model on the arm model",
    "start": "1749740",
    "end": "1755910"
  },
  {
    "text": "they're all West and put as part of our graph and we're going to swap out the",
    "start": "1755910",
    "end": "1761650"
  },
  {
    "text": "random a/b test and put in a multi-armed bandit here so that's going to look at the feedback coming back from the model",
    "start": "1761650",
    "end": "1767020"
  },
  {
    "text": "and see which one of these models is I should be forming best and then push traffic to the one that's performing best and just keep a small amount of",
    "start": "1767020",
    "end": "1772840"
  },
  {
    "text": "traffic on the other ones so let's just see where that is so now yeah we've got",
    "start": "1772840",
    "end": "1778840"
  },
  {
    "text": "the previous one is terminating and we've got the new one running so I'm going to do a little test which will take a second but let's just run a test",
    "start": "1778840",
    "end": "1787630"
  },
  {
    "text": "through so now you've seen its change the metadata coming back this is the epsilon greedy Reuters that it's taken",
    "start": "1787630",
    "end": "1793150"
  },
  {
    "text": "root two for this request and so I'm just going to send in a hundred requests",
    "start": "1793150",
    "end": "1798220"
  },
  {
    "text": "to that and we can also just check the graph on our dashboard so that should hopefully be changing now as well so now",
    "start": "1798220",
    "end": "1804550"
  },
  {
    "text": "we should have three models running what's got it so we've got the new one which is the R model that's coming up",
    "start": "1804550",
    "end": "1811510"
  },
  {
    "text": "we've got the S killer model that's there previously and we've got the previous deep deep deep M this model and",
    "start": "1811510",
    "end": "1819700"
  },
  {
    "text": "this is the epsilon we did that's also getting what is the overall reward it's saying all those are there you see",
    "start": "1819700",
    "end": "1826420"
  },
  {
    "text": "they're actually quite quite different in terms of accuracies and in terms of the number of requests they serve the",
    "start": "1826420",
    "end": "1833170"
  },
  {
    "text": "our model is actually very very slow in terms of what it can serve so here we've seen so here it shows that the most",
    "start": "1833170",
    "end": "1839500"
  },
  {
    "text": "young baddest gives a different output so basically you see it's quickly chosen that the tensorflow model is the best",
    "start": "1839500",
    "end": "1845110"
  },
  {
    "text": "and it's sending all the traffic to the tensorflow model while it's still sampling from the other two models",
    "start": "1845110",
    "end": "1850180"
  },
  {
    "text": "because you know things might change in some circumstances of you have a shopping app maybe things in the evening",
    "start": "1850180",
    "end": "1855610"
  },
  {
    "text": "or better for one model on the wild another day so you can make this all dynamic flow between the models so",
    "start": "1855610",
    "end": "1864410"
  },
  {
    "text": "yeah and I think that's basically if we go back to the presentation so that's",
    "start": "1864410",
    "end": "1870539"
  },
  {
    "text": "showing that and you can try this out I'm on github as I said what I've shown",
    "start": "1870539",
    "end": "1876390"
  },
  {
    "text": "there and that is it so thank you very much great any question so you guys are",
    "start": "1876390",
    "end": "1889350"
  },
  {
    "text": "heading for the plane like me hey so I",
    "start": "1889350",
    "end": "1898440"
  },
  {
    "text": "can you say that yeah certainly we're",
    "start": "1898440",
    "end": "1904350"
  },
  {
    "text": "looking for the next stage because what the moment we're so synchronous so sorry the question was will be support rabbit",
    "start": "1904350",
    "end": "1909690"
  },
  {
    "text": "all 0 mq so at the moment we're looking at synchronous requests so in the future we'll want to look at asynchronous",
    "start": "1909690",
    "end": "1915360"
  },
  {
    "text": "definitely we're not sure what direction will take I mean what obviously want to be as flexible as possible the honest",
    "start": "1915360",
    "end": "1920549"
  },
  {
    "text": "answer is I don't know which which which is it which is the best way to go that for that point and it's all also be",
    "start": "1920549",
    "end": "1925860"
  },
  {
    "text": "interested to see how queue flow is handling sort a synchronous requests and in theirs and stuff like pachyderm and",
    "start": "1925860",
    "end": "1931620"
  },
  {
    "text": "stuff and how that would fit in to the overall pipelines there so yeah I don't know so the question was what's what is",
    "start": "1931620",
    "end": "1945390"
  },
  {
    "text": "the recommend is still wish for models I'm you mean the models will also the model parameters or both yeah I'm so the",
    "start": "1945390",
    "end": "1953880"
  },
  {
    "text": "model parameters I mean I suppose you could use whichever is most appropriate in this example it was like NFS but I",
    "start": "1953880",
    "end": "1959039"
  },
  {
    "text": "mean maybe use it other storage sounds I mean suppose you could use cluster FS if you want so distributed storage you know",
    "start": "1959039",
    "end": "1964470"
  },
  {
    "text": "anything that makes most sense and yeah also object storage in terms of the",
    "start": "1964470",
    "end": "1970559"
  },
  {
    "text": "actual models yeah these are all put in to get so I mean that's sort of works for easily in terms of the guitars in terms of you know tying in to your CI CD",
    "start": "1970559",
    "end": "1977909"
  },
  {
    "text": "so I think yeah that's what we've done here it's not tied into CI CD here but",
    "start": "1977909",
    "end": "1982919"
  },
  {
    "text": "as you can see you could really easily extend it to push so I have commit hopes that could easily start off your CI CD",
    "start": "1982919",
    "end": "1988320"
  },
  {
    "text": "hi yeah",
    "start": "1988320",
    "end": "1991580"
  },
  {
    "text": "unsquare sure was what are we gonna spoil model DB I haven't looked into that yet but it sounds like a good idea",
    "start": "1998009",
    "end": "2004169"
  },
  {
    "text": "sounds like the summer should be part of coop flow maybe you should chat with Jeremy over that so yeah no definitely",
    "start": "2004169",
    "end": "2011460"
  },
  {
    "text": "I'm gonna be great I mean obviously we're trying to get like an end to end set of components and that would be great in terms of taking you know so you",
    "start": "2011460",
    "end": "2017730"
  },
  {
    "text": "can easily go back and I understand what what's performing well on that yeah exactly yeah up or down yep that's great",
    "start": "2017730",
    "end": "2025700"
  },
  {
    "text": "okay they know several things so thanks for joining me today [Applause]",
    "start": "2028369",
    "end": "2036360"
  }
]