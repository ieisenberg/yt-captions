[
  {
    "text": "um we have we have lukonde from uh we used to work together we look on the in",
    "start": "1800",
    "end": "8700"
  },
  {
    "text": "the same company and uh then he decided to leave paradise and jump in hell",
    "start": "8700",
    "end": "16940"
  },
  {
    "text": "no it's just AWS um look only please give it up for him thank you",
    "start": "17760",
    "end": "23900"
  },
  {
    "text": "[Applause] all right hello everyone",
    "start": "23900",
    "end": "30480"
  },
  {
    "text": "how are you doing looks looks like people are cold I'm not the only one feeling it I'm kind of",
    "start": "30480",
    "end": "36840"
  },
  {
    "text": "hoping being up here will warm me up a little unless I'm just delusional so thanks for coming to the talk as you",
    "start": "36840",
    "end": "42600"
  },
  {
    "text": "can see I'm going to be speaking about spreading applications controlling traffic and optimizing costs in",
    "start": "42600",
    "end": "47820"
  },
  {
    "text": "kubernetes as William already pointed out my name is Le Conde muella or you can also call me Luke and I'm a",
    "start": "47820",
    "end": "53700"
  },
  {
    "text": "developer advocate for kubernetes at AWS",
    "start": "53700",
    "end": "58460"
  },
  {
    "text": "so if any of you want to get in touch with me you can feel free to do so you can reach out on LinkedIn I use my full",
    "start": "59280",
    "end": "64920"
  },
  {
    "text": "name so you can search for me hopefully you'll be able to recognize me also I produce a lot of kubernetes and Cloud",
    "start": "64920",
    "end": "71220"
  },
  {
    "text": "native related content and you can search for that on YouTube subscribe to the channel if you find it interesting but also the developer advocacy team",
    "start": "71220",
    "end": "78060"
  },
  {
    "text": "that I'm a part of has a dedicated YouTube channel called containers from the couch similarly lots of great",
    "start": "78060",
    "end": "83100"
  },
  {
    "text": "content that would be useful for you and your teams",
    "start": "83100",
    "end": "87680"
  },
  {
    "text": "great we're going to jump right in so this talk was inspired by a combination of things from the past and the present",
    "start": "89700",
    "end": "97079"
  },
  {
    "text": "the first one being an event several years ago on a specific Black Friday and at the time I was working as a developer",
    "start": "97079",
    "end": "102960"
  },
  {
    "text": "for a software company not the same one as William",
    "start": "102960",
    "end": "108000"
  },
  {
    "text": "and um all the developers worked together in a shared open space so from my vantage point I could see the screens",
    "start": "108000",
    "end": "114180"
  },
  {
    "text": "of some of my work colleagues now because it was Black Friday as you'd",
    "start": "114180",
    "end": "119220"
  },
  {
    "text": "expect a number of my work colleagues were especially excited to take advantage of the discounts on a",
    "start": "119220",
    "end": "124320"
  },
  {
    "text": "particular e-commerce website and that site was really popular amongst most of the folks at the company",
    "start": "124320",
    "end": "129420"
  },
  {
    "text": "unfortunately on that day some of you might know where this is going the site",
    "start": "129420",
    "end": "134819"
  },
  {
    "text": "crashed and I could see this from where I was seated",
    "start": "134819",
    "end": "141480"
  },
  {
    "text": "my work colleagues that were trying to access the site were all met with the same screen and the company had",
    "start": "141480",
    "end": "146700"
  },
  {
    "text": "essentially issued out an apology letting people know that their systems had crashed due to a high volume and",
    "start": "146700",
    "end": "152879"
  },
  {
    "text": "traffic they didn't have that bit there about trying to restore balance in the force I'm a Star Wars fan so I threw that in",
    "start": "152879",
    "end": "159660"
  },
  {
    "text": "there so just for the sake of historic accuracy that wasn't there now when you experience something like this as a",
    "start": "159660",
    "end": "166379"
  },
  {
    "text": "developer and you're the end user or even if you're just involved in software architecture and you understand",
    "start": "166379",
    "end": "171599"
  },
  {
    "text": "it to some degree you can't help but ask yourself the question what would I do if",
    "start": "171599",
    "end": "176879"
  },
  {
    "text": "I was in that position or how would I have prevented that kind of situation you you can kind of empathize with the",
    "start": "176879",
    "end": "183720"
  },
  {
    "text": "people involved in that situation because they're also involved in software so naturally it spawned a lot of discussions between myself and some",
    "start": "183720",
    "end": "190440"
  },
  {
    "text": "of my work colleagues at the time but even over the years because it was a relatively significant incident",
    "start": "190440",
    "end": "195959"
  },
  {
    "text": "and the general consensus from a lot of those discussions was that if you want to prevent something like this then you",
    "start": "195959",
    "end": "203879"
  },
  {
    "text": "should probably have some form of high availability and auto scaling mechanisms involved in your architecture",
    "start": "203879",
    "end": "210840"
  },
  {
    "text": "now I just want to be clear we didn't have context as to what the root causes were so we were basically guessing but",
    "start": "210840",
    "end": "218280"
  },
  {
    "text": "that was a safe guess in terms of the kind of measures that you want to have in place to mitigate the risks of such",
    "start": "218280",
    "end": "224459"
  },
  {
    "text": "an outage especially for a business of its stature and on such an important day",
    "start": "224459",
    "end": "231019"
  },
  {
    "text": "now I want to start off by focusing on high availability before I get to auto scaling and these two components today",
    "start": "232080",
    "end": "240360"
  },
  {
    "text": "are widely considered best practices they're relatively standard or common",
    "start": "240360",
    "end": "245459"
  },
  {
    "text": "patterns and in many ways you think of them as straightforward but you need to give ample thoughts to both High",
    "start": "245459",
    "end": "251700"
  },
  {
    "text": "availability and auto scaling because they come with implications and the implications that I want to focus on are",
    "start": "251700",
    "end": "257820"
  },
  {
    "text": "the cost implications and so it's all fair and fine to have a highly available architecture and these",
    "start": "257820",
    "end": "263340"
  },
  {
    "text": "architectures are generally fronted by a load balancer that proxies traffic to different Upstream servers or",
    "start": "263340",
    "end": "268979"
  },
  {
    "text": "destinations however you need to give thought to the load balancing algorithm or approach",
    "start": "268979",
    "end": "274740"
  },
  {
    "text": "that you're taking in these kinds of setups for example if you're going with a round robin or a random approach you",
    "start": "274740",
    "end": "280500"
  },
  {
    "text": "need to be aware of the fact that that means that you're going to have a lot of egress cross Zone traffic and this is",
    "start": "280500",
    "end": "286139"
  },
  {
    "text": "one of the points where you incur a lot of costs and this is something that has been coming up a whole lot with some of",
    "start": "286139",
    "end": "291180"
  },
  {
    "text": "the teams that I've been engaging with especially in in Cloud contexts and so high availability is important",
    "start": "291180",
    "end": "297180"
  },
  {
    "text": "because it addresses a resilience issue you want to eliminate having a single point of failure in addition to that it",
    "start": "297180",
    "end": "302520"
  },
  {
    "text": "improves performance by essentially taking care of the fact that you have",
    "start": "302520",
    "end": "307919"
  },
  {
    "text": "you don't want to have a minimal number of resources that gets strained but bearing that in mind you need to be",
    "start": "307919",
    "end": "314699"
  },
  {
    "text": "aware of what your particular constraints are for a particular project in order to",
    "start": "314699",
    "end": "319919"
  },
  {
    "text": "implement the solution in the best possible way so there are a lot of costs associated with the load balancing",
    "start": "319919",
    "end": "325320"
  },
  {
    "text": "strategy in addition to that we also have to consider",
    "start": "325320",
    "end": "331259"
  },
  {
    "text": "Auto scaling so this traffic that is being proxied from the load balancer is headed up to Upstream destinations and",
    "start": "331259",
    "end": "338180"
  },
  {
    "text": "one of the other common pitfalls is wasted compute capacity even while I've been here speaking to a couple of people",
    "start": "338180",
    "end": "345060"
  },
  {
    "text": "who shared that they've also been seeing this a whole lot do your nodes actually align with the",
    "start": "345060",
    "end": "351840"
  },
  {
    "text": "workload requirements and so Auto scaling is important but another thing that we see that happens a whole lot is",
    "start": "351840",
    "end": "357180"
  },
  {
    "text": "over provisioning of compute capacity and so it's great that now you're your infrastructure is essentially scaled out",
    "start": "357180",
    "end": "363840"
  },
  {
    "text": "to accommodate those events where you need to scale however there are situations where you've provided a whole",
    "start": "363840",
    "end": "370020"
  },
  {
    "text": "lot more than what you actually need the reason for that is probably using a cluster Auto scaler that has a more",
    "start": "370020",
    "end": "375240"
  },
  {
    "text": "static approach working with a scaling group in a particular cloud provider that you may be working with so these",
    "start": "375240",
    "end": "381479"
  },
  {
    "text": "two areas incur you a whole lot of costs and now it's a whole lot more critical",
    "start": "381479",
    "end": "388319"
  },
  {
    "text": "because we're in a difficult economic time and a lot of teams do want to take advantage of high availability of their",
    "start": "388319",
    "end": "394800"
  },
  {
    "text": "workloads and their infrastructure with kubernetes and they want to achieve that in a cloud environment because of what",
    "start": "394800",
    "end": "400500"
  },
  {
    "text": "it has to offer including the elasticity they can they can get but they're also trying to find the best ways to address",
    "start": "400500",
    "end": "406139"
  },
  {
    "text": "the operational costs associated with such a model and that's what I'm going to be focusing on",
    "start": "406139",
    "end": "412620"
  },
  {
    "text": "so the first thing that I want to look at is spreading of your application and they're two different features that you",
    "start": "412620",
    "end": "418259"
  },
  {
    "text": "can make use of in kubernetes in order to spread your application the first one that I want to focus on is POD Affinity",
    "start": "418259",
    "end": "423960"
  },
  {
    "text": "rules and so pod Affinity rules how you can essentially apply scheduling constraints",
    "start": "423960",
    "end": "429300"
  },
  {
    "text": "through the form of certain rules that Define a relationship between different workloads it could be it could be the",
    "start": "429300",
    "end": "435840"
  },
  {
    "text": "same workload as well and that's what you'd essentially want in the case of achieving High availability and so these",
    "start": "435840",
    "end": "442440"
  },
  {
    "text": "specific rules influence the scheduler's behavior when it's placing pods on nodes so if there is a specific Affinity role",
    "start": "442440",
    "end": "448979"
  },
  {
    "text": "between pod a and pod b or rather let me say workload a and workload B and the",
    "start": "448979",
    "end": "454199"
  },
  {
    "text": "scheduler has already placed the workload a on a specific node depending on the Affinity rule that you have",
    "start": "454199",
    "end": "459419"
  },
  {
    "text": "defined when the scheduler is about to place a pod on a specific node it's going to look at that relationship and",
    "start": "459419",
    "end": "465060"
  },
  {
    "text": "see whether or not they should actually it should actually be placed on the same host or that or that should be on a",
    "start": "465060",
    "end": "470759"
  },
  {
    "text": "different AZ and these rules are based on topology domains and the topology domain can either be a host or an or an",
    "start": "470759",
    "end": "477000"
  },
  {
    "text": "availability Zone now what we're interested in as you can see in this diagram over here is an",
    "start": "477000",
    "end": "482940"
  },
  {
    "text": "anti-affinity rule so an affinity rule would essentially be if we want our pods in close proximity whether that's based",
    "start": "482940",
    "end": "488520"
  },
  {
    "text": "on a host topology or if we want it to be if we want them to be in the same AZ with anti-affinity that's essentially",
    "start": "488520",
    "end": "495060"
  },
  {
    "text": "wanting to have somewhat of a repelling effect we want the pods to repel each other so that they're spread across the",
    "start": "495060",
    "end": "501120"
  },
  {
    "text": "different topology domains and so that's how we would achieve High availability with our application",
    "start": "501120",
    "end": "507780"
  },
  {
    "text": "next I want us to consider pod topology spread constraints so this is the other approach now pod Affinity",
    "start": "507780",
    "end": "515580"
  },
  {
    "text": "rules or rather pot anti-affinity rules can help you achieve High availability but the issue with them is that it",
    "start": "515580",
    "end": "522060"
  },
  {
    "text": "doesn't necessarily address or rather it introduces a problem of fault tolerance pods apology spread constraints not only",
    "start": "522060",
    "end": "528120"
  },
  {
    "text": "provide you with high availability but also deal with the issue of fault tolerance it gives you more control over",
    "start": "528120",
    "end": "533940"
  },
  {
    "text": "how you actually want your pods to be distributed across the different topology domains in your cluster so if you take a look at this diagram over",
    "start": "533940",
    "end": "540000"
  },
  {
    "text": "here you see that we have 10 pod replicas and they're close to even when",
    "start": "540000",
    "end": "545279"
  },
  {
    "text": "it comes to doing this in three different availability zones we've got a spread of three four three so this is",
    "start": "545279",
    "end": "550800"
  },
  {
    "text": "something that you can't really achieve with pod anti-affinity rules and the reason for that is because they have an",
    "start": "550800",
    "end": "555899"
  },
  {
    "text": "anti-affinity toward each other so you could easily end up with a situation where you have a single replica running",
    "start": "555899",
    "end": "561000"
  },
  {
    "text": "on a node which is not good for fault tolerance and it's not good for resource utilization either so this works out a whole lot better if",
    "start": "561000",
    "end": "567600"
  },
  {
    "text": "you're trying to not only address availability but also false tolerance of",
    "start": "567600",
    "end": "572640"
  },
  {
    "text": "your workloads and because this is the one I'm going to be showing you or demonstrating a little bit later",
    "start": "572640",
    "end": "577800"
  },
  {
    "text": "I'm going to focus on the kind of properties that you would be defining so these are the properties that you",
    "start": "577800",
    "end": "583440"
  },
  {
    "text": "would primarily be concerned with if you're going to use pod topology spread constraints I'm going to start off by focusing on Max skew and Max Q is how",
    "start": "583440",
    "end": "591420"
  },
  {
    "text": "you would Define the maximum point to which you want imbalance or inequality for the distribution of your pods across",
    "start": "591420",
    "end": "597360"
  },
  {
    "text": "the different topology domains so if we take the example from the previous diagram just going to go back quickly so you can",
    "start": "597360",
    "end": "604080"
  },
  {
    "text": "see that so that's 10 replicas for three",
    "start": "604080",
    "end": "609300"
  },
  {
    "text": "different availability zones there's no way to equally spread the number of PODS but what we can do is say we want to",
    "start": "609300",
    "end": "614519"
  },
  {
    "text": "maximum imbalance or inequality of one which is why we've got three four three so that could easily end up also being",
    "start": "614519",
    "end": "620399"
  },
  {
    "text": "four three three now the max Q can be anything between the value of one and",
    "start": "620399",
    "end": "625800"
  },
  {
    "text": "the number of replicas that you have so in that case it would be between 1 and 10. so if you went with 10 then that",
    "start": "625800",
    "end": "631440"
  },
  {
    "text": "means there'd be a chance of which you end up with 10 replicas in a single topology domain whether that's a host or",
    "start": "631440",
    "end": "636959"
  },
  {
    "text": "an availability Zone and you'll see over there there's also the topology key and this is the key that's going to be",
    "start": "636959",
    "end": "642360"
  },
  {
    "text": "attached to the nodes it's going to be one of the labels and it's how you essentially Define the kind of topology",
    "start": "642360",
    "end": "648420"
  },
  {
    "text": "that you want to work with whether you want it to be a zonal approach or if you want it to be a host and when unsatisfiable is somewhat",
    "start": "648420",
    "end": "655980"
  },
  {
    "text": "self-explanatory this is how you want the scheduler to respond in the case that it can't meet these scheduling",
    "start": "655980",
    "end": "661800"
  },
  {
    "text": "constraints so if you wanted to still go ahead and schedule the pods anyway or if you want it if you want those pods to",
    "start": "661800",
    "end": "667620"
  },
  {
    "text": "remain in a pending State and then lastly we have the label selector and this is similar to pod Affinity rules",
    "start": "667620",
    "end": "673860"
  },
  {
    "text": "and in this case it's essentially saying which pods or rather any pods that have",
    "start": "673860",
    "end": "679800"
  },
  {
    "text": "this particular label or these labels are the ones that have the relationship for which these constraints should be",
    "start": "679800",
    "end": "685500"
  },
  {
    "text": "applied so that's part Affinity roles and pod topology spread constraints",
    "start": "685500",
    "end": "692120"
  },
  {
    "text": "that would be how you would achieve availability of your application but the next thing that we want to consider now is to start addressing those two main",
    "start": "692519",
    "end": "699180"
  },
  {
    "text": "areas of optimizing our costs the load balancing area and then we'll get to the nodes a little later but let's start off",
    "start": "699180",
    "end": "705779"
  },
  {
    "text": "with controlling traffic and I'm going to demonstrate or rather speak about two different approaches that you can take the first",
    "start": "705779",
    "end": "712200"
  },
  {
    "text": "approach I'm going to discuss will be through the use of istio for those who aren't familiar with istio it's an",
    "start": "712200",
    "end": "717660"
  },
  {
    "text": "implementation of a service mesh and the job of a service mesh in a nutshell is essentially to unburden applications",
    "start": "717660",
    "end": "722940"
  },
  {
    "text": "from having to deal with networking concerns and they generally do that in these four main domains connecting of",
    "start": "722940",
    "end": "729480"
  },
  {
    "text": "the applications securing those connections controlling traffic or traffic Management in addition with that",
    "start": "729480",
    "end": "735180"
  },
  {
    "text": "also adding resilience mechanisms and as well as having observability features",
    "start": "735180",
    "end": "741240"
  },
  {
    "text": "now what we're concerned about in this particular case is controlling traffic so changing the way load balancing is",
    "start": "741240",
    "end": "747360"
  },
  {
    "text": "actually going to work so if we look at the diagram over here you'll notice that we have a request that starts from a particular end user",
    "start": "747360",
    "end": "754560"
  },
  {
    "text": "and their request goes through to the load balancer that is exposed by the istio Ingress Gateway that traffic that",
    "start": "754560",
    "end": "760680"
  },
  {
    "text": "traffic then gets proxied to that component there that says virtual service the job of a virtual service in",
    "start": "760680",
    "end": "766019"
  },
  {
    "text": "istio works a lot like Ingress so if you're familiar with the Ingress resource this is essentially how routing",
    "start": "766019",
    "end": "771899"
  },
  {
    "text": "takes place you define your routing rules using virtual Services after rooting takes place you can apply",
    "start": "771899",
    "end": "777480"
  },
  {
    "text": "additional policies such as what we're actually about to carry out which is controlling the",
    "start": "777480",
    "end": "783120"
  },
  {
    "text": "traffic and you would do that with destination rules and with the destination rules what we can actually do is essentially say for traffic coming",
    "start": "783120",
    "end": "789839"
  },
  {
    "text": "from a certain point of origin we want it to go to a certain destination and so some of you might already be familiar",
    "start": "789839",
    "end": "796019"
  },
  {
    "text": "with these Concepts when you think of different deployment strategies directing a certain amount of traffic to",
    "start": "796019",
    "end": "802079"
  },
  {
    "text": "a certain version of an application for either a blue green deployment or a canary deployment but in this case what",
    "start": "802079",
    "end": "807360"
  },
  {
    "text": "we actually want to be doing is taking advantage of istio's feature known as locality weighted load",
    "start": "807360",
    "end": "814200"
  },
  {
    "text": "balancing and if I'm correct other service measures also have this so istio essentially takes the information of the",
    "start": "814200",
    "end": "820620"
  },
  {
    "text": "topology domains in your cluster and uses that information in order for you to actually carry this out so for a load balancer which is highly",
    "start": "820620",
    "end": "827639"
  },
  {
    "text": "available and I'll demonstrate demonstrate that a little bit later so let's say it we have our cluster running",
    "start": "827639",
    "end": "833639"
  },
  {
    "text": "in the region EU West one and we've also got a load balancer that's highly available so it's across three different",
    "start": "833639",
    "end": "838980"
  },
  {
    "text": "availability zones EU West one A B and C traffic that hits that particular load",
    "start": "838980",
    "end": "845100"
  },
  {
    "text": "balancer we want to configure it in such a way that we say for traffic coming from EU West 1A it should go to an",
    "start": "845100",
    "end": "852300"
  },
  {
    "text": "application that is running in EU West 1A and we can determine how much of that traffic goes where so in this case",
    "start": "852300",
    "end": "859380"
  },
  {
    "text": "you'll see with this particular diagram I'm saying 60 percent of traffic to EU West 1A 40 to EU West 1B and this is a",
    "start": "859380",
    "end": "866160"
  },
  {
    "text": "powerful mechanism because this is one of the ways that you can drastically reduce the egress traffic costs",
    "start": "866160",
    "end": "873320"
  },
  {
    "text": "so I think some of you are probably being kind and just nodding your head as I was talking about istio because I know",
    "start": "876000",
    "end": "881220"
  },
  {
    "text": "there's sometimes not so much love for istio because of the operational complexity associated with it and it's",
    "start": "881220",
    "end": "886800"
  },
  {
    "text": "totally understandable but something that has been interesting while these kinds of issues have been coming up and",
    "start": "886800",
    "end": "892079"
  },
  {
    "text": "I've had the chance to engage with different teams they've essentially been faced with two main options it's either they go the",
    "start": "892079",
    "end": "898560"
  },
  {
    "text": "approach of using the destination rules that istio has to offer or they accelerate upgrading their cluster to a",
    "start": "898560",
    "end": "905160"
  },
  {
    "text": "point that they can make use of topology aware hints and if I'm correct apology aware hands became hit a beta level as",
    "start": "905160",
    "end": "912720"
  },
  {
    "text": "of 1.23 and I think it's still in a beta state but you might have situations where some teams are running older",
    "start": "912720",
    "end": "918300"
  },
  {
    "text": "versions of kubernetes and so they can't make use of topology aware hints but I still want to speak about it so that some of you may be aware of that in case",
    "start": "918300",
    "end": "924779"
  },
  {
    "text": "you're running a version that allows you to make use of this feature and just for a bit more context I'm",
    "start": "924779",
    "end": "930000"
  },
  {
    "text": "going to go through how the process of routing traffic happens when uh when we're load balancing for",
    "start": "930000",
    "end": "936300"
  },
  {
    "text": "our particular applications so we've got Services again most of you if not all are already familiar with this and our",
    "start": "936300",
    "end": "942540"
  },
  {
    "text": "services our stable Network abstraction layer that sit in front of our pods because our pods are ephemeral so they",
    "start": "942540",
    "end": "948540"
  },
  {
    "text": "have a static IP now because our pods are have a short life cycle those IPS are continuously",
    "start": "948540",
    "end": "955380"
  },
  {
    "text": "changing but when I when those pods are actually alive their IPs are stored in what are known as endpoints and every",
    "start": "955380",
    "end": "961740"
  },
  {
    "text": "time that a service is created there are endpoint slices that get created and those endpoint slices are",
    "start": "961740",
    "end": "967800"
  },
  {
    "text": "created by the endpoint slice controller and the endpoint slice controller is what's actually responsible for",
    "start": "967800",
    "end": "973019"
  },
  {
    "text": "allocating the different endpoints for the IPS to the different topology domains in your cluster",
    "start": "973019",
    "end": "979320"
  },
  {
    "text": "and so if you have a highly available cluster across EU West one A B and C the",
    "start": "979320",
    "end": "984480"
  },
  {
    "text": "endpoint slice controller is going to be responsible for allocating the different endpoints into these different topology domains",
    "start": "984480",
    "end": "990300"
  },
  {
    "text": "now when you when you do that the next thing is the Q proxy",
    "start": "990300",
    "end": "996720"
  },
  {
    "text": "is a Daemon set that's running on each of the different nodes and the Q proxy is also serving a form of internal",
    "start": "996720",
    "end": "1003320"
  },
  {
    "text": "routing and what it does is it consumes from the endpoint slices now outside of topology aware hints each",
    "start": "1003320",
    "end": "1011240"
  },
  {
    "text": "of these endpoints just has information about the specific pod its IP address the node that it's running on and any",
    "start": "1011240",
    "end": "1017660"
  },
  {
    "text": "additional topology information but when you have topology aware hints enabled",
    "start": "1017660",
    "end": "1022759"
  },
  {
    "text": "this is what happens so you can see without hints the endpoint slices have endpoints that",
    "start": "1022759",
    "end": "1028819"
  },
  {
    "text": "essentially say we serve traffic for just about any Zone but when hints are enabled",
    "start": "1028819",
    "end": "1034220"
  },
  {
    "text": "it essentially has there's an endpoint with a specific availability Zone based",
    "start": "1034220",
    "end": "1039500"
  },
  {
    "text": "on the one where that pod is actually running so if we have a pod running an EU West 1A that information is stored in",
    "start": "1039500",
    "end": "1046339"
  },
  {
    "text": "a particular endpoint and then the endpoint slice controller is going to add a hint saying that you should serve traffic coming from EU West 1A so that's",
    "start": "1046339",
    "end": "1053840"
  },
  {
    "text": "the mechanism to essentially control traffic to be within a specific Zone to minimize the costs associated with",
    "start": "1053840",
    "end": "1059900"
  },
  {
    "text": "egress cross Zone traffic so those are the two approaches",
    "start": "1059900",
    "end": "1065720"
  },
  {
    "text": "hopefully that helps you with when you're trying to consider which approach to take again having a service mesh does",
    "start": "1065720",
    "end": "1070760"
  },
  {
    "text": "come with operational complexity additional domain knowledge and it might be a case where your team is not in a position to take on that",
    "start": "1070760",
    "end": "1076940"
  },
  {
    "text": "alternatively if you decide to go with topology aware hands you just need to be aware of the specific version of kubernetes that you would have to be",
    "start": "1076940",
    "end": "1083120"
  },
  {
    "text": "running in order to take advantage of that approach but the process of actually going getting running with",
    "start": "1083120",
    "end": "1089660"
  },
  {
    "text": "topology aware hints is really simple it's just ensuring that your nodes have the relevant topology domains and if",
    "start": "1089660",
    "end": "1095960"
  },
  {
    "text": "you're running a kubernetes cluster in a cloud environment that automatically gets generated for you those topology domains are attached as labels to the",
    "start": "1095960",
    "end": "1102679"
  },
  {
    "text": "respective nodes and then you would simply need to add an annotation to the service that you want to have actually",
    "start": "1102679",
    "end": "1110539"
  },
  {
    "text": "managing the the service that's going to be proxying traffic to the different pods in order for hints to work out",
    "start": "1110539",
    "end": "1117440"
  },
  {
    "text": "all right so then lastly before I get to the demo I want to turn to managing",
    "start": "1117440",
    "end": "1124400"
  },
  {
    "text": "nodes so just by show of hands is anyone here heard of carpenter",
    "start": "1124400",
    "end": "1130580"
  },
  {
    "text": "okay great a couple of people um and who here is familiar with the kubernetes cluster Auto scaler",
    "start": "1130580",
    "end": "1137720"
  },
  {
    "text": "okay most people which is great and it's a fantastic project and several years ago before I was in developer advocacy",
    "start": "1137720",
    "end": "1144380"
  },
  {
    "text": "and was still Consulting that's the project that we primarily used but a challenge that we run into then was the",
    "start": "1144380",
    "end": "1149720"
  },
  {
    "text": "fact that the cluster Auto scaler takes a static approach to scaling it works specifically with an auto scaling group",
    "start": "1149720",
    "end": "1156200"
  },
  {
    "text": "in a particular Cloud environment whether you're using AWS or Google or a different cloud provider and so this can",
    "start": "1156200",
    "end": "1163280"
  },
  {
    "text": "be particular particularly challenging when you're trying to deal with the whole issue of reducing costs and",
    "start": "1163280",
    "end": "1168320"
  },
  {
    "text": "improving resource utilization for the underlying nodes and what Carpenter does differently is",
    "start": "1168320",
    "end": "1174679"
  },
  {
    "text": "the fact that it's more Dynamic and instead of working with a scaling group instead it looks at pods that are in a",
    "start": "1174679",
    "end": "1180320"
  },
  {
    "text": "pending State and takes into consideration into consideration their specific pod requirements as well as",
    "start": "1180320",
    "end": "1185900"
  },
  {
    "text": "their scheduling constraints and then reaches out directly to the ec2 API in",
    "start": "1185900",
    "end": "1191240"
  },
  {
    "text": "order for it to provision the nodes that are actually needed for those particular workloads",
    "start": "1191240",
    "end": "1197059"
  },
  {
    "text": "and so this um drastically improves your research resource utilization as well in",
    "start": "1197059",
    "end": "1202340"
  },
  {
    "text": "addition to that Carpenter also has a feature called cons workload consolidation when workload",
    "start": "1202340",
    "end": "1207980"
  },
  {
    "text": "consolidation is enabled Carpenter is continuously monitoring the nodes in your cluster that it controls to see",
    "start": "1207980",
    "end": "1214220"
  },
  {
    "text": "whether or not resource utilization is at a good level and in the case that a number in in case that any node is",
    "start": "1214220",
    "end": "1220520"
  },
  {
    "text": "underutilized then it will essentially remove that node from your cluster and",
    "start": "1220520",
    "end": "1225799"
  },
  {
    "text": "consolidate things to make sure that there's improved resource utilization on the nodes that are actually needed in",
    "start": "1225799",
    "end": "1232039"
  },
  {
    "text": "order for you to save on costs as well now obviously the big challenge with Carpenter is even though it's an open",
    "start": "1232039",
    "end": "1238580"
  },
  {
    "text": "source project at this particular point in time it only has there's only support for the AWS cloud provider so there's",
    "start": "1238580",
    "end": "1244160"
  },
  {
    "text": "still an open issue this is also somewhat of an invitation for more people to get involved with the project in order for it to be extended to",
    "start": "1244160",
    "end": "1250280"
  },
  {
    "text": "additional Cloud providers as well now one of the other things that I love about Carpenter is the fact that it also",
    "start": "1250280",
    "end": "1256520"
  },
  {
    "text": "respects scheduling constraints so to Circle back to where I started from in terms of high availability when you put",
    "start": "1256520",
    "end": "1263059"
  },
  {
    "text": "particular topology spread constraints in place for your workloads Carpenter will be able to respect those",
    "start": "1263059",
    "end": "1269900"
  },
  {
    "text": "things when it's adding nodes to your cluster all right so",
    "start": "1269900",
    "end": "1276860"
  },
  {
    "text": "that's the talk what I'm going to do now is switch to a demo to show you a specific",
    "start": "1276860",
    "end": "1283220"
  },
  {
    "text": "focus on controlling traffic",
    "start": "1283220",
    "end": "1287679"
  },
  {
    "text": "I'm going to zoom in a little here okay can everyone see that clearly",
    "start": "1293960",
    "end": "1299659"
  },
  {
    "text": "great so the application that I want to focus on",
    "start": "1299659",
    "end": "1305720"
  },
  {
    "text": "is a basic node.js application and so it's actually two different versions of the same application and the reason I'm",
    "start": "1305720",
    "end": "1311720"
  },
  {
    "text": "doing that is because they're going to give me different responses for the exact same endpoint and I just want to",
    "start": "1311720",
    "end": "1316940"
  },
  {
    "text": "be able to differentiate between the the two different applications when traffic is being proxied to see whether or not",
    "start": "1316940",
    "end": "1323360"
  },
  {
    "text": "our destination rule configurations are actually working as expected so I'm just going to scroll down slightly here so",
    "start": "1323360",
    "end": "1329240"
  },
  {
    "text": "you can see that you'll notice that we have one application that is using version",
    "start": "1329240",
    "end": "1335740"
  },
  {
    "text": "1.1.2 and this one is called Express test",
    "start": "1335740",
    "end": "1341659"
  },
  {
    "text": "if I scroll down further this is Express test two and express Test 2 is running 1.1.4",
    "start": "1341659",
    "end": "1350059"
  },
  {
    "text": "now both of them have topology spread constraints applied now if you take a look at this you'll",
    "start": "1350059",
    "end": "1355880"
  },
  {
    "text": "see that this is the exact same code block that I walked through earlier when I was talking about topology spread constraints in addition to that I've got",
    "start": "1355880",
    "end": "1362600"
  },
  {
    "text": "a node selector to ensure that these pods are only placed on nodes that are added by Carpenter",
    "start": "1362600",
    "end": "1370480"
  },
  {
    "text": "so that's our application and they're going to be fronted by a cluster IP service that proxies to those two",
    "start": "1371960",
    "end": "1378020"
  },
  {
    "text": "different applications treating them as if they're the same one or rather two different versions of the same application then next over here is a",
    "start": "1378020",
    "end": "1384260"
  },
  {
    "text": "custom resource definition file known as a provisioner so this is the file that would essentially control the life cycle",
    "start": "1384260",
    "end": "1390799"
  },
  {
    "text": "of your nodes in Carpenter and you can have multiple provisioners where you can have a single one",
    "start": "1390799",
    "end": "1395840"
  },
  {
    "text": "so in this case I've got a provisioner dedicated to my Express test workload as you can see here it's called Express",
    "start": "1395840",
    "end": "1401840"
  },
  {
    "text": "test and you can apply different parameters or constraints for how you want Carpenter to pre to add",
    "start": "1401840",
    "end": "1409280"
  },
  {
    "text": "nodes to your cluster so in this case it's already defined to add nodes to every one of the availability zones in",
    "start": "1409280",
    "end": "1415159"
  },
  {
    "text": "the case that you wanted to add constraints to that you can do that if you want to restrict it to only adding spot instances that's something that you",
    "start": "1415159",
    "end": "1421580"
  },
  {
    "text": "can also apply in my case I want both spot instances and on-demand instances and you can also add further",
    "start": "1421580",
    "end": "1427820"
  },
  {
    "text": "configurations like defining the instance families you want which is a really powerful feature because obviously there are certain instance",
    "start": "1427820",
    "end": "1433700"
  },
  {
    "text": "families that are more costly than others and so you can be able to manage that and then right at the top here is",
    "start": "1433700",
    "end": "1440080"
  },
  {
    "text": "consolidation and you can see it's enabled so remember this is that feature that I spoke about that allows Carpenter",
    "start": "1440080",
    "end": "1445460"
  },
  {
    "text": "to continuously watch our nodes to ensure that it's checking whether whether resource utilization is at an",
    "start": "1445460",
    "end": "1452120"
  },
  {
    "text": "Optimum level so that it also just keeps working to reduce your cluster costs and removing nodes that aren't needed",
    "start": "1452120",
    "end": "1459700"
  },
  {
    "text": "all right next up over here is our destination rule this is that custom resource definition file for istio and",
    "start": "1463760",
    "end": "1472520"
  },
  {
    "text": "as you can see over here I'm just going to focus on the distribution section and",
    "start": "1472520",
    "end": "1477620"
  },
  {
    "text": "you can see what I'm saying is for traffic coming from EU West 1A I want 80 of it to go to EU West 1A and 10 to be",
    "start": "1477620",
    "end": "1485120"
  },
  {
    "text": "in 10 to C and the other sections are very similar 80 percent of traffic coming from EU",
    "start": "1485120",
    "end": "1491900"
  },
  {
    "text": "West 1B should go to EU West 1B so this is basically minimizing the amount of cross Zone traffic",
    "start": "1491900",
    "end": "1498799"
  },
  {
    "text": "and then real quick just to show you I'm gonna this is the script that I'm going to be running they're all sharing the same istio",
    "start": "1498799",
    "end": "1505580"
  },
  {
    "text": "Ingress Gateway and you can see that's the endpoint I'm going to be accessing because remember I have those two different versions of the same",
    "start": "1505580",
    "end": "1511580"
  },
  {
    "text": "application so the requests are going to be sent there and it will go through that same flow from the diagram that I",
    "start": "1511580",
    "end": "1517039"
  },
  {
    "text": "had up virtual service destination rules and eventually through to our application",
    "start": "1517039",
    "end": "1523520"
  },
  {
    "text": "so let's see what this looks like",
    "start": "1523520",
    "end": "1526779"
  },
  {
    "text": "yes okay actually",
    "start": "1533480",
    "end": "1539320"
  },
  {
    "text": "before we get to that before I run it just want to quickly show you over here is Express test two it's",
    "start": "1543860",
    "end": "1551539"
  },
  {
    "text": "already running in my eks cluster and here we have Express test one Express test two is running on this node over",
    "start": "1551539",
    "end": "1557779"
  },
  {
    "text": "here you'll see two one six five and express test one is running on zero zero eighty seven",
    "start": "1557779",
    "end": "1563299"
  },
  {
    "text": "so I want to quickly come here so you can see that you see we have zero zero eighty seven",
    "start": "1563299",
    "end": "1569840"
  },
  {
    "text": "over there I'm going to describe that and I'm going to scroll down",
    "start": "1569840",
    "end": "1575860"
  },
  {
    "text": "and I just want to highlight this particular thing to you so you can see there this particular node is running in",
    "start": "1576919",
    "end": "1583039"
  },
  {
    "text": "EU West 1A so that's where one of our applications lives and then",
    "start": "1583039",
    "end": "1589220"
  },
  {
    "text": "this is our other Carpenter controlled node if I scroll down you'll see that this one is running",
    "start": "1589220",
    "end": "1595700"
  },
  {
    "text": "in EU West 1C",
    "start": "1595700",
    "end": "1599980"
  },
  {
    "text": "all right so next thing I'm going to do is simply run this load balancing script",
    "start": "1602960",
    "end": "1610159"
  },
  {
    "text": "there we go so we've got responses of version 1.1.2 and",
    "start": "1610159",
    "end": "1616159"
  },
  {
    "text": "some from version 1.1.4 now if we're honest it's really hard to deduce that",
    "start": "1616159",
    "end": "1621260"
  },
  {
    "text": "the destination rules are actually working for all we know it's just the random approach so",
    "start": "1621260",
    "end": "1627020"
  },
  {
    "text": "the best way to verify this is to actually go back to our code editor and modify the destination rules",
    "start": "1627020",
    "end": "1636520"
  },
  {
    "text": "and instead what we're going to say is we want regardless of where the traffic is coming from we want 98 percent of it",
    "start": "1636980",
    "end": "1645140"
  },
  {
    "text": "to be sent to EU West 1A",
    "start": "1645140",
    "end": "1648880"
  },
  {
    "text": "so I'm going to apply this for to each of them",
    "start": "1650720",
    "end": "1654520"
  },
  {
    "text": "that's wrong",
    "start": "1658539",
    "end": "1661778"
  },
  {
    "text": "foreign",
    "start": "1665779",
    "end": "1667960"
  },
  {
    "text": "that okay so that's configured",
    "start": "1675460",
    "end": "1679658"
  },
  {
    "text": "run the load balancing script again and you'll see now this time we're just",
    "start": "1681380",
    "end": "1687020"
  },
  {
    "text": "getting version 1.1.2 so that's just one way to verify to ensure that our destination rules are actually working",
    "start": "1687020",
    "end": "1692299"
  },
  {
    "text": "as expected great all right so I'll move over to a bit of",
    "start": "1692299",
    "end": "1697460"
  },
  {
    "text": "time of q a now that was brilliant 10 kilocona do we",
    "start": "1697460",
    "end": "1704179"
  },
  {
    "text": "have any questions yes we do thank you thank you",
    "start": "1704179",
    "end": "1710980"
  },
  {
    "text": "thank you for can you hear me yes thank you for your talk um I just wanted to go back to the",
    "start": "1716900",
    "end": "1723260"
  },
  {
    "text": "carpet config uh specifically under the topology spread constraints",
    "start": "1723260",
    "end": "1729679"
  },
  {
    "text": "I noticed you set a condition to schedule anyway for when unsatisfiable",
    "start": "1729679",
    "end": "1734720"
  },
  {
    "text": "and I feel like that kind of introduces variation right and some things might just slip under under the bus was that",
    "start": "1734720",
    "end": "1741380"
  },
  {
    "text": "intentional yes yeah now that was intentional um so again it's going to depend on the how critical your workload is if",
    "start": "1741380",
    "end": "1748279"
  },
  {
    "text": "you're fine with having those pawns those pods ending up in a pending State um then that would then you can",
    "start": "1748279",
    "end": "1755000"
  },
  {
    "text": "essentially change that value to a different approach um so it's totally up to you depending",
    "start": "1755000",
    "end": "1760159"
  },
  {
    "text": "on the type of application you're running and how critical it is to continue running obviously the the desired",
    "start": "1760159",
    "end": "1767299"
  },
  {
    "text": "approach is to have high availability across your different topology domains but then this is an attempt by the",
    "start": "1767299",
    "end": "1773659"
  },
  {
    "text": "scheduler to take those rules into consideration when it's applying them so it just comes down to how do you want it",
    "start": "1773659",
    "end": "1779360"
  },
  {
    "text": "to respond in the case that those constraints can't be satisfied yeah we have more questions I suppose yes",
    "start": "1779360",
    "end": "1787039"
  },
  {
    "text": "take the left one first look thank you very much first of all uh",
    "start": "1787039",
    "end": "1793940"
  },
  {
    "text": "I would like to ask you something related to Carpenter because I I've been using it in production for over a client",
    "start": "1793940",
    "end": "1799159"
  },
  {
    "text": "of mine yeah and one of the issue I find out is that while Carpenter worked",
    "start": "1799159",
    "end": "1804559"
  },
  {
    "text": "really well with scaling and regarding like memory or CPU or this kind of statistic",
    "start": "1804559",
    "end": "1810399"
  },
  {
    "text": "disk space is actually a problem like I had no the crushing like application",
    "start": "1810399",
    "end": "1816440"
  },
  {
    "text": "crushing just simply because the node was that this space was not enough and stuff like that right is that something",
    "start": "1816440",
    "end": "1823399"
  },
  {
    "text": "that it's possible to monitor with Carpenter and like handle this kind of problem too or no yeah so in that case I",
    "start": "1823399",
    "end": "1831980"
  },
  {
    "text": "would I think it's probably best to have to make use of an observability stack in that situation",
    "start": "1831980",
    "end": "1838279"
  },
  {
    "text": "um because you want to use take advantage of tools like Prometheus and",
    "start": "1838279",
    "end": "1844039"
  },
  {
    "text": "have that be monitoring what's actually taking place on your particular nodes and rather be sending alert messages so",
    "start": "1844039",
    "end": "1849740"
  },
  {
    "text": "that you're aware of it um in terms of optimizing those particular nodes",
    "start": "1849740",
    "end": "1856899"
  },
  {
    "text": "probably just making sure if it's a case of disk space making sure you have the road the amount the right amount of",
    "start": "1856899",
    "end": "1862039"
  },
  {
    "text": "storage in place for that and also just reviewing the instance families as well um yeah those those would probably be",
    "start": "1862039",
    "end": "1867980"
  },
  {
    "text": "the two main starting points just reviewing the disk storage space that you're actually using trying to see what",
    "start": "1867980",
    "end": "1873320"
  },
  {
    "text": "exactly is consuming that quickly and also and better yet making use of the observability stack to actually monitor",
    "start": "1873320",
    "end": "1879260"
  },
  {
    "text": "your notes yeah thanks for the talk yeah um I have a",
    "start": "1879260",
    "end": "1885020"
  },
  {
    "text": "question regarding when you're talking about the topology spread like in the",
    "start": "1885020",
    "end": "1890539"
  },
  {
    "text": "beginning and the fact that we have quite a lot of information or on where our workloads are actually deployed in",
    "start": "1890539",
    "end": "1895880"
  },
  {
    "text": "which zone in the given cloud provider we have this information on the pods",
    "start": "1895880",
    "end": "1901039"
  },
  {
    "text": "because polls are in the nodes yeah and we also have this resource called the endpoint so where we know how where to",
    "start": "1901039",
    "end": "1909080"
  },
  {
    "text": "root our traffic so that it ends up in the particular availability Zone however",
    "start": "1909080",
    "end": "1914840"
  },
  {
    "text": "I also noticed that in one of the configuration files when you're showing how to set up istio yeah uh there you",
    "start": "1914840",
    "end": "1922580"
  },
  {
    "text": "had the configuration on basically you had the paths configured in such a way",
    "start": "1922580",
    "end": "1927860"
  },
  {
    "text": "that um there were names of these zones in the paths yeah so my guess is that there",
    "start": "1927860",
    "end": "1935000"
  },
  {
    "text": "is no information right now on how to assist in kubernetes like vanilla kubernetes on where the traffic",
    "start": "1935000",
    "end": "1941480"
  },
  {
    "text": "comes from like this has to be done on the external load balancer like either on-premises or provided by the cloud",
    "start": "1941480",
    "end": "1947059"
  },
  {
    "text": "provider right right I see what you mean yes that's correct yeah so in this case because my load I knew that my load",
    "start": "1947059",
    "end": "1952700"
  },
  {
    "text": "balancer it's an external load balancer created by the istio Ingress Gateway it's an EU West one and so that's why I",
    "start": "1952700",
    "end": "1959240"
  },
  {
    "text": "was able to implement those specific roles thank you sir",
    "start": "1959240",
    "end": "1964539"
  },
  {
    "text": "take a look under yeah um that was uh last question we have",
    "start": "1965600",
    "end": "1970899"
  },
  {
    "text": "we are a little bit off schedule at the moment but we're gonna we're gonna come back with it I'm going to be very",
    "start": "1970899",
    "end": "1976399"
  },
  {
    "text": "selfish and I'm going to ask luconda to have a photo with me right yeah",
    "start": "1976399",
    "end": "1983080"
  },
  {
    "text": "thank you sir thanks cool thanks thank you very much",
    "start": "1983120",
    "end": "1988820"
  },
  {
    "text": "thank you very much please give it up for him",
    "start": "1988820",
    "end": "1992799"
  },
  {
    "text": "okay we are we are back in schedule now um we're gonna we're gonna have the next",
    "start": "1994940",
    "end": "2001059"
  },
  {
    "text": "talk in uh 10 minutes at 2 30. so please don't leave my room or call your friends",
    "start": "2001059",
    "end": "2007779"
  },
  {
    "text": "to come upstairs and have a full room thank you very much",
    "start": "2007779",
    "end": "2012419"
  },
  {
    "text": "I have I have sorry",
    "start": "2018360",
    "end": "2021960"
  }
]