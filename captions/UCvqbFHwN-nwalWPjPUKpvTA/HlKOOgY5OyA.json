[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "hi welcome to our serverless",
    "start": "80",
    "end": "1760"
  },
  {
    "text": "practitioner summit talk",
    "start": "1760",
    "end": "3360"
  },
  {
    "text": "our talk is serverless machine learning",
    "start": "3360",
    "end": "5120"
  },
  {
    "text": "inference with care serving",
    "start": "5120",
    "end": "7120"
  },
  {
    "text": "i'm clive cooks i'm cto at selden",
    "start": "7120",
    "end": "10800"
  },
  {
    "text": "hi everyone i'm eugene i'm a senior",
    "start": "10800",
    "end": "13519"
  },
  {
    "text": "software engineer from bloomberg data",
    "start": "13519",
    "end": "15599"
  },
  {
    "text": "science platform team",
    "start": "15599",
    "end": "17279"
  },
  {
    "text": "great thanks so our talk is going to be",
    "start": "17279",
    "end": "19119"
  },
  {
    "text": "in two parts the first part is going to",
    "start": "19119",
    "end": "20640"
  },
  {
    "text": "be discussing",
    "start": "20640",
    "end": "21920"
  },
  {
    "text": "how serverless technologies can help",
    "start": "21920",
    "end": "23439"
  },
  {
    "text": "data scientists deploy their models and",
    "start": "23439",
    "end": "24960"
  },
  {
    "text": "how we collaborate on a new project",
    "start": "24960",
    "end": "26560"
  },
  {
    "text": "called care serving to actually solve",
    "start": "26560",
    "end": "28160"
  },
  {
    "text": "these needs",
    "start": "28160",
    "end": "29199"
  },
  {
    "text": "the second part will be delving into how",
    "start": "29199",
    "end": "31359"
  },
  {
    "text": "bloomberg used this in production and",
    "start": "31359",
    "end": "32960"
  },
  {
    "text": "the challenges and solutions they came",
    "start": "32960",
    "end": "34559"
  },
  {
    "text": "up with",
    "start": "34559",
    "end": "35040"
  },
  {
    "text": "with using these service technologies at",
    "start": "35040",
    "end": "36800"
  },
  {
    "text": "scale",
    "start": "36800",
    "end": "38640"
  },
  {
    "start": "38000",
    "end": "38000"
  },
  {
    "text": "so to start the talk so data scientists",
    "start": "38640",
    "end": "40960"
  },
  {
    "text": "have a variety of toolkits that they",
    "start": "40960",
    "end": "42480"
  },
  {
    "text": "need to put into production using",
    "start": "42480",
    "end": "44000"
  },
  {
    "text": "various uh classical tickets such as",
    "start": "44000",
    "end": "46399"
  },
  {
    "text": "tensorflow pytorch scikit-learn actually",
    "start": "46399",
    "end": "48559"
  },
  {
    "text": "boost various levels so they need to",
    "start": "48559",
    "end": "50000"
  },
  {
    "text": "deploy them they need to scale them they",
    "start": "50000",
    "end": "51440"
  },
  {
    "text": "need to update them",
    "start": "51440",
    "end": "52399"
  },
  {
    "text": "and they need to put that on a cluster",
    "start": "52399",
    "end": "53840"
  },
  {
    "text": "with cpus gpus and tpus",
    "start": "53840",
    "end": "56800"
  },
  {
    "text": "so in in recent years there's been a",
    "start": "56800",
    "end": "58640"
  },
  {
    "text": "variety of technologies that have come",
    "start": "58640",
    "end": "60160"
  },
  {
    "text": "into existence to actually help this",
    "start": "60160",
    "end": "61840"
  },
  {
    "text": "from obviously container runtimes and",
    "start": "61840",
    "end": "63359"
  },
  {
    "text": "kubernetes itself helping with the",
    "start": "63359",
    "end": "65119"
  },
  {
    "text": "cluster management",
    "start": "65119",
    "end": "66240"
  },
  {
    "text": "at scale and then technologies on top of",
    "start": "66240",
    "end": "68080"
  },
  {
    "text": "that have been built such as istio for",
    "start": "68080",
    "end": "69840"
  },
  {
    "text": "service meshing",
    "start": "69840",
    "end": "70880"
  },
  {
    "text": "and then in particular k native which is",
    "start": "70880",
    "end": "72640"
  },
  {
    "text": "a more recent technology that's come in",
    "start": "72640",
    "end": "74000"
  },
  {
    "text": "for serverless that we all know about",
    "start": "74000",
    "end": "75680"
  },
  {
    "text": "and this is helping data scientists",
    "start": "75680",
    "end": "78080"
  },
  {
    "text": "really in two ways one is the scale to",
    "start": "78080",
    "end": "79920"
  },
  {
    "text": "zero",
    "start": "79920",
    "end": "80720"
  },
  {
    "text": "um given machine learning is very",
    "start": "80720",
    "end": "82240"
  },
  {
    "text": "expensive and requires gpus",
    "start": "82240",
    "end": "84240"
  },
  {
    "text": "the ability to scale to zero if your",
    "start": "84240",
    "end": "86000"
  },
  {
    "text": "machine learning model is not receiving",
    "start": "86000",
    "end": "87280"
  },
  {
    "text": "any requests is",
    "start": "87280",
    "end": "88320"
  },
  {
    "text": "is really a great thing so that's one",
    "start": "88320",
    "end": "90320"
  },
  {
    "text": "thing while k native would be a great",
    "start": "90320",
    "end": "91759"
  },
  {
    "text": "uh project to use for data scientists",
    "start": "91759",
    "end": "94240"
  },
  {
    "text": "the other one is scaling up on demand",
    "start": "94240",
    "end": "96240"
  },
  {
    "text": "because actually auto scaling machine",
    "start": "96240",
    "end": "97600"
  },
  {
    "text": "learning models has quite a few um some",
    "start": "97600",
    "end": "100079"
  },
  {
    "text": "technical challenges which we'll go into",
    "start": "100079",
    "end": "101600"
  },
  {
    "text": "and we'll",
    "start": "101600",
    "end": "102079"
  },
  {
    "text": "also help show how k native actually",
    "start": "102079",
    "end": "103680"
  },
  {
    "text": "helps to solve these so",
    "start": "103680",
    "end": "105439"
  },
  {
    "text": "we have this great stack that built up",
    "start": "105439",
    "end": "107119"
  },
  {
    "text": "on top of kubernetes and really",
    "start": "107119",
    "end": "109119"
  },
  {
    "text": "data scientists would like to use that",
    "start": "109119",
    "end": "111600"
  },
  {
    "text": "but if you want to use that",
    "start": "111600",
    "end": "113360"
  },
  {
    "text": "that stack of data scientists there's",
    "start": "113360",
    "end": "114880"
  },
  {
    "text": "various things you're going to need to",
    "start": "114880",
    "end": "116159"
  },
  {
    "text": "solve first you're going to need to",
    "start": "116159",
    "end": "117360"
  },
  {
    "text": "understand how to",
    "start": "117360",
    "end": "118479"
  },
  {
    "text": "actually deploy your model servers onto",
    "start": "118479",
    "end": "120000"
  },
  {
    "text": "onto kubernetes you're going to",
    "start": "120000",
    "end": "121200"
  },
  {
    "text": "understand how to configure those",
    "start": "121200",
    "end": "122479"
  },
  {
    "text": "endpoints for http and grpc",
    "start": "122479",
    "end": "124960"
  },
  {
    "text": "how to manage all the underlying",
    "start": "124960",
    "end": "126799"
  },
  {
    "text": "kubernetes resources such as deployment",
    "start": "126799",
    "end": "128879"
  },
  {
    "text": "services and pods",
    "start": "128879",
    "end": "130319"
  },
  {
    "text": "how to manage the actual scaling that",
    "start": "130319",
    "end": "132160"
  },
  {
    "text": "you need to put in such as hpas and vpas",
    "start": "132160",
    "end": "135040"
  },
  {
    "text": "and",
    "start": "135040",
    "end": "135440"
  },
  {
    "text": "in k natives the kpas which we'll",
    "start": "135440",
    "end": "137200"
  },
  {
    "text": "discuss and then i'll also",
    "start": "137200",
    "end": "139520"
  },
  {
    "text": "manage tying up readiness and liveness",
    "start": "139520",
    "end": "141680"
  },
  {
    "text": "probes managing how you're going to get",
    "start": "141680",
    "end": "143440"
  },
  {
    "text": "those model assets onto the volumes next",
    "start": "143440",
    "end": "145599"
  },
  {
    "text": "to the model servers",
    "start": "145599",
    "end": "147200"
  },
  {
    "text": "uh things such as managing istio and",
    "start": "147200",
    "end": "149200"
  },
  {
    "text": "service message",
    "start": "149200",
    "end": "150959"
  },
  {
    "text": "service meshes understanding how you're",
    "start": "150959",
    "end": "152560"
  },
  {
    "text": "going to use the cloud events and",
    "start": "152560",
    "end": "154160"
  },
  {
    "text": "asynchronous processing",
    "start": "154160",
    "end": "155360"
  },
  {
    "text": "and manage those gpus there's a huge",
    "start": "155360",
    "end": "157440"
  },
  {
    "text": "amount that you need to actually solve",
    "start": "157440",
    "end": "159280"
  },
  {
    "text": "so for this reason uh we decided to",
    "start": "159280",
    "end": "161840"
  },
  {
    "text": "create a project cap sharing to really",
    "start": "161840",
    "end": "163200"
  },
  {
    "text": "make it easy for data scientists to go",
    "start": "163200",
    "end": "164879"
  },
  {
    "text": "that last step and use these great",
    "start": "164879",
    "end": "166319"
  },
  {
    "text": "technologies out there and in particular",
    "start": "166319",
    "end": "167840"
  },
  {
    "text": "k native for the surface",
    "start": "167840",
    "end": "169200"
  },
  {
    "text": "capabilities it actually provides as a",
    "start": "169200",
    "end": "172080"
  },
  {
    "text": "care certainly as i said came in to",
    "start": "172080",
    "end": "173599"
  },
  {
    "text": "solve these problems for",
    "start": "173599",
    "end": "174879"
  },
  {
    "text": "data scientists we need to create an",
    "start": "174879",
    "end": "176239"
  },
  {
    "text": "intuitive and consistent experience",
    "start": "176239",
    "end": "178319"
  },
  {
    "text": "and something that's really powerful",
    "start": "178319",
    "end": "179680"
  },
  {
    "text": "enough to use on day one but can be used",
    "start": "179680",
    "end": "181599"
  },
  {
    "text": "right up to day 100",
    "start": "181599",
    "end": "182879"
  },
  {
    "text": "to do all the needs so there's various",
    "start": "182879",
    "end": "184879"
  },
  {
    "text": "companies that came together to actually",
    "start": "184879",
    "end": "186560"
  },
  {
    "text": "solve",
    "start": "186560",
    "end": "187200"
  },
  {
    "text": "these needs that were active in the",
    "start": "187200",
    "end": "189599"
  },
  {
    "text": "domain of machine learning",
    "start": "189599",
    "end": "191120"
  },
  {
    "text": "uh deployment such as google seldom",
    "start": "191120",
    "end": "193040"
  },
  {
    "text": "ourselves bloomberg",
    "start": "193040",
    "end": "194560"
  },
  {
    "text": "nvidia microsoft and ibm and we came",
    "start": "194560",
    "end": "196720"
  },
  {
    "text": "together under the auspices of the q",
    "start": "196720",
    "end": "198720"
  },
  {
    "text": "flat project which is an ecosystem",
    "start": "198720",
    "end": "200480"
  },
  {
    "text": "some of you might know about to really",
    "start": "200480",
    "end": "202000"
  },
  {
    "text": "uh do machine learning on top of",
    "start": "202000",
    "end": "203599"
  },
  {
    "text": "kubernetes this was a great place to",
    "start": "203599",
    "end": "205200"
  },
  {
    "text": "really",
    "start": "205200",
    "end": "205760"
  },
  {
    "text": "get various experts in the field to come",
    "start": "205760",
    "end": "207599"
  },
  {
    "text": "together to really solve",
    "start": "207599",
    "end": "209360"
  },
  {
    "text": "this problem for data scientists so i'm",
    "start": "209360",
    "end": "212000"
  },
  {
    "text": "not going to take you through",
    "start": "212000",
    "end": "213120"
  },
  {
    "text": "the interface that we created that makes",
    "start": "213120",
    "end": "214560"
  },
  {
    "text": "it really easy for data scientists to",
    "start": "214560",
    "end": "216000"
  },
  {
    "text": "put their models out on kubernetes using",
    "start": "216000",
    "end": "218239"
  },
  {
    "text": "uh k native and those technologies so",
    "start": "218239",
    "end": "220319"
  },
  {
    "text": "here's three specs for three different",
    "start": "220319",
    "end": "221920"
  },
  {
    "text": "types of",
    "start": "221920",
    "end": "222560"
  },
  {
    "text": "uh model as you can see it's very simple",
    "start": "222560",
    "end": "225040"
  },
  {
    "text": "specs all they define",
    "start": "225040",
    "end": "226080"
  },
  {
    "text": "in the top one is a default i want a",
    "start": "226080",
    "end": "227599"
  },
  {
    "text": "predictor it's a",
    "start": "227599",
    "end": "229440"
  },
  {
    "text": "sky kit learn model and this is the",
    "start": "229440",
    "end": "231440"
  },
  {
    "text": "storage uri so very very simple exactly",
    "start": "231440",
    "end": "233519"
  },
  {
    "text": "what they want",
    "start": "233519",
    "end": "234080"
  },
  {
    "text": "they just have to put one in piece of",
    "start": "234080",
    "end": "235280"
  },
  {
    "text": "information for where their artifacts",
    "start": "235280",
    "end": "236959"
  },
  {
    "text": "are stored",
    "start": "236959",
    "end": "237680"
  },
  {
    "text": "in this case on google but it could be",
    "start": "237680",
    "end": "239120"
  },
  {
    "text": "s3 or on a pvc inside the cluster",
    "start": "239120",
    "end": "242319"
  },
  {
    "text": "similar if we go down for tensorflow",
    "start": "242319",
    "end": "243840"
  },
  {
    "text": "it's a very similar situation you",
    "start": "243840",
    "end": "245439"
  },
  {
    "text": "very almost exactly the same as the",
    "start": "245439",
    "end": "246640"
  },
  {
    "text": "previous one but they just say this is",
    "start": "246640",
    "end": "247840"
  },
  {
    "text": "the case my model is a tensorflow model",
    "start": "247840",
    "end": "250000"
  },
  {
    "text": "and here's where the model artifacts are",
    "start": "250000",
    "end": "251439"
  },
  {
    "text": "stored and very similar for pi torch",
    "start": "251439",
    "end": "253760"
  },
  {
    "text": "same thing one more aspect for pytorch",
    "start": "253760",
    "end": "256320"
  },
  {
    "text": "to really give you the actual model",
    "start": "256320",
    "end": "257759"
  },
  {
    "text": "class you've got going to deploy",
    "start": "257759",
    "end": "259199"
  },
  {
    "text": "it's very simple interface that's all",
    "start": "259199",
    "end": "260560"
  },
  {
    "text": "they need to define push that out onto",
    "start": "260560",
    "end": "262000"
  },
  {
    "text": "kubernetes we'll manage it and create",
    "start": "262000",
    "end": "263600"
  },
  {
    "text": "the actual underlying resources for them",
    "start": "263600",
    "end": "265840"
  },
  {
    "text": "so taking to the next stage if they want",
    "start": "265840",
    "end": "267280"
  },
  {
    "text": "to then update their models we can",
    "start": "267280",
    "end": "268720"
  },
  {
    "text": "easily do that in cash saving by adding",
    "start": "268720",
    "end": "270639"
  },
  {
    "text": "a canary uh endpoint as part of the",
    "start": "270639",
    "end": "273440"
  },
  {
    "text": "actual spec so here you've got a spec",
    "start": "273440",
    "end": "275120"
  },
  {
    "text": "which is default strike and loan model",
    "start": "275120",
    "end": "277120"
  },
  {
    "text": "uh for a particular iris model and",
    "start": "277120",
    "end": "279600"
  },
  {
    "text": "they've and then you've added here a",
    "start": "279600",
    "end": "281360"
  },
  {
    "text": "canary",
    "start": "281360",
    "end": "282000"
  },
  {
    "text": "section this is going to push a small",
    "start": "282000",
    "end": "283840"
  },
  {
    "text": "percentage of traffic",
    "start": "283840",
    "end": "285120"
  },
  {
    "text": "to a canary model so you can test it out",
    "start": "285120",
    "end": "287280"
  },
  {
    "text": "uh while you decide whether it's okay to",
    "start": "287280",
    "end": "288960"
  },
  {
    "text": "push this into production when you're",
    "start": "288960",
    "end": "290160"
  },
  {
    "text": "happy you can go back",
    "start": "290160",
    "end": "291360"
  },
  {
    "text": "and push the top model let's say in this",
    "start": "291360",
    "end": "293520"
  },
  {
    "text": "case rsv2",
    "start": "293520",
    "end": "294800"
  },
  {
    "text": "to be the main model so in this case for",
    "start": "294800",
    "end": "296639"
  },
  {
    "text": "the canary we've got a canary sky color",
    "start": "296639",
    "end": "298400"
  },
  {
    "text": "model",
    "start": "298400",
    "end": "298960"
  },
  {
    "text": "rsv2 now this spec also shows various",
    "start": "298960",
    "end": "302160"
  },
  {
    "text": "aspects where you can customize the",
    "start": "302160",
    "end": "304160"
  },
  {
    "text": "interface so you can decide how it's",
    "start": "304160",
    "end": "306160"
  },
  {
    "text": "going to scale between different ranges",
    "start": "306160",
    "end": "307520"
  },
  {
    "text": "here we've got the top model can scale",
    "start": "307520",
    "end": "309039"
  },
  {
    "text": "between three and ten replicas",
    "start": "309039",
    "end": "310800"
  },
  {
    "text": "and also you can put in a particular",
    "start": "310800",
    "end": "312720"
  },
  {
    "text": "resource request so how much cpu or gpu",
    "start": "312720",
    "end": "314960"
  },
  {
    "text": "or memory does my model require",
    "start": "314960",
    "end": "316479"
  },
  {
    "text": "so it gives you the capability to add",
    "start": "316479",
    "end": "317840"
  },
  {
    "text": "these extra customizations really easily",
    "start": "317840",
    "end": "319680"
  },
  {
    "text": "into the spec if you need to",
    "start": "319680",
    "end": "321840"
  },
  {
    "text": "add the the core components you need to",
    "start": "321840",
    "end": "324400"
  },
  {
    "text": "decide how your model is going to run",
    "start": "324400",
    "end": "326000"
  },
  {
    "text": "correctly",
    "start": "326000",
    "end": "327600"
  },
  {
    "text": "um taking it one step further then if",
    "start": "327600",
    "end": "329520"
  },
  {
    "start": "328000",
    "end": "328000"
  },
  {
    "text": "you once you've got the model out",
    "start": "329520",
    "end": "330560"
  },
  {
    "text": "there's various more advanced uh cases",
    "start": "330560",
    "end": "332400"
  },
  {
    "text": "where you can add in easily to the spec",
    "start": "332400",
    "end": "334080"
  },
  {
    "text": "so",
    "start": "334080",
    "end": "334479"
  },
  {
    "text": "if you want to get model explanations",
    "start": "334479",
    "end": "335919"
  },
  {
    "text": "you can easily add those on so in the",
    "start": "335919",
    "end": "337440"
  },
  {
    "text": "top spec here we've got a",
    "start": "337440",
    "end": "338960"
  },
  {
    "text": "again a sky kit layer model and we've",
    "start": "338960",
    "end": "340400"
  },
  {
    "text": "added an explainer using the alibi",
    "start": "340400",
    "end": "342240"
  },
  {
    "text": "explain uh tool kit from sullivan",
    "start": "342240",
    "end": "344160"
  },
  {
    "text": "so it's a particular type of explainer",
    "start": "344160",
    "end": "345680"
  },
  {
    "text": "that will use that you can use then get",
    "start": "345680",
    "end": "347199"
  },
  {
    "text": "explanations of why your model is giving",
    "start": "347199",
    "end": "348960"
  },
  {
    "text": "certain predictions and that can be",
    "start": "348960",
    "end": "351120"
  },
  {
    "text": "created at the same time as your core",
    "start": "351120",
    "end": "352960"
  },
  {
    "text": "model and then you can use that if i",
    "start": "352960",
    "end": "354320"
  },
  {
    "text": "explain requests",
    "start": "354320",
    "end": "355360"
  },
  {
    "text": "uh to actually understand which for the",
    "start": "355360",
    "end": "357600"
  },
  {
    "text": "predictions you've got back for the",
    "start": "357600",
    "end": "358639"
  },
  {
    "text": "model why",
    "start": "358639",
    "end": "359360"
  },
  {
    "text": "those uh predictions are coming back",
    "start": "359360",
    "end": "361440"
  },
  {
    "text": "that's one extension that we have in",
    "start": "361440",
    "end": "363199"
  },
  {
    "text": "care and the second one is shown below",
    "start": "363199",
    "end": "365039"
  },
  {
    "text": "you can add an actual transformer so if",
    "start": "365039",
    "end": "366880"
  },
  {
    "text": "you want to transform",
    "start": "366880",
    "end": "368160"
  },
  {
    "text": "the payloads before they reach the model",
    "start": "368160",
    "end": "370080"
  },
  {
    "text": "or transform the outputs",
    "start": "370080",
    "end": "371759"
  },
  {
    "text": "when it comes back for the model you can",
    "start": "371759",
    "end": "373520"
  },
  {
    "text": "simply add a new spec next to it which",
    "start": "373520",
    "end": "375759"
  },
  {
    "text": "in this case a transformer in this case",
    "start": "375759",
    "end": "377280"
  },
  {
    "text": "is a custom container",
    "start": "377280",
    "end": "378720"
  },
  {
    "text": "so one you've added that follows the",
    "start": "378720",
    "end": "380319"
  },
  {
    "text": "capsule of inspect and add a particular",
    "start": "380319",
    "end": "381919"
  },
  {
    "text": "transformation on your payloads as they",
    "start": "381919",
    "end": "383600"
  },
  {
    "text": "go in",
    "start": "383600",
    "end": "384319"
  },
  {
    "text": "so again we would make it easier for you",
    "start": "384319",
    "end": "386160"
  },
  {
    "text": "to add these extra components into your",
    "start": "386160",
    "end": "387919"
  },
  {
    "text": "spec to",
    "start": "387919",
    "end": "388479"
  },
  {
    "text": "get these extra technologies and extra",
    "start": "388479",
    "end": "390319"
  },
  {
    "text": "functionalities that you need",
    "start": "390319",
    "end": "392319"
  },
  {
    "text": "so they bring it together so this is",
    "start": "392319",
    "end": "394000"
  },
  {
    "text": "what we provide we provide a default and",
    "start": "394000",
    "end": "395919"
  },
  {
    "text": "canary endpoints each of those has a",
    "start": "395919",
    "end": "397840"
  },
  {
    "text": "predictor",
    "start": "397840",
    "end": "398800"
  },
  {
    "text": "uh and then optional components which",
    "start": "398800",
    "end": "400560"
  },
  {
    "text": "you can add on for transformers and",
    "start": "400560",
    "end": "402240"
  },
  {
    "text": "explainers all that is managed for you",
    "start": "402240",
    "end": "404000"
  },
  {
    "text": "uh",
    "start": "404000",
    "end": "404319"
  },
  {
    "text": "in terms of scaling that out and running",
    "start": "404319",
    "end": "405919"
  },
  {
    "text": "it under k native",
    "start": "405919",
    "end": "407360"
  },
  {
    "text": "and then exposing explain and predict",
    "start": "407360",
    "end": "409440"
  },
  {
    "text": "endpoints",
    "start": "409440",
    "end": "411280"
  },
  {
    "start": "411000",
    "end": "411000"
  },
  {
    "text": "um so now i'm going to go into one of",
    "start": "411280",
    "end": "412479"
  },
  {
    "text": "the aspects that k native uh really",
    "start": "412479",
    "end": "414400"
  },
  {
    "text": "helps a lot one that is gpu auto scaling",
    "start": "414400",
    "end": "417199"
  },
  {
    "text": "so a lot of models need maybe gpus",
    "start": "417199",
    "end": "420400"
  },
  {
    "text": "at inference time and maybe also tpus",
    "start": "420400",
    "end": "422319"
  },
  {
    "text": "obviously they have a cpu running as",
    "start": "422319",
    "end": "424000"
  },
  {
    "text": "the core processor all these different",
    "start": "424000",
    "end": "426479"
  },
  {
    "text": "uh",
    "start": "426479",
    "end": "427120"
  },
  {
    "text": "aspects the gcp and gpu and tpu have",
    "start": "427120",
    "end": "430240"
  },
  {
    "text": "various metrics that they'll expose you",
    "start": "430240",
    "end": "431840"
  },
  {
    "text": "know cpu usage",
    "start": "431840",
    "end": "433120"
  },
  {
    "text": "gpu memory sorry gpu due to cycle gpu",
    "start": "433120",
    "end": "436000"
  },
  {
    "text": "memory and also the same for tpu",
    "start": "436000",
    "end": "438240"
  },
  {
    "text": "and the challenge is if you want to",
    "start": "438240",
    "end": "439520"
  },
  {
    "text": "decide when to scale up or scale down",
    "start": "439520",
    "end": "441120"
  },
  {
    "text": "your model to have more",
    "start": "441120",
    "end": "442240"
  },
  {
    "text": "or some small more or less replicas",
    "start": "442240",
    "end": "444400"
  },
  {
    "text": "you're going to have to decide",
    "start": "444400",
    "end": "445680"
  },
  {
    "text": "based on all these different metrics and",
    "start": "445680",
    "end": "447120"
  },
  {
    "text": "craft a very particular",
    "start": "447120",
    "end": "449440"
  },
  {
    "text": "decision law to decide when you're going",
    "start": "449440",
    "end": "450720"
  },
  {
    "text": "to scale up when you're going to scale",
    "start": "450720",
    "end": "451919"
  },
  {
    "text": "down this is very difficult to do given",
    "start": "451919",
    "end": "453440"
  },
  {
    "text": "all these different metrics",
    "start": "453440",
    "end": "454639"
  },
  {
    "text": "and also on some kubernetes clusters",
    "start": "454639",
    "end": "456240"
  },
  {
    "text": "even uh some of those metrics will",
    "start": "456240",
    "end": "457840"
  },
  {
    "text": "not be available such as gpu metrics",
    "start": "457840",
    "end": "460240"
  },
  {
    "text": "this makes it very challenging to create",
    "start": "460240",
    "end": "461919"
  },
  {
    "text": "a good uh",
    "start": "461919",
    "end": "462639"
  },
  {
    "text": "autoscaling decision for your machine",
    "start": "462639",
    "end": "464560"
  },
  {
    "text": "learning models",
    "start": "464560",
    "end": "465840"
  },
  {
    "text": "so k native really helps to solve this",
    "start": "465840",
    "end": "467840"
  },
  {
    "text": "uh",
    "start": "467840",
    "end": "468960"
  },
  {
    "text": "by really making it much more simpler in",
    "start": "468960",
    "end": "470879"
  },
  {
    "text": "terms of how it decides how to scale up",
    "start": "470879",
    "end": "472479"
  },
  {
    "text": "and scale down",
    "start": "472479",
    "end": "473360"
  },
  {
    "text": "now for those of you who know k native",
    "start": "473360",
    "end": "474960"
  },
  {
    "text": "this diagram will probably be quite",
    "start": "474960",
    "end": "476080"
  },
  {
    "text": "familiar so i'll go over it quite",
    "start": "476080",
    "end": "477360"
  },
  {
    "text": "quickly",
    "start": "477360",
    "end": "478400"
  },
  {
    "text": "so basically native is just going to use",
    "start": "478400",
    "end": "479840"
  },
  {
    "text": "two things you decide how much",
    "start": "479840",
    "end": "481280"
  },
  {
    "text": "concurrency your model can have you know",
    "start": "481280",
    "end": "482800"
  },
  {
    "text": "how many concurrent",
    "start": "482800",
    "end": "483919"
  },
  {
    "text": "requests can a model server take at the",
    "start": "483919",
    "end": "485759"
  },
  {
    "text": "same time and then kn80 will look at the",
    "start": "485759",
    "end": "487919"
  },
  {
    "text": "number of in-flight requests coming into",
    "start": "487919",
    "end": "489440"
  },
  {
    "text": "your model",
    "start": "489440",
    "end": "490080"
  },
  {
    "text": "and decide if there's many waiting to be",
    "start": "490080",
    "end": "491840"
  },
  {
    "text": "served and then you need to",
    "start": "491840",
    "end": "493520"
  },
  {
    "text": "increase the replicas or or there's",
    "start": "493520",
    "end": "495599"
  },
  {
    "text": "lower than your expected consent",
    "start": "495599",
    "end": "497360"
  },
  {
    "text": "concurrency being served at the moment",
    "start": "497360",
    "end": "498800"
  },
  {
    "text": "and therefore you can decrease the",
    "start": "498800",
    "end": "499759"
  },
  {
    "text": "replicas down",
    "start": "499759",
    "end": "500720"
  },
  {
    "text": "so it does this by adding various",
    "start": "500720",
    "end": "502080"
  },
  {
    "text": "components into the actual uh flow",
    "start": "502080",
    "end": "504080"
  },
  {
    "text": "there's obviously acute proxy that is",
    "start": "504080",
    "end": "505759"
  },
  {
    "text": "looking at those metrics how many",
    "start": "505759",
    "end": "507280"
  },
  {
    "text": "in-flight requests are coming to this",
    "start": "507280",
    "end": "508560"
  },
  {
    "text": "particular model server and those",
    "start": "508560",
    "end": "510319"
  },
  {
    "text": "metrics are fed back to an auto scaler",
    "start": "510319",
    "end": "512080"
  },
  {
    "text": "which will then decide whether to scale",
    "start": "512080",
    "end": "513440"
  },
  {
    "text": "up or scale down",
    "start": "513440",
    "end": "514399"
  },
  {
    "text": "and there's other aspects added in like",
    "start": "514399",
    "end": "515839"
  },
  {
    "text": "the activator which will be handling",
    "start": "515839",
    "end": "517200"
  },
  {
    "text": "buffering of requests for those initial",
    "start": "517200",
    "end": "518719"
  },
  {
    "text": "requests",
    "start": "518719",
    "end": "519599"
  },
  {
    "text": "while uh your actual components scale",
    "start": "519599",
    "end": "522240"
  },
  {
    "text": "from zero so if there's no",
    "start": "522240",
    "end": "523440"
  },
  {
    "text": "records there the quest will be buffered",
    "start": "523440",
    "end": "525760"
  },
  {
    "text": "up and then they'll scale up",
    "start": "525760",
    "end": "527680"
  },
  {
    "text": "as uh initial requests come in once it's",
    "start": "527680",
    "end": "529839"
  },
  {
    "text": "scaled up and running the activator will",
    "start": "529839",
    "end": "531600"
  },
  {
    "text": "allow those requests",
    "start": "531600",
    "end": "532560"
  },
  {
    "text": "to go through the activator is also",
    "start": "532560",
    "end": "534240"
  },
  {
    "text": "there to allow if there's a sudden burst",
    "start": "534240",
    "end": "535680"
  },
  {
    "text": "of requests if",
    "start": "535680",
    "end": "536399"
  },
  {
    "text": "it's running suddenly you get a massive",
    "start": "536399",
    "end": "537920"
  },
  {
    "text": "spike in request those requests can be",
    "start": "537920",
    "end": "539519"
  },
  {
    "text": "buffered",
    "start": "539519",
    "end": "540240"
  },
  {
    "text": "in the activator while your replicas are",
    "start": "540240",
    "end": "542000"
  },
  {
    "text": "scaling up to manage it so it really",
    "start": "542000",
    "end": "543440"
  },
  {
    "text": "gives a nice interface",
    "start": "543440",
    "end": "544720"
  },
  {
    "text": "to basically scale up scale down and",
    "start": "544720",
    "end": "546480"
  },
  {
    "text": "manage bursting effects",
    "start": "546480",
    "end": "548000"
  },
  {
    "text": "in the actual use of your models and it",
    "start": "548000",
    "end": "550000"
  },
  {
    "text": "means that the data scientist doesn't",
    "start": "550000",
    "end": "551279"
  },
  {
    "text": "have to worry",
    "start": "551279",
    "end": "552160"
  },
  {
    "text": "about defining uh really uh particular",
    "start": "552160",
    "end": "554800"
  },
  {
    "text": "uh auto scaling decisions really makes",
    "start": "554800",
    "end": "556480"
  },
  {
    "text": "it much simpler and that's one of the",
    "start": "556480",
    "end": "557519"
  },
  {
    "text": "great boons of using a k native in this",
    "start": "557519",
    "end": "559680"
  },
  {
    "text": "aspect",
    "start": "559680",
    "end": "560399"
  },
  {
    "text": "for data science models putting putting",
    "start": "560399",
    "end": "562560"
  },
  {
    "text": "them into production",
    "start": "562560",
    "end": "565040"
  },
  {
    "text": "okay so we've seen how kf serving solve",
    "start": "565040",
    "end": "566880"
  },
  {
    "text": "some of these nate so in summary kf75 is",
    "start": "566880",
    "end": "569040"
  },
  {
    "text": "a very",
    "start": "569040",
    "end": "569680"
  },
  {
    "text": "intuitive and small spec uh to actually",
    "start": "569680",
    "end": "572320"
  },
  {
    "text": "get the needs the data scientist",
    "start": "572320",
    "end": "574320"
  },
  {
    "text": "has put the model into production uh so",
    "start": "574320",
    "end": "576720"
  },
  {
    "text": "it allows you",
    "start": "576720",
    "end": "577519"
  },
  {
    "text": "uh to actually it pushes out the live",
    "start": "577519",
    "end": "579839"
  },
  {
    "text": "model at http endpoint and grpc soon it",
    "start": "579839",
    "end": "582480"
  },
  {
    "text": "solves the scale to zero needs",
    "start": "582480",
    "end": "584399"
  },
  {
    "text": "uh to allow you to really scale down the",
    "start": "584399",
    "end": "585920"
  },
  {
    "text": "model and scale up as we've seen",
    "start": "585920",
    "end": "587440"
  },
  {
    "text": "to handle the gpu auto scaling issues",
    "start": "587440",
    "end": "589839"
  },
  {
    "text": "and it provides safe rollout so we've",
    "start": "589839",
    "end": "591360"
  },
  {
    "text": "seen how you can define canaries so you",
    "start": "591360",
    "end": "593279"
  },
  {
    "text": "can push out canaries easily",
    "start": "593279",
    "end": "594720"
  },
  {
    "text": "to get your new model out and then roll",
    "start": "594720",
    "end": "596399"
  },
  {
    "text": "them out to the actual the",
    "start": "596399",
    "end": "597760"
  },
  {
    "text": "final model in production and then it",
    "start": "597760",
    "end": "600000"
  },
  {
    "text": "provides optimized serving containers so",
    "start": "600000",
    "end": "602160"
  },
  {
    "text": "so you don't have to worry about you",
    "start": "602160",
    "end": "603279"
  },
  {
    "text": "know if you're a tensorflow model to get",
    "start": "603279",
    "end": "604640"
  },
  {
    "text": "the correct version of tensorflow",
    "start": "604640",
    "end": "605760"
  },
  {
    "text": "serving or pytorch model get out the",
    "start": "605760",
    "end": "607360"
  },
  {
    "text": "correct version",
    "start": "607360",
    "end": "608079"
  },
  {
    "text": "of our survey module that serves that so",
    "start": "608079",
    "end": "609839"
  },
  {
    "text": "you just need to get focus on your",
    "start": "609839",
    "end": "611360"
  },
  {
    "text": "actual models",
    "start": "611360",
    "end": "612320"
  },
  {
    "text": "and finally it handles all the low level",
    "start": "612320",
    "end": "614079"
  },
  {
    "text": "network policy and offering all the",
    "start": "614079",
    "end": "615440"
  },
  {
    "text": "tracing and metrics all provided for you",
    "start": "615440",
    "end": "617200"
  },
  {
    "text": "out of the box",
    "start": "617200",
    "end": "618800"
  },
  {
    "text": "so now it's so now we've seen how",
    "start": "618800",
    "end": "620480"
  },
  {
    "text": "carefully solved those needs and uh how",
    "start": "620480",
    "end": "622320"
  },
  {
    "text": "we've defined the spec we're actually",
    "start": "622320",
    "end": "623600"
  },
  {
    "text": "going to go forward and look at how it's",
    "start": "623600",
    "end": "624959"
  },
  {
    "text": "actually been used in production",
    "start": "624959",
    "end": "626640"
  },
  {
    "text": "so i'm going to hand over to my",
    "start": "626640",
    "end": "627680"
  },
  {
    "text": "colleague to actually discuss how it was",
    "start": "627680",
    "end": "629440"
  },
  {
    "text": "being actually used at bloomberg",
    "start": "629440",
    "end": "632320"
  },
  {
    "text": "thank you very much clive in this",
    "start": "632320",
    "end": "634640"
  },
  {
    "text": "section i'm going to talk about the use",
    "start": "634640",
    "end": "636560"
  },
  {
    "text": "cases",
    "start": "636560",
    "end": "637120"
  },
  {
    "text": "and challenges we are facing in",
    "start": "637120",
    "end": "638720"
  },
  {
    "text": "bloomberg data science platform",
    "start": "638720",
    "end": "641360"
  },
  {
    "text": "one of the important use case we have is",
    "start": "641360",
    "end": "644000"
  },
  {
    "text": "to",
    "start": "644000",
    "end": "644399"
  },
  {
    "text": "use cave serving to serve a bird model",
    "start": "644399",
    "end": "647519"
  },
  {
    "text": "it will take two questions and compare",
    "start": "647519",
    "end": "650640"
  },
  {
    "text": "the similarities",
    "start": "650640",
    "end": "651920"
  },
  {
    "text": "between those two questions the",
    "start": "651920",
    "end": "654399"
  },
  {
    "text": "challenge",
    "start": "654399",
    "end": "655120"
  },
  {
    "start": "655000",
    "end": "655000"
  },
  {
    "text": "of this use case is that bird model must",
    "start": "655120",
    "end": "658000"
  },
  {
    "text": "be served on a gpu",
    "start": "658000",
    "end": "659680"
  },
  {
    "text": "to achieve reasonable latency in one of",
    "start": "659680",
    "end": "662880"
  },
  {
    "text": "our tests",
    "start": "662880",
    "end": "663760"
  },
  {
    "text": "we found out that if we serve a testing",
    "start": "663760",
    "end": "666399"
  },
  {
    "text": "birth model on cpu",
    "start": "666399",
    "end": "668079"
  },
  {
    "text": "the average latency is around 10 seconds",
    "start": "668079",
    "end": "671839"
  },
  {
    "text": "but once we move to gpu the latency can",
    "start": "671839",
    "end": "674480"
  },
  {
    "text": "be reduced to 50 milliseconds",
    "start": "674480",
    "end": "677040"
  },
  {
    "text": "however when we monitor the gpu",
    "start": "677040",
    "end": "679440"
  },
  {
    "text": "utilization",
    "start": "679440",
    "end": "680480"
  },
  {
    "text": "the utilization matrix is very low",
    "start": "680480",
    "end": "682880"
  },
  {
    "text": "that's because when it process a request",
    "start": "682880",
    "end": "685519"
  },
  {
    "text": "doesn't need the",
    "start": "685519",
    "end": "686720"
  },
  {
    "text": "full gpu's computing power to process it",
    "start": "686720",
    "end": "690640"
  },
  {
    "text": "however on the kubernetes ecosystem",
    "start": "690640",
    "end": "693600"
  },
  {
    "text": "there isn't a very",
    "start": "693600",
    "end": "694880"
  },
  {
    "text": "easy way to request a fraction of a gpu",
    "start": "694880",
    "end": "698320"
  },
  {
    "text": "so how to share gpu computing",
    "start": "698320",
    "end": "702000"
  },
  {
    "text": "resource in inference platform is a",
    "start": "702000",
    "end": "704240"
  },
  {
    "text": "challenge we want to address",
    "start": "704240",
    "end": "707200"
  },
  {
    "start": "706000",
    "end": "706000"
  },
  {
    "text": "another important use case is",
    "start": "707200",
    "end": "708959"
  },
  {
    "text": "personalized news monitoring",
    "start": "708959",
    "end": "711600"
  },
  {
    "text": "in this use case our user want to be",
    "start": "711600",
    "end": "714320"
  },
  {
    "text": "able to train",
    "start": "714320",
    "end": "715279"
  },
  {
    "text": "personalized models for news related to",
    "start": "715279",
    "end": "719120"
  },
  {
    "text": "a specific topic",
    "start": "719120",
    "end": "720399"
  },
  {
    "text": "for example i can get news stream",
    "start": "720399",
    "end": "723200"
  },
  {
    "text": "related to covet 19 in english and",
    "start": "723200",
    "end": "725760"
  },
  {
    "text": "spanish",
    "start": "725760",
    "end": "726560"
  },
  {
    "text": "from news source and twitter data source",
    "start": "726560",
    "end": "729920"
  },
  {
    "text": "in order to",
    "start": "729920",
    "end": "731760"
  },
  {
    "text": "solve this use cases we ingest",
    "start": "731760",
    "end": "734959"
  },
  {
    "text": "news sources and they will go through",
    "start": "734959",
    "end": "738079"
  },
  {
    "text": "the inference platform",
    "start": "738079",
    "end": "739760"
  },
  {
    "text": "through multiple machine learning models",
    "start": "739760",
    "end": "741839"
  },
  {
    "text": "and eventually generate a relevant new",
    "start": "741839",
    "end": "744160"
  },
  {
    "text": "string",
    "start": "744160",
    "end": "745279"
  },
  {
    "start": "745000",
    "end": "745000"
  },
  {
    "text": "the challenge for this use case is how",
    "start": "745279",
    "end": "748000"
  },
  {
    "text": "to deploy",
    "start": "748000",
    "end": "748720"
  },
  {
    "text": "many models since we have personalized",
    "start": "748720",
    "end": "751680"
  },
  {
    "text": "new string",
    "start": "751680",
    "end": "752560"
  },
  {
    "text": "so more topic and more new strings",
    "start": "752560",
    "end": "755120"
  },
  {
    "text": "people use",
    "start": "755120",
    "end": "755839"
  },
  {
    "text": "there will be more models be trained and",
    "start": "755839",
    "end": "758160"
  },
  {
    "text": "deployed",
    "start": "758160",
    "end": "759839"
  },
  {
    "text": "and in the diagram uh in below",
    "start": "759839",
    "end": "763680"
  },
  {
    "text": "we can see that each model can have a",
    "start": "763680",
    "end": "766160"
  },
  {
    "text": "transformer",
    "start": "766160",
    "end": "767120"
  },
  {
    "text": "and a predictor let's say on average",
    "start": "767120",
    "end": "769519"
  },
  {
    "text": "each transformer needs two parts and",
    "start": "769519",
    "end": "771680"
  },
  {
    "text": "each",
    "start": "771680",
    "end": "772079"
  },
  {
    "text": "predictor also needs two parts it means",
    "start": "772079",
    "end": "774880"
  },
  {
    "text": "that",
    "start": "774880",
    "end": "775200"
  },
  {
    "text": "one model can have an average four parts",
    "start": "775200",
    "end": "778240"
  },
  {
    "text": "running",
    "start": "778240",
    "end": "778800"
  },
  {
    "text": "in the cluster let's also take a look at",
    "start": "778800",
    "end": "781519"
  },
  {
    "text": "a typical kubernetes class",
    "start": "781519",
    "end": "783600"
  },
  {
    "text": "cluster with 100 nodes the default",
    "start": "783600",
    "end": "786720"
  },
  {
    "text": "setting of",
    "start": "786720",
    "end": "787920"
  },
  {
    "text": "kubernetes that each node can run 110",
    "start": "787920",
    "end": "791519"
  },
  {
    "text": "parts",
    "start": "791519",
    "end": "792480"
  },
  {
    "text": "so in a typical 100 nodes cluster we can",
    "start": "792480",
    "end": "795920"
  },
  {
    "text": "run 11 000",
    "start": "795920",
    "end": "797120"
  },
  {
    "text": "parts so it means that i can deploy",
    "start": "797120",
    "end": "799920"
  },
  {
    "text": "around 2000 models into this cluster",
    "start": "799920",
    "end": "802880"
  },
  {
    "text": "assuming there are no other jobs running",
    "start": "802880",
    "end": "807600"
  },
  {
    "start": "807000",
    "end": "807000"
  },
  {
    "text": "in order to stop above two challenges we",
    "start": "808079",
    "end": "811440"
  },
  {
    "text": "keep serving working group have proposed",
    "start": "811440",
    "end": "813760"
  },
  {
    "text": "a solution",
    "start": "813760",
    "end": "814639"
  },
  {
    "text": "called multi-modal inference services",
    "start": "814639",
    "end": "817760"
  },
  {
    "text": "we are rewriting cave service model",
    "start": "817760",
    "end": "819839"
  },
  {
    "text": "server to be able to load",
    "start": "819839",
    "end": "821920"
  },
  {
    "text": "or unload multiple models during runtime",
    "start": "821920",
    "end": "824959"
  },
  {
    "text": "we also introduced the trained model crd",
    "start": "824959",
    "end": "827519"
  },
  {
    "text": "to decouple",
    "start": "827519",
    "end": "828560"
  },
  {
    "text": "models and model servers we are also",
    "start": "828560",
    "end": "831600"
  },
  {
    "text": "working on extending the cave serving",
    "start": "831600",
    "end": "833600"
  },
  {
    "text": "control plane",
    "start": "833600",
    "end": "834720"
  },
  {
    "text": "so when user create update or delete a",
    "start": "834720",
    "end": "838399"
  },
  {
    "text": "train model cr",
    "start": "838399",
    "end": "839760"
  },
  {
    "text": "the control plane will call the model",
    "start": "839760",
    "end": "842320"
  },
  {
    "text": "server",
    "start": "842320",
    "end": "843199"
  },
  {
    "text": "to modify a model in the runtime",
    "start": "843199",
    "end": "846880"
  },
  {
    "text": "if you are interested feel free to take",
    "start": "846880",
    "end": "848720"
  },
  {
    "text": "a look at our proposal",
    "start": "848720",
    "end": "852079"
  },
  {
    "start": "853000",
    "end": "853000"
  },
  {
    "text": "and in this section i'd like to talk",
    "start": "854560",
    "end": "857040"
  },
  {
    "text": "about some lessons we learned",
    "start": "857040",
    "end": "859600"
  },
  {
    "text": "from building an inference platform",
    "start": "859600",
    "end": "861920"
  },
  {
    "text": "using kf serving",
    "start": "861920",
    "end": "863279"
  },
  {
    "text": "uh i will talk about how we reduce our",
    "start": "863279",
    "end": "865600"
  },
  {
    "text": "inference tail latency",
    "start": "865600",
    "end": "867199"
  },
  {
    "text": "how we reduce our cold start latency and",
    "start": "867199",
    "end": "870000"
  },
  {
    "text": "how we set up",
    "start": "870000",
    "end": "870959"
  },
  {
    "text": "alerting and monitoring system",
    "start": "870959",
    "end": "874639"
  },
  {
    "start": "874000",
    "end": "874000"
  },
  {
    "text": "during one of our performance tests",
    "start": "874880",
    "end": "878560"
  },
  {
    "text": "we found out the p99 latency for a very",
    "start": "878560",
    "end": "882399"
  },
  {
    "text": "simple sk learn model",
    "start": "882399",
    "end": "884000"
  },
  {
    "text": "could be larger than 20 milliseconds",
    "start": "884000",
    "end": "886959"
  },
  {
    "text": "that's",
    "start": "886959",
    "end": "887440"
  },
  {
    "text": "a way longer than we expected we also",
    "start": "887440",
    "end": "890639"
  },
  {
    "text": "found",
    "start": "890639",
    "end": "891120"
  },
  {
    "text": "um this scaling rotor",
    "start": "891120",
    "end": "894160"
  },
  {
    "text": "experience very high cpu throttling even",
    "start": "894160",
    "end": "897120"
  },
  {
    "text": "when the container's cpu usage is way",
    "start": "897120",
    "end": "899440"
  },
  {
    "text": "below",
    "start": "899440",
    "end": "900000"
  },
  {
    "text": "its cpu limit we did some investigation",
    "start": "900000",
    "end": "903839"
  },
  {
    "text": "and we found out that kubernetes uses",
    "start": "903839",
    "end": "907600"
  },
  {
    "text": "kernel throttling to implement cpu",
    "start": "907600",
    "end": "910000"
  },
  {
    "text": "limits",
    "start": "910000",
    "end": "910800"
  },
  {
    "text": "and there is a bug in the cfs bandwidth",
    "start": "910800",
    "end": "913839"
  },
  {
    "text": "controller",
    "start": "913839",
    "end": "915440"
  },
  {
    "text": "so kubernetes may unnecessarily throttle",
    "start": "915440",
    "end": "918560"
  },
  {
    "text": "some containers",
    "start": "918560",
    "end": "920160"
  },
  {
    "text": "and one of the major containers that",
    "start": "920160",
    "end": "922480"
  },
  {
    "text": "being sweltered very",
    "start": "922480",
    "end": "923680"
  },
  {
    "text": "heavily is called cube proxy sidecar",
    "start": "923680",
    "end": "926560"
  },
  {
    "text": "that's a",
    "start": "926560",
    "end": "927600"
  },
  {
    "text": "connective side car container that will",
    "start": "927600",
    "end": "930800"
  },
  {
    "text": "be",
    "start": "930800",
    "end": "931360"
  },
  {
    "text": "deployed alongside any kf serving",
    "start": "931360",
    "end": "933839"
  },
  {
    "text": "inference service",
    "start": "933839",
    "end": "935440"
  },
  {
    "text": "because the kernel block or the request",
    "start": "935440",
    "end": "938959"
  },
  {
    "text": "that goes through the q proxy container",
    "start": "938959",
    "end": "941040"
  },
  {
    "text": "can be throttled",
    "start": "941040",
    "end": "942320"
  },
  {
    "text": "which cause the long latency",
    "start": "942320",
    "end": "945839"
  },
  {
    "text": "the solution we suggest is first",
    "start": "945839",
    "end": "949199"
  },
  {
    "text": "monitor cpu throttling using c advisor",
    "start": "949199",
    "end": "952880"
  },
  {
    "text": "very closely also this kernel block is",
    "start": "952880",
    "end": "955839"
  },
  {
    "text": "already fixed and",
    "start": "955839",
    "end": "956959"
  },
  {
    "text": "back ported in multiple kernel versions",
    "start": "956959",
    "end": "959600"
  },
  {
    "text": "so we",
    "start": "959600",
    "end": "960240"
  },
  {
    "text": "upgraded our system to one of those",
    "start": "960240",
    "end": "962399"
  },
  {
    "text": "kernel versions",
    "start": "962399",
    "end": "964399"
  },
  {
    "text": "on the left side you can see this is a",
    "start": "964399",
    "end": "966720"
  },
  {
    "text": "dashboard that's monitoring the cpu",
    "start": "966720",
    "end": "969279"
  },
  {
    "text": "throttling",
    "start": "969279",
    "end": "970480"
  },
  {
    "text": "for testing inference service",
    "start": "970480",
    "end": "973920"
  },
  {
    "text": "during the kernel upgrade we saw that",
    "start": "973920",
    "end": "976480"
  },
  {
    "text": "the cpu throttling for q proxy container",
    "start": "976480",
    "end": "979600"
  },
  {
    "text": "was dropped from about ten percent to",
    "start": "979600",
    "end": "982399"
  },
  {
    "text": "zero",
    "start": "982399",
    "end": "983120"
  },
  {
    "text": "also the p99 latency for a simple scalar",
    "start": "983120",
    "end": "986720"
  },
  {
    "text": "model",
    "start": "986720",
    "end": "987279"
  },
  {
    "text": "is reduced to about seven milliseconds",
    "start": "987279",
    "end": "991920"
  },
  {
    "start": "991000",
    "end": "991000"
  },
  {
    "text": "another problem we solved for our users",
    "start": "991920",
    "end": "994399"
  },
  {
    "text": "is how to reduce the cold start latency",
    "start": "994399",
    "end": "998320"
  },
  {
    "text": "some of our users need to train very",
    "start": "998320",
    "end": "1000560"
  },
  {
    "text": "large machine learning models",
    "start": "1000560",
    "end": "1002880"
  },
  {
    "text": "it means that it will take quite a while",
    "start": "1002880",
    "end": "1005120"
  },
  {
    "text": "to be loaded into model server",
    "start": "1005120",
    "end": "1007120"
  },
  {
    "text": "inference servers need to download the",
    "start": "1007120",
    "end": "1009120"
  },
  {
    "text": "model from s3 bucket",
    "start": "1009120",
    "end": "1010880"
  },
  {
    "text": "and then read it into memory",
    "start": "1010880",
    "end": "1014480"
  },
  {
    "text": "one very nice serverless feature that",
    "start": "1014480",
    "end": "1017040"
  },
  {
    "text": "cave serving has",
    "start": "1017040",
    "end": "1018160"
  },
  {
    "text": "is scaled down to zero it means that",
    "start": "1018160",
    "end": "1020720"
  },
  {
    "text": "when there's no traffic coming into the",
    "start": "1020720",
    "end": "1022959"
  },
  {
    "text": "service",
    "start": "1022959",
    "end": "1023680"
  },
  {
    "text": "the number of replica will be scaled",
    "start": "1023680",
    "end": "1026240"
  },
  {
    "text": "down to zero",
    "start": "1026240",
    "end": "1027199"
  },
  {
    "text": "but it also means that when the traffic",
    "start": "1027199",
    "end": "1029678"
  },
  {
    "text": "starts to come back",
    "start": "1029679",
    "end": "1031120"
  },
  {
    "text": "um kf serving need to download model",
    "start": "1031120",
    "end": "1034480"
  },
  {
    "text": "and read them into memory one more time",
    "start": "1034480",
    "end": "1037038"
  },
  {
    "text": "that can take a while depending on how",
    "start": "1037039",
    "end": "1039600"
  },
  {
    "text": "large",
    "start": "1039600",
    "end": "1040079"
  },
  {
    "text": "the model so our proposal",
    "start": "1040079",
    "end": "1043360"
  },
  {
    "text": "suggestion to our client is to set",
    "start": "1043360",
    "end": "1046558"
  },
  {
    "text": "mean replica to one or use persistent",
    "start": "1046559",
    "end": "1051200"
  },
  {
    "text": "volumes to store the model artifact",
    "start": "1051200",
    "end": "1054320"
  },
  {
    "text": "so that can reduce the network latency",
    "start": "1054320",
    "end": "1059360"
  },
  {
    "start": "1058000",
    "end": "1058000"
  },
  {
    "text": "for any production level system",
    "start": "1060640",
    "end": "1063600"
  },
  {
    "text": "monitoring",
    "start": "1063600",
    "end": "1064480"
  },
  {
    "text": "and the alerting system are very",
    "start": "1064480",
    "end": "1066480"
  },
  {
    "text": "important",
    "start": "1066480",
    "end": "1068000"
  },
  {
    "text": "here i'm going to talk about some tips",
    "start": "1068000",
    "end": "1070240"
  },
  {
    "text": "about setting up monitoring and",
    "start": "1070240",
    "end": "1072000"
  },
  {
    "text": "alerting for cave serving platform",
    "start": "1072000",
    "end": "1076799"
  },
  {
    "text": "first i'm going to suggest to monitor",
    "start": "1076799",
    "end": "1079200"
  },
  {
    "text": "the control plane very closely",
    "start": "1079200",
    "end": "1082240"
  },
  {
    "text": "for example connective steel and the",
    "start": "1082240",
    "end": "1085200"
  },
  {
    "text": "cave sterling has",
    "start": "1085200",
    "end": "1086400"
  },
  {
    "text": "several controller running in the system",
    "start": "1086400",
    "end": "1090480"
  },
  {
    "text": "i recommend to use a project called coop",
    "start": "1090480",
    "end": "1093440"
  },
  {
    "text": "state metrics",
    "start": "1093440",
    "end": "1095200"
  },
  {
    "text": "to monitor the system parts um health",
    "start": "1095200",
    "end": "1098880"
  },
  {
    "text": "healthness um the coop state",
    "start": "1098880",
    "end": "1102000"
  },
  {
    "text": "metrics listen to the api server and",
    "start": "1102000",
    "end": "1104320"
  },
  {
    "text": "generate metrics for kubernetes api",
    "start": "1104320",
    "end": "1106640"
  },
  {
    "text": "object some important metrics to monitor",
    "start": "1106640",
    "end": "1110480"
  },
  {
    "text": "for example how many replicas are",
    "start": "1110480",
    "end": "1113039"
  },
  {
    "text": "available for deployments",
    "start": "1113039",
    "end": "1115440"
  },
  {
    "text": "sdok native also has a",
    "start": "1115440",
    "end": "1118559"
  },
  {
    "text": "public dashboard that you can use to",
    "start": "1118559",
    "end": "1121919"
  },
  {
    "text": "configure in your system",
    "start": "1121919",
    "end": "1123600"
  },
  {
    "text": "for istio there is a",
    "start": "1123600",
    "end": "1126880"
  },
  {
    "text": "dashboard called kiali which can be used",
    "start": "1126880",
    "end": "1130000"
  },
  {
    "text": "to visualize the service mesh",
    "start": "1130000",
    "end": "1131919"
  },
  {
    "text": "that's also a very useful tool when we",
    "start": "1131919",
    "end": "1135039"
  },
  {
    "text": "need to debug issues",
    "start": "1135039",
    "end": "1137520"
  },
  {
    "text": "um the second important topic to",
    "start": "1137520",
    "end": "1140960"
  },
  {
    "text": "monitor is the cluster resource usage",
    "start": "1140960",
    "end": "1144480"
  },
  {
    "text": "uh when there's not enough resource in",
    "start": "1144480",
    "end": "1146960"
  },
  {
    "text": "the cluster the controller",
    "start": "1146960",
    "end": "1148799"
  },
  {
    "text": "the control plane or some other system",
    "start": "1148799",
    "end": "1151679"
  },
  {
    "text": "part",
    "start": "1151679",
    "end": "1152320"
  },
  {
    "text": "may be evicted or cannot run properly",
    "start": "1152320",
    "end": "1155600"
  },
  {
    "text": "so using ksm to monitor how much",
    "start": "1155600",
    "end": "1158880"
  },
  {
    "text": "resource still allocatable on nodes is",
    "start": "1158880",
    "end": "1161600"
  },
  {
    "text": "important",
    "start": "1161600",
    "end": "1162640"
  },
  {
    "text": "also if you are managing a multi-tenancy",
    "start": "1162640",
    "end": "1165679"
  },
  {
    "text": "infrastructure and you apply resource",
    "start": "1165679",
    "end": "1168240"
  },
  {
    "text": "code to each namespace",
    "start": "1168240",
    "end": "1169919"
  },
  {
    "text": "it's also important to monitor if each",
    "start": "1169919",
    "end": "1172160"
  },
  {
    "text": "namespace reach",
    "start": "1172160",
    "end": "1173120"
  },
  {
    "text": "their resource code lastly is monitor",
    "start": "1173120",
    "end": "1177039"
  },
  {
    "text": "control plane's container level resource",
    "start": "1177039",
    "end": "1180320"
  },
  {
    "text": "usage i recommend to use c advisor",
    "start": "1180320",
    "end": "1183600"
  },
  {
    "text": "to monitor the cpu memory gpu usage",
    "start": "1183600",
    "end": "1187600"
  },
  {
    "text": "network traffic and disk io time",
    "start": "1187600",
    "end": "1193840"
  },
  {
    "start": "1191000",
    "end": "1191000"
  },
  {
    "text": "another very important monitoring tool",
    "start": "1195120",
    "end": "1197520"
  },
  {
    "text": "is the access locks",
    "start": "1197520",
    "end": "1199760"
  },
  {
    "text": "each deal has a gateway level access",
    "start": "1199760",
    "end": "1202240"
  },
  {
    "text": "locks",
    "start": "1202240",
    "end": "1203120"
  },
  {
    "text": "if you turn on this access lock it will",
    "start": "1203120",
    "end": "1205600"
  },
  {
    "text": "log all the requests that come into the",
    "start": "1205600",
    "end": "1207679"
  },
  {
    "text": "inference platform",
    "start": "1207679",
    "end": "1209280"
  },
  {
    "text": "um can native also have access log",
    "start": "1209280",
    "end": "1212640"
  },
  {
    "text": "if you turn on the connective access log",
    "start": "1212640",
    "end": "1214880"
  },
  {
    "text": "you can",
    "start": "1214880",
    "end": "1215679"
  },
  {
    "text": "find a lot of information about all the",
    "start": "1215679",
    "end": "1218080"
  },
  {
    "text": "requests",
    "start": "1218080",
    "end": "1218720"
  },
  {
    "text": "coming into a specific inference service",
    "start": "1218720",
    "end": "1222000"
  },
  {
    "text": "both access logs i found a very useful",
    "start": "1222000",
    "end": "1225679"
  },
  {
    "text": "during support client",
    "start": "1225679",
    "end": "1229200"
  },
  {
    "start": "1227000",
    "end": "1227000"
  },
  {
    "text": "lastly i recommend to establish",
    "start": "1230159",
    "end": "1232559"
  },
  {
    "text": "dashboard to monitor",
    "start": "1232559",
    "end": "1234159"
  },
  {
    "text": "each inference service metrics",
    "start": "1234159",
    "end": "1237200"
  },
  {
    "text": "they include container level metrics",
    "start": "1237200",
    "end": "1239760"
  },
  {
    "text": "like cpu",
    "start": "1239760",
    "end": "1241039"
  },
  {
    "text": "gpu usage and the network traffic and",
    "start": "1241039",
    "end": "1243679"
  },
  {
    "text": "also request",
    "start": "1243679",
    "end": "1244720"
  },
  {
    "text": "volume success rate available rate cpu",
    "start": "1244720",
    "end": "1248080"
  },
  {
    "text": "throttling level and the response time",
    "start": "1248080",
    "end": "1252320"
  },
  {
    "text": "as of early 2020 cape serving already",
    "start": "1253440",
    "end": "1256720"
  },
  {
    "text": "have",
    "start": "1256720",
    "end": "1257120"
  },
  {
    "text": "three production users their bloomberg",
    "start": "1257120",
    "end": "1260240"
  },
  {
    "text": "core weave and go jack called",
    "start": "1260240",
    "end": "1263360"
  },
  {
    "text": "using kf serving to serve gpt2 model",
    "start": "1263360",
    "end": "1267200"
  },
  {
    "text": "for ai game called ai dungeon gojack",
    "start": "1267200",
    "end": "1270880"
  },
  {
    "text": "combined their feature store as a",
    "start": "1270880",
    "end": "1273280"
  },
  {
    "text": "pre-transformers gap with cave serving",
    "start": "1273280",
    "end": "1277520"
  },
  {
    "text": "now i'm going to let my colleague clive",
    "start": "1277520",
    "end": "1280159"
  },
  {
    "text": "to talk about the roadmap",
    "start": "1280159",
    "end": "1282480"
  },
  {
    "text": "great thanks um so just looking at the",
    "start": "1282480",
    "end": "1284159"
  },
  {
    "text": "roadmap for kf serving going forward in",
    "start": "1284159",
    "end": "1286080"
  },
  {
    "start": "1286000",
    "end": "1286000"
  },
  {
    "text": "2020",
    "start": "1286080",
    "end": "1287280"
  },
  {
    "text": "there's various aspects you want to look",
    "start": "1287280",
    "end": "1288640"
  },
  {
    "text": "at one is the actual interface so we",
    "start": "1288640",
    "end": "1290880"
  },
  {
    "text": "think it's quite intuitive but we think",
    "start": "1290880",
    "end": "1292320"
  },
  {
    "text": "it can be extended particularly in the",
    "start": "1292320",
    "end": "1294000"
  },
  {
    "text": "area of overriding so there's aspects of",
    "start": "1294000",
    "end": "1295840"
  },
  {
    "text": "the pod spec kubernetes prospect people",
    "start": "1295840",
    "end": "1297679"
  },
  {
    "text": "want to override those",
    "start": "1297679",
    "end": "1298960"
  },
  {
    "text": "we want to make it uh simpler to do that",
    "start": "1298960",
    "end": "1301360"
  },
  {
    "text": "also um",
    "start": "1301360",
    "end": "1302559"
  },
  {
    "text": "you know we love k natives and",
    "start": "1302559",
    "end": "1303679"
  },
  {
    "text": "serverless aspects in some cases if",
    "start": "1303679",
    "end": "1305520"
  },
  {
    "text": "you've got constant traffic and other",
    "start": "1305520",
    "end": "1306960"
  },
  {
    "text": "aspects",
    "start": "1306960",
    "end": "1307520"
  },
  {
    "text": "you might not need some some of the",
    "start": "1307520",
    "end": "1308799"
  },
  {
    "text": "capabilities although auto scaling is",
    "start": "1308799",
    "end": "1310240"
  },
  {
    "text": "still",
    "start": "1310240",
    "end": "1310559"
  },
  {
    "text": "a great boon so having those components",
    "start": "1310559",
    "end": "1313120"
  },
  {
    "text": "of uh if you want is geo or no",
    "start": "1313120",
    "end": "1314880"
  },
  {
    "text": "1k native not allowing to plug those in",
    "start": "1314880",
    "end": "1316960"
  },
  {
    "text": "making those more optional",
    "start": "1316960",
    "end": "1318320"
  },
  {
    "text": "is something we're going to look at and",
    "start": "1318320",
    "end": "1320080"
  },
  {
    "text": "then find the actual data plane so",
    "start": "1320080",
    "end": "1322080"
  },
  {
    "text": "how data scientists actually call the",
    "start": "1322080",
    "end": "1323679"
  },
  {
    "text": "models we've got a standard protocol",
    "start": "1323679",
    "end": "1325200"
  },
  {
    "text": "that we're actually proposing that's",
    "start": "1325200",
    "end": "1326320"
  },
  {
    "text": "already available in the",
    "start": "1326320",
    "end": "1327520"
  },
  {
    "text": "nvidia triton server which supports a",
    "start": "1327520",
    "end": "1329840"
  },
  {
    "text": "variety of",
    "start": "1329840",
    "end": "1330640"
  },
  {
    "text": "models and also will be available in our",
    "start": "1330640",
    "end": "1333039"
  },
  {
    "text": "own cash serving python",
    "start": "1333039",
    "end": "1335280"
  },
  {
    "text": "server and we're going to push that out",
    "start": "1335280",
    "end": "1336880"
  },
  {
    "text": "as a standard model so data scientists",
    "start": "1336880",
    "end": "1338880"
  },
  {
    "text": "can just have a standard interface to",
    "start": "1338880",
    "end": "1340400"
  },
  {
    "text": "their actual models",
    "start": "1340400",
    "end": "1341600"
  },
  {
    "text": "probably already discussed in the",
    "start": "1341600",
    "end": "1342960"
  },
  {
    "text": "previous slides that the challenges of",
    "start": "1342960",
    "end": "1344320"
  },
  {
    "text": "multi-modal servers",
    "start": "1344320",
    "end": "1345360"
  },
  {
    "text": "are scaling to thousands of models and",
    "start": "1345360",
    "end": "1347039"
  },
  {
    "text": "sharing those resources",
    "start": "1347039",
    "end": "1348480"
  },
  {
    "text": "and so we'll be actually implementing",
    "start": "1348480",
    "end": "1350159"
  },
  {
    "text": "those things that were discussed earlier",
    "start": "1350159",
    "end": "1351919"
  },
  {
    "text": "and finally looking um further ahead",
    "start": "1351919",
    "end": "1353760"
  },
  {
    "text": "into more complex uh actual inference",
    "start": "1353760",
    "end": "1355840"
  },
  {
    "text": "graphs so when you have multiple",
    "start": "1355840",
    "end": "1357280"
  },
  {
    "text": "components together maybe different",
    "start": "1357280",
    "end": "1358640"
  },
  {
    "text": "types of welting with multi-on bandits",
    "start": "1358640",
    "end": "1360960"
  },
  {
    "text": "and how you can tie those and define",
    "start": "1360960",
    "end": "1362720"
  },
  {
    "text": "those uh together into some pipelines",
    "start": "1362720",
    "end": "1364799"
  },
  {
    "text": "and",
    "start": "1364799",
    "end": "1365280"
  },
  {
    "text": "directed acyclic graphs and which also",
    "start": "1365280",
    "end": "1367200"
  },
  {
    "text": "be great for serverless use cases where",
    "start": "1367200",
    "end": "1369120"
  },
  {
    "text": "you know parts of the graph in",
    "start": "1369120",
    "end": "1370320"
  },
  {
    "text": "multi-unbanded they might be serving",
    "start": "1370320",
    "end": "1372000"
  },
  {
    "text": "30 models some of those models might not",
    "start": "1372000",
    "end": "1374000"
  },
  {
    "text": "be used right now but then",
    "start": "1374000",
    "end": "1375120"
  },
  {
    "text": "you know come the evening when they're",
    "start": "1375120",
    "end": "1376159"
  },
  {
    "text": "multi-arm banded thinks that those",
    "start": "1376159",
    "end": "1377440"
  },
  {
    "text": "models are better",
    "start": "1377440",
    "end": "1378400"
  },
  {
    "text": "they might then start to be used more",
    "start": "1378400",
    "end": "1380240"
  },
  {
    "text": "and so certainly service aspects should",
    "start": "1380240",
    "end": "1382000"
  },
  {
    "text": "be great in those cases",
    "start": "1382000",
    "end": "1383679"
  },
  {
    "text": "um so now i'm just going to hand back to",
    "start": "1383679",
    "end": "1384880"
  },
  {
    "text": "my colleague usually how you in the",
    "start": "1384880",
    "end": "1386640"
  },
  {
    "text": "community can help us",
    "start": "1386640",
    "end": "1388000"
  },
  {
    "start": "1387000",
    "end": "1387000"
  },
  {
    "text": "actually create these different things",
    "start": "1388000",
    "end": "1390240"
  },
  {
    "text": "thank you",
    "start": "1390240",
    "end": "1391360"
  },
  {
    "text": "to clive the cape serving working group",
    "start": "1391360",
    "end": "1394799"
  },
  {
    "text": "is open to",
    "start": "1394799",
    "end": "1396080"
  },
  {
    "text": "everyone who want to contribute who want",
    "start": "1396080",
    "end": "1398240"
  },
  {
    "text": "to make deploy machine learning",
    "start": "1398240",
    "end": "1399840"
  },
  {
    "text": "model into production easier we have a",
    "start": "1399840",
    "end": "1402799"
  },
  {
    "text": "steering committee from five different",
    "start": "1402799",
    "end": "1404720"
  },
  {
    "text": "companies we bring over a use cases",
    "start": "1404720",
    "end": "1408159"
  },
  {
    "text": "perspective into the",
    "start": "1408159",
    "end": "1409440"
  },
  {
    "text": "cave selling product to make it really",
    "start": "1409440",
    "end": "1411280"
  },
  {
    "text": "great we have",
    "start": "1411280",
    "end": "1412480"
  },
  {
    "text": "weekly working group meetings that open",
    "start": "1412480",
    "end": "1414880"
  },
  {
    "text": "to the public",
    "start": "1414880",
    "end": "1416000"
  },
  {
    "text": "we also have special topic meetings to",
    "start": "1416000",
    "end": "1418799"
  },
  {
    "text": "discuss",
    "start": "1418799",
    "end": "1419600"
  },
  {
    "text": "upcoming designs so if you like to join",
    "start": "1419600",
    "end": "1423120"
  },
  {
    "text": "us",
    "start": "1423120",
    "end": "1423360"
  },
  {
    "text": "feel free to take a look at our website",
    "start": "1423360",
    "end": "1425840"
  },
  {
    "text": "our github repos",
    "start": "1425840",
    "end": "1427360"
  },
  {
    "text": "contact us on slack or twitter or you",
    "start": "1427360",
    "end": "1430159"
  },
  {
    "text": "can email clive and",
    "start": "1430159",
    "end": "1431600"
  },
  {
    "text": "i directly and thank you very much for",
    "start": "1431600",
    "end": "1434799"
  },
  {
    "text": "listening now we can open up four",
    "start": "1434799",
    "end": "1437679"
  },
  {
    "text": "questions",
    "start": "1437679",
    "end": "1438799"
  },
  {
    "text": "great thank you",
    "start": "1438799",
    "end": "1442639"
  }
]