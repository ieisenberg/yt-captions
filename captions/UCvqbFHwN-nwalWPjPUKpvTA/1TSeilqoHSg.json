[
  {
    "start": "0",
    "end": "56000"
  },
  {
    "text": "hello welcome to kubecon 2021 today's topic is disaster recovery the",
    "start": "3600",
    "end": "10480"
  },
  {
    "text": "ability to recover from a full data center loss making sure you have the",
    "start": "10480",
    "end": "16000"
  },
  {
    "text": "data and can recover your application my name is vassalman i'm after chief data",
    "start": "16000",
    "end": "21439"
  },
  {
    "text": "foundation architecture trader and with me today is my colleague sean ranks",
    "start": "21439",
    "end": "28160"
  },
  {
    "text": "from where that was also an architecting astrology today i will",
    "start": "29199",
    "end": "35120"
  },
  {
    "text": "go over the foundation of disaster recovery and then talk about",
    "start": "35120",
    "end": "40160"
  },
  {
    "text": "storage application in kubernetes then xiaomi will go over to multi-cluster",
    "start": "40160",
    "end": "46000"
  },
  {
    "text": "management probably relocating orchestration then we'll have a short demo",
    "start": "46000",
    "end": "52879"
  },
  {
    "text": "and talk about the future work a demo will be based on ceph with a",
    "start": "52879",
    "end": "58559"
  },
  {
    "start": "56000",
    "end": "56000"
  },
  {
    "text": "software defined storage completely open source quite mature the idbi unsafe",
    "start": "58559",
    "end": "64640"
  },
  {
    "text": "is installing the software on standard servers",
    "start": "64640",
    "end": "70320"
  },
  {
    "text": "using third disk from http memory and using standard ip",
    "start": "70320",
    "end": "76479"
  },
  {
    "text": "network to get a real very reliable highly available very scalable distributed storage system",
    "start": "76479",
    "end": "84880"
  },
  {
    "text": "in a single step cluster you get all the storage need you get cloud object storage with redux gateway block storage",
    "start": "84880",
    "end": "91920"
  },
  {
    "text": "from rbd and set refresh will provide you with disability file systems",
    "start": "91920",
    "end": "98159"
  },
  {
    "start": "98000",
    "end": "98000"
  },
  {
    "text": "in order to deploy self in kubernetes we are going to use book a cncf graduated",
    "start": "98320",
    "end": "104159"
  },
  {
    "text": "project uh it manage self def you can manage",
    "start": "104159",
    "end": "109920"
  },
  {
    "text": "self deploy configurate activate and features like industrial recovery and",
    "start": "109920",
    "end": "115600"
  },
  {
    "text": "replication using basic kubernetes with method operators and custom",
    "start": "115600",
    "end": "122560"
  },
  {
    "text": "resource definition let's talk about disaster recovery",
    "start": "122560",
    "end": "129039"
  },
  {
    "start": "126000",
    "end": "126000"
  },
  {
    "text": "disaster recovery is making sure we can",
    "start": "129039",
    "end": "134560"
  },
  {
    "text": "serve our customers even if we lose a data center or e or a cloud do a gen",
    "start": "134560",
    "end": "140480"
  },
  {
    "text": "and sadly this can happen on the left for example we have a photo of ovh one of the largest data center in",
    "start": "140480",
    "end": "148879"
  },
  {
    "text": "europe that was completely destroyed by fire last year",
    "start": "148879",
    "end": "154080"
  },
  {
    "text": "in most cases our disaster recovery site should be found and that will mean high network latency",
    "start": "154080",
    "end": "162480"
  },
  {
    "text": "in that case we won't be able to use synchronous applications but asynchronous which means we don't",
    "start": "162480",
    "end": "168640"
  },
  {
    "text": "you need to drive we don't wait for the ride to sink and that will mean there will be some",
    "start": "168640",
    "end": "174800"
  },
  {
    "text": "data loss in case of recovery if you can maintain low enough",
    "start": "174800",
    "end": "182560"
  },
  {
    "text": "network latency and using classification then we can",
    "start": "182720",
    "end": "188000"
  },
  {
    "text": "even get high availability we in this talk we're going to focus on",
    "start": "188000",
    "end": "195440"
  },
  {
    "start": "192000",
    "end": "192000"
  },
  {
    "text": "regional dr which means two separate remote sites with high network latency",
    "start": "195440",
    "end": "201599"
  },
  {
    "text": "two separate kubernetes cluster two separate explorations in case we can stretch the solid system",
    "start": "201599",
    "end": "208799"
  },
  {
    "text": "these three thunder reference method demand are on there",
    "start": "208799",
    "end": "215360"
  },
  {
    "start": "215000",
    "end": "215000"
  },
  {
    "text": "a very important way we measure the glass recovery our rpo and rtu recovery",
    "start": "216239",
    "end": "222959"
  },
  {
    "text": "point objective is the amount of acceptable data loss in case of recovery",
    "start": "222959",
    "end": "228159"
  },
  {
    "text": "from disaster it's measured in time unit becoming a time objective is the amount",
    "start": "228159",
    "end": "234799"
  },
  {
    "text": "of time the duplication can be done and before it will impact the business",
    "start": "234799",
    "end": "242239"
  },
  {
    "text": "again measured in a minute so what is the ideal disaster recovery",
    "start": "242239",
    "end": "248959"
  },
  {
    "start": "245000",
    "end": "245000"
  },
  {
    "text": "we want the rpo the amount of data we use to be as low as possible",
    "start": "248959",
    "end": "254799"
  },
  {
    "text": "minutes no more we wanted to reduce that y'all the amount of time it takes us to recover",
    "start": "254799",
    "end": "261040"
  },
  {
    "text": "from the for when we detected the disaster and",
    "start": "261040",
    "end": "266240"
  },
  {
    "text": "it will be much easier if you can have a single plane of glass to orchestrate the fell over and held back",
    "start": "266240",
    "end": "272880"
  },
  {
    "text": "much better user experience example is f",
    "start": "272880",
    "end": "278000"
  },
  {
    "start": "276000",
    "end": "276000"
  },
  {
    "text": "rook and saf regional dr we have two clusters western is",
    "start": "278000",
    "end": "283759"
  },
  {
    "text": "it has their own rook safe deployments we are using that",
    "start": "283759",
    "end": "288800"
  },
  {
    "text": "gear application in this example rbd are synced not mirroring that is based on stations",
    "start": "288800",
    "end": "296240"
  },
  {
    "text": "and we have the pvs active and writable in cluster west",
    "start": "296240",
    "end": "301840"
  },
  {
    "text": "and they are standby replicated to clusterist we can have two ways of application",
    "start": "301840",
    "end": "308720"
  },
  {
    "text": "which meaning east can be extended by the west and waste can be a standard twist much more",
    "start": "308720",
    "end": "314560"
  },
  {
    "text": "efficient use of resources and in that case we can support up to",
    "start": "314560",
    "end": "320960"
  },
  {
    "text": "minutes of rpo few minutes of value",
    "start": "320960",
    "end": "327120"
  },
  {
    "text": "so what would happen in our kubernetes cluster when we have a region failure",
    "start": "327440",
    "end": "333120"
  },
  {
    "text": "in that case we will recover to the east region",
    "start": "333120",
    "end": "339360"
  },
  {
    "text": "this is done without any cooperation from the storage and the kubernetes",
    "start": "339360",
    "end": "344639"
  },
  {
    "text": "cluster in west we will promote forcefully the pvs and make them primary and",
    "start": "344639",
    "end": "350560"
  },
  {
    "text": "writable in cluster is making them unavailable across the waste",
    "start": "350560",
    "end": "356960"
  },
  {
    "text": "we may have some data loss when you want to recover back or",
    "start": "357440",
    "end": "363120"
  },
  {
    "start": "360000",
    "end": "360000"
  },
  {
    "text": "relocate back to plaster waste this case we don't want to subtract data",
    "start": "363120",
    "end": "368560"
  },
  {
    "text": "loss and in this case we'll actually have both regions participating in the",
    "start": "368560",
    "end": "375120"
  },
  {
    "text": "relocation operation that's the west will resync we cluster",
    "start": "375120",
    "end": "382000"
  },
  {
    "text": "it catch up then the application and the pv will be",
    "start": "382000",
    "end": "388319"
  },
  {
    "text": "down in class to east making sure all the data is synchronized to the source system first of all it will take only",
    "start": "388319",
    "end": "395120"
  },
  {
    "text": "the latest changes and then we'll make the purviews priming again and",
    "start": "395120",
    "end": "401039"
  },
  {
    "text": "writeable end application in cluster ways without any data loss",
    "start": "401039",
    "end": "408039"
  },
  {
    "start": "409000",
    "end": "409000"
  },
  {
    "text": "so let's talk about storage application first why can't we use backup",
    "start": "410080",
    "end": "417280"
  },
  {
    "text": "so backups are great very important you should backup your data always very important they're bad let's think about",
    "start": "417280",
    "end": "425039"
  },
  {
    "text": "how frequently we back up our data daily",
    "start": "425039",
    "end": "430720"
  },
  {
    "text": "in extreme cases every few hours this means rpo the amount of data we can",
    "start": "430720",
    "end": "437840"
  },
  {
    "text": "lose in case of disaster is ours lots of data",
    "start": "437840",
    "end": "443199"
  },
  {
    "text": "then let's think about recovery in case of recovery we're doing a restore from backup operation",
    "start": "443199",
    "end": "450400"
  },
  {
    "text": "this is usually also a very long operation very high high oto",
    "start": "450400",
    "end": "458639"
  },
  {
    "text": "then let's think about the amount of data we need to transfer in case of disaster",
    "start": "458639",
    "end": "464400"
  },
  {
    "text": "so let's say we back up to a remote object storage so in case of recovery we",
    "start": "464400",
    "end": "470080"
  },
  {
    "text": "need to read full backup in order to restore the data all that data that's a",
    "start": "470080",
    "end": "475840"
  },
  {
    "text": "lot if we use incremental backup in that case",
    "start": "475840",
    "end": "481199"
  },
  {
    "text": "we can use less data but it will increase rto the amount of",
    "start": "481199",
    "end": "486240"
  },
  {
    "text": "time of restoration also incremental backup requires us to be able",
    "start": "486240",
    "end": "492800"
  },
  {
    "text": "to detect incremental changes to the snapshots we took in in kubernetes there is enhancement to",
    "start": "492800",
    "end": "500160"
  },
  {
    "text": "support change block tracking cbt and snapshot but it's not",
    "start": "500160",
    "end": "505199"
  },
  {
    "text": "ga yet let's think about kubernetes recently",
    "start": "505199",
    "end": "512640"
  },
  {
    "text": "set up a point in time between we may lose some resources state of changes",
    "start": "512640",
    "end": "519360"
  },
  {
    "text": "but do we have to they don't change that frequently we can do it symbolously or maybe in a",
    "start": "519360",
    "end": "526480"
  },
  {
    "text": "declarative source like a github in that case we can even get to rp or",
    "start": "526480",
    "end": "533680"
  },
  {
    "text": "we won't lose any data in case of disaster we have the exact resources we had in",
    "start": "533680",
    "end": "540000"
  },
  {
    "text": "the primary cluster luckily for us storage systems do have",
    "start": "540000",
    "end": "546160"
  },
  {
    "start": "543000",
    "end": "543000"
  },
  {
    "text": "your application built in usually it's based on periodic snapshots",
    "start": "546160",
    "end": "552080"
  },
  {
    "text": "and only that only transferred incremental snapshot the delta change between",
    "start": "552080",
    "end": "558160"
  },
  {
    "text": "the period in that period that will allow slower appeal",
    "start": "558160",
    "end": "563839"
  },
  {
    "text": "and they also have much more efficient uh storage traffic and separate between the",
    "start": "563839",
    "end": "569600"
  },
  {
    "text": "storage traffic and the user io and recovery is almost",
    "start": "569600",
    "end": "575760"
  },
  {
    "text": "instantly or very close because we already have the latest beta in our",
    "start": "575760",
    "end": "580880"
  },
  {
    "text": "recovery site so the rto from soy specific is very very low",
    "start": "580880",
    "end": "588160"
  },
  {
    "text": "so it's the frame has it on api it's on configuration it don't flow",
    "start": "589040",
    "end": "594320"
  },
  {
    "text": "and require lots of complexity of the system that are using the gear application",
    "start": "594320",
    "end": "600399"
  },
  {
    "text": "so what we desire is more stronger api",
    "start": "600399",
    "end": "605920"
  },
  {
    "start": "605000",
    "end": "605000"
  },
  {
    "text": "the idea is that we want an api that will allow us to set up the replication",
    "start": "605920",
    "end": "611680"
  },
  {
    "text": "relationship between the two clusters and allow us to manage the application state",
    "start": "611680",
    "end": "617680"
  },
  {
    "text": "and also in a can support vendor specific",
    "start": "617680",
    "end": "623200"
  },
  {
    "text": "management what we did in self-csi we extended the",
    "start": "623200",
    "end": "629839"
  },
  {
    "text": "standard csi api and added a new resource volume replication",
    "start": "629839",
    "end": "636320"
  },
  {
    "text": "volume when we create a volume application we basically enable application for that volume between the",
    "start": "636320",
    "end": "643040"
  },
  {
    "text": "two clusters when we delete the resource we disable the application",
    "start": "643040",
    "end": "649279"
  },
  {
    "text": "we have the application state meaning if the volume is primary or not in this case um",
    "start": "649600",
    "end": "656959"
  },
  {
    "text": "the volume is finally in this primary site and is active invitability and",
    "start": "656959",
    "end": "663279"
  },
  {
    "text": "replicated to best the data source will point to the pvc that",
    "start": "663279",
    "end": "668800"
  },
  {
    "text": "is being replicated we have a new csi that club that is",
    "start": "668800",
    "end": "674560"
  },
  {
    "text": "handling the reconciliation of those resources this is an example of this",
    "start": "674560",
    "end": "681519"
  },
  {
    "text": "in order to enhance more capabilities we also added volume application class the",
    "start": "682800",
    "end": "688320"
  },
  {
    "text": "class contains secrets we need in order to destroy system to be able to",
    "start": "688320",
    "end": "694079"
  },
  {
    "text": "communicate we can define as replication schedule as not all application i need the same",
    "start": "694079",
    "end": "701760"
  },
  {
    "text": "schedule some have you don't change the data much and we can add",
    "start": "701760",
    "end": "706959"
  },
  {
    "text": "higher longer periods and of course to store all the vendor specific parameters",
    "start": "706959",
    "end": "712480"
  },
  {
    "text": "needed for the storage application so i will recover and look",
    "start": "712480",
    "end": "719680"
  },
  {
    "start": "716000",
    "end": "716000"
  },
  {
    "text": "so already have a validification created in clusterings and we",
    "start": "719680",
    "end": "725440"
  },
  {
    "text": "define and that previous primary in clusters now that we we know",
    "start": "725440",
    "end": "732000"
  },
  {
    "text": "cluster is held we want to recover to plaster waste we'll create a new volume applications",
    "start": "732000",
    "end": "738639"
  },
  {
    "text": "resource in cluster waste mark this replication state primary",
    "start": "738639",
    "end": "743760"
  },
  {
    "text": "use the same data source the same ppc as we did in class to east",
    "start": "743760",
    "end": "749200"
  },
  {
    "text": "the site i would detect the change and use grpc command and to communicate with",
    "start": "749200",
    "end": "754959"
  },
  {
    "text": "the csi that we the storage and will cross promote",
    "start": "754959",
    "end": "760800"
  },
  {
    "text": "and first of all making it writable and primary",
    "start": "760800",
    "end": "766639"
  },
  {
    "text": "we locate on the haddad we require changes on both cluster so there are",
    "start": "766639",
    "end": "772880"
  },
  {
    "text": "applications that we need to change the secondary in cluster west and",
    "start": "772880",
    "end": "778880"
  },
  {
    "text": "primary cluster is and again uh the side cut will involve side will do",
    "start": "778880",
    "end": "785120"
  },
  {
    "text": "the reconciliation and then the drivers in both sides will apply the needed changes",
    "start": "785120",
    "end": "791279"
  },
  {
    "text": "are we done well this will work great if the pvc is statically",
    "start": "791279",
    "end": "798639"
  },
  {
    "text": "what about dynamic provisioning so here we have a problem the previous year provision dynamically",
    "start": "798639",
    "end": "806000"
  },
  {
    "start": "800000",
    "end": "800000"
  },
  {
    "text": "when we want um recover for example we need to attach",
    "start": "806000",
    "end": "812160"
  },
  {
    "text": "previously not to a neodymically provision",
    "start": "812160",
    "end": "817360"
  },
  {
    "text": "npvc but to ex through an existing pvp",
    "start": "817360",
    "end": "823040"
  },
  {
    "text": "the replicated one so we are actually create a matching pv in cluster west",
    "start": "823040",
    "end": "830959"
  },
  {
    "text": "that is connected to the volume that we replicate to cluster with then when we recover",
    "start": "830959",
    "end": "838000"
  },
  {
    "text": "we want use the data source and but we need a way to",
    "start": "838000",
    "end": "844639"
  },
  {
    "text": "tell kubernetes we want to use the existing pv so we want to connect between",
    "start": "844639",
    "end": "850639"
  },
  {
    "text": "the pv and the dynamically provision previously so now we won't do a dynamic provision",
    "start": "850639",
    "end": "856720"
  },
  {
    "text": "in the static we connect the previously to develop the pv",
    "start": "856720",
    "end": "862639"
  },
  {
    "text": "that it's connected to the replicated volume",
    "start": "862639",
    "end": "867440"
  },
  {
    "text": "if we talk about about relocate that will require also and",
    "start": "868880",
    "end": "875839"
  },
  {
    "text": "changing both the volume application on both sides",
    "start": "876079",
    "end": "881440"
  },
  {
    "text": "we are using a volume handle and a crosstalk system to identify those pv",
    "start": "881440",
    "end": "888480"
  },
  {
    "text": "and it works well for us since with soft csi but it may be a",
    "start": "888480",
    "end": "894639"
  },
  {
    "text": "problem to other csi or storage system and we need to standardize those",
    "start": "894639",
    "end": "902000"
  },
  {
    "text": "now let's go to sam we will give you more detail about multi-cluster manager",
    "start": "902639",
    "end": "911320"
  },
  {
    "text": "thanks eric let's talk about multi-cluster management so we've seen that disaster recovery",
    "start": "916320",
    "end": "922720"
  },
  {
    "text": "requires multiple clusters peer clusters so that workloads can be recovered or relocated across each other",
    "start": "922720",
    "end": "929839"
  },
  {
    "start": "923000",
    "end": "923000"
  },
  {
    "text": "which basically means we need clusters that are configured equivalent so that the applications can run across these",
    "start": "929839",
    "end": "935920"
  },
  {
    "text": "clusters we also need that any custom resources that these applications use their",
    "start": "935920",
    "end": "941519"
  },
  {
    "text": "operators and custom resources are deployed on both clusters or all clusters",
    "start": "941519",
    "end": "947120"
  },
  {
    "text": "and we've also looked at storage storage has to be set up across these clusters in an equivalent fashion",
    "start": "947120",
    "end": "953040"
  },
  {
    "text": "such that uh the volume replication class the storage class uh everything is equal and that the application can be",
    "start": "953040",
    "end": "960240"
  },
  {
    "text": "redeployed onto a target cluster so from a cross-cluster action perspective cluster configuration is",
    "start": "960240",
    "end": "965360"
  },
  {
    "text": "something that we want uh to maintain across these clusters from a user perspective for",
    "start": "965360",
    "end": "971440"
  },
  {
    "text": "application recovery and relocation users would need access to congruent namespaces in these clusters where they",
    "start": "971440",
    "end": "977759"
  },
  {
    "text": "can place their workloads they would also need a declarative copy of their resources the application",
    "start": "977759",
    "end": "983600"
  },
  {
    "text": "manifests that they can recreate on these clusters and finally they would need to reroute",
    "start": "983600",
    "end": "989600"
  },
  {
    "text": "their global traffic manager or the inbound traffic to the cluster that's actually running that workload at any",
    "start": "989600",
    "end": "996480"
  },
  {
    "text": "given instant further than this across these clusters we also need a level of health",
    "start": "996480",
    "end": "1002800"
  },
  {
    "text": "monitoring for alerting when a cluster is available or unavailable for various reasons",
    "start": "1002800",
    "end": "1008800"
  },
  {
    "text": "and finally to do the recovery relocation orchestration uh which is a cross-cluster action",
    "start": "1008800",
    "end": "1015279"
  },
  {
    "text": "we need to manage multiple kubernetes clusters we chose the open cluster management",
    "start": "1015279",
    "end": "1021040"
  },
  {
    "start": "1020000",
    "end": "1020000"
  },
  {
    "text": "project which as the tagline goes is a community driven project focused on multi-cluster and multi-cloud scenarios",
    "start": "1021040",
    "end": "1027760"
  },
  {
    "text": "for communities apps works perfectly for us we are looking at multi-cluster and managing apps across",
    "start": "1027760",
    "end": "1033678"
  },
  {
    "text": "multiple clusters they provide a cluster registry distribution of work across clusters in",
    "start": "1033679",
    "end": "1039199"
  },
  {
    "text": "the registry placement of content in a vendor neutral api manner which again is useful for",
    "start": "1039199",
    "end": "1045760"
  },
  {
    "text": "a wider adoption we leverage open cluster management primarily",
    "start": "1045760",
    "end": "1051039"
  },
  {
    "text": "for the cluster configuration and more importantly to manage the application life cycle so",
    "start": "1051039",
    "end": "1056720"
  },
  {
    "text": "we need application manifests coming from a declarative source and ocm",
    "start": "1056720",
    "end": "1062640"
  },
  {
    "text": "has a channel crd which is basically pointing to a git helm or an object store for the declarative copy of the",
    "start": "1062640",
    "end": "1069760"
  },
  {
    "text": "application manifests ocm also has a placement rule crd which",
    "start": "1069760",
    "end": "1075440"
  },
  {
    "text": "decides where an application which clusters an application should be placed too and so that's leveraged uh so that we",
    "start": "1075440",
    "end": "1082480"
  },
  {
    "text": "can do the orchestration during disaster recovery by by scheduling the placement rule appropriately",
    "start": "1082480",
    "end": "1088480"
  },
  {
    "text": "there are gaps uh disaster recovery orchestration is not a part of it they are more around stateless apps",
    "start": "1088480",
    "end": "1094640"
  },
  {
    "text": "uh which is where we come in and we'll talk about what we're going to do about that so for disaster recovery orchestration",
    "start": "1094640",
    "end": "1101440"
  },
  {
    "text": "uh although we have these various components what does it mean for a user and how",
    "start": "1101440",
    "end": "1106720"
  },
  {
    "text": "easy or complex is it so as it stands it actually becomes quite complex as we look at three use",
    "start": "1106720",
    "end": "1112720"
  },
  {
    "start": "1108000",
    "end": "1108000"
  },
  {
    "text": "cases of deploy relocate and recover let's start at deploy so for example the",
    "start": "1112720",
    "end": "1118480"
  },
  {
    "text": "user would have to deploy their application resources to a cluster let's say east in this particular example",
    "start": "1118480",
    "end": "1124320"
  },
  {
    "text": "we'd have to create volume replication resources as primary to establish volume replication for every pvc in this",
    "start": "1124320",
    "end": "1131039"
  },
  {
    "text": "particular name space they would have to ensure that the volume replication is occurring before",
    "start": "1131039",
    "end": "1137039"
  },
  {
    "text": "they decide to back up the pv cluster data because that's what's going to be used in the alternate cluster as orig",
    "start": "1137039",
    "end": "1143520"
  },
  {
    "text": "mentioned to reattach to the volume in the storage backend and finally if there's a new pvc that's",
    "start": "1143520",
    "end": "1149840"
  },
  {
    "text": "created by the application in the namespace they'll have to repeat the protection",
    "start": "1149840",
    "end": "1155039"
  },
  {
    "text": "for that particular pc not too bad uh but let's go to recover",
    "start": "1155039",
    "end": "1161440"
  },
  {
    "start": "1160000",
    "end": "1160000"
  },
  {
    "text": "and see how it starts increasing in complexity so in the event that the east cluster goes down the user would have to",
    "start": "1161440",
    "end": "1167919"
  },
  {
    "text": "first recover the workload on west and hence have to restore the pv cluster data",
    "start": "1167919",
    "end": "1173760"
  },
  {
    "text": "first so that the pvcs that are part of the application manifests as",
    "start": "1173760",
    "end": "1178880"
  },
  {
    "text": "they are restored reattached to the respective tvs and then they would have to create the volume replication resource to mark",
    "start": "1178880",
    "end": "1185360"
  },
  {
    "text": "these against these pvs so that they can be marked primary and they're ready for use on the west cluster",
    "start": "1185360",
    "end": "1191520"
  },
  {
    "text": "technically at the at the point where we've covered these steps the rto goal is met we have recovered the application",
    "start": "1191520",
    "end": "1198960"
  },
  {
    "text": "in some time but it doesn't end there when east cluster recovers uh in this particular",
    "start": "1198960",
    "end": "1204640"
  },
  {
    "text": "case east is temporarily offline let's say we would have to ensure that the application resources at least are",
    "start": "1204640",
    "end": "1211039"
  },
  {
    "text": "deleted the volume replication resource is marked as secondary so that it starts re-syncing data from the new primary on",
    "start": "1211039",
    "end": "1217919"
  },
  {
    "text": "west and then once resync is established we can delete the volume replication",
    "start": "1217919",
    "end": "1223360"
  },
  {
    "text": "resource which would in turn delete the pvc resource and free up uh east cluster",
    "start": "1223360",
    "end": "1228559"
  },
  {
    "text": "and of course once this happens as usual we're gonna have to protect any new pvcs that appear on west as part of the",
    "start": "1228559",
    "end": "1234720"
  },
  {
    "text": "application's namespace relocate increases the complexity",
    "start": "1234720",
    "end": "1239840"
  },
  {
    "start": "1237000",
    "end": "1237000"
  },
  {
    "text": "because we need to uh ensure first we undeploy or you know remove the",
    "start": "1239840",
    "end": "1245919"
  },
  {
    "text": "application from the west cluster for which we need to make sure we delete the application resources on",
    "start": "1245919",
    "end": "1252240"
  },
  {
    "text": "rest we need to ensure the vr volume replications mark the secondary we need",
    "start": "1252240",
    "end": "1257679"
  },
  {
    "text": "to wait for the final sync of data to be reported by volume replication resource because relocate is when both clusters",
    "start": "1257679",
    "end": "1264480"
  },
  {
    "text": "are active and recovery point objective is zero in other words we need all the data",
    "start": "1264480",
    "end": "1269520"
  },
  {
    "text": "and once the final resync is complete is detected we can get rid of the volume replication resource etc on the west",
    "start": "1269520",
    "end": "1276080"
  },
  {
    "text": "cluster but then we can start bringing up the application on the east cluster restoring the pvs first recreating the",
    "start": "1276080",
    "end": "1283039"
  },
  {
    "text": "apps recreating volume replication as primary so there are quite a few steps to perform",
    "start": "1283039",
    "end": "1289360"
  },
  {
    "text": "these actions and so what we did next was to create a dr",
    "start": "1289360",
    "end": "1295360"
  },
  {
    "start": "1294000",
    "end": "1294000"
  },
  {
    "text": "orchestrator called ramen what raman does as per its tagline is",
    "start": "1295360",
    "end": "1300799"
  },
  {
    "text": "instant cloud native workload recovery and relocation across communities clusters yes we are interested in recovery and",
    "start": "1300799",
    "end": "1307280"
  },
  {
    "text": "relocation cross-cube clusters that's the use case we talked about that's what random helps us with",
    "start": "1307280",
    "end": "1312799"
  },
  {
    "text": "raman basically enhances the ocm placement rule scheduler by adding its",
    "start": "1312799",
    "end": "1319840"
  },
  {
    "text": "own scheduler so that it can orchestrate workload placement in the ocm control plane",
    "start": "1319840",
    "end": "1326559"
  },
  {
    "text": "it further also provides for label selectors for pvcs that need protection",
    "start": "1326559",
    "end": "1331840"
  },
  {
    "text": "and auto creates volume replication relationships for those pvcs as primary or secondary and manages the replication",
    "start": "1331840",
    "end": "1338480"
  },
  {
    "text": "state so that users do not have to worry about dynamically created",
    "start": "1338480",
    "end": "1343679"
  },
  {
    "text": "pvcs for example when stateful stats set certain use so raman provides two apis the first one",
    "start": "1343679",
    "end": "1351440"
  },
  {
    "start": "1349000",
    "end": "1349000"
  },
  {
    "text": "being a dr policy api which is a claustroscope resource which talks about which pair of clusters",
    "start": "1351440",
    "end": "1358240"
  },
  {
    "text": "are in a dr relationship so that we know that the cluster configuration is equivalent storage is set up as needed",
    "start": "1358240",
    "end": "1363919"
  },
  {
    "text": "across these two peer clusters it defines a replication schedule which basically defines the recovery point",
    "start": "1363919",
    "end": "1369600"
  },
  {
    "text": "objective for the uh for the application that wants to use this particular policy and it optionally has a volume",
    "start": "1369600",
    "end": "1375919"
  },
  {
    "text": "replication class selector to disambiguate in case there are multiple volume replication classes with the same",
    "start": "1375919",
    "end": "1381120"
  },
  {
    "text": "schedule on the on the clusters the dr policy object is a cluster scope",
    "start": "1381120",
    "end": "1386240"
  },
  {
    "text": "object set up by the administrator and then the next api which is the dr",
    "start": "1386240",
    "end": "1391440"
  },
  {
    "start": "1390000",
    "end": "1390000"
  },
  {
    "text": "placement control api is per application and the namespace resource which basically helps control the",
    "start": "1391440",
    "end": "1398159"
  },
  {
    "text": "placement and orchestration of an application across these clusters so it reconciles the placement rule",
    "start": "1398159",
    "end": "1404880"
  },
  {
    "text": "that's referenced by spec placement ref in the dr policy uh placement control",
    "start": "1404880",
    "end": "1409919"
  },
  {
    "text": "api it refers to the dr policy so that it knows what the scheduling interval is",
    "start": "1409919",
    "end": "1416080"
  },
  {
    "text": "and or which clusters this workload can be placed on it auto protects pvcs based on the pvc",
    "start": "1416080",
    "end": "1422000"
  },
  {
    "text": "selector that's provided in its definition and its spec and it provides",
    "start": "1422000",
    "end": "1427039"
  },
  {
    "text": "for two actions recovery action or a failover action which",
    "start": "1427039",
    "end": "1432559"
  },
  {
    "text": "moves the workload to the failover cluster and the relocate action which relocates the workload to the preferred",
    "start": "1432559",
    "end": "1439279"
  },
  {
    "text": "cluster initially a preferred cluster could be specified if a particular region is desired for an application",
    "start": "1439279",
    "end": "1445679"
  },
  {
    "text": "or left empty and dynamic lineage scheduled on on either cluster",
    "start": "1445679",
    "end": "1450960"
  },
  {
    "text": "and the referred to in the policy their policy reference",
    "start": "1450960",
    "end": "1456400"
  },
  {
    "text": "so just this di placement control api we could relocate or recover uh an application and we'll soon see a demo of",
    "start": "1456400",
    "end": "1462880"
  },
  {
    "text": "that but before we go there uh how's raman deployed on these various clusters so",
    "start": "1462880",
    "end": "1468320"
  },
  {
    "start": "1464000",
    "end": "1464000"
  },
  {
    "text": "raman has two operators one of the operators is running on the ocm hub cluster which is the",
    "start": "1468320",
    "end": "1474320"
  },
  {
    "text": "multi-cluster orchestration plane uh where the dr policy and the dr placement control objects are created",
    "start": "1474320",
    "end": "1481919"
  },
  {
    "text": "its responsibilities obviously to reconcile the dr placement control object on the managed clusters where the",
    "start": "1481919",
    "end": "1487919"
  },
  {
    "text": "workload should be running there is an additional volume replication group api resource that is present and managed by",
    "start": "1487919",
    "end": "1494080"
  },
  {
    "text": "the raman cluster operator we're not going into that because it's auto managed by raman",
    "start": "1494080",
    "end": "1499279"
  },
  {
    "text": "but volume replication group api is the one that helps detect uh new pvcs that",
    "start": "1499279",
    "end": "1504559"
  },
  {
    "text": "are created and are when pvcs are no longer in use so that a final sync is initiated with volume replication",
    "start": "1504559",
    "end": "1512080"
  },
  {
    "text": "and you know all in cluster activities are managed by the ones in replication group api which is controlled by the dia",
    "start": "1512080",
    "end": "1518159"
  },
  {
    "text": "placement control api at the hub so with that uh let's go into a quick",
    "start": "1518159",
    "end": "1523679"
  },
  {
    "text": "demo what we have here is a demonstration of how we control the location based on tia",
    "start": "1523679",
    "end": "1530559"
  },
  {
    "text": "placement control we have three clusters east west",
    "start": "1530559",
    "end": "1535840"
  },
  {
    "text": "and the hub cluster the hub cluster being the raman hub east and west being to manage clusters where applications",
    "start": "1535840",
    "end": "1542799"
  },
  {
    "text": "would be deployed to what we've actually done is we've already deployed the application on the",
    "start": "1542799",
    "end": "1549200"
  },
  {
    "text": "east cluster the application that's been deployed is a busy box pod in the busy boxing sample",
    "start": "1549200",
    "end": "1555840"
  },
  {
    "text": "name space it very simply echoes the current timestamp to a file every 10 seconds",
    "start": "1555840",
    "end": "1562240"
  },
  {
    "text": "and the file is actually an amount point which is backed by a pvc which is backed by cfrbd which is set up across east and",
    "start": "1562240",
    "end": "1569360"
  },
  {
    "text": "west for storage replication we deployed this so obviously the dr placement control on the hub is",
    "start": "1569360",
    "end": "1575919"
  },
  {
    "text": "responsible for having deployed this particular resource let's take a look at that",
    "start": "1575919",
    "end": "1582559"
  },
  {
    "text": "it basically has a dr policy which is the policy has east and west clusters in it it is going to reconcile the busy box",
    "start": "1582559",
    "end": "1589120"
  },
  {
    "text": "placement placement rule and finally it has a preferred cluster mentioned in it",
    "start": "1589120",
    "end": "1595440"
  },
  {
    "text": "which which tells it which cluster to prefer and it was east and that's why it's deployed it on east and it's",
    "start": "1595440",
    "end": "1600720"
  },
  {
    "text": "already set up volume replication and other such requirements for the pvc in use and the user doesn't have to worry about",
    "start": "1600720",
    "end": "1606720"
  },
  {
    "text": "those things now as we are writing timestamps to the file and we",
    "start": "1606720",
    "end": "1612320"
  },
  {
    "text": "want to demonstrate recovery let's take a look let's tail the part of dzbox parts that we know the timestamps",
    "start": "1612320",
    "end": "1618799"
  },
  {
    "text": "as we come back to the hub to perform a recovery to the west cluster",
    "start": "1618799",
    "end": "1624640"
  },
  {
    "text": "and we'll see what kind of data loss we encountered so to perform a recovery of the uh",
    "start": "1624640",
    "end": "1631200"
  },
  {
    "text": "workload on the best cluster we need to edit the dr placement control",
    "start": "1631200",
    "end": "1636320"
  },
  {
    "text": "and we need to provide it with an action of uh failover",
    "start": "1636320",
    "end": "1641840"
  },
  {
    "text": "and we need to give it a failover cluster and that failover cluster in this particular case would be west so",
    "start": "1641840",
    "end": "1647760"
  },
  {
    "text": "the workload can move from east to west without any coordination it assumes east is down although east it's",
    "start": "1647760",
    "end": "1653520"
  },
  {
    "text": "really not down in the example so now that we've committed that diamond started moving it",
    "start": "1653520",
    "end": "1658880"
  },
  {
    "text": "watching for the busy box part on west and once that comes into running state we'll take a look at the data",
    "start": "1658880",
    "end": "1665279"
  },
  {
    "text": "speeding up we see the last series of timestamps",
    "start": "1665279",
    "end": "1671120"
  },
  {
    "text": "that have been dropped on the east cluster which is what we compare in the west cluster and on the west cluster we see that the",
    "start": "1671120",
    "end": "1677279"
  },
  {
    "text": "part is already running so let's take a look at the data within the pod and see what data we lost because this",
    "start": "1677279",
    "end": "1684240"
  },
  {
    "text": "was a recovery operation failover operation that we performed so looking at out file here",
    "start": "1684240",
    "end": "1690880"
  },
  {
    "text": "we see that uh on east we actually wrote 3755 time",
    "start": "1690880",
    "end": "1696799"
  },
  {
    "text": "stamp and a series of 38 time stamps and a 39 time stamp and",
    "start": "1696799",
    "end": "1702640"
  },
  {
    "text": "so on the on the west cluster we look at uh what",
    "start": "1702640",
    "end": "1710720"
  },
  {
    "text": "what the timestamps are and how much of",
    "start": "1710720",
    "end": "1715840"
  },
  {
    "text": "data did we lose so if you look at it here we got 3755 time",
    "start": "1715840",
    "end": "1721120"
  },
  {
    "text": "stamp we lost the series of 38 and 139 which was about a minute worth of data",
    "start": "1721120",
    "end": "1726159"
  },
  {
    "text": "that lost that was lost the replication schedule that we've actually set up for this demo is one minute so it lost about",
    "start": "1726159",
    "end": "1732480"
  },
  {
    "text": "a minute of data from east it recovered it started running again at 39.13 so the recovery time was",
    "start": "1732480",
    "end": "1739520"
  },
  {
    "text": "approximately a minute and 20 seconds give or take and how did we achieve this we basically",
    "start": "1739520",
    "end": "1746640"
  },
  {
    "text": "uh edited the dr placement control for an action of failover and gave it a failover cluster and let the entire",
    "start": "1746640",
    "end": "1752240"
  },
  {
    "text": "orchestration happen for that same the uh next uh",
    "start": "1752240",
    "end": "1759120"
  },
  {
    "text": "part of the demo we're going to do a relocate where we're going to show zero data laws so let's start tailing the busy box board on the west cluster",
    "start": "1759120",
    "end": "1766559"
  },
  {
    "text": "and in this particular case we're just going to see that there's zero data loss so we need all time stamps back on east once",
    "start": "1766559",
    "end": "1773279"
  },
  {
    "text": "we relocate it and to relocate it we go ahead we edit the dr placement control object again",
    "start": "1773279",
    "end": "1779440"
  },
  {
    "text": "this time around we change the action instead of a failover we're going to say relocate",
    "start": "1779440",
    "end": "1784799"
  },
  {
    "text": "we already have a preferred cluster of east down there so it's just going to relocate to the east cluster the",
    "start": "1784799",
    "end": "1790320"
  },
  {
    "text": "failover cluster is still in there but that's immaterial because it's not going to be used for this purpose",
    "start": "1790320",
    "end": "1797120"
  },
  {
    "text": "so moving on watching for the pod on the east cluster which is where it's getting relocated to just retail the logs on the",
    "start": "1797120",
    "end": "1804159"
  },
  {
    "text": "west cluster to make sure all time stamps made it after some time we find the pod is",
    "start": "1804159",
    "end": "1810640"
  },
  {
    "text": "running on the east cluster so uh what we're really looking for is the 4309 timestamp should still be present",
    "start": "1810640",
    "end": "1818799"
  },
  {
    "text": "in the east cluster to ensure rpo of zero",
    "start": "1818799",
    "end": "1823840"
  },
  {
    "text": "so let's get the logs uh sorry get the content of the volume in the east cluster",
    "start": "1823840",
    "end": "1829039"
  },
  {
    "text": "and take a look at the timestamps so we were looking at 4309",
    "start": "1829039",
    "end": "1835600"
  },
  {
    "text": "and on the east cluster we have 4309 uh times time present and if you look at",
    "start": "1835600",
    "end": "1840880"
  },
  {
    "text": "it the next time stamp is 44 39 so it took about a minute and 30 seconds which",
    "start": "1840880",
    "end": "1845919"
  },
  {
    "text": "is your rto although this is not purely recovery but that is an application downtime during relocation",
    "start": "1845919",
    "end": "1852799"
  },
  {
    "text": "and how did we do this uh again we just edited the uh dr",
    "start": "1852799",
    "end": "1857919"
  },
  {
    "text": "placement control changed the action to relocate it had a preferred cluster and moved over there so tr placement control",
    "start": "1857919",
    "end": "1865919"
  },
  {
    "text": "basically simplifies the tip of the iceberg simplifies the complexity of the remaining parts of the stack that we are",
    "start": "1865919",
    "end": "1870960"
  },
  {
    "text": "using volume replication storage application management with ocm",
    "start": "1870960",
    "end": "1876480"
  },
  {
    "text": "and provides an easy to use interface to actually orchestrate relocation and recovery",
    "start": "1876480",
    "end": "1883519"
  },
  {
    "text": "let's look at uh future work in this area",
    "start": "1883679",
    "end": "1889360"
  },
  {
    "start": "1889000",
    "end": "1889000"
  },
  {
    "text": "so what do we want to do next we talked about regional dr that is the metro dr case where storage",
    "start": "1889360",
    "end": "1895519"
  },
  {
    "text": "the assumption is storage replication is synchronous so there is no data loss there is rpo",
    "start": "1895519",
    "end": "1901440"
  },
  {
    "text": "is zero but we will still need to recover and relocate applications across these clusters and we might want to type a",
    "start": "1901440",
    "end": "1907919"
  },
  {
    "text": "storage fencing when we know that a particular cluster is unavailable because it may still be accessing storage and we do not want multiple",
    "start": "1907919",
    "end": "1915039"
  },
  {
    "text": "writers to the same storage endpoint corrupting the data so that would be an interesting use case to tackle with a",
    "start": "1915039",
    "end": "1920799"
  },
  {
    "text": "same or a similar stack uh upstream data protection working group",
    "start": "1920799",
    "end": "1926640"
  },
  {
    "text": "seg is actually provided any volume data source feature gate which allows a pvc to be created by an",
    "start": "1926640",
    "end": "1933039"
  },
  {
    "text": "arbitrary kind in the communities cluster we want to leverage that so that we can use volume replication as a kind to",
    "start": "1933039",
    "end": "1939679"
  },
  {
    "text": "create a volume to create a pvc from which can help us move away from",
    "start": "1939679",
    "end": "1945279"
  },
  {
    "text": "copying pv across and static provisioning and assuming that the pvs csi handle volume handle",
    "start": "1945279",
    "end": "1952799"
  },
  {
    "text": "across storage vendors across clusters can be reused in the same fashion",
    "start": "1952799",
    "end": "1958559"
  },
  {
    "text": "from a replication consistency perspective the storage replication is based on",
    "start": "1958559",
    "end": "1963679"
  },
  {
    "text": "storage snapshots and hence it is crash consistent it's not necessarily application consistent work going on in",
    "start": "1963679",
    "end": "1969279"
  },
  {
    "text": "the dp data protection sig for application consistent snapshots we need to integrate or leverage that to",
    "start": "1969279",
    "end": "1976320"
  },
  {
    "text": "provide application consistent replication snapshots so that they replicate it as",
    "start": "1976320",
    "end": "1981760"
  },
  {
    "text": "required to further improve consistency uh applications do not always use a single",
    "start": "1981760",
    "end": "1988559"
  },
  {
    "text": "pvcs they have a group of pvcs again there is work going on around volume groups and volume group snapshots in the",
    "start": "1988559",
    "end": "1995600"
  },
  {
    "text": "data protection sig and then that becomes a reality uh the",
    "start": "1995600",
    "end": "2000640"
  },
  {
    "text": "the replication of this these set of pvcs will also be point in time consistent across each",
    "start": "2000640",
    "end": "2006640"
  },
  {
    "text": "other providing better resilience and recovery for the application on the target clusters",
    "start": "2006640",
    "end": "2012880"
  },
  {
    "text": "we also want to move towards more data agnostic replication mechanisms for example proposals like the change block",
    "start": "2012880",
    "end": "2019120"
  },
  {
    "text": "tracking uh can help reduce rpo and transfer data across uh various storage",
    "start": "2019120",
    "end": "2026399"
  },
  {
    "text": "systems they do not have to be in the same storage system so that can make us storage agnostic and address our peers",
    "start": "2026399",
    "end": "2031919"
  },
  {
    "text": "instead of backups using backups address rpos using change blocks across snapshots",
    "start": "2031919",
    "end": "2037760"
  },
  {
    "text": "i will also look at providing more arbitrary replication scheme than volume replication so that",
    "start": "2037760",
    "end": "2044000"
  },
  {
    "text": "data can be transferred across peer clusters and the storage systems probably don't have to be the same it",
    "start": "2044000",
    "end": "2050079"
  },
  {
    "text": "kind of provides for a hybrid approach in the future and with that",
    "start": "2050079",
    "end": "2055520"
  },
  {
    "start": "2055000",
    "end": "2055000"
  },
  {
    "text": "here are some links and references for the various components that we talked about in this presentation and used to",
    "start": "2055520",
    "end": "2061599"
  },
  {
    "text": "build up ramen and volume replication please do take a look",
    "start": "2061599",
    "end": "2067358"
  },
  {
    "text": "participate as the case may be and thank you at enza presentation and we",
    "start": "2067359",
    "end": "2073358"
  },
  {
    "text": "are open for any questions that you may have",
    "start": "2073359",
    "end": "2077960"
  }
]