[
  {
    "text": "good afternoon thanks for hanging in",
    "start": "640",
    "end": "2240"
  },
  {
    "text": "there it's getting pretty close to the",
    "start": "2240",
    "end": "3760"
  },
  {
    "text": "end of the day",
    "start": "3760",
    "end": "4880"
  },
  {
    "text": "um i'm anne holler and i'm happy to be",
    "start": "4880",
    "end": "7120"
  },
  {
    "text": "here with travis on an mp4 that he",
    "start": "7120",
    "end": "10080"
  },
  {
    "text": "recorded uh earlier uh to present uh",
    "start": "10080",
    "end": "13920"
  },
  {
    "text": "efficient deep learning training with",
    "start": "13920",
    "end": "15360"
  },
  {
    "text": "ludwig automl",
    "start": "15360",
    "end": "16880"
  },
  {
    "text": "ray and nodeless kubernetes",
    "start": "16880",
    "end": "20000"
  },
  {
    "text": "uh i want to start off by just a shout",
    "start": "20000",
    "end": "22080"
  },
  {
    "text": "out to several",
    "start": "22080",
    "end": "23920"
  },
  {
    "text": "recent articles that contributed to",
    "start": "23920",
    "end": "26400"
  },
  {
    "text": "material that's in this presentation and",
    "start": "26400",
    "end": "28400"
  },
  {
    "text": "to the people from the ludwig ray and",
    "start": "28400",
    "end": "30560"
  },
  {
    "text": "the local communities that contributed",
    "start": "30560",
    "end": "32880"
  },
  {
    "text": "to these um",
    "start": "32880",
    "end": "34239"
  },
  {
    "text": "this material so the first is a recent",
    "start": "34239",
    "end": "36320"
  },
  {
    "text": "cncf block from february on managing",
    "start": "36320",
    "end": "38559"
  },
  {
    "text": "public cloud resources for deep learning",
    "start": "38559",
    "end": "40480"
  },
  {
    "text": "training the second is a medium blog",
    "start": "40480",
    "end": "43280"
  },
  {
    "text": "from that same month on lidwig automl",
    "start": "43280",
    "end": "46000"
  },
  {
    "text": "for deep learning this was focused on",
    "start": "46000",
    "end": "48160"
  },
  {
    "text": "tabular data sets",
    "start": "48160",
    "end": "50079"
  },
  {
    "text": "and then thirdly our presentation from",
    "start": "50079",
    "end": "52800"
  },
  {
    "text": "cloud native rejects this past fall",
    "start": "52800",
    "end": "54800"
  },
  {
    "text": "where we created a poc for running ray",
    "start": "54800",
    "end": "57520"
  },
  {
    "text": "on public cloud kubernetes",
    "start": "57520",
    "end": "59760"
  },
  {
    "text": "so",
    "start": "59760",
    "end": "60559"
  },
  {
    "text": "without further ado let's get on with it",
    "start": "60559",
    "end": "62320"
  },
  {
    "text": "so deep learning has been applied to",
    "start": "62320",
    "end": "64080"
  },
  {
    "text": "many fields but it's well known to be uh",
    "start": "64080",
    "end": "66799"
  },
  {
    "text": "complex to get it from you know planning",
    "start": "66799",
    "end": "69680"
  },
  {
    "text": "to development to production",
    "start": "69680",
    "end": "71840"
  },
  {
    "text": "ray and ludwig open source systems",
    "start": "71840",
    "end": "74320"
  },
  {
    "text": "gladly reduce the complexity barriers to",
    "start": "74320",
    "end": "76640"
  },
  {
    "text": "training scaling deploying and serving",
    "start": "76640",
    "end": "78799"
  },
  {
    "text": "deep learning",
    "start": "78799",
    "end": "80080"
  },
  {
    "text": "however even when complexity barriers",
    "start": "80080",
    "end": "82240"
  },
  {
    "text": "are reduced the cost and operational",
    "start": "82240",
    "end": "84960"
  },
  {
    "text": "overhead of deep learning presents",
    "start": "84960",
    "end": "86799"
  },
  {
    "text": "significant challenges",
    "start": "86799",
    "end": "88479"
  },
  {
    "text": "so deep learning intermittently needs",
    "start": "88479",
    "end": "90560"
  },
  {
    "text": "substantial gpu resources public cloud",
    "start": "90560",
    "end": "93439"
  },
  {
    "text": "vendors are perfectly happy to provide",
    "start": "93439",
    "end": "95600"
  },
  {
    "text": "those but at non-trivial prices",
    "start": "95600",
    "end": "98560"
  },
  {
    "text": "so managing gpu resources and",
    "start": "98560",
    "end": "100479"
  },
  {
    "text": "operational overhead is critical to",
    "start": "100479",
    "end": "102799"
  },
  {
    "text": "practical use of deep learning",
    "start": "102799",
    "end": "105600"
  },
  {
    "text": "um",
    "start": "105600",
    "end": "106799"
  },
  {
    "text": "lodal's nodeless kubernetes nicknamed",
    "start": "106799",
    "end": "108880"
  },
  {
    "text": "luna commoditizes compute for kubernetes",
    "start": "108880",
    "end": "112479"
  },
  {
    "text": "clusters so it provisions just-in-time",
    "start": "112479",
    "end": "115280"
  },
  {
    "text": "right-sized cost-effective compute for",
    "start": "115280",
    "end": "117840"
  },
  {
    "text": "kubernetes applications when they start",
    "start": "117840",
    "end": "120320"
  },
  {
    "text": "and removes those resources from the",
    "start": "120320",
    "end": "122079"
  },
  {
    "text": "kubernetes cluster when they end so its",
    "start": "122079",
    "end": "124399"
  },
  {
    "text": "purpose is to manage public cloud",
    "start": "124399",
    "end": "127360"
  },
  {
    "text": "resources judiciously so bringing all",
    "start": "127360",
    "end": "129599"
  },
  {
    "text": "this stuff together this talk is on",
    "start": "129599",
    "end": "131840"
  },
  {
    "text": "running ray and ludwig on cloud",
    "start": "131840",
    "end": "134000"
  },
  {
    "text": "kubernetes clusters using luna as a",
    "start": "134000",
    "end": "136400"
  },
  {
    "text": "smart cluster provisioner",
    "start": "136400",
    "end": "138319"
  },
  {
    "text": "and so we'll look at experiments using",
    "start": "138319",
    "end": "140400"
  },
  {
    "text": "ludwig automl deep learning training as",
    "start": "140400",
    "end": "143440"
  },
  {
    "text": "the experimental",
    "start": "143440",
    "end": "145360"
  },
  {
    "text": "workload that shows sizeable",
    "start": "145360",
    "end": "147280"
  },
  {
    "text": "improvements in efficiency and usability",
    "start": "147280",
    "end": "149599"
  },
  {
    "text": "versus the way i was running lidwig",
    "start": "149599",
    "end": "151599"
  },
  {
    "text": "automl deep learning training prior to",
    "start": "151599",
    "end": "153840"
  },
  {
    "text": "setting it up this way so compared to my",
    "start": "153840",
    "end": "156000"
  },
  {
    "text": "prior way uh decreased",
    "start": "156000",
    "end": "158800"
  },
  {
    "text": "this",
    "start": "158800",
    "end": "159680"
  },
  {
    "text": "elapsed time was decreased by 61 percent",
    "start": "159680",
    "end": "161760"
  },
  {
    "text": "computing cost by 54",
    "start": "161760",
    "end": "163840"
  },
  {
    "text": "and idle rate cluster cost by 66",
    "start": "163840",
    "end": "166879"
  },
  {
    "text": "and lowered my operational complexity",
    "start": "166879",
    "end": "169200"
  },
  {
    "text": "and also retain the performance results",
    "start": "169200",
    "end": "171680"
  },
  {
    "text": "of the automl",
    "start": "171680",
    "end": "173680"
  },
  {
    "text": "so we'll start up let me now",
    "start": "173680",
    "end": "175840"
  },
  {
    "text": "turn it over to",
    "start": "175840",
    "end": "177920"
  },
  {
    "text": "um",
    "start": "177920",
    "end": "178879"
  },
  {
    "text": "travis",
    "start": "178879",
    "end": "180560"
  },
  {
    "text": "hi everyone thanks for coming to our",
    "start": "180560",
    "end": "182480"
  },
  {
    "text": "talk today my name is travis adair i'm",
    "start": "182480",
    "end": "185200"
  },
  {
    "text": "the cto of a company called predabase",
    "start": "185200",
    "end": "187200"
  },
  {
    "text": "building an enterprise low code machine",
    "start": "187200",
    "end": "189120"
  },
  {
    "text": "learning platform",
    "start": "189120",
    "end": "190800"
  },
  {
    "text": "built on top of lubric and today i'd",
    "start": "190800",
    "end": "192640"
  },
  {
    "text": "like to tell you a little bit about the",
    "start": "192640",
    "end": "194319"
  },
  {
    "text": "background behind the ludwig project and",
    "start": "194319",
    "end": "196959"
  },
  {
    "text": "how uh automl fits into the vision of",
    "start": "196959",
    "end": "199760"
  },
  {
    "text": "what we're doing with the open source",
    "start": "199760",
    "end": "201280"
  },
  {
    "text": "luba project",
    "start": "201280",
    "end": "203280"
  },
  {
    "text": "to start i want to present the",
    "start": "203280",
    "end": "205120"
  },
  {
    "text": "background on why",
    "start": "205120",
    "end": "206560"
  },
  {
    "text": "we believe that ludwig is a valuable",
    "start": "206560",
    "end": "208319"
  },
  {
    "text": "addition to the ml ecosystem",
    "start": "208319",
    "end": "210879"
  },
  {
    "text": "so our observation is that if you look",
    "start": "210879",
    "end": "213120"
  },
  {
    "text": "at the way ml is done in industry today",
    "start": "213120",
    "end": "215519"
  },
  {
    "text": "there are essentially two incomplete",
    "start": "215519",
    "end": "217680"
  },
  {
    "text": "options that are available to companies",
    "start": "217680",
    "end": "219760"
  },
  {
    "text": "and organizations that want to",
    "start": "219760",
    "end": "220959"
  },
  {
    "text": "operationalize ml on the one hand you",
    "start": "220959",
    "end": "223599"
  },
  {
    "text": "have low level apis like tensorflow and",
    "start": "223599",
    "end": "225760"
  },
  {
    "text": "pytorch that provide a great deal of",
    "start": "225760",
    "end": "227280"
  },
  {
    "text": "flexibility and on the other hand you",
    "start": "227280",
    "end": "229280"
  },
  {
    "text": "have traditional automl systems that",
    "start": "229280",
    "end": "231040"
  },
  {
    "text": "provide a lot of simplicity but neither",
    "start": "231040",
    "end": "233439"
  },
  {
    "text": "of them end up being ideal because",
    "start": "233439",
    "end": "235360"
  },
  {
    "text": "oftentimes",
    "start": "235360",
    "end": "236879"
  },
  {
    "text": "the low-level apis are difficult to",
    "start": "236879",
    "end": "239439"
  },
  {
    "text": "gain the production for non-expert users",
    "start": "239439",
    "end": "241439"
  },
  {
    "text": "while the automl systems end up being",
    "start": "241439",
    "end": "243920"
  },
  {
    "text": "these black boxes that you end up",
    "start": "243920",
    "end": "246000"
  },
  {
    "text": "graduating out of because they don't",
    "start": "246000",
    "end": "247519"
  },
  {
    "text": "always",
    "start": "247519",
    "end": "248720"
  },
  {
    "text": "solve the problem the first time around",
    "start": "248720",
    "end": "251360"
  },
  {
    "text": "and so when we look at",
    "start": "251360",
    "end": "253120"
  },
  {
    "text": "what we're doing with ludwig the core",
    "start": "253120",
    "end": "255760"
  },
  {
    "text": "insight is that we believe that there is",
    "start": "255760",
    "end": "257759"
  },
  {
    "text": "a third option that needs to be",
    "start": "257759",
    "end": "260160"
  },
  {
    "text": "explored which is the what we call",
    "start": "260160",
    "end": "261840"
  },
  {
    "text": "declarative machine learning systems",
    "start": "261840",
    "end": "264160"
  },
  {
    "text": "with declarative what we intend to do is",
    "start": "264160",
    "end": "266400"
  },
  {
    "text": "provide a high level of abstraction a",
    "start": "266400",
    "end": "268560"
  },
  {
    "text": "higher level abstraction that provides",
    "start": "268560",
    "end": "270080"
  },
  {
    "text": "the flexibility and automation use of",
    "start": "270080",
    "end": "272240"
  },
  {
    "text": "use of automl",
    "start": "272240",
    "end": "273840"
  },
  {
    "text": "while still giving you",
    "start": "273840",
    "end": "275360"
  },
  {
    "text": "the flexibility of lower level tools",
    "start": "275360",
    "end": "278960"
  },
  {
    "text": "like pytorch",
    "start": "278960",
    "end": "281280"
  },
  {
    "text": "and opening the door for non-experts to",
    "start": "281280",
    "end": "283440"
  },
  {
    "text": "harness the power of ml",
    "start": "283440",
    "end": "285759"
  },
  {
    "text": "uh without needing to resort to these",
    "start": "285759",
    "end": "288479"
  },
  {
    "text": "more granular tools",
    "start": "288479",
    "end": "290720"
  },
  {
    "text": "and the way that ludwig",
    "start": "290720",
    "end": "292960"
  },
  {
    "text": "works",
    "start": "292960",
    "end": "294080"
  },
  {
    "text": "to kind of make this declare division",
    "start": "294080",
    "end": "296080"
  },
  {
    "text": "possible",
    "start": "296080",
    "end": "297120"
  },
  {
    "text": "is",
    "start": "297120",
    "end": "297919"
  },
  {
    "text": "uh similar to kind of systems that",
    "start": "297919",
    "end": "299759"
  },
  {
    "text": "provide infrastructure as code i'm sure",
    "start": "299759",
    "end": "302000"
  },
  {
    "text": "people who in the kubernetes community",
    "start": "302000",
    "end": "303759"
  },
  {
    "text": "community are very familiar with",
    "start": "303759",
    "end": "305600"
  },
  {
    "text": "we provide yaml configurations that",
    "start": "305600",
    "end": "307919"
  },
  {
    "text": "declare declaratively define",
    "start": "307919",
    "end": "310400"
  },
  {
    "text": "uh models that you might wish to train",
    "start": "310400",
    "end": "312400"
  },
  {
    "text": "and so for example it's very easy to get",
    "start": "312400",
    "end": "314560"
  },
  {
    "text": "started in ludwig you just say here's a",
    "start": "314560",
    "end": "316560"
  },
  {
    "text": "yaml config saying what my input",
    "start": "316560",
    "end": "318400"
  },
  {
    "text": "features and their types are what my",
    "start": "318400",
    "end": "320479"
  },
  {
    "text": "output features and their types are and",
    "start": "320479",
    "end": "322639"
  },
  {
    "text": "then everything else the kind of how",
    "start": "322639",
    "end": "325199"
  },
  {
    "text": "will get filled in automatically on your",
    "start": "325199",
    "end": "327360"
  },
  {
    "text": "behalf",
    "start": "327360",
    "end": "328320"
  },
  {
    "text": "but at the same time we provide a lot of",
    "start": "328320",
    "end": "330720"
  },
  {
    "text": "expert level control as well so if you",
    "start": "330720",
    "end": "332720"
  },
  {
    "text": "say i want to use a specific type of",
    "start": "332720",
    "end": "334160"
  },
  {
    "text": "mall architecture to encode a particular",
    "start": "334160",
    "end": "336400"
  },
  {
    "text": "feature",
    "start": "336400",
    "end": "337280"
  },
  {
    "text": "if you want to use a particular learning",
    "start": "337280",
    "end": "338720"
  },
  {
    "text": "rate or regularization or dropout all",
    "start": "338720",
    "end": "341360"
  },
  {
    "text": "those options are available to you as",
    "start": "341360",
    "end": "343199"
  },
  {
    "text": "well as more advanced features like",
    "start": "343199",
    "end": "344639"
  },
  {
    "text": "hyper parameter search on any of the",
    "start": "344639",
    "end": "347120"
  },
  {
    "text": "different parameters within the config",
    "start": "347120",
    "end": "350160"
  },
  {
    "text": "and what makes this all possible is the",
    "start": "350160",
    "end": "351759"
  },
  {
    "text": "ludwig architecture so every input",
    "start": "351759",
    "end": "354240"
  },
  {
    "text": "feature and output feature in your data",
    "start": "354240",
    "end": "356160"
  },
  {
    "text": "set",
    "start": "356160",
    "end": "356880"
  },
  {
    "text": "passes through an architecture we call",
    "start": "356880",
    "end": "358560"
  },
  {
    "text": "ecd for encoder compiler decoder uh",
    "start": "358560",
    "end": "361759"
  },
  {
    "text": "every feature is pre-processed according",
    "start": "361759",
    "end": "364639"
  },
  {
    "text": "to",
    "start": "364639",
    "end": "365520"
  },
  {
    "text": "pre-processing rules that you can",
    "start": "365520",
    "end": "367199"
  },
  {
    "text": "configure in the in the emo config",
    "start": "367199",
    "end": "370160"
  },
  {
    "text": "and then encode it into a vector",
    "start": "370160",
    "end": "372639"
  },
  {
    "text": "which can be a machine learning model",
    "start": "372639",
    "end": "374800"
  },
  {
    "text": "pre-trained or otherwise or learned and",
    "start": "374800",
    "end": "377120"
  },
  {
    "text": "then all the different features are",
    "start": "377120",
    "end": "378560"
  },
  {
    "text": "combined into an embedding space",
    "start": "378560",
    "end": "381919"
  },
  {
    "text": "and then individual output features then",
    "start": "381919",
    "end": "385039"
  },
  {
    "text": "pass through a very similar decoding",
    "start": "385039",
    "end": "386800"
  },
  {
    "text": "step where we get the final prediction",
    "start": "386800",
    "end": "389120"
  },
  {
    "text": "and the benefit of this architecture is",
    "start": "389120",
    "end": "390800"
  },
  {
    "text": "that it provides a great deal of task",
    "start": "390800",
    "end": "392319"
  },
  {
    "text": "flexibility without a lot of additional",
    "start": "392319",
    "end": "394160"
  },
  {
    "text": "complexities so",
    "start": "394160",
    "end": "395680"
  },
  {
    "text": "if you want to do a regression problem",
    "start": "395680",
    "end": "397440"
  },
  {
    "text": "you can have any types of inputs and",
    "start": "397440",
    "end": "398880"
  },
  {
    "text": "then just specify a numerical output you",
    "start": "398880",
    "end": "401280"
  },
  {
    "text": "want to do speech verification you can",
    "start": "401280",
    "end": "403120"
  },
  {
    "text": "have two different audio inputs that",
    "start": "403120",
    "end": "404720"
  },
  {
    "text": "then have a binary",
    "start": "404720",
    "end": "406319"
  },
  {
    "text": "output which tells you whether or not",
    "start": "406319",
    "end": "408160"
  },
  {
    "text": "the audio streams are for example",
    "start": "408160",
    "end": "410080"
  },
  {
    "text": "equivalent or something to that effect",
    "start": "410080",
    "end": "411680"
  },
  {
    "text": "for the same speaker",
    "start": "411680",
    "end": "413520"
  },
  {
    "text": "and any number of other problems",
    "start": "413520",
    "end": "414960"
  },
  {
    "text": "including text or image or forecasting",
    "start": "414960",
    "end": "418240"
  },
  {
    "text": "tabular data problems they're all",
    "start": "418240",
    "end": "420319"
  },
  {
    "text": "possible with rubric",
    "start": "420319",
    "end": "422479"
  },
  {
    "text": "another core component of blue wig is",
    "start": "422479",
    "end": "424160"
  },
  {
    "text": "scalability and so because we integrate",
    "start": "424160",
    "end": "426720"
  },
  {
    "text": "heavily with kubernetes we also",
    "start": "426720",
    "end": "428880"
  },
  {
    "text": "integrate heavily with other distributed",
    "start": "428880",
    "end": "430639"
  },
  {
    "text": "systems that sit on top of the build on",
    "start": "430639",
    "end": "432479"
  },
  {
    "text": "top of kubernetes like ray and so all of",
    "start": "432479",
    "end": "435199"
  },
  {
    "text": "the pre-processing uh can be distributed",
    "start": "435199",
    "end": "437680"
  },
  {
    "text": "across a cluster of",
    "start": "437680",
    "end": "439520"
  },
  {
    "text": "pods",
    "start": "439520",
    "end": "440479"
  },
  {
    "text": "um using uh das genre",
    "start": "440479",
    "end": "443039"
  },
  {
    "text": "and our training system um",
    "start": "443039",
    "end": "445520"
  },
  {
    "text": "uses a framework called horovad",
    "start": "445520",
    "end": "448000"
  },
  {
    "text": "that allows you to distribute training",
    "start": "448000",
    "end": "449759"
  },
  {
    "text": "across multiple nodes and multiple gpus",
    "start": "449759",
    "end": "452319"
  },
  {
    "text": "and then model artifacts can then all be",
    "start": "452319",
    "end": "454880"
  },
  {
    "text": "uploaded to a registry like something",
    "start": "454880",
    "end": "456639"
  },
  {
    "text": "like ml flow which we support",
    "start": "456639",
    "end": "458160"
  },
  {
    "text": "integration with out of the box as well",
    "start": "458160",
    "end": "461039"
  },
  {
    "text": "for hyper parameter search it's very",
    "start": "461039",
    "end": "462639"
  },
  {
    "text": "similar and very modular again so we use",
    "start": "462639",
    "end": "466319"
  },
  {
    "text": "ray tune which sits at a level on top of",
    "start": "466319",
    "end": "468960"
  },
  {
    "text": "the training process and can perturb",
    "start": "468960",
    "end": "470960"
  },
  {
    "text": "different parts of the config and every",
    "start": "470960",
    "end": "473120"
  },
  {
    "text": "one of those config",
    "start": "473120",
    "end": "475199"
  },
  {
    "text": "variants then becomes its own trial that",
    "start": "475199",
    "end": "477199"
  },
  {
    "text": "goes through the same training",
    "start": "477199",
    "end": "478800"
  },
  {
    "text": "pre-processing training and evaluation",
    "start": "478800",
    "end": "480720"
  },
  {
    "text": "step",
    "start": "480720",
    "end": "481759"
  },
  {
    "text": "as any other training process in moodwig",
    "start": "481759",
    "end": "483919"
  },
  {
    "text": "and then at the end of the day you can",
    "start": "483919",
    "end": "485360"
  },
  {
    "text": "get all the different model trials that",
    "start": "485360",
    "end": "487520"
  },
  {
    "text": "were explored and choose the one that",
    "start": "487520",
    "end": "489840"
  },
  {
    "text": "you would like to use in production",
    "start": "489840",
    "end": "493120"
  },
  {
    "text": "and when we started to look at building",
    "start": "493120",
    "end": "494560"
  },
  {
    "text": "an automl layer on top of this",
    "start": "494560",
    "end": "497440"
  },
  {
    "text": "our goal was that we wanted to be",
    "start": "497440",
    "end": "499360"
  },
  {
    "text": "something that was ultimately a glass",
    "start": "499360",
    "end": "501199"
  },
  {
    "text": "box and not a black box and so one thing",
    "start": "501199",
    "end": "503280"
  },
  {
    "text": "that",
    "start": "503280",
    "end": "504000"
  },
  {
    "text": "is very nice about the automl system",
    "start": "504000",
    "end": "506560"
  },
  {
    "text": "moodwig is at the end of the day um you",
    "start": "506560",
    "end": "509440"
  },
  {
    "text": "can see it as like a co-pilot that's",
    "start": "509440",
    "end": "511520"
  },
  {
    "text": "helping you generate an ideal lubricant",
    "start": "511520",
    "end": "513518"
  },
  {
    "text": "figure for your data set so you can",
    "start": "513519",
    "end": "514880"
  },
  {
    "text": "start by saying something as simple as",
    "start": "514880",
    "end": "517120"
  },
  {
    "text": "uh create a configuration from my data",
    "start": "517120",
    "end": "519599"
  },
  {
    "text": "set which can be a data frame or rk file",
    "start": "519599",
    "end": "521760"
  },
  {
    "text": "or whatever and then i want to predict",
    "start": "521760",
    "end": "523839"
  },
  {
    "text": "this particular column in this case",
    "start": "523839",
    "end": "525279"
  },
  {
    "text": "intent and then it can give you a config",
    "start": "525279",
    "end": "527519"
  },
  {
    "text": "that then you can do whatever you want",
    "start": "527519",
    "end": "528800"
  },
  {
    "text": "with modify anything and uh to your",
    "start": "528800",
    "end": "531360"
  },
  {
    "text": "heart's content",
    "start": "531360",
    "end": "532720"
  },
  {
    "text": "so how this works under the hood is you",
    "start": "532720",
    "end": "534480"
  },
  {
    "text": "just provide those two parameters plus",
    "start": "534480",
    "end": "536240"
  },
  {
    "text": "an optional time budget and then ludwig",
    "start": "536240",
    "end": "538480"
  },
  {
    "text": "automl will do some influence to",
    "start": "538480",
    "end": "540560"
  },
  {
    "text": "determine the input and output feature",
    "start": "540560",
    "end": "542080"
  },
  {
    "text": "types choose the appropriate mall",
    "start": "542080",
    "end": "544080"
  },
  {
    "text": "architecture based on your task select",
    "start": "544080",
    "end": "546720"
  },
  {
    "text": "the parameters and hyperparameter ranges",
    "start": "546720",
    "end": "549040"
  },
  {
    "text": "that wants to explore",
    "start": "549040",
    "end": "550640"
  },
  {
    "text": "given the time constraints and resource",
    "start": "550640",
    "end": "552399"
  },
  {
    "text": "constraints and then launch the hyper",
    "start": "552399",
    "end": "554320"
  },
  {
    "text": "parameter search trial",
    "start": "554320",
    "end": "556000"
  },
  {
    "text": "trials on ray tune using your gpu",
    "start": "556000",
    "end": "558560"
  },
  {
    "text": "workers and the outputs will be the best",
    "start": "558560",
    "end": "561200"
  },
  {
    "text": "tuned model along with other models that",
    "start": "561200",
    "end": "562800"
  },
  {
    "text": "were explored",
    "start": "562800",
    "end": "564080"
  },
  {
    "text": "and you can then take those results and",
    "start": "564080",
    "end": "566160"
  },
  {
    "text": "deploy them into production",
    "start": "566160",
    "end": "568480"
  },
  {
    "text": "as well",
    "start": "568480",
    "end": "570320"
  },
  {
    "text": "and now i'd like to hand it back to uh",
    "start": "570320",
    "end": "572720"
  },
  {
    "text": "and to talk a little bit more about the",
    "start": "572720",
    "end": "574959"
  },
  {
    "text": "lodo the important thing that i want to",
    "start": "574959",
    "end": "577519"
  },
  {
    "text": "emphasize here is that there's more to",
    "start": "577519",
    "end": "578959"
  },
  {
    "text": "this story than just the",
    "start": "578959",
    "end": "580800"
  },
  {
    "text": "automotive side because there's when",
    "start": "580800",
    "end": "582800"
  },
  {
    "text": "you're running this thing",
    "start": "582800",
    "end": "584320"
  },
  {
    "text": "in production or kind of in a large",
    "start": "584320",
    "end": "586240"
  },
  {
    "text": "distributed setting",
    "start": "586240",
    "end": "587839"
  },
  {
    "text": "there's also a component of how you want",
    "start": "587839",
    "end": "589360"
  },
  {
    "text": "to do this process efficiently to",
    "start": "589360",
    "end": "591920"
  },
  {
    "text": "optimize the usage of these uh commodity",
    "start": "591920",
    "end": "595120"
  },
  {
    "text": "resources like gpus so that you're using",
    "start": "595120",
    "end": "598240"
  },
  {
    "text": "them judiciously and not wasting",
    "start": "598240",
    "end": "600800"
  },
  {
    "text": "resources so",
    "start": "600800",
    "end": "602480"
  },
  {
    "text": "this is where liberal fits into the",
    "start": "602480",
    "end": "603839"
  },
  {
    "text": "picture particularly for running",
    "start": "603839",
    "end": "605600"
  },
  {
    "text": "kubernetes workloads and so now i'd like",
    "start": "605600",
    "end": "608079"
  },
  {
    "text": "to hand it back to ann to tell you more",
    "start": "608079",
    "end": "609440"
  },
  {
    "text": "about uh a little and the work that",
    "start": "609440",
    "end": "611760"
  },
  {
    "text": "she's done on combining the automl and",
    "start": "611760",
    "end": "614640"
  },
  {
    "text": "uh these other systems together",
    "start": "614640",
    "end": "618680"
  },
  {
    "text": "smart cluster sorry about that it's a",
    "start": "618800",
    "end": "621279"
  },
  {
    "text": "smart cluster provisioner that runs in",
    "start": "621279",
    "end": "623519"
  },
  {
    "text": "standard kubernetes clusters it monitors",
    "start": "623519",
    "end": "626079"
  },
  {
    "text": "for pending pod creation requests and",
    "start": "626079",
    "end": "628480"
  },
  {
    "text": "adds additional",
    "start": "628480",
    "end": "630079"
  },
  {
    "text": "compute to the kubernetes clusters to",
    "start": "630079",
    "end": "631839"
  },
  {
    "text": "satisfy those requests that compute can",
    "start": "631839",
    "end": "634480"
  },
  {
    "text": "be in the form of vms uh on demand or",
    "start": "634480",
    "end": "636880"
  },
  {
    "text": "spot vms or it can be in in the form of",
    "start": "636880",
    "end": "638800"
  },
  {
    "text": "serverless compute like aws fargate and",
    "start": "638800",
    "end": "641760"
  },
  {
    "text": "it chooses the compute based on current",
    "start": "641760",
    "end": "643760"
  },
  {
    "text": "availability of that kind of compute the",
    "start": "643760",
    "end": "646079"
  },
  {
    "text": "cost of that kind of compute and other",
    "start": "646079",
    "end": "648240"
  },
  {
    "text": "user specific requirements you may want",
    "start": "648240",
    "end": "650480"
  },
  {
    "text": "a gpu you may want to prefer not to have",
    "start": "650480",
    "end": "652959"
  },
  {
    "text": "a certain kind of gp things like this",
    "start": "652959",
    "end": "655760"
  },
  {
    "text": "and on an ongoing basis luna is",
    "start": "655760",
    "end": "657760"
  },
  {
    "text": "monitoring the node usage in the cluster",
    "start": "657760",
    "end": "660079"
  },
  {
    "text": "and will remove compute from the",
    "start": "660079",
    "end": "661519"
  },
  {
    "text": "kubernetes cluster when it's no longer",
    "start": "661519",
    "end": "663120"
  },
  {
    "text": "needed so luna is comparable to the",
    "start": "663120",
    "end": "665200"
  },
  {
    "text": "kubernetes cluster auto scaler but it",
    "start": "665200",
    "end": "667440"
  },
  {
    "text": "provides more flexible node selection",
    "start": "667440",
    "end": "669200"
  },
  {
    "text": "without the need to create and maintain",
    "start": "669200",
    "end": "670959"
  },
  {
    "text": "what can be hundreds of node groups to",
    "start": "670959",
    "end": "672640"
  },
  {
    "text": "represent all the image types",
    "start": "672640",
    "end": "674720"
  },
  {
    "text": "i mean all the instance types and it's",
    "start": "674720",
    "end": "676959"
  },
  {
    "text": "similar to aws carpenter but it works",
    "start": "676959",
    "end": "678880"
  },
  {
    "text": "across cloud vendors provides instant",
    "start": "678880",
    "end": "681200"
  },
  {
    "text": "family exclusion and supports a",
    "start": "681200",
    "end": "682800"
  },
  {
    "text": "deterministic",
    "start": "682800",
    "end": "684399"
  },
  {
    "text": "application of rules",
    "start": "684399",
    "end": "686399"
  },
  {
    "text": "so with all that background let's go",
    "start": "686399",
    "end": "688160"
  },
  {
    "text": "into uh what we did to look at the",
    "start": "688160",
    "end": "690480"
  },
  {
    "text": "different ways we could run um ludwig",
    "start": "690480",
    "end": "693120"
  },
  {
    "text": "automl and how they much they would cost",
    "start": "693120",
    "end": "695440"
  },
  {
    "text": "and how easy they would be",
    "start": "695440",
    "end": "696959"
  },
  {
    "text": "so just some background the ludwig",
    "start": "696959",
    "end": "698640"
  },
  {
    "text": "automl heuristics that we developed for",
    "start": "698640",
    "end": "700880"
  },
  {
    "text": "tabular data sets were developed after",
    "start": "700880",
    "end": "703440"
  },
  {
    "text": "analyzing thousands of hours of model",
    "start": "703440",
    "end": "705600"
  },
  {
    "text": "training across 12 tabular data sets",
    "start": "705600",
    "end": "708320"
  },
  {
    "text": "and after we had those heuristics you",
    "start": "708320",
    "end": "710800"
  },
  {
    "text": "know we ran them on the training set of",
    "start": "710800",
    "end": "712399"
  },
  {
    "text": "12 data sets to make sure they could",
    "start": "712399",
    "end": "714000"
  },
  {
    "text": "produce good models in a short period of",
    "start": "714000",
    "end": "716000"
  },
  {
    "text": "time like an hour rather than thousands",
    "start": "716000",
    "end": "717760"
  },
  {
    "text": "of hours and then we said okay that's",
    "start": "717760",
    "end": "719600"
  },
  {
    "text": "like you know running on the training",
    "start": "719600",
    "end": "721120"
  },
  {
    "text": "set now let's take an additional nine",
    "start": "721120",
    "end": "723360"
  },
  {
    "text": "validation data sets we've never seen",
    "start": "723360",
    "end": "725200"
  },
  {
    "text": "them before they're tabular and let's",
    "start": "725200",
    "end": "727519"
  },
  {
    "text": "run ludwig automl on them and let's get",
    "start": "727519",
    "end": "730240"
  },
  {
    "text": "the resulting models and compare them to",
    "start": "730240",
    "end": "732160"
  },
  {
    "text": "highly tuned publicly um reported models",
    "start": "732160",
    "end": "735680"
  },
  {
    "text": "and so that's i did that and that's the",
    "start": "735680",
    "end": "737920"
  },
  {
    "text": "i did that on nine uh data those nine",
    "start": "737920",
    "end": "740480"
  },
  {
    "text": "data sets and so the workload we'll look",
    "start": "740480",
    "end": "742320"
  },
  {
    "text": "at here is three of those data sets",
    "start": "742320",
    "end": "745440"
  },
  {
    "text": "running for one hour two hour and four",
    "start": "745440",
    "end": "747839"
  },
  {
    "text": "hour raytoon time budgets",
    "start": "747839",
    "end": "750000"
  },
  {
    "text": "and we'll look at the way i originally",
    "start": "750000",
    "end": "751680"
  },
  {
    "text": "ran them and the way i would run them on",
    "start": "751680",
    "end": "753279"
  },
  {
    "text": "kubernetes with luna available so the",
    "start": "753279",
    "end": "756399"
  },
  {
    "text": "remainder of this talk we'll look at the",
    "start": "756399",
    "end": "758480"
  },
  {
    "text": "baseline configuration the two other",
    "start": "758480",
    "end": "760480"
  },
  {
    "text": "configurations first look at a high",
    "start": "760480",
    "end": "762079"
  },
  {
    "text": "level you know a description of the",
    "start": "762079",
    "end": "763760"
  },
  {
    "text": "three and then deep dive each one",
    "start": "763760",
    "end": "767360"
  },
  {
    "text": "so the top level is how i ran things",
    "start": "767600",
    "end": "770000"
  },
  {
    "text": "originally and did the original",
    "start": "770000",
    "end": "771600"
  },
  {
    "text": "validation of automl for the nine data",
    "start": "771600",
    "end": "774000"
  },
  {
    "text": "sets i had a three node ray cluster i",
    "start": "774000",
    "end": "776399"
  },
  {
    "text": "deployed it directly on aws vms all",
    "start": "776399",
    "end": "779519"
  },
  {
    "text": "three of the nodes were gpu enabled",
    "start": "779519",
    "end": "781200"
  },
  {
    "text": "meaning the ray head itself could run",
    "start": "781200",
    "end": "782959"
  },
  {
    "text": "part of the workload during the",
    "start": "782959",
    "end": "784320"
  },
  {
    "text": "auto-tune process as well as the two",
    "start": "784320",
    "end": "786800"
  },
  {
    "text": "workers",
    "start": "786800",
    "end": "787839"
  },
  {
    "text": "i ran them on nvidia t4 gpu vms as being",
    "start": "787839",
    "end": "790800"
  },
  {
    "text": "a kind of a commodity",
    "start": "790800",
    "end": "792720"
  },
  {
    "text": "gpu system that did a good job for these",
    "start": "792720",
    "end": "795600"
  },
  {
    "text": "workloads",
    "start": "795600",
    "end": "797120"
  },
  {
    "text": "the first alternative to that basic uh",
    "start": "797120",
    "end": "799600"
  },
  {
    "text": "configuration is to instead of deploying",
    "start": "799600",
    "end": "802800"
  },
  {
    "text": "ray directly onto vms to repo deploy ray",
    "start": "802800",
    "end": "806800"
  },
  {
    "text": "into a kubernetes cluster that's got",
    "start": "806800",
    "end": "809680"
  },
  {
    "text": "luna installed in it and to enable the",
    "start": "809680",
    "end": "812240"
  },
  {
    "text": "ray auto scaler so to deploy ray with",
    "start": "812240",
    "end": "815600"
  },
  {
    "text": "the head being gpu enabled and allow ray",
    "start": "815600",
    "end": "818320"
  },
  {
    "text": "to scale up to eight workers you'll see",
    "start": "818320",
    "end": "820399"
  },
  {
    "text": "here instead of just two we'll talk",
    "start": "820399",
    "end": "822320"
  },
  {
    "text": "about that in a minute um and so what",
    "start": "822320",
    "end": "824720"
  },
  {
    "text": "happens here is when the ray auto scaler",
    "start": "824720",
    "end": "826720"
  },
  {
    "text": "realizes that more",
    "start": "826720",
    "end": "828560"
  },
  {
    "text": "workers are needed it asks for the",
    "start": "828560",
    "end": "830480"
  },
  {
    "text": "workers not in terms of an instance type",
    "start": "830480",
    "end": "832639"
  },
  {
    "text": "but in terms of the amount of resources",
    "start": "832639",
    "end": "834240"
  },
  {
    "text": "that are needed and when luna sees those",
    "start": "834240",
    "end": "837600"
  },
  {
    "text": "resource requests those pending pod",
    "start": "837600",
    "end": "839360"
  },
  {
    "text": "requests that aren't satisfied it's",
    "start": "839360",
    "end": "841120"
  },
  {
    "text": "going to go out and pull",
    "start": "841120",
    "end": "842880"
  },
  {
    "text": "available instance type and put it into",
    "start": "842880",
    "end": "844639"
  },
  {
    "text": "the kubernetes cluster on demand",
    "start": "844639",
    "end": "846880"
  },
  {
    "text": "and then lay when later when the ray",
    "start": "846880",
    "end": "848320"
  },
  {
    "text": "auto scaler doesn't need a worker it's",
    "start": "848320",
    "end": "850320"
  },
  {
    "text": "going to",
    "start": "850320",
    "end": "851440"
  },
  {
    "text": "get rid of the worker and the luna",
    "start": "851440",
    "end": "853920"
  },
  {
    "text": "system that k it's nodeless is going to",
    "start": "853920",
    "end": "855760"
  },
  {
    "text": "see that that node is no longer needed",
    "start": "855760",
    "end": "857360"
  },
  {
    "text": "and pull it out of the kubernetes",
    "start": "857360",
    "end": "858720"
  },
  {
    "text": "cluster",
    "start": "858720",
    "end": "859920"
  },
  {
    "text": "so in fact in this case even the ray",
    "start": "859920",
    "end": "862240"
  },
  {
    "text": "head when it's originally deployed luna",
    "start": "862240",
    "end": "864480"
  },
  {
    "text": "is the one that uses a gpu enabled",
    "start": "864480",
    "end": "867360"
  },
  {
    "text": "node and puts it into the cluster",
    "start": "867360",
    "end": "869120"
  },
  {
    "text": "alternative two is just like alternative",
    "start": "869120",
    "end": "870880"
  },
  {
    "text": "one with one change which is that the",
    "start": "870880",
    "end": "872959"
  },
  {
    "text": "head of the right cluster is cpu only",
    "start": "872959",
    "end": "875839"
  },
  {
    "text": "this is good because gpus are expensive",
    "start": "875839",
    "end": "877920"
  },
  {
    "text": "and so in this case",
    "start": "877920",
    "end": "879440"
  },
  {
    "text": "you can run an idle ray cluster and it",
    "start": "879440",
    "end": "882160"
  },
  {
    "text": "would cost less money",
    "start": "882160",
    "end": "885360"
  },
  {
    "text": "all right so",
    "start": "885360",
    "end": "886560"
  },
  {
    "text": "why did i run this you know how did i",
    "start": "886560",
    "end": "888160"
  },
  {
    "text": "choose this baseline well the basic",
    "start": "888160",
    "end": "890399"
  },
  {
    "text": "principles of why i ran the original",
    "start": "890399",
    "end": "892399"
  },
  {
    "text": "experiments in this",
    "start": "892399",
    "end": "894800"
  },
  {
    "text": "configuration of the fixed size three",
    "start": "894800",
    "end": "897519"
  },
  {
    "text": "node ray cluster",
    "start": "897519",
    "end": "899120"
  },
  {
    "text": "with um t4 uh gpu instances was i wanted",
    "start": "899120",
    "end": "903360"
  },
  {
    "text": "a standardized comp amount of compute",
    "start": "903360",
    "end": "905760"
  },
  {
    "text": "for the automl time budget so if i say",
    "start": "905760",
    "end": "907760"
  },
  {
    "text": "i'm going to run automl for one hour",
    "start": "907760",
    "end": "909680"
  },
  {
    "text": "automatic well for two hours auto mouth",
    "start": "909680",
    "end": "911680"
  },
  {
    "text": "for four hours there needs to be a",
    "start": "911680",
    "end": "913120"
  },
  {
    "text": "standard amount of compute behind that",
    "start": "913120",
    "end": "915120"
  },
  {
    "text": "you know time",
    "start": "915120",
    "end": "916320"
  },
  {
    "text": "basis",
    "start": "916320",
    "end": "917519"
  },
  {
    "text": "so",
    "start": "917519",
    "end": "918560"
  },
  {
    "text": "i cared a lot about that i cared a lot",
    "start": "918560",
    "end": "920399"
  },
  {
    "text": "about operational complexity i didn't",
    "start": "920399",
    "end": "922000"
  },
  {
    "text": "want to reason about whether my",
    "start": "922000",
    "end": "923120"
  },
  {
    "text": "experiment was good or not i wanted to",
    "start": "923120",
    "end": "924560"
  },
  {
    "text": "have confidence that all the compete was",
    "start": "924560",
    "end": "926639"
  },
  {
    "text": "available for the entire time budget and",
    "start": "926639",
    "end": "928320"
  },
  {
    "text": "what i was getting was a legit result",
    "start": "928320",
    "end": "930720"
  },
  {
    "text": "and my final thing was i wanted to",
    "start": "930720",
    "end": "932160"
  },
  {
    "text": "control idle cost so when i when the",
    "start": "932160",
    "end": "934800"
  },
  {
    "text": "experiment was finished i would log into",
    "start": "934800",
    "end": "936480"
  },
  {
    "text": "the ray head i would make sure that i",
    "start": "936480",
    "end": "938639"
  },
  {
    "text": "didn't see anything bogus about the",
    "start": "938639",
    "end": "940320"
  },
  {
    "text": "experiment i would poke around i would",
    "start": "940320",
    "end": "941920"
  },
  {
    "text": "record things so i was pretty sensitive",
    "start": "941920",
    "end": "944240"
  },
  {
    "text": "to idle because i might leave the raid",
    "start": "944240",
    "end": "946079"
  },
  {
    "text": "cluster running for you know a",
    "start": "946079",
    "end": "947519"
  },
  {
    "text": "non-trivial amount of time after the",
    "start": "947519",
    "end": "949040"
  },
  {
    "text": "experiment was over so g4dn means t4",
    "start": "949040",
    "end": "952160"
  },
  {
    "text": "gpus and i could have used any of the",
    "start": "952160",
    "end": "954399"
  },
  {
    "text": "variants of t4 gpus because for this",
    "start": "954399",
    "end": "956399"
  },
  {
    "text": "workload it's all about the gpu and the",
    "start": "956399",
    "end": "958320"
  },
  {
    "text": "gpu memory um but i chose 4x large a",
    "start": "958320",
    "end": "961680"
  },
  {
    "text": "little bit spendy because when i tried",
    "start": "961680",
    "end": "963759"
  },
  {
    "text": "to get cheaper instances in my region",
    "start": "963759",
    "end": "965519"
  },
  {
    "text": "they were often not available and so",
    "start": "965519",
    "end": "967519"
  },
  {
    "text": "that was operational complexity for me",
    "start": "967519",
    "end": "969519"
  },
  {
    "text": "to keep trying to redeploy the ray",
    "start": "969519",
    "end": "971120"
  },
  {
    "text": "cluster and so on so",
    "start": "971120",
    "end": "972880"
  },
  {
    "text": "you know that was my choice three nodes",
    "start": "972880",
    "end": "975279"
  },
  {
    "text": "i knew would do a good job running the",
    "start": "975279",
    "end": "977199"
  },
  {
    "text": "10 hyper parameter search trials which",
    "start": "977199",
    "end": "979600"
  },
  {
    "text": "is the default number of search trials",
    "start": "979600",
    "end": "981519"
  },
  {
    "text": "run by automl and ludwig so i knew three",
    "start": "981519",
    "end": "984160"
  },
  {
    "text": "nodes could complete 10 trials in a",
    "start": "984160",
    "end": "986240"
  },
  {
    "text": "reasonable amount of time",
    "start": "986240",
    "end": "989199"
  },
  {
    "text": "i used non-spot instances for the same",
    "start": "989199",
    "end": "991360"
  },
  {
    "text": "reason that i had a fixed size cluster i",
    "start": "991360",
    "end": "993279"
  },
  {
    "text": "didn't want anything to go away during",
    "start": "993279",
    "end": "994720"
  },
  {
    "text": "the run and i ran a single you know job",
    "start": "994720",
    "end": "997199"
  },
  {
    "text": "at a time not",
    "start": "997199",
    "end": "998399"
  },
  {
    "text": "six or nine nodes for that idle issue",
    "start": "998399",
    "end": "1001759"
  },
  {
    "text": "so of course the baseline for these were",
    "start": "1001759",
    "end": "1003680"
  },
  {
    "text": "three workloads uh running for the",
    "start": "1003680",
    "end": "1005839"
  },
  {
    "text": "three-time budgets matched our",
    "start": "1005839",
    "end": "1007279"
  },
  {
    "text": "expectations for uh you know attuned",
    "start": "1007279",
    "end": "1010480"
  },
  {
    "text": "accuracy of the models versus uh",
    "start": "1010480",
    "end": "1012880"
  },
  {
    "text": "manually tuned models the elapsed time",
    "start": "1012880",
    "end": "1015040"
  },
  {
    "text": "for this run was 22.6 hours and you",
    "start": "1015040",
    "end": "1017759"
  },
  {
    "text": "might be saying well why wasn't it 21",
    "start": "1017759",
    "end": "1019360"
  },
  {
    "text": "hours it's you know three times one plus",
    "start": "1019360",
    "end": "1021279"
  },
  {
    "text": "three times two plus three times four",
    "start": "1021279",
    "end": "1022800"
  },
  {
    "text": "but the extra 1.6 hours were for parts",
    "start": "1022800",
    "end": "1025600"
  },
  {
    "text": "of the job that only run on the head so",
    "start": "1025600",
    "end": "1027918"
  },
  {
    "text": "when the data is loaded up and",
    "start": "1027919",
    "end": "1030000"
  },
  {
    "text": "pre-processed that's done on the head",
    "start": "1030000",
    "end": "1032400"
  },
  {
    "text": "and when the final auto-tune job is",
    "start": "1032400",
    "end": "1034880"
  },
  {
    "text": "complete the head also runs the",
    "start": "1034880",
    "end": "1036798"
  },
  {
    "text": "evaluation of the best model from each",
    "start": "1036799",
    "end": "1038798"
  },
  {
    "text": "trial",
    "start": "1038799",
    "end": "1039918"
  },
  {
    "text": "uh the cost of this workload because g4",
    "start": "1039919",
    "end": "1042880"
  },
  {
    "text": "uh dn4x large uh instances cost 1.204",
    "start": "1042880",
    "end": "1046880"
  },
  {
    "text": "dollars an hour the overall workload",
    "start": "1046880",
    "end": "1048960"
  },
  {
    "text": "cost here was 81.63",
    "start": "1048960",
    "end": "1052320"
  },
  {
    "text": "and the idle cost of course is 3.612",
    "start": "1052320",
    "end": "1056160"
  },
  {
    "text": "an hour",
    "start": "1056160",
    "end": "1057600"
  },
  {
    "text": "so some observations about this baseline",
    "start": "1057600",
    "end": "1060080"
  },
  {
    "text": "these baseline runs i did well it would",
    "start": "1060080",
    "end": "1062880"
  },
  {
    "text": "be nice to get you know the results in",
    "start": "1062880",
    "end": "1065440"
  },
  {
    "text": "quicker than 22.6 hours and this is just",
    "start": "1065440",
    "end": "1068160"
  },
  {
    "text": "three of the nine",
    "start": "1068160",
    "end": "1069600"
  },
  {
    "text": "um and you know the obvious way to do",
    "start": "1069600",
    "end": "1071280"
  },
  {
    "text": "that would be to run more than one of",
    "start": "1071280",
    "end": "1073360"
  },
  {
    "text": "the jobs at a time run the three",
    "start": "1073360",
    "end": "1074960"
  },
  {
    "text": "one-hour jobs in parallel and three",
    "start": "1074960",
    "end": "1077280"
  },
  {
    "text": "two-hour jobs in parallel and so on and",
    "start": "1077280",
    "end": "1079120"
  },
  {
    "text": "of course i didn't do that originally",
    "start": "1079120",
    "end": "1080559"
  },
  {
    "text": "because i was worried that the ray auto",
    "start": "1080559",
    "end": "1082240"
  },
  {
    "text": "scaler wouldn't be able to obtain the",
    "start": "1082240",
    "end": "1084799"
  },
  {
    "text": "instance types needed when i needed them",
    "start": "1084799",
    "end": "1087760"
  },
  {
    "text": "but that's where luna comes in so now",
    "start": "1087760",
    "end": "1090080"
  },
  {
    "text": "you know if you combine the ray auto",
    "start": "1090080",
    "end": "1091919"
  },
  {
    "text": "scaling with luna then the ray auto",
    "start": "1091919",
    "end": "1093760"
  },
  {
    "text": "scaler asks for resources and luna",
    "start": "1093760",
    "end": "1096720"
  },
  {
    "text": "satisfies them so basically that's",
    "start": "1096720",
    "end": "1100480"
  },
  {
    "text": "the magic that allows this to you know",
    "start": "1100480",
    "end": "1102720"
  },
  {
    "text": "reduce idle cost because now i don't",
    "start": "1102720",
    "end": "1104480"
  },
  {
    "text": "need all three nodes running at the end",
    "start": "1104480",
    "end": "1106400"
  },
  {
    "text": "and reduce elapsed time but you might be",
    "start": "1106400",
    "end": "1108400"
  },
  {
    "text": "thinking okay well that's fine but what",
    "start": "1108400",
    "end": "1109840"
  },
  {
    "text": "about workload cost can you really",
    "start": "1109840",
    "end": "1111440"
  },
  {
    "text": "reduce that i mean sure you could get",
    "start": "1111440",
    "end": "1113520"
  },
  {
    "text": "rid of the 1.6 hours where only the head",
    "start": "1113520",
    "end": "1115919"
  },
  {
    "text": "is needed but what about the workers",
    "start": "1115919",
    "end": "1118240"
  },
  {
    "text": "that are needed to run the um you know",
    "start": "1118240",
    "end": "1120240"
  },
  {
    "text": "the autotune",
    "start": "1120240",
    "end": "1121679"
  },
  {
    "text": "searches for hyper parameter well",
    "start": "1121679",
    "end": "1123760"
  },
  {
    "text": "actually those workers aren't always",
    "start": "1123760",
    "end": "1125600"
  },
  {
    "text": "needed during the entire run either so",
    "start": "1125600",
    "end": "1128080"
  },
  {
    "text": "we in the automl for ludwig uses um",
    "start": "1128080",
    "end": "1131200"
  },
  {
    "text": "something a search strategy called async",
    "start": "1131200",
    "end": "1133280"
  },
  {
    "text": "hyperband and what async hyperband does",
    "start": "1133280",
    "end": "1135760"
  },
  {
    "text": "is just continue trials that don't look",
    "start": "1135760",
    "end": "1137600"
  },
  {
    "text": "very promising compared to the trials",
    "start": "1137600",
    "end": "1139520"
  },
  {
    "text": "it's already run and so depending on the",
    "start": "1139520",
    "end": "1141919"
  },
  {
    "text": "data set a lot of trials may be",
    "start": "1141919",
    "end": "1143440"
  },
  {
    "text": "discontinued quickly",
    "start": "1143440",
    "end": "1145200"
  },
  {
    "text": "and when there's fewer than three trials",
    "start": "1145200",
    "end": "1147200"
  },
  {
    "text": "left and fewer of three workers are",
    "start": "1147200",
    "end": "1149039"
  },
  {
    "text": "needed",
    "start": "1149039",
    "end": "1150559"
  },
  {
    "text": "so this is a picture of the 22.6 hours",
    "start": "1150559",
    "end": "1153600"
  },
  {
    "text": "on the x-axis and on the y-axis you see",
    "start": "1153600",
    "end": "1156880"
  },
  {
    "text": "you know the three one-hour",
    "start": "1156880",
    "end": "1158960"
  },
  {
    "text": "data sets running for one hour each then",
    "start": "1158960",
    "end": "1161200"
  },
  {
    "text": "for two hours each then for four hours",
    "start": "1161200",
    "end": "1163600"
  },
  {
    "text": "each and you see a trial you know the",
    "start": "1163600",
    "end": "1165679"
  },
  {
    "text": "ten trials for the first data set so you",
    "start": "1165679",
    "end": "1167760"
  },
  {
    "text": "can see that for the first data set only",
    "start": "1167760",
    "end": "1169840"
  },
  {
    "text": "two trials really run into the end of",
    "start": "1169840",
    "end": "1171679"
  },
  {
    "text": "the time budget the other ones are",
    "start": "1171679",
    "end": "1173280"
  },
  {
    "text": "discontinued as being not promising and",
    "start": "1173280",
    "end": "1175760"
  },
  {
    "text": "for the second data set you can see only",
    "start": "1175760",
    "end": "1177520"
  },
  {
    "text": "one really survives till the end of the",
    "start": "1177520",
    "end": "1179360"
  },
  {
    "text": "hour",
    "start": "1179360",
    "end": "1180480"
  },
  {
    "text": "third data set you know is the trials",
    "start": "1180480",
    "end": "1182480"
  },
  {
    "text": "are more competitive but hey that's what",
    "start": "1182480",
    "end": "1184080"
  },
  {
    "text": "auto scaling is all about so there's a",
    "start": "1184080",
    "end": "1186080"
  },
  {
    "text": "lot of opportunity here to save",
    "start": "1186080",
    "end": "1188400"
  },
  {
    "text": "resources even during the run",
    "start": "1188400",
    "end": "1191280"
  },
  {
    "text": "so that's brings us to the um",
    "start": "1191280",
    "end": "1193280"
  },
  {
    "text": "configuration where ray uh is running",
    "start": "1193280",
    "end": "1196000"
  },
  {
    "text": "with its auto scaler where luna is",
    "start": "1196000",
    "end": "1198000"
  },
  {
    "text": "managing the kubernetes cluster in terms",
    "start": "1198000",
    "end": "1200000"
  },
  {
    "text": "of scaling it um and where we deploy ray",
    "start": "1200000",
    "end": "1203840"
  },
  {
    "text": "onto a kubernetes cluster which is eks",
    "start": "1203840",
    "end": "1206559"
  },
  {
    "text": "in this case and we actually used a",
    "start": "1206559",
    "end": "1208559"
  },
  {
    "text": "control to make sure that uh luna didn't",
    "start": "1208559",
    "end": "1211039"
  },
  {
    "text": "choose nvidia m60 gpus which are",
    "start": "1211039",
    "end": "1213520"
  },
  {
    "text": "actually cheaper",
    "start": "1213520",
    "end": "1214720"
  },
  {
    "text": "than t4 because they didn't work well",
    "start": "1214720",
    "end": "1216720"
  },
  {
    "text": "for machine learning workloads or kind",
    "start": "1216720",
    "end": "1218080"
  },
  {
    "text": "of for graphics workloads um and so",
    "start": "1218080",
    "end": "1221760"
  },
  {
    "text": "now we uh we run the",
    "start": "1221760",
    "end": "1223600"
  },
  {
    "text": "the three uh jobs in parallel of the",
    "start": "1223600",
    "end": "1225919"
  },
  {
    "text": "same time budget and we set max",
    "start": "1225919",
    "end": "1228159"
  },
  {
    "text": "concurrent trials to three so that they",
    "start": "1228159",
    "end": "1230000"
  },
  {
    "text": "still only will run three at a time just",
    "start": "1230000",
    "end": "1232159"
  },
  {
    "text": "like they would have in the fixed size",
    "start": "1232159",
    "end": "1234000"
  },
  {
    "text": "cluster to begin with",
    "start": "1234000",
    "end": "1237200"
  },
  {
    "text": "so in this configuration we got",
    "start": "1237200",
    "end": "1239440"
  },
  {
    "text": "competitive accuracy results so there",
    "start": "1239440",
    "end": "1241360"
  },
  {
    "text": "was no compromise on the accuracy that",
    "start": "1241360",
    "end": "1243679"
  },
  {
    "text": "the models that auto-tune",
    "start": "1243679",
    "end": "1245679"
  },
  {
    "text": "automl found using the ray autotune",
    "start": "1245679",
    "end": "1248640"
  },
  {
    "text": "but the elapsed time was greatly reduced",
    "start": "1248640",
    "end": "1250480"
  },
  {
    "text": "so in this parallel run the elapsed time",
    "start": "1250480",
    "end": "1252720"
  },
  {
    "text": "was reduced to 8.75 hours which was a",
    "start": "1252720",
    "end": "1255840"
  },
  {
    "text": "big difference",
    "start": "1255840",
    "end": "1257039"
  },
  {
    "text": "of course the idle cost was reduced by",
    "start": "1257039",
    "end": "1259120"
  },
  {
    "text": "two-thirds because only that gpu head is",
    "start": "1259120",
    "end": "1261280"
  },
  {
    "text": "running at the end",
    "start": "1261280",
    "end": "1262960"
  },
  {
    "text": "and the workload cost was reduced by 54",
    "start": "1262960",
    "end": "1266320"
  },
  {
    "text": "let's let's look at where that's coming",
    "start": "1266320",
    "end": "1267919"
  },
  {
    "text": "from",
    "start": "1267919",
    "end": "1270080"
  },
  {
    "text": "more than half of that is coming from",
    "start": "1270080",
    "end": "1272080"
  },
  {
    "text": "exploiting the auto scaling we talked",
    "start": "1272080",
    "end": "1273600"
  },
  {
    "text": "about you know scaling down workers not",
    "start": "1273600",
    "end": "1275600"
  },
  {
    "text": "needed during the run um for the",
    "start": "1275600",
    "end": "1277600"
  },
  {
    "text": "head-only parts or for the parts where",
    "start": "1277600",
    "end": "1279200"
  },
  {
    "text": "there's fewer than three trials left",
    "start": "1279200",
    "end": "1281440"
  },
  {
    "text": "um but a little bit",
    "start": "1281440",
    "end": "1282880"
  },
  {
    "text": "you know uh about 20 some percent was",
    "start": "1282880",
    "end": "1285360"
  },
  {
    "text": "also coming from using cheaper instances",
    "start": "1285360",
    "end": "1287919"
  },
  {
    "text": "so for when um luna ran this job it",
    "start": "1287919",
    "end": "1291440"
  },
  {
    "text": "shows the g4 dn4x large for the head",
    "start": "1291440",
    "end": "1294400"
  },
  {
    "text": "because i had said the head needs more",
    "start": "1294400",
    "end": "1296000"
  },
  {
    "text": "cpu memory to handle the evaluations of",
    "start": "1296000",
    "end": "1298720"
  },
  {
    "text": "the data processing but i had said that",
    "start": "1298720",
    "end": "1300799"
  },
  {
    "text": "the workers need less cpu memory and you",
    "start": "1300799",
    "end": "1303760"
  },
  {
    "text": "know",
    "start": "1303760",
    "end": "1304480"
  },
  {
    "text": "luna patient and willing to uh to look",
    "start": "1304480",
    "end": "1307600"
  },
  {
    "text": "for uh available instances was able to",
    "start": "1307600",
    "end": "1309679"
  },
  {
    "text": "get uh you know 2x large instances which",
    "start": "1309679",
    "end": "1312960"
  },
  {
    "text": "are cheaper",
    "start": "1312960",
    "end": "1315360"
  },
  {
    "text": "okay cool so now that's you know way",
    "start": "1316480",
    "end": "1319120"
  },
  {
    "text": "better in elapsed time",
    "start": "1319120",
    "end": "1320960"
  },
  {
    "text": "idle cost and workload cost but we still",
    "start": "1320960",
    "end": "1324000"
  },
  {
    "text": "have that you know um",
    "start": "1324000",
    "end": "1326320"
  },
  {
    "text": "gpu machine running if the ray head is",
    "start": "1326320",
    "end": "1329120"
  },
  {
    "text": "up and i guess there's two things there",
    "start": "1329120",
    "end": "1331120"
  },
  {
    "text": "one is you know it makes it means you're",
    "start": "1331120",
    "end": "1333200"
  },
  {
    "text": "going to feel a little guilty just",
    "start": "1333200",
    "end": "1334320"
  },
  {
    "text": "leaving it running all the time and so",
    "start": "1334320",
    "end": "1335679"
  },
  {
    "text": "you're probably going to spin it down",
    "start": "1335679",
    "end": "1337600"
  },
  {
    "text": "once you've gotten all the data that you",
    "start": "1337600",
    "end": "1339679"
  },
  {
    "text": "got",
    "start": "1339679",
    "end": "1340400"
  },
  {
    "text": "that you needed to get off the head",
    "start": "1340400",
    "end": "1342480"
  },
  {
    "text": "and also it's just you know a waste of",
    "start": "1342480",
    "end": "1344880"
  },
  {
    "text": "money in general because you don't need",
    "start": "1344880",
    "end": "1346640"
  },
  {
    "text": "those gpus once the job is finished",
    "start": "1346640",
    "end": "1349280"
  },
  {
    "text": "so in this case with the cpu only head",
    "start": "1349280",
    "end": "1352799"
  },
  {
    "text": "we can see how cheap we can get this and",
    "start": "1352799",
    "end": "1354640"
  },
  {
    "text": "possibly even leave the cluster up if",
    "start": "1354640",
    "end": "1356640"
  },
  {
    "text": "you're comfortable with the cost of idle",
    "start": "1356640",
    "end": "1358640"
  },
  {
    "text": "once you've switched to the cpu head",
    "start": "1358640",
    "end": "1360960"
  },
  {
    "text": "uh now in this kind of deployment the",
    "start": "1360960",
    "end": "1362880"
  },
  {
    "text": "head can't run a worker so we need to",
    "start": "1362880",
    "end": "1364640"
  },
  {
    "text": "bring up nine workers max to get three",
    "start": "1364640",
    "end": "1366799"
  },
  {
    "text": "three three um and also because",
    "start": "1366799",
    "end": "1369440"
  },
  {
    "text": "the way ludwig automl checks for",
    "start": "1369440",
    "end": "1371600"
  },
  {
    "text": "resources it looks at the",
    "start": "1371600",
    "end": "1374080"
  },
  {
    "text": "right head to see if there's gpu enabled",
    "start": "1374080",
    "end": "1376000"
  },
  {
    "text": "in the cluster so there's a slight",
    "start": "1376000",
    "end": "1377600"
  },
  {
    "text": "option you need to add to ludwig automl",
    "start": "1377600",
    "end": "1379600"
  },
  {
    "text": "to run it this way",
    "start": "1379600",
    "end": "1382320"
  },
  {
    "text": "so again in this configuration",
    "start": "1382480",
    "end": "1384880"
  },
  {
    "text": "we were able to uh to get match the",
    "start": "1384880",
    "end": "1387679"
  },
  {
    "text": "accuracy of the models so that's good",
    "start": "1387679",
    "end": "1389919"
  },
  {
    "text": "the elapsed time we suffered a little",
    "start": "1389919",
    "end": "1391760"
  },
  {
    "text": "bit instead of 8.75 hours it was nine",
    "start": "1391760",
    "end": "1394480"
  },
  {
    "text": "because a cpu head running the",
    "start": "1394480",
    "end": "1396400"
  },
  {
    "text": "evaluation uh was a little less",
    "start": "1396400",
    "end": "1398480"
  },
  {
    "text": "efficient but you know it's still a way",
    "start": "1398480",
    "end": "1400400"
  },
  {
    "text": "better than 22.6 hours so if you're",
    "start": "1400400",
    "end": "1402880"
  },
  {
    "text": "willing to take a little hit there",
    "start": "1402880",
    "end": "1404480"
  },
  {
    "text": "that's fine the idle cost was",
    "start": "1404480",
    "end": "1407480"
  },
  {
    "text": "0.452 dollars an hour so 62 then leaving",
    "start": "1407480",
    "end": "1411120"
  },
  {
    "text": "the gpu head enabled um and 87 percent",
    "start": "1411120",
    "end": "1413840"
  },
  {
    "text": "that should be a baseline so basically",
    "start": "1413840",
    "end": "1416400"
  },
  {
    "text": "it was a big savings and you know might",
    "start": "1416400",
    "end": "1419120"
  },
  {
    "text": "i mean i felt way less guilty leaving",
    "start": "1419120",
    "end": "1420960"
  },
  {
    "text": "the cluster ray cluster up in this case",
    "start": "1420960",
    "end": "1423760"
  },
  {
    "text": "the workload cost was essentially the",
    "start": "1423760",
    "end": "1425919"
  },
  {
    "text": "same uh as in the previous case",
    "start": "1425919",
    "end": "1429600"
  },
  {
    "text": "so overall the lessons we learned here",
    "start": "1429600",
    "end": "1431520"
  },
  {
    "text": "was",
    "start": "1431520",
    "end": "1432320"
  },
  {
    "text": "you know the um efficiency you can get",
    "start": "1432320",
    "end": "1435520"
  },
  {
    "text": "along with operational uh",
    "start": "1435520",
    "end": "1437840"
  },
  {
    "text": "ease of use of using load",
    "start": "1437840",
    "end": "1440640"
  },
  {
    "text": "luna nodeless kubernetes and using ray",
    "start": "1440640",
    "end": "1444080"
  },
  {
    "text": "on top of kubernetes was really",
    "start": "1444080",
    "end": "1445600"
  },
  {
    "text": "worthwhile um gpu",
    "start": "1445600",
    "end": "1448559"
  },
  {
    "text": "only had i mean gpu enabled head was",
    "start": "1448559",
    "end": "1450960"
  },
  {
    "text": "good cpu only head was better",
    "start": "1450960",
    "end": "1455120"
  },
  {
    "text": "so uh we've shown these benefits in the",
    "start": "1455120",
    "end": "1458480"
  },
  {
    "text": "future we want to continue to enhance",
    "start": "1458480",
    "end": "1460320"
  },
  {
    "text": "luna to handle efficient scaling of all",
    "start": "1460320",
    "end": "1462240"
  },
  {
    "text": "sorts of workloads you know certainly",
    "start": "1462240",
    "end": "1464720"
  },
  {
    "text": "deep learning training is not the only",
    "start": "1464720",
    "end": "1466000"
  },
  {
    "text": "workload that can benefit from this ci",
    "start": "1466000",
    "end": "1467760"
  },
  {
    "text": "cd and many other workloads can as well",
    "start": "1467760",
    "end": "1469919"
  },
  {
    "text": "and we want to continue to extend",
    "start": "1469919",
    "end": "1471440"
  },
  {
    "text": "literally automl to new domains to",
    "start": "1471440",
    "end": "1473840"
  },
  {
    "text": "enable efficient development and scaling",
    "start": "1473840",
    "end": "1476480"
  },
  {
    "text": "and we've already just recently that",
    "start": "1476480",
    "end": "1478080"
  },
  {
    "text": "like in the past two weeks uh announced",
    "start": "1478080",
    "end": "1480240"
  },
  {
    "text": "that lidwig auto well ml now works for",
    "start": "1480240",
    "end": "1482320"
  },
  {
    "text": "text classification data sets and also",
    "start": "1482320",
    "end": "1484640"
  },
  {
    "text": "shows good savings for those as well",
    "start": "1484640",
    "end": "1487679"
  },
  {
    "text": "so that's it thank you and any questions",
    "start": "1487679",
    "end": "1493000"
  },
  {
    "text": "so do i get at the right that lunar",
    "start": "1502640",
    "end": "1504799"
  },
  {
    "text": "doesn't actually",
    "start": "1504799",
    "end": "1506000"
  },
  {
    "text": "add nodes to the cluster but rather add",
    "start": "1506000",
    "end": "1508480"
  },
  {
    "text": "some compute and you're handing off the",
    "start": "1508480",
    "end": "1510720"
  },
  {
    "text": "workload to that",
    "start": "1510720",
    "end": "1512159"
  },
  {
    "text": "thing and if so what is the difference",
    "start": "1512159",
    "end": "1514159"
  },
  {
    "text": "between just",
    "start": "1514159",
    "end": "1515520"
  },
  {
    "text": "spawning notes on demand that fits your",
    "start": "1515520",
    "end": "1518080"
  },
  {
    "text": "needs",
    "start": "1518080",
    "end": "1521080"
  },
  {
    "text": "sorry just make sure i understand the",
    "start": "1528799",
    "end": "1530000"
  },
  {
    "text": "question so right now luna during the",
    "start": "1530000",
    "end": "1531919"
  },
  {
    "text": "experiment is adding virtual machines to",
    "start": "1531919",
    "end": "1533760"
  },
  {
    "text": "the kubernetes cluster to satisfy the",
    "start": "1533760",
    "end": "1536240"
  },
  {
    "text": "pending pod requests that couldn't be",
    "start": "1536240",
    "end": "1538080"
  },
  {
    "text": "placed when the kubernetes cluster uh at",
    "start": "1538080",
    "end": "1540480"
  },
  {
    "text": "the current size of the kubernetes",
    "start": "1540480",
    "end": "1541840"
  },
  {
    "text": "cluster",
    "start": "1541840",
    "end": "1544240"
  },
  {
    "text": "i'm sorry they're",
    "start": "1549440",
    "end": "1550559"
  },
  {
    "text": "i'm not sure if i understand their",
    "start": "1550559",
    "end": "1555158"
  },
  {
    "text": "their nodes in kubernetes",
    "start": "1556240",
    "end": "1559360"
  },
  {
    "text": "you mentioned there the higgs data set",
    "start": "1568559",
    "end": "1570640"
  },
  {
    "text": "oh yeah so i'm very curious what is this",
    "start": "1570640",
    "end": "1574880"
  },
  {
    "text": "so there's a whole it's it's pretty cool",
    "start": "1574880",
    "end": "1576799"
  },
  {
    "text": "but there's a whole bunch of famous",
    "start": "1576799",
    "end": "1578799"
  },
  {
    "text": "tabular data sets and i wanted to choose",
    "start": "1578799",
    "end": "1580640"
  },
  {
    "text": "famous ones that people had done a lot",
    "start": "1580640",
    "end": "1582400"
  },
  {
    "text": "of disclosures of best models for so",
    "start": "1582400",
    "end": "1584720"
  },
  {
    "text": "that i could make sure that automl was",
    "start": "1584720",
    "end": "1586720"
  },
  {
    "text": "competitive with those so there's one",
    "start": "1586720",
    "end": "1588000"
  },
  {
    "text": "for higgs",
    "start": "1588000",
    "end": "1589039"
  },
  {
    "text": "higgs boson data set it's a very large",
    "start": "1589039",
    "end": "1591919"
  },
  {
    "text": "very nice data set it's a very",
    "start": "1591919",
    "end": "1593360"
  },
  {
    "text": "challenging data set to run so yeah it's",
    "start": "1593360",
    "end": "1595679"
  },
  {
    "text": "it's an interesting data set so it's out",
    "start": "1595679",
    "end": "1597360"
  },
  {
    "text": "there you can download it and run it",
    "start": "1597360",
    "end": "1599120"
  },
  {
    "text": "it's also these data sets are built into",
    "start": "1599120",
    "end": "1600960"
  },
  {
    "text": "lid wigs so they're available if you're",
    "start": "1600960",
    "end": "1603120"
  },
  {
    "text": "using the lidwig platform there's a",
    "start": "1603120",
    "end": "1604720"
  },
  {
    "text": "bunch of standard data sets so each of",
    "start": "1604720",
    "end": "1606320"
  },
  {
    "text": "the ones for automl are checked in",
    "start": "1606320",
    "end": "1609440"
  },
  {
    "text": "just a quick question like did you",
    "start": "1609440",
    "end": "1611279"
  },
  {
    "text": "compare like luna with for example gk",
    "start": "1611279",
    "end": "1613440"
  },
  {
    "text": "autoscaler or carpenter like i think",
    "start": "1613440",
    "end": "1615360"
  },
  {
    "text": "here you're comparing with yourself",
    "start": "1615360",
    "end": "1616720"
  },
  {
    "text": "right like",
    "start": "1616720",
    "end": "1618240"
  },
  {
    "text": "yes so here i'm comparing with my own",
    "start": "1618240",
    "end": "1620799"
  },
  {
    "text": "lame you know baseline but yes i i kind",
    "start": "1620799",
    "end": "1623440"
  },
  {
    "text": "of mentioned on the uh the first uh kind",
    "start": "1623440",
    "end": "1626400"
  },
  {
    "text": "of description of luna that it is",
    "start": "1626400",
    "end": "1628000"
  },
  {
    "text": "comparable to aws carpenter and it is",
    "start": "1628000",
    "end": "1630240"
  },
  {
    "text": "comparable to the cluster auto scaler i",
    "start": "1630240",
    "end": "1633200"
  },
  {
    "text": "would say the difference for the cluster",
    "start": "1633200",
    "end": "1634720"
  },
  {
    "text": "auto scalers typically you have to end",
    "start": "1634720",
    "end": "1636400"
  },
  {
    "text": "up creating a whole bunch of node groups",
    "start": "1636400",
    "end": "1638240"
  },
  {
    "text": "to cover every possible instance type",
    "start": "1638240",
    "end": "1640240"
  },
  {
    "text": "you want whereas you don't have to do",
    "start": "1640240",
    "end": "1641840"
  },
  {
    "text": "that here and for carpenter",
    "start": "1641840",
    "end": "1644159"
  },
  {
    "text": "i've actually run experiments on other",
    "start": "1644159",
    "end": "1646159"
  },
  {
    "text": "cloud clouds other than aws so that's",
    "start": "1646159",
    "end": "1648320"
  },
  {
    "text": "one issue with carpenter at least right",
    "start": "1648320",
    "end": "1649919"
  },
  {
    "text": "now is that it's really aws carbon also",
    "start": "1649919",
    "end": "1652320"
  },
  {
    "text": "doesn't allow you to do instance",
    "start": "1652320",
    "end": "1653679"
  },
  {
    "text": "exclusion so i really wanted to do",
    "start": "1653679",
    "end": "1655360"
  },
  {
    "text": "instance exclusion here because i didn't",
    "start": "1655360",
    "end": "1656799"
  },
  {
    "text": "want the crappy m60 gpus um it also has",
    "start": "1656799",
    "end": "1660240"
  },
  {
    "text": "this weird thing with the rules where",
    "start": "1660240",
    "end": "1661679"
  },
  {
    "text": "it's not deterministic what order a",
    "start": "1661679",
    "end": "1663840"
  },
  {
    "text": "carpenter applies the rules so these are",
    "start": "1663840",
    "end": "1665679"
  },
  {
    "text": "little picky things but but um but so i",
    "start": "1665679",
    "end": "1667679"
  },
  {
    "text": "feel like this is kind of robust for my",
    "start": "1667679",
    "end": "1669919"
  },
  {
    "text": "use case",
    "start": "1669919",
    "end": "1672398"
  },
  {
    "text": "thank you",
    "start": "1674720",
    "end": "1676230"
  },
  {
    "text": "[Applause]",
    "start": "1676230",
    "end": "1679390"
  }
]