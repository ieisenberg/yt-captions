[
  {
    "text": "all right hello everybody uh welcome um uh pretty good turnout here i hope",
    "start": "80",
    "end": "6160"
  },
  {
    "text": "you're not not too sleepy after lunch um well uh today we're going to talk a",
    "start": "6160",
    "end": "11360"
  },
  {
    "text": "little bit about some of the really future-looking things we're doing in the Kubernetes project um what we're going",
    "start": "11360",
    "end": "18480"
  },
  {
    "text": "to show you today uh is only partly available today and some of it's coming in the release that's about to happen",
    "start": "18480",
    "end": "25119"
  },
  {
    "text": "but um hopefully you'll you'll uh see some of the the great work that the community is doing um to to handle these",
    "start": "25119",
    "end": "31840"
  },
  {
    "text": "more advanced use cases that we're seeing these days um my name is John Bellame i'm from",
    "start": "31840",
    "end": "38640"
  },
  {
    "text": "Google and I've been involved in Kubernetes for maybe seven or eight",
    "start": "38640",
    "end": "43840"
  },
  {
    "text": "years um so uh but for the last year I've been working on DRRA um which was",
    "start": "43840",
    "end": "51360"
  },
  {
    "text": "uh initiated by Patrick Oolie from Intel who's here in the third row and and",
    "start": "51360",
    "end": "56879"
  },
  {
    "text": "Kevin Clues has done a ton of work on this as well but many other contributors um from the community so we are uh you",
    "start": "56879",
    "end": "64239"
  },
  {
    "text": "know we're all um we're we're all in it together here and with me yeah my name",
    "start": "64239",
    "end": "69680"
  },
  {
    "text": "is Morton i'm also from Google uh I've been involved with the communities",
    "start": "69680",
    "end": "74799"
  },
  {
    "text": "project for like few years a little bit on and off and been working on dr for",
    "start": "74799",
    "end": "80560"
  },
  {
    "text": "last six months roughly all right so let's see what we're uh",
    "start": "80560",
    "end": "85759"
  },
  {
    "text": "what we're talking about here as far as these problems i mean I think that the really simple way to put it is as",
    "start": "85759",
    "end": "93200"
  },
  {
    "text": "workloads uh get larger when you're doing large training jobs or things like that you're using lots of accelerators",
    "start": "93200",
    "end": "100400"
  },
  {
    "text": "that means lots of nodes and of course lots of any kind of machine means more probability for failure so more failure",
    "start": "100400",
    "end": "107759"
  },
  {
    "text": "points more failures and um with some of these kind of coordinated mechanisms",
    "start": "107759",
    "end": "114799"
  },
  {
    "text": "across nodes the failure of one node can kind of uh impact all of the nodes involved in the job so how can",
    "start": "114799",
    "end": "122040"
  },
  {
    "text": "Kubernetes help manage those type of situations to kind of illustrate the the",
    "start": "122040",
    "end": "129759"
  },
  {
    "text": "problems we're talking about um I'm going to use uh TPUs as an example but a",
    "start": "129759",
    "end": "136160"
  },
  {
    "text": "lot of these same um problems uh apply also in GPU training",
    "start": "136160",
    "end": "143040"
  },
  {
    "text": "issues although they may solve the problems in different ways um we'll see a little bit of that later and and uh",
    "start": "143040",
    "end": "149440"
  },
  {
    "text": "but for now for our illustr illustration we're going to assume we have um 64",
    "start": "149440",
    "end": "156879"
  },
  {
    "text": "accelerators TPUs they're called um but they're similar to GPUs um and there's",
    "start": "156879",
    "end": "162239"
  },
  {
    "text": "four per node and so we have 16 nodes um",
    "start": "162239",
    "end": "167280"
  },
  {
    "text": "for for these particular uh jobs um or rather for for this particular",
    "start": "167280",
    "end": "174720"
  },
  {
    "text": "cluster in our example and then we'll have several different jobs that are each using a portion of that cluster but",
    "start": "174720",
    "end": "180560"
  },
  {
    "text": "not the whole thing when we allocate a set of uh say 16 TPUs which means four",
    "start": "180560",
    "end": "187680"
  },
  {
    "text": "nodes we want a few things out of that allocation we want compact placement",
    "start": "187680",
    "end": "193040"
  },
  {
    "text": "what does that mean it means things that are close to each other so in in uh TPUs",
    "start": "193040",
    "end": "198239"
  },
  {
    "text": "in particular you have topological constraints but even with GPUs like you'd want them all out of the same rack",
    "start": "198239",
    "end": "203920"
  },
  {
    "text": "right you you know ideally because then they're only one hop away well with GPUs we have similar kind of things where",
    "start": "203920",
    "end": "210159"
  },
  {
    "text": "there's wiring between the chips and we'll see it in a minute and we can create certain arrangements and certain",
    "start": "210159",
    "end": "216080"
  },
  {
    "text": "topologies we want to make sure that the pods land on nodes that are close to each other in some way we also want if",
    "start": "216080",
    "end": "223080"
  },
  {
    "text": "we're doing multiple jobs that are grabbing therefore multiple nodes and we",
    "start": "223080",
    "end": "229840"
  },
  {
    "text": "launch them simultaneously we don't want them to pollute each other's um compact",
    "start": "229840",
    "end": "235120"
  },
  {
    "text": "placement groups so we want to have some way to atomically kind of partition",
    "start": "235120",
    "end": "240239"
  },
  {
    "text": "those requests into the sets of nodes and then we want a way to deal with",
    "start": "240239",
    "end": "245280"
  },
  {
    "text": "failures that doesn't require you to wake up at 3:00 a.m and go make changes to some podspec",
    "start": "245280",
    "end": "251159"
  },
  {
    "text": "somewhere how does it work today well this is the multi-node",
    "start": "251159",
    "end": "257639"
  },
  {
    "text": "um this is kind of the the hardware layer infrastructure in one of these these multi-node um TPU slices as we",
    "start": "257639",
    "end": "265759"
  },
  {
    "text": "call them and in this case you're seeing 16 nodes each of those nodes has four",
    "start": "265759",
    "end": "272160"
  },
  {
    "text": "TPUs and they're interconnected with particular wiring that's programmable",
    "start": "272160",
    "end": "277280"
  },
  {
    "text": "but it's it's you know there's a hardware layer that's fixed um you can",
    "start": "277280",
    "end": "283919"
  },
  {
    "text": "segment that then dynamically into these different partitions where you know you",
    "start": "283919",
    "end": "289600"
  },
  {
    "text": "can consume one whole node at a time you can consume two nodes at a time you can consume four nodes at a time eight nodes",
    "start": "289600",
    "end": "296000"
  },
  {
    "text": "at a time 16 nodes at a time and there's different um uh ways or arrangements",
    "start": "296000",
    "end": "301520"
  },
  {
    "text": "topologies um by which you can do that we're showing a certain set of them here",
    "start": "301520",
    "end": "307039"
  },
  {
    "text": "um to be to illustrate it so um how do we do this in in in",
    "start": "307039",
    "end": "312800"
  },
  {
    "text": "Kubernetes today well we'll label each of those so if I go back a slide oh if I",
    "start": "312800",
    "end": "317919"
  },
  {
    "text": "go back a slide you see the little dotted lines around each of those sets of nodes we can associate a node label",
    "start": "317919",
    "end": "324880"
  },
  {
    "text": "with each of those dotted lines um if we do that you know and then we're going to",
    "start": "324880",
    "end": "330479"
  },
  {
    "text": "put that that node label on our p on our say our job spec um so all the the pods",
    "start": "330479",
    "end": "336720"
  },
  {
    "text": "will be confined to to those set of labeled nodes um if we do that we probably want",
    "start": "336720",
    "end": "343199"
  },
  {
    "text": "to do the partitioning in a way that they don't overlap otherwise if two different users schedule two different",
    "start": "343199",
    "end": "349600"
  },
  {
    "text": "jobs and they use label selectors that are uh overlapping then they might",
    "start": "349600",
    "end": "356479"
  },
  {
    "text": "actually take away a node a node that's need is needed for a different job",
    "start": "356479",
    "end": "362600"
  },
  {
    "text": "um so what that means is I guess we have it summarized in the next slide is that",
    "start": "362600",
    "end": "368000"
  },
  {
    "text": "like we have to choose between kind of this static partitioning which means we may not get full utilization if we have",
    "start": "368000",
    "end": "373919"
  },
  {
    "text": "different jobs of different sizes trying to run um or we have to risk those kind of race",
    "start": "373919",
    "end": "381199"
  },
  {
    "text": "conditions we have to coordinate between two different job authors about which",
    "start": "381199",
    "end": "386400"
  },
  {
    "text": "label selectors they choose because it's not scheduled it's it's sort of",
    "start": "386400",
    "end": "391919"
  },
  {
    "text": "manually specified in the spec and so the people have to talk to each other um",
    "start": "391919",
    "end": "398160"
  },
  {
    "text": "secondly if if if there is a failure of a node um you have to kind of go and",
    "start": "398160",
    "end": "403600"
  },
  {
    "text": "back out the job and change the label selector and reinitiate the job um so",
    "start": "403600",
    "end": "409759"
  },
  {
    "text": "that that's not really the kind of self-healing we expect from a Kubernetes solution",
    "start": "409759",
    "end": "416240"
  },
  {
    "text": "uh so let's see let's kind of walk through what that looks like in a real use case um here we have uh we we run a",
    "start": "416240",
    "end": "425199"
  },
  {
    "text": "job it needs a 4x4 slice for meaning four nodes by four TPUs so we manually",
    "start": "425199",
    "end": "432319"
  },
  {
    "text": "go into some spreadsheet or something and we say oh that's this label selector and so we go and we set that that node",
    "start": "432319",
    "end": "439199"
  },
  {
    "text": "selector on our job and um the scheduler comes in and it starts scheduling these",
    "start": "439199",
    "end": "445120"
  },
  {
    "text": "pods and it's working pretty well and everybody's happy and it gets almost done but then node 12 goes offline for",
    "start": "445120",
    "end": "452080"
  },
  {
    "text": "some reason i don't know you know there's a kernel bug and it crashes so what happens we can't schedule that pod",
    "start": "452080",
    "end": "460039"
  },
  {
    "text": "now we're stuck it's just going to sit there forever unless we actually go in",
    "start": "460039",
    "end": "465680"
  },
  {
    "text": "and we kill that job and we create a new job with a different we go in and manually pick a different 4x4 slice and",
    "start": "465680",
    "end": "473199"
  },
  {
    "text": "uh a different node label or node selector and we rerun resubmit the job",
    "start": "473199",
    "end": "478639"
  },
  {
    "text": "the the scheduler can't do it for us because that node selector is fixed in our in our",
    "start": "478639",
    "end": "485639"
  },
  {
    "text": "spec so what do we do about it well one",
    "start": "485639",
    "end": "490720"
  },
  {
    "text": "we we can use an over-the-top solution like Q which is awesome we've got that's that's also one of I work with a bunch",
    "start": "490720",
    "end": "496560"
  },
  {
    "text": "of people who build Q and so that that can really help in these kind of situations um but we're also working on",
    "start": "496560",
    "end": "502720"
  },
  {
    "text": "something uh called DRRA which you may have heard of in other context contexts",
    "start": "502720",
    "end": "508639"
  },
  {
    "text": "and we're going to give you a crash course on it right now but this dynamic resource allocation is a a new resource",
    "start": "508639",
    "end": "515120"
  },
  {
    "text": "request API in Kubernetes that as I mentioned uh Patrick and Kevin and others have been working on for quite",
    "start": "515120",
    "end": "521518"
  },
  {
    "text": "some time um we're beta in 132 and the features that we're talking about here",
    "start": "521519",
    "end": "527120"
  },
  {
    "text": "today are coming in 133 so Pat uh Morton why don't you uh take over here yeah so",
    "start": "527120",
    "end": "535279"
  },
  {
    "text": "DRA um I guess is that a lot of people here probably heard about it but we're going to do a quick intro here um it's",
    "start": "535279",
    "end": "543040"
  },
  {
    "text": "essentially an API for requesting devices and making them available to pods and containers um can",
    "start": "543040",
    "end": "551519"
  },
  {
    "text": "think of it as has four parts the first is an API to describe devices this is",
    "start": "551519",
    "end": "558880"
  },
  {
    "text": "what is a resource slice API and this gives like nodes the possibility makes",
    "start": "558880",
    "end": "565440"
  },
  {
    "text": "it possible for node to say like I have a Nvidia GPU and it has 40 GB of",
    "start": "565440",
    "end": "572440"
  },
  {
    "text": "memory um the second part is an API for letting users request devices so that's",
    "start": "572440",
    "end": "581519"
  },
  {
    "text": "what we call a resource claim uh and that might be like I need two Nvidia",
    "start": "581519",
    "end": "587560"
  },
  {
    "text": "GPUs each of which must have at least 30 GB of",
    "start": "587560",
    "end": "592760"
  },
  {
    "text": "memory um part three is essentially the thing that brings those things together",
    "start": "592760",
    "end": "599200"
  },
  {
    "text": "it takes the available devices and the request from in the form of the resource",
    "start": "599200",
    "end": "605040"
  },
  {
    "text": "claims and then allocates devices to satisfy claims",
    "start": "605040",
    "end": "612399"
  },
  {
    "text": "finally the fourth part is the cubit API that actuates the decisions by the",
    "start": "612399",
    "end": "619079"
  },
  {
    "text": "scheduler so makes the devices available to pods and",
    "start": "619079",
    "end": "624760"
  },
  {
    "text": "containers so this is an example of the driver side so you can see a dur",
    "start": "624760",
    "end": "632399"
  },
  {
    "text": "resource driver that then is a cublet plugin that publishes one or more resource slices which has a list of",
    "start": "632399",
    "end": "639880"
  },
  {
    "text": "devices and those then become available in the API server the scheduleuler can",
    "start": "639880",
    "end": "646079"
  },
  {
    "text": "see them so that means that the scheduleuler understands what's available in the",
    "start": "646079",
    "end": "651720"
  },
  {
    "text": "cluster and this information is also available to the Kubernetes autoscaler which means",
    "start": "651720",
    "end": "658720"
  },
  {
    "text": "that it can cons take devices in into consideration when it for autoscaling",
    "start": "658720",
    "end": "664399"
  },
  {
    "text": "decisions uh which is one of the uh one of the big advantages of this latest iteration of",
    "start": "664399",
    "end": "671920"
  },
  {
    "text": "DRRA um and then we have the sort of c",
    "start": "671920",
    "end": "677360"
  },
  {
    "text": "consumer side which is the resource claim which lets users specify a set of",
    "start": "677360",
    "end": "683200"
  },
  {
    "text": "requests which specifies like which type of device how many and through selectors",
    "start": "683200",
    "end": "689120"
  },
  {
    "text": "which typically sell expressions can specify like things like it needs at",
    "start": "689120",
    "end": "694640"
  },
  {
    "text": "least 30 gigs of memory or it needs a certain number of cores",
    "start": "694640",
    "end": "699680"
  },
  {
    "text": "um these resource claims are then referenced by pods and pods can",
    "start": "699680",
    "end": "704720"
  },
  {
    "text": "reference multiple resource claims or a resource claims can be referenced by multiple pods in the letter case all",
    "start": "704720",
    "end": "712640"
  },
  {
    "text": "those pods will then share the devices that are allocated to that claim we'll we'll get back to that",
    "start": "712640",
    "end": "719120"
  },
  {
    "text": "a little bit later um and yeah the resource claim",
    "start": "719120",
    "end": "724279"
  },
  {
    "text": "is used by the scheduler the results of the decisions that the schedule make is",
    "start": "724279",
    "end": "731120"
  },
  {
    "text": "written into the resource claim status and that is used by the cubelet to to um",
    "start": "731120",
    "end": "737920"
  },
  {
    "text": "make those devices available to uh the containers",
    "start": "737920",
    "end": "743160"
  },
  {
    "text": "um but I describe now and in general the simplest use case that is often used for the array is like the devices that are",
    "start": "743160",
    "end": "751760"
  },
  {
    "text": "attached to individual nodes but the array can also model devices that are",
    "start": "751760",
    "end": "757680"
  },
  {
    "text": "accessible for multiple nodes that could be through things like the network",
    "start": "757680",
    "end": "763440"
  },
  {
    "text": "um and as Sean mentioned in 133 there's a few useful features that are being",
    "start": "763440",
    "end": "769200"
  },
  {
    "text": "added um the first one is what called partitional devices which makes it",
    "start": "769200",
    "end": "775040"
  },
  {
    "text": "possible to model devices that can be partitioned into smaller overlap devices",
    "start": "775040",
    "end": "780800"
  },
  {
    "text": "like Nvidia MIGs is an example of this um we're also adding device taints that",
    "start": "780800",
    "end": "787680"
  },
  {
    "text": "makes it possible to taint devices not not nodes that is already available and",
    "start": "787680",
    "end": "794079"
  },
  {
    "text": "this makes it possible to automatically evict all pods that has a claim on a",
    "start": "794079",
    "end": "801320"
  },
  {
    "text": "device and by combining these features we can then provide a a solution for",
    "start": "801320",
    "end": "808560"
  },
  {
    "text": "multihost accelerators um so yeah let's look at what this looks",
    "start": "808560",
    "end": "814240"
  },
  {
    "text": "like for um how we model those devices so this is",
    "start": "814240",
    "end": "819279"
  },
  {
    "text": "the same um set of TPUs and nodes as John showed earlier and what we do is",
    "start": "819279",
    "end": "827040"
  },
  {
    "text": "that we map slices each slice becomes a logical",
    "start": "827040",
    "end": "832800"
  },
  {
    "text": "device so and each of those devices will then advertise their capacity and",
    "start": "832800",
    "end": "839480"
  },
  {
    "text": "capacity in this example is the node the number of TPUs so in this example like the 8 by8",
    "start": "839480",
    "end": "847360"
  },
  {
    "text": "slice would be advertised as a device with 64 TPUs and similar for the",
    "start": "847360",
    "end": "853320"
  },
  {
    "text": "others um each device will also have a node selector that defines which nodes",
    "start": "853320",
    "end": "860399"
  },
  {
    "text": "is this device available at and like if you look at the let's say the 4x4 slice",
    "start": "860399",
    "end": "866800"
  },
  {
    "text": "on the top left it will have a node selector that selects node 1 2 5 and",
    "start": "866800",
    "end": "873320"
  },
  {
    "text": "six um and yeah as we also notice in this case there are overlap so a single",
    "start": "873320",
    "end": "879279"
  },
  {
    "text": "device is a member of multiple devices and the reason that we can do that is",
    "start": "879279",
    "end": "886560"
  },
  {
    "text": "that the array understands the relationship between these devices so in",
    "start": "886560",
    "end": "891839"
  },
  {
    "text": "this example if the 2x4 slice here is allocated the array",
    "start": "891839",
    "end": "897839"
  },
  {
    "text": "understands that the 4x4 and the two 2x2s that are formed by by node 11 and",
    "start": "897839",
    "end": "904760"
  },
  {
    "text": "12 cannot be allocated so the array allows the the same",
    "start": "904760",
    "end": "911600"
  },
  {
    "text": "underlying device to be advertised as part of multiple devices but it makes",
    "start": "911600",
    "end": "916959"
  },
  {
    "text": "sure that only a single of those devices can be allocated at the same",
    "start": "916959",
    "end": "923639"
  },
  {
    "text": "time yeah so if we look at how consuming these um multihost TPUs would look like",
    "start": "923639",
    "end": "931040"
  },
  {
    "text": "here's an example of a resource claim and you can see that it has",
    "start": "931040",
    "end": "936920"
  },
  {
    "text": "a it uses the TPU device class and it has a selector that essentially says we",
    "start": "936920",
    "end": "943760"
  },
  {
    "text": "want a TPU where the number of TPUs equals to 16",
    "start": "943760",
    "end": "950399"
  },
  {
    "text": "and as we mentioned each node has four TPUs with 16 TPUs that means we'll have",
    "start": "950399",
    "end": "958000"
  },
  {
    "text": "four nodes and that means also means we need four pods and in this example all those pods",
    "start": "958000",
    "end": "966680"
  },
  {
    "text": "will reference the same resource claim so it's an example of multiple pods",
    "start": "966680",
    "end": "971920"
  },
  {
    "text": "sharing the same resource claim and the that reference is sort of an example of",
    "start": "971920",
    "end": "977759"
  },
  {
    "text": "that is shown on u the right so um if you look at how this",
    "start": "977759",
    "end": "985279"
  },
  {
    "text": "allocation will happen um we have a resource claim that says give me a",
    "start": "985279",
    "end": "990720"
  },
  {
    "text": "device with 16 TPUs in our cluster we have four devices",
    "start": "990720",
    "end": "996800"
  },
  {
    "text": "that satisfy that uh resource claim and it's up to theuler to decide",
    "start": "996800",
    "end": "1005279"
  },
  {
    "text": "which one should be used uh today the scheduler or essentially the the arrayul",
    "start": "1005279",
    "end": "1012279"
  },
  {
    "text": "plugin does this with like a first fit so it looks at what's available the first time it finds something that works",
    "start": "1012279",
    "end": "1018399"
  },
  {
    "text": "it allocates that one in the future we're looking at implementing a scoring which would allow it to implement sort",
    "start": "1018399",
    "end": "1025839"
  },
  {
    "text": "of a not a first fit but a best fit and best would then mean according to some",
    "start": "1025839",
    "end": "1032839"
  },
  {
    "text": "criteria um but yeah um when the",
    "start": "1032839",
    "end": "1039079"
  },
  {
    "text": "um when the scheduleuler then decides on a device it'll then do that as part of",
    "start": "1039079",
    "end": "1047760"
  },
  {
    "text": "scheduling of the first pod so the first time a pods the scheduleuler sees a pod",
    "start": "1047760",
    "end": "1053360"
  },
  {
    "text": "that references a resource claim that's when it makes this decision and then the",
    "start": "1053360",
    "end": "1059280"
  },
  {
    "text": "scheduleuler will then select the device the device has a node selector as",
    "start": "1059280",
    "end": "1064799"
  },
  {
    "text": "you mentioned earlier um and that will then tell theuler which nodes in the",
    "start": "1064799",
    "end": "1072280"
  },
  {
    "text": "cluster those workloads can run on and in this case we see it selected the",
    "start": "1072280",
    "end": "1077760"
  },
  {
    "text": "bottom left 4x4 and it decided to place the first uh",
    "start": "1077760",
    "end": "1084320"
  },
  {
    "text": "worker pod on node 11 and this node selector",
    "start": "1084320",
    "end": "1092200"
  },
  {
    "text": "then restricts the pods where any other pods referring the same resource claim",
    "start": "1092200",
    "end": "1099039"
  },
  {
    "text": "can run so um this is sort of similar to what John described earlier with the",
    "start": "1099039",
    "end": "1106600"
  },
  {
    "text": "um device plugin but in that case the the decision of which nodes the pods",
    "start": "1106600",
    "end": "1114000"
  },
  {
    "text": "could run on was provided by the user through selectors on the pod but now",
    "start": "1114000",
    "end": "1119600"
  },
  {
    "text": "theuler gets information from the device that it selected so this happens",
    "start": "1119600",
    "end": "1126840"
  },
  {
    "text": "dynamically and yeah this we see the first pods ends up on node 11 the other",
    "start": "1126840",
    "end": "1133200"
  },
  {
    "text": "ones in a schedule queue queue and they'll all get scheduled on the devices that belong to this device and then the",
    "start": "1133200",
    "end": "1140400"
  },
  {
    "text": "workload can run now circling back to the example earlier what happens now if node 12",
    "start": "1140400",
    "end": "1148080"
  },
  {
    "text": "turns out to be to fail we end up in the same situation at first the fourth pod",
    "start": "1148080",
    "end": "1154640"
  },
  {
    "text": "cannot schedule but with device taints that was added is being added in 133 the",
    "start": "1154640",
    "end": "1162799"
  },
  {
    "text": "driver can now taint the device with a no execute effect and what happens then",
    "start": "1162799",
    "end": "1169760"
  },
  {
    "text": "is that all pods that references resource claims that allocate that",
    "start": "1169760",
    "end": "1176000"
  },
  {
    "text": "device will automatically be um be evicted so all those pods will go",
    "start": "1176000",
    "end": "1182400"
  },
  {
    "text": "back into the pending state they'll be up they'll be picked up by the scheduleuler again and the time of the",
    "start": "1182400",
    "end": "1188320"
  },
  {
    "text": "first pod that the schedule sees it'll pick a new device and all those pods",
    "start": "1188320",
    "end": "1193679"
  },
  {
    "text": "will be scheduled on new nodes and the",
    "start": "1193679",
    "end": "1198799"
  },
  {
    "text": "workload can continue running uh this is then obviously different from",
    "start": "1198799",
    "end": "1204160"
  },
  {
    "text": "what you saw with the device plugin where this required a the user to actually go in and start changing you",
    "start": "1204160",
    "end": "1211520"
  },
  {
    "text": "can you can stay stay asleep when this happens at 3:00 a.m correct um there a few open issues or",
    "start": "1211520",
    "end": "1220160"
  },
  {
    "text": "things that we're still looking to improve in the array um in the example now I mentioned that the the nodes or",
    "start": "1220160",
    "end": "1227840"
  },
  {
    "text": "the pods end up on the right nodes um what the array does not guarantee",
    "start": "1227840",
    "end": "1233360"
  },
  {
    "text": "today is that those four pods end up one on each node in",
    "start": "1233360",
    "end": "1239600"
  },
  {
    "text": "theory they could all get scheduled on the same node um there are few ways",
    "start": "1239600",
    "end": "1244720"
  },
  {
    "text": "around this um you can make sure that or request enough resources on each that",
    "start": "1244720",
    "end": "1250159"
  },
  {
    "text": "each pod request enough resources that only one can fit on a node at a time forcing theuler to place them on",
    "start": "1250159",
    "end": "1256240"
  },
  {
    "text": "different ones it's possible to use anti- affinity to force them onto different nodes or it's al possible to",
    "start": "1256240",
    "end": "1263039"
  },
  {
    "text": "use a per pod claim for all TPUs on each node um but all of these are sort of",
    "start": "1263039",
    "end": "1271600"
  },
  {
    "text": "increases the complexity a bit and we're looking to simplify that um we talked",
    "start": "1271600",
    "end": "1276640"
  },
  {
    "text": "about scoring so making sure that we choose the best fit not just the first fit and we're also looking to support",
    "start": "1276640",
    "end": "1284640"
  },
  {
    "text": "preeemption so a higher priority workload can uh preempt",
    "start": "1284640",
    "end": "1290880"
  },
  {
    "text": "a lower priority one so it gets to run and gets to sort of steal the device and",
    "start": "1290880",
    "end": "1296159"
  },
  {
    "text": "um yeah and thanks Morton um yes so so that's",
    "start": "1296159",
    "end": "1302799"
  },
  {
    "text": "kind of the story of how we do that with TPUs um as I mentioned early on GPUs",
    "start": "1302799",
    "end": "1308240"
  },
  {
    "text": "like especially when you look at something like the GB200 which has 72 GPUs um it's using DRRA as well for",
    "start": "1308240",
    "end": "1317039"
  },
  {
    "text": "managing and coordinating um the allocation of the in the configuration of the different um compute domains and",
    "start": "1317039",
    "end": "1325360"
  },
  {
    "text": "in uh NV IMAX domains kevin is is here um from Nvidia if and he can be happy to",
    "start": "1325360",
    "end": "1333039"
  },
  {
    "text": "chat with you afterwards if you have any questions specifically on on how we're doing that uh with um within with the",
    "start": "1333039",
    "end": "1340559"
  },
  {
    "text": "the the GPUs and and uh Patrick and Kevin have a talk tomorrow um giving an",
    "start": "1340559",
    "end": "1347440"
  },
  {
    "text": "update on what uh the working group responsible for DRRA in the Kubernetes community has been doing and uh that's",
    "start": "1347440",
    "end": "1354640"
  },
  {
    "text": "another great great uh talk to go to if you're interested in and all of this um",
    "start": "1354640",
    "end": "1361559"
  },
  {
    "text": "so there's a bunch more talks this deck is up on the um the gray ones are",
    "start": "1361559",
    "end": "1368400"
  },
  {
    "text": "already already happened I think but this this deck is up on the um sketch uh",
    "start": "1368400",
    "end": "1375120"
  },
  {
    "text": "and so if you download it the PDF you'll have links to the sketch entries for all the different talks associated with the",
    "start": "1375120",
    "end": "1381840"
  },
  {
    "text": "RA um so we're opening the floor to questions",
    "start": "1381840",
    "end": "1387559"
  },
  {
    "text": "now i think there's a mic here if anybody wants to line",
    "start": "1387559",
    "end": "1393440"
  },
  {
    "text": "[Applause]",
    "start": "1394730",
    "end": "1402039"
  },
  {
    "text": "up while we're waiting for people to line up I'll I'll mention 133 is not yet released and so",
    "start": "1402039",
    "end": "1411679"
  },
  {
    "text": "some of the things we're talking about here aren't available yet and even when they are they'll be alpha but um we",
    "start": "1411679",
    "end": "1417600"
  },
  {
    "text": "think it's important for people to get a head start on what we're building yes sir",
    "start": "1417600",
    "end": "1423039"
  },
  {
    "text": "last year in Paris uh DRRA was still felt a bit controversial or up in the air it seems like it's much more stable",
    "start": "1423039",
    "end": "1429440"
  },
  {
    "text": "now is that a good read or yes absolutely so so out of Paris we formed",
    "start": "1429440",
    "end": "1436240"
  },
  {
    "text": "this working group uh along with another working group uh called working group serving to try and address two of the",
    "start": "1436240",
    "end": "1443600"
  },
  {
    "text": "biggest issues we are seeing around a IML workloads one was Kubernetes relationship to the hardware that's",
    "start": "1443600",
    "end": "1450080"
  },
  {
    "text": "working group device management and DRRA was one of the you know is basically the the main thing we've been working on",
    "start": "1450080",
    "end": "1456159"
  },
  {
    "text": "there and we revised kind of the design to be more uh amendable to autoscaling",
    "start": "1456159",
    "end": "1463679"
  },
  {
    "text": "and that's how we uh came where we are we've done a a ton of work in the last year and um made a lot of progress and",
    "start": "1463679",
    "end": "1470720"
  },
  {
    "text": "and I it seems like uh the design as far as I understand anyway meets everyone's",
    "start": "1470720",
    "end": "1476400"
  },
  {
    "text": "kind of concerns that they had with competing approaches that's kind of unified everything is that a good read or I mean it's not perfect but nothing",
    "start": "1476400",
    "end": "1484320"
  },
  {
    "text": "ever is but it I think it's people are pretty satisfied with it there there's a couple of like if you look at the",
    "start": "1484320",
    "end": "1490799"
  },
  {
    "text": "original use cases document for like say the the the GPU use cases there's a few",
    "start": "1490799",
    "end": "1496799"
  },
  {
    "text": "that probably we will struggle to meet with this design very few though out of",
    "start": "1496799",
    "end": "1502559"
  },
  {
    "text": "those that list there's also a bunch of additional things we're doing I think that um that you know weren't even in",
    "start": "1502559",
    "end": "1510559"
  },
  {
    "text": "those original discussions so that and now it's autoscalable right so that the",
    "start": "1510559",
    "end": "1516559"
  },
  {
    "text": "the data is there to drive scaling decisions in a way that it wasn't before",
    "start": "1516559",
    "end": "1523279"
  },
  {
    "text": "awesome thanks so much hello thank you for the great",
    "start": "1523279",
    "end": "1529640"
  },
  {
    "text": "presentation correct me if I'm wrong what I see as a DRA even though it says resource it is 101 mapping to a device",
    "start": "1529640",
    "end": "1538240"
  },
  {
    "text": "so when you think about resources it's much more than devices it could be say",
    "start": "1538240",
    "end": "1543919"
  },
  {
    "text": "between GPUs there is a throughput latency power all the different resource",
    "start": "1543919",
    "end": "1550400"
  },
  {
    "text": "ingredients tied to each other how are you going to address that in DRRA thank",
    "start": "1550400",
    "end": "1555919"
  },
  {
    "text": "you that's a great question um so today we",
    "start": "1555919",
    "end": "1561520"
  },
  {
    "text": "don't manage um standard resources CPU memory huge pages ephemeral disc things",
    "start": "1561520",
    "end": "1568480"
  },
  {
    "text": "like that in DRRA that's managed um as it's always been um DRRA has been really",
    "start": "1568480",
    "end": "1575919"
  },
  {
    "text": "focused on devices thus far we do allow",
    "start": "1575919",
    "end": "1581039"
  },
  {
    "text": "we're working on a number of features I don't think I have the no I don't have",
    "start": "1581039",
    "end": "1586080"
  },
  {
    "text": "the list here of all the features we're working on but if Come to is it tomorrow Patrick yeah tomorrow there's the uh",
    "start": "1586080",
    "end": "1594400"
  },
  {
    "text": "workg groupoup devices update meeting and you'll see the list of things we're working on and one of those is what we call consumable capacity and it allows",
    "start": "1594400",
    "end": "1601440"
  },
  {
    "text": "devices to publish capacity values the same way nodes publish capacity values",
    "start": "1601440",
    "end": "1607200"
  },
  {
    "text": "and then allows users to request those via a claim and rather than so today sharing",
    "start": "1607200",
    "end": "1614480"
  },
  {
    "text": "is done by pointing multiple pods or containers at the same claim which means it's a user initi iated sharing and if",
    "start": "1614480",
    "end": "1624159"
  },
  {
    "text": "we have consumable capacity then for the types of devices that can do it for example nyx doesn't work well for GPUs",
    "start": "1624159",
    "end": "1630720"
  },
  {
    "text": "without like vGPU technology but for ny you can do like uh you could request say",
    "start": "1630720",
    "end": "1636640"
  },
  {
    "text": "a two gigabit uh two gigabit partition out of a 10 gigabit nick and we could",
    "start": "1636640",
    "end": "1643840"
  },
  {
    "text": "let the platform schedule multiple unrelated workloads on top of that nick",
    "start": "1643840",
    "end": "1649039"
  },
  {
    "text": "and consume the capacity out of the nick so we're looking at doing those things it doesn't get to the level of you're",
    "start": "1649039",
    "end": "1654240"
  },
  {
    "text": "talking about where there's relationships between the resources we don't really have a we have some ways to handle that around constraining the set",
    "start": "1654240",
    "end": "1661120"
  },
  {
    "text": "of devices you pick but it's it's quite limited so come to the workg groupoup update meeting come to the Kubernetes",
    "start": "1661120",
    "end": "1667200"
  },
  {
    "text": "community and join us and help us build that",
    "start": "1667200",
    "end": "1672640"
  },
  {
    "text": "hi John uh my name is Yan and from Nvidia so we had uh some discussions and",
    "start": "1672640",
    "end": "1679520"
  },
  {
    "text": "uh yeah offline about how to and handle the additional tolerations right for GPU",
    "start": "1679520",
    "end": "1686080"
  },
  {
    "text": "tents in GKE because yeah so can you comment on that so the problem is yeah",
    "start": "1686080",
    "end": "1691520"
  },
  {
    "text": "so for the uh port request GPU resources use DIA right how can we and uh patch",
    "start": "1691520",
    "end": "1699600"
  },
  {
    "text": "the right so let me explain the issue so in if you're using device plug-in which",
    "start": "1699600",
    "end": "1705840"
  },
  {
    "text": "is the traditional way of allocating devices um at least in GKE and probably",
    "start": "1705840",
    "end": "1710960"
  },
  {
    "text": "in other clouds we we whenever you create a node pool with GPUs or TPUs we",
    "start": "1710960",
    "end": "1716960"
  },
  {
    "text": "taint the nodes and And then if the user forgets to put a toleration on their pod",
    "start": "1716960",
    "end": "1722399"
  },
  {
    "text": "it would never schedule so we actually have a web hook that goes \"Oh oh oh you're using you're using GPUs i'm going",
    "start": "1722399",
    "end": "1727919"
  },
  {
    "text": "to add this toleration so that you can you can schedule.\" The problem with DRRA",
    "start": "1727919",
    "end": "1733120"
  },
  {
    "text": "is that the resource claims are not part of the pod so we can't look at pod admission time and know just by looking",
    "start": "1733120",
    "end": "1740000"
  },
  {
    "text": "at the pod we can't even really load the resource claims that are referred to because it's an eventually consistent",
    "start": "1740000",
    "end": "1746399"
  },
  {
    "text": "system it doesn't have referential integrity like it doesn't have to exist yet and so how do we know to um",
    "start": "1746399",
    "end": "1755640"
  },
  {
    "text": "to tolerate those workloads so I can give you what we want to do from a",
    "start": "1755640",
    "end": "1761919"
  },
  {
    "text": "long-term perspective I haven't figured out the short-term perspective yet but so sig node has had a problem for a long",
    "start": "1761919",
    "end": "1769520"
  },
  {
    "text": "time where for example if you use uh certain features that are require a",
    "start": "1769520",
    "end": "1776080"
  },
  {
    "text": "feature gate to be set on the cublet um the scheduler has no visibility into",
    "start": "1776080",
    "end": "1781200"
  },
  {
    "text": "that and it could deliver your pod to a node that doesn't have that feature gate set and the pod won't run so this is not",
    "start": "1781200",
    "end": "1787919"
  },
  {
    "text": "a new thing this is a pattern throughout the system so we're working with sign node to build infrastructure so that",
    "start": "1787919",
    "end": "1795360"
  },
  {
    "text": "just like taints and tolerations which are sort of administratively controlled we would have a transparent mechanism",
    "start": "1795360",
    "end": "1801520"
  },
  {
    "text": "where nodes publish capabilities and the scheduler analyzes the podspec and",
    "start": "1801520",
    "end": "1806960"
  },
  {
    "text": "related resources and determines what capabilities are needed transparent to",
    "start": "1806960",
    "end": "1812720"
  },
  {
    "text": "the user and then adds those uh uh uh",
    "start": "1812720",
    "end": "1818240"
  },
  {
    "text": "you know considers those mechanisms in the scheduling process so this could be used for things like cublet features and",
    "start": "1818240",
    "end": "1823520"
  },
  {
    "text": "it it could be used to both attract and repel pods from certain sets of nodes so that's our long-term solution i don't",
    "start": "1823520",
    "end": "1829679"
  },
  {
    "text": "know when that's going to happen though so short-term may request uh require a",
    "start": "1829679",
    "end": "1837520"
  },
  {
    "text": "user to manually and uh to it long-term solution will take sometimes right yeah",
    "start": "1837520",
    "end": "1844559"
  },
  {
    "text": "I think there's some things we could I mean you could build a build a controller rather than a web hook that",
    "start": "1844559",
    "end": "1851399"
  },
  {
    "text": "actually waits for like you know when the resource claim gets added that wasn't there at the time the pod was",
    "start": "1851399",
    "end": "1857200"
  },
  {
    "text": "admitted and goes but it gets pretty ugly so short term maybe just the user",
    "start": "1857200",
    "end": "1862559"
  },
  {
    "text": "okay thank you i think we're out of time but um but Morton and I will be uh",
    "start": "1862559",
    "end": "1871440"
  },
  {
    "text": "around here so uh happy to answer any other questions um afterward thank you",
    "start": "1871440",
    "end": "1878320"
  }
]