[
  {
    "text": "hello welcome to sign network intro and deep dive my name is rich runner i work for sunder networks and i've been a kubernetes user",
    "start": "1760",
    "end": "8160"
  },
  {
    "text": "for around three years i'll start off with the introduction and later we'll hear from bowie do works at google about the deep dive and",
    "start": "8160",
    "end": "16560"
  },
  {
    "text": "ongoing efforts within sig network so as i mentioned we'll start off with",
    "start": "16560",
    "end": "24160"
  },
  {
    "text": "kind of an overview some of the basics material to get you started and then we'll transition into a deeper dive",
    "start": "24160",
    "end": "30960"
  },
  {
    "text": "and look at the ongoing work first off what is sig network this is a",
    "start": "30960",
    "end": "37920"
  },
  {
    "text": "special interest group which is a subset of the broader kubernetes community focused on a particular area in this",
    "start": "37920",
    "end": "43760"
  },
  {
    "text": "case it's networking we're charged with pod networking either",
    "start": "43760",
    "end": "49520"
  },
  {
    "text": "between nodes or between pods the service attraction ingress and egress flows as well as the network",
    "start": "49520",
    "end": "54960"
  },
  {
    "text": "policies and access control that a user may want to apply to those flows any of these things sound",
    "start": "54960",
    "end": "61199"
  },
  {
    "text": "interesting i encourage you to join us either on our slack channel segnetwork",
    "start": "61199",
    "end": "67280"
  },
  {
    "text": "the community mailer or you can see us on github under community sig network",
    "start": "67280",
    "end": "74799"
  },
  {
    "text": "this will be available at the end to grab the links",
    "start": "74799",
    "end": "79200"
  },
  {
    "text": "there's many apis in the kubernetes domain some i'd like to call out in particular",
    "start": "80640",
    "end": "87040"
  },
  {
    "text": "which cignet owns are service endpoint and endpoint slice uh this is",
    "start": "87040",
    "end": "92960"
  },
  {
    "text": "everything regarding either the service registration or publishing and",
    "start": "92960",
    "end": "98079"
  },
  {
    "text": "uh from the client side the discovery when you go up a layer from just the networking and start talking",
    "start": "98079",
    "end": "105520"
  },
  {
    "text": "about the application l7 http routing that would fall under the ingress api and feedback from the community as",
    "start": "105520",
    "end": "113119"
  },
  {
    "text": "well as some challenges that have been seen in the initial or v1 rollout have led to this gateway",
    "start": "113119",
    "end": "120399"
  },
  {
    "text": "next generation uh http routing and service ingress there's a lot of exciting work going on",
    "start": "120399",
    "end": "125759"
  },
  {
    "text": "there and we'll hear about that from bowie later on finally we have network policy api which",
    "start": "125759",
    "end": "131120"
  },
  {
    "text": "is your application firewall",
    "start": "131120",
    "end": "134400"
  },
  {
    "text": "so the implementation of these apis and components that kind of build up uh",
    "start": "137440",
    "end": "142800"
  },
  {
    "text": "the responsibility that sig network has first start off with the cubelet",
    "start": "142800",
    "end": "147920"
  },
  {
    "text": "uh cni this is the container network interface this is that low-level network plumbing",
    "start": "147920",
    "end": "155599"
  },
  {
    "text": "you know how does this pod uh have its ip and how is it plugged to the network",
    "start": "155599",
    "end": "161680"
  },
  {
    "text": "cube proxy is responsible for the service layer uh implements that service api and we'll look at",
    "start": "161680",
    "end": "167680"
  },
  {
    "text": "uh some more of the details here in just a moment as far as the controllers we have an",
    "start": "167680",
    "end": "172879"
  },
  {
    "text": "endpoint in point slice controller endpoint slices number feature going ga hopefully in 120",
    "start": "172879",
    "end": "178879"
  },
  {
    "text": "as well as the service load balancer controller it's constantly reconciling the state of the",
    "start": "178879",
    "end": "185840"
  },
  {
    "text": "service objects and ensuring that in the cluster additionally i mentioned ips there's an",
    "start": "185840",
    "end": "191360"
  },
  {
    "text": "ipam controller and by default you have a dns name based discovery with with your",
    "start": "191360",
    "end": "198800"
  },
  {
    "text": "default kubernetes cluster we started to begin to talk about the",
    "start": "198800",
    "end": "205040"
  },
  {
    "text": "networking model it's important to note that while the",
    "start": "205040",
    "end": "210640"
  },
  {
    "text": "networking model dictates all pods can reach all other pods across nodes in any location in the",
    "start": "210640",
    "end": "218640"
  },
  {
    "text": "cluster there is no special meaning necessarily for a pod's ip",
    "start": "218640",
    "end": "223760"
  },
  {
    "text": "it's just like anything else so whether it be a vm or a container on in",
    "start": "223760",
    "end": "229280"
  },
  {
    "text": "the inside of a node depending on your provider networking you have many options a few of them are listed here",
    "start": "229280",
    "end": "236879"
  },
  {
    "text": "like flat or overlays or if you're looking to scale and get",
    "start": "236879",
    "end": "242480"
  },
  {
    "text": "that routing update there are solutions such as bgp to",
    "start": "242480",
    "end": "249120"
  },
  {
    "text": "accomplish that",
    "start": "249120",
    "end": "254319"
  },
  {
    "text": "let's start with the problem at hand i have a client and a bunch of servers",
    "start": "254319",
    "end": "260560"
  },
  {
    "text": "which one should i connect to well hopefully i didn't pick server 2",
    "start": "260639",
    "end": "267600"
  },
  {
    "text": "because after a long enough time period everything will fail eventually",
    "start": "267600",
    "end": "273040"
  },
  {
    "text": "but maybe this is only temporary if we try again i'll pick maybe server 1. how long",
    "start": "273040",
    "end": "281680"
  },
  {
    "text": "can we continue on we're just picking a random ip and hoping that service stays up forever or that server stays up forever",
    "start": "281680",
    "end": "290320"
  },
  {
    "text": "and you know there's a lot of my new issues that come up from clients that",
    "start": "290320",
    "end": "296240"
  },
  {
    "text": "have to either reconnect they might have to reconfigure the ip of the server or",
    "start": "296240",
    "end": "304080"
  },
  {
    "text": "other challenges that we look to solve by abstracting",
    "start": "304800",
    "end": "309840"
  },
  {
    "text": "this this problem so that is the service um hopefully everyone has heard of the",
    "start": "309840",
    "end": "314880"
  },
  {
    "text": "service uh object and it's uh logically placed in between the client and these back-end servers",
    "start": "314880",
    "end": "321600"
  },
  {
    "text": "again so that we can connect to the same service without",
    "start": "321600",
    "end": "327440"
  },
  {
    "text": "having to worry about whether the servers in the back end are going up or down",
    "start": "327440",
    "end": "332800"
  },
  {
    "text": "so service it exposes a group of pods has a durable vip there are cases where",
    "start": "334560",
    "end": "341840"
  },
  {
    "text": "you don't have to take this option if you have smarter clients that can handle dns",
    "start": "341840",
    "end": "346880"
  },
  {
    "text": "changes but in most cases clients like to have that more durable ip and that is",
    "start": "346880",
    "end": "355440"
  },
  {
    "text": "the benefit that you get from using a service to connect to as opposed to direct pod connection",
    "start": "355440",
    "end": "366880"
  },
  {
    "text": "so cube proxy implements us using iptables and ipvs but",
    "start": "366880",
    "end": "374000"
  },
  {
    "text": "if we think about the flow of operations from a client it goes to connect to a service it's",
    "start": "374880",
    "end": "380240"
  },
  {
    "text": "gonna have to resolve that and then eventually uh it'll reach its back-end server",
    "start": "380240",
    "end": "385280"
  },
  {
    "text": "let's kind of watch how that flow uh comes out in real life now the dns pod",
    "start": "385280",
    "end": "392800"
  },
  {
    "text": "is actually a service itself but to avoid uh getting too wrapped up with our",
    "start": "392800",
    "end": "399039"
  },
  {
    "text": "explanation we'll we'll uh ignore that fact for now and just imagine that we can resolve",
    "start": "399039",
    "end": "404800"
  },
  {
    "text": "a service that we're concerned with pod server in this case we query the dns service",
    "start": "404800",
    "end": "411599"
  },
  {
    "text": "it returns the vip that we were talking about the client starts the tcp session or udp",
    "start": "411599",
    "end": "417520"
  },
  {
    "text": "depending on the protocol with this bit and",
    "start": "417520",
    "end": "422720"
  },
  {
    "text": "those cute proxy rules we were talking about translates the step to a actual pod ip",
    "start": "422720",
    "end": "430080"
  },
  {
    "text": "now when we actually meet the backend server we won't have that destination vip in",
    "start": "431280",
    "end": "438080"
  },
  {
    "text": "the packet anymore but the cube proxy has a record that knows how to reverse",
    "start": "438080",
    "end": "444479"
  },
  {
    "text": "this abstraction on the return trip back to the client",
    "start": "444479",
    "end": "451840"
  },
  {
    "text": "so now if we go back to our initial example a server two were to go down",
    "start": "453599",
    "end": "458960"
  },
  {
    "text": "i'm still connecting to the same ip address just just as before the worst thing that i have to do",
    "start": "459919",
    "end": "465199"
  },
  {
    "text": "is have a client start a reconnection so no reconfiguration of other ips and",
    "start": "465199",
    "end": "472319"
  },
  {
    "text": "all in all this tends to work out better than directly contacting the server",
    "start": "472319",
    "end": "479840"
  },
  {
    "text": "so the actual specification for this object has the standard",
    "start": "481120",
    "end": "487039"
  },
  {
    "text": "metadata that you'll see across all kubernetes objects and a selector so the label selection",
    "start": "487039",
    "end": "494479"
  },
  {
    "text": "mechanism is pretty popular across you know not just the networking area but kubernetes in general",
    "start": "494479",
    "end": "500879"
  },
  {
    "text": "the service object leverages that same label selector and it indicates which pods are",
    "start": "500879",
    "end": "509440"
  },
  {
    "text": "implementers or our servers you know of this service abstraction you have to supply a port as well as a",
    "start": "509440",
    "end": "516719"
  },
  {
    "text": "target port the port being the front end",
    "start": "516719",
    "end": "521760"
  },
  {
    "text": "tcp udp depending protocol port that you would contact and the target port is the backend",
    "start": "521760",
    "end": "527200"
  },
  {
    "text": "server so actually having this layer of port abstraction turns out to make",
    "start": "527200",
    "end": "533519"
  },
  {
    "text": "styles of development a lot easier so not only do you get that benefit of the durability of the",
    "start": "533519",
    "end": "540480"
  },
  {
    "text": "client ip address that you're connecting to or dns service additionally you have the ability to",
    "start": "540480",
    "end": "546959"
  },
  {
    "text": "remap protocol ports",
    "start": "546959",
    "end": "550800"
  },
  {
    "text": "the controller will see this new object that you've created and fill out some of the fields if you chose",
    "start": "553920",
    "end": "560399"
  },
  {
    "text": "to not specify them as a user namely in the ports section we see that we",
    "start": "560399",
    "end": "566320"
  },
  {
    "text": "actually have the protocol tcp now which is the default but there are other protocols such as",
    "start": "566320",
    "end": "572320"
  },
  {
    "text": "udp that you can use as well we're allocated a cluster ip and this is",
    "start": "572320",
    "end": "578399"
  },
  {
    "text": "that ephemeral excuse me this is that durable vip that we discussed and that'll stay",
    "start": "578399",
    "end": "584399"
  },
  {
    "text": "throughout the lifetime of this service even as servers selected",
    "start": "584399",
    "end": "589680"
  },
  {
    "text": "by this app my app selector go up and down or come into and out of existence",
    "start": "589680",
    "end": "597040"
  },
  {
    "text": "next we'll hear from bowie",
    "start": "597200",
    "end": "605839"
  },
  {
    "text": "now let's talk about endpoints a service is virtual so how does the proxy know what pods to",
    "start": "607120",
    "end": "613839"
  },
  {
    "text": "send traffic to like most things in kubernetes it's going to be a controller that helps facilitate this the endpoints",
    "start": "613839",
    "end": "620880"
  },
  {
    "text": "controller converts n pods into a smaller set of endpoints and inside the endpoints resource this",
    "start": "620880",
    "end": "627519"
  },
  {
    "text": "represents a list of ips that are behind the service now this is usually pods but it's not always you can manually",
    "start": "627519",
    "end": "634000"
  },
  {
    "text": "populate an endpoint resource as well now recall that service had a port and target port fields so an endpoints",
    "start": "634000",
    "end": "640560"
  },
  {
    "text": "resource can also remap ports you may have a port coming in that remaps to an actual port on the",
    "start": "640560",
    "end": "645760"
  },
  {
    "text": "container and generally endpoints are managed by the system uh the controller we alluded to earlier",
    "start": "645760",
    "end": "652880"
  },
  {
    "text": "but as i said before they can always be manually managed in some cases",
    "start": "652880",
    "end": "657920"
  },
  {
    "text": "now let's look into what a main endpoints controller does so let's say we are given the service",
    "start": "657920",
    "end": "663839"
  },
  {
    "text": "name foo selector app foo ports 80 and a target port",
    "start": "663839",
    "end": "669560"
  },
  {
    "text": "9376. let's say we have a bunch of pods that match the label selector for the",
    "start": "669560",
    "end": "674800"
  },
  {
    "text": "service so these are a bunch of pause that are floating around and they have app foo some of them have app bar some",
    "start": "674800",
    "end": "682320"
  },
  {
    "text": "of the app quoks the service will only select the pods that have apfu",
    "start": "682320",
    "end": "688000"
  },
  {
    "text": "now what happens is that the endpoints controller will then take the pods that match the",
    "start": "688000",
    "end": "694160"
  },
  {
    "text": "service and put them in the endpoints object now the endpoints object doesn't contain",
    "start": "694160",
    "end": "700720"
  },
  {
    "text": "all the information about the pods it only contains the endpoints information which is basically the ip",
    "start": "700720",
    "end": "707120"
  },
  {
    "text": "another way of doing service discovery is dns so dns starts with a specification",
    "start": "707120",
    "end": "713440"
  },
  {
    "text": "describing a records quadruple a records srv ptr records",
    "start": "713440",
    "end": "718560"
  },
  {
    "text": "and this generally runs as a service implemented using pods inside the",
    "start": "718560",
    "end": "724320"
  },
  {
    "text": "cluster but it doesn't have to anything that matches the dns",
    "start": "724320",
    "end": "730000"
  },
  {
    "text": "specification can serve as a kubernetes dns finally the containers",
    "start": "730000",
    "end": "736079"
  },
  {
    "text": "are configured by cubelet to use cubedns or the provider of cubedns",
    "start": "736079",
    "end": "742480"
  },
  {
    "text": "as their dns provider and part of this is actually to use uh libsy search paths",
    "start": "742480",
    "end": "749600"
  },
  {
    "text": "to make it even easier to use the records that come with kubernetes",
    "start": "749600",
    "end": "755040"
  },
  {
    "text": "currently the default implementation is core dns which lots of people run as a service",
    "start": "755040",
    "end": "760800"
  },
  {
    "text": "inside their cluster but you could imagine any such implementation that matches that",
    "start": "760800",
    "end": "766399"
  },
  {
    "text": "specification could also be a provider for dns for kubernetes now let's look into",
    "start": "766399",
    "end": "771839"
  },
  {
    "text": "details about how dns and kubernetes works so here is sort of the most typical record that you",
    "start": "771839",
    "end": "779760"
  },
  {
    "text": "will use it's a record a dns record for a service and you'll see that it's composed of",
    "start": "779760",
    "end": "786079"
  },
  {
    "text": "four pieces the first one is the name of your service then there's the namespace of your",
    "start": "786079",
    "end": "792000"
  },
  {
    "text": "service lives in and then there's just svc which is a sigil or a sort of like a",
    "start": "792000",
    "end": "798240"
  },
  {
    "text": "constant there that tells us that this is a s service name and finally there is the clusters dns",
    "start": "798240",
    "end": "805440"
  },
  {
    "text": "zone in this case we have cluster.local but this is actually not strictly required",
    "start": "805440",
    "end": "810880"
  },
  {
    "text": "you can configure your cluster dns zone during setup to something else now let's",
    "start": "810880",
    "end": "818480"
  },
  {
    "text": "talk about cube proxy so q proxy is",
    "start": "818480",
    "end": "824399"
  },
  {
    "text": "the default implementation of services but one thing to note is like most things in kubernetes it can be replaced",
    "start": "824399",
    "end": "831040"
  },
  {
    "text": "so q proxy is a default implementation it's part of the main repository and it basically allows every pod",
    "start": "831040",
    "end": "839440"
  },
  {
    "text": "and every node on the cluster to access the service virtual ips",
    "start": "839440",
    "end": "847279"
  },
  {
    "text": "to access the pods behind the service what q proxy does in its implementation",
    "start": "847279",
    "end": "854320"
  },
  {
    "text": "is it uses the node as a proxy for traffic from pods on that node",
    "start": "854320",
    "end": "859920"
  },
  {
    "text": "to the back ends of the service and there are actually many different ways to do this we can use iptables ipvs on windows you",
    "start": "859920",
    "end": "867680"
  },
  {
    "text": "can use the windows kernel or there's even a user space option in fact that was the original",
    "start": "867680",
    "end": "873600"
  },
  {
    "text": "implementation on linux iptables and ipvs are the best choice they're in kernel and they're",
    "start": "873600",
    "end": "880079"
  },
  {
    "text": "very high performance the key to q proxy is that it's transparent to the",
    "start": "880079",
    "end": "885360"
  },
  {
    "text": "consumers anyone accessing the service vip virtual ip won't actually have to be",
    "start": "885360",
    "end": "891760"
  },
  {
    "text": "aware of the fact that there's this q proxy in between now what does q proxy do on",
    "start": "891760",
    "end": "897279"
  },
  {
    "text": "the control path well it mainly watches services and endpoints",
    "start": "897279",
    "end": "903440"
  },
  {
    "text": "as part of this watch it applies some filters for example it ignores headless surfaces and then it links the endpoints which",
    "start": "903440",
    "end": "909920"
  },
  {
    "text": "are the back ends the pods that are selected which services which are the front ends",
    "start": "909920",
    "end": "915600"
  },
  {
    "text": "and basically by service we mean the virtual eyepiece it accumulates changes to both and then",
    "start": "915600",
    "end": "921199"
  },
  {
    "text": "updates the kernel state which is the node level proxy on the node itself what does q proxy do",
    "start": "921199",
    "end": "929040"
  },
  {
    "text": "on the data path well on the data path it has to recognize that service traffic exists",
    "start": "929040",
    "end": "934399"
  },
  {
    "text": "for example your pod is sending traffic to a particular destination virtual ip and port then it has to apply",
    "start": "934399",
    "end": "942639"
  },
  {
    "text": "some kind of round robin load balancing technique actually it's random it chooses a back",
    "start": "942639",
    "end": "947839"
  },
  {
    "text": "end and also implements some of the service attributes such as client affinity",
    "start": "947839",
    "end": "953600"
  },
  {
    "text": "on this path then finally it uses network address translation to rewrite",
    "start": "953600",
    "end": "959600"
  },
  {
    "text": "packets to go to a new destination and of course any kind of load balancing with network address translation you",
    "start": "959600",
    "end": "966079"
  },
  {
    "text": "have to undo the address translation on the response a couple of common questions that come",
    "start": "966079",
    "end": "972320"
  },
  {
    "text": "up why not just use dns to do load balancing well people have done this in the past",
    "start": "972320",
    "end": "980320"
  },
  {
    "text": "but what we have found is that typically dns clients are",
    "start": "980320",
    "end": "986480"
  },
  {
    "text": "somewhat historically broken i think one of the common offenders was java",
    "start": "986480",
    "end": "992959"
  },
  {
    "text": "way back in the day and they don't handle changes to dns records very well",
    "start": "992959",
    "end": "998800"
  },
  {
    "text": "you know if you think about it dns typically you do a single lookup and then you kind of",
    "start": "998800",
    "end": "1004399"
  },
  {
    "text": "forget about the fact that that record could change out from under you the next question would be well my",
    "start": "1004399",
    "end": "1009680"
  },
  {
    "text": "clients already can do this kind of you know load balancing inside the client itself common examples are thick clients like",
    "start": "1009680",
    "end": "1017279"
  },
  {
    "text": "the redis client library or for example grpc can i opt out",
    "start": "1017279",
    "end": "1022399"
  },
  {
    "text": "the answer is yes so the way you opt out is you can have a headless service which basically gets a dns name that",
    "start": "1022399",
    "end": "1029280"
  },
  {
    "text": "returns a list of back ends as a records but doesn't really need the virtual ip",
    "start": "1029280",
    "end": "1034400"
  },
  {
    "text": "and hence has no virtual ip services and kubernetes are also how you configure l4 load balancers so q proxy provides",
    "start": "1034400",
    "end": "1042640"
  },
  {
    "text": "l4 load balancing within the cluster and between pods to cluster ip virtual ips",
    "start": "1042640",
    "end": "1050799"
  },
  {
    "text": "of kubernetes services but you can also configure l4 load bounces that take traffic from",
    "start": "1050799",
    "end": "1058400"
  },
  {
    "text": "clients that are outside of the cluster so different lbs work in different ways which is too broad for this talk",
    "start": "1058400",
    "end": "1064960"
  },
  {
    "text": "but basically kubernetes integrates with most cloud providers to use their network load balancer the",
    "start": "1064960",
    "end": "1071520"
  },
  {
    "text": "external load balancer internal load balancer and configure a cloud-based load balancer for your kubernetes service",
    "start": "1071520",
    "end": "1077840"
  },
  {
    "text": "let's talk about ingress so ingress describes a http proxy and routing rules and it's a",
    "start": "1077840",
    "end": "1085039"
  },
  {
    "text": "very simple api what it does is it matches hostnames and url path what an ingress does",
    "start": "1085039",
    "end": "1091039"
  },
  {
    "text": "is it describes a very basic http router and it can target a service for",
    "start": "1091039",
    "end": "1097840"
  },
  {
    "text": "each rule in the ingress kubernetes defines this api but implementations are third party so",
    "start": "1097840",
    "end": "1105440"
  },
  {
    "text": "actually there's many many many implementations of the ingress api out there and we find that there is basically",
    "start": "1105440",
    "end": "1111679"
  },
  {
    "text": "integration with almost every cloud and with every popular software lb here's an example",
    "start": "1111679",
    "end": "1117600"
  },
  {
    "text": "of an ingress configuration on the left the yellow we have the",
    "start": "1117600",
    "end": "1122799"
  },
  {
    "text": "ingress with the host name and the number of pads now since ingress is a l7 concept",
    "start": "1122799",
    "end": "1129280"
  },
  {
    "text": "it typically stitches together multiple services to serve a given for example website",
    "start": "1129280",
    "end": "1137360"
  },
  {
    "text": "so in this case we have a website that has a foo service and a bar service and they're mapped to these http paths foo",
    "start": "1137360",
    "end": "1143919"
  },
  {
    "text": "slash foo and slash bar and they point to a service which is foo service in the",
    "start": "1143919",
    "end": "1149760"
  },
  {
    "text": "service which is bar service which points to the actual back ends of the pods that serve",
    "start": "1149760",
    "end": "1155679"
  },
  {
    "text": "that service so common questions that people ask about ingress one question how is this different from",
    "start": "1155679",
    "end": "1161760"
  },
  {
    "text": "service load balancer so surface load balancer basically is somewhat of a l4",
    "start": "1161760",
    "end": "1167039"
  },
  {
    "text": "ish concept talking about virtual ips ports and protocol like tcp udp now sdtp",
    "start": "1167039",
    "end": "1175679"
  },
  {
    "text": "whereas the ingress talks about an l7 protocol and actually stitches together",
    "start": "1175679",
    "end": "1181919"
  },
  {
    "text": "potentially multiple services into one l7 service",
    "start": "1181919",
    "end": "1188960"
  },
  {
    "text": "next question is why isn't there a controller in the box so what is interesting about at least the",
    "start": "1189200",
    "end": "1196799"
  },
  {
    "text": "ingress space is that there were you know http proxying has been around forever basically",
    "start": "1196799",
    "end": "1203440"
  },
  {
    "text": "and we didn't want to pick any winners there were just so many implementations out there that were very",
    "start": "1203440",
    "end": "1209440"
  },
  {
    "text": "very mature um now looking back this might have been a mistake",
    "start": "1209440",
    "end": "1214880"
  },
  {
    "text": "but given the just the number of implementations i would say that the api is fairly",
    "start": "1214880",
    "end": "1220400"
  },
  {
    "text": "successful even despite this network policy network policy is a way",
    "start": "1220400",
    "end": "1225679"
  },
  {
    "text": "for kubernetes users to describe how pods",
    "start": "1225679",
    "end": "1231679"
  },
  {
    "text": "can talk to each other and how pods actually can't talk to each other it basically describes the allowed call",
    "start": "1231679",
    "end": "1238000"
  },
  {
    "text": "graph for communications for example a back end can talk to a",
    "start": "1238000",
    "end": "1243039"
  },
  {
    "text": "front end a front end can talk to a back end and backend can talk to a db but the front end can never talk directly to db",
    "start": "1243039",
    "end": "1250240"
  },
  {
    "text": "if a front end ever attempts to talk to a db that probably is a compromise or a bug so just like ingress",
    "start": "1250240",
    "end": "1256320"
  },
  {
    "text": "implementations are mostly third party and they're often coupled tightly with the low-level",
    "start": "1256320",
    "end": "1262000"
  },
  {
    "text": "network driver the reason for this is that a lot of the",
    "start": "1262000",
    "end": "1267520"
  },
  {
    "text": "logic for echoling will also involve dealing with the packets that flow",
    "start": "1267520",
    "end": "1273039"
  },
  {
    "text": "between the containers network policy is a very simple api so it has very simple rules",
    "start": "1273039",
    "end": "1279440"
  },
  {
    "text": "and has been designed focused on app owners rather than cluster or network admins so this is something that is currently",
    "start": "1279440",
    "end": "1286400"
  },
  {
    "text": "evolving in the sig and we may need a related but different api for the cluster operators",
    "start": "1286400",
    "end": "1292640"
  },
  {
    "text": "here's an example of how network policy works we have a bunch of front ends on the left you have a bunch of back",
    "start": "1292640",
    "end": "1299039"
  },
  {
    "text": "ends in the middle and we have a bunch of databases in the back and we can write a network policy rule",
    "start": "1299039",
    "end": "1305679"
  },
  {
    "text": "that allows only front ends to talk to back-ends and back-ends to talk to databases",
    "start": "1305679",
    "end": "1311280"
  },
  {
    "text": "but as you see here those red lines are front-end talking to a database or",
    "start": "1311280",
    "end": "1316559"
  },
  {
    "text": "front-end talking to another front end are disallowed and if the front end tries to send traffic in this way",
    "start": "1316559",
    "end": "1322400"
  },
  {
    "text": "the packets will just get dropped let's get into part two which is a deep dive into new work that's going on in the sig",
    "start": "1322400",
    "end": "1329840"
  },
  {
    "text": "okay so in part two of the deep dive we're going to talk about ongoing work in the sig some of these",
    "start": "1329840",
    "end": "1336320"
  },
  {
    "text": "things have reached ga stage some are in alpha and some are in beta topics that we want to highlight today",
    "start": "1336320",
    "end": "1343760"
  },
  {
    "text": "in this session are work on improving dns scalability improving endpoint api scalability",
    "start": "1343760",
    "end": "1352799"
  },
  {
    "text": "improving services expressibility orthogonality and you know being able to",
    "start": "1352799",
    "end": "1360240"
  },
  {
    "text": "map services and uh to express a more complicated role-based uh",
    "start": "1360240",
    "end": "1368640"
  },
  {
    "text": "delegation of responsibilities for services and finally ipv6 dual stack",
    "start": "1368640",
    "end": "1376240"
  },
  {
    "text": "let's talk about node local dns so this has been a perennial topic kubernetes i think there's been",
    "start": "1376240",
    "end": "1384799"
  },
  {
    "text": "uh probably more than five talks at kubecon about how to deal with dns but the truth",
    "start": "1384799",
    "end": "1391520"
  },
  {
    "text": "be told kubernite's dns resource cost is high so first we have expansions of dns queries due to",
    "start": "1391520",
    "end": "1398480"
  },
  {
    "text": "alias names alias names are great because you can create applications",
    "start": "1398480",
    "end": "1403919"
  },
  {
    "text": "that look up names that are sort of independent of which namespace it's living in which uh cluster it's living in but",
    "start": "1403919",
    "end": "1413039"
  },
  {
    "text": "you know implementing these aliases is pretty expensive because it requires this expansion",
    "start": "1413039",
    "end": "1418480"
  },
  {
    "text": "the other interesting thing is that we notice that kubernetes also has high dns requests because of application latency",
    "start": "1418480",
    "end": "1426880"
  },
  {
    "text": "so if you have a bunch of microservices now instead of your vm having",
    "start": "1426880",
    "end": "1433279"
  },
  {
    "text": "a single application you'll actually pack it uh quite full with different applications then they all are doing the",
    "start": "1433279",
    "end": "1439600"
  },
  {
    "text": "same amount of dns we also have noticed that there are many dns heavy application libraries such as",
    "start": "1439600",
    "end": "1446480"
  },
  {
    "text": "node.js and finally because the way dns is typically implemented using",
    "start": "1446480",
    "end": "1451840"
  },
  {
    "text": "cubeproxy uh the way that qproxy uses contract interacts badly with udp contract",
    "start": "1451840",
    "end": "1459919"
  },
  {
    "text": "wants to track connections udp has no sense of connections",
    "start": "1459919",
    "end": "1464960"
  },
  {
    "text": "so what's the solution no local dns and this has gone ga in 1.18.",
    "start": "1464960",
    "end": "1472559"
  },
  {
    "text": "as with most systems problems you know there's the two things you go to cache level of",
    "start": "1472559",
    "end": "1478960"
  },
  {
    "text": "indirection while the solution in this case is to run a cache on every node and add a level of indirection to the",
    "start": "1478960",
    "end": "1485120"
  },
  {
    "text": "cube dns service so we have to be careful about this because as soon as you talk about",
    "start": "1485120",
    "end": "1490720"
  },
  {
    "text": "running something on every node now you have increased the per node overhead and this can become easily",
    "start": "1490720",
    "end": "1496240"
  },
  {
    "text": "dominate in large clusters the other thing that we need to take be aware of is that as a system critical",
    "start": "1496240",
    "end": "1503679"
  },
  {
    "text": "service in a daemon set we need to be really careful about high availability during upgrades and failures",
    "start": "1503679",
    "end": "1510799"
  },
  {
    "text": "let's take a look in detail about how node local dns works so here we have the current",
    "start": "1510799",
    "end": "1517279"
  },
  {
    "text": "way cube dns operates cube dns in yellow on the right are a set of back",
    "start": "1517279",
    "end": "1523440"
  },
  {
    "text": "ends that are part of a service qdns service that has a virtual ip",
    "start": "1523440",
    "end": "1530240"
  },
  {
    "text": "and that virtual ip is used by cubelet to populate the upstream dns server for the pods",
    "start": "1530240",
    "end": "1537840"
  },
  {
    "text": "here's what happens when no local dns is installed on the node node local dns",
    "start": "1537840",
    "end": "1545120"
  },
  {
    "text": "is a daemon set so it runs side by side with the pod on the node",
    "start": "1545120",
    "end": "1551440"
  },
  {
    "text": "and as part of running it also installs a dummy interface and a set of no track rules no track",
    "start": "1551440",
    "end": "1558720"
  },
  {
    "text": "rules basically tell q proxy that if you are sending traffic",
    "start": "1558720",
    "end": "1564240"
  },
  {
    "text": "to the cube dns virtual ip which is 10.0.0.10 actually to skip q proxy and send the",
    "start": "1564240",
    "end": "1571679"
  },
  {
    "text": "packets to the node local dns daemon",
    "start": "1571679",
    "end": "1577120"
  },
  {
    "text": "no local dns will then take those packets and send them out directly to a replica",
    "start": "1577120",
    "end": "1584080"
  },
  {
    "text": "of the cube dns service called cube dns upstream which has a different ip address and that will reach the cube dns or core",
    "start": "1584080",
    "end": "1590480"
  },
  {
    "text": "dns service now what's nice about this is that",
    "start": "1590480",
    "end": "1595919"
  },
  {
    "text": "when no local dns let's say runs into trouble or crashes in some way",
    "start": "1595919",
    "end": "1601520"
  },
  {
    "text": "there is an agent that's a sidecar of no local dns that will remove the no track",
    "start": "1601520",
    "end": "1606799"
  },
  {
    "text": "rule in that case after this crash the no track rule will disappear your",
    "start": "1606799",
    "end": "1613279"
  },
  {
    "text": "pod will then have its dns tracker traffic not captured",
    "start": "1613279",
    "end": "1618559"
  },
  {
    "text": "by the dummy interface in fact it will go through q proxy and it will go back up to the upstream",
    "start": "1618559",
    "end": "1623919"
  },
  {
    "text": "so in this way no local dns is able to transparently insert itself and remove itself from your node without",
    "start": "1623919",
    "end": "1630880"
  },
  {
    "text": "actually disrupting the dns for your pod and this is super important because",
    "start": "1630880",
    "end": "1636240"
  },
  {
    "text": "many applications if dns starts acting up bad things can happen let's talk about",
    "start": "1636240",
    "end": "1642640"
  },
  {
    "text": "endpoint slice endpoint slice is an interesting",
    "start": "1642640",
    "end": "1647840"
  },
  {
    "text": "exercise in figuring out how to scale a kubernetes object that has gotten very",
    "start": "1647840",
    "end": "1653039"
  },
  {
    "text": "big now in larger clusters think 15k nodes",
    "start": "1653039",
    "end": "1658159"
  },
  {
    "text": "and very large services can lead to scalability issues in the api so as you remember the",
    "start": "1658159",
    "end": "1664320"
  },
  {
    "text": "endpoints object contains a list of all the endpoints that are part of a",
    "start": "1664320",
    "end": "1669440"
  },
  {
    "text": "service for a service that's like five pods not a big deal",
    "start": "1669440",
    "end": "1675360"
  },
  {
    "text": "but imagine that you have 5 000 nodes and you have enough endpoints to make up",
    "start": "1675360",
    "end": "1682399"
  },
  {
    "text": "one megabyte so maybe a thousand ten thousand pods in the service then the amount of",
    "start": "1682399",
    "end": "1689200"
  },
  {
    "text": "data that's going to be sent both written to the database and sent to all the",
    "start": "1689200",
    "end": "1694399"
  },
  {
    "text": "watchers in the whole system which is basically q proxy in this case",
    "start": "1694399",
    "end": "1699840"
  },
  {
    "text": "actually becomes quite enormous especially when they get updated so here we have a simple calculation",
    "start": "1699840",
    "end": "1706880"
  },
  {
    "text": "5000 nodes endpoints are one megabyte you'll basically transfer five gigabytes",
    "start": "1706880",
    "end": "1712640"
  },
  {
    "text": "of data which is basically a dvd and then you know if you do a rolling update on the deployment of",
    "start": "1712640",
    "end": "1719120"
  },
  {
    "text": "a 10 000 pod service and each one results in an endpoints",
    "start": "1719120",
    "end": "1724640"
  },
  {
    "text": "update you might end up transferring up to 25 terabytes which is",
    "start": "1724640",
    "end": "1729679"
  },
  {
    "text": "a lot and all it's doing is trying to tell every q proxy what the list of endpoints are so what's",
    "start": "1729679",
    "end": "1736480"
  },
  {
    "text": "the endpoint slice solution so here we have that's the ep and endpoint",
    "start": "1736480",
    "end": "1742559"
  },
  {
    "text": "we have a single update to one of the entries in the endpoint and we end up sending the whole thing",
    "start": "1742559",
    "end": "1747840"
  },
  {
    "text": "to every single cube proxy now with endpoint slice what we have done is we have sliced up the endpoint we",
    "start": "1747840",
    "end": "1755200"
  },
  {
    "text": "have multiple slices sliced objects that represent a single endpoint here if given a single update",
    "start": "1755200",
    "end": "1763919"
  },
  {
    "text": "to one of the slices we only need to send the slice now clearly this is a trade-off between",
    "start": "1763919",
    "end": "1769600"
  },
  {
    "text": "how many slices you have to deal with versus how small you can make that incremental",
    "start": "1769600",
    "end": "1775440"
  },
  {
    "text": "update and that's an interesting design question",
    "start": "1775440",
    "end": "1780720"
  },
  {
    "text": "so endpoint slice to implement it we have a new controller actually two new",
    "start": "1780720",
    "end": "1786000"
  },
  {
    "text": "controllers an endpoint slice controller which basically takes services in the service selector",
    "start": "1786000",
    "end": "1792799"
  },
  {
    "text": "instead of creating an endpoints object actually creates slices multiple objects from the service",
    "start": "1792799",
    "end": "1799279"
  },
  {
    "text": "selector we also have an endpoint slice mirroring controller which creates slices from a",
    "start": "1799279",
    "end": "1804880"
  },
  {
    "text": "selector-less surface and other users can also create endpoint slices",
    "start": "1804880",
    "end": "1810799"
  },
  {
    "text": "and they set a managed by to say that hey these slices belong to some other controller",
    "start": "1810799",
    "end": "1816240"
  },
  {
    "text": "that's not the standard ones managed by kubernetes now one of the challenges of slicing is",
    "start": "1816240",
    "end": "1822640"
  },
  {
    "text": "that you don't have this one-to-one correspondence between a service and for example his endpoints object",
    "start": "1822640",
    "end": "1829679"
  },
  {
    "text": "instead we have this label which is a service name that maps between a bunch of slices",
    "start": "1829679",
    "end": "1836159"
  },
  {
    "text": "and the service that it represents so i said before that an endpoint slice is a trade-off",
    "start": "1836159",
    "end": "1843200"
  },
  {
    "text": "between how many slices you make how small they are and also how do you update them",
    "start": "1843200",
    "end": "1850559"
  },
  {
    "text": "such that you don't end up in the same situation as before you kind of want to do three things you want to keep the number",
    "start": "1850559",
    "end": "1856240"
  },
  {
    "text": "of slices low you want to minimize the changes of slices per update and you want to keep the amount of data",
    "start": "1856240",
    "end": "1862559"
  },
  {
    "text": "sent low so we've settled on basically a fairly simple algorithm for now",
    "start": "1862559",
    "end": "1868880"
  },
  {
    "text": "seems to work okay but really we need information in practice to see how this behaves but",
    "start": "1868880",
    "end": "1875360"
  },
  {
    "text": "the current algorithm does the following first it gets a list of all the endpoint",
    "start": "1875360",
    "end": "1880399"
  },
  {
    "text": "slices corresponding to service it runs through the service selector on the pause to figure out what the set of",
    "start": "1880399",
    "end": "1887120"
  },
  {
    "text": "endpoints should be then it runs through all the slices in memory",
    "start": "1887120",
    "end": "1892640"
  },
  {
    "text": "and it removes stale endpoints in the existing slices next it will fill in new endpoints into",
    "start": "1892640",
    "end": "1898720"
  },
  {
    "text": "the free space left over from the stale endpoints and finally it will create new slices only if there's no more room",
    "start": "1898720",
    "end": "1906000"
  },
  {
    "text": "what this is trying to do is basically try to reuse the existing slice objects",
    "start": "1906000",
    "end": "1911360"
  },
  {
    "text": "without having to create new ones now the thing that's not currently happening here is that there's no active",
    "start": "1911360",
    "end": "1917200"
  },
  {
    "text": "rebalancing for example if you had a bunch of slices and they basically had a bun",
    "start": "1917200",
    "end": "1922799"
  },
  {
    "text": "all the stale end points go down except for one of them those slices will still be there but the claim is that this is",
    "start": "1922799",
    "end": "1929679"
  },
  {
    "text": "kind of pathological and really in practice you wouldn't get that kind of situation",
    "start": "1929679",
    "end": "1935679"
  },
  {
    "text": "and instead we would want to avoid churning deleting and adding slices so what's the status",
    "start": "1935679",
    "end": "1942000"
  },
  {
    "text": "of endpoint slice so in v117 endpoint slice controller was beta in",
    "start": "1942000",
    "end": "1947919"
  },
  {
    "text": "v118 the slice controller was enabled by default but there was no q proxy",
    "start": "1947919",
    "end": "1953840"
  },
  {
    "text": "in 119 everything is beta and everything is enabled by default",
    "start": "1953840",
    "end": "1959200"
  },
  {
    "text": "and finally we're going to ga and 120. so endpoint slice is also interesting",
    "start": "1959200",
    "end": "1964720"
  },
  {
    "text": "because since it's a core api and so fundamentally the operation of the system it actually has taken these many",
    "start": "1964720",
    "end": "1971279"
  },
  {
    "text": "releases to kind of get it right and make sure that if you upgrade or downgrade",
    "start": "1971279",
    "end": "1976640"
  },
  {
    "text": "you won't actually end up in a situation where everything breaks let's talk about services across",
    "start": "1976640",
    "end": "1982080"
  },
  {
    "text": "clusters so as kubernetes installations get bigger multiple clusters is becoming the norm",
    "start": "1982080",
    "end": "1987600"
  },
  {
    "text": "and this is especially true on for example cloud providers where the cost of creating a cluster",
    "start": "1987600",
    "end": "1993279"
  },
  {
    "text": "is to just spin up a bunch of vms lots of reasons to have multiple clusters ha you could",
    "start": "1993279",
    "end": "1999760"
  },
  {
    "text": "have multiple clusters in different regions blast radius of configuration changes so",
    "start": "1999760",
    "end": "2005519"
  },
  {
    "text": "if you do a sort of cluster scope configuration change you can only blow up a small cluster rather than a large",
    "start": "2005519",
    "end": "2012720"
  },
  {
    "text": "cluster geography latency and so forth many kubernetes",
    "start": "2012720",
    "end": "2018080"
  },
  {
    "text": "abstractions in fact most of them have sort of assumed to be cluster-centric services",
    "start": "2018080",
    "end": "2025840"
  },
  {
    "text": "is one of these things surfaces have always been a cluster-centric abstraction and now in sig network and actually sig",
    "start": "2025840",
    "end": "2032640"
  },
  {
    "text": "multi-cluster we're starting to look at how to basically extend the service abstraction",
    "start": "2032640",
    "end": "2038320"
  },
  {
    "text": "and these sets of resources to work across clusters so how does this work given a setup like",
    "start": "2038320",
    "end": "2044799"
  },
  {
    "text": "this how do clients in the front end talk to back ends there are always ways uh existing right",
    "start": "2044799",
    "end": "2050320"
  },
  {
    "text": "now such as talking through load bouncer and it feels like it should be easier than this so what we're looking at uh with the",
    "start": "2050320",
    "end": "2056960"
  },
  {
    "text": "multi-cluster service is something like this that first you define a service",
    "start": "2056960",
    "end": "2062480"
  },
  {
    "text": "just a plain old kubernetes service but you also say that the service is exported but what",
    "start": "2062480",
    "end": "2068800"
  },
  {
    "text": "does that mean so what it means is that the service is exported to some group",
    "start": "2068800",
    "end": "2075200"
  },
  {
    "text": "so we assume that there is some manner of grouping and in the sig multi-cluster this",
    "start": "2075200",
    "end": "2080560"
  },
  {
    "text": "grouping is called a cluster set so it's a set of clusters that are sort of cooperating in the same",
    "start": "2080560",
    "end": "2087040"
  },
  {
    "text": "administrative domain we don't make very strong requirements on this grouping but only",
    "start": "2087040",
    "end": "2093520"
  },
  {
    "text": "that such a grouping exists now this leaves room for interesting implementations",
    "start": "2093520",
    "end": "2099680"
  },
  {
    "text": "by not imposing such a strict requirement but within that grouping there has to be some sort of semantics",
    "start": "2099680",
    "end": "2106079"
  },
  {
    "text": "and one of the semantics that we are asserting in this group is that namespaces",
    "start": "2106079",
    "end": "2111280"
  },
  {
    "text": "between different clusters mean the same thing if you have a prod namespace in this",
    "start": "2111280",
    "end": "2116960"
  },
  {
    "text": "group of clusters in the cluster set that that prod namespace will sort of have the same ownership and",
    "start": "2116960",
    "end": "2123680"
  },
  {
    "text": "admin model across the clusters we can build controllers that operate across these",
    "start": "2123680",
    "end": "2129359"
  },
  {
    "text": "clusters and sort of operate across these name spaces that are spanning cluster what service export",
    "start": "2129359",
    "end": "2136079"
  },
  {
    "text": "does is it triggers the multi-cluster service controller to publish this backend service when you",
    "start": "2136079",
    "end": "2141359"
  },
  {
    "text": "have exported it what this means is that it will create a service import it also creates endpoint",
    "start": "2141359",
    "end": "2148800"
  },
  {
    "text": "slices to represent the union of all endpoints across all exported services",
    "start": "2148800",
    "end": "2154079"
  },
  {
    "text": "in this case it's just one that is exporting now clients in the front end namespace can talk to the backend",
    "start": "2154079",
    "end": "2160720"
  },
  {
    "text": "namespace even though it exists in a different cluster now with services comes dns records multi-cluster service",
    "start": "2160720",
    "end": "2167920"
  },
  {
    "text": "also defines dns records as well just like before we have the name of your service the",
    "start": "2167920",
    "end": "2173280"
  },
  {
    "text": "namespace it lives in and a multi-cluster dns zone a couple of",
    "start": "2173280",
    "end": "2178320"
  },
  {
    "text": "interesting things you may think of is one this is supercluster.local",
    "start": "2178320",
    "end": "2183920"
  },
  {
    "text": "this may be changed and named to clusterset.local the other thing is do cluster sets have their own domains",
    "start": "2183920",
    "end": "2190720"
  },
  {
    "text": "and these are things that we are currently working with uh say multi-cluster to kind of figure out",
    "start": "2190720",
    "end": "2196160"
  },
  {
    "text": "what are the use cases and where this should go kind of as a more advanced example",
    "start": "2196160",
    "end": "2202480"
  },
  {
    "text": "you know that the backend service actually might exist across multiple clusters so you might have a service export and",
    "start": "2202480",
    "end": "2208960"
  },
  {
    "text": "import that actually aggregates the service back ends from both cluster a and cluster b and if you access the",
    "start": "2208960",
    "end": "2216560"
  },
  {
    "text": "cluster set ip or the super cluster ip you will access back-ends from all of these",
    "start": "2216560",
    "end": "2222800"
  },
  {
    "text": "services across clusters is mostly kepware right now but people are very busy hammering out the implementation",
    "start": "2222800",
    "end": "2228400"
  },
  {
    "text": "and the api and the names and we're still working on some of the semantics but one of the big challenges of",
    "start": "2228400",
    "end": "2234320"
  },
  {
    "text": "kubernetes or multi-cluster is that it's okay if the things that you're defining in the",
    "start": "2234320",
    "end": "2239520"
  },
  {
    "text": "api don't conflict but what if you have a bunch of services that are linked",
    "start": "2239520",
    "end": "2244640"
  },
  {
    "text": "together by surface export but you put some config on one cluster that's conflicting with another cluster",
    "start": "2244640",
    "end": "2250560"
  },
  {
    "text": "those things have to be resolved now let's talk about ipv6 ipv4 dual stack ipv6 is something that's",
    "start": "2250560",
    "end": "2258880"
  },
  {
    "text": "been rolling out for ages and kubernetes is no exception and some users will need to run",
    "start": "2258880",
    "end": "2265920"
  },
  {
    "text": "network services and pods in both ipv4 and ipv6 at the same time",
    "start": "2265920",
    "end": "2272079"
  },
  {
    "text": "currently kubernetes only supports one pod ip and this ip has to be either ipv4",
    "start": "2272079",
    "end": "2279440"
  },
  {
    "text": "or ipv6 it cannot be both some users need services that are in",
    "start": "2279440",
    "end": "2284800"
  },
  {
    "text": "both ib families so kubernetes currently only supports one service ib it has to be ipv4 or ipv6",
    "start": "2284800",
    "end": "2292800"
  },
  {
    "text": "so while this change is very small it's actually quite fundamental and i think one of the harder things",
    "start": "2292800",
    "end": "2299040"
  },
  {
    "text": "that we do in computer science is to take something that was singular",
    "start": "2299040",
    "end": "2304480"
  },
  {
    "text": "and make it plural so one question also was wasn't this done already",
    "start": "2304480",
    "end": "2310000"
  },
  {
    "text": "the answer is yes but we found a couple of problems and this actually needed a major reboot",
    "start": "2310000",
    "end": "2316160"
  },
  {
    "text": "what does supporting dual stack mean for resources in fact it affects almost every resource",
    "start": "2316160",
    "end": "2322000"
  },
  {
    "text": "that refers to ip addresses so for pod in the status we have a pod ip right now it has to be either ipv4",
    "start": "2322000",
    "end": "2329599"
  },
  {
    "text": "or ipv6 the way we sort of resolve this is to actually add a second field",
    "start": "2329599",
    "end": "2336560"
  },
  {
    "text": "called pod ips that's the plural of podip and it basically allows us to have",
    "start": "2336560",
    "end": "2342800"
  },
  {
    "text": "more than one ip where one of them is v4 and one of them is v6 there's a bunch of api machinery to",
    "start": "2342800",
    "end": "2349920"
  },
  {
    "text": "handle the fact that the singular field and the plural field have to be in sync",
    "start": "2349920",
    "end": "2355440"
  },
  {
    "text": "in this case it's invalid if you submit a pod resource with a pod ip that's",
    "start": "2355440",
    "end": "2362240"
  },
  {
    "text": "different than the one that appears in the list of pod ips the same change has to be done",
    "start": "2362240",
    "end": "2368000"
  },
  {
    "text": "for node here the pod siders are pluralized and api machinery validation guarantees",
    "start": "2368000",
    "end": "2375119"
  },
  {
    "text": "that the pod cider and the pod ciders remain the same now let's talk about",
    "start": "2375119",
    "end": "2380240"
  },
  {
    "text": "service so here we have service and it has a cluster ip that also has to be",
    "start": "2380240",
    "end": "2387839"
  },
  {
    "text": "pluralized there's an interesting subtlety here which one do we give depending on the",
    "start": "2387839",
    "end": "2395520"
  },
  {
    "text": "user's preference should we always give ipv4 should we give an ipv6 address should",
    "start": "2395520",
    "end": "2403119"
  },
  {
    "text": "one of them be the default and that is kind of the crux of the reboot it turns out that we can't",
    "start": "2403119",
    "end": "2410720"
  },
  {
    "text": "make very strong assumptions about which family is returned",
    "start": "2410720",
    "end": "2416640"
  },
  {
    "text": "in the cluster ip we have various requirements and turns out all",
    "start": "2416640",
    "end": "2422480"
  },
  {
    "text": "of these seem to be valid the user says i need a dual stack",
    "start": "2422480",
    "end": "2428000"
  },
  {
    "text": "i would like dual stack if it's available or i need dual stack and all of these",
    "start": "2428000",
    "end": "2433440"
  },
  {
    "text": "are things that users could request so the solution is that it will default",
    "start": "2433440",
    "end": "2438960"
  },
  {
    "text": "to single stack if users don't express a requirement but if they need a specific requirement they can actually",
    "start": "2438960",
    "end": "2446240"
  },
  {
    "text": "express it in their yamls and this works for headless services node ports and lbs",
    "start": "2446240",
    "end": "2453520"
  },
  {
    "text": "if the cloud provider supports basically dual stack lbs and we're shooting for a second alpha in",
    "start": "2453520",
    "end": "2460400"
  },
  {
    "text": "a 120 it's like a very big pr but it looks like it's on its way there",
    "start": "2460400",
    "end": "2466160"
  },
  {
    "text": "so this is great let's talk about services and the sort of next iteration in how",
    "start": "2466160",
    "end": "2472720"
  },
  {
    "text": "we're thinking about the api so looking at the current service api it's a resource that describes many",
    "start": "2472720",
    "end": "2478240"
  },
  {
    "text": "things it's a method of exposure cluster ip node port load balancer it's a way to group pods so it's like a",
    "start": "2478240",
    "end": "2484880"
  },
  {
    "text": "selector as we saw in the endpoints discussion it also contains a bunch of attributes such as external traffic",
    "start": "2484880",
    "end": "2490640"
  },
  {
    "text": "policy session affinity and so forth it turns out that sort of evolving extending the resource",
    "start": "2490640",
    "end": "2496160"
  },
  {
    "text": "has become quite hard to the fact that all of these fields interact with each other",
    "start": "2496160",
    "end": "2501520"
  },
  {
    "text": "and the other thing that makes this complicated is that this as you see on the right hand side",
    "start": "2501520",
    "end": "2506560"
  },
  {
    "text": "there's actually a hierarchy almost the inheritance of different service types",
    "start": "2506560",
    "end": "2511599"
  },
  {
    "text": "one of the goals of re-looking at services is to evolve the l7 and l4 and kind of",
    "start": "2511599",
    "end": "2518400"
  },
  {
    "text": "break up service into these more orthogonal features along the way we want to",
    "start": "2518400",
    "end": "2524800"
  },
  {
    "text": "provide sort of a way to model different roles that are present in the cluster one of",
    "start": "2524800",
    "end": "2532160"
  },
  {
    "text": "the ideas is to decouple some of these service concepts and",
    "start": "2532160",
    "end": "2539040"
  },
  {
    "text": "features along roll axes now whenever you do a role-based design you have to first",
    "start": "2539040",
    "end": "2544720"
  },
  {
    "text": "think about the personas that you want to kind of address and we kind of split up the",
    "start": "2544720",
    "end": "2549839"
  },
  {
    "text": "roles into the following sets there's that of a cluster provider an infrastructure provider you",
    "start": "2549839",
    "end": "2556480"
  },
  {
    "text": "know this could be your cloud provider or this could be the person that runs your infrastructure as a service",
    "start": "2556480",
    "end": "2561520"
  },
  {
    "text": "there's a cluster operator or network operator who basically manages the entire cluster as a whole and then there are the",
    "start": "2561520",
    "end": "2569119"
  },
  {
    "text": "application developers who basically are given for example namespaces in which to do their thing",
    "start": "2569119",
    "end": "2575280"
  },
  {
    "text": "but not control the entire cluster now the concepts we talked about in the",
    "start": "2575280",
    "end": "2580400"
  },
  {
    "text": "previous slide we also have split up in into",
    "start": "2580400",
    "end": "2585440"
  },
  {
    "text": "different axes so grouping and selection will remain the domain of the kubernetes service routing and",
    "start": "2585440",
    "end": "2592400"
  },
  {
    "text": "protocol specific attributes will be put into a protocol specific routing resource",
    "start": "2592400",
    "end": "2598800"
  },
  {
    "text": "and finally exposure and access is basically the virtual ip the endpoint that the clients will be contacting",
    "start": "2598800",
    "end": "2605440"
  },
  {
    "text": "will be part of gateway and finally at the top of this is gateway class what gateway class does",
    "start": "2605440",
    "end": "2611359"
  },
  {
    "text": "is it says that this particular gateway route and route will be implemented by",
    "start": "2611359",
    "end": "2619040"
  },
  {
    "text": "a particular type of provider very similar to how ingresses can have",
    "start": "2619040",
    "end": "2625200"
  },
  {
    "text": "ingress class or storage can have a storage class",
    "start": "2625200",
    "end": "2630640"
  },
  {
    "text": "let's go through each of these roles and kind of see get a taste of like what the api looks like so the",
    "start": "2630640",
    "end": "2636880"
  },
  {
    "text": "infrastructure provider will basically create a bunch of gateway classes that then are backed by controllers that implement",
    "start": "2636880",
    "end": "2644160"
  },
  {
    "text": "gateway class so these define a type of service access for the clusters for example",
    "start": "2644160",
    "end": "2649839"
  },
  {
    "text": "you can give your consumers of the gateway classes a generic name such as",
    "start": "2649839",
    "end": "2656720"
  },
  {
    "text": "internal proxy internet lb and kind of abstract them from how these things are implemented now",
    "start": "2656720",
    "end": "2663280"
  },
  {
    "text": "this as i said before this is simple similar to storage classes which abstract the implementation of mechanism",
    "start": "2663280",
    "end": "2669119"
  },
  {
    "text": "from the consumer the second persona that we're looking at",
    "start": "2669119",
    "end": "2675200"
  },
  {
    "text": "is the cluster operator netops and this person we find is typically determining how services are accessed by",
    "start": "2675200",
    "end": "2682079"
  },
  {
    "text": "the user you know what port protocol addresses tls certificates",
    "start": "2682079",
    "end": "2687680"
  },
  {
    "text": "whether or not it's on the internet whether or not it's internal what resources you know maybe it's",
    "start": "2687680",
    "end": "2693839"
  },
  {
    "text": "expensive uh gold low bouncing or maybe a cheaper infrastructure",
    "start": "2693839",
    "end": "2701359"
  },
  {
    "text": "and really the cluster operator we feel will control the gateway resource this is a keystone",
    "start": "2701359",
    "end": "2708240"
  },
  {
    "text": "resource that's one-to-one with active configuration of some part of the infrastructure",
    "start": "2708240",
    "end": "2713359"
  },
  {
    "text": "creating a gateway resource will spawn for example a software lb it may add a configuration stanza to",
    "start": "2713359",
    "end": "2720319"
  },
  {
    "text": "an existing lb may program the sdn so the gateway will sort of",
    "start": "2720319",
    "end": "2727599"
  },
  {
    "text": "describe how the whole service will be accessed",
    "start": "2727599",
    "end": "2734319"
  },
  {
    "text": "and we also note that this gateway resource may be underspecified so you can kind of",
    "start": "2734319",
    "end": "2739839"
  },
  {
    "text": "use it to request a particular kind of gateway but the system and the implementation",
    "start": "2739839",
    "end": "2746400"
  },
  {
    "text": "can kind of fill in the details so here's a sort of a sketch of the",
    "start": "2746400",
    "end": "2752079"
  },
  {
    "text": "gateway resource and you'll see that it's under specified because all we're doing is asking for",
    "start": "2752079",
    "end": "2757440"
  },
  {
    "text": "a port and uh route selector to show you how which routes are linked to this",
    "start": "2757440",
    "end": "2763839"
  },
  {
    "text": "gateway but we don't really talk about for example which address it's going to get and those will be dependent on",
    "start": "2763839",
    "end": "2771440"
  },
  {
    "text": "and filled in by the gateway class the final role that we're looking at",
    "start": "2771440",
    "end": "2776640"
  },
  {
    "text": "when designing these sort of like orthogonal services is that of the application developer",
    "start": "2776640",
    "end": "2782160"
  },
  {
    "text": "the application developer is going to come up with you know the set",
    "start": "2782160",
    "end": "2787680"
  },
  {
    "text": "of routes that make up the their for example in this case the store app",
    "start": "2787680",
    "end": "2794319"
  },
  {
    "text": "you know which services is going to route to and kind of the protocol specific matching and filtering",
    "start": "2794319",
    "end": "2801119"
  },
  {
    "text": "that's how they implement their application one interesting thing is that we have",
    "start": "2801119",
    "end": "2807280"
  },
  {
    "text": "done here is in modeling this we have actually split out each of these routes into their own",
    "start": "2807280",
    "end": "2813760"
  },
  {
    "text": "protocol specific resources so we have a http route tcp route and so",
    "start": "2813760",
    "end": "2818800"
  },
  {
    "text": "forth and this allows us to one make a resource that's specific to",
    "start": "2818800",
    "end": "2824960"
  },
  {
    "text": "that protocol so it's much easier to understand and two because they're actually separate resources that can all linked",
    "start": "2824960",
    "end": "2830800"
  },
  {
    "text": "into gateway it actually creates a bit of extensibility for example if your gateway class",
    "start": "2830800",
    "end": "2838720"
  },
  {
    "text": "provider can handle like a new protocol for example let's say you do",
    "start": "2838720",
    "end": "2843920"
  },
  {
    "text": "redis load balancing you can have a redis route that can talk about redis",
    "start": "2843920",
    "end": "2849760"
  },
  {
    "text": "protocol specific details and actually add that to the system without having to say modify",
    "start": "2849760",
    "end": "2856079"
  },
  {
    "text": "the entire standard or send it upstream",
    "start": "2856079",
    "end": "2861119"
  },
  {
    "text": "what happens to service so service still exists and you know all the basic functionality since it's ga and part of",
    "start": "2861359",
    "end": "2866880"
  },
  {
    "text": "the core will always remain but we're seeing that we're hoping to evolve it to a place where",
    "start": "2866880",
    "end": "2873680"
  },
  {
    "text": "basically it's limited to grouping and selection so it's basically mostly just talking",
    "start": "2873680",
    "end": "2878880"
  },
  {
    "text": "about the set of back ends that comprise the service so v1 functionality still works",
    "start": "2878880",
    "end": "2884240"
  },
  {
    "text": "but hopefully we won't have to add significantly to that surface area to add new",
    "start": "2884240",
    "end": "2891280"
  },
  {
    "text": "interesting functionality on top of it so here's an example kind of broken out",
    "start": "2891280",
    "end": "2897839"
  },
  {
    "text": "so we have a gateway class that's referred to by the gateway and the gateway class defines sort of",
    "start": "2897839",
    "end": "2905359"
  },
  {
    "text": "how the implementation will be wiring up the rest of it for example internet lb that's",
    "start": "2905359",
    "end": "2911680"
  },
  {
    "text": "implemented by a cloud provider that way but particular gateway class then we have gateway which",
    "start": "2911680",
    "end": "2916960"
  },
  {
    "text": "talks about sort of protocol termination which ports how the client is going to access this",
    "start": "2916960",
    "end": "2923920"
  },
  {
    "text": "and then that will be attached to a number of routes that the application",
    "start": "2923920",
    "end": "2929200"
  },
  {
    "text": "developer will be writing and then the application developer will also be managing the services attached to it so the initial",
    "start": "2929200",
    "end": "2937119"
  },
  {
    "text": "v1 alpha one it will cover basic applications and data types",
    "start": "2937119",
    "end": "2942240"
  },
  {
    "text": "gateway class for inter operation between different controllers a gateway and route so http tcp https",
    "start": "2942240",
    "end": "2950400"
  },
  {
    "text": "server secrets and we actually have a whole bunch of implementers",
    "start": "2950400",
    "end": "2956960"
  },
  {
    "text": "who are going to participate in this alpha and we're looking at both merging style",
    "start": "2956960",
    "end": "2963839"
  },
  {
    "text": "implementers which is sort of multiple gateways that are hosted on a single proxy infrastructure for example",
    "start": "2963839",
    "end": "2972000"
  },
  {
    "text": "you run ingress nginx that's like a merging style gateway that's running inside your cluster",
    "start": "2972000",
    "end": "2977520"
  },
  {
    "text": "as well as a provisioning style or cloud-based gateway so you have a gateway",
    "start": "2977520",
    "end": "2982800"
  },
  {
    "text": "that when you create it it actually maps to a cloud load balancer that is just some of the things that",
    "start": "2982800",
    "end": "2987920"
  },
  {
    "text": "we're up to so how do you get involved there's issues.kates.io which will send",
    "start": "2987920",
    "end": "2993440"
  },
  {
    "text": "you to a place to file bugs cleanup ideas and feature requests and you can also use this to find issues",
    "start": "2993440",
    "end": "3000079"
  },
  {
    "text": "to help with so our large changes to kubernetes",
    "start": "3000079",
    "end": "3005359"
  },
  {
    "text": "are done through the enhancement process which is also known as caps kubernetes enhancement proposals",
    "start": "3005359",
    "end": "3011680"
  },
  {
    "text": "and enhancements are user-visible changes or even infrastructure changes of a sufficient size so please and submit",
    "start": "3011680",
    "end": "3018880"
  },
  {
    "text": "enhancement proposals the best way to kind of get your cap moving is to show up at sig network and discuss",
    "start": "3018880",
    "end": "3027359"
  },
  {
    "text": "what you're proposing probably in that discussion it will spill onto the mailing list or",
    "start": "3027359",
    "end": "3033520"
  },
  {
    "text": "maybe a shared document and then after we have kind of consensus",
    "start": "3033520",
    "end": "3038880"
  },
  {
    "text": "it will become a cap so if you want to get involved there's our community page we have a",
    "start": "3038880",
    "end": "3045760"
  },
  {
    "text": "zoom meeting which is every other thursday the slack is hashtag sig network",
    "start": "3045760",
    "end": "3051280"
  },
  {
    "text": "and then the mailing list",
    "start": "3051280",
    "end": "3054400"
  }
]