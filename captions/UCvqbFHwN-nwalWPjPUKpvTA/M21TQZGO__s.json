[
  {
    "text": "so hi uh I'm David Bell i'm a senior staff software engineer at Uber that is my partner Ron Jane who's a staff",
    "start": "240",
    "end": "7440"
  },
  {
    "text": "software engineer at Uber uh today we're going to be talking about uh not not too creatively title like",
    "start": "7440",
    "end": "13519"
  },
  {
    "text": "securing every bit at Uber uh so we're going to go over you know what was the problem why was this",
    "start": "13519",
    "end": "20240"
  },
  {
    "text": "problem hard like why are you even listening to this talk uh our approach to solving the problem and then some of",
    "start": "20240",
    "end": "25840"
  },
  {
    "text": "the performance numbers we've gotten as well as some of the challenges we've overcome in developing",
    "start": "25840",
    "end": "32360"
  },
  {
    "text": "this and so first off with the problem you know what do we want at Uber uh we",
    "start": "32360",
    "end": "37840"
  },
  {
    "text": "basically want to have a zero trust architecture i won't go into what zero trust architecture is too much but it's",
    "start": "37840",
    "end": "43120"
  },
  {
    "text": "basically you know we want to prevent bad people from doing bad things within our network uh and affecting our",
    "start": "43120",
    "end": "48399"
  },
  {
    "text": "customers uh you know excfiltrating data uh compromising services things of that",
    "start": "48399",
    "end": "53440"
  },
  {
    "text": "bad stuff um through that we want to assume that there's no implicit trust",
    "start": "53440",
    "end": "58800"
  },
  {
    "text": "between workloads uh and we want to establish basically a baseline of encryption authentication authorization",
    "start": "58800",
    "end": "67280"
  },
  {
    "text": "uh and auditabilitability between every interaction between workloads and when I",
    "start": "67280",
    "end": "72960"
  },
  {
    "text": "say workloads I mean things like services databases batch jobs things of that nature uh and so the reality is you",
    "start": "72960",
    "end": "80560"
  },
  {
    "text": "know we had some of the stuff in some of the places uh some of the time everywhere uh so for",
    "start": "80560",
    "end": "86400"
  },
  {
    "text": "example in our batch ecosystem we had heavy usage of keraros uh but effectively none of that outside of",
    "start": "86400",
    "end": "92079"
  },
  {
    "text": "batch uh we had some homegrown authentication authorization solutions and inconsistent deployment of",
    "start": "92079",
    "end": "97439"
  },
  {
    "text": "encryption transit kind of based on you know what was the criticality uh of that service or that interaction uh and",
    "start": "97439",
    "end": "104960"
  },
  {
    "text": "because of this we didn't really have any kind of centralized auditing like we're trying to integrate many different systems together to figure out you know",
    "start": "104960",
    "end": "111600"
  },
  {
    "text": "what are these controls were in place at what time and so our goal was we want to raise the baseline of uh encryption",
    "start": "111600",
    "end": "118159"
  },
  {
    "text": "authentication authorization at Uber and basically apply something 100% across everything at",
    "start": "118159",
    "end": "125119"
  },
  {
    "text": "Uber and so why was this hard uh so first off you there's scale uh we have",
    "start": "125320",
    "end": "130399"
  },
  {
    "text": "hundreds of thousand machines running millions of containerized workloads we have hundreds of terabytes of traffic uh",
    "start": "130399",
    "end": "136720"
  },
  {
    "text": "over billions of connections and uh in our L7 layer we uh are supporting hundreds of millions of RPS uh per",
    "start": "136720",
    "end": "145120"
  },
  {
    "text": "requests per second uh that uh we also have to deal with uh when we're also applying encryption authent",
    "start": "145120",
    "end": "151680"
  },
  {
    "text": "uh for performance uh we have service owners that have dialed in their SLAs's around throughput latency availability",
    "start": "151680",
    "end": "158720"
  },
  {
    "text": "and so we we have some wiggle room but not a ton uh so when we're introducing uh whatever we do we need to make sure",
    "start": "158720",
    "end": "164879"
  },
  {
    "text": "that we're not going to suddenly uh trigger a bunch of alerts because we've regressed their SLAs's uh we also have",
    "start": "164879",
    "end": "171120"
  },
  {
    "text": "correctness concerns uh you know if there's any behavior you have in your network somebody's going to take a",
    "start": "171120",
    "end": "176400"
  },
  {
    "text": "dependency on that and so we had to be very careful that we weren't uh breaking",
    "start": "176400",
    "end": "181519"
  },
  {
    "text": "anybody that was doing something kind of at the edge or odd uh and lastly for",
    "start": "181519",
    "end": "187120"
  },
  {
    "text": "reliability like I mentioned you know our goal that's kind of a business goal we want to have this secure baseline but",
    "start": "187120",
    "end": "193280"
  },
  {
    "text": "for service owners they just want to make sure they hit their SLOs's uh and so uh you know if in the process of",
    "start": "193280",
    "end": "199840"
  },
  {
    "text": "delivering this we start to impact service availability that's going to create a lot of noise for us those",
    "start": "199840",
    "end": "204959"
  },
  {
    "text": "people are going to complain that's going to slow us down and make it harder for us to actually achieve our",
    "start": "204959",
    "end": "210599"
  },
  {
    "text": "goals uh and you know stop me if you've heard this uh you know so we have many different applications on the order of",
    "start": "210599",
    "end": "216959"
  },
  {
    "text": "thousands of stateless services uh stateful databases and tens of thousands of batch jobs across many different",
    "start": "216959",
    "end": "223760"
  },
  {
    "text": "frameworks so we have a lot of go and java but also python in the data stack we have open- source uh frameworks being",
    "start": "223760",
    "end": "230480"
  },
  {
    "text": "used like yarn and spark open source databases like reddus my sql but also a bunch of custom stuff uh and kind of the",
    "start": "230480",
    "end": "237040"
  },
  {
    "text": "most interesting is we actually had four different orchestrators uh so only one of them being kubernetes",
    "start": "237040",
    "end": "243360"
  },
  {
    "text": "we're fixing that uh but you also just had four different ways of deploying things and some of them using Kubernetes",
    "start": "243360",
    "end": "250239"
  },
  {
    "text": "some of them using containerd some of them doing docker run and so having kind of any consistency in the data plane was",
    "start": "250239",
    "end": "256079"
  },
  {
    "text": "really challenging uh and also we don't have network name spaces at least not",
    "start": "256079",
    "end": "261359"
  },
  {
    "text": "yet um everything's host networking using dynamic dynamically allocated",
    "start": "261359",
    "end": "267120"
  },
  {
    "text": "ports and so a lot of the nicities that you have in the ecosystem just weren't available to us",
    "start": "267120",
    "end": "274560"
  },
  {
    "text": "uh and so for our approach kind of set out with these three tenants uh so first you know we want to target the lowest",
    "start": "274800",
    "end": "280880"
  },
  {
    "text": "common denominator and drive that to 100% uh and so despite all those different languages and frameworks and",
    "start": "280880",
    "end": "287479"
  },
  {
    "text": "implementations everything use TCP so let's target that and you know maybe we don't get all the nicities we want like",
    "start": "287479",
    "end": "293520"
  },
  {
    "text": "you know per request uh authorization but we could at least get like connection level and so you also see",
    "start": "293520",
    "end": "298720"
  },
  {
    "text": "this in like ISTO with Z tunnel being able to target a very common you know base layer uh we also made sure we",
    "start": "298720",
    "end": "305040"
  },
  {
    "text": "didn't rebuild what we didn't have to um reusing what what we knew worked uh so",
    "start": "305040",
    "end": "310080"
  },
  {
    "text": "for example we have a lot of experience running Envoy on our edge and we're doing a migration internally from our",
    "start": "310080",
    "end": "315680"
  },
  {
    "text": "homegrown L7 load balancer to envoy uh and so we had a lot of experience there with envoy we built out a lot of",
    "start": "315680",
    "end": "321199"
  },
  {
    "text": "infrastructure for it and we knew we could hit the ground running uh if we were able to baseline on that similarly",
    "start": "321199",
    "end": "326479"
  },
  {
    "text": "for using Spire for certificates as well as a lot of the integrations we had for how do we associate identity with",
    "start": "326479",
    "end": "332240"
  },
  {
    "text": "containers in this kind of multi- orchestrator world and lastly you know graceful degradation you know this thing",
    "start": "332240",
    "end": "339520"
  },
  {
    "text": "didn't exist before we built it and deployed it and so we should be easily able to kind of degrade back to the",
    "start": "339520",
    "end": "345520"
  },
  {
    "text": "state of the world pre prior to that uh so if we have any issues with deployment we have any performance concerns even",
    "start": "345520",
    "end": "351759"
  },
  {
    "text": "like a hint that something's going wrong we should be able to ideally automatically autonomously just revert",
    "start": "351759",
    "end": "357680"
  },
  {
    "text": "back to the default state um so our approach uh ended up",
    "start": "357680",
    "end": "364080"
  },
  {
    "text": "using combination of uh Linux kernel primitives EVPF and NF tables to redirect traffic over an envoy proxy so",
    "start": "364080",
    "end": "371199"
  },
  {
    "text": "envoy does form the base of our um kind of encrypted authenticated",
    "start": "371199",
    "end": "376800"
  },
  {
    "text": "authorized data plane uh so with envoy we got basically batteries included for",
    "start": "376800",
    "end": "381880"
  },
  {
    "text": "MTLS Arbok uh we use proxy protocol which I'll talk about in next slide as",
    "start": "381880",
    "end": "387199"
  },
  {
    "text": "well as original destination dynamic forwarding uh and you know basically using just features that come out of the",
    "start": "387199",
    "end": "393280"
  },
  {
    "text": "box configured in envoy uh and for things that didn't exist in envoy we were able to use envoys uh uh custom",
    "start": "393280",
    "end": "400400"
  },
  {
    "text": "plug-in filter system to easily extend it uh so we can do like that uh",
    "start": "400400",
    "end": "405520"
  },
  {
    "text": "connection at test station and identify by uh when we redirect a connection to the proxy like what is what is actually",
    "start": "405520",
    "end": "411360"
  },
  {
    "text": "behind that connection what is the identity of that um we run envoy using a go nanny process um uh and this is",
    "start": "411360",
    "end": "419680"
  },
  {
    "text": "actually something we'd originally built for our L7 internal uh proxy layer that we're migrating to but because uh you",
    "start": "419680",
    "end": "427840"
  },
  {
    "text": "know effectively we can create different brains uh where we have you know the kind of the L4 secure data plane brain",
    "start": "427840",
    "end": "433759"
  },
  {
    "text": "the L7 you know routing data plane we're able to share a lot of the logic between that and so even though they had been",
    "start": "433759",
    "end": "440240"
  },
  {
    "text": "working on that first we were able to u repurpose it for our use case and now we've actually contributed back a lot of",
    "start": "440240",
    "end": "445840"
  },
  {
    "text": "operational tooling around things like restarts monitoring uh debugging so now they're able to use that for their uh L7",
    "start": "445840",
    "end": "453039"
  },
  {
    "text": "internal data plane um and so for traffic reduction I think",
    "start": "453039",
    "end": "458160"
  },
  {
    "text": "I mentioned we're using NF tables in EPF so uh when a container launches uh we",
    "start": "458160",
    "end": "463280"
  },
  {
    "text": "have a labels on that container that allow us to determine whether that should be onboarded to the system and",
    "start": "463280",
    "end": "468720"
  },
  {
    "text": "then we'll uh configure that in NF tables to redirect its packets to the proxy and then using a bit of ebpf to",
    "start": "468720",
    "end": "475919"
  },
  {
    "text": "propagate some state on the connection that we can look up in uh the proxy and",
    "start": "475919",
    "end": "481039"
  },
  {
    "text": "then lastly uh to solve that multi- orchestrator problem uh we ended up wrapping run c uh so for those that",
    "start": "481039",
    "end": "488560"
  },
  {
    "text": "hardware uh you know you have to say docker below that you have container d below that you have run c uh and that's",
    "start": "488560",
    "end": "494720"
  },
  {
    "text": "kind of the basis of where you know containers actually get launched uh so we took a page out of uh some other",
    "start": "494720",
    "end": "499840"
  },
  {
    "text": "playbooks like uh I think AWS did this with um something they called like hot dog uh where they basically wrap run C",
    "start": "499840",
    "end": "505680"
  },
  {
    "text": "to inject additional hooks into the container launch uh so we're able to use that to um set up the attestation",
    "start": "505680",
    "end": "513039"
  },
  {
    "text": "information for container but we're also able to hook in right before we launch the container process and actually do a",
    "start": "513039",
    "end": "519120"
  },
  {
    "text": "end toend check uh to make sure that uh everything's configured on the host uh",
    "start": "519120",
    "end": "524159"
  },
  {
    "text": "so all the stuff is happening asynchronously like a container gets launched we update the envoy with information about how to bind the",
    "start": "524159",
    "end": "530640"
  },
  {
    "text": "certificates and policies for that container and we want to make sure that's all configured before we actually let that uh uh container launch the",
    "start": "530640",
    "end": "537600"
  },
  {
    "text": "entry point and start actually interacting with the proxy",
    "start": "537600",
    "end": "543320"
  },
  {
    "text": "uh on the wire uh we're just using you know one to one TCP connections uh with MTLS on top and then using proxy",
    "start": "543519",
    "end": "550880"
  },
  {
    "text": "protocol on top of that to signal the original destination uh we did look at alternatives for this um using um like",
    "start": "550880",
    "end": "558480"
  },
  {
    "text": "H2 connect uh which I think is using zunnel um we opted not to use that uh",
    "start": "558480",
    "end": "564880"
  },
  {
    "text": "one at the time we found some crashes in it and we were trying to move fast uh but also uh we were a bit concerned",
    "start": "564880",
    "end": "570800"
  },
  {
    "text": "about the overhead of having the HP stack involved on top of uh everything else and because we are very cost",
    "start": "570800",
    "end": "578000"
  },
  {
    "text": "sensitive uh we're very concerned about that um additionally we're also concerned about packet loss uh so at",
    "start": "578000",
    "end": "584160"
  },
  {
    "text": "Uber uh we operate multi-reion but also multi-provider uh so right now we're in",
    "start": "584160",
    "end": "589519"
  },
  {
    "text": "a combination of on premises data centers um Google cloud and OCI and while we",
    "start": "589519",
    "end": "597360"
  },
  {
    "text": "generally find that you know packet loss is pretty minimal within uh the provider uh we do find that we have packet loss",
    "start": "597360",
    "end": "603200"
  },
  {
    "text": "events between providers between regions and so we're concerned about head of line blocking on shared H2 connect uh",
    "start": "603200",
    "end": "609320"
  },
  {
    "text": "connections uh that does mean and Ronx can go into this later that uh we are kind of churning more uh TLS sessions",
    "start": "609320",
    "end": "616399"
  },
  {
    "text": "and connections than we would otherwise um it also made it a little bit easier to integrate it into our L7",
    "start": "616399",
    "end": "622320"
  },
  {
    "text": "proxy layer uh so not only are we doing this for the P2P connections uh but we've also integrated this protocol into",
    "start": "622320",
    "end": "629120"
  },
  {
    "text": "the outbound side of our L7 proxies so we're actually able to do that uh authentication authorization kind of on",
    "start": "629120",
    "end": "635920"
  },
  {
    "text": "behalf of the callers so rather than having our L7 proxy say like hi I'm L7 proxy I want to connect to the fooar",
    "start": "635920",
    "end": "642560"
  },
  {
    "text": "service we can actually do it on behalf of the client um and then for deployments we're",
    "start": "642560",
    "end": "648720"
  },
  {
    "text": "just using the standard envoy live migration and traffic drains uh so we're",
    "start": "648720",
    "end": "653760"
  },
  {
    "text": "able to incrementally drain uh connections uh after we've established the new proxy um and have clients",
    "start": "653760",
    "end": "660240"
  },
  {
    "text": "gracefully reconnect um we did actually fix an issue with idle connections where we found that while established",
    "start": "660240",
    "end": "666560"
  },
  {
    "text": "connections will get uh drained normally throughout that kind of drain period uh idle connections uh because they weren't",
    "start": "666560",
    "end": "672560"
  },
  {
    "text": "triggering events were actually getting drained at the very end uh which caused some issues for some workloads where we",
    "start": "672560",
    "end": "677680"
  },
  {
    "text": "basically have like a a big thundering herd of idle connections that they were using for um quorum get drained very at",
    "start": "677680",
    "end": "683920"
  },
  {
    "text": "the end of the drain period and so we resolved that um and lastly we're also using proxy protocol to do extensions uh",
    "start": "683920",
    "end": "691200"
  },
  {
    "text": "So for example we have quality of service classification where we basically down prioritize bulk traffic",
    "start": "691200",
    "end": "697279"
  },
  {
    "text": "for batch jobs in the uh in our network over default and so we're able to propagate that over the proxy uh using",
    "start": "697279",
    "end": "703839"
  },
  {
    "text": "uh ppb2 uh for graceful degradation uh we basically decided to be able to fail",
    "start": "703839",
    "end": "710640"
  },
  {
    "text": "open and gracefully degrade at every point in the stack uh so if we're not able to retrieve authorization",
    "start": "710640",
    "end": "716200"
  },
  {
    "text": "policies just let the traffic through log the decision and audit later uh if",
    "start": "716200",
    "end": "722079"
  },
  {
    "text": "we're not able to bind that identity because we can't get a certificate or because something's not working on the machine um we modeled unauthenticated as",
    "start": "722079",
    "end": "730079"
  },
  {
    "text": "a first class identity in the system so just log uh do authorization with unauthenticated log which machine that's",
    "start": "730079",
    "end": "736240"
  },
  {
    "text": "on let the traffic through and then if the proxyy's failing itself whether it's",
    "start": "736240",
    "end": "741720"
  },
  {
    "text": "overloaded crashed or something about how we're doing the traffic redirection isn't actually working uh again just",
    "start": "741720",
    "end": "748320"
  },
  {
    "text": "fail open disable all the traffic redirection and so as part of this we have some uh signals where we run a",
    "start": "748320",
    "end": "754560"
  },
  {
    "text": "selfch check on every machine that is actually continuously testing the proxy to make sure that it's responding and",
    "start": "754560",
    "end": "759680"
  },
  {
    "text": "that we can send data end to end through the proxy on both the outbound side as well as the inbound",
    "start": "759680",
    "end": "765279"
  },
  {
    "text": "um and we also on the uh outbound side if we detect that the destination proxy",
    "start": "765279",
    "end": "770320"
  },
  {
    "text": "for whatever reason we can't connect to it we also do a custom fallback where we do plain text TCP direct to the",
    "start": "770320",
    "end": "775920"
  },
  {
    "text": "destination application and so that ensures that even if the destination's not running we still attempt to",
    "start": "775920",
    "end": "781440"
  },
  {
    "text": "establish a connection and uh uh maintain service availability um and",
    "start": "781440",
    "end": "787360"
  },
  {
    "text": "that's also nice because then it makes it really easy for us to kind of do like incident mitigation you know we can always kind of disable traffic",
    "start": "787360",
    "end": "793440"
  },
  {
    "text": "redirection anywhere in the fleet we can basically shut down any proxy and generally the rest of the fleet will",
    "start": "793440",
    "end": "798480"
  },
  {
    "text": "adapt to that and reestablish connections",
    "start": "798480",
    "end": "803959"
  },
  {
    "text": "uh so then uh for how we brought this to production so we've been working on this for the better part of two years now uh",
    "start": "803959",
    "end": "811360"
  },
  {
    "text": "and so uh basically end of 2023 beginning of 2024 we did a kind of proof",
    "start": "811360",
    "end": "816880"
  },
  {
    "text": "of concept which again envoy was very helpful for uh we were able to get up and running in about like about a week",
    "start": "816880",
    "end": "823680"
  },
  {
    "text": "um and then about after 3 months we kind of put together a like very minimally viable version of this basically none of",
    "start": "823680",
    "end": "830000"
  },
  {
    "text": "the productionalization basically you put a workload on the machine it gets put over the proxy and we started testing",
    "start": "830000",
    "end": "835839"
  },
  {
    "text": "workloads immediately especially in our storage and batch ecosystem because there we considered they'd be the most",
    "start": "835839",
    "end": "841360"
  },
  {
    "text": "sensitive to performance and also the ones with the most oddities like generally for our stateless traffic we know they can tolerate more lat more",
    "start": "841360",
    "end": "848720"
  },
  {
    "text": "latency and also are more tolerant of connection resets and so this gave us a lot of signal uh we were able to",
    "start": "848720",
    "end": "854639"
  },
  {
    "text": "integrate uh and add features that we turned out we needed well before we actually had anything running in",
    "start": "854639",
    "end": "860560"
  },
  {
    "text": "production uh end of 2023 uh beginning of 2024 we",
    "start": "860560",
    "end": "866320"
  },
  {
    "text": "had built out that production stack uh and we started by onboarding somewhere between about a half a percent and 1% of",
    "start": "866320",
    "end": "872320"
  },
  {
    "text": "all workloads at Uber making sure that they were representative uh so not just running kind of the junky uh you know",
    "start": "872320",
    "end": "879760"
  },
  {
    "text": "staging services that like nobody really cares about actually getting real like you know for us tier one workloads so",
    "start": "879760",
    "end": "885920"
  },
  {
    "text": "the things that actually run the business uh and we started doing that pretty much in February of last year",
    "start": "885920",
    "end": "891680"
  },
  {
    "text": "even before we done like large scale testing and that ensured that you know we started to get production experience",
    "start": "891680",
    "end": "897040"
  },
  {
    "text": "very early uh and so that meant that we were able to build up our operational experience set up runbooks uh really",
    "start": "897040",
    "end": "904079"
  },
  {
    "text": "debug our operational excellence um once we got past that",
    "start": "904079",
    "end": "909199"
  },
  {
    "text": "point we decided to go very big uh because while we knew this kind of worked for the things we tested we kind",
    "start": "909199",
    "end": "914560"
  },
  {
    "text": "of assumed that there were some oddities out there somewhere and the only way we were going to catch them was by putting them on the proxy um and so at Uber like",
    "start": "914560",
    "end": "923120"
  },
  {
    "text": "I assume at many companies like we uh run multiszone multi-reion and we're generally pretty good about being able",
    "start": "923120",
    "end": "929360"
  },
  {
    "text": "to fail away from a zone uh so we have centralized controls to be able to drain traffic from a zone and drain it back",
    "start": "929360",
    "end": "935519"
  },
  {
    "text": "and so we opted to do a series of uh full zone tests where we drain a zone",
    "start": "935519",
    "end": "941680"
  },
  {
    "text": "onboard it over a few minutes and then run it for about 30 minutes at a time uh",
    "start": "941680",
    "end": "946880"
  },
  {
    "text": "and so this allowed us to get a lot of information about how this thing performed at scale very uh quickly while",
    "start": "946880",
    "end": "952800"
  },
  {
    "text": "also being able to bound our worst case because we always knew if we blew up this zone well you know we have about a",
    "start": "952800",
    "end": "958320"
  },
  {
    "text": "dozen more we can just drain from this one and still be okay and so then this influenced kind some of the uh work that",
    "start": "958320",
    "end": "964560"
  },
  {
    "text": "Ronach did and some of the performance challenges and how we solved them um and so once we' done that you know we kind",
    "start": "964560",
    "end": "971120"
  },
  {
    "text": "of demonstrated that this thing worked you know at zone scale uh worked uh at",
    "start": "971120",
    "end": "976560"
  },
  {
    "text": "you know peak peak traffic and so really at that point we kind of demonstrated this thing worked we just had to show",
    "start": "976560",
    "end": "981759"
  },
  {
    "text": "that it was stable over time uh and so then we were able to in the end of last year do a roll out over a period of",
    "start": "981759",
    "end": "987839"
  },
  {
    "text": "about a four months um now in that roll out we did have people have concerns because with any rollout any issue",
    "start": "987839",
    "end": "994720"
  },
  {
    "text": "suddenly becomes you know like did you cause it uh and so you know our methodology was you know don't try and",
    "start": "994720",
    "end": "1001839"
  },
  {
    "text": "debug uh in the moment roll back analyze demonstrate that uh you know you weren't",
    "start": "1001839",
    "end": "1008160"
  },
  {
    "text": "the problem or in the rare cases we were uh but you know make sure that we preserve service availability and at at",
    "start": "1008160",
    "end": "1014959"
  },
  {
    "text": "all costs um then the result of that was that you know generally the rest of Uber had a lot of trust that we weren't going",
    "start": "1014959",
    "end": "1021040"
  },
  {
    "text": "to break them um I think one of the big concerns up front was we were going to prioritize security and then kind of all",
    "start": "1021040",
    "end": "1027839"
  },
  {
    "text": "the way down here availability and performance and you know again as I mentioned on the that second slide you",
    "start": "1027839",
    "end": "1033520"
  },
  {
    "text": "know availability and performance is what they really care about you know they want security but they don't want to sacrifice that and so because we",
    "start": "1033520",
    "end": "1040798"
  },
  {
    "text": "demonstrated that we uh really uh you know cared about you know their performance and availability and that we",
    "start": "1040799",
    "end": "1046880"
  },
  {
    "text": "weren't going to regress them just for security that gave us a lot of leeway and a lot of trust to be able to deliver",
    "start": "1046880",
    "end": "1052320"
  },
  {
    "text": "very quickly uh so where are we now uh so onboarding we have stateless and",
    "start": "1052320",
    "end": "1058480"
  },
  {
    "text": "stateful basically fully onboarded there's like one or two exceptional cases that we're working through uh but all new workloads are enabled by default",
    "start": "1058480",
    "end": "1065919"
  },
  {
    "text": "for batch we're about halfway done we're sort of working through our different uh uh frameworks and ecosystems and getting them incrementally onboarded uh we also",
    "start": "1065919",
    "end": "1072960"
  },
  {
    "text": "need to make that quality of service uh feature work uh before we could onboard them uh and they're just kind of slow",
    "start": "1072960",
    "end": "1078400"
  },
  {
    "text": "solely incrementally rolling out and for scale so right now we have billions of established connections tens",
    "start": "1078400",
    "end": "1084320"
  },
  {
    "text": "of millions of newest connections per second across our fleet and about 40% of all network",
    "start": "1084320",
    "end": "1090799"
  },
  {
    "text": "traffic within Uber is now on the platform uh where that kind of 60% is you know basically those batch jobs",
    "start": "1090799",
    "end": "1097280"
  },
  {
    "text": "which isn't too surprising they're the ones that are kind of driving a lot of bits in terms of availability uh we're",
    "start": "1097280",
    "end": "1103039"
  },
  {
    "text": "about five nights of uptime uh generally where we see unavailability is uh host",
    "start": "1103039",
    "end": "1108200"
  },
  {
    "text": "crashes uh and uh kind of noisy batch workloads that decide that they're going to drive the machine to 100%",
    "start": "1108200",
    "end": "1114360"
  },
  {
    "text": "utilization um and so while we're unavailable nothing not much else is available on that machine either um and",
    "start": "1114360",
    "end": "1121679"
  },
  {
    "text": "we see about half a percent of connection fallbacks um and that's actually because generally we find that",
    "start": "1121679",
    "end": "1127360"
  },
  {
    "text": "services have stale topology so they're trying to connect to something that no longer is running uh we still try that connection",
    "start": "1127360",
    "end": "1133160"
  },
  {
    "text": "anyways um so yeah now I'm going to hand it over to Ronach to go over performance",
    "start": "1133160",
    "end": "1138640"
  },
  {
    "text": "and challenges right right thanks David um so just",
    "start": "1138640",
    "end": "1147360"
  },
  {
    "text": "going to be talking about performance challenges and how we scale uh secroxy",
    "start": "1147360",
    "end": "1152640"
  },
  {
    "text": "what we call uh on voy so uh this slide shows about the single host performance",
    "start": "1152640",
    "end": "1158799"
  },
  {
    "text": "that we see uh on our fleet uh we handle",
    "start": "1158799",
    "end": "1164080"
  },
  {
    "text": "about 10k connections on an average and about 300 connections on the biggest",
    "start": "1164080",
    "end": "1170240"
  },
  {
    "text": "machines that we have uh these are large fanout services or services that are",
    "start": "1170240",
    "end": "1175840"
  },
  {
    "text": "talking to like large clusters in uh in terms of topology and then the rate of",
    "start": "1175840",
    "end": "1181360"
  },
  {
    "text": "connections on these machines could be around 3,000 on the bigger ones i mean we've also seen worse about 8,000",
    "start": "1181360",
    "end": "1188480"
  },
  {
    "text": "connections per second uh unique sessions that's being created to different endpoints",
    "start": "1188480",
    "end": "1194320"
  },
  {
    "text": "uh throughput wise since we don't have batch onboarded fully yet uh the",
    "start": "1194320",
    "end": "1199520"
  },
  {
    "text": "stateless stuff traffic is mostly RPC high number of RPCs but less number of",
    "start": "1199520",
    "end": "1205120"
  },
  {
    "text": "uh bytes in those so be averages around 300 Mbps and the the biggest we've seen",
    "start": "1205120",
    "end": "1211679"
  },
  {
    "text": "are like 4 gigs um the reason why I'm going over this is just trying to set",
    "start": "1211679",
    "end": "1217200"
  },
  {
    "text": "the baseline of why we started seeing performance issues after we started onboarding uh some performance",
    "start": "1217200",
    "end": "1223760"
  },
  {
    "text": "latency sensitive workloads um before we do that just a",
    "start": "1223760",
    "end": "1228960"
  },
  {
    "text": "quick background on how Envoy works so there's the main thread which is the",
    "start": "1228960",
    "end": "1235039"
  },
  {
    "text": "control plane of envoy u handles XTS uh",
    "start": "1235039",
    "end": "1240600"
  },
  {
    "text": "endpoints you receive admin requests uh health checks metrics everything is coming on main thread and then there's",
    "start": "1240600",
    "end": "1248080"
  },
  {
    "text": "ma then there are worker threads uh which actually manage the connection life cycle uh which is the essentially",
    "start": "1248080",
    "end": "1254080"
  },
  {
    "text": "the data plane of envoy and one nice thing is here it's isolated shared nothing architecture",
    "start": "1254080",
    "end": "1261480"
  },
  {
    "text": "uh everything every worker thread has its own listeners own set of connections",
    "start": "1261480",
    "end": "1267360"
  },
  {
    "text": "not they're not sharing anything so um you We have listener filters network",
    "start": "1267360",
    "end": "1272640"
  },
  {
    "text": "filters clusters on a very high level and each of those are like accepting connections processing IO events and",
    "start": "1272640",
    "end": "1279960"
  },
  {
    "text": "then also process the dispatched uh task from main thread",
    "start": "1279960",
    "end": "1286400"
  },
  {
    "text": "i mean if you squeeze that down and then just look at it uh in terms of worker thread you have bunch of events and",
    "start": "1286400",
    "end": "1294159"
  },
  {
    "text": "there's an event loop that's continuously pulling the events and uh processing",
    "start": "1294159",
    "end": "1299320"
  },
  {
    "text": "those and uh you see like bunch of events here one is you have IO events on",
    "start": "1299320",
    "end": "1305039"
  },
  {
    "text": "connection one connection two and there are new there are some events called new connections that I represent here um",
    "start": "1305039",
    "end": "1312080"
  },
  {
    "text": "those are essentially connections that we receive from downstream that needs to be proxyed upstream and when you try to",
    "start": "1312080",
    "end": "1318960"
  },
  {
    "text": "proxy it upstream you need to uh set up TLS uh state on those connections that",
    "start": "1318960",
    "end": "1324960"
  },
  {
    "text": "is TLS handshake uh proxy protocol and then finally uh that's ready for",
    "start": "1324960",
    "end": "1330559"
  },
  {
    "text": "transferring data so um when you have like large number of",
    "start": "1330559",
    "end": "1338159"
  },
  {
    "text": "events most of IO events are not expensive we when we benchmark we see",
    "start": "1338159",
    "end": "1343440"
  },
  {
    "text": "those around 10 microsconds or less so you are churning",
    "start": "1343440",
    "end": "1348880"
  },
  {
    "text": "through events very fast and overall you see a very uh stable latency in on void",
    "start": "1348880",
    "end": "1356039"
  },
  {
    "text": "um that is even if you are accepting connections reading and writing data you can actually produce uh very good",
    "start": "1356039",
    "end": "1363520"
  },
  {
    "text": "throughput on void we we see about 15 gigs of uh throughput per core u and",
    "start": "1363520",
    "end": "1370640"
  },
  {
    "text": "multi-core is much better of course um but then there's this one interesting",
    "start": "1370640",
    "end": "1376080"
  },
  {
    "text": "thing in the event loop uh which is TLS connections u TLS connections have uh",
    "start": "1376080",
    "end": "1383960"
  },
  {
    "text": "special uh load to them that is they are CPU intensive",
    "start": "1383960",
    "end": "1390440"
  },
  {
    "text": "um it's essentially the asymmetric algorithms that we use during handshakes",
    "start": "1390440",
    "end": "1396080"
  },
  {
    "text": "uh that is not hardware accelerated so you see a new socket a new connection",
    "start": "1396080",
    "end": "1403600"
  },
  {
    "text": "that needs TLS can take about 1 millisecond of your CPU time and",
    "start": "1403600",
    "end": "1410360"
  },
  {
    "text": "uh we've done this benchmark across x86 um and few other processors but 1",
    "start": "1410360",
    "end": "1415679"
  },
  {
    "text": "millond is what an average time that we've seen across uh different uh",
    "start": "1415679",
    "end": "1421480"
  },
  {
    "text": "SKs so uh let's look at an event loop where you have hundreds of connections",
    "start": "1421480",
    "end": "1427559"
  },
  {
    "text": "that are blocking uh it's again head-of-the-line blocking where you have",
    "start": "1427559",
    "end": "1432640"
  },
  {
    "text": "hundreds of connections that need to be uh that need to be processed for with TLS handshake and then there are IO",
    "start": "1432640",
    "end": "1439280"
  },
  {
    "text": "events behind those so before you get to the IO IO events that is RPC request or",
    "start": "1439280",
    "end": "1445120"
  },
  {
    "text": "RPC responses that they're holding uh you have to finish these entire uh",
    "start": "1445120",
    "end": "1450559"
  },
  {
    "text": "connection handling so uh while we process those connections fast enough the latency the",
    "start": "1450559",
    "end": "1458159"
  },
  {
    "text": "CPU time that we spent on those connections is going to get added to the latency that request and response are",
    "start": "1458159",
    "end": "1464720"
  },
  {
    "text": "going to see so this is what actually started happening in production where",
    "start": "1464720",
    "end": "1471919"
  },
  {
    "text": "uh we would see random P99 increase for especially the latency sensitive",
    "start": "1471919",
    "end": "1477640"
  },
  {
    "text": "applications um generally uh stateless and uh uh",
    "start": "1477640",
    "end": "1484480"
  },
  {
    "text": "traffic is not that sensitive but when it comes to radius and uh calls that are",
    "start": "1484480",
    "end": "1489760"
  },
  {
    "text": "going to memcache or like geo store those those are like super sensitive in Uber um so as soon as you do 100",
    "start": "1489760",
    "end": "1497279"
  },
  {
    "text": "millconds time on processing new connections the existing RPCs time out",
    "start": "1497279",
    "end": "1502720"
  },
  {
    "text": "they start retrying and some of those timeouts in gcas uh cancel the stream whereas HTTP1 is",
    "start": "1502720",
    "end": "1511919"
  },
  {
    "text": "close the connection open a new connection so eventually you would start seeing a loop where you have slow down",
    "start": "1511919",
    "end": "1520320"
  },
  {
    "text": "the existing RPCs you have more connections that are coming in and then",
    "start": "1520320",
    "end": "1525520"
  },
  {
    "text": "that that starts thrashing the proxy basically uh two cases where we see this",
    "start": "1525520",
    "end": "1531120"
  },
  {
    "text": "is uh fan fanning where you have Reddis clients that discover a new Reddus node",
    "start": "1531120",
    "end": "1537200"
  },
  {
    "text": "and they start sending uh traffic to this new Reddus node so thousands of connections coming in to one single node",
    "start": "1537200",
    "end": "1543919"
  },
  {
    "text": "and fan out where one single application launches boots up and then starts connecting to all the reddest nodes",
    "start": "1543919",
    "end": "1549840"
  },
  {
    "text": "backend databases and others so uh we tried bunch of things",
    "start": "1549840",
    "end": "1555919"
  },
  {
    "text": "first we set the the standard thing rate limits on each connection each client",
    "start": "1555919",
    "end": "1561440"
  },
  {
    "text": "each server u what rate limits did was had hard failures on connections so if",
    "start": "1561440",
    "end": "1567840"
  },
  {
    "text": "application starting up it cannot connect to radius they crash loop and uh this generally very sensitive on",
    "start": "1567840",
    "end": "1575000"
  },
  {
    "text": "startups and then we tried CPU overload protection where uh once you reach",
    "start": "1575000",
    "end": "1580400"
  },
  {
    "text": "certain percentage of CPU you stop accepting connections start closing connections but this was too reactive um",
    "start": "1580400",
    "end": "1588400"
  },
  {
    "text": "we by the time we started reacting the loop was already full uh and then the",
    "start": "1588400",
    "end": "1593919"
  },
  {
    "text": "latency started kicking in the next thing that we tried was reduce the number of accepts per loop this was nice",
    "start": "1593919",
    "end": "1600960"
  },
  {
    "text": "it worked up for a bit the only thing was instead of queuing up connections in",
    "start": "1600960",
    "end": "1606159"
  },
  {
    "text": "the loop we started queuing connections in the kernel uh except Q so uh the the",
    "start": "1606159",
    "end": "1613679"
  },
  {
    "text": "over the the kernel uh except Q started overflowing syndrops are more expensive take 1 second for retries so uh and",
    "start": "1613679",
    "end": "1620880"
  },
  {
    "text": "there's no ability for us to apply uh LIFO or any any better approach uh on",
    "start": "1620880",
    "end": "1627000"
  },
  {
    "text": "these so uh to solve this we built adaptive queuing and dispatching within",
    "start": "1627000",
    "end": "1632799"
  },
  {
    "text": "uh envoy so Q holds a set of connections that that we receive uh from listeners",
    "start": "1632799",
    "end": "1639440"
  },
  {
    "text": "into a DQ and then uh we smoothly start spreading these connections in the loop",
    "start": "1639440",
    "end": "1645600"
  },
  {
    "text": "that is five connections every 10 millconds or 20 millconds based on what the space on uh usage on that machine",
    "start": "1645600",
    "end": "1652640"
  },
  {
    "text": "looks like and then uh once we start hitting certain threshold we switch from",
    "start": "1652640",
    "end": "1658240"
  },
  {
    "text": "FIFO processing to LIFO so uh we we have higher chance of successful",
    "start": "1658240",
    "end": "1665240"
  },
  {
    "text": "establishment uh so in reality it's more about start accepting connections in the DQ and then uh batch those accepted",
    "start": "1665240",
    "end": "1673520"
  },
  {
    "text": "connections uh and then leave space for IO events so uh this is what we started",
    "start": "1673520",
    "end": "1679880"
  },
  {
    "text": "doing so before queuing uh I won't go into too much of details here but uh we",
    "start": "1679880",
    "end": "1686159"
  },
  {
    "text": "had as soon as number of connections picked up the number of RPCs and the",
    "start": "1686159",
    "end": "1691679"
  },
  {
    "text": "throughput on that proxy used to crash down uh pretty hard uh latency used to",
    "start": "1691679",
    "end": "1697120"
  },
  {
    "text": "jump to 500 milliseconds and immediate drop in success rate as well so after we",
    "start": "1697120",
    "end": "1702799"
  },
  {
    "text": "built queuing uh we had consistent success rate and latency improvement",
    "start": "1702799",
    "end": "1708320"
  },
  {
    "text": "across the fleet um so far it's been working really well",
    "start": "1708320",
    "end": "1713520"
  },
  {
    "text": "um due to time constraints I'm going to jump ahead and then the next challenge was",
    "start": "1713520",
    "end": "1718799"
  },
  {
    "text": "with session reuse where uh you all know there's zero RT uh session reuse in TLS",
    "start": "1718799",
    "end": "1725760"
  },
  {
    "text": "you get a session ticket from server and you can reuse that ticket to reach the upstream",
    "start": "1725760",
    "end": "1733000"
  },
  {
    "text": "um since we use shared nothing architecture in our uh uh in our fleet where you have user",
    "start": "1733000",
    "end": "1740799"
  },
  {
    "text": "service instance A instance B instance C all of these use unique searchs and they don't share uh uh session ticket keys so",
    "start": "1740799",
    "end": "1750240"
  },
  {
    "text": "uh when envoy tries reaching to one of the destinations that is user service it gets a ticket back handed back to that",
    "start": "1750240",
    "end": "1757120"
  },
  {
    "text": "and the next time application needs to reach user service B uh you send the",
    "start": "1757120",
    "end": "1762159"
  },
  {
    "text": "ticket in but the ticket is rejected because we do not have uh the session",
    "start": "1762159",
    "end": "1768080"
  },
  {
    "text": "key does not match the session key that instance A shares so uh which is why we started uh",
    "start": "1768080",
    "end": "1776960"
  },
  {
    "text": "we fixed this in the TLS transport where uh session key now starts caching uh",
    "start": "1776960",
    "end": "1784080"
  },
  {
    "text": "keys by upstream endpoints so if you have 10 endpoints we would have tickets associated with each of those endpoints",
    "start": "1784080",
    "end": "1790720"
  },
  {
    "text": "and uh overall we started seeing much much better uh session reuse with envoy",
    "start": "1790720",
    "end": "1796799"
  },
  {
    "text": "about 30% or 400k connections per second uh in the fleet",
    "start": "1796799",
    "end": "1803919"
  },
  {
    "text": "uh we also use original test uh to the setup is you have listeners original",
    "start": "1803919",
    "end": "1810360"
  },
  {
    "text": "test and uh generally you have high fan out that is you you reach out to tens of",
    "start": "1810360",
    "end": "1817039"
  },
  {
    "text": "thousands of connections so the thing with original test is every endpoint that gets added is sent to main thread",
    "start": "1817039",
    "end": "1824320"
  },
  {
    "text": "and then main thread needs to process that and also uh spread it to all the",
    "start": "1824320",
    "end": "1829440"
  },
  {
    "text": "worker threads uh essentially we started hitting a bottleneck where um original",
    "start": "1829440",
    "end": "1835919"
  },
  {
    "text": "test starts skewing too many of these events and stops starts blocking main thread and then that starts uh failing a",
    "start": "1835919",
    "end": "1843760"
  },
  {
    "text": "health checks and overall crashes the proxy for us so uh we added batching",
    "start": "1843760",
    "end": "1849120"
  },
  {
    "text": "support in original test so now we have thousands of events batched together so",
    "start": "1849120",
    "end": "1854159"
  },
  {
    "text": "uh it doesn't take down main thread overall and uh original source we also",
    "start": "1854159",
    "end": "1860960"
  },
  {
    "text": "use original source filter one of the things that it does is it on the upstream connection it copies the source",
    "start": "1860960",
    "end": "1868080"
  },
  {
    "text": "address that we see in downstream uh using bind sys call but then when you do",
    "start": "1868080",
    "end": "1873679"
  },
  {
    "text": "bind sys call the source port that fml port that's allocated by kernel is also removed from the reuse so we uh in",
    "start": "1873679",
    "end": "1881360"
  },
  {
    "text": "production we started hitting issues where after 25k connections uh all the",
    "start": "1881360",
    "end": "1887520"
  },
  {
    "text": "services started seeing out of fml port that is exhaustion basically so the",
    "start": "1887520",
    "end": "1892960"
  },
  {
    "text": "solution was to add a socket option IP bind address no port so kernel ignores",
    "start": "1892960",
    "end": "1899320"
  },
  {
    "text": "uh the port from reservation and just uses the address uh",
    "start": "1899320",
    "end": "1904440"
  },
  {
    "text": "that um yeah so these were like quick walkthrough of the performance challenges that we saw",
    "start": "1904440",
    "end": "1909880"
  },
  {
    "text": "um just jump to the questions if you have",
    "start": "1909880",
    "end": "1914559"
  },
  {
    "text": "[Applause]",
    "start": "1916100",
    "end": "1921599"
  },
  {
    "text": "any of course somebody at the back i have seen you i do have some fairly in-depth questions",
    "start": "1929880",
    "end": "1936080"
  },
  {
    "text": "about how you got started with um injecting the proxies everywhere would you be around to talk after this because",
    "start": "1936080",
    "end": "1942080"
  },
  {
    "text": "I don't want to take up everybody's time i'm aware time's tensitive yeah sure thank you here all week",
    "start": "1942080",
    "end": "1949279"
  },
  {
    "text": "yeah thanks so there's a 10-minute break between every talk in the schedule the idea with that is that this is to allow",
    "start": "1949279",
    "end": "1954799"
  },
  {
    "text": "folks to get up and maybe go to a different room if they want to see a different talk obviously we know you would never want to leave on VCON this",
    "start": "1954799",
    "end": "1959919"
  },
  {
    "text": "is the best content um so I think what I'm gonna do is probably just let folks",
    "start": "1959919",
    "end": "1964960"
  },
  {
    "text": "run into that 10 minutes and do Q&A in that 10 minutes if you do obviously want to go to another talk if you could just",
    "start": "1964960",
    "end": "1970480"
  },
  {
    "text": "get up quietly I guess um and just make your way out it's better than us just stopping for 10 minutes and all playing",
    "start": "1970480",
    "end": "1975919"
  },
  {
    "text": "on our phones when we could be hearing stuff like this um but yeah do feel free obviously to move between the rooms um",
    "start": "1975919",
    "end": "1981679"
  },
  {
    "text": "but I think we'll use the space up we do have to keep to the schedule though i'll go completely off jack always will miss",
    "start": "1981679",
    "end": "1987760"
  },
  {
    "text": "the coffee and we'll miss the lunch so",
    "start": "1987760",
    "end": "1992279"
  },
  {
    "text": "um you mentioned that in a half% of the cases you would fill open completely um",
    "start": "1996720",
    "end": "2002799"
  },
  {
    "text": "is security less of a concern for that or like what are for our organization we",
    "start": "2002799",
    "end": "2008480"
  },
  {
    "text": "rely heavily on envoyto etc for security measures but the whole fill open thing",
    "start": "2008480",
    "end": "2014399"
  },
  {
    "text": "triggered a lot of questions for me yeah so for that half a percent those are actually connections that we find when",
    "start": "2014399",
    "end": "2020799"
  },
  {
    "text": "we go in and debug into it that are just never succeeding so it's like application A is trying to talk to B the",
    "start": "2020799",
    "end": "2026799"
  },
  {
    "text": "connection fails so we try the fall back and then that also fails there is um a very small rate I think we see like on",
    "start": "2026799",
    "end": "2033519"
  },
  {
    "text": "the order of like 10 connections per second across the fleet where we are failing back because of just some",
    "start": "2033519",
    "end": "2040279"
  },
  {
    "text": "instability um we do log those so for every instance of that we have source IP",
    "start": "2040279",
    "end": "2045519"
  },
  {
    "text": "desk IP and able to match that to workloads um and so eventually we do",
    "start": "2045519",
    "end": "2051440"
  },
  {
    "text": "want to move all this to fail closed at least by default we want the controls to be in place where we can fail things",
    "start": "2051440",
    "end": "2056480"
  },
  {
    "text": "open but for this roll out and for like this initial onboarding uh we want to",
    "start": "2056480",
    "end": "2061679"
  },
  {
    "text": "default everything to fail open because that kind of gave us gives us you know in some sense the leeway to roll this",
    "start": "2061679",
    "end": "2066878"
  },
  {
    "text": "out and have issues uh happen but not actually negatively impact the business",
    "start": "2066879",
    "end": "2072638"
  },
  {
    "text": "uh so uh later this year we plan to do kind of another round of what we call production readiness review where we you",
    "start": "2072639",
    "end": "2078960"
  },
  {
    "text": "know take our previous kind of readiness model which relies on fail open and say well now if we're going to fail closed",
    "start": "2078960",
    "end": "2084960"
  },
  {
    "text": "on things like how does that change our risk what additional controls do we have to put in place how do we have to improve our operations as well as just",
    "start": "2084960",
    "end": "2091679"
  },
  {
    "text": "kind of demonstrating like hey we've been running this thing for a year now this is how it actually works in production",
    "start": "2091679",
    "end": "2096878"
  },
  {
    "text": "thanks",
    "start": "2096879",
    "end": "2099879"
  },
  {
    "text": "hi uh you did mention earlier that uh you saw quite a bit of CPU utilization",
    "start": "2102400",
    "end": "2108800"
  },
  {
    "text": "on the TLS uh sessions have you done any uh any work on the quantum ciphers and",
    "start": "2108800",
    "end": "2117680"
  },
  {
    "text": "whether that impacted the CPU utilization more or was it the same",
    "start": "2117680",
    "end": "2124440"
  },
  {
    "text": "um so I think the benchmark that we've done is uh ECDS uh that we use for uh uh",
    "start": "2124440",
    "end": "2134960"
  },
  {
    "text": "signatures and then RSA and uh uh sorry AES that we use for uh the symmetric",
    "start": "2134960",
    "end": "2141200"
  },
  {
    "text": "keys we haven't looked at using different uh uh algorithms yet since we",
    "start": "2141200",
    "end": "2147920"
  },
  {
    "text": "have uh mix of mix fleet where we have AMD uh sorry ARM and then x86 everywhere",
    "start": "2147920",
    "end": "2154560"
  },
  {
    "text": "so most of these uh processors have uh accelerated uh instruction set for AES",
    "start": "2154560",
    "end": "2161440"
  },
  {
    "text": "uh but we uh haven't investigated into uh the quantum uh algorithms yet",
    "start": "2161440",
    "end": "2170039"
  },
  {
    "text": "cool we could probably take one more",
    "start": "2170560",
    "end": "2174160"
  },
  {
    "text": "question cool in that case thank you very much guys thank you thank you",
    "start": "2176040",
    "end": "2181940"
  },
  {
    "text": "[Applause]",
    "start": "2181940",
    "end": "2185069"
  }
]