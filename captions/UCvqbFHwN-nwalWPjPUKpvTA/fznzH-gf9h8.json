[
  {
    "text": "all right I welcome everyone to V session Thanks for staying with us all",
    "start": "160",
    "end": "5520"
  },
  {
    "text": "the way until the end of the CubeCon You are some of the brave souls who are remaining today for the the last slot So",
    "start": "5520",
    "end": "12000"
  },
  {
    "text": "thank you Uh I am Vincent Uh I'm the CTO for Asia Pacific at Red Hat based in",
    "start": "12000",
    "end": "19960"
  },
  {
    "text": "Singapore and I have with me Tamar Hi So I'm Tamar I'm from IBM Research",
    "start": "19960",
    "end": "26320"
  },
  {
    "text": "I'm an IBM fellow and a chief scientist for sustainable computing",
    "start": "26320",
    "end": "31439"
  },
  {
    "text": "All right So what are we going to cover today uh we'll start with a bit of introduction about green AI why it",
    "start": "31840",
    "end": "37600"
  },
  {
    "text": "matters Uh we've talked a lot during KCON about optimizing AI system So",
    "start": "37600",
    "end": "43840"
  },
  {
    "text": "effectively we are going to talk a lot about this but really from the angle of energy efficiency Uh we'll uh talk about",
    "start": "43840",
    "end": "51440"
  },
  {
    "text": "the key challenges and opportunities specific to cloud native AI uh and today",
    "start": "51440",
    "end": "57039"
  },
  {
    "text": "we've made a choice to uh really focus on the optimization of AI in France and",
    "start": "57039",
    "end": "62480"
  },
  {
    "text": "we'll speak about why AI in France is so important and then some of what's happening in the community right now uh",
    "start": "62480",
    "end": "69760"
  },
  {
    "text": "and the possible next steps So let's start by framing a bit uh the debate Uh",
    "start": "69760",
    "end": "75600"
  },
  {
    "text": "we are actually at the very uh uh beginning of an AI sustainability crisis",
    "start": "75600",
    "end": "82960"
  },
  {
    "text": "uh but the the challenge that we have in front of us is not new So what I want to",
    "start": "82960",
    "end": "88240"
  },
  {
    "text": "share is the explosion that you see today in aid man uh essentially started",
    "start": "88240",
    "end": "93840"
  },
  {
    "text": "as early as 2010 uh with the growing huge use of deep",
    "start": "93840",
    "end": "99439"
  },
  {
    "text": "neural network technology essentially So since 2010 which is the deep learning",
    "start": "99439",
    "end": "105520"
  },
  {
    "text": "era that you see pictured here we've seen on average a growth of four to five times a year of the energy consumption",
    "start": "105520",
    "end": "113040"
  },
  {
    "text": "to train models So we are very much in a 15 years period already of this growth and this",
    "start": "113040",
    "end": "121200"
  },
  {
    "text": "constant increase in number of resource used to train model Uh not",
    "start": "121200",
    "end": "127399"
  },
  {
    "text": "surprisingly this has led to a huge surge in energy demand for data centers",
    "start": "127399",
    "end": "133599"
  },
  {
    "text": "Uh and this is an interesting statistics that we expect by 2028 so we are just",
    "start": "133599",
    "end": "139280"
  },
  {
    "text": "three years short of that Uh 19% of data center power demand will come from AI So",
    "start": "139280",
    "end": "146480"
  },
  {
    "text": "about almost 20% of it will be purely used for AI",
    "start": "146480",
    "end": "152319"
  },
  {
    "text": "As a result of this we see the emergence of regulations to try to contain uh uh",
    "start": "152319",
    "end": "158080"
  },
  {
    "text": "the use of energy Uh we are in Europe here So you may be familiar with the uh",
    "start": "158080",
    "end": "164640"
  },
  {
    "text": "the EUA act although it's actually for the European Union but you are very close in UK Uh this is actually the",
    "start": "164640",
    "end": "171040"
  },
  {
    "text": "first regulation globally that is going to mandate the disclosure of energy",
    "start": "171040",
    "end": "176720"
  },
  {
    "text": "consumption for AI systems uh that is probably the first of many regulations",
    "start": "176720",
    "end": "182000"
  },
  {
    "text": "to come I am personally based in Asia Pacific and I'm actually working with a",
    "start": "182000",
    "end": "187120"
  },
  {
    "text": "number of government entity that are already elaborating their own regulations So this is going to be uh uh",
    "start": "187120",
    "end": "195120"
  },
  {
    "text": "probably a mandatory requirement for business to understand their consumption Now ironically uh today if you look at",
    "start": "195120",
    "end": "202280"
  },
  {
    "text": "enterprise and this is actually a statistics that was published by the state of AI infrastructure last year 74%",
    "start": "202280",
    "end": "210640"
  },
  {
    "text": "of companies that use AI today are actually struggling to leverage their",
    "start": "210640",
    "end": "215680"
  },
  {
    "text": "compute and acceleration infrastructure So that means there are lot of",
    "start": "215680",
    "end": "220799"
  },
  {
    "text": "operational problem in optimizing the use of resources",
    "start": "220799",
    "end": "226239"
  },
  {
    "text": "However it's not uh you know only a sad picture because AI also helps us with a",
    "start": "226239",
    "end": "232879"
  },
  {
    "text": "lot of improvements uh and in particular in sustainability as well I'm just",
    "start": "232879",
    "end": "238400"
  },
  {
    "text": "sharing a few use case here Uh they are very close to us because they they are use case that uh my my friends at IBM",
    "start": "238400",
    "end": "245439"
  },
  {
    "text": "research have all contributed to in the climate science space uh we've",
    "start": "245439",
    "end": "250640"
  },
  {
    "text": "collaborated with NASA to take 250,000 terabyte of earth observation data and",
    "start": "250640",
    "end": "257840"
  },
  {
    "text": "build foundation model to basically read uh satellite imaging in a much more",
    "start": "257840",
    "end": "263120"
  },
  {
    "text": "efficient way Uh we see result of about you know four times improvement in speed",
    "start": "263120",
    "end": "268160"
  },
  {
    "text": "of interpretation of of satellite imaging with this type of foundation model So they allow us to do some",
    "start": "268160",
    "end": "275600"
  },
  {
    "text": "prediction of extreme climate events for example and obviously this is a huge benefit to humanity in material science",
    "start": "275600",
    "end": "283919"
  },
  {
    "text": "uh you know foundation model have been increasingly used to uh simulate and",
    "start": "283919",
    "end": "288960"
  },
  {
    "text": "model new uh material structure and that is a huge problem as well for humankind",
    "start": "288960",
    "end": "295919"
  },
  {
    "text": "uh I'll take the example of US There's actually uh uh close to 800 substances that are toxic that are being monitored",
    "start": "295919",
    "end": "303199"
  },
  {
    "text": "by EPA in consumer product and there's hundreds of thousand of consumer product that now need to be",
    "start": "303199",
    "end": "310120"
  },
  {
    "text": "retrofitted with kind of green and safe alternative So that's a huge problem to",
    "start": "310120",
    "end": "315880"
  },
  {
    "text": "solve Um in environmental science we have this problem with what we call",
    "start": "315880",
    "end": "321520"
  },
  {
    "text": "these forever chemicals uh those are chemicals that take thousand of years to clear in the environment And uh last",
    "start": "321520",
    "end": "329360"
  },
  {
    "text": "year actually IBM research had a partnership partnership with the national science agency to look at the",
    "start": "329360",
    "end": "335840"
  },
  {
    "text": "identification and remediation of this material Uh and last but not least in",
    "start": "335840",
    "end": "340880"
  },
  {
    "text": "health sciences uh I would say AI has literally changed the game in terms of early detection of",
    "start": "340880",
    "end": "348360"
  },
  {
    "text": "disease particularly when applied to medical imaging So those are just a few",
    "start": "348360",
    "end": "353880"
  },
  {
    "text": "example Uh so we definitely want to keep AI and make use of it especially if it's",
    "start": "353880",
    "end": "359840"
  },
  {
    "text": "not to generate funny pictures uh through a chatbot but to actually you know build real value uh with the",
    "start": "359840",
    "end": "367000"
  },
  {
    "text": "application and so we contend that in order to do this we should be looking at",
    "start": "367000",
    "end": "372240"
  },
  {
    "text": "the whole life cycle and supply chain of AI and the system to actually improve",
    "start": "372240",
    "end": "378000"
  },
  {
    "text": "from top to bottom So there's a few way of looking at it",
    "start": "378000",
    "end": "383520"
  },
  {
    "text": "like the first way is through the machine learning life cycle uh a bit you know when we discuss about security or",
    "start": "383520",
    "end": "390160"
  },
  {
    "text": "devops and the shift left actually I feel this applies to AI when you look at the machine learning life cycle from",
    "start": "390160",
    "end": "396639"
  },
  {
    "text": "data preparation to feature engineering model development all the way to serving",
    "start": "396639",
    "end": "401919"
  },
  {
    "text": "the earlier you actually start to uh uh improve and simplify your flow then the",
    "start": "401919",
    "end": "409440"
  },
  {
    "text": "more results you have So an example would be from a data perspective if you can actually make more meaningful data",
    "start": "409440",
    "end": "415840"
  },
  {
    "text": "set through data distillation you could reduce your model training by up to",
    "start": "415840",
    "end": "421720"
  },
  {
    "text": "70% Because you basically have to process less data Yeah Then the whole life cycle is actually impacted by this",
    "start": "421720",
    "end": "428479"
  },
  {
    "text": "because you have smaller model faster testing less resources used in the process Um so we want to optimize",
    "start": "428479",
    "end": "436400"
  },
  {
    "text": "through the machine learning life cycle and then the other angle is more in terms of pillars We have a data",
    "start": "436400",
    "end": "443120"
  },
  {
    "text": "optimization pillar We have a model optimization How do we make the model as small as possible and then we have a",
    "start": "443120",
    "end": "450160"
  },
  {
    "text": "system centric pillar which is more the efficient operation So those are three levers three types of uh potential",
    "start": "450160",
    "end": "457599"
  },
  {
    "text": "optimization we can do uh pretty much across the system At the end of the day",
    "start": "457599",
    "end": "463039"
  },
  {
    "text": "we are trying to reduce the use of compute accelerator networking and then",
    "start": "463039",
    "end": "471080"
  },
  {
    "text": "storage Now I'll come to why uh the focus of inference Uh",
    "start": "471080",
    "end": "478319"
  },
  {
    "text": "obviously the whole life cycle is important but inference actually has a",
    "start": "478319",
    "end": "483360"
  },
  {
    "text": "very special place right now in terms of the overall consumption I'm actually using some figures published uh publicly",
    "start": "483360",
    "end": "491039"
  },
  {
    "text": "by Facebook AI and uh they are interesting in the sense that I'm going",
    "start": "491039",
    "end": "496720"
  },
  {
    "text": "to say for some of you are enterprise here whatever you see here published from Facebook you are probably way",
    "start": "496720",
    "end": "503520"
  },
  {
    "text": "higher than this in terms of the the place of inference in your consumption I mean let's let's be reminded that",
    "start": "503520",
    "end": "509599"
  },
  {
    "text": "Facebook really has a business based on data and they actually tend to retrain and retune their model on a very",
    "start": "509599",
    "end": "515760"
  },
  {
    "text": "frequent basis Most of you and and us in the audience who actually use more classical use case",
    "start": "515760",
    "end": "522560"
  },
  {
    "text": "of data we tend not to do this on such a frequent basis Yeah But so Facebook",
    "start": "522560",
    "end": "528560"
  },
  {
    "text": "published this study and it's very easy for them to actually measure where know",
    "start": "528560",
    "end": "533920"
  },
  {
    "text": "the different part of the life cycle and their contribution because they are they have specialized fleet of of hardware",
    "start": "533920",
    "end": "540080"
  },
  {
    "text": "and infrastructure dealing with the different part of the processing Uh so in their infrastructure data and model",
    "start": "540080",
    "end": "546800"
  },
  {
    "text": "tuning and inference are literally different fleet of servers and different type of optimization Uh so what they",
    "start": "546800",
    "end": "554160"
  },
  {
    "text": "actually showed is close to 65% of an AI system operational carbon uh footprint",
    "start": "554160",
    "end": "562000"
  },
  {
    "text": "actually is spent on inference Operational means the energy spend uh uh",
    "start": "562000",
    "end": "567440"
  },
  {
    "text": "to basically run the inference of the model So that's almost you know like",
    "start": "567440",
    "end": "573040"
  },
  {
    "text": "it's around two3 of pretty much the overall total Uh at the same time we",
    "start": "573040",
    "end": "578399"
  },
  {
    "text": "also demonstrated that optimizing the the the platform level Uh so that's the",
    "start": "578399",
    "end": "585560"
  },
  {
    "text": "operation can lead to as much as 800 times improvement in consumption So",
    "start": "585560",
    "end": "590959"
  },
  {
    "text": "that's like a huge reduction If you actually take care of optimizing your inference you can get fantastic results",
    "start": "590959",
    "end": "597760"
  },
  {
    "text": "Um and let's not forget something to to finish on this topic A big part of the",
    "start": "597760",
    "end": "603920"
  },
  {
    "text": "carbon consumption of AI is also actually embodied in the hardware So",
    "start": "603920",
    "end": "609920"
  },
  {
    "text": "when for example you buy uh a two cluster of H200 when you actually effectively only",
    "start": "609920",
    "end": "617120"
  },
  {
    "text": "using one obviously you are you actually have a lot of embodied carbon in the process",
    "start": "617120",
    "end": "623519"
  },
  {
    "text": "which is coming from the manufacturing of the hardware the shipping and so on So the other interesting aspect is 50%",
    "start": "623519",
    "end": "630959"
  },
  {
    "text": "of the embodied carbon comes from the hardware manufacturing So when you optimize inference you basically need",
    "start": "630959",
    "end": "637920"
  },
  {
    "text": "less hardware and you are also getting a 50% reduction in overall carbon So that",
    "start": "637920",
    "end": "643839"
  },
  {
    "text": "is super interesting So that's what I wanted to share really on explaining why we have this focus on efficient",
    "start": "643839",
    "end": "650720"
  },
  {
    "text": "inference and I'll let Tamar now explain some of the work we are doing in this space",
    "start": "650720",
    "end": "657920"
  },
  {
    "text": "All right All right Can Okay So my first statement here is that green AI is",
    "start": "657920",
    "end": "664480"
  },
  {
    "text": "efficient AI And why is that when you work on your efficiency you save",
    "start": "664480",
    "end": "670160"
  },
  {
    "text": "resources If you use less resources by definition you use less energy Energy is",
    "start": "670160",
    "end": "676000"
  },
  {
    "text": "a resource So you can also think about it like this So it's all about running your work um meeting your SLOs's",
    "start": "676000",
    "end": "684240"
  },
  {
    "text": "respecting your SLOs's while making the best use of resources using less",
    "start": "684240",
    "end": "691200"
  },
  {
    "text": "resources and using them more efficiently Now there are lots of different techniques for efficient AI",
    "start": "691200",
    "end": "698640"
  },
  {
    "text": "across the life cycle and I'm going to mention a few of them So you see on the purple the purple boxes there uh there",
    "start": "698640",
    "end": "707279"
  },
  {
    "text": "are techniques that have to do with model architecture So this is the AI",
    "start": "707279",
    "end": "712600"
  },
  {
    "text": "scientist who is working on new architectures such as Laura adapters",
    "start": "712600",
    "end": "718560"
  },
  {
    "text": "With Laura adapters you freeze some of the weights uh when you're doing fine",
    "start": "718560",
    "end": "723760"
  },
  {
    "text": "tuning for particular tasks and then you get these adapters that are just being",
    "start": "723760",
    "end": "728959"
  },
  {
    "text": "used for that particular task basically breaking breaking the monolith Um",
    "start": "728959",
    "end": "736000"
  },
  {
    "text": "mixtures of experts that's another architectural model architectural principle that can help us have more",
    "start": "736000",
    "end": "742480"
  },
  {
    "text": "efficient inference and more efficient fine-tuning Quantization that's another",
    "start": "742480",
    "end": "747680"
  },
  {
    "text": "thing Speculative decoding This is a runtime inference technique where you",
    "start": "747680",
    "end": "753920"
  },
  {
    "text": "are guessing ahead of time like optimistically a a a number of tokens",
    "start": "753920",
    "end": "759040"
  },
  {
    "text": "together using a smaller model These are all techniques which are used in order",
    "start": "759040",
    "end": "764800"
  },
  {
    "text": "to optimize a single model Okay Now the",
    "start": "764800",
    "end": "770320"
  },
  {
    "text": "second category is what you see with the blue rectangles here We're talking about",
    "start": "770320",
    "end": "777680"
  },
  {
    "text": "what's going on in deployment and operations Completely different story So",
    "start": "777680",
    "end": "784000"
  },
  {
    "text": "what's going on there is that you have multiple different models that are",
    "start": "784000",
    "end": "789200"
  },
  {
    "text": "serving multiple different users Uh these models have very different",
    "start": "789200",
    "end": "795639"
  },
  {
    "text": "characteristics and the requests are also could be of very different nature",
    "start": "795639",
    "end": "801760"
  },
  {
    "text": "Some of them are latency bound Some of them are more about throughput Okay And",
    "start": "801760",
    "end": "807680"
  },
  {
    "text": "all of this is running on a cluster with heterogeneous hardware Heterogeneous",
    "start": "807680",
    "end": "814000"
  },
  {
    "text": "hardware is really important And I will tell you why We believe in fitforpurpose",
    "start": "814000",
    "end": "820279"
  },
  {
    "text": "accelerators You don't want to run your entire workload on GPUs because you're",
    "start": "820279",
    "end": "826800"
  },
  {
    "text": "going to waste a lot of cost and a lot of energy So if you have a large model",
    "start": "826800",
    "end": "832399"
  },
  {
    "text": "in your training and you have a training job obviously you want to use GPUs If",
    "start": "832399",
    "end": "838720"
  },
  {
    "text": "you have inference requests or if you have smaller models then you may be better off with other types of",
    "start": "838720",
    "end": "845959"
  },
  {
    "text": "accelerators Okay So this is really the role of the platform The role of the platform is to bridge between the models",
    "start": "845959",
    "end": "853360"
  },
  {
    "text": "and the different jobs and requests and the infrastructure that is a",
    "start": "853360",
    "end": "858440"
  },
  {
    "text": "heterogeneous infrastructure with fit for purpose accelerators and to do that",
    "start": "858440",
    "end": "864160"
  },
  {
    "text": "in the most effective way All right So going a little bit into the challenges of that platform So you",
    "start": "864160",
    "end": "871839"
  },
  {
    "text": "really want to you have really multiple different patterns that are emerging in that landscape of AI support chat box So",
    "start": "871839",
    "end": "879920"
  },
  {
    "text": "you have low latency This is where you go to the bank and you can't find where can I find my big number or something",
    "start": "879920",
    "end": "886480"
  },
  {
    "text": "like that and you're chatting with an AI chatbot and you can be tolerant because",
    "start": "886480",
    "end": "892800"
  },
  {
    "text": "you're used to getting support from people and people are really slow So you can be tolerant but you still want low latency Okay So then you go to code",
    "start": "892800",
    "end": "900639"
  },
  {
    "text": "assist Code assist is like you're coding and you want autocomp completion for example here You also want low latency",
    "start": "900639",
    "end": "907440"
  },
  {
    "text": "but you want much more much lower latency because if it's not fast enough you're going to complete the code",
    "start": "907440",
    "end": "913440"
  },
  {
    "text": "yourself right developers have no patience We know that Um and the context",
    "start": "913440",
    "end": "919360"
  },
  {
    "text": "window here is much bigger because you you need to understand the entire codebase in order to help you in order",
    "start": "919360",
    "end": "924880"
  },
  {
    "text": "to help the AI needs to understand the entire codebase in order to help you debug do autocomplete and so on Then you",
    "start": "924880",
    "end": "931519"
  },
  {
    "text": "have LLM service Here we're talking about high volume multiple tenants lots",
    "start": "931519",
    "end": "937279"
  },
  {
    "text": "of interactive requests all coming together and you need to support that",
    "start": "937279",
    "end": "942320"
  },
  {
    "text": "Document processing completely different idea here It's about throughput Document processing is all about throughput LLM",
    "start": "942320",
    "end": "950079"
  },
  {
    "text": "powered search This is a hybrid real-time LLM and rag which is transactional database kind of workload",
    "start": "950079",
    "end": "956959"
  },
  {
    "text": "and identic completely different story where you have you now need to worry about the end to end latency the context",
    "start": "956959",
    "end": "964480"
  },
  {
    "text": "across all of these transactions topology awareness and so on So all of these are different patterns and what",
    "start": "964480",
    "end": "970880"
  },
  {
    "text": "you see on the right is you see a bunch of metrics throughput which is token per",
    "start": "970880",
    "end": "976079"
  },
  {
    "text": "second time to first token time between tokens blah blah blah all these metrics and you see some selected strategies",
    "start": "976079",
    "end": "982000"
  },
  {
    "text": "there are lots of different strategies the point here is for each one of these patterns you're going to need other",
    "start": "982000",
    "end": "987199"
  },
  {
    "text": "strategies for chat bots you need sticky management for example for code assist",
    "start": "987199",
    "end": "992480"
  },
  {
    "text": "you want to uh context truncation so you want to eliminate the code that is not important and keep the code that is is",
    "start": "992480",
    "end": "998800"
  },
  {
    "text": "important in order to reduce the context for LLM service You need you need um SLO",
    "start": "998800",
    "end": "1005759"
  },
  {
    "text": "based routing and queuing because you're going to have requests with different SLOs's and and and so on and so forth So",
    "start": "1005759",
    "end": "1014600"
  },
  {
    "text": "the really the variation in context land degree of similarity across queries latency and throughput trade-offs and so",
    "start": "1014600",
    "end": "1021440"
  },
  {
    "text": "on will affect the strategy that you want to use",
    "start": "1021440",
    "end": "1027038"
  },
  {
    "text": "and that's what makes it so complicated So in really just to summarize this is a",
    "start": "1027039",
    "end": "1032079"
  },
  {
    "text": "summary chart which says look based on the use case you may want to use",
    "start": "1032079",
    "end": "1037600"
  },
  {
    "text": "different techniques in order to achieve your goals and your highle goals at the end of the day is you know like I'm",
    "start": "1037600",
    "end": "1044959"
  },
  {
    "text": "showing here 30% reduction but maybe it's more you know a a significant reduction or a significant increase in",
    "start": "1044959",
    "end": "1052080"
  },
  {
    "text": "your effective throughput uh which is a successfully processed token sometime we call it good put you",
    "start": "1052080",
    "end": "1059039"
  },
  {
    "text": "know a a a significant reduction of cost reduction of energy and uh and still",
    "start": "1059039",
    "end": "1065280"
  },
  {
    "text": "while still maintaining user perceived latency and so on That's what you want at the end of the day The challenge here",
    "start": "1065280",
    "end": "1071039"
  },
  {
    "text": "is how to match the patterns that you have with the strategy that you use So",
    "start": "1071039",
    "end": "1076400"
  },
  {
    "text": "this is where the platform comes into play The goal is to maximize the overall",
    "start": "1076400",
    "end": "1081679"
  },
  {
    "text": "output while meeting your SLOs's and minimizing your cost bridging between",
    "start": "1081679",
    "end": "1088280"
  },
  {
    "text": "these heterogeneous LLMs and adapters and these heterogeneous infrastructure",
    "start": "1088280",
    "end": "1094000"
  },
  {
    "text": "with fitforpurpose accelerators And how do you do that so you need to integrate",
    "start": "1094000",
    "end": "1100720"
  },
  {
    "text": "best of breed optimization techniques in a coherent fashion And what I mean by",
    "start": "1100720",
    "end": "1106720"
  },
  {
    "text": "coherent fashion is that you want all of these optimization techniques to work with each other rather than against each",
    "start": "1106720",
    "end": "1114559"
  },
  {
    "text": "other And this is very important So you're going to see that lots of techniques are being introduced This",
    "start": "1114559",
    "end": "1120240"
  },
  {
    "text": "area is evolving really really quickly So we're going to go quickly through some of them Right sizing and GPU",
    "start": "1120240",
    "end": "1127400"
  },
  {
    "text": "slicing Why do you want to do GPU slicing of course you want smaller LLM models to use just a slice of a GPU They",
    "start": "1127400",
    "end": "1134559"
  },
  {
    "text": "don't need the entire thing So you want to share GPUs across models You can use",
    "start": "1134559",
    "end": "1139600"
  },
  {
    "text": "MC partitions or MPS And you will need right sizing techniques What is right",
    "start": "1139600",
    "end": "1145520"
  },
  {
    "text": "sizing techniques is techniques that allow you either through profiling or by analytic methods to assess the size of",
    "start": "1145520",
    "end": "1153200"
  },
  {
    "text": "the of the slice that you need for a particular model And by the way I'm mentioning here under sources you're",
    "start": "1153200",
    "end": "1159760"
  },
  {
    "text": "going to see the links to some work that we did in IBM research in collaboration with um Red Hat and Nvidia on Insta",
    "start": "1159760",
    "end": "1167440"
  },
  {
    "text": "Slice which is an open-source project that allows you to dynamically slice GPUs And there is another work that we",
    "start": "1167440",
    "end": "1174240"
  },
  {
    "text": "did here that is really showing you how to right size When you combine the two",
    "start": "1174240",
    "end": "1180160"
  },
  {
    "text": "then you can get really really good results and save a lot of energy and a lot of cost So then we go to the other",
    "start": "1180160",
    "end": "1186480"
  },
  {
    "text": "one routing and queuing Why do you need routing and queuing why is it not just load balancing the good old load",
    "start": "1186480",
    "end": "1192799"
  },
  {
    "text": "balancing well that's because you have different requests with very different nature Batch versus interactive right",
    "start": "1192799",
    "end": "1201919"
  },
  {
    "text": "and it's very unpredictable to know how long it's going to take a to process a",
    "start": "1201919",
    "end": "1208640"
  },
  {
    "text": "single request Why because you don't know how many tokens are going to be",
    "start": "1208640",
    "end": "1213960"
  },
  {
    "text": "generated So you need intelligent load balancing and intelligent queue",
    "start": "1213960",
    "end": "1219039"
  },
  {
    "text": "management You want to have you want to avoid the head of the line problem where",
    "start": "1219039",
    "end": "1224400"
  },
  {
    "text": "one request is blocking everything else uh you want to uh potentially introduce",
    "start": "1224400",
    "end": "1229919"
  },
  {
    "text": "eviction and techniques like that You may want to reorder the queue or the even the way you arrange the cues matter",
    "start": "1229919",
    "end": "1237760"
  },
  {
    "text": "Um so we did some work in collaboration with UIC So this is IBM research and UIC",
    "start": "1237760",
    "end": "1243440"
  },
  {
    "text": "Um very interesting work I I recommend to read this paper because it's going into all these policies and a lot of",
    "start": "1243440",
    "end": "1250080"
  },
  {
    "text": "work that already went into uh the VLM open source project All right So the point here is",
    "start": "1250080",
    "end": "1257919"
  },
  {
    "text": "that these things depend on one another So as you do um routing and queuing you",
    "start": "1257919",
    "end": "1264080"
  },
  {
    "text": "also want to do caching So one of the most important things here is that when",
    "start": "1264080",
    "end": "1272039"
  },
  {
    "text": "you KV cache management is a very important techniques that is being used and this is because of the um auto",
    "start": "1272039",
    "end": "1278960"
  },
  {
    "text": "reggressive nature of LLMs So you want to reuse it's it works in iterations and",
    "start": "1278960",
    "end": "1284240"
  },
  {
    "text": "every iteration generates the next token But what you want is you don't want to recomputee everything again and again",
    "start": "1284240",
    "end": "1291280"
  },
  {
    "text": "That's why you use KV cache However in your load balancing or in your routing",
    "start": "1291280",
    "end": "1296880"
  },
  {
    "text": "you better do it based on where the cache is right otherwise you're going to get suboptimal results So you see that",
    "start": "1296880",
    "end": "1304320"
  },
  {
    "text": "there's some interdependencies that are starting to appear here Um so we talked about caching and loading and why you",
    "start": "1304320",
    "end": "1311120"
  },
  {
    "text": "need them right auto reggressive nature H that by itself is a very complicated",
    "start": "1311120",
    "end": "1317200"
  },
  {
    "text": "problem because not only that you need a KV cache in every node it's also how do you share KVK cache across nodes because",
    "start": "1317200",
    "end": "1325919"
  },
  {
    "text": "you're not always going to be able to send all of the u prompts that are uh",
    "start": "1325919",
    "end": "1332400"
  },
  {
    "text": "that relate to the same user or to the same session or that are similar to the",
    "start": "1332400",
    "end": "1337520"
  },
  {
    "text": "same node because of load balancing issues Okay so you need to share the KV cache across nodes How do you do that",
    "start": "1337520",
    "end": "1344480"
  },
  {
    "text": "lots of ideas Um project moon cake is a good example that is really really sort",
    "start": "1344480",
    "end": "1349919"
  },
  {
    "text": "of stretching um the the possible here and looking at",
    "start": "1349919",
    "end": "1354960"
  },
  {
    "text": "techniques such as this aggregated prefill and decoding and so on Um and then Laura management Uh so",
    "start": "1354960",
    "end": "1363840"
  },
  {
    "text": "remember that we discussed the Laura adapters That's another thing here um we",
    "start": "1363840",
    "end": "1369360"
  },
  {
    "text": "can have thousands of adapters and these adapters could be dynamic So where do we",
    "start": "1369360",
    "end": "1376840"
  },
  {
    "text": "cache each adapter so that it's going to be available for the requests that need it",
    "start": "1376840",
    "end": "1385520"
  },
  {
    "text": "because not all requests are equal They're going to need different adapters And then how do routing and",
    "start": "1385520",
    "end": "1391799"
  },
  {
    "text": "queuing actually factor it in in the decision making All right So here we",
    "start": "1391799",
    "end": "1397679"
  },
  {
    "text": "created another dependency and then finally autoscaling Autoscaling is all about the",
    "start": "1397679",
    "end": "1403520"
  },
  {
    "text": "number of VLM instances So you have a model which is running on three or five",
    "start": "1403520",
    "end": "1409039"
  },
  {
    "text": "VLM instances and you need to grow the number of VLM instances based on your load and shrink it based on your load In",
    "start": "1409039",
    "end": "1416720"
  },
  {
    "text": "that process you also need to make placement decisions So where am I starting that new VM instance that also",
    "start": "1416720",
    "end": "1424000"
  },
  {
    "text": "depends on all of these other elements So one thing is you want to look at the size of the queue in order to know how",
    "start": "1424000",
    "end": "1430320"
  },
  {
    "text": "many instances you need for a VM request and the other thing is you need to understand what is guest where in order",
    "start": "1430320",
    "end": "1437039"
  },
  {
    "text": "to place the new VLM server for a particular model in the optimal place So",
    "start": "1437039",
    "end": "1443039"
  },
  {
    "text": "we're introducing all of these interdependencies and I think that this is more more than just dependencies",
    "start": "1443039",
    "end": "1448240"
  },
  {
    "text": "Dependencies is like when you need a library in order to run something and it's easy to handle I call it intersectionality intersectionality in",
    "start": "1448240",
    "end": "1455760"
  },
  {
    "text": "the sense that there needs to be some sort of of a very intelligence awareness so that these control mechanisms are",
    "start": "1455760",
    "end": "1461840"
  },
  {
    "text": "going to work together so that 1 + 1 is going to equal three not minus one So",
    "start": "1461840",
    "end": "1468240"
  },
  {
    "text": "the question is how to keep the balance on one way we see a huge like a light",
    "start": "1468240",
    "end": "1476080"
  },
  {
    "text": "speeded evolution of the technology and I bet you probably in one month I'm",
    "start": "1476080",
    "end": "1481600"
  },
  {
    "text": "going to have to have another pink box here because there is going to be a new technique but I can bet you 100% that",
    "start": "1481600",
    "end": "1488240"
  },
  {
    "text": "they're going to be more algorithms to do better distributed cache management and so on and better routing and better",
    "start": "1488240",
    "end": "1494559"
  },
  {
    "text": "queuing and so on So the area evolves really quickly and we really want to maximize a combined outcome of these of",
    "start": "1494559",
    "end": "1502720"
  },
  {
    "text": "these uh that we're getting from all of these optimization techniques right so how do we do that so these are some",
    "start": "1502720",
    "end": "1508720"
  },
  {
    "text": "architectural principles first of all modularity and separation of concerns we really have to have well- definfined",
    "start": "1508720",
    "end": "1515919"
  },
  {
    "text": "APIs well defined control uh control flow and data flow what does the routing",
    "start": "1515919",
    "end": "1522480"
  },
  {
    "text": "need to know about caching so that the routing could be optimal right and um we",
    "start": "1522480",
    "end": "1528799"
  },
  {
    "text": "really want datadriven optimization So we need benchmarking tools that allows",
    "start": "1528799",
    "end": "1534240"
  },
  {
    "text": "us to understand what are the benefits that we're going to get from particular techniques but also when we combine",
    "start": "1534240",
    "end": "1540000"
  },
  {
    "text": "techniques together right uh we want to leverage best of breed open source technologies and we want to support this",
    "start": "1540000",
    "end": "1547440"
  },
  {
    "text": "rapid evolution working together as a community so that we can test new",
    "start": "1547440",
    "end": "1552559"
  },
  {
    "text": "algorithms quickly So if I'm using my favorite router but I have a new distributed KV cache algorithm I can",
    "start": "1552559",
    "end": "1559279"
  },
  {
    "text": "test the two together and I don't need to reinvent the wheel every time that a new technology comes into play and",
    "start": "1559279",
    "end": "1565520"
  },
  {
    "text": "obviously support atrogenity because that's also good for efficiency and for",
    "start": "1565520",
    "end": "1571440"
  },
  {
    "text": "cost reduction Um so now I'm going to pass it back to Vincent to talk about",
    "start": "1571440",
    "end": "1576559"
  },
  {
    "text": "the role of the CNCF in supporting the community effort around this article and",
    "start": "1576559",
    "end": "1583760"
  },
  {
    "text": "around this technology All right So we we've seen extremely",
    "start": "1583760",
    "end": "1590080"
  },
  {
    "text": "complex problem to actually solve the platform efficiency problem uh and",
    "start": "1590080",
    "end": "1596480"
  },
  {
    "text": "actually that opens really an avenue for the CNCF to really help the industry to",
    "start": "1596480",
    "end": "1604000"
  },
  {
    "text": "look at how we standardize some of these practices and efforts Uh so you probably",
    "start": "1604000",
    "end": "1610080"
  },
  {
    "text": "have heard this morning during one of the keynote uh examples such as the",
    "start": "1610080",
    "end": "1615200"
  },
  {
    "text": "gateway API extension for LLM in France This is an example of standard that the",
    "start": "1615200",
    "end": "1621440"
  },
  {
    "text": "CNCF as a group can promote for technology vendors and user to really",
    "start": "1621440",
    "end": "1626720"
  },
  {
    "text": "standardize the way they are going to look at optimizing inference 2 model Uh",
    "start": "1626720",
    "end": "1632000"
  },
  {
    "text": "you may have heard as well uh in the telco session about a project called Kepler Kepler essentially is a standard",
    "start": "1632000",
    "end": "1638960"
  },
  {
    "text": "for energy observability uh uh in workload and obviously we are using it to evaluate the energy span of",
    "start": "1638960",
    "end": "1646400"
  },
  {
    "text": "AI workload in a cloud native context So the CNCF is has this huge role to play",
    "start": "1646400",
    "end": "1653279"
  },
  {
    "text": "to help us actually standardize our technology contribution to some of the platform capability",
    "start": "1653279",
    "end": "1660720"
  },
  {
    "text": "Now the challenge though is having technology and standard is good but it's",
    "start": "1660720",
    "end": "1666640"
  },
  {
    "text": "actually usually difficult for any player to build a consistent platform So",
    "start": "1666640",
    "end": "1671840"
  },
  {
    "text": "what we started about a year ago is a joint uh partnership between the the",
    "start": "1671840",
    "end": "1677520"
  },
  {
    "text": "CNCF uh AI work group uh as well as the tag environmental sustainability to look",
    "start": "1677520",
    "end": "1684159"
  },
  {
    "text": "at best practices of architecting sustainable platform So we started to",
    "start": "1684159",
    "end": "1689279"
  },
  {
    "text": "write a white paper uh and this white paper essentially is is uh looking at",
    "start": "1689279",
    "end": "1694880"
  },
  {
    "text": "four dimension which influence the approach to sustainable AI which is a type of deployment environment that you",
    "start": "1694880",
    "end": "1702080"
  },
  {
    "text": "actually uh deploy the technology on It could be anything from a public cloud footprint to an edge computing footprint",
    "start": "1702080",
    "end": "1709600"
  },
  {
    "text": "uh it looks like the type of AI system Tamar has explained earlier that this literally drives the type of",
    "start": "1709600",
    "end": "1715039"
  },
  {
    "text": "optimization uh you can leverage obviously the AI life cycle are you",
    "start": "1715039",
    "end": "1720320"
  },
  {
    "text": "optimizing the data the model the operation and then of course the persona so who is actually operating the",
    "start": "1720320",
    "end": "1726799"
  },
  {
    "text": "platform and can take active steps to optimize so this white paper is meant to",
    "start": "1726799",
    "end": "1733039"
  },
  {
    "text": "uh provide a collection of best practices in uh uh building and operating sustainable AI platform",
    "start": "1733039",
    "end": "1741039"
  },
  {
    "text": "And so I'll finish with this Uh if this topic is of interest uh I would invite",
    "start": "1743679",
    "end": "1749600"
  },
  {
    "text": "you to find out more about the work by the cloud native AI working group which I'm part of We did a lot with AI in",
    "start": "1749600",
    "end": "1757960"
  },
  {
    "text": "general But as you've probably inferred from your time in the past three day at KubeCon a lot of the focus right now is",
    "start": "1757960",
    "end": "1765840"
  },
  {
    "text": "operating AI efficiently Uh the second link is really uh on the working group",
    "start": "1765840",
    "end": "1772399"
  },
  {
    "text": "on the sustainable AI uh uh white paper and so we are very close to publish an",
    "start": "1772399",
    "end": "1779760"
  },
  {
    "text": "open draft for consultation but right now you can find about 10 to 15 different best practices and technique",
    "start": "1779760",
    "end": "1786799"
  },
  {
    "text": "already in the white paper uh that are shared across different organization and we very much welcome more technical",
    "start": "1786799",
    "end": "1793640"
  },
  {
    "text": "contribution or reviews Thank you very much for your attention Uh and have a",
    "start": "1793640",
    "end": "1799679"
  },
  {
    "text": "great weekend ahead [Applause]",
    "start": "1799679",
    "end": "1805339"
  }
]