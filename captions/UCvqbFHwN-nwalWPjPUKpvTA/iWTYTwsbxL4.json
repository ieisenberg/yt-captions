[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "hello everyone my name is urishi monani and i'm a software engineer at red hat as well as a crime maintainer hey folks",
    "start": "640",
    "end": "8080"
  },
  {
    "text": "thanks for joining my name is peter hunt i'm a software engineer at red hat primarily working on cryo",
    "start": "8080",
    "end": "13679"
  },
  {
    "text": "hey friends i'm sasha one of the maintainers of cryo and i'm happy to be here today hello everyone i'm renault patel i am",
    "start": "13679",
    "end": "20640"
  },
  {
    "text": "also a software engineer at radak and a cryo maintainer welcome to our talk cryo still lost kubernetes",
    "start": "20640",
    "end": "27359"
  },
  {
    "text": "cry was designed as a runtime just for kubernetes and in today's talk we will go over some of the things that we have been working on",
    "start": "27359",
    "end": "33760"
  },
  {
    "text": "in cryo to make your kubernetes clusters run even better i will first give a",
    "start": "33760",
    "end": "40239"
  },
  {
    "start": "38000",
    "end": "48000"
  },
  {
    "text": "brief overview of the latest developments happening in cryo and then my co-presenters will dive in deeper into some of the notable features",
    "start": "40239",
    "end": "47039"
  },
  {
    "text": "we have added we have been adding more metrics such as improved cra level stats that could be",
    "start": "47039",
    "end": "52160"
  },
  {
    "start": "48000",
    "end": "150000"
  },
  {
    "text": "exported and queried through meteors sasha is also working on adding um related metrics that will give",
    "start": "52160",
    "end": "57840"
  },
  {
    "text": "a better visibility into room kills of your parts these metrics provide a good foundation to add",
    "start": "57840",
    "end": "63760"
  },
  {
    "text": "custom alerts to your cluster you may have seen our prior talk about dropping the infra container",
    "start": "63760",
    "end": "69920"
  },
  {
    "text": "i am happy to report that the feature is now getting close to stable as we have been fixing bugs and issues there",
    "start": "69920",
    "end": "76720"
  },
  {
    "text": "hardman added a feature called short name aliases that we have now integrated into cryo as well",
    "start": "76720",
    "end": "82960"
  },
  {
    "text": "this feature allows you to create aliases for your preferred images and prevents registry image squatters",
    "start": "82960",
    "end": "89280"
  },
  {
    "text": "from running malicious images on your cluster you can configure aliases for your critical images to better protect your",
    "start": "89280",
    "end": "95360"
  },
  {
    "text": "cluster you can read more about this feature from the article linked in the slides",
    "start": "95360",
    "end": "101280"
  },
  {
    "text": "another small feature that i would like to highlight is the p prof over unix socket support by default",
    "start": "101280",
    "end": "107920"
  },
  {
    "text": "pprof works over http endpoint and that needs to be protected enabling it over the unix socket keeps",
    "start": "107920",
    "end": "115200"
  },
  {
    "text": "the endpoint access locked down by default and provides valuable profiling information to admins",
    "start": "115200",
    "end": "120399"
  },
  {
    "text": "and developers more easily we have been using the pattern of enabling experimental features",
    "start": "120399",
    "end": "126799"
  },
  {
    "text": "using annotations and cryo to test and refine them before proposing them for inclusion as",
    "start": "126799",
    "end": "132160"
  },
  {
    "text": "first class features in kubernetes ubershe will cover username spaces and sasha will cover sitcom profile",
    "start": "132160",
    "end": "139040"
  },
  {
    "text": "generation in the talk today besides those we have support for specifying",
    "start": "139040",
    "end": "144080"
  },
  {
    "text": "shim size and enabling devices and containers using annotations you can check out our docs for more",
    "start": "144080",
    "end": "149520"
  },
  {
    "text": "details if you aren't aware docker shim is slated to be removed from the cubelet in kubernetes 124.",
    "start": "149520",
    "end": "156239"
  },
  {
    "start": "150000",
    "end": "184000"
  },
  {
    "text": "you can read the linked cap for more information the guidance from sig node is to move to a cri based runtime and we",
    "start": "156239",
    "end": "162319"
  },
  {
    "text": "want cryo to be your preferred runtime now how can you actually switch to cryo",
    "start": "162319",
    "end": "167519"
  },
  {
    "text": "simple actually there are two steps the first step is to install and start",
    "start": "167519",
    "end": "172640"
  },
  {
    "text": "cryo service on your node we package cryo for several distributions the second step is to configure the",
    "start": "172640",
    "end": "178959"
  },
  {
    "text": "cubelet to point to the cryosocket as a container runtime endpoint and you are all set",
    "start": "178959",
    "end": "185120"
  },
  {
    "text": "folks are used to the docker cli for debugging and building images today we have alternatives for that cryctl is",
    "start": "185120",
    "end": "191840"
  },
  {
    "text": "a debugging tool that can be used to interact with a cri runtime directory you can list pods",
    "start": "191840",
    "end": "197120"
  },
  {
    "text": "list containers inspect them exact into them using chrysatel you can check out the link documentation",
    "start": "197120",
    "end": "203519"
  },
  {
    "text": "for more details we recommend using podmen for your local container development workflows",
    "start": "203519",
    "end": "209200"
  },
  {
    "text": "including building your container images tagging and pushing them to a registry next up i'm going to do a go through a",
    "start": "209200",
    "end": "217040"
  },
  {
    "start": "213000",
    "end": "252000"
  },
  {
    "text": "case study that we did and describe some improvements we've made with cryo",
    "start": "217040",
    "end": "222560"
  },
  {
    "text": "and handling heavy load this will hopefully highlight a way that",
    "start": "222560",
    "end": "228640"
  },
  {
    "text": "cryo is uniquely uh capable of tuning its behavior to the needs of the cubelet to improve",
    "start": "228640",
    "end": "235920"
  },
  {
    "text": "these kinds of heavy load situations uh in the following slides i will",
    "start": "235920",
    "end": "241200"
  },
  {
    "text": "describe something called a resource which is just a pod or a container something that the cubelet asks cryo to",
    "start": "241200",
    "end": "247760"
  },
  {
    "text": "create whose creation time partially depends on the load of the node so we're going to talk",
    "start": "247760",
    "end": "254400"
  },
  {
    "start": "252000",
    "end": "280000"
  },
  {
    "text": "through the fundamental problem much like the docker shim and other cri implementations gilbert and cryo have a client and",
    "start": "254400",
    "end": "261040"
  },
  {
    "text": "server relationship because of that the cubit must set timeouts on resource creation requests",
    "start": "261040",
    "end": "266479"
  },
  {
    "text": "this is to guarantee the eventual part of eventual consistency but what happens when one of these",
    "start": "266479",
    "end": "272320"
  },
  {
    "text": "timeouts is actually hit when a potter container is being created",
    "start": "272320",
    "end": "278000"
  },
  {
    "text": "so i'm going to walk through this scenario this is the behavior that cryo",
    "start": "278000",
    "end": "284400"
  },
  {
    "start": "280000",
    "end": "758000"
  },
  {
    "text": "did before crow 119. so the the cubelet requests a resource",
    "start": "284400",
    "end": "291680"
  },
  {
    "text": "from cryo early on in that process crowl reserves the name this is a primary",
    "start": "291680",
    "end": "297040"
  },
  {
    "text": "key that is uh used to make sure multiple clients would try to create the same thing",
    "start": "297040",
    "end": "303600"
  },
  {
    "text": "it includes the attempt number the namespace the pod name or the container name depending on what type of resource",
    "start": "303600",
    "end": "309039"
  },
  {
    "text": "it is due to system load at some point the cryo",
    "start": "309039",
    "end": "314880"
  },
  {
    "text": "request takes longer than the cubit is expecting and the cuba times out this takes four",
    "start": "314880",
    "end": "320639"
  },
  {
    "text": "minutes for a pod and container creation and can happen because of time spent",
    "start": "320639",
    "end": "328880"
  },
  {
    "text": "with the provisioning networking resources by the cni plug-in or from i o throttling",
    "start": "328880",
    "end": "336720"
  },
  {
    "text": "from the cloud provider or from cryo recursively relabeling selinux",
    "start": "336720",
    "end": "343199"
  },
  {
    "text": "directory volumes for selinux whatever the reason is the cubelet",
    "start": "343199",
    "end": "351360"
  },
  {
    "text": "uh that the original request fails from the perspective of the cubelet because the timeout happened",
    "start": "351360",
    "end": "356400"
  },
  {
    "text": "it doesn't know that the request didn't just go into the void because of that it sends a new create",
    "start": "356400",
    "end": "362800"
  },
  {
    "text": "resource request but cryo is already in the middle of creating the original resource and",
    "start": "362800",
    "end": "367840"
  },
  {
    "text": "because of that crowd returns a name as reserved error because that unique name is already actively being created in",
    "start": "367840",
    "end": "373680"
  },
  {
    "text": "another go routine eventually cryo detects uh",
    "start": "373680",
    "end": "379600"
  },
  {
    "text": "cryo finishes creating the resource and detects that there was a timeout and cleans up",
    "start": "379600",
    "end": "386560"
  },
  {
    "text": "all of its work uh this is to prevent the uh resource from leaking",
    "start": "386560",
    "end": "391600"
  },
  {
    "text": "until the cubic garbage collector has to come in and clean it up but this is not very elegant for uh a",
    "start": "391600",
    "end": "398560"
  },
  {
    "text": "couple of reasons number one during this time the cubelet",
    "start": "398560",
    "end": "404639"
  },
  {
    "text": "where the cuban cryo are doing this dance where the cupid requests the resource and crowd returns saying name is reserved uh that happens every 10",
    "start": "404639",
    "end": "411680"
  },
  {
    "text": "seconds and that every time that happens an error propagates through the",
    "start": "411680",
    "end": "416960"
  },
  {
    "text": "kubernetes event system saying name is reserved the pod sandbox creation failed or container creation failed uh for that",
    "start": "416960",
    "end": "423759"
  },
  {
    "text": "reason it's kind of an ugly uh user experience on top of that cryo completely undoes",
    "start": "423759",
    "end": "431280"
  },
  {
    "text": "all of its work after the timeout happens crowd cleans up the resource so that nothing leaks but then we end up",
    "start": "431280",
    "end": "439440"
  },
  {
    "text": "redoing all of that work again this not only is wasteful but it also",
    "start": "439440",
    "end": "445120"
  },
  {
    "text": "further exacerbates a situation that the you know is already a heavy load the",
    "start": "445120",
    "end": "451199"
  },
  {
    "text": "node is already under load and that's how we got into this situation and it's only making the matters worse",
    "start": "451199",
    "end": "456400"
  },
  {
    "text": "so we need to do something better and luckily crowl can rely on the fact that it's not a generic",
    "start": "456400",
    "end": "461840"
  },
  {
    "text": "uh container manager but it specifically tailored its behavior is specifically tailored to",
    "start": "461840",
    "end": "467199"
  },
  {
    "text": "the needs of the cubelet so uh one thing that we can do uh to mitigate one of the problems is",
    "start": "467199",
    "end": "473520"
  },
  {
    "text": "finish creating the resource and then until the cubelet asks a gun",
    "start": "473520",
    "end": "478639"
  },
  {
    "text": "so the process is largely the same the reserve the name is reserved timeout",
    "start": "478639",
    "end": "484000"
  },
  {
    "text": "happens resources uh create resource comes in again name is observed error so those errors are",
    "start": "484000",
    "end": "489520"
  },
  {
    "text": "still propagating through the system but when crowd protect detects the timeout",
    "start": "489520",
    "end": "495039"
  },
  {
    "text": "instead of undoing all of the work that it had previously done it stashes it it saves that resource um",
    "start": "495039",
    "end": "501360"
  },
  {
    "text": "it's able to do this because the kubelet is a you know a special client in that it has",
    "start": "501360",
    "end": "506720"
  },
  {
    "text": "very specific behavior that it does uh we the crowd developers know that the cubelet is going to re-request",
    "start": "506720",
    "end": "513039"
  },
  {
    "text": "that same resource until it gets created it's going to keep going going going going not only that but it's going to keep the",
    "start": "513039",
    "end": "519440"
  },
  {
    "text": "same unique name uh the name that we had reserved unless the",
    "start": "519440",
    "end": "524800"
  },
  {
    "text": "api server changes the request but then it's a new attempt number which is a new name",
    "start": "524800",
    "end": "531120"
  },
  {
    "text": "so we know that the request is going to stay the same for that unique name and uh for that reason we",
    "start": "531120",
    "end": "537920"
  },
  {
    "text": "can save the request knowing that if a new request comes in with the same name it's going to be the same request",
    "start": "537920",
    "end": "543040"
  },
  {
    "text": "and we know that that request is coming because we know what the keyboard is doing so uh this is",
    "start": "543040",
    "end": "550399"
  },
  {
    "text": "great this solves one of our problems we're no longer uh redoing work that we had previously done",
    "start": "550399",
    "end": "556720"
  },
  {
    "text": "when the new request comes in we realize hey i've already saved this resource i'm gonna return it to the cubelet",
    "start": "556720",
    "end": "562160"
  },
  {
    "text": "problem solved so we got one of those problems we're no longer doing redoing work but there's still the",
    "start": "562160",
    "end": "568640"
  },
  {
    "text": "problem of the api events where during this process if cryo is taking five minutes to create this",
    "start": "568640",
    "end": "574399"
  },
  {
    "text": "uh this resource the cubelet is going to be attempting to get you know about every",
    "start": "574399",
    "end": "580720"
  },
  {
    "text": "10 seconds cuba's gonna re-ask for this thing which uh you know four minutes minus",
    "start": "580720",
    "end": "586000"
  },
  {
    "text": "five that's one left and then uh so that's about six times we're gonna get this event through the system",
    "start": "586000",
    "end": "591440"
  },
  {
    "text": "and that's not a very pretty user experience uh the user is gonna see that their pod is failing because if name is",
    "start": "591440",
    "end": "597040"
  },
  {
    "text": "reserved it's gonna be failing a lot name it's reserved name is reserved and that is so we can do better so another",
    "start": "597040",
    "end": "604079"
  },
  {
    "text": "thing that cryo is able to do and has done is we're going to throttle new requests and i've",
    "start": "604079",
    "end": "610240"
  },
  {
    "text": "kind of changed up the diagram a little bit because it's not entirely clear if there aren't different routines so cryo r1 stands for",
    "start": "610240",
    "end": "615839"
  },
  {
    "text": "routine one the routine the go routine that is processing the original request so same thing",
    "start": "615839",
    "end": "623360"
  },
  {
    "text": "happens request comes in crowl reserves the name cuba times out that's all normal cuba sends a new",
    "start": "623360",
    "end": "630160"
  },
  {
    "text": "request to a new cryo routine which was happening before but instead of immediately saying i am",
    "start": "630160",
    "end": "636160"
  },
  {
    "text": "already working in this cube but name is reserved air cryo's going to wait it's going to stall until either",
    "start": "636160",
    "end": "642079"
  },
  {
    "text": "the request times out from the cuba's perspective or for a uh you know a a fixed time like",
    "start": "642079",
    "end": "649279"
  },
  {
    "text": "six minutes just to make sure that the cuba didn't disappear into the void and we're not waiting forever and or until the resource is created",
    "start": "649279",
    "end": "657839"
  },
  {
    "text": "basically what we're doing is we're stalling the cubelet who would otherwise be sending these requests every 10 seconds but now",
    "start": "657839",
    "end": "664320"
  },
  {
    "text": "is forced to wait for its own timeout length at least or at most its own timeout length",
    "start": "664320",
    "end": "671839"
  },
  {
    "text": "eventually cryo the original routine detects that the timeout happened and it saves the resource this is",
    "start": "671839",
    "end": "679120"
  },
  {
    "text": "consistent with what we did before and cryo returns an error this",
    "start": "679120",
    "end": "684640"
  },
  {
    "text": "the second routine also needs to return an error and this is to mitigate against a situation where the cubelet decides",
    "start": "684640",
    "end": "691600"
  },
  {
    "text": "that the request has timed out at the same time cryo decides that the resource is available",
    "start": "691600",
    "end": "696720"
  },
  {
    "text": "base cuba cancels cryo attempts to send and that resource gets lost",
    "start": "696720",
    "end": "702320"
  },
  {
    "text": "so to protect against races uh and uh to with cryo this second one",
    "start": "702320",
    "end": "709360"
  },
  {
    "text": "returns an error luckily cryo knows that the cube is going to re-request that",
    "start": "709360",
    "end": "715600"
  },
  {
    "text": "resource pretty much immediately 10 seconds later and on that next re-request",
    "start": "715600",
    "end": "720639"
  },
  {
    "text": "cryo can look into his resource store realize oh i've already got this and return it immediately",
    "start": "720639",
    "end": "726560"
  },
  {
    "text": "so this is a much better user experience and also mitigates against the load we're now you know instead of returning",
    "start": "726560",
    "end": "733680"
  },
  {
    "text": "one of these errors every 10 seconds it's once every four minutes and we're not re uh doing all of the work that we did in the",
    "start": "733680",
    "end": "740079"
  },
  {
    "text": "original routine we're saving that resource just as we uh you know should",
    "start": "740079",
    "end": "745680"
  },
  {
    "text": "and uh the moment that we're able to we send that resource back and we're only this is all possible",
    "start": "745680",
    "end": "751920"
  },
  {
    "text": "because cryo is able to tune its behavior to the cubelet so next up i'm going to go through a",
    "start": "751920",
    "end": "758240"
  },
  {
    "text": "quick demo so here we have a kubernetes local kubernetes cluster setup",
    "start": "758240",
    "end": "764880"
  },
  {
    "text": "um we're going to have two different versions one version is one where we throw away the progress and have this",
    "start": "764880",
    "end": "772240"
  },
  {
    "text": "namer's reserved error pop up and the other one is going to be",
    "start": "772240",
    "end": "777440"
  },
  {
    "text": "one where the new version where we uh we can save the results and don't",
    "start": "777440",
    "end": "785200"
  },
  {
    "text": "throw away it and also stall the the cubelet so here the note is just bootstrapping",
    "start": "785200",
    "end": "792320"
  },
  {
    "text": "um so i have to admit i was not able to",
    "start": "792320",
    "end": "797519"
  },
  {
    "text": "accurately uh create this situation it's hard to put a node under load to",
    "start": "797519",
    "end": "804000"
  },
  {
    "text": "the point where this situation pops up but the node is not totally overwhelmed um it's hard to artificially do that",
    "start": "804000",
    "end": "811120"
  },
  {
    "text": "so i have spoofed it i uh i hope that you'll trust me that this situation is",
    "start": "811120",
    "end": "816560"
  },
  {
    "text": "largely mitigated in production it's a situation that came up quite a bit and um we're seeing a lot of improvements with it um",
    "start": "816560",
    "end": "823920"
  },
  {
    "text": "but yes it's difficult to artificially do um so the way that i artificially did it",
    "start": "823920",
    "end": "831440"
  },
  {
    "text": "was i have a cni plugin that i created which is called bridge timeout and all",
    "start": "831440",
    "end": "838240"
  },
  {
    "text": "that it does is if the cni command is add which is called on a run pod sandbox request",
    "start": "838240",
    "end": "845360"
  },
  {
    "text": "it sleeps for four minutes and then after that uh it calls the bridge plug-in it's very",
    "start": "845360",
    "end": "851199"
  },
  {
    "text": "simple basically does what the bridge plug-in does but a lot worse the",
    "start": "851199",
    "end": "856880"
  },
  {
    "text": "um the motivation here is we have seen in production situations where our cni plug-ins are overwhelmed by the rate",
    "start": "856880",
    "end": "864480"
  },
  {
    "text": "of sandbox creation and they stall uh the node from being able to actually",
    "start": "864480",
    "end": "871760"
  },
  {
    "text": "um you know create the resources so we ran into a lot of names reserved errors from cni plug-ins being the bottleneck so",
    "start": "871760",
    "end": "879519"
  },
  {
    "text": "this is a realistic situation just kind of spoofed so uh we have the node running with the",
    "start": "879519",
    "end": "885680"
  },
  {
    "text": "old cryo we're gonna create this resource um the resource is just gonna be a pod that",
    "start": "885680",
    "end": "891360"
  },
  {
    "text": "calls top it's my favorite one to demo and then we are going to um and so we're gonna create it and",
    "start": "891360",
    "end": "899040"
  },
  {
    "text": "we're going to um time travel a bit because i don't want to wait four minutes",
    "start": "899040",
    "end": "904880"
  },
  {
    "text": "and so soon you're seeing you know 258 340 so we're about to finish creating",
    "start": "904880",
    "end": "911600"
  },
  {
    "text": "the pod and um we will see that",
    "start": "911600",
    "end": "916639"
  },
  {
    "text": "the the request is going to fail and that's because the cubit is going to hit that four minute",
    "start": "916639",
    "end": "922560"
  },
  {
    "text": "um that four minute timeout we're to get a context deadline exceeded error",
    "start": "922560",
    "end": "927600"
  },
  {
    "text": "so this is our first failure and this is expected because the cni plug-in took that long",
    "start": "927600",
    "end": "934800"
  },
  {
    "text": "and then what's going to happen is the qubit's going to attempt to recreate that so here we see this name is",
    "start": "934800",
    "end": "940959"
  },
  {
    "text": "reserved air and this propagates all through the system and this is basically the cubit uh you",
    "start": "940959",
    "end": "948000"
  },
  {
    "text": "know re-requesting and proud being like i'm in the middle of it calm down relax doing nothing to actually slow down the",
    "start": "948000",
    "end": "954160"
  },
  {
    "text": "cubelet so this uh air is gonna pop up a couple of times um",
    "start": "954160",
    "end": "959199"
  },
  {
    "text": "as we can see here we have a you know they're separated by like 15 seconds and",
    "start": "959199",
    "end": "966160"
  },
  {
    "text": "so this is a bad user um user experience uh eventually this",
    "start": "966160",
    "end": "972720"
  },
  {
    "text": "pod will attempt to be recreated and uh that could pass or as we know it will fail because four",
    "start": "972720",
    "end": "978880"
  },
  {
    "text": "minutes will have uh been exceeded again um so in this situation the cubelet and cryo would",
    "start": "978880",
    "end": "984320"
  },
  {
    "text": "never reconcile but we can do better so next up we have",
    "start": "984320",
    "end": "989600"
  },
  {
    "text": "an example with our new cryobinary and we're going to do the same thing we're going to create this top pod uh",
    "start": "989600",
    "end": "996320"
  },
  {
    "text": "with the new crowd binary and it's going to be running in the same way um with the cni plug-in taking four",
    "start": "996320",
    "end": "1003519"
  },
  {
    "text": "minutes much too long for the cubits request then uh let's fast forward to when the",
    "start": "1003519",
    "end": "1010079"
  },
  {
    "text": "timeout is about to happen so the timeout's about to happen we're 332 348",
    "start": "1010079",
    "end": "1015440"
  },
  {
    "text": "355. so here we have the expected context deadline exceeded which we can't do anything about i mean",
    "start": "1015440",
    "end": "1021920"
  },
  {
    "text": "it's the cni plug-in that's taking a long time cryo can't make it go any faster it's the same way about",
    "start": "1021920",
    "end": "1027120"
  },
  {
    "text": "uh you know the latency on the cloud provider's perspective but what kryol can do is",
    "start": "1027120",
    "end": "1034558"
  },
  {
    "text": "successfully save that resource and be able to move like allow the cubelet to move",
    "start": "1034559",
    "end": "1040079"
  },
  {
    "text": "forward so um what we're about to see is so it's kind of confusing here but we're",
    "start": "1040079",
    "end": "1046720"
  },
  {
    "text": "gonna we're gonna show the described pods so here we have the failed sandbox request 19 seconds",
    "start": "1046720",
    "end": "1054080"
  },
  {
    "text": "ago and then we see the pulling message what this means is in between that time",
    "start": "1054080",
    "end": "1059280"
  },
  {
    "text": "the creates on pod sandbox succeeded because the cubelet wants the pod",
    "start": "1059280",
    "end": "1064400"
  },
  {
    "text": "sandbox uh request succeeds the cuba then starts pulling the images for the uh containers themselves",
    "start": "1064400",
    "end": "1071280"
  },
  {
    "text": "so we see that we're pulling the alpine container it's created and it started so we have a running pod",
    "start": "1071280",
    "end": "1076559"
  },
  {
    "text": "um it only took you know 14 seconds a time time for cuba to re-request from cryo",
    "start": "1076559",
    "end": "1082960"
  },
  {
    "text": "and we can see this in the uh cryologs um that you know we had this deadline",
    "start": "1082960",
    "end": "1090000"
  },
  {
    "text": "exceeded issue and we had some name is reserved errors but eventually once the resource was",
    "start": "1090000",
    "end": "1097200"
  },
  {
    "text": "actually created crowd realizes that it's in its resource cache and it returns it and this means that we successfully",
    "start": "1097200",
    "end": "1105840"
  },
  {
    "text": "saved the work that we did before return the error and we only returned one error through the whole system so a",
    "start": "1105840",
    "end": "1113280"
  },
  {
    "text": "user will have seen that the context deadline exceeded but then soon thereafter the the request will have passed and",
    "start": "1113280",
    "end": "1120240"
  },
  {
    "text": "they'll be much happier so that's it for the demo um and all in",
    "start": "1120240",
    "end": "1126559"
  },
  {
    "text": "all we have a situation where cryo is able to better tune its behavior to the needs of",
    "start": "1126559",
    "end": "1132480"
  },
  {
    "text": "the cuba and this is uniquely helpful in this situation where we can mitigate the problems of heavy load due",
    "start": "1132480",
    "end": "1140799"
  },
  {
    "text": "that come and a lot of the pitfalls that resource creation falls into",
    "start": "1140799",
    "end": "1146720"
  },
  {
    "start": "1146000",
    "end": "1393000"
  },
  {
    "text": "all right so now we're going to talk about username spaces a lot of work has been done on enhancing",
    "start": "1146720",
    "end": "1153280"
  },
  {
    "text": "the username space support with kubernetes and cryo over the past few months the username space support is currently",
    "start": "1153280",
    "end": "1159760"
  },
  {
    "text": "an experimental phase as we continue to polish it up but it is something that you can try out and use",
    "start": "1159760",
    "end": "1165360"
  },
  {
    "text": "today there's also a lot of discussions going on in the upstream kubernetes enhancement plan for username spaces to",
    "start": "1165360",
    "end": "1171760"
  },
  {
    "text": "continue to drive this to be a fully supported feature in kubernetes so you can enable username spaces for a",
    "start": "1171760",
    "end": "1178960"
  },
  {
    "text": "certain runtime by adding the io.kubernetes.cryo the userness mode annotation",
    "start": "1178960",
    "end": "1185919"
  },
  {
    "text": "to the list of allowed annotations for that one time and the picture on the left here is an",
    "start": "1185919",
    "end": "1190960"
  },
  {
    "text": "example of such an entry in the cry.com you can also use runtime classes to",
    "start": "1190960",
    "end": "1197760"
  },
  {
    "text": "create a custom runtime with this annotation enabled so that if you want to use username",
    "start": "1197760",
    "end": "1203360"
  },
  {
    "text": "spaces on certain parts such as your billboards you can spec you can specify that one time class for that body ammo and while",
    "start": "1203360",
    "end": "1209440"
  },
  {
    "text": "your other workloads run without username spaces um and the picture on the right is an",
    "start": "1209440",
    "end": "1215679"
  },
  {
    "text": "example of a party ammo that has the usernamespace annotation there are certain options you can pass",
    "start": "1215679",
    "end": "1222000"
  },
  {
    "text": "to this annotation so one of them is the size which sets the size the range of uids the username",
    "start": "1222000",
    "end": "1228320"
  },
  {
    "text": "space you want to have the map to root option means that it would map it to the uidsu and your container if you want",
    "start": "1228320",
    "end": "1235360"
  },
  {
    "text": "to run your container without roots so with any other users such as 1000 you can use the keep id option here and",
    "start": "1235360",
    "end": "1241840"
  },
  {
    "text": "it will keep that uid and then some other options are you can specify the uid",
    "start": "1241840",
    "end": "1248000"
  },
  {
    "text": "map and the gid map range and this as an option over here as well and that",
    "start": "1248000",
    "end": "1254000"
  },
  {
    "text": "will be custom to your container so uh one more thing is that the kernel 5",
    "start": "1254000",
    "end": "1260720"
  },
  {
    "text": "has added many more features which will make username spaces better and we're working on",
    "start": "1260720",
    "end": "1265919"
  },
  {
    "text": "incorporating that into cryo as well so i've also put together",
    "start": "1265919",
    "end": "1271520"
  },
  {
    "text": "a quick demo showing how all of this works together so i have a local kubernetes cluster up",
    "start": "1271520",
    "end": "1277520"
  },
  {
    "text": "as you can see my node is up and ready and i have no pods running right now um",
    "start": "1277520",
    "end": "1283440"
  },
  {
    "text": "this is the party animal that i will be using to create my pod as you can see here i set the size to 65",
    "start": "1283440",
    "end": "1289679"
  },
  {
    "text": "536 and i want to keep the id as i'm going to be running as user 1000. to make usernamespaces",
    "start": "1289679",
    "end": "1296880"
  },
  {
    "text": "work i have also added the arrange for the containers user and my sub uid in sub gid so it will map",
    "start": "1296880",
    "end": "1303360"
  },
  {
    "text": "from 200 000 to whatever this large number is what this means is that uh on the host",
    "start": "1303360",
    "end": "1309919"
  },
  {
    "text": "it'll be two hundred thousand one in a container it will map it to uid zero and so forth so two hundred",
    "start": "1309919",
    "end": "1314960"
  },
  {
    "text": "thousand and one will be uid one two thousand and 0002 view id2 in the container",
    "start": "1314960",
    "end": "1321039"
  },
  {
    "text": "the next thing i'm going to show is the annotation that i have added for my run c runtime and my cry.conf as",
    "start": "1321200",
    "end": "1328799"
  },
  {
    "text": "run c is the runtime that i am using currently so as you can see over here i have set",
    "start": "1328799",
    "end": "1335200"
  },
  {
    "text": "the user ns mode um annotation for run c",
    "start": "1335200",
    "end": "1341440"
  },
  {
    "text": "all right so i have all my setup done and now i'm going to create the pod that i showed earlier",
    "start": "1341440",
    "end": "1348400"
  },
  {
    "text": "uh and we're just going to wait for it to come up it should take a few seconds okay it's",
    "start": "1348400",
    "end": "1355440"
  },
  {
    "text": "creating now and let's check again and awesome it's running now so now",
    "start": "1355440",
    "end": "1360640"
  },
  {
    "text": "um let's exact into the pod and let's check what id the pod is",
    "start": "1360640",
    "end": "1365840"
  },
  {
    "text": "running with it should be thousand as i specified in my 40 ammo and that that it is it's thousand",
    "start": "1365840",
    "end": "1371280"
  },
  {
    "text": "and now let's check uh what uid is running as on host so i'm just going to look for that process",
    "start": "1371280",
    "end": "1376400"
  },
  {
    "text": "and as we can see here it's running as uid 200 and 1000 because",
    "start": "1376400",
    "end": "1383440"
  },
  {
    "text": "as the mapping will start a thousand to zero and so forth so yeah my phone is running in username",
    "start": "1383440",
    "end": "1388480"
  },
  {
    "text": "spaces with more isolation and that's it for the username space part",
    "start": "1388480",
    "end": "1393760"
  },
  {
    "start": "1393000",
    "end": "1474000"
  },
  {
    "text": "all right i would like to talk about how we can record second profiles using cryo and kubernetes",
    "start": "1393760",
    "end": "1399840"
  },
  {
    "text": "the first thing i have to notice is that second profiles for kubernetes have to be available as json files on disk",
    "start": "1399840",
    "end": "1405520"
  },
  {
    "text": "so we need something like this we have to specify a default action we which is in our case throwing an",
    "start": "1405520",
    "end": "1412400"
  },
  {
    "text": "error we have to specify architectures which is in our case our local architecture for 64-bit",
    "start": "1412400",
    "end": "1418159"
  },
  {
    "text": "and we need a list of this calls which are allowed and in our little example here we are just allowed to clone this",
    "start": "1418159",
    "end": "1424000"
  },
  {
    "text": "call this profile has to be on every disk because if the scheduler schedules the workload",
    "start": "1424000",
    "end": "1430799"
  },
  {
    "text": "on a node then the cryo assumes that the profile is available there and then it passes it",
    "start": "1430799",
    "end": "1436400"
  },
  {
    "text": "down to the lower level ocr runtime and if it's not available then the scheduling or the creation of the workload will fail at",
    "start": "1436400",
    "end": "1442240"
  },
  {
    "text": "all this has a major drawback because different ocran temps have different",
    "start": "1442240",
    "end": "1447440"
  },
  {
    "text": "minimal syscalls in their footprint to actually be able to run a container so run c for example has this list of",
    "start": "1447440",
    "end": "1454400"
  },
  {
    "text": "syscalls and if we compare it to this list of ciscos for z run which is a complementary",
    "start": "1454400",
    "end": "1459600"
  },
  {
    "text": "runtime written in c then we can see that they actually differ so this is also not ensured by every",
    "start": "1459600",
    "end": "1466320"
  },
  {
    "text": "version of there so it could change for aversion release foreign c and it can also change for the new release for",
    "start": "1466320",
    "end": "1473039"
  },
  {
    "text": "c run this is a main drawback because if you now want to create second profiles we",
    "start": "1473039",
    "end": "1478320"
  },
  {
    "start": "1474000",
    "end": "1512000"
  },
  {
    "text": "have to require detailed knowledge about the underlying or zero in term used by cryo how it is how the cluster",
    "start": "1478320",
    "end": "1483760"
  },
  {
    "text": "is configured and which version we actually use so we also need necessarily the application",
    "start": "1483760",
    "end": "1489520"
  },
  {
    "text": "systems which may be executed in all code paths and if we miss a code path then it may be possible that the",
    "start": "1489520",
    "end": "1495360"
  },
  {
    "text": "workload gets terminated during runtime which is actually not what we want so what is the solution around this i",
    "start": "1495360",
    "end": "1502240"
  },
  {
    "text": "think a good solution around this would be to record second profiles and a test environment and then applic",
    "start": "1502240",
    "end": "1507600"
  },
  {
    "text": "apply it to the production workload and how this works is part of my demo for today so this",
    "start": "1507600",
    "end": "1515360"
  },
  {
    "start": "1512000",
    "end": "1617000"
  },
  {
    "text": "demo should show you how we can record those second profiles using cryo and to be able to run this demo we have",
    "start": "1515360",
    "end": "1522000"
  },
  {
    "text": "to use the latest version for cryo which is 121 and it may not been released yet um but",
    "start": "1522000",
    "end": "1529120"
  },
  {
    "text": "we can verify which cryo version we are running by just running chrysler version and here we can see that we use a",
    "start": "1529120",
    "end": "1535760"
  },
  {
    "text": "runtime version 121 a development version and that we actually use the container on the cryo which is important as well",
    "start": "1535760",
    "end": "1542960"
  },
  {
    "text": "so it is also necessary to use a project like the oci second bpf hook and it has",
    "start": "1542960",
    "end": "1549760"
  },
  {
    "text": "to be installed and configured on the system so this is a pre-start oci hook which",
    "start": "1549760",
    "end": "1555200"
  },
  {
    "text": "actually runs exactly before the container workload would start and it creates a different child process",
    "start": "1555200",
    "end": "1563120"
  },
  {
    "text": "which compiles a bpf module and then attaches a trace point to our running workload and then if the",
    "start": "1563120",
    "end": "1571120"
  },
  {
    "text": "application the hook is possible is able to record all our syscalls",
    "start": "1571120",
    "end": "1576240"
  },
  {
    "text": "into an intermediate structure and if the application terminates or if the pod",
    "start": "1576240",
    "end": "1581520"
  },
  {
    "text": "sket gets deleted then the hook will write the resulting second profile to disk",
    "start": "1581520",
    "end": "1587760"
  },
  {
    "text": "but this has a few drawbacks for example um the hook needs capsis admin so it's",
    "start": "1587760",
    "end": "1593039"
  },
  {
    "text": "highly privileged so it's only recommended to run it in the test environment and it also compiles c code on the fly",
    "start": "1593039",
    "end": "1599039"
  },
  {
    "text": "which means that we need to be linux headers available on the local system to actually run the hook but i can",
    "start": "1599039",
    "end": "1607120"
  },
  {
    "text": "just recommend you to give this hook a look because you can also run it with podman and play around with it",
    "start": "1607120",
    "end": "1613679"
  },
  {
    "text": "so now we have to look at our cryostatus configuration and we what we can see is that we",
    "start": "1613679",
    "end": "1620480"
  },
  {
    "start": "1617000",
    "end": "1704000"
  },
  {
    "text": "have the hooks path configured correctly to look at the oci ppf",
    "start": "1620480",
    "end": "1628720"
  },
  {
    "text": "second hook so the hooks directory is configured to point to that direction that the cryo is able to pick up that hook",
    "start": "1628720",
    "end": "1635600"
  },
  {
    "text": "and in our case we are using zeron as default runtime which means then we that we create that",
    "start": "1635600",
    "end": "1641440"
  },
  {
    "text": "we allow the annotation i o container stray syscall which is the annotation for the hook",
    "start": "1641440",
    "end": "1646720"
  },
  {
    "text": "to be executed by cryo so this is kind of a security mechanism and it will be enabled by default for",
    "start": "1646720",
    "end": "1652799"
  },
  {
    "text": "when 121 but for all configurations we have to ensure that this annotation is actually allowed",
    "start": "1652799",
    "end": "1659279"
  },
  {
    "text": "and the annotation key io container stress is called provides all information for cryo to",
    "start": "1659279",
    "end": "1664799"
  },
  {
    "text": "record a statcom profile we can now create a first workload via cube ctl run",
    "start": "1664799",
    "end": "1670480"
  },
  {
    "text": "what we do is that we create a simple part which never restarts and then we run mkdiatest but we also",
    "start": "1670480",
    "end": "1677520"
  },
  {
    "text": "pass the annotation i o container stray syscall and specify our output file to attempt",
    "start": "1677520",
    "end": "1683159"
  },
  {
    "text": "mypod.json let's give this a try if the container",
    "start": "1683159",
    "end": "1688240"
  },
  {
    "text": "is running and is terminating then it will be immediately get deleted by cubectl afterwards",
    "start": "1688240",
    "end": "1694960"
  },
  {
    "text": "so now the pot is get got deleted again and now cryo uses this of annotation as actually output file we",
    "start": "1694960",
    "end": "1702159"
  },
  {
    "text": "can have a look at the actual second profile our profile looks like this which is kind of interesting because it nearly",
    "start": "1702159",
    "end": "1708720"
  },
  {
    "start": "1704000",
    "end": "1750000"
  },
  {
    "text": "looks like the same in the same way as we specify the default cisco requires this minimal this",
    "start": "1708720",
    "end": "1715200"
  },
  {
    "text": "requires this calls for c run but we also include our mkdio this call",
    "start": "1715200",
    "end": "1720960"
  },
  {
    "text": "in our case and this would be actually already a",
    "start": "1720960",
    "end": "1726320"
  },
  {
    "text": "usable second profile but um usually we run multiple containers",
    "start": "1726320",
    "end": "1732000"
  },
  {
    "text": "within a pot and we cryo was also able to separate those containers and",
    "start": "1732000",
    "end": "1737840"
  },
  {
    "text": "record into different files so for that we made a custom modification to cryo that cryo has the",
    "start": "1737840",
    "end": "1744880"
  },
  {
    "text": "intelligence to split up the annotation based on the actual container workload",
    "start": "1744880",
    "end": "1751039"
  },
  {
    "start": "1750000",
    "end": "1833000"
  },
  {
    "text": "for example if we now run a pot like this we can specify multiple annotations um using the tray syscon annotation",
    "start": "1751039",
    "end": "1758320"
  },
  {
    "text": "referencing the container name so we now use the engine x and redis",
    "start": "1758320",
    "end": "1763440"
  },
  {
    "text": "containers which are down here and reference them by their name to specify different",
    "start": "1763440",
    "end": "1769760"
  },
  {
    "text": "output files otherwise they will overwrite each other if we specify the same output file",
    "start": "1769760",
    "end": "1775200"
  },
  {
    "text": "so this wouldn't make any sense at all but if we now run that part and uh wait",
    "start": "1775200",
    "end": "1782240"
  },
  {
    "text": "for to be ready that it's actually running then we can have a look at the hook is",
    "start": "1782240",
    "end": "1788559"
  },
  {
    "text": "currently running and attaches to the container pit so if we now look at the processes running on my local system",
    "start": "1788559",
    "end": "1794000"
  },
  {
    "text": "then crap for the oci is second ppf hook then we can see that we have two instances of the hook running and",
    "start": "1794000",
    "end": "1800000"
  },
  {
    "text": "recording into different files so yeah for sure we have to delete the",
    "start": "1800000",
    "end": "1805200"
  },
  {
    "text": "hook again to be able to see what is actually written in the into those files",
    "start": "1805200",
    "end": "1811200"
  },
  {
    "text": "now the pod got deleted and cryo uses the annotation value f again to look to",
    "start": "1811200",
    "end": "1818799"
  },
  {
    "text": "separate our profiles so if we now uh give our profiles a first quick look",
    "start": "1818799",
    "end": "1824559"
  },
  {
    "text": "then we can see okay they look different but do we actually see that they are",
    "start": "1824559",
    "end": "1829840"
  },
  {
    "text": "actually different at all we can lift them and yeah now we can see",
    "start": "1829840",
    "end": "1835440"
  },
  {
    "start": "1833000",
    "end": "1947000"
  },
  {
    "text": "those profiles have different z scores in it and they are not the same so this test works at all",
    "start": "1835440",
    "end": "1843840"
  },
  {
    "text": "all those profiles in kubernetes have to be part of the second route so which is usually the cubelet route",
    "start": "1844640",
    "end": "1850720"
  },
  {
    "text": "slash second and which can be different configurated into a different directory",
    "start": "1850720",
    "end": "1856399"
  },
  {
    "text": "but in our case i just have to copy our second profiles into wallet cube.com",
    "start": "1856399",
    "end": "1862000"
  },
  {
    "text": "and if we did this then we are able to use both profiles in our workload so if",
    "start": "1862000",
    "end": "1867200"
  },
  {
    "text": "we now use create a deployment workload for example which uses two replicas",
    "start": "1867200",
    "end": "1872240"
  },
  {
    "text": "then we can just use the new security context.com profile which got introduced in kubernetes 1.19",
    "start": "1872240",
    "end": "1879760"
  },
  {
    "text": "guru reference our localhost profile and now we can use the nginx profile for the nginx container and the writers profile",
    "start": "1879760",
    "end": "1885519"
  },
  {
    "text": "for the radius container and if we now create this deployment",
    "start": "1885519",
    "end": "1891200"
  },
  {
    "text": "and wait for it to be available in my local cluster setup then we can look at our pots which got",
    "start": "1891200",
    "end": "1899279"
  },
  {
    "text": "created and if we just choose one of those spots then we can see that we also have the container",
    "start": "1899279",
    "end": "1904640"
  },
  {
    "text": "annotation for the second profile now available for both so we have the nginx annotation for the nginx",
    "start": "1904640",
    "end": "1911760"
  },
  {
    "text": "profile and the redis annotation for the radius profile and if we want to check how this",
    "start": "1911760",
    "end": "1918720"
  },
  {
    "text": "actually looks like and on a low level oci runtime then we can use crystal inspect and in",
    "start": "1918720",
    "end": "1925360"
  },
  {
    "text": "our case we just look for our nginx profile engine x workload",
    "start": "1925360",
    "end": "1930559"
  },
  {
    "text": "and one of those containers we just choose one because we have two running then we can see that we have a second",
    "start": "1930559",
    "end": "1937760"
  },
  {
    "text": "profile applied which actually matches the same in the same way as we have recorded it and that's my",
    "start": "1937760",
    "end": "1944559"
  },
  {
    "text": "demo for recordings.com profiles of cryo one more thing i'd like to mention is",
    "start": "1944559",
    "end": "1949760"
  },
  {
    "start": "1947000",
    "end": "1982000"
  },
  {
    "text": "that cryo targets to make kubernetes more secure by default so our overall idea was to make the runtime default profile",
    "start": "1949760",
    "end": "1956240"
  },
  {
    "text": "for second the default profile for all workloads right now all workloads run unconfined",
    "start": "1956240",
    "end": "1961919"
  },
  {
    "text": "by default and we created a cap for that so if you're interested in maybe providing some feedback to the",
    "start": "1961919",
    "end": "1968720"
  },
  {
    "text": "kubernetes enhancement proposal then feel free to ping us on slack or directly at a pull request",
    "start": "1968720",
    "end": "1974960"
  },
  {
    "text": "okay and that's it for our talk on cryo still love kubernetes uh we'll take some questions now thank",
    "start": "1974960",
    "end": "1980880"
  },
  {
    "text": "you all",
    "start": "1980880",
    "end": "1984240"
  }
]