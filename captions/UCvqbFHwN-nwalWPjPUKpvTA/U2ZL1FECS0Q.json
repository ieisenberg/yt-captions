[
  {
    "text": "all right I hear we're good to start um hello everyone Thanks for Lending us",
    "start": "280",
    "end": "5879"
  },
  {
    "text": "your time and joining us uh on the stock on zonal outage operational stories uh",
    "start": "5879",
    "end": "11200"
  },
  {
    "text": "my name is sham Junta and I'm joined by Joi mahapatra both of us uh are software Engineers from Amazon eks at AWS and",
    "start": "11200",
    "end": "19680"
  },
  {
    "text": "come from a team that ensures reliability of clusters during zonal outages of or partial failures of",
    "start": "19680",
    "end": "26000"
  },
  {
    "text": "various kinds uh together our team has successfully mitigated dozens of such",
    "start": "26000",
    "end": "31960"
  },
  {
    "text": "events learned some valuable lessons along the way um and used those to engineer highly available and reliable",
    "start": "31960",
    "end": "38239"
  },
  {
    "text": "kubernetes clusters and that's exactly what we're going to talk today our Focus",
    "start": "38239",
    "end": "43520"
  },
  {
    "text": "will be on the kubernetes control plane uh but some of the learnings also apply more broadly to um user applications or",
    "start": "43520",
    "end": "51399"
  },
  {
    "text": "or components within the data plane of the kubernetes",
    "start": "51399",
    "end": "56079"
  },
  {
    "text": "cluster um so I'd like to start with uh this model it's called a Swiss Cheese",
    "start": "56840",
    "end": "63400"
  },
  {
    "text": "model um I wish I could say French cheese but I guess swiss cheese just has more holes uh in it so um this is a",
    "start": "63400",
    "end": "72759"
  },
  {
    "text": "model that's used to analyze and manage risks in large complex systems with many",
    "start": "72759",
    "end": "77880"
  },
  {
    "text": "moving Parts um it's an effective mental model that's been used time and again in",
    "start": "77880",
    "end": "83560"
  },
  {
    "text": "various Industries like Aviation Healthcare nuclear power plants and even",
    "start": "83560",
    "end": "88799"
  },
  {
    "text": "distributed Software System systems essentially each hole in the cheese represents a weakness or a",
    "start": "88799",
    "end": "96200"
  },
  {
    "text": "dormant failure uh that could manifest itself in the presence of an external Trigger or a hazard uh for example it",
    "start": "96200",
    "end": "104399"
  },
  {
    "text": "could be a bug in the flights uh Manu S A system that gets triggered at a",
    "start": "104399",
    "end": "109759"
  },
  {
    "text": "specific air speed or altitude um if you're familiar with the whole conquered Fiasco you probably know what I'm",
    "start": "109759",
    "end": "116360"
  },
  {
    "text": "talking about uh there we go a French reference um okay",
    "start": "116360",
    "end": "123520"
  },
  {
    "text": "so diving a little deeper as software Engineers we often",
    "start": "123520",
    "end": "128640"
  },
  {
    "text": "see ourselves striving to build systems or Services uh that operate reliably",
    "start": "128640",
    "end": "134800"
  },
  {
    "text": "always but the reality is distributed systems are practically impossible to be",
    "start": "134800",
    "end": "140000"
  },
  {
    "text": "made Flawless um your block of cheese will have holes in it and in the fullness of time and scale uh there will",
    "start": "140000",
    "end": "147000"
  },
  {
    "text": "be different kinds of failures that will happen individual components um",
    "start": "147000",
    "end": "152160"
  },
  {
    "text": "microservices and even their interactions uh for instance Hardware that your kubernetes cluster runs on",
    "start": "152160",
    "end": "158160"
  },
  {
    "text": "will eventually fail and this uh includes compute uh your CPUs gpus and",
    "start": "158160",
    "end": "164599"
  },
  {
    "text": "even um volumes and disk storage right even the best health checks sometimes",
    "start": "164599",
    "end": "170000"
  },
  {
    "text": "they don't detect certain failures um nodes sometimes get partitioned or network um uh in viewed ways sometimes",
    "start": "170000",
    "end": "177879"
  },
  {
    "text": "it's partitioned by Direction Ally sometimes only in One Direction so the best bet is actually to try as much as",
    "start": "177879",
    "end": "185440"
  },
  {
    "text": "possible to decouple these failure modes uh make them",
    "start": "185440",
    "end": "190879"
  },
  {
    "text": "uncorrelated and thereby reducing the overall single points of failures that your system um has uh the idea is that",
    "start": "190879",
    "end": "200239"
  },
  {
    "text": "by by reducing such uh single points of failures you're increasing the number of",
    "start": "200239",
    "end": "206440"
  },
  {
    "text": "things that need to go down in your system before it it it breaks down uh",
    "start": "206440",
    "end": "213040"
  },
  {
    "text": "which basically steeply even exponentially reduces the the probability of that happening and that's",
    "start": "213040",
    "end": "219200"
  },
  {
    "text": "exactly what this swiss cheese model view is showing here um there are few ways to interpret it the most common one",
    "start": "219200",
    "end": "225879"
  },
  {
    "text": "is each slice of the cheese uh can be thought of as different components or",
    "start": "225879",
    "end": "231080"
  },
  {
    "text": "layers uh within your uh layers of defenses in your system that uh have",
    "start": "231080",
    "end": "236120"
  },
  {
    "text": "been built to protect it from let's say downtime and each layer of course has",
    "start": "236120",
    "end": "241319"
  },
  {
    "text": "some sort of holes in it at different places and of different magnitudes and the red lines that go through these are",
    "start": "241319",
    "end": "248720"
  },
  {
    "text": "hazards which uh the system is facing and the one red line that actually made it all the way till the end that was",
    "start": "248720",
    "end": "255319"
  },
  {
    "text": "able to successfully RI Havoc because it had something good going on for it that is uh all the holes in its path have",
    "start": "255319",
    "end": "263440"
  },
  {
    "text": "lined up and making it a single point of failure for the system so avoiding these",
    "start": "263440",
    "end": "269880"
  },
  {
    "text": "sort of uh failures needs deeper thought and rigorous engineering uh we're going to talk about that a little bit going",
    "start": "269880",
    "end": "276440"
  },
  {
    "text": "forward but first I also want to present another sorry uh yeah another concept of",
    "start": "276440",
    "end": "282600"
  },
  {
    "text": "redundancy so this is a foundational mechanism that a lot of distributed systems leverage to achieve High",
    "start": "282600",
    "end": "289479"
  },
  {
    "text": "availability um redundancy is it's essentially the concept of running multiple replicas of your software and",
    "start": "289479",
    "end": "296680"
  },
  {
    "text": "Hardware so that if one or more replicas fail the others can continue to keep the lights on similarly in databases land",
    "start": "296680",
    "end": "303639"
  },
  {
    "text": "high durability is achieved by uh maintaining redundant copies of the data across multiple hosts um redundancy",
    "start": "303639",
    "end": "311479"
  },
  {
    "text": "helps reduce the probability of the system uh failing but in the face of",
    "start": "311479",
    "end": "318479"
  },
  {
    "text": "uncorrelated independent failures not correlated failures so let's say there's",
    "start": "318479",
    "end": "323560"
  },
  {
    "text": "a bug in uh the component that you've replicated in a way that let's say a bad",
    "start": "323560",
    "end": "329120"
  },
  {
    "text": "right to a database has crashed all three replicas then it is a correlated",
    "start": "329120",
    "end": "334880"
  },
  {
    "text": "failure and redundancy in that case doesn't help actually um So to avoid",
    "start": "334880",
    "end": "340280"
  },
  {
    "text": "these sort of all for one one for all situation uh let's talk more about how can we UNC correlate these",
    "start": "340280",
    "end": "347240"
  },
  {
    "text": "failures um and I want to introduce at this point",
    "start": "347240",
    "end": "352680"
  },
  {
    "text": "the concept of a failure domain so uh if you're familiar commercial data centers",
    "start": "352680",
    "end": "358280"
  },
  {
    "text": "layout the hardware and the on the connecting network infrastructure over a specific topology and this topology is",
    "start": "358280",
    "end": "365800"
  },
  {
    "text": "typically hierarchical and a simple version of that is what's shown here it's basically a tree uh where servers",
    "start": "365800",
    "end": "373240"
  },
  {
    "text": "and uh uh the other Hardware like uh discs are arranged into rows or racks",
    "start": "373240",
    "end": "379720"
  },
  {
    "text": "and which kind of roll up into maybe rooms and that rolls up into your entire uh Data Center and each rack is",
    "start": "379720",
    "end": "386560"
  },
  {
    "text": "typically connected with uh L2 switches uh it's commonly called Leaf level switches",
    "start": "386560",
    "end": "392520"
  },
  {
    "text": "and they in turn are connected to uh larger groups of servers through L2 or L3 spine level uh switches and as you",
    "start": "392520",
    "end": "400440"
  },
  {
    "text": "can imagine there are failure points at each level for instance uh single server in a given rack uh could become",
    "start": "400440",
    "end": "407840"
  },
  {
    "text": "inoperable due to a hardware malfunction or a broken ethernet cable or the switch could take down the whole rack right um",
    "start": "407840",
    "end": "415879"
  },
  {
    "text": "and similarly a power loss could take down a whole room of servers so there are layers of failure domains here um",
    "start": "415879",
    "end": "423280"
  },
  {
    "text": "then which one do we actually use for redundancy",
    "start": "423280",
    "end": "429000"
  },
  {
    "text": "um as it turns out the most common practice uh especially with uh some of",
    "start": "429000",
    "end": "435800"
  },
  {
    "text": "the larger Cloud providers or data centers is is to use uh a single data",
    "start": "435800",
    "end": "442240"
  },
  {
    "text": "center as the unit of redundancy it's essentially a physical building or a bunch of collocated buildings uh housing",
    "start": "442240",
    "end": "450599"
  },
  {
    "text": "the hardware and other supporting infrastructure like power supply cooling systems uh Disaster Recovery",
    "start": "450599",
    "end": "457240"
  },
  {
    "text": "mechanisms and this it's typically called a zone or an availability Zone um",
    "start": "457240",
    "end": "463720"
  },
  {
    "text": "now a region is essentially a collection a geographical area uh a cloud provider",
    "start": "463720",
    "end": "470159"
  },
  {
    "text": "May operate in uh which is a collection of multiple of these uh data centers or",
    "start": "470159",
    "end": "475319"
  },
  {
    "text": "zones uh they're often placed in close proximity to allow for low latency",
    "start": "475319",
    "end": "481240"
  },
  {
    "text": "communication between between those a is typically connected physically um uh",
    "start": "481240",
    "end": "486479"
  },
  {
    "text": "through massive backbone cables and uh while they're kept close enough",
    "start": "486479",
    "end": "493159"
  },
  {
    "text": "uh in terms of proximity they um they also need to be kept far enough so that",
    "start": "493159",
    "end": "499560"
  },
  {
    "text": "the chances of being affected uh multiple of them being affected at the same time due to the same Hazard or a",
    "start": "499560",
    "end": "506520"
  },
  {
    "text": "force of nature like um like a flood or a power loss should be low so this this",
    "start": "506520",
    "end": "513080"
  },
  {
    "text": "idea essentially the idea is that when a zone is down uh the rest of the region continues to operate",
    "start": "513080",
    "end": "520360"
  },
  {
    "text": "smoothly all right so with that in mind with an understanding of redundancy and",
    "start": "520360",
    "end": "527360"
  },
  {
    "text": "failure domains let's apply it to the kubernetes control plan uh this diagram may be a bit oversimplified but uh it",
    "start": "527360",
    "end": "535399"
  },
  {
    "text": "shows a typical highly available architecture where all the key uh",
    "start": "535399",
    "end": "541480"
  },
  {
    "text": "control plane components like hcd API server and the core kubernetes controllers they run on a set of uh",
    "start": "541480",
    "end": "548279"
  },
  {
    "text": "three instances that are spread across three different AES and",
    "start": "548279",
    "end": "553320"
  },
  {
    "text": "um even the load balancer that actually takes north south API traffic from clients is replicated across the a",
    "start": "553320",
    "end": "560720"
  },
  {
    "text": "similarly uh the in cluster uh API traffic that comes from clients via the",
    "start": "560720",
    "end": "566920"
  },
  {
    "text": "kubernetes uh service IP that is also backed by APS seror endpoints from",
    "start": "566920",
    "end": "573399"
  },
  {
    "text": "multiple zones um now let's go back to the earlier discussion around single",
    "start": "573399",
    "end": "578760"
  },
  {
    "text": "points of failures um so with redundancy we might think we are at a better place",
    "start": "578760",
    "end": "584880"
  },
  {
    "text": "um especially uh given these these sort of components have a failover um",
    "start": "584880",
    "end": "590839"
  },
  {
    "text": "mechanism inbuilt into them like uh for instance ETD it's a quorum and leader",
    "start": "590839",
    "end": "596399"
  },
  {
    "text": "based system right if you're familiar with it it needs two of three replicas to be running at any given time to be",
    "start": "596399",
    "end": "602160"
  },
  {
    "text": "operational so you can lose one replica um and among the other two replicas that are still available which form the",
    "start": "602160",
    "end": "608040"
  },
  {
    "text": "Quorum you need to have one of them as the leader uh an active leader at all times um similarly controllers in",
    "start": "608040",
    "end": "615079"
  },
  {
    "text": "cubanet is uh work based on a lease based leadership concept uh where if one replica is unhealthy ideally the other",
    "start": "615079",
    "end": "622560"
  },
  {
    "text": "uh that one should fail health checks and um another replica should just take over similarly with API",
    "start": "622560",
    "end": "630000"
  },
  {
    "text": "uh let's say a single API server is unhealthy the uh the load balancers it",
    "start": "630000",
    "end": "635360"
  },
  {
    "text": "should stop receiving traffic from the load balancer uh typically because of the uh ready Z live Z heal checks that",
    "start": "635360",
    "end": "642079"
  },
  {
    "text": "are configured on the load bancer so sounds simple right it feels like we kind of have what we want with terms of",
    "start": "642079",
    "end": "648519"
  },
  {
    "text": "uh in terms of redundancy and high availability so if az1 completely goes",
    "start": "648519",
    "end": "653959"
  },
  {
    "text": "down say uh we have all these mechanisms that should allow us to smoothly fail over to um az2 and",
    "start": "653959",
    "end": "661839"
  },
  {
    "text": "az3 except zonal failures are not always",
    "start": "661839",
    "end": "667160"
  },
  {
    "text": "hard failures uh where everything in that zone just comes down to a grinding halt",
    "start": "667160",
    "end": "673200"
  },
  {
    "text": "with a failure rate of 100% that's just not uh the reality all the time sure",
    "start": "673200",
    "end": "678839"
  },
  {
    "text": "there are failures like fiber cuts and like I don't know hurricanes and complete power laws that leave a Zone",
    "start": "678839",
    "end": "684639"
  },
  {
    "text": "completely unreachable but zonal failures can actually vir",
    "start": "684639",
    "end": "690680"
  },
  {
    "text": "take unlimited forms in terms of size and shape and most of the ones we've",
    "start": "690680",
    "end": "695839"
  },
  {
    "text": "actually seen uh operating kubernetes clusters are gray failures um so like",
    "start": "695839",
    "end": "702760"
  },
  {
    "text": "power or thermal issues that affect individual rooms uh in a zone or networking problems between zones that",
    "start": "702760",
    "end": "710399"
  },
  {
    "text": "uh cause some sort of elevated latencies or spiky uh throughput behavior and so",
    "start": "710399",
    "end": "716200"
  },
  {
    "text": "on um also zonal failures aren't always just caused due to lower level physical",
    "start": "716200",
    "end": "722000"
  },
  {
    "text": "stuff but it can also be due to uh if you're familiar with zonal infrastructure services like block",
    "start": "722000",
    "end": "728120"
  },
  {
    "text": "storage or L4 L7 load balancers these are services that are designed zonally",
    "start": "728120",
    "end": "733600"
  },
  {
    "text": "and the services that actually operate uh these components they sometimes also",
    "start": "733600",
    "end": "739480"
  },
  {
    "text": "deploy software in a zonal fashion uh just to reduce the blast radius of bad",
    "start": "739480",
    "end": "744720"
  },
  {
    "text": "bad uh software bugs in a region uh which means that if a service that you",
    "start": "744720",
    "end": "750199"
  },
  {
    "text": "depend on has uh done a bad deployment in the zone that could cause an outage",
    "start": "750199",
    "end": "755320"
  },
  {
    "text": "um or some sort of impact for you um so yeah I think the um the bottom line here",
    "start": "755320",
    "end": "762480"
  },
  {
    "text": "really is zones May completely fail they may totally become unreachable or",
    "start": "762480",
    "end": "768399"
  },
  {
    "text": "partially u in terms of slow requests occasional timeouts um Etc and basically",
    "start": "768399",
    "end": "774199"
  },
  {
    "text": "everywhere in between um and to be able to respond to such achievements as",
    "start": "774199",
    "end": "780240"
  },
  {
    "text": "cluster operators We Believe at Amazon eeks and more generally within AWS that",
    "start": "780240",
    "end": "786240"
  },
  {
    "text": "we need to be prepared with um deterministic mechanisms to respond to",
    "start": "786240",
    "end": "792440"
  },
  {
    "text": "such non-deterministic events um which means despite the nuances of a specific",
    "start": "792440",
    "end": "798720"
  },
  {
    "text": "failure mode um or a potentially limited understanding of the failure modes emits",
    "start": "798720",
    "end": "805240"
  },
  {
    "text": "the event while there's already a burning fire uh we still want to to be able to guarantee uh very clear positive",
    "start": "805240",
    "end": "812959"
  },
  {
    "text": "outcome which uh in in the case of the cumus control plan means the API has",
    "start": "812959",
    "end": "818800"
  },
  {
    "text": "continued availability it's uh and there no uh by continued availability it's not",
    "start": "818800",
    "end": "824600"
  },
  {
    "text": "just up time or Hy it's things like you don't see uh API errors you don't see",
    "start": "824600",
    "end": "830279"
  },
  {
    "text": "high latencies and stuff uh that and also for all the various controller",
    "start": "830279",
    "end": "835560"
  },
  {
    "text": "workflows uh they should be able to operate without um any increase in their downtime so essentially we want to be",
    "start": "835560",
    "end": "842560"
  },
  {
    "text": "prepared uh for these sort of non-deterministic failures with a strategy where we can without even",
    "start": "842560",
    "end": "849480"
  },
  {
    "text": "sometimes fully understanding the event safely way away from a bad Zone uh into",
    "start": "849480",
    "end": "854880"
  },
  {
    "text": "a healthy zones uh with that I'll pass on the mic to uh JY to talk about the",
    "start": "854880",
    "end": "860519"
  },
  {
    "text": "fun part around the case studies we've seen thank you sham hi everyone uh I'll",
    "start": "860519",
    "end": "868480"
  },
  {
    "text": "walk us through uh some interesting use cases and case studies that we have seen while operating this uh we learned that",
    "start": "868480",
    "end": "875279"
  },
  {
    "text": "zonal failures are not deterministic and failure modes depend heavily on the nature of co-ed failures when co-",
    "start": "875279",
    "end": "882720"
  },
  {
    "text": "failures happen in a single zone kuber's components fail very differently based on their retri strategy circuit breaking",
    "start": "882720",
    "end": "890480"
  },
  {
    "text": "Health checking and general failure any other failure handling technique uh the failure modes that",
    "start": "890480",
    "end": "896720"
  },
  {
    "text": "we'll talk about today are for API server hcd controller manager Cloud controll manager and these are just",
    "start": "896720",
    "end": "903759"
  },
  {
    "text": "different components APS over stateless hcd has chorum the control managers work um based on leadership where even if",
    "start": "903759",
    "end": "911000"
  },
  {
    "text": "there are more than one replica one is doing the work really and that makes the",
    "start": "911000",
    "end": "916519"
  },
  {
    "text": "system very complex uh the components itself do not have a notion of Zone",
    "start": "916519",
    "end": "922560"
  },
  {
    "text": "built into them it is up to the platform administrators who is putting the",
    "start": "922560",
    "end": "927600"
  },
  {
    "text": "components to work to understand the nuances of uh zones zonal failures and",
    "start": "927600",
    "end": "932839"
  },
  {
    "text": "how it may impact the components uh these depend on the administrator to be able to pull a card",
    "start": "932839",
    "end": "940440"
  },
  {
    "text": "to say I know something is happening in a particular Zone can we not use this and remove the chance of corelated",
    "start": "940440",
    "end": "948160"
  },
  {
    "text": "failures let's look at the picture and see what's happening here so in this diagram there are three zones and in",
    "start": "948560",
    "end": "955720"
  },
  {
    "text": "this diagram Zone one is partition this partition is caused by a slow volume",
    "start": "955720",
    "end": "962319"
  },
  {
    "text": "attached to the HD node again here the network is not fully gone the instance or the host are not fully G gone just",
    "start": "962319",
    "end": "969360"
  },
  {
    "text": "that the volumes attached to the host are responding slowly when that happens",
    "start": "969360",
    "end": "975279"
  },
  {
    "text": "the general latency of the system reduces sorry increases and any calls or",
    "start": "975279",
    "end": "982160"
  },
  {
    "text": "request to that particular HD node in zone one has a high latency now HD has",
    "start": "982160",
    "end": "988199"
  },
  {
    "text": "peer heal checks other HD nodes are H checking on this HD node in zone one to",
    "start": "988199",
    "end": "994399"
  },
  {
    "text": "say here how are you and it is just taking a long time to respond because of the latency eventually the latency may",
    "start": "994399",
    "end": "1001000"
  },
  {
    "text": "increase so much that the heal checks fail when the H checks fail this HD node in zone one loses Quorum it thinks it is",
    "start": "1001000",
    "end": "1009399"
  },
  {
    "text": "the only node in the system now coming back to the API server part we use client side load balancing",
    "start": "1009399",
    "end": "1016319"
  },
  {
    "text": "on API servers now the API servers know about all the three SD hosts when they",
    "start": "1016319",
    "end": "1021440"
  },
  {
    "text": "started and uh when a request comes from comes for a particular resource to the API server it creates one connection per",
    "start": "1021440",
    "end": "1028880"
  },
  {
    "text": "resource called a watch uh these connections are load balanced and these",
    "start": "1028880",
    "end": "1035240"
  },
  {
    "text": "are load balance across the zones so an API server here in this case Zone 2 which is healthy really has a connection",
    "start": "1035240",
    "end": "1041959"
  },
  {
    "text": "to an SD host in zone one this means that a problem in zone one is already",
    "start": "1041959",
    "end": "1048160"
  },
  {
    "text": "leaking out in Zone 2 it causes non- determinism in the system in this case the hcd is not",
    "start": "1048160",
    "end": "1056120"
  },
  {
    "text": "terminating the watches that it already has any new traffic to that HD node will",
    "start": "1056120",
    "end": "1062880"
  },
  {
    "text": "not go through because reads writes and watches they need Quorum while creating the connection but once the connection",
    "start": "1062880",
    "end": "1069160"
  },
  {
    "text": "is created it does not terminate them when the leadership loss happens in this",
    "start": "1069160",
    "end": "1074720"
  },
  {
    "text": "situation because the connection never terminated API server cach got still it",
    "start": "1074720",
    "end": "1080000"
  },
  {
    "text": "never got an update even though other HD noes are progressing in this case again we had a",
    "start": "1080000",
    "end": "1086440"
  },
  {
    "text": "leader uh in Cube control manager also in zone 2 because",
    "start": "1086440",
    "end": "1093200"
  },
  {
    "text": "uh KCM is watching an API server to give it a node or whatever resource it what",
    "start": "1093200",
    "end": "1099200"
  },
  {
    "text": "looking for it's not getting it in the meantime let's say node controller node controller has a problem it stops",
    "start": "1099200",
    "end": "1105880"
  },
  {
    "text": "working and any other new nodes joining the club at this time is just not joining a cluster or doing tains and",
    "start": "1105880",
    "end": "1113559"
  },
  {
    "text": "tolerations nothing is working um so what do we do with it we",
    "start": "1113559",
    "end": "1119840"
  },
  {
    "text": "uh but let's talk about why did this happen this happened due to a core related failure in our system we have",
    "start": "1119840",
    "end": "1126080"
  },
  {
    "text": "reconcilers that look at health of the HD nodes and terminate them when it",
    "start": "1126080",
    "end": "1132480"
  },
  {
    "text": "thinks that this node is not healthy let's terminate that and let a new instance comes up uh and reconcile",
    "start": "1132480",
    "end": "1137960"
  },
  {
    "text": "itself but this reconciler itself also was having a zonal failure uh this",
    "start": "1137960",
    "end": "1144760"
  },
  {
    "text": "correlation caused the reconciler not to be able to terminate the host in zone One D host so the fix was simple in our",
    "start": "1144760",
    "end": "1153520"
  },
  {
    "text": "case uh because we uh handled the inra we made sure that the reconciler also",
    "start": "1153520",
    "end": "1159799"
  },
  {
    "text": "terminated the LCD process first followed by the host termination now even if the host termination does not go",
    "start": "1159799",
    "end": "1166679"
  },
  {
    "text": "through fine because of its retra or it's just taking a lot of time because",
    "start": "1166679",
    "end": "1172080"
  },
  {
    "text": "the SD process is gone the connections are severed and any new uh connections are going to the healthy",
    "start": "1172080",
    "end": "1178440"
  },
  {
    "text": "zones uh what could we do next uh we could go to the hcd uh node and make",
    "start": "1178440",
    "end": "1186240"
  },
  {
    "text": "changes where the server always terminates connections when the",
    "start": "1186240",
    "end": "1193080"
  },
  {
    "text": "leadership is lost we are also thinking that we could do more client side stuff",
    "start": "1193080",
    "end": "1199480"
  },
  {
    "text": "with grpc um Discovery service grpc has this concept of envoy Discovery service",
    "start": "1199480",
    "end": "1205280"
  },
  {
    "text": "we could use that to say if certain host is experiencing higher latency or higher",
    "start": "1205280",
    "end": "1210720"
  },
  {
    "text": "error rates don't use that for load balancing uh we we could use grpc heal",
    "start": "1210720",
    "end": "1216039"
  },
  {
    "text": "checks to influence the traffic routing so thatp server does not use the CD um",
    "start": "1216039",
    "end": "1221960"
  },
  {
    "text": "in zone one in this case so a lot of uh issue links are are here we are working on them so we'll probably arrive at",
    "start": "1221960",
    "end": "1231600"
  },
  {
    "text": "something let's look at scenario two in this picture again Zone one is partitioned but it's not fully",
    "start": "1231600",
    "end": "1237559"
  },
  {
    "text": "partitioned just that the API server is experiencing higher than expected latency the users now are not having a",
    "start": "1237559",
    "end": "1244919"
  },
  {
    "text": "consistent experience sometimes the requests take are quick but sometimes",
    "start": "1244919",
    "end": "1250440"
  },
  {
    "text": "they take long and in this case uh this particular APS in zone one uh any",
    "start": "1250440",
    "end": "1257039"
  },
  {
    "text": "traffic coming to it through the load by balancer or through the cluster IP is experiencing a very um heavy latency now",
    "start": "1257039",
    "end": "1265159"
  },
  {
    "text": "to make this uh experience consistent for users we employ a mechanism called",
    "start": "1265159",
    "end": "1272440"
  },
  {
    "text": "way away we deal with the load balance in a",
    "start": "1272440",
    "end": "1277480"
  },
  {
    "text": "particular way and the cluster IP in a different way for load balancer we use an uh concept called application",
    "start": "1277480",
    "end": "1282720"
  },
  {
    "text": "recovery controller it's a public API and for all clusters in uh in a region we go and tell the load balancer that",
    "start": "1282720",
    "end": "1288760"
  },
  {
    "text": "hey don't advertise this particular IP in zone one due to that any new connections coming to the load balancer",
    "start": "1288760",
    "end": "1295559"
  },
  {
    "text": "does not use Zone one anymore until we ask it again back to the cluster IP is",
    "start": "1295559",
    "end": "1301760"
  },
  {
    "text": "tricky because uh kubernetes handles uh this part the API server has the advertised",
    "start": "1301760",
    "end": "1309400"
  },
  {
    "text": "address and an endpoint reconciler the endpoint reconciler takes the advertse address and makes U endpoint objects of",
    "start": "1309400",
    "end": "1316559"
  },
  {
    "text": "kubernetes and they don't fail heal checks even if API server is unhealthy",
    "start": "1316559",
    "end": "1321720"
  },
  {
    "text": "they are like DNS Q proy uses the cluster IP to Round Rob in load balance",
    "start": "1321720",
    "end": "1327080"
  },
  {
    "text": "between the API servers based on these IPS we made a change say that uh if we",
    "start": "1327080",
    "end": "1333320"
  },
  {
    "text": "tell through the fleet that in zone is experiencing a problem then the aps Ser",
    "start": "1333320",
    "end": "1338880"
  },
  {
    "text": "is now zonally aware we made it so and with that the endpoint reconciler stops",
    "start": "1338880",
    "end": "1344000"
  },
  {
    "text": "advertising the IP in zone one with that any PS or any part of the system which",
    "start": "1344000",
    "end": "1349200"
  },
  {
    "text": "is depending on customer IP or endpoints they directly go to the other API servers they don't go to Zone one",
    "start": "1349200",
    "end": "1355600"
  },
  {
    "text": "anymore now an important Nuance to that is the concept of over",
    "start": "1355600",
    "end": "1362360"
  },
  {
    "text": "provisioning let's say we take one APS out that means we are reducing the APF",
    "start": "1362360",
    "end": "1369080"
  },
  {
    "text": "quas of the system by proportion by certain proportion right and most times",
    "start": "1369080",
    "end": "1375400"
  },
  {
    "text": "this just works out uh because we provision the API server more than what",
    "start": "1375400",
    "end": "1381720"
  },
  {
    "text": "it would need so we look at metrics throttling metrics and we have certain uh laddering so uh with certain requests",
    "start": "1381720",
    "end": "1389000"
  },
  {
    "text": "we set certain quotas but when this kind of zonal failures happen most probably customers",
    "start": "1389000",
    "end": "1397080"
  },
  {
    "text": "other customers are also seeing that which means worker notes are shifting traffic ports are moving away and",
    "start": "1397080",
    "end": "1402559"
  },
  {
    "text": "evicting so it pounds the aps over with way many requests at this time so we",
    "start": "1402559",
    "end": "1409320"
  },
  {
    "text": "have also have a system where we track the throttling uh parameters or metrics",
    "start": "1409320",
    "end": "1414520"
  },
  {
    "text": "in the API server and if we see that the throttling is occurring way more we go",
    "start": "1414520",
    "end": "1420320"
  },
  {
    "text": "and scale it out uh we create more replica of a server to withstand the",
    "start": "1420320",
    "end": "1425760"
  },
  {
    "text": "problem so that users don't experience a",
    "start": "1425760",
    "end": "1430120"
  },
  {
    "text": "problem let's look at the third one in this case we have a reconciler",
    "start": "1431720",
    "end": "1438159"
  },
  {
    "text": "that does a hell check throughout the fleet and the hell check is if x number of",
    "start": "1438159",
    "end": "1445799"
  },
  {
    "text": "Hell checks fail in y amount of time terminate an instance maybe something is wrong with the instance and let the res",
    "start": "1445799",
    "end": "1450919"
  },
  {
    "text": "system reconcile back in this case when zonal failures happen let's say the health Checker is",
    "start": "1450919",
    "end": "1457799"
  },
  {
    "text": "fine the API server is also just doing fine but the network stack in between is",
    "start": "1457799",
    "end": "1462840"
  },
  {
    "text": "just behaving abnormal then the heal checks will fail we observed that in such St cases a large number of clusters",
    "start": "1462840",
    "end": "1470520"
  },
  {
    "text": "just lose one API server because the H Checker terminated them it's not bad sorry it's not good for especially for",
    "start": "1470520",
    "end": "1477440"
  },
  {
    "text": "large clusters because API servers has a ton of uh watch cache and KMS encrypted",
    "start": "1477440",
    "end": "1484480"
  },
  {
    "text": "decryption done and it's just not good to for a brief Interruption to kill all",
    "start": "1484480",
    "end": "1489720"
  },
  {
    "text": "of that and start a new APS over again most possibly during this coated",
    "start": "1489720",
    "end": "1495240"
  },
  {
    "text": "failures we don't want to try and put load lot of load on dependencies hcd in",
    "start": "1495240",
    "end": "1501399"
  },
  {
    "text": "this case or KMS services or other dependent apis because they may fail and eventually back off when it when they",
    "start": "1501399",
    "end": "1508559"
  },
  {
    "text": "back off uh recovery takes even longer so in this case what we do is we uh",
    "start": "1508559",
    "end": "1515360"
  },
  {
    "text": "circuit break our H Checker so that when it sees that certain threshold of uh H",
    "start": "1515360",
    "end": "1521520"
  },
  {
    "text": "checks are failing it slows down and if it is even more it just stops and Pages",
    "start": "1521520",
    "end": "1527320"
  },
  {
    "text": "us um an interesting aspect is that we track a lot of zonal metrics and this H",
    "start": "1527320",
    "end": "1535000"
  },
  {
    "text": "Checker is H checking all the API servers and pushing a lot of metrics to our backend systems we consume those",
    "start": "1535000",
    "end": "1540120"
  },
  {
    "text": "metrics and create um like we feed it into a system that can detect and tell us that one zone is really having a",
    "start": "1540120",
    "end": "1547840"
  },
  {
    "text": "certain problem in uh and we use API server healthy perone uh requests",
    "start": "1547840",
    "end": "1554640"
  },
  {
    "text": "failing perone throttling all these parameters and feed into system and then the system tells us and Pages us saying",
    "start": "1554640",
    "end": "1562039"
  },
  {
    "text": "I think something is going on you should go check and it automatically starts the",
    "start": "1562039",
    "end": "1567240"
  },
  {
    "text": "the weight shift process the weight shift is such a safe process that you can just deal with that and then um and",
    "start": "1567240",
    "end": "1574279"
  },
  {
    "text": "on call can go check release release it or keep it going the next one is incorrect control",
    "start": "1574279",
    "end": "1582360"
  },
  {
    "text": "leadership in this example again Zone one is partitioned but it's partitioned",
    "start": "1582360",
    "end": "1587679"
  },
  {
    "text": "in a way that just the DNS resolution is failing in zone one uh when that happens",
    "start": "1587679",
    "end": "1593159"
  },
  {
    "text": "most quence components don't fail except anything that depends on remote apis or internet in this case Cloud controller",
    "start": "1593159",
    "end": "1600559"
  },
  {
    "text": "manager uh because the cloud controller manager is able to still get the lease",
    "start": "1600559",
    "end": "1606279"
  },
  {
    "text": "from APS Ser because APS ser and HD are just working fine it continues to maintain leadership it is not failing H",
    "start": "1606279",
    "end": "1612320"
  },
  {
    "text": "checks many components today uh up stream and our own components they they",
    "start": "1612320",
    "end": "1618799"
  },
  {
    "text": "can't detect such nuanced DNS failure API failure from remote apis and feed",
    "start": "1618799",
    "end": "1624600"
  },
  {
    "text": "into health check and as a platform owner there are like 10 different components we are running it will be",
    "start": "1624600",
    "end": "1629760"
  },
  {
    "text": "very hard across very many code bases to deterministically find out how to fail",
    "start": "1629760",
    "end": "1634799"
  },
  {
    "text": "these um heal checks so we implemented something where we added zonal awareness",
    "start": "1634799",
    "end": "1641520"
  },
  {
    "text": "to the controllers and when we understand from the previous slides I mentioned that we can detect zonal",
    "start": "1641520",
    "end": "1647640"
  },
  {
    "text": "failures when we understand that something is happening zonally we go again um push an API which instructs all",
    "start": "1647640",
    "end": "1654679"
  },
  {
    "text": "of these controllers to relinquish leaves deterministically even if a controller",
    "start": "1654679",
    "end": "1660159"
  },
  {
    "text": "is not facing a problem we make it very deterministic and say all controllers please move out of the",
    "start": "1660159",
    "end": "1666440"
  },
  {
    "text": "zone and with that no cluster will see any downtime at all uh because this is",
    "start": "1666440",
    "end": "1673120"
  },
  {
    "text": "so undetermined because DNS is casted in most cases so if even if DNS is out it's",
    "start": "1673120",
    "end": "1679760"
  },
  {
    "text": "okay these control will keep working unless some controll restarts some host restarts and now DNS is not available",
    "start": "1679760",
    "end": "1685360"
  },
  {
    "text": "anymore it's so UND deterministic let's look at how it looks like so in this picture we uh the colors",
    "start": "1685360",
    "end": "1693480"
  },
  {
    "text": "show the leadership distribution across zones and we uh this is from one of our",
    "start": "1693480",
    "end": "1698760"
  },
  {
    "text": "test simulations where we let the system um fail and the our zonal ship kicks in",
    "start": "1698760",
    "end": "1707200"
  },
  {
    "text": "and the leadership moves away so the blue line goes up because it is now taking all the leadership and the other",
    "start": "1707200",
    "end": "1714159"
  },
  {
    "text": "lines I think the orange uh goes down and uh it is relinquishing all the uh leadership we do it in a very safe way",
    "start": "1714159",
    "end": "1721919"
  },
  {
    "text": "such that if there's no replica available really to take leadership don't take it don't do",
    "start": "1721919",
    "end": "1728799"
  },
  {
    "text": "this um okay",
    "start": "1728799",
    "end": "1734320"
  },
  {
    "text": "let's this is the last one so the next one is is uh hcd",
    "start": "1734600",
    "end": "1740399"
  },
  {
    "text": "durability during hcd up upgrades uh we do a rolling update we have three HD",
    "start": "1740399",
    "end": "1748320"
  },
  {
    "text": "nodes at steady state and we kill one node let it come back and join the cluster kill the second let it happen",
    "start": "1748320",
    "end": "1756320"
  },
  {
    "text": "repeatedly now if a correlated zonal uh failure happens at this time and and",
    "start": "1756320",
    "end": "1763760"
  },
  {
    "text": "host has been terminated and yet to come back up the system is vulnerable uh it",
    "start": "1763760",
    "end": "1769320"
  },
  {
    "text": "is available it can still serving Quorum but if any other other node has a",
    "start": "1769320",
    "end": "1775240"
  },
  {
    "text": "problem it loses Corum immediately and that's a risk uh most uh",
    "start": "1775240",
    "end": "1781799"
  },
  {
    "text": "implementations uh out there take a backup of the hcd periodically and then apply them when Quorum is lost although",
    "start": "1781799",
    "end": "1790279"
  },
  {
    "text": "uh it has a availability risk where from the time when the Quorum is lost until the time the uh the restore happens the",
    "start": "1790279",
    "end": "1798640"
  },
  {
    "text": "system has an availability loss no reads or rights can happen but the most riskiest part of it is the durability",
    "start": "1798640",
    "end": "1805799"
  },
  {
    "text": "loss when we restore we are going back in time anything that has happened from that time say 15 minute 5 minute",
    "start": "1805799",
    "end": "1811600"
  },
  {
    "text": "whatever the periodic interval is we lost the data so we improve the durability poster of the system by doing",
    "start": "1811600",
    "end": "1818399"
  },
  {
    "text": "a static volume approach so we have static membership and static volume all",
    "start": "1818399",
    "end": "1824559"
  },
  {
    "text": "the H data is on the disk and those discs are never thrown away and this discs are spread out across the zones",
    "start": "1824559",
    "end": "1831080"
  },
  {
    "text": "and when if for any chance the a two host go down at the same time they just",
    "start": "1831080",
    "end": "1837279"
  },
  {
    "text": "come back up and start right there there's no chance of any data loss uh what is the chance that this may",
    "start": "1837279",
    "end": "1844200"
  },
  {
    "text": "happen so in the our workflows the most critical part is now a new host came up",
    "start": "1844200",
    "end": "1850200"
  },
  {
    "text": "uh we detach the volume from the old one and attached to the new one and assign it the membership it needed there's a",
    "start": "1850200",
    "end": "1857000"
  },
  {
    "text": "slim chance that something may go wrong at that time so what we do is we CH",
    "start": "1857000",
    "end": "1862840"
  },
  {
    "text": "remove the chance of correlated failures Again by saying we won't let SD updates",
    "start": "1862840",
    "end": "1868000"
  },
  {
    "text": "go through when we know that a zonal problem is happening with that uh we have decoupled of infrastructure",
    "start": "1868000",
    "end": "1874159"
  },
  {
    "text": "infrastructure such that if we issue an API server update it will uh it will go",
    "start": "1874159",
    "end": "1881679"
  },
  {
    "text": "through without updating hcd we can put that in a queue and when we know that",
    "start": "1881679",
    "end": "1887240"
  },
  {
    "text": "the system has recovered we can go look at that queue and recover it HD is an implementation detail anything that bad",
    "start": "1887240",
    "end": "1893960"
  },
  {
    "text": "happening in the HD should not leak out uh to the customers and we try to ensure",
    "start": "1893960",
    "end": "1899720"
  },
  {
    "text": "that these are uh slack threads uh slack Handles in uh SC Channel please reach",
    "start": "1900600",
    "end": "1906519"
  },
  {
    "text": "out if you have suggestion feedback or anything that you want to brainstorm plenty of links uh all of them are where",
    "start": "1906519",
    "end": "1912840"
  },
  {
    "text": "spread out across the slides they're all gathered here I will be publishing this as a PDF Doc in the uh sced um so please",
    "start": "1912840",
    "end": "1920639"
  },
  {
    "text": "uh go through read them and uh we work working with our stream to to suggest",
    "start": "1920639",
    "end": "1927039"
  },
  {
    "text": "things that we see and uh make the system better",
    "start": "1927039",
    "end": "1932158"
  }
]