[
  {
    "text": "thank you all for your interest in our work I'm Aur and this is Aditya we work in the compute platform team at Uber and",
    "start": "280",
    "end": "6839"
  },
  {
    "text": "today we are going to talk about our ongoing migration to kubernetes here is a brief summary of",
    "start": "6839",
    "end": "13920"
  },
  {
    "text": "what we are going to talk about first we will introduce the compute platform team at Uber and what",
    "start": "13920",
    "end": "19640"
  },
  {
    "text": "we work on then we will briefly capture the current status of our",
    "start": "19640",
    "end": "25000"
  },
  {
    "text": "migration then we want to take this opportunity to give credit where it to you with the community and talk about",
    "start": "25000",
    "end": "31759"
  },
  {
    "text": "the unique features which we heavily use and have found to work really well",
    "start": "31759",
    "end": "37079"
  },
  {
    "text": "without requiring any changes next we talk about a few features and customizations we Implement",
    "start": "37079",
    "end": "43960"
  },
  {
    "text": "on top of kubernetes um why we Implement them so as to make it better suited for",
    "start": "43960",
    "end": "49360"
  },
  {
    "text": "Uber's needs and finally we talk about some of the interesting uh learnings we",
    "start": "49360",
    "end": "55480"
  },
  {
    "text": "had during the course of the migration first let me introduce the",
    "start": "55480",
    "end": "60879"
  },
  {
    "text": "compute platform team at Uber today Uber manages its own on-prem",
    "start": "60879",
    "end": "66400"
  },
  {
    "text": "data center as well as leverages capacity from Oracle and Google Cloud",
    "start": "66400",
    "end": "72159"
  },
  {
    "text": "these providers are abstracted away from the platforms via layer we call crane which essentially implements host as a",
    "start": "72159",
    "end": "80560"
  },
  {
    "text": "service it ingests capacity from these providers Provisions the host and the",
    "start": "80560",
    "end": "85600"
  },
  {
    "text": "VMS with the right OS the Right image installs the right set of packages and essentially makes the node ready for use",
    "start": "85600",
    "end": "92240"
  },
  {
    "text": "by platforms above crane we have the container orchestration layer which",
    "start": "92240",
    "end": "97960"
  },
  {
    "text": "essentially provides container as a service to the rest of the company today this layer is built on top",
    "start": "97960",
    "end": "106520"
  },
  {
    "text": "of misos and pettin where peltin is a custom framework on misos and we are in the process of migrating this layer to",
    "start": "106520",
    "end": "115479"
  },
  {
    "text": "kubernetes this layer is or this platform is is used to run all the",
    "start": "116039",
    "end": "121399"
  },
  {
    "text": "stateless microservices at Uber including the ones which run on shared",
    "start": "121399",
    "end": "126799"
  },
  {
    "text": "infrastructure the ones which required its own dedicated infrastructure to run as well as lowlevel infrastructure",
    "start": "126799",
    "end": "132520"
  },
  {
    "text": "Services which are required to boot up the rest of the infrastructure a number of batch",
    "start": "132520",
    "end": "139200"
  },
  {
    "text": "workloads are also run on this platform including all machine learning workloads all Jupiter notebook sessions and a",
    "start": "139200",
    "end": "146560"
  },
  {
    "text": "subset of spark workloads the spark workloads which run on this platform are the ones which don't run on Yan very",
    "start": "146560",
    "end": "153200"
  },
  {
    "text": "well due to numerous reasons like requiring large containers or requiring customized features like gang scheding",
    "start": "153200",
    "end": "161440"
  },
  {
    "text": "or custom Docker images Etc this talk is primarily going to focus on the stateless side of things we",
    "start": "161440",
    "end": "168560"
  },
  {
    "text": "have another talk at 325 by Amit and Kevin uh which will talk about the bad side of things at",
    "start": "168560",
    "end": "176200"
  },
  {
    "text": "Uber before moving forward let me me uh briefly capture the scale at which we",
    "start": "177480",
    "end": "182680"
  },
  {
    "text": "operate and how we think about scale today our stateless Fleet runs",
    "start": "182680",
    "end": "188519"
  },
  {
    "text": "more than uh actually thousands of services across millions of cores however the one big factor which",
    "start": "188519",
    "end": "196959"
  },
  {
    "text": "has a significant impact on how we think about scale is the number of deploys",
    "start": "196959",
    "end": "202480"
  },
  {
    "text": "which run every day in our Fleet we have fairly sophisticated cicd",
    "start": "202480",
    "end": "208159"
  },
  {
    "text": "platforms and nearly all our services are onboarded onto it and every service",
    "start": "208159",
    "end": "214120"
  },
  {
    "text": "gets deployed multiple times in a single day in a fleet so averaged over 30 days we see",
    "start": "214120",
    "end": "221720"
  },
  {
    "text": "more than a million and a half containers launched every day in our",
    "start": "221720",
    "end": "226799"
  },
  {
    "text": "Fleet and this is averaged over 30 days um there are times during uh the heavy",
    "start": "226799",
    "end": "234319"
  },
  {
    "text": "deploy times where we see new pod launches at the rate of 120 to 130 new",
    "start": "234319",
    "end": "240280"
  },
  {
    "text": "pod launches every second and note that this is just the",
    "start": "240280",
    "end": "245599"
  },
  {
    "text": "new pod launch date the actual pod CH rate is actually much higher so whenever",
    "start": "245599",
    "end": "251040"
  },
  {
    "text": "we design our system or we think about migrating to a new platform like kubernetes this is the one factor the",
    "start": "251040",
    "end": "257720"
  },
  {
    "text": "high part ched is something which we explicitly account for and design",
    "start": "257720",
    "end": "263600"
  },
  {
    "text": "for next for context let me also capture where we are in terms of our m addtion to kubernetes we started at the start of",
    "start": "265080",
    "end": "271919"
  },
  {
    "text": "this year and as of today More than 70% of our stateless Fleet is running on kubernetes and we expect to be done by",
    "start": "271919",
    "end": "280240"
  },
  {
    "text": "sometime uh next half we have multiple 5,000 node clusters and our largest cluster is",
    "start": "280240",
    "end": "287960"
  },
  {
    "text": "between 5,000 and 6,000 notes today and we expect it to grow to around 7500",
    "start": "287960",
    "end": "295199"
  },
  {
    "text": "notes when we start started this migration based on our previous experiences with using open source",
    "start": "298039",
    "end": "304400"
  },
  {
    "text": "Technologies at Uber including misource and others and also having done similar",
    "start": "304400",
    "end": "310720"
  },
  {
    "text": "migrations like this in the past we laid out a number of principles for us I'm going to capture the three most",
    "start": "310720",
    "end": "316840"
  },
  {
    "text": "important ones here the first one is seamless upgrades we would like to run kubernetes",
    "start": "316840",
    "end": "323919"
  },
  {
    "text": "version in our Fleet at the same version as what the cloud providers are running at that point in time",
    "start": "323919",
    "end": "330840"
  },
  {
    "text": "in the past with misource and other open source code we had really struggled to upgrade our Fleet to the new open source",
    "start": "330840",
    "end": "337720"
  },
  {
    "text": "version and based on those learnings we decided that when we move to kubernetes",
    "start": "337720",
    "end": "343400"
  },
  {
    "text": "we are going to use the Upstream code Asis with no to minimal changes to the Upstream code and we are going to rely",
    "start": "343400",
    "end": "349800"
  },
  {
    "text": "on kubernetes Native extensibility like plugins and crds to inject any",
    "start": "349800",
    "end": "355759"
  },
  {
    "text": "customization the second principle is reliable upgrades in the past whenever we try to upgrade",
    "start": "355759",
    "end": "362520"
  },
  {
    "text": "open source to a new version invariably we always had incidents and issues in",
    "start": "362520",
    "end": "368120"
  },
  {
    "text": "production to mitigate against that for kubernetes we have built extensive",
    "start": "368120",
    "end": "373199"
  },
  {
    "text": "release validation with numerous integration end to endend and most importantly performance tests so as to",
    "start": "373199",
    "end": "380000"
  },
  {
    "text": "capture any regression before we roll out into release and another thing which we did which has really helped us is",
    "start": "380000",
    "end": "385919"
  },
  {
    "text": "that we have continuous probes running in our cluster which keep testing our cluster control planes so as to ensure",
    "start": "385919",
    "end": "394080"
  },
  {
    "text": "no regression and if any uh any issue is detected we immediately roll back the new",
    "start": "394080",
    "end": "399560"
  },
  {
    "text": "release the final principle is around transparent and automated migrations during previous migrations we",
    "start": "399560",
    "end": "407199"
  },
  {
    "text": "found that anytime we require any developer to do any effort or we change",
    "start": "407199",
    "end": "413720"
  },
  {
    "text": "the developer experience in in in an unexpected way that migration invariably",
    "start": "413720",
    "end": "419039"
  },
  {
    "text": "failed so as a principle we said that we don't we want to do this migration which is",
    "start": "419039",
    "end": "424240"
  },
  {
    "text": "fully automated and completely transparent from all developers that is the developers just keep running their",
    "start": "424240",
    "end": "429879"
  },
  {
    "text": "service as is um like any normal day underneath we change the platform from",
    "start": "429879",
    "end": "435560"
  },
  {
    "text": "misos to kubernetes without anyone doing anything or even noticing that something has",
    "start": "435560",
    "end": "441240"
  },
  {
    "text": "changed we want to keep this incident free with no business impact Uber so let",
    "start": "441240",
    "end": "446680"
  },
  {
    "text": "me talk about how we accomplish the transparent and and automated migration and for that let me introduce up up um",
    "start": "446680",
    "end": "454680"
  },
  {
    "text": "is a platform owned by our sister team and it implements Uber's Global",
    "start": "454680",
    "end": "459840"
  },
  {
    "text": "stateless Federation layer it is the primary service owner interface and provides a number of uh of federation",
    "start": "459840",
    "end": "467919"
  },
  {
    "text": "features including safe rollouts continuous deployments and distributing",
    "start": "467919",
    "end": "473039"
  },
  {
    "text": "service capacity across multiple availability zones for service High availability import importantly it",
    "start": "473039",
    "end": "479800"
  },
  {
    "text": "abstracts away the cluster technology of misos or kubernetes away from developers one interesting feature it",
    "start": "479800",
    "end": "486879"
  },
  {
    "text": "has is what we call cluster selection where within one availability Zone if we have multiple clusters and IT rebalances",
    "start": "486879",
    "end": "493840"
  },
  {
    "text": "services away from clusters with high allocation to clusters with low allocation so this allows us to do",
    "start": "493840",
    "end": "500479"
  },
  {
    "text": "automated migration because what we do is that we just move the physical capacity from misos to",
    "start": "500479",
    "end": "505759"
  },
  {
    "text": "kubernetes thus the allocation in misos becomes High the allocation percentage in kubernetes becomes lower and up",
    "start": "505759",
    "end": "512000"
  },
  {
    "text": "automatically rebalances so this allows us to just run an automated migration um so how do we do a",
    "start": "512000",
    "end": "519159"
  },
  {
    "text": "transparent migration the fact that we have up which is abstracting away the underlying",
    "start": "519159",
    "end": "525000"
  },
  {
    "text": "cluster technology obviously is a very important step in achieving transparent",
    "start": "525000",
    "end": "530640"
  },
  {
    "text": "migration and that's what allows us to even think about doing a transparent",
    "start": "530640",
    "end": "536000"
  },
  {
    "text": "migration however as you may guess compute is is a central infrastructure",
    "start": "536000",
    "end": "541120"
  },
  {
    "text": "piece which integrates with numerous other infrastructure platforms which all developers use so when migrating to",
    "start": "541120",
    "end": "549040"
  },
  {
    "text": "kubernetes we have to rebuild all these existing Integrations and given kubernetes and",
    "start": "549040",
    "end": "554800"
  },
  {
    "text": "misos have numerous subtle differences each of these Integrations",
    "start": "554800",
    "end": "560720"
  },
  {
    "text": "require um a a very a very thoughtful design so as to ensure that the",
    "start": "560720",
    "end": "566640"
  },
  {
    "text": "developer experience remains exactly the same in the subsequent slides you will see a few examples of the customizations we",
    "start": "566640",
    "end": "573800"
  },
  {
    "text": "had to build on top of kubernetes to ensure",
    "start": "573800",
    "end": "578000"
  },
  {
    "text": "this next we want to take this opportunity to thank the community for providing",
    "start": "581040",
    "end": "587079"
  },
  {
    "text": "numerous well well-built features which we directly use without making any",
    "start": "587079",
    "end": "592560"
  },
  {
    "text": "changes to it note that this is not an exhaustive list this is just a subset of some things we thought are fairly unique",
    "start": "592560",
    "end": "598640"
  },
  {
    "text": "to kubernetes and uh pretty much anyone who is running kubernetes should be using these the",
    "start": "598640",
    "end": "604399"
  },
  {
    "text": "first is the default Cube Schuler it's awesome it's super stable super scalable six scheduling and six scalability in",
    "start": "604399",
    "end": "610600"
  },
  {
    "text": "the past few releases have done an amazing job in just scaling it up um another thing which we want to call out",
    "start": "610600",
    "end": "617079"
  },
  {
    "text": "is the plug-in architecture which Cube schedular has we heavily leverage it not",
    "start": "617079",
    "end": "622839"
  },
  {
    "text": "only to use the numerous plugins which other companies have built an open",
    "start": "622839",
    "end": "628760"
  },
  {
    "text": "source but also to inject a couple of our own specific",
    "start": "628760",
    "end": "634560"
  },
  {
    "text": "customizations the next is security the security first nature of kubernetes",
    "start": "634560",
    "end": "639959"
  },
  {
    "text": "stands out I think like one of the teams who is the most happiest with a migration to",
    "start": "639959",
    "end": "646360"
  },
  {
    "text": "kubernetes is the engineering security team because they are able to reuse most",
    "start": "646360",
    "end": "652320"
  },
  {
    "text": "of the features directly from kubernetes and significantly upgrade uh security posture an example is how do we secure",
    "start": "652320",
    "end": "660160"
  },
  {
    "text": "our own cluster control plane we use search for auen and arback for odzi both",
    "start": "660160",
    "end": "665920"
  },
  {
    "text": "are super intuitive and provide enough granularity for us to be able to really secure it we are now actually exploring",
    "start": "665920",
    "end": "673200"
  },
  {
    "text": "an authentication proxy to as a validating admission controller to potentially set up personal access",
    "start": "673200",
    "end": "680240"
  },
  {
    "text": "control the next feature is API priority and fairness which we heavily use to protect API server and atcd we have set",
    "start": "680240",
    "end": "687800"
  },
  {
    "text": "up limits not not only for every controller every operator every service",
    "start": "687800",
    "end": "693800"
  },
  {
    "text": "which integrates with a control plane but also we limit what operations can",
    "start": "693800",
    "end": "700160"
  },
  {
    "text": "they perform on the cluster for example we have pretty much disabled the use of",
    "start": "700160",
    "end": "705320"
  },
  {
    "text": "direct get and direct list except during an Informer startup on to API server",
    "start": "705320",
    "end": "713000"
  },
  {
    "text": "directly this particular feature has protected our clusters from going hard",
    "start": "713560",
    "end": "719680"
  },
  {
    "text": "down at least once actually more than once during the migration is and is one",
    "start": "719680",
    "end": "725279"
  },
  {
    "text": "of the primary reasons we have had an incident-free migration up till now the next is controller runtime",
    "start": "725279",
    "end": "731839"
  },
  {
    "text": "ecosystem we all our controllers and operators are build on top of it it's great it's intuitive to use great",
    "start": "731839",
    "end": "737959"
  },
  {
    "text": "Telemetry no performance hit in using it finally and uh something which is not",
    "start": "737959",
    "end": "744279"
  },
  {
    "text": "that well known is support for separate events database which is super helpful to us because it allows us to scale our",
    "start": "744279",
    "end": "751360"
  },
  {
    "text": "clusters pretty well without losing the auditability and debuggability provided",
    "start": "751360",
    "end": "756480"
  },
  {
    "text": "by events I'm not going to hand it over to adya who will talk about the customizations we have added for Uber",
    "start": "756480",
    "end": "763440"
  },
  {
    "text": "developers and also discuss why we added them and also talk about some interesting learnings we have had during",
    "start": "763440",
    "end": "769360"
  },
  {
    "text": "the course of this",
    "start": "769360",
    "end": "772040"
  },
  {
    "text": "migration hello everyone uh my name is Aditya I work with with apur on the container platform team at Uber uh as",
    "start": "774760",
    "end": "782680"
  },
  {
    "text": "apur mentioned a few slides earlier transparent migration is one of our key",
    "start": "782680",
    "end": "787880"
  },
  {
    "text": "migration guiding principles as part of this project what that means is as we move",
    "start": "787880",
    "end": "793839"
  },
  {
    "text": "from msos to kubernetes uh our developers should continue to have the same developer experience uh the same",
    "start": "793839",
    "end": "801440"
  },
  {
    "text": "levels of deployment safety as well as developer velocity uh to that end we",
    "start": "801440",
    "end": "807279"
  },
  {
    "text": "have built numerous features uh to achieve this on top of uh Native Gates",
    "start": "807279",
    "end": "812320"
  },
  {
    "text": "uh for example we have abstracted away service intent uh into a crd we allow",
    "start": "812320",
    "end": "818600"
  },
  {
    "text": "for retrieving container artifacts after a pod has exited uh we allow setting U",
    "start": "818600",
    "end": "824440"
  },
  {
    "text": "limits on your container uh which then uh let service owners set things like FD limits for example uh we have improved",
    "start": "824440",
    "end": "832360"
  },
  {
    "text": "the scale of kubernetes UI uh for example if you take the Kates UI natively and point it to a 7 7500 note",
    "start": "832360",
    "end": "839720"
  },
  {
    "text": "cluster with say 200k Parts uh it is going to take about over 7 8 minutes to",
    "start": "839720",
    "end": "846240"
  },
  {
    "text": "load with our optimizations this happens under 10 seconds uh I'm going to pause here and",
    "start": "846240",
    "end": "852959"
  },
  {
    "text": "uh the features that are highlighted we are going to talk in detail uh in the subsequent slides but if you think there",
    "start": "852959",
    "end": "860320"
  },
  {
    "text": "is anything here that is applicable outside of uber as well we are all years we want to talk about it we want to find",
    "start": "860320",
    "end": "866440"
  },
  {
    "text": "ways in which we can give back to the community",
    "start": "866440",
    "end": "870680"
  },
  {
    "text": "so to take AB to take transparent migrations further what that really",
    "start": "871880",
    "end": "877079"
  },
  {
    "text": "means is that we do not want uh our service owners to care about any of the kubernetes internals nor do we want to",
    "start": "877079",
    "end": "883680"
  },
  {
    "text": "expose them uh in in a lot of uh depth to our Federation platform which is up",
    "start": "883680",
    "end": "889600"
  },
  {
    "text": "uh for example if a service wants to run on custom skes like some Nvidia specific",
    "start": "889600",
    "end": "896519"
  },
  {
    "text": "GPU uh service owners should just just tell us that and not tell us to use a",
    "start": "896519",
    "end": "901680"
  },
  {
    "text": "specific node selector feature to so that their service runs there uh so what",
    "start": "901680",
    "end": "907000"
  },
  {
    "text": "we have done is abstracted away service intent as part of uber deployment crd so",
    "start": "907000",
    "end": "912800"
  },
  {
    "text": "the intent can be uh the service needs image prefetching or in place updates or",
    "start": "912800",
    "end": "918079"
  },
  {
    "text": "dedicated hardware for example uh and the crd controller's job is to then",
    "start": "918079",
    "end": "923440"
  },
  {
    "text": "translate this intent into uh meaningful one or more kubernetes specific",
    "start": "923440",
    "end": "929040"
  },
  {
    "text": "expressions and actually make it happen so to keep developer experience",
    "start": "929040",
    "end": "936720"
  },
  {
    "text": "uh seamless we support a widely used feature at Uber called container artifacts retrieval uh today our",
    "start": "936720",
    "end": "942880"
  },
  {
    "text": "developers are able to access their artifacts like uh core dumps or Heap profiles even after their container",
    "start": "942880",
    "end": "949680"
  },
  {
    "text": "exits these artifacts are written to local disk and exposed to the users via",
    "start": "949680",
    "end": "954720"
  },
  {
    "text": "a mesos endpoint uh for example Java services are configured to write their P Prof and locally and these are very",
    "start": "954720",
    "end": "962040"
  },
  {
    "text": "useful for debugging uh for developers to debug their crashed or room killed containers uh for some time after the",
    "start": "962040",
    "end": "969120"
  },
  {
    "text": "container has already crashed we lose this functionality when we move to Cades",
    "start": "969120",
    "end": "974199"
  },
  {
    "text": "because uh all the local volumes and this local data on the host gets cleaned up after like during pot deletion to",
    "start": "974199",
    "end": "982639"
  },
  {
    "text": "support this in Cates we introduced a artifact uploader demon that uploads these artifacts to a block store on",
    "start": "982639",
    "end": "989519"
  },
  {
    "text": "container exit so how does this work uh we have introduced a side car",
    "start": "989519",
    "end": "996920"
  },
  {
    "text": "container as part of all of our stateless pods uh the main objective of the sidecar container is to buy us time",
    "start": "996920",
    "end": "1004839"
  },
  {
    "text": "after a primary container exits until a pod is deleted so once a primary",
    "start": "1004839",
    "end": "1010480"
  },
  {
    "text": "container exits be it for normal uh updates or abnormal exits like um kills",
    "start": "1010480",
    "end": "1016800"
  },
  {
    "text": "or SE FS uh the artifact uploader demon on the host gets a signal it execs into",
    "start": "1016800",
    "end": "1022800"
  },
  {
    "text": "the side car it tars up all these artifacts that the developers care about",
    "start": "1022800",
    "end": "1028640"
  },
  {
    "text": "and then it uploads them to blob store uh and then it asks the sidecar container to kill itself uh upon that",
    "start": "1028640",
    "end": "1035760"
  },
  {
    "text": "delete uh the Pod gets deleted and all the local artifacts get cleaned",
    "start": "1035760",
    "end": "1041839"
  },
  {
    "text": "up in terms of uh deployment safety a widely used feature at Uber is",
    "start": "1042760",
    "end": "1048919"
  },
  {
    "text": "controlled or gradual scaling we have multiple Services which uh which use",
    "start": "1048919",
    "end": "1054919"
  },
  {
    "text": "membership based protocols like Apache Helix we have celery workers we have sharded services and all of them are",
    "start": "1054919",
    "end": "1061799"
  },
  {
    "text": "very sensitive to Rapid scale UPS or rapid scale Downs uh so to support this we looked at",
    "start": "1061799",
    "end": "1069200"
  },
  {
    "text": "what comes closest in the native ecosystem and the thing that comes closest is uh rolling update spec uh but",
    "start": "1069200",
    "end": "1076240"
  },
  {
    "text": "this is applicable only during actual updates uh on the contrary kubernetes is",
    "start": "1076240",
    "end": "1082960"
  },
  {
    "text": "actually optimized to make scales go as fast as possible uh so to support this",
    "start": "1082960",
    "end": "1089559"
  },
  {
    "text": "we introduced a bat sizing concept uh in Uber deployment crd so our Federation",
    "start": "1089559",
    "end": "1095360"
  },
  {
    "text": "layer can uh specify a batch size during scale operations so if you specify a",
    "start": "1095360",
    "end": "1100400"
  },
  {
    "text": "batch size of three and you want to go from 10 to 20 instances you go in steps",
    "start": "1100400",
    "end": "1105640"
  },
  {
    "text": "of 10 13 16 19 and then 20",
    "start": "1105640",
    "end": "1110320"
  },
  {
    "text": "so we talked about slowing down your scale operations but a majority of",
    "start": "1112440",
    "end": "1117520"
  },
  {
    "text": "service owners they actually want to deploy their code to production as fast as possible we heavily use cicd service",
    "start": "1117520",
    "end": "1125640"
  },
  {
    "text": "owners deploy multiple times based using cicd U and therefore the desire is to do",
    "start": "1125640",
    "end": "1131240"
  },
  {
    "text": "this as quickly as possible also with with these rapid rollouts we also want",
    "start": "1131240",
    "end": "1136440"
  },
  {
    "text": "to make sure that incident mitigation is uh super quick so we can deploy like hot fixes very",
    "start": "1136440",
    "end": "1143200"
  },
  {
    "text": "quickly some services at Uber uh it's very hard to do this for those Services",
    "start": "1143200",
    "end": "1148400"
  },
  {
    "text": "because uh for one reason is that their containers are super large uh for",
    "start": "1148400",
    "end": "1153799"
  },
  {
    "text": "example a container of a service can take up to like more than 25% of a host",
    "start": "1153799",
    "end": "1159360"
  },
  {
    "text": "resource uh our clusters run fairly hot at 85 to 90% allocation which means that",
    "start": "1159360",
    "end": "1166400"
  },
  {
    "text": "uh plus plus there is a lot of churn like we talked about earlier so lot of pods are restarting being moved around",
    "start": "1166400",
    "end": "1173120"
  },
  {
    "text": "which means that our clusters are inherently fragmented and that means that there is not enough hosts typically",
    "start": "1173120",
    "end": "1179960"
  },
  {
    "text": "that have that much free capacity to house these pods so it takes a long time to place them once we actually place",
    "start": "1179960",
    "end": "1187320"
  },
  {
    "text": "them it it is it is highly desirable that they do not lose this placement",
    "start": "1187320",
    "end": "1192880"
  },
  {
    "text": "across updates now when the Pod actually gets placed",
    "start": "1192880",
    "end": "1198919"
  },
  {
    "text": "it is also running into issues like cold start because these pods will have image sizes close to 5 gigs sometimes it takes",
    "start": "1198919",
    "end": "1206679"
  },
  {
    "text": "a long time to download these images and then start the part so all of these things uh they combine towards like a",
    "start": "1206679",
    "end": "1214200"
  },
  {
    "text": "slower roll out so to to to sort of uh",
    "start": "1214200",
    "end": "1219400"
  },
  {
    "text": "solve this problem we introduced uh we started using clone sets which are uh which are developed by Alibaba uh they",
    "start": "1219400",
    "end": "1226440"
  },
  {
    "text": "are a kides resource that provide in place updates uh using pod patching so",
    "start": "1226440",
    "end": "1232080"
  },
  {
    "text": "now that we have solved the in place updating problem we we introduced a image prefetch demon uh which uh allows",
    "start": "1232080",
    "end": "1240039"
  },
  {
    "text": "for prefetching all these uh images that are being downloaded so when an update",
    "start": "1240039",
    "end": "1245080"
  },
  {
    "text": "is taking place we go Zone by zone so if if we updating Zone a we will notify the",
    "start": "1245080",
    "end": "1251039"
  },
  {
    "text": "image prefetched demon in zones B and C to pre-download the image that is being",
    "start": "1251039",
    "end": "1256080"
  },
  {
    "text": "updated so that when the update hits Zone B the image is already present on those",
    "start": "1256080",
    "end": "1262480"
  },
  {
    "text": "horse another interesting feature uh we provide is a unique 32-bit instance ID",
    "start": "1262840",
    "end": "1268159"
  },
  {
    "text": "for per pod they're unique within a service and an environment developers want to identify issues with uh single",
    "start": "1268159",
    "end": "1275120"
  },
  {
    "text": "instance failures uh they do so by tagging their metrics and Logs with uh",
    "start": "1275120",
    "end": "1280400"
  },
  {
    "text": "instance IDs to improve the debuggability uh if they're not assigned unique IDs their metrics can get jumbled",
    "start": "1280400",
    "end": "1286960"
  },
  {
    "text": "so uniqueness is very important uh to developers we support this by using the last five characters of a pod ID uh and",
    "start": "1286960",
    "end": "1294880"
  },
  {
    "text": "we add service name an environment name with some padding to the first 58 characters so the last five characters",
    "start": "1294880",
    "end": "1303000"
  },
  {
    "text": "are random but they're still unique to that scope of service and environment so",
    "start": "1303000",
    "end": "1308640"
  },
  {
    "text": "an ask from the community is to provide hopefully ways of making this uh slightly",
    "start": "1308640",
    "end": "1315159"
  },
  {
    "text": "better even before we started uh with this migration we talked to a lot of",
    "start": "1315159",
    "end": "1321360"
  },
  {
    "text": "community members we read did a lot of research and the general uh General",
    "start": "1321360",
    "end": "1326600"
  },
  {
    "text": "consensus within the community was that we should set up large number of small",
    "start": "1326600",
    "end": "1332279"
  },
  {
    "text": "clusters uh the cluster sizes were like 1, to 1500 nodes at Uber we were doing",
    "start": "1332279",
    "end": "1338880"
  },
  {
    "text": "the exact opposite of this our cluster sizes were uh for mesos were between",
    "start": "1338880",
    "end": "1345480"
  },
  {
    "text": "5,000 to 7500 nodes why did we do this we did this so that",
    "start": "1345480",
    "end": "1352159"
  },
  {
    "text": "uh we reduce fragmentation issues we reduce the amount of stranded capacity",
    "start": "1352159",
    "end": "1357440"
  },
  {
    "text": "that we can no longer use if there are like smaller clusters and we reduced uh",
    "start": "1357440",
    "end": "1362520"
  },
  {
    "text": "operational toil so with kubernetes we wanted to see for ourselves uh we wanted to get a",
    "start": "1362520",
    "end": "1368880"
  },
  {
    "text": "reproducible setup where we can scale kubernetes uh to our uh requirements uh we we set up a",
    "start": "1368880",
    "end": "1375760"
  },
  {
    "text": "state-of-the-art benchmarking cluster using Cube Mark and uh cluster loader and I'm happy to note uh that we were",
    "start": "1375760",
    "end": "1383840"
  },
  {
    "text": "able to get 7500 nodes 200k pods and 150 pod launches per second reliably using",
    "start": "1383840",
    "end": "1390320"
  },
  {
    "text": "this cluster with no like minimal changes to the core uh kubernetes uh",
    "start": "1390320",
    "end": "1396200"
  },
  {
    "text": "control plane so kudos to the community for that there were some minor config",
    "start": "1396200",
    "end": "1401799"
  },
  {
    "text": "and software changes that we had to do for example we had to carefully tune QPS settings parallelism for controller",
    "start": "1401799",
    "end": "1408679"
  },
  {
    "text": "manager and scheduler uh we restrict the API calls like list and get uh using API",
    "start": "1408679",
    "end": "1414440"
  },
  {
    "text": "priority and fairness we use Proto encoding instead of Json so we heard in the talk that crds now support Proto",
    "start": "1414440",
    "end": "1421080"
  },
  {
    "text": "instead of Json which is awesome uh we we made some software changes to speed",
    "start": "1421080",
    "end": "1427039"
  },
  {
    "text": "up uh things like the part topology spread schedular plug-in with all of these changes we could actually get to",
    "start": "1427039",
    "end": "1433080"
  },
  {
    "text": "the desired scale that we wanted so far we talked about what we",
    "start": "1433080",
    "end": "1440200"
  },
  {
    "text": "proactively did to make sure this migration was seamless but as we went ahead uh with the migration we saw some",
    "start": "1440200",
    "end": "1448480"
  },
  {
    "text": "unexpected issues uh some unexpected behaviors some quirks and some lack of",
    "start": "1448480",
    "end": "1453640"
  },
  {
    "text": "tooling uh which I will get into with this uh slide so generally we did not see a",
    "start": "1453640",
    "end": "1462640"
  },
  {
    "text": "holistic monitoring solution out of the box to help us reason about the state of",
    "start": "1462640",
    "end": "1468120"
  },
  {
    "text": "the cluster for for for example uh we wanted to we we started seeing a lot more",
    "start": "1468120",
    "end": "1474640"
  },
  {
    "text": "fragmentation issues on kubernetes uh versus misos and one of",
    "start": "1474640",
    "end": "1479840"
  },
  {
    "text": "the reasons was we did not have in place updates uh there were P higher P churns",
    "start": "1479840",
    "end": "1485760"
  },
  {
    "text": "we used make before break and so on uh we could not we could not find a tool to",
    "start": "1485760",
    "end": "1493080"
  },
  {
    "text": "uh to really investigate where this fragmentation is and answer questions like why is my Bo not being placed uh",
    "start": "1493080",
    "end": "1500600"
  },
  {
    "text": "there are some ways with which you can use Cube cutle and like script around it but they all use some variant of like",
    "start": "1500600",
    "end": "1507320"
  },
  {
    "text": "aggressive listing which we wanted to avoid uh we saw issues where uh pods",
    "start": "1507320",
    "end": "1513360"
  },
  {
    "text": "kept getting rescheduled on same degraded hosts and get get crash looped",
    "start": "1513360",
    "end": "1518480"
  },
  {
    "text": "all the time uh we saw issues with noisy neighbors but we could not figure out",
    "start": "1518480",
    "end": "1523520"
  },
  {
    "text": "why a set of host were seeing degraded performance we wanted something to tell us uh if there are common set of",
    "start": "1523520",
    "end": "1530399"
  },
  {
    "text": "services running across this uh these many these many nodes uh we could we",
    "start": "1530399",
    "end": "1535640"
  },
  {
    "text": "don't we didn't have enough uh visibility into that uh we we we wanted more visibility into the kind of churn",
    "start": "1535640",
    "end": "1542360"
  },
  {
    "text": "this cluster is seeing in terms of how many parallel updates are there straggling updates are there struck",
    "start": "1542360",
    "end": "1548000"
  },
  {
    "text": "updates so to fix all of that we we built a a uh deployment uh an",
    "start": "1548000",
    "end": "1553320"
  },
  {
    "text": "observability tool ourselves another another Quirk was that",
    "start": "1553320",
    "end": "1559799"
  },
  {
    "text": "we saw was with uh Native informers and the way they reconcile uh their their",
    "start": "1559799",
    "end": "1566480"
  },
  {
    "text": "events so the way native kubernetes informers reconcile is uh every 8 to 10",
    "start": "1566480",
    "end": "1572279"
  },
  {
    "text": "hours they will replay all the events in their cache to their controller uh so",
    "start": "1572279",
    "end": "1578320"
  },
  {
    "text": "that they make sure that none of the events is missed this works fairly okay",
    "start": "1578320",
    "end": "1583919"
  },
  {
    "text": "for a smallish cluster with no churn but we have have seen issues where sometimes",
    "start": "1583919",
    "end": "1589640"
  },
  {
    "text": "a deployment gets created and the deployment create event is not acted",
    "start": "1589640",
    "end": "1594919"
  },
  {
    "text": "upon because at the same time the controller is having a leadership change so that deployment create event gets",
    "start": "1594919",
    "end": "1602200"
  },
  {
    "text": "lost and then the next time it is actually acted upon is after 8 to 10 hours which is not desirable so we",
    "start": "1602200",
    "end": "1608480"
  },
  {
    "text": "created a custom reconciliation mechanism for high level objects like uber",
    "start": "1608480",
    "end": "1615399"
  },
  {
    "text": "deployments similar to faster rollouts we also want to ensure that our roll",
    "start": "1615399",
    "end": "1621919"
  },
  {
    "text": "backs are pretty quick uh and deterministic so to do that we started",
    "start": "1621919",
    "end": "1627960"
  },
  {
    "text": "using progress deadline seconds and uh we treated PDS as a wall clock timer uh",
    "start": "1627960",
    "end": "1636240"
  },
  {
    "text": "that didn't work out quite as expected because we had some Services which had disabled their heal checks but they",
    "start": "1636240",
    "end": "1642919"
  },
  {
    "text": "still had crash looping pods we had services that had H checks",
    "start": "1642919",
    "end": "1648000"
  },
  {
    "text": "enabled but they had like a long initial delay for those H checks for both these",
    "start": "1648000",
    "end": "1653720"
  },
  {
    "text": "cases the deployment kept making some sort of progress uh because the pods crash",
    "start": "1653720",
    "end": "1660799"
  },
  {
    "text": "looped but before that they were marked ready immediately so whenever it appears to make progress the PDS timer gets",
    "start": "1660799",
    "end": "1667760"
  },
  {
    "text": "reset so we couldn't use it so we decided to use a heuristic like uh number of container restarts during a",
    "start": "1667760",
    "end": "1674360"
  },
  {
    "text": "roll out for example if we see more than 10% of PODS getting restarted five or",
    "start": "1674360",
    "end": "1680720"
  },
  {
    "text": "more times during a roll out we consider that as a bad roll out and we roll",
    "start": "1680720",
    "end": "1686360"
  },
  {
    "text": "back lastly I want to note that we couldn't have made this rapid progress",
    "start": "1686360",
    "end": "1691399"
  },
  {
    "text": "without a global Federation layer of up as well as investing in portability of services uh so because of these two",
    "start": "1691399",
    "end": "1699039"
  },
  {
    "text": "things plus our extensive efforts in making sure things are reliable uh at at",
    "start": "1699039",
    "end": "1704240"
  },
  {
    "text": "our Peak we were moving about 250k to 300K course per week which is a pretty",
    "start": "1704240",
    "end": "1709880"
  },
  {
    "text": "high number in my opinion so so far we have talked about",
    "start": "1709880",
    "end": "1716120"
  },
  {
    "text": "our stateless side of uh story where we are going with this is we have uh",
    "start": "1716120",
    "end": "1722720"
  },
  {
    "text": "multiple cluster management Technologies at Uber we have yarn to schedule batch workloads like spark jobs Flink Presto",
    "start": "1722720",
    "end": "1730799"
  },
  {
    "text": "jobs and we have Odin to manage stateful workloads like Cassandra redis and so on",
    "start": "1730799",
    "end": "1737000"
  },
  {
    "text": "as ay company we have decided to converge on kubernetes as a unified platform for all of these workloads so",
    "start": "1737000",
    "end": "1743679"
  },
  {
    "text": "yeah watch this space for the next year or two uh we'll have more updates on that lastly I want to thank all the",
    "start": "1743679",
    "end": "1751000"
  },
  {
    "text": "teams mentioned here for their support and work during this uh migration as well as the kids Community you guys are",
    "start": "1751000",
    "end": "1757679"
  },
  {
    "text": "awesome Aur and I are merely representing your hard work so thank you very much for your efforts and with that",
    "start": "1757679",
    "end": "1763760"
  },
  {
    "text": "we can go to Q",
    "start": "1763760",
    "end": "1769320"
  },
  {
    "text": "yeah uh if I understand correctly during your meses to kubernetes migration you spread the workloads per instance across",
    "start": "1781240",
    "end": "1789039"
  },
  {
    "text": "mesas and kubernetes is that correct if that's so uh how does uh an instance in",
    "start": "1789039",
    "end": "1794559"
  },
  {
    "text": "mesos discover the parts in in kubernetes and and vice versa service",
    "start": "1794559",
    "end": "1799919"
  },
  {
    "text": "how does your service Discovery looks like between mesos and and kubernetes during the migration process so we have a service mesh I mean",
    "start": "1799919",
    "end": "1806720"
  },
  {
    "text": "so basically we have a service mesh and we integrate the service same service mesh with both misos and kubernetes um",
    "start": "1806720",
    "end": "1813679"
  },
  {
    "text": "and the integration looks exactly the same so the pods in kubernetes as well as",
    "start": "1813679",
    "end": "1819799"
  },
  {
    "text": "containers and misos are visible to each other through the service mesh all right",
    "start": "1819799",
    "end": "1824840"
  },
  {
    "text": "thank you hi uh thank you for the presentation was",
    "start": "1824840",
    "end": "1830080"
  },
  {
    "text": "very informative so my question I have one question and then to follow up based on the same question so um I remember I",
    "start": "1830080",
    "end": "1836840"
  },
  {
    "text": "think I remember you said you were using 48 core hosts um so and then you also made some other optimizations with u the",
    "start": "1836840",
    "end": "1844080"
  },
  {
    "text": "API limits Etc burst Etc so how do you define the trade-off between throwing",
    "start": "1844080",
    "end": "1849880"
  },
  {
    "text": "more resources and actually optimizing for the various parameters within the uh",
    "start": "1849880",
    "end": "1856960"
  },
  {
    "text": "object objects and the followup based on that is um since you said it's a 48 core",
    "start": "1856960",
    "end": "1862880"
  },
  {
    "text": "host how do you again Define the trade-off between having let's say 12 eight core uh eight core hosts um which",
    "start": "1862880",
    "end": "1869880"
  },
  {
    "text": "give might give you more resiliency probably versus single large host to run",
    "start": "1869880",
    "end": "1877000"
  },
  {
    "text": "a big cluster like that um so those are two of my question that's an excellent question so um so the way so the way so",
    "start": "1877000",
    "end": "1884799"
  },
  {
    "text": "the way we think about it is that we want as large hor as possible because larger the hor uh the less fragmentation",
    "start": "1884799",
    "end": "1891200"
  },
  {
    "text": "we see uh I mean it's the same thing like we have large clusters as well as large host within the cluster so as to",
    "start": "1891200",
    "end": "1896320"
  },
  {
    "text": "reduce fragmentation right now why we can't go just to like let's say like a thousand core machine or you know like",
    "start": "1896320",
    "end": "1902399"
  },
  {
    "text": "something like that I mean uh the reason is that uh we have host agents which run including the service mesh the metrics",
    "start": "1902399",
    "end": "1909679"
  },
  {
    "text": "logging and so on and as we keep scaling the host up uh the demons and the kernel itself needs to keep scaling up uh we",
    "start": "1909679",
    "end": "1917159"
  },
  {
    "text": "found some issues as we move forward and we are fixing them so yes we use 48 core",
    "start": "1917159",
    "end": "1923000"
  },
  {
    "text": "machines right now we are already moving to 96 we hope to move to 12 and 256 um",
    "start": "1923000",
    "end": "1928559"
  },
  {
    "text": "over time uh but it requires like a careful scaling of the Demons on the",
    "start": "1928559",
    "end": "1933639"
  },
  {
    "text": "host for example Docker in scale so when we moved to kubernetes we got container D and that scales pretty well uh so",
    "start": "1933639",
    "end": "1940919"
  },
  {
    "text": "moving to kubernetes allowed us to also be able to move to a larger machine okay and do you also are at your control",
    "start": "1940919",
    "end": "1947919"
  },
  {
    "text": "plane um on larger machines as well uh the control plane um so the control",
    "start": "1947919",
    "end": "1955200"
  },
  {
    "text": "plane is so basically what we did is as part of our benchmarking we figured out what is the minimum size of the machine",
    "start": "1955200",
    "end": "1961880"
  },
  {
    "text": "and what is the dis iops and what's the memory and so on like what is the best instance size instance type which fits",
    "start": "1961880",
    "end": "1969360"
  },
  {
    "text": "that control plan and that's what we have chosen right so we want to keep the cost low uh so we chose",
    "start": "1969360",
    "end": "1974880"
  },
  {
    "text": "the cheapest possible machine machine which gives us the maximum um which allows us to reach the",
    "start": "1974880",
    "end": "1981279"
  },
  {
    "text": "scale we want to reach thank",
    "start": "1981279",
    "end": "1986840"
  },
  {
    "text": "you thanks for the talk I'm curious to hear about the optimization that you did on the reconciliation you were saying",
    "start": "1987159",
    "end": "1993799"
  },
  {
    "text": "that you have the recing period every eight hours and then you did some optimization on it yeah so you're",
    "start": "1993799",
    "end": "1999559"
  },
  {
    "text": "talking about the reconciliation uh like how we did it yeah the reing period to not have to wait like eight hours right",
    "start": "1999559",
    "end": "2006559"
  },
  {
    "text": "right so so by default informers have a setting which says which is kind of a",
    "start": "2006559",
    "end": "2012120"
  },
  {
    "text": "little misleading it says reconcile which I thought would like relist but it",
    "start": "2012120",
    "end": "2017720"
  },
  {
    "text": "doesn't relist uh it actually just replays its cache every there's like a randomized timer between 8 or 12 hours",
    "start": "2017720",
    "end": "2024559"
  },
  {
    "text": "anytime this happens between 8 to 12 hours it all events are replayed uh so",
    "start": "2024559",
    "end": "2029960"
  },
  {
    "text": "for example if a deployment creation event is part of that uh new events list",
    "start": "2029960",
    "end": "2035919"
  },
  {
    "text": "and it is not acted upon by a controller at that same time because for example we",
    "start": "2035919",
    "end": "2041360"
  },
  {
    "text": "had a leadership failover right that's when this becomes a problem because then",
    "start": "2041360",
    "end": "2047080"
  },
  {
    "text": "you have to wait for the whole resync to happen every 12 hours so your deployment",
    "start": "2047080",
    "end": "2052158"
  },
  {
    "text": "got created so the developer is like hey I created my deployment why is why are you not doing anything with it so that's",
    "start": "2052159",
    "end": "2057599"
  },
  {
    "text": "why we want to reconcile high level objects uh Force reconcile them every 15 minutes okay",
    "start": "2057599",
    "end": "2065440"
  },
  {
    "text": "thanks hello thank you for the talk I was just curious like if you can share",
    "start": "2065440",
    "end": "2070679"
  },
  {
    "text": "more about your experience moving from mesos to kubernetes about resource uh enforcement resource limit enforcement",
    "start": "2070679",
    "end": "2077040"
  },
  {
    "text": "so request and limits uh in the mesos platform versus on",
    "start": "2077040",
    "end": "2082480"
  },
  {
    "text": "kubernetes um so can you repeat so you talked about the request limit in",
    "start": "2082480",
    "end": "2087560"
  },
  {
    "text": "kubernetes there is like request and limits like so that are enforc essentially at the cubet level using CFS",
    "start": "2087560",
    "end": "2094000"
  },
  {
    "text": "I'm just curious like what was your experience like moving from mes so um so in misos we didn't use",
    "start": "2094000",
    "end": "2102000"
  },
  {
    "text": "revocable if if you know what that means right so uh so we didn't use suuk overc",
    "start": "2102000",
    "end": "2107240"
  },
  {
    "text": "commitment as much so we used to use Su over commitment before but we disabled it long some time back so when we move",
    "start": "2107240",
    "end": "2113920"
  },
  {
    "text": "to kubernetes essentially it is equals request equals to limit is what we use because we don't have overc commitment",
    "start": "2113920",
    "end": "2120119"
  },
  {
    "text": "of CPU uh moving forward if you have to enable over commitment we will do it on a Case by case basis um",
    "start": "2120119",
    "end": "2128560"
  },
  {
    "text": "what else about yeah no that that's fine yeah that's cool yeah overall I think like like kubernetes is a more I mean",
    "start": "2128560",
    "end": "2135240"
  },
  {
    "text": "like the just the amount of features available in kubernetes as compared to misos are huge and overall the migration",
    "start": "2135240",
    "end": "2140720"
  },
  {
    "text": "has been like pretty good um for our for security as well as uh numerous other",
    "start": "2140720",
    "end": "2146839"
  },
  {
    "text": "reasons awesome thank",
    "start": "2146839",
    "end": "2150480"
  },
  {
    "text": "you thank you thank you for your thank you so",
    "start": "2155800",
    "end": "2161119"
  },
  {
    "text": "much",
    "start": "2161119",
    "end": "2164119"
  }
]