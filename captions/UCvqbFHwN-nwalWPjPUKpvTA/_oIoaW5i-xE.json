[
  {
    "text": "hello everybody good morning uh my name is Justin Santa",
    "start": "320",
    "end": "5440"
  },
  {
    "text": "Barbara i am a software engineer at Google i work on a number of open source projects related to Kubernetes uh one of",
    "start": "5440",
    "end": "12000"
  },
  {
    "text": "which is Config Connector and we are going to be talking to you today about um how we are using AI to generate uh",
    "start": "12000",
    "end": "19920"
  },
  {
    "text": "those thousand controllers that make up Config Connector this is Walter hello i'm Walter Fender uh I also work on uh",
    "start": "19920",
    "end": "27439"
  },
  {
    "text": "several open source Kubernetes projects uh I'm both the software engineer and the EM on the config connector project",
    "start": "27439",
    "end": "34960"
  },
  {
    "text": "as well as a few others awesome so we're not really going",
    "start": "34960",
    "end": "40160"
  },
  {
    "text": "to dwell on config connector too much this is really about the AI and Kubernetes controllers but we are going to give a little bit of uh sort of our",
    "start": "40160",
    "end": "46960"
  },
  {
    "text": "due diligence about you know how did we come to a place where we ended up having to write a thousand Kubernetes",
    "start": "46960",
    "end": "53280"
  },
  {
    "text": "controllers so the basic idea of config connector is",
    "start": "53280",
    "end": "59039"
  },
  {
    "text": "we have rest GCP APIs to manage all the Google resources uh it's very simple",
    "start": "59039",
    "end": "65600"
  },
  {
    "text": "similar to what you would do for many of the other cloud providers but those REST",
    "start": "65600",
    "end": "71200"
  },
  {
    "text": "APIs are not inherently Kubernetes native and so config connector is our",
    "start": "71200",
    "end": "77119"
  },
  {
    "text": "attempt to write a KRM Kubernetes resource model that allows you to",
    "start": "77119",
    "end": "82320"
  },
  {
    "text": "control all the Google APIs and there are similar projects like ACK for AWS",
    "start": "82320",
    "end": "88320"
  },
  {
    "text": "and ASO for Azure that have the same problem and this means that if there are",
    "start": "88320",
    "end": "93840"
  },
  {
    "text": "and there are a thousand different REST APIs to control various things we need",
    "start": "93840",
    "end": "99360"
  },
  {
    "text": "to have a thousand controllers uh each of which have their individual",
    "start": "99360",
    "end": "105200"
  },
  {
    "text": "business logic that is going to to allow you to do this so you know we have one",
    "start": "105200",
    "end": "110399"
  },
  {
    "text": "CRD and controller for SQL instance we have another one for IM service account",
    "start": "110399",
    "end": "118039"
  },
  {
    "text": "so then the question is well why not build on Terraform and we tried let me start with we tried uh and the problem",
    "start": "118240",
    "end": "126079"
  },
  {
    "text": "we get is it ends up being a magic machine you have one core place that is",
    "start": "126079",
    "end": "131680"
  },
  {
    "text": "the guts of your controller and anytime you make changes to one resource to make",
    "start": "131680",
    "end": "139520"
  },
  {
    "text": "something work some other resource breaks and that magic machine is really",
    "start": "139520",
    "end": "145440"
  },
  {
    "text": "hard and the more resources you try to do it this way the more things break",
    "start": "145440",
    "end": "150480"
  },
  {
    "text": "every time you make a change and it becomes just too complicated having this",
    "start": "150480",
    "end": "156879"
  },
  {
    "text": "one very huge intricate magic machine that is the heart of your controller and",
    "start": "156879",
    "end": "163840"
  },
  {
    "text": "and we want to emphasize that is not a knock on Terraform this was true also of other things we tried as well it is it is the nature of the magic machine that",
    "start": "163840",
    "end": "170400"
  },
  {
    "text": "we become keepers of the magic machine a customer will say \"Please fix this simple problem which I could fix in one",
    "start": "170400",
    "end": "175920"
  },
  {
    "text": "line.\" And we're like \"Well first we have to go ask the magic machine to do this and this and this.\" And it becomes a week-long uh escapade as it",
    "start": "175920",
    "end": "183720"
  },
  {
    "text": "were so how do we go from magic machines to LLMs and you know as many of us who've",
    "start": "183720",
    "end": "190879"
  },
  {
    "text": "used LLMs like aren't they just the ultimate magic machine i ask it",
    "start": "190879",
    "end": "196400"
  },
  {
    "text": "something i have no idea how it's processing it and then it gives me a result isn't that the worst observation",
    "start": "196400",
    "end": "202159"
  },
  {
    "text": "of what a magic machine is and I think the key here the the big",
    "start": "202159",
    "end": "207200"
  },
  {
    "text": "breakthrough that enabled us to leverage the ultimate magic machine of LLMs is this idea uh Walter actually created it",
    "start": "207200",
    "end": "214000"
  },
  {
    "text": "of sort of code as the artifact um the problem with the Terraform or DCL magic",
    "start": "214000",
    "end": "220000"
  },
  {
    "text": "machine was that that complexity the complexity of the magic machine existed at runtime we would ship it and",
    "start": "220000",
    "end": "226000"
  },
  {
    "text": "customers would run that very complicated machine um and that runtime complexity made for project complexity",
    "start": "226000",
    "end": "233040"
  },
  {
    "text": "and what we wanted instead was we wanted lots of simple code it's okay to have lots of code as long as each individual",
    "start": "233040",
    "end": "240239"
  },
  {
    "text": "piece is isolated and simple when we want to make a simple change we simply make the change to the simple code so we",
    "start": "240239",
    "end": "247280"
  },
  {
    "text": "accept that we have a lot more code as long as each individual file is simpler and self-contained um and it is okay to",
    "start": "247280",
    "end": "254959"
  },
  {
    "text": "continue to have magic to create that lots and lots of simple code that is an",
    "start": "254959",
    "end": "260639"
  },
  {
    "text": "an asset and now we can leverage the magic machine for our own efficiency but as long as we aren't shipping it and",
    "start": "260639",
    "end": "266960"
  },
  {
    "text": "requiring customers to run that magic machine we feel like we're in a much better place and the the important thing",
    "start": "266960",
    "end": "272639"
  },
  {
    "text": "here is um we recognize also that the nature of the magic machine will improve over time like we know that LLMs are",
    "start": "272639",
    "end": "279280"
  },
  {
    "text": "going to get better tomorrow we know that you know we will write better tooling to create particular problems we",
    "start": "279280",
    "end": "285199"
  },
  {
    "text": "have and we we have people that are going to fix problems also um both you know ourselves on the config connector",
    "start": "285199",
    "end": "291520"
  },
  {
    "text": "team and the community here right and we want to make sure that everyone machines",
    "start": "291520",
    "end": "297440"
  },
  {
    "text": "humans tooling can all collaborate and work together on that codebase so a big requirement is that all of those tools",
    "start": "297440",
    "end": "305199"
  },
  {
    "text": "and particularly the AI tools must be able to work with the code as it exists today they must not assume that they",
    "start": "305199",
    "end": "311520"
  },
  {
    "text": "created all the code but they must just be able to make small incremental improvements to the code as it exists",
    "start": "311520",
    "end": "316960"
  },
  {
    "text": "it's the idea that the code is the primary artifact the code is what defines our product and that is what we",
    "start": "316960",
    "end": "322320"
  },
  {
    "text": "must work with not some abstraction on the code",
    "start": "322320",
    "end": "327400"
  },
  {
    "text": "so we've mentioned before this isn't the first time and we actually came up with",
    "start": "327400",
    "end": "332960"
  },
  {
    "text": "this idea of pull the magic machine to build time from run from production",
    "start": "332960",
    "end": "338120"
  },
  {
    "text": "runtime before we even got to LLMs and so the first thing I tried to build was",
    "start": "338120",
    "end": "344800"
  },
  {
    "text": "an abased tooling right that that standard classical approach but that a",
    "start": "344800",
    "end": "350800"
  },
  {
    "text": "is complicated the processes are complicated and it takes a huge huge",
    "start": "350800",
    "end": "356080"
  },
  {
    "text": "amount of human investment the generated code that we got out of it was better than with it's",
    "start": "356080",
    "end": "363520"
  },
  {
    "text": "nice simple code that we get in production but it's a very complicated build system to try and maintain and",
    "start": "363520",
    "end": "370960"
  },
  {
    "text": "every time something new happens you you have to spend the time to make it better",
    "start": "370960",
    "end": "377120"
  },
  {
    "text": "it was an improvement but it wasn't enough of an improvement to meet that thousand",
    "start": "377120",
    "end": "383759"
  },
  {
    "text": "uh controller goal that we had and coincidentally or happily enough",
    "start": "383759",
    "end": "390720"
  },
  {
    "text": "this was actually around the time when LLM started appearing in the wild as it were the chat GPT moment coincided with",
    "start": "390720",
    "end": "397360"
  },
  {
    "text": "a lot of this uh exploration that we were doing um and one of the great things about LLMs is most of them happen",
    "start": "397360",
    "end": "404319"
  },
  {
    "text": "to be pretty good at manipulating code sort of straight out of the box so we don't need to do a lot of that heavy",
    "start": "404319",
    "end": "409919"
  },
  {
    "text": "lifting that we needed to do with the classical tooling um another thing you know they're they're good um but we also",
    "start": "409919",
    "end": "417520"
  },
  {
    "text": "recognized that we didn't need the 100% reliability that we get with based",
    "start": "417520",
    "end": "422960"
  },
  {
    "text": "tooling we can accept the idea that you know this this can sometimes produce absolute garbage and sometimes produce",
    "start": "422960",
    "end": "429680"
  },
  {
    "text": "wonderful results we just run it twice right uh and that is a great asset because we aren't running that magic",
    "start": "429680",
    "end": "435759"
  },
  {
    "text": "machine in production it's it's only ourselves that has to deal with the you know moments of brilliance and moments",
    "start": "435759",
    "end": "441199"
  },
  {
    "text": "of less brilliance so that sounds great problem",
    "start": "441199",
    "end": "448240"
  },
  {
    "text": "solved right well there are some interesting problems we got to a little bit of it i mean the first thing and it",
    "start": "448240",
    "end": "454160"
  },
  {
    "text": "some of the problems are the LLMs and some of it is actually our assumptions",
    "start": "454160",
    "end": "459680"
  },
  {
    "text": "our fears our interpretations of the LLMs llms are non-deterministic we can",
    "start": "459680",
    "end": "465599"
  },
  {
    "text": "make them deterministic but you lose a lot of the power when you do it and so",
    "start": "465599",
    "end": "470639"
  },
  {
    "text": "for those of us with a very classical background in computer science we're",
    "start": "470639",
    "end": "475680"
  },
  {
    "text": "used to you know the A or the other deterministic tools it's kind of scary",
    "start": "475680",
    "end": "481840"
  },
  {
    "text": "going to a non-deterministic solution a lot of the our",
    "start": "481840",
    "end": "487400"
  },
  {
    "text": "assumptions just get thrown out the window it's like if I have this new LLM tooling and I run a test and the test",
    "start": "487400",
    "end": "494520"
  },
  {
    "text": "fails does that mean my LLM is wrong or does it just mean that I happen to hit a",
    "start": "494520",
    "end": "500479"
  },
  {
    "text": "bad path through the LLM and if I just run it again I'll get something better how many more times do I run it you know",
    "start": "500479",
    "end": "509080"
  },
  {
    "text": "um excuse me if I make a change you know and I run the test and the test",
    "start": "509080",
    "end": "514680"
  },
  {
    "text": "fails did my fix not work i mean is is was that prompt change not good how many",
    "start": "514680",
    "end": "520479"
  },
  {
    "text": "times do I have to run it right so that's a that's a problem that we need to start learning how to deal with and",
    "start": "520479",
    "end": "527519"
  },
  {
    "text": "it's it's not just a problem on how to deal with it but when you do when you have an engineering team or teammates",
    "start": "527519",
    "end": "534560"
  },
  {
    "text": "that are trying to use this tool and they try it and the immediate response is well I tried it once and it didn't",
    "start": "534560",
    "end": "540640"
  },
  {
    "text": "work well okay but you need to try it again right and so there is human elements to this as well our intuition",
    "start": "540640",
    "end": "547839"
  },
  {
    "text": "of how it works is also an interesting problem right we think well it didn't work this one time so I'm going to build",
    "start": "547839",
    "end": "554399"
  },
  {
    "text": "this heavy solution on how to parse YAML well I don't know we did this right and",
    "start": "554399",
    "end": "559839"
  },
  {
    "text": "then I pulled all that YAML parsing out and I went I'm just going to pass the YAML in and see whether or not the LLM",
    "start": "559839",
    "end": "566080"
  },
  {
    "text": "can deal with it and surprise it can right so don't assume that it can't do",
    "start": "566080",
    "end": "571519"
  },
  {
    "text": "things even if you try it once and it doesn't and and start simple see how much of the heavy lifting you can make",
    "start": "571519",
    "end": "577839"
  },
  {
    "text": "the LLM do for you also remember that you know the the these models keep",
    "start": "577839",
    "end": "583519"
  },
  {
    "text": "working and they keep the people building them keep making them better if you wait you will get better results",
    "start": "583519",
    "end": "589839"
  },
  {
    "text": "just by going to the newer model and you hear this again and again but there are other things you can do while you're",
    "start": "589839",
    "end": "595040"
  },
  {
    "text": "waiting you can come up with the right answer right you can iterate you can generate the controller that is of the",
    "start": "595040",
    "end": "601519"
  },
  {
    "text": "form that you want as a result that's then can be go back into the learning that reinforces and that helps make that",
    "start": "601519",
    "end": "609040"
  },
  {
    "text": "next model better i will also say there is something you may try today and it",
    "start": "609040",
    "end": "614959"
  },
  {
    "text": "doesn't work that doesn't mean it won't work on the next model right so one of the",
    "start": "614959",
    "end": "620000"
  },
  {
    "text": "things we found is it's very useful to flag our experiments so that they're easier to rerun on that next model",
    "start": "620000",
    "end": "627600"
  },
  {
    "text": "because we've found things that didn't work on earlier models do work on later",
    "start": "627600",
    "end": "634000"
  },
  {
    "text": "models it's also important that you understand your requirements right we've",
    "start": "634519",
    "end": "639760"
  },
  {
    "text": "talked a little about this for our use case do you need predictable results in production you know it's possible to",
    "start": "639760",
    "end": "647040"
  },
  {
    "text": "generate an LLM that will give you predictable results in in production it won't run over the nice kitten um but",
    "start": "647040",
    "end": "655519"
  },
  {
    "text": "that's a lot of extra work and do you need to run the LLM at runtime so we",
    "start": "655519",
    "end": "661120"
  },
  {
    "text": "don't right and so we can get predict more predictable results at production",
    "start": "661120",
    "end": "666320"
  },
  {
    "text": "time by running the LLM at build time and that you know and then have a much",
    "start": "666320",
    "end": "672320"
  },
  {
    "text": "easier time determining which were the right results do you need the LLM to make a rapid decision we don't we do",
    "start": "672320",
    "end": "678800"
  },
  {
    "text": "things at build time that means it's fine for our build system to go away while I'm sleeping and give me the",
    "start": "678800",
    "end": "685600"
  },
  {
    "text": "results back when I get up right do we need to generate predictable results",
    "start": "685600",
    "end": "691120"
  },
  {
    "text": "well yeah I do right i mean and but it doesn't have to be every time so to to",
    "start": "691120",
    "end": "697440"
  },
  {
    "text": "Justin's earlier point I can run it twice i can run it thrice as long as I have some way to validate which ones of",
    "start": "697440",
    "end": "705279"
  },
  {
    "text": "the controllers that it wrote were the right ones i think one of the most interesting",
    "start": "705279",
    "end": "712160"
  },
  {
    "text": "things I've read about AI is is uh Rich Sutton's The Bitter Lesson it is a great essay we've linked it there it's you can",
    "start": "712160",
    "end": "718079"
  },
  {
    "text": "Google it um it's mostly about AI research but it is this idea that I",
    "start": "718079",
    "end": "724640"
  },
  {
    "text": "think applies generally like to software engineering and our usage of AI also and the idea is um you as a researcher or",
    "start": "724640",
    "end": "732480"
  },
  {
    "text": "someone wanting to use AI as a person will invest you know your your knowledge and try to get better results out of",
    "start": "732480",
    "end": "739200"
  },
  {
    "text": "today's AI system and you will get good results um however uh you will be",
    "start": "739200",
    "end": "744720"
  },
  {
    "text": "overtaken by the next generation of LLMs by someone that puts more GPU use in a box or uses the next generation of",
    "start": "744720",
    "end": "750560"
  },
  {
    "text": "hardware and essentially what we have internalized here is the need to accept",
    "start": "750560",
    "end": "756160"
  },
  {
    "text": "that we have to accept the bitter lesson and we have to adopt strategies that do not fall down in the face of the bitter",
    "start": "756160",
    "end": "761760"
  },
  {
    "text": "lesson and in general what that means is um basically avoiding any investment in",
    "start": "761760",
    "end": "768079"
  },
  {
    "text": "sort of model specific optimization you know you have to do your prompt tweaking but we're not going to spend weeks or",
    "start": "768079",
    "end": "774000"
  },
  {
    "text": "months tweaking the prompts because we know there's a new LLM coming and that new LLM will probably respond",
    "start": "774000",
    "end": "779519"
  },
  {
    "text": "differently to the prompts and all your work will then have to be thrown away so instead focus on techniques things like",
    "start": "779519",
    "end": "785680"
  },
  {
    "text": "how do we get the right information into the context like LM are still obey the",
    "start": "785680",
    "end": "790959"
  },
  {
    "text": "basic laws of of information theory and computer science you know garbage in garbage out so if you want something to",
    "start": "790959",
    "end": "796880"
  },
  {
    "text": "appear in the output it sure is a lot easier to make sure that information appears in the input to the LLM right it",
    "start": "796880",
    "end": "802240"
  },
  {
    "text": "is much easier to do that than to rely on the training um that will be true of every foreseeable generation of LLMs to",
    "start": "802240",
    "end": "808880"
  },
  {
    "text": "come um can we break down big problems into lots of little problems right that",
    "start": "808880",
    "end": "814240"
  },
  {
    "text": "smaller problems should be easier problems and easier problems should be able to be solved more readily um can we",
    "start": "814240",
    "end": "819680"
  },
  {
    "text": "get the feedback loops going um so that we can improve um both each the way we",
    "start": "819680",
    "end": "825839"
  },
  {
    "text": "do those strategies and sort of overall understand where we're going and how how we're doing and where we need to use",
    "start": "825839",
    "end": "831600"
  },
  {
    "text": "more people and where we need to use more LLMs can we you know use the LLM for what they do today what they're good",
    "start": "831600",
    "end": "837360"
  },
  {
    "text": "at like today that's exposing tools or functions to the LLMs can we basically do that and and have it work the idea is",
    "start": "837360",
    "end": "845360"
  },
  {
    "text": "we want to be in a situation where when that new model is released we see it purely as a win and not something that",
    "start": "845360",
    "end": "851360"
  },
  {
    "text": "invalidates all our work we use the broad techniques we used like with the last generation maybe we tweak the",
    "start": "851360",
    "end": "857360"
  },
  {
    "text": "prompts a little but we find that our results just get better and we aren't sort of clinging to the work we did in",
    "start": "857360",
    "end": "862480"
  },
  {
    "text": "the past so jigs and",
    "start": "862480",
    "end": "868279"
  },
  {
    "text": "interlocks we're going to build lots of little tools we're going to work out how to make those tools work together and",
    "start": "868279",
    "end": "875199"
  },
  {
    "text": "then we're going to work out how to do these things again and again as quickly as possible as reliably as possible it's",
    "start": "875199",
    "end": "882480"
  },
  {
    "text": "very similar for those of you who maybe do something like woodworking you build tools that are just there to help you",
    "start": "882480",
    "end": "889040"
  },
  {
    "text": "build other tools and to to to do some one operation really easily really",
    "start": "889040",
    "end": "894720"
  },
  {
    "text": "quickly and so one of the first jigs this is a simple one um we basically inject uh",
    "start": "894720",
    "end": "902480"
  },
  {
    "text": "variables substitute values into a prompt um we have a couple of functions that we've exposed to the LLM like write",
    "start": "902480",
    "end": "909279"
  },
  {
    "text": "a file and we basically say hey LLM go do this relatively simple task um you're",
    "start": "909279",
    "end": "914560"
  },
  {
    "text": "also allowed it's in the the three dots but it's also allowed to run G-Cloud help for example here so this is like a",
    "start": "914560",
    "end": "920720"
  },
  {
    "text": "simple task where it reads the help files for G-Cloud and tries to construct a test case right this is a simple task",
    "start": "920720",
    "end": "927120"
  },
  {
    "text": "so we think we can use simple tooling like basic prompt injection or prompt",
    "start": "927120",
    "end": "932639"
  },
  {
    "text": "templating uh with uh with tool usage um that works well for simple tasks um but",
    "start": "932639",
    "end": "938639"
  },
  {
    "text": "I think one of the one of the things that we learned is it doesn't work well for more complicated tasks so this is",
    "start": "938639",
    "end": "944560"
  },
  {
    "text": "like more of the sort of vibe coding type task can you actually write a hey LLM can you write a fuzzer can you write",
    "start": "944560",
    "end": "951040"
  },
  {
    "text": "a controller can you write a CRD right people are getting good results but in general we find that the more",
    "start": "951040",
    "end": "957040"
  },
  {
    "text": "complicated tasks need a bit more structure to get the best results from and so what we do is this we call it",
    "start": "957040",
    "end": "962880"
  },
  {
    "text": "induction is our little nickname for it but if you look at this is actual code from the KCC project this is a fuzzer um",
    "start": "962880",
    "end": "969440"
  },
  {
    "text": "for a TPU node or TPU virtual machine and you can see the important bit is these three bits at the three lines at",
    "start": "969440",
    "end": "975279"
  },
  {
    "text": "the top where we basically have some structured input data um we're saying we want to run a fuzz gen it's just a name",
    "start": "975279",
    "end": "981920"
  },
  {
    "text": "of a tool it doesn't really matter um and we are fuzzing uh a resource that has a proto representation as this",
    "start": "981920",
    "end": "987920"
  },
  {
    "text": "particular kind or type and the CRD kind is TPU virtual machine and what we do is",
    "start": "987920",
    "end": "994160"
  },
  {
    "text": "we take these as examples uh from the codebase and we sort of think of them as",
    "start": "994160",
    "end": "1000160"
  },
  {
    "text": "inputs and outputs and what we're going to do is we're going to use the LLM to give it the input and ask it for the",
    "start": "1000160",
    "end": "1006000"
  },
  {
    "text": "output for the next case so in this case you know we can degrade it into just the",
    "start": "1006000",
    "end": "1011440"
  },
  {
    "text": "annotations and the file the file which is the output and then the way we feed that to",
    "start": "1011440",
    "end": "1017440"
  },
  {
    "text": "today's LLM is by wrapping them in XML which of course makes us sad but uh you know we move past that quickly um and",
    "start": "1017440",
    "end": "1025120"
  },
  {
    "text": "then we come to the actual principle which is uh we write a couple of these fuzzers by hand uh we add those",
    "start": "1025120",
    "end": "1031880"
  },
  {
    "text": "annotations to those first files that's sort of the the first step of the inductive loop and then now when we want",
    "start": "1031880",
    "end": "1039120"
  },
  {
    "text": "to do the next one uh we write the annotations for that n plus1 case we",
    "start": "1039120",
    "end": "1044400"
  },
  {
    "text": "have from our existing codebase we scan the codebase construct those inputs and outputs and we put those into the",
    "start": "1044400",
    "end": "1050160"
  },
  {
    "text": "context and then we say all right here's the prompt here's the input we take that output from the LLM uh we correct any",
    "start": "1050160",
    "end": "1058000"
  },
  {
    "text": "issues that might exist or we choose to run it again or whatever it is we might handcode it if it's that bad um but we",
    "start": "1058000",
    "end": "1064400"
  },
  {
    "text": "commit that into the codebase with code review that becomes the n plus1 case so this is a very powerful way to get from",
    "start": "1064400",
    "end": "1071760"
  },
  {
    "text": "you know 2 to 3 to 4 to 5 to a th00and um for all of these sort of sub problems",
    "start": "1071760",
    "end": "1076960"
  },
  {
    "text": "that we are facing and it works much better than simple vibe",
    "start": "1076960",
    "end": "1082240"
  },
  {
    "text": "coding so that sounds great um how many steps are there and the answer is I",
    "start": "1085080",
    "end": "1093840"
  },
  {
    "text": "think right now about 12 15 such steps um which sounds like a lot for creating",
    "start": "1093840",
    "end": "1101760"
  },
  {
    "text": "a controller but you know there are pre-steps for some of the steps one of the first things you saw from Justin's",
    "start": "1101760",
    "end": "1108880"
  },
  {
    "text": "stuff was this G-Cloud well what was the point of doing that G-Cloud it turns out the point of doing that G-Cloud was so",
    "start": "1108880",
    "end": "1116160"
  },
  {
    "text": "that we would make HTTP requests see the HTTP response capture that HTTP log",
    "start": "1116160",
    "end": "1122720"
  },
  {
    "text": "that's one step the next step is now that we understand what a request response looks like for that resource",
    "start": "1122720",
    "end": "1129919"
  },
  {
    "text": "now we can tell have the LLM read the templatized version and sample version",
    "start": "1129919",
    "end": "1135840"
  },
  {
    "text": "of the induct of the the induction loop and the logs and use those two pieces of",
    "start": "1135840",
    "end": "1141679"
  },
  {
    "text": "information to then create the mock right and so what we have is breaking",
    "start": "1141679",
    "end": "1147440"
  },
  {
    "text": "down and we we can do things like when we look generate that HTTP log there are",
    "start": "1147440",
    "end": "1153840"
  },
  {
    "text": "basic checks we can make on it to determine if there are a bunch of 404s",
    "start": "1153840",
    "end": "1159360"
  },
  {
    "text": "then something went wrong and we should probably stop here and so when we get",
    "start": "1159360",
    "end": "1165440"
  },
  {
    "text": "either we need to regenerate the the G-Cloud commands or we need to rerun the the the the actual test that generated",
    "start": "1165440",
    "end": "1173360"
  },
  {
    "text": "those things and that's where we get this whole interlock right so that that check for 404 is one of the interlocks",
    "start": "1173360",
    "end": "1180720"
  },
  {
    "text": "and we can run this stuff in parallel and the the the requests that get a 404",
    "start": "1180720",
    "end": "1186400"
  },
  {
    "text": "are paused we capture the status we go and investigate and other of the",
    "start": "1186400",
    "end": "1192320"
  },
  {
    "text": "parallel change which didn't hit a 404 get to proceed yeah and I mean I think this is",
    "start": "1192320",
    "end": "1198480"
  },
  {
    "text": "this is a hypothesis but I think it's quite important like we're not just breaking down the the tasks because the",
    "start": "1198480",
    "end": "1203919"
  },
  {
    "text": "smaller tasks are easier it is the idea that you know we go over here with this task we go over here with the other task",
    "start": "1203919",
    "end": "1210559"
  },
  {
    "text": "if there is a hallucination on one of the two we hypothesize that it is unlikely that these two different tasks",
    "start": "1210559",
    "end": "1216720"
  },
  {
    "text": "will both fail in a way that's come together in a way that still works right so the hallucination on one is c",
    "start": "1216720",
    "end": "1224000"
  },
  {
    "text": "hopefully caught by the correct output on the other or at least we think there",
    "start": "1224000",
    "end": "1229440"
  },
  {
    "text": "are many ways for the LLM to hallucinate but ideally there is only one way for them to to get it right and so that is",
    "start": "1229440",
    "end": "1235440"
  },
  {
    "text": "the that is the core of the hypothesis of this interlock idea break down the problems lots of independent steps if",
    "start": "1235440",
    "end": "1242240"
  },
  {
    "text": "they can mesh together hopefully they if they mesh together that is evidence that they are meshing",
    "start": "1242240",
    "end": "1248480"
  },
  {
    "text": "correctly so iterative improvement tools",
    "start": "1249720",
    "end": "1256679"
  },
  {
    "text": "um there are varying things we can do there are varying things we've built along the way you know we have examples",
    "start": "1256679",
    "end": "1263440"
  },
  {
    "text": "of things like well I have an LLM i want it to be able to generate a CR for my",
    "start": "1263440",
    "end": "1269919"
  },
  {
    "text": "this newly generated CRD type so we build we build something that can go",
    "start": "1269919",
    "end": "1275039"
  },
  {
    "text": "talk to Kubernetes download the C the open API schema and make that available",
    "start": "1275039",
    "end": "1280240"
  },
  {
    "text": "to the LLM i can then have it look at the mocks make the mocks available to",
    "start": "1280240",
    "end": "1285840"
  },
  {
    "text": "the LLM and when we start building you know I can building these steps the here's the build here's the build output",
    "start": "1285840",
    "end": "1292720"
  },
  {
    "text": "each of these things that we make available to the LLM is one more thing that the LLM can then use to not",
    "start": "1292720",
    "end": "1300640"
  },
  {
    "text": "hallucinate to understand when it had a problem and I think one of the interesting",
    "start": "1300640",
    "end": "1306640"
  },
  {
    "text": "things is I think this is you know this is the cursor workflow this is the agent agentic workflow we've actually found",
    "start": "1306640",
    "end": "1312080"
  },
  {
    "text": "this one hasn't worked as well for us yet as some of the other uh workflows like the iterative loop um this is the",
    "start": "1312080",
    "end": "1318960"
  },
  {
    "text": "one we sort of expect to we we we should be able to give the LLM lots of tools ask it to fix a compiler error and have",
    "start": "1318960",
    "end": "1325520"
  },
  {
    "text": "it work in our experience with the way we've built it it doesn't which is all credit to other tools right of course um",
    "start": "1325520",
    "end": "1332000"
  },
  {
    "text": "we haven't had as good results and so this is where like humans typically have to get involved today we obviously hope",
    "start": "1332000",
    "end": "1338400"
  },
  {
    "text": "that you know we will figure it out or the LLMs will figure it out and that will go away or get lessened but today",
    "start": "1338400",
    "end": "1344640"
  },
  {
    "text": "like a lot of the basic fixing of compiler errors uh has to be fixed by a",
    "start": "1344640",
    "end": "1350000"
  },
  {
    "text": "person going in and fixing those things",
    "start": "1350000",
    "end": "1354280"
  },
  {
    "text": "so yeah so we have um thinking also about the way cursor works right that is",
    "start": "1355240",
    "end": "1360799"
  },
  {
    "text": "a a UI sidebar and we don't want our engineers to have to go and type in into",
    "start": "1360799",
    "end": "1366080"
  },
  {
    "text": "a little uh sidebar every time we have built a automated pipeline to basically",
    "start": "1366080",
    "end": "1372320"
  },
  {
    "text": "combine those complicated or large large number of steps into a repeatable",
    "start": "1372320",
    "end": "1378480"
  },
  {
    "text": "pattern which Walter will describe so decompose combined strategies um we've kind of gotten a",
    "start": "1378480",
    "end": "1386000"
  },
  {
    "text": "breakdown here so we've said there are 12 15 steps that",
    "start": "1386000",
    "end": "1391120"
  },
  {
    "text": "need to be run through to generate one controller a lot of this needs to be metadata driven is actually probably",
    "start": "1391120",
    "end": "1396880"
  },
  {
    "text": "what I would say is there was a one-time step before we even get here which was asking the LLM to generate the metadata",
    "start": "1396880",
    "end": "1403600"
  },
  {
    "text": "for a thousand resources then I may have spent more hours than I want to recollect actually getting rid of the",
    "start": "1403600",
    "end": "1410760"
  },
  {
    "text": "hallucinations in that metadata uh but we get the metadata and",
    "start": "1410760",
    "end": "1416760"
  },
  {
    "text": "now we have a description of you know varying things it can be the uh git",
    "start": "1416760",
    "end": "1423039"
  },
  {
    "text": "branch that we're going to make each resource in the name of the resource where it can find the protoile for the",
    "start": "1423039",
    "end": "1430080"
  },
  {
    "text": "resource uh all of these sort of things and now",
    "start": "1430080",
    "end": "1435360"
  },
  {
    "text": "we can basically just say hey it takes maybe 10 minutes to run one step on one",
    "start": "1435360",
    "end": "1442400"
  },
  {
    "text": "resource but I have a thousand of them to do and if we've written that first step jig correctly I can just go to bed",
    "start": "1442400",
    "end": "1450480"
  },
  {
    "text": "and come back the following morning and see that it's run that first jig across",
    "start": "1450480",
    "end": "1456080"
  },
  {
    "text": "a thousand resources and maybe 600 of them were successful right and now I can",
    "start": "1456080",
    "end": "1464240"
  },
  {
    "text": "kick it off running the second step on the 600 that were successful while I start debugging went what went wrong on",
    "start": "1464240",
    "end": "1471120"
  },
  {
    "text": "the 400 where it didn't work and in this way we can get much better",
    "start": "1471120",
    "end": "1476840"
  },
  {
    "text": "throughput to get things actually working and the other thing is not all",
    "start": "1476840",
    "end": "1482000"
  },
  {
    "text": "of these steps have to be LLM based right we can talk about hybrid solutions",
    "start": "1482000",
    "end": "1487559"
  },
  {
    "text": "so if I've written something and I can like the the rest or the go",
    "start": "1487559",
    "end": "1495480"
  },
  {
    "text": "structure for what I want my CRD to look like i don't need the LLM to generate",
    "start": "1495480",
    "end": "1500640"
  },
  {
    "text": "the open API schema for me i can just go and have CRD generators there are",
    "start": "1500640",
    "end": "1506880"
  },
  {
    "text": "several of them out there that will do that next step for me uh and you can see here there are a",
    "start": "1506880",
    "end": "1515440"
  },
  {
    "text": "lot of steps i'm not going to necessarily go through all of them but some of these steps are we we've kind of",
    "start": "1515440",
    "end": "1520559"
  },
  {
    "text": "coded as they're ones that the LLM does we've already talked a bit about the compile errors there are classes of",
    "start": "1520559",
    "end": "1526640"
  },
  {
    "text": "compile errors that we're actually gotten the LLM to do a good job fixing if it's a missing import the LLM does a",
    "start": "1526640",
    "end": "1533360"
  },
  {
    "text": "great job uh there are certainly other classes of errors that it doesn't do a great job on and you know this is why we",
    "start": "1533360",
    "end": "1540159"
  },
  {
    "text": "have two passes we also have the compile as the validator for those sort of steps",
    "start": "1540159",
    "end": "1547360"
  },
  {
    "text": "and one of the things to remember is each of these steps is some actual action and then some validation to",
    "start": "1547360",
    "end": "1555120"
  },
  {
    "text": "determine whether that action was successful or not",
    "start": "1555120",
    "end": "1560520"
  },
  {
    "text": "so how do we scale like we've we've now written a large number of controllers",
    "start": "1560640",
    "end": "1568159"
  },
  {
    "text": "and we know that sometimes LLM hallucinate so how do we create trust",
    "start": "1568159",
    "end": "1574679"
  },
  {
    "text": "right one option reviewing code right if I can review the code and the code looks good that's great but reviewing a",
    "start": "1574679",
    "end": "1582720"
  },
  {
    "text": "thousand controllers is a lot of code to review right it may be faster than",
    "start": "1582720",
    "end": "1588400"
  },
  {
    "text": "writing the code but we had the LLM write the code because it was going to take so long to write the code so we",
    "start": "1588400",
    "end": "1595520"
  },
  {
    "text": "have a new bottleneck right excuse me um and there are varying",
    "start": "1595520",
    "end": "1601039"
  },
  {
    "text": "things we can do um there are members of this community who are doing things like creating llinters for CRDs uh we have",
    "start": "1601039",
    "end": "1608480"
  },
  {
    "text": "llinters for code that that catches a certain number of the problems",
    "start": "1608480",
    "end": "1614000"
  },
  {
    "text": "uh if we can have a way to generate tests that we trust and one of the",
    "start": "1614000",
    "end": "1619279"
  },
  {
    "text": "things we have in this particular domain space is if I have a a known input and a",
    "start": "1619279",
    "end": "1625840"
  },
  {
    "text": "known output then at some level I can kind of trust that that test works by just",
    "start": "1625840",
    "end": "1632720"
  },
  {
    "text": "checking that the input and the output make sense and even if I'm generating",
    "start": "1632720",
    "end": "1637799"
  },
  {
    "text": "mocks I can I can have an AB switch where I run the test with the mocks I",
    "start": "1637799",
    "end": "1642880"
  },
  {
    "text": "run the test with the live system and if the everything lines up then I've generated a much higher confidence in my",
    "start": "1642880",
    "end": "1650000"
  },
  {
    "text": "output um I can also use a different LLM that knows how to find particular",
    "start": "1650000",
    "end": "1656240"
  },
  {
    "text": "classes of problems right and this is a great way um and we can use humans right",
    "start": "1656240",
    "end": "1663440"
  },
  {
    "text": "but the key is work out what the human is good at right so if what I want to do",
    "start": "1663440",
    "end": "1668480"
  },
  {
    "text": "is determine my CRD is an API I'm going to have to support for forever and I",
    "start": "1668480",
    "end": "1676399"
  },
  {
    "text": "want to make sure that that API seems like one that my team is willing to support then I want my best API",
    "start": "1676399",
    "end": "1682880"
  },
  {
    "text": "reviewers to go and gener to review that API because that's a very hard thing to",
    "start": "1682880",
    "end": "1688320"
  },
  {
    "text": "actually get an LLM today to to to say is this a good API or not",
    "start": "1688320",
    "end": "1696919"
  },
  {
    "text": "so automating I mean we talked about these jigs their automation what we've",
    "start": "1698279",
    "end": "1703360"
  },
  {
    "text": "been talking about is then automating that automation right it's how can I get",
    "start": "1703360",
    "end": "1709279"
  },
  {
    "text": "the computer to do all the heavy lifting uh and we have a we'll we'll send the",
    "start": "1709279",
    "end": "1715440"
  },
  {
    "text": "code the code is available it's in GitHub it's open to anyone to review but",
    "start": "1715440",
    "end": "1721360"
  },
  {
    "text": "I will say a couple of key takeaways are you So we're trying to work out how to scale up controlling you know creating",
    "start": "1721360",
    "end": "1728520"
  },
  {
    "text": "controllers make sure you're solving the problem you find uh we have re we have",
    "start": "1728520",
    "end": "1733840"
  },
  {
    "text": "made the mistake on multiple occasions of thinking oh I need to be able to run",
    "start": "1733840",
    "end": "1739279"
  },
  {
    "text": "this thing in massive project in massive parallel so I can return it quickly do I",
    "start": "1739279",
    "end": "1745039"
  },
  {
    "text": "or can I just let it run overnight um validation validation is key right like",
    "start": "1745039",
    "end": "1752960"
  },
  {
    "text": "validation is the way that I know that I'm not building on top of a car a a a",
    "start": "1752960",
    "end": "1758960"
  },
  {
    "text": "house of cards that's going to fall down and recording intention and results if",
    "start": "1758960",
    "end": "1764080"
  },
  {
    "text": "something went wrong I want to know what I asked the LLM to do i want to know what the feedback was and what went",
    "start": "1764080",
    "end": "1771360"
  },
  {
    "text": "wrong and so it's really important that you record intention and results also",
    "start": "1771360",
    "end": "1776640"
  },
  {
    "text": "there may not be one true solution right we have to deal with both the green",
    "start": "1776640",
    "end": "1782720"
  },
  {
    "text": "field new resources that we've never had to write before and so we can say what",
    "start": "1782720",
    "end": "1788159"
  },
  {
    "text": "we generate is the right behavior as long as it does what a human expects but we have some of these old resources that",
    "start": "1788159",
    "end": "1794880"
  },
  {
    "text": "we had built with Terraform and so we need to be backward compatible and so we",
    "start": "1794880",
    "end": "1800320"
  },
  {
    "text": "actually have two different paths through our generator because we have",
    "start": "1800320",
    "end": "1806240"
  },
  {
    "text": "two different problems and they need to be a little bit",
    "start": "1806240",
    "end": "1810880"
  },
  {
    "text": "different so OSS parallels you know optimize for more simpler code lots of",
    "start": "1811320",
    "end": "1818399"
  },
  {
    "text": "tests you know when we did the when Justin and I did the initial analysis",
    "start": "1818399",
    "end": "1823520"
  },
  {
    "text": "before we even got to LLMs of the A solution versus the nonAST solution our",
    "start": "1823520",
    "end": "1830799"
  },
  {
    "text": "realization was the A solution was going to be 10 times more code than the magic",
    "start": "1830799",
    "end": "1837399"
  },
  {
    "text": "module but the key is that the magic module solution all of that code you was",
    "start": "1837399",
    "end": "1844240"
  },
  {
    "text": "processed on every request so even though there's a lot more code for the a or in this case the",
    "start": "1844240",
    "end": "1851120"
  },
  {
    "text": "LLM solution the actual executing code is 100 times smaller so when you are",
    "start": "1851120",
    "end": "1858000"
  },
  {
    "text": "looking at an individual resource there is much less code for you to understand when you're trying to work out what's",
    "start": "1858000",
    "end": "1864559"
  },
  {
    "text": "going on there are a lot more resources so there's a lot more code but the the actual code for any given request is",
    "start": "1864559",
    "end": "1872000"
  },
  {
    "text": "much smaller and and that is sort of something we've worked very hard in the Kubernetes project also like we have",
    "start": "1872000",
    "end": "1877200"
  },
  {
    "text": "tried to make sure that we can merge contributions from the wonderful community um and have confidence in",
    "start": "1877200",
    "end": "1882640"
  },
  {
    "text": "doing so and we try to merge those things quickly right this is this is an open-source problem and so a AI and open",
    "start": "1882640",
    "end": "1889600"
  },
  {
    "text": "source are not actually these completely different things we have a huge head start here oh so uh just to conclude quickly",
    "start": "1889600",
    "end": "1898399"
  },
  {
    "text": "because I know we're over time uh have we finished sorry um we're not claiming to solve all the problems uh we think",
    "start": "1898399",
    "end": "1904320"
  },
  {
    "text": "there's a lot of applicability of our technique techniques that we've described here i won't read out the slide because uh we're out of time um",
    "start": "1904320",
    "end": "1910399"
  },
  {
    "text": "but if you would like to talk to us more about you know how we've uh approached this problem in our domain and you know",
    "start": "1910399",
    "end": "1916480"
  },
  {
    "text": "how you can solve it or ideas you might have that we can better solve it in our domain or from your domain please come",
    "start": "1916480",
    "end": "1922960"
  },
  {
    "text": "and find us afterwards for questions and thank you very much",
    "start": "1922960",
    "end": "1928600"
  },
  {
    "text": "[Applause]",
    "start": "1928710",
    "end": "1931649"
  }
]