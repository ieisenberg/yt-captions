[
  {
    "text": "uh welcome everyone uh we are gathered here to have Rook storage for Kubernetes",
    "start": "320",
    "end": "5920"
  },
  {
    "text": "talk i'm Dupa from Clyiso i have been working with Rukf for almost 5 years now",
    "start": "5920",
    "end": "12240"
  },
  {
    "text": "uh Madu yeah hello i'm Madu Raja i work for IBM so I'm one of the maintainer of",
    "start": "12240",
    "end": "17920"
  },
  {
    "text": "SEC CSI SECSI operator CSON also take care of integrating CSI with RO yeah",
    "start": "17920",
    "end": "24640"
  },
  {
    "text": "hello I'm Artum also working for Clyo joint project uh last year mostly",
    "start": "24640",
    "end": "30160"
  },
  {
    "text": "working on object storage part hi I'm Travis Nielsen one of the",
    "start": "30160",
    "end": "35360"
  },
  {
    "text": "original creators of the Rook project happy to hear and talking more about Rook",
    "start": "35360",
    "end": "41360"
  },
  {
    "text": "so to get started uh we'll have the agenda for the day is we'll have a short",
    "start": "41360",
    "end": "46640"
  },
  {
    "text": "introduction to Rook and Seph then we'll look into what new new features are added to Rook then we'll have a short",
    "start": "46640",
    "end": "53280"
  },
  {
    "text": "overview of SEC CSI driver then a major use case for Rook which is object storage lastly we'll see what",
    "start": "53280",
    "end": "60559"
  },
  {
    "text": "maintenance considerations you should have while working with Rookf so to get",
    "start": "60559",
    "end": "65760"
  },
  {
    "text": "started on introduction to Rook uh where did it all begin uh so coming to the",
    "start": "65760",
    "end": "71119"
  },
  {
    "text": "question if you're using storage for Kubernetes you generally go for the cloud providers but why not have the",
    "start": "71119",
    "end": "78880"
  },
  {
    "text": "storage in your data centers why the storage is not as scalable as the",
    "start": "78880",
    "end": "84240"
  },
  {
    "text": "applications you use uh is there even a storage solution that's doing it so with",
    "start": "84240",
    "end": "90159"
  },
  {
    "text": "that question we uh begin to design a storage platform and uh the",
    "start": "90159",
    "end": "95280"
  },
  {
    "text": "considerations we had was we didn't want to reinvent the wheel we just wanted to have the uh best way to run storage in",
    "start": "95280",
    "end": "103119"
  },
  {
    "text": "Kubernetes and for that we chose SE which has been running in production for almost a decade now so uh looking into",
    "start": "103119",
    "end": "111840"
  },
  {
    "text": "what Rook is uh Rook essentially is just a Kubernetes operator that automates",
    "start": "111840",
    "end": "117920"
  },
  {
    "text": "self storage for you it's completely cloudnative management so you can use CRDs uh or uh for handling deployment",
    "start": "117920",
    "end": "126479"
  },
  {
    "text": "scaling and management of storage it's easy to consume you can use PVs storage",
    "start": "126479",
    "end": "132239"
  },
  {
    "text": "classes and also have CSI driver for access and it's completely open source",
    "start": "132239",
    "end": "139040"
  },
  {
    "text": "so looking into how uh the architecture looks like it has three uh three uh",
    "start": "139040",
    "end": "145520"
  },
  {
    "text": "layers the first one is Rook which deploys and manages SE storage then we",
    "start": "145520",
    "end": "150640"
  },
  {
    "text": "have CSI which mounts and provisions storage for you and lastly uh uh and we",
    "start": "150640",
    "end": "157040"
  },
  {
    "text": "have SE which is uh the data layer where uh your application's data is getting",
    "start": "157040",
    "end": "162160"
  },
  {
    "text": "stored so uh no uh the question comes why we chose SEF uh uh the it's a comp",
    "start": "162160",
    "end": "169920"
  },
  {
    "text": "it's an open- source distributed storage solution complementing Velvitz Rook's philosophy and it really ticks all the",
    "start": "169920",
    "end": "178360"
  },
  {
    "text": "boxes if you see uh you want to have block storage you can have that you can",
    "start": "178360",
    "end": "184640"
  },
  {
    "text": "have shared file system you can have S3 like object storage and uh adding to",
    "start": "184640",
    "end": "190560"
  },
  {
    "text": "that it it's very scalable you can scale up and out very easily without any",
    "start": "190560",
    "end": "195599"
  },
  {
    "text": "downtime it's thin provision so if the storage capacity used would be based on",
    "start": "195599",
    "end": "201280"
  },
  {
    "text": "actuals and not on how many uh PVCs you are creating so uh looking into the sca",
    "start": "201280",
    "end": "208400"
  },
  {
    "text": "uh scalability numbers we have seen multipetabytes to uh uh rook cluster",
    "start": "208400",
    "end": "214159"
  },
  {
    "text": "already with uh with the recent cluster reporting around 250 nodes used",
    "start": "214159",
    "end": "220280"
  },
  {
    "text": "1,800 OSDs uh each having three terabyte uh of NVME amounting to around 5.2 two",
    "start": "220280",
    "end": "227680"
  },
  {
    "text": "pabyte storage and uh if you see the SE public telemetry numbers it's already",
    "start": "227680",
    "end": "234000"
  },
  {
    "text": "reaching exabytes and that's not just uh uh the numbers can uh be many times of",
    "start": "234000",
    "end": "240560"
  },
  {
    "text": "this uh because few few people enable public telemetry and SE is also very",
    "start": "240560",
    "end": "246000"
  },
  {
    "text": "performant if you configure it correctly uh you can see the blog uh for more details it's already um uh reaching to 1",
    "start": "246000",
    "end": "253840"
  },
  {
    "text": "TBTE per second uh speed as well And uh um now you have a complete solution uh",
    "start": "253840",
    "end": "261759"
  },
  {
    "text": "using rook and where can you run it short answer is anywhere Kubernetes run",
    "start": "261759",
    "end": "267440"
  },
  {
    "text": "so you can run it in a cloud provider environment with using EV EBS or",
    "start": "267440",
    "end": "273120"
  },
  {
    "text": "persistent disk or you can have your own uh on-prem bare metal uh data center",
    "start": "273120",
    "end": "279600"
  },
  {
    "text": "using it um with SD SSDs and HDDs for more performance and control or you can",
    "start": "279600",
    "end": "286400"
  },
  {
    "text": "also have a hybrid and multi cloud environment mix match for storage uh",
    "start": "286400",
    "end": "291919"
  },
  {
    "text": "across cloud and on-prem for resilience and uh uh if you See the advantages of",
    "start": "291919",
    "end": "298639"
  },
  {
    "text": "deploying Rook in cloud environment uh the challenge people generally face in cloud environment is they fear data loss",
    "start": "298639",
    "end": "306560"
  },
  {
    "text": "uh as there are not availability zones rook replicates and distributes storage",
    "start": "306560",
    "end": "312240"
  },
  {
    "text": "uh uh data uh across as for you and uh there can be limitations on the p",
    "start": "312240",
    "end": "319360"
  },
  {
    "text": "persistent volumes per node in some of the cloud providers or there could be",
    "start": "319360",
    "end": "324479"
  },
  {
    "text": "poor performance of small PVs rook has an uh um intelligent and optimized uh um",
    "start": "324479",
    "end": "332000"
  },
  {
    "text": "object store data placement and uh it virtually gives you unlimited sto uh un",
    "start": "332000",
    "end": "337840"
  },
  {
    "text": "unlimited scaling and you can also have cl uh crosscloud support uh as you see",
    "start": "337840",
    "end": "343840"
  },
  {
    "text": "in the next slide uh you can have an external uh rrooksef cluster and have",
    "start": "343840",
    "end": "349840"
  },
  {
    "text": "your kubernetes client clusters uh accessing this unified uh uh rrookf u",
    "start": "349840",
    "end": "356160"
  },
  {
    "text": "kubernetes etes cluster for storage demands and uh looking into the new",
    "start": "356160",
    "end": "361199"
  },
  {
    "text": "features that we had we had a recent release in uh December 2024 1.16 with",
    "start": "361199",
    "end": "367919"
  },
  {
    "text": "that with this we have mirroring now available for redos name spaces there were improvements in object storage with",
    "start": "367919",
    "end": "374880"
  },
  {
    "text": "S3 uh storage class improvements and you can also access audit logs for S3 now",
    "start": "374880",
    "end": "381440"
  },
  {
    "text": "and uh we have now direct multiple uh multis network integration ation so no",
    "start": "381440",
    "end": "386960"
  },
  {
    "text": "more holder pods needed we also support Reef and Squid SE versions and in the",
    "start": "386960",
    "end": "393840"
  },
  {
    "text": "next release that we plan 1.17 in April 2025 we want to have CSI operator",
    "start": "393840",
    "end": "400880"
  },
  {
    "text": "enabled by default uh allow Rook operator to run in multiple namespaces",
    "start": "400880",
    "end": "406280"
  },
  {
    "text": "independently and support multiple SE clusters we want to have SEC CSI version",
    "start": "406280",
    "end": "412479"
  },
  {
    "text": "uh 3.14 integration and uh have some uh object uh bucket claim improvements",
    "start": "412479",
    "end": "419680"
  },
  {
    "text": "planned for greater control over buckets we also have uh uh u external monitor",
    "start": "419680",
    "end": "425680"
  },
  {
    "text": "support for two uh data center scenarios also we plan to support SE's latest",
    "start": "425680",
    "end": "432080"
  },
  {
    "text": "version tentacle and uh if you want to uh download and get started with using",
    "start": "432080",
    "end": "437759"
  },
  {
    "text": "uh Rook you can use a Helm chart or latest release from Docker Quay or GCR",
    "start": "437759",
    "end": "443280"
  },
  {
    "text": "uh now in the next slides we'll cover CSI driver over to you Mu thank you DA",
    "start": "443280",
    "end": "450479"
  },
  {
    "text": "so let's talk about CSI driver like CSI driver like we host uh three projects uh",
    "start": "450479",
    "end": "455840"
  },
  {
    "text": "basically three drivers in it like one is FFS RBD and NFS so it's a single",
    "start": "455840",
    "end": "461199"
  },
  {
    "text": "project which holds like u three drivers together so this the architecture looks like we have like a controller plug-in",
    "start": "461199",
    "end": "468400"
  },
  {
    "text": "which runs as a deployment and we have a node plug-in runs as a demon set the deployment um runs with like uh HA it's",
    "start": "468400",
    "end": "475919"
  },
  {
    "text": "always like uh replica 2 which is responsible for volume snapshot group snapshot creation deletion volume",
    "start": "475919",
    "end": "482319"
  },
  {
    "text": "expansion all the stuff so whenever you want or wherever you want to mount uh the PC to our application we need to run",
    "start": "482319",
    "end": "488720"
  },
  {
    "text": "the node plug-in port that is responsible for mounting and unmounting the PVC they run like as a demon set one",
    "start": "488720",
    "end": "494960"
  },
  {
    "text": "per node what there are like few key key features of CSI driver like we CSI",
    "start": "494960",
    "end": "500639"
  },
  {
    "text": "driver like we use OMAP extensively for the state maintenance in SEC CSI even if",
    "start": "500639",
    "end": "505759"
  },
  {
    "text": "it restarts like we don't want to leave any garbage values so we use go for the",
    "start": "505759",
    "end": "511039"
  },
  {
    "text": "API calls to talk to SE to get a better performance because reuse the connections which is connected to a",
    "start": "511039",
    "end": "516880"
  },
  {
    "text": "pools so a single CSI driver is able to talk to multiple SEC cluster we don't need to run CSI driver per se clusters",
    "start": "516880",
    "end": "524560"
  },
  {
    "text": "so we have an isolation uh at the SE level like we could use redos name space or sub volume group to get a",
    "start": "524560",
    "end": "530360"
  },
  {
    "text": "multi-tenency as well uh uh there are like we support thin provisioning for",
    "start": "530360",
    "end": "535680"
  },
  {
    "text": "RBD we support RWX block mode this is for VMs we support RW file system for",
    "start": "535680",
    "end": "541279"
  },
  {
    "text": "databases for SFS we support RX file system and NFS we support RX which is on",
    "start": "541279",
    "end": "546880"
  },
  {
    "text": "top of CFS we support PVC encryption for both SFS and RBD we have tested with",
    "start": "546880",
    "end": "552560"
  },
  {
    "text": "various KMSs like such as example WA Azure like uh IBM HPCS like bring your",
    "start": "552560",
    "end": "558720"
  },
  {
    "text": "own key secrets etc so we also support online PVC expansion for all the three",
    "start": "558720",
    "end": "564120"
  },
  {
    "text": "drivers oh we support volume snapshot for SFS RBD and NFS we recently added",
    "start": "564120",
    "end": "569600"
  },
  {
    "text": "support for volume group snapshot for RBD and CFS uh the PVC clone is",
    "start": "569600",
    "end": "574959"
  },
  {
    "text": "supported for all three drivers so there was some problem with CFS backups because FFS clones are like costly",
    "start": "574959",
    "end": "581839"
  },
  {
    "text": "operation it's full copy so we implemented something for uh specifically backup tools where they can",
    "start": "581839",
    "end": "587279"
  },
  {
    "text": "clone into an ROX so they can just mount and copy the data to some remote site so this was one of the use case uh solved",
    "start": "587279",
    "end": "594560"
  },
  {
    "text": "for SFS uh we support topology based provisioning like uh read from the re uh nearest OST so that we get better",
    "start": "594560",
    "end": "600720"
  },
  {
    "text": "performance we support static provisioning entry uh migration as well we we support kernel mounts and user",
    "start": "600720",
    "end": "607600"
  },
  {
    "text": "space mount for both CFS and RBD so we have everything with CSI so we",
    "start": "607600",
    "end": "613839"
  },
  {
    "text": "need something on top of it like uh support like in SE we have replications etc so we built something similar to CSI",
    "start": "613839",
    "end": "621760"
  },
  {
    "text": "it's called CSI add-on so it runs in a similar manner where we have one controller and site car running with the",
    "start": "621760",
    "end": "628079"
  },
  {
    "text": "CSI driver so in RBD like when you delete an file from a PVC so that in the",
    "start": "628079",
    "end": "634560"
  },
  {
    "text": "back end the space will still be occupied so we have built a communities way of mechanism like user can see",
    "start": "634560",
    "end": "641279"
  },
  {
    "text": "create a CR so in the back end we run FS stream or RBD specify to just uh make",
    "start": "641279",
    "end": "646959"
  },
  {
    "text": "sure like we have a balance between the data and like what you see on the user and the back end we support network",
    "start": "646959",
    "end": "652079"
  },
  {
    "text": "fence class this is like recently added one uh where like on all the nodes like we will display like what's the IP",
    "start": "652079",
    "end": "658000"
  },
  {
    "text": "visible on SE cluster and this is very useful for disaster recovery especially for RWO PCs where they want to move to",
    "start": "658000",
    "end": "664720"
  },
  {
    "text": "another node so we don't want to have any problem with the data and also it's useful for like where you want to move",
    "start": "664720",
    "end": "670399"
  },
  {
    "text": "your application from one cluster to another cluster so they can you can fail over fence the whole cluster uh and you",
    "start": "670399",
    "end": "676000"
  },
  {
    "text": "will have a data so we have we support like PVC encryption for surface RBD we",
    "start": "676000",
    "end": "681279"
  },
  {
    "text": "added a key rotation policy as well where like user can run like uh kind of a chron job for rotating the keys for",
    "start": "681279",
    "end": "688240"
  },
  {
    "text": "the encrypted PVCs uh this is one of the important feature for CS add-on that's volume replication we support",
    "start": "688240",
    "end": "694240"
  },
  {
    "text": "replication of a PVC and uh replication of a group as well so this is like cub",
    "start": "694240",
    "end": "700079"
  },
  {
    "text": "uh cubernetes way like we have a CR so you don't need to do any manual steps in like to promote the remote and all the",
    "start": "700079",
    "end": "706640"
  },
  {
    "text": "operation just one state change it so that you will be able to do the GS recovery from like one cluster to",
    "start": "706640",
    "end": "712320"
  },
  {
    "text": "another cluster easily so we have CSI operator so we want to move away the like all the CSI functionality or the",
    "start": "712320",
    "end": "719279"
  },
  {
    "text": "maintenance from Rook to CSI itself so that we export the CR for the user so that with a minimal configuration you",
    "start": "719279",
    "end": "725839"
  },
  {
    "text": "could have your own CSI drivers created we support Helm installations and uh Kubernetes charts as well uh some of the",
    "start": "725839",
    "end": "733920"
  },
  {
    "text": "future road maps for CSI driver is like we plan to support NFS um shallow volume as well because NFS uses SFS underneath",
    "start": "733920",
    "end": "741360"
  },
  {
    "text": "we have the same problem we have planned to support brows authentication for NFS volume so a volume group snapshot for",
    "start": "741360",
    "end": "748320"
  },
  {
    "text": "RBD though it's supported with upstream uh main SE we are waiting for a release",
    "start": "748320",
    "end": "753680"
  },
  {
    "text": "uh as well to make it a GI feature so we support mirroring uh group snapshot we",
    "start": "753680",
    "end": "759279"
  },
  {
    "text": "are working on chain block tracking for RBDPCs for better backup we have planned to support QoS for RBDPs as well this is",
    "start": "759279",
    "end": "766160"
  },
  {
    "text": "one of the long waiting feature so I'll hand it over to Adam to talk about object store yep thank you",
    "start": "766160",
    "end": "773120"
  },
  {
    "text": "um object storage uh SE implements object storage in Rados gateway application or RJW uh RJW is a web",
    "start": "773120",
    "end": "781680"
  },
  {
    "text": "server implementing S3 and Swift protocols and using SE as a storage layer uh in a simple case user define SE",
    "start": "781680",
    "end": "789120"
  },
  {
    "text": "object store custom resource with desired number of RGW instances and parameters for SE RBD pools uh and root",
    "start": "789120",
    "end": "797519"
  },
  {
    "text": "will create airbd pool in se and um ed deployment in kubernates uh another advanced scenario",
    "start": "797519",
    "end": "804560"
  },
  {
    "text": "is a multiffrontend deployment uh it allows to have dedicated edge double deployments with different configuration",
    "start": "804560",
    "end": "812079"
  },
  {
    "text": "but serving the same data for example have a separate deployment to host only",
    "start": "812079",
    "end": "817120"
  },
  {
    "text": "S3 only swift or admin APIs have a dedicated deployment per customer for",
    "start": "817120",
    "end": "822639"
  },
  {
    "text": "better load balancing or a dedicated instance to perform garbage collection",
    "start": "822639",
    "end": "828320"
  },
  {
    "text": "um in this case it will be disabled for userf facing instances for performance",
    "start": "828320",
    "end": "834040"
  },
  {
    "text": "reasons uh here you can see uh object store custom resource with a long list",
    "start": "834040",
    "end": "839440"
  },
  {
    "text": "of parameters which can be divided in two groups uh the first one is a front- end configuration it configures",
    "start": "839440",
    "end": "846320"
  },
  {
    "text": "deployment like desired number number of replicas uh resources enabled protocols",
    "start": "846320",
    "end": "852959"
  },
  {
    "text": "uh also certificates domains and so on uh and the second group is a backend config telling where and how store data",
    "start": "852959",
    "end": "860079"
  },
  {
    "text": "in SE uh Rook allows to move backend configuration into separate custom resource and referate from object store",
    "start": "860079",
    "end": "868240"
  },
  {
    "text": "in this way we can have a second object store using the same back end for",
    "start": "868240",
    "end": "873279"
  },
  {
    "text": "example one on the left uh hosts only S3 API and the second one only swift uh but",
    "start": "873279",
    "end": "880399"
  },
  {
    "text": "the same data so you will be able to access objects um from the same S3",
    "start": "880399",
    "end": "885920"
  },
  {
    "text": "bucket or swift container uh now let's explore advanced backend configuration specifically pool placement and storage",
    "start": "885920",
    "end": "893000"
  },
  {
    "text": "classes so here is again uh SE object store custom resource uh with the",
    "start": "893000",
    "end": "898720"
  },
  {
    "text": "backend uh configuration zoomed in uh you can see a list of pool placement uh",
    "start": "898720",
    "end": "904000"
  },
  {
    "text": "each placement has a name uh and the set of pools uh the first one is metadata",
    "start": "904000",
    "end": "909440"
  },
  {
    "text": "pool it's responsible for storing uh bucket index and metadata we configured pool to store it",
    "start": "909440",
    "end": "916560"
  },
  {
    "text": "on SSD in three replicas we also have a data pool storing object payload on HDD",
    "start": "916560",
    "end": "922399"
  },
  {
    "text": "in three replicas and we also defined uh storage class reduced redundancy and",
    "start": "922399",
    "end": "928000"
  },
  {
    "text": "override data pool with HDD single uh it's also storing object payload on HDD",
    "start": "928000",
    "end": "933199"
  },
  {
    "text": "but only in a single replica now S3 clients can create bucket and refer our",
    "start": "933199",
    "end": "939279"
  },
  {
    "text": "placement as S3 region uh and if user for example put a object in such a",
    "start": "939279",
    "end": "945920"
  },
  {
    "text": "bucket it will end up in HDD3 replica pool and if user set storage class uh",
    "start": "945920",
    "end": "952320"
  },
  {
    "text": "reduce redundancy it will be stored in a single replican rook allows to have uh",
    "start": "952320",
    "end": "957519"
  },
  {
    "text": "any number of pool placement and any number of storage classes per placement",
    "start": "957519",
    "end": "963279"
  },
  {
    "text": "uh now let's see how Rook provisions object storage to other Kubernetes applications uh the most popular way is",
    "start": "963279",
    "end": "970240"
  },
  {
    "text": "to use object bucket claim it's similar pattern to PVC uh here user creates",
    "start": "970240",
    "end": "976000"
  },
  {
    "text": "object bucket claim with the storage class name bucket name and optional",
    "start": "976000",
    "end": "981920"
  },
  {
    "text": "uh storage quer uh then rrook creates actual bucket and credentials in se and",
    "start": "981920",
    "end": "988399"
  },
  {
    "text": "provides secret and config map with connection information another way is to use",
    "start": "988399",
    "end": "994240"
  },
  {
    "text": "container object storage interface or coy uh its open specification currently",
    "start": "994240",
    "end": "999360"
  },
  {
    "text": "in alpha version it defines how orchestrators like kubernates can",
    "start": "999360",
    "end": "1004639"
  },
  {
    "text": "provision storage from providers like or for example uh cloud providers like",
    "start": "1004639",
    "end": "1010600"
  },
  {
    "text": "AWS uh it defines admin custom resources bucket class and bucket access class",
    "start": "1010600",
    "end": "1016959"
  },
  {
    "text": "describing uh storage quality of service and uh access permissions there is also",
    "start": "1016959",
    "end": "1023279"
  },
  {
    "text": "user custom resource to actually request the bucket and access for it and finally",
    "start": "1023279",
    "end": "1028480"
  },
  {
    "text": "Rook also implements coy driver uh creating bucket and credentials and um",
    "start": "1028480",
    "end": "1034880"
  },
  {
    "text": "creating a secret uh for application to connect uh the bucket uh now uh Travis",
    "start": "1034880",
    "end": "1044038"
  },
  {
    "text": "okay thanks everyone so we've heard a lot about the features of Rook um storage is a huge topic too and we're",
    "start": "1046000",
    "end": "1051200"
  },
  {
    "text": "just scratching on the surface but one thing that's really important to know is that of course your data is is important",
    "start": "1051200",
    "end": "1056880"
  },
  {
    "text": "it's critical and Rook does everything and Seth does everything we can to protect your data so it's important to",
    "start": "1056880",
    "end": "1064160"
  },
  {
    "text": "understand well what do you need to do as system admin to to make sure the data stays safe what do you need to think",
    "start": "1064160",
    "end": "1070320"
  },
  {
    "text": "about during your cluster maintenance tasks because it's not something that you can just deploy and expect to work",
    "start": "1070320",
    "end": "1075520"
  },
  {
    "text": "indefinitely it it takes some attention and and some planning so as you're doing your",
    "start": "1075520",
    "end": "1082640"
  },
  {
    "text": "maintenance operations uh you need to maintain the nodes you need to update Kubernetes you need to to do to do this",
    "start": "1082640",
    "end": "1088880"
  },
  {
    "text": "and that uh so what can the cluster survive what can I take down and what",
    "start": "1088880",
    "end": "1094480"
  },
  {
    "text": "will keep running what can I expect at my data layer um to keep running what's the severity of my outages if something",
    "start": "1094480",
    "end": "1101360"
  },
  {
    "text": "goes really wrong and what can the cluster survive and one of the basic uh",
    "start": "1101360",
    "end": "1107440"
  },
  {
    "text": "things that's important to to configure then and plan for is your topology in the cluster so your topology being how",
    "start": "1107440",
    "end": "1113919"
  },
  {
    "text": "do I lay out my data center do I have multiple data centers zones racks uh",
    "start": "1113919",
    "end": "1119760"
  },
  {
    "text": "hosts how many uh devices per each node there's there are lots of ways to do it and I'll just touch on some of the",
    "start": "1119760",
    "end": "1126480"
  },
  {
    "text": "considerations today so this picture shows us uh a zone topology where okay",
    "start": "1126480",
    "end": "1132000"
  },
  {
    "text": "generally you'll have three zones you need at least three zones you can have more and each with within each zone you",
    "start": "1132000",
    "end": "1138559"
  },
  {
    "text": "can have multiple hosts and each of those hosts can have multiple OSDs where",
    "start": "1138559",
    "end": "1143840"
  },
  {
    "text": "one sethosd maps to a single uh disk u so as as you place your data in",
    "start": "1143840",
    "end": "1152559"
  },
  {
    "text": "the cluster generally you'll you'll configure with three replicas that's the default replic recommendation So Seph",
    "start": "1152559",
    "end": "1158559"
  },
  {
    "text": "will place one replica of the data across each zone if you're running in the cloud that means you you get a",
    "start": "1158559",
    "end": "1164400"
  },
  {
    "text": "replica across each zone and the data is available from any of those zones as Depica touched on",
    "start": "1164400",
    "end": "1170440"
  },
  {
    "text": "earlier um and so what happens now as as maintenance operations need to happen as",
    "start": "1170440",
    "end": "1176960"
  },
  {
    "text": "things go down uh what can we survive so let's say we need to do ma maintenance",
    "start": "1176960",
    "end": "1183520"
  },
  {
    "text": "on a zone there's a a network outage in a zone there's something that needs to",
    "start": "1183520",
    "end": "1188960"
  },
  {
    "text": "happen in one of the zones and let's say everything in zone A goes down what",
    "start": "1188960",
    "end": "1194840"
  },
  {
    "text": "happens well from your data perspective the cluster is fully online seph is able",
    "start": "1194840",
    "end": "1200559"
  },
  {
    "text": "to still serve the data for reads and writes and and everything will be working and and staying online so",
    "start": "1200559",
    "end": "1207679"
  },
  {
    "text": "there's no downtime there's no outage there's no data loss even with having a full zone that is down for some time or",
    "start": "1207679",
    "end": "1214880"
  },
  {
    "text": "even permanently no no data loss no downtime uh what if it gets worse what",
    "start": "1214880",
    "end": "1221200"
  },
  {
    "text": "if we have an outage where two zones go down this is where because the data is",
    "start": "1221200",
    "end": "1228000"
  },
  {
    "text": "replicated across all three zones the data itself is still safe so zone C",
    "start": "1228000",
    "end": "1233600"
  },
  {
    "text": "still contains all of the data uh but what does this mean for availability where the data the cluster is down no",
    "start": "1233600",
    "end": "1240640"
  },
  {
    "text": "writes or reads are allowed at this point so you'll need to get one of the zones back online in order to to bring",
    "start": "1240640",
    "end": "1247760"
  },
  {
    "text": "the the cluster back online but at least the data is safe and there are measures you can take to bring the cluster back",
    "start": "1247760",
    "end": "1254000"
  },
  {
    "text": "online with that single zone if the other two are completely lost um but that's how you know the data",
    "start": "1254000",
    "end": "1260640"
  },
  {
    "text": "is safe as long as the data on one zone is safe",
    "start": "1260640",
    "end": "1266799"
  },
  {
    "text": "um so for your various maintenance operations how do we how do we help guide this this process so pod",
    "start": "1266799",
    "end": "1273679"
  },
  {
    "text": "disruption bud budgets are kubernetes method for signaling how much can I take",
    "start": "1273679",
    "end": "1279600"
  },
  {
    "text": "down at a time so Rook will manage the PTBs dynamically to say oh it looks like",
    "start": "1279600",
    "end": "1286720"
  },
  {
    "text": "you're draining this node let me uh adjust the pod disruption budgets so that I'll allow any node in that uh in",
    "start": "1286720",
    "end": "1294799"
  },
  {
    "text": "that zone to go down so so back on this this picture here if we see you're",
    "start": "1294799",
    "end": "1300799"
  },
  {
    "text": "draining host X Rook will adjust the PDBs to allow host Y to also go down at",
    "start": "1300799",
    "end": "1306880"
  },
  {
    "text": "the same time the PDBs however will block zone B or zone C from going down",
    "start": "1306880",
    "end": "1312240"
  },
  {
    "text": "at the same time so that while you're doing no maintenance as long as the PTBs are respected that you won't expect any",
    "start": "1312240",
    "end": "1319039"
  },
  {
    "text": "downtime during those maintenance operations so it is so the PDBs are",
    "start": "1319039",
    "end": "1325200"
  },
  {
    "text": "topology aware and the PTBs will help guide you through through that to keep availability another type of maintenance",
    "start": "1325200",
    "end": "1332240"
  },
  {
    "text": "is when you're upgrading so if you're upgrading Rook itself Rook will uh will",
    "start": "1332240",
    "end": "1337919"
  },
  {
    "text": "do it in a rolling manner so rolling upgrades automatically so that you don't have any downtime and it's only one at",
    "start": "1337919",
    "end": "1344720"
  },
  {
    "text": "most one failure domain affected at a time so here's kind of a case study so",
    "start": "1344720",
    "end": "1351360"
  },
  {
    "text": "in the last month or so working with a user where kind of the worst case scenario during one of their maintenance",
    "start": "1351360",
    "end": "1357840"
  },
  {
    "text": "operations the control plane and all the worker nodes were accidentally imaged oh whoops i just rebooted all my nodes and",
    "start": "1357840",
    "end": "1363919"
  },
  {
    "text": "they all got a new image uh that's rather disastrous kubernetes is gone",
    "start": "1363919",
    "end": "1369039"
  },
  {
    "text": "right so what do we do is there any hope to recover the data well the data is persisted to disk by",
    "start": "1369039",
    "end": "1376120"
  },
  {
    "text": "Seph and so the question became well how can we get the data back from the disc",
    "start": "1376120",
    "end": "1381280"
  },
  {
    "text": "even though Kubernetes and all its metadata is completely lost how can we bring it back up well in this case we",
    "start": "1381280",
    "end": "1388960"
  },
  {
    "text": "were able to help them with with a lot of manual operations admittedly to be able to get their data out their",
    "start": "1388960",
    "end": "1394880"
  },
  {
    "text": "critical data they were able to bring back up from you know from from the discs that they were still available",
    "start": "1394880",
    "end": "1400400"
  },
  {
    "text": "bring up basically a a temporary Kubernetes cluster mount just the the",
    "start": "1400400",
    "end": "1405520"
  },
  {
    "text": "devices RBD or SFS volumes that they needed and extract the critical data",
    "start": "1405520",
    "end": "1412320"
  },
  {
    "text": "so we're we're working on documenting this case and adding it to our our Rick documentation so that if anybody hits",
    "start": "1412320",
    "end": "1418880"
  },
  {
    "text": "this case it is possible to to recover we do have some steps in this already in",
    "start": "1418880",
    "end": "1424320"
  },
  {
    "text": "our disaster recovery guide in the documentation but there's plenty of room for improvement and and we're clarifying",
    "start": "1424320",
    "end": "1432440"
  },
  {
    "text": "that so as far as tooling what can you do during your maintenance operations uh",
    "start": "1432440",
    "end": "1437760"
  },
  {
    "text": "we do have a coupe cuddle plugin which is essentially a CLI tool for running different maintenance tasks on your",
    "start": "1437760",
    "end": "1444400"
  },
  {
    "text": "cluster these are things that don't really fit the model of CRDs where you need to do a one-off maintenance task",
    "start": "1444400",
    "end": "1450320"
  },
  {
    "text": "instead of expressing desired state in the CRDs so there are things like you",
    "start": "1450320",
    "end": "1455440"
  },
  {
    "text": "need to you've got some downtime you need to restore Monorum to that single zone that that is only remaining you",
    "start": "1455440",
    "end": "1462559"
  },
  {
    "text": "might need to remove OSDs you might need to uh perform other maintenance operations for you know advanced things",
    "start": "1462559",
    "end": "1468960"
  },
  {
    "text": "with Monzo OSDs and if there are other maintenance operations that that you've seen you",
    "start": "1468960",
    "end": "1475919"
  },
  {
    "text": "need to to run in your cluster we'd love to know well what what would you like to see automated because this tool is is",
    "start": "1475919",
    "end": "1482000"
  },
  {
    "text": "the perfect place to run those one-off tasks where um if it's not a CRD",
    "start": "1482000",
    "end": "1488840"
  },
  {
    "text": "setting and just to finish up I want to talk about project health how does the work project work how does how do we",
    "start": "1488840",
    "end": "1495200"
  },
  {
    "text": "value the community well really you know since the project was started I think we're coming up on our ninth anniversary",
    "start": "1495200",
    "end": "1501200"
  },
  {
    "text": "now amazing it's been that long but community has always been important to us uh we've got over 400 contributors to",
    "start": "1501200",
    "end": "1509200"
  },
  {
    "text": "the to the project out of all those I forget how many thousands they said in the in the keynotes this morning to",
    "start": "1509200",
    "end": "1514320"
  },
  {
    "text": "Kubernetes but had a lot of lot of downloads we've we graduated CNCF",
    "start": "1514320",
    "end": "1520159"
  },
  {
    "text": "graduation is almost five years ago now in 2020 and thanks to all our contributors we've got uh currently",
    "start": "1520159",
    "end": "1526720"
  },
  {
    "text": "Kaiso Ibosu IBM and Red Hat and Outbound as contributing with maintainers and",
    "start": "1526720",
    "end": "1532400"
  },
  {
    "text": "we're always looking for new maintainers if you get involved in the project uh we have a process to you know to advance",
    "start": "1532400",
    "end": "1538080"
  },
  {
    "text": "maintainers and those who are active in the project um the stability you know being that we",
    "start": "1538080",
    "end": "1545200"
  },
  {
    "text": "provide a data plan data plane stability is of course critical so we've we",
    "start": "1545200",
    "end": "1551360"
  },
  {
    "text": "declared Rook stable six years ago there are many upstream users running in",
    "start": "1551360",
    "end": "1556480"
  },
  {
    "text": "production many downstream deployments and products running on top of it as well so just happy to say um it's it's",
    "start": "1556480",
    "end": "1564000"
  },
  {
    "text": "great to see so many people running it in production how often do we release uh we",
    "start": "1564000",
    "end": "1569919"
  },
  {
    "text": "do have a minor release about every four months kind of to follow the spirit of Kubernetes itself which which releases",
    "start": "1569919",
    "end": "1575919"
  },
  {
    "text": "about every four months we do have the 1.17 release planned uh in the next couple of weeks and on a just to have a",
    "start": "1575919",
    "end": "1584960"
  },
  {
    "text": "cadence of patch releases you know about every two weeks we will release a release a patch or on demand if there is",
    "start": "1584960",
    "end": "1591919"
  },
  {
    "text": "a critical need what's our road map for Rook uh",
    "start": "1591919",
    "end": "1597120"
  },
  {
    "text": "well essentially you know sometimes we try to say well here's a here's a feature list here's what we want to do but ultimately our philosophy is we",
    "start": "1597120",
    "end": "1604960"
  },
  {
    "text": "always want to support the latest Seth features that the data plane is providing we always want to make sure we're integrated with the latest",
    "start": "1604960",
    "end": "1610720"
  },
  {
    "text": "Kubernetes features as there are always new features coming out and to make the Seth the best storage platform for",
    "start": "1610720",
    "end": "1617120"
  },
  {
    "text": "Kubernetes that it can be as Kubernetes keeps growing and growing we've seen you",
    "start": "1617120",
    "end": "1622480"
  },
  {
    "text": "know everybody needs storage and they need they need it to be the best it can",
    "start": "1622480",
    "end": "1627720"
  },
  {
    "text": "be uh if you're still in London in a couple of months there is Seth Days uh here's a link to go register for that",
    "start": "1627720",
    "end": "1636360"
  },
  {
    "text": "and then I think we've got some time for questions so thanks everyone please come to the project pavilion we do have a",
    "start": "1636360",
    "end": "1642559"
  },
  {
    "text": "Rook booth for the first half of the day until 3 p.m today for example uh so come find us there and and we'll be there",
    "start": "1642559",
    "end": "1649039"
  },
  {
    "text": "happy to answer questions if we don't have time right after this so thank",
    "start": "1649039",
    "end": "1655260"
  },
  {
    "text": "[Applause]",
    "start": "1655260",
    "end": "1663080"
  },
  {
    "text": "you anyone",
    "start": "1663080",
    "end": "1667080"
  },
  {
    "text": "the mic is walking around we got one in front here hi thank you for the talk um noob",
    "start": "1668960",
    "end": "1678559"
  },
  {
    "text": "question but what's the performance penalty compared to running like just in",
    "start": "1678559",
    "end": "1684159"
  },
  {
    "text": "local storage compared to running it in SE with Rook in a container storage mhm",
    "start": "1684159",
    "end": "1690080"
  },
  {
    "text": "so the performance of local storage just the local disk compared to running with with Rook and stuff yeah like the",
    "start": "1690080",
    "end": "1695760"
  },
  {
    "text": "throughput and maybe the latencies also because of this distributed",
    "start": "1695760",
    "end": "1700799"
  },
  {
    "text": "uh I it probably has a penalty hit yes so you know with a local disc of course",
    "start": "1700799",
    "end": "1707440"
  },
  {
    "text": "you don't have any latency with network or other things so that will be the highest performance of a local disc but",
    "start": "1707440",
    "end": "1715360"
  },
  {
    "text": "yeah when you introduce Seth where you've got multiple replicas being replicated to other nodes Seph is fully",
    "start": "1715360",
    "end": "1722159"
  },
  {
    "text": "consistent and so it does require committing to at least two of the three replicas before it will act the any",
    "start": "1722159",
    "end": "1728440"
  },
  {
    "text": "rights so the yeah so there's a network performance to write to at least one other node and and even yeah two nodes",
    "start": "1728440",
    "end": "1736240"
  },
  {
    "text": "so there is a performance there it depends exactly on the numbers as far as your hardware and network and um we",
    "start": "1736240",
    "end": "1743919"
  },
  {
    "text": "don't have numbers published because there's so many so much variety in in that performance but I does anybody else",
    "start": "1743919",
    "end": "1750320"
  },
  {
    "text": "want to comment on that or there's a blog linked in the presentation as well you can take a look into how to uh get",
    "start": "1750320",
    "end": "1757279"
  },
  {
    "text": "the performance number for your use case you can use fio you can have benchmark",
    "start": "1757279",
    "end": "1762480"
  },
  {
    "text": "the local disk and then run uh fio with u maybe RBD or the workload and then see",
    "start": "1762480",
    "end": "1770640"
  },
  {
    "text": "the numbers for yourself yeah thanks yeah another comment comment on",
    "start": "1770640",
    "end": "1775919"
  },
  {
    "text": "that also is that Seph is as a distributed file system essentially a distributed storage it works best when",
    "start": "1775919",
    "end": "1783039"
  },
  {
    "text": "there are many clients because many clients can distribute the load across many nodes so the bigger the cluster the",
    "start": "1783039",
    "end": "1788960"
  },
  {
    "text": "more clients the better overall throughput you'll definitely see if you just have one client it yeah you're not",
    "start": "1788960",
    "end": "1795039"
  },
  {
    "text": "going to be as impressed with the performance probably but the bigger things are that that's where it shines",
    "start": "1795039",
    "end": "1803158"
  },
  {
    "text": "oh is that about time okay one more one more yeah",
    "start": "1807440",
    "end": "1814440"
  },
  {
    "text": "um I have a question to your case study so if only the Kubernetes cluster is",
    "start": "1818240",
    "end": "1824480"
  },
  {
    "text": "nuked but the data is still available on the disks wouldn't anc backup be enough",
    "start": "1824480",
    "end": "1829600"
  },
  {
    "text": "to get the data back because you theoretically spin up a new cluster with similar number of nodes and then with",
    "start": "1829600",
    "end": "1837039"
  },
  {
    "text": "the CD backup you could be able to recover the Kubernetes resources you need to get the data yes correct with",
    "start": "1837039",
    "end": "1844000"
  },
  {
    "text": "that if that city is backed up and you can essentially bring back Kubernetes then it's it's a much simpler recovery",
    "start": "1844000",
    "end": "1849440"
  },
  {
    "text": "problem and not difficult in this case they even um if they had anc backup they",
    "start": "1849440",
    "end": "1855360"
  },
  {
    "text": "couldn't recover it something went completely wrong with with allcd backup and but that yes that will improve your",
    "start": "1855360",
    "end": "1862640"
  },
  {
    "text": "life definitely if you can back up at CD and restore it in that case okay thank you",
    "start": "1862640",
    "end": "1870120"
  },
  {
    "text": "okay If no other questions feel free to come up and we'll be here for a few minutes thank you",
    "start": "1872880",
    "end": "1880360"
  }
]