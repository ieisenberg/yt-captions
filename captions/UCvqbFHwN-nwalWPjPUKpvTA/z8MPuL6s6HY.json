[
  {
    "text": "welcome everyone to The Rook talk on storage for kubernetes I'm Travis neelon I'm with the IBM storage team I'm one of",
    "start": "359",
    "end": "7120"
  },
  {
    "text": "the original creators of of Rook one of the maintainers uh it's been a great journey we created the project announced",
    "start": "7120",
    "end": "13639"
  },
  {
    "text": "it it's already been seven years so happy to be here with you again today that's a little bit different size of",
    "start": "13639",
    "end": "19680"
  },
  {
    "text": "conference now since Seattle where we announced it when there were only a few hundred people at cucon imagine that far",
    "start": "19680",
    "end": "25760"
  },
  {
    "text": "back but yep happy to be here um Annette yeah hi I'm Anette clid um work with",
    "start": "25760",
    "end": "32200"
  },
  {
    "text": "Travis and met Travis about 5 years ago at that time I was uh doing kubernetes",
    "start": "32200",
    "end": "37680"
  },
  {
    "text": "and storage and uh Travis gave me a demo and I was all in on Rook after that so",
    "start": "37680",
    "end": "44120"
  },
  {
    "text": "I've been doing that with Travis and others in particular uh Disaster Recovery right now so I'll go over that",
    "start": "44120",
    "end": "51520"
  },
  {
    "text": "later thanks Demetri uh I'm Dimitri Miss I'm in the University of California San",
    "start": "51520",
    "end": "57559"
  },
  {
    "text": "Diego San Diego supercomputer Center and and we've been using Rook since the almost beginning of Rook we're still",
    "start": "57559",
    "end": "65040"
  },
  {
    "text": "using it a lot uh and I will talk about our use case and how we are excited",
    "start": "65040",
    "end": "71479"
  },
  {
    "text": "about the capabilities Rook provides thanks all right so what what",
    "start": "71479",
    "end": "77840"
  },
  {
    "text": "are we going to talk about today so I'm going to start us off by talking about what is Rook uh and talk a little bit",
    "start": "77840",
    "end": "83119"
  },
  {
    "text": "about why would you use it why do you need storage I think we heard a bit this morning about in the Keynotes you know",
    "start": "83119",
    "end": "89040"
  },
  {
    "text": "why data is important we all know why data is important we need to protect it um then Dimitri will talk to us about",
    "start": "89040",
    "end": "95240"
  },
  {
    "text": "how he's using uh rook in the National research platform give us some background show us his topology about",
    "start": "95240",
    "end": "102320"
  },
  {
    "text": "how he's deployed it it's an interesting use case of how Rook is really used in production and large scales and then",
    "start": "102320",
    "end": "109399"
  },
  {
    "text": "Annette will finish up with some application Disaster Recovery talk about these scenarios for how to protect your",
    "start": "109399",
    "end": "115439"
  },
  {
    "text": "applications across multiple data centers multiple regions",
    "start": "115439",
    "end": "120880"
  },
  {
    "text": "just to get an idea of who's in our audience today I'd love to know you know who who's here to learn about Rook for",
    "start": "120880",
    "end": "126039"
  },
  {
    "text": "the first time all right we got a good crowd uh who's heard of seph before most",
    "start": "126039",
    "end": "131640"
  },
  {
    "text": "of you okay uh how many have experimented with Rook before okay good crowd also and have you",
    "start": "131640",
    "end": "138239"
  },
  {
    "text": "deployed in Rook in production a lot of you also okay thank you for that background um so let's go",
    "start": "138239",
    "end": "144400"
  },
  {
    "text": "through this quickly what what Rook is um originally when we were starting with Rook even before we took a bet on",
    "start": "144400",
    "end": "151519"
  },
  {
    "text": "kubernetes we were looking at Cloud native storage what do we do for storage um what about storage in your own data",
    "start": "151519",
    "end": "157720"
  },
  {
    "text": "center if you're running in a cloud provider there are options there you've got AWS has its Solutions Google Cloud",
    "start": "157720",
    "end": "163800"
  },
  {
    "text": "Azure they all have their Cloud Solutions but what about your own data center what do you do um and then if we",
    "start": "163800",
    "end": "171400"
  },
  {
    "text": "need storage for kubernetes how do we plug it in uh kubernetes has traditionally treated storage as an",
    "start": "171400",
    "end": "177840"
  },
  {
    "text": "external thing oh we'll just worry about stateless applications it's just an external problem for storage but why not",
    "start": "177840",
    "end": "185120"
  },
  {
    "text": "manage storage with kubernetes applications why does it have to be separate why not treat it as any other",
    "start": "185120",
    "end": "190480"
  },
  {
    "text": "kubernetes application so as we started our journey we were looking at what storage platform",
    "start": "190480",
    "end": "196959"
  },
  {
    "text": "should we trust I didn't work for the seph team at the time um you know it it",
    "start": "196959",
    "end": "202159"
  },
  {
    "text": "was an independent Viewpoint we wanted to choose a platform that enterprises trusted we didn't want to build a new St",
    "start": "202159",
    "end": "208799"
  },
  {
    "text": "storage platform because we know data is a it's a hard problem and so we made the",
    "start": "208799",
    "end": "214920"
  },
  {
    "text": "decision to build on seph seph has been inter production ready uh for many years",
    "start": "214920",
    "end": "220080"
  },
  {
    "text": "already but back to Rook so what Rook is then Rook is making storage available in",
    "start": "220080",
    "end": "226319"
  },
  {
    "text": "your cluster so we we looked at seph we said it wasn't built for kubernetes let's bring seph to kubernetes uh we",
    "start": "226319",
    "end": "232920"
  },
  {
    "text": "created an operat operator with custom resource definitions to Define how you want to deploy Rook how how you want to",
    "start": "232920",
    "end": "240120"
  },
  {
    "text": "deploy storage and then Rook will take over the rest we'll automate the deployment configuration upgrades and",
    "start": "240120",
    "end": "247280"
  },
  {
    "text": "allow your apps then to consume the storage uh just like any other storage",
    "start": "247280",
    "end": "253840"
  },
  {
    "text": "application now we use storage classes persistent volume claims all these other terms that you're familiar with in",
    "start": "253840",
    "end": "260560"
  },
  {
    "text": "kubernetes um Rook from the start we wanted to make sure it was open source that we do the right thing for the",
    "start": "260560",
    "end": "265639"
  },
  {
    "text": "community uh so that is one of the basic principles open source and open to open to",
    "start": "265639",
    "end": "271520"
  },
  {
    "text": "contributions well then what is seph so what did we love about seph that made us really want to choose it well seph from",
    "start": "271520",
    "end": "278320"
  },
  {
    "text": "the from the GetGo is a distributed software defined storage solution and it provides all three common types of",
    "start": "278320",
    "end": "284720"
  },
  {
    "text": "storage so block which US used for readed write once volumes shared file systems with readed",
    "start": "284720",
    "end": "291560"
  },
  {
    "text": "write many volumes so you need to you have several pods several applications or multiple instances of applications",
    "start": "291560",
    "end": "297240"
  },
  {
    "text": "that need to share the same volume you can use the shared file system with sefs and then if you need an object store if",
    "start": "297240",
    "end": "304360"
  },
  {
    "text": "you're not running in the cloud um or don't have access to the cloud you want your own cloud with Object Store you've",
    "start": "304360",
    "end": "310120"
  },
  {
    "text": "got access to S3 buckets locally with seph as well uh more information on the",
    "start": "310120",
    "end": "315199"
  },
  {
    "text": "SEF website se. and another thing we loved about seph was it's also purely",
    "start": "315199",
    "end": "320440"
  },
  {
    "text": "open- source and available for contributions uh it has a a proen",
    "start": "320440",
    "end": "327800"
  },
  {
    "text": "history with Enterprise adoption and first release back in July 2012 so over",
    "start": "327800",
    "end": "332919"
  },
  {
    "text": "11 years ago and a great story with cern's large collider um or yeah just",
    "start": "332919",
    "end": "341080"
  },
  {
    "text": "huge data processing needs that's that they're using seph for seph itself is designed to be",
    "start": "341080",
    "end": "348600"
  },
  {
    "text": "consistent it's not eventually consistent but once you commit your data you know it's it's committed and",
    "start": "348600",
    "end": "354440"
  },
  {
    "text": "replicated uh seph has a great architecture for replication across different different azs or whatever",
    "start": "354440",
    "end": "361039"
  },
  {
    "text": "topologies you have racks nodes discs we can take your storage and in the",
    "start": "361039",
    "end": "366319"
  },
  {
    "text": "topology you have and create the storage platform that replication is configurable how many replicas do you",
    "start": "366319",
    "end": "372639"
  },
  {
    "text": "want and it's proven highly durable even in extreme disasters data can be",
    "start": "372639",
    "end": "377840"
  },
  {
    "text": "recovered uh with you know troubleshooting guides so what does this look like as we",
    "start": "377840",
    "end": "383919"
  },
  {
    "text": "brought it together in Rook uh so architecturally we really have three layers to think about so Rook being the",
    "start": "383919",
    "end": "390479"
  },
  {
    "text": "the management layer so as the operator uh with the crds you can tell us how to",
    "start": "390479",
    "end": "395919"
  },
  {
    "text": "configure SEF and then Rook manages it then the plug-in layer is with CSI C",
    "start": "395919",
    "end": "403080"
  },
  {
    "text": "there CSI plugins for any of the storage platforms C seph CSI will manage that",
    "start": "403080",
    "end": "408560"
  },
  {
    "text": "provisioning and that mounting of the storage to your application pods and then once your data is provisioned and",
    "start": "408560",
    "end": "415520"
  },
  {
    "text": "mounted under the covers seph provides the pure data layers at the end of the day seph doesn't even",
    "start": "415520",
    "end": "421800"
  },
  {
    "text": "know it's running in kubernetes it's just providing that data layer and as if",
    "start": "421800",
    "end": "427680"
  },
  {
    "text": "it were running outside of outside of kubernetes but it's all running together or it can all run together or it can run",
    "start": "427680",
    "end": "436400"
  },
  {
    "text": "separately uh moving on how do you install Rook uh there's multiple ways you can use Helm charts uh we also have",
    "start": "436400",
    "end": "442440"
  },
  {
    "text": "example manifests for all sorts of different configurations there's there's many ways you can choose if you want to",
    "start": "442440",
    "end": "448400"
  },
  {
    "text": "run the three platforms block file and object or just one or um multiple",
    "start": "448400",
    "end": "454759"
  },
  {
    "text": "combinations of them you can get started on Rook doio uh now where can you run Rook so",
    "start": "454759",
    "end": "462240"
  },
  {
    "text": "anywhere ketes runs that's our goal to run uh storage so whether you're in the cloud or on Prem there are you know if",
    "start": "462240",
    "end": "470039"
  },
  {
    "text": "you're on Prem you have this need for storage clearly that's where we started the project but users have even found in",
    "start": "470039",
    "end": "475599"
  },
  {
    "text": "the cloud uses for Rook to have that consistent plat platform for various",
    "start": "475599",
    "end": "480919"
  },
  {
    "text": "reasons uh you can do it virtual or bare metal hardware the underlying storage",
    "start": "480919",
    "end": "486240"
  },
  {
    "text": "can also be Noe attach devices or PVS from the cloud or luk back devices for",
    "start": "486240",
    "end": "492599"
  },
  {
    "text": "testing and then Rook really helps enable cross Cloud support So to have",
    "start": "492599",
    "end": "497759"
  },
  {
    "text": "that consistent data platform to run AC in your own data center or in any",
    "start": "497759",
    "end": "503479"
  },
  {
    "text": "Cloud we have a mode where you can run stf externally so essentially the CSI",
    "start": "503479",
    "end": "509280"
  },
  {
    "text": "driver you configure it to just connect to Seth that you've already got running outside of your",
    "start": "509280",
    "end": "514640"
  },
  {
    "text": "cluster uh so you don't have to you don't have to redeploy it inside kubernetes and then one feature I'll",
    "start": "514640",
    "end": "521599"
  },
  {
    "text": "just mention that's under active development object storage provisioning uh with the kubernetes community we're",
    "start": "521599",
    "end": "528040"
  },
  {
    "text": "working through the container object storage interface which is for provisioning buckets with object storage",
    "start": "528040",
    "end": "533800"
  },
  {
    "text": "we do have that implemented in the latest release and or an experimental mode",
    "start": "533800",
    "end": "539240"
  },
  {
    "text": "happy to hear your feedback and until that's more stable we do have the object bucket claims which we've been using for",
    "start": "539240",
    "end": "545399"
  },
  {
    "text": "several years already to provide that bucket provisioning so a little about the",
    "start": "545399",
    "end": "550880"
  },
  {
    "text": "project and the health of the community so really our philosophy has always been Community First we want to know what the",
    "start": "550880",
    "end": "557160"
  },
  {
    "text": "community wants for storage and we want to again make sure it stays open source",
    "start": "557160",
    "end": "562399"
  },
  {
    "text": "we have maintainers across four companies currently with Sibu IBM red",
    "start": "562399",
    "end": "567680"
  },
  {
    "text": "hat and K and upbound um I'm with the seph team Annette and I are with the",
    "start": "567680",
    "end": "572760"
  },
  {
    "text": "seph team we moved from Red Hat to IBM just with acquisition things going on but yeah",
    "start": "572760",
    "end": "578959"
  },
  {
    "text": "it's all the same SEF team and and Rook working together we've had over 400 contributors to the GitHub project and",
    "start": "578959",
    "end": "586760"
  },
  {
    "text": "just this week we hit the Milestone of 300 million container downloads so kind of",
    "start": "586760",
    "end": "592600"
  },
  {
    "text": "exciting Rook did graduate three years ago October 2020 with the cncf so I just",
    "start": "592600",
    "end": "599519"
  },
  {
    "text": "a testament to how much the community appreciates that openness and that Community First approach and running in",
    "start": "599519",
    "end": "606200"
  },
  {
    "text": "production for for a long time now uh a little bit about that Journey",
    "start": "606200",
    "end": "612880"
  },
  {
    "text": "so three years since graduation 5 years since we declared it stable for production and then seven years since we",
    "start": "612880",
    "end": "619360"
  },
  {
    "text": "announced it uh just so many people running Upstream we we never even know how many people are running it Upstream",
    "start": "619360",
    "end": "624800"
  },
  {
    "text": "always love to hear your stories um about that and there are several companies with Downstream products",
    "start": "624800",
    "end": "630480"
  },
  {
    "text": "around it as well that we're not going to talk about today but release Cycles",
    "start": "630480",
    "end": "635720"
  },
  {
    "text": "so Upstream when do we release we kind of shoot for about every four months similar to the kubernetes cycle 1.12 was",
    "start": "635720",
    "end": "643639"
  },
  {
    "text": "in July 1.13 just with holidays and things coming up kind of moov to early December a little over four months in",
    "start": "643639",
    "end": "650760"
  },
  {
    "text": "this case but we do have regular patch releases uh where we shoot for bi-weekly",
    "start": "650760",
    "end": "656160"
  },
  {
    "text": "unless there's a critical need then we can we can can uh ship that as soon as we need just our CI processes we try and",
    "start": "656160",
    "end": "663079"
  },
  {
    "text": "keep those streamlined so we can release whenever needed and now we'll pass the torch off",
    "start": "663079",
    "end": "669720"
  },
  {
    "text": "to Demitri for the national research platform",
    "start": "669720",
    "end": "675639"
  },
  {
    "text": "hello uh hello everyone uh I'm going to talk about our uh use case for Rook uh",
    "start": "675639",
    "end": "681839"
  },
  {
    "text": "so I'm a part of the team of national research platform National research",
    "start": "681839",
    "end": "686880"
  },
  {
    "text": "platform is a NSF funded project project that's providing uh Computer Resources to scientists from more than 50",
    "start": "686880",
    "end": "694440"
  },
  {
    "text": "institutions mostly in us but we also have collaborators from Europe and",
    "start": "694440",
    "end": "699800"
  },
  {
    "text": "Asia uh and it's based on uh San Diego uh University of California San",
    "start": "699800",
    "end": "706519"
  },
  {
    "text": "Diego so the project started uh from Project called PRP Pacific research",
    "start": "706519",
    "end": "713000"
  },
  {
    "text": "platform it was mostly measuring Network performance between uh universities um",
    "start": "713000",
    "end": "718279"
  },
  {
    "text": "so they are connect Ed with 10 to 100 GB networks and it was making sure that you",
    "start": "718279",
    "end": "724200"
  },
  {
    "text": "really get uh the network performance You're Expecting uh but we put kubernetes on",
    "start": "724200",
    "end": "730600"
  },
  {
    "text": "those nodes um to just uh handle them more easily and that's",
    "start": "730600",
    "end": "737680"
  },
  {
    "text": "how the notilus cluster was born so notilus in this case is name of the clust of kubernetes cluster not the",
    "start": "737680",
    "end": "744199"
  },
  {
    "text": "version of SEF uh eventually last year uh the project evolved into National research",
    "start": "744199",
    "end": "751199"
  },
  {
    "text": "platform so that's uh NSF testbed for the national infrastructure for uh",
    "start": "751199",
    "end": "757399"
  },
  {
    "text": "computations and uh what it does is uh it allows uh different universities to",
    "start": "757399",
    "end": "764000"
  },
  {
    "text": "attach their nodes to a single cluster and uh also we provide all those",
    "start": "764000",
    "end": "769920"
  },
  {
    "text": "resources for free to scientists who have research project uh that makes it a global uh",
    "start": "769920",
    "end": "776600"
  },
  {
    "text": "kubernetes cluster as I said we have no in US Europe Asia Africa is not covered",
    "start": "776600",
    "end": "782680"
  },
  {
    "text": "yet and all nodes are connected with 10 to 100 gbit science DMZ so no firewalls",
    "start": "782680",
    "end": "788959"
  },
  {
    "text": "jumbo frames uh very well connected all monitored and uh because we are",
    "start": "788959",
    "end": "796160"
  },
  {
    "text": "providing Computer Resources and uh we need uh persistent storage and we were",
    "start": "796160",
    "end": "801440"
  },
  {
    "text": "using Rook from the beginning uh for all uh persistent storage needs in our",
    "start": "801440",
    "end": "807279"
  },
  {
    "text": "cluster Uh current ly it includes uh six local and Regional SEF clusters inside",
    "start": "807279",
    "end": "813880"
  },
  {
    "text": "one kubernetes cluster I will uh talk about them more and because uh",
    "start": "813880",
    "end": "819320"
  },
  {
    "text": "kubernetes provides uh connectivity between all nodes so each noes is every other node is a next hub uh all nodes",
    "start": "819320",
    "end": "827360"
  },
  {
    "text": "around the world can mount any uh SEF pool which is pretty cool uh if you",
    "start": "827360",
    "end": "832920"
  },
  {
    "text": "mount from far away you get less performance but it's possible to mount we don't have problem that you need to",
    "start": "832920",
    "end": "838480"
  },
  {
    "text": "move data first and then uh do your",
    "start": "838480",
    "end": "843680"
  },
  {
    "text": "computation so research and providers can add their own resources uh which",
    "start": "844560",
    "end": "849800"
  },
  {
    "text": "means they tell us hey I have this node connected to science DMZ uh they provide uh maintenance Power",
    "start": "849800",
    "end": "857759"
  },
  {
    "text": "Cooling uh networking if something breaks we are expecting that they fix the hardware so it's remote hands but we",
    "start": "857759",
    "end": "864920"
  },
  {
    "text": "take it uh from there and we install operating system we manage all the software on it and node becomes",
    "start": "864920",
    "end": "872440"
  },
  {
    "text": "the part of the cluster and gets all monitoring and gets the jobs from users",
    "start": "872440",
    "end": "877600"
  },
  {
    "text": "and optionally they can request that uh they get preferential access to their own Hardware but it's not required for",
    "start": "877600",
    "end": "884360"
  },
  {
    "text": "to uh use our uh Computer Resources scientists can just come and say Hey I want to run and your cluster they they",
    "start": "884360",
    "end": "891199"
  },
  {
    "text": "are more than welcome too uh so it takes 5 minutes from uh the",
    "start": "891199",
    "end": "896320"
  },
  {
    "text": "node with Ubuntu install to become the part of the cluster with just tensible",
    "start": "896320",
    "end": "903000"
  },
  {
    "text": "Playbook uh this is a map of gpus distribution around us uh so we're",
    "start": "904360",
    "end": "909800"
  },
  {
    "text": "covering a lot of abscore Institutions and minority serving institutions and uh",
    "start": "909800",
    "end": "915360"
  },
  {
    "text": "most gpus currently we have are in California and West Coast uh but we have",
    "start": "915360",
    "end": "920560"
  },
  {
    "text": "a good representation in Central States and theast Coast so cluster is constantly growing we're adding couple",
    "start": "920560",
    "end": "926759"
  },
  {
    "text": "nodes per month and currently we have 19,000 uh CPU",
    "start": "926759",
    "end": "933000"
  },
  {
    "text": "cores 1,00 gpus of different Generations from oldest 1080 TI to newest A1 100s",
    "start": "933000",
    "end": "941000"
  },
  {
    "text": "and so on and a map of uh Self Storage So",
    "start": "941000",
    "end": "947199"
  },
  {
    "text": "currently uh it's 5 petabytes uh again scattered across Us in uh six SEF",
    "start": "947199",
    "end": "954680"
  },
  {
    "text": "pools and it's covering uh many Regional uh Network networks uh cic is providing",
    "start": "954680",
    "end": "961480"
  },
  {
    "text": "Network in California and all the internet to is covering most of the",
    "start": "961480",
    "end": "966800"
  },
  {
    "text": "years and so everything is very well connected with fast",
    "start": "966800",
    "end": "972680"
  },
  {
    "text": "networks uh map of our nodes uh again as I said California is historically the",
    "start": "973600",
    "end": "979839"
  },
  {
    "text": "biggest one uh we have many nodes in Central States so this is showing um",
    "start": "979839",
    "end": "986560"
  },
  {
    "text": "memory CPU and GPU uh with the size of the uh Dot and number of",
    "start": "986560",
    "end": "995399"
  },
  {
    "text": "nodes uh so also we have three nodes in Europe uh and I think five nodes in",
    "start": "995399",
    "end": "1001600"
  },
  {
    "text": "Pacific region and uh most nodes in California",
    "start": "1001600",
    "end": "1008720"
  },
  {
    "text": "are uh bought by NSF grants and uh other nodes",
    "start": "1008720",
    "end": "1014480"
  },
  {
    "text": "um partially just donated by different universities partially also purchased by the uh NRP",
    "start": "1014480",
    "end": "1023480"
  },
  {
    "text": "project so this is the distribution of sizes uh capacity and usage of our SEF",
    "start": "1023480",
    "end": "1030000"
  },
  {
    "text": "clusters uh first one is the largest it's historically just called Rook but that's uh Western uh SEF poool uh this",
    "start": "1030000",
    "end": "1037678"
  },
  {
    "text": "is showing two petabytes uh this week we added another petabyte to it so now it's",
    "start": "1037679",
    "end": "1043120"
  },
  {
    "text": "actually three petabytes and it's uh 1.5 petabytes used",
    "start": "1043120",
    "end": "1048199"
  },
  {
    "text": "uh that was after uh heavy purging so we had some capacity issues in that and",
    "start": "1048199",
    "end": "1054160"
  },
  {
    "text": "that's the most uh used pool by users uh second one is Eastern uh that's covering",
    "start": "1054160",
    "end": "1060520"
  },
  {
    "text": "uh New York New Jersey Delaware uh that's uh usage is growing",
    "start": "1060520",
    "end": "1066080"
  },
  {
    "text": "now and it's one petabyte of capacity next one is Central States uh also close",
    "start": "1066080",
    "end": "1072600"
  },
  {
    "text": "to 1 petabyte and 600 terab used uh Southeast Florida uh it's",
    "start": "1072600",
    "end": "1079159"
  },
  {
    "text": "it's uh our newest one so it's not used uh much yet but again usage is growing",
    "start": "1079159",
    "end": "1086240"
  },
  {
    "text": "uh than Pacific uh almost 400 tabt and the last one is uh the smallest uh local",
    "start": "1086240",
    "end": "1093960"
  },
  {
    "text": "for UCSD that's nvme only so all other uh pools are spinning drives with",
    "start": "1093960",
    "end": "1101760"
  },
  {
    "text": "database on nvme and yellow is uh above 70% %",
    "start": "1101760",
    "end": "1111320"
  },
  {
    "text": "used so this is our dashboard for uh our largest Western pool so as I said it's",
    "start": "1112240",
    "end": "1119440"
  },
  {
    "text": "uh above three petabytes right now 1.7 petabytes used uh we usually see uh",
    "start": "1119440",
    "end": "1127400"
  },
  {
    "text": "between five and uh 10 uh th000 iops uh it's picking up to",
    "start": "1127400",
    "end": "1135400"
  },
  {
    "text": "15,000 iops sometimes and uh the pool can deliver up to 10 gigabytes per",
    "start": "1135400",
    "end": "1141960"
  },
  {
    "text": "second so we're trying to keep all the pools be uh below 10 millisecond range",
    "start": "1141960",
    "end": "1147840"
  },
  {
    "text": "between nodes uh just because uh that's the requirement of SEF and that makes",
    "start": "1147840",
    "end": "1153880"
  },
  {
    "text": "the pool faster but users as I said can mount it from far and SEF is caching the",
    "start": "1153880",
    "end": "1161120"
  },
  {
    "text": "data and so uh they get less performance but it's still usable even if you mount",
    "start": "1161120",
    "end": "1166720"
  },
  {
    "text": "across us",
    "start": "1166720",
    "end": "1170240"
  },
  {
    "text": "uh we also experiment with other storages in our cluster um so this is the diagram of all uh storage in uh in",
    "start": "1172679",
    "end": "1182240"
  },
  {
    "text": "the cluster uh first one the biggest is uh Western uh then we have three um OSG",
    "start": "1182240",
    "end": "1191480"
  },
  {
    "text": "open science grid Origins uh so that's the project uh working with CERN on high",
    "start": "1191480",
    "end": "1196679"
  },
  {
    "text": "energy physics uh mostly and those are just three nodes with petabyte of storage attached and using",
    "start": "1196679",
    "end": "1204159"
  },
  {
    "text": "xrd and stash caches to access that data so that's three next three uh petabytes",
    "start": "1204159",
    "end": "1211799"
  },
  {
    "text": "then again uh number of SEF and Dro uh pools and then uh there are Lin store",
    "start": "1211799",
    "end": "1219360"
  },
  {
    "text": "and C with defs storages those are small and just used this experiment and for uh",
    "start": "1219360",
    "end": "1226000"
  },
  {
    "text": "very um small use cases so majority of users are still using rook and SEF and",
    "start": "1226000",
    "end": "1233360"
  },
  {
    "text": "that satisfies all the needs for",
    "start": "1233360",
    "end": "1237480"
  },
  {
    "text": "users okay thank you dri",
    "start": "1239720",
    "end": "1246799"
  },
  {
    "text": "I hi so I want to switch gears just a little bit here and talk about a a",
    "start": "1247080",
    "end": "1253679"
  },
  {
    "text": "solution I've been working on for the last couple years which is to take uh",
    "start": "1253679",
    "end": "1258720"
  },
  {
    "text": "Rook which uh orchestrates and manages stff and combine it with a few other um",
    "start": "1258720",
    "end": "1265840"
  },
  {
    "text": "Upstream projects to uh automate application disaster recovery on um",
    "start": "1265840",
    "end": "1272440"
  },
  {
    "text": "kubernetes so what we're talking about is a situation that it could be a true",
    "start": "1272440",
    "end": "1277480"
  },
  {
    "text": "actual disaster um I was with a financial company when Sandy hurricane",
    "start": "1277480",
    "end": "1282880"
  },
  {
    "text": "hit New Jersey New York and the company I was with figured out they had way too many Data Center and the circumference",
    "start": "1282880",
    "end": "1289840"
  },
  {
    "text": "of that disaster and some of the data centers were unavailable for months so",
    "start": "1289840",
    "end": "1295799"
  },
  {
    "text": "sometimes you can communicate when you have a disaster with what you need to and sometimes you can't so let me just",
    "start": "1295799",
    "end": "1304120"
  },
  {
    "text": "sort of give you a visual and this is you know Regional is um or a",
    "start": "1304120",
    "end": "1312919"
  },
  {
    "text": "region doesn't necessarily have to be that far away but the main thing here is",
    "start": "1312919",
    "end": "1318159"
  },
  {
    "text": "this solution is asynchronous so we'll see that you're going to have some data",
    "start": "1318159",
    "end": "1323240"
  },
  {
    "text": "loss but you can sort of cap that data loss based on how often you move data",
    "start": "1323240",
    "end": "1328720"
  },
  {
    "text": "from one region to the other the the applications um the applications will",
    "start": "1328720",
    "end": "1336039"
  },
  {
    "text": "only exist on one cluster at a time so the data will be replicated from one",
    "start": "1336039",
    "end": "1342799"
  },
  {
    "text": "cluster to the other but until you need to use the application on the opposite cluster it doesn't exist so you're not",
    "start": "1342799",
    "end": "1349880"
  },
  {
    "text": "using resources or just waiting for a disaster both clusters can be used but",
    "start": "1349880",
    "end": "1357080"
  },
  {
    "text": "just on any one application only exists on one at a",
    "start": "1357080",
    "end": "1362760"
  },
  {
    "text": "time so and again Disaster Recovery resiliency Is Not A New Concept",
    "start": "1363520",
    "end": "1370320"
  },
  {
    "text": "um it's been around for a long time I think the difference with containerized",
    "start": "1370320",
    "end": "1375360"
  },
  {
    "text": "platforms is we don't see a lot of it yet um but as containerized platforms",
    "start": "1375360",
    "end": "1382320"
  },
  {
    "text": "become more sort of critical they are going to be required to have Disaster",
    "start": "1382320",
    "end": "1387480"
  },
  {
    "text": "Recovery planning and resiliency so we have two measures recovery Point",
    "start": "1387480",
    "end": "1393320"
  },
  {
    "text": "objective and recovery time objective again not new but what we want to do",
    "start": "1393320",
    "end": "1398360"
  },
  {
    "text": "with containerized platforms is we want to get those down to minutes instead of",
    "start": "1398360",
    "end": "1404080"
  },
  {
    "text": "days um in the case of RPO the for an",
    "start": "1404080",
    "end": "1409120"
  },
  {
    "text": "asynchronous replication your replication uh interval decides really",
    "start": "1409120",
    "end": "1414799"
  },
  {
    "text": "how much data can be outstanding so if you're replicating every five minutes then you could lose up to five minutes",
    "start": "1414799",
    "end": "1421600"
  },
  {
    "text": "of data for RTO even if you lose zero data you're still going to probably have",
    "start": "1421600",
    "end": "1428000"
  },
  {
    "text": "application downtime because you have to reinstall the application on the",
    "start": "1428000",
    "end": "1434559"
  },
  {
    "text": "alternate cluster so Rook um has been really",
    "start": "1434559",
    "end": "1441279"
  },
  {
    "text": "instrumental in helping the solution along one of the The Rook CRS or crds is",
    "start": "1441279",
    "end": "1447400"
  },
  {
    "text": "a SEF RBD mirror so seph has and has had quite a long time the ability to do",
    "start": "1447400",
    "end": "1453520"
  },
  {
    "text": "mirroring which is to do the replication between SEF clusters so this is not one",
    "start": "1453520",
    "end": "1459520"
  },
  {
    "text": "cluster this is two clusters and we're replicating via like snapshot the data",
    "start": "1459520",
    "end": "1465840"
  },
  {
    "text": "from one to another based Bas on volumes or images so um that is coming out of",
    "start": "1465840",
    "end": "1473120"
  },
  {
    "text": "rook and then out of the CSI add-ons we have the volume replication CR and the",
    "start": "1473120",
    "end": "1479360"
  },
  {
    "text": "volume replication class and those are really important to the solution because volume replication is what we use to",
    "start": "1479360",
    "end": "1486960"
  },
  {
    "text": "enable and disable mirring instead of having to do it with st commands and",
    "start": "1486960",
    "end": "1492320"
  },
  {
    "text": "then volume replication class is going to have the interval of replication so",
    "start": "1492320",
    "end": "1498840"
  },
  {
    "text": "those two are coming again out of the CSI add-ons so another thing that we need to",
    "start": "1498840",
    "end": "1507600"
  },
  {
    "text": "have um to be able to make this solution work and this is coming again from CSI",
    "start": "1507600",
    "end": "1513760"
  },
  {
    "text": "we need a volume replication operator and we need an omap generator so these are side cars if you've ever um used or",
    "start": "1513760",
    "end": "1523000"
  },
  {
    "text": "deployed Rook you'll see that there's CSI um pods that get created",
    "start": "1523000",
    "end": "1528360"
  },
  {
    "text": "one of them is called the RBD provisioner that these two um side cars",
    "start": "1528360",
    "end": "1534200"
  },
  {
    "text": "will become containers within that RBD provisioner and you you can enable those",
    "start": "1534200",
    "end": "1541640"
  },
  {
    "text": "if you're doing it manually um via the config map for",
    "start": "1541640",
    "end": "1547679"
  },
  {
    "text": "RF so if if we're doing this again um",
    "start": "1547679",
    "end": "1552720"
  },
  {
    "text": "using rooka and using the capabilities there we're going to be able to change",
    "start": "1552720",
    "end": "1560520"
  },
  {
    "text": "the action of the volume replication so that it will go ahead and",
    "start": "1560520",
    "end": "1566559"
  },
  {
    "text": "promote and demote um actually let me say that you will be able to enable mirring and then",
    "start": "1566559",
    "end": "1573840"
  },
  {
    "text": "what we'll be able to promote and and demote storage will be the volume",
    "start": "1573840",
    "end": "1579039"
  },
  {
    "text": "operator that we saw the CSI add-on so when if you think about it if I have a",
    "start": "1579039",
    "end": "1584720"
  },
  {
    "text": "volume that is using an image and that image is replicated over to an alternate cluster I have to be able to demote the",
    "start": "1584720",
    "end": "1592600"
  },
  {
    "text": "storage on one cluster if I have access and promote it on the other so that it",
    "start": "1592600",
    "end": "1598039"
  },
  {
    "text": "can be used so application failover is",
    "start": "1598039",
    "end": "1604240"
  },
  {
    "text": "using the the custom resources I went through there's one case that I think",
    "start": "1604240",
    "end": "1609600"
  },
  {
    "text": "doesn't get enough attention which is the the second one which is what if I just want to migrate the application to",
    "start": "1609600",
    "end": "1616360"
  },
  {
    "text": "a different cluster because it's closer to the users or maybe I don't like the",
    "start": "1616360",
    "end": "1621640"
  },
  {
    "text": "cluster I have is out of resources so in that case you can scale the application",
    "start": "1621640",
    "end": "1627080"
  },
  {
    "text": "down sync the replication all the outstanding data Therefore your RPO is",
    "start": "1627080",
    "end": "1632279"
  },
  {
    "text": "equal to zero it does require though that both clusters are healthy for just",
    "start": "1632279",
    "end": "1639200"
  },
  {
    "text": "doing a migration in the case of Disaster Recovery it doesn't require one",
    "start": "1639200",
    "end": "1644520"
  },
  {
    "text": "of the Clusters could be like I said in the Sandy hurricane and not communicating and you'd still be",
    "start": "1644520",
    "end": "1650159"
  },
  {
    "text": "able to recover on an alternate cluster because the image the persistent data is",
    "start": "1650159",
    "end": "1656320"
  },
  {
    "text": "on the alternate cluster so what I want to now go to is",
    "start": "1656320",
    "end": "1662519"
  },
  {
    "text": "sort of the solution part of this which is how we can combine a couple of other",
    "start": "1662519",
    "end": "1668679"
  },
  {
    "text": "um open source projects open cluster management I don't know if you've heard of it uh it is in a cncf process um so",
    "start": "1668679",
    "end": "1679760"
  },
  {
    "text": "it but what what it's good at is application life cycle in particular",
    "start": "1679760",
    "end": "1685799"
  },
  {
    "text": "deploying and scheduling apps if they are available on a g source so we can",
    "start": "1685799",
    "end": "1692120"
  },
  {
    "text": "use Helm charts we can use customize but basically this will allow you to",
    "start": "1692120",
    "end": "1698360"
  },
  {
    "text": "schedule an application via ocm and we're going to combine that so that we",
    "start": "1698360",
    "end": "1703679"
  },
  {
    "text": "can Auto automate the creation of the application on the initial primary cluster and then",
    "start": "1703679",
    "end": "1711480"
  },
  {
    "text": "if needed recreate it on a secondary cluster really important um sort of glue",
    "start": "1711480",
    "end": "1717880"
  },
  {
    "text": "for all this is the ramen Dr project that's the one I'm involved with and we",
    "start": "1717880",
    "end": "1724080"
  },
  {
    "text": "use ocm we use all the custom resources that you saw but in addition there's",
    "start": "1724080",
    "end": "1730120"
  },
  {
    "text": "some new uh CRS that are coming out of this uh project and they are Dr policy",
    "start": "1730120",
    "end": "1737000"
  },
  {
    "text": "and Dr our policy everything that you do with this solution is in groups of two so even if I had a 100 kubernetes",
    "start": "1737000",
    "end": "1744080"
  },
  {
    "text": "clusters I'm going to divide them into 50 um peers so each each cluster has a",
    "start": "1744080",
    "end": "1752519"
  },
  {
    "text": "failover cluster so the Dr policy defines which",
    "start": "1752519",
    "end": "1759039"
  },
  {
    "text": "two clusters are going to protect each other the Dr clusters then is um sort of",
    "start": "1759039",
    "end": "1765159"
  },
  {
    "text": "an outcome of that which is what what are the clusters those are cluster scoped custom resources and then we have",
    "start": "1765159",
    "end": "1772880"
  },
  {
    "text": "the Dr placement control it is going to be a it's going to basically the action whether it be",
    "start": "1772880",
    "end": "1781039"
  },
  {
    "text": "failover or a planned uh maintenance uh migration it's going to define the",
    "start": "1781039",
    "end": "1787760"
  },
  {
    "text": "action and that will be it'll be on the Hub cluster which we'll see in a minute here but it will Define the action of",
    "start": "1787760",
    "end": "1795399"
  },
  {
    "text": "what are we going to do another really important um custom resource coming out of the ramen project it's volume",
    "start": "1795399",
    "end": "1801320"
  },
  {
    "text": "replication group so that is that is actually created on what we call the manage cluster and in that case once a",
    "start": "1801320",
    "end": "1810679"
  },
  {
    "text": "Dr placement control is created then an Associated uh volume replication group",
    "start": "1810679",
    "end": "1815960"
  },
  {
    "text": "is created on where the application actually lives so we've seen this um at the",
    "start": "1815960",
    "end": "1824320"
  },
  {
    "text": "beginning here but I just want to define a little bit more here you see in the middle there the the um open cluster",
    "start": "1824320",
    "end": "1831640"
  },
  {
    "text": "management so this is going to take three kubernetes clusters one of them",
    "start": "1831640",
    "end": "1836960"
  },
  {
    "text": "will be the Hub and that is why we don't need the Clusters to be",
    "start": "1836960",
    "end": "1842760"
  },
  {
    "text": "communicating because the Hub is actually where Dr",
    "start": "1842760",
    "end": "1848480"
  },
  {
    "text": "placement control is going to decide the action so the say the cluster on the",
    "start": "1848480",
    "end": "1853640"
  },
  {
    "text": "left is no longer communicating the app say there's you know 100 applications there we can use",
    "start": "1853640",
    "end": "1860720"
  },
  {
    "text": "the Hub to actually move those applications or recreate those applications on the surviving",
    "start": "1860720",
    "end": "1868039"
  },
  {
    "text": "cluster and once the other cluster comes back online we can move them back using",
    "start": "1868039",
    "end": "1875360"
  },
  {
    "text": "like a plan migration uh so it's really powerful the fact that you have the Hub and open",
    "start": "1875360",
    "end": "1881960"
  },
  {
    "text": "cluster management remember has deployed the application via git source so it",
    "start": "1881960",
    "end": "1887559"
  },
  {
    "text": "knows how to redeploy the application once the storage is promoted on the",
    "start": "1887559",
    "end": "1893200"
  },
  {
    "text": "alternate cluster and there's some other things there that that you can read but the",
    "start": "1893200",
    "end": "1899600"
  },
  {
    "text": "whole thing is we want low data loss and we're talking minutes and we don't want",
    "start": "1899600",
    "end": "1905320"
  },
  {
    "text": "low uh for the application to be able to be reinstalled uh we want that to be as low",
    "start": "1905320",
    "end": "1911639"
  },
  {
    "text": "as possible",
    "start": "1911639",
    "end": "1919159"
  },
  {
    "text": "H just a minute okay let",
    "start": "1919880",
    "end": "1927120"
  },
  {
    "text": "me how do we put that back in I think I hit the",
    "start": "1927120",
    "end": "1932679"
  },
  {
    "text": "wrong sorry there okay okay no it's and some",
    "start": "1932679",
    "end": "1938600"
  },
  {
    "text": "other okay so what I want to do if if you're",
    "start": "1938600",
    "end": "1945559"
  },
  {
    "text": "interested in this idea um the team I'm on just for our own because you can you know three clusters is a",
    "start": "1945559",
    "end": "1952120"
  },
  {
    "text": "little bit of a heavy kit especially if you have you know control nodes and",
    "start": "1952120",
    "end": "1957240"
  },
  {
    "text": "compute nodes on in every one of them so we came up with a way to use um a VM or",
    "start": "1957240",
    "end": "1963080"
  },
  {
    "text": "a machine that you have that has uh 8 CPUs 32 gigs still you know a heavy lift",
    "start": "1963080",
    "end": "1969440"
  },
  {
    "text": "but and running some kind of Linux we've been testing in Fedora and with this um",
    "start": "1969440",
    "end": "1975799"
  },
  {
    "text": "you can go to the the ramen uh Dr GitHub go to the",
    "start": "1975799",
    "end": "1981080"
  },
  {
    "text": "docs uh I recently rewrote the the quick start guide so that it hopefully um",
    "start": "1981080",
    "end": "1988039"
  },
  {
    "text": "there's not any missing steps and then also I did a video that's in uh in blue",
    "start": "1988039",
    "end": "1993480"
  },
  {
    "text": "there but what's nice is it uses mini Cube and it sets up the entire environment let me just show",
    "start": "1993480",
    "end": "2000679"
  },
  {
    "text": "you sets up this entire environment uh sets up the stf mirroring creates the",
    "start": "2000679",
    "end": "2007159"
  },
  {
    "text": "three clusters um sets up in the middle there you see a S3 bucket we use that",
    "start": "2007159",
    "end": "2014320"
  },
  {
    "text": "there's the application can be redeployed but what we absolutely have to make sure is the PV and PVC are using",
    "start": "2014320",
    "end": "2022039"
  },
  {
    "text": "exactly the same name and the same definition so what we do is we move that",
    "start": "2022039",
    "end": "2027320"
  },
  {
    "text": "definition to an S3 bucket so that on the alternate cluster it can get that",
    "start": "2027320",
    "end": "2032880"
  },
  {
    "text": "data and make sure that's hooked up correctly so feel free to um try it",
    "start": "2032880",
    "end": "2039120"
  },
  {
    "text": "yourself and uh one once you you know and I I think I showed it but on here",
    "start": "2039120",
    "end": "2045760"
  },
  {
    "text": "it's one once you get the the prere done like installing podma and some other things you can basically use the drv",
    "start": "2045760",
    "end": "2053638"
  },
  {
    "text": "tool do that start against that yaml file creates the whole environment for",
    "start": "2053639",
    "end": "2059320"
  },
  {
    "text": "you so I think we're at questions thank",
    "start": "2059320",
    "end": "2064638"
  },
  {
    "text": "you yes thanks to me Tre and n",
    "start": "2064639",
    "end": "2068878"
  },
  {
    "text": "and I think we're about out of time but we'd love to you know see at the Rook booth in the project Pavilion it's way at the end next to the cncf store so",
    "start": "2072040",
    "end": "2079720"
  },
  {
    "text": "we'll be there for a few hours this afternoon and then tomorrow it's just for the first half of of the time in the",
    "start": "2079720",
    "end": "2085118"
  },
  {
    "text": "booth area but yeah but if there's any burning questions that we could do here in a few minutes if anybody has",
    "start": "2085119",
    "end": "2094280"
  },
  {
    "text": "anything no one question",
    "start": "2095879",
    "end": "2100920"
  },
  {
    "text": "okay so I see uh I mean in your slides you have you're replicating the volumes",
    "start": "2101880",
    "end": "2108560"
  },
  {
    "text": "as well as replicating the3 object storage systems so just wondering like you know what kind of consistency",
    "start": "2108560",
    "end": "2115480"
  },
  {
    "text": "guarantees the system allows or does I I think you said in terms of of",
    "start": "2115480",
    "end": "2123599"
  },
  {
    "text": "consistency right or or when you is that what you asked about yeah so data loss",
    "start": "2123599",
    "end": "2129480"
  },
  {
    "text": "is one thing but like having the volumes in inconsistent State or the storage well I mean yeah so it'll be crash",
    "start": "2129480",
    "end": "2136640"
  },
  {
    "text": "consistent but it won't be application consistent like if you have a database you could lose transactions okay yeah",
    "start": "2136640",
    "end": "2143560"
  },
  {
    "text": "it's using a snapshot technology so it does a snapshot and then transfers the snapshot I mean you could possibly have",
    "start": "2143560",
    "end": "2151119"
  },
  {
    "text": "hooks that would Quest it and we're working on looking at that but it's you know the hooks are pretty much per",
    "start": "2151119",
    "end": "2157160"
  },
  {
    "text": "application yeah because the application can look at like a bunch of files like a set of files like application might need",
    "start": "2157160",
    "end": "2164240"
  },
  {
    "text": "a bunch of or set of files to be there in order to see it as consistent and",
    "start": "2164240",
    "end": "2170280"
  },
  {
    "text": "sometimes yeah I mean right now we're using what we call declarative so you declare it in a get source so when you",
    "start": "2170280",
    "end": "2176240"
  },
  {
    "text": "recreate the application you know it's all it's declarative we are looking at",
    "start": "2176240",
    "end": "2181359"
  },
  {
    "text": "um what we're calling imperative which is something where the application is not in a good but you you still want to",
    "start": "2181359",
    "end": "2188319"
  },
  {
    "text": "use the same process which maybe is more of what you're talking about yeah sounds good I'll we'll catch up later thank",
    "start": "2188319",
    "end": "2195359"
  },
  {
    "text": "you yeah well thanks everyone again feel free to come up for questions or we'll be at the Rook Booth okay thank",
    "start": "2195359",
    "end": "2203400"
  },
  {
    "text": "you",
    "start": "2203880",
    "end": "2206880"
  }
]