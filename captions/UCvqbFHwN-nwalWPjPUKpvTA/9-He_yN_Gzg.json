[
  {
    "text": "so thank you for coming and thanks to Nate one of our regular uh members here he's going to talk to us about uh",
    "start": "0",
    "end": "6240"
  },
  {
    "text": "container support in slurm so over to you Nate",
    "start": "6240",
    "end": "11960"
  },
  {
    "text": "okay hopefully that'll be quiet though",
    "start": "15680",
    "end": "19940"
  },
  {
    "text": "I have a neighbor like a small wiener dog and they really want to go and lick it so whenever the neighbor walks it",
    "start": "24359",
    "end": "31019"
  },
  {
    "text": "they just bark the little heads off foreign",
    "start": "31019",
    "end": "35719"
  },
  {
    "text": "I think you can go ahead Nathan",
    "start": "40399",
    "end": "44718"
  },
  {
    "text": "all right let's see hello hello everybody knows me but uh I'm Nate I work for schedmd",
    "start": "50100",
    "end": "57960"
  },
  {
    "text": "with the guys behind slurm uh today I'm gonna present to you some",
    "start": "57960",
    "end": "63359"
  },
  {
    "text": "container support stuff that we just we're adding in the upcoming 2302 release so that's just a March",
    "start": "63359",
    "end": "72299"
  },
  {
    "text": "everything here uh I am presenting actually exists I'm not just waving my hands and if I",
    "start": "72299",
    "end": "78479"
  },
  {
    "text": "have time I'll even do a live presentation afterwards",
    "start": "78479",
    "end": "82820"
  },
  {
    "text": "all right uh quick rundown what I'm going to talk about I'll skip that here so um",
    "start": "84119",
    "end": "91159"
  },
  {
    "text": "I mean I think everybody here knows this everybody likes Docker they want to be able to use it lots of users really like",
    "start": "91320",
    "end": "96540"
  },
  {
    "text": "it uh there's lots of problems with Docker but we're not going to work worry about that for this presentation",
    "start": "96540",
    "end": "104420"
  },
  {
    "text": "um one of the things that I've been asked for a lot is users just want to be able to use docker",
    "start": "104520",
    "end": "111479"
  },
  {
    "text": "um and so this is the way that uh our first real good attempt at actually getting Docker to work natively with",
    "start": "111479",
    "end": "117780"
  },
  {
    "text": "slurm and by extension podman and uh possibly some love for kubernetes",
    "start": "117780",
    "end": "124500"
  },
  {
    "text": "later uh first thing was we had to add support for oci containers uh we did that last",
    "start": "124500",
    "end": "132180"
  },
  {
    "text": "release uh it's runs it works uses all of existing controls",
    "start": "132180",
    "end": "139080"
  },
  {
    "text": "it's not very user friendly never was really intended to be but the sport was required",
    "start": "139080",
    "end": "144660"
  },
  {
    "text": "and now with our upcoming release uh we're adding something I like to call a",
    "start": "144660",
    "end": "150540"
  },
  {
    "text": "oci runtime proxy which I'll explain in a few minutes but more or less it allows us to bolt in",
    "start": "150540",
    "end": "156900"
  },
  {
    "text": "slurm into docker uh so this is the container support that we",
    "start": "156900",
    "end": "163140"
  },
  {
    "text": "got added um I don't think it's too relevant for this thing I just have an address for reference containers are now first class",
    "start": "163140",
    "end": "170459"
  },
  {
    "text": "Citizen and slurm uh his examples just running against",
    "start": "170459",
    "end": "176099"
  },
  {
    "text": "some container that you have intent FS uh",
    "start": "176099",
    "end": "182220"
  },
  {
    "text": "this is the current way jobs are run and slurm user logs in usually via SSH into",
    "start": "182220",
    "end": "187260"
  },
  {
    "text": "a login node and then they call one of the commands to start their jobs the job runs on a",
    "start": "187260",
    "end": "192540"
  },
  {
    "text": "compute node um this is the way it's been for",
    "start": "192540",
    "end": "197760"
  },
  {
    "text": "oh geez 20 years and Slim's model is the continuation of existing models which",
    "start": "197760",
    "end": "203040"
  },
  {
    "text": "are all the way from the 40s or sorry 50s",
    "start": "203040",
    "end": "209719"
  },
  {
    "text": "um tried intrude works great uh new model that we're adding now is user will log",
    "start": "210000",
    "end": "218340"
  },
  {
    "text": "into a login node via SSH or whatever but instead they will interact directly",
    "start": "218340",
    "end": "224819"
  },
  {
    "text": "with podman or docker then podman or Docker we'll call our new",
    "start": "224819",
    "end": "230280"
  },
  {
    "text": "command which we're ever so hilariously calling scram um and then SC run will do all the magic",
    "start": "230280",
    "end": "237360"
  },
  {
    "text": "that requires to get slim to work and to the user it'll look like they're just working with uh Docker directly but",
    "start": "237360",
    "end": "244019"
  },
  {
    "text": "in reality their job will be running on a compute node um",
    "start": "244019",
    "end": "249120"
  },
  {
    "text": "and following the previous model for the job itself and you know the expectation is they'll",
    "start": "249120",
    "end": "255840"
  },
  {
    "text": "be contain error like an artifactory or something like that where they pull their images from and everything they need",
    "start": "255840",
    "end": "260940"
  },
  {
    "text": "foreign [Music] and in this case uh swapping out kubernetes there for",
    "start": "260940",
    "end": "267840"
  },
  {
    "text": "Docker is definitely a possibility because everything works by standard which is really nice",
    "start": "267840",
    "end": "273240"
  },
  {
    "text": "uh definitely not perfect which is one of the things so for instance uh we're",
    "start": "273240",
    "end": "279180"
  },
  {
    "text": "I'm only problem uh we're only giving documentation for Dr bobman but since",
    "start": "279180",
    "end": "285000"
  },
  {
    "text": "this is the uh research group I also you know we'll look at kubernetes",
    "start": "285000",
    "end": "292199"
  },
  {
    "text": "definitely not something I'm promising anytime soon at least not for this release",
    "start": "292199",
    "end": "297960"
  },
  {
    "text": "uh so I'm calling a runtime proxy I assume everybody here is familiar with them",
    "start": "297960",
    "end": "304259"
  },
  {
    "text": "what an oci runtime is um in this case",
    "start": "304259",
    "end": "309479"
  },
  {
    "text": "all this functionality exists in slurum one way or another it's just not very",
    "start": "309479",
    "end": "314759"
  },
  {
    "text": "clearly set up for containers so it's the game of a thousand small",
    "start": "314759",
    "end": "321720"
  },
  {
    "text": "edits all over the place to get it to work um my entire goal here is to make it",
    "start": "321720",
    "end": "327840"
  },
  {
    "text": "completely boring for users I want the user available called Docker as they would call it on their laptop",
    "start": "327840",
    "end": "333479"
  },
  {
    "text": "and have it work as they would expect um and like all things in Computing",
    "start": "333479",
    "end": "339960"
  },
  {
    "text": "there's a cost to this which means the system administrator is going to have to do a little bit extra work to make sure",
    "start": "339960",
    "end": "347160"
  },
  {
    "text": "that the container images can be pushed around and the default config is what they want",
    "start": "347160",
    "end": "352740"
  },
  {
    "text": "so maybe having a specific queue that they send all the jobs to that are containers",
    "start": "352740",
    "end": "357780"
  },
  {
    "text": "in most cases I assume most containers will be you know a single core or something like that something simple",
    "start": "357780",
    "end": "364259"
  },
  {
    "text": "but it's definitely configurable and then you continue to use all the existing stuff that slurm has",
    "start": "364259",
    "end": "371720"
  },
  {
    "text": "uh one of the big gotchas here is when Docker runs or podman runs for that",
    "start": "371720",
    "end": "377400"
  },
  {
    "text": "matter it sets up about namespace where it mounts everything usually with",
    "start": "377400",
    "end": "382560"
  },
  {
    "text": "overlay Fest depending on how you configure it and that",
    "start": "382560",
    "end": "387600"
  },
  {
    "text": "Mount name space has to be copied out or somehow exported to the compute node",
    "start": "387600",
    "end": "393780"
  },
  {
    "text": "and it only exists in that specific uh namespace so that's why we have something called",
    "start": "393780",
    "end": "400319"
  },
  {
    "text": "staging in and out of the image it's done by a script I will talk about in a minute",
    "start": "400319",
    "end": "409039"
  },
  {
    "text": "um the whole idea here is users be able to just use existing HPC resources",
    "start": "410220",
    "end": "415259"
  },
  {
    "text": "and Docker now or podman podman tends to be a little bit friendlier on the HPC",
    "start": "415259",
    "end": "420660"
  },
  {
    "text": "boxes um so here's the first example",
    "start": "420660",
    "end": "426300"
  },
  {
    "text": "uh and I will note here that this is only rootless docker rootful doctor is a uh absolutely",
    "start": "426300",
    "end": "433680"
  },
  {
    "text": "unacceptable security risk on almost every HBC system",
    "start": "433680",
    "end": "438800"
  },
  {
    "text": "um that's not just like a per user host uh Cloud hosted system",
    "start": "438840",
    "end": "444419"
  },
  {
    "text": "uh so here the first line is you're exporting the docker rootless control",
    "start": "444419",
    "end": "450180"
  },
  {
    "text": "just telling Docker hey talk to rootless instead of trying to talk to the system blocker",
    "start": "450180",
    "end": "456000"
  },
  {
    "text": "uh the riffle Docker and then the second part is Docker unlike podman requires that you",
    "start": "456000",
    "end": "462060"
  },
  {
    "text": "pass um security settings directly over the command line you can't just figure it",
    "start": "462060",
    "end": "467340"
  },
  {
    "text": "out at least not a way that I've been able to find and I have been searching the source code for it",
    "start": "467340",
    "end": "472740"
  },
  {
    "text": "in this case all the security features don't make any sense because you're not actually",
    "start": "472740",
    "end": "478500"
  },
  {
    "text": "running the container on the login node",
    "start": "478500",
    "end": "483900"
  },
  {
    "text": "so we just need to turn them all off so uh no Advanced Network any stuff supported yet so we just turn that off",
    "start": "483900",
    "end": "490199"
  },
  {
    "text": "app armor and SC Linux and um the security uh containment whatever",
    "start": "490199",
    "end": "496500"
  },
  {
    "text": "all that just things that get disabled because it doesn't even apply because you're not running on the Node you're actually running on it's all headed by",
    "start": "496500",
    "end": "503520"
  },
  {
    "text": "storm anyway on the computer notes so I just make quick little export here to make it",
    "start": "503520",
    "end": "510180"
  },
  {
    "text": "explicit um and you just turn all of it off I really wish it was an easier way to do",
    "start": "510180",
    "end": "516240"
  },
  {
    "text": "this and I might end up sending some patches in I don't know uh First Command is you're just calling",
    "start": "516240",
    "end": "522899"
  },
  {
    "text": "Docker run all the normal things hey I want to run an Ubuntu and just verify the release and the",
    "start": "522899",
    "end": "530220"
  },
  {
    "text": "second one is same thing but with Centos just proof of uh that it works",
    "start": "530220",
    "end": "537139"
  },
  {
    "text": "you guys feel free to ask questions at any time I didn't mean this to be too formal",
    "start": "537240",
    "end": "544220"
  },
  {
    "text": "um so here's all the processes that'll actually end up happening in the run so",
    "start": "544560",
    "end": "550800"
  },
  {
    "text": "the user logs in Via SSH or whatever this kit is running docker",
    "start": "550800",
    "end": "556680"
  },
  {
    "text": "which is running container D which is running the shim and the shim is called scram which is our oci runtime proxy and",
    "start": "556680",
    "end": "564480"
  },
  {
    "text": "it handles the uh work of calling out the slurm and initiating the job and all the other things",
    "start": "564480",
    "end": "570540"
  },
  {
    "text": "and then slurm d which is basic which is close what slur",
    "start": "570540",
    "end": "577260"
  },
  {
    "text": "mode it's basically the cubelet for uh slurm is running on the compute node and it's",
    "start": "577260",
    "end": "582300"
  },
  {
    "text": "actually calling C run or run C whichever oh say runtime you want that's definitely configurable",
    "start": "582300",
    "end": "588300"
  },
  {
    "text": "so there's a lot of extra um processes involved in this",
    "start": "588300",
    "end": "593519"
  },
  {
    "text": "but to the user it shouldn't be visible to just work as expected",
    "start": "593519",
    "end": "598980"
  },
  {
    "text": "and most of these processes don't actually do much so it doesn't slow down anything at the end of the day the user",
    "start": "598980",
    "end": "606360"
  },
  {
    "text": "is actually communicating directly with s run and containerd doesn't actually have to do much",
    "start": "606360",
    "end": "613519"
  },
  {
    "text": "a quick example of the conflict that's required to activate this uh I do disable everything I can that I",
    "start": "614459",
    "end": "621720"
  },
  {
    "text": "don't want in the config that they let you do it um and then even activate no new",
    "start": "621720",
    "end": "627600"
  },
  {
    "text": "privileges because why not more of the security the better and in this case the most important part",
    "start": "627600",
    "end": "633420"
  },
  {
    "text": "is new runtime just call SC run which is the new binary provided by slurm",
    "start": "633420",
    "end": "639720"
  },
  {
    "text": "which is the oci runtime proxy uh same thing for podman although podman",
    "start": "639720",
    "end": "645360"
  },
  {
    "text": "actually has configuration choices to disable all the extra security stuff which is really nice",
    "start": "645360",
    "end": "650760"
  },
  {
    "text": "so in this case normal podman command I want to run Ubuntu and run Centos and",
    "start": "650760",
    "end": "656160"
  },
  {
    "text": "then just for verification uh I'm just having it print the slurm job environment ID",
    "start": "656160",
    "end": "662459"
  },
  {
    "text": "so that you know that you're actually running as a job each time you call it",
    "start": "662459",
    "end": "667399"
  },
  {
    "text": "for people who are unfamiliar with slurm every time you run a job it gets a new job ID number",
    "start": "668579",
    "end": "674700"
  },
  {
    "text": "uh in this case just verifying that",
    "start": "674700",
    "end": "680180"
  },
  {
    "text": "pod band's a little simpler uh it just has podman which calls card man",
    "start": "680339",
    "end": "686100"
  },
  {
    "text": "and con man calls us C run um not too fundamentally different a little less abstraction",
    "start": "686100",
    "end": "693620"
  },
  {
    "text": "uh the config for podman is simpler than docker uh you can just disable all those extra",
    "start": "694019",
    "end": "701279"
  },
  {
    "text": "security things say I just want to run the host for everything because it's not running on the login node your job is",
    "start": "701279",
    "end": "707279"
  },
  {
    "text": "actually running all the way onto compute node somewhere far away and just telling it hey",
    "start": "707279",
    "end": "713720"
  },
  {
    "text": "I assume you guys can see in my mouse yeah it looks like you can",
    "start": "713720",
    "end": "718760"
  },
  {
    "text": "yeah we can see your mouse just fine",
    "start": "718760",
    "end": "722899"
  },
  {
    "text": "so uh one of the gotchas is container staging I mentioned this a little earlier",
    "start": "726660",
    "end": "731940"
  },
  {
    "text": "uh when SC runs starts it's actually round inside of the username space mount",
    "start": "731940",
    "end": "738360"
  },
  {
    "text": "namespace it's its job to get that image and push",
    "start": "738360",
    "end": "743519"
  },
  {
    "text": "it out to the compute node via whatever means is most efficient",
    "start": "743519",
    "end": "748700"
  },
  {
    "text": "now since every single HPC system I've ever gotten my hands on or seen is different",
    "start": "748740",
    "end": "754620"
  },
  {
    "text": "file systems are different storage locations are different um",
    "start": "754620",
    "end": "761240"
  },
  {
    "text": "this has to be really customizable so and slurm it's done via plugin which",
    "start": "762120",
    "end": "767760"
  },
  {
    "text": "calls Lua script so that a sysadmin can do whatever they need",
    "start": "767760",
    "end": "773220"
  },
  {
    "text": "um I expect this will actually have different differences between different Hardware Types on certain clusters",
    "start": "773220",
    "end": "780180"
  },
  {
    "text": "especially for the cloud bursting ones um they'll even need to be some calculations of paying egress and",
    "start": "780180",
    "end": "787260"
  },
  {
    "text": "Ingress fees I imagine so there's just a Lewis script uh Lua",
    "start": "787260",
    "end": "794579"
  },
  {
    "text": "tends to be super friendly to access admins I mean if you don't like it you can just Call Exec out to whatever script or code",
    "start": "794579",
    "end": "801600"
  },
  {
    "text": "you want it doesn't really matter um nice thing about Lou is it has built-in Json support so you can edit",
    "start": "801600",
    "end": "809279"
  },
  {
    "text": "and look at the images spec file and all the other fun",
    "start": "809279",
    "end": "814560"
  },
  {
    "text": "things and they change it as you please uh and then there's callbacks inside of",
    "start": "814560",
    "end": "821040"
  },
  {
    "text": "the Lua thing to tell serum what you have done like hey this uh image was",
    "start": "821040",
    "end": "827040"
  },
  {
    "text": "here but now it's over here on the shared file system or on this S3 bucket",
    "start": "827040",
    "end": "832800"
  },
  {
    "text": "whatever's required here's a incredibly simplified example",
    "start": "832800",
    "end": "838860"
  },
  {
    "text": "in this case I'm just calling rsync and pushing out the image root",
    "start": "838860",
    "end": "846000"
  },
  {
    "text": "to a shared file system and I'm telling slurm hey the bundle is now here and the new root path is here",
    "start": "846000",
    "end": "854399"
  },
  {
    "text": "and it modifies the config file too very simplified uh",
    "start": "854399",
    "end": "860579"
  },
  {
    "text": "you can look at our documentation later for the full one but yeah I hope this gets the idea across",
    "start": "860579",
    "end": "865800"
  },
  {
    "text": "and then in the case of my example uh when it's done with the job it just",
    "start": "865800",
    "end": "871980"
  },
  {
    "text": "deletes all of it on the remote file system um first sites that have lots of rules",
    "start": "871980",
    "end": "878220"
  },
  {
    "text": "or stuff like that reproducibility requirements they can always just send this stuff off the tape",
    "start": "878220",
    "end": "883500"
  },
  {
    "text": "or something like that or just release the S3 buckets directly something fun like that or on Mount if you use file",
    "start": "883500",
    "end": "889199"
  },
  {
    "text": "systems uh there are lots of limitations",
    "start": "889199",
    "end": "894959"
  },
  {
    "text": "involved with this and I've been glancing them over uh for instance we don't support any kind of network name spaces it's only",
    "start": "894959",
    "end": "901199"
  },
  {
    "text": "host for now and on most and most Supercuts that's what it's probably going to be forever",
    "start": "901199",
    "end": "906540"
  },
  {
    "text": "because uh RDMA drivers and other things like that don't play nice with uh no spaces",
    "start": "906540",
    "end": "913139"
  },
  {
    "text": "RC groups although there has been some efforts in",
    "start": "913139",
    "end": "918480"
  },
  {
    "text": "the um infiniband area to make that uh play nice but it's not covered for now",
    "start": "918480",
    "end": "926339"
  },
  {
    "text": "um stuff like c groups our Palmer SC Linux support on the login host don't actually make any sense because you're",
    "start": "926339",
    "end": "932820"
  },
  {
    "text": "not running the job there so we just disable all that",
    "start": "932820",
    "end": "937160"
  },
  {
    "text": "um finding out where things fail can be difficult I'm still working on that but",
    "start": "938160",
    "end": "943260"
  },
  {
    "text": "there's lots of places where logs can happen I mean every single demon has its own logging and you have to be able to",
    "start": "943260",
    "end": "949920"
  },
  {
    "text": "look at all those depending on when things fail I've found mostly once you get working it tends to be pretty Rock stable",
    "start": "949920",
    "end": "957120"
  },
  {
    "text": "or rock hard um uh Minor Details you got to make sure",
    "start": "957120",
    "end": "963480"
  },
  {
    "text": "you compile things in the right order uh slim has lots of customize that",
    "start": "963480",
    "end": "968519"
  },
  {
    "text": "customization for sites something called clf filter and spank",
    "start": "968519",
    "end": "975180"
  },
  {
    "text": "plugins that let sysadmins insert plugins that modify user requests at",
    "start": "975180",
    "end": "980699"
  },
  {
    "text": "certain locations really giving the admin lots of power control these things because in a lot of",
    "start": "980699",
    "end": "986940"
  },
  {
    "text": "cases users are not basically power users but it's definitely configurable",
    "start": "986940",
    "end": "992579"
  },
  {
    "text": "and if it's not done anything then the user can do as they please",
    "start": "992579",
    "end": "997740"
  },
  {
    "text": "uh authentication we currently only support the manjov",
    "start": "997740",
    "end": "1004040"
  },
  {
    "text": "there's a lots of work done in the background to make sure that the username spaces",
    "start": "1004040",
    "end": "1010639"
  },
  {
    "text": "are translated correctly which is currently only possible by a Munch but future releases",
    "start": "1010639",
    "end": "1016880"
  },
  {
    "text": "um Json web tokens and stuff like openoff oauth can be set up to work it",
    "start": "1016880",
    "end": "1021920"
  },
  {
    "text": "just hasn't been done anybody have questions these are just",
    "start": "1021920",
    "end": "1027918"
  },
  {
    "text": "minor technical limitations I just wanted to list out in case people have questions",
    "start": "1027919",
    "end": "1033699"
  },
  {
    "text": "I mean I have a couple of questions but maybe maybe I don't know how much more you have it's a bit of an",
    "start": "1037579",
    "end": "1043938"
  },
  {
    "text": "a question of understanding if I if I got that right so I mean first of all I think this is um pretty cool",
    "start": "1043939",
    "end": "1050059"
  },
  {
    "text": "um oh our questions officially yeah I'm sorry um",
    "start": "1050059",
    "end": "1057080"
  },
  {
    "text": "so so the the the the the the the the the socket like the the docker socket",
    "start": "1057080",
    "end": "1063500"
  },
  {
    "text": "and so on they are not available on the login notes but there are only available on on the compute nodes so the like this",
    "start": "1063500",
    "end": "1070160"
  },
  {
    "text": "learn cluster is that um correct uh no actually Docker runs on the logger nodes",
    "start": "1070160",
    "end": "1076340"
  },
  {
    "text": "yeah let me go find that but it mounts the socket from the from the like batch",
    "start": "1076340",
    "end": "1081679"
  },
  {
    "text": "node no Docker doesn't have anything to do with it um because slurm has the native",
    "start": "1081679",
    "end": "1087320"
  },
  {
    "text": "container support so I'm talking directly to the oci runtime so either see run run C whatever you want uh where",
    "start": "1087320",
    "end": "1094340"
  },
  {
    "text": "is where is that little graph I had um yeah so Docker will run here Docker",
    "start": "1094340",
    "end": "1100640"
  },
  {
    "text": "socket will be here the job won't have that access to that because it's running on some compute",
    "start": "1100640",
    "end": "1107000"
  },
  {
    "text": "node off in the cluster potentially being burst out in the cloud somewhere",
    "start": "1107000",
    "end": "1112340"
  },
  {
    "text": "but um once the job is over there and",
    "start": "1112340",
    "end": "1117799"
  },
  {
    "text": "running it can talk the slurm directly or it can talk to its own container runtime and the container runtime",
    "start": "1117799",
    "end": "1124100"
  },
  {
    "text": "pass proper interface of the slope because slurring my nose the relatively simple config and",
    "start": "1124100",
    "end": "1129919"
  },
  {
    "text": "state for it okay okay but in principle yeah okay",
    "start": "1129919",
    "end": "1136520"
  },
  {
    "text": "I mean uh it doesn't currently have the like the kubernetes thing where you can",
    "start": "1136520",
    "end": "1142160"
  },
  {
    "text": "talk to itself and change the job around um outside of the existing slim stuff I",
    "start": "1142160",
    "end": "1147860"
  },
  {
    "text": "mean swim has all that functionality but it's not specific to Containers but anybody can change around their job and",
    "start": "1147860",
    "end": "1152960"
  },
  {
    "text": "say please okay and the main use case would would",
    "start": "1152960",
    "end": "1158539"
  },
  {
    "text": "be interactive um work with a container because I mean if you wanted to use",
    "start": "1158539",
    "end": "1163760"
  },
  {
    "text": "container workloads then you could use the the native slurm container support right and just submit your drops",
    "start": "1163760",
    "end": "1170419"
  },
  {
    "text": "and like if you want to have like a massive batch um parallel drops and and yes but this this",
    "start": "1170419",
    "end": "1178160"
  },
  {
    "text": "implementation here the point here is really to enable people to um you know interactively work with us",
    "start": "1178160",
    "end": "1184700"
  },
  {
    "text": "uh interactively and then um even though my example is really simple uh once the image is made the user can",
    "start": "1184700",
    "end": "1192440"
  },
  {
    "text": "probably have some kind of an option to push it out to his common location and then submit large amounts of batch jobs",
    "start": "1192440",
    "end": "1197960"
  },
  {
    "text": "to do production uh batch work the whole idea is hit the let's Docker",
    "start": "1197960",
    "end": "1204140"
  },
  {
    "text": "podman or whatever you want to generate your containers and have them run or be",
    "start": "1204140",
    "end": "1209660"
  },
  {
    "text": "prepared in the way that users would expect same way as their laptop or work machine Workstation",
    "start": "1209660",
    "end": "1216559"
  },
  {
    "text": "and then you can run the jobs as you please yeah I think that's great and maybe one one last question",
    "start": "1216559",
    "end": "1222620"
  },
  {
    "text": "um I mean what are the like system requirements I mean we're still running quite a lot of let's say um you know",
    "start": "1222620",
    "end": "1229340"
  },
  {
    "text": "redhead or Centos um seven um uh systems um are there any internet for",
    "start": "1229340",
    "end": "1236600"
  },
  {
    "text": "since you have like rootless I mean does that require like a Centos um eight or I",
    "start": "1236600",
    "end": "1242780"
  },
  {
    "text": "mean do you have an idea what's like the minimum requirement for that um so this is the fun part uh on the",
    "start": "1242780",
    "end": "1249679"
  },
  {
    "text": "login node I would suggest running the latest because rootless Docker has been doing massive amounts of development and",
    "start": "1249679",
    "end": "1256460"
  },
  {
    "text": "improvements and it just tends to work better on Sentosa and I know whatever it is but",
    "start": "1256460",
    "end": "1263360"
  },
  {
    "text": "that being said the compute nodes run C has been Rock Solid stable for a",
    "start": "1263360",
    "end": "1268580"
  },
  {
    "text": "few years now so if you're running to Centos 7 and you want to run the job there on the compute nodes I'll be fine",
    "start": "1268580",
    "end": "1274640"
  },
  {
    "text": "and doing heterogeneous clusters is um been working for years so you could",
    "start": "1274640",
    "end": "1279799"
  },
  {
    "text": "have the log of node be you know current uh revision of Centos and then",
    "start": "1279799",
    "end": "1284840"
  },
  {
    "text": "have compete notes be something old that's stable uh you do need to have username space",
    "start": "1284840",
    "end": "1290179"
  },
  {
    "text": "support so red hat six and tests centos6 doesn't work that's just the kernel",
    "start": "1290179",
    "end": "1295220"
  },
  {
    "text": "limitation there's nothing we can do about that but besides that if you have a username space support it should more or less",
    "start": "1295220",
    "end": "1301760"
  },
  {
    "text": "work in your um efforts in your kernel then it should more or less work Ron C is actually really simple so it",
    "start": "1301760",
    "end": "1307940"
  },
  {
    "text": "works pretty nicely okay yeah very cool maybe I'll stop in",
    "start": "1307940",
    "end": "1313400"
  },
  {
    "text": "case like other people have questions no I'm I'm here for a question and answer",
    "start": "1313400",
    "end": "1319400"
  },
  {
    "text": "um I guess this is a better example of the wrote the stock yeah I mean at the end of the day it's running under C run or run C whatever",
    "start": "1319400",
    "end": "1327320"
  },
  {
    "text": "uh runtime you want to use or even Singularity or Charlie cloud",
    "start": "1327320",
    "end": "1332720"
  },
  {
    "text": "yeah ask like what where what is the life",
    "start": "1332720",
    "end": "1338059"
  },
  {
    "text": "cycle of the container again so you you prepare the container on the login node",
    "start": "1338059",
    "end": "1343400"
  },
  {
    "text": "if I understand correctly like you do maybe potment pool on the on the",
    "start": "1343400",
    "end": "1348500"
  },
  {
    "text": "um on the login node right but then the configuration based on what the compute",
    "start": "1348500",
    "end": "1354919"
  },
  {
    "text": "node needs is also done in slurm like in in podman or Singularity or seros they",
    "start": "1354919",
    "end": "1360500"
  },
  {
    "text": "use specific configuration that is done on the compute node like oci hooks or plugins in podman to maybe specialize",
    "start": "1360500",
    "end": "1368059"
  },
  {
    "text": "the container and then tweak the container to use the right MPI or userite GPU libraries and so on",
    "start": "1368059",
    "end": "1376840"
  },
  {
    "text": "so funny about that is you can actually just activate that stuff um",
    "start": "1377000",
    "end": "1382820"
  },
  {
    "text": "for instance you could act well Cyrus Kelsey run eventually",
    "start": "1382820",
    "end": "1387919"
  },
  {
    "text": "um the new commands so it's scram it has all the same semantics as s Alec and it",
    "start": "1387919",
    "end": "1394580"
  },
  {
    "text": "has its entire set of environment variables that you can set up the same way",
    "start": "1394580",
    "end": "1401120"
  },
  {
    "text": "um so you can modify the job to be 100 nodes 10 nodes whatever you want all that's there",
    "start": "1401120",
    "end": "1407000"
  },
  {
    "text": "the creation of the container like the the Docker create or Sarah's like the life",
    "start": "1407000",
    "end": "1413419"
  },
  {
    "text": "cycle management of the container is it done on the compute node or on the on the head node because if you",
    "start": "1413419",
    "end": "1419720"
  },
  {
    "text": "configure servers on a specific compute node then your Sarah's run will be the",
    "start": "1419720",
    "end": "1425000"
  },
  {
    "text": "lifecycle hooks will be executed on the compute node and it sounds like you are not using seros on the compute node",
    "start": "1425000",
    "end": "1430760"
  },
  {
    "text": "you're just using C run on the compute nodes um so there's this adds several steps to it",
    "start": "1430760",
    "end": "1439520"
  },
  {
    "text": "so yes um all the hooks will be run by docker doc will maintain the image as you",
    "start": "1439520",
    "end": "1445220"
  },
  {
    "text": "please unless you decide to copy it out um and then you can have the hooks that c run calls also",
    "start": "1445220",
    "end": "1452780"
  },
  {
    "text": "a t-run doesn't call any hooks all right um but they actually are there and they do",
    "start": "1452780",
    "end": "1459320"
  },
  {
    "text": "work um for some of them the docker thing you actually have to disable a few of them",
    "start": "1459320",
    "end": "1464980"
  },
  {
    "text": "okay um but yeah they're actually called on both sides",
    "start": "1466400",
    "end": "1471760"
  },
  {
    "text": "okay interesting yeah interesting to see a live demo on it um I think that's cool",
    "start": "1472340",
    "end": "1477559"
  },
  {
    "text": "let me go uh get it going see if I can actually share my screen",
    "start": "1477559",
    "end": "1483400"
  },
  {
    "text": "foreign",
    "start": "1501679",
    "end": "1503919"
  },
  {
    "text": "yesterday when I prepared it but you know it's the way of life demos",
    "start": "1512380",
    "end": "1517460"
  },
  {
    "text": "let me get it shared",
    "start": "1517460",
    "end": "1520899"
  },
  {
    "text": "okay can everybody see this yep",
    "start": "1524740",
    "end": "1529760"
  },
  {
    "text": "all right um now naturally I have uh conveniently removed any of the",
    "start": "1529760",
    "end": "1536179"
  },
  {
    "text": "errors and other stuff in the presentation so right here I just called uh podman",
    "start": "1536179",
    "end": "1541640"
  },
  {
    "text": "run um and I did break it a little",
    "start": "1541640",
    "end": "1547900"
  },
  {
    "text": "bit of the uh",
    "start": "1548240",
    "end": "1550240"
  },
  {
    "text": "so I have all the debug activating here too",
    "start": "1556580",
    "end": "1562700"
  },
  {
    "text": "so here I'm just a normal user on the cluster I'm calling Ubuntu run or a",
    "start": "1562700",
    "end": "1567980"
  },
  {
    "text": "podman run Ubuntu I'm just telling up time um these are the authentication",
    "start": "1567980",
    "end": "1574279"
  },
  {
    "text": "workarounds I have active that's going to be fixed before uh March um there's a whole bunch of uh movement",
    "start": "1574279",
    "end": "1580760"
  },
  {
    "text": "in and out of the name spaces that I have to account for slurm was written before all this stuff",
    "start": "1580760",
    "end": "1586039"
  },
  {
    "text": "existed so uh it's requiring a good bit of uh effort to get storm to play nice with",
    "start": "1586039",
    "end": "1592820"
  },
  {
    "text": "them and here's some more logging um",
    "start": "1592820",
    "end": "1600380"
  },
  {
    "text": "I can explain it later but uh the expectation from the noci runtime is that you have a process that runs",
    "start": "1600380",
    "end": "1607419"
  },
  {
    "text": "that Docker or podman uh communicate against",
    "start": "1607419",
    "end": "1613039"
  },
  {
    "text": "so in this case we make one week and I just call it anchor and it gets split off and then it starts doing all the",
    "start": "1613039",
    "end": "1619100"
  },
  {
    "text": "effort and then eventually at some point s run is called",
    "start": "1619100",
    "end": "1624860"
  },
  {
    "text": "and it has the container location uh it'll know where so that's the data",
    "start": "1624860",
    "end": "1632120"
  },
  {
    "text": "location that's where it's been pushed to by the Lewis script um and then the container ID which is",
    "start": "1632120",
    "end": "1637640"
  },
  {
    "text": "handed To Us by docker and then the job runs",
    "start": "1637640",
    "end": "1643820"
  },
  {
    "text": "uh just a little up time I mean if you might call different command that's cool too and we're just going to ignore these",
    "start": "1643820",
    "end": "1650480"
  },
  {
    "text": "errors for the terminal stuff for the time being uh definitely some interesting issues with",
    "start": "1650480",
    "end": "1657080"
  },
  {
    "text": "the movement of um the terminal permissions some minor bugs to fix",
    "start": "1657080",
    "end": "1664220"
  },
  {
    "text": "and then it does the stage out and in which case it's just deleting it I mean",
    "start": "1664220",
    "end": "1670779"
  },
  {
    "text": "let's see OS release",
    "start": "1670779",
    "end": "1675940"
  },
  {
    "text": "faster",
    "start": "1682940",
    "end": "1685539"
  },
  {
    "text": "and of course it runs slow for some reason probably doing some compile in the background or something",
    "start": "1689659",
    "end": "1695860"
  },
  {
    "text": "um reference here whoops I killed it off um this I'm running it",
    "start": "1695900",
    "end": "1701779"
  },
  {
    "text": "on a percent of us so this is uh Alma Linux 8.6",
    "start": "1701779",
    "end": "1709340"
  },
  {
    "text": "and I'm calling Ubuntu",
    "start": "1709340",
    "end": "1712900"
  },
  {
    "text": "I'll check make sure I don't have the cluster full of other jobs something funny like that",
    "start": "1717520",
    "end": "1723880"
  },
  {
    "text": "okay that's just being slow for some reason uh this is a problem of active development",
    "start": "1725900",
    "end": "1731600"
  },
  {
    "text": "but I hope it gets the idea across for the user they'll just see the normal",
    "start": "1731600",
    "end": "1737840"
  },
  {
    "text": "um pod band commands or Docker ones honestly I I would expect the most sites to do podman just because it's a whole",
    "start": "1737840",
    "end": "1744919"
  },
  {
    "text": "lot less of a pain for them than Docker or rootless docker but do they both work",
    "start": "1744919",
    "end": "1752240"
  },
  {
    "text": "uh my example here only has apartment set up because uh it was just simpler because this is",
    "start": "1752240",
    "end": "1758240"
  },
  {
    "text": "actually a Docker cluster so uh codes",
    "start": "1758240",
    "end": "1765679"
  },
  {
    "text": "so this is a docker base cluster inside a compose and it's",
    "start": "1770179",
    "end": "1776120"
  },
  {
    "text": "actually doing that so there's multiple levels of uh name spaces used here",
    "start": "1776120",
    "end": "1782539"
  },
  {
    "text": "um doing Docker inside Docker is a pain in the butt because I have to actually move around a whole bunch of the mounts so I'm not exposing that right now",
    "start": "1782539",
    "end": "1789500"
  },
  {
    "text": "which I assume you know all about Christian foreign",
    "start": "1789500",
    "end": "1796480"
  },
  {
    "text": "[Music] and all the normal things so I just hit Ctrl C to cancel it out and it sends the",
    "start": "1801130",
    "end": "1807740"
  },
  {
    "text": "signals as expected uh let me see if after time works",
    "start": "1807740",
    "end": "1814159"
  },
  {
    "text": "maybe I did something funky there um yeah uptime work so I must have did",
    "start": "1814159",
    "end": "1820039"
  },
  {
    "text": "something odd definitely it's not completely ready but it does work",
    "start": "1820039",
    "end": "1826340"
  },
  {
    "text": "um I might have had the interactive thing there's some weird issues with uh TTY controls but I'm working those out",
    "start": "1826340",
    "end": "1833240"
  },
  {
    "text": "most of it involves just having to turn off a whole bunch of the features because we just don't need them or use them",
    "start": "1833240",
    "end": "1840340"
  },
  {
    "text": "hey Nate I'm curious about one thing uh for the for running the container uh",
    "start": "1840740",
    "end": "1847100"
  },
  {
    "text": "with the resources request like usually I've seen you create some kind of serum scripts does this work well with Docker",
    "start": "1847100",
    "end": "1853820"
  },
  {
    "text": "like with this SC run or um yeah if you wanted to you could do",
    "start": "1853820",
    "end": "1861100"
  },
  {
    "text": "uh let me see if I have a cute neighbor for this thing",
    "start": "1861559",
    "end": "1867679"
  },
  {
    "text": "uh I'd have to add the login node as a partition which I can do pretty quick",
    "start": "1867679",
    "end": "1873440"
  },
  {
    "text": "but then once you do that you could just do X batch so something along these lines",
    "start": "1873440",
    "end": "1880299"
  },
  {
    "text": "uh log in so I'll just tell it to run a login node and then",
    "start": "1880399",
    "end": "1886340"
  },
  {
    "text": "something like that you know that's what you wanted to do it's not going to work because I don't have it set up for that",
    "start": "1886340",
    "end": "1891440"
  },
  {
    "text": "right now but that's definitely something that should work and then I guess it'd be a little I",
    "start": "1891440",
    "end": "1897559"
  },
  {
    "text": "guess I mean I guess I wonder what the overhead would be for like the containers like requirements for the CPU",
    "start": "1897559",
    "end": "1903440"
  },
  {
    "text": "or memory versus what s batch requires like I don't know if those are quite one to one right",
    "start": "1903440",
    "end": "1909679"
  },
  {
    "text": "um it's gonna be a little extra but if you're just running on the login node it's the same price as these are calling",
    "start": "1909679",
    "end": "1916100"
  },
  {
    "text": "it I mean there shouldn't be much of anything I think most of it is Idol the second",
    "start": "1916100",
    "end": "1922760"
  },
  {
    "text": "that so most of the work done by Docker is done when it's creating the image mounting it all up and then staging it",
    "start": "1922760",
    "end": "1929240"
  },
  {
    "text": "out to the logo node and then it's idle until the job's done um it's just moving the i o back and",
    "start": "1929240",
    "end": "1935779"
  },
  {
    "text": "forth um got it okay",
    "start": "1935779",
    "end": "1940778"
  },
  {
    "text": "all right well thank you uh give me a second and then the existing stuff you know odd man's log should work",
    "start": "1943700",
    "end": "1952419"
  },
  {
    "text": "I mean you're just paying the price for having podman or Docker do the log movement and",
    "start": "1953240",
    "end": "1959480"
  },
  {
    "text": "then storing it somewhere uh slurm does the direct writing of the Json log files",
    "start": "1959480",
    "end": "1965679"
  },
  {
    "text": "so hopefully that's not gonna result in too much overhead",
    "start": "1965679",
    "end": "1972320"
  },
  {
    "text": "I mean unless the logs a lot but in which case you can always just",
    "start": "1973340",
    "end": "1978559"
  },
  {
    "text": "tell slurm to log into a different file and not have it go through docker",
    "start": "1978559",
    "end": "1983679"
  },
  {
    "text": "maybe I'll ask another question so if you if you were to do something like a podman images",
    "start": "1989620",
    "end": "1995899"
  },
  {
    "text": "um to list them so because I mean you can have them on like a shared items would you see your own images only or",
    "start": "1995899",
    "end": "2002200"
  },
  {
    "text": "everyone's images at that point because I mean you made some comment about um like",
    "start": "2002200",
    "end": "2008019"
  },
  {
    "text": "you know what what is run in as a user and what isn't",
    "start": "2008019",
    "end": "2013360"
  },
  {
    "text": "um podman here is running as a user it's all specific to the user and yes that is",
    "start": "2013360",
    "end": "2018880"
  },
  {
    "text": "a possibility but that's actually outside of slurm I mean you can configure Docker apartment as you please",
    "start": "2018880",
    "end": "2025260"
  },
  {
    "text": "uh slurm only becomes involved the second calls scram and then storm is going to",
    "start": "2025260",
    "end": "2030580"
  },
  {
    "text": "push the image back and forth to the computer notes as needed uh and yeah you can do all the fun",
    "start": "2030580",
    "end": "2037059"
  },
  {
    "text": "Docker um caching if you want you know I'd expect any large site to have their",
    "start": "2037059",
    "end": "2042399"
  },
  {
    "text": "own internal cash that they control and bless certain images and whatnot",
    "start": "2042399",
    "end": "2048220"
  },
  {
    "text": "although there are you know more open sites out there that could just let you pull from Docker Hub as you please",
    "start": "2048220",
    "end": "2053560"
  },
  {
    "text": "that's definitely up to the site to decide it's nothing that I that we're going to apply limitations on",
    "start": "2053560",
    "end": "2060398"
  },
  {
    "text": "and and I mean so at some point you showed that it's pushing the the",
    "start": "2060399",
    "end": "2065800"
  },
  {
    "text": "um like I guess the extracted image um like I mean you have it here it's like a",
    "start": "2065800",
    "end": "2071020"
  },
  {
    "text": "surf containers thread something um so does this also like if you were to",
    "start": "2071020",
    "end": "2076179"
  },
  {
    "text": "submit similar containers or does it do any lead applications is this like the",
    "start": "2076179",
    "end": "2082118"
  },
  {
    "text": "common shared file system for for the um unpacked images or",
    "start": "2082119",
    "end": "2088179"
  },
  {
    "text": "uh that doesn't exist yet right now it's just a simple push of you know coin",
    "start": "2088179",
    "end": "2093339"
  },
  {
    "text": "rsync um deduplication is something I definitely like to look into the future",
    "start": "2093339",
    "end": "2098800"
  },
  {
    "text": "but it's not something we do right now there will be as well along this lines that you so you you have the images you",
    "start": "2098800",
    "end": "2105520"
  },
  {
    "text": "should put my images right and to create the image this is unpacked or a smooth mounted on",
    "start": "2105520",
    "end": "2112540"
  },
  {
    "text": "this particular then you us to uh to the",
    "start": "2112540",
    "end": "2118119"
  },
  {
    "text": "compute node but usually what what I see is that hot man creates images",
    "start": "2118119",
    "end": "2125200"
  },
  {
    "text": "so by everyone on a shared file system and then just Loop mounted on the on the compute node because some compute nodes",
    "start": "2125200",
    "end": "2132160"
  },
  {
    "text": "don't have storage right so local storage so you want to just Loop Mount those yeah um for everybody it's",
    "start": "2132160",
    "end": "2138460"
  },
  {
    "text": "different um that's why I'm giving as much flexibility as possible I mean yeah I'll",
    "start": "2138460",
    "end": "2144040"
  },
  {
    "text": "show you the um I gotta leave the music modified so",
    "start": "2144040",
    "end": "2150160"
  },
  {
    "text": "let's see um but not it's too hard to to not copy",
    "start": "2150160",
    "end": "2155320"
  },
  {
    "text": "over the the root file system of the compute node but just Loop mount the image on the compute node instead",
    "start": "2155320",
    "end": "2161740"
  },
  {
    "text": "like yeah you could do something along those lines this is the simplest version uh but you know I set up with is just",
    "start": "2161740",
    "end": "2168339"
  },
  {
    "text": "calling our sync okay but you could definitely do a mount",
    "start": "2168339",
    "end": "2173800"
  },
  {
    "text": "um yeah I'm accounting for all these things because I mean I have sites that have",
    "start": "2173800",
    "end": "2179200"
  },
  {
    "text": "really fast local file systems fast luster files slim gpfs stuff like",
    "start": "2179200",
    "end": "2184660"
  },
  {
    "text": "that so I want to be able to make it work for them and then we have sites with the ultra slow egress and Ingress of pushing",
    "start": "2184660",
    "end": "2191140"
  },
  {
    "text": "the images out to the cloud um I mean and I'm giving as many options",
    "start": "2191140",
    "end": "2197800"
  },
  {
    "text": "here I mean future work could definitely go to uh making that faster and deduplication",
    "start": "2197800",
    "end": "2203980"
  },
  {
    "text": "stuff along those lines yeah so the this slurm stage in allocator would be the magic tool or the",
    "start": "2203980",
    "end": "2210400"
  },
  {
    "text": "place to make magic work in different ways um yeah I mean I'd hope most sites could",
    "start": "2210400",
    "end": "2215980"
  },
  {
    "text": "just use our sync but definitely configurable like this is the actual thing that's being called for",
    "start": "2215980",
    "end": "2222040"
  },
  {
    "text": "what I'm the demo that I'm doing right now um it's extracting out of the environment who's running",
    "start": "2222040",
    "end": "2228460"
  },
  {
    "text": "uh it's I have surf containers as the shared file system that's pushed out to everybody which is just a really simple",
    "start": "2228460",
    "end": "2235420"
  },
  {
    "text": "volume that I have mounted everywhere in this demo and all right go on again the life cycle",
    "start": "2235420",
    "end": "2242440"
  },
  {
    "text": "like maybe mapping in a GPU is it also a callback or like a function",
    "start": "2242440",
    "end": "2248680"
  },
  {
    "text": "that you you can't prepare to make it work for different computers if you have like two queues one with gpus one",
    "start": "2248680",
    "end": "2255640"
  },
  {
    "text": "without um so if you want to switch between what you're calling and go back to Fred my",
    "start": "2255640",
    "end": "2262540"
  },
  {
    "text": "hilarious press user you'll just be passing it as the environment um to the container",
    "start": "2262540",
    "end": "2268780"
  },
  {
    "text": "so let's see actually remember how to do a podman give me one second but um when slime is",
    "start": "2268780",
    "end": "2277000"
  },
  {
    "text": "called so SC run the environment that exists there at the time of um you can uh set any environment",
    "start": "2277000",
    "end": "2284380"
  },
  {
    "text": "variable and slim will read it and process it just as normal so if you do uh go online with export",
    "start": "2284380",
    "end": "2291838"
  },
  {
    "text": "let me just Define the command for you the docs aren't written yet but um the same all the same environment variables",
    "start": "2293380",
    "end": "2299619"
  },
  {
    "text": "for S Alec will exist for this too so uh",
    "start": "2299619",
    "end": "2305280"
  },
  {
    "text": "so you could do something like this uh inside of the tub",
    "start": "2305920",
    "end": "2312940"
  },
  {
    "text": "I don't know what podman does to export environments and yeah we'll have to document this",
    "start": "2312940",
    "end": "2319359"
  },
  {
    "text": "better so one of the things that we are hitting",
    "start": "2319359",
    "end": "2324820"
  },
  {
    "text": "is that um we can't do something as simple as this and this is a limitation of Docker and podman so what I really would like",
    "start": "2324820",
    "end": "2331660"
  },
  {
    "text": "to be able to do this but the actual calling environment",
    "start": "2331660",
    "end": "2337619"
  },
  {
    "text": "doesn't get passed by automatic or Docker it might actually get passed by podman",
    "start": "2339700",
    "end": "2346180"
  },
  {
    "text": "um but the expectation is I will we will be getting the environment so",
    "start": "2346180",
    "end": "2351760"
  },
  {
    "text": "you can export it oh it's not nice I would expect this to be like",
    "start": "2351760",
    "end": "2358119"
  },
  {
    "text": "configured on the host and then you don't need to pass anything it's just depending on where you're oh yeah yeah",
    "start": "2358119",
    "end": "2363339"
  },
  {
    "text": "so um the thing that I do know that does absolutely work right now is passing it",
    "start": "2363339",
    "end": "2368440"
  },
  {
    "text": "in the config file so in one second uh when I put that",
    "start": "2368440",
    "end": "2375180"
  },
  {
    "text": "see container and there's the comp",
    "start": "2375180",
    "end": "2381220"
  },
  {
    "text": "um so if you pass the environment here but that's what",
    "start": "2381220",
    "end": "2386880"
  },
  {
    "text": "and where it's actually running right so that the global configuration for all your jobs uh this is the one for all of",
    "start": "2386880",
    "end": "2394060"
  },
  {
    "text": "them but you could definitely do a per user one if you feel like it and",
    "start": "2394060",
    "end": "2399579"
  },
  {
    "text": "um if you really want to be able to customize it that's what the spank and the CLI filter plugins are for",
    "start": "2399579",
    "end": "2405099"
  },
  {
    "text": "that the user if the site really wants to do something smart so I'm working on getting all those to",
    "start": "2405099",
    "end": "2410980"
  },
  {
    "text": "work together the passing the environment is there it does work you can do it here which I don't know the format off and",
    "start": "2410980",
    "end": "2417880"
  },
  {
    "text": "container.com",
    "start": "2417880",
    "end": "2420838"
  },
  {
    "text": "but you just pass Ed don't worry I'll give it to myself I think maybe other people have questions",
    "start": "2423099",
    "end": "2430000"
  },
  {
    "text": "as well so no no these are definitely valid concerns I mean I'm more than happy to take all these questions because it helps me make sure I'm not",
    "start": "2430000",
    "end": "2436000"
  },
  {
    "text": "missing something um yes the whole goal is that you can pass all the normal slurm uh confident",
    "start": "2436000",
    "end": "2442720"
  },
  {
    "text": "you can say number nodes gpus per node all that stuff um and even if you want to do multiple",
    "start": "2442720",
    "end": "2448240"
  },
  {
    "text": "nodes and stuff like that um as long as you make sure that the image is pushed out to all those nodes it'll run",
    "start": "2448240",
    "end": "2454839"
  },
  {
    "text": "or with the shared file system you don't even have to worry about it yeah maybe just to to state my use case",
    "start": "2454839",
    "end": "2461200"
  },
  {
    "text": "let's say I have two two queues one older cue one newer Q what I would usually do with Sarah's Putman whatever",
    "start": "2461200",
    "end": "2467859"
  },
  {
    "text": "I would configure the hook on the the container of oci hook on the compute node to pick up the correct MPI",
    "start": "2467859",
    "end": "2474760"
  },
  {
    "text": "libraries to be pushed into the Container right so it's a oh on a compute node configuration so I and I",
    "start": "2474760",
    "end": "2481480"
  },
  {
    "text": "would need this compute node hooks to be triggered somehow so I guess my this um",
    "start": "2481480",
    "end": "2487180"
  },
  {
    "text": "stage in a location thing would need to reach out to the host stage the",
    "start": "2487180",
    "end": "2492220"
  },
  {
    "text": "container on the compute node instead of the login node so that you put the books",
    "start": "2492220",
    "end": "2497440"
  },
  {
    "text": "and then it's independent on configuration of the topic node it's just reaching out to the compute node",
    "start": "2497440",
    "end": "2502599"
  },
  {
    "text": "make sure that all the hooks for the compute node are run and then the container is maybe already present on",
    "start": "2502599",
    "end": "2507880"
  },
  {
    "text": "the compute node so I would paste the container on the compute node is what I'm saying I guess",
    "start": "2507880",
    "end": "2514380"
  },
  {
    "text": "um my thought is that the users would run a Docker pod member there but you can definitely run it on the computer node",
    "start": "2514660",
    "end": "2521740"
  },
  {
    "text": "instead by using S batch that's definitely an option open option",
    "start": "2521740",
    "end": "2526920"
  },
  {
    "text": "and your use case here making sure the MPI hooks are called I'm gonna go and double check but that's definitely",
    "start": "2526920",
    "end": "2532359"
  },
  {
    "text": "something I want to get working if it doesn't work already so yeah thanks for pointing that one out",
    "start": "2532359",
    "end": "2537400"
  },
  {
    "text": "to me I got to make sure that works cool no worries",
    "start": "2537400",
    "end": "2544260"
  },
  {
    "text": "the fun of development uh any other questions I'm I'm very open",
    "start": "2545560",
    "end": "2551500"
  },
  {
    "text": "though oh yeah Docker build does work um",
    "start": "2551500",
    "end": "2556920"
  },
  {
    "text": "well in this case podman build will work but uh yeah there's no reason that won't work",
    "start": "2556920",
    "end": "2563320"
  },
  {
    "text": "and it'll actually build on the compute node so if you want to do you know GCC uh M",
    "start": "2563320",
    "end": "2569079"
  },
  {
    "text": "native you know you can have that done as long as your compete nodes are heterogeneous uh homogeneous",
    "start": "2569079",
    "end": "2576160"
  },
  {
    "text": "or you just be careful to make sure you compile on the right node",
    "start": "2576160",
    "end": "2580680"
  },
  {
    "text": "um I had a question based on your um first diagram that you're talking through similar to what Clements was",
    "start": "2585099",
    "end": "2590200"
  },
  {
    "text": "asking I think uh just a couple of things when for a basic Docker run I think I",
    "start": "2590200",
    "end": "2597460"
  },
  {
    "text": "understand your login node and uh it's effectively then",
    "start": "2597460",
    "end": "2603339"
  },
  {
    "text": "instructing the compute node to do the business what's the kind of flow if you want to run many jobs how does that",
    "start": "2603339",
    "end": "2609700"
  },
  {
    "text": "actually work you just hit it many times um I would expect if you want to run a",
    "start": "2609700",
    "end": "2616240"
  },
  {
    "text": "ton of jobs you could just pass the environment request they'd give it an array so you know use this image but do an",
    "start": "2616240",
    "end": "2624280"
  },
  {
    "text": "array of 10 000 jobs something like that so you don't have to pay the cost of the",
    "start": "2624280",
    "end": "2629319"
  },
  {
    "text": "pushing stage in and out each time but that's definitely a possibility if",
    "start": "2629319",
    "end": "2634720"
  },
  {
    "text": "you want to the actual layers of the image and so forth that flows through the login node on your diagram is that",
    "start": "2634720",
    "end": "2641680"
  },
  {
    "text": "right um when stage out runs or yeah when stage",
    "start": "2641680",
    "end": "2647859"
  },
  {
    "text": "and run sorry stage out is one of pulls it back so stay tuned",
    "start": "2647859",
    "end": "2654220"
  },
  {
    "text": "uh so this is just a log for it uh what",
    "start": "2654220",
    "end": "2661300"
  },
  {
    "text": "stage it runs it's running inside of the namespace provided by podman or docker",
    "start": "2661300",
    "end": "2666339"
  },
  {
    "text": "in this case I just do a really simple rsync of pushing the final mounts out",
    "start": "2666339",
    "end": "2671800"
  },
  {
    "text": "there more advanced um image slicing is definitely something",
    "start": "2671800",
    "end": "2676900"
  },
  {
    "text": "I want to look into but for now it's not there and how does that scale if you are",
    "start": "2676900",
    "end": "2682180"
  },
  {
    "text": "running lots of things in parallel was that okay um",
    "start": "2682180",
    "end": "2687960"
  },
  {
    "text": "so it depends on what your file system is if you have a shared file system it's one rsync command push it out to the",
    "start": "2687960",
    "end": "2694900"
  },
  {
    "text": "file system and then the file system handle the dirty details of that I mean you got Thunder you heard problems but that's",
    "start": "2694900",
    "end": "2700300"
  },
  {
    "text": "nothing new um and then with this case of the job you just tell it hey I want",
    "start": "2700300",
    "end": "2706660"
  },
  {
    "text": "X number nodes for this and slurm will run it just like any other slurm job so I want a thousand nodes running this",
    "start": "2706660",
    "end": "2713319"
  },
  {
    "text": "point it to this one image I just pointed uh pushed out to the shared file system and it'll run just like any other",
    "start": "2713319",
    "end": "2719140"
  },
  {
    "text": "swim job right I see um if the user wants to run a thousand",
    "start": "2719140",
    "end": "2724180"
  },
  {
    "text": "different jobs um you have to pay the price of pushing",
    "start": "2724180",
    "end": "2729220"
  },
  {
    "text": "out to the file system and that's definitely something we got to work on later but um",
    "start": "2729220",
    "end": "2734319"
  },
  {
    "text": "baby steps",
    "start": "2734319",
    "end": "2737520"
  },
  {
    "text": "cool okay well that's that's great I mean we've got 10 minutes left has anyone got any final",
    "start": "2740980",
    "end": "2746319"
  },
  {
    "text": "questions nope and I'll point out here that when",
    "start": "2746319",
    "end": "2752200"
  },
  {
    "text": "the stage in is called it's inside the namespace and so I just have it logging right now where the config is",
    "start": "2752200",
    "end": "2758260"
  },
  {
    "text": "so in this case it's where podman has the overlay FS and then there's the config file",
    "start": "2758260",
    "end": "2766540"
  },
  {
    "text": "anybody who wants to go into advanced case of parsing the conflict file could go and grab each slice and only push",
    "start": "2766540",
    "end": "2772720"
  },
  {
    "text": "those out as needed and that's definitely a possibility but it's not something I'm implementing right now",
    "start": "2772720",
    "end": "2779818"
  },
  {
    "text": "sorry did we get the question in the chat already does Docker build work was asked by Timothy yeah it works",
    "start": "2782980",
    "end": "2790599"
  },
  {
    "text": "um definitely if it doesn't work it'd be a bug but uh it will build on the compute node that's the important part",
    "start": "2790599",
    "end": "2796720"
  },
  {
    "text": "so you have to make sure that you understand that um especially if you're calling compile or GCC or something fun like that call",
    "start": "2796720",
    "end": "2803079"
  },
  {
    "text": "an M native or something along those lines just need to make sure that where you execute the job",
    "start": "2803079",
    "end": "2809079"
  },
  {
    "text": "it matches to where you can you have to make sure that it matches where you build it to where you execute",
    "start": "2809079",
    "end": "2815079"
  },
  {
    "text": "it so you don't have incompatibility which in most cases I had some site would just set up a queue for it that",
    "start": "2815079",
    "end": "2821440"
  },
  {
    "text": "would be shared between the two because we're taking over the S um The",
    "start": "2821440",
    "end": "2829060"
  },
  {
    "text": "OSI runtime now that's where slime gets uh introduced so Docker build calls that",
    "start": "2829060",
    "end": "2835300"
  },
  {
    "text": "anyway so it's just getting pushed out to that instead",
    "start": "2835300",
    "end": "2841800"
  },
  {
    "text": "great okay well thanks very much night appreciate you taking the time really",
    "start": "2845140",
    "end": "2850480"
  },
  {
    "text": "interesting stuff I realized I skipped over at the",
    "start": "2850480",
    "end": "2856000"
  },
  {
    "text": "beginning we normally make sure that everyone's uh put their names on the attendee list on the Google and uh not",
    "start": "2856000",
    "end": "2862000"
  },
  {
    "text": "straight into your presentation sorry wonderful um and if there's any new joiners or new people who haven't been before just",
    "start": "2862000",
    "end": "2867579"
  },
  {
    "text": "stick your hands up and say hi I can see a couple of names uh El Ricardo's here with no microphone",
    "start": "2867579",
    "end": "2874500"
  },
  {
    "text": "uh yeah we have one more session before cubecon so in two weeks time uh which",
    "start": "2874500",
    "end": "2880359"
  },
  {
    "text": "will be the 19th of October which is just before kubecon um",
    "start": "2880359",
    "end": "2886000"
  },
  {
    "text": "I think Ricardo's saying the Gateway API possibly for the next session we'll confirm that and",
    "start": "2886000",
    "end": "2891339"
  },
  {
    "text": "some notifications on slack and email other than that we'll also work on",
    "start": "2891339",
    "end": "2896500"
  },
  {
    "text": "putting together a list of gender items uh following cubecon because yeah",
    "start": "2896500",
    "end": "2903220"
  },
  {
    "text": "we've gone to the end of it I think I'll be a keep going in Detroit um anybody's welcome to talk to me",
    "start": "2903220",
    "end": "2909099"
  },
  {
    "text": "especially on Monday in the batch day yeah I will be there too so if you can meet your life",
    "start": "2909099",
    "end": "2916319"
  },
  {
    "text": "um is anyone else coming actually how this group uh yeah I'll be there this is Kevin",
    "start": "2916480",
    "end": "2922240"
  },
  {
    "text": "Kevin cool um and I'm um I'm attending that's my",
    "start": "2922240",
    "end": "2927880"
  },
  {
    "text": "plan if I can get a hotel in downtown Detroit it's really hard holy cow it was",
    "start": "2927880",
    "end": "2933460"
  },
  {
    "text": "so hard for me to go to a hotel I had to go there like every hour and just check hotels.com until one just popped up",
    "start": "2933460",
    "end": "2940060"
  },
  {
    "text": "because somebody canceled yeah that's what I'm doing right now my colleague ended up on an Airbnb like in Midtown",
    "start": "2940060",
    "end": "2946359"
  },
  {
    "text": "he's getting some he's gonna get a tram every day he reckons or something yeah the thing that sucks is that if he could",
    "start": "2946359",
    "end": "2952540"
  },
  {
    "text": "just go to on the other side of uh the water in Canada they have so many hotels",
    "start": "2952540",
    "end": "2958119"
  },
  {
    "text": "available but you gotta you have to have a car to pass through customs every day I was looking at it I was like ah can I",
    "start": "2958119",
    "end": "2964780"
  },
  {
    "text": "pull this off our corporate booking system recommends hotels just based on radius and ultimately yeah rice and in",
    "start": "2964780",
    "end": "2971800"
  },
  {
    "text": "some place called Windsor I was like yeah sure they're closer look at that and it's only like one third of a mile",
    "start": "2971800",
    "end": "2977260"
  },
  {
    "text": "so you could totally walk right no no I actually checked that so the Ambassador Bridge they do not allow you to walk",
    "start": "2977260",
    "end": "2983560"
  },
  {
    "text": "across right exactly it sucks because I'm like I could walk this easily every day",
    "start": "2983560",
    "end": "2989079"
  },
  {
    "text": "yeah yeah yeah it's uh it was a bit of a football",
    "start": "2989079",
    "end": "2995818"
  },
  {
    "text": "oh okay all right well I think that's it then thank you everyone for your time and uh see you next time and then see",
    "start": "2996700",
    "end": "3003060"
  },
  {
    "text": "some of your coupon as well yep feel free to hit me up with uh questions or whatever comments on the slack Channel",
    "start": "3003060",
    "end": "3009780"
  },
  {
    "text": "I'm definitely hoping that hearing stuff like uh Christian Singles MPI I'll make I definitely got to make sure that works",
    "start": "3009780",
    "end": "3016440"
  },
  {
    "text": "and the intent of this is not to replace uh Cyrus but to get the container stuff",
    "start": "3016440",
    "end": "3022140"
  },
  {
    "text": "as a as a properly supported part of slurm and SARS can be bolted on top",
    "start": "3022140",
    "end": "3029040"
  },
  {
    "text": "they do it's extra Magic good stuff all right thank you everyone",
    "start": "3029040",
    "end": "3035280"
  },
  {
    "text": "see you later",
    "start": "3035280",
    "end": "3038119"
  }
]