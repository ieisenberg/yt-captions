[
  {
    "text": "hi everyone in today's talk we'll be",
    "start": "0",
    "end": "2960"
  },
  {
    "text": "addressing creating an open platform for",
    "start": "2960",
    "end": "4880"
  },
  {
    "text": "equity trading",
    "start": "4880",
    "end": "5839"
  },
  {
    "text": "at scale using analytics and deep",
    "start": "5839",
    "end": "9040"
  },
  {
    "text": "learning",
    "start": "9040",
    "end": "9519"
  },
  {
    "text": "as the tools for the underlying platform",
    "start": "9519",
    "end": "12639"
  },
  {
    "text": "the goal is to implement both a",
    "start": "12639",
    "end": "14880"
  },
  {
    "text": "development",
    "start": "14880",
    "end": "15759"
  },
  {
    "text": "and a production component of the system",
    "start": "15759",
    "end": "19920"
  },
  {
    "text": "in a production level trading we care",
    "start": "19920",
    "end": "22160"
  },
  {
    "text": "about too many aspects",
    "start": "22160",
    "end": "23519"
  },
  {
    "text": "not just the stock price we care about",
    "start": "23519",
    "end": "25920"
  },
  {
    "text": "high and low in the day",
    "start": "25920",
    "end": "27519"
  },
  {
    "text": "volume which many primitive trading",
    "start": "27519",
    "end": "30000"
  },
  {
    "text": "examples rely on",
    "start": "30000",
    "end": "31279"
  },
  {
    "text": "for example the relationship between",
    "start": "31279",
    "end": "33440"
  },
  {
    "text": "precious metals",
    "start": "33440",
    "end": "35120"
  },
  {
    "text": "like gold and silver availability of",
    "start": "35120",
    "end": "38719"
  },
  {
    "text": "social media",
    "start": "38719",
    "end": "39600"
  },
  {
    "text": "feeds automatic analysis of analysis",
    "start": "39600",
    "end": "43200"
  },
  {
    "text": "reports",
    "start": "43200",
    "end": "44559"
  },
  {
    "text": "like sec reports or earning calls",
    "start": "44559",
    "end": "48800"
  },
  {
    "text": "calculation of momentum and cash",
    "start": "48800",
    "end": "51520"
  },
  {
    "text": "valuation",
    "start": "51520",
    "end": "53360"
  },
  {
    "text": "in addition to this we also have to",
    "start": "53360",
    "end": "55440"
  },
  {
    "text": "comprehend the analysis with market",
    "start": "55440",
    "end": "57920"
  },
  {
    "text": "indicators",
    "start": "57920",
    "end": "59039"
  },
  {
    "text": "volatility indices as well as",
    "start": "59039",
    "end": "62079"
  },
  {
    "text": "many other aspects that will have to get",
    "start": "62079",
    "end": "64320"
  },
  {
    "text": "integrated",
    "start": "64320",
    "end": "65198"
  },
  {
    "text": "into the model",
    "start": "65199",
    "end": "67840"
  },
  {
    "text": "this is the workflow of a single asset",
    "start": "68479",
    "end": "71360"
  },
  {
    "text": "whether being",
    "start": "71360",
    "end": "72000"
  },
  {
    "text": "a stock or a metal or commodity and so",
    "start": "72000",
    "end": "74080"
  },
  {
    "text": "on sources",
    "start": "74080",
    "end": "75759"
  },
  {
    "text": "and inputs to the system would include",
    "start": "75759",
    "end": "78159"
  },
  {
    "text": "pricing",
    "start": "78159",
    "end": "78960"
  },
  {
    "text": "volume media like videos and sound and",
    "start": "78960",
    "end": "83040"
  },
  {
    "text": "news and",
    "start": "83040",
    "end": "83759"
  },
  {
    "text": "text tweets facebook",
    "start": "83759",
    "end": "87439"
  },
  {
    "text": "statuses company financial data in",
    "start": "87439",
    "end": "90400"
  },
  {
    "text": "addition",
    "start": "90400",
    "end": "91200"
  },
  {
    "text": "to historical data once sources are",
    "start": "91200",
    "end": "94079"
  },
  {
    "text": "available",
    "start": "94079",
    "end": "94799"
  },
  {
    "text": "they are processed through stream",
    "start": "94799",
    "end": "97280"
  },
  {
    "text": "processing",
    "start": "97280",
    "end": "98560"
  },
  {
    "text": "polling and batch processing data will",
    "start": "98560",
    "end": "101439"
  },
  {
    "text": "be explored visualized",
    "start": "101439",
    "end": "103360"
  },
  {
    "text": "cleansed and augmented and in reached",
    "start": "103360",
    "end": "106799"
  },
  {
    "text": "once data is ready we start building the",
    "start": "106799",
    "end": "109920"
  },
  {
    "text": "model",
    "start": "109920",
    "end": "110479"
  },
  {
    "text": "and we start validating this model",
    "start": "110479",
    "end": "113360"
  },
  {
    "text": "finally",
    "start": "113360",
    "end": "114000"
  },
  {
    "text": "the model gets through cycles of testing",
    "start": "114000",
    "end": "116320"
  },
  {
    "text": "until",
    "start": "116320",
    "end": "117280"
  },
  {
    "text": "it's ready for deployment that's",
    "start": "117280",
    "end": "120320"
  },
  {
    "text": "when it's monitored for accuracy and",
    "start": "120320",
    "end": "122320"
  },
  {
    "text": "performance post deployment",
    "start": "122320",
    "end": "125840"
  },
  {
    "text": "in this slide we're showing an example",
    "start": "126640",
    "end": "129039"
  },
  {
    "text": "of developing production clusters",
    "start": "129039",
    "end": "131120"
  },
  {
    "text": "for example in the development side the",
    "start": "131120",
    "end": "132560"
  },
  {
    "text": "tensorflow model",
    "start": "132560",
    "end": "134480"
  },
  {
    "text": "will be driving the training service",
    "start": "134480",
    "end": "136720"
  },
  {
    "text": "based on",
    "start": "136720",
    "end": "137760"
  },
  {
    "text": "long short-term memory to predict the",
    "start": "137760",
    "end": "139760"
  },
  {
    "text": "price and volume of an asset while in",
    "start": "139760",
    "end": "142239"
  },
  {
    "text": "production",
    "start": "142239",
    "end": "142879"
  },
  {
    "text": "we'll be deploying this this model at",
    "start": "142879",
    "end": "145200"
  },
  {
    "text": "scale",
    "start": "145200",
    "end": "147599"
  },
  {
    "text": "to give a sense of the complexity of the",
    "start": "148400",
    "end": "150239"
  },
  {
    "text": "trading floor",
    "start": "150239",
    "end": "152000"
  },
  {
    "text": "there are multiple types of trading",
    "start": "152000",
    "end": "154640"
  },
  {
    "text": "desks",
    "start": "154640",
    "end": "155760"
  },
  {
    "text": "for example equity trading desks would",
    "start": "155760",
    "end": "157840"
  },
  {
    "text": "handle everything from equity trading to",
    "start": "157840",
    "end": "160080"
  },
  {
    "text": "exotic options trading",
    "start": "160080",
    "end": "161760"
  },
  {
    "text": "fixed income trading desks would handle",
    "start": "161760",
    "end": "164319"
  },
  {
    "text": "government bonds corporate bonds and",
    "start": "164319",
    "end": "166080"
  },
  {
    "text": "other bonds and bond",
    "start": "166080",
    "end": "167519"
  },
  {
    "text": "like instruments that pay a yield",
    "start": "167519",
    "end": "170400"
  },
  {
    "text": "foreign exchange",
    "start": "170400",
    "end": "171519"
  },
  {
    "text": "trading desks facilitate trading and",
    "start": "171519",
    "end": "174560"
  },
  {
    "text": "currency payers by acting as market",
    "start": "174560",
    "end": "177280"
  },
  {
    "text": "makers",
    "start": "177280",
    "end": "178480"
  },
  {
    "text": "they can also engage in proprietary",
    "start": "178480",
    "end": "180159"
  },
  {
    "text": "trading activities",
    "start": "180159",
    "end": "181599"
  },
  {
    "text": "commodity trading desks are focused on",
    "start": "181599",
    "end": "183360"
  },
  {
    "text": "agriculture products",
    "start": "183360",
    "end": "184879"
  },
  {
    "text": "metals and other commodities such as",
    "start": "184879",
    "end": "187120"
  },
  {
    "text": "crude oil gold and coffee",
    "start": "187120",
    "end": "189280"
  },
  {
    "text": "each desk has multiple personas like",
    "start": "189280",
    "end": "192400"
  },
  {
    "text": "brokers hedgers managers",
    "start": "192400",
    "end": "195599"
  },
  {
    "text": "risk managers and so on",
    "start": "195599",
    "end": "199840"
  },
  {
    "text": "in our architecture we have multiple",
    "start": "199920",
    "end": "201840"
  },
  {
    "text": "clusters for each assets",
    "start": "201840",
    "end": "204080"
  },
  {
    "text": "for each asset so clusters for stocks",
    "start": "204080",
    "end": "207040"
  },
  {
    "text": "clusters for precious metals",
    "start": "207040",
    "end": "209519"
  },
  {
    "text": "another one for commodities another one",
    "start": "209519",
    "end": "211680"
  },
  {
    "text": "from the mdcs and so on",
    "start": "211680",
    "end": "215040"
  },
  {
    "text": "in this we show the need for kubernetes",
    "start": "217680",
    "end": "220560"
  },
  {
    "text": "as our platform",
    "start": "220560",
    "end": "222159"
  },
  {
    "text": "to handle all the complications",
    "start": "222159",
    "end": "223760"
  },
  {
    "text": "surrounding the execution of the",
    "start": "223760",
    "end": "225200"
  },
  {
    "text": "prediction service",
    "start": "225200",
    "end": "226560"
  },
  {
    "text": "on a single asset really the size of the",
    "start": "226560",
    "end": "229120"
  },
  {
    "text": "box here",
    "start": "229120",
    "end": "230000"
  },
  {
    "text": "is entailing the complexity as we",
    "start": "230000",
    "end": "232879"
  },
  {
    "text": "captured it from scully and others",
    "start": "232879",
    "end": "235120"
  },
  {
    "text": "in their published reference at the",
    "start": "235120",
    "end": "238400"
  },
  {
    "text": "bottom",
    "start": "238400",
    "end": "239120"
  },
  {
    "text": "but the idea simply here is there are a",
    "start": "239120",
    "end": "242080"
  },
  {
    "text": "lot of",
    "start": "242080",
    "end": "242560"
  },
  {
    "text": "other services and tasks and processes",
    "start": "242560",
    "end": "246319"
  },
  {
    "text": "that",
    "start": "246319",
    "end": "246879"
  },
  {
    "text": "go before and around and after the",
    "start": "246879",
    "end": "249519"
  },
  {
    "text": "machine learning",
    "start": "249519",
    "end": "250720"
  },
  {
    "text": "code is done and being able to",
    "start": "250720",
    "end": "252480"
  },
  {
    "text": "comprehend all of this is extremely",
    "start": "252480",
    "end": "254480"
  },
  {
    "text": "important",
    "start": "254480",
    "end": "255200"
  },
  {
    "text": "for the valid execution of the system",
    "start": "255200",
    "end": "259280"
  },
  {
    "text": "from a machine learning perspective this",
    "start": "259280",
    "end": "261199"
  },
  {
    "text": "is the machine learning path",
    "start": "261199",
    "end": "262880"
  },
  {
    "text": "where the data gets ingested analyze",
    "start": "262880",
    "end": "265520"
  },
  {
    "text": "transformed validated",
    "start": "265520",
    "end": "267040"
  },
  {
    "text": "you split it in the model you build the",
    "start": "267040",
    "end": "268800"
  },
  {
    "text": "model you validate it",
    "start": "268800",
    "end": "270240"
  },
  {
    "text": "then you augment with the training at",
    "start": "270240",
    "end": "272479"
  },
  {
    "text": "scale",
    "start": "272479",
    "end": "273280"
  },
  {
    "text": "you start rolling out the model and",
    "start": "273280",
    "end": "275600"
  },
  {
    "text": "serving it and then you monitor and log",
    "start": "275600",
    "end": "277600"
  },
  {
    "text": "it",
    "start": "277600",
    "end": "278160"
  },
  {
    "text": "and we'll be talking about each stage in",
    "start": "278160",
    "end": "280400"
  },
  {
    "text": "the coming few slides",
    "start": "280400",
    "end": "283840"
  },
  {
    "text": "for each asset we'll always have to",
    "start": "286160",
    "end": "288320"
  },
  {
    "text": "ensure that",
    "start": "288320",
    "end": "290000"
  },
  {
    "text": "we're remembering that components of",
    "start": "290000",
    "end": "291840"
  },
  {
    "text": "development and production",
    "start": "291840",
    "end": "293600"
  },
  {
    "text": "they have to be dealt with separately so",
    "start": "293600",
    "end": "296160"
  },
  {
    "text": "that you ensure the agility of the",
    "start": "296160",
    "end": "298160"
  },
  {
    "text": "development while deployment has scaled",
    "start": "298160",
    "end": "300320"
  },
  {
    "text": "for production",
    "start": "300320",
    "end": "303039"
  },
  {
    "text": "as we said before stocks are not the",
    "start": "304800",
    "end": "307759"
  },
  {
    "text": "only asset",
    "start": "307759",
    "end": "308639"
  },
  {
    "text": "there are other assets that are",
    "start": "308639",
    "end": "310800"
  },
  {
    "text": "extremely important",
    "start": "310800",
    "end": "311919"
  },
  {
    "text": "with cross impacts across all of these",
    "start": "311919",
    "end": "314960"
  },
  {
    "text": "components",
    "start": "314960",
    "end": "315759"
  },
  {
    "text": "like precious metals commodities and",
    "start": "315759",
    "end": "317759"
  },
  {
    "text": "bonds",
    "start": "317759",
    "end": "320080"
  },
  {
    "text": "for multiple equities for example stocks",
    "start": "320800",
    "end": "324080"
  },
  {
    "text": "precious metals we use more clusters for",
    "start": "324080",
    "end": "327199"
  },
  {
    "text": "us to be able to comprehend",
    "start": "327199",
    "end": "329120"
  },
  {
    "text": "and run this analysis in separation",
    "start": "329120",
    "end": "331840"
  },
  {
    "text": "since",
    "start": "331840",
    "end": "332400"
  },
  {
    "text": "each one of them will have its own",
    "start": "332400",
    "end": "335440"
  },
  {
    "text": "set of constraints and development and",
    "start": "335440",
    "end": "340240"
  },
  {
    "text": "deployment different from the others so",
    "start": "340240",
    "end": "342479"
  },
  {
    "text": "being able to isolate them is extremely",
    "start": "342479",
    "end": "344560"
  },
  {
    "text": "important",
    "start": "344560",
    "end": "345360"
  },
  {
    "text": "to ensure the continuity of this work",
    "start": "345360",
    "end": "349120"
  },
  {
    "text": "again we're just getting you back to the",
    "start": "349120",
    "end": "351440"
  },
  {
    "text": "machine learning path just to keep in",
    "start": "351440",
    "end": "352800"
  },
  {
    "text": "mind that",
    "start": "352800",
    "end": "353759"
  },
  {
    "text": "not only you have to isolate the",
    "start": "353759",
    "end": "355680"
  },
  {
    "text": "different commodity stocks and",
    "start": "355680",
    "end": "357840"
  },
  {
    "text": "others but you will have to ensure this",
    "start": "357840",
    "end": "360160"
  },
  {
    "text": "process",
    "start": "360160",
    "end": "360880"
  },
  {
    "text": "goes forward with every cluster you",
    "start": "360880",
    "end": "363520"
  },
  {
    "text": "haven't had",
    "start": "363520",
    "end": "365360"
  },
  {
    "text": "the first path which the data is",
    "start": "365360",
    "end": "367280"
  },
  {
    "text": "ingested in and processed for example",
    "start": "367280",
    "end": "370000"
  },
  {
    "text": "the thicker data are fed into the ticker",
    "start": "370000",
    "end": "372400"
  },
  {
    "text": "processing pods",
    "start": "372400",
    "end": "373840"
  },
  {
    "text": "which in turn are ingested into",
    "start": "373840",
    "end": "376080"
  },
  {
    "text": "alexandra",
    "start": "376080",
    "end": "378319"
  },
  {
    "text": "interface pod similarly the tweets are",
    "start": "378319",
    "end": "381520"
  },
  {
    "text": "processed through",
    "start": "381520",
    "end": "382800"
  },
  {
    "text": "pods which are later saved into the",
    "start": "382800",
    "end": "384960"
  },
  {
    "text": "cassandra database",
    "start": "384960",
    "end": "386319"
  },
  {
    "text": "we relied on already published libraries",
    "start": "386319",
    "end": "388639"
  },
  {
    "text": "from",
    "start": "388639",
    "end": "389600"
  },
  {
    "text": "google for example like click to deploy",
    "start": "389600",
    "end": "392000"
  },
  {
    "text": "for the ease of managed services",
    "start": "392000",
    "end": "394160"
  },
  {
    "text": "the scalability is achieved through",
    "start": "394160",
    "end": "397000"
  },
  {
    "text": "vertical.scalar",
    "start": "397000",
    "end": "398400"
  },
  {
    "text": "which actually helps us a lot that we",
    "start": "398400",
    "end": "400319"
  },
  {
    "text": "don't have to worry about it since it's",
    "start": "400319",
    "end": "401919"
  },
  {
    "text": "a managed",
    "start": "401919",
    "end": "402560"
  },
  {
    "text": "service from cloud provider",
    "start": "402560",
    "end": "405919"
  },
  {
    "text": "data analysis segment is extremely",
    "start": "405919",
    "end": "407840"
  },
  {
    "text": "important and",
    "start": "407840",
    "end": "409199"
  },
  {
    "text": "it's really the foundation of building",
    "start": "409199",
    "end": "411759"
  },
  {
    "text": "what comes next",
    "start": "411759",
    "end": "412880"
  },
  {
    "text": "in the model so process data are stored",
    "start": "412880",
    "end": "416000"
  },
  {
    "text": "inside cassandra including twitter",
    "start": "416000",
    "end": "418560"
  },
  {
    "text": "analytics",
    "start": "418560",
    "end": "419759"
  },
  {
    "text": "which have their servers read the",
    "start": "419759",
    "end": "421360"
  },
  {
    "text": "process data and provide the analytics",
    "start": "421360",
    "end": "423520"
  },
  {
    "text": "around it",
    "start": "423520",
    "end": "424479"
  },
  {
    "text": "many are based functions based on",
    "start": "424479",
    "end": "426639"
  },
  {
    "text": "chinese",
    "start": "426639",
    "end": "427759"
  },
  {
    "text": "provided pre-defined analysis",
    "start": "427759",
    "end": "430960"
  },
  {
    "text": "for development part it is computed on",
    "start": "430960",
    "end": "434160"
  },
  {
    "text": "joplin notebook",
    "start": "434160",
    "end": "435599"
  },
  {
    "text": "within the queue flow to provide direct",
    "start": "435599",
    "end": "438560"
  },
  {
    "text": "analysis to the data stored",
    "start": "438560",
    "end": "440479"
  },
  {
    "text": "inside cassandra",
    "start": "440479",
    "end": "443440"
  },
  {
    "text": "data scientists will use the jupiter hub",
    "start": "443680",
    "end": "446720"
  },
  {
    "text": "notebook",
    "start": "446720",
    "end": "447360"
  },
  {
    "text": "installed on the development cluster for",
    "start": "447360",
    "end": "449280"
  },
  {
    "text": "data exploration visualization",
    "start": "449280",
    "end": "451680"
  },
  {
    "text": "and analysis of processed and inferenced",
    "start": "451680",
    "end": "454639"
  },
  {
    "text": "data",
    "start": "454639",
    "end": "455520"
  },
  {
    "text": "that were stored in the cassandra",
    "start": "455520",
    "end": "457840"
  },
  {
    "text": "database",
    "start": "457840",
    "end": "460319"
  },
  {
    "text": "many algorithms have been implemented on",
    "start": "461680",
    "end": "463759"
  },
  {
    "text": "our platform",
    "start": "463759",
    "end": "464800"
  },
  {
    "text": "like correlation momentum martin",
    "start": "464800",
    "end": "468400"
  },
  {
    "text": "angles arima macd and sentiment analysis",
    "start": "468400",
    "end": "472720"
  },
  {
    "text": "and all them work directly on the",
    "start": "472720",
    "end": "475520"
  },
  {
    "text": "cassandra database",
    "start": "475520",
    "end": "478720"
  },
  {
    "text": "we use dataflow where parallel execution",
    "start": "478720",
    "end": "480639"
  },
  {
    "text": "of algorithms to determine the best",
    "start": "480639",
    "end": "482240"
  },
  {
    "text": "correlation",
    "start": "482240",
    "end": "483039"
  },
  {
    "text": "and protection here the multiple",
    "start": "483039",
    "end": "487280"
  },
  {
    "text": "here there are multiple services one",
    "start": "487280",
    "end": "489440"
  },
  {
    "text": "that development service",
    "start": "489440",
    "end": "490639"
  },
  {
    "text": "pods read ingested tweets analyze the",
    "start": "490639",
    "end": "493280"
  },
  {
    "text": "cinnamon",
    "start": "493280",
    "end": "494800"
  },
  {
    "text": "analyze the sentiments through",
    "start": "494800",
    "end": "496720"
  },
  {
    "text": "communication with natural language",
    "start": "496720",
    "end": "498160"
  },
  {
    "text": "processing parsers",
    "start": "498160",
    "end": "499440"
  },
  {
    "text": "and finally recording the analyze tweets",
    "start": "499440",
    "end": "501919"
  },
  {
    "text": "into cassandra database",
    "start": "501919",
    "end": "503599"
  },
  {
    "text": "data scientists test their algorithms",
    "start": "503599",
    "end": "506319"
  },
  {
    "text": "through running jupiter hub",
    "start": "506319",
    "end": "507919"
  },
  {
    "text": "and communicating with the sentiment",
    "start": "507919",
    "end": "509680"
  },
  {
    "text": "analysis service",
    "start": "509680",
    "end": "512878"
  },
  {
    "text": "another threthopods that generates",
    "start": "513919",
    "end": "516560"
  },
  {
    "text": "correlation between two different stocks",
    "start": "516560",
    "end": "519120"
  },
  {
    "text": "gold and other stock this is very useful",
    "start": "519120",
    "end": "521599"
  },
  {
    "text": "in prediction",
    "start": "521599",
    "end": "522479"
  },
  {
    "text": "of prices",
    "start": "522479",
    "end": "525279"
  },
  {
    "text": "some of these filters are",
    "start": "526480",
    "end": "529760"
  },
  {
    "text": "given here as examples in this list of",
    "start": "529760",
    "end": "532000"
  },
  {
    "text": "filters that we'll be talking about",
    "start": "532000",
    "end": "533760"
  },
  {
    "text": "for example an arima filter",
    "start": "533760",
    "end": "536399"
  },
  {
    "text": "autoregressive integrated",
    "start": "536399",
    "end": "537760"
  },
  {
    "text": "moving average on all stocks and testing",
    "start": "537760",
    "end": "540000"
  },
  {
    "text": "them against",
    "start": "540000",
    "end": "540800"
  },
  {
    "text": "actual return for every filter we run",
    "start": "540800",
    "end": "543200"
  },
  {
    "text": "many parallel algorithms",
    "start": "543200",
    "end": "544560"
  },
  {
    "text": "and tested the best possible outcome",
    "start": "544560",
    "end": "547360"
  },
  {
    "text": "from analysis",
    "start": "547360",
    "end": "548080"
  },
  {
    "text": "of variance anova we looked at the",
    "start": "548080",
    "end": "551279"
  },
  {
    "text": "independent two samples t testing",
    "start": "551279",
    "end": "555839"
  },
  {
    "text": "for analysis of covariance we looked at",
    "start": "556560",
    "end": "558800"
  },
  {
    "text": "the covariance as the main",
    "start": "558800",
    "end": "560080"
  },
  {
    "text": "driver including predicted marginal",
    "start": "560080",
    "end": "562720"
  },
  {
    "text": "means",
    "start": "562720",
    "end": "563200"
  },
  {
    "text": "and linear prediction",
    "start": "563200",
    "end": "566880"
  },
  {
    "text": "halt winter tests trends and seasonality",
    "start": "569279",
    "end": "573040"
  },
  {
    "text": "in the data in hand and being able to",
    "start": "573040",
    "end": "575279"
  },
  {
    "text": "understand that is important for",
    "start": "575279",
    "end": "577360"
  },
  {
    "text": "addressing that trending that is",
    "start": "577360",
    "end": "580480"
  },
  {
    "text": "key for addressing the performance",
    "start": "580480",
    "end": "584560"
  },
  {
    "text": "auto regressive moving average arma were",
    "start": "584880",
    "end": "587200"
  },
  {
    "text": "also used",
    "start": "587200",
    "end": "588080"
  },
  {
    "text": "for being able to understand the",
    "start": "588080",
    "end": "591279"
  },
  {
    "text": "the stock behavior",
    "start": "591279",
    "end": "594800"
  },
  {
    "text": "and now getting into the next part as",
    "start": "597360",
    "end": "600160"
  },
  {
    "text": "we're creating",
    "start": "600160",
    "end": "602800"
  },
  {
    "text": "multiple sets of chrome jobs",
    "start": "602800",
    "end": "606320"
  },
  {
    "text": "want to transform data into formats used",
    "start": "606320",
    "end": "608560"
  },
  {
    "text": "by tensorflow another",
    "start": "608560",
    "end": "610720"
  },
  {
    "text": "to is to do the transformation",
    "start": "610720",
    "end": "613920"
  },
  {
    "text": "for example pca",
    "start": "613920",
    "end": "617519"
  },
  {
    "text": "whitening normalization outliers and so",
    "start": "617519",
    "end": "620240"
  },
  {
    "text": "on so being able to run the cro the cron",
    "start": "620240",
    "end": "622880"
  },
  {
    "text": "jobs",
    "start": "622880",
    "end": "623600"
  },
  {
    "text": "is very important for us to be able to",
    "start": "623600",
    "end": "625680"
  },
  {
    "text": "fork off these different tasks",
    "start": "625680",
    "end": "628000"
  },
  {
    "text": "simultaneously here's an example",
    "start": "628000",
    "end": "631279"
  },
  {
    "text": "where moving outliers the cron job for",
    "start": "631279",
    "end": "634079"
  },
  {
    "text": "data augmentation",
    "start": "634079",
    "end": "635920"
  },
  {
    "text": "pca principal component analysis as well",
    "start": "635920",
    "end": "638640"
  },
  {
    "text": "as",
    "start": "638640",
    "end": "639279"
  },
  {
    "text": "whitening",
    "start": "639279",
    "end": "641839"
  },
  {
    "text": "when we move to the data splitting this",
    "start": "643279",
    "end": "645360"
  },
  {
    "text": "is where it fits",
    "start": "645360",
    "end": "646320"
  },
  {
    "text": "in the machine learning path which is",
    "start": "646320",
    "end": "648160"
  },
  {
    "text": "extremely important before you start",
    "start": "648160",
    "end": "650160"
  },
  {
    "text": "training your model so that you will do",
    "start": "650160",
    "end": "652240"
  },
  {
    "text": "this",
    "start": "652240",
    "end": "653279"
  },
  {
    "text": "multiple times until you reach the",
    "start": "653279",
    "end": "656079"
  },
  {
    "text": "converge or the consistent model",
    "start": "656079",
    "end": "658000"
  },
  {
    "text": "that meets the desired workflow as well",
    "start": "658000",
    "end": "661120"
  },
  {
    "text": "as expectation means",
    "start": "661120",
    "end": "662880"
  },
  {
    "text": "basically the incoming data is split",
    "start": "662880",
    "end": "665040"
  },
  {
    "text": "into training validation and testing",
    "start": "665040",
    "end": "666800"
  },
  {
    "text": "data sets",
    "start": "666800",
    "end": "667760"
  },
  {
    "text": "and any changes in in the data will",
    "start": "667760",
    "end": "670640"
  },
  {
    "text": "re-trigger",
    "start": "670640",
    "end": "671519"
  },
  {
    "text": "the rebuilding of of the underlying",
    "start": "671519",
    "end": "674240"
  },
  {
    "text": "components",
    "start": "674240",
    "end": "676480"
  },
  {
    "text": "examples of splitting is actual",
    "start": "676480",
    "end": "679519"
  },
  {
    "text": "splitting",
    "start": "679519",
    "end": "680720"
  },
  {
    "text": "is more complicated than just dividing",
    "start": "680720",
    "end": "683600"
  },
  {
    "text": "them but it's extremely important to",
    "start": "683600",
    "end": "685760"
  },
  {
    "text": "try different sets of the splitting that",
    "start": "685760",
    "end": "688640"
  },
  {
    "text": "would result",
    "start": "688640",
    "end": "689760"
  },
  {
    "text": "in directly influencing the model as we",
    "start": "689760",
    "end": "693040"
  },
  {
    "text": "build it",
    "start": "693040",
    "end": "694000"
  },
  {
    "text": "and this part is executed using",
    "start": "694000",
    "end": "696959"
  },
  {
    "text": "tensorflow jobs tf jobs",
    "start": "696959",
    "end": "698880"
  },
  {
    "text": "created on kubeflow where the longshore",
    "start": "698880",
    "end": "701279"
  },
  {
    "text": "terminary lstm",
    "start": "701279",
    "end": "703279"
  },
  {
    "text": "for price prediction the random forest",
    "start": "703279",
    "end": "706560"
  },
  {
    "text": "models for asset market prediction and",
    "start": "706560",
    "end": "709120"
  },
  {
    "text": "the natural language processing analysis",
    "start": "709120",
    "end": "711519"
  },
  {
    "text": "for the media and the tweets and textual",
    "start": "711519",
    "end": "714000"
  },
  {
    "text": "information",
    "start": "714000",
    "end": "714959"
  },
  {
    "text": "multiple simultaneous jobs are created",
    "start": "714959",
    "end": "717360"
  },
  {
    "text": "and validated",
    "start": "717360",
    "end": "720000"
  },
  {
    "text": "here is an example of a yaml project the",
    "start": "720000",
    "end": "722399"
  },
  {
    "text": "data was read",
    "start": "722399",
    "end": "723440"
  },
  {
    "text": "from multiple data sets before we",
    "start": "723440",
    "end": "726079"
  },
  {
    "text": "started working",
    "start": "726079",
    "end": "727200"
  },
  {
    "text": "on them after that we extend the model",
    "start": "727200",
    "end": "731040"
  },
  {
    "text": "on larger number of assets and different",
    "start": "731040",
    "end": "733760"
  },
  {
    "text": "number of intervals",
    "start": "733760",
    "end": "735120"
  },
  {
    "text": "predictions within the last five minutes",
    "start": "735120",
    "end": "737360"
  },
  {
    "text": "the minutes and so on",
    "start": "737360",
    "end": "740399"
  },
  {
    "text": "yet another example here of yml for the",
    "start": "741120",
    "end": "744079"
  },
  {
    "text": "worker",
    "start": "744079",
    "end": "744720"
  },
  {
    "text": "as we build the system up",
    "start": "744720",
    "end": "748959"
  },
  {
    "text": "next id is the hyper parameter",
    "start": "752320",
    "end": "754079"
  },
  {
    "text": "optimization technique",
    "start": "754079",
    "end": "755760"
  },
  {
    "text": "test for different parameters making",
    "start": "755760",
    "end": "757920"
  },
  {
    "text": "sure the sensitivity is within",
    "start": "757920",
    "end": "759360"
  },
  {
    "text": "acceptable",
    "start": "759360",
    "end": "760240"
  },
  {
    "text": "range and then you start delivering",
    "start": "760240",
    "end": "763360"
  },
  {
    "text": "onto the next steps for the buildup",
    "start": "763360",
    "end": "767360"
  },
  {
    "text": "once the model has converged and settled",
    "start": "769040",
    "end": "771440"
  },
  {
    "text": "we start",
    "start": "771440",
    "end": "772240"
  },
  {
    "text": "worrying about the rollout and the",
    "start": "772240",
    "end": "773839"
  },
  {
    "text": "serving of these models",
    "start": "773839",
    "end": "777040"
  },
  {
    "text": "at scale for it to be deployed",
    "start": "777040",
    "end": "780959"
  },
  {
    "text": "we use selden to serve the model",
    "start": "785760",
    "end": "790160"
  },
  {
    "text": "and we use sdu in correlation with",
    "start": "790160",
    "end": "792560"
  },
  {
    "text": "selden",
    "start": "792560",
    "end": "793200"
  },
  {
    "text": "to control that traffic",
    "start": "793200",
    "end": "796639"
  },
  {
    "text": "keyflow has been key in building the",
    "start": "797600",
    "end": "800720"
  },
  {
    "text": "whole process and controlling it",
    "start": "800720",
    "end": "802720"
  },
  {
    "text": "we created container images for",
    "start": "802720",
    "end": "804560"
  },
  {
    "text": "pre-processing training analysis",
    "start": "804560",
    "end": "806160"
  },
  {
    "text": "prediction deployment",
    "start": "806160",
    "end": "807600"
  },
  {
    "text": "and each employment has been scaled",
    "start": "807600",
    "end": "810000"
  },
  {
    "text": "independently",
    "start": "810000",
    "end": "812399"
  },
  {
    "text": "thank you for your time i hope this was",
    "start": "812399",
    "end": "814639"
  },
  {
    "text": "of value to you",
    "start": "814639",
    "end": "815760"
  },
  {
    "text": "and looking forward to your questions",
    "start": "815760",
    "end": "818240"
  },
  {
    "text": "thank you",
    "start": "818240",
    "end": "820959"
  }
]