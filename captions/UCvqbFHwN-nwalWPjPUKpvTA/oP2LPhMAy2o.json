[
  {
    "start": "0",
    "end": "58000"
  },
  {
    "text": "if you came to the deep dive on to Burnett deep dive into kubernetes",
    "start": "30",
    "end": "5370"
  },
  {
    "text": "scheduling you're at the right spot so this is going to coach you on running kubernetes on top of vSphere it could be",
    "start": "5370",
    "end": "13049"
  },
  {
    "text": "useful if you're running in a VMware distribution but this stuff also applies if you're running any other cougar any's",
    "start": "13049",
    "end": "20130"
  },
  {
    "text": "distribution on top of vSphere your speakers today are myself I'm Steve Wang",
    "start": "20130",
    "end": "26310"
  },
  {
    "text": "I'm a software engineer with cloud native apps team at VMware I came here",
    "start": "26310",
    "end": "32130"
  },
  {
    "text": "from Los Angeles I've been working on the kubernetes project since 2015 and",
    "start": "32130",
    "end": "37230"
  },
  {
    "text": "I'm chair of the kubernetes VMware cig",
    "start": "37230",
    "end": "42719"
  },
  {
    "text": "also lead of the edge and IOT cig Michael yeah and I Michael gosh I'm",
    "start": "42719",
    "end": "47760"
  },
  {
    "text": "based out of Germany work for VM as a platform architect which is mostly field facing work doing architecture design",
    "start": "47760",
    "end": "54750"
  },
  {
    "text": "deployments on kind of any communities on vSphere garments so you already read this abstract if you're here I'm gonna",
    "start": "54750",
    "end": "60750"
  },
  {
    "start": "58000",
    "end": "131000"
  },
  {
    "text": "skip it but the only reason I left it in the deck is so that if somebody downloads the the the deck later that",
    "start": "60750",
    "end": "66330"
  },
  {
    "text": "they can scan that at the beginning and know whether they want to read the rest this is the agenda for today we're going",
    "start": "66330",
    "end": "72450"
  },
  {
    "text": "to start with covering kind of kubernetes default scheduling how it works regardless of whether you're on a",
    "start": "72450",
    "end": "79020"
  },
  {
    "text": "hypervisor platform or not moving into the zones feature that can be used with",
    "start": "79020",
    "end": "84330"
  },
  {
    "text": "vSphere to get some valuable and enhanced features next we'll go on to",
    "start": "84330",
    "end": "90540"
  },
  {
    "text": "Numa we'll explain what it is when we get there non-uniform memory architecture Michael is going to pick up",
    "start": "90540",
    "end": "99210"
  },
  {
    "text": "and cover kubernetes default resource management and ways of extending the the",
    "start": "99210",
    "end": "105000"
  },
  {
    "text": "default functionality of kubernetes by utilizing vSphere features in",
    "start": "105000",
    "end": "110460"
  },
  {
    "text": "conjunction with the built-in features of kubernetes finally I left it here",
    "start": "110460",
    "end": "116009"
  },
  {
    "text": "we've delivered this a month ago in Cuba and Shanghai and ran into the time limit",
    "start": "116009",
    "end": "122549"
  },
  {
    "text": "so I left it in the deck if you download it later we may or may not have time to",
    "start": "122549",
    "end": "127680"
  },
  {
    "text": "cover highly high availability options so let's move on to kubernetes",
    "start": "127680",
    "end": "133020"
  },
  {
    "start": "131000",
    "end": "166000"
  },
  {
    "text": "scheduling what is it what exactly does the scheduler do well through the API pods",
    "start": "133020",
    "end": "139709"
  },
  {
    "text": "are created and they're placed in a queue there's a beta feature that potentially can cause things placed in",
    "start": "139709",
    "end": "147239"
  },
  {
    "text": "the queue to jump ahead of others but you'd have to enable it then the scheduler after they pour into this",
    "start": "147239",
    "end": "154110"
  },
  {
    "text": "funnel the scheduler continuously pulls pods out of the queue looks at their",
    "start": "154110",
    "end": "159239"
  },
  {
    "text": "requirements and assigns it to a worker node assuming you know you have more than a one node cluster when it does",
    "start": "159239",
    "end": "168360"
  },
  {
    "start": "166000",
    "end": "209000"
  },
  {
    "text": "this assignment it first goes through a stage where it filters out impossible",
    "start": "168360",
    "end": "175260"
  },
  {
    "text": "worker notes these are nodes that couldn't run that workload and these filters are called predicates kubernetes",
    "start": "175260",
    "end": "182880"
  },
  {
    "text": "is very modular so it comes with some default predicates but this is",
    "start": "182880",
    "end": "188010"
  },
  {
    "text": "extensible so some of them are available off-the-shelf open-source but you could",
    "start": "188010",
    "end": "193410"
  },
  {
    "text": "write your own so this initial filter stage if you look at that picture you",
    "start": "193410",
    "end": "198570"
  },
  {
    "text": "see the X's over those and those are just a determination by one of these predicates that there's no way that this",
    "start": "198570",
    "end": "205590"
  },
  {
    "text": "this this pod is going to run on this node afterward you're left with a list",
    "start": "205590",
    "end": "214380"
  },
  {
    "start": "209000",
    "end": "252000"
  },
  {
    "text": "of eligible nodes and in this case it would be those orange ones so we've got",
    "start": "214380",
    "end": "219810"
  },
  {
    "text": "six possible nodes that are not meaning they're not impossible and there might",
    "start": "219810",
    "end": "225600"
  },
  {
    "text": "still be preferred nodes to run those odd so this ranking is called priorities",
    "start": "225600",
    "end": "232079"
  },
  {
    "text": "in the kubernetes architecture this once again is extensible so there's a default",
    "start": "232079",
    "end": "237480"
  },
  {
    "text": "list an example of this is zones which we'll get into in just a bit but there",
    "start": "237480",
    "end": "244470"
  },
  {
    "text": "are others and like I say you come with some of those enabled by default some are non default and if you want to you",
    "start": "244470",
    "end": "251220"
  },
  {
    "text": "can write your own so after all of these",
    "start": "251220",
    "end": "256739"
  },
  {
    "start": "252000",
    "end": "268000"
  },
  {
    "text": "stages of eliminating the impossible ranking them and jumping you know to a",
    "start": "256739",
    "end": "262710"
  },
  {
    "text": "preferred order list finally that pod is assigned to a node and is going to get kicked off and",
    "start": "262710",
    "end": "270420"
  },
  {
    "start": "268000",
    "end": "410000"
  },
  {
    "text": "running now I will say I'll go into the this is a deep dive so in that ranking",
    "start": "270420",
    "end": "277590"
  },
  {
    "text": "if you have a very large cluster the thing doesn't necessarily evaluate every",
    "start": "277590",
    "end": "283680"
  },
  {
    "text": "single note you know I'm talking like say I have a thousand worker nodes on my kubernetes the system and this this can",
    "start": "283680",
    "end": "290190"
  },
  {
    "text": "be overridden but by default there's some number saying that if there's a humongous number of kubernetes nodes and",
    "start": "290190",
    "end": "298110"
  },
  {
    "text": "i've got hundreds of them I'm going to just take a percentage of these and rank",
    "start": "298110",
    "end": "303780"
  },
  {
    "text": "those and it's a latency concern if you crawled over these it just isn't worth",
    "start": "303780",
    "end": "309210"
  },
  {
    "text": "it and once again you can turn off that behavior but I'm just telling you since it's a deep dive but that is something",
    "start": "309210",
    "end": "316380"
  },
  {
    "text": "going on under the covers if you're running massively large clusters so",
    "start": "316380",
    "end": "322100"
  },
  {
    "text": "there are scheduling modifiers that can affect this the first one is a node",
    "start": "322100",
    "end": "328020"
  },
  {
    "text": "selector and this is based on key value labels you put on the worker nodes",
    "start": "328020",
    "end": "333330"
  },
  {
    "text": "themselves some of these are automatically created but if you want to you can annotate this with your own for",
    "start": "333330",
    "end": "341100"
  },
  {
    "text": "whatever purposes you might have pods can define rules based on these node",
    "start": "341100",
    "end": "347820"
  },
  {
    "text": "labels to achieve preferred placements called affinity so the the sum total",
    "start": "347820",
    "end": "354990"
  },
  {
    "text": "list of the elements that influence these pod placements are labels unknowns",
    "start": "354990",
    "end": "362460"
  },
  {
    "text": "and zones as an example of this taints in Toleration x' where you'd mark nodes with arbitrary labels which could",
    "start": "362460",
    "end": "369690"
  },
  {
    "text": "correspond to a specific resource or whatever you like that could either clue",
    "start": "369690",
    "end": "375090"
  },
  {
    "text": "in the kubernetes scheduler that I don't want to go to this node or a Toleration says yeah you know all things aside I'm",
    "start": "375090",
    "end": "385050"
  },
  {
    "text": "willing to go there there are admission controllers and a wide varieties are available an",
    "start": "385050",
    "end": "391500"
  },
  {
    "text": "admission controller would take the pod spec and either validate it meaning it's",
    "start": "391500",
    "end": "398070"
  },
  {
    "text": "going to scan it for acceptable behavior or potentially even mutated we're a pod",
    "start": "398070",
    "end": "404040"
  },
  {
    "text": "spec might be missing material and this would inject the default behavior and if",
    "start": "404040",
    "end": "412650"
  },
  {
    "start": "410000",
    "end": "458000"
  },
  {
    "text": "I use zones I'll turn it over to my colleague Michael yeah just briefly so it has a question",
    "start": "412650",
    "end": "419960"
  },
  {
    "text": "right you mean discovery from the actual",
    "start": "428090",
    "end": "434400"
  },
  {
    "text": "hardware perspective or or saving notes from being scheduled by pods that don't use GPUs is that your question yes yes",
    "start": "434400",
    "end": "449180"
  },
  {
    "text": "yes the device plug-in yeah right will",
    "start": "449180",
    "end": "454500"
  },
  {
    "text": "do I just have to understand it first yeah so your question is if I'm using special devices like GPUs or FPGAs in my",
    "start": "454500",
    "end": "461970"
  },
  {
    "text": "cluster how would I protect those nodes from pods being scheduled on on the",
    "start": "461970",
    "end": "467040"
  },
  {
    "text": "nodes that are Pro yeah but spots you the pods using that kind of hardware",
    "start": "467040",
    "end": "472610"
  },
  {
    "text": "making sure they land on that one and protecting the nodes from parts not using GPUs for example to trouble the",
    "start": "472610",
    "end": "479340"
  },
  {
    "text": "node so it's cut you could do both first of all you could make sure that your pods using or needing GPUs could have a",
    "start": "479340",
    "end": "485370"
  },
  {
    "text": "requirement like requirement on this kind of extended resource as a device plug-in to make sure they actually land",
    "start": "485370",
    "end": "491310"
  },
  {
    "text": "on on the on a machine and then use Taine's to repel other parts that don't",
    "start": "491310",
    "end": "496830"
  },
  {
    "text": "need to be use or fpga PGA's to not land on that node which are soft or hard kind",
    "start": "496830",
    "end": "502560"
  },
  {
    "text": "of requirements that you can express right and so now the question your question might be okay why we are here",
    "start": "502560",
    "end": "509670"
  },
  {
    "text": "in cig VM or while we're talking about scheduling because there's another sake called six scheduling it's it's",
    "start": "509670",
    "end": "515880"
  },
  {
    "text": "important to understand that in vSphere we're kind of running kubernetes on vSphere we have like two levels of scheduling or abstractions we've got the",
    "start": "515880",
    "end": "522210"
  },
  {
    "text": "vSphere scheduling which is like DRS and the kernel it's actually multiple layers there and on top of that we have the",
    "start": "522210",
    "end": "528360"
  },
  {
    "text": "kubernetes scheduler and logic and probably you have other systems on top of that running so we have we have multiple hierarchies of schedule",
    "start": "528360",
    "end": "534350"
  },
  {
    "text": "involved when running kubernetes on vSphere that could cause if those layers of scheduling are not Alayne aligned to",
    "start": "534350",
    "end": "541010"
  },
  {
    "text": "each other and that could chorus under deterministic behavior for example until",
    "start": "541010",
    "end": "546020"
  },
  {
    "text": "recently in Cuba until kubernetes V 1.12 we did not support the zones interface",
    "start": "546020",
    "end": "551750"
  },
  {
    "text": "in kubernetes that means that a node would not express a node running on vSphere would not express that it's part",
    "start": "551750",
    "end": "558710"
  },
  {
    "text": "of a certain zone or RAC or whatever you want to define zone in that case meaning",
    "start": "558710",
    "end": "563750"
  },
  {
    "text": "kubernetes could not make use from a scheduled new perspective to define like zone affinity or zone anti affinity or",
    "start": "563750",
    "end": "570350"
  },
  {
    "text": "node selectors or whatever Steve just has described in kubernetes we 1.12 we implemented actually the",
    "start": "570350",
    "end": "576620"
  },
  {
    "text": "zones interface which is part of the cloud provider specification that kubernetes has so now you have the",
    "start": "576620",
    "end": "582740"
  },
  {
    "text": "option to tag your clusters or Asics I hosts or data centers it's very kind of",
    "start": "582740",
    "end": "588500"
  },
  {
    "text": "granular what you can do to define what a zone in your environment is because",
    "start": "588500",
    "end": "593720"
  },
  {
    "text": "every busier environment is different you're probably not running Street data centers like an AWS or or Google cloud",
    "start": "593720",
    "end": "599390"
  },
  {
    "text": "so you could run like three hosts which make up a zone or you couldn't run three recs",
    "start": "599390",
    "end": "604640"
  },
  {
    "text": "which could make up a zone or you could have three data centers modeling a zone so we implemented that and we 1.12 to",
    "start": "604640",
    "end": "610670"
  },
  {
    "text": "have like an arbitration and knowledge sharing between the kubernetes scheduling and the vSphere scheduler any",
    "start": "610670",
    "end": "620150"
  },
  {
    "start": "620000",
    "end": "880000"
  },
  {
    "text": "questions so far just throw them in I think it's we're pretty yeah and ultimately the zones feature is to spread things across failure zones",
    "start": "620150",
    "end": "627020"
  },
  {
    "text": "like automatically with you minimizing the operational cost of caught of",
    "start": "627020",
    "end": "633310"
  },
  {
    "text": "achieving that now we're moving on to a subject called Numa what is it non-uniform memory architecture when",
    "start": "633310",
    "end": "641150"
  },
  {
    "text": "you're dealing with server grade x86",
    "start": "641150",
    "end": "646370"
  },
  {
    "text": "hardware multi socket those things are laid out in a motherboard and between",
    "start": "646370",
    "end": "652280"
  },
  {
    "text": "the sockets you've got this pipe line drawn in red there and you might have",
    "start": "652280",
    "end": "660290"
  },
  {
    "text": "much faster access to the memory in the dims near the socket as opposed to the",
    "start": "660290",
    "end": "666650"
  },
  {
    "text": "one on the other side the data a workload running on any",
    "start": "666650",
    "end": "672020"
  },
  {
    "text": "either of these CPUs can still get to the memory on the other side but in this",
    "start": "672020",
    "end": "678650"
  },
  {
    "text": "case and this varies by motherboard and chip family but in this case that",
    "start": "678650",
    "end": "685100"
  },
  {
    "text": "bottleneck in the center there is 41 gigabytes a second and there's also a latency penalty when you cross that so",
    "start": "685100",
    "end": "692750"
  },
  {
    "text": "what you've got a scenario where when these workloads are laid down by",
    "start": "692750",
    "end": "698360"
  },
  {
    "text": "kubernetes you'd really like to have an outcome where things that can fit on one",
    "start": "698360",
    "end": "706520"
  },
  {
    "text": "side or the other actually end up running there so why should you care",
    "start": "706520",
    "end": "713450"
  },
  {
    "text": "about it well nearly all databases database servers that the the legacy",
    "start": "713450",
    "end": "720220"
  },
  {
    "text": "transactional type ones anyway like Oracle but also MongoDB present a",
    "start": "720220",
    "end": "725480"
  },
  {
    "text": "workload that will opportunistically attempt to detect and consume as much as",
    "start": "725480",
    "end": "730910"
  },
  {
    "text": "the system's memory is out there and when these things get containerized",
    "start": "730910",
    "end": "737420"
  },
  {
    "text": "under kubernetes and scheduled on to some piece of hardware and this doesn't matter whether it's vSphere or better",
    "start": "737420",
    "end": "743690"
  },
  {
    "text": "Merrill that behavior can end up causing something to cross one of these Numa",
    "start": "743690",
    "end": "749390"
  },
  {
    "text": "boundaries and it could be depending on this caching situation that you would be",
    "start": "749390",
    "end": "754850"
  },
  {
    "text": "better off with a smaller cache that is higher performant then a larger one that",
    "start": "754850",
    "end": "761420"
  },
  {
    "text": "really is sometimes highly performant sometimes not and you end up with",
    "start": "761420",
    "end": "766640"
  },
  {
    "text": "non-deterministic behavior the these non-deterministic scenarios tend to be",
    "start": "766640",
    "end": "772220"
  },
  {
    "text": "pretty ugly when you're in an organization that has to sign up for a specific service level agreement so you",
    "start": "772220",
    "end": "781130"
  },
  {
    "text": "know Linux has some recognition of this when it initially allocates a thread it's assigned to a preferred node with",
    "start": "781130",
    "end": "788660"
  },
  {
    "text": "default allocations from this but there are some popular application runtimes",
    "start": "788660",
    "end": "795710"
  },
  {
    "text": "like Java at least Java preten that don't respect the container",
    "start": "795710",
    "end": "801560"
  },
  {
    "text": "resource declaration go out and effectively get the memory quota from",
    "start": "801560",
    "end": "806810"
  },
  {
    "text": "the underlying kernel seeing the whole thing rather than just the amount in",
    "start": "806810",
    "end": "813970"
  },
  {
    "text": "allocated to the container and would attempt to go use it all that maybe",
    "start": "813970",
    "end": "819620"
  },
  {
    "text": "isn't preferred behavior when you run kubernetes on top of a hypervisor where",
    "start": "819620",
    "end": "825170"
  },
  {
    "text": "you have the luxury of composing VMs that would carve this thing up in that",
    "start": "825170",
    "end": "832910"
  },
  {
    "text": "drawing before like split many of these servers will obviously have more than two sockets but if you were to carve",
    "start": "832910",
    "end": "839060"
  },
  {
    "text": "this up into two VMs the size of either side you might be better off now if you",
    "start": "839060",
    "end": "846230"
  },
  {
    "text": "have monster workloads that in fact would suck up that whole machine maybe you don't want to do this but in many",
    "start": "846230",
    "end": "852380"
  },
  {
    "text": "cases you'll have clusters where you know for efficiency in your data center",
    "start": "852380",
    "end": "858440"
  },
  {
    "text": "you'll be buying larger density machines just so you can fit more CPU cores in",
    "start": "858440",
    "end": "864610"
  },
  {
    "text": "yet you don't have workloads that hog an entire machine at once you may be better",
    "start": "864610",
    "end": "870680"
  },
  {
    "text": "off doing this in a way that would allow the underlying hypervisor to give you a",
    "start": "870680",
    "end": "877160"
  },
  {
    "text": "hand with the non-uniform memory architecture so how can these be avoided",
    "start": "877160",
    "end": "883820"
  },
  {
    "start": "880000",
    "end": "959000"
  },
  {
    "text": "well if you didn't have this if you're running pure Linux you can wrap an application in this Numa act I'll I'm",
    "start": "883820",
    "end": "891440"
  },
  {
    "text": "not sure I'm pronouncing it right to interleave memory that's something where you're telling it just to get",
    "start": "891440",
    "end": "896839"
  },
  {
    "text": "predictability that I want half my memory on this side half on the other and it's you know it's going to be bad",
    "start": "896839",
    "end": "905000"
  },
  {
    "text": "half the time but at least it's predictable you know you you've interleaved these things you can move to",
    "start": "905000",
    "end": "912459"
  },
  {
    "text": "for some of these application runtimes if you're on Java Java JRE it turns out",
    "start": "912459",
    "end": "918200"
  },
  {
    "text": "that they recently added enhancements so moving up to JRE 10 might cause at least",
    "start": "918200",
    "end": "924470"
  },
  {
    "text": "Java workloads to behave more acceptable but sometimes this kind of stuff isn't",
    "start": "924470",
    "end": "930140"
  },
  {
    "text": "available or you're sucking in Java components that you didn't write yourself you don't",
    "start": "930140",
    "end": "935329"
  },
  {
    "text": "you don't have control over that maybe in your organization",
    "start": "935329",
    "end": "940399"
  },
  {
    "text": "you've standardized on something else and there are non Java things that encounter the same kind of issue there",
    "start": "940399",
    "end": "947480"
  },
  {
    "text": "are active talks going on in enhancing these kinds of Numa behaviors in a the",
    "start": "947480",
    "end": "953839"
  },
  {
    "text": "resource management working group and get the deck later and the link is live here if you want to read more about it",
    "start": "953839",
    "end": "959149"
  },
  {
    "start": "959000",
    "end": "1050000"
  },
  {
    "text": "so you can use an uma where hypervisor to help you right now without waiting",
    "start": "959149",
    "end": "965720"
  },
  {
    "text": "for these enhancements that are still being discussed you know assuming your",
    "start": "965720",
    "end": "971480"
  },
  {
    "text": "workload fits you make a VM as a walled garden sized right you know to an",
    "start": "971480",
    "end": "976759"
  },
  {
    "text": "appropriate size to respect those boundaries if you can't there's some",
    "start": "976759",
    "end": "982429"
  },
  {
    "text": "there's some things you ought to do when you're laying out these VMs and laying",
    "start": "982429",
    "end": "990410"
  },
  {
    "text": "out your pods to cause the system to not have a tendency to cross these",
    "start": "990410",
    "end": "996619"
  },
  {
    "text": "boundaries so don't declare things with larger socket counts than you need for",
    "start": "996619",
    "end": "1003699"
  },
  {
    "text": "your requirement in general never pick a VM with an odd number of CPUs it's just",
    "start": "1003699",
    "end": "1010299"
  },
  {
    "text": "going to tend to maybe it won't cause an uma boundary but even if it doesn't it could tend to waste resources but where",
    "start": "1010299",
    "end": "1017559"
  },
  {
    "text": "it does cause one is you end up with this odd number CPU that appears to be available resource the kubernetes",
    "start": "1017559",
    "end": "1024428"
  },
  {
    "text": "scheduler might think it can throw more in there yet the only way it's ever gonna run is to cross the boundary you",
    "start": "1024429",
    "end": "1030788"
  },
  {
    "text": "don't want to be crossing never ever compose a VM larger than the number of",
    "start": "1030789",
    "end": "1036610"
  },
  {
    "text": "physical cores and if you do that there",
    "start": "1036610",
    "end": "1042069"
  },
  {
    "text": "are also some additional enhancements look at the deck later rare rarely do you need to change those but the link is",
    "start": "1042069",
    "end": "1048548"
  },
  {
    "text": "on the bottom of this page so I'll turn it back to Michael to talk about kubernetes resource management thank you",
    "start": "1048549",
    "end": "1055690"
  },
  {
    "start": "1050000",
    "end": "1223000"
  },
  {
    "text": "any questions so far on the Numa thing that sometimes is a little bit hard to understand okay so actually just just a",
    "start": "1055690",
    "end": "1065320"
  },
  {
    "text": "note on the new method I'd this is one of them most like top three that have faced in the field and working with customers performance issues in",
    "start": "1065320",
    "end": "1071470"
  },
  {
    "text": "nuuma so and we make it pretty easy for you as Steve said as long as you stay within the Neumann boundary meaning look",
    "start": "1071470",
    "end": "1078550"
  },
  {
    "text": "at your socket design two sockets 10 cores each you'll probably define your VMs in a way that they are less than 10",
    "start": "1078550",
    "end": "1085030"
  },
  {
    "text": "or equal to 10v CPUs there so they nicely fit into a socket that means vSphere the hypervisor presents a",
    "start": "1085030",
    "end": "1091750"
  },
  {
    "text": "uniform host to the VM so the Linux kernel wouldn't actually see an uma socket or layout so it wouldn't actually",
    "start": "1091750",
    "end": "1098770"
  },
  {
    "text": "have to handle and deal with the Numa optimizations there and you'll defer it to the platform to the vSphere",
    "start": "1098770",
    "end": "1104380"
  },
  {
    "text": "hypervisor moving on another or concern",
    "start": "1104380",
    "end": "1111820"
  },
  {
    "text": "or a lot of questions or issues that I've seen in the field are around CPU",
    "start": "1111820",
    "end": "1117100"
  },
  {
    "text": "memory and special devices has already mentioned GPUs in the field running kubernetes on vSphere and starting with",
    "start": "1117100",
    "end": "1124150"
  },
  {
    "text": "CPU and memory in Cuba Nettie's you express CPUs in milli course or course",
    "start": "1124150",
    "end": "1130450"
  },
  {
    "text": "but then they are broken down into milli cores and Millie cores they are a relative unit they are no not an act",
    "start": "1130450",
    "end": "1136330"
  },
  {
    "text": "like an an absolute unit that you specify meaning they can vary between the environments where you run the",
    "start": "1136330",
    "end": "1142420"
  },
  {
    "text": "kubernetes vm the workers on on top of that the two cores on a AWS ec2 instance",
    "start": "1142420",
    "end": "1147940"
  },
  {
    "text": "are definitely different and two cores on HTTP instance because they were just relative now in this fear we are not",
    "start": "1147940",
    "end": "1154560"
  },
  {
    "text": "counting chorus like internally we're using megahertz and the indie hypervisor",
    "start": "1154560",
    "end": "1160180"
  },
  {
    "text": "so in the in the UI you have the option to add cores but internally they are mapped to a megahertz and India is",
    "start": "1160180",
    "end": "1166690"
  },
  {
    "text": "already a little bit of difference going on here which I'm gonna touch later and for memories it's not a big deal because",
    "start": "1166690",
    "end": "1172510"
  },
  {
    "text": "we also work with bytes so kubernetes as of like an absolute number of a unit in here that you specify and there are",
    "start": "1172510",
    "end": "1178630"
  },
  {
    "text": "several other like resources that you can specify like already GPUs were mentioned or FPGAs or any other special",
    "start": "1178630",
    "end": "1186940"
  },
  {
    "text": "devices some of them are not available on the vSphere platform like for example FPGAs mapping FPGAs through or very",
    "start": "1186940",
    "end": "1194110"
  },
  {
    "text": "specific network cards that we're not supporting on the hypervisor so that could be an issue for GPUs we are",
    "start": "1194110",
    "end": "1200160"
  },
  {
    "text": "usually pretty good because we work closely with AMD and NVIDIA so we have good decent GPU support we're",
    "start": "1200160",
    "end": "1207279"
  },
  {
    "text": "also supporting the NVIDIA GPU about your GPU meaning you cough up physical CPU into multiple virtual GPUs and you",
    "start": "1207279",
    "end": "1214570"
  },
  {
    "text": "spread them across the VMS so on the GPU side we are good but for certain hardware make sure that you're validated",
    "start": "1214570",
    "end": "1220120"
  },
  {
    "text": "you're tested you have the drivers and the hypervisor so in kubernetes then a",
    "start": "1220120",
    "end": "1226419"
  },
  {
    "start": "1223000",
    "end": "1277000"
  },
  {
    "text": "developer or a user or whoever can define requests and limits for each",
    "start": "1226419",
    "end": "1231580"
  },
  {
    "text": "resource that you can specify in a pot so for CPU and memory or your GPUs some",
    "start": "1231580",
    "end": "1237130"
  },
  {
    "text": "of them are allow different numbers meaning that you can specify different",
    "start": "1237130",
    "end": "1242230"
  },
  {
    "text": "number of requests or limits for CPU and memory for GPUs you have to set them to",
    "start": "1242230",
    "end": "1247840"
  },
  {
    "text": "the same because the scheduler doesn't support over commit on GPUs for example and those requests are done internally",
    "start": "1247840",
    "end": "1253720"
  },
  {
    "text": "mapped into Linux constructs and they were mapped into the vSphere hypervisor which we'll see in two slides but",
    "start": "1253720",
    "end": "1259809"
  },
  {
    "text": "generically speaking you have some options to control the resource management side on kubernetes but you",
    "start": "1259809",
    "end": "1265029"
  },
  {
    "text": "have to also make sure that kubernetes runs on a shared platform which is vSphere and as we already said there's a two-level scheduling going on between",
    "start": "1265029",
    "end": "1271960"
  },
  {
    "text": "the vSphere platform and kubernetes so you have to make sure that they are aligned unfortunately there are often",
    "start": "1271960",
    "end": "1280779"
  },
  {
    "start": "1277000",
    "end": "1321000"
  },
  {
    "text": "different goals that you want to achieve in a kubernetes cluster and this is a from a talk that I gave in Kubik on Copenhagen earlier this year going",
    "start": "1280779",
    "end": "1288220"
  },
  {
    "text": "deeper into that matter but just by having this controls of limits and requests they are not always like",
    "start": "1288220",
    "end": "1296260"
  },
  {
    "text": "represent what your goals are when you're running kubernetes cluster for example you could your priority could be running high utilization in your",
    "start": "1296260",
    "end": "1303580"
  },
  {
    "text": "kubernetes cluster or you want to be very fair between workloads or you want",
    "start": "1303580",
    "end": "1308799"
  },
  {
    "text": "to prioritize some of the workloads in a community's cluster so you have to really think about how your model",
    "start": "1308799",
    "end": "1313840"
  },
  {
    "text": "resource management in your kubernetes cluster and I'm just going to refer here to the talk that I gave at cube con CPU",
    "start": "1313840",
    "end": "1322840"
  },
  {
    "start": "1321000",
    "end": "1419000"
  },
  {
    "text": "and memory when it comes to like over-committing so you you express more",
    "start": "1322840",
    "end": "1327909"
  },
  {
    "text": "resources you give more resources out to your developers then you actually physically have environment kubernetes has some",
    "start": "1327909",
    "end": "1335770"
  },
  {
    "text": "protection mechanisms when it comes to over-commitment but they are different between CPU and",
    "start": "1335770",
    "end": "1340809"
  },
  {
    "text": "memory because those are the resources that you can overcome it for CPU that's pretty easy because CPU is a",
    "start": "1340809",
    "end": "1346299"
  },
  {
    "text": "compressible unit meaning if you overcome it in CPU and there's contention on that note on the Booker VM",
    "start": "1346299",
    "end": "1351400"
  },
  {
    "text": "the kubernetes node the couplet which starts throttling actually it's the Linux kernel starting to throttle your",
    "start": "1351400",
    "end": "1358450"
  },
  {
    "text": "your workloads so you better make sure that requests and limits you get them right for CPUs especially for workloads",
    "start": "1358450",
    "end": "1364090"
  },
  {
    "text": "are critical like your interest controllers or your API servers or whatever you run as a part in kubernetes",
    "start": "1364090",
    "end": "1370090"
  },
  {
    "text": "for memory it's a little bit harder because we cannot compress memory so in",
    "start": "1370090",
    "end": "1375760"
  },
  {
    "text": "kubernetes the only mechanism to protect against memory of our commitment is to either get the requests right so you",
    "start": "1375760",
    "end": "1381640"
  },
  {
    "text": "figure out the right values for your workloads but if kubernetes has to act and decide that it has to kind of",
    "start": "1381640",
    "end": "1388090"
  },
  {
    "text": "remediate because the cluster is overcommitted then it has to stop workloads and",
    "start": "1388090",
    "end": "1393539"
  },
  {
    "text": "stopping workloads is based on certain priorities and functions in the queue bled in there and typically it's the one",
    "start": "1393539",
    "end": "1400090"
  },
  {
    "text": "that using - most resources versus the requests that are assigned to the workload and stopping means your",
    "start": "1400090",
    "end": "1405789"
  },
  {
    "text": "workloads getting killed which is unfortunate for some workloads probably so you again get the requests and limits",
    "start": "1405789",
    "end": "1412510"
  },
  {
    "text": "right and refer back to the queue con talk that I gave and Copenhagen now as",
    "start": "1412510",
    "end": "1421150"
  },
  {
    "start": "1419000",
    "end": "1611000"
  },
  {
    "text": "we already stressed and I'm already I'm stressing this running kubernetes on vSphere is kind of the multi-level",
    "start": "1421150",
    "end": "1426580"
  },
  {
    "text": "scheduling being involved here and on that slide you basically the higher you",
    "start": "1426580",
    "end": "1431620"
  },
  {
    "text": "see the hierarchies that those values of CPU requests and limits and memories are being translated into the Linux kernel",
    "start": "1431620",
    "end": "1438340"
  },
  {
    "text": "which is the couplet in your case the worker the worker VM running on vSphere and then underneath you also have some",
    "start": "1438340",
    "end": "1444039"
  },
  {
    "text": "controls on the ESXi hypervisor to control CPU memory behavior from the hypervisor side unfortunately not all of",
    "start": "1444039",
    "end": "1451990"
  },
  {
    "text": "them directly translate into the lower level primitives for example CPU",
    "start": "1451990",
    "end": "1458440"
  },
  {
    "text": "requests from a pot spec manifest perspective which says I request two CPUs translates into CPU shares",
    "start": "1458440",
    "end": "1466119"
  },
  {
    "text": "Phares is not a absolute value it's just a relative value and if your host gets",
    "start": "1466119",
    "end": "1472239"
  },
  {
    "text": "constrained the number of shares are being used to determine whether your workload is getting throttled or not so",
    "start": "1472239",
    "end": "1478809"
  },
  {
    "text": "if you specify two CPUs as a request that doesn't mean you are really getting two CPUs at the end of the day if you're",
    "start": "1478809",
    "end": "1484329"
  },
  {
    "text": "over committing on the note CPU limits are translated into CPU quota now the",
    "start": "1484329",
    "end": "1490809"
  },
  {
    "text": "downside of using CPU limits is that you will always get throttled no matter",
    "start": "1490809",
    "end": "1496239"
  },
  {
    "text": "whether this host the worker is actually under constraint or not so for the",
    "start": "1496239",
    "end": "1501819"
  },
  {
    "text": "limits you have to make sure that you set them right if you set them too low your workloads will always be struggled",
    "start": "1501819",
    "end": "1507129"
  },
  {
    "text": "which is not good for example for ingress or api's if you set them too high chances are good that you",
    "start": "1507129",
    "end": "1513219"
  },
  {
    "text": "over-committing your node and then the node gets under trouble and is being oversubscribed in ease here we have some",
    "start": "1513219",
    "end": "1520989"
  },
  {
    "text": "controls so we have CPU shares as well so they would translate but in the field we advise our customers and users of",
    "start": "1520989",
    "end": "1528159"
  },
  {
    "text": "kubernetes on vSphere to use CPU reservation unlike the CPU shares in the",
    "start": "1528159",
    "end": "1533589"
  },
  {
    "text": "Linux kernel a CPU reservation is a guarantee that you get if you give to your VM so the Asics I hypervisor will",
    "start": "1533589",
    "end": "1540999"
  },
  {
    "text": "make sure that under any cases you will get what you specify in the reservation and that's specified in megahertz so",
    "start": "1540999",
    "end": "1549159"
  },
  {
    "text": "contrast that with shares shares are just a relative weight when it comes to over-commitment CPU reservations are a",
    "start": "1549159",
    "end": "1556539"
  },
  {
    "text": "guarantee that you get from the vSphere hypervisor and so for critical VMs critical workloads user-facing workloads",
    "start": "1556539",
    "end": "1563109"
  },
  {
    "text": "databases whatsoever we recommend setting 100% reservation for memory and",
    "start": "1563109",
    "end": "1568629"
  },
  {
    "text": "CPU on that VM just to make sure the hypervisor will always give you the resources that you need in case you need",
    "start": "1568629",
    "end": "1574779"
  },
  {
    "text": "them and you can still overcome it that means you can run other VMs which are less critical with 50% reservation or",
    "start": "1574779",
    "end": "1580899"
  },
  {
    "text": "20% or no reservation at all and if kubernetes does not if that worker like",
    "start": "1580899",
    "end": "1586119"
  },
  {
    "text": "the production VM doesn't need all the resources we give them to other ones but if the production PM needs the resources",
    "start": "1586119",
    "end": "1592539"
  },
  {
    "text": "we reclaim them back via the hypervisor kernel and give them to the production",
    "start": "1592539",
    "end": "1599110"
  },
  {
    "text": "so mapping and understanding those constructs requests and shares and reservations is critical to like a",
    "start": "1599110",
    "end": "1605289"
  },
  {
    "text": "stable and robust kubernetes environment not just for the masters but also for their father your workers we're still",
    "start": "1605289",
    "end": "1613149"
  },
  {
    "start": "1611000",
    "end": "1813000"
  },
  {
    "text": "good in time so we can cover DRS and AJ and a little bit deeper here so these here they are as for those of you not",
    "start": "1613149",
    "end": "1619630"
  },
  {
    "text": "familiar with DRS DRS is a distributed resource scheduler meaning it looks at the cluster of e-cigs I host and it",
    "start": "1619630",
    "end": "1626890"
  },
  {
    "text": "looks for balanced resource utilization of your of your VMs to avoid hot spots in your cluster especially in cases when",
    "start": "1626890",
    "end": "1633549"
  },
  {
    "text": "a hypervisor is over committed or there's a lot of trouble on one node DRS",
    "start": "1633549",
    "end": "1639519"
  },
  {
    "text": "will look like by default right now it's for every five minutes Michael I think we have a question in back oh yes of",
    "start": "1639519",
    "end": "1645820"
  },
  {
    "text": "course I'm sorry the question is does",
    "start": "1645820",
    "end": "1651010"
  },
  {
    "text": "DRS require Visa Visa no so yes we son the storage solution from members not required for DRS the",
    "start": "1651010",
    "end": "1657940"
  },
  {
    "text": "IRS requires Enterprise Plus as I saw like license so it's not part of all of the vSphere editions but it has no",
    "start": "1657940",
    "end": "1667149"
  },
  {
    "text": "requirement from other products that we have right and so DRS is a kind of as it",
    "start": "1667149",
    "end": "1672880"
  },
  {
    "text": "says in the name it's a disability resource scheduler you could think think of like kubernetes as well as being a",
    "start": "1672880",
    "end": "1677889"
  },
  {
    "text": "disability resource scheduler so it looks at your Asics I host it has no understanding of kubernetes all the",
    "start": "1677889",
    "end": "1683169"
  },
  {
    "text": "workloads within those VMs it just looks for CPU and memory and if you want",
    "start": "1683169",
    "end": "1688210"
  },
  {
    "text": "storage and network contention and then it tries to balance your cluster so this is super helpful but you also have to",
    "start": "1688210",
    "end": "1694179"
  },
  {
    "text": "constrain it meaning in DRS it could happen that a VM on one horse is being migrated to a VM on the other host now",
    "start": "1694179",
    "end": "1700990"
  },
  {
    "text": "without the zone support or without any support to model your vSphere topology",
    "start": "1700990",
    "end": "1706360"
  },
  {
    "text": "be it racks or data centers to the kubernetes constructs like zones or failure domains without them the VMS",
    "start": "1706360",
    "end": "1714070"
  },
  {
    "text": "could migrate in your cluster and potentially violating an affinity or anti affinity decision that a cubed is",
    "start": "1714070",
    "end": "1720159"
  },
  {
    "text": "scheduler has been made on top of this as an easy example imagine three nodes",
    "start": "1720159",
    "end": "1725200"
  },
  {
    "text": "on the Left are part of one zone and the other nodes are part of the alisonj without tying those things together like",
    "start": "1725200",
    "end": "1732850"
  },
  {
    "text": "in forming kubernetes that it has different topologies a kubernetes scheduler could schedule a pod",
    "start": "1732850",
    "end": "1738310"
  },
  {
    "text": "on the same VM or on different hose in the same zone even though a user would have requested an anti affinity because",
    "start": "1738310",
    "end": "1744910"
  },
  {
    "text": "from a kubernetes perspective all it looks like for the virtual notes which is a VM in that case those two VMs could",
    "start": "1744910",
    "end": "1750400"
  },
  {
    "text": "end up running on the same physics I have proviso in order to prevent that we have some controls and this is a 1 to 12",
    "start": "1750400",
    "end": "1756490"
  },
  {
    "text": "we have the topology support and the VCO cloud provider we have DRS affinity",
    "start": "1756490",
    "end": "1761890"
  },
  {
    "text": "groups where you can model like those hosts or part of one group and those are some part of the other group and then",
    "start": "1761890",
    "end": "1767890"
  },
  {
    "text": "you can also specify anti affinity and affinity between the VMs in your cluster for example you want to make sure that",
    "start": "1767890",
    "end": "1773890"
  },
  {
    "text": "your master of three masters 5 mas por masters how many you run that not all of them run on the same as ex I host based",
    "start": "1773890",
    "end": "1780880"
  },
  {
    "text": "on the arrest rules you can actually model that I do not see a lot of customers using those rules because they",
    "start": "1780880",
    "end": "1787480"
  },
  {
    "text": "think that kubernetes will figure that out but if you don't try those kind of two domains like the vSphere domain and",
    "start": "1787480",
    "end": "1793390"
  },
  {
    "text": "the kubernetes domain together by informing them they are the cloud provider integration what could end up",
    "start": "1793390",
    "end": "1799780"
  },
  {
    "text": "happen is like an outage because kubernetes schedule to the same domain because it doesn't know there's",
    "start": "1799780",
    "end": "1805030"
  },
  {
    "text": "different domains underneath I think I have another slide on AJ any questions",
    "start": "1805030",
    "end": "1810700"
  },
  {
    "text": "so far looks like keep going Michael yes we're",
    "start": "1810700",
    "end": "1816550"
  },
  {
    "start": "1813000",
    "end": "1894000"
  },
  {
    "text": "a bit in time yeah so this is just an image of how you would specify a anti affinity rule or I find it a finicky",
    "start": "1816550",
    "end": "1823720"
  },
  {
    "text": "rule for for the masters here so we've got different different roles and in this scenario here we have modeled a",
    "start": "1823720",
    "end": "1829900"
  },
  {
    "text": "fault domain which is basically a rack or your zone out of those four hosts they go in host vm rules and then you",
    "start": "1829900",
    "end": "1836710"
  },
  {
    "text": "can model your VM enter affinity rules to put the master on different host even in the same domain so it's pretty",
    "start": "1836710",
    "end": "1842080"
  },
  {
    "text": "flexible it can it can get easily confusing and complicated so we don't advise our users to choose the UI and do",
    "start": "1842080",
    "end": "1849910"
  },
  {
    "text": "it manually we have in one of our CL eyes or like SDKs we have support for",
    "start": "1849910",
    "end": "1856630"
  },
  {
    "text": "setting this programmatically for example as a post deployment job and you provision a VM that after the",
    "start": "1856630",
    "end": "1861970"
  },
  {
    "text": "provisioning is completed you set the rules we are an API call and darkus here is I want to",
    "start": "1861970",
    "end": "1867830"
  },
  {
    "text": "just give it a shout-out so go VC is a go binary it's basically a CLI tool to set those things and if you",
    "start": "1867830",
    "end": "1875240"
  },
  {
    "text": "want to go crazy we also have Gobi momi which is it go library where you can go way deeper than what go VC does but we",
    "start": "1875240",
    "end": "1882110"
  },
  {
    "text": "have many customers who use Gogi C to do the post provisioning tasks to make sure the VM lands on the same or on different",
    "start": "1882110",
    "end": "1887809"
  },
  {
    "text": "different hosts for example one last comment on some things to watch out the",
    "start": "1887809",
    "end": "1896019"
  },
  {
    "text": "zone interface implementations actually split into two phases the first phase we",
    "start": "1896019",
    "end": "1902120"
  },
  {
    "text": "implemented in kubernetes 1.12 make sure that if you run different host groups",
    "start": "1902120",
    "end": "1908899"
  },
  {
    "text": "like you have different vSphere clusters modeling your zone topology be the data",
    "start": "1908899",
    "end": "1914840"
  },
  {
    "text": "center or be at a separate vSphere cluster we define than a vSphere cluster or data center as a separate zone",
    "start": "1914840",
    "end": "1920529"
  },
  {
    "text": "meaning that those these four clusters don't know each other now we've seen",
    "start": "1920529",
    "end": "1925610"
  },
  {
    "text": "customers or we have many customers running stretch clusters stretch these four clusters across two data centers",
    "start": "1925610",
    "end": "1930679"
  },
  {
    "text": "for example where a vSphere cluster is kind of a large vSphere cluster stretches across two different data",
    "start": "1930679",
    "end": "1936590"
  },
  {
    "text": "centers but it's still the same vSphere cluster so we don't have multiple vSphere clusters in that game right now",
    "start": "1936590",
    "end": "1942230"
  },
  {
    "text": "that implementation that we did in kubernetes 1.12 does not automatically support that topology we have some",
    "start": "1942230",
    "end": "1949490"
  },
  {
    "text": "workarounds that you can use if you're running stretch clusters for example like Metro clusters or IV plexus or",
    "start": "1949490",
    "end": "1955490"
  },
  {
    "text": "whatever in the environment so just like use that link or the number of the issue",
    "start": "1955490",
    "end": "1960950"
  },
  {
    "text": "there and you get some feedback or our slack Channel 4:40 p.m. are sick and we kind of give you some help in that case",
    "start": "1960950",
    "end": "1967370"
  },
  {
    "text": "but if you have the luxury of running separate vSphere environments like separate VC for clusters then it's easy",
    "start": "1967370",
    "end": "1973429"
  },
  {
    "text": "really easy to model the disowns implementation as we did so that is the phase one implementation another issue",
    "start": "1973429",
    "end": "1979580"
  },
  {
    "text": "right now that were working on is coming from that having separate vSphere",
    "start": "1979580",
    "end": "1984980"
  },
  {
    "text": "clusters for your environment you might want to have storage dynamic storage",
    "start": "1984980",
    "end": "1990019"
  },
  {
    "text": "provisioning which is a nice feature in kubernetes if you want to work with with persistent volumes now right now the vSphere cloud provider",
    "start": "1990019",
    "end": "1996169"
  },
  {
    "text": "does not support deploying dynamic storage or volumes in those setups where",
    "start": "1996169",
    "end": "2002019"
  },
  {
    "text": "there's no shared storage amongst the clusters just to repeat that if you have separate vSphere clusters or hosts it",
    "start": "2002019",
    "end": "2008980"
  },
  {
    "text": "could also be single hosts and they don't share storage amongst them in between them like there's no NFS shared",
    "start": "2008980",
    "end": "2015070"
  },
  {
    "text": "storage or there's no VMFS stretched across those nodes it's right now it's not possible to use dynamic volume",
    "start": "2015070",
    "end": "2020950"
  },
  {
    "text": "provisioning in kubernetes so we're working on that that's just a logic a logic constraint that we have in a cloud",
    "start": "2020950",
    "end": "2027370"
  },
  {
    "text": "provider that we need to fix and we'll hope to fix it in the next in the next portion so those are the two major",
    "start": "2027370",
    "end": "2033010"
  },
  {
    "text": "issues that besides performance I see in the field and we just want to make sure that we prevent them by educating or our",
    "start": "2033010",
    "end": "2039909"
  },
  {
    "text": "user base and there's workarounds for for all of them but all of them are nice but we're working on on that dressing",
    "start": "2039909",
    "end": "2046240"
  },
  {
    "text": "that all right that's it from my side I think we are good on time I think we have two minutes I see some a couple",
    "start": "2046240",
    "end": "2052720"
  },
  {
    "text": "people poked in but I think we have time for some questions yes so there are questions where we have one go ahead yes",
    "start": "2052720",
    "end": "2061618"
  },
  {
    "text": "ah you mean yeah a what's the name for",
    "start": "2069760",
    "end": "2075190"
  },
  {
    "text": "it data store clusters the name right you have multiple data stores forming up",
    "start": "2075190",
    "end": "2081669"
  },
  {
    "text": "a storage cluster this is currently not supported in a vehicle a provider it will well we're also working on that we",
    "start": "2081669",
    "end": "2087940"
  },
  {
    "text": "are actually we're working on completely revamping the way storage is implemented in the vSphere cloud provider right now",
    "start": "2087940",
    "end": "2093250"
  },
  {
    "text": "and you might be also aware of the transitioning but from the entry cloud provider to the out of three cloud provider which is also an undertaking",
    "start": "2093250",
    "end": "2100390"
  },
  {
    "text": "that we are working and being involved in so and the container storage interface si si eventually like just g8 and so we have",
    "start": "2100390",
    "end": "2108370"
  },
  {
    "text": "now kind of more flexibility and freedom to to implement that but right now on your question it's not supported to run",
    "start": "2108370",
    "end": "2113710"
  },
  {
    "text": "data store clusters for storage good question yes yes the comment was this is",
    "start": "2113710",
    "end": "2121390"
  },
  {
    "text": "I driver does about datastore clusters but it currently not supports the entry",
    "start": "2121390",
    "end": "2126610"
  },
  {
    "text": "cloud provider any last questions otherwise if we do run over we can take",
    "start": "2126610",
    "end": "2131800"
  },
  {
    "text": "it into the hallway but I think we have time for one more if anybody's got one yeah okay yeah okay let's thank you for",
    "start": "2131800",
    "end": "2139630"
  },
  {
    "text": "coming q very much oh by the way let me throw up one other thing as part of kubernetes itself we have a bi-weekly",
    "start": "2139630",
    "end": "2146280"
  },
  {
    "text": "vmware cig meeting and this is a thing where we have developers of the cloud",
    "start": "2146280",
    "end": "2153010"
  },
  {
    "text": "provider the storage plugins as well as users and we have guest speakers it's a",
    "start": "2153010",
    "end": "2159130"
  },
  {
    "text": "zoom meeting you can join the group and I do bite you to do that if you're just catching up now the notes from the past",
    "start": "2159130",
    "end": "2165460"
  },
  {
    "text": "meetings are a rolling Google shared doc and the videos are uploaded to YouTube",
    "start": "2165460",
    "end": "2173770"
  },
  {
    "text": "so if you're using kubernetes on vSphere by all means join the group you can we",
    "start": "2173770",
    "end": "2181480"
  },
  {
    "text": "also have a channel on slack and a mailing list once again thanks for coming yeah",
    "start": "2181480",
    "end": "2191610"
  }
]