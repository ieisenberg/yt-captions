[
  {
    "text": "hello welcome today we brought a couple of failure stories that occurred to us when running kubernetes on premise on",
    "start": "0",
    "end": "7040"
  },
  {
    "text": "bare metal machines i'm david miller marco elli",
    "start": "7040",
    "end": "12559"
  },
  {
    "text": "i work at one and one million media as an expert for continuous delivery and are currently filling the role of a",
    "start": "12559",
    "end": "19359"
  },
  {
    "text": "product owner for the entire delivery platform",
    "start": "19359",
    "end": "24560"
  },
  {
    "text": "and hi everybody from my side as well my name is stefan fudios i'm an expert for container platforms at",
    "start": "24560",
    "end": "30000"
  },
  {
    "text": "one and one main media and my current role is being a product owner for our internal kubernetes platform",
    "start": "30000",
    "end": "37920"
  },
  {
    "text": "to give you a bit of context where kubernetes is running of course we are developing in-house",
    "start": "38399",
    "end": "43840"
  },
  {
    "text": "software and one of the key factors that we strive for is time to market and for optimizing time",
    "start": "43840",
    "end": "51600"
  },
  {
    "text": "to market there's three main components which are tightly coupled to each other one we need a strong devops culture",
    "start": "51600",
    "end": "60399"
  },
  {
    "text": "which only can work if we have a reliable and a good cicd platform",
    "start": "60399",
    "end": "68080"
  },
  {
    "text": "which in turn requires a robust and scalable quantitative runtime",
    "start": "68080",
    "end": "74159"
  },
  {
    "text": "platform and this is where kubernetes is being used so let's talk a bit about the context in",
    "start": "74159",
    "end": "80880"
  },
  {
    "text": "which we are using kubernetes we provide kubernetes centrally so there's one central operations teams",
    "start": "80880",
    "end": "86799"
  },
  {
    "text": "which provides kubernetes clusters for our operations and devops teams",
    "start": "86799",
    "end": "92880"
  },
  {
    "text": "those operations and devops teams are tenants for our platform we focus on soft multi-tenancy so we do",
    "start": "92880",
    "end": "99200"
  },
  {
    "text": "not try to strictly isolate the tenants from each other because they are considered friendly users",
    "start": "99200",
    "end": "104799"
  },
  {
    "text": "but we still try to do our best to help them isolate their their compute workloads",
    "start": "104799",
    "end": "112880"
  },
  {
    "text": "and their network traffic from each other by using network policies and limits and requests and we try to",
    "start": "112880",
    "end": "119759"
  },
  {
    "text": "provide them with tenant specific isolated infrastructure like dedicated ingress controllers pretend",
    "start": "119759",
    "end": "127600"
  },
  {
    "text": "our main focus is stateless microservices but by now we have some stateful services as well in",
    "start": "127600",
    "end": "133840"
  },
  {
    "text": "kubernetes we have usually fast deployment cycles every week we are",
    "start": "133840",
    "end": "139520"
  },
  {
    "text": "rolling redeploying all of our kubernetes clusters so any node within kubernetes is redeployed once a week to guarantee",
    "start": "139520",
    "end": "146959"
  },
  {
    "text": "that we can anytime add new hosts and that anytime a node can be replaced easily",
    "start": "146959",
    "end": "154480"
  },
  {
    "text": "we serve multiple clusters currently it's 15 clusters we are operating mostly uh separated on network",
    "start": "154640",
    "end": "161200"
  },
  {
    "text": "dimensions so we have dedicated clusters in the front end layer we have dedicated backend clusters and dedicated clusters on the",
    "start": "161200",
    "end": "167840"
  },
  {
    "text": "infrastructure layer we separate as well on the data center level we have three separated",
    "start": "167840",
    "end": "173920"
  },
  {
    "text": "worldwide distributed data centers and we isolate clusters regarding live usage and non-live usage",
    "start": "173920",
    "end": "181360"
  },
  {
    "text": "all our clusters use bare metal nodes on premise in our own data centers we do",
    "start": "181360",
    "end": "187360"
  },
  {
    "text": "not use any cloud providers at all for our services and one thing",
    "start": "187360",
    "end": "192640"
  },
  {
    "text": "which is relevant for some failure stories later on we use cg nut pod siders and service sliders so",
    "start": "192640",
    "end": "199840"
  },
  {
    "text": "they are isolated and reused within each clusters and there is no direct network-wise",
    "start": "199840",
    "end": "205360"
  },
  {
    "text": "interaction between our clusters we are using uh of course an open source",
    "start": "205360",
    "end": "212959"
  },
  {
    "text": "and cloud native stack you see some of the examples uh we use here that's relatively",
    "start": "212959",
    "end": "218480"
  },
  {
    "text": "straightforward we just wanted to showcase which of the great open source and cloud native projects we",
    "start": "218480",
    "end": "223760"
  },
  {
    "text": "are using thanks to them okay so let's start with a couple of",
    "start": "223760",
    "end": "229120"
  },
  {
    "text": "stories and issues we encountered which are related to the control plane so i have my application deployed on",
    "start": "229120",
    "end": "236159"
  },
  {
    "text": "kubernetes and the thing i noticed is that",
    "start": "236159",
    "end": "241760"
  },
  {
    "text": "once in a while i see a fraction of my connections hanging and timing out from the point of",
    "start": "241760",
    "end": "248000"
  },
  {
    "text": "view of a client and as far as i got it's correlated to",
    "start": "248000",
    "end": "253280"
  },
  {
    "text": "the rolling redeploys you just mentioned is there any explanation and anything i can do about it",
    "start": "253280",
    "end": "260720"
  },
  {
    "text": "of course that's not trivial so let's have a bit of a look how our setup is looking like",
    "start": "260799",
    "end": "265919"
  },
  {
    "text": "this problem you described is usually occurring or can usually occur in our frontend clusters our frontend clusters",
    "start": "265919",
    "end": "272800"
  },
  {
    "text": "do use a hardware load balancers we use the f5 big-ip appliances to expose our clusters to the public",
    "start": "272800",
    "end": "280240"
  },
  {
    "text": "internet and we use them as kind of an edge gateway to protect from malicious public",
    "start": "280240",
    "end": "285680"
  },
  {
    "text": "traffic and as you all might know services in kubernetes can be can have two different external",
    "start": "285680",
    "end": "293360"
  },
  {
    "text": "traffic policies there is external traffic policy cluster and external traffic policy local",
    "start": "293360",
    "end": "298560"
  },
  {
    "text": "and the difference between both of them is that with external traffic policy cluster each traffic entering a kubernetes node",
    "start": "298560",
    "end": "307360"
  },
  {
    "text": "is distributed across all pots within the cluster q proxy takes care of programming iv tables",
    "start": "307360",
    "end": "313280"
  },
  {
    "text": "in that way so that everything is distributed for the integration of the hardware load",
    "start": "313280",
    "end": "318960"
  },
  {
    "text": "balancer services are of type noteport so it's a port-based addressing scheme",
    "start": "318960",
    "end": "324400"
  },
  {
    "text": "and so the f5 forwards traffic to one of the host ips on a specific service specific port",
    "start": "324400",
    "end": "332320"
  },
  {
    "text": "f5 manages then so-called pool member tables and in these tables you see all the",
    "start": "332320",
    "end": "337919"
  },
  {
    "text": "nodes of the kubernetes cluster with their node port at least that's one mechanism on how to integrate f5",
    "start": "337919",
    "end": "344639"
  },
  {
    "text": "into the kubernetes cluster and if we have a look at external traffic policy local that's this example then you would",
    "start": "344639",
    "end": "351759"
  },
  {
    "text": "see that each q q proxy just forwards the traffic to single",
    "start": "351759",
    "end": "357039"
  },
  {
    "text": "local ports it could be multiple parts on the on the same node so that it would balance there but there's no cross node communication",
    "start": "357039",
    "end": "364160"
  },
  {
    "text": "anymore and in that example if you have a look at the pool member table on our third node um the health check",
    "start": "364160",
    "end": "371680"
  },
  {
    "text": "of the pool member would be red because there is no orange pot so the q proxy or ip tables could not",
    "start": "371680",
    "end": "378080"
  },
  {
    "text": "forward traffic on that node and now we get into the detail why that is a problem uh for your service",
    "start": "378080",
    "end": "385680"
  },
  {
    "text": "direct um because we notice that we are losing",
    "start": "385680",
    "end": "390800"
  },
  {
    "text": "traffic any time a new node joins a cluster imagine the pool member table from f5",
    "start": "390800",
    "end": "396560"
  },
  {
    "text": "and we have two nodes in there and both of them are healthy regarding that single service on node",
    "start": "396560",
    "end": "402240"
  },
  {
    "text": "port 3001 and now a new node joins the cluster and the new node is added",
    "start": "402240",
    "end": "408720"
  },
  {
    "text": "by the f5 operator to the pool member table in an unknown health state and as soon as the",
    "start": "408720",
    "end": "414880"
  },
  {
    "text": "health state has been established there are two different options regarding the external traffic policy",
    "start": "414880",
    "end": "420479"
  },
  {
    "text": "when the external traffic policy was local then uh in the service example from before um",
    "start": "420479",
    "end": "426800"
  },
  {
    "text": "the health check would be red and if we have external traffic policy cluster the health state would be green because",
    "start": "426800",
    "end": "434880"
  },
  {
    "text": "the pots are reachable even over the new node and as you can imagine if you know it",
    "start": "434880",
    "end": "441360"
  },
  {
    "text": "joins the cluster usually it would not contain a workload of that specific type",
    "start": "441360",
    "end": "446960"
  },
  {
    "text": "so for every service regarding external traffic policy local the service check would",
    "start": "446960",
    "end": "453120"
  },
  {
    "text": "not be healthy at the beginning so but let's have a look at the state in",
    "start": "453120",
    "end": "458479"
  },
  {
    "text": "between what happens when the health status unknown and now we stumble upon our problem",
    "start": "458479",
    "end": "464319"
  },
  {
    "text": "f5 has a logic when traffic is unknown um then it's still routed there",
    "start": "464319",
    "end": "471440"
  },
  {
    "text": "only if the health state is red then the traffic will not be routed",
    "start": "471440",
    "end": "477759"
  },
  {
    "text": "and if you use external traffic policy local and as just mentioned a new node does not contain any workload",
    "start": "477759",
    "end": "483440"
  },
  {
    "text": "in the beginning you have a risk that the traffic is routed to a node which cannot",
    "start": "483440",
    "end": "488639"
  },
  {
    "text": "serve the workload so the solutions to that are you can tune the ramp up times um",
    "start": "488639",
    "end": "495440"
  },
  {
    "text": "because you can configure f5 that any newly joining pool member um only gets a",
    "start": "495440",
    "end": "501599"
  },
  {
    "text": "fraction of its original part and only over time gets more so that's a that's the slow ramp timer in a five",
    "start": "501599",
    "end": "508720"
  },
  {
    "text": "in addition you can tune your health checks so that you notice earlier that the health state is red and",
    "start": "508720",
    "end": "514560"
  },
  {
    "text": "not green uh the standard settings would take 16 seconds until the health state would be established",
    "start": "514560",
    "end": "521120"
  },
  {
    "text": "and of course you can tune that to notice that earlier you do the health checks more often but of course that",
    "start": "521120",
    "end": "526640"
  },
  {
    "text": "puts more health check load on the cluster so we use this as mitigations but the",
    "start": "526640",
    "end": "533680"
  },
  {
    "text": "real thing we did is we patched the f5 operator and we opened the issue and provided a merge",
    "start": "533680",
    "end": "540000"
  },
  {
    "text": "request um to to fix it or to at least ease that and we explicitly activate nodes now after the",
    "start": "540000",
    "end": "547839"
  },
  {
    "text": "health state has been established so first we join a node to the cluster and it is still inactive but it's already known to",
    "start": "547839",
    "end": "555120"
  },
  {
    "text": "f5 the health state can be established and only if it's established we enable",
    "start": "555120",
    "end": "560560"
  },
  {
    "text": "the node in the pool member table so that it gets its share of traffic",
    "start": "560560",
    "end": "566240"
  },
  {
    "text": "by that we guarantee that this this case where the health state is",
    "start": "566240",
    "end": "572080"
  },
  {
    "text": "still unknown does not occur and traffic is not received when the node is not enabled um",
    "start": "572080",
    "end": "579519"
  },
  {
    "text": "another option is to reduce the number of nodes where a five forwards traffic turn",
    "start": "579519",
    "end": "586480"
  },
  {
    "text": "by default every kubernetes node would be eligible to receive traffic directly from f5 but you could",
    "start": "586480",
    "end": "593120"
  },
  {
    "text": "mark only a specific subset of your nodes as ingress nodes like a onenote per rack for example and",
    "start": "593120",
    "end": "599440"
  },
  {
    "text": "only forward traffic there so you have a lower amount a smaller amount of notes",
    "start": "599440",
    "end": "605519"
  },
  {
    "text": "which are registered in f5 and you have less fluctuation on the pool members",
    "start": "605519",
    "end": "612640"
  },
  {
    "text": "and the last thing which is the deepest integration you can do a direct vxlan integration",
    "start": "612640",
    "end": "617920"
  },
  {
    "text": "with f5 then you would not use the node port mechanism anymore but the f5 balancers",
    "start": "617920",
    "end": "625600"
  },
  {
    "text": "would be considered pot in your cluster they would receive ips from the pot cider range and uh by that",
    "start": "625600",
    "end": "634320"
  },
  {
    "text": "the f5 instances directly address pots and have pot ips in their pool member tables",
    "start": "634320",
    "end": "639680"
  },
  {
    "text": "and not node ips anymore and the problem with the unknown health state for a node would not occur",
    "start": "639680",
    "end": "646480"
  },
  {
    "text": "anymore okay great but not all of the problems",
    "start": "646480",
    "end": "652320"
  },
  {
    "text": "and issues we encounter are located in the control plane some also occur on the data plane let's have",
    "start": "652320",
    "end": "658560"
  },
  {
    "text": "a look at these so now that my application's running fine i'm looking",
    "start": "658560",
    "end": "663680"
  },
  {
    "text": "at the data and processing and i notice that for some use cases where",
    "start": "663680",
    "end": "670000"
  },
  {
    "text": "in the old world i could use the source ip address i only see internal ip addresses",
    "start": "670000",
    "end": "676399"
  },
  {
    "text": "like for geo location services or for classifying traffic or for",
    "start": "676399",
    "end": "681839"
  },
  {
    "text": "for identifying botnets so from my point of view it looks like the traffic",
    "start": "681839",
    "end": "687279"
  },
  {
    "text": "from the web outside is going through some magic box and afterwards all the",
    "start": "687279",
    "end": "693680"
  },
  {
    "text": "original ips have disappeared is there any way i can get them back",
    "start": "693680",
    "end": "700800"
  },
  {
    "text": "so thanks for that question that's a tricky one as well um so in kubernetes a bunch of network",
    "start": "701440",
    "end": "706959"
  },
  {
    "text": "address translation is involved and the standard thing is when a user",
    "start": "706959",
    "end": "712240"
  },
  {
    "text": "connects to a service within kubernetes traffic is directed to one of the",
    "start": "712240",
    "end": "717519"
  },
  {
    "text": "kubernetes nodes and then the ip tables rules kick in which are programmed by q proxy",
    "start": "717519",
    "end": "723839"
  },
  {
    "text": "and usually two layers of network address translation take place there the",
    "start": "723839",
    "end": "729040"
  },
  {
    "text": "one is that the destination address is changed um this is the load balancing mechanism",
    "start": "729040",
    "end": "734320"
  },
  {
    "text": "of kubernetes so that the service ip destination address is changed into a pots ip address for",
    "start": "734320",
    "end": "741440"
  },
  {
    "text": "routing the the traffic to one of the pods but the second thing which happens uh when uh traffic uh is",
    "start": "741440",
    "end": "749279"
  },
  {
    "text": "uh sent from one kubernetes node to another one is that sourcenet is applied as well to",
    "start": "749279",
    "end": "755760"
  },
  {
    "text": "allow for the traffic returning on the same path as it entered the pod so then a pod can",
    "start": "755760",
    "end": "762399"
  },
  {
    "text": "respond to the original host um there s net and destination net",
    "start": "762399",
    "end": "767839"
  },
  {
    "text": "is reversed and the traffic can be or the response can be sent back to the to the original user and there's",
    "start": "767839",
    "end": "775519"
  },
  {
    "text": "more net and proxying involved in a standard kubernetes setup imagine you have a client",
    "start": "775519",
    "end": "781040"
  },
  {
    "text": "and you have some workload pot and the first thing the client does is",
    "start": "781040",
    "end": "787200"
  },
  {
    "text": "usually uh the request is sent to some load balancer already part of your corporate network",
    "start": "787200",
    "end": "793120"
  },
  {
    "text": "and uh in you remember in our case these are f5 big-ip load balancers and they are for full isolation we apply",
    "start": "793120",
    "end": "800880"
  },
  {
    "text": "already source network address translation on the load balancer so already when the traffic enters the",
    "start": "800880",
    "end": "807120"
  },
  {
    "text": "kubernetes cluster at the cube proxy it's not the original client ip anymore but it's one of the source net",
    "start": "807120",
    "end": "813279"
  },
  {
    "text": "addresses of the f5 and now we have the one from the slide before cue proxy",
    "start": "813279",
    "end": "819040"
  },
  {
    "text": "does a source net and destination net to forward the packet but now there's",
    "start": "819040",
    "end": "824560"
  },
  {
    "text": "ingress controllers as well in our example here it's traffic so the packet is not only forwarded to",
    "start": "824560",
    "end": "830800"
  },
  {
    "text": "the workload directly it's for what it took traffic and now traffic acts as a layer 7 reverse proxy",
    "start": "830800",
    "end": "836959"
  },
  {
    "text": "and then for what's the traffic to your workload so what the workload part sees as the",
    "start": "836959",
    "end": "842000"
  },
  {
    "text": "physical source ip is just the ip address of a traffic pod so you see a bunch of",
    "start": "842000",
    "end": "848240"
  },
  {
    "text": "addresses have been involved here so what can we do about that if you have a layer 7 protocol like http",
    "start": "848240",
    "end": "855920"
  },
  {
    "text": "or grpc you can use application level headers to preserve the original ip address you",
    "start": "855920",
    "end": "862639"
  },
  {
    "text": "need to do that of course on the first point where the packet is entering your system in our case it would be our load balancer",
    "start": "862639",
    "end": "869680"
  },
  {
    "text": "and you would preserve and use a forwarded or x forwarded four or x forwarded host headers",
    "start": "869680",
    "end": "875519"
  },
  {
    "text": "um for what it is based on an rfc and x4 x4.4 and x4 what it hosts are best",
    "start": "875519",
    "end": "881600"
  },
  {
    "text": "practices which are not really rfc backed if you really want to make sure that",
    "start": "881600",
    "end": "888240"
  },
  {
    "text": "nobody injected wrong ip addresses then you would even reset potentially existing x forwarded four",
    "start": "888240",
    "end": "894480"
  },
  {
    "text": "headers on the load balancer and only add the physical client ips the first hop",
    "start": "894480",
    "end": "900000"
  },
  {
    "text": "when you within your x forwarded for list for traffic which is not uh capable",
    "start": "900000",
    "end": "906560"
  },
  {
    "text": "uh on the on the application protocol level to uh to add some headers with original ip",
    "start": "906560",
    "end": "912480"
  },
  {
    "text": "information you have less options but if you use external traffic policy local",
    "start": "912480",
    "end": "918959"
  },
  {
    "text": "you can guarantee that no host to host traffic occurs and so the source nut from q proxy does",
    "start": "918959",
    "end": "924959"
  },
  {
    "text": "not occur or if you use a different service routing provider like kaliko ebpf",
    "start": "924959",
    "end": "930480"
  },
  {
    "text": "but of course that makes only sense if the traffic if the original traffic is directly entering your kubernetes",
    "start": "930480",
    "end": "936240"
  },
  {
    "text": "cluster without forced network address translation on the gateway for example if it's about internal",
    "start": "936240",
    "end": "943839"
  },
  {
    "text": "traffic within your data center and you have an interest on the original ip address there",
    "start": "943839",
    "end": "951040"
  },
  {
    "text": "okay nice but in my application i observed another problem this application is running with lots of",
    "start": "951040",
    "end": "958000"
  },
  {
    "text": "replicas and one specific instance of that application once in a while goes",
    "start": "958000",
    "end": "965120"
  },
  {
    "text": "into a crash loop back off it's just a single instance on a particular node",
    "start": "965120",
    "end": "973040"
  },
  {
    "text": "and what i can observe when i cube cover describe the part that's",
    "start": "973040",
    "end": "978880"
  },
  {
    "text": "affected by that is that it's unhealthy because the liveness probe failed with a connection refused",
    "start": "978880",
    "end": "986959"
  },
  {
    "text": "but when i look at the logs from the pot itself it looks like the application would be",
    "start": "986959",
    "end": "992639"
  },
  {
    "text": "running healthy so something's going wrong here do you have an idea what's the",
    "start": "992639",
    "end": "998399"
  },
  {
    "text": "reason for that that was a tricky one uh we observed that with our calico",
    "start": "998399",
    "end": "1004399"
  },
  {
    "text": "network we are using calico with the ipip encapsulation and",
    "start": "1004399",
    "end": "1010160"
  },
  {
    "text": "since uh some recent calico version calico by default assigns smaller",
    "start": "1010160",
    "end": "1016240"
  },
  {
    "text": "ip blocks originally we use 24 ip blocks so the",
    "start": "1016240",
    "end": "1022720"
  },
  {
    "text": "the block where the pot ips come from which is attached to a specific when it is node",
    "start": "1022720",
    "end": "1028240"
  },
  {
    "text": "since a more recent version the default is slash 26 and that means if on your nodes are",
    "start": "1028240",
    "end": "1034959"
  },
  {
    "text": "running more than 64 pots you need more ip addresses than fit into a single slash 26 block",
    "start": "1034959",
    "end": "1040319"
  },
  {
    "text": "so you have multiple blocks attached to a host that in itself is not really a problem",
    "start": "1040319",
    "end": "1046720"
  },
  {
    "text": "so for each of the ip blocks a dedicated ip address is used for the ipip tunneling and we observed",
    "start": "1046720",
    "end": "1053679"
  },
  {
    "text": "that an ip address is used twice once for a pot and once for the ipip tunnel",
    "start": "1053679",
    "end": "1060400"
  },
  {
    "text": "interface and of course the tunnel interface won at that point so if traffic was sent to",
    "start": "1060400",
    "end": "1066080"
  },
  {
    "text": "a specific pot it was received by the tunnel interface and dropped there because it didn't know what to do with it",
    "start": "1066080",
    "end": "1072480"
  },
  {
    "text": "the original cost for that is a race condition when calico is managing its ipam block",
    "start": "1072480",
    "end": "1077600"
  },
  {
    "text": "and ipam handles objects you see the url here for the bug which",
    "start": "1077600",
    "end": "1082799"
  },
  {
    "text": "was open for that they were by now able to reproduce the problem so that's a good thing",
    "start": "1082799",
    "end": "1088640"
  },
  {
    "text": "just the fix for that is missing yet our intermediate solution was to use",
    "start": "1088640",
    "end": "1095360"
  },
  {
    "text": "slash 24 blocks again because then we didn't have a problem",
    "start": "1095360",
    "end": "1101039"
  },
  {
    "text": "that problem affects the reachability of a pot on a network level and it showed only up in our monitoring",
    "start": "1101039",
    "end": "1107919"
  },
  {
    "text": "because we had network based liveness checks of the pods the",
    "start": "1107919",
    "end": "1113200"
  },
  {
    "text": "the cubelet was not able to reach the the network part of the workload because",
    "start": "1113200",
    "end": "1119919"
  },
  {
    "text": "of the ipip tunnel interface so kubelet checked for liveness by using the",
    "start": "1119919",
    "end": "1125440"
  },
  {
    "text": "network stack and found the application not being live and this is why it went into a crash loop backup okay",
    "start": "1125440",
    "end": "1132080"
  },
  {
    "text": "interesting but now i'm having a look at my resource consumption metrics and",
    "start": "1132080",
    "end": "1139039"
  },
  {
    "text": "i notice that i sometimes get less cpu cover than i expect for my application i have",
    "start": "1139039",
    "end": "1148000"
  },
  {
    "text": "requested resources and also cpu and limits set and as you can see on on that graph",
    "start": "1148000",
    "end": "1155600"
  },
  {
    "text": "the the blue line on top shows the limit and i'm sure that the application will have enough workload",
    "start": "1155600",
    "end": "1161440"
  },
  {
    "text": "to fully use the limit and the node would have the capacity but actually i'm getting what the orange",
    "start": "1161440",
    "end": "1167919"
  },
  {
    "text": "line shows which is less than half of that and it's essentially a pretty",
    "start": "1167919",
    "end": "1176080"
  },
  {
    "text": "strongly threaded multi-threaded application so i'd really like to make use of the",
    "start": "1176080",
    "end": "1183280"
  },
  {
    "text": "power i could receive from from the node is there any way to improve that",
    "start": "1183280",
    "end": "1188480"
  },
  {
    "text": "situation uh so what you hit now is even one layer",
    "start": "1188480",
    "end": "1193679"
  },
  {
    "text": "deeper this now is a buck in the linux kernel so the linux kernel has a completely",
    "start": "1193679",
    "end": "1201840"
  },
  {
    "text": "fair scheduler and that has a bandwidth control management and that's responsible for the pod cpu limits",
    "start": "1201840",
    "end": "1208720"
  },
  {
    "text": "there is a different mechanism for managing cpu requests um and if you have now a multi-threaded",
    "start": "1208720",
    "end": "1216240"
  },
  {
    "text": "workload which is not really cpu bound but does a lot of i o weights or network weights then you might get hit by this bug the",
    "start": "1216240",
    "end": "1223600"
  },
  {
    "text": "work was even so severe that multiple users especially those which use different clusters for multi-tenancy",
    "start": "1223600",
    "end": "1232480"
  },
  {
    "text": "they even disabled the limit support because the performance degradation was so severe there have been multiple",
    "start": "1232480",
    "end": "1240640"
  },
  {
    "text": "patches for the kernel and it took some time the first one started mid 2018 and the last one is really fixing the",
    "start": "1240640",
    "end": "1247520"
  },
  {
    "text": "issue came in in september 2019 and uh then for example like in our case",
    "start": "1247520",
    "end": "1253760"
  },
  {
    "text": "where we use container linux uh the patches were ported there in november 2019",
    "start": "1253760",
    "end": "1259840"
  },
  {
    "text": "so in december we were able to roll out patched kernels which solved the problem for us i added",
    "start": "1259840",
    "end": "1266320"
  },
  {
    "text": "some links here for the kubernetes issue tracking these problems the coreos bug for porting that to container linux and",
    "start": "1266320",
    "end": "1273919"
  },
  {
    "text": "the two main commits fixing the problem within the linux grant",
    "start": "1273919",
    "end": "1279679"
  },
  {
    "text": "okay i found another problem sometimes i lose tcp connections which",
    "start": "1279679",
    "end": "1285600"
  },
  {
    "text": "already have been established but all of a sudden they are dropped again this is happening when you do cluster",
    "start": "1285600",
    "end": "1292880"
  },
  {
    "text": "redeployments but it also happens when my application is scaling up and down",
    "start": "1292880",
    "end": "1298640"
  },
  {
    "text": "because spikes cause the autoscaler to create more replicas or remove",
    "start": "1298640",
    "end": "1304240"
  },
  {
    "text": "replicas is there an explanation for that",
    "start": "1304240",
    "end": "1309440"
  },
  {
    "text": "so now we come back a bit uh we return a bit from the low level and that's more of a configuration and design thing",
    "start": "1309440",
    "end": "1316240"
  },
  {
    "text": "in our bare metal setups so um i mentioned earlier that we use the",
    "start": "1316240",
    "end": "1321919"
  },
  {
    "text": "five load balancers as gateways for our fronted networks but in our backend networks and in our",
    "start": "1321919",
    "end": "1328080"
  },
  {
    "text": "infrastructure clusters we do not use hardware load balancers but we use direct bgp announcements of",
    "start": "1328080",
    "end": "1336720"
  },
  {
    "text": "service ips or in fact of load balancer ips to our routing infrastructure and we use",
    "start": "1336720",
    "end": "1342400"
  },
  {
    "text": "metal lb for that and when those service ips are announced via bgp",
    "start": "1342400",
    "end": "1347440"
  },
  {
    "text": "then we use equal cost multi-path routing so the routers then would know multiple",
    "start": "1347440",
    "end": "1353520"
  },
  {
    "text": "paths on how to reach that load balancer ip address when using when knowing multiple paths",
    "start": "1353520",
    "end": "1360799"
  },
  {
    "text": "and of course the router has to decide which path to use and the router in general is stateless",
    "start": "1360799",
    "end": "1365840"
  },
  {
    "text": "so it needs to be some reliable algorithm on where to send traffic and that means that anytime the",
    "start": "1365840",
    "end": "1371760"
  },
  {
    "text": "routing tables are changing then potentially the association where to route connections",
    "start": "1371760",
    "end": "1377440"
  },
  {
    "text": "are sent because there's no real connection tracking in place so every time the routing tables are changing it",
    "start": "1377440",
    "end": "1383919"
  },
  {
    "text": "can cause resets of your tcp connections because the follow-up packets are sent",
    "start": "1383919",
    "end": "1389039"
  },
  {
    "text": "to a different destination host which does not know of the tcp connection and first sends a reset reply",
    "start": "1389039",
    "end": "1395840"
  },
  {
    "text": "so only when the tcp connection is newly established everything works again fine the solution to that one is to",
    "start": "1395840",
    "end": "1403120"
  },
  {
    "text": "reduce the number of speakers to stabilize the changes the less speakers you have",
    "start": "1403120",
    "end": "1408240"
  },
  {
    "text": "the less changes to your routing tables you have but the speakers you have the less ingress",
    "start": "1408240",
    "end": "1413760"
  },
  {
    "text": "paths you have as well to ingest your traffic so you cannot reduce them",
    "start": "1413760",
    "end": "1418799"
  },
  {
    "text": "the speakers to just a single one or two because you need to have sufficient amount of speakers to process",
    "start": "1418799",
    "end": "1425520"
  },
  {
    "text": "all your inbound traffic the second thing you mentioned was when",
    "start": "1425520",
    "end": "1431679"
  },
  {
    "text": "you have load spikes and you are using auto scaling we observed that when auto scaling our",
    "start": "1431679",
    "end": "1437279"
  },
  {
    "text": "traffic ingress controller and for being able to keep our",
    "start": "1437279",
    "end": "1442400"
  },
  {
    "text": "source ip addresses we use traffic with external traffic policy local and metal lb is aware of external",
    "start": "1442400",
    "end": "1449039"
  },
  {
    "text": "traffic policy local and is then only announcing the service ip address of traffic if on the local host",
    "start": "1449039",
    "end": "1455279"
  },
  {
    "text": "really is a traffic pod up and running so what that means if you have",
    "start": "1455279",
    "end": "1462960"
  },
  {
    "text": "um if you have less traffic pods then you have bgp speaker pods and your horizontal",
    "start": "1462960",
    "end": "1470559"
  },
  {
    "text": "port auto autoscaler is scaling up your traffic because you have a load spike then a new",
    "start": "1470559",
    "end": "1476400"
  },
  {
    "text": "traffic pod comes up on a new speaker so a new route is established or a new",
    "start": "1476400",
    "end": "1481600"
  },
  {
    "text": "route is announced to the to the routing infrastructure and the routers change the routing tables because they",
    "start": "1481600",
    "end": "1488000"
  },
  {
    "text": "now have one routing entry more in their tables and so you have an ecmp change and tcp",
    "start": "1488000",
    "end": "1494400"
  },
  {
    "text": "connections are disrupted and yeah if you have spiky workloads so scaling up means an ecmp",
    "start": "1494400",
    "end": "1500640"
  },
  {
    "text": "change scaling down means again an ic an ecmp change and that of course is then a",
    "start": "1500640",
    "end": "1507120"
  },
  {
    "text": "problem because you have a lot of changes to the routing tables our solution in that case was",
    "start": "1507120",
    "end": "1513520"
  },
  {
    "text": "that we have as many traffic instances always as we have bgp speakers",
    "start": "1513520",
    "end": "1520000"
  },
  {
    "text": "so if a new traffic pod comes up that comes up on a node which already has the bgp speaker",
    "start": "1520000",
    "end": "1526400"
  },
  {
    "text": "and does not change the routing tables but that hints to another additional",
    "start": "1526400",
    "end": "1531760"
  },
  {
    "text": "caveat the real traffic balancing",
    "start": "1531760",
    "end": "1537520"
  },
  {
    "text": "or the distribution occurs on the routers so if you have one node with the bgp",
    "start": "1537520",
    "end": "1543360"
  },
  {
    "text": "speaker and five traffic pods and you have one node with the bgp speaker and just a single traffic pod then the",
    "start": "1543360",
    "end": "1549840"
  },
  {
    "text": "node with a single traffic pod will get the same amount of traffic into your pot as the other note so it",
    "start": "1549840",
    "end": "1556720"
  },
  {
    "text": "will have five times as much traffic as the other traffic",
    "start": "1556720",
    "end": "1562799"
  },
  {
    "text": "okay as the last takeaway there's still a few more remaining open",
    "start": "1564000",
    "end": "1569919"
  },
  {
    "text": "issues recently we observed again network",
    "start": "1569919",
    "end": "1575120"
  },
  {
    "text": "ingress issues where actually we lost a traffic and it certainly has",
    "start": "1575120",
    "end": "1582880"
  },
  {
    "text": "got something to do with the load balancing mechanisms and or egp the analysis is still ongoing",
    "start": "1582880",
    "end": "1591039"
  },
  {
    "text": "so we weren't actually able to show that to you yet",
    "start": "1591039",
    "end": "1596559"
  },
  {
    "text": "we stumbled upon pot affinities and anti-affinities which did not work as expected um",
    "start": "1596559",
    "end": "1604320"
  },
  {
    "text": "to to guarantee the highest amount of redundancy in our deployment pipelines we add",
    "start": "1604320",
    "end": "1611200"
  },
  {
    "text": "standard affinities to distribute the application first in different rooms then in different uh top of rack",
    "start": "1611200",
    "end": "1618640"
  },
  {
    "text": "switches then to different racks and then to different hosts and we noticed at multiple occasions",
    "start": "1618640",
    "end": "1627440"
  },
  {
    "text": "that these affinities did not work as expected so setting up affinities",
    "start": "1627440",
    "end": "1634320"
  },
  {
    "text": "is not as trivial as one might expect and maybe there are even bugs lurking in there which we did not really",
    "start": "1634320",
    "end": "1640559"
  },
  {
    "text": "pinpoint so far but we are working on that and one last thing we haven't been able to",
    "start": "1640559",
    "end": "1647520"
  },
  {
    "text": "clarify yet is an issue with q proxy",
    "start": "1647520",
    "end": "1652559"
  },
  {
    "text": "that was completely overloaded through a single application with just 20 parts and",
    "start": "1652559",
    "end": "1660399"
  },
  {
    "text": "these pods for some reason kept crashing and went in ultimately into a crash loop back off",
    "start": "1660399",
    "end": "1667200"
  },
  {
    "text": "but q proxy was completely overloaded did consume lots of cpu and",
    "start": "1667200",
    "end": "1674080"
  },
  {
    "text": "wasn't able to update the ip table's rules quickly enough so that sort of screwed",
    "start": "1674080",
    "end": "1681039"
  },
  {
    "text": "up the entire nodes where that happened",
    "start": "1681039",
    "end": "1685519"
  },
  {
    "text": "and to conclude from what we showed and learned there's a couple of",
    "start": "1687679",
    "end": "1694720"
  },
  {
    "text": "architectural implications to take away so the entire design through the entire stack",
    "start": "1694720",
    "end": "1700240"
  },
  {
    "text": "the platform but also the applications should take it seriously to design for failure",
    "start": "1700240",
    "end": "1708640"
  },
  {
    "text": "failures will happen once in a while and over and over again there's no perfectly safe and",
    "start": "1708640",
    "end": "1714960"
  },
  {
    "text": "perfectly reliable system so if your design respects that the impact is probably way lower than it",
    "start": "1714960",
    "end": "1720799"
  },
  {
    "text": "could be what helped us in many places was having metrics so we could do post mortems but of",
    "start": "1720799",
    "end": "1728320"
  },
  {
    "text": "course we didn't know initially what all the metrics where we needed so over time we built up a bunch",
    "start": "1728320",
    "end": "1736000"
  },
  {
    "text": "of metrics that we continuously record and that grow more and more useful for failure",
    "start": "1736000",
    "end": "1742720"
  },
  {
    "text": "analysis in particular when the failures get more difficult more subtle more complicated",
    "start": "1742720",
    "end": "1750398"
  },
  {
    "text": "also whenever something goes wrong it's not unlikely that it's",
    "start": "1751200",
    "end": "1758480"
  },
  {
    "text": "not only in either the infrastructure or the application more often than not it shows that",
    "start": "1758480",
    "end": "1765840"
  },
  {
    "text": "there's an interaction or interference between both um and so whenever a failure occurs you",
    "start": "1765840",
    "end": "1773279"
  },
  {
    "text": "should keep in mind to look at both uh at both sides and see if there's a an interconnection",
    "start": "1773279",
    "end": "1781039"
  },
  {
    "text": "between them and what helped us in many",
    "start": "1781039",
    "end": "1787039"
  },
  {
    "text": "places is to have really well designed proper liveness and",
    "start": "1787039",
    "end": "1792880"
  },
  {
    "text": "readiness checks so if these are well designed they can quickly point to problems and",
    "start": "1792880",
    "end": "1800399"
  },
  {
    "text": "quickly help you to find the root cause of whatever goes wrong",
    "start": "1800399",
    "end": "1806240"
  },
  {
    "text": "so and at the very end um of course we are not the only people",
    "start": "1806480",
    "end": "1811760"
  },
  {
    "text": "experienced saying failures um and i would like to point you",
    "start": "1811760",
    "end": "1817039"
  },
  {
    "text": "for more failure stories to the collection maintained by hanging jakobs at the",
    "start": "1817039",
    "end": "1822799"
  },
  {
    "text": "address shown here so thanks for attending this presentation and have fun with all the remaining",
    "start": "1822799",
    "end": "1829120"
  },
  {
    "text": "presentations of kubecon",
    "start": "1829120",
    "end": "1834320"
  }
]