[
  {
    "text": "today Dixie and myself Auntie are going to talk about CPU Affinity that means",
    "start": "320",
    "end": "8599"
  },
  {
    "text": "that which set of CPUs should run which set of",
    "start": "8599",
    "end": "14519"
  },
  {
    "text": "containers why it makes a big difference and how you can actually do",
    "start": "14519",
    "end": "21439"
  },
  {
    "text": "it so when we already got our presentation accepted I asked Dixie if",
    "start": "21439",
    "end": "27599"
  },
  {
    "text": "she had some like relevant work Lo in mind with which we could demonstrate the",
    "start": "27599",
    "end": "33120"
  },
  {
    "text": "benefits of CPU Affinity she provided me with the docker file for running tensor",
    "start": "33120",
    "end": "40200"
  },
  {
    "text": "flow AI model training on CPU and we had a note kuus worker",
    "start": "40200",
    "end": "48680"
  },
  {
    "text": "note uh with this kind of Hardware maybe it's not sorry maybe it's not fully",
    "start": "48680",
    "end": "55520"
  },
  {
    "text": "visible uh there but it's like a two socket system having 128 CPUs in it and",
    "start": "55520",
    "end": "65080"
  },
  {
    "text": "operating system was telling that okay there are two new man noes here like one CPU for one Numan node the other CPU for",
    "start": "65080",
    "end": "71720"
  },
  {
    "text": "the other Numan node and when we went to the bias and enabled subn Numa",
    "start": "71720",
    "end": "79280"
  },
  {
    "text": "clustering uh then more details about this Hardware was exposed to the",
    "start": "79280",
    "end": "84439"
  },
  {
    "text": "operating system and it resulted in showing that you have eight Numa noes",
    "start": "84439",
    "end": "90479"
  },
  {
    "text": "so eight set of CPUs eight memories and that's something which",
    "start": "90479",
    "end": "98040"
  },
  {
    "text": "makes a difference when we are assigning workload to these",
    "start": "98040",
    "end": "103640"
  },
  {
    "text": "CPUs so what I'm presenting here is a um",
    "start": "103640",
    "end": "109360"
  },
  {
    "text": "throughput from that AI model training when we are increasing the number of",
    "start": "109360",
    "end": "115200"
  },
  {
    "text": "containers on that node that are doing the training in parallel",
    "start": "115200",
    "end": "121079"
  },
  {
    "text": "so in the beginning we are running just a single container and there is no big",
    "start": "121079",
    "end": "126479"
  },
  {
    "text": "difference in the throughputs in these three cases so one case is we do not",
    "start": "126479",
    "end": "132080"
  },
  {
    "text": "have any CPU Affinity at all the other one is that we are assigning these",
    "start": "132080",
    "end": "139760"
  },
  {
    "text": "containers to a set of four CPUs each finally there's a case with running a",
    "start": "139760",
    "end": "147599"
  },
  {
    "text": "container on only two two CPUs for each",
    "start": "147599",
    "end": "153760"
  },
  {
    "text": "container and when we were adding more containers on this note already there's",
    "start": "153760",
    "end": "161200"
  },
  {
    "text": "a big difference now when we are running eight containers that is eight trainings",
    "start": "161200",
    "end": "166800"
  },
  {
    "text": "concurrently so line for no CPU affinity and then top",
    "start": "166800",
    "end": "173800"
  },
  {
    "text": "lines for two and four CPUs and when increasing the number of containers again now up to 15 containers",
    "start": "173800",
    "end": "181879"
  },
  {
    "text": "running in parallel the chomp is pretty clear when we are doing CPU Affinity",
    "start": "181879",
    "end": "188159"
  },
  {
    "text": "setting there and if there is no CPU Affinity that is all tensor flow",
    "start": "188159",
    "end": "193640"
  },
  {
    "text": "trainings can see all the CPS on the Note then this doesn't really increase",
    "start": "193640",
    "end": "199400"
  },
  {
    "text": "through but that much and finally when pushing to the limits we are running 20",
    "start": "199400",
    "end": "205360"
  },
  {
    "text": "containers simultaneously there on the Note uh when I tried to push even more",
    "start": "205360",
    "end": "211879"
  },
  {
    "text": "that resulted in out of memory error so we do not have just enough memory for that purpose this time um but there we",
    "start": "211879",
    "end": "220200"
  },
  {
    "text": "can see now maybe a small difference between two CPUs and four CPUs cases so",
    "start": "220200",
    "end": "227439"
  },
  {
    "text": "if we allowed using four CPUs for each training we have a bit more variation",
    "start": "227439",
    "end": "233640"
  },
  {
    "text": "there in the results than what we have with two CPUs anyway it again gave small",
    "start": "233640",
    "end": "240840"
  },
  {
    "text": "throughput increase and maybe not that small actually to increase the number of",
    "start": "240840",
    "end": "246879"
  },
  {
    "text": "containers so I guess that this requires and need like calls for an explanation",
    "start": "246879",
    "end": "252599"
  },
  {
    "text": "where this extra performance is coming from when you are actually only limiting",
    "start": "252599",
    "end": "257759"
  },
  {
    "text": "the number of CPUs that these workloads can see so there are different sources for",
    "start": "257759",
    "end": "266000"
  },
  {
    "text": "the extra performance it depends on the case actually bit that's where it is coming from in which case so First Data",
    "start": "266000",
    "end": "274400"
  },
  {
    "text": "locality we are allocating these CPUs where we find these containers so that",
    "start": "274400",
    "end": "281280"
  },
  {
    "text": "they are all always close to some memory node so all the memory accesses are to",
    "start": "281280",
    "end": "288360"
  },
  {
    "text": "the most local fastest memory and that that already improves",
    "start": "288360",
    "end": "294120"
  },
  {
    "text": "performance a lot there uh another Point cash hits",
    "start": "294120",
    "end": "300720"
  },
  {
    "text": "so uh for each cach level there is actually less users less",
    "start": "300720",
    "end": "307120"
  },
  {
    "text": "processes that sort of use the cash that pollute the cash in other words and then",
    "start": "307120",
    "end": "313880"
  },
  {
    "text": "also there are lacking this far away cash invalidation so that here some CPU",
    "start": "313880",
    "end": "319800"
  },
  {
    "text": "would be reading memory from there and here would be writing memory from there and that right then invalidates the cash",
    "start": "319800",
    "end": "326880"
  },
  {
    "text": "in this different socket even again another reason is CPU frequency so",
    "start": "326880",
    "end": "335160"
  },
  {
    "text": "now that we are packing the workload to a smaller set of CPUs it is more likely",
    "start": "335160",
    "end": "340639"
  },
  {
    "text": "that there are some CPUs actually idle and those idle CPUs can donate their",
    "start": "340639",
    "end": "346960"
  },
  {
    "text": "power budget to those CPUs that are busy which means that those CPUs will be then",
    "start": "346960",
    "end": "352880"
  },
  {
    "text": "running automatically with higher CPU frequency you don't have to do anything for that to happen",
    "start": "352880",
    "end": "360560"
  },
  {
    "text": "finally workloads can be behave sorry behave smarter when they don't see the",
    "start": "360560",
    "end": "367960"
  },
  {
    "text": "whole system and way they don't think that they actually own the whole system so some Frameworks and run times may be",
    "start": "367960",
    "end": "375840"
  },
  {
    "text": "like overwhelmed oh oh we we have like 128 CPUs that's a lot let's start a new",
    "start": "375840",
    "end": "382720"
  },
  {
    "text": "threat for each CPU and let's take the advantage of all of those imagine that",
    "start": "382720",
    "end": "387880"
  },
  {
    "text": "you are running 20 of this kind of workloads on your system and you have le 20 competing threats for each CPU core",
    "start": "387880",
    "end": "396400"
  },
  {
    "text": "when we split the CPUs to smaller sets this doesn't happen so now taking a step back from",
    "start": "396400",
    "end": "404759"
  },
  {
    "text": "this single performance trial and trying to form a bigger picture that what we",
    "start": "404759",
    "end": "409840"
  },
  {
    "text": "are actually talking about so we are dealing and doing with the kues worker",
    "start": "409840",
    "end": "416240"
  },
  {
    "text": "note scope we want to to see affinity for all qos classes of workload so",
    "start": "416240",
    "end": "423560"
  },
  {
    "text": "guaranteed personable and best effort and our goals are both performance and",
    "start": "423560",
    "end": "429960"
  },
  {
    "text": "isolation and we are doing this by introducing a highly configurable uh resource policy for",
    "start": "429960",
    "end": "437879"
  },
  {
    "text": "assigning CPUs and containers and this talk is addressing",
    "start": "437879",
    "end": "445840"
  },
  {
    "text": "these well-known problems like data locality Noisy Neighbor device locality",
    "start": "445840",
    "end": "451319"
  },
  {
    "text": "CPU attributes so we we are going to allow also CPU frequency configurations",
    "start": "451319",
    "end": "456479"
  },
  {
    "text": "for containers and there's a lot of room and",
    "start": "456479",
    "end": "461720"
  },
  {
    "text": "requests actually also for all sorts of application specific",
    "start": "461720",
    "end": "467039"
  },
  {
    "text": "tweaking to mention one there was someone who wanted to run uh virtual",
    "start": "467039",
    "end": "472639"
  },
  {
    "text": "machines in their containers and each container would have like four",
    "start": "472639",
    "end": "478440"
  },
  {
    "text": "CPUs sorry yeah yeah we have four CPUs but also so that we could run more",
    "start": "478440",
    "end": "486400"
  },
  {
    "text": "virtual machines if we took these four CPU sets and run actually two containers",
    "start": "486400",
    "end": "491560"
  },
  {
    "text": "on each set so share each CP four CPUs with two containers so this is the",
    "start": "491560",
    "end": "499039"
  },
  {
    "text": "beyond the default and now Dixie let's start from what is the default and how",
    "start": "499039",
    "end": "505159"
  },
  {
    "text": "we could get here thank you thing um I'm going to",
    "start": "505159",
    "end": "512599"
  },
  {
    "text": "talk about what are the different things that we use to Achieve Beyond default",
    "start": "512599",
    "end": "517880"
  },
  {
    "text": "and also how does it compare with the default so if you have a port spec yaml",
    "start": "517880",
    "end": "523399"
  },
  {
    "text": "today and you uh apply that uh using Cube cutle what control pin would do is",
    "start": "523399",
    "end": "529440"
  },
  {
    "text": "it determines that there is a need to create some containers and then the",
    "start": "529440",
    "end": "534560"
  },
  {
    "text": "scheduler would find the appropriate nodes that specify uh that U have the capacity it to run uh your workload and",
    "start": "534560",
    "end": "542040"
  },
  {
    "text": "after that the cuet is in charge of um making sure that the containers are created cuet would uh take the resource",
    "start": "542040",
    "end": "550519"
  },
  {
    "text": "request and limits from your uh Port spec and it would uh communicate with",
    "start": "550519",
    "end": "556120"
  },
  {
    "text": "the container runtime using the grpc protocol that's specified in the container runtime interface and from",
    "start": "556120",
    "end": "563440"
  },
  {
    "text": "there uh the uh resources are translated into the oci uh spec and oci Linux",
    "start": "563440",
    "end": "571680"
  },
  {
    "text": "resources spec and it is forwarded to the runy which is responsible for eventually creating the containers and",
    "start": "571680",
    "end": "578720"
  },
  {
    "text": "writing um writing the resource spec and mapping those to the various cgroup uh",
    "start": "578720",
    "end": "585120"
  },
  {
    "text": "files so this is uh the bpc that we used um that that that's the default for uh",
    "start": "585120",
    "end": "591920"
  },
  {
    "text": "our use case now um for us what we wanted to do was we wanted to have a",
    "start": "591920",
    "end": "598000"
  },
  {
    "text": "more granular control over the CPUs that would run our workloads and today uh",
    "start": "598000",
    "end": "604120"
  },
  {
    "text": "kubernetes does have CPU manager but the policies different policies that would",
    "start": "604120",
    "end": "609839"
  },
  {
    "text": "help us uh manage the CPU sets allocation for our workload at a more granular level are kind of work in",
    "start": "609839",
    "end": "616560"
  },
  {
    "text": "progress right now so we used uh NRI plugins to have",
    "start": "616560",
    "end": "623120"
  },
  {
    "text": "something which is beyond default NRI plug-in uh sit in the container AC along",
    "start": "623120",
    "end": "630279"
  },
  {
    "text": "with the container uh runtime layer it would intercept the container uh life cycle events and make adjustments to the",
    "start": "630279",
    "end": "638639"
  },
  {
    "text": "resource resource policy and enable you to uh specify the custom way to allocate",
    "start": "638639",
    "end": "645279"
  },
  {
    "text": "the CPU resources this is what um the balloon",
    "start": "645279",
    "end": "651000"
  },
  {
    "text": "policy spec looks like for our use case um so if you see there is an option to",
    "start": "651000",
    "end": "657000"
  },
  {
    "text": "specify um the option that says prefer spread on physical course today CPU",
    "start": "657000",
    "end": "663600"
  },
  {
    "text": "manager and kubernetes doesn't have this option it's being worked on and it will be available in 131 but uh as of now",
    "start": "663600",
    "end": "670880"
  },
  {
    "text": "since it's not available we use the balloon policy uh in NRI plug-in and we",
    "start": "670880",
    "end": "676320"
  },
  {
    "text": "have this option to specify uh the CPU set allocation across different coures and not on the same course and we also",
    "start": "676320",
    "end": "684000"
  },
  {
    "text": "specified um prefer new balloons this help us uh achieve some sort of of",
    "start": "684000",
    "end": "689560"
  },
  {
    "text": "isolation since this configuration would create new balloons instead of uh",
    "start": "689560",
    "end": "694920"
  },
  {
    "text": "allocate instead of placing our workloads on the existing uh CPUs and hence we were able to achieve some sort",
    "start": "694920",
    "end": "701279"
  },
  {
    "text": "of uh some level of isolation there using this and then we have name spaces star which would mean that all the",
    "start": "701279",
    "end": "708399"
  },
  {
    "text": "workloads running on the Node would comply with this balloon",
    "start": "708399",
    "end": "713639"
  },
  {
    "text": "policy and um there is also an option to specify annotation",
    "start": "713920",
    "end": "719959"
  },
  {
    "text": "uh at the Port spec level in case you would want that for your use case but for our use case at least we were we are",
    "start": "719959",
    "end": "727040"
  },
  {
    "text": "running all the workloads that comply with this balloon",
    "start": "727040",
    "end": "731639"
  },
  {
    "text": "policy um let's walk over uh how are we",
    "start": "732600",
    "end": "737639"
  },
  {
    "text": "uh doing the installation of the NRI plugin so the First Command would add the NRI plug-in repository the second",
    "start": "737639",
    "end": "744440"
  },
  {
    "text": "one is the helm install it would um install the NRI balloon Damon",
    "start": "744440",
    "end": "749880"
  },
  {
    "text": "and the crd and it would also um take care of whatever is required for the uh",
    "start": "749880",
    "end": "755920"
  },
  {
    "text": "balloon policy plug-in and with the patch runtime config true it would enable the functionality at at the",
    "start": "755920",
    "end": "762959"
  },
  {
    "text": "container runtime level uh this functionality is default enabled uh on",
    "start": "762959",
    "end": "768240"
  },
  {
    "text": "container D 2.0 and further and it is available on uh",
    "start": "768240",
    "end": "773399"
  },
  {
    "text": "1.7 U but it's enabled only in 2.0 and for the default balloon policy",
    "start": "773399",
    "end": "780160"
  },
  {
    "text": "plugin you can actually live tune it and edit it as per your needs we use Cube cutle edit to uh edit the balloon policy",
    "start": "780160",
    "end": "787760"
  },
  {
    "text": "for our need and so what we are trying to",
    "start": "787760",
    "end": "793320"
  },
  {
    "text": "address is what's the need for uh actually um actually uh granularly uh",
    "start": "793320",
    "end": "801240"
  },
  {
    "text": "specifying the CPU sets for the workloads so there are different workloads that have different use cases",
    "start": "801240",
    "end": "807760"
  },
  {
    "text": "and they might want to run um a user might want to run a particular set of workloads on particular CPUs while the",
    "start": "807760",
    "end": "814720"
  },
  {
    "text": "other set of workloads on different CPUs without any interference across each others so we do not have a lot of uh",
    "start": "814720",
    "end": "822199"
  },
  {
    "text": "different policies in the CPU manager today and N plugin solves that problem and I will uh walk over the next",
    "start": "822199",
    "end": "831320"
  },
  {
    "text": "the the details about the balloon policy plugin and deep dive into it",
    "start": "831320",
    "end": "836519"
  },
  {
    "text": "thanks so here what happens once you have installed B N Balance",
    "start": "836519",
    "end": "844600"
  },
  {
    "text": "policy the policy a demon set of course it starts a p on every every note in",
    "start": "844600",
    "end": "851480"
  },
  {
    "text": "your cluster and each of these pods then uh registers to The Container run time",
    "start": "851480",
    "end": "858680"
  },
  {
    "text": "so cryo and container D have this NRI server there so in the registration it",
    "start": "858680",
    "end": "864160"
  },
  {
    "text": "tells to the runtime that which are the events pot and container life cycle events that is interested in and the",
    "start": "864160",
    "end": "871800"
  },
  {
    "text": "container runtime responds by telling that what are the running containers already so if there are containers on",
    "start": "871800",
    "end": "879360"
  },
  {
    "text": "that node already then this plugin can already start adjusting those without",
    "start": "879360",
    "end": "885360"
  },
  {
    "text": "restarting the containers once uh the new containers",
    "start": "885360",
    "end": "893199"
  },
  {
    "text": "are being scheduled and started on that note then CET is telling to the runtime",
    "start": "893199",
    "end": "900480"
  },
  {
    "text": "that now we are creating a new container runtime is saying and forwarding that event to the N plugin and the plug-in",
    "start": "900480",
    "end": "908800"
  },
  {
    "text": "then does the assignment so in this case when there is like first container",
    "start": "908800",
    "end": "914160"
  },
  {
    "text": "coming in N balloons could create a new balloon which is a new set of CPUs and",
    "start": "914160",
    "end": "922000"
  },
  {
    "text": "assign the new container on this balloons so that this balloon contains in enough CPUs to satisfy the CPU",
    "start": "922000",
    "end": "929560"
  },
  {
    "text": "requests of that container that is coming in and when again new container is being",
    "start": "929560",
    "end": "939160"
  },
  {
    "text": "created in addition to the previous one then the balloons policy can inflate",
    "start": "939160",
    "end": "945399"
  },
  {
    "text": "this Balloon by adding one more CPU there so that it satisfies the CPU",
    "start": "945399",
    "end": "950519"
  },
  {
    "text": "requests of both of these containers and here I want to highlight that we are",
    "start": "950519",
    "end": "957480"
  },
  {
    "text": "actually modifying this CPU set where this already running container is",
    "start": "957480",
    "end": "962600"
  },
  {
    "text": "already uh maybe already running so it gets more CPUs to its use in this case",
    "start": "962600",
    "end": "969079"
  },
  {
    "text": "when we are like inflating balloons but not all workloads are happy",
    "start": "969079",
    "end": "975839"
  },
  {
    "text": "with that so some workloads may want to stick with the CPUs that they F first",
    "start": "975839",
    "end": "981399"
  },
  {
    "text": "see when they are started and we can't actually uh then increase and decrease",
    "start": "981399",
    "end": "987600"
  },
  {
    "text": "the number of CPUs live in that case and to address those",
    "start": "987600",
    "end": "993000"
  },
  {
    "text": "balloons policy also has options for having these kind of fixed set fixed",
    "start": "993000",
    "end": "999639"
  },
  {
    "text": "balloons which have like a static set of CPUs and there can be even balloons that",
    "start": "999639",
    "end": "1006959"
  },
  {
    "text": "are created without any containers being already there but this is important for",
    "start": "1006959",
    "end": "1013279"
  },
  {
    "text": "the cases where you know that this node is going to run some workload some Edge",
    "start": "1013279",
    "end": "1019079"
  },
  {
    "text": "cases for instance have these kind of situations where there is some special Hardware where you want to ensure that",
    "start": "1019079",
    "end": "1026160"
  },
  {
    "text": "you have CPUs reserved for your coming workload so that it can communicate very",
    "start": "1026160",
    "end": "1032839"
  },
  {
    "text": "efficiently with that special Hardware again some more options so",
    "start": "1032839",
    "end": "1040000"
  },
  {
    "text": "balloons also allow doing these kind of things that you have some dedicated CPUs",
    "start": "1040000",
    "end": "1046280"
  },
  {
    "text": "onto balloons but you are also sharing some CPUs that are idle so that those",
    "start": "1046280",
    "end": "1054440"
  },
  {
    "text": "CPUs that do not belong to any balloon are shared with the workloads in these",
    "start": "1054440",
    "end": "1060400"
  },
  {
    "text": "balloons and you can specify the level in the uh topology that where these idle",
    "start": "1060400",
    "end": "1068360"
  },
  {
    "text": "CPUs are shared so you can share idle CPUs in the same Numa node or you can",
    "start": "1068360",
    "end": "1074000"
  },
  {
    "text": "share I CPS on the same D or on the same socket",
    "start": "1074000",
    "end": "1079480"
  },
  {
    "text": "or even the whole system if you like so this this gives a data locality",
    "start": "1079480",
    "end": "1088320"
  },
  {
    "text": "when you specify specify different topology levels there okay so let's a",
    "start": "1088320",
    "end": "1094600"
  },
  {
    "text": "bit compare the default and the be Beyond default cases so in coulet",
    "start": "1094600",
    "end": "1101799"
  },
  {
    "text": "managers they manage currently guaranteed workloads only while our goal",
    "start": "1101799",
    "end": "1108080"
  },
  {
    "text": "with balloons is that you can run um containers of any work any qos class in",
    "start": "1108080",
    "end": "1115760"
  },
  {
    "text": "balloons another point is that managers currently give exclusive set of",
    "start": "1115760",
    "end": "1123120"
  },
  {
    "text": "CPUs uh per a container or you can have a note level switch saying that you want",
    "start": "1123120",
    "end": "1129840"
  },
  {
    "text": "an ex exclusive set of CPUs per a pod uh in balloons we have exclusive and",
    "start": "1129840",
    "end": "1136640"
  },
  {
    "text": "shared CPUs and you can def find any containers which should have which",
    "start": "1136640",
    "end": "1143000"
  },
  {
    "text": "option so for instance you can have like database pots which contain two",
    "start": "1143000",
    "end": "1150240"
  },
  {
    "text": "containers like one database container and one logger container and you can",
    "start": "1150240",
    "end": "1155559"
  },
  {
    "text": "give exclusive set of CPUs for all the database containers and then put all the",
    "start": "1155559",
    "end": "1160760"
  },
  {
    "text": "logger containers from every B to the same set of",
    "start": "1160760",
    "end": "1166000"
  },
  {
    "text": "CPUs again managers provide static CPU sets which do not uh change during the",
    "start": "1166000",
    "end": "1172520"
  },
  {
    "text": "run time which is safe of course uh balloons offers also static and dynamic",
    "start": "1172520",
    "end": "1179000"
  },
  {
    "text": "CPU sets so if you are if you know your workloads that these are just fine to be adjusted when they are running then then",
    "start": "1179000",
    "end": "1186559"
  },
  {
    "text": "fine go with it and you can take advantage of it uh topology levels managers support",
    "start": "1186559",
    "end": "1193679"
  },
  {
    "text": "um Numa nodes up to up to eight Numa nodes then there is some uh algorithmic",
    "start": "1193679",
    "end": "1199799"
  },
  {
    "text": "problems actually which prevent using much more um balloons",
    "start": "1199799",
    "end": "1206039"
  },
  {
    "text": "policy detects like several topology layers we have we know how which course",
    "start": "1206039",
    "end": "1213400"
  },
  {
    "text": "are hyperthreaded we know numar noes we know dice which are basically uh",
    "start": "1213400",
    "end": "1219960"
  },
  {
    "text": "something that could run almost like a as a different CPU socket but they are",
    "start": "1219960",
    "end": "1225760"
  },
  {
    "text": "you can like pack several doys in the same package so it's just a Sub sub",
    "start": "1225760",
    "end": "1230840"
  },
  {
    "text": "package I would say and then full package is of course and you can share",
    "start": "1230840",
    "end": "1235960"
  },
  {
    "text": "as I mentioned you can share idle CPUs for instance on any level that you just Define in the",
    "start": "1235960",
    "end": "1242919"
  },
  {
    "text": "configuration in addition to that we have C CPU frequency controls we are",
    "start": "1242919",
    "end": "1248799"
  },
  {
    "text": "implementing some CPU power controls as well so that you can Define for those CPUs in some balloon that's how the",
    "start": "1248799",
    "end": "1257080"
  },
  {
    "text": "frequency and power should be uh handled there we can do cach level",
    "start": "1257080",
    "end": "1262520"
  },
  {
    "text": "sharing in different levels and we can do live tuning I would also let's jump",
    "start": "1262520",
    "end": "1268919"
  },
  {
    "text": "with the live tuning to another example so now here I'm editing uh Ballo policy",
    "start": "1268919",
    "end": "1277400"
  },
  {
    "text": "default that is the default policy to be used on every node in the cluster but you can actually Define also policies",
    "start": "1277400",
    "end": "1284880"
  },
  {
    "text": "for separate notes so that they have then a bit different balloons",
    "start": "1284880",
    "end": "1290440"
  },
  {
    "text": "configurations um I'm having here a balloon named compress this is my um",
    "start": "1290440",
    "end": "1297440"
  },
  {
    "text": "synthetic synthetic Benchmark which with I just want to demonstrate that how how",
    "start": "1297440",
    "end": "1303520"
  },
  {
    "text": "you can do the configuring live and I'm telling that maximum CPUs per each",
    "start": "1303520",
    "end": "1310799"
  },
  {
    "text": "balloon is three and minimum number of CPUs per each balloon is three that means that whenever a balloon is created",
    "start": "1310799",
    "end": "1317640"
  },
  {
    "text": "it will have three CPUs and no more no less and there's also the option Dixie",
    "start": "1317640",
    "end": "1323520"
  },
  {
    "text": "already mentioned prefer spreading on uh physical course that is when we are",
    "start": "1323520",
    "end": "1328720"
  },
  {
    "text": "allocating these three CPUs for a balloon we are picking up them from",
    "start": "1328720",
    "end": "1334240"
  },
  {
    "text": "different physical cores so that they do not have sipling hyper threats in",
    "start": "1334240",
    "end": "1340799"
  },
  {
    "text": "them um this is actually an option that I'm changing here in this live tuning uh",
    "start": "1340799",
    "end": "1348080"
  },
  {
    "text": "result so here's the Baseline um I'm just reading reading random device then just",
    "start": "1348080",
    "end": "1356120"
  },
  {
    "text": "doing some Bas 64 encoding and uh compressing and doing it again so a",
    "start": "1356120",
    "end": "1363880"
  },
  {
    "text": "synthetic Benchmark as I mentioned and uh Baseline throughput looks like that",
    "start": "1363880",
    "end": "1371240"
  },
  {
    "text": "without balloons policy without any CPU Affinity no limits in in those resources",
    "start": "1371240",
    "end": "1377679"
  },
  {
    "text": "when I apply balloons policy with mean CPUs one then it runs only on single CPU",
    "start": "1377679",
    "end": "1383799"
  },
  {
    "text": "and we get very flat line so that's very predictable performance but the throughput is definitely below what was",
    "start": "1383799",
    "end": "1391480"
  },
  {
    "text": "here in the Baseline when I add a new another CPU to this balloon so change",
    "start": "1391480",
    "end": "1397200"
  },
  {
    "text": "the mean CPUs to two I can immediately see an big in increase in the throughput",
    "start": "1397200",
    "end": "1403360"
  },
  {
    "text": "and again we get the Flatline so very predictable throughput which is actually on a is already above the",
    "start": "1403360",
    "end": "1411080"
  },
  {
    "text": "Baseline again adding one more CPU then we are getting some variation already",
    "start": "1411080",
    "end": "1416320"
  },
  {
    "text": "there I was thinking at that point that do we get this variation because of the uh frequency changes so I changed",
    "start": "1416320",
    "end": "1423840"
  },
  {
    "text": "actually minimum frequency for those CPUs in this balloon to the maximum that",
    "start": "1423840",
    "end": "1429600"
  },
  {
    "text": "was available but it didn't didn't change the situation so the same fluctuation was still there uh so which",
    "start": "1429600",
    "end": "1437720"
  },
  {
    "text": "means that actually the CPU was already topped on those CPU cores that were used",
    "start": "1437720",
    "end": "1444880"
  },
  {
    "text": "uh so I tried out the other option uh changing this prefer spread on physical",
    "start": "1444880",
    "end": "1450720"
  },
  {
    "text": "course and then I was able to see that okay now the now the variation at least",
    "start": "1450720",
    "end": "1455840"
  },
  {
    "text": "got a bit smaller so it's it's more predictable predictable in that case in",
    "start": "1455840",
    "end": "1461600"
  },
  {
    "text": "this case this uh note didn't have much any any workloads to run so this this of",
    "start": "1461600",
    "end": "1469399"
  },
  {
    "text": "is an example of the one of the cases where I mentioned that where this extra performance is coming from so this is",
    "start": "1469399",
    "end": "1475720"
  },
  {
    "text": "actually coming from the fact that uh you get higher CPU frequency when you are when you have few very busy CPU",
    "start": "1475720",
    "end": "1485399"
  },
  {
    "text": "course so Dix would you like to conclude sure so uh again what we are trying to",
    "start": "1485399",
    "end": "1492919"
  },
  {
    "text": "say is uh there can be different use cases uh in which a user would like to",
    "start": "1492919",
    "end": "1498840"
  },
  {
    "text": "like their workloads to be placed say for example on disjoint CPU sets there can be a workload that the user wants to",
    "start": "1498840",
    "end": "1505600"
  },
  {
    "text": "uh place on one CPU set and the other workload should not maybe go on that CPU set for better performance and say there",
    "start": "1505600",
    "end": "1513120"
  },
  {
    "text": "can be a workload that should allocate that should have CPU allocation closer to the network device or the block",
    "start": "1513120",
    "end": "1520039"
  },
  {
    "text": "device so today uh we do not have this level of uh configuration options in",
    "start": "1520039",
    "end": "1525760"
  },
  {
    "text": "kubernetes some of them are being worked on but uh we use NRI plugin which is",
    "start": "1525760",
    "end": "1531440"
  },
  {
    "text": "able to provide us these options and we achieve some sort of uh isolation as well and mitigated some problems that we",
    "start": "1531440",
    "end": "1538799"
  },
  {
    "text": "have around Noisy Neighbor and NRI plug-in is available in container D 1.7",
    "start": "1538799",
    "end": "1545120"
  },
  {
    "text": "it's enabled by default in 2.0 and uh in cryo it's available in 1.26 if you would",
    "start": "1545120",
    "end": "1551919"
  },
  {
    "text": "like to try it out feel free to next slide please we also have attached the yaml",
    "start": "1551919",
    "end": "1559120"
  },
  {
    "text": "files that we used for you to try at home in case you would like and the most important thing if you",
    "start": "1559120",
    "end": "1567360"
  },
  {
    "text": "would like to contribute to NRI or different resource policies or kubera signote we have added the links here the",
    "start": "1567360",
    "end": "1574080"
  },
  {
    "text": "first two are for the NRI uh I have also added a couple of more links which I will upload later so uh there are a",
    "start": "1574080",
    "end": "1581760"
  },
  {
    "text": "bunch of open issues in kubernetes one is around adding a new CPU manager",
    "start": "1581760",
    "end": "1587440"
  },
  {
    "text": "static policy that will help you to place your workloads across different CPU CES there is another one which kind",
    "start": "1587440",
    "end": "1594039"
  },
  {
    "text": "of tries to address uh adding CPU policies that will enable you to configure different cgroup options uh",
    "start": "1594039",
    "end": "1601039"
  },
  {
    "text": "such that you can have different uh require different requirements satisfied for CPU allocation and if you would like",
    "start": "1601039",
    "end": "1608039"
  },
  {
    "text": "to learn more about what we are doing in Sig node please attend the maintainance track on",
    "start": "1608039",
    "end": "1613240"
  },
  {
    "text": "Friday and here is the QR code for feedback and we are open open for",
    "start": "1613240",
    "end": "1620280"
  },
  {
    "text": "[Applause]",
    "start": "1620280",
    "end": "1627869"
  },
  {
    "text": "questions thanks for great performance um ex imagine that you have a lot of",
    "start": "1637520",
    "end": "1644240"
  },
  {
    "text": "different deployments thousands of deployments and you have hundreds of of kubernetes nodes and now you have to",
    "start": "1644240",
    "end": "1650919"
  },
  {
    "text": "decide how to split them into balloons how to choose the",
    "start": "1650919",
    "end": "1656520"
  },
  {
    "text": "optimal sizes of balloons how to understand that in such an Dynamic environment where you can have a lot of",
    "start": "1656520",
    "end": "1663720"
  },
  {
    "text": "different deployments yeah that's a very good question um I think that uh eventually",
    "start": "1663720",
    "end": "1673519"
  },
  {
    "text": "it boils down to measuring so if you want to know know how to really optimize",
    "start": "1673519",
    "end": "1679799"
  },
  {
    "text": "how to squeeze out everything out of your node you need to try it out I I",
    "start": "1679799",
    "end": "1685320"
  },
  {
    "text": "can't tell you that what what would be the magic solution for there there are",
    "start": "1685320",
    "end": "1690360"
  },
  {
    "text": "good defaults so we we can provide you with uh different policies actually so",
    "start": "1690360",
    "end": "1696240"
  },
  {
    "text": "we are presenting here only NRI balloons policy but there is also available NRI",
    "start": "1696240",
    "end": "1703320"
  },
  {
    "text": "topology aware policy which is like a zero configuration policy here in the",
    "start": "1703320",
    "end": "1709440"
  },
  {
    "text": "balloons policy you are defining different balloon types CPU settings for",
    "start": "1709440",
    "end": "1714559"
  },
  {
    "text": "each so it's a lot of configuration options which really are help you in squeezing out the every single bit of",
    "start": "1714559",
    "end": "1720760"
  },
  {
    "text": "the performance but with the topology aware policy you just deploy the policy and enjoy the the top the awareness so",
    "start": "1720760",
    "end": "1729200"
  },
  {
    "text": "you will get like Numa node alignment uh and lots of things for free without",
    "start": "1729200",
    "end": "1734720"
  },
  {
    "text": "without bothering with the configuration",
    "start": "1734720",
    "end": "1739919"
  },
  {
    "text": "thanks uh hi very good presentation it's very flexible seems uh when you",
    "start": "1739919",
    "end": "1745640"
  },
  {
    "text": "mentioned this topology awareness is it count device Numa topology awareness if",
    "start": "1745640",
    "end": "1752000"
  },
  {
    "text": "I want to schedule it on the same Numa node when my Nick is taking injest for",
    "start": "1752000",
    "end": "1757519"
  },
  {
    "text": "example uh these policies do not really",
    "start": "1757519",
    "end": "1762760"
  },
  {
    "text": "uh affect the kuber scheduler so there is a gap there just like with the",
    "start": "1762760",
    "end": "1768120"
  },
  {
    "text": "default policies currently as well so it might be that you are not getting",
    "start": "1768120",
    "end": "1774799"
  },
  {
    "text": "exactly what you wish and then then your Port might be like failed to run so",
    "start": "1774799",
    "end": "1783159"
  },
  {
    "text": "that's that's a loss option like if you say that my pot should be run in this and this balloon but that balloon is",
    "start": "1783159",
    "end": "1789480"
  },
  {
    "text": "can't be created anymore on the Node then the the shadling will fail running it will fail okay but in",
    "start": "1789480",
    "end": "1796559"
  },
  {
    "text": "principle the topology manager will do something and then in the right plugin",
    "start": "1796559",
    "end": "1801960"
  },
  {
    "text": "either perform it or fail uh are you talking about a cuy manager apology",
    "start": "1801960",
    "end": "1808440"
  },
  {
    "text": "manager of a cube enforce node okay sorry sorry um I would say that do not",
    "start": "1808440",
    "end": "1815039"
  },
  {
    "text": "use if you are using NRI uh topologies sorry NRI resource policies",
    "start": "1815039",
    "end": "1821640"
  },
  {
    "text": "like topology manager n or or balloons you should not use CBE PL topology",
    "start": "1821640",
    "end": "1828960"
  },
  {
    "text": "manager no CET CPU manager or memory manager at all so just switch those off",
    "start": "1828960",
    "end": "1835039"
  },
  {
    "text": "and because it it would be otherwise just a waste of time okay they their",
    "start": "1835039",
    "end": "1840120"
  },
  {
    "text": "suggestions are completely like thrown away by these n policies okay I see uh",
    "start": "1840120",
    "end": "1847600"
  },
  {
    "text": "one short another question can you exclude some CPUs from scheduling um you can ex you can",
    "start": "1847600",
    "end": "1856000"
  },
  {
    "text": "annotate your Port saying that do not touch the CPU sets or memories that this",
    "start": "1856000",
    "end": "1863200"
  },
  {
    "text": "is uh using so this is again great question thanks uh there are cases where",
    "start": "1863200",
    "end": "1869840"
  },
  {
    "text": "your workload might need access to every single CPU core that is available in the",
    "start": "1869840",
    "end": "1875440"
  },
  {
    "text": "system so for instance when you are taking measures and accessing like CPU counters Hardware counters on each CPU",
    "start": "1875440",
    "end": "1882960"
  },
  {
    "text": "you can't do that without having access to all of those and with this kind of special case you can say that this",
    "start": "1882960",
    "end": "1890039"
  },
  {
    "text": "workload is special don't put it into any balloon just let it run everywhere",
    "start": "1890039",
    "end": "1897080"
  },
  {
    "text": "than um I would like to add a bit more here so we were having a discussion as",
    "start": "1897080",
    "end": "1902679"
  },
  {
    "text": "to what would happen if a pod is already scheduled uh on a node and then uh the",
    "start": "1902679",
    "end": "1909799"
  },
  {
    "text": "the balloon policy plug-in tries to inflate or deflate CPUs and those CPUs",
    "start": "1909799",
    "end": "1914880"
  },
  {
    "text": "are not available on the Node so again I would say the balloon policy uh plugin",
    "start": "1914880",
    "end": "1920600"
  },
  {
    "text": "would fail here so there is an ongoing discussion about whether uh these things should be at the cuet level so that uh",
    "start": "1920600",
    "end": "1927639"
  },
  {
    "text": "the scheduler is aware of uh what resources are attached to the node and then makes the decision there or whether",
    "start": "1927639",
    "end": "1934679"
  },
  {
    "text": "it should be at the container runtime level I have a different opinion about this so we just have uh we were just",
    "start": "1934679",
    "end": "1941559"
  },
  {
    "text": "having this discussion and I just wanted to bring it",
    "start": "1941559",
    "end": "1946240"
  },
  {
    "text": "up yeah let's work together hello I have a couple of",
    "start": "1946840",
    "end": "1953559"
  },
  {
    "text": "questions so um the first one are there any defining workload as you will are",
    "start": "1953559",
    "end": "1959399"
  },
  {
    "text": "there any workloads which you've seen no benefit at all or even degradation with",
    "start": "1959399",
    "end": "1965200"
  },
  {
    "text": "your plugin I haven't seen that before I I mean we have done some experiments",
    "start": "1965200",
    "end": "1971039"
  },
  {
    "text": "with like both synthetic uh CPU some some which are very intensive to CPU",
    "start": "1971039",
    "end": "1976200"
  },
  {
    "text": "some which are memory bound we we have done like benchmarks on inmemory",
    "start": "1976200",
    "end": "1981639"
  },
  {
    "text": "databases and all of them so so actually benefits so far so I haven't seen this",
    "start": "1981639",
    "end": "1988240"
  },
  {
    "text": "kind of case though if if something comes up I'd be very glad to know about",
    "start": "1988240",
    "end": "1993279"
  },
  {
    "text": "it right thank you um my last question is so I see your in in your demo so",
    "start": "1993279",
    "end": "1999760"
  },
  {
    "text": "you're a data scientist you run your um cluster so you can say I want this",
    "start": "1999760",
    "end": "2006880"
  },
  {
    "text": "workload on the you know CES and these on the another so my my use case is I've",
    "start": "2006880",
    "end": "2012399"
  },
  {
    "text": "got a cluster and 100 teams and I have no real control over that so I can't",
    "start": "2012399",
    "end": "2017840"
  },
  {
    "text": "create static policies per se is there any plans to for example just create",
    "start": "2017840",
    "end": "2022880"
  },
  {
    "text": "lots of balloons and then have something scheduled like this balloon has less load so let's move some containers",
    "start": "2022880",
    "end": "2030360"
  },
  {
    "text": "over actually yeah we have discussed and we have such plans to like sort of",
    "start": "2030360",
    "end": "2037200"
  },
  {
    "text": "migration of workloads between balloons and also the other way around so exchanging CPUs if some balloon is",
    "start": "2037200",
    "end": "2043720"
  },
  {
    "text": "getting very loaded on CPUs and then other is quite idle then why not move",
    "start": "2043720",
    "end": "2048960"
  },
  {
    "text": "CPU to between ball so these kind of considerations have been done they are not implemented yet though okay do you",
    "start": "2048960",
    "end": "2056000"
  },
  {
    "text": "have any aspiration or date for that or yeah",
    "start": "2056000",
    "end": "2061839"
  },
  {
    "text": "not okay thank you",
    "start": "2061839",
    "end": "2069720"
  },
  {
    "text": "hello um Intel has the new processor with three uh core",
    "start": "2070200",
    "end": "2076679"
  },
  {
    "text": "types um how is that going to work you will get uh options for",
    "start": "2076679",
    "end": "2085398"
  },
  {
    "text": "selecting this kind of different core types but so not not there yet but okay",
    "start": "2085399",
    "end": "2092079"
  },
  {
    "text": "coming thank you",
    "start": "2092079",
    "end": "2099640"
  },
  {
    "text": "hey thanks so I have two quick questions I mean one is not quick one is more",
    "start": "2100640",
    "end": "2106400"
  },
  {
    "text": "quick so how is the discoverability of what balloons is the part attached to",
    "start": "2106400",
    "end": "2112280"
  },
  {
    "text": "for example is it something that you can see in a status field or like what kind",
    "start": "2112280",
    "end": "2117560"
  },
  {
    "text": "of pods are assigned to this balloon or great question so for this kind of",
    "start": "2117560",
    "end": "2125200"
  },
  {
    "text": "it's sort of debuging purposes current ly we are providing metric interface so",
    "start": "2125200",
    "end": "2132040"
  },
  {
    "text": "that can be enabled in Ballo configuration that you can get like Prometheus metric interface where you",
    "start": "2132040",
    "end": "2138160"
  },
  {
    "text": "can see that which containers run in which balloons which CPUs are assigned to which balloons and which like extra",
    "start": "2138160",
    "end": "2146599"
  },
  {
    "text": "idle CPUs might be allowed to be used by the containers in a balloon if if this",
    "start": "2146599",
    "end": "2152760"
  },
  {
    "text": "kind of share I sorry idle CPU sharing is enabled so",
    "start": "2152760",
    "end": "2158560"
  },
  {
    "text": "it's a we we do it with we do it with curl currently on the Node that is",
    "start": "2158560",
    "end": "2164960"
  },
  {
    "text": "running this bance policy so if if you have some good ideas that how this could be exposed I would",
    "start": "2164960",
    "end": "2171640"
  },
  {
    "text": "be also very glad to get those ideas even like a GitHub issues or something like that yeah definitely I mean one",
    "start": "2171640",
    "end": "2178599"
  },
  {
    "text": "thing could be status fied on either the balloon or I mean the balloon policy would be easier but yeah we can talk",
    "start": "2178599",
    "end": "2185079"
  },
  {
    "text": "about it later yeah um the next question is sort of like more General how do you say this coexist with Dr right because",
    "start": "2185079",
    "end": "2192079"
  },
  {
    "text": "it feels somehow like a little bit similar like right we have some um",
    "start": "2192079",
    "end": "2197640"
  },
  {
    "text": "Dynamic resource allocation in this case CPU right uh sorry how how this cooperates",
    "start": "2197640",
    "end": "2205440"
  },
  {
    "text": "with what did you mean Dr ah you mentioned you also have some",
    "start": "2205440",
    "end": "2212480"
  },
  {
    "text": "gaps right in schedu and everything I think in some it's some areas the are",
    "start": "2212480",
    "end": "2218119"
  },
  {
    "text": "trying to to solve yeah I think that these are quite different topics Sasha you see you look",
    "start": "2218119",
    "end": "2226000"
  },
  {
    "text": "like you want to comment something into this so maybe I give mic to you uh yes",
    "start": "2226000",
    "end": "2233280"
  },
  {
    "text": "so simple answer it's not anyhow connected with",
    "start": "2233280",
    "end": "2239119"
  },
  {
    "text": "di a bit more complex answer is what Dr",
    "start": "2239119",
    "end": "2244240"
  },
  {
    "text": "is handling the allocation of the device Dev so for example like GPU so Dr says",
    "start": "2244240",
    "end": "2251119"
  },
  {
    "text": "use this particular GPU 1 and gpu2 N starts when the uh container is",
    "start": "2251119",
    "end": "2258920"
  },
  {
    "text": "created and at that stage the device is already selected so what we can do on an",
    "start": "2258920",
    "end": "2265200"
  },
  {
    "text": "level is what we can see what kind of devices are used and when we can find",
    "start": "2265200",
    "end": "2270880"
  },
  {
    "text": "the needed pair of CPU cores or memory regions which is closed to that device",
    "start": "2270880",
    "end": "2277560"
  },
  {
    "text": "so it happens practically like behind the scenes and that's actually answer to",
    "start": "2277560",
    "end": "2282960"
  },
  {
    "text": "the previous question about the topology manager so topolog manager tries to dictate what CPU to use what memory",
    "start": "2282960",
    "end": "2290079"
  },
  {
    "text": "regions to use and so on here is the opposite so you first select what kind",
    "start": "2290079",
    "end": "2295760"
  },
  {
    "text": "of device you are trying to use and we will find the memory and CPU regions to",
    "start": "2295760",
    "end": "2302040"
  },
  {
    "text": "accommodate the best that accelerator device so that's the connection between",
    "start": "2302040",
    "end": "2308560"
  },
  {
    "text": "those two things at the moment okay now it looks we are running",
    "start": "2308560",
    "end": "2315560"
  },
  {
    "text": "out of time so I time to thank you for your attention great questions thank you",
    "start": "2315560",
    "end": "2321839"
  },
  {
    "text": "thank",
    "start": "2321839",
    "end": "2324040"
  },
  {
    "text": "you",
    "start": "2327520",
    "end": "2330520"
  }
]