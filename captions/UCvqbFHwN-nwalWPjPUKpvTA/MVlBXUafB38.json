[
  {
    "text": "all right welcome everybody i think let's give it a few more minutes and",
    "start": "293520",
    "end": "298880"
  },
  {
    "text": "let other people join",
    "start": "298880",
    "end": "301919"
  },
  {
    "text": "how's it going eric i see you on the call okay i finally figured out how to",
    "start": "304560",
    "end": "310080"
  },
  {
    "text": "make make it into this particular call i'm good how are you good good",
    "start": "310080",
    "end": "325840"
  },
  {
    "text": "how's everybody doing everybody else hope you guys are doing great",
    "start": "333520",
    "end": "338880"
  },
  {
    "text": "still at home nothing changing",
    "start": "338880",
    "end": "343039"
  },
  {
    "text": "staying safe i'm trying to stay sane yeah i'm pretty sure i'm gonna volunteer",
    "start": "345840",
    "end": "353680"
  },
  {
    "text": "myself to as soon as possible just work from anywhere above my house but",
    "start": "353680",
    "end": "359280"
  },
  {
    "text": "safety first we'll see yeah california has actually gone into",
    "start": "359280",
    "end": "366560"
  },
  {
    "text": "lockdown phase two so it's just it's kind of crazy",
    "start": "366560",
    "end": "374880"
  },
  {
    "text": "something like most companies that will say that you can work from home until the end of the year",
    "start": "375199",
    "end": "382479"
  },
  {
    "text": "hey michael hey how you doing good",
    "start": "393039",
    "end": "400319"
  },
  {
    "text": "where do you live uh i'm in indiana oh",
    "start": "400319",
    "end": "407280"
  },
  {
    "text": "how's the luck down there uh it's not too bad because i'm not in a heavily",
    "start": "407280",
    "end": "413680"
  },
  {
    "text": "populated area but still staying safe and staying away from things as much as",
    "start": "413680",
    "end": "419520"
  },
  {
    "text": "possible yeah are you close to a metro area",
    "start": "419520",
    "end": "425680"
  },
  {
    "text": "uh not really about an hour away from indianapolis which is okay yeah",
    "start": "425680",
    "end": "432800"
  },
  {
    "text": "all right cool i think we have enough people so yeah uh welcome everybody so we have you michael today so uh thank",
    "start": "433280",
    "end": "440560"
  },
  {
    "text": "you for deciding to present uh so i guess you're gonna be talking about",
    "start": "440560",
    "end": "446639"
  },
  {
    "text": "the resource interface that's something you've been working on i guess in the last few months",
    "start": "446639",
    "end": "453680"
  },
  {
    "text": "so yeah we're happy to hear about it and because we want to learn more and and hopefully we can make this",
    "start": "453680",
    "end": "458880"
  },
  {
    "text": "interactive too so yeah everybody can just jump in and ask questions",
    "start": "458880",
    "end": "465120"
  },
  {
    "text": "yeah sounds good i'll go ahead and share my screen here",
    "start": "465120",
    "end": "479840"
  },
  {
    "text": "all right i have to rejoin to share my screen sorry no worries sorry i haven't used",
    "start": "486639",
    "end": "493840"
  },
  {
    "text": "default mac permissions i think yeah i'm not sure",
    "start": "496960",
    "end": "504400"
  },
  {
    "text": "we're waiting for him i see um zoom released uh dedicated standalone screen thing for",
    "start": "504400",
    "end": "511680"
  },
  {
    "text": "zoom calls recently uh the facebook one seems to work really well uh the only problem is at the moment it",
    "start": "511680",
    "end": "517680"
  },
  {
    "text": "doesn't do zoom which is a pity",
    "start": "517680",
    "end": "521839"
  },
  {
    "text": "okay i think we should be good now",
    "start": "528320",
    "end": "533360"
  },
  {
    "text": "are you able to see the slide deck yep okay so this is the first time i've",
    "start": "533360",
    "end": "541360"
  },
  {
    "text": "presented this to anyone so you all are the the first stop in showing what i've been",
    "start": "541360",
    "end": "548399"
  },
  {
    "text": "working on and so right now it's called the node resource",
    "start": "548399",
    "end": "554800"
  },
  {
    "text": "interface kind of the extensible way to manage resources for containers",
    "start": "554800",
    "end": "561200"
  },
  {
    "text": "so the entire problem is that we have resource management and we're",
    "start": "561200",
    "end": "567920"
  },
  {
    "text": "specifically talking about kind of c groups and topology on the system",
    "start": "567920",
    "end": "573760"
  },
  {
    "text": "we have different workload requirements you have batch workloads latency sensitive workloads",
    "start": "573760",
    "end": "580000"
  },
  {
    "text": "customers have their own slas and slos or you have different classes of",
    "start": "580000",
    "end": "586080"
  },
  {
    "text": "workloads like p1 critical it always needs to run and then",
    "start": "586080",
    "end": "591120"
  },
  {
    "text": "all the way down to where batch would probably be classified as like v3 before things like",
    "start": "591120",
    "end": "597680"
  },
  {
    "text": "that so when you think about resource management with containers",
    "start": "597680",
    "end": "604880"
  },
  {
    "text": "cpus are a big one today especially when you're wanting to run multi-tenant workloads on the",
    "start": "604880",
    "end": "610720"
  },
  {
    "text": "same system with batch and latency sensitive web services things like that you want to be able to schedule",
    "start": "610720",
    "end": "616959"
  },
  {
    "text": "containers on specific cores whether they have entire cords or running on hyper threads",
    "start": "616959",
    "end": "623760"
  },
  {
    "text": "and then we also have pneuma allocations where you need to have your workload running",
    "start": "623760",
    "end": "631200"
  },
  {
    "text": "on a particular socket because that's where other devices are attached and so on",
    "start": "631200",
    "end": "636640"
  },
  {
    "text": "and then also we can dig down even lower than like newman cpu on to",
    "start": "636640",
    "end": "642480"
  },
  {
    "text": "like l3 cache and then huge pages and then like i said before kind of",
    "start": "642480",
    "end": "650160"
  },
  {
    "text": "proximity um what socket is my gpu connected on what sockets are",
    "start": "650160",
    "end": "656320"
  },
  {
    "text": "my network cards on uh in large deployments it's kind of uh",
    "start": "656320",
    "end": "663760"
  },
  {
    "text": "things you you have to think about at the end of the day so this creates a large matrix there's a",
    "start": "663760",
    "end": "670160"
  },
  {
    "text": "lot of things to consider and there's some current solutions so cubelet today has the",
    "start": "670160",
    "end": "676399"
  },
  {
    "text": "cpu manager and there's a few caps that are already outstanding with the community",
    "start": "676399",
    "end": "683600"
  },
  {
    "text": "on proposing like how do we improve improve this how do we start adding",
    "start": "683600",
    "end": "688880"
  },
  {
    "text": "pneuma support to this but when i was researching this there's a lot of",
    "start": "688880",
    "end": "694560"
  },
  {
    "text": "weird us so like you're a guaranteed pod and your requests equal your limits then",
    "start": "694560",
    "end": "699680"
  },
  {
    "text": "you get cpu sets and you're scheduled on dedicated cores and i i don't think",
    "start": "699680",
    "end": "705440"
  },
  {
    "text": "that's very uh friendly a very friendly ux it's kind of uh hidden away and you have",
    "start": "705440",
    "end": "713279"
  },
  {
    "text": "to know the right knobs to turn and things like that and it's off by default and then there's topology",
    "start": "713279",
    "end": "718800"
  },
  {
    "text": "manager which basically only the cpu manager and device manager take advantage of",
    "start": "718800",
    "end": "724880"
  },
  {
    "text": "and that's provided by a hint providers",
    "start": "724880",
    "end": "730720"
  },
  {
    "text": "so there's another solution from intel they have the intel cpu manager for cube there's a",
    "start": "730720",
    "end": "737839"
  },
  {
    "text": "cli tool called cmk which does kind of the low-level allocating pools um and then",
    "start": "737839",
    "end": "745839"
  },
  {
    "text": "placing workloads within those pools depending on different things and and picking what cpus go into whatpool",
    "start": "745839",
    "end": "752800"
  },
  {
    "text": "and then they have the cri resource manager that builds on top of cmk for use within cube",
    "start": "752800",
    "end": "759680"
  },
  {
    "text": "and uh one thing with that is they have to hijack the entire cri",
    "start": "759680",
    "end": "766000"
  },
  {
    "text": "socket and they also have some api extensions for this",
    "start": "766000",
    "end": "772079"
  },
  {
    "text": "and the cri interface is kind of a really big interface",
    "start": "772079",
    "end": "777519"
  },
  {
    "text": "whenever you only want to deal with like scheduling containers on specific cores so",
    "start": "777519",
    "end": "786000"
  },
  {
    "text": "overall qs qos is hard there's lots of users putting on your scale it's hard to",
    "start": "786000",
    "end": "794560"
  },
  {
    "text": "solve this for everyone so my kind of proposal from the beginning",
    "start": "794560",
    "end": "800800"
  },
  {
    "text": "is with the cubelet having cpu manager and these hard these implementations",
    "start": "800800",
    "end": "806639"
  },
  {
    "text": "within the cubelet cores we need to focus on apis and not implementations",
    "start": "806639",
    "end": "812079"
  },
  {
    "text": "because it's very hard to solve a problem like this that's very dependent on scale and what",
    "start": "812079",
    "end": "818720"
  },
  {
    "text": "devices are on machine for everyone and so it's better to focus on the api",
    "start": "818720",
    "end": "824480"
  },
  {
    "text": "is to enable people to to build their own solutions",
    "start": "824480",
    "end": "830320"
  },
  {
    "text": "so if we look at kind of existing interfaces we have cni the container network interface and",
    "start": "830399",
    "end": "839199"
  },
  {
    "text": "this is something that really stands out to me within the container community i think it's simple elegant it's",
    "start": "839199",
    "end": "845519"
  },
  {
    "text": "extensible you can compose various plugins together and there's no controversy that i've seen within the",
    "start": "845519",
    "end": "853600"
  },
  {
    "text": "design of cni and it's basically we've all accepted it and um and and use it within",
    "start": "853600",
    "end": "861600"
  },
  {
    "text": "cube and and other container projects so my my proposal is let's make cni for",
    "start": "861600",
    "end": "870560"
  },
  {
    "text": "resources right i like this api everyone that worked on cni should be proud of what",
    "start": "870560",
    "end": "875839"
  },
  {
    "text": "they built and we need to start extending apis like this into other areas for containers",
    "start": "875839",
    "end": "883760"
  },
  {
    "text": "so came up with nri because cri was already taken i'd rather be named",
    "start": "883760",
    "end": "890480"
  },
  {
    "text": "container resource interface but we already have container runtime interface so nri is the best i could come up with",
    "start": "890480",
    "end": "897519"
  },
  {
    "text": "right now and designing this like i don't think cubelet is the right abstraction",
    "start": "897519",
    "end": "904160"
  },
  {
    "text": "for this we have the cubelet and then we have cris and cris are very low level they know",
    "start": "904160",
    "end": "910320"
  },
  {
    "text": "how to interact with the system whether you're on linux or windows",
    "start": "910320",
    "end": "916320"
  },
  {
    "text": "they deal with this but then the lines between cubelet and the cris are starting to get very blurry there's c groups code and",
    "start": "916320",
    "end": "923760"
  },
  {
    "text": "the cubelet it can uh it's hard to tell who's responsible for resource management",
    "start": "923760",
    "end": "930880"
  },
  {
    "text": "right now with cpu manager and topology manager there and at the cri level we have very robust",
    "start": "930880",
    "end": "938639"
  },
  {
    "text": "ways to hook into the actual life cycle of a container on a system so it goes way beyond like oci runtime",
    "start": "938639",
    "end": "946720"
  },
  {
    "text": "hooks where you hook in kind of the recent developments are",
    "start": "946720",
    "end": "953040"
  },
  {
    "text": "we have this crate and start split within oci where create we'll create the namespaces set up c groups",
    "start": "953040",
    "end": "959519"
  },
  {
    "text": "and then we have this pause in between and this is where like cni comes in and where i'm proposing nri comes in",
    "start": "959519",
    "end": "966160"
  },
  {
    "text": "where where it can take what's the existing setup you you can modify the resources add",
    "start": "966160",
    "end": "972320"
  },
  {
    "text": "additional things and then we start the container which is the user's process",
    "start": "972320",
    "end": "979600"
  },
  {
    "text": "so designing this uh kind of taking a lot of inspiration from",
    "start": "979920",
    "end": "985680"
  },
  {
    "text": "cni we have kind of a global systems config and you have a list of these plugins",
    "start": "985680",
    "end": "991279"
  },
  {
    "text": "where you can compose these together chain them plugins can have specific",
    "start": "991279",
    "end": "996720"
  },
  {
    "text": "um configuration so for this confine plugin we have system reserve cores",
    "start": "996720",
    "end": "1002720"
  },
  {
    "text": "where we say when you're dealing with topology and scheduling these workloads on cores",
    "start": "1002720",
    "end": "1007759"
  },
  {
    "text": "i need zero and one to be reserved for the system so don't touch those and you have the rest of it and then",
    "start": "1007759",
    "end": "1017920"
  },
  {
    "text": "kind of enable to build a good ecosystem around this like cni's done you need kind of skeleton",
    "start": "1018480",
    "end": "1024160"
  },
  {
    "text": "code make it easy for people to build these plug-ins and not get in",
    "start": "1024160",
    "end": "1029438"
  },
  {
    "text": "their way so kind of worked on packages for as a plug-in developer i want to develop",
    "start": "1029439",
    "end": "1036000"
  },
  {
    "text": "a plug-in that does x here's here's all the skeleton and boilerplate so you can quickly get up",
    "start": "1036000",
    "end": "1041280"
  },
  {
    "text": "and running and then if we look at the integration in the cri level",
    "start": "1041280",
    "end": "1047918"
  },
  {
    "text": "it's extremely simple because i believe cri is the right place to add things",
    "start": "1047919",
    "end": "1054320"
  },
  {
    "text": "like this so you would go into different life cycles of the container",
    "start": "1054320",
    "end": "1060240"
  },
  {
    "text": "in the create step you would invoke the nri plug-ins at this step at deletion you would invoke the",
    "start": "1060240",
    "end": "1067039"
  },
  {
    "text": "deletion handles and then we start the container so it's very robust we have explicit",
    "start": "1067039",
    "end": "1074480"
  },
  {
    "text": "places to inject these in the life cycle of the container and it's not um it's not bleeding over into other",
    "start": "1074480",
    "end": "1082559"
  },
  {
    "text": "people's functionality in the stack so",
    "start": "1082559",
    "end": "1088240"
  },
  {
    "text": "during this work i worked on this confined plug-in with the k for cube and",
    "start": "1088240",
    "end": "1094480"
  },
  {
    "text": "what it does is dynamic topology management and qos so as pods and workloads are",
    "start": "1094480",
    "end": "1102160"
  },
  {
    "text": "scheduled on the system they're labeled with their qos class and depending on whether they're",
    "start": "1102160",
    "end": "1108320"
  },
  {
    "text": "latency sensitive they get placed on entire cores the way this works is you have kind of",
    "start": "1108320",
    "end": "1114400"
  },
  {
    "text": "the default pool which is where batch workloads go batch contains their cfs",
    "start": "1114400",
    "end": "1121440"
  },
  {
    "text": "quotas as well so they can't use the entire core but if the latency sensitive service comes in",
    "start": "1121440",
    "end": "1127520"
  },
  {
    "text": "we go ahead and clear the cfs quotas on that because they're saying this workload needs two",
    "start": "1127520",
    "end": "1133600"
  },
  {
    "text": "cores they get allocated the entire core and they're able to use all of that and this does it dynamically so as",
    "start": "1133600",
    "end": "1142480"
  },
  {
    "text": "latency sensitive and batch hit the system we keep high utilization",
    "start": "1142480",
    "end": "1147840"
  },
  {
    "text": "because if a latency sensitive application stops those cores can be returned to the batch batch workloads",
    "start": "1147840",
    "end": "1157360"
  },
  {
    "text": "so we kind of build a dynamic node topology it dynamically places workloads on the",
    "start": "1157840",
    "end": "1164799"
  },
  {
    "text": "system based on the qos class we have pneuma support so if your",
    "start": "1164799",
    "end": "1170320"
  },
  {
    "text": "latency sensitive service says i need to be i need to be on a specific numa node or",
    "start": "1170320",
    "end": "1176480"
  },
  {
    "text": "i need to reserve the entire numa node it can do that and it will still steal that node away from the batch",
    "start": "1176480",
    "end": "1184480"
  },
  {
    "text": "workloads or return that whenever that workload's done",
    "start": "1184480",
    "end": "1189840"
  },
  {
    "text": "so kind of with these plug-ins there's no need to wait for longer cube release",
    "start": "1190080",
    "end": "1195840"
  },
  {
    "text": "cycles for updates to cpu manager topology manager",
    "start": "1195840",
    "end": "1201039"
  },
  {
    "text": "you have a community being built up of all these plug-ins with nri you just update them as you need to and",
    "start": "1201039",
    "end": "1208000"
  },
  {
    "text": "you're not tied to a cube release cycle as you would be with cpu manager",
    "start": "1208000",
    "end": "1213280"
  },
  {
    "text": "you can kind of chain all these together you can make plugins that do one thing and do them well",
    "start": "1213280",
    "end": "1218799"
  },
  {
    "text": "and uh it keeps your code simpler and more robust and things we care about at the",
    "start": "1218799",
    "end": "1224640"
  },
  {
    "text": "infrastructure layer and like if if a specific plug-in",
    "start": "1224640",
    "end": "1230000"
  },
  {
    "text": "doesn't work for you then fork it change it make your own or build more plugins for your own needs",
    "start": "1230000",
    "end": "1237440"
  },
  {
    "text": "so kind of on this journey sig run times first stop to present this and start",
    "start": "1238159",
    "end": "1244159"
  },
  {
    "text": "getting some feedback i'll have a formal spec up hopefully today within",
    "start": "1244159",
    "end": "1250799"
  },
  {
    "text": "the container d project because um it's kind of where i have my default",
    "start": "1250799",
    "end": "1256720"
  },
  {
    "text": "implementation and hooks into ctr and the cri",
    "start": "1256720",
    "end": "1262400"
  },
  {
    "text": "for cube in that project and then kind of expand out to different sigs and things",
    "start": "1262400",
    "end": "1268400"
  },
  {
    "text": "like that so that's kind of the high level overview",
    "start": "1268400",
    "end": "1273440"
  },
  {
    "text": "and if you have any questions and stuff we can do that now and i can go back to any",
    "start": "1273440",
    "end": "1278720"
  },
  {
    "text": "slide if you need so with this essentially we",
    "start": "1278720",
    "end": "1286480"
  },
  {
    "text": "replace the topology manager and the cpu manager or so or it will",
    "start": "1286480",
    "end": "1293840"
  },
  {
    "text": "actually work side by side or or it will be something that eventually will become just one",
    "start": "1293840",
    "end": "1302240"
  },
  {
    "text": "uh they they don't work side by side because they would start to",
    "start": "1302840",
    "end": "1309039"
  },
  {
    "text": "conflict with each other or at this stage it would be able to override",
    "start": "1309039",
    "end": "1316400"
  },
  {
    "text": "the cubelet so um it's best you would want to have cpu",
    "start": "1316400",
    "end": "1322159"
  },
  {
    "text": "manager off which right now it's off by default but yeah",
    "start": "1322159",
    "end": "1329280"
  },
  {
    "text": "so it sounds like the idea just to eventually kind of make this the standard later on you know maybe a",
    "start": "1330799",
    "end": "1337679"
  },
  {
    "text": "cpu manager and topology manager can potentially go away you know maybe to improve the",
    "start": "1337679",
    "end": "1344640"
  },
  {
    "text": "experience right the user or they could be broken out in the plug-ins themselves with those specific",
    "start": "1344640",
    "end": "1351280"
  },
  {
    "text": "implementations but the the overall goal is to have an api to develop this stuff and not try to",
    "start": "1351280",
    "end": "1358960"
  },
  {
    "text": "code everything in the cubelet",
    "start": "1358960",
    "end": "1362640"
  },
  {
    "text": "yeah i i've seen the cpu manager the ux is kind of weird too because i mean you",
    "start": "1364640",
    "end": "1370559"
  },
  {
    "text": "specify the requests and the limits need to be the same so that's not really clear i mean",
    "start": "1370559",
    "end": "1378400"
  },
  {
    "text": "and when you specify the limits in the in the pot spec right so i yeah i can see that this i",
    "start": "1378400",
    "end": "1386480"
  },
  {
    "text": "mean the ux well it's worse than that you need that and you need it to be integer",
    "start": "1386480",
    "end": "1391600"
  },
  {
    "text": "and it needs to be a tuesday and then you get a confused yeah yeah yeah so",
    "start": "1391600",
    "end": "1398720"
  },
  {
    "text": "and when i was looking at the intel intel cpu manager work it seems like they could easily",
    "start": "1400640",
    "end": "1408240"
  },
  {
    "text": "plug into this nri interface for the cmk tool and do their existing work and then",
    "start": "1408240",
    "end": "1415120"
  },
  {
    "text": "they wouldn't have to implement the surface area of cr cri to hijack into this they have a very",
    "start": "1415120",
    "end": "1421039"
  },
  {
    "text": "specific um api for doing this so i think",
    "start": "1421039",
    "end": "1426400"
  },
  {
    "text": "i think it aligns well with the work that's being done there i told sasha about this he had a hard",
    "start": "1426400",
    "end": "1433279"
  },
  {
    "text": "conflict this morning so i'll make sure he gets the recording um i think you'd",
    "start": "1433279",
    "end": "1438640"
  },
  {
    "text": "probably be very intrigued by this and maybe has some good feedback",
    "start": "1438640",
    "end": "1445600"
  },
  {
    "text": "yeah he keeps slacking me i showed him some of the pictures from it and he'll reach out",
    "start": "1445600",
    "end": "1452960"
  },
  {
    "text": "all right cool cool yeah is there like a mechanism where like uh you also get feedback from um",
    "start": "1452960",
    "end": "1461600"
  },
  {
    "text": "from the systems right so you it basically uh some monitoring that says okay my i'm",
    "start": "1462320",
    "end": "1468400"
  },
  {
    "text": "kind of full or i'm kind of overloaded can you move this stuff away from me",
    "start": "1468400",
    "end": "1473760"
  },
  {
    "text": "into some other node or something like that uh yeah so my general idea",
    "start": "1473760",
    "end": "1481200"
  },
  {
    "text": "of it was that the cube schedulers will still handle placing workloads on the node and if they decide",
    "start": "1481200",
    "end": "1488559"
  },
  {
    "text": "to over subscribe a node or not that's kind of a high level scheduling decision where",
    "start": "1488559",
    "end": "1495600"
  },
  {
    "text": "at this level it's hard to provide feedback back up the stack like we could always",
    "start": "1495600",
    "end": "1502000"
  },
  {
    "text": "kill a container but then the cubelet wouldn't know why we killed that so i think",
    "start": "1502000",
    "end": "1509039"
  },
  {
    "text": "with different scheduling strategies you may want to over subscribe so",
    "start": "1509039",
    "end": "1516720"
  },
  {
    "text": "i don't think it's too much of a a big deal that we don't have a current method to for the cri to kind of talk",
    "start": "1517760",
    "end": "1525919"
  },
  {
    "text": "back to the cubelet on rejecting workloads or not but it's something i've thought about",
    "start": "1525919",
    "end": "1532400"
  },
  {
    "text": "and needs to look into more yeah and i guess essentially the",
    "start": "1532400",
    "end": "1539840"
  },
  {
    "text": "the scheduler would make those decisions i mean the the scheduler would have to get that",
    "start": "1539840",
    "end": "1546400"
  },
  {
    "text": "information and based on that or you know decide where to move the the pods right yeah ideally the schedule",
    "start": "1546400",
    "end": "1554559"
  },
  {
    "text": "where like we're we're thinking more about type topology and placing on what cores",
    "start": "1554559",
    "end": "1561360"
  },
  {
    "text": "the scheduler still knows we have 24 cores to schedule on so",
    "start": "1561360",
    "end": "1566640"
  },
  {
    "text": "hopefully it wouldn't push too much workload where would be an issue yeah",
    "start": "1566640",
    "end": "1575519"
  },
  {
    "text": "how does this sorry i missed the beginning part this is diane how would this work with special devices",
    "start": "1577840",
    "end": "1583200"
  },
  {
    "text": "like fpgas and you so this would handle all that because there would be a plug-in for each",
    "start": "1583200",
    "end": "1588720"
  },
  {
    "text": "specialized hardware type that the idea uh yeah i am working on a way to",
    "start": "1588720",
    "end": "1597279"
  },
  {
    "text": "chain plug-ins together so they can get feedback within the chain of where they know who executed before them",
    "start": "1597279",
    "end": "1604400"
  },
  {
    "text": "what changes they made but yeah it would be where vendor-specific",
    "start": "1604400",
    "end": "1610320"
  },
  {
    "text": "implementations like plugins can be made for devices that you need to do something",
    "start": "1610320",
    "end": "1615840"
  },
  {
    "text": "more powerful on any what maybe just an example use case",
    "start": "1615840",
    "end": "1621760"
  },
  {
    "text": "for that would be placement based on which device is because we're still looking at more topology",
    "start": "1621760",
    "end": "1628000"
  },
  {
    "text": "they would still have a device plug in everything else you're just looking at where you would place the actual",
    "start": "1628000",
    "end": "1633679"
  },
  {
    "text": "container or the pod processes uh with the context of where your device is",
    "start": "1633679",
    "end": "1640240"
  },
  {
    "text": "and the topology too is that is that right so like it wouldn't be a device plug-in but it",
    "start": "1640240",
    "end": "1645760"
  },
  {
    "text": "would based on using a device you would have have a wiser placement of the workload is is that kind of the",
    "start": "1645760",
    "end": "1653600"
  },
  {
    "text": "idea or expectation yeah the expectation is",
    "start": "1653600",
    "end": "1659200"
  },
  {
    "text": "we're giving you a powerful interface where where you can make those decisions",
    "start": "1659200",
    "end": "1669840"
  },
  {
    "text": "i had to i had a quick question um first of all thanks for a really interesting presentation and you've raised some",
    "start": "1672159",
    "end": "1679200"
  },
  {
    "text": "what i think a very fundamental and very interesting questions um and so first of all i must prefix",
    "start": "1679200",
    "end": "1686240"
  },
  {
    "text": "this by saying i'm not super familiar with cubelet and the various interfaces that are",
    "start": "1686240",
    "end": "1692799"
  },
  {
    "text": "down at that level cni cri and the stuff you guys are working on but but it seems clear to me that",
    "start": "1692799",
    "end": "1700320"
  },
  {
    "text": "a lot of that stuff kind of developed organically and i think if i read between the lines what",
    "start": "1700320",
    "end": "1706080"
  },
  {
    "text": "you're saying like not all of it is ideal if we were to sort of start from with a blank sheet of paper we might",
    "start": "1706080",
    "end": "1711840"
  },
  {
    "text": "architect things differently and i don't mean this to be offensive to anybody who who has been involved in the current",
    "start": "1711840",
    "end": "1717840"
  },
  {
    "text": "architecture but it seems to me like it might be useful to actually",
    "start": "1717840",
    "end": "1723039"
  },
  {
    "text": "um kind of sketch out what we think a sort of reference architecture for",
    "start": "1723039",
    "end": "1728320"
  },
  {
    "text": "these kinds of things might be and you know look at where we came from look at where",
    "start": "1728320",
    "end": "1734880"
  },
  {
    "text": "we'd like to go to and this doesn't necessarily need to be kubernetes specific this can be sort of from a point of view",
    "start": "1734880",
    "end": "1741520"
  },
  {
    "text": "of like if you're going to do container orchestration in a cloud",
    "start": "1741520",
    "end": "1746960"
  },
  {
    "text": "native way you know these are the kinds of things you run into and this is what we think",
    "start": "1746960",
    "end": "1752000"
  },
  {
    "text": "is a good reference architecture and have you know the various different implementations of these things",
    "start": "1752000",
    "end": "1758159"
  },
  {
    "text": "at least have a kind of a guiding light as to what we think a good approach is does",
    "start": "1758159",
    "end": "1764559"
  },
  {
    "text": "that i know it's a little hand wavy and airy fairy but is that something anybody's working on",
    "start": "1764559",
    "end": "1770559"
  },
  {
    "text": "at the moment that we're aware of or or if not is it something that we as a group want to take on as a project",
    "start": "1770559",
    "end": "1781840"
  },
  {
    "text": "yeah i don't i don't know of anything from my research like i think it's a good idea",
    "start": "1782080",
    "end": "1789760"
  },
  {
    "text": "and like within that architecture like my whole thing and let's focus on apis",
    "start": "1789760",
    "end": "1797520"
  },
  {
    "text": "instead of implementations like we you can tell the cubelet",
    "start": "1797520",
    "end": "1803679"
  },
  {
    "text": "grew organically just like docker which is one project i worked on in the past so",
    "start": "1803679",
    "end": "1809360"
  },
  {
    "text": "like you make compromises and you have growing pains things like that",
    "start": "1809360",
    "end": "1814960"
  },
  {
    "text": "but i think interfaces kind of stand the test of time where",
    "start": "1814960",
    "end": "1821600"
  },
  {
    "text": "implementations get refactored and you can always break things out like cpu manager could start in cubelet",
    "start": "1821600",
    "end": "1829120"
  },
  {
    "text": "now and say like actually this needs to be factored out behind an interface",
    "start": "1829120",
    "end": "1837039"
  },
  {
    "text": "yeah that makes sense is is anyone else on the call sort of intimately involved in for example sig",
    "start": "1837039",
    "end": "1842480"
  },
  {
    "text": "node and does anybody know what the status of like coming up with a grand vision for",
    "start": "1842480",
    "end": "1850320"
  },
  {
    "text": "the node interfaces stands because it's the other problem that and this is completely sort",
    "start": "1850320",
    "end": "1855840"
  },
  {
    "text": "of anecdotally from looking from the outside it looks like a lot of these interfaces have developed somewhat in",
    "start": "1855840",
    "end": "1862640"
  },
  {
    "text": "isolation from each other so you know the networking people that cni and the the container",
    "start": "1862640",
    "end": "1868080"
  },
  {
    "text": "people did cri and and it's not clear that that anybody with a holistic vision",
    "start": "1868080",
    "end": "1874000"
  },
  {
    "text": "across all of these things necessarily kind of uh weighed in uh and that that",
    "start": "1874000",
    "end": "1879919"
  },
  {
    "text": "may be wrong but that's the impression i get and i was you know i know dawn very well and i",
    "start": "1879919",
    "end": "1885120"
  },
  {
    "text": "was in the google kubernetes team who sort of started all the stuff great backside so he's a very competent engineers but",
    "start": "1885120",
    "end": "1892720"
  },
  {
    "text": "things happen the way they do organically and maybe it's a good good time to sort of",
    "start": "1892720",
    "end": "1897919"
  },
  {
    "text": "look at that stuff",
    "start": "1897919",
    "end": "1900559"
  },
  {
    "text": "yeah it sounds good to me i mean i mean to the the next question that i have is that is",
    "start": "1904799",
    "end": "1912559"
  },
  {
    "text": "a group of people interested in starting uh sort of like a working group",
    "start": "1912559",
    "end": "1918000"
  },
  {
    "text": "and in address some of these issues right so and and quentin you you brought up the",
    "start": "1918000",
    "end": "1924880"
  },
  {
    "text": "issue of uh maybe bringing up some of this all the interfaces together so",
    "start": "1924880",
    "end": "1930559"
  },
  {
    "text": "or people can have that communication across the different teams right so that",
    "start": "1930559",
    "end": "1936480"
  },
  {
    "text": "would you know so that so they they agree on certain standards um",
    "start": "1936480",
    "end": "1943600"
  },
  {
    "text": "what i've seen in the past is that a lot of the teams actually work on their own like like you mentioned and and you know a",
    "start": "1943600",
    "end": "1949840"
  },
  {
    "text": "lot of these interfaces kind of come up and and they're kind of different in different ways uh",
    "start": "1949840",
    "end": "1956399"
  },
  {
    "text": "in this case uh you're following the cni aspect which is great i think uh you know there are some other cases",
    "start": "1956399",
    "end": "1962480"
  },
  {
    "text": "where i mean some of the other teams are actually working independently and it may not be the best best",
    "start": "1962480",
    "end": "1969360"
  },
  {
    "text": "experience for some of the end users um so yeah so the idea here is just to",
    "start": "1969360",
    "end": "1975200"
  },
  {
    "text": "to to make the exp the user experience um more more uh",
    "start": "1975200",
    "end": "1982240"
  },
  {
    "text": "together and and so that people you know make you know make better decisions on how to",
    "start": "1982240",
    "end": "1988159"
  },
  {
    "text": "want to use the tools and and they become more useful in the end",
    "start": "1988159",
    "end": "1996720"
  },
  {
    "text": "yeah i think before we start any group to do any brainstorming in that regard i would definitely want to",
    "start": "1996720",
    "end": "2002559"
  },
  {
    "text": "catch up with the people i think the de facto place where these conversations happen or did in the past happen anyway was was",
    "start": "2002559",
    "end": "2009760"
  },
  {
    "text": "signode the kubernetes signaled um so we should definitely go and chat to whoever runs",
    "start": "2009760",
    "end": "2015039"
  },
  {
    "text": "the show there and just get a sort of state of play from them i don't want us to create",
    "start": "2015039",
    "end": "2022320"
  },
  {
    "text": "some other you know working group that is now like solving problems that already have been thought about deeply somewhere else",
    "start": "2022320",
    "end": "2028640"
  },
  {
    "text": "uh be very cautious of that we even have a subgroup for i'm sorry uh",
    "start": "2028640",
    "end": "2035679"
  },
  {
    "text": "i thought we already created the working group for a container device interface or is it still in process",
    "start": "2035679",
    "end": "2042000"
  },
  {
    "text": "that's already created but i i don't know if this is exactly within the scope of that uh yeah so",
    "start": "2042000",
    "end": "2048560"
  },
  {
    "text": "there's a resource management work group as well um that because there's a lot of discussion",
    "start": "2048560",
    "end": "2054158"
  },
  {
    "text": "around like crirm and topology manager and things like this because i don't think anybody's happy with it um",
    "start": "2054159",
    "end": "2060320"
  },
  {
    "text": "as is in signod because that was consuming a lot of time they created a different work group specifically focused on resource",
    "start": "2060320",
    "end": "2066960"
  },
  {
    "text": "management um i haven't been attending that just because it's very um eu friendly it's a little bit too early",
    "start": "2066960",
    "end": "2073919"
  },
  {
    "text": "on my side and haven't had a reason but i'd be curious to see if you could talk in signode but it might be a more",
    "start": "2073919",
    "end": "2080398"
  },
  {
    "text": "focused discussion with a bit more um focus opinion in the resource management one as well",
    "start": "2080399",
    "end": "2087440"
  },
  {
    "text": "and if we are to create a working group in us in cncf uh maybe we can we can expand that working",
    "start": "2087440",
    "end": "2094158"
  },
  {
    "text": "group for continued device interface to something more generic that would incorporate more more interfaces not just a specific one",
    "start": "2094159",
    "end": "2102960"
  },
  {
    "text": "yeah yeah that that's uh another option or a feasible option i",
    "start": "2102960",
    "end": "2109520"
  },
  {
    "text": "think but uh what quentin brings up uh also makes sense which is uh you know talk to some of the other",
    "start": "2109520",
    "end": "2116079"
  },
  {
    "text": "teams and sign out and we don't want to kind of you know say that we're going to be working on this and when",
    "start": "2116079",
    "end": "2122400"
  },
  {
    "text": "some other people might be already be talking about doing something similar",
    "start": "2122400",
    "end": "2128839"
  },
  {
    "text": "yeah so i think it's it's some work is uh to be done",
    "start": "2128839",
    "end": "2136079"
  },
  {
    "text": "related to reaching out to some of those groups and if somebody's on the call today that",
    "start": "2136079",
    "end": "2142560"
  },
  {
    "text": "it's in one of those groups like quinton mentioned then they can reach out to some of those uh",
    "start": "2142560",
    "end": "2148240"
  },
  {
    "text": "other working groups yeah i think it needs to be someone with",
    "start": "2148240",
    "end": "2153680"
  },
  {
    "text": "a with a good sound technical understanding of the issues at that level i i don't have that understanding",
    "start": "2153680",
    "end": "2159680"
  },
  {
    "text": "and i would definitely not want to be the person leading this technical kind of coordination role um but but if",
    "start": "2159680",
    "end": "2166720"
  },
  {
    "text": "anyone is on the call or knows of anyone who would be interested in doing that kind of code actually we have",
    "start": "2166720",
    "end": "2172400"
  },
  {
    "text": "some tech leads on this group um diane and others i think that seems to have",
    "start": "2172400",
    "end": "2178800"
  },
  {
    "text": "dropped off but uh but yeah i think it would be very important to get the right person there",
    "start": "2178800",
    "end": "2184000"
  },
  {
    "text": "there's a lot of technical detail but there's also a lot of coordination of different groups there's this sort of area has a",
    "start": "2184000",
    "end": "2191119"
  },
  {
    "text": "history of of becoming politically charged we have networking vendors and you know",
    "start": "2191119",
    "end": "2198160"
  },
  {
    "text": "container vendors and all sorts of people involved so we have to be very careful not to",
    "start": "2198160",
    "end": "2203680"
  },
  {
    "text": "uh kind of create a 17th committee to kind of try and standardize these things",
    "start": "2203680",
    "end": "2208800"
  },
  {
    "text": "if the people involved don't actually want to standardize them the way we do so yeah",
    "start": "2208800",
    "end": "2214960"
  },
  {
    "text": "that's my two cents",
    "start": "2214960",
    "end": "2217920"
  },
  {
    "text": "so anybody on the call would like to take this one",
    "start": "2222000",
    "end": "2228480"
  },
  {
    "text": "i think rather than necessarily um force people to make decisions now let's let's put that",
    "start": "2228480",
    "end": "2234079"
  },
  {
    "text": "on our to-do list of finding the right person for that role and and put a sort of i mean it's going",
    "start": "2234079",
    "end": "2240800"
  },
  {
    "text": "to take a while to get all of this stuff together this is kind of like a multi-quarter project i think and it's going to be",
    "start": "2240800",
    "end": "2247520"
  },
  {
    "text": "very important to start with the right people we've got enough examples of attempts in this direction that haven't gone anywhere so",
    "start": "2247520",
    "end": "2254320"
  },
  {
    "text": "let's put that on our agenda for the next few months to figure out the details and get the right",
    "start": "2254320",
    "end": "2260000"
  },
  {
    "text": "people involved and they may not be in this meeting you know they could be working for companies of other people",
    "start": "2260000",
    "end": "2267599"
  },
  {
    "text": "involved here for example",
    "start": "2267599",
    "end": "2271838"
  },
  {
    "text": "yeah make sense",
    "start": "2273119",
    "end": "2285200"
  },
  {
    "text": "any other comments anything you'd like to talk about uh related to this",
    "start": "2285200",
    "end": "2291838"
  },
  {
    "text": "any other topics that you want to bring up",
    "start": "2293440",
    "end": "2299839"
  },
  {
    "text": "sounds like that's it thanks for a very thought-provoking talk michael and uh thanks for uh",
    "start": "2308880",
    "end": "2316880"
  },
  {
    "text": "getting all the interesting presenters together ricardo yep no problem thanks for having",
    "start": "2316880",
    "end": "2323839"
  },
  {
    "text": "me thank you guys",
    "start": "2323839",
    "end": "2327838"
  },
  {
    "text": "thanks all",
    "start": "2329119",
    "end": "2341838"
  },
  {
    "text": "you",
    "start": "7416080",
    "end": "7418159"
  }
]