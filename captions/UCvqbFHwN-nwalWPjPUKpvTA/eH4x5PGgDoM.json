[
  {
    "text": "hi thank you for tuning in i am sunita mal i work as a director of ai model systems",
    "start": "480",
    "end": "6480"
  },
  {
    "text": "at mia map today we are going to talk about a peculiar kind of crime scene",
    "start": "6480",
    "end": "12000"
  },
  {
    "text": "investigation this is cube csi and focus is who killed my part exactly who",
    "start": "12000",
    "end": "19039"
  },
  {
    "text": "has done it every intriguing investigation begins",
    "start": "19039",
    "end": "24400"
  },
  {
    "text": "with a disaster and ours is no different it started off by cube control",
    "start": "24400",
    "end": "30880"
  },
  {
    "text": "apply my pixie dust magical app which is we thought is well tested in local",
    "start": "30880",
    "end": "36320"
  },
  {
    "text": "environment and bare metal and as soon as we deployed deployed excuse me all hell broke loose we started to see a",
    "start": "36320",
    "end": "43680"
  },
  {
    "text": "container being killed left and right with om kill as a reason and error code exit code 137 and this is then going",
    "start": "43680",
    "end": "51760"
  },
  {
    "text": "into crash loopback which is a pure joy to look at when you are getting things",
    "start": "51760",
    "end": "56879"
  },
  {
    "text": "out into production so so then that sparked a massive hunt for",
    "start": "56879",
    "end": "62960"
  },
  {
    "text": "what exactly happened and who killed the pot so let's start with the investigation",
    "start": "62960",
    "end": "68400"
  },
  {
    "text": "bone we have a particular container process that's being killed repeatedly",
    "start": "68400",
    "end": "73840"
  },
  {
    "text": "potentially uh because it's being a memory hogger now we know that the kill is a forceful",
    "start": "73840",
    "end": "80240"
  },
  {
    "text": "brutal and it's killed with a sickle as a signal so it's either somebody in",
    "start": "80240",
    "end": "86159"
  },
  {
    "text": "somebody with more power has killed the process without the the desired wishes",
    "start": "86159",
    "end": "92479"
  },
  {
    "text": "and it's a runtime error then okay so we we know uh some context and let's",
    "start": "92479",
    "end": "100400"
  },
  {
    "text": "look at the details for how the containers run on a host machine and",
    "start": "100400",
    "end": "106399"
  },
  {
    "text": "then who are all in charge um in making the container process life cycle",
    "start": "106399",
    "end": "111520"
  },
  {
    "text": "run on a host so we obviously have a host the the their metal or the instance or virtual and then on top we have the",
    "start": "111520",
    "end": "118640"
  },
  {
    "text": "container runtime and container runtime is comprised of two different levels the lower level runtime and higher level",
    "start": "118640",
    "end": "125200"
  },
  {
    "text": "and time the lower level deals with interacting with systems host using system calls and making use of",
    "start": "125200",
    "end": "132560"
  },
  {
    "text": "kernel and os features container d which is the higher level runtime then deals with a higher level",
    "start": "132560",
    "end": "139200"
  },
  {
    "text": "apis api that interfaces users and deals with image management etc aspects so",
    "start": "139200",
    "end": "144959"
  },
  {
    "text": "then we have the runtime that's broken down into two levels low level and high level run c and container d are two very",
    "start": "144959",
    "end": "152239"
  },
  {
    "text": "popular examples of these runtimes obviously there are many different other runtimes available too",
    "start": "152239",
    "end": "158720"
  },
  {
    "text": "so now we have container on time and then your process that we call container then runs on top of it and the the layer",
    "start": "158720",
    "end": "166000"
  },
  {
    "text": "underneath the runtime and the host then guarantees the isolation for your container process so they are kind of",
    "start": "166000",
    "end": "172239"
  },
  {
    "text": "not stepping onto each other's toes so this is what happens on the host",
    "start": "172239",
    "end": "177280"
  },
  {
    "text": "level now when we put kubernetes in the mix that is a",
    "start": "177280",
    "end": "182640"
  },
  {
    "text": "container orchestrator not just on one host on many host then we need another",
    "start": "182640",
    "end": "188959"
  },
  {
    "text": "process who is responsible for managing container processes on the host on its",
    "start": "188959",
    "end": "194560"
  },
  {
    "text": "own way and also managing across all the other servers which is essentially what",
    "start": "194560",
    "end": "199840"
  },
  {
    "text": "kubernetes is a part of it i should say has become very big so",
    "start": "199840",
    "end": "205840"
  },
  {
    "text": "that process that we're talking about that deals the management at a host level is called tubelit that is a",
    "start": "205840",
    "end": "211519"
  },
  {
    "text": "systemd process that manages uh your process running and the way it does it is by implicitly talking to your",
    "start": "211519",
    "end": "218319"
  },
  {
    "text": "container runtime that's provisioned on your node now then that then talks to image",
    "start": "218319",
    "end": "225280"
  },
  {
    "text": "registry and deals with the container process life cycle now um",
    "start": "225280",
    "end": "230640"
  },
  {
    "text": "c advisor is is is a is a way to grab uh container resources utilization and",
    "start": "230640",
    "end": "237120"
  },
  {
    "text": "stats about it which is a part of a cubenet so cubelet then talks to the container runtime to facilitate the",
    "start": "237120",
    "end": "243920"
  },
  {
    "text": "lifecycle of container processes and it also has the advisors for the matrix collection",
    "start": "243920",
    "end": "249360"
  },
  {
    "text": "now there are other parts in that figure that i'm not going to touch upon very much because that's kind of not the",
    "start": "249360",
    "end": "255599"
  },
  {
    "text": "focus of the discussion now uh in current version all the way up to k20",
    "start": "255599",
    "end": "261280"
  },
  {
    "text": "the the way cubelet talks to container runtime is via a thin layer called docker ship which is which is a means to",
    "start": "261280",
    "end": "268560"
  },
  {
    "text": "mitigate the communication uh or overhead with talking to docker this is all going to change a lot in k 20 and 22",
    "start": "268560",
    "end": "278400"
  },
  {
    "text": "but for what we're talking about it's not so much relevant okay so coming back to uh",
    "start": "278400",
    "end": "285440"
  },
  {
    "text": "to to our actual investigation then uh we know that our containers are",
    "start": "285440",
    "end": "292960"
  },
  {
    "text": "being forcefully killed but it's not to manual intervention then based on what we have discussed it's either the",
    "start": "292960",
    "end": "299280"
  },
  {
    "text": "cubelet um who is in charge of managing the process running from a cube viewpoint or an os",
    "start": "299280",
    "end": "305840"
  },
  {
    "text": "kernel then who is in charge of running processes on the host one of the twos are killing the process they they it",
    "start": "305840",
    "end": "312800"
  },
  {
    "text": "doesn't look like there could be other suspects that we would have to deal with",
    "start": "312800",
    "end": "318320"
  },
  {
    "text": "so uh let's talk about what container processes and the isolation how that's",
    "start": "318320",
    "end": "323440"
  },
  {
    "text": "guaranteed at the os level um namespace and c group these are the two important aspects that allow",
    "start": "323440",
    "end": "331520"
  },
  {
    "text": "for the isolation of the container process now namespace is responsible for isolations from a file system networking",
    "start": "331520",
    "end": "338880"
  },
  {
    "text": "in other aspects c group on the other hand is a feature of kernel that allows",
    "start": "338880",
    "end": "344240"
  },
  {
    "text": "for hierarchical management of resource and resource requirements and c group memory and",
    "start": "344240",
    "end": "351600"
  },
  {
    "text": "other uh features or resources can be used to to to uh",
    "start": "351600",
    "end": "357120"
  },
  {
    "text": "to manage viac group now uh due to elaborate more on that i",
    "start": "357120",
    "end": "363039"
  },
  {
    "text": "think it it a bit of a demo would would help and that's what we would now",
    "start": "363039",
    "end": "368960"
  },
  {
    "text": "look at",
    "start": "368960",
    "end": "371360"
  },
  {
    "text": "okay so what i'm going to do now is start a container process",
    "start": "375360",
    "end": "380400"
  },
  {
    "text": "and the container process that i'm going to start is essentially ipython um on a container from numpy direct and",
    "start": "380400",
    "end": "388240"
  },
  {
    "text": "all i'm doing here is essentially saying give me ipython as a process from the image don't be there and enter on it",
    "start": "388240",
    "end": "396800"
  },
  {
    "text": "and now what i want to do is i want to look at the stacks for how much",
    "start": "396800",
    "end": "402000"
  },
  {
    "text": "footprint this container is is running with and as you can see it's telling here that it's using 35 meg of",
    "start": "402000",
    "end": "409599"
  },
  {
    "text": "memory and some cpu",
    "start": "409599",
    "end": "413199"
  },
  {
    "text": "and now what i'm going to do is start the same container again under different name with the 40 mag as memory",
    "start": "414880",
    "end": "421440"
  },
  {
    "text": "requirements now if you see i have two processes running they have",
    "start": "421440",
    "end": "427599"
  },
  {
    "text": "been running for a little bit now let's talk c group here",
    "start": "427599",
    "end": "433680"
  },
  {
    "text": "so c group is managed through a pi system in a way like this and you can see all the kind of",
    "start": "435360",
    "end": "441840"
  },
  {
    "text": "resources that cgroup then would manage now if i look into sorry look into memory i would see these",
    "start": "441840",
    "end": "449039"
  },
  {
    "text": "are the kind of aspects of memory that's then managed by c group what's of interest is kernel memory and also",
    "start": "449039",
    "end": "455599"
  },
  {
    "text": "memory uh limits and in few other aspects",
    "start": "455599",
    "end": "461199"
  },
  {
    "text": "okay so if you look closely here you would see a",
    "start": "461199",
    "end": "466800"
  },
  {
    "text": "folder here called docker",
    "start": "466800",
    "end": "472380"
  },
  {
    "text": "[Music] yeah it's there",
    "start": "472380",
    "end": "478720"
  },
  {
    "text": "so uh under memory then we have docker and you can see there are two",
    "start": "478720",
    "end": "484319"
  },
  {
    "text": "uh these um ids that corresponds to the container that we have run",
    "start": "484319",
    "end": "490400"
  },
  {
    "text": "just now four four three seven four four three seven and six five",
    "start": "490400",
    "end": "496240"
  },
  {
    "text": "um c six c six c so this is short id and this is the full id of the container now",
    "start": "496240",
    "end": "502000"
  },
  {
    "text": "let's have a bit of a look into what the memory limits for the containers are",
    "start": "502000",
    "end": "507759"
  },
  {
    "text": "and i'm just going to cut the memory limit type and all i'm doing here is say",
    "start": "507759",
    "end": "513518"
  },
  {
    "text": "give me full full id of my container that's 40 meg limit",
    "start": "513519",
    "end": "518560"
  },
  {
    "text": "and you can see when i do this i am getting about 40 mag as a process now if i just go in here",
    "start": "518560",
    "end": "526720"
  },
  {
    "text": "and make this",
    "start": "526720",
    "end": "530199"
  },
  {
    "text": "i would see a really large number this is actually the maximum 864 container by",
    "start": "536720",
    "end": "542320"
  },
  {
    "text": "the multiple of uh 4kb for page size which is the linux page size so it's a",
    "start": "542320",
    "end": "548399"
  },
  {
    "text": "way of saying that this container is running without a memory limit that is unlimited",
    "start": "548399",
    "end": "553519"
  },
  {
    "text": "memory now let's do another thing let's bring up another container with a",
    "start": "553519",
    "end": "559600"
  },
  {
    "text": "with the 20 mag memory and this is obviously not enough memory that we needed to run with",
    "start": "559600",
    "end": "566000"
  },
  {
    "text": "and then we started the container now if we look at what happened we see that actually the",
    "start": "566000",
    "end": "572160"
  },
  {
    "text": "20 meg container was just killed",
    "start": "572160",
    "end": "577839"
  },
  {
    "text": "now what we want to do is look at the inspect this container and all i'm doing",
    "start": "578160",
    "end": "584720"
  },
  {
    "text": "doing is saying do i inspect on this container and i want to only look at the state measure",
    "start": "584720",
    "end": "591440"
  },
  {
    "text": "and you can see it's saying this process was actually omkil with an exit code of 137",
    "start": "591440",
    "end": "598640"
  },
  {
    "text": "and we could do the same thing with 40 mag container",
    "start": "598640",
    "end": "604480"
  },
  {
    "text": "and we would see then it's not omt it's still running etc",
    "start": "604480",
    "end": "611360"
  },
  {
    "text": "so we just simulated the omkil scenario in our local environment",
    "start": "611360",
    "end": "618399"
  },
  {
    "text": "now i want to show you something interesting and this is about the the swap error that we were getting earlier",
    "start": "618399",
    "end": "625279"
  },
  {
    "text": "here it was saying that your container doesn't support sub capabilities and memory limited without",
    "start": "625279",
    "end": "632399"
  },
  {
    "text": "swap so that means if i asked for container to have only 20 mag memory my container is only going to get limited",
    "start": "632399",
    "end": "639120"
  },
  {
    "text": "to 20 meg because the swap which is a capability of uh systems to go beyond what's uh available",
    "start": "639120",
    "end": "646720"
  },
  {
    "text": "in physical ram and and extend use a little bit more than what the capacity is and that aspect is disabled",
    "start": "646720",
    "end": "653920"
  },
  {
    "text": "now so we we can't really use more than what we have",
    "start": "653920",
    "end": "659040"
  },
  {
    "text": "but this is my mac now and i would run the same two command like before so",
    "start": "659040",
    "end": "667200"
  },
  {
    "text": "i'm running limited limitless kcsi and 20 mag kcsi",
    "start": "667200",
    "end": "673519"
  },
  {
    "text": "and i would you would see that when i run this actually the 20 mag container",
    "start": "673519",
    "end": "679279"
  },
  {
    "text": "is actually still running it hasn't been killed and i could see",
    "start": "679279",
    "end": "685040"
  },
  {
    "text": "dr stacks on this and i would say it's still using 90 mag",
    "start": "685040",
    "end": "691519"
  },
  {
    "text": "but it's actually making um use of the swap for for the extra that it needs",
    "start": "691519",
    "end": "698320"
  },
  {
    "text": "now let's do a differently let's give it a little let's make it swap list and i do that by",
    "start": "698320",
    "end": "705440"
  },
  {
    "text": "um explicitly specifying the same memory as swap and thereby disabling the stop",
    "start": "705440",
    "end": "712240"
  },
  {
    "text": "and the moment i do that you would see that my container the strict one is killed which is exactly",
    "start": "712240",
    "end": "719040"
  },
  {
    "text": "for the same reason and we could inspect on this",
    "start": "719040",
    "end": "724759"
  },
  {
    "text": "and you could see that it's actually overkill for the same reason okay so we're talking about docker here like",
    "start": "740240",
    "end": "747519"
  },
  {
    "text": "this um and if we are in a cube context as in if uh if the node is a",
    "start": "747519",
    "end": "754399"
  },
  {
    "text": "member of a cube cluster then we would also see here cube pod at the same level",
    "start": "754399",
    "end": "760000"
  },
  {
    "text": "as docker now we can see here in this example that",
    "start": "760000",
    "end": "765120"
  },
  {
    "text": "there is actually one part running in here in the same way as we had a docker",
    "start": "765120",
    "end": "771279"
  },
  {
    "text": "container running and if you look into that pod we would see that we have now two containers running",
    "start": "771279",
    "end": "777680"
  },
  {
    "text": "inside this pod okay so presumably one is the pause",
    "start": "777680",
    "end": "782800"
  },
  {
    "text": "container which is the container that first gets created to create the networking in the right name space and",
    "start": "782800",
    "end": "788800"
  },
  {
    "text": "environments and then the actual process container is created so everything is set up beforehand",
    "start": "788800",
    "end": "796079"
  },
  {
    "text": "but if the if this pod had two containers in it then we would see here three",
    "start": "796079",
    "end": "801839"
  },
  {
    "text": "um three entries for three different containers including one for pause so that was about",
    "start": "801839",
    "end": "808800"
  },
  {
    "text": "uh that was about q pod now quickly want to touch on",
    "start": "808800",
    "end": "816160"
  },
  {
    "text": "the process and if you do",
    "start": "816160",
    "end": "820000"
  },
  {
    "text": "okay so let's just so we said a container is a",
    "start": "824000",
    "end": "832800"
  },
  {
    "text": "linux process how do i know what is the process id for my container",
    "start": "832800",
    "end": "837920"
  },
  {
    "text": "and to do that is essentially this what we say is we say docker inspect and i",
    "start": "837920",
    "end": "843839"
  },
  {
    "text": "want to inspect the process id for this container with an id",
    "start": "843839",
    "end": "851680"
  },
  {
    "text": "that is that belongs to this particular limitless kcsi container so in a way of",
    "start": "853040",
    "end": "858320"
  },
  {
    "text": "saying is give me uh we ask for the process id by",
    "start": "858320",
    "end": "863760"
  },
  {
    "text": "doing a docker inspect using the container id and then we do a process tags and you can see that three five zero five is the",
    "start": "863760",
    "end": "871680"
  },
  {
    "text": "process id on the host level as in the host sees it as this three",
    "start": "871680",
    "end": "876720"
  },
  {
    "text": "five zero five id and this is pointing to an ipython process now there's an interesting",
    "start": "876720",
    "end": "883519"
  },
  {
    "text": "pseudo file system based on fifo that linux manages and this is basically",
    "start": "883519",
    "end": "890320"
  },
  {
    "text": "under proc so prop is for process and we could say",
    "start": "890320",
    "end": "895639"
  },
  {
    "text": "3505 and then we would see all these",
    "start": "895639",
    "end": "901279"
  },
  {
    "text": "properties of the process i guess some of them are some of them are the",
    "start": "901279",
    "end": "907760"
  },
  {
    "text": "the the stream processes or pipe processes that other systems uses to to communicate then on",
    "start": "907760",
    "end": "915199"
  },
  {
    "text": "now i want to particularly talk about the limits here",
    "start": "915199",
    "end": "920959"
  },
  {
    "text": "and you could and you can see all the limits are",
    "start": "920959",
    "end": "927279"
  },
  {
    "text": "mentioned here in terms of various kinds of limits now",
    "start": "927279",
    "end": "932320"
  },
  {
    "text": "particularly want to talk about om score",
    "start": "932320",
    "end": "937680"
  },
  {
    "text": "and this is basically this is how the system manages what is the score for om",
    "start": "938320",
    "end": "944639"
  },
  {
    "text": "for this process and in the event of a memory crunch this codes are used to",
    "start": "944639",
    "end": "949759"
  },
  {
    "text": "decide which process should get killed there are actually three files",
    "start": "949759",
    "end": "955759"
  },
  {
    "text": "on unproc or om one is the score which actually is used",
    "start": "955759",
    "end": "963040"
  },
  {
    "text": "to manage this code or record the score the other are two adjustment files as in how much you want to offset the score um",
    "start": "963040",
    "end": "970880"
  },
  {
    "text": "and there's two for different linux versions but that's not so much relevant for what we're talking about",
    "start": "970880",
    "end": "977040"
  },
  {
    "text": "so we had to look at how cgroup works uh and allows for container",
    "start": "977040",
    "end": "982880"
  },
  {
    "text": "processes isolation and we also look through how om scores are managed",
    "start": "982880",
    "end": "988320"
  },
  {
    "text": "and how container knows which process to then kill",
    "start": "988320",
    "end": "993920"
  },
  {
    "text": "okay it's slightly changing the gear and talking more about how do we ensure",
    "start": "993920",
    "end": "1000560"
  },
  {
    "text": "the how do we ensure the quality of service and resource",
    "start": "1000560",
    "end": "1005839"
  },
  {
    "text": "requirements so when a pod is running under kubernetes it allows uh",
    "start": "1005839",
    "end": "1012959"
  },
  {
    "text": "users to specify the resource requirements and it does this in three different fashion one is you basically",
    "start": "1013120",
    "end": "1019120"
  },
  {
    "text": "don't specify any limits and in that case the quality of service becomes the best effort",
    "start": "1019120",
    "end": "1024959"
  },
  {
    "text": "and then the other one is bustable where you give it a range and based on the range the cube will try to best place it",
    "start": "1024959",
    "end": "1031520"
  },
  {
    "text": "this kind of part give a quality of service as bustable now the third one is guaranteed if you give the same",
    "start": "1031520",
    "end": "1040400"
  },
  {
    "text": "if you give the same memory requirement as limit then the container the process",
    "start": "1040880",
    "end": "1046000"
  },
  {
    "text": "is called uh guaranteed quality of service from the point viewpoint and the guaranteed ones are the best kind of",
    "start": "1046000",
    "end": "1052559"
  },
  {
    "text": "parts they are least likely to get killed in the event of om and they are also good for it in a way that it's",
    "start": "1052559",
    "end": "1059120"
  },
  {
    "text": "easier for cube to manage um in in placing them there is no variability or",
    "start": "1059120",
    "end": "1064480"
  },
  {
    "text": "fuzziness in what kind of resource requirement that would uh give us and in saying that the best effort pods are",
    "start": "1064480",
    "end": "1071280"
  },
  {
    "text": "then the most likely to get killed because they are the most unpredictable from resource management viewpoint",
    "start": "1071280",
    "end": "1079960"
  },
  {
    "text": "so we talked about resource requirement and quality of service and how likely a ford is likely to get killed now",
    "start": "1081039",
    "end": "1087919"
  },
  {
    "text": "in terms of resource there's two broad kind of resources compressible and incompressible compressible means if you",
    "start": "1087919",
    "end": "1095520"
  },
  {
    "text": "breach your limits you are likely to get throttled but it won't be a deadly event",
    "start": "1095520",
    "end": "1101760"
  },
  {
    "text": "incompressible on the other hand says that if you breach your limits",
    "start": "1101760",
    "end": "1106880"
  },
  {
    "text": "c group or something would kill your process and memory is an incompressible",
    "start": "1106880",
    "end": "1112720"
  },
  {
    "text": "resource and cpu then is is a",
    "start": "1112720",
    "end": "1117840"
  },
  {
    "text": "compressible resource so based on what we have discussed i think the question here is can cube overcome",
    "start": "1117840",
    "end": "1125120"
  },
  {
    "text": "it or can you allow your commit and how about linux kernel",
    "start": "1125120",
    "end": "1130640"
  },
  {
    "text": "and this is a very important question and it's not we are posing now it's been asked for many",
    "start": "1130640",
    "end": "1137360"
  },
  {
    "text": "years from a kernel viewpoint and the common view is that it's a feature not a bug but there are contrary opinions as",
    "start": "1137360",
    "end": "1144080"
  },
  {
    "text": "well around uh over committal is a bug so can cube overcommit absolutely if we are",
    "start": "1144080",
    "end": "1151760"
  },
  {
    "text": "allowing pods to run with the limit and it's possible that uh there are two",
    "start": "1151760",
    "end": "1158000"
  },
  {
    "text": "parts running with a big range of limits and collectively together they breach the",
    "start": "1158000",
    "end": "1164720"
  },
  {
    "text": "total available resources and mind you the swap spaces are disabled on",
    "start": "1164720",
    "end": "1169840"
  },
  {
    "text": "communities for for the right reason so the cube can overcome it but it when it",
    "start": "1169840",
    "end": "1175440"
  },
  {
    "text": "overcome it it basically overcome its without a backing to go",
    "start": "1175440",
    "end": "1180640"
  },
  {
    "text": "further now uh sure same thing applies to cpu as well",
    "start": "1180640",
    "end": "1185840"
  },
  {
    "text": "you can can overcome it from cpu and a memory viewpoint and the os are",
    "start": "1185840",
    "end": "1192799"
  },
  {
    "text": "configured to overcome it as well by default but they do have a soft space to",
    "start": "1192799",
    "end": "1197840"
  },
  {
    "text": "fall back on to to kind of use a little bit more space when when",
    "start": "1197840",
    "end": "1203520"
  },
  {
    "text": "the physical memories is reached okay so going back to our actual crime",
    "start": "1203520",
    "end": "1210400"
  },
  {
    "text": "scene when we profile locally we never really saw the resource requirement to",
    "start": "1210400",
    "end": "1215440"
  },
  {
    "text": "go beyond six gig it was kind of always in the six gig landscape landscape so we",
    "start": "1215440",
    "end": "1221039"
  },
  {
    "text": "tried many different combinations of quality of service um and trying to just mitigate the issue while",
    "start": "1221039",
    "end": "1227280"
  },
  {
    "text": "we investigate how to fix it and finally we settled on to raise really crazy 31 gig as a memory",
    "start": "1227280",
    "end": "1233760"
  },
  {
    "text": "requirement for the container leaving to the um as an enough for the systems to",
    "start": "1233760",
    "end": "1239440"
  },
  {
    "text": "run and uh but we would still see omk we would still see the guaranteed parts",
    "start": "1239440",
    "end": "1245679"
  },
  {
    "text": "being killed even though we don't really notice any spikes or a case where the limits have been",
    "start": "1245679",
    "end": "1252159"
  },
  {
    "text": "breached and and at no point the node was found to be under duress it had enough memory",
    "start": "1252159",
    "end": "1259440"
  },
  {
    "text": "left from the monitoring viewpoint that it was never really reaching its own to the limit",
    "start": "1259440",
    "end": "1266720"
  },
  {
    "text": "so just reviewing our board back we know that container behaves kind of reasonably we are still getting a kill",
    "start": "1266960",
    "end": "1273360"
  },
  {
    "text": "uh the quality of service is configured to the best capacity still there is a kill",
    "start": "1273360",
    "end": "1278559"
  },
  {
    "text": "it is likely to be an over-the-middle related issue and we know that the suspect is",
    "start": "1278559",
    "end": "1285120"
  },
  {
    "text": "now either the kernel or cubelet okay so we had a bit of a",
    "start": "1285120",
    "end": "1291360"
  },
  {
    "text": "lucky draw we found a critical events where we noticed a particular spike",
    "start": "1291360",
    "end": "1297280"
  },
  {
    "text": "in the in the memory usage and this would go about all the way to a little over 28 gig still within the",
    "start": "1297280",
    "end": "1304320"
  },
  {
    "text": "limits of 31 gig and uh it it was a lot less frequent than actual",
    "start": "1304320",
    "end": "1311120"
  },
  {
    "text": "key events um and like i said in the bounds of memory limit but we still don't know why",
    "start": "1311120",
    "end": "1317120"
  },
  {
    "text": "the containers were getting killed but we do have an indication that sometimes our processes are spiking up memory",
    "start": "1317120",
    "end": "1323840"
  },
  {
    "text": "usage based on some unknown reason okay so we talked about om scores before",
    "start": "1323840",
    "end": "1331200"
  },
  {
    "text": "what happens how does the om workflow gets triggered so um linux manages this",
    "start": "1331200",
    "end": "1337760"
  },
  {
    "text": "um k message kernel messaging stream where all these um events are then put in now",
    "start": "1337760",
    "end": "1346799"
  },
  {
    "text": "k message kernel messaging parser then passes it and translate it into events through event handler now the two events",
    "start": "1346799",
    "end": "1354320"
  },
  {
    "text": "that we are interested in is om event and omg 11 om event kind of indicating",
    "start": "1354320",
    "end": "1359600"
  },
  {
    "text": "hey memory surge is happening now there is there is actually an oven situation ahead",
    "start": "1359600",
    "end": "1365120"
  },
  {
    "text": "and the other one is okay we have an om we just need to do a recovery let's start the killing which",
    "start": "1365120",
    "end": "1370559"
  },
  {
    "text": "is the um kill and then there are watchers which would watch to these events and take appropriate action so we",
    "start": "1370559",
    "end": "1376559"
  },
  {
    "text": "would have watched cubelet would have the watch implemented and so would be the some process in in linux kernel",
    "start": "1376559",
    "end": "1385120"
  },
  {
    "text": "okay so there is nothing in application log that would indicate any problem with memory",
    "start": "1386159",
    "end": "1391919"
  },
  {
    "text": "search but we know there is a search uh very rarely we don't see any event of",
    "start": "1391919",
    "end": "1397520"
  },
  {
    "text": "any kind of om on cube system pods cube system processes or in any of the event",
    "start": "1397520",
    "end": "1404000"
  },
  {
    "text": "log as far as cube is concerned uh the node is all healthy and it's all hakuna",
    "start": "1404000",
    "end": "1409120"
  },
  {
    "text": "matata okay coming back to again so then based on that we conclude that it's not",
    "start": "1409120",
    "end": "1416720"
  },
  {
    "text": "actually tubelet who is killing the process so let's look at kernel logs then and",
    "start": "1416720",
    "end": "1423039"
  },
  {
    "text": "this is where we landed up then this is an excerpt from our kernel log and we",
    "start": "1423039",
    "end": "1428640"
  },
  {
    "text": "can see that actually om kill was invoked and it was ignored because c group ran out of memory",
    "start": "1428640",
    "end": "1435840"
  },
  {
    "text": "and you can see here in here that it's actually omkil invoked and then q pod this is the application",
    "start": "1435840",
    "end": "1443200"
  },
  {
    "text": "part was killed as a result of memory limit and the actual usage was 31 4 55",
    "start": "1443200",
    "end": "1451520"
  },
  {
    "text": "or 31.4 gig to 31.45 gig again there is a difference of",
    "start": "1451520",
    "end": "1457679"
  },
  {
    "text": "about 1.5 uh 1000 kb now",
    "start": "1457679",
    "end": "1463039"
  },
  {
    "text": "that it's still kind of short of the reaching memory limit but it's almost reaching",
    "start": "1463039",
    "end": "1468559"
  },
  {
    "text": "and you can see that swap usage is still zero and on the os level the the swap is",
    "start": "1468559",
    "end": "1476559"
  },
  {
    "text": "unlimited so like you know memory usage is still unlimited from from a system viewpoint",
    "start": "1476559",
    "end": "1482640"
  },
  {
    "text": "kernel is using a little bit of memory but uh it's",
    "start": "1482640",
    "end": "1488159"
  },
  {
    "text": "it's it's not the application usage so much and it has its own limit as you were seeing earlier",
    "start": "1488159",
    "end": "1494159"
  },
  {
    "text": "okay so talking about the some of the metrics of c group here now rss is the",
    "start": "1494159",
    "end": "1500400"
  },
  {
    "text": "resident set size which is a portion of the memory occupied by the process that's held in",
    "start": "1500400",
    "end": "1506080"
  },
  {
    "text": "in ram and it's it's uh represents the anonymous memory and the subspace memory",
    "start": "1506080",
    "end": "1512320"
  },
  {
    "text": "now rs is huge on the other hand is actually the uh anonymous transparent",
    "start": "1512320",
    "end": "1518240"
  },
  {
    "text": "memory that corresponds to the huge pages which is which is when 4kb page",
    "start": "1518240",
    "end": "1523440"
  },
  {
    "text": "size is not enough uh for for for the sort of requirement you have if you're using large memory allocation and in",
    "start": "1523440",
    "end": "1529840"
  },
  {
    "text": "that case the page size will become a lot bigger than 4kb in that case your residence set size huge would indicate",
    "start": "1529840",
    "end": "1537360"
  },
  {
    "text": "the anonymous huge page memory okay and then we also know that",
    "start": "1537360",
    "end": "1543200"
  },
  {
    "text": "anonymous memory often activated as anon is is a memory mapped with no device or",
    "start": "1543200",
    "end": "1548559"
  },
  {
    "text": "file backing and then it it also indicates the memory that's on heap and stack",
    "start": "1548559",
    "end": "1556200"
  },
  {
    "text": "so coming back to the second part of the log as you can see here we are saying that the pod",
    "start": "1556480",
    "end": "1563360"
  },
  {
    "text": "usage the actual usage is this and then within that part",
    "start": "1563360",
    "end": "1568480"
  },
  {
    "text": "there are two containers one is the pause container which um is really using very little memory so",
    "start": "1568480",
    "end": "1576879"
  },
  {
    "text": "not so much the focus right now but this is the application part and you can see it is actually using an active anonymous",
    "start": "1577120",
    "end": "1584159"
  },
  {
    "text": "memory of 31.38 gig and cash is a little weight the rss",
    "start": "1584159",
    "end": "1589360"
  },
  {
    "text": "memory is also then 31.38 gig and the the huge page memory",
    "start": "1589360",
    "end": "1596159"
  },
  {
    "text": "is 29 gig so all in all there is only very little memory that's being used that's",
    "start": "1596159",
    "end": "1603120"
  },
  {
    "text": "not on the huge pages and uh but we are still very much within the limit of",
    "start": "1603120",
    "end": "1608559"
  },
  {
    "text": "container which was 31.45 as it was said",
    "start": "1608559",
    "end": "1613760"
  },
  {
    "text": "so um so this is where we are we are almost the process is almost reaching the limit",
    "start": "1613760",
    "end": "1619760"
  },
  {
    "text": "and it's getting killed the om is getting invoked and it is getting killed there is still enough uh memory",
    "start": "1619760",
    "end": "1625919"
  },
  {
    "text": "available on the node so node is not under duress and if you look at this",
    "start": "1625919",
    "end": "1631279"
  },
  {
    "text": "um total vm we could see that we have a really ridiculous amount of virtual",
    "start": "1631279",
    "end": "1636400"
  },
  {
    "text": "memory that's being allocated 62.59 meg is being allocated and it does look",
    "start": "1636400",
    "end": "1642640"
  },
  {
    "text": "like we have some kind of uh unused memory allocation that's that's going on",
    "start": "1642640",
    "end": "1648000"
  },
  {
    "text": "but coming back to like you know why the process was killed we know that it was because it was reaching the the memory",
    "start": "1648000",
    "end": "1655679"
  },
  {
    "text": "limit it was just a little bit shy i to be honest i'm not sure why it was still getting killed when it was just",
    "start": "1655679",
    "end": "1662080"
  },
  {
    "text": "shy of the requirements but it is getting killed just being on on the shy of requirements",
    "start": "1662080",
    "end": "1669840"
  },
  {
    "text": "and yeah i must mention if you haven't seen um ian lewis blogger on almighty container and if you i want to know more",
    "start": "1669840",
    "end": "1676640"
  },
  {
    "text": "about what pause container is i highly recommend reading through that blog so then um as i mentioned we were quite",
    "start": "1676640",
    "end": "1684799"
  },
  {
    "text": "confused why the process is getting killed despite being shy of the limit",
    "start": "1684799",
    "end": "1689840"
  },
  {
    "text": "but uh given it's only a few uh few kb shy uh we're just going to accept",
    "start": "1689840",
    "end": "1696559"
  },
  {
    "text": "that it is getting killed uh there now the problem we have is when we get",
    "start": "1696559",
    "end": "1702559"
  },
  {
    "text": "overkill the behaviors are exactly like that it's a runtime error you can't take away from it gracefully and you can't",
    "start": "1702559",
    "end": "1709120"
  },
  {
    "text": "even fail safely and this is exactly not what we want so what we want is we want a safe recovery",
    "start": "1709120",
    "end": "1716880"
  },
  {
    "text": "and handle more in the predictable fashion so we looked at disabling the over",
    "start": "1716880",
    "end": "1721919"
  },
  {
    "text": "capital and saving failing more at an application level rather than abruptly being killed and there's two",
    "start": "1721919",
    "end": "1729039"
  },
  {
    "text": "settings on vm level where you could say oh computer memory is disabled two for disabled here",
    "start": "1729039",
    "end": "1735039"
  },
  {
    "text": "there two for disabled and then this another ratio aspect which which is only used when you disable the over committee",
    "start": "1735039",
    "end": "1742320"
  },
  {
    "text": "and it's a way of saying that i don't want my system to overcome it and only",
    "start": "1742320",
    "end": "1747840"
  },
  {
    "text": "commit to their total allowed memory up until cube18 this there was no way to",
    "start": "1747840",
    "end": "1755120"
  },
  {
    "text": "provide this setting if you provide this via user data script on your node provisioning site it would essentially",
    "start": "1755120",
    "end": "1761840"
  },
  {
    "text": "get overwritten in cube 18 onwards this",
    "start": "1761840",
    "end": "1766960"
  },
  {
    "text": "system parameters property that now k ops allows that you can use to",
    "start": "1766960",
    "end": "1772799"
  },
  {
    "text": "configure your cube clusters but even in that the over committal setting explicitly overcome it memory is equal",
    "start": "1772799",
    "end": "1779360"
  },
  {
    "text": "to two is actually disabled there is no way to systematically provide this setting and and make sure your your",
    "start": "1779360",
    "end": "1786399"
  },
  {
    "text": "nodes are actually disabling the over committee so really the only choice we have here",
    "start": "1786399",
    "end": "1792159"
  },
  {
    "text": "is to to um on the fly update the nodes after they have come up and they've joined the",
    "start": "1792159",
    "end": "1798960"
  },
  {
    "text": "queue cluster to override this over commercial setting so we kind of follow the script to to",
    "start": "1798960",
    "end": "1806240"
  },
  {
    "text": "update all the nodes with this overcommittal setting we obviously there's probably a reason why it's",
    "start": "1806240",
    "end": "1812240"
  },
  {
    "text": "disabled uh but we don't know the the details uh and that's why we selectively",
    "start": "1812240",
    "end": "1817760"
  },
  {
    "text": "apply only for a particular fleet of nodes where this particular process goes in",
    "start": "1817760",
    "end": "1823600"
  },
  {
    "text": "so just revealing our board we know who are who who the killer was it was os kernel and we also know that actually",
    "start": "1823600",
    "end": "1830880"
  },
  {
    "text": "our well-behaved application was not truly well behaving but disabling the over committee",
    "start": "1830880",
    "end": "1837520"
  },
  {
    "text": "definitely mitigated the issue we can see that the process has been running for about nine hours without any",
    "start": "1837520",
    "end": "1843600"
  },
  {
    "text": "interruption when the kills would happen uh at least 15 20 minutes so so frequently",
    "start": "1843600",
    "end": "1851600"
  },
  {
    "text": "so this was a good mitigation but obviously it is not a solution we have an",
    "start": "1851600",
    "end": "1856799"
  },
  {
    "text": "application that's uh hogging more memory than it should",
    "start": "1856799",
    "end": "1862159"
  },
  {
    "text": "and then we reviewed the application and bought the memory footprint download that more chunking option to again help",
    "start": "1862159",
    "end": "1869039"
  },
  {
    "text": "ameliorate this problem so all in all in summary we have a",
    "start": "1869039",
    "end": "1874159"
  },
  {
    "text": "container that was killed because it was reaching its memory limit and it was killed by os kernel and disabling the",
    "start": "1874159",
    "end": "1881519"
  },
  {
    "text": "old committee was uh mitigating the issue and but the actual fix is to",
    "start": "1881519",
    "end": "1886559"
  },
  {
    "text": "reduce the memory footprint and guarantee the the resource quality of service and resource",
    "start": "1886559",
    "end": "1892399"
  },
  {
    "text": "requirements on the part to have more reliable experience on divinities",
    "start": "1892399",
    "end": "1898000"
  },
  {
    "text": "and that's all i had um thank you very much i hope this was an interesting listening in to our experience and it",
    "start": "1898000",
    "end": "1904720"
  },
  {
    "text": "was there was some learning for you in it if there are any questions i'm happy to",
    "start": "1904720",
    "end": "1911519"
  },
  {
    "text": "thank them thank you",
    "start": "1911519",
    "end": "1915559"
  }
]