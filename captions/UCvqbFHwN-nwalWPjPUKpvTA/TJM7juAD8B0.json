[
  {
    "text": "cool uh all right so uh Welcome to our talk everyone uh we we know it's in the",
    "start": "6259",
    "end": "13620"
  },
  {
    "text": "uh last section of the day so we're between you and the uh Booth crawl and",
    "start": "13620",
    "end": "19440"
  },
  {
    "text": "um yeah I don't know appreciate you everyone coming by um so yes uh welcome to whose packet Is",
    "start": "19440",
    "end": "26100"
  },
  {
    "text": "It Anyways awesome thanks Kevin uh my name is Doug",
    "start": "26100",
    "end": "31560"
  },
  {
    "text": "Jordan and I work at Airbnb on our istio-based service mesh called air mesh",
    "start": "31560",
    "end": "36660"
  },
  {
    "text": "I focus on extending the mesh to Virtual Machine based workloads TCP and other Ingress use cases",
    "start": "36660",
    "end": "43860"
  },
  {
    "text": "previously at Microsoft I worked on our bare metal control point where we adopted Linker d",
    "start": "43860",
    "end": "49920"
  },
  {
    "text": "my handle on GitHub Twitter and other socials is usually dwj300",
    "start": "49920",
    "end": "55440"
  },
  {
    "text": "and as you can see from the photo on the right I enjoy cycling climbing mountains and here's a photo of me doing both",
    "start": "55440",
    "end": "61140"
  },
  {
    "text": "those uh yeah so my name is Kevin lime cooler",
    "start": "61140",
    "end": "67020"
  },
  {
    "text": "I am a software engineer at buoyant the creators of linker D I've been working",
    "start": "67020",
    "end": "73439"
  },
  {
    "text": "there four years now and have worked on all the control plane components as well as the proxy uh my social handle on",
    "start": "73439",
    "end": "80880"
  },
  {
    "text": "GitHub and Twitter is K lime cooler you can also reach me on the linkready slack",
    "start": "80880",
    "end": "86400"
  },
  {
    "text": "for any questions you may have after the talk or in watching this recording",
    "start": "86400",
    "end": "93560"
  },
  {
    "text": "thanks Kevin here's a quick agenda of what we'll be talking about today first Kevin will walk us through the how",
    "start": "94619",
    "end": "101720"
  },
  {
    "text": "specifically how a TCP packet gets routed through the mesh then I'll discuss TCP debugging and walk",
    "start": "101720",
    "end": "108840"
  },
  {
    "text": "you through a real world example breaking out TCP dump and Wireshark",
    "start": "108840",
    "end": "113898"
  },
  {
    "text": "thank you cool uh so we'd like to start out with an overview on how a packet is routed in",
    "start": "114840",
    "end": "121320"
  },
  {
    "text": "a service mesh so the things that we cover here will help lay the foundation for understanding some of the debugging",
    "start": "121320",
    "end": "128340"
  },
  {
    "text": "steps Doug will take us through later I'd also like to call out that we're going to try to keep this as service",
    "start": "128340",
    "end": "135360"
  },
  {
    "text": "mesh generic as possible while both Doug and I have had a lot of experience with",
    "start": "135360",
    "end": "140580"
  },
  {
    "text": "sdo and Linker D the concepts we talk about today are generally shared between",
    "start": "140580",
    "end": "145620"
  },
  {
    "text": "the two as well as some other service meshes foreign",
    "start": "145620",
    "end": "151080"
  },
  {
    "text": "so this talk is ultimately about debugging traffic in a cluster with a service mesh so just want to make sure",
    "start": "151080",
    "end": "156480"
  },
  {
    "text": "that we're on the same page about what a service mesh is and the common architecture of one that we or you may",
    "start": "156480",
    "end": "164640"
  },
  {
    "text": "be debugging so mesh provides Key Properties today",
    "start": "164640",
    "end": "171000"
  },
  {
    "text": "those tend to be the four listed here observability for things like logs and",
    "start": "171000",
    "end": "176819"
  },
  {
    "text": "metrics routing things like traffic splitting and endpoint Discovery security think mtls and authorization",
    "start": "176819",
    "end": "185700"
  },
  {
    "text": "policies and reliability for example transparent retries of HTTP requests",
    "start": "185700",
    "end": "192300"
  },
  {
    "text": "circuit breaking things like that in order to provide these features the",
    "start": "192300",
    "end": "198239"
  },
  {
    "text": "service mesh needs to intercept traffic into and out of the Pod so how do we",
    "start": "198239",
    "end": "203760"
  },
  {
    "text": "achieve this so this leads us to the service mess architecture here we see the sidecar",
    "start": "203760",
    "end": "211319"
  },
  {
    "text": "proxy model this is the model that most meshes follow these days it's worth",
    "start": "211319",
    "end": "217200"
  },
  {
    "text": "noting because some message some meshes including the previous Linker D version",
    "start": "217200",
    "end": "222840"
  },
  {
    "text": "ran a proxy per node uh in the sidecar proxy model each pod gets its own proxy",
    "start": "222840",
    "end": "230040"
  },
  {
    "text": "container inbound and outbound traffic can be redirected through this container",
    "start": "230040",
    "end": "235260"
  },
  {
    "text": "which is how some of those features I just discussed are implemented multiple pods may be injected across multiple",
    "start": "235260",
    "end": "242940"
  },
  {
    "text": "nodes and the grouping of all injected pods makes up what we call the data plane",
    "start": "242940",
    "end": "249799"
  },
  {
    "text": "the second part to the sidecar proxy model is the control plane in Linker D this consists of the components that",
    "start": "249900",
    "end": "256799"
  },
  {
    "text": "inform each of the proxies in the data plane it probably has a destination",
    "start": "256799",
    "end": "262320"
  },
  {
    "text": "component used for routing decisions an identity component used for assigning",
    "start": "262320",
    "end": "267600"
  },
  {
    "text": "TLS identities to the proxies and a policy component for determining",
    "start": "267600",
    "end": "272699"
  },
  {
    "text": "who can talk to who depending on the mesh there may be different components",
    "start": "272699",
    "end": "279918"
  },
  {
    "text": "so the sidecar proxy model injects a proxy container into each pod somehow",
    "start": "281460",
    "end": "286860"
  },
  {
    "text": "inbound and outbound traffic from the other containers end up going through that proxy",
    "start": "286860",
    "end": "292580"
  },
  {
    "text": "so in order to understand how it's happening let's take a look at what a",
    "start": "292580",
    "end": "297660"
  },
  {
    "text": "container actually is so the first thing to know is that Linux",
    "start": "297660",
    "end": "304139"
  },
  {
    "text": "doesn't actually have containers it has namespaces namespaces partition the",
    "start": "304139",
    "end": "309240"
  },
  {
    "text": "kernel resources such that different sets of processes see different sets of resources this means that each process",
    "start": "309240",
    "end": "316320"
  },
  {
    "text": "or group of processes is associated with a namespace and can only see the",
    "start": "316320",
    "end": "321720"
  },
  {
    "text": "resources within it the isolated resources depend on the namespace and",
    "start": "321720",
    "end": "327720"
  },
  {
    "text": "we'll see what some of those are next so here we see that by using namespaces",
    "start": "327720",
    "end": "335340"
  },
  {
    "text": "we can create a pod with multiple containers the blue boxes represent containers within the red bordered box",
    "start": "335340",
    "end": "342180"
  },
  {
    "text": "pod the redboarded Box represents a single Network namespace there can be multiple",
    "start": "342180",
    "end": "349380"
  },
  {
    "text": "network name spaces on a host which is how we end up with multiple pods on a node",
    "start": "349380",
    "end": "355139"
  },
  {
    "text": "looking at the blue boxes each process ID is associated with the network",
    "start": "355139",
    "end": "360240"
  },
  {
    "text": "namespace with a network name space and can be a container each of those process IDs can be",
    "start": "360240",
    "end": "367620"
  },
  {
    "text": "associated with for example a separate Mount name space so that they see their own file system",
    "start": "367620",
    "end": "374699"
  },
  {
    "text": "so now that we have a idea about like a higher level representation of a",
    "start": "374699",
    "end": "380160"
  },
  {
    "text": "container and a pod in Linux we're going to build one up from scratch and focus on the parts important for routing a",
    "start": "380160",
    "end": "387660"
  },
  {
    "text": "packet so first we start off with a host uh",
    "start": "387660",
    "end": "393000"
  },
  {
    "text": "which we're referring to as a node and next we create a network namespace",
    "start": "393000",
    "end": "400199"
  },
  {
    "text": "on the host you'll recognize the red border box from the previous slides again this virtualizes the network",
    "start": "400199",
    "end": "407220"
  },
  {
    "text": "resources so that processes using this namespace only see those network",
    "start": "407220",
    "end": "412259"
  },
  {
    "text": "resources not the ones on the host or other network name spaces",
    "start": "412259",
    "end": "418440"
  },
  {
    "text": "upon creation we have the loopback interface for local traffic as well as a",
    "start": "418440",
    "end": "423960"
  },
  {
    "text": "virtualized eth0 interface for traffic into and out of the pod",
    "start": "423960",
    "end": "429240"
  },
  {
    "text": "additionally we have a private set of IP addresses a routing table socket listing",
    "start": "429240",
    "end": "436160"
  },
  {
    "text": "connection tracking table firewall all the network related resources",
    "start": "436160",
    "end": "444139"
  },
  {
    "text": "then each container we create is a process that shares this view of the network resources you'll recognize the",
    "start": "445020",
    "end": "452099"
  },
  {
    "text": "blue boxes from the previous slides",
    "start": "452099",
    "end": "456440"
  },
  {
    "text": "and finally we return to the fact that we can create multiple network name spaces on a single host and end up with",
    "start": "457979",
    "end": "464400"
  },
  {
    "text": "multiple pods on the Node I want to reinforce the idea that each network",
    "start": "464400",
    "end": "469440"
  },
  {
    "text": "namespace has its own view of network resources this is highlighted by the",
    "start": "469440",
    "end": "474599"
  },
  {
    "text": "fact that here we have four containers running two of which bind to port 8080",
    "start": "474599",
    "end": "480060"
  },
  {
    "text": "and two of which bind to Port 3306 these bindings don't conflict because they",
    "start": "480060",
    "end": "486660"
  },
  {
    "text": "take place on separate pods which means that they're in set separate network",
    "start": "486660",
    "end": "491699"
  },
  {
    "text": "name spaces um each pod is then also given and I've uh given an IP address that is reachable",
    "start": "491699",
    "end": "499319"
  },
  {
    "text": "from other pots so in the last few sides we've seen that",
    "start": "499319",
    "end": "506280"
  },
  {
    "text": "within each Network namespace I've been highlighting two specific Resources with green boxes iptables and sockets",
    "start": "506280",
    "end": "515700"
  },
  {
    "text": "um so why are our IP Tables important for the stock so well since IP tables",
    "start": "515700",
    "end": "521760"
  },
  {
    "text": "are unique to the network namespace we know that all containers within that",
    "start": "521760",
    "end": "526800"
  },
  {
    "text": "namespace observe the same IP tables configuration",
    "start": "526800",
    "end": "533060"
  },
  {
    "text": "so they are responsible for the redirection of certain packets to the sidecar proxy before arriving at an",
    "start": "533820",
    "end": "540300"
  },
  {
    "text": "application at an application container or leaving the pod so if a packet matches any of the configured rules they",
    "start": "540300",
    "end": "547860"
  },
  {
    "text": "reroute that packet so that it is redirected to the proxy container but",
    "start": "547860",
    "end": "553140"
  },
  {
    "text": "how are they actually doing this so here we get into the fact that every",
    "start": "553140",
    "end": "561779"
  },
  {
    "text": "IP packet has a header and we can see what that looks like here a packet header is the part of a packet",
    "start": "561779",
    "end": "570000"
  },
  {
    "text": "that precedes its body each row that we're looking at is a 32-bit word that",
    "start": "570000",
    "end": "575040"
  },
  {
    "text": "encodes all sorts of addressing information for that packet this can include things like the total length of",
    "start": "575040",
    "end": "581580"
  },
  {
    "text": "the header and data packet identifier and most importantly for this talk the",
    "start": "581580",
    "end": "587940"
  },
  {
    "text": "source and the destination I've highlighted the destination address with the red box because this is the",
    "start": "587940",
    "end": "595500"
  },
  {
    "text": "field and the packet header that IP tables is rewriting iptables has determined based off some",
    "start": "595500",
    "end": "601560"
  },
  {
    "text": "piece of information say in this case the destination that this packet matches a rule that it is responsible for",
    "start": "601560",
    "end": "608760"
  },
  {
    "text": "rewriting the destination to a different one the proxy in this case",
    "start": "608760",
    "end": "616100"
  },
  {
    "text": "so returning back to this picture we can see now that instead of going directly to the original destination inbound and",
    "start": "617640",
    "end": "625380"
  },
  {
    "text": "outbound traffic first passes through the proxy container um so inbound ends outbound traffic we",
    "start": "625380",
    "end": "633420"
  },
  {
    "text": "have to consider the fact that the proxy will have different Behavior depending on the direction of traffic so therefore",
    "start": "633420",
    "end": "641339"
  },
  {
    "text": "the proxy will actually bind um a separate port for each Direction",
    "start": "641339",
    "end": "647959"
  },
  {
    "text": "so here uh the proxy container is going to bind uh Port 4143 for inbound traffic",
    "start": "648959",
    "end": "656700"
  },
  {
    "text": "and Port 4140 for outbound traffic iptables will have separate rules that",
    "start": "656700",
    "end": "663240"
  },
  {
    "text": "rewrite the destination address fields to um to these ports depending on the",
    "start": "663240",
    "end": "669120"
  },
  {
    "text": "direction of the packets also note that these ports are not arbitrarily chosen these are the actual",
    "start": "669120",
    "end": "675240"
  },
  {
    "text": "ports that we use in thinker D for example",
    "start": "675240",
    "end": "679640"
  },
  {
    "text": "so this is great IP tables have done their job and the proxy container is now receiving traffic we have another issue",
    "start": "681480",
    "end": "689220"
  },
  {
    "text": "though remember that the packets destination addresses were Rewritten so",
    "start": "689220",
    "end": "694620"
  },
  {
    "text": "even though the proxy is now receiving its traffic it still needs to ensure",
    "start": "694620",
    "end": "699720"
  },
  {
    "text": "that it all ends up at the originally intended destinations",
    "start": "699720",
    "end": "705860"
  },
  {
    "text": "this is where the other network resource I've been highlighting in previous slides comes into play socket tables",
    "start": "705860",
    "end": "713160"
  },
  {
    "text": "so when a connection is opened we can examine the listing in socket tables",
    "start": "713160",
    "end": "718320"
  },
  {
    "text": "that corresponds to that connection of the things tracked in each listing the one that we care about here is the",
    "start": "718320",
    "end": "725160"
  },
  {
    "text": "original destination for that connection the proxy calls into lib C and the get",
    "start": "725160",
    "end": "732360"
  },
  {
    "text": "stock opt function and gets the original destination for that socket using that",
    "start": "732360",
    "end": "738060"
  },
  {
    "text": "we ensure that the traffic is going to the pre-ip tables destination",
    "start": "738060",
    "end": "744860"
  },
  {
    "text": "I've been talking about iptables having rules that match traffic the mesh care is about and ensuring that destinations",
    "start": "747120",
    "end": "753779"
  },
  {
    "text": "are Rewritten the presence of these rules are also a responsibility of the mesh and there are a few ways that these",
    "start": "753779",
    "end": "760680"
  },
  {
    "text": "can be added to the pod the most common way to handle this is",
    "start": "760680",
    "end": "766019"
  },
  {
    "text": "using an init container a knit container runs before any of the pods application",
    "start": "766019",
    "end": "771180"
  },
  {
    "text": "containers start the mesh is responsible for injecting this in the container similar to how",
    "start": "771180",
    "end": "777540"
  },
  {
    "text": "it's responsible for injecting the proxy container uh the unit container will run to",
    "start": "777540",
    "end": "783240"
  },
  {
    "text": "completion and add the necessary rules to IP tables for that pod",
    "start": "783240",
    "end": "789180"
  },
  {
    "text": "if we're just adding rules though why do we need a separate container so",
    "start": "789180",
    "end": "794760"
  },
  {
    "text": "rewriting IP tables requires elevated permissions a problem that Doug will cover in his side of the talk and we",
    "start": "794760",
    "end": "802500"
  },
  {
    "text": "don't want to give the long running proxy elevated permissions for something that really is only going to happen when",
    "start": "802500",
    "end": "808440"
  },
  {
    "text": "the container starts so in a knit container helps solve this by separating",
    "start": "808440",
    "end": "813480"
  },
  {
    "text": "the need for permissions between containers",
    "start": "813480",
    "end": "819120"
  },
  {
    "text": "so the init container is run and the application containers have started and now they observe the IP tables",
    "start": "819540",
    "end": "825839"
  },
  {
    "text": "configuration required for the service mesh so therefore traffic that we care about is redirected to the proxy",
    "start": "825839",
    "end": "833220"
  },
  {
    "text": "container so another way to solve this issue is to",
    "start": "833220",
    "end": "838620"
  },
  {
    "text": "use a cni plugin without getting into the details on this it ensures that each",
    "start": "838620",
    "end": "843779"
  },
  {
    "text": "pod has properly configured IP tables one of my co-workers Alex gave a talk",
    "start": "843779",
    "end": "849959"
  },
  {
    "text": "yesterday at service mesh con about how Linker D's cni plugin is implemented if",
    "start": "849959",
    "end": "855959"
  },
  {
    "text": "you're interested in that I would definitely recommend checking out the recording on that",
    "start": "855959",
    "end": "861079"
  },
  {
    "text": "awesome thanks Kevin now that we understand a bit more about how packets actually flow through a",
    "start": "862980",
    "end": "869220"
  },
  {
    "text": "service mesh I'm going to walk through how to debug a real TCP stream I encountered the need to capture TCP",
    "start": "869220",
    "end": "875459"
  },
  {
    "text": "packets when debugging an issue with our istio-based service mesh at Airbnb called air mesh specifically within our",
    "start": "875459",
    "end": "882420"
  },
  {
    "text": "custom metadata Exchange for this talk we're going to rely on a",
    "start": "882420",
    "end": "888600"
  },
  {
    "text": "few key assumptions note that they aren't requirements per se but they will make talking about and visualizing the",
    "start": "888600",
    "end": "894779"
  },
  {
    "text": "underlying networking model much simpler first is that you're using a cni",
    "start": "894779",
    "end": "900000"
  },
  {
    "text": "container networking interface such that each pod has unique IP address from the directly from the VPC that's routable",
    "start": "900000",
    "end": "906899"
  },
  {
    "text": "from other pods on the network for instance the AWS VPC cni",
    "start": "906899",
    "end": "912420"
  },
  {
    "text": "second we use we'll be using the cryo container runtime but all the commands shown commands shown today could easily",
    "start": "912420",
    "end": "918839"
  },
  {
    "text": "be modified for container d last and probably most importantly having SSH specifically root access to",
    "start": "918839",
    "end": "926160"
  },
  {
    "text": "the nodes we'll discuss how to avoid this requirement at the end of The Talk",
    "start": "926160",
    "end": "931760"
  },
  {
    "text": "the use case that brought about this debugging involved talking to Apache Kafka so I'd like to start with some",
    "start": "932760",
    "end": "938399"
  },
  {
    "text": "quick context about what Kafka is and a little bit about how it works for those who may be unfamiliar",
    "start": "938399",
    "end": "945300"
  },
  {
    "text": "Kafka is a distributed messaging Queue at a very basic level a client can either produce a message to a topic or",
    "start": "945300",
    "end": "952079"
  },
  {
    "text": "consume one internally Kafka consists of Brokers which are just instances of the service",
    "start": "952079",
    "end": "957920"
  },
  {
    "text": "topics which are like categories or feeds and partitions which are well you know",
    "start": "957920",
    "end": "963240"
  },
  {
    "text": "partitions of the data within a topic Kafka uses zookeeper to run weeder",
    "start": "963240",
    "end": "968579"
  },
  {
    "text": "election at the partition level so every partition will only have one reader shown here on the slide in green",
    "start": "968579",
    "end": "975839"
  },
  {
    "text": "when a producer wants to send a message to a topic they'll first compute the partition ID often used in a consistent",
    "start": "975839",
    "end": "981959"
  },
  {
    "text": "hash function then it will use its internal metadata of the cluster state to write that",
    "start": "981959",
    "end": "987480"
  },
  {
    "text": "message to a broker that contains the leader of that partition when a consumer consumes that message",
    "start": "987480",
    "end": "992880"
  },
  {
    "text": "it's actually pulling the Kafka broker for recent messages on that particular partition",
    "start": "992880",
    "end": "1000220"
  },
  {
    "text": "now that we know a bit more about Kafka we need to explain where our service mesh fits into the picture to help us",
    "start": "1001160",
    "end": "1007040"
  },
  {
    "text": "Trace some packets in order to send produce or consuming requests Kafka kafka's Client First",
    "start": "1007040",
    "end": "1013759"
  },
  {
    "text": "needs to discover the initial state of the world I.E all the Brokers topics and partitions",
    "start": "1013759",
    "end": "1019639"
  },
  {
    "text": "to do this it uses a special request called metadata request this request happens on client",
    "start": "1019639",
    "end": "1026360"
  },
  {
    "text": "initialization and can be routed to any broker in the cluster as they all share the same view of the world thanks to",
    "start": "1026360",
    "end": "1032418"
  },
  {
    "text": "zookeeper once the clients have this information it will then route requests to specific",
    "start": "1032419",
    "end": "1038120"
  },
  {
    "text": "Brokers on based on topics in our case we'll use our service mesh",
    "start": "1038120",
    "end": "1043280"
  },
  {
    "text": "for that initial metadata request because we don't actually care which broker responds to it and we just want",
    "start": "1043280",
    "end": "1048799"
  },
  {
    "text": "to Route it to a healthy node something that the service mesh is really good at",
    "start": "1048799",
    "end": "1053980"
  },
  {
    "text": "after that all subsequent publish or consume requests will go directly to the specific Brokers thus skipping the",
    "start": "1054260",
    "end": "1061100"
  },
  {
    "text": "service mesh entirely this is because kafka's client wants to be in control of exactly which broker",
    "start": "1061100",
    "end": "1066620"
  },
  {
    "text": "it's talking to as it knows the internal mapping of partitions to topics",
    "start": "1066620",
    "end": "1072340"
  },
  {
    "text": "the issue we encountered was during this initial metadata request so in order to reproduce the issue let's use Kafka CAD",
    "start": "1074600",
    "end": "1082340"
  },
  {
    "text": "fukat is a CLI used to talk to Kafka and is incredibly useful when you don't want to spin up a heavyweight jvm just to",
    "start": "1082340",
    "end": "1089539"
  },
  {
    "text": "check the state of the cluster here we'll use clue control exec to",
    "start": "1089539",
    "end": "1094580"
  },
  {
    "text": "exactly to run a command in a pod in this case a pod called pod test against our app container",
    "start": "1094580",
    "end": "1101900"
  },
  {
    "text": "we'll specify the following arguments to Kafka cat minus L for metadata listing and then minus B to specify the broker",
    "start": "1101900",
    "end": "1109340"
  },
  {
    "text": "we want to talk to in this case it'll be the address of our istio service or our service mesh",
    "start": "1109340",
    "end": "1116539"
  },
  {
    "text": "service called Kafka dot service on Port 1992 which is kafka's default TCP USM",
    "start": "1116539",
    "end": "1122539"
  },
  {
    "text": "support and we get this error right fatal error at some line of metadata West",
    "start": "1122539",
    "end": "1130400"
  },
  {
    "text": "broker transport failure what does that mean well after a bunch of Googling and lots of code splunking through kafka's",
    "start": "1130400",
    "end": "1137120"
  },
  {
    "text": "code base all I can find out is that the request is malformed",
    "start": "1137120",
    "end": "1142280"
  },
  {
    "text": "but what does that tell me isn't our service mesh supposed to give us all this rich observability",
    "start": "1142280",
    "end": "1148760"
  },
  {
    "text": "well yes and no for HTTP the Telemetry emitted by service mesh is incredibly useful",
    "start": "1148760",
    "end": "1155240"
  },
  {
    "text": "we have response codes logs and even response Flags in the case of Envoy",
    "start": "1155240",
    "end": "1161720"
  },
  {
    "text": "as shown in this slide there's 25 unique flags for HTTP requests that are emitted",
    "start": "1161720",
    "end": "1167120"
  },
  {
    "text": "in the logs when things go wrong but for TCP connections we only have seven flags that are extremely generic",
    "start": "1167120",
    "end": "1175340"
  },
  {
    "text": "without response codes or detailed response Flags it can be really difficult to find the exact cause of",
    "start": "1175340",
    "end": "1181520"
  },
  {
    "text": "some of these issues so what do we do we break out our favorite tool TCP dump",
    "start": "1181520",
    "end": "1188840"
  },
  {
    "text": "and then look at Power captures and Wireshark but how do we do this in the context of kubernetes specifically kubernetes with",
    "start": "1188840",
    "end": "1196280"
  },
  {
    "text": "service mesh we're going to take a look at two",
    "start": "1196280",
    "end": "1202039"
  },
  {
    "text": "different packet captures the first one is what the client Kafka cut is actually seeing",
    "start": "1202039",
    "end": "1208039"
  },
  {
    "text": "and the second one is what the proxy is seeing as Kevin explained earlier all the",
    "start": "1208039",
    "end": "1214280"
  },
  {
    "text": "containers in a pod share a same single Network namespace so they used IP tables to rewrite packets",
    "start": "1214280",
    "end": "1221120"
  },
  {
    "text": "destined for mesh services to the proxy's inbound port on the loopback interface",
    "start": "1221120",
    "end": "1226520"
  },
  {
    "text": "so to capture what coffeecod is actually seeing we can just TCP dump on that",
    "start": "1226520",
    "end": "1231559"
  },
  {
    "text": "loopback interface once we have that capture we can take a second one this time to see what the",
    "start": "1231559",
    "end": "1238220"
  },
  {
    "text": "sidecar proxy sees by Ronnie TCP dump against the host on a virtual interface for the pod",
    "start": "1238220",
    "end": "1245360"
  },
  {
    "text": "we'll talk more about that later let's start out simple we'll just run",
    "start": "1245360",
    "end": "1250460"
  },
  {
    "text": "tsp dump in the main container we'll run Cube control and stack against our pod same one pod test and we'll run it",
    "start": "1250460",
    "end": "1257419"
  },
  {
    "text": "against our application container in this case app and we got another error executable file",
    "start": "1257419",
    "end": "1263419"
  },
  {
    "text": "not found in path well that's obvious right we don't include PCP dump in our container images",
    "start": "1263419",
    "end": "1268880"
  },
  {
    "text": "we did this for a variety of reasons hopefully these are kind of obvious but things like we run distros images we'd",
    "start": "1268880",
    "end": "1275299"
  },
  {
    "text": "want a minimal security footprint and in general we don't include debugging Utilities in our images",
    "start": "1275299",
    "end": "1282080"
  },
  {
    "text": "so that we can reduce our security surface so let's just install it right",
    "start": "1282080",
    "end": "1288020"
  },
  {
    "text": "if it's a VM just app update app to install and call it a day but it's not that simple doing this in",
    "start": "1288020",
    "end": "1294919"
  },
  {
    "text": "kubernetes at runtime in the container would require both root and a package manager so it's probably just easier to",
    "start": "1294919",
    "end": "1302059"
  },
  {
    "text": "bake it into the image at build time in order to print in extra dependencies",
    "start": "1302059",
    "end": "1308480"
  },
  {
    "text": "to our main container image we've gone ahead and modified our deployments back to run an additional sidecar container",
    "start": "1308480",
    "end": "1315140"
  },
  {
    "text": "we use the net shoot image which already has t-speedump installed as a reminder all containers share that",
    "start": "1315140",
    "end": "1321799"
  },
  {
    "text": "single Network namespace so we can just run gcp dump in any of those sidecar containers to capture traffic on the",
    "start": "1321799",
    "end": "1327559"
  },
  {
    "text": "woodback interface and observe everything that's going on in the pod",
    "start": "1327559",
    "end": "1332780"
  },
  {
    "text": "but again it still doesn't work you don't have permission to capture on that device what gives",
    "start": "1332780",
    "end": "1340360"
  },
  {
    "text": "turns out it's because we're running our containers as non-root users using non-privileged containers is",
    "start": "1341840",
    "end": "1347960"
  },
  {
    "text": "becoming more and more common and to be clear this is a good thing but how do we capture on an interface",
    "start": "1347960",
    "end": "1354980"
  },
  {
    "text": "without it we have two options we could add net",
    "start": "1354980",
    "end": "1360080"
  },
  {
    "text": "admin and net raw security capabilities to our pod security context but that would be less secure and",
    "start": "1360080",
    "end": "1365960"
  },
  {
    "text": "doesn't really help with the just-in-time access model if we only need temporary access this",
    "start": "1365960",
    "end": "1371240"
  },
  {
    "text": "would be too heavy-handed alternatively if we have or can get",
    "start": "1371240",
    "end": "1376820"
  },
  {
    "text": "pseudo a stage access to the nodes we can take a packet capture directly from there so let's go do that",
    "start": "1376820",
    "end": "1384700"
  },
  {
    "text": "okay well how do we actually do that many of you folks know how to do this but as a reminder we first are going to",
    "start": "1384799",
    "end": "1390679"
  },
  {
    "text": "get the node name from the Pod object we'll run Coupe control get pod and specify this Json path to get the",
    "start": "1390679",
    "end": "1397820"
  },
  {
    "text": "node name in this case the Pod that we're trying to look at is running on node one",
    "start": "1397820",
    "end": "1403039"
  },
  {
    "text": "okay now that we know the node's name let's get its IP address we use Cube control get node with Node 1",
    "start": "1403039",
    "end": "1410600"
  },
  {
    "text": "and then this Json path to get its internal IP address and note that in our situation we want",
    "start": "1410600",
    "end": "1417200"
  },
  {
    "text": "to get the internal IP address because it's accessible on our corporate VPN as opposed to say maybe a public IP address",
    "start": "1417200",
    "end": "1423799"
  },
  {
    "text": "which may not be while we're here oh sorry and the node is running internally on our 192.168 1.9",
    "start": "1423799",
    "end": "1431840"
  },
  {
    "text": "Network so let's use that while we're here we're also going to",
    "start": "1431840",
    "end": "1436940"
  },
  {
    "text": "grab the container ID and I'll explain why we need that in a second this can be any container in the Pod but",
    "start": "1436940",
    "end": "1442340"
  },
  {
    "text": "since we know we have our sidecar proxy injected we'll just use that one it's all around Cube control get pod and",
    "start": "1442340",
    "end": "1448640"
  },
  {
    "text": "pass this to JQ look at the container statuses and finally select the one",
    "start": "1448640",
    "end": "1453860"
  },
  {
    "text": "whose name is proxy and then get its container ID it's kind of a on command",
    "start": "1453860",
    "end": "1459919"
  },
  {
    "text": "and we see our container ID is cryo colon slash 94ad dot dot which makes sense right",
    "start": "1459919",
    "end": "1466880"
  },
  {
    "text": "we're using cry then lastly what's a stage to that internal IP",
    "start": "1466880",
    "end": "1473620"
  },
  {
    "text": "awesome as Kevin mentioned earlier each pod has unique net NS so in order to run TCP",
    "start": "1474440",
    "end": "1481820"
  },
  {
    "text": "dump within it we need to First find it now that we're on the Node and have the",
    "start": "1481820",
    "end": "1487039"
  },
  {
    "text": "container ID we can get the network namespace by using CRI CTL which is the co I used to talk to the cryo container",
    "start": "1487039",
    "end": "1494179"
  },
  {
    "text": "runtime so we'll run cryo-inspect on the container ID and specifically select the namespace of",
    "start": "1494179",
    "end": "1501679"
  },
  {
    "text": "type Network's path using JQ note this namespaces slice has all the namespaces",
    "start": "1501679",
    "end": "1508039"
  },
  {
    "text": "not just Network right the mount the PID the user Etc but we'll get the network name space in this case it looks like",
    "start": "1508039",
    "end": "1515059"
  },
  {
    "text": "VAR run.ns and then 149 someone good",
    "start": "1515059",
    "end": "1520960"
  },
  {
    "text": "now that we have this network name space let's run a quick sanity check if we run ifconfig from inside this",
    "start": "1522679",
    "end": "1529400"
  },
  {
    "text": "containers Network namespace it should see the same IP as the pods IP right because ultimately it is the pod",
    "start": "1529400",
    "end": "1536779"
  },
  {
    "text": "so our pod is running on 100.116.95.102 so",
    "start": "1536779",
    "end": "1543020"
  },
  {
    "text": "we'll see if we can see that same IP address to run a process inside a particular",
    "start": "1543020",
    "end": "1548059"
  },
  {
    "text": "name space of any type not just Network we can use the NS enter command",
    "start": "1548059",
    "end": "1553520"
  },
  {
    "text": "so here we'll run NS enter with the minus minus net argument and specify our Network namespace that",
    "start": "1553520",
    "end": "1560659"
  },
  {
    "text": "VAR run NS and then some on good and look if we look at the output we see",
    "start": "1560659",
    "end": "1566539"
  },
  {
    "text": "the inet address of 100.116.95.102 which is our podside IP",
    "start": "1566539",
    "end": "1572000"
  },
  {
    "text": "address so we know we're on the right path",
    "start": "1572000",
    "end": "1576760"
  },
  {
    "text": "now that we've found and verified the network name space we can finally run key speed up the moment we've always",
    "start": "1577640",
    "end": "1583279"
  },
  {
    "text": "been waiting for again we'll use nsenter to enter the network namespace and run TCP dump with",
    "start": "1583279",
    "end": "1589580"
  },
  {
    "text": "the following options minus I for loopback because again we want to capture traffic between the",
    "start": "1589580",
    "end": "1596059"
  },
  {
    "text": "application container Kafka cut and the proxy we'll use s0 to capture all packet",
    "start": "1596059",
    "end": "1602059"
  },
  {
    "text": "sizes we'll use nn to not resolve for or host names and",
    "start": "1602059",
    "end": "1607460"
  },
  {
    "text": "lastly we'll use minus W capture.p cap to write it to a file pretty standard",
    "start": "1607460",
    "end": "1614080"
  },
  {
    "text": "we can actually open a Wireshark so here it is and we're gonna look for something very",
    "start": "1615700",
    "end": "1620840"
  },
  {
    "text": "specific we want to make sure we observed this metadata Exchange I don't have time to go into the full",
    "start": "1620840",
    "end": "1627200"
  },
  {
    "text": "wire protocol of Kafka nor do I fully understand it but if we look at the first packet after the standard TCP",
    "start": "1627200",
    "end": "1633380"
  },
  {
    "text": "handshake number 137 we can see the client ID in the body's plain text",
    "start": "1633380",
    "end": "1639500"
  },
  {
    "text": "Rd Kafka this is the default client ID set by Liberty Kafka which is the underlying",
    "start": "1639500",
    "end": "1645380"
  },
  {
    "text": "library that kafkad is using note that the reason we can actually see this body in plain text is because the",
    "start": "1645380",
    "end": "1653000"
  },
  {
    "text": "capture is taken between the proxy and the application next we'll look at capturing data from",
    "start": "1653000",
    "end": "1659840"
  },
  {
    "text": "the external side of the proxy so if we want to observe the external",
    "start": "1659840",
    "end": "1665720"
  },
  {
    "text": "traffic which is encrypted between pods we can use excuse me we need to know which",
    "start": "1665720",
    "end": "1671960"
  },
  {
    "text": "interface to listen on if we want to avoid getting hit by a fire hose of packets I.E we could capture everything",
    "start": "1671960",
    "end": "1678140"
  },
  {
    "text": "on the host but it would be pretty hard to find the signal in the noise since we're using the VPC cni there is a",
    "start": "1678140",
    "end": "1685940"
  },
  {
    "text": "route on the host's routing table for each pod IP to the virtual interface created for that pod",
    "start": "1685940",
    "end": "1692179"
  },
  {
    "text": "so we can use iprout show to look at the host's routing table and just grab for the Pod IP",
    "start": "1692179",
    "end": "1698360"
  },
  {
    "text": "as we can see the route for R pods IP address the one ending in dot 102 is",
    "start": "1698360",
    "end": "1703820"
  },
  {
    "text": "using the virtual interface eni blah blah blah and in097",
    "start": "1703820",
    "end": "1710360"
  },
  {
    "text": "using that we can finally run TCP dump on the host to capture traffic outside the proxy",
    "start": "1710419",
    "end": "1716720"
  },
  {
    "text": "this is again traffic that's transiting the pod's boundary we use the same tsp dump command as",
    "start": "1716720",
    "end": "1722659"
  },
  {
    "text": "before except this time we'll listen on this interface instead of loopback and we won't run it inside the network",
    "start": "1722659",
    "end": "1729020"
  },
  {
    "text": "namespace so we run just vanilla tsp dump minus I",
    "start": "1729020",
    "end": "1734480"
  },
  {
    "text": "with the eni the virtual interface ending in zero nine seven and this other",
    "start": "1734480",
    "end": "1739700"
  },
  {
    "text": "the rest of the options are the same as before as you can see we've captured 579",
    "start": "1739700",
    "end": "1745159"
  },
  {
    "text": "packets I won't open this pcap up now as the traffic is encrypted using the proxies",
    "start": "1745159",
    "end": "1750500"
  },
  {
    "text": "TOs certificate and won't be very useful to us I'll leave it as an exercise for the attendee to figure out how to get the",
    "start": "1750500",
    "end": "1757100"
  },
  {
    "text": "certificate needed to decrypt this traffic spoiler you can use the debug Port of",
    "start": "1757100",
    "end": "1763100"
  },
  {
    "text": "your proxy after going through all these hoops and hurdles just to take a simple packet",
    "start": "1763100",
    "end": "1769580"
  },
  {
    "text": "capture there must be a better way and the good news is there is ephemeral containers are a relatively",
    "start": "1769580",
    "end": "1775520"
  },
  {
    "text": "new feature of kubernetes that allow launching a new container that shares some of the Linux namespaces",
    "start": "1775520",
    "end": "1781700"
  },
  {
    "text": "of the running pod in our case we want to share the network namespace which is done by default",
    "start": "1781700",
    "end": "1788240"
  },
  {
    "text": "there is one major limitation however they currently do not support changing the security context of a running pod so",
    "start": "1788240",
    "end": "1795620"
  },
  {
    "text": "if you launch a pod with a restrictive security policy that prevents net raw or",
    "start": "1795620",
    "end": "1801080"
  },
  {
    "text": "net admin you still can't use ephemeral containers to run tsp dump",
    "start": "1801080",
    "end": "1806919"
  },
  {
    "text": "but for argument's sake let's say we don't have that restrictive security policy how would we use ephemeral",
    "start": "1807200",
    "end": "1813080"
  },
  {
    "text": "containers assuming that security context allowed it well the coupe controlled debug command",
    "start": "1813080",
    "end": "1818360"
  },
  {
    "text": "will attach a new container to the running pod using the provided image so we can just run Coupe control debug",
    "start": "1818360",
    "end": "1825200"
  },
  {
    "text": "pod against start pod again pod test and pass in a custom image in this case",
    "start": "1825200",
    "end": "1830240"
  },
  {
    "text": "we'll use net shoot again as we know it has TCP dump installed same arguing supplies before and we can",
    "start": "1830240",
    "end": "1836480"
  },
  {
    "text": "capture packets okay so ephemeral containers were cool and all but it doesn't help us get",
    "start": "1836480",
    "end": "1842779"
  },
  {
    "text": "around this core issue of not being able to set a different security context within an ephemeral container",
    "start": "1842779",
    "end": "1848360"
  },
  {
    "text": "I have good news I was kind of lying before this issue is actually just recently been fixed upstream and will be",
    "start": "1848360",
    "end": "1854240"
  },
  {
    "text": "coming to a coupe control debug version Coupe control debug command in a future version",
    "start": "1854240",
    "end": "1860539"
  },
  {
    "text": "going forward I hope that we can integrate ephemeral containers into other open source tools like caseniff a",
    "start": "1860539",
    "end": "1867020"
  },
  {
    "text": "coupe control plugin that runs TCP dump and use a Wireshark to capture remote to start remote captures on pods",
    "start": "1867020",
    "end": "1874840"
  },
  {
    "text": "alright so for a quick recap cool uh thanks yeah so Linux doesn't",
    "start": "1875600",
    "end": "1881299"
  },
  {
    "text": "have containers it has namespaces these isolate certain resources from each",
    "start": "1881299",
    "end": "1886880"
  },
  {
    "text": "other the network namespace isolates the network resources so when multiple",
    "start": "1886880",
    "end": "1893240"
  },
  {
    "text": "containers run in a pod they all observe the same network resources",
    "start": "1893240",
    "end": "1898520"
  },
  {
    "text": "i p tables rewrite the packet headers in order for traffic to be redirected to",
    "start": "1898520",
    "end": "1904700"
  },
  {
    "text": "the proxy iptables we'll rewrite the destination part of the packet header",
    "start": "1904700",
    "end": "1910820"
  },
  {
    "text": "and then the proxy looks at the socket table since the destinations on the",
    "start": "1910820",
    "end": "1916640"
  },
  {
    "text": "packets were Rewritten by iptables the proxy needs to ensure that the original address is used so we can look that up",
    "start": "1916640",
    "end": "1924919"
  },
  {
    "text": "in the socket table which has a corresponding entry for the connection",
    "start": "1924919",
    "end": "1930879"
  },
  {
    "text": "and on the debugging side TCP observability is extremely limited outside of things like response box",
    "start": "1930919",
    "end": "1938179"
  },
  {
    "text": "to capture traffic within a pod you can use nsenter on the host and run TCP dump on woodback",
    "start": "1938179",
    "end": "1944539"
  },
  {
    "text": "to capture traffic leaving the Pod you can run TCP on the host tsp dump on the host against this virtual interface for",
    "start": "1944539",
    "end": "1950899"
  },
  {
    "text": "the pod and lastly ephemeral containers are here to save us from all this friction",
    "start": "1950899",
    "end": "1957639"
  },
  {
    "text": "if this work is interesting to you or you're passionate about kubernetes or service meshes at Airbnb we're actively hiring please",
    "start": "1958279",
    "end": "1965360"
  },
  {
    "text": "go to airbnb.com careers to learn more cool and yeah if you enjoyed the",
    "start": "1965360",
    "end": "1971840"
  },
  {
    "text": "presentation uh buoyant runs a service mesh Academy you can sign up for these are monthly Hands-On workshops for uh",
    "start": "1971840",
    "end": "1977899"
  },
  {
    "text": "real world production users and it is free and we also work on a service mesh as a",
    "start": "1977899",
    "end": "1984679"
  },
  {
    "text": "service product that manages linkready for you on your own clusters you can book a demo or come find us at the",
    "start": "1984679",
    "end": "1991520"
  },
  {
    "text": "buoyant Booth um yeah would love to talk wow that was a great session if you",
    "start": "1991520",
    "end": "1998179"
  },
  {
    "text": "learned something yes subscribe to your hands thank you thank you",
    "start": "1998179",
    "end": "2003658"
  }
]