[
  {
    "start": "0",
    "end": "80000"
  },
  {
    "text": "all right hello everybody I'm gonna start 30 seconds early because I have",
    "start": "0",
    "end": "5910"
  },
  {
    "text": "the microphone and I can so I'm gonna my name is Laura Frank I am the director of",
    "start": "5910",
    "end": "11519"
  },
  {
    "text": "engineering at cloudBees formerly coach ship we just got acquired about three",
    "start": "11519",
    "end": "16590"
  },
  {
    "text": "months ago so we're very happy to be joining up with a bunch of other people trying to solve DevOps problems however",
    "start": "16590",
    "end": "22289"
  },
  {
    "text": "I'm not here to talk about CI or CV or anything like that I'm here to talk",
    "start": "22289",
    "end": "28349"
  },
  {
    "text": "about distributed consensus which is a really scary thing but I'm gonna try to",
    "start": "28349",
    "end": "33450"
  },
  {
    "text": "make it a little bit less scary for you I'm gonna go over a couple main topics",
    "start": "33450",
    "end": "38820"
  },
  {
    "text": "today first is demystifying this very intimidating phrase called distributed consensus we're gonna talk specifically",
    "start": "38820",
    "end": "45690"
  },
  {
    "text": "about the raft consensus algorithm what it is where you find it why it's there we will talk about quorum which is",
    "start": "45690",
    "end": "52199"
  },
  {
    "text": "essential to understand and if you're operating any system that depends on distributed consensus we'll talk about",
    "start": "52199",
    "end": "57600"
  },
  {
    "text": "leader election and log replication and then just maybe at the very end I will",
    "start": "57600",
    "end": "62850"
  },
  {
    "text": "break a cluster because it's my favorite thing to do so I'm not necessarily showing best practices if you're",
    "start": "62850",
    "end": "69510"
  },
  {
    "text": "operating a cluster in production that's not really the point of this talk I'm here to give you a little bit more theoretical background for the sort of",
    "start": "69510",
    "end": "76890"
  },
  {
    "text": "theoretical underpinnings of what makes the kubernetes world go round so let's get started first with",
    "start": "76890",
    "end": "83280"
  },
  {
    "start": "80000",
    "end": "205000"
  },
  {
    "text": "distributed consensus and I really like saying this term or like bringing it up at a party I'm real fun at parties",
    "start": "83280",
    "end": "90000"
  },
  {
    "text": "it sounds really fancy but what it means is just getting more than one party to agree on something and that those",
    "start": "90000",
    "end": "96360"
  },
  {
    "text": "parties are not all located in the same place so we're really focusing on agreement and the reason that we have to",
    "start": "96360",
    "end": "102899"
  },
  {
    "text": "do that is because we've sort of made it our own problem her we've sort of created the problem for ourselves so we",
    "start": "102899",
    "end": "108479"
  },
  {
    "text": "used to not really have to worry about distributed consensus because the world was really easy we had a client that maybe made a request to a server to",
    "start": "108479",
    "end": "115320"
  },
  {
    "text": "update a value and the server kind of knew what the values were and said yep cool I'm gonna append that value and",
    "start": "115320",
    "end": "121649"
  },
  {
    "text": "then my new value is seven for example we use tools like kubernetes though",
    "start": "121649",
    "end": "127049"
  },
  {
    "text": "because we don't want that model anymore we don't want there to be a single point of failure just one database",
    "start": "127049",
    "end": "133420"
  },
  {
    "text": "we want redundancy and fault tolerance and that means having multiple entities in a lot of cases that means having a",
    "start": "133420",
    "end": "140020"
  },
  {
    "text": "management group or consensus group that is comprised of more than one machine so",
    "start": "140020",
    "end": "145450"
  },
  {
    "text": "we have to get all of those machines to agree on the state before we can take an action on our cluster so in this",
    "start": "145450",
    "end": "152800"
  },
  {
    "text": "distributed model here's a visualization so we have a client making a request but then that request needs to be decided",
    "start": "152800",
    "end": "159250"
  },
  {
    "text": "upon by a manager group and in a manager group there's always a leader that's the the node that's taking in the requests",
    "start": "159250",
    "end": "165910"
  },
  {
    "text": "and then there are followers who are participating in that consensus group but aren't necessarily in control of making making the final decisions so",
    "start": "165910",
    "end": "173950"
  },
  {
    "text": "they'll get requests to say hey from the from the leader like hey can this happen is this is this possible to happen and",
    "start": "173950",
    "end": "180640"
  },
  {
    "text": "those followers can respond yep thumbs up let's do it and then the leader can confirm it but there's different roles",
    "start": "180640",
    "end": "186610"
  },
  {
    "text": "and we'll talk about those roles in a management group as as the talk progresses so the problem with clusters",
    "start": "186610",
    "end": "193930"
  },
  {
    "text": "is that having n machines agree on a single thing or having n anything whether it's people or cats perhaps",
    "start": "193930",
    "end": "199630"
  },
  {
    "text": "agree on a single thing is really hard and that's where Raph comes in and kind of does the heavy lifting for us so what",
    "start": "199630",
    "end": "206830"
  },
  {
    "start": "205000",
    "end": "296000"
  },
  {
    "text": "is raft this has been I think one of those underground parts of our system that we didn't really talk about much",
    "start": "206830",
    "end": "212560"
  },
  {
    "text": "but in the last couple of years it's gotten a lot more popular and a lot more visibility which makes me happy because",
    "start": "212560",
    "end": "218170"
  },
  {
    "text": "I find it very interesting raft is an algorithm to manage consensus based",
    "start": "218170",
    "end": "223420"
  },
  {
    "text": "systems such as container orchestrators it can be certainly used in other kinds of applications or other other ways as",
    "start": "223420",
    "end": "229060"
  },
  {
    "text": "well I would also encourage you not to get too intimidated maybe by the word algorithm in raft because it is a",
    "start": "229060",
    "end": "236980"
  },
  {
    "text": "complex system and it's just a system of patterns and I think it can be really accessible it just sounds a little bit",
    "start": "236980",
    "end": "243220"
  },
  {
    "text": "intimidating which hopefully you'll you'll find out by the end of this talk that it's easier to understand it's",
    "start": "243220",
    "end": "249700"
  },
  {
    "text": "designed to be easier to understand so if you are from a traditional CS background you might have studied paxos",
    "start": "249700",
    "end": "255190"
  },
  {
    "text": "or multi Paxos that was a bad time in in my in my study so the laughter tells me",
    "start": "255190",
    "end": "263919"
  },
  {
    "text": "it was probably a bad time for you as well raft sort of came out as like the opposite of that like hey this was really painful",
    "start": "263919",
    "end": "270280"
  },
  {
    "text": "why don't we figure out a way to make this not only perform as well but also a little bit easier to understand so that",
    "start": "270280",
    "end": "276340"
  },
  {
    "text": "the people who are responsible for operating systems that use this algorithm can actually you know understand what's going on and be able",
    "start": "276340",
    "end": "282580"
  },
  {
    "text": "to kind of process it at a human level it also has a cute logo which I just found out about yesterday I didn't know",
    "start": "282580",
    "end": "287770"
  },
  {
    "text": "I had a logo but there it is I guess it's named Annie at least that's what the PNG file was called so I'm",
    "start": "287770",
    "end": "294610"
  },
  {
    "text": "gonna go with it and either raft so paxos multipack sews all of these",
    "start": "294610",
    "end": "300790"
  },
  {
    "start": "296000",
    "end": "382000"
  },
  {
    "text": "distributed consensus algorithms are really painful and they're very complex and and have so much risk in so many",
    "start": "300790",
    "end": "306910"
  },
  {
    "text": "edge cases and they're dealing with lots of responsibilities that no sensible person wants to write their own except",
    "start": "306910",
    "end": "313060"
  },
  {
    "text": "the authors of raft but they had good reason to and because this is such a kind of a solved problem algorithms or",
    "start": "313060",
    "end": "321250"
  },
  {
    "text": "systems like graph do have a lot of popularity because there's no reason to implement it yourself it's also it's like really hard to implement yourself",
    "start": "321250",
    "end": "327130"
  },
  {
    "text": "and for that reason raft is used in a lot of places so orchestration systems I",
    "start": "327130",
    "end": "332890"
  },
  {
    "text": "think are the most visible way or most visible place that that raft is used so kubernetes using SCD and then by",
    "start": "332890",
    "end": "339790"
  },
  {
    "text": "extension every single system that uses @cd in any way shape or form is using rafts because raft is embedded inside of",
    "start": "339790",
    "end": "345850"
  },
  {
    "text": "@cd docker uses raft it's embedded inside of docker itself dr. doesn't have a reliance a net CD for",
    "start": "345850",
    "end": "353140"
  },
  {
    "text": "for their orchestration nomad has a similar approach where raft is kind of baked in the internals if you are",
    "start": "353140",
    "end": "360250"
  },
  {
    "text": "familiar with zookeeper zookeeper uses something slightly different from raft called the zookeeper atomic broadcast",
    "start": "360250",
    "end": "365410"
  },
  {
    "text": "which is sab which is fun to say it's similar to raft has a lot of the same responsibilities a lot of the same",
    "start": "365410",
    "end": "371320"
  },
  {
    "text": "patterns just it's not raft but luckily the patterns are very similar so if you're using zookeeper zookeeper in some",
    "start": "371320",
    "end": "379120"
  },
  {
    "text": "way you can kind of translate what we'll talk about today onto that raft has",
    "start": "379120",
    "end": "384820"
  },
  {
    "start": "382000",
    "end": "493000"
  },
  {
    "text": "three main responsibilities wherever it's being used the first one and probably the most important one is log",
    "start": "384820",
    "end": "391479"
  },
  {
    "text": "replication raft is there to maintain a consistent state across your distributed",
    "start": "391479",
    "end": "396700"
  },
  {
    "text": "node that's participated nodes that are participating in that consensus group and then by way of that",
    "start": "396700",
    "end": "403110"
  },
  {
    "text": "consensus group to your entire cluster we'll talk about each of these in more depth as kind of the main meat of the",
    "start": "403110",
    "end": "411450"
  },
  {
    "text": "talk leader election is another thing so as I said before inside of any consensus",
    "start": "411450",
    "end": "417060"
  },
  {
    "text": "group there's a leader that's taking in requests and then asking basically for permission or asking the the followers",
    "start": "417060",
    "end": "424770"
  },
  {
    "text": "if that action is allowed to take place so there has to be a leader if that leader goes offline or is unreachable",
    "start": "424770",
    "end": "431730"
  },
  {
    "text": "then we have to elect another leader in order for the world to keep going around safety is another responsibility of raft",
    "start": "431730",
    "end": "439020"
  },
  {
    "text": "and I'm not gonna go into depth on this because it sort of baked into both log replication and leader election safety",
    "start": "439020",
    "end": "445380"
  },
  {
    "text": "is the sense that any action that is happening in the cluster should be allowed to happen and raft is designed",
    "start": "445380",
    "end": "451950"
  },
  {
    "text": "to make actions that shouldn't be allowed to happen impossible to occur and that's sort of",
    "start": "451950",
    "end": "458220"
  },
  {
    "text": "baked into both log replication and leader election so I won't go deeply into safety today but that was one of",
    "start": "458220",
    "end": "464430"
  },
  {
    "text": "the design goals of raft I think one of the other big things that raft is",
    "start": "464430",
    "end": "469770"
  },
  {
    "text": "responsible for is being easier to understand as we are using tools like kubernetes more in production and we",
    "start": "469770",
    "end": "475740"
  },
  {
    "text": "have people responsible for operating them if something happens with your management group are your consensus",
    "start": "475740",
    "end": "481830"
  },
  {
    "text": "group if that you lose form or something goes wrong it's not fun and I think it's",
    "start": "481830",
    "end": "488070"
  },
  {
    "text": "always better to be able to really understand what's going on so you can troubleshoot and that's one of my goals also for this talk is to give you some",
    "start": "488070",
    "end": "493950"
  },
  {
    "start": "493000",
    "end": "705000"
  },
  {
    "text": "more fundamental knowledge so the first thing that we'll talk about in terms of",
    "start": "493950",
    "end": "499080"
  },
  {
    "text": "fundamental RAF knowledge is quorum and this is a buzzword maybe it's fun to say",
    "start": "499080",
    "end": "506190"
  },
  {
    "text": "I think quorum is not a term unique to computer science or orchestration systems or at CD or raft or anything",
    "start": "506190",
    "end": "512760"
  },
  {
    "text": "this is a borrowed term it is just the minimum number of votes needed to perform an operation I think usually we",
    "start": "512760",
    "end": "518310"
  },
  {
    "text": "hear this term in government depending on what country you're from sometimes the groups of elected",
    "start": "518310",
    "end": "524550"
  },
  {
    "text": "officials need quorum in order to conduct a vote and this is the same idea it's just a borrowed concept without",
    "start": "524550",
    "end": "530910"
  },
  {
    "text": "quorum nothing can happen because the system is not allowed to do work and I think we have a general idea",
    "start": "530910",
    "end": "539690"
  },
  {
    "text": "that quorum means majority and it doesn't just apply to nodes being online",
    "start": "539690",
    "end": "545089"
  },
  {
    "text": "they have to actually agree and vote for the same thing in order for that action to take place so majority is a good way",
    "start": "545089",
    "end": "552980"
  },
  {
    "text": "to think about quorum it's also a much easier word to understand and to spell and to pronounce but it's it's I guess",
    "start": "552980",
    "end": "559779"
  },
  {
    "text": "slightly different quorum is specifically defined if n is the number",
    "start": "559779",
    "end": "565430"
  },
  {
    "text": "of nodes in your cluster and divided by two plus one so the most important thing about that is distinctly and precisely",
    "start": "565430",
    "end": "572630"
  },
  {
    "text": "more than 50% 50% does not constitute quorum it has to be some value above 50%",
    "start": "572630",
    "end": "578800"
  },
  {
    "text": "and thankfully because this is just an easy mathematical expression we can fill",
    "start": "578800",
    "end": "585320"
  },
  {
    "text": "out a chart and you've probably seen a chart like this if you are operating at CD or another system with a consensus",
    "start": "585320",
    "end": "591800"
  },
  {
    "text": "group this is a pretty common thing to say okay how many nodes should I have in my consensus group in that cluster if",
    "start": "591800",
    "end": "597740"
  },
  {
    "text": "you have one node in here I've labeled those Raft consensus group participants",
    "start": "597740",
    "end": "604130"
  },
  {
    "text": "managers that's a common term to describe them if you have one note obviously quorum is one if that node",
    "start": "604130",
    "end": "609470"
  },
  {
    "text": "goes away then you can't do any more work on your cluster if you have two",
    "start": "609470",
    "end": "615680"
  },
  {
    "text": "nodes quorum is 2 because 2 divided by 1 or sorry 2 divided by 2 is 1 plus 1 is 2",
    "start": "615680",
    "end": "622610"
  },
  {
    "text": "so it's really not getting you anything extra but when we go to 3 or 5 or 7 we",
    "start": "622610",
    "end": "627770"
  },
  {
    "text": "can start to see that there's a pattern that the the odd numbers seem to be more",
    "start": "627770",
    "end": "633350"
  },
  {
    "text": "efficient one other way to think about quorum is not about how many you need to",
    "start": "633350",
    "end": "638570"
  },
  {
    "text": "satisfy quorum but what your fault tolerance is when you're making these choices so if we go all the way back up",
    "start": "638570",
    "end": "643970"
  },
  {
    "text": "to 1 our fault tolerance is obviously zero we can't survive a failure of a node in that case the same is true for",
    "start": "643970",
    "end": "651529"
  },
  {
    "text": "two we can't survive a failure but if we have three nodes in our consensus group quorum is two that means one can go down",
    "start": "651529",
    "end": "658820"
  },
  {
    "text": "or maybe there's a network partition or something happens where it's unreachable and our cluster can still be up and",
    "start": "658820",
    "end": "664790"
  },
  {
    "text": "running and and perform new work so my recommendation is that if you're",
    "start": "664790",
    "end": "670310"
  },
  {
    "text": "running something in production especially if it's making money five is sort of the magic number because then",
    "start": "670310",
    "end": "676009"
  },
  {
    "text": "you can tolerate one failure and then have one node down for maintenance so just my experience working with many",
    "start": "676009",
    "end": "682249"
  },
  {
    "text": "different projects in production that seems to be sort of the magic number again odd are sorry even numbers are not",
    "start": "682249",
    "end": "689600"
  },
  {
    "text": "very efficient in this type of model because of that n divided by two plus one and over 50% you don't actually get",
    "start": "689600",
    "end": "697160"
  },
  {
    "text": "any more benefit if you have one manager just adding another isn't going to give you more fault tolerance and in a very",
    "start": "697160",
    "end": "706490"
  },
  {
    "start": "705000",
    "end": "802000"
  },
  {
    "text": "counterintuitive point that's always mind-blowing when I share it with people who haven't thought about it in this way",
    "start": "706490",
    "end": "711499"
  },
  {
    "text": "having two managers actually doubles your chances of losing quorum because you still have zero fault tolerance and",
    "start": "711499",
    "end": "716839"
  },
  {
    "text": "now instead of having one single point of failure you've doubled your chances of having a node failure by introducing another one so stick to odd numbers stay",
    "start": "716839",
    "end": "726110"
  },
  {
    "text": "away from even numbers and you'll be good if you are deploying your consensus",
    "start": "726110",
    "end": "733519"
  },
  {
    "text": "group across multiple AZ's which you might have a good reason to do so I think deploying them across multiple",
    "start": "733519",
    "end": "740329"
  },
  {
    "text": "regions can get a little bit hairy because of latency and we'll talk more about like node to node communication in",
    "start": "740329",
    "end": "746779"
  },
  {
    "text": "a bit and why that can be a problem but multiple AZ's might be reasonable so",
    "start": "746779",
    "end": "752779"
  },
  {
    "text": "just pay attention to a data set data center topology when you do that anticipate that one of those Easy's will",
    "start": "752779",
    "end": "758540"
  },
  {
    "text": "go down and that the Status page won't be updated for that cloud provider not going to name any names but there's a I",
    "start": "758540",
    "end": "766939"
  },
  {
    "text": "mean it's just it's math it it's um follows that same quorum principle just",
    "start": "766939",
    "end": "772750"
  },
  {
    "text": "anticipate failure I think that's a really good strategy in general for distributed computing a lot of turnkey",
    "start": "772750",
    "end": "780679"
  },
  {
    "text": "solutions that might stand up a bootstrap a cluster for you magically kind of have this functionality in it if",
    "start": "780679",
    "end": "787279"
  },
  {
    "text": "you're using something like an auto scaling group as part of that generally we'll try to distribute evenly across the across the easies to make sure that",
    "start": "787279",
    "end": "794750"
  },
  {
    "text": "you're resistant if one of the easiest has a failure so keep an eye out but I",
    "start": "794750",
    "end": "799850"
  },
  {
    "text": "would always double check to make sure that you're you're insulated from that or isolated",
    "start": "799850",
    "end": "805579"
  },
  {
    "start": "802000",
    "end": "915000"
  },
  {
    "text": "from that failure so that's quorum quorum is going to become a very important thing as we talk",
    "start": "805579",
    "end": "812570"
  },
  {
    "text": "about both leader election and log replication so leader election is very important and one of the core",
    "start": "812570",
    "end": "818570"
  },
  {
    "text": "responsibilities of Raft because without a leader your management group or your consensus group can't do any work",
    "start": "818570",
    "end": "824300"
  },
  {
    "text": "because there's no one to delegate responsibility and control whether or not a log can be appended or should be",
    "start": "824300",
    "end": "830630"
  },
  {
    "text": "committed into the log in a consensus group there are really three states of",
    "start": "830630",
    "end": "836000"
  },
  {
    "text": "nodes that participate one of them is the leader the leader is again the one",
    "start": "836000",
    "end": "841250"
  },
  {
    "text": "taking in the requests and then sending messages out to the followers which is the other state to make sure that that",
    "start": "841250",
    "end": "848060"
  },
  {
    "text": "action is able to take place and gets confirmation from the followers commits it and then sends that message back to",
    "start": "848060",
    "end": "854930"
  },
  {
    "text": "the followers to say okay yeah you can really really write that into your log now we've gotten enough votes there's a",
    "start": "854930",
    "end": "860570"
  },
  {
    "text": "third state called a candidate which happens in leader election if the leader is unreachable or goes offline for some",
    "start": "860570",
    "end": "868130"
  },
  {
    "text": "reason if you execute chaos monkey scripts in your cluster like some people",
    "start": "868130",
    "end": "874130"
  },
  {
    "text": "might do then your your consensus group doesn't have a leader and we need to go",
    "start": "874130",
    "end": "879620"
  },
  {
    "text": "into leader election the first follower to notice that there's no leader will put itself into candidate state in order",
    "start": "879620",
    "end": "886820"
  },
  {
    "text": "to solicit votes from the rest of the nodes that are online to elect itself as",
    "start": "886820",
    "end": "893180"
  },
  {
    "text": "the new leader it will also vote for itself there's also another another state which",
    "start": "893180",
    "end": "899360"
  },
  {
    "text": "is kind of an unofficial one which is unreachable I think that's important to remember that there can be notes that are formally participating in the",
    "start": "899360",
    "end": "907160"
  },
  {
    "text": "consensus group that could be unreachable so that could be the downed manager or a Down node or the downed leader that's spawning the next leader",
    "start": "907160",
    "end": "914029"
  },
  {
    "text": "election they still exist even if they're not online so I want to talk",
    "start": "914029",
    "end": "919699"
  },
  {
    "start": "915000",
    "end": "1236000"
  },
  {
    "text": "through a couple scenarios of leader election and to do that I have a little",
    "start": "919699",
    "end": "925480"
  },
  {
    "text": "handy visual aid actually forked this from let me see if I can make this",
    "start": "925480",
    "end": "931100"
  },
  {
    "text": "bigger cool yeah you can all see that okay I this from the author of raft and kind of",
    "start": "931100",
    "end": "937160"
  },
  {
    "text": "tweaked it a little bit for this talk and if you've ever looked at the secret lives of data if you are a independently",
    "start": "937160",
    "end": "943279"
  },
  {
    "text": "curious raft consumer then you may have seen something like this before I want",
    "start": "943279",
    "end": "948770"
  },
  {
    "text": "to talk through what happens during a leader election we're gonna start out really slow so a couple things that pay",
    "start": "948770",
    "end": "954680"
  },
  {
    "text": "attention to we have a 5 node consensus group up here starting with s1 s2 s3",
    "start": "954680",
    "end": "959690"
  },
  {
    "text": "those are the names of the nodes in the in the consensus group we have this",
    "start": "959690",
    "end": "964850"
  },
  {
    "text": "little ticker that's happened going around and that's a timeout and I said before like the first manager to notice",
    "start": "964850",
    "end": "971089"
  },
  {
    "text": "or the first node to notice that there's no leader is gonna put itself up for election and that's exactly what",
    "start": "971089",
    "end": "976490"
  },
  {
    "text": "happened right here with us to notice that there was no leader so it timed out",
    "start": "976490",
    "end": "981890"
  },
  {
    "text": "and put itself up for election solicited votes and I can rewind time a little bit here I'm sorry I got I just got a new",
    "start": "981890",
    "end": "995450"
  },
  {
    "text": "Mac a new MacBook with the touch bar and like the huge attract pad and I'm very unhappy with it so please bear with me",
    "start": "995450",
    "end": "1003240"
  },
  {
    "text": "there we go okay so we see that s2 timed out and now we have a new election cycle",
    "start": "1003360",
    "end": "1010720"
  },
  {
    "text": "so election term two we see five spaces for votes in there those little circles we can see that as two voted for itself",
    "start": "1010720",
    "end": "1017440"
  },
  {
    "text": "it gets a black circle from itself it's asking for votes from the rest and everyone's online and everyone votes and",
    "start": "1017440",
    "end": "1024760"
  },
  {
    "text": "that means yep s2 gets elected as the leader and we see that in red a little bit probably hard to see for those of",
    "start": "1024760",
    "end": "1029829"
  },
  {
    "text": "you in the back so that's great and things will just carry on until oh no s2",
    "start": "1029829",
    "end": "1039040"
  },
  {
    "text": "went down whether or not it actually there was a hardware failure or there's a network partition or who knows what",
    "start": "1039040",
    "end": "1045520"
  },
  {
    "text": "could happen but the point is that it went down and just as in the last time",
    "start": "1045520",
    "end": "1050940"
  },
  {
    "text": "the first node to notice that there's no leader is going to put itself up for candidacy now quorum becomes important here",
    "start": "1050940",
    "end": "1057820"
  },
  {
    "text": "because we have one node that's down but since we have a five node cluster quorum is three which means that four is fine",
    "start": "1057820",
    "end": "1064240"
  },
  {
    "text": "and we can keep going and we can tolerate that failure so s3 is going to notice that there's no",
    "start": "1064240",
    "end": "1069520"
  },
  {
    "text": "leader it will vote for itself solicit votes it's not getting any vote back from s2 but that's fine because it has",
    "start": "1069520",
    "end": "1075580"
  },
  {
    "text": "already four votes one from itself and then one from each of the other nodes that are online speed this up a little",
    "start": "1075580",
    "end": "1083620"
  },
  {
    "text": "bit cool there's one thing actually",
    "start": "1083620",
    "end": "1089860"
  },
  {
    "text": "there's many edge cases that can happen in this and one that I find particularly interesting and one thing that's a",
    "start": "1089860",
    "end": "1094870"
  },
  {
    "text": "little bit different in raft versus other implementations like paxos and multi Paxos is the event of a split vote",
    "start": "1094870",
    "end": "1100510"
  },
  {
    "text": "so this is highly unlikely because these timeouts are randomized across all of",
    "start": "1100510",
    "end": "1106210"
  },
  {
    "text": "the nodes so it's highly unlikely that two nodes in your group have the same timeout but it can happen and in this",
    "start": "1106210",
    "end": "1112090"
  },
  {
    "text": "case it has so I can go back in time just a little bit and see that these two",
    "start": "1112090",
    "end": "1118150"
  },
  {
    "text": "nodes s 1 and s 5 timed out exactly the same time and put themselves up for candidacy for election term 4 at exactly",
    "start": "1118150",
    "end": "1125919"
  },
  {
    "text": "the same time they also sent votes out at exactly the same time and they are voting for themselves so they have one",
    "start": "1125919",
    "end": "1132429"
  },
  {
    "text": "vote but they can only manage to get one",
    "start": "1132429",
    "end": "1137530"
  },
  {
    "text": "other vote back because it is a truly split election quorum is again how many",
    "start": "1137530",
    "end": "1143110"
  },
  {
    "text": "in this situation 3 so two votes isn't enough to elect a new leader but what",
    "start": "1143110",
    "end": "1149919"
  },
  {
    "text": "happens then because it looks like we'd be just at a standstill raft is designed though if there's no leader whatever the",
    "start": "1149919",
    "end": "1157390"
  },
  {
    "text": "next node is which in that case was s3 oops to notice that there was no leader",
    "start": "1157390",
    "end": "1165580"
  },
  {
    "text": "is gonna put itself up for election and",
    "start": "1165580",
    "end": "1171250"
  },
  {
    "text": "let's just go through that so s3 right here is just about to timeout it doesn't",
    "start": "1171250",
    "end": "1176860"
  },
  {
    "text": "matter the candidacy state of s 1 and S 5 they don't have enough votes to be elected so what Raph does is just throws out",
    "start": "1176860",
    "end": "1183970"
  },
  {
    "text": "that cycle it just counts it as lost and we're moving on we're not going to try to repair it or try to do anything we're",
    "start": "1183970",
    "end": "1189490"
  },
  {
    "text": "just gonna plow forward so s3 will put itself up as can't a candidate for for",
    "start": "1189490",
    "end": "1195850"
  },
  {
    "text": "election term 5 and since it was the only one to do that it's going to be able to get the votes",
    "start": "1195850",
    "end": "1201460"
  },
  {
    "text": "having a candidate state of a node doesn't prevent it from voting for another node as well so s 1 and s 5 are",
    "start": "1201460",
    "end": "1208810"
  },
  {
    "text": "able to participate in that election it can get the requisite votes so it gets four in this case and time marches on",
    "start": "1208810",
    "end": "1215950"
  },
  {
    "text": "we've just moved on to a different election cycle so that is leader",
    "start": "1215950",
    "end": "1221980"
  },
  {
    "text": "election kind of a brief intro to it",
    "start": "1221980",
    "end": "1228040"
  },
  {
    "text": "there's a couple other edge cases that are captured by think that split vote is really the most common that that is",
    "start": "1228040",
    "end": "1234220"
  },
  {
    "text": "possible to happen the next thing that I want to go into that more depth on is",
    "start": "1234220",
    "end": "1240430"
  },
  {
    "start": "1236000",
    "end": "1315000"
  },
  {
    "text": "log replication because it's kind of similar with that heartbeat timeout",
    "start": "1240430",
    "end": "1245800"
  },
  {
    "text": "transaction between the manager and the and the followers but it's a little bit",
    "start": "1245800",
    "end": "1251890"
  },
  {
    "text": "more more complicated so we're gonna combine sort of everything we've talked about so far and talk about log replication before we do that though I",
    "start": "1251890",
    "end": "1258640"
  },
  {
    "text": "want to make it clear what I'm talking about when I'm talking about a log I think this is also one of those terms",
    "start": "1258640",
    "end": "1263830"
  },
  {
    "text": "that's like highly loaded and it needs like that Wikipedia like which log are you talking about page when we when we",
    "start": "1263830",
    "end": "1270220"
  },
  {
    "text": "discuss it this is the source of truth for your application I'm not talking about log output or system log I'm",
    "start": "1270220",
    "end": "1276340"
  },
  {
    "text": "talking about like imagine someone hammering values into a stone tablet kind of log a record of truth so this is",
    "start": "1276340",
    "end": "1284620"
  },
  {
    "text": "an append-only time-based record of data we can visualize that really simply by",
    "start": "1284620",
    "end": "1289740"
  },
  {
    "text": "imagining just a list of you know some values that let's say X has so at one",
    "start": "1289740",
    "end": "1295480"
  },
  {
    "text": "point in time X had two and now we're going to maybe append an entry for whatever the new value of X is this log",
    "start": "1295480",
    "end": "1303010"
  },
  {
    "text": "is specifically for computers really it's not for humans luckily for you all",
    "start": "1303010",
    "end": "1308440"
  },
  {
    "text": "I am a human with a computer so I'm going to show you how you can actually read the raf log and where it's found",
    "start": "1308440",
    "end": "1313570"
  },
  {
    "text": "which i think is pretty interesting before we do that let's look at what happens in this log replication scenario",
    "start": "1313570",
    "end": "1321790"
  },
  {
    "start": "1315000",
    "end": "1410000"
  },
  {
    "text": "and go over a couple a couple bad states with some data inconsistency and see how",
    "start": "1321790",
    "end": "1328750"
  },
  {
    "text": "raft repairs them so in this case we have",
    "start": "1328750",
    "end": "1334350"
  },
  {
    "text": "States for a log entry to be one is uncommitted and those are represented by the little dashed lines if we see s1 has",
    "start": "1334350",
    "end": "1341789"
  },
  {
    "text": "two to two those are all uncommitted that means that the leader has scented the follower please update this to two",
    "start": "1341789",
    "end": "1347370"
  },
  {
    "text": "but has not gotten enough votes and enough confirmation to make that truth yet so that's uncommitted state that's",
    "start": "1347370",
    "end": "1353220"
  },
  {
    "text": "sort of like you know pre-commit not yet in in the log so in this case we don't",
    "start": "1353220",
    "end": "1361770"
  },
  {
    "text": "have quorum so nothing can be committed we can have these things in an uncommitted state just sort of like preparing it to be committed but because",
    "start": "1361770",
    "end": "1368400"
  },
  {
    "text": "we can't get enough votes there's no way for that to become true this will go on",
    "start": "1368400",
    "end": "1376710"
  },
  {
    "text": "and will just populate whatever nodes are listening with these uncommitted log",
    "start": "1376710",
    "end": "1382080"
  },
  {
    "text": "entries let's move a little bit faster and see what happens then when we do",
    "start": "1382080",
    "end": "1389190"
  },
  {
    "text": "have quorum one came back which means that we can now start committing these so now that enough votes are possible",
    "start": "1389190",
    "end": "1395929"
  },
  {
    "text": "even though those nodes knew about the possibility for change that change hadn't yet been voted on so it's not",
    "start": "1395929",
    "end": "1401940"
  },
  {
    "text": "been committed in the log now they have been so that's represented by this solid line across or around the the log entry",
    "start": "1401940",
    "end": "1409880"
  },
  {
    "text": "the one really interesting thing about raft is what happens in the case of data",
    "start": "1409880",
    "end": "1415710"
  },
  {
    "start": "1410000",
    "end": "1521000"
  },
  {
    "text": "against inconsistency so in this example we had know some nodes offline but I don't know what they were doing they",
    "start": "1415710",
    "end": "1422159"
  },
  {
    "text": "could have been you know on a separate partition and had a new election cycle and had a new state of truth we don't",
    "start": "1422159",
    "end": "1428460"
  },
  {
    "text": "know but now let's say that happened and maybe the network partition is restored",
    "start": "1428460",
    "end": "1433679"
  },
  {
    "text": "what happens now we have two sets of of data and how do we repair the inconsistencies and in general the",
    "start": "1433679",
    "end": "1440250"
  },
  {
    "text": "answer to that in Raph is that whatever the most recent election term was that data gets precedent precedence so in",
    "start": "1440250",
    "end": "1447570"
  },
  {
    "text": "this case we've already moved on to election cycle three we have some the old leader which was s1 is offline now",
    "start": "1447570",
    "end": "1454919"
  },
  {
    "text": "so we have kind of like two conflicting sets of data when s1 comes back online",
    "start": "1454919",
    "end": "1459960"
  },
  {
    "text": "it still thinks that it's like the year 1995 or it's like election term two it doesn't know what leader has been",
    "start": "1459960",
    "end": "1466169"
  },
  {
    "text": "elected it will get in forms soon by the new leader so it updates itself to understand that it's",
    "start": "1466169",
    "end": "1472220"
  },
  {
    "text": "election term 3 and because it had only uncommitted logs it can throw those out and then reconcile itself with the new",
    "start": "1472220",
    "end": "1478820"
  },
  {
    "text": "the new version cool so there's of",
    "start": "1478820",
    "end": "1485570"
  },
  {
    "text": "course other edge cases but I think that network partition is particularly the",
    "start": "1485570",
    "end": "1490669"
  },
  {
    "text": "one that you might find in production and is more likely to happen in real life so if you are new to logs or kind",
    "start": "1490669",
    "end": "1499340"
  },
  {
    "text": "of new to log replication in just distributed consensus there's a really awesome blog post about logs and what",
    "start": "1499340",
    "end": "1505610"
  },
  {
    "text": "you all should understand about them if you're gonna be operating any system that depends on the log I have it up on",
    "start": "1505610",
    "end": "1511460"
  },
  {
    "text": "a handy little bit Lee it's a LinkedIn engineering blog post so highly recommend that it's a kind of a long",
    "start": "1511460",
    "end": "1517610"
  },
  {
    "text": "read but a good one so if you want to check that out cool so I promise that",
    "start": "1517610",
    "end": "1523429"
  },
  {
    "text": "I'm a person with a computer or a human with a computer which means that I'm",
    "start": "1523429",
    "end": "1528559"
  },
  {
    "text": "able to look at the logs so I actually wanted to do this live but the Wi-Fi was",
    "start": "1528559",
    "end": "1535429"
  },
  {
    "text": "choking a little bit so instead of me kind of like fumbling and waiting for Wi-Fi to error out I just prepared this",
    "start": "1535429",
    "end": "1541159"
  },
  {
    "text": "beforehand but I can talk through maybe I can talk through what this is so I'm",
    "start": "1541159",
    "end": "1550039"
  },
  {
    "text": "gonna I'm gonna do something like scandalous and I'm actually gonna use docker for the example for for raft",
    "start": "1550039",
    "end": "1556029"
  },
  {
    "text": "instead of at CD because I find the tooling just slightly slightly better for conference talk demo specifically",
    "start": "1556029",
    "end": "1562960"
  },
  {
    "text": "there is a choice I said before raft is embedded directly in docker and",
    "start": "1562960",
    "end": "1569200"
  },
  {
    "text": "everything lives in a directory in at CD it's living in var Lib at CD member",
    "start": "1569200",
    "end": "1575690"
  },
  {
    "text": "there's a snap directory and a wall directory so snapshots and then the right head log right upend log right",
    "start": "1575690",
    "end": "1582230"
  },
  {
    "text": "ahead log those are a net CD they're just file and it's the same thing",
    "start": "1582230",
    "end": "1587389"
  },
  {
    "text": "because raft is the consistent part here so it doesn't really matter what tool we're talking about that's also",
    "start": "1587389",
    "end": "1592700"
  },
  {
    "text": "available in docker and when I use this there's a tool called raft there's a",
    "start": "1592700",
    "end": "1600990"
  },
  {
    "text": "dump wall tool in in swarm kit and I'll have links in the slide so you can check it out later it's a little bit too much",
    "start": "1600990",
    "end": "1607350"
  },
  {
    "text": "detail to kind of try to process during a talk but it basically looks at that log crosses it into something that's",
    "start": "1607350",
    "end": "1613230"
  },
  {
    "text": "human readable and then spits it back out so I have a cluster here of three in",
    "start": "1613230",
    "end": "1620280"
  },
  {
    "text": "a raft consensus group and basically what's stored in the raft logs is everything that you could possibly need",
    "start": "1620280",
    "end": "1627120"
  },
  {
    "text": "to know about the state of the cluster so we have the action that happened we",
    "start": "1627120",
    "end": "1632250"
  },
  {
    "text": "know that index we know exactly when it was created when it was updated we can see the image we can see resource limits",
    "start": "1632250",
    "end": "1639140"
  },
  {
    "text": "status kind of everything is here which is really great because if you have this",
    "start": "1639140",
    "end": "1644220"
  },
  {
    "text": "raft log you can basically breathe life into your cluster in case of a failure and there's a couple different failure",
    "start": "1644220",
    "end": "1651210"
  },
  {
    "text": "scenarios that I want to talk about and we'll talk about this raft log and its",
    "start": "1651210",
    "end": "1656850"
  },
  {
    "text": "role in each of those the first one is the hard failure of like your data",
    "start": "1656850",
    "end": "1664320"
  },
  {
    "start": "1659000",
    "end": "1843000"
  },
  {
    "text": "center being on fire so this is like worst case scenario but I want to",
    "start": "1664320",
    "end": "1669900"
  },
  {
    "text": "emphasize that you should be backing up these logs that are controlled by raft",
    "start": "1669900",
    "end": "1675830"
  },
  {
    "text": "as long as you have that log you can restore the state of your cluster you",
    "start": "1675830",
    "end": "1681480"
  },
  {
    "text": "know like asterisks there's probably some little things that you're not going to be able to restore and you can't restore what happened after you took the",
    "start": "1681480",
    "end": "1687480"
  },
  {
    "text": "backup but you can kind of rewind time and breathe life back into a failed",
    "start": "1687480",
    "end": "1692520"
  },
  {
    "text": "cluster if you've sort of declared bankruptcy and your incident response and need to start from scratch as long",
    "start": "1692520",
    "end": "1697890"
  },
  {
    "text": "as you have that backup you can do it generally speaking wherever raft is implemented there's going to be two",
    "start": "1697890",
    "end": "1703650"
  },
  {
    "text": "directories there will be a snapshot directory and that log directory in wall in xcd you can specify where these come",
    "start": "1703650",
    "end": "1710760"
  },
  {
    "text": "from or where they're stored so you can put them somewhere else you can back them up somewhere else you can kind of",
    "start": "1710760",
    "end": "1718770"
  },
  {
    "text": "define a disk and adder of your choice in at CD at least so that's a really",
    "start": "1718770",
    "end": "1723930"
  },
  {
    "text": "cool feature some tools will manage this for you I think that's becoming like the",
    "start": "1723930",
    "end": "1729900"
  },
  {
    "text": "new horizon of kubernetes management is just to automatically backup at CD so that you always have something on hand if you if",
    "start": "1729900",
    "end": "1737190"
  },
  {
    "text": "your cluster goes down in a dumpster fire you can just sort of bring it back up if you're gonna restore from a backup",
    "start": "1737190",
    "end": "1744840"
  },
  {
    "text": "so a net CD you want to pass in where those things come from so where that data is coming from where the wall is",
    "start": "1744840",
    "end": "1750390"
  },
  {
    "text": "coming from and then there's this really important thing called - - forced new cluster and this seems to be the agreed",
    "start": "1750390",
    "end": "1756600"
  },
  {
    "text": "upon flag for kind of all tools that are dealing with distributed consensus that",
    "start": "1756600",
    "end": "1762299"
  },
  {
    "text": "will force a one node cluster to kind of come online with the data that is in",
    "start": "1762299",
    "end": "1768480"
  },
  {
    "text": "that backup you can do this the same thing on docker again because it's not",
    "start": "1768480",
    "end": "1774059"
  },
  {
    "text": "anything specific to the tooling it's about raft and about that raft log same thing docker swarm in it with that - -",
    "start": "1774059",
    "end": "1780750"
  },
  {
    "text": "forced new cluster will kind of disregard the cluster configuration you had before take that log and start a",
    "start": "1780750",
    "end": "1787919"
  },
  {
    "text": "brand new cluster with one node using that backup one important thing to keep",
    "start": "1787919",
    "end": "1793679"
  },
  {
    "text": "in mind and I'm I'm hoping that this will be a solved problem in maybe a",
    "start": "1793679",
    "end": "1798929"
  },
  {
    "text": "couple months time I know there's a lot of smart people working on this problem when you restore from the backup those",
    "start": "1798929",
    "end": "1804690"
  },
  {
    "text": "like member IPS of the nodes that are in the cluster are also stored as part of that log which means that if you have a",
    "start": "1804690",
    "end": "1810540"
  },
  {
    "text": "brand new cluster at the IP addresses are probably not going to be the same in ED CD what's really awesome and probably",
    "start": "1810540",
    "end": "1817080"
  },
  {
    "text": "one of my favorite features that's very underappreciated is that it's super easy to update a member IP you can just like",
    "start": "1817080",
    "end": "1822900"
  },
  {
    "text": "go in there and say oh I have a new node that's replacing this old node this is the new IP address swarm at least the",
    "start": "1822900",
    "end": "1829590"
  },
  {
    "text": "last I checked maybe it's maybe it's different you can't really do that in an in a graceful way I think that's one",
    "start": "1829590",
    "end": "1835440"
  },
  {
    "text": "really great thing that CD has for it if you're an SRE or someone that's on call that's something that makes me sleep a",
    "start": "1835440",
    "end": "1841440"
  },
  {
    "text": "little bit better at night so let's talk about a soft failure and I'll demo a",
    "start": "1841440",
    "end": "1848070"
  },
  {
    "start": "1843000",
    "end": "1976000"
  },
  {
    "text": "soft failure because that's hopefully fingers crossed when I can recover from more easily and with the time allotted",
    "start": "1848070",
    "end": "1854429"
  },
  {
    "text": "so that I can get you all to lunch on time so losing quorum is probably the most",
    "start": "1854429",
    "end": "1859950"
  },
  {
    "text": "common failure it's not common that our data centers may come up in smoke so you probably want to practice that",
    "start": "1859950",
    "end": "1866460"
  },
  {
    "text": "backup recovery you know every month or every week or whatever your policies are practice also what",
    "start": "1866460",
    "end": "1872320"
  },
  {
    "text": "happens when you lose quorum because it can mean different things for your application so losing quorum just means that no new action can take place it",
    "start": "1872320",
    "end": "1879280"
  },
  {
    "text": "doesn't mean that everything that's running falls down all right and that's an important distinction so understand",
    "start": "1879280",
    "end": "1884740"
  },
  {
    "text": "what that means for your app if you have just a bunch of services that are long-running and you're just using a",
    "start": "1884740",
    "end": "1890140"
  },
  {
    "text": "tool like kubernetes because you want Phil you know fault tolerance and resistance to failure then that's maybe",
    "start": "1890140",
    "end": "1896050"
  },
  {
    "text": "okay and not such an emergency because unless there's a failure everything's",
    "start": "1896050",
    "end": "1901210"
  },
  {
    "text": "just gonna stay up and running but if you're using like you're processing tons and tons of jobs those new jobs aren't",
    "start": "1901210",
    "end": "1906220"
  },
  {
    "text": "going to be able to be run the interesting thing about losing quorum is",
    "start": "1906220",
    "end": "1911980"
  },
  {
    "text": "that you also can't add new nodes to that management group so let's say you have three nodes you can't just and two",
    "start": "1911980",
    "end": "1919450"
  },
  {
    "text": "of them go down you can't just start a new node and add it to the group and try to like force quorum to be regained",
    "start": "1919450",
    "end": "1925210"
  },
  {
    "text": "because your management group can't agree that it's possible for a new manager to be added so how do you",
    "start": "1925210",
    "end": "1933640"
  },
  {
    "text": "survive a quorum like losing quorum you can regain quorum by bringing the down",
    "start": "1933640",
    "end": "1939070"
  },
  {
    "text": "nodes back up like that's kind of Captain Obvious probably not something useful in the talk but it is there and I",
    "start": "1939070",
    "end": "1945430"
  },
  {
    "text": "think it's like maybe you don't think about that an instant response the other thing you can do is that this is sort of",
    "start": "1945430",
    "end": "1951040"
  },
  {
    "text": "like that disaster recovery that we talked about before is but you can use force new cluster on a healthy manager",
    "start": "1951040",
    "end": "1956080"
  },
  {
    "text": "that's part of the management group or the the consensus group and do - - new force new cluster and that will just",
    "start": "1956080",
    "end": "1962320"
  },
  {
    "text": "look at the current raft log that it already has there's no need to restore from backup and just disregard the fact",
    "start": "1962320",
    "end": "1967540"
  },
  {
    "text": "that there were at one point other managers you will have to re add new new",
    "start": "1967540",
    "end": "1973300"
  },
  {
    "text": "nodes to that leader group if you do it this way and again you can't just add a",
    "start": "1973300",
    "end": "1978700"
  },
  {
    "start": "1976000",
    "end": "2219000"
  },
  {
    "text": "new healthy manager to the cluster because that would require consensus and that's something that we don't have so",
    "start": "1978700",
    "end": "1985110"
  },
  {
    "text": "that's what I will show you right now and I have little visualizer here I have",
    "start": "1985110",
    "end": "1993490"
  },
  {
    "text": "three nodes participating in this consensus group and I have elasticsearch running",
    "start": "1993490",
    "end": "1999539"
  },
  {
    "text": "here so we can go down and get rid of",
    "start": "1999539",
    "end": "2008929"
  },
  {
    "text": "this oh I'm in a container always in the container when I don't expect to be I",
    "start": "2008929",
    "end": "2016840"
  },
  {
    "text": "can curl and just look at what my elasticsearch is doing and we can see",
    "start": "2016989",
    "end": "2022309"
  },
  {
    "text": "it's like online you know for search so it's here and it's running and that's",
    "start": "2022309",
    "end": "2027649"
  },
  {
    "text": "great I have five replicas right now",
    "start": "2027649",
    "end": "2032590"
  },
  {
    "text": "let's not have consent so I'm gonna take one away and this might just take a",
    "start": "2035710",
    "end": "2043119"
  },
  {
    "text": "second to realize that that note is down we can see that failure so we can expect",
    "start": "2043119",
    "end": "2050030"
  },
  {
    "text": "that that fifth one will be rescheduled soon cool everything's happening just as we expect that's because we still have",
    "start": "2050030",
    "end": "2056388"
  },
  {
    "text": "quorum and things are fine but if I always makes me a little nervous it's",
    "start": "2056389",
    "end": "2064280"
  },
  {
    "text": "fine it's just it's just virtual machines it's okay like I should put a",
    "start": "2064280",
    "end": "2070878"
  },
  {
    "text": "warning like no vm's were harmed in this it's fine but now I'm kind of I kind of",
    "start": "2070879",
    "end": "2077000"
  },
  {
    "text": "I did a bad thing because now not only did I kill a node where I had like work",
    "start": "2077000",
    "end": "2083450"
  },
  {
    "text": "running but now I just like I lost everything because I only have one node",
    "start": "2083450",
    "end": "2089059"
  },
  {
    "text": "that's up but the interesting thing is though oh no oh no no I lost Wi-Fi Oh No",
    "start": "2089059",
    "end": "2098410"
  },
  {
    "text": "oh crap see I don't know I didn't",
    "start": "2098410",
    "end": "2104660"
  },
  {
    "text": "anticipate that happening yeah yeah",
    "start": "2104660",
    "end": "2111170"
  },
  {
    "text": "that's fine let's uh let's see",
    "start": "2111170",
    "end": "2117309"
  },
  {
    "text": "man okay okay that's fine um we have one",
    "start": "2117309",
    "end": "2127549"
  },
  {
    "text": "minute left I'll just talk to you about what happens so I'll get I'll give it like a hot minute and then",
    "start": "2127549",
    "end": "2133310"
  },
  {
    "text": "we can see oh man I expected a lot of failures I did not expect that one so I was very trusting of conference Wi-Fi so",
    "start": "2133310",
    "end": "2141230"
  },
  {
    "text": "what happens in this case if we so I killed node 2 and node 3 leaving node 1",
    "start": "2141230",
    "end": "2147320"
  },
  {
    "text": "and what I'm gonna do on there is just go docker swarm an it or use the sed",
    "start": "2147320",
    "end": "2152570"
  },
  {
    "text": "command that's the equivalent and - force new cluster it's gonna look back to the raft log that's stored in that",
    "start": "2152570",
    "end": "2161420"
  },
  {
    "text": "that wall directory that we talked about before and just kind of revive itself",
    "start": "2161420",
    "end": "2166780"
  },
  {
    "text": "understanding that then there were supposed to be 5 elastic searches replicas running and it's gonna revive",
    "start": "2166780",
    "end": "2173210"
  },
  {
    "text": "those five but we still have kind of some cleanup to do because we didn't actually gracefully terminate the other",
    "start": "2173210",
    "end": "2179390"
  },
  {
    "text": "node so we have to get rid of those two old nodes that were in our consensus group manually or scripting or whatever",
    "start": "2179390",
    "end": "2185420"
  },
  {
    "text": "your your tool of choices and then re-add new managers if I want more than one let me try this one more time before",
    "start": "2185420",
    "end": "2192110"
  },
  {
    "text": "admitting defeat that's fine so I will leave that actually this is always up",
    "start": "2192110",
    "end": "2197630"
  },
  {
    "text": "because I have a really bad habit of buying domain name so I own consensus tech group it's like I really love raft",
    "start": "2197630",
    "end": "2202880"
  },
  {
    "text": "so this is up in perpetuity you can check this out or secret lives of data I",
    "start": "2202880",
    "end": "2208250"
  },
  {
    "text": "will let you all go to lunch but I'm happy if you want to come up to the stage after to answer any questions so thank you so much sorry about the Wi-Fi",
    "start": "2208250",
    "end": "2216140"
  },
  {
    "text": "[Music]",
    "start": "2216140",
    "end": "2219299"
  }
]