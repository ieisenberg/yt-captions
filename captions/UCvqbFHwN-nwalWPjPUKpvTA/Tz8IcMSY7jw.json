[
  {
    "text": "seems like we should get started So thank you everyone for coming Uh today we would like to talk about writing",
    "start": "240",
    "end": "6480"
  },
  {
    "text": "dynamic multicluster controllers with controller runtime My name is Marvin Beckers I am a team lead at Kubernetic",
    "start": "6480",
    "end": "13040"
  },
  {
    "text": "Yeah I'm Stefan working on API server things for a long time Many of you will have seen me before working at Nvidia on",
    "start": "13040",
    "end": "19920"
  },
  {
    "text": "API server topics So um yeah All right Uh before we get started I would also",
    "start": "19920",
    "end": "25600"
  },
  {
    "text": "like to briefly mention that my part of this work has been made possible by the IPSIS project by the European Union You",
    "start": "25600",
    "end": "32719"
  },
  {
    "text": "might have heard that coming up during this CubeCon but yeah really nice to be",
    "start": "32719",
    "end": "38079"
  },
  {
    "text": "able to work on cool stuff here Okay let's get started So first of all let's",
    "start": "38079",
    "end": "43680"
  },
  {
    "text": "set the stage Why are we here and this room is pretty full So I think a lot of",
    "start": "43680",
    "end": "48719"
  },
  {
    "text": "you are facing the same problem asking the same questions So why would you want to reconcile across multiple Kubernetes",
    "start": "48719",
    "end": "57320"
  },
  {
    "text": "clusters we live in a multicluster world Um I think most people that are running",
    "start": "57320",
    "end": "62719"
  },
  {
    "text": "Kubernetes are running more than one Kubernetes cluster maybe also more than two maybe also more than three Um and",
    "start": "62719",
    "end": "69920"
  },
  {
    "text": "the tooling around that is quite sophisticated these days You have tools like cluster API You have also other",
    "start": "69920",
    "end": "76640"
  },
  {
    "text": "solutions like vclustluster who give you lightweight kubernetes API servers and you know countless others So I think",
    "start": "76640",
    "end": "84159"
  },
  {
    "text": "it's fair to say multicluster or like many clusters is the standard but",
    "start": "84159",
    "end": "89200"
  },
  {
    "text": "reconciling across that fleet of clusters that you have it's not trivial and it's not something that you know is",
    "start": "89200",
    "end": "96720"
  },
  {
    "text": "standardized and controller runtime itself is fairly single cluster in you",
    "start": "96720",
    "end": "102240"
  },
  {
    "text": "know how how to use it and the tooling around it So but the fact remains that you need to",
    "start": "102240",
    "end": "111840"
  },
  {
    "text": "reconcile across many clusters if you have many clusters and a lot of tools have written solutions around this So",
    "start": "111840",
    "end": "119520"
  },
  {
    "text": "for example cluster API has the cluster cache package which allows to reconcile",
    "start": "119520",
    "end": "124799"
  },
  {
    "text": "against multiple clusters Um we at the KCP project we have a fork of controller runtime that added multicluster",
    "start": "124799",
    "end": "131039"
  },
  {
    "text": "capabilities to it Um there's also some older projects like a multicluster controller by Admirality and honestly",
    "start": "131039",
    "end": "137840"
  },
  {
    "text": "there are probably countless others on out there Uh probably all the multicluster management solutions have",
    "start": "137840",
    "end": "144720"
  },
  {
    "text": "their own implementation But that's the thing none of these solutions are widespread They are not generic and they are not native",
    "start": "144720",
    "end": "152000"
  },
  {
    "text": "to control controller runtime either",
    "start": "152000",
    "end": "157480"
  },
  {
    "text": "So let's set some more context Let's take a quick look at scaling models So how can you scale your controllers so",
    "start": "157480",
    "end": "164720"
  },
  {
    "text": "one of the ways this is the classical one is per Kubernetes cluster you have a controller pot running within that",
    "start": "164720",
    "end": "170720"
  },
  {
    "text": "cluster It has a manager it has a cache it has a workq it has a reconciler and you run each of like this construct for",
    "start": "170720",
    "end": "177760"
  },
  {
    "text": "every Kubernetes cluster that you have Another scaling model would be to have a",
    "start": "177760",
    "end": "182879"
  },
  {
    "text": "management cluster and run one controller pot um per external cluster",
    "start": "182879",
    "end": "188319"
  },
  {
    "text": "that you want to reconcile So you have one management cluster that's basically kind of the you know workhorse and there",
    "start": "188319",
    "end": "194560"
  },
  {
    "text": "is a process manager maybe another operator or controller um that launches",
    "start": "194560",
    "end": "199840"
  },
  {
    "text": "controller ports against these Kubernetes clusters that you want to reconcile um",
    "start": "199840",
    "end": "206440"
  },
  {
    "text": "dynamically and then there's this model as well um also you could have a host cluster management cluster and the",
    "start": "206440",
    "end": "213519"
  },
  {
    "text": "controller pot here would start multiple managers so controller runtime managers",
    "start": "213519",
    "end": "218720"
  },
  {
    "text": "to recon reconcile against external Kubernetes clusters that you would want to reconcile against So these are a",
    "start": "218720",
    "end": "225920"
  },
  {
    "text": "couple of the scaling models that we've seen already Um but we think that",
    "start": "225920",
    "end": "231040"
  },
  {
    "text": "there's a way to make it more generic and to build like a tighter integration with controller runtime",
    "start": "231040",
    "end": "239080"
  },
  {
    "text": "Yeah So we mentioned the the word here already on the slides So there's a new project we introduced a couple of weeks",
    "start": "239280",
    "end": "245519"
  },
  {
    "text": "ago and it's called multicluster runtime and there's a reason it sounds like controller runtime Um it's a friendly",
    "start": "245519",
    "end": "252560"
  },
  {
    "text": "extension of controller runtime So it's it's not forking anything Um it's just",
    "start": "252560",
    "end": "258400"
  },
  {
    "text": "using the primitives of controller runtime under the hood And we will see in a couple of slides um how this looks",
    "start": "258400",
    "end": "264160"
  },
  {
    "text": "like Um the project is hosted in the Kubernetes 6 uh on GitHub So um you will",
    "start": "264160",
    "end": "270960"
  },
  {
    "text": "find it here Commun 6 multicluster runtime and it's sponsored and owned by",
    "start": "270960",
    "end": "276080"
  },
  {
    "text": "sik multicluster So it's an official sik project and um what we mainly add to",
    "start": "276080",
    "end": "281919"
  },
  {
    "text": "controller runtime is adding a provider um concept So there's a concept um a",
    "start": "281919",
    "end": "287520"
  },
  {
    "text": "provider can discover or should discover um the clusters which the controllers will talk to and we will see a couple of",
    "start": "287520",
    "end": "293600"
  },
  {
    "text": "those providers in a second and also how you build them And if you look on the",
    "start": "293600",
    "end": "298639"
  },
  {
    "text": "repository there is a kind of disclaimer in the beginning It's an experiment and we are all here or the",
    "start": "298639",
    "end": "305759"
  },
  {
    "text": "talk exists because we want feedback and people trying that out whether this is a",
    "start": "305759",
    "end": "311120"
  },
  {
    "text": "model which is actually useful Um the two of us we we have background in KCP",
    "start": "311120",
    "end": "316400"
  },
  {
    "text": "and in KCP we needed something like that and we were for quite some time uh",
    "start": "316400",
    "end": "321600"
  },
  {
    "text": "worked on on uh such a project and but we realize that there are many many more use cases and the provider concept is",
    "start": "321600",
    "end": "327840"
  },
  {
    "text": "basically opening that up We talk about two topologies which",
    "start": "327840",
    "end": "332960"
  },
  {
    "text": "are probably common The most easy one is a uniform reconciler It's a reconiler",
    "start": "332960",
    "end": "338240"
  },
  {
    "text": "which actually doesn't know about clusters It's not interested in crosscluster operation It just does its",
    "start": "338240",
    "end": "345280"
  },
  {
    "text": "thing for events from a cluster talk I mean reads from a cluster it writes to the same cluster That's why it's uniform",
    "start": "345280",
    "end": "352000"
  },
  {
    "text": "Super simple Um if you run an existing reconciler but against n different clusters That's probably the pattern you",
    "start": "352000",
    "end": "358240"
  },
  {
    "text": "will follow But of course you can build clusters or imagine clust uh not clusters you can",
    "start": "358240",
    "end": "363440"
  },
  {
    "text": "imagine reconcilers which know about the cluster concept and maybe they have um",
    "start": "363440",
    "end": "369600"
  },
  {
    "text": "yeah dedicated special um clusters like a host cluster for example and then some",
    "start": "369600",
    "end": "375280"
  },
  {
    "text": "child clusters a sinker is a typical example if you sync objects from the left to the right um that's a",
    "start": "375280",
    "end": "380880"
  },
  {
    "text": "non-uniform controller um you can also imagine you have a search manager and the search manager has CAS the issuer in",
    "start": "380880",
    "end": "388000"
  },
  {
    "text": "one cluster like in host um central location and it creates certificates for many many clusters but",
    "start": "388000",
    "end": "395360"
  },
  {
    "text": "it never exposes the credentials or the secrets um to create um certificates So",
    "start": "395360",
    "end": "400639"
  },
  {
    "text": "there are many examples for that and we want to support both and the provider concept is basically attached to the",
    "start": "400639",
    "end": "407039"
  },
  {
    "text": "multicluster um runtime core So this is a multiluster runtime you will see in action in a second and there's a",
    "start": "407039",
    "end": "413039"
  },
  {
    "text": "provider concept and you can have multi multiple different implementations of the provider So imagine cluster runtime",
    "start": "413039",
    "end": "419759"
  },
  {
    "text": "You could have a cluster runtime controller provider which knows how to find discover clusters in controller uh",
    "start": "419759",
    "end": "427599"
  },
  {
    "text": "in in cluster um in cluster API sorry in cluster API You could also find um imagine a provider for kind or provider",
    "start": "427599",
    "end": "434240"
  },
  {
    "text": "for cloud providers Um that's just an interface you have to implement and we",
    "start": "434240",
    "end": "439280"
  },
  {
    "text": "see in a second how this looks like All right so this is a picture that Marvin already showed Um that's not what",
    "start": "439280",
    "end": "446160"
  },
  {
    "text": "we want to do We want a tighter integration and the model we follow at the moment is this",
    "start": "446160",
    "end": "451479"
  },
  {
    "text": "one So you have a manager and we call it a multicluster manager It's basically",
    "start": "451479",
    "end": "456720"
  },
  {
    "text": "like the the one from controller runtime and we see the interface um very soon Um",
    "start": "456720",
    "end": "462160"
  },
  {
    "text": "here's a provider The provider engages clusters when it finds out um there's a new one and it disengages them when a",
    "start": "462160",
    "end": "467759"
  },
  {
    "text": "cluster goes away And here's your reconciler code Here's a work And you",
    "start": "467759",
    "end": "473520"
  },
  {
    "text": "have sources like um yeah basically those which get events from the caches",
    "start": "473520",
    "end": "478639"
  },
  {
    "text": "put them into a work a joint work and then the recon does does its thing On",
    "start": "478639",
    "end": "484240"
  },
  {
    "text": "this picture the work looks super innocent but um it plays a crucial role",
    "start": "484240",
    "end": "489599"
  },
  {
    "text": "and um lots of thought must go into that how this work must look like You can imagine there might be one cluster here",
    "start": "489599",
    "end": "496080"
  },
  {
    "text": "one cache which has thousands of events and the others are pretty silent but the",
    "start": "496080",
    "end": "501120"
  },
  {
    "text": "queue is full because the one cluster exhausts all the capacity of the reconciling process So you have to think",
    "start": "501120",
    "end": "506639"
  },
  {
    "text": "about that you can imagine even cluster the",
    "start": "506639",
    "end": "513120"
  },
  {
    "text": "cluster concept doesn't mean it's a real physical cluster You could for example",
    "start": "513120",
    "end": "518320"
  },
  {
    "text": "um build a provider which knows about namespaces and it uses a different client depending on the namespace you're",
    "start": "518320",
    "end": "525040"
  },
  {
    "text": "talking about So if there are some isolation reasons or you want audit locks or something like that which um",
    "start": "525040",
    "end": "531600"
  },
  {
    "text": "shows a tenant something in this direction this could be also provider So the cluster concept is very wide very",
    "start": "531600",
    "end": "537600"
  },
  {
    "text": "generic It doesn't have to be a real cluster One special case is um the KCP",
    "start": "537600",
    "end": "543120"
  },
  {
    "text": "case where in the background you basically only have one cache one informer and the",
    "start": "543120",
    "end": "549480"
  },
  {
    "text": "provider simulates whatever the sources need here So we have workspaces in KCP A",
    "start": "549480",
    "end": "556240"
  },
  {
    "text": "workspace looks like a real Kubernetes cluster to the outside but they actually all running in one KCP instance And if",
    "start": "556240",
    "end": "564000"
  },
  {
    "text": "you look on the on the um yeah the informer the watch interface here there's just one connection to KCP So",
    "start": "564000",
    "end": "570720"
  },
  {
    "text": "it's super efficient to have many many many workspaces thousands if you want but you have only one cache even that",
    "start": "570720",
    "end": "576320"
  },
  {
    "text": "could be modeled in this provider concept All right so we have seen the",
    "start": "576320",
    "end": "582160"
  },
  {
    "text": "topology um the objects the components and now let's become more practical right see how providers work how",
    "start": "582160",
    "end": "588320"
  },
  {
    "text": "controllers work All right So let's take a look No",
    "start": "588320",
    "end": "596279"
  },
  {
    "text": "I keep talking Okay Um All right So uh how to build a multicluster controller",
    "start": "597120",
    "end": "604519"
  },
  {
    "text": "Okay Yeah I guess we'll do this way Okay Uh so how to build a multicluster controller So let's take a look at",
    "start": "607920",
    "end": "615360"
  },
  {
    "text": "controller runtime first So if you've ever written a controller this is probably somewhat familiar to you from",
    "start": "615360",
    "end": "621360"
  },
  {
    "text": "your main.go file Um you set up a manager That manager is coming from a",
    "start": "621360",
    "end": "627600"
  },
  {
    "text": "package of controller runtime and well you eventually register your controllers with that your reconcilers with that and",
    "start": "627600",
    "end": "634640"
  },
  {
    "text": "then you start your manager So far so good So with multicluster runtime uh we",
    "start": "634640",
    "end": "640399"
  },
  {
    "text": "have a couple of additions but they are not too big So the first thing that you want to look at is the provider So we",
    "start": "640399",
    "end": "648160"
  },
  {
    "text": "instantiate a provider and in this case we're using the kind provider So that is one of the examples that we've built",
    "start": "648160",
    "end": "654560"
  },
  {
    "text": "already And then what you do is you create a multicluster manager here So",
    "start": "654560",
    "end": "660720"
  },
  {
    "text": "this is what Stefan already mentioned earlier Um and it's fairly similar to a",
    "start": "660720",
    "end": "665920"
  },
  {
    "text": "normal um you know a control runtime manager with the difference that you're passing a provider in So this is where I",
    "start": "665920",
    "end": "672720"
  },
  {
    "text": "plug in the kind provider and I also start the provider at a later part of",
    "start": "672720",
    "end": "678560"
  },
  {
    "text": "the code And you know this is basically it This is your controller now being a multicluster runtime controller",
    "start": "678560",
    "end": "687560"
  },
  {
    "text": "And even if you don't know what provider to use yet in your controller you could adopt this already because if you set",
    "start": "687920",
    "end": "694160"
  },
  {
    "text": "the provider to nil um well then you get the same behavior the same single",
    "start": "694160",
    "end": "699279"
  },
  {
    "text": "cluster behavior that you're getting out of controller runtime as well So if you don't know yet what provider to use go",
    "start": "699279",
    "end": "705920"
  },
  {
    "text": "with nil You could already use this No change really to what you've been doing with controller runtime",
    "start": "705920",
    "end": "712800"
  },
  {
    "text": "already Let's take a look at the reconil So the reconil in controller runtime you",
    "start": "712839",
    "end": "718160"
  },
  {
    "text": "probably have seen this already maybe your ch your code looks slightly different but the gist is there is the",
    "start": "718160",
    "end": "724000"
  },
  {
    "text": "builder package from controller runtime Um it uses the reconcile package most notably the request type from that and",
    "start": "724000",
    "end": "731120"
  },
  {
    "text": "you have a reconcile function and that reconcile function can access the cluster object embedded in the manager",
    "start": "731120",
    "end": "738639"
  },
  {
    "text": "to access the client and the cache for the underlying cluster for the underlying like physical Kubernetes",
    "start": "738639",
    "end": "744000"
  },
  {
    "text": "cluster In multicluster runtime uh we have our own builder package Um this builder",
    "start": "744000",
    "end": "751360"
  },
  {
    "text": "package um is you know multicluster where it adds on top of what what",
    "start": "751360",
    "end": "757120"
  },
  {
    "text": "controller runtime is adding and we have our own reconcile package and that reconcile package has also its own",
    "start": "757120",
    "end": "764399"
  },
  {
    "text": "request type which has the controller runtime request type embedded plus the information about the cluster where this",
    "start": "764399",
    "end": "771600"
  },
  {
    "text": "is coming from So it enriches information that you would also get with controller runtime with multicluster",
    "start": "771600",
    "end": "777399"
  },
  {
    "text": "runtime So the main difference in your reconciler function is that uh",
    "start": "777399",
    "end": "782680"
  },
  {
    "text": "previously we had the static cluster um coming from the manager Um but here we",
    "start": "782680",
    "end": "791040"
  },
  {
    "text": "need to dynamically fetch the cluster object because now we don't know which cluster to use until the",
    "start": "791040",
    "end": "797399"
  },
  {
    "text": "requ the request comes into our reconciling pipeline Basically um we need to dynamically fetch the cluster",
    "start": "797399",
    "end": "804160"
  },
  {
    "text": "object from the manager and then use that in the in the like following code",
    "start": "804160",
    "end": "810399"
  },
  {
    "text": "But that's basically it Like all of the rest of the code can look exactly the",
    "start": "810399",
    "end": "816000"
  },
  {
    "text": "same This is really important to stress for us Multicluster runtime is not a fork of controller runtime It's a",
    "start": "816279",
    "end": "822480"
  },
  {
    "text": "testament to controller runtime's architecture and most notably the go generic support that was added I think",
    "start": "822480",
    "end": "829360"
  },
  {
    "text": "last year something I don't know exactly Um so we so multicluster runtime uses",
    "start": "829360",
    "end": "836480"
  },
  {
    "text": "the generic types from controller runtime with the concrete multicluster aware types that we are adding in our",
    "start": "836480",
    "end": "843680"
  },
  {
    "text": "packages So not a fork it's an extension It's kind of an add-on that you can use",
    "start": "843680",
    "end": "849440"
  },
  {
    "text": "on top of controller runtime or like in conjunction with controller runtime",
    "start": "849440",
    "end": "856600"
  },
  {
    "text": "Yeah the too long didn't listen is select a provider um kind provider KCP",
    "start": "856800",
    "end": "862000"
  },
  {
    "text": "provider cluster API provider Uh switch to the multicluster runtime builder and",
    "start": "862000",
    "end": "867440"
  },
  {
    "text": "reconcile packages and dynamically fetch your cluster object which includes the client from the manager and then ideally",
    "start": "867440",
    "end": "874959"
  },
  {
    "text": "you're already done You have a multicluster controller Tada",
    "start": "874959",
    "end": "880680"
  },
  {
    "text": "Yeah that's I think your sister working It works I think it works Oh good All",
    "start": "881760",
    "end": "887519"
  },
  {
    "text": "right Sorry So let's dive a bit into um the interfaces like the Golang",
    "start": "887519",
    "end": "893000"
  },
  {
    "text": "interfaces And we talked about the manager and we handwaved it basically like the normal manager plus some stuff",
    "start": "893000",
    "end": "899360"
  },
  {
    "text": "And here it's a bit more concrete Um I didn't put everything here on the slide but you can imagine just take all",
    "start": "899360",
    "end": "905040"
  },
  {
    "text": "methods of of the controller runtime manager manager and remove everything that the cluster the embedded cluster um",
    "start": "905040",
    "end": "911519"
  },
  {
    "text": "adds to it Like there's a get client in controller runtime This will not be here because um the embedding doesn't exist",
    "start": "911519",
    "end": "918000"
  },
  {
    "text": "But we re um the get cluster to get access to the cluster um by cluster name",
    "start": "918000",
    "end": "923920"
  },
  {
    "text": "And if cluster name is um the empty string it's a host cluster where the controller is running the multi um",
    "start": "923920",
    "end": "929440"
  },
  {
    "text": "cluster aware controller And um you can do for compatibility uh reasons you can",
    "start": "929440",
    "end": "935120"
  },
  {
    "text": "also ask for a manager which is a normal controller runtime manager but now scoped to one cluster And the special",
    "start": "935120",
    "end": "941920"
  },
  {
    "text": "case um without any error here when the cluster name is empty again empty string then you can get even that manager in",
    "start": "941920",
    "end": "947760"
  },
  {
    "text": "one call and super simple um you can also ask for the provider which might be nil um if you provide nil on the",
    "start": "947760",
    "end": "954399"
  },
  {
    "text": "constructor but you might want to use that um provider if you want to create indexes you can imagine there are many",
    "start": "954399",
    "end": "960320"
  },
  {
    "text": "caches many informers and if you want an index there might be clusters being",
    "start": "960320",
    "end": "965519"
  },
  {
    "text": "created in the future right a new cluster comes online and it's engaged and then all your index definitions must",
    "start": "965519",
    "end": "971920"
  },
  {
    "text": "be um applied to that new cluster objects new cache That's why there is a connection to the provider and the",
    "start": "971920",
    "end": "977120"
  },
  {
    "text": "provider has a job to manage indexes as well Um the provider interface that's what you have to implement when you want",
    "start": "977120",
    "end": "982959"
  },
  {
    "text": "to write your own um provider So cluster API or the kind provider that Marvin has shown super simple um has a getter again",
    "start": "982959",
    "end": "990399"
  },
  {
    "text": "the same getter we have just seen for um getting a cluster by cluster name and um",
    "start": "990399",
    "end": "996800"
  },
  {
    "text": "this is of course called by reconcilers indirectly We see in a second how this works but it might be called often So um",
    "start": "996800",
    "end": "1004079"
  },
  {
    "text": "don't recreate your cache every time right don't recreate the watches to your clusters all the time but use some some",
    "start": "1004079",
    "end": "1010959"
  },
  {
    "text": "kind of state some some map of um of caches which are running um and then returns the right one for the cluster",
    "start": "1010959",
    "end": "1016800"
  },
  {
    "text": "name And as I said already index management So remember the indexes which must be added whenever there's a new",
    "start": "1016800",
    "end": "1023360"
  },
  {
    "text": "cluster coming online read the indexes that the user or the developer of the reconciler wanted That's all super",
    "start": "1023360",
    "end": "1030319"
  },
  {
    "text": "simple and you can build um providers in 50 lines or something So not very",
    "start": "1030319",
    "end": "1035600"
  },
  {
    "text": "complex So this picture we have seen already and if we now um add the the methods names here It's not surprising",
    "start": "1035600",
    "end": "1042079"
  },
  {
    "text": "here again there's a the manager I said there's a get cluster getter That's the one that Marvin had in the in the",
    "start": "1042079",
    "end": "1048439"
  },
  {
    "text": "reconiler The reconiler asked for the get cluster This call is forwarded here to the provider to the get functions",
    "start": "1048440",
    "end": "1055280"
  },
  {
    "text": "Provider returns the cluster Inversely when a new cluster goes online and it's discovered then it's engaged on the",
    "start": "1055280",
    "end": "1062640"
  },
  {
    "text": "manager So the manager knows about it and it will manage the sources So then you can imagine another cache comes um",
    "start": "1062640",
    "end": "1069360"
  },
  {
    "text": "here into that that list So it's created and all the sources which are um yeah the watches basically against um the",
    "start": "1069360",
    "end": "1076240"
  },
  {
    "text": "caches they are managed by the manager here So they're recreated or created for the new cluster and they then produce",
    "start": "1076240",
    "end": "1083039"
  },
  {
    "text": "events which go into the work QE and the reconciler just um processes events from the new cluster And inversely when a",
    "start": "1083039",
    "end": "1090160"
  },
  {
    "text": "cluster goes away then the provider has to disengage Disengage is at the moment it's a context cancel Maybe we move it",
    "start": "1090160",
    "end": "1097039"
  },
  {
    "text": "into a disengage function again for reasons Um but um basically that's it",
    "start": "1097039",
    "end": "1102480"
  },
  {
    "text": "That's the whole um yeah the whole technique behind the scenes what um provider and managers do in the",
    "start": "1102480",
    "end": "1108000"
  },
  {
    "text": "multicluster context Um to sum up a provider responsibility is to watch",
    "start": "1108000",
    "end": "1114080"
  },
  {
    "text": "changes in the cluster fleet This is highly specific to which um which cluster manager which fleet manager you",
    "start": "1114080",
    "end": "1121160"
  },
  {
    "text": "use and if there's new cluster construct the cache object and the client and put",
    "start": "1121160",
    "end": "1126640"
  },
  {
    "text": "it into the clusters controller runtime cluster um object Remember that in some maps so keep track of them of those",
    "start": "1126640",
    "end": "1133160"
  },
  {
    "text": "clusters engage them on the manager so the manager knows about them and when it",
    "start": "1133160",
    "end": "1138720"
  },
  {
    "text": "disengages so when the cluster goes away um cancel the context that's all what",
    "start": "1138720",
    "end": "1143760"
  },
  {
    "text": "you have to do bottlenecks um I I promise to talk about the the work here",
    "start": "1143760",
    "end": "1150640"
  },
  {
    "text": "um you have to think about what is the bottleneck of your reconciler now you might have many many clusters and um you",
    "start": "1150640",
    "end": "1157679"
  },
  {
    "text": "have to think how to um Yeah to organize um yeah maybe fair queuing So if there's",
    "start": "1157679",
    "end": "1164400"
  },
  {
    "text": "one cache which produces tons of events another one hardly any event maybe you",
    "start": "1164400",
    "end": "1170000"
  },
  {
    "text": "want something like individual cues here and then some fair queueing mechanism which picks events from every of those",
    "start": "1170000",
    "end": "1176960"
  },
  {
    "text": "um in a in a fair manner so that not one of them can exhaust um the capacity and",
    "start": "1176960",
    "end": "1182160"
  },
  {
    "text": "maybe priority cues will play a role um in the same way as in controller runtime So this this thing will depend on where",
    "start": "1182160",
    "end": "1188160"
  },
  {
    "text": "your bottlenecks are So you can imagine maybe the QPS of the client as a bottleneck could be or maybe um you run",
    "start": "1188160",
    "end": "1195440"
  },
  {
    "text": "a heavy um scheduleuler here which is super CPU heavy and um the CPU of the",
    "start": "1195440",
    "end": "1201039"
  },
  {
    "text": "pot is the bottleneck or maybe the workers um are the bottleneck So um",
    "start": "1201039",
    "end": "1206160"
  },
  {
    "text": "depending on that you have to think about what a what a good um queuing strate strategy",
    "start": "1206160",
    "end": "1211240"
  },
  {
    "text": "is and um I bet here in the room many will have experiences with that and also related topics like charting of",
    "start": "1211240",
    "end": "1217840"
  },
  {
    "text": "controllers So we are very open for ideas and previous experience to to design that At the moment um the sphere",
    "start": "1217840",
    "end": "1224160"
  },
  {
    "text": "queuing only exists as an idea and a very crude implementation Of course there's more work necessary So if you",
    "start": "1224160",
    "end": "1230080"
  },
  {
    "text": "have background in that um very welcome to hear about that Yeah And now let's go",
    "start": "1230080",
    "end": "1235679"
  },
  {
    "text": "to the demo I would say All right let's see if I can do that",
    "start": "1235679",
    "end": "1242799"
  },
  {
    "text": "this way But let's have a quick look at um you know how this works Like let me",
    "start": "1242799",
    "end": "1249120"
  },
  {
    "text": "let me show you a small little controller that I wrote It's an extension of the code that we've seen before And let me show",
    "start": "1249120",
    "end": "1257520"
  },
  {
    "text": "[Music] you let me show you this running against KCP So we start with KCP first So this",
    "start": "1257900",
    "end": "1266640"
  },
  {
    "text": "controller that I've written it it just looks for config maps um and when it",
    "start": "1266640",
    "end": "1272080"
  },
  {
    "text": "finds config maps in the KCP instance then it will log a line You know simplest reconciler you could write to",
    "start": "1272080",
    "end": "1278880"
  },
  {
    "text": "be honest because it doesn't do anything So something that you can see from the logs here already is that we have the",
    "start": "1278880",
    "end": "1286159"
  },
  {
    "text": "cluster name Um and I'm not sure if I well should be fine Um the cluster names",
    "start": "1286159",
    "end": "1292159"
  },
  {
    "text": "are in here in the lock line So each of these reconciles that you see here they come from a different cluster So from a",
    "start": "1292159",
    "end": "1298480"
  },
  {
    "text": "different source than what you've seen in the diagrams before and all of them are actually reconciling this config map",
    "start": "1298480",
    "end": "1303840"
  },
  {
    "text": "with the same name um which is the cube root CAert Uh but it's different instances of that object because they",
    "start": "1303840",
    "end": "1310480"
  },
  {
    "text": "come from different clusters So um how difficult would it be to switch this",
    "start": "1310480",
    "end": "1316159"
  },
  {
    "text": "controller now to use the kind provider which we've talked about before So",
    "start": "1316159",
    "end": "1321559"
  },
  {
    "text": "um let me quickly show you something namely me",
    "start": "1321559",
    "end": "1329080"
  },
  {
    "text": "typing Thank you very much So",
    "start": "1329080",
    "end": "1334240"
  },
  {
    "text": "all right got it Uh okay So this is basically all the changes that I need to do uh when I want to switch providers So",
    "start": "1334240",
    "end": "1342200"
  },
  {
    "text": "um I import a different provider I set it up instead of the previous KCP",
    "start": "1342200",
    "end": "1348320"
  },
  {
    "text": "provider that I've been using and I'm done So if I",
    "start": "1348320",
    "end": "1354440"
  },
  {
    "text": "now check all good",
    "start": "1354440",
    "end": "1361440"
  },
  {
    "text": "um if I check this out now and it's of course the wrong checkout",
    "start": "1361440",
    "end": "1368200"
  },
  {
    "text": "commit So let's see should be this one Perfect So now I'm using the kind",
    "start": "1368200",
    "end": "1375679"
  },
  {
    "text": "provider the like the changes that you've seen and I do a go run",
    "start": "1375679",
    "end": "1381000"
  },
  {
    "text": "again and now my provider will pick up config maps coming from a kind cluster",
    "start": "1381000",
    "end": "1386559"
  },
  {
    "text": "that is running on my local machine and I could also start another kind cluster",
    "start": "1386559",
    "end": "1392159"
  },
  {
    "text": "It will dynamically be uh put into here and then it will reconile against two",
    "start": "1392159",
    "end": "1397760"
  },
  {
    "text": "clusters but this is how easy it was for me to switch from one provider to the other And let's have a brief look at how",
    "start": "1397760",
    "end": "1405840"
  },
  {
    "text": "complex that kind provider is because the answer is not that much So this is basically the the gist",
    "start": "1405840",
    "end": "1414320"
  },
  {
    "text": "of this provider Um it is rating in a loop It is listing the available kind",
    "start": "1414320",
    "end": "1419840"
  },
  {
    "text": "clusters it's doing some uh some uh filtering on that and then it goes",
    "start": "1419840",
    "end": "1425360"
  },
  {
    "text": "through the process of creating a cube config of eventually setting up a cache and a client and um basically",
    "start": "1425360",
    "end": "1432720"
  },
  {
    "text": "remembering that So that is one of the things that Stefan mentioned earlier and",
    "start": "1432720",
    "end": "1438400"
  },
  {
    "text": "that is really it So this whole file with you know all the type definitions in the in the beginning it's 200 lines",
    "start": "1438400",
    "end": "1445600"
  },
  {
    "text": "That's it That's the provider I'm done",
    "start": "1445600",
    "end": "1451158"
  },
  {
    "text": "So with that in",
    "start": "1452200",
    "end": "1455919"
  },
  {
    "text": "mind I think we're a bit slow on time Yeah And we have a couple of providers",
    "start": "1457720",
    "end": "1463120"
  },
  {
    "text": "as a prototype already in the repository but nothing fancy It's really PC's So we",
    "start": "1463120",
    "end": "1468559"
  },
  {
    "text": "have a cluster API here Um the kind provider we just saw some experimental",
    "start": "1468559",
    "end": "1474240"
  },
  {
    "text": "namespace provider Um and yeah basically two trivial ones one which has no cluster at all one is just one cluster",
    "start": "1474240",
    "end": "1481279"
  },
  {
    "text": "super uninteresting Um our goal is not to have all the providers here obviously",
    "start": "1481279",
    "end": "1486400"
  },
  {
    "text": "So a monor repo is an antiattern Um so if you build something like a really",
    "start": "1486400",
    "end": "1491679"
  },
  {
    "text": "serious cluster API provider um probably outside is a better place Uh we have uh",
    "start": "1491679",
    "end": "1498000"
  },
  {
    "text": "the KCP provider for example here So it's in KCPDE dev multicluster runtime and yeah it implements what Marvin has",
    "start": "1498000",
    "end": "1504880"
  },
  {
    "text": "just shown um uses virtual workspace API endpoint for exports Um and yeah that's",
    "start": "1504880",
    "end": "1512240"
  },
  {
    "text": "the KCP one Hopefully we get more So if you have ideas um I'm looking at the",
    "start": "1512240",
    "end": "1517520"
  },
  {
    "text": "cloud provider attendees here Um would be interesting to to run something",
    "start": "1517520",
    "end": "1522960"
  },
  {
    "text": "against all clusters in your project or something like that Um what is next so",
    "start": "1522960",
    "end": "1528320"
  },
  {
    "text": "we have many ideas Um the exper experimental message here should eventually go away I guess",
    "start": "1528320",
    "end": "1535120"
  },
  {
    "text": "Yeah exactly So as we said it's I forgot Um so yeah so far this is an",
    "start": "1535120",
    "end": "1542640"
  },
  {
    "text": "experiment Um but of course we want to stabilize it and you know make it usable for everyone and basically hope that it",
    "start": "1542640",
    "end": "1548960"
  },
  {
    "text": "you know gives you some value So we have you know so many ideas we don't have the time to talk about them Maybe out of",
    "start": "1548960",
    "end": "1555039"
  },
  {
    "text": "process providers maybe you know as Chefan said better providers a cluster inventory provider for the multicluster",
    "start": "1555039",
    "end": "1561600"
  },
  {
    "text": "API and you know go on go on there there's so many things to do Um next",
    "start": "1561600",
    "end": "1567440"
  },
  {
    "text": "thing that we want to do is gather some experience with existing code bases We have some patch sets with like trying",
    "start": "1567440",
    "end": "1573279"
  },
  {
    "text": "things out but of course we would love to work with projects to see if they can gain value from adopting multicluster",
    "start": "1573279",
    "end": "1579600"
  },
  {
    "text": "runtime and also see if we're missing the mark because this is you know something we want to build for the",
    "start": "1579600",
    "end": "1585120"
  },
  {
    "text": "community and you all know the XKCD comic Uh we hope to not like create another standard but something that is",
    "start": "1585120",
    "end": "1592080"
  },
  {
    "text": "meaningful So we need to hear from you if this is really what you need Um or if there's something you know it's too",
    "start": "1592080",
    "end": "1598080"
  },
  {
    "text": "simple it's too simple it's too complex it misses the mark So please please tell",
    "start": "1598080",
    "end": "1603159"
  },
  {
    "text": "us And with that thank you for listening These are the QR codes for the repository for the Zig multicluster",
    "start": "1603159",
    "end": "1608960"
  },
  {
    "text": "select channel and um for rating this talk whether you liked it or not We'd",
    "start": "1608960",
    "end": "1614000"
  },
  {
    "text": "love to hear your feedback Thank you very much Thank you",
    "start": "1614000",
    "end": "1619400"
  },
  {
    "text": "So if we have we have a few minutes for questions if there are any and there's a microphone in the",
    "start": "1623279",
    "end": "1630400"
  },
  {
    "text": "middle if anyone wants to ask one",
    "start": "1630400",
    "end": "1635000"
  },
  {
    "text": "I have a question Um in probably your second this is not a",
    "start": "1637840",
    "end": "1645080"
  },
  {
    "text": "multicluster situation If you remember you have a central manager managing",
    "start": "1645080",
    "end": "1651679"
  },
  {
    "text": "other clusters Finally what is different with this multicluster manager or",
    "start": "1651679",
    "end": "1657120"
  },
  {
    "text": "multicluster controller because if I understand correctly it's still living",
    "start": "1657120",
    "end": "1662159"
  },
  {
    "text": "in one cluster and it is able to reconcile all the clusters Yeah But it's",
    "start": "1662159",
    "end": "1668080"
  },
  {
    "text": "still master or central So it's like your management cluster you were illustrating but different I don't know",
    "start": "1668080",
    "end": "1676559"
  },
  {
    "text": "So you can run multiple managers This pattern works is totally valid and for",
    "start": "1676559",
    "end": "1682279"
  },
  {
    "text": "uniform controllers it's yeah you can do that and people do it Um what you get",
    "start": "1682279",
    "end": "1688240"
  },
  {
    "text": "here is tighter integration like the ver so you can better control what your pot",
    "start": "1688240",
    "end": "1694559"
  },
  {
    "text": "is actually doing Um if you have multiple managers they're completely independent right um they do their thing",
    "start": "1694559",
    "end": "1700080"
  },
  {
    "text": "and they use resources um as they want Um you have no control over that Okay So the network architecture is the same You",
    "start": "1700080",
    "end": "1707760"
  },
  {
    "text": "have one central management cluster with the multicluster controller Yeah But",
    "start": "1707760",
    "end": "1714000"
  },
  {
    "text": "just the fact it's cubernet is native and integrated as a controller following",
    "start": "1714000",
    "end": "1720399"
  },
  {
    "text": "all the patterns the cues all the things it's the main difference if I",
    "start": "1720399",
    "end": "1725919"
  },
  {
    "text": "understand not sure I understand the fact the",
    "start": "1725919",
    "end": "1731240"
  },
  {
    "text": "multi-cluster controller is well integrated using the cues as you",
    "start": "1731240",
    "end": "1738279"
  },
  {
    "text": "said and so maybe think",
    "start": "1738279",
    "end": "1745960"
  },
  {
    "text": "well I mean so the it's basically you know what what what part of the controller are you scaling right so um",
    "start": "1746240",
    "end": "1752159"
  },
  {
    "text": "in the other examples we had like one manager one like Q uh sorry one one one",
    "start": "1752159",
    "end": "1758000"
  },
  {
    "text": "source one one event handler all of that and you just you know you scale at a different level you have one manager and",
    "start": "1758000",
    "end": "1764080"
  },
  {
    "text": "you have one reconcile and you have one work Q but you're scaling in the middle basically because you can you know start",
    "start": "1764080",
    "end": "1769840"
  },
  {
    "text": "multiple sources so I think that's just the difference in the scaling mechanism Okay",
    "start": "1769840",
    "end": "1776840"
  },
  {
    "text": "Um thank you guys for contributing this This is really awesome and I look forward for the integration with cluster",
    "start": "1778159",
    "end": "1783840"
  },
  {
    "text": "profile provider on this sig multicluster Um the question for you",
    "start": "1783840",
    "end": "1789279"
  },
  {
    "text": "would be around graduation from the experiment to GA What do you see uh as",
    "start": "1789279",
    "end": "1795120"
  },
  {
    "text": "the most important thing you want to tackle and maybe a time frame for that i think the main input we need is the",
    "start": "1795120",
    "end": "1802640"
  },
  {
    "text": "feedback that this works like this model is a correct one So we have some",
    "start": "1802640",
    "end": "1808000"
  },
  {
    "text": "experiments taking existing bigger code bases and trying to implement that but more of this kind would be helpful So we",
    "start": "1808000",
    "end": "1816080"
  },
  {
    "text": "have seen the other models um which also exist and they are used does this bring value um that's the question basically",
    "start": "1816080",
    "end": "1823760"
  },
  {
    "text": "if we agree if we agree on the topology and it makes sense then I think this is the bar Do we want to put a number on it",
    "start": "1823760",
    "end": "1831039"
  },
  {
    "text": "or I don't know Oh no the the number of this implementation is like 3 weeks four",
    "start": "1831039",
    "end": "1837679"
  },
  {
    "text": "weeks since we started basically or since we made the project uh public So",
    "start": "1837679",
    "end": "1843360"
  },
  {
    "text": "could be a matter of weeks matter of the feedback we get here So thank you",
    "start": "1843360",
    "end": "1851200"
  },
  {
    "text": "A couple of questions Really quick one First uh I didn't see the slides attached in the app I don't know if that",
    "start": "1851200",
    "end": "1856720"
  },
  {
    "text": "was a mistake with the app or if they've not been uploaded but if not do you plan to add the slides we'll add them Yep",
    "start": "1856720",
    "end": "1863360"
  },
  {
    "text": "Great Thanks Uh the the main question then um is there support for um",
    "start": "1863360",
    "end": "1868399"
  },
  {
    "text": "heterogeneous providers so if I've got uh multiple clusters some on AWS some on",
    "start": "1868399",
    "end": "1873679"
  },
  {
    "text": "GCP for example Um yeah I I I think you can just um build a union provider and",
    "start": "1873679",
    "end": "1879440"
  },
  {
    "text": "attach multiple of them I mean the interface you have seen right there's a getter and there's the index function so this should be possible Yeah I guess",
    "start": "1879440",
    "end": "1886240"
  },
  {
    "text": "Yeah Cool Thank you Um hi I'm pretty excited about this",
    "start": "1886240",
    "end": "1893520"
  },
  {
    "text": "project as someone who manages a relatively large uncommonly large number",
    "start": "1893520",
    "end": "1898880"
  },
  {
    "text": "of clusters in production and I think this could really help us and like help us also control costs So I'm pretty",
    "start": "1898880",
    "end": "1904640"
  },
  {
    "text": "excited about it Uh the main question I have for you guys is how to um like what",
    "start": "1904640",
    "end": "1911919"
  },
  {
    "text": "how how this is going to work with uh CRDs that might not be the same version",
    "start": "1911919",
    "end": "1919679"
  },
  {
    "text": "um in the various different clusters that are being managed Yeah technically um in with the exception of the KCP",
    "start": "1919679",
    "end": "1926320"
  },
  {
    "text": "provider this is different but um the providers we have shown here there's one cube config per cluster So there's an",
    "start": "1926320",
    "end": "1932559"
  },
  {
    "text": "informer um behind the scene one cache per cluster Mhm And um this will use a",
    "start": "1932559",
    "end": "1938480"
  },
  {
    "text": "version of the specific cluster So there is no difference in your reconciler You might have to distinguish right you will",
    "start": "1938480",
    "end": "1944559"
  },
  {
    "text": "use go types and maybe there's an old or a new cluster So you could imagine that",
    "start": "1944559",
    "end": "1949679"
  },
  {
    "text": "you built some meta information into your provider You can query and then you know which version is installed or",
    "start": "1949679",
    "end": "1955600"
  },
  {
    "text": "something in this direction Okay So so within the reconcile loop it would uh try to differentiate between the the",
    "start": "1955600",
    "end": "1962720"
  },
  {
    "text": "older version of the CRD and and possibly a newer version and okay when you can you can ask the manager for the",
    "start": "1962720",
    "end": "1968880"
  },
  {
    "text": "provider you do some interface uh type check whether it has this meta information interface and then you ask",
    "start": "1968880",
    "end": "1974799"
  },
  {
    "text": "which C something this direction okay will work cool thank you",
    "start": "1974799",
    "end": "1981158"
  },
  {
    "text": "thank you so in the code walk through that Marvin was doing in a kind pro kind",
    "start": "1981360",
    "end": "1986960"
  },
  {
    "text": "provider right uh I saw you were initializing indexes and caches yourself",
    "start": "1986960",
    "end": "1992320"
  },
  {
    "text": "uh like is that going to be responsibility of the provider as opposed to just providing the provider",
    "start": "1992320",
    "end": "1998240"
  },
  {
    "text": "yeah because it differs by by um use case I think there's one class of providers which basically provide cube",
    "start": "1998240",
    "end": "2004960"
  },
  {
    "text": "configs and then we could basically build a library which unifies that if you want to if that's the model you you",
    "start": "2004960",
    "end": "2011360"
  },
  {
    "text": "want to implement yeah I mean the re like from a layering perspective it sounds like the cluster provider would",
    "start": "2011360",
    "end": "2016799"
  },
  {
    "text": "probably just provide you the cube config right and most everything else probably is common across all providers",
    "start": "2016799",
    "end": "2022640"
  },
  {
    "text": "for most of them Yeah Yeah So we need something like a unified frame I mean",
    "start": "2022640",
    "end": "2027919"
  },
  {
    "text": "framework is a big word here I think um a package which does that and the GPC topic like the remote or out of process",
    "start": "2027919",
    "end": "2034799"
  },
  {
    "text": "provider maybe it just provides cube configs as simple as that and everything else is done process and what's the",
    "start": "2034799",
    "end": "2041440"
  },
  {
    "text": "position of the controller runtime maintainers on merging the two um I didn't mention that in this in this",
    "start": "2041440",
    "end": "2048079"
  },
  {
    "text": "experimental message there there was a link to the design um in controller runtime and Um it's a friendly extension",
    "start": "2048079",
    "end": "2056158"
  },
  {
    "text": "Um so Stefan is here He proposed actually to try that So um maybe",
    "start": "2056159",
    "end": "2061839"
  },
  {
    "text": "eventually it might move back when we agree on the shape of the thing This is one possibility if we need something",
    "start": "2061839",
    "end": "2068800"
  },
  {
    "text": "more in controller runtime Um I hope they are open um to add it Um I agree",
    "start": "2068800",
    "end": "2077200"
  }
]