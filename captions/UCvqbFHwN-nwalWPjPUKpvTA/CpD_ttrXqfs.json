[
  {
    "start": "0",
    "end": "39000"
  },
  {
    "text": "hi everyone thanks for joining us on this talk about latest scalability",
    "start": "179",
    "end": "8250"
  },
  {
    "text": "improvements that have been happening with kubernetes I'm Sean I am one of the",
    "start": "8250",
    "end": "14420"
  },
  {
    "text": "developers on the kubernetes project I've been working on critters for a little more than two and a half years",
    "start": "14420",
    "end": "19740"
  },
  {
    "text": "and I'm one of the chairs for six scalability under machine to journey of",
    "start": "19740",
    "end": "26490"
  },
  {
    "text": "a member of technical staff VMware I've been working on kubernetes for more than two years now all right so let's start",
    "start": "26490",
    "end": "35130"
  },
  {
    "text": "with a little bit of background so Kuban I just started scaling to large clusters",
    "start": "35130",
    "end": "42899"
  },
  {
    "start": "39000",
    "end": "39000"
  },
  {
    "text": "a while ago and this was around two years ago I think it was in Cuba on",
    "start": "42899",
    "end": "48090"
  },
  {
    "text": "Berlin where we made a big announcement that Cuba native scales to 5,000 nodes",
    "start": "48090",
    "end": "53489"
  },
  {
    "text": "and this got a lot of people excited and indeed that is the way it should be and",
    "start": "53489",
    "end": "61430"
  },
  {
    "text": "consequently a lot of people started trying out bigger clusters in their",
    "start": "61430",
    "end": "67619"
  },
  {
    "text": "production environment and they were trying out different configurations and",
    "start": "67619",
    "end": "72659"
  },
  {
    "text": "different features and soon these exposed some bottlenecks that we did not",
    "start": "72659",
    "end": "80280"
  },
  {
    "text": "know before over time some particular usages showed us that some of the limits",
    "start": "80280",
    "end": "86610"
  },
  {
    "text": "we are we are hitting in the system and we we started looking into those",
    "start": "86610",
    "end": "92130"
  },
  {
    "text": "bottlenecks and the important thing is we realized that we need to move to a",
    "start": "92130",
    "end": "98520"
  },
  {
    "text": "new definition of scalability until that point we were mostly looking at",
    "start": "98520",
    "end": "105329"
  },
  {
    "text": "scalability as mainly about the size of the cluster or the number of nodes in",
    "start": "105329",
    "end": "111390"
  },
  {
    "text": "the cluster but scalability is really much broader than that because there are so",
    "start": "111390",
    "end": "118480"
  },
  {
    "text": "many dimensions as you can see on on that picture there are so many dimensions that Cuba netis has and you",
    "start": "118480",
    "end": "126100"
  },
  {
    "text": "can stress along each of those so you you have number of nodes you have number of pods of pods on Pol Pot density",
    "start": "126100",
    "end": "133540"
  },
  {
    "text": "number of services and and a lot of these things so Cuba so scalability is really a multi-dimensional problem and",
    "start": "133540",
    "end": "140400"
  },
  {
    "text": "we realize that with the bottlenecks that we came to know from a lot of users we realize that there are some",
    "start": "140400",
    "end": "145960"
  },
  {
    "text": "improvements that we need to make in different areas and we ended up improving a lot of these so I'd go ahead",
    "start": "145960",
    "end": "154150"
  },
  {
    "text": "and we'll go and look at some of these new and exciting improvements that have",
    "start": "154150",
    "end": "160540"
  },
  {
    "text": "happened in the last few months so the first one is about having too many node",
    "start": "160540",
    "end": "167380"
  },
  {
    "start": "162000",
    "end": "162000"
  },
  {
    "text": "revisions in Cuba knitters cubelets post",
    "start": "167380",
    "end": "172900"
  },
  {
    "text": "periodic heartbeats to the control plane to tell the control plane that the node",
    "start": "172900",
    "end": "178780"
  },
  {
    "text": "is still alive so this is done every 10 seconds so this means that the cubelet",
    "start": "178780",
    "end": "183940"
  },
  {
    "text": "posts a new version of the node object once every 10 seconds and each of these",
    "start": "183940",
    "end": "190420"
  },
  {
    "text": "objects each of this revision that is posted is stored as one revision on HCD",
    "start": "190420",
    "end": "197440"
  },
  {
    "text": "and if your node object is really big probably because there are too many",
    "start": "197440",
    "end": "203140"
  },
  {
    "text": "images that have been pulled onto the node or there are there are a lot of volumes that have been attached to the",
    "start": "203140",
    "end": "208150"
  },
  {
    "text": "node which can make the node object really big like 10 KB or even more then",
    "start": "208150",
    "end": "214140"
  },
  {
    "text": "then that multiplied with this factor that there are so many revisions of the object can eat up a lot of space and",
    "start": "214140",
    "end": "220989"
  },
  {
    "text": "even worse when there is a really big cluster like let's say 5,000 nodes the this gets multiplied by a factor of",
    "start": "220989",
    "end": "227440"
  },
  {
    "text": "5,000 which means your HDD is pretty much ends up being filled with just node objects and we have actually seen this",
    "start": "227440",
    "end": "233320"
  },
  {
    "text": "happen to some some really big users of Cuba natives",
    "start": "233320",
    "end": "238670"
  },
  {
    "text": "and once a theory fills up your cluster is pretty much frozen with respect to",
    "start": "238670",
    "end": "244880"
  },
  {
    "text": "rights because it's silly can't take anymore right requests it enters a read-only model because of this problem",
    "start": "244880",
    "end": "250460"
  },
  {
    "text": "there are a few dimensions that software it's mainly the number of nodes and also",
    "start": "250460",
    "end": "256910"
  },
  {
    "text": "the number of images or volumes that you can have per node so so how did we solve",
    "start": "256910",
    "end": "262010"
  },
  {
    "text": "this problem we started out with a simple idea of splitting away heartbeats",
    "start": "262010",
    "end": "269900"
  },
  {
    "text": "from the node objects because conceptually heart beats is something",
    "start": "269900",
    "end": "275330"
  },
  {
    "text": "different from the specification of the node like the spec and the status of the node so we introduced a new API called",
    "start": "275330",
    "end": "283940"
  },
  {
    "text": "the leases API for for representing these heartbeats and because now we've",
    "start": "283940",
    "end": "290780"
  },
  {
    "text": "separated heartbeats from the node objects we are able to independently update both we don't have to tie",
    "start": "290780",
    "end": "295820"
  },
  {
    "text": "together like updates of node status and these heartbeats this is a tricky thing",
    "start": "295820",
    "end": "302510"
  },
  {
    "text": "to make work because there are some many small caveats that we had to think about",
    "start": "302510",
    "end": "309530"
  },
  {
    "text": "and making this backward compatible with all clients and and things like that so",
    "start": "309530",
    "end": "317920"
  },
  {
    "text": "so the goal of doing this is that once we separate heartbeats from the node",
    "start": "317920",
    "end": "323450"
  },
  {
    "text": "objects these heartbeat objects are really small so even if you have many revisions of this it's going to be okay",
    "start": "323450",
    "end": "328760"
  },
  {
    "text": "and the node object now can only cubelet can only make updates to the node object",
    "start": "328760",
    "end": "336080"
  },
  {
    "text": "when there's actually a meaningful change in the status no in the node status so we go ahead and look at this",
    "start": "336080",
    "end": "344270"
  },
  {
    "text": "new API but just a little small node on the feature availability this is in",
    "start": "344270",
    "end": "350240"
  },
  {
    "text": "alpha in 113 and in 114 this is in beta then the node heartbeat and it is also",
    "start": "350240",
    "end": "355820"
  },
  {
    "text": "turned on by default so if you're running your 114 cluster expect to find these heartbeat objects in your clusters",
    "start": "355820",
    "end": "363640"
  },
  {
    "text": "so let me quickly go through this new API I won't go into too much details but",
    "start": "363640",
    "end": "369909"
  },
  {
    "text": "the the object on the left is the node heartbeat the blue fields represents basically whatever the things that you",
    "start": "369909",
    "end": "377239"
  },
  {
    "text": "should be focusing on it's a pretty small object it holds a few things like what is the name of the holder of this",
    "start": "377239",
    "end": "385009"
  },
  {
    "text": "lease which is the name of the node itself and how long the lease is valid for and what time it was last renewed so",
    "start": "385009",
    "end": "393109"
  },
  {
    "text": "cubelet is now responsible for periodically updating this object and the node status now no longer has to",
    "start": "393109",
    "end": "398929"
  },
  {
    "text": "include the no longer has to publish these new heartbeat time stamps so if",
    "start": "398929",
    "end": "404509"
  },
  {
    "text": "you basically isolated both problems all right the next problem is with cubelet",
    "start": "404509",
    "end": "411379"
  },
  {
    "start": "408000",
    "end": "408000"
  },
  {
    "text": "periodically polling for secrets and config maps so for all the pods that",
    "start": "411379",
    "end": "418149"
  },
  {
    "text": "cubelet runs it has to get the secrets and config maps that those pods reference the way it was doing it up",
    "start": "418149",
    "end": "426619"
  },
  {
    "text": "until awhile ago was periodically polling for them from the API server by making get requests for each of these",
    "start": "426619",
    "end": "432439"
  },
  {
    "text": "individual secrets and config maps and this ended up being really expensive",
    "start": "432439",
    "end": "437929"
  },
  {
    "text": "from the API server perspective especially when there's a really big",
    "start": "437929",
    "end": "443199"
  },
  {
    "text": "cluster where there are so many nodes and and each node like needs a lot of",
    "start": "443199",
    "end": "448249"
  },
  {
    "text": "secrets and config maps because that eats away a significant chunk of the API service request queue starving starving",
    "start": "448249",
    "end": "456049"
  },
  {
    "text": "it from other useful requests so there are some short-term stopgap s-- to kind",
    "start": "456049",
    "end": "463489"
  },
  {
    "text": "of mitigate this issue to some extent we introduced caching on the client side for these secrets and config maps and we",
    "start": "463489",
    "end": "470299"
  },
  {
    "text": "reduce the pole frequency for larger clusters to like reduce the effect of",
    "start": "470299",
    "end": "475779"
  },
  {
    "text": "these large number of calls but they're not really solving the problem from the",
    "start": "475779",
    "end": "481099"
  },
  {
    "text": "root and they're more like just band-aids to the problem so so because",
    "start": "481099",
    "end": "486859"
  },
  {
    "text": "of this the dimensions that suffer are mainly the number of nodes and the number of secrets and config maps that",
    "start": "486859",
    "end": "492409"
  },
  {
    "text": "are needed by your nodes we saw this again with something that seems like a",
    "start": "492409",
    "end": "498439"
  },
  {
    "text": "simple idea we just to watch for these secrets instead of poll poll for these secrets",
    "start": "498439",
    "end": "504060"
  },
  {
    "text": "periodically and watching is much lighter from the control plane perspective so so yeah the the feature",
    "start": "504060",
    "end": "514440"
  },
  {
    "text": "availability for this is a little bit tricky because we actually enabled this cubelet watching secret watching in",
    "start": "514440",
    "end": "521159"
  },
  {
    "text": "release 112 but we later realized that there was a tricky bug with with : that",
    "start": "521159",
    "end": "529800"
  },
  {
    "text": "caused some issues when when when there are some nodes which have too many secrets or conflict conflict maps I'll",
    "start": "529800",
    "end": "536300"
  },
  {
    "text": "briefly explain what that issue is but basically what you need to know is this",
    "start": "536300",
    "end": "542520"
  },
  {
    "text": "feature is not currently like usable in 112 and 113 releases but it has been",
    "start": "542520",
    "end": "548910"
  },
  {
    "text": "enabled in 114 to patch release and that patch ileus actually has the golang bug",
    "start": "548910",
    "end": "555510"
  },
  {
    "text": "fix as well so you can safely use this feature and get the performance benefit of watching so the bug itself is that",
    "start": "555510",
    "end": "563990"
  },
  {
    "text": "golang 110 and 111 made a breaking change with the HTTP client library that",
    "start": "564380",
    "end": "570680"
  },
  {
    "text": "when when all the streams of your TCP connection are exhausted the client does",
    "start": "570680",
    "end": "576510"
  },
  {
    "text": "not automatically start a new TCP connection and in Kuban it is we multiplex watches on to a single TCP",
    "start": "576510",
    "end": "582450"
  },
  {
    "text": "connection and each watch uses one stream so as soon as like there are 250 secrets or config Maps you have 250",
    "start": "582450",
    "end": "590250"
  },
  {
    "text": "watches and that that eats away your TCP connection in the node can then go dead this was fixed this behavior is fixing go 112 so we",
    "start": "590250",
    "end": "598650"
  },
  {
    "text": "went ahead and fixed that for release 114 all right so I PD tables performance",
    "start": "598650",
    "end": "605420"
  },
  {
    "start": "601000",
    "end": "601000"
  },
  {
    "text": "raise your hands if you if any of you have ever had any issue with performance",
    "start": "605420",
    "end": "611280"
  },
  {
    "text": "of IP tables okay that's quite some bunch all right so so IP tables is used",
    "start": "611280",
    "end": "620370"
  },
  {
    "text": "by cuban it is to program the networking layer on the nodes for service routing",
    "start": "620370",
    "end": "628709"
  },
  {
    "text": "and load bar and sing within the cluster and IP Devils has been around for a long time",
    "start": "628709",
    "end": "634259"
  },
  {
    "text": "with the Linux kernel which of course comes with its own advantages that it's a really mature tool and it doesn't have",
    "start": "634259",
    "end": "642660"
  },
  {
    "text": "many bugs and it is quite feature-rich but as come with the advantages there",
    "start": "642660",
    "end": "648929"
  },
  {
    "text": "are also some disadvantages of legacy systems and that it was not very well written with respect to performance the",
    "start": "648929",
    "end": "656999"
  },
  {
    "text": "main problem is that IP table stores the rules in the chain as a linear list so",
    "start": "656999",
    "end": "663869"
  },
  {
    "text": "so this means that whenever you have a large number of services and so so each",
    "start": "663869",
    "end": "670800"
  },
  {
    "text": "service is represented as one rule in this IP tables chain so when their large",
    "start": "670800",
    "end": "676379"
  },
  {
    "text": "number of services this chain becomes really big and this affects both your IP tables data plane operations and control",
    "start": "676379",
    "end": "682470"
  },
  {
    "text": "plane operations so data plane operations is basically routing of the packets which is done by evaluating",
    "start": "682470",
    "end": "688829"
  },
  {
    "text": "those rules and because it's a linear list you have to go through the whole list so it can be really slow for to to",
    "start": "688829",
    "end": "694949"
  },
  {
    "text": "route the packets and the control plane operations are changing these rules themselves which is updating IP tables",
    "start": "694949",
    "end": "701309"
  },
  {
    "text": "and IP tables doesn't allow you to incrementally add new rules so what you",
    "start": "701309",
    "end": "707579"
  },
  {
    "text": "need to do is you need to like get the current chain and add a new rule and just post this whole thing which can",
    "start": "707579",
    "end": "713490"
  },
  {
    "text": "take up to a really long time and it can take up to like 30 seconds if you have many services like of the order of",
    "start": "713490",
    "end": "720179"
  },
  {
    "text": "thousands of services so because of this some of the dimensions that suffer of the number of services and the size of",
    "start": "720179",
    "end": "726600"
  },
  {
    "text": "services like how many pods you can have in a service so how did we solve this IP",
    "start": "726600",
    "end": "733319"
  },
  {
    "text": "vs so I'm slightly hesitant to call this",
    "start": "733319",
    "end": "738569"
  },
  {
    "text": "a solution just yet because partly",
    "start": "738569",
    "end": "746249"
  },
  {
    "text": "because the community is still on its way to decide and make up its mind on if",
    "start": "746249",
    "end": "752100"
  },
  {
    "text": "this is actually the route we want to take down the lane if this is the long",
    "start": "752100",
    "end": "757230"
  },
  {
    "text": "term path we want to choose and also because IP vs even though it is",
    "start": "757230",
    "end": "762270"
  },
  {
    "text": "just like GA in 111 it's it's it's something that is still kind of like",
    "start": "762270",
    "end": "767880"
  },
  {
    "text": "considered in the experimental mode because not many providers have like yet started supporting it in production and",
    "start": "767880",
    "end": "774450"
  },
  {
    "text": "also it's a relatively newer thing is compared to iptables so it's still like",
    "start": "774450",
    "end": "779660"
  },
  {
    "text": "going through the bug stabilization phase alright so so what is IP vs IP V s",
    "start": "779660",
    "end": "785700"
  },
  {
    "text": "is an alternative to IP tables especially for the service load balancing part it's mainly a it's mainly",
    "start": "785700",
    "end": "793650"
  },
  {
    "text": "written keeping load balancing part in mind so it also holds the holds rules -",
    "start": "793650",
    "end": "801480"
  },
  {
    "text": "rules for packet routing but it stores them in a different way as compared to",
    "start": "801480",
    "end": "807450"
  },
  {
    "text": "IP tables it uses better data structures which is it uses hash tables and hash",
    "start": "807450",
    "end": "813660"
  },
  {
    "text": "tables are much more efficient as compared to linear list so so basically",
    "start": "813660",
    "end": "819840"
  },
  {
    "text": "that that allows for much more efficient operations with I with data plane and",
    "start": "819840",
    "end": "825630"
  },
  {
    "text": "control plane operations that I was talking about earlier and it gives really high scale with respect to",
    "start": "825630",
    "end": "831210"
  },
  {
    "text": "services it's still missing some functionality and still a way for it to",
    "start": "831210",
    "end": "836280"
  },
  {
    "text": "mature but this is one really good option for us right now all right so one",
    "start": "836280",
    "end": "844290"
  },
  {
    "start": "843000",
    "end": "843000"
  },
  {
    "text": "of the issues that we encountered is also scheduled in performance some",
    "start": "844290",
    "end": "849360"
  },
  {
    "text": "workload really needs some high scaling performance and on large clusters",
    "start": "849360",
    "end": "854970"
  },
  {
    "text": "scheduling throughput is usually around 90 pot per second on a 2,000 clone",
    "start": "854970",
    "end": "860540"
  },
  {
    "text": "cluster and 30 pots per second on a $5,000 sir when we use the affinity",
    "start": "860540",
    "end": "869010"
  },
  {
    "text": "feature it drops down to 5 parts per minute on a 5,000 Kuster so here the dimensions that are",
    "start": "869010",
    "end": "874980"
  },
  {
    "text": "suffering our pattern and the number of nodes so how did we solve this",
    "start": "874980",
    "end": "887070"
  },
  {
    "text": "first of all one of the optimization that we've done is we score only a",
    "start": "887070",
    "end": "893360"
  },
  {
    "text": "subset of note meaning that we have the notes of our cluster we start filtering",
    "start": "893360",
    "end": "899399"
  },
  {
    "text": "the notes using predicates and once we found a subset of notes we start only",
    "start": "899399",
    "end": "906360"
  },
  {
    "text": "scour a scoring priority function only against this subset so it makes it cheaper also we improved the affinity",
    "start": "906360",
    "end": "914250"
  },
  {
    "text": "computing by split it in splitting it in two phases first one is that we find all",
    "start": "914250",
    "end": "920940"
  },
  {
    "text": "the parts that are matching the affinity term and then on all the subsets of pods",
    "start": "920940",
    "end": "927329"
  },
  {
    "text": "the art that are matching the affinity terms we start filtering by topology key",
    "start": "927329",
    "end": "933000"
  },
  {
    "text": "so this actually also make it cheaper and for the pod scheduling latency by",
    "start": "933000",
    "end": "940620"
  },
  {
    "text": "profiling the scheduler we found on one scheduling cycle the scheduler spends",
    "start": "940620",
    "end": "946139"
  },
  {
    "text": "most of its its time snapshot in the scatterers cache so it",
    "start": "946139",
    "end": "951839"
  },
  {
    "text": "can get a consistent view of the cluster state so one of the optimizations that",
    "start": "951839",
    "end": "957449"
  },
  {
    "text": "we've done is we change the data structure of the schedulers cache to",
    "start": "957449",
    "end": "964769"
  },
  {
    "text": "make the snapshot an even faster as for feature availability the scoring and the",
    "start": "964769",
    "end": "970860"
  },
  {
    "text": "affinity are available since 112 and the cache snapshotting is available since",
    "start": "970860",
    "end": "976940"
  },
  {
    "text": "114 okay so another issue also is the",
    "start": "976940",
    "end": "984870"
  },
  {
    "start": "979000",
    "end": "979000"
  },
  {
    "text": "way we handle events in kubernetes we've encountered multiple scalability issues",
    "start": "984870",
    "end": "992040"
  },
  {
    "text": "with events either they fill the HDD database our also overloading the API",
    "start": "992040",
    "end": "1000470"
  },
  {
    "text": "server by making requests my clients making requests to create events also we",
    "start": "1000470",
    "end": "1006980"
  },
  {
    "text": "had a very poor user experience to query events use tools such as monitoring tracing and",
    "start": "1006980",
    "end": "1015550"
  },
  {
    "text": "automation to use these events and there are some dimension that we wanted to",
    "start": "1015550",
    "end": "1021580"
  },
  {
    "text": "improve which are the number of the API calls the number of note the arm for example emitting events and pot chart",
    "start": "1021580",
    "end": "1030600"
  },
  {
    "text": "the way we solve this is basically we introduced a new events API and with",
    "start": "1030930",
    "end": "1037990"
  },
  {
    "text": "this new events API we introduced a new deduplication logic this logic",
    "start": "1037990",
    "end": "1044760"
  },
  {
    "text": "introduces a concept which is called zoomorphic events two events are is",
    "start": "1044760",
    "end": "1051160"
  },
  {
    "text": "amorphic if they have the same subset of fields for example the reporting",
    "start": "1051160",
    "end": "1056560"
  },
  {
    "text": "controller the reporting instance the type of the event the action that was",
    "start": "1056560",
    "end": "1062050"
  },
  {
    "text": "taken and so on we also try to avoid aggregation by you creating a new event",
    "start": "1062050",
    "end": "1069700"
  },
  {
    "text": "object on top of all the events that create that we've created to say hey I",
    "start": "1069700",
    "end": "1074920"
  },
  {
    "text": "just created all the events this feature is available in beta on 115 so let's",
    "start": "1074920",
    "end": "1085870"
  },
  {
    "text": "take an example here we have a node we try to we schedule a node on it and we",
    "start": "1085870",
    "end": "1093520"
  },
  {
    "text": "try to pull an image the image is not found so we're on an image go back off",
    "start": "1093520",
    "end": "1099730"
  },
  {
    "text": "the cube will try to create an event write it to client-side cache and then",
    "start": "1099730",
    "end": "1107350"
  },
  {
    "text": "create it on the API server when we will try to back off and try to recall the",
    "start": "1107350",
    "end": "1113590"
  },
  {
    "text": "image we're gonna check if the event is amorphic and if it's the same event",
    "start": "1113590",
    "end": "1121090"
  },
  {
    "text": "we're gonna start what we call an event theory which is basically we increment",
    "start": "1121090",
    "end": "1127600"
  },
  {
    "text": "account on the non the object of the event we write it to the client-side",
    "start": "1127600",
    "end": "1132790"
  },
  {
    "text": "cache and then we we patch the API server to increment account and whenever we try to",
    "start": "1132790",
    "end": "1140780"
  },
  {
    "text": "back off we check if this is an isomorphic event if it is we write it to",
    "start": "1140780",
    "end": "1146330"
  },
  {
    "text": "the client cache but we do not make extra calls to the API server and when",
    "start": "1146330",
    "end": "1152870"
  },
  {
    "text": "we when we try when we when we detect that the series has ended and we do not",
    "start": "1152870",
    "end": "1158270"
  },
  {
    "text": "see any event theory anymore we write what we have on the client cache to the",
    "start": "1158270",
    "end": "1165560"
  },
  {
    "text": "API server to reflect the latest count also we have on the background something",
    "start": "1165560",
    "end": "1173060"
  },
  {
    "text": "that pulls the cache and oh and make a bulk to the API server to refresh TTL",
    "start": "1173060",
    "end": "1180230"
  },
  {
    "text": "keys because the events are entries on it CD that have TTL keys and we do not",
    "start": "1180230",
    "end": "1186080"
  },
  {
    "text": "want to lose them",
    "start": "1186080",
    "end": "1188980"
  },
  {
    "start": "1189000",
    "end": "1189000"
  },
  {
    "text": "another issue also is related to watch cache it's watch restart ghost the issue here",
    "start": "1191410",
    "end": "1201650"
  },
  {
    "text": "is that whenever we restart the watch connection which overloads the API",
    "start": "1201650",
    "end": "1207530"
  },
  {
    "text": "server for example let's say we we want",
    "start": "1207530",
    "end": "1213590"
  },
  {
    "text": "to watch for a number of of pods that has a specific label and for example 10",
    "start": "1213590",
    "end": "1221180"
  },
  {
    "text": "pots are matching of 100 and these Jen pots are not changing but the other pots",
    "start": "1221180",
    "end": "1229430"
  },
  {
    "text": "are a lot changing so this list gonna have what we call a resource version to",
    "start": "1229430",
    "end": "1235610"
  },
  {
    "text": "indicate the version of the risk pot less resource and since the 10 pots are",
    "start": "1235610",
    "end": "1240890"
  },
  {
    "text": "not changing and the others are changing the pot list resource version is going",
    "start": "1240890",
    "end": "1246410"
  },
  {
    "text": "to increment but we're not going to get to watch connection because the subset",
    "start": "1246410",
    "end": "1251420"
  },
  {
    "text": "of pots that is Matchett is not changed so when the connection will timeout and",
    "start": "1251420",
    "end": "1257510"
  },
  {
    "text": "we will try to reestablish connection we're going to try to establish a connection with a resource version that",
    "start": "1257510",
    "end": "1263600"
  },
  {
    "text": "is still and this is where we get the famous error called - all the resource version and what this",
    "start": "1263600",
    "end": "1271860"
  },
  {
    "text": "means is that the client need to do a full relist to get the latest resource",
    "start": "1271860",
    "end": "1278010"
  },
  {
    "text": "version to re-establish the watch connection and the dimension that we want to improve here are to watch the",
    "start": "1278010",
    "end": "1285330"
  },
  {
    "text": "number of watches and the number of calls that we make against the API server so how did we solve this we",
    "start": "1285330",
    "end": "1294080"
  },
  {
    "text": "introduced a new watch event type which is called event bookmark it basically",
    "start": "1294080",
    "end": "1301830"
  },
  {
    "text": "tells the clients the resource version they're at so one day time out there",
    "start": "1301830",
    "end": "1307919"
  },
  {
    "text": "establish the connection with the right resource version this is a feature that",
    "start": "1307919",
    "end": "1313769"
  },
  {
    "text": "needs to be explicitly obtained by the clients so we don't break the existing",
    "start": "1313769",
    "end": "1320250"
  },
  {
    "text": "clients so when you establish a watch connection on one of the options that",
    "start": "1320250",
    "end": "1325440"
  },
  {
    "text": "you can specify is a low watch bookmarks the initial back benchmark shows 40",
    "start": "1325440",
    "end": "1332700"
  },
  {
    "text": "times in provement and decrease in wasteful event processing as for the",
    "start": "1332700",
    "end": "1339840"
  },
  {
    "text": "availability of this feature it's going to be available in 115 as an alpha so",
    "start": "1339840",
    "end": "1348210"
  },
  {
    "text": "let's take an example we we're watching for a subset of resource so we make the",
    "start": "1348210",
    "end": "1355950"
  },
  {
    "text": "list call to get the list and the resource version we start watching",
    "start": "1355950",
    "end": "1361289"
  },
  {
    "text": "against the API server with the resource version X and specifying a low watch",
    "start": "1361289",
    "end": "1368340"
  },
  {
    "text": "bookmark to true right before the timeout timeout - two seconds the API",
    "start": "1368340",
    "end": "1375899"
  },
  {
    "text": "server gonna send a watch event a new watch event type called bookmark that",
    "start": "1375899",
    "end": "1381840"
  },
  {
    "text": "reflects the latest resource version so when the timeout is going to happen the",
    "start": "1381840",
    "end": "1387480"
  },
  {
    "text": "cubelet gonna erase or establish the watch with the right resource version",
    "start": "1387480",
    "end": "1392490"
  },
  {
    "text": "against the API server and thus there is no need to realist",
    "start": "1392490",
    "end": "1397500"
  },
  {
    "text": "against the API server all right so so",
    "start": "1397500",
    "end": "1403260"
  },
  {
    "text": "those are pretty much the main performance and scalability improvements",
    "start": "1403260",
    "end": "1408480"
  },
  {
    "text": "that we've made in the last few months and all the course over the course of last three to four releases so what are",
    "start": "1408480",
    "end": "1416880"
  },
  {
    "text": "we planning to do next we are still trying to hear from people and from",
    "start": "1416880",
    "end": "1424290"
  },
  {
    "text": "users like what are some of the pains pain points they're having but some of",
    "start": "1424290",
    "end": "1430830"
  },
  {
    "text": "the things which are in our radar at this point and and there are a few",
    "start": "1430830",
    "end": "1439140"
  },
  {
    "start": "1431000",
    "end": "1431000"
  },
  {
    "text": "things which which we are working on already right now which are planned for in your near term and there are some",
    "start": "1439140",
    "end": "1445320"
  },
  {
    "text": "things which we are thinking and let me",
    "start": "1445320",
    "end": "1450690"
  },
  {
    "text": "go through through the things that we have here so we are planning to have a",
    "start": "1450690",
    "end": "1456960"
  },
  {
    "text": "new endpoints API endpoints v2 API there's a very good talk on this on Tuesday by Wojtek which explains the new",
    "start": "1456960",
    "end": "1466530"
  },
  {
    "text": "API so currently the endpoints API is not very performant for the way it has",
    "start": "1466530",
    "end": "1474360"
  },
  {
    "text": "been designed because for a service you have one single endpoint object which",
    "start": "1474360",
    "end": "1480870"
  },
  {
    "text": "stores a list of all the eyepiece of the pods that belong to that service so that",
    "start": "1480870",
    "end": "1485880"
  },
  {
    "text": "means that even if like one port goes up or comes down you have to update this object and repost this whole thing even",
    "start": "1485880",
    "end": "1493320"
  },
  {
    "text": "if all the other ip's didn't change so this this becomes kind of costly as you",
    "start": "1493320",
    "end": "1498600"
  },
  {
    "text": "go to bigger clusters and as you have bigger services because it's not just about updating these objects it's also",
    "start": "1498600",
    "end": "1504480"
  },
  {
    "text": "about then publishing these the newer version of these endpoints to all the",
    "start": "1504480",
    "end": "1510060"
  },
  {
    "text": "nodes because queue proxies are watching for endpoints yeah so the next one is to",
    "start": "1510060",
    "end": "1517860"
  },
  {
    "text": "allow for a higher pot density so port density is the number of pods that are",
    "start": "1517860",
    "end": "1523350"
  },
  {
    "text": "allowed to be run on one would currently it's limited to 110 by default and kubernetes and officially we",
    "start": "1523350",
    "end": "1529770"
  },
  {
    "text": "support for like a maximum 110 pods per node but because of some of the recent",
    "start": "1529770",
    "end": "1535650"
  },
  {
    "text": "improvements with docker continuity and and even improvements in cubelet itself",
    "start": "1535650",
    "end": "1540930"
  },
  {
    "text": "of we are now able to go higher and there are a bunch of folks who are",
    "start": "1540930",
    "end": "1546000"
  },
  {
    "text": "already trying like 200 to 300 parts per node so we might soon increase this",
    "start": "1546000",
    "end": "1551930"
  },
  {
    "text": "default limit or at least like go go higher for bigger instance types the",
    "start": "1551930",
    "end": "1559740"
  },
  {
    "text": "next one is about rethinking or current affinity and anti affinity logic in",
    "start": "1559740",
    "end": "1567390"
  },
  {
    "text": "scheduler because this is started to hunters and large clusters because this",
    "start": "1567390",
    "end": "1572580"
  },
  {
    "text": "this has become a bottleneck for our scheduler throughput in large clusters because the way we are computing",
    "start": "1572580",
    "end": "1578190"
  },
  {
    "text": "affinity and anti affinity currently we are we are considering the state of the whole cluster for every single for",
    "start": "1578190",
    "end": "1585420"
  },
  {
    "text": "evaluating the validity of every single node for the part and this this becomes",
    "start": "1585420",
    "end": "1590730"
  },
  {
    "text": "a very computationally expensive so yeah so we're still thinking about ideas of",
    "start": "1590730",
    "end": "1596430"
  },
  {
    "text": "what we can do here to to improve this may be there might be some significant",
    "start": "1596430",
    "end": "1602430"
  },
  {
    "text": "changes and in the feature itself something to think through and the last",
    "start": "1602430",
    "end": "1608790"
  },
  {
    "text": "one we have your is watching for arbitrary fields this is again something",
    "start": "1608790",
    "end": "1615590"
  },
  {
    "text": "far term where we still again thinking about this we're not yet hundred percent",
    "start": "1615590",
    "end": "1623250"
  },
  {
    "text": "convinced if we want to do this but this is about watching for particular fields",
    "start": "1623250",
    "end": "1628290"
  },
  {
    "text": "of an object instead of watching for the whole object if you look at some of the controllers and kubernetes they are",
    "start": "1628290",
    "end": "1634890"
  },
  {
    "text": "taking actions based on just like one or two fields of some object so but so",
    "start": "1634890",
    "end": "1642060"
  },
  {
    "text": "pretty much whatever else is and the object is like not being used by it and currently watch gives you the whole",
    "start": "1642060",
    "end": "1648990"
  },
  {
    "text": "object that you are watching for which which kind of increases the resource",
    "start": "1648990",
    "end": "1654330"
  },
  {
    "text": "footprint of your client consequently your controllers because you have bigger objects to process and",
    "start": "1654330",
    "end": "1661350"
  },
  {
    "text": "more memory allocations to make yeah and I put those three dots there because",
    "start": "1661350",
    "end": "1668830"
  },
  {
    "text": "maybe there is something that someone among you have ideas about and you might",
    "start": "1668830",
    "end": "1675370"
  },
  {
    "text": "want to let us know because this is a good opportunity and thank you for",
    "start": "1675370",
    "end": "1687970"
  },
  {
    "text": "questions",
    "start": "1687970",
    "end": "1690510"
  },
  {
    "text": "so with some of these optimizations in terms of data structures",
    "start": "1701970",
    "end": "1707440"
  },
  {
    "text": "have you seen memory resident size improvements with the API server Shea",
    "start": "1707440",
    "end": "1714390"
  },
  {
    "text": "like memory click RAM usage improvements with some of these optimizations like it",
    "start": "1714390",
    "end": "1720880"
  },
  {
    "text": "seems like if if they've had server caches events then this bookmarks feature should show some memory",
    "start": "1720880",
    "end": "1727780"
  },
  {
    "text": "improvements just wondering about that yeah do you want to answer I think so",
    "start": "1727780",
    "end": "1734920"
  },
  {
    "text": "basically we are for those who don't know me I invite a kind another sig lead from SiC scalability so we are basically",
    "start": "1734920",
    "end": "1746560"
  },
  {
    "text": "not focusing that much on memory it's not still that not not one of the things",
    "start": "1746560",
    "end": "1751720"
  },
  {
    "text": "that is it's what most users are",
    "start": "1751720",
    "end": "1756910"
  },
  {
    "text": "affecting out though like we've go 112 that we started using in 114 go 112 is",
    "start": "1756910",
    "end": "1765160"
  },
  {
    "text": "way way better with handling memory like the drop that we've seen in our like continuous testing is something like",
    "start": "1765160",
    "end": "1772710"
  },
  {
    "text": "between 2 & 4 X so once you get go to",
    "start": "1772710",
    "end": "1779020"
  },
  {
    "text": "like get to this version you will see significant improvement",
    "start": "1779020",
    "end": "1784200"
  },
  {
    "text": "so I had a question about the numbers you showed four before the fixes for pod",
    "start": "1791130",
    "end": "1798790"
  },
  {
    "text": "scheduling on five thousand out clusters especially with the anti affinity feature so you said something like five",
    "start": "1798790",
    "end": "1805510"
  },
  {
    "text": "pods per minute that sounds like very very slow I'm wondering if you could expand a bit on what was happening to",
    "start": "1805510",
    "end": "1812440"
  },
  {
    "text": "cause such a slow processing speed for you know 5,000 elements in the list",
    "start": "1812440",
    "end": "1817929"
  },
  {
    "text": "doesn't sound like that much so what sort of thing was happening that was if you can I don't know maybe it's too",
    "start": "1817929",
    "end": "1824590"
  },
  {
    "text": "large to explain that so so that throughput of five boards less than five parts per minute is the",
    "start": "1824590",
    "end": "1830530"
  },
  {
    "text": "throughput when these pods are using anti affinity feature so also this is",
    "start": "1830530",
    "end": "1836800"
  },
  {
    "text": "like before some of the improvements were made but the reason why it was so slow is because the the scheduler",
    "start": "1836800",
    "end": "1844260"
  },
  {
    "text": "predicated and priority which actually evaluate like which took evaluate this",
    "start": "1844260",
    "end": "1850809"
  },
  {
    "text": "code for the node the the way they are written is not very efficient because how it works is like let's say you have",
    "start": "1850809",
    "end": "1857380"
  },
  {
    "text": "you'd if you define a deployment of pods with the anti affinity ad at the node",
    "start": "1857380",
    "end": "1862809"
  },
  {
    "text": "level so so node is your failure domain so basically I see why they kept smiling",
    "start": "1862809",
    "end": "1869530"
  },
  {
    "text": "to you so basically the main problem here is that an TF importante affinity",
    "start": "1869530",
    "end": "1875230"
  },
  {
    "text": "is like symmetric here so when you are scheduling a pot you we are not only",
    "start": "1875230",
    "end": "1881260"
  },
  {
    "text": "looking into how how it affects other",
    "start": "1881260",
    "end": "1886720"
  },
  {
    "text": "pot like we are all we are also looking into like whoever scheduling our pot",
    "start": "1886720",
    "end": "1891910"
  },
  {
    "text": "will not break any other assumptions of any other pot anti affinities that are already scheduled in the system so if",
    "start": "1891910",
    "end": "1898690"
  },
  {
    "text": "you are scheduling a pot that in a cluster that has let's say 10,000 pots",
    "start": "1898690",
    "end": "1904540"
  },
  {
    "text": "already running then we need to check whether my pot and any of those other",
    "start": "1904540",
    "end": "1910120"
  },
  {
    "text": "hundred 10,000 pots aren't breaking the assumption for any of the existing like",
    "start": "1910120",
    "end": "1917140"
  },
  {
    "text": "anti affinity of anything else so that's kind of like more than quadratic or something like that that is like has",
    "start": "1917140",
    "end": "1925019"
  },
  {
    "text": "significantly changed like when I was last looking into the numbers I think in",
    "start": "1925019",
    "end": "1932260"
  },
  {
    "text": "113 we are somewhere around between 10",
    "start": "1932260",
    "end": "1939340"
  },
  {
    "text": "and 20 pots per second already with ante affinity and there's what there is there are a couple more things coming around",
    "start": "1939340",
    "end": "1945880"
  },
  {
    "text": "15 so we are getting to at least the same order of magnitude with ante",
    "start": "1945880",
    "end": "1951010"
  },
  {
    "text": "affinity and there are like couple plans to not support ante affinity on any",
    "start": "1951010",
    "end": "1956980"
  },
  {
    "text": "arbitrary topology also which I was suggesting for quite a long time it",
    "start": "1956980",
    "end": "1963010"
  },
  {
    "text": "seems that it's currently getting some traction so so yeah so for the for these",
    "start": "1963010",
    "end": "1973149"
  },
  {
    "text": "numbers is that just a single scheduler process or is this multiple schedulers",
    "start": "1973149",
    "end": "1980789"
  },
  {
    "text": "so this is the you mean like as is this across multiple part schedules or so",
    "start": "1980789",
    "end": "1989190"
  },
  {
    "text": "each cube scheduler has its own cache of all the world right so this is just read",
    "start": "1989190",
    "end": "1995559"
  },
  {
    "text": "like one scheduler okay so you're not like parallelizing across okay do you",
    "start": "1995559",
    "end": "2001500"
  },
  {
    "text": "lose any other question [Music]",
    "start": "2001500",
    "end": "2009520"
  },
  {
    "text": "okay thank you",
    "start": "2013900",
    "end": "2017440"
  },
  {
    "text": "[Applause]",
    "start": "2019150",
    "end": "2021749"
  }
]