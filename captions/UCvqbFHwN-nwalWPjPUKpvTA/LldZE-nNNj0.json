[
  {
    "text": "my name is Zan uh I am a software engineer at data bricks and today I'm going to talk about automated multic",
    "start": "80",
    "end": "6040"
  },
  {
    "text": "Cloud multif flavor kubernetes cluster upgrades using operators so just a quick introduction",
    "start": "6040",
    "end": "13400"
  },
  {
    "text": "of uh what data brakes is uh we are an endtoend data and AI platform we help",
    "start": "13400",
    "end": "19240"
  },
  {
    "text": "customers understand their data so we run all of these workloads um on our",
    "start": "19240",
    "end": "24880"
  },
  {
    "text": "platform for our customers so these include like machine learning streaming generative AI data data warehouse ETL uh",
    "start": "24880",
    "end": "32680"
  },
  {
    "text": "and many other things all data brakes products run on",
    "start": "32680",
    "end": "38600"
  },
  {
    "text": "kubernetes so we have uh a really large footprint of kubernetes uh it's really a",
    "start": "38600",
    "end": "44920"
  },
  {
    "text": "multi-billion Dollar business that's totally run on",
    "start": "44920",
    "end": "49280"
  },
  {
    "text": "kubernetes very early on we realized that we needed to upgrade all kubernetes",
    "start": "51520",
    "end": "57039"
  },
  {
    "text": "nodes uh frequently and in this case monthly so we had uh required frequent OS",
    "start": "57039",
    "end": "63000"
  },
  {
    "text": "security patches um for kubernetes notes so those are for compliance reasons and",
    "start": "63000",
    "end": "69320"
  },
  {
    "text": "uh we had a kubernetes version upgrades so we need to uh get ined features from",
    "start": "69320",
    "end": "74600"
  },
  {
    "text": "the community we want to get uh bug fixes and uh also security patches on",
    "start": "74600",
    "end": "80439"
  },
  {
    "text": "kubernetes and uh we needed a way to roll out infrastructure changes uh so",
    "start": "80439",
    "end": "85479"
  },
  {
    "text": "the changes include for example the node OS image uh we make a lot of changes to",
    "start": "85479",
    "end": "90920"
  },
  {
    "text": "it over time uh the instance type upgrades um updates um we want to update",
    "start": "90920",
    "end": "96479"
  },
  {
    "text": "no configurations we want to update the launch templates of the virtual machines",
    "start": "96479",
    "end": "101799"
  },
  {
    "text": "that uh back the notes so uh yeah for all of these we needed to run upgrades",
    "start": "101799",
    "end": "109320"
  },
  {
    "text": "very frequently and uh usually upgrades are",
    "start": "109320",
    "end": "114560"
  },
  {
    "text": "considered to be just some operational work so like some engineer will just go and do it but we soon realized that for",
    "start": "114560",
    "end": "121880"
  },
  {
    "text": "us uh it was very challenging we run on three clouds uh we",
    "start": "121880",
    "end": "128440"
  },
  {
    "text": "have AWS edger and Google Cloud we have multiple regions per Cloud around the",
    "start": "128440",
    "end": "134160"
  },
  {
    "text": "world like double digit of regions per cloud and uh we have four different",
    "start": "134160",
    "end": "139920"
  },
  {
    "text": "kubernetes flavors AKA drrs we have uh eks on AWS AKs on aure uh gke on Google",
    "start": "139920",
    "end": "147920"
  },
  {
    "text": "cloud and we also have our self-managed kubernetes clusters that actually run on",
    "start": "147920",
    "end": "153480"
  },
  {
    "text": "all three clouds and uh at data breakes kubernetes",
    "start": "153480",
    "end": "159760"
  },
  {
    "text": "runs workflows for both internal uh users and external customers so it's really just everything so we have uh",
    "start": "159760",
    "end": "167200"
  },
  {
    "text": "microservices we have like serverless customer workflows like all the serverless functions uh serverless SQL",
    "start": "167200",
    "end": "173640"
  },
  {
    "text": "UDF um all these data Brak uh products they uh automat automated run on",
    "start": "173640",
    "end": "179200"
  },
  {
    "text": "kubernetes we have uh machine learning workloads and that's both uh internal and external",
    "start": "179200",
    "end": "185599"
  },
  {
    "text": "um it uh it includes uh training and inference we also run internal tools and",
    "start": "185599",
    "end": "191120"
  },
  {
    "text": "systems uh all the internal tools run on kubernetes for example cicd testing jobs",
    "start": "191120",
    "end": "196360"
  },
  {
    "text": "pki observability Etc so the business has uh really grown",
    "start": "196360",
    "end": "203080"
  },
  {
    "text": "um in a really massive way in the last few years and uh that has resulted in a",
    "start": "203080",
    "end": "210000"
  },
  {
    "text": "very large growth in workloads both in scale and in um the types of",
    "start": "210000",
    "end": "215519"
  },
  {
    "text": "workloads however with all of that we uh and we have to do all the frequent updates so we still have to do all the",
    "start": "215519",
    "end": "222640"
  },
  {
    "text": "upgrades uh with no downtime to uh any of the uh the",
    "start": "222640",
    "end": "229159"
  },
  {
    "text": "workloads and for our scale we have uh we now have over a thousand kubernetes",
    "start": "230599",
    "end": "235760"
  },
  {
    "text": "clusters we have hundreds of thousands of notes and I mean that's just a uh",
    "start": "235760",
    "end": "242200"
  },
  {
    "text": "like bottom line like can be much higher and we need to draw we need to run monster node upgrades for all nodes uh",
    "start": "242200",
    "end": "249120"
  },
  {
    "text": "in all clusters and uh the upgrades can be more frequent uh in some cases for example",
    "start": "249120",
    "end": "254439"
  },
  {
    "text": "when we have hot fixes rollbacks Etc uh so uh assuming an engineer spends like",
    "start": "254439",
    "end": "260840"
  },
  {
    "text": "two minutes to manually upgrade a node in kubernetes and that's like a very",
    "start": "260840",
    "end": "266320"
  },
  {
    "text": "optimistic estimate uh so somebody just runs a bunch of commands uh in our scale it would take 139 person",
    "start": "266320",
    "end": "273120"
  },
  {
    "text": "days to upgrade all notes at once uh like and that's uh certainly not scalable so um to be fair we didn't",
    "start": "273120",
    "end": "281000"
  },
  {
    "text": "start with this scale a few years ago but even for the smaller scale back then",
    "start": "281000",
    "end": "287000"
  },
  {
    "text": "uh it would not have been feasible to upgrade all of these manually so uh yeah everything uh",
    "start": "287000",
    "end": "295000"
  },
  {
    "text": "basically led to automation we wanted to build an automated system to upgrade nodes so we",
    "start": "295000",
    "end": "302160"
  },
  {
    "text": "started doing that so here are the requirements we summarized um to uh build such uh such a",
    "start": "302160",
    "end": "310720"
  },
  {
    "text": "system first of all we uh there there can be absolutely no downtime or outage",
    "start": "310720",
    "end": "317320"
  },
  {
    "text": "caused by uh the upgrade process like on any of the workloads and we wanted to be",
    "start": "317320",
    "end": "323000"
  },
  {
    "text": "able to roll back easily in case of a problem so uh this is because when you do infrastructure updates or node upat",
    "start": "323000",
    "end": "329680"
  },
  {
    "text": "upgrades uh like kuet version OS image upgrades these can Al often cause like",
    "start": "329680",
    "end": "337039"
  },
  {
    "text": "uh problems for example the new configuration may contain bugs the the new version may not work properly for",
    "start": "337039",
    "end": "343440"
  },
  {
    "text": "some workloads and uh you always in that case want to roll back",
    "start": "343440",
    "end": "349759"
  },
  {
    "text": "immediately we uh wanted a consistent upgrade flow for all three clouds and four KU flavors so we didn't want our",
    "start": "349759",
    "end": "357639"
  },
  {
    "text": "Engineers to have to run different steps steps to upgrade kubernetes clusters just because they are AKs or eks or gke",
    "start": "357639",
    "end": "365919"
  },
  {
    "text": "or our self-managed kubernetes or like whatever Cloud they're on and we want",
    "start": "365919",
    "end": "371639"
  },
  {
    "text": "all of this to be uh very low effort for our Engineers so here's the first version we",
    "start": "371639",
    "end": "379000"
  },
  {
    "text": "built uh it was an automation using off shelf cicd so we had Spiner at data",
    "start": "379000",
    "end": "384639"
  },
  {
    "text": "briak we had Jenkins uh like like a Jenkins uh setup so we decided to to",
    "start": "384639",
    "end": "390080"
  },
  {
    "text": "automate this using um a Spiner Pipeline and in a pipeline every uh every stage",
    "start": "390080",
    "end": "396479"
  },
  {
    "text": "corresponds to um a Jenkins job and the jenin the Jenkins job in this case runs",
    "start": "396479",
    "end": "402039"
  },
  {
    "text": "um a combination of python and groovy scripts so the scripts uh talk to the",
    "start": "402039",
    "end": "407479"
  },
  {
    "text": "Target kubernetes clusters API server and also the cloud apis and uh what they",
    "start": "407479",
    "end": "412680"
  },
  {
    "text": "do is that they bring up new nodes and then they move the workf clows over uh",
    "start": "412680",
    "end": "418080"
  },
  {
    "text": "basically draining the uh the noes and also evicting the pots until all the",
    "start": "418080",
    "end": "424000"
  },
  {
    "text": "workloads are running on the new noes and then uh it shuts down the old notes",
    "start": "424000",
    "end": "430160"
  },
  {
    "text": "so that basically completes the um upgrade of a no pool so yeah we uh launched this first",
    "start": "430160",
    "end": "438080"
  },
  {
    "text": "first version of the system but it didn't work well uh for many uh for many",
    "start": "438080",
    "end": "443800"
  },
  {
    "text": "reasons so it caused many outages due to uh first of all it was very hard to test",
    "start": "443800",
    "end": "450280"
  },
  {
    "text": "because we uh like all the upgrade logic was very complex we had to have some",
    "start": "450280",
    "end": "455720"
  },
  {
    "text": "like a lot of custom Logic for different clouds and also uh specific workloads uh",
    "start": "455720",
    "end": "461599"
  },
  {
    "text": "so we ended up having uh over 10K uh lines of poorly tested Python and groovy",
    "start": "461599",
    "end": "467639"
  },
  {
    "text": "code and it's inherently hard to uh do unit testing and integration integration",
    "start": "467639",
    "end": "473599"
  },
  {
    "text": "testing on these kind of uh cicd or workflow engines uh it's just quite hard",
    "start": "473599",
    "end": "480120"
  },
  {
    "text": "and then uh sometimes we would deploy the change to like a DA instance of",
    "start": "480120",
    "end": "485840"
  },
  {
    "text": "Spiner or Jenkins and then we would run try to run the uh upgrades there but",
    "start": "485840",
    "end": "491159"
  },
  {
    "text": "then uh the test coverage was very poor in general and then it was hard to make a change that um behaves consistently on",
    "start": "491159",
    "end": "498720"
  },
  {
    "text": "all clouds and all kubernetes flavors because when uh for example if there's a change in the upgrade flow uh you would",
    "start": "498720",
    "end": "506039"
  },
  {
    "text": "have to make it uh four times on like the three kubernetes flavors provided by",
    "start": "506039",
    "end": "511680"
  },
  {
    "text": "Cloud providers and the self-managed flavor and sometimes you have to do it three times for the self-managed flavor",
    "start": "511680",
    "end": "517518"
  },
  {
    "text": "on three clouds so it was very easy to make mistakes uh as a result we had very high",
    "start": "517519",
    "end": "523360"
  },
  {
    "text": "human operational cost uh we had from 100 to 150 on call Pages a week uh just",
    "start": "523360",
    "end": "529080"
  },
  {
    "text": "due to the upgrade process because the upgrade pipelines were failing uh the commands were uh not executing um and",
    "start": "529080",
    "end": "538200"
  },
  {
    "text": "also like outages were caused by the upgrade process and uh in general the the system",
    "start": "538200",
    "end": "544000"
  },
  {
    "text": "suffered from transient and temporary failures because of the complexity of the upgrade steps and in uh these kind",
    "start": "544000",
    "end": "550839"
  },
  {
    "text": "of workflow engines or cicd systems it's very easy to uh for like a temporary",
    "start": "550839",
    "end": "555959"
  },
  {
    "text": "failure to cause the whole stage to fail and then someone some human will have to",
    "start": "555959",
    "end": "561240"
  },
  {
    "text": "go there and manually click restart and for our scale we had to do thousands of mouse clicks just clicking the failed",
    "start": "561240",
    "end": "568399"
  },
  {
    "text": "Jenkins jobs so uh obviously nobody wanted that and uh even worse um the",
    "start": "568399",
    "end": "574079"
  },
  {
    "text": "racks were often blocked due to the uh blocked due to the same issues because robox they were uh they would encounter",
    "start": "574079",
    "end": "580720"
  },
  {
    "text": "the same issues uh causing like additional outages or uh the robox could be blocked so often the time uh it took",
    "start": "580720",
    "end": "588920"
  },
  {
    "text": "to mitigate incidents was very long and we often miss upgrade deadlines because",
    "start": "588920",
    "end": "594720"
  },
  {
    "text": "of all these issues so after uh operating after",
    "start": "594720",
    "end": "601240"
  },
  {
    "text": "having operating that system for actually more than a year we finally had enough so we decided to do something",
    "start": "601240",
    "end": "607600"
  },
  {
    "text": "better so we decided to uh build a solution using kubernetes",
    "start": "607600",
    "end": "614200"
  },
  {
    "text": "operators so here's uh our abstraction so a node pool uh a node upgrade is uh",
    "start": "614360",
    "end": "621200"
  },
  {
    "text": "really just uh we call it the not rotation process because in kubernetes",
    "start": "621200",
    "end": "627160"
  },
  {
    "text": "uh we organize notes into multiple not pools and for each not pool we have the",
    "start": "627160",
    "end": "632440"
  },
  {
    "text": "concepts of the old and the new nodes so the old nodes are the noes that are running a previous configuration the new",
    "start": "632440",
    "end": "639120"
  },
  {
    "text": "nodes are the nodes that are running uh like a new configuration that we want to uh upgrade to so uh what we do is that",
    "start": "639120",
    "end": "647360"
  },
  {
    "text": "we gradually replace all the nodes running all the old nodes in a not pool",
    "start": "647360",
    "end": "652880"
  },
  {
    "text": "with new notes um so and we do this for every single not poool uh after that we",
    "start": "652880",
    "end": "658079"
  },
  {
    "text": "can uh call the uh cluster we can say that the cluster has been fully upgraded so for self-managed kubernetes there's a",
    "start": "658079",
    "end": "664920"
  },
  {
    "text": "Nuance here because there are also kubernetes Master nodes however uh from an upgrade standpoint uh really the",
    "start": "664920",
    "end": "671399"
  },
  {
    "text": "Masters are not that different from the workers because the Masters uh is are just a set of nodes and we can treat",
    "start": "671399",
    "end": "678480"
  },
  {
    "text": "them as a nopool um of course there are some nuances here for example uh if you",
    "start": "678480",
    "end": "683519"
  },
  {
    "text": "want to do like a KU version upgrade you should upgrade the Masters first and you need to obey the version skill policies",
    "start": "683519",
    "end": "690160"
  },
  {
    "text": "and also when you upgrade the Masters you want to replace one master at a time instead of replacing all of them at a",
    "start": "690160",
    "end": "695200"
  },
  {
    "text": "time to avoid uh downtime in uh ha",
    "start": "695200",
    "end": "700639"
  },
  {
    "text": "Masters and we uh here are some key observations that uh let us to uh use",
    "start": "701720",
    "end": "706880"
  },
  {
    "text": "kuber kubernetes operators so uh a node upgrade is declarative because the go",
    "start": "706880",
    "end": "713399"
  },
  {
    "text": "state is very clear in this case basically you just say I want all nodes in a not pool to be running um the new",
    "start": "713399",
    "end": "721360"
  },
  {
    "text": "configuration that I'm upgrading to and then a rowback conceptually is actually",
    "start": "721360",
    "end": "726720"
  },
  {
    "text": "not different from uh from an upgrade because a rowback is just an an upgrade",
    "start": "726720",
    "end": "732480"
  },
  {
    "text": "in the reverse Direction so as long as the go state is clearly defined and the system is implemented well there should",
    "start": "732480",
    "end": "739279"
  },
  {
    "text": "not be additional steps or logic needed to do the rback uh either in the middle",
    "start": "739279",
    "end": "744560"
  },
  {
    "text": "of or after an upgrade because you just reverse the direction of the no p",
    "start": "744560",
    "end": "751160"
  },
  {
    "text": "notation and uh here's part of our thought process back then um so nobody",
    "start": "752279",
    "end": "758079"
  },
  {
    "text": "wants to sit there and watch for upgrades like seriously nobody wants to do that it's a waste of time for",
    "start": "758079",
    "end": "763279"
  },
  {
    "text": "engineers so uh upgrades should be a continuous background process rather than a job run by an engineer because",
    "start": "763279",
    "end": "770240"
  },
  {
    "text": "like it's often uh you know you a lot of places it's like thought to be like a",
    "start": "770240",
    "end": "775680"
  },
  {
    "text": "piece of operational work and somebody should run some job to do it but but really it should be an online process oh",
    "start": "775680",
    "end": "782480"
  },
  {
    "text": "sorry yeah so um and it needs to tolerate temporary failures and move gradually towards the goal",
    "start": "782480",
    "end": "789160"
  },
  {
    "text": "state in this case kubernetes operators are a good choice to implement such a process and uh also uh it's very easy to",
    "start": "789160",
    "end": "797560"
  },
  {
    "text": "write tests like unit tests and integration tests for kubernetes operators compared to cicd and Python",
    "start": "797560",
    "end": "803959"
  },
  {
    "text": "and groovy scripts and uh and that's partially because uh there's good ecosystem support we uh all like The",
    "start": "803959",
    "end": "810959"
  },
  {
    "text": "Operators are retaining go and we have CU Builder we have like uh gock controller runtime all these kind of uh",
    "start": "810959",
    "end": "818519"
  },
  {
    "text": "tools and uh another uh Point here is that we wanted each kubernetes cluster",
    "start": "818519",
    "end": "823959"
  },
  {
    "text": "to be able to upgrade itself so we decided to run operators as Parts with",
    "start": "823959",
    "end": "829959"
  },
  {
    "text": "within each kubernetes cluster as opposed to running them uh Elsewhere on some other piece of infrastructure uh",
    "start": "829959",
    "end": "836759"
  },
  {
    "text": "yeah so the reason is we really want kubernetes is to be the base layer of infrastructure if we had run uh the",
    "start": "836759",
    "end": "842759"
  },
  {
    "text": "operator PA for upgrades elsewhere then we would have uh we would have to have",
    "start": "842759",
    "end": "848040"
  },
  {
    "text": "some other infrastructure set up to run those operators and that would uh would have meant like just more um management",
    "start": "848040",
    "end": "856079"
  },
  {
    "text": "overhead and uh more chances of failures and here's the uh high level",
    "start": "856079",
    "end": "863839"
  },
  {
    "text": "architecture that we came up with we have three operators working together we have uh something called the no rotation",
    "start": "863839",
    "end": "870880"
  },
  {
    "text": "operator that's like the central operator that's coordinating the upgrades and we have uh two other",
    "start": "870880",
    "end": "876320"
  },
  {
    "text": "operators called not drain and Noto capacity operators the Noto rotation",
    "start": "876320",
    "end": "881440"
  },
  {
    "text": "operator interacts with uh the the other two operators via custom resources so it",
    "start": "881440",
    "end": "886480"
  },
  {
    "text": "would uh patch custom patch or create custom resources AKA CRS and then check",
    "start": "886480",
    "end": "891720"
  },
  {
    "text": "their statuses so that's how they interact and all the operators are built uh using uh the controller runtime",
    "start": "891720",
    "end": "897560"
  },
  {
    "text": "framework so it's pretty easy easy to Define um the CRS they watch the reconciliation Loops uh",
    "start": "897560",
    "end": "904880"
  },
  {
    "text": "Etc so let's uh dive deep into each of the operators uh the first one is called the node drain operator so to move",
    "start": "904880",
    "end": "912360"
  },
  {
    "text": "workloads from the old nodes to the new nodes you uh really want to move them safely and the way you do it is via",
    "start": "912360",
    "end": "919000"
  },
  {
    "text": "eviction apis in kubernetes so um in the no drain operator it watches uh no drain",
    "start": "919000",
    "end": "926639"
  },
  {
    "text": "CRS in the CRS uh the spec specifies a declarative state so",
    "start": "926639",
    "end": "932160"
  },
  {
    "text": "basically it says we need to drain the node with uh uid X so why are we using",
    "start": "932160",
    "end": "937800"
  },
  {
    "text": "uids it's because in kubernetes the uh no names can often be reused so we want",
    "start": "937800",
    "end": "943079"
  },
  {
    "text": "to uh distinguish between different nodes that came up at different times but like they may have the same name and",
    "start": "943079",
    "end": "949759"
  },
  {
    "text": "we have one CR per node and a garbage collection go routine cleans up ocrs so the reconciliation Loop just uh tries to",
    "start": "949759",
    "end": "957680"
  },
  {
    "text": "evict all the parts using the eviction API um in um kubernetes trying to evict",
    "start": "957680",
    "end": "963959"
  },
  {
    "text": "all the PS on the Node and also coordin the node to prevent uh new nodes from to prevent new parts from landing on the",
    "start": "963959",
    "end": "970079"
  },
  {
    "text": "Node and if anything fails it will just recue and retry so uh a Nuance here is like when",
    "start": "970079",
    "end": "977079"
  },
  {
    "text": "any of the three operators pods are evicted uh what happens because like",
    "start": "977079",
    "end": "982279"
  },
  {
    "text": "they're doing the work but then they have just uh evicted themselves so here's the good thing about declarative",
    "start": "982279",
    "end": "989079"
  },
  {
    "text": "States uh because all the declarative all the goal states are clearly like for all these three operators are clearly",
    "start": "989079",
    "end": "995720"
  },
  {
    "text": "defined in the custom resources so when they uh when they are evicted they will",
    "start": "995720",
    "end": "1001000"
  },
  {
    "text": "eventually come back again on some other node and they should be able to just do the work without having to know like",
    "start": "1001000",
    "end": "1008639"
  },
  {
    "text": "that they were previously killed so there's no state in these operators and uh the go the only thing is the G State",
    "start": "1008639",
    "end": "1015199"
  },
  {
    "text": "that's based on the custom resource and here's the contract between the no",
    "start": "1015199",
    "end": "1020920"
  },
  {
    "text": "train operator and the workloads and this is very important to prevent uh outages and downtime to workloads so the",
    "start": "1020920",
    "end": "1028038"
  },
  {
    "text": "first one is pdbs uh we have pdbs for most of most of the services but uh",
    "start": "1028039",
    "end": "1034079"
  },
  {
    "text": "there are some services that just don't have pdbs for whatever reason maybe they forgot or uh there was a mistake or like",
    "start": "1034079",
    "end": "1041918"
  },
  {
    "text": "they just didn't have time to add it or like for some practical reason reasons they just like can't have it so in that",
    "start": "1041919",
    "end": "1048558"
  },
  {
    "text": "case uh the no Trin operator actually internally has something called a def a default internal pdb with a Max on",
    "start": "1048559",
    "end": "1056160"
  },
  {
    "text": "unavailable equals one and that is enforced to uh like onto all workloads without pdbs so that uh reduces like",
    "start": "1056160",
    "end": "1063880"
  },
  {
    "text": "minimizes the chances of uh misconfigurations causing unplanned",
    "start": "1063880",
    "end": "1069280"
  },
  {
    "text": "outages we also have a serice service specific configurations uh we have uh",
    "start": "1069280",
    "end": "1074360"
  },
  {
    "text": "something called maintenance windows so it's like my services Parts can only be Ed on Fridays from 9:00 to 10 p.m.",
    "start": "1074360",
    "end": "1081679"
  },
  {
    "text": "Pacific um so this uh is often used by services that can't tolerate any replica",
    "start": "1081679",
    "end": "1087440"
  },
  {
    "text": "being down or our single replica so like there are applications multi- repca",
    "start": "1087440",
    "end": "1092919"
  },
  {
    "text": "applications that we have that just can't tolerate any replica being down for some application specific reasons",
    "start": "1092919",
    "end": "1100280"
  },
  {
    "text": "and they would use this to have planned downtime and uh we also have Grace periods so it's like uh give my pod",
    "start": "1100280",
    "end": "1107640"
  },
  {
    "text": "eight hours to finish if it's not done by then evicted so the uh the purpose of",
    "start": "1107640",
    "end": "1113039"
  },
  {
    "text": "this is to uh uh ensure that for example jobs running in the kubernetes Clusters",
    "start": "1113039",
    "end": "1119159"
  },
  {
    "text": "are not prematurely uh terminated because of the upgrade process we also provide a custom Handler framework uh",
    "start": "1119159",
    "end": "1125960"
  },
  {
    "text": "that's like written goand it's part of the no drain operator code so service teams can build their own logic to",
    "start": "1125960",
    "end": "1132600"
  },
  {
    "text": "migrate their workloads for example I would like my uh inference pause to not be evicted using the eviction Pi but",
    "start": "1132600",
    "end": "1139640"
  },
  {
    "text": "redeployed by making a request to the uh orchestrator service for machine learning based on application Level",
    "start": "1139640",
    "end": "1148559"
  },
  {
    "text": "signals and uh the second operator we have is called the notp put capacity operator so it talks to the cloud",
    "start": "1149159",
    "end": "1154919"
  },
  {
    "text": "provider and uh it ask delet VMS for noes in a kuber no pool so it has two",
    "start": "1154919",
    "end": "1160880"
  },
  {
    "text": "CRS one is uh basically increasing the size of a noo and the interesting point",
    "start": "1160880",
    "end": "1166960"
  },
  {
    "text": "here is that the operator can cannot decrease the size of a no pool directly",
    "start": "1166960",
    "end": "1172440"
  },
  {
    "text": "uh and I'll talk about that uh soon and then the operator will call the cloud API to add VMS to a nopol if the desired",
    "start": "1172440",
    "end": "1179799"
  },
  {
    "text": "size of a nopol is higher than what it currently has and uh it has another thing called the no termination CR so",
    "start": "1179799",
    "end": "1186480"
  },
  {
    "text": "tell me which nose BM you want to delete based on the uid well it will go to the cloud uh API to find the corres",
    "start": "1186480",
    "end": "1193559"
  },
  {
    "text": "corresponding VM and terminate the specific VM so uh we only have this and",
    "start": "1193559",
    "end": "1198720"
  },
  {
    "text": "we don't have a way to scale down uh the no pools based on the desired count and",
    "start": "1198720",
    "end": "1204200"
  },
  {
    "text": "the reason is that is like we want we really only want to terminate the nodes that have been successfully drained and",
    "start": "1204200",
    "end": "1212080"
  },
  {
    "text": "cordoned by the no drain operator if we just decrease the desired count of a no",
    "start": "1212080",
    "end": "1217799"
  },
  {
    "text": "pool often it would just randomly shut down nodes and those nodes may have actually active workflows running on",
    "start": "1217799",
    "end": "1223799"
  },
  {
    "text": "them so that's why we don't allow just like a random scale down but we only allow um specific VMS to be",
    "start": "1223799",
    "end": "1232039"
  },
  {
    "text": "terminated and this operator notle capacity that's where we hide all the differences between clouds and flavors",
    "start": "1232039",
    "end": "1238840"
  },
  {
    "text": "so it has different apis to uh scale up an noo and terminate VMS based on um the",
    "start": "1238840",
    "end": "1245600"
  },
  {
    "text": "flavor and the cloud for example AKs eks GK and self-manage kubernetes and um it",
    "start": "1245600",
    "end": "1252360"
  },
  {
    "text": "abstracts all of these differences from the other operators so uh the other two operators are totally oper operating on",
    "start": "1252360",
    "end": "1259200"
  },
  {
    "text": "the kubernetes level and are totally Cloud",
    "start": "1259200",
    "end": "1263760"
  },
  {
    "text": "agnostic and now let's go to the last operator which is called um no rotation",
    "start": "1264360",
    "end": "1269400"
  },
  {
    "text": "operator it uh at its core it has this algorithm called the blue green rolling",
    "start": "1269400",
    "end": "1274520"
  },
  {
    "text": "upgrade so it maintains two Cloud Stacks well the so we always have like multiple",
    "start": "1274520",
    "end": "1280039"
  },
  {
    "text": "Stacks per nopol and then uh yeah like they uh we have two stacks mapping to",
    "start": "1280039",
    "end": "1285760"
  },
  {
    "text": "the same logical nopol so rather than trying to do a search upgrade within one stack let's say we have like on AWS we",
    "start": "1285760",
    "end": "1293159"
  },
  {
    "text": "have one ASG and then we do like a search upgrade where we temporarily increase the size of the ASG uh like and",
    "start": "1293159",
    "end": "1300400"
  },
  {
    "text": "then we gradually move the workflows over rather than doing that we have two stacks AKA two asgs and then we scale",
    "start": "1300400",
    "end": "1307159"
  },
  {
    "text": "one up and scale the other one down gradually while we move over the pods uh",
    "start": "1307159",
    "end": "1312279"
  },
  {
    "text": "move the PA over gradually uh and there are some benefits to this I'll explain later and uh it",
    "start": "1312279",
    "end": "1318720"
  },
  {
    "text": "gradually replaces old notes in small batches so it never replaces all of them at once uh instead it's like in very",
    "start": "1318720",
    "end": "1325320"
  },
  {
    "text": "small batches and the batch size is tunable and this allows us to control the speed of the node upgrades it also",
    "start": "1325320",
    "end": "1331960"
  },
  {
    "text": "reduces our chances of getting into let's say quota issues during upgrades and it limits the blast radius of node",
    "start": "1331960",
    "end": "1339720"
  },
  {
    "text": "misconfigurations so let's say our new configuration has a bug um because it only does so in small batches like only",
    "start": "1339720",
    "end": "1347400"
  },
  {
    "text": "well like uh only a small number of noes will be affected uh with the bug in the new configuration and then we'll know",
    "start": "1347400",
    "end": "1353919"
  },
  {
    "text": "and we'll roll back here's the uh reconciliation Loop",
    "start": "1353919",
    "end": "1360520"
  },
  {
    "text": "of the not rotation operator so it uh does this coordination between itself",
    "start": "1360520",
    "end": "1365720"
  },
  {
    "text": "and the other two operators it looks at the uh CR basically it's called the no rotation CR uh the CR says we need all",
    "start": "1365720",
    "end": "1372880"
  },
  {
    "text": "notes in a no pool to be running this configuration and then it keeps uh watching and listing all the notes in",
    "start": "1372880",
    "end": "1379080"
  },
  {
    "text": "the no pool it replaces all uh like old notes in the no pool with new notes in",
    "start": "1379080",
    "end": "1384120"
  },
  {
    "text": "small batches and it does so via uh by by patching and creating CRS to the uh",
    "start": "1384120",
    "end": "1390760"
  },
  {
    "text": "two other operators that I mentioned so it will first patch the noal capacity CR to add some new notes and then you will",
    "start": "1390760",
    "end": "1397320"
  },
  {
    "text": "create no drain CRS to drain um that batch of old noes and coordin them and",
    "start": "1397320",
    "end": "1402360"
  },
  {
    "text": "then you will create no termination CRS to kill the drained ootes and after each batch you will do some health checks",
    "start": "1402360",
    "end": "1408200"
  },
  {
    "text": "based on application Level signals cluster level signals uh and Cloud level signals if it can't make any progress",
    "start": "1408200",
    "end": "1415960"
  },
  {
    "text": "for some hours or encountered unrecoverable errors or detects the",
    "start": "1415960",
    "end": "1421520"
  },
  {
    "text": "cluster is unhealthy then it will alert our un call Via pager duty but it will just like keep running in this Loop um",
    "start": "1421520",
    "end": "1428000"
  },
  {
    "text": "until uh all nodes are running the new",
    "start": "1428000",
    "end": "1432640"
  },
  {
    "text": "configuration so you might wonder with uh with this uh system like how is it different from Cloud manage kubernetes",
    "start": "1433120",
    "end": "1439679"
  },
  {
    "text": "node like Auto upgrade features I know some Cloud providers they provide that",
    "start": "1439679",
    "end": "1444760"
  },
  {
    "text": "for their kubernetes products so uh the main reason is that uh this system works",
    "start": "1444760",
    "end": "1451039"
  },
  {
    "text": "consistently with the same behavior across three clouds and four kubernetes flavors so it works for both uh",
    "start": "1451039",
    "end": "1457120"
  },
  {
    "text": "self-managed and Cloud managed kubernetes and it has exactly the same upgrade behavior in uh for example in",
    "start": "1457120",
    "end": "1463480"
  },
  {
    "text": "like Cloud manage kubernetes in the node Auto upgrade features let's say in AKs",
    "start": "1463480",
    "end": "1468840"
  },
  {
    "text": "and gke they actually work differently they have different rules to uh drain uh",
    "start": "1468840",
    "end": "1473919"
  },
  {
    "text": "nodes to the big pods and uh that can cause issues for us because there can be unexpected behaviors on one Cloud",
    "start": "1473919",
    "end": "1480640"
  },
  {
    "text": "compared to the other cloud and uh another thing is what I mentioned we have we always maintain two stacks the",
    "start": "1480640",
    "end": "1487559"
  },
  {
    "text": "old no Cloud stacks for example the asgs V mssc mix uh on the three clouds are",
    "start": "1487559",
    "end": "1493080"
  },
  {
    "text": "kept for a while so this allows uh us to reliably do rollbacks the reason is that",
    "start": "1493080",
    "end": "1499600"
  },
  {
    "text": "in Cloud managed node Auto operates a Rob back a rback May Fail if a node group is uh is in a broken State causing",
    "start": "1499600",
    "end": "1506520"
  },
  {
    "text": "prolonged disruptions so for example uh if you have a misconfiguration in your new uh in your new nodes like in the OS",
    "start": "1506520",
    "end": "1514240"
  },
  {
    "text": "or in the user data script in the launch template the nodes may fail to join the cluster and then uh in some cloud",
    "start": "1514240",
    "end": "1521080"
  },
  {
    "text": "provider kubernetes that will actually result in the uh manage no pool to enter",
    "start": "1521080",
    "end": "1526279"
  },
  {
    "text": "a broken State and in that case you can't even roll back unless you manually clean it up so uh in our case we can",
    "start": "1526279",
    "end": "1533159"
  },
  {
    "text": "immediately roll back by scaling up the previous stack so it's uh it's very",
    "start": "1533159",
    "end": "1538720"
  },
  {
    "text": "quick and it does not require pushing the old configuration again to the cloud",
    "start": "1538720",
    "end": "1544760"
  },
  {
    "text": "and we support customizing upgrade behavior for each workload based on application Level signals and that's",
    "start": "1544760",
    "end": "1550880"
  },
  {
    "text": "just not possible for cloud managed uh Auto upgrades because they don't have visibility into the nuances our specific",
    "start": "1550880",
    "end": "1558679"
  },
  {
    "text": "workflows have and uh the system can continuously replace non-conforming nodes uh so we have like uh for example",
    "start": "1558679",
    "end": "1566840"
  },
  {
    "text": "sometimes people scale up noes running the old configuration by mistake or",
    "start": "1566840",
    "end": "1572039"
  },
  {
    "text": "we've seen like sometimes the autoscaler had some bugs that it in it like incorrectly scaled up um noes with the",
    "start": "1572039",
    "end": "1579799"
  },
  {
    "text": "old configuration the system is always online and you will be able to detect all these uh like non-compliant nodes in",
    "start": "1579799",
    "end": "1587200"
  },
  {
    "text": "real time and immediately run the uh rolling upgrade to uh replace them with",
    "start": "1587200",
    "end": "1592240"
  },
  {
    "text": "new nodes and here's the new SLP uh standard",
    "start": "1592240",
    "end": "1597520"
  },
  {
    "text": "operating procedure uh we have uh with the new system so the engineer will just",
    "start": "1597520",
    "end": "1603120"
  },
  {
    "text": "do a one click it it basically triggers some update orchestrator we've had multile versions of this it's a very",
    "start": "1603120",
    "end": "1609080"
  },
  {
    "text": "simple system so like it's like some some simple Spiner pipeline Argo workflow or some Custom Service it will",
    "start": "1609080",
    "end": "1615399"
  },
  {
    "text": "just patch the all the no rotation CRS in all clusters basically saying I want all of you to be running our new",
    "start": "1615399",
    "end": "1622720"
  },
  {
    "text": "configuration for this month and then um the engineer really just forgets about",
    "start": "1622720",
    "end": "1628240"
  },
  {
    "text": "the upgrade like they don't even have to care about it they'll just continue doing other pieces uh pieces of work and",
    "start": "1628240",
    "end": "1635600"
  },
  {
    "text": "uh in the background all the operators in all clusters uh just work together to",
    "start": "1635600",
    "end": "1640840"
  },
  {
    "text": "gradually upgrade all the noes to the Target configuration and we only have",
    "start": "1640840",
    "end": "1646720"
  },
  {
    "text": "human Intervention when something goes wrong and we get alerted uh so uh the system can",
    "start": "1646720",
    "end": "1653080"
  },
  {
    "text": "automatically makes Pro make progress if uh a blocker is resolved let's say there's some uh issue on the cloud",
    "start": "1653080",
    "end": "1659360"
  },
  {
    "text": "provider side that uh caused the upgrade to fail um so we don't have to do",
    "start": "1659360",
    "end": "1665720"
  },
  {
    "text": "anything really we file some support ticket to the cloud and they will resolve the issue and then uh the system",
    "start": "1665720",
    "end": "1672679"
  },
  {
    "text": "will automatically proceed because it's it's based on operators they're always reconciling regardless of failures so",
    "start": "1672679",
    "end": "1679320"
  },
  {
    "text": "there's no need for someone to go there and uh send some a command or some click",
    "start": "1679320",
    "end": "1684840"
  },
  {
    "text": "to tell the system to uh to resume and for rollback we just trigger and upgrade back to the old",
    "start": "1684840",
    "end": "1691080"
  },
  {
    "text": "configuration there's really no special uh SLP for doing rollbacks um it's just",
    "start": "1691080",
    "end": "1696760"
  },
  {
    "text": "an upgrade like I said back to the old",
    "start": "1696760",
    "end": "1701000"
  },
  {
    "text": "configuration and here are the results uh we fully automated the upgrade process at data rck we eliminated did",
    "start": "1701840",
    "end": "1708600"
  },
  {
    "text": "more than 99% of the workload outages caused by the upgrade process compared to the first version I mean the Spiner",
    "start": "1708600",
    "end": "1715559"
  },
  {
    "text": "and Jenkins automation that we had and uh so we almost never get paged uh for",
    "start": "1715559",
    "end": "1720679"
  },
  {
    "text": "these kind of issues and we've been consistently upgrading uh more than a thousand clusters hundreds of thousands",
    "start": "1720679",
    "end": "1727720"
  },
  {
    "text": "of notes every month since late 2022 uh that's when we launched the system while",
    "start": "1727720",
    "end": "1733440"
  },
  {
    "text": "the kubernetes fleet uh scaled 5x and we uh we are using it to roll out",
    "start": "1733440",
    "end": "1738919"
  },
  {
    "text": "most of the infrastructure changes actually at data breaks so anything that can be rolled out using the nopo",
    "start": "1738919",
    "end": "1745159"
  },
  {
    "text": "rotation concept we use uh this system to roll roll it out and uh it can easily",
    "start": "1745159",
    "end": "1751559"
  },
  {
    "text": "onboard new workloads uh that are protected during upgrades because the no drain operator like I said we had this",
    "start": "1751559",
    "end": "1758519"
  },
  {
    "text": "uh interface with multiple ways to support and protect workloads so it's",
    "start": "1758519",
    "end": "1763640"
  },
  {
    "text": "very easy to onboard new types of workloads and here's a company blog post",
    "start": "1763640",
    "end": "1768919"
  },
  {
    "text": "uh where you can find more details uh about this system and lastly uh we are actively",
    "start": "1768919",
    "end": "1776279"
  },
  {
    "text": "hiring at data bricks so uh yeah if you're interested just go to our website and and you can also contact to me uh on",
    "start": "1776279",
    "end": "1785039"
  },
  {
    "text": "LinkedIn all right so that concludes the presentation now it's Q&A so there's a QR code uh feel free to scan it to",
    "start": "1785960",
    "end": "1793080"
  },
  {
    "text": "provide feedback to the session it's uh much appreciated",
    "start": "1793080",
    "end": "1798440"
  },
  {
    "text": "[Applause]",
    "start": "1799050",
    "end": "1807440"
  },
  {
    "text": "cool",
    "start": "1813440",
    "end": "1816440"
  },
  {
    "text": "yeah yeah yeah so the question was how we handle Master upgrades on uh cloud",
    "start": "1819720",
    "end": "1825320"
  },
  {
    "text": "provider kuties so in Cloud uh in cloud provider kubernetes really the upgrade is managed by the cloud providers and",
    "start": "1825320",
    "end": "1832480"
  },
  {
    "text": "the way you trigger that is by uh either setting up some Auto upgrade schedule or",
    "start": "1832480",
    "end": "1837600"
  },
  {
    "text": "sending some API call to the cloud provider uh so all we need to do is that",
    "start": "1837600",
    "end": "1842760"
  },
  {
    "text": "we actually have some special logic so in the noo uh rotation operator it will",
    "start": "1842760",
    "end": "1848600"
  },
  {
    "text": "check the um like Master version so if the master version is already uh a",
    "start": "1848600",
    "end": "1854880"
  },
  {
    "text": "version that for example let's say you want to upgrade your noes to 1 30 but your master is still on 1.29 it will",
    "start": "1854880",
    "end": "1861399"
  },
  {
    "text": "just be spinning there and waiting and then if the if it if it detex that the",
    "start": "1861399",
    "end": "1866440"
  },
  {
    "text": "master version is already 1 one30 or higher then you will just proceed with",
    "start": "1866440",
    "end": "1872000"
  },
  {
    "text": "the worker node upgrades so uh and then the the API call",
    "start": "1872000",
    "end": "1877159"
  },
  {
    "text": "is actually sent by the uh we had multiple iterations by the orchestrator of the opgrade it's like a simple still",
    "start": "1877159",
    "end": "1883799"
  },
  {
    "text": "like a simple workflow engine but because it's just one API call per clust CL usually is pretty easy and for some",
    "start": "1883799",
    "end": "1889840"
  },
  {
    "text": "other clusters we have Auto upgrade setup so we don't even have to do anything so the operators will just be",
    "start": "1889840",
    "end": "1895480"
  },
  {
    "text": "spinning and checking for uh version skill",
    "start": "1895480",
    "end": "1900320"
  },
  {
    "text": "compliance how do yous yeah yeah so like there can be uh",
    "start": "1901039",
    "end": "1908360"
  },
  {
    "text": "it's called we call it like invalid pdbs for example they can have like Max un unavailable equals zero so we actually",
    "start": "1908360",
    "end": "1915480"
  },
  {
    "text": "have some uh there are two things uh so we have some configuration uh check in our configuration system that you know",
    "start": "1915480",
    "end": "1922679"
  },
  {
    "text": "try to uh gate against that uh but it can happen uh so when it happens we will",
    "start": "1922679",
    "end": "1928679"
  },
  {
    "text": "know and we will like recommend ask the teams to use the uh either the",
    "start": "1928679",
    "end": "1934000"
  },
  {
    "text": "maintenance window that I mentioned or like basically the maintenance window that I mentioned so because like when a",
    "start": "1934000",
    "end": "1940080"
  },
  {
    "text": "pdb when they specify a pdb that has like no like that allows no part",
    "start": "1940080",
    "end": "1945679"
  },
  {
    "text": "downtime it basically it's saying oh like my workload cannot have any downtime and in that case really the",
    "start": "1945679",
    "end": "1951600"
  },
  {
    "text": "only way to upgrade is via planed downtime so they can configure it that way uh and we also have alerts on like",
    "start": "1951600",
    "end": "1958600"
  },
  {
    "text": "for example if like somebody just like unintentionally uh pushed like a bad pdb",
    "start": "1958600",
    "end": "1964039"
  },
  {
    "text": "we have alerts on that as well because like it will make it will result in the NOP rotation operator not being able to",
    "start": "1964039",
    "end": "1970440"
  },
  {
    "text": "make progress for like forever and we'll be alerted on that related to that question like what",
    "start": "1970440",
    "end": "1976720"
  },
  {
    "text": "if you have a p 50% disruption at that point in time application is % down yeah",
    "start": "1976720",
    "end": "1984559"
  },
  {
    "text": "yeah so it will just be spinning you'll just be waiting but like because we uh the whole process is like infinitely",
    "start": "1984559",
    "end": "1990760"
  },
  {
    "text": "retriable and it's always running in the background oh I mean it's fine to be",
    "start": "1990760",
    "end": "1995880"
  },
  {
    "text": "spinning for hours or even days because we have this monthly deadline we just have to meet the monthly deadline so",
    "start": "1995880",
    "end": "2002840"
  },
  {
    "text": "yeah if we need it to be faster sometimes like we will get alerted for that because we can tune uh the",
    "start": "2002840",
    "end": "2008840"
  },
  {
    "text": "threshold of alerts so we can be alerted on that and then we can work with the teams to block that unblock that if",
    "start": "2008840",
    "end": "2015200"
  },
  {
    "text": "there's like uh any emergency yeah Carpenter is open tool",
    "start": "2015200",
    "end": "2024638"
  },
  {
    "text": "right oh",
    "start": "2032360",
    "end": "2036360"
  },
  {
    "text": "yeah yeah okay okay I guess I forgot to repeat the question yeah I'll repeat so the question did you ever",
    "start": "2039039",
    "end": "2044639"
  },
  {
    "text": "consider yeah for Carpenter uh no we haven't yeah the question was like uh do",
    "start": "2044639",
    "end": "2049919"
  },
  {
    "text": "we consider using Carpenter no we didn't because like yeah just we didn't use",
    "start": "2049919",
    "end": "2055000"
  },
  {
    "text": "that because of various reasons we for like self-managed kubernetes we actually we actually have a very customized setup",
    "start": "2055000",
    "end": "2061560"
  },
  {
    "text": "that's like difficult to like it's totally from scratch so it's hard to uh set it up using some external tools",
    "start": "2061560",
    "end": "2068480"
  },
  {
    "text": "and then we use eks AKs GK so all the things work out the",
    "start": "2068480",
    "end": "2074398"
  },
  {
    "text": "box uh creation of clusters yeah so there is a lot of automation uh there is",
    "start": "2076760",
    "end": "2083358"
  },
  {
    "text": "like there are system that systems that provision clusters quickly we've actually done a lot of work on being",
    "start": "2083359",
    "end": "2089720"
  },
  {
    "text": "able to provision new clusters quickly and reliably so including bootstrapping the cluster itself and also",
    "start": "2089720",
    "end": "2095599"
  },
  {
    "text": "bootstrapping its dependencies for example the pki side of things the certificate distribution and also being",
    "start": "2095599",
    "end": "2101400"
  },
  {
    "text": "able to deploy the uh essential services to new clusters reibly so we've done",
    "start": "2101400",
    "end": "2106920"
  },
  {
    "text": "quite a lot of work on that as well hey quick question yeah how do you",
    "start": "2106920",
    "end": "2112680"
  },
  {
    "text": "validate a cluster ready to upgrade given uh API deprecations and so forth",
    "start": "2112680",
    "end": "2118200"
  },
  {
    "text": "how how can you ensure that the workflow is not going to break yeah so there is some coordination between different",
    "start": "2118200",
    "end": "2123560"
  },
  {
    "text": "clusters uh there's the uh we actually divide our clusters to uh we don't",
    "start": "2123560",
    "end": "2128680"
  },
  {
    "text": "really like trigger it's like a little bit simplified in the talk we don't really trigger upgrades on all clusters",
    "start": "2128680",
    "end": "2134000"
  },
  {
    "text": "at the same time at the beginning of the month we do have like deployment stages so we have like Dev and staging and",
    "start": "2134000",
    "end": "2140400"
  },
  {
    "text": "production environments so we'll try the upgrades in Dev first and then in staging and prod so uh and then uh if",
    "start": "2140400",
    "end": "2147960"
  },
  {
    "text": "there are issues uh we have a lot of health checks in the operators so um for example if there are like API problems",
    "start": "2147960",
    "end": "2155160"
  },
  {
    "text": "like the teams will be alerted because there you know their clients will be failing in one way or the other any Integrations",
    "start": "2155160",
    "end": "2162119"
  },
  {
    "text": "for like Cub Noe trouble or upgrade insights to validate before try inev or",
    "start": "2162119",
    "end": "2167160"
  },
  {
    "text": "keyway uh to validate as what was the first part of the question any integration or feature set for like Cube",
    "start": "2167160",
    "end": "2174160"
  },
  {
    "text": "no trouble to validate hey all these API are going to break before uh you trying the upgraded Dev yeah yeah we do have",
    "start": "2174160",
    "end": "2181839"
  },
  {
    "text": "validations before we even try to upgrade in Dev uh it's uh unfortunately as far as I know it's uh a fairly simple",
    "start": "2181839",
    "end": "2189359"
  },
  {
    "text": "process so it doesn't really cover like catch all the issues it's just because the uh surface area of kubernetes apis",
    "start": "2189359",
    "end": "2196440"
  },
  {
    "text": "that we use is like very large you mentioned that you have a",
    "start": "2196440",
    "end": "2203599"
  },
  {
    "text": "component both to scale down node pools but as well to scale them up does that mean you have your completely own",
    "start": "2203599",
    "end": "2209839"
  },
  {
    "text": "implementation of a clust auto scaler yeah so uh it's a an interesting problem like working uh the upgrade process",
    "start": "2209839",
    "end": "2216240"
  },
  {
    "text": "working with the cluster a scaler I didn't cover that in the slides but there is careful coordination between",
    "start": "2216240",
    "end": "2222520"
  },
  {
    "text": "that and closer AO scaler so the way it works is that in closer Auto scaler you have this uh this thing called the",
    "start": "2222520",
    "end": "2228000"
  },
  {
    "text": "priority expander config map so during the upgrade we will uh gradually shift",
    "start": "2228000",
    "end": "2233599"
  },
  {
    "text": "the weight of the new stack in the priority expander config map so that the",
    "start": "2233599",
    "end": "2239119"
  },
  {
    "text": "cluster Auto scaler will also gradually prefer scaling up new nodes uh in the new stack as opposed to the old stack so",
    "start": "2239119",
    "end": "2246160"
  },
  {
    "text": "it's possible that we make get like you know uh old noes scaled up by cluster",
    "start": "2246160",
    "end": "2251480"
  },
  {
    "text": "autoscaler but because we gradually shift the expander config map while we",
    "start": "2251480",
    "end": "2257359"
  },
  {
    "text": "continuously like drain the old notes and replace them with new notes eventually it will converge to a state",
    "start": "2257359",
    "end": "2263040"
  },
  {
    "text": "where you have all the notes being the new noes and then you have the priority expander config map always pointing to",
    "start": "2263040",
    "end": "2269119"
  },
  {
    "text": "the new stack uh how do you handle data for your",
    "start": "2269119",
    "end": "2275680"
  },
  {
    "text": "self-managed kubernetes clusters where does the data live you mean how uh how how do I hand",
    "start": "2275680",
    "end": "2281359"
  },
  {
    "text": "how do we handle data so do you have data attached to your worker nodes are you like how do you replace the nodes",
    "start": "2281359",
    "end": "2287800"
  },
  {
    "text": "and not worry about the data that's on directly attached to the nodes uh yeah I see yeah so like the uh uh so there are",
    "start": "2287800",
    "end": "2295040"
  },
  {
    "text": "workloads using uh you know like storage especially if they're using like local infal storage then It's Tricky uh so",
    "start": "2295040",
    "end": "2302359"
  },
  {
    "text": "often it requires uh I mentioned there's like the uh some custom Handler framework that we provide to workloads",
    "start": "2302359",
    "end": "2308880"
  },
  {
    "text": "uh usually for example for some databases like uh when you do eviction uh of one of the workers you actually",
    "start": "2308880",
    "end": "2315960"
  },
  {
    "text": "want the uh the new worker to be up on a new not and you want it to like fully",
    "start": "2315960",
    "end": "2321480"
  },
  {
    "text": "replicate the state before you can join another one so that's how we that's why we have this custom uh logic that we",
    "start": "2321480",
    "end": "2327720"
  },
  {
    "text": "allow teams to build so that they can build that like to ensure that it's always safe from their application",
    "start": "2327720",
    "end": "2335079"
  },
  {
    "text": "standpoint teams sorry so you don't worry about the data it's the team's responsibility make sure",
    "start": "2335960",
    "end": "2341839"
  },
  {
    "text": "the data is replicated before they're replaced yeah because it's uh uh I mean",
    "start": "2341839",
    "end": "2347359"
  },
  {
    "text": "it's very different for different applications right uh so part of the responsibilities will be on the team and",
    "start": "2347359",
    "end": "2353920"
  },
  {
    "text": "our team mainly focuses on making sure that the new nodes always have the",
    "start": "2353920",
    "end": "2359440"
  },
  {
    "text": "ability to store data and then the replication is like very application",
    "start": "2359440",
    "end": "2364520"
  },
  {
    "text": "specific okay thank you",
    "start": "2364520",
    "end": "2369079"
  },
  {
    "text": "how about now okay there we go yeah uh thank you for the presentation I really enjoyed it I very very cool stuff um one",
    "start": "2371880",
    "end": "2377240"
  },
  {
    "text": "of the questions I had though when architecting and coming up with this scheme where did you draw the line with",
    "start": "2377240",
    "end": "2383160"
  },
  {
    "text": "Cloud apis doing some of the work and you doing the work because that that seems to be like a a really fine line",
    "start": "2383160",
    "end": "2390160"
  },
  {
    "text": "there where where eks can do some of the stuff but not as well as if you control it all yourself so how did you think",
    "start": "2390160",
    "end": "2395800"
  },
  {
    "text": "about that particular problem when you did this so uh there are two things one is the master upgrade like in eks AKs and GK so",
    "start": "2395800",
    "end": "2404000"
  },
  {
    "text": "that totally falls on to the responsibilities of the cloud providers so we just you know issue API calls for",
    "start": "2404000",
    "end": "2409720"
  },
  {
    "text": "them to do the upgrades and then for the worker node upgrades we try to push the differences all the way down to the uh",
    "start": "2409720",
    "end": "2417760"
  },
  {
    "text": "VM level so uh as you see in the presentation in the Noto capacity",
    "start": "2417760",
    "end": "2422880"
  },
  {
    "text": "operator it really just talks talks to the VMS and the vmss is as SGS um mix it",
    "start": "2422880",
    "end": "2429480"
  },
  {
    "text": "tries to uh manipulate those apis to uh increase and decrease the counts of no",
    "start": "2429480",
    "end": "2435920"
  },
  {
    "text": "pools and we try to avoid uh using like more uh Cloud specific and uh cloud",
    "start": "2435920",
    "end": "2443599"
  },
  {
    "text": "provider kubernetes specific features to do these kind of things uh for the reason I mentioned we have like",
    "start": "2443599",
    "end": "2448960"
  },
  {
    "text": "different flavors different dis trolls so it would be hard to ensure consistency that way so just try to push",
    "start": "2448960",
    "end": "2454520"
  },
  {
    "text": "all the things down and the uh really just manage the uh VMS",
    "start": "2454520",
    "end": "2460240"
  },
  {
    "text": "directly yeah uh hi how do you handle upgrades to",
    "start": "2463200",
    "end": "2470560"
  },
  {
    "text": "Cluster add-ons like cord DNS and croxy yeah so um so it's uh this is a kind of",
    "start": "2470560",
    "end": "2477880"
  },
  {
    "text": "U like a different process from uh here because like in different uh flavors uh",
    "start": "2477880",
    "end": "2484560"
  },
  {
    "text": "they are managed differently for example in uh eks like you can have this thing",
    "start": "2484560",
    "end": "2490000"
  },
  {
    "text": "called manage addons where if you upgrade you can issue API calls to uh",
    "start": "2490000",
    "end": "2495960"
  },
  {
    "text": "like eks for them to upgrade that for you and then in uh some other clouds and",
    "start": "2495960",
    "end": "2501079"
  },
  {
    "text": "self-managed for sure like there's no way to do that kind of uh upgrade um",
    "start": "2501079",
    "end": "2508079"
  },
  {
    "text": "like uh automatically right you have to try to orchestrate the upgrades yourself so uh yeah the way it works is that uh",
    "start": "2508079",
    "end": "2516000"
  },
  {
    "text": "we have a different process say so sometimes um so it's more about uh like",
    "start": "2516000",
    "end": "2521359"
  },
  {
    "text": "the critical add-ons are considered to be part of the control the master upgrade process so we want want to make",
    "start": "2521359",
    "end": "2528720"
  },
  {
    "text": "sure that actually all the critical add-ons have been upgraded fully before we can actually do the node upgrade of",
    "start": "2528720",
    "end": "2534720"
  },
  {
    "text": "course like for Q proxy we have to go with the node but like for let's say for core DNS it's like it has to we we",
    "start": "2534720",
    "end": "2541160"
  },
  {
    "text": "bundle it with the master upgrades uh just to follow up to Matt's",
    "start": "2541160",
    "end": "2546960"
  },
  {
    "text": "question question so just very specifically uh for example in the eks implementation you do not use eks",
    "start": "2546960",
    "end": "2553880"
  },
  {
    "text": "managed node groups as that would compete with the what the operator does correct do you just use asgs directly I",
    "start": "2553880",
    "end": "2559040"
  },
  {
    "text": "mean we do use diens no groups and it actually doesn't really compete with what the operator does because uh I mean",
    "start": "2559040",
    "end": "2565559"
  },
  {
    "text": "the there's a Nuance like I think the the difference we see here is that it's hard to terminate a specific VM in a eks",
    "start": "2565559",
    "end": "2573440"
  },
  {
    "text": "node manage node group uh but otherwise I think it's been fine like as long as",
    "start": "2573440",
    "end": "2578559"
  },
  {
    "text": "long as you don't enable like a lot the managed features uh so like for virtual",
    "start": "2578559",
    "end": "2585040"
  },
  {
    "text": "machines for example if you want to kill like a specific VM in an eks Management Group It's Tricky and then the way we do",
    "start": "2585040",
    "end": "2592119"
  },
  {
    "text": "this is like we actually on eks there's a special handling for these kind of things after we have drained and fully",
    "start": "2592119",
    "end": "2599359"
  },
  {
    "text": "evicted a node we uh actually don't kill the VM uh for managed node groups like",
    "start": "2599359",
    "end": "2605520"
  },
  {
    "text": "we uh we actually like give it like uh we annotate all the other no the other",
    "start": "2605520",
    "end": "2612359"
  },
  {
    "text": "old noes to prevent them from being scaled down by the cluster Auto scaler and but then we uh for the notes that we",
    "start": "2612359",
    "end": "2619000"
  },
  {
    "text": "have corded and drained we don't have The annotation so cluster Auto scaler will actually take care of it because",
    "start": "2619000",
    "end": "2625240"
  },
  {
    "text": "there's no workload on it so that's like a special handling for uh manage node",
    "start": "2625240",
    "end": "2631079"
  },
  {
    "text": "groups on eks but like if you update a manage node",
    "start": "2631079",
    "end": "2636200"
  },
  {
    "text": "group with like a new launch template doesn't that trigger Like a Rolling update of nodes in your kubernetes in",
    "start": "2636200",
    "end": "2642160"
  },
  {
    "text": "your eks cluster oh uh so yeah so we uh yeah so that's why we have two stacks right it's",
    "start": "2642160",
    "end": "2648240"
  },
  {
    "text": "not one manage node group we have like actually two manage node groups for the same logical nopool so we actually never",
    "start": "2648240",
    "end": "2655000"
  },
  {
    "text": "upgrade the launch template of one nopool but then we always create a new one but the new one the noes have the",
    "start": "2655000",
    "end": "2661400"
  },
  {
    "text": "same labels and tains and annotations so they'll map like from a kubernetes standpoint uh it bels to to the same",
    "start": "2661400",
    "end": "2668280"
  },
  {
    "text": "nopol got it thank you yeah",
    "start": "2668280",
    "end": "2673119"
  }
]