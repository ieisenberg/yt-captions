[
  {
    "text": "uh hello everyone thanks for joining my",
    "start": "160",
    "end": "1839"
  },
  {
    "text": "session um",
    "start": "1839",
    "end": "3520"
  },
  {
    "text": "so",
    "start": "3520",
    "end": "4319"
  },
  {
    "text": "my name is dan i i lead the bloomberg",
    "start": "4319",
    "end": "7359"
  },
  {
    "text": "data science serverless platform and i'm",
    "start": "7359",
    "end": "9679"
  },
  {
    "text": "also the working group lead for k-serve",
    "start": "9679",
    "end": "12639"
  },
  {
    "text": "and i've been working with canadian",
    "start": "12639",
    "end": "15440"
  },
  {
    "text": "computer creative community",
    "start": "15440",
    "end": "17600"
  },
  {
    "text": "since 2019 and worked with a lot of the",
    "start": "17600",
    "end": "20800"
  },
  {
    "text": "developers here to build the uh k-serve",
    "start": "20800",
    "end": "23519"
  },
  {
    "text": "which is another open source project we",
    "start": "23519",
    "end": "25680"
  },
  {
    "text": "are talking about talking about today",
    "start": "25680",
    "end": "28960"
  },
  {
    "text": "yeah so today uh my topic is to uh",
    "start": "28960",
    "end": "31840"
  },
  {
    "text": "discuss how we built the mr platform on",
    "start": "31840",
    "end": "34399"
  },
  {
    "text": "top of canadiv",
    "start": "34399",
    "end": "35920"
  },
  {
    "text": "and uh um i want to discuss uh some of",
    "start": "35920",
    "end": "38559"
  },
  {
    "text": "what works well in canada and what other",
    "start": "38559",
    "end": "40640"
  },
  {
    "text": "features are we also looking for",
    "start": "40640",
    "end": "43040"
  },
  {
    "text": "to push canada to the next level",
    "start": "43040",
    "end": "47280"
  },
  {
    "text": "um yeah",
    "start": "47280",
    "end": "49920"
  },
  {
    "text": "unfortunately my co-speaker cannot make",
    "start": "50079",
    "end": "52399"
  },
  {
    "text": "today and also",
    "start": "52399",
    "end": "53920"
  },
  {
    "text": "credits to for my kids to do all the",
    "start": "53920",
    "end": "56000"
  },
  {
    "text": "slides animations",
    "start": "56000",
    "end": "59039"
  },
  {
    "text": "um so",
    "start": "59039",
    "end": "60960"
  },
  {
    "text": "first what is a mer influence platform",
    "start": "60960",
    "end": "64320"
  },
  {
    "text": "mri influence platform provides a",
    "start": "64320",
    "end": "66000"
  },
  {
    "text": "standard",
    "start": "66000",
    "end": "67280"
  },
  {
    "text": "manager inference service",
    "start": "67280",
    "end": "69200"
  },
  {
    "text": "that provides helps to unify the model",
    "start": "69200",
    "end": "72080"
  },
  {
    "text": "deployment across our",
    "start": "72080",
    "end": "74240"
  },
  {
    "text": "multiple ml frameworks and it also",
    "start": "74240",
    "end": "76799"
  },
  {
    "text": "simplify and",
    "start": "76799",
    "end": "78799"
  },
  {
    "text": "the model serving and monitoring",
    "start": "78799",
    "end": "81840"
  },
  {
    "text": "at scale on production cloud environment",
    "start": "81840",
    "end": "84880"
  },
  {
    "text": "so a ml inference",
    "start": "84880",
    "end": "86880"
  },
  {
    "text": "inference service is a service that can",
    "start": "86880",
    "end": "88560"
  },
  {
    "text": "generate predictions",
    "start": "88560",
    "end": "90640"
  },
  {
    "text": "from a trainer model",
    "start": "90640",
    "end": "92960"
  },
  {
    "text": "in response to",
    "start": "92960",
    "end": "94880"
  },
  {
    "text": "inference requests that can be a single",
    "start": "94880",
    "end": "96880"
  },
  {
    "text": "request or a bachelor of request",
    "start": "96880",
    "end": "101159"
  },
  {
    "text": "so besides the common",
    "start": "101439",
    "end": "103600"
  },
  {
    "text": "problems like infrastructure challenges",
    "start": "103600",
    "end": "106320"
  },
  {
    "text": "to deploy",
    "start": "106320",
    "end": "107439"
  },
  {
    "text": "cloud microservices",
    "start": "107439",
    "end": "110000"
  },
  {
    "text": "so we also have a unique",
    "start": "110000",
    "end": "113200"
  },
  {
    "text": "challenges for deploying ml influence",
    "start": "113200",
    "end": "116479"
  },
  {
    "text": "inference services so first of all we",
    "start": "116479",
    "end": "119119"
  },
  {
    "text": "want to",
    "start": "119119",
    "end": "120960"
  },
  {
    "text": "enable the auto scale",
    "start": "120960",
    "end": "123280"
  },
  {
    "text": "influence workload both on cpu and gpu",
    "start": "123280",
    "end": "126399"
  },
  {
    "text": "workload so as you know",
    "start": "126399",
    "end": "129679"
  },
  {
    "text": "the default accommodate is",
    "start": "129679",
    "end": "131840"
  },
  {
    "text": "hpa does not support auto scanning based",
    "start": "131840",
    "end": "134319"
  },
  {
    "text": "on gpu",
    "start": "134319",
    "end": "135599"
  },
  {
    "text": "in order to support that you need to",
    "start": "135599",
    "end": "138319"
  },
  {
    "text": "implement hpl with custom metrics",
    "start": "138319",
    "end": "141280"
  },
  {
    "text": "on the set of gpu metrics like duty",
    "start": "141280",
    "end": "143760"
  },
  {
    "text": "cycle",
    "start": "143760",
    "end": "145680"
  },
  {
    "text": "power consumption and",
    "start": "145680",
    "end": "147840"
  },
  {
    "text": "gpu memory which is sometimes can be",
    "start": "147840",
    "end": "151120"
  },
  {
    "text": "really hard to reason about how it makes",
    "start": "151120",
    "end": "153360"
  },
  {
    "text": "the auto scaling decisions so we are",
    "start": "153360",
    "end": "155680"
  },
  {
    "text": "really looking forward looking for a",
    "start": "155680",
    "end": "157760"
  },
  {
    "text": "solution which can",
    "start": "157760",
    "end": "159599"
  },
  {
    "text": "work the auto scale work the same way",
    "start": "159599",
    "end": "161440"
  },
  {
    "text": "both on cpu and gpu device",
    "start": "161440",
    "end": "164560"
  },
  {
    "text": "and",
    "start": "164560",
    "end": "166160"
  },
  {
    "text": "and secondly another important feature",
    "start": "166160",
    "end": "168720"
  },
  {
    "text": "we want to do is to",
    "start": "168720",
    "end": "170959"
  },
  {
    "text": "when we're doing a model layout",
    "start": "170959",
    "end": "173760"
  },
  {
    "text": "so we want to employ a safe model",
    "start": "173760",
    "end": "176879"
  },
  {
    "text": "rollout strategy with deeper validations",
    "start": "176879",
    "end": "179920"
  },
  {
    "text": "in addition to",
    "start": "179920",
    "end": "181920"
  },
  {
    "text": "readiness probes",
    "start": "181920",
    "end": "183599"
  },
  {
    "text": "so this is crucial for enable like a",
    "start": "183599",
    "end": "186319"
  },
  {
    "text": "continuous deployment",
    "start": "186319",
    "end": "188319"
  },
  {
    "text": "for the model updates without humans",
    "start": "188319",
    "end": "190080"
  },
  {
    "text": "loop",
    "start": "190080",
    "end": "191760"
  },
  {
    "text": "and",
    "start": "191760",
    "end": "193760"
  },
  {
    "text": "besides in addition to the request",
    "start": "193760",
    "end": "196080"
  },
  {
    "text": "response style",
    "start": "196080",
    "end": "197519"
  },
  {
    "text": "service and we are there i also use case",
    "start": "197519",
    "end": "200080"
  },
  {
    "text": "where we want to",
    "start": "200080",
    "end": "201440"
  },
  {
    "text": "perform the inference um",
    "start": "201440",
    "end": "204400"
  },
  {
    "text": "based on some",
    "start": "204400",
    "end": "205920"
  },
  {
    "text": "event source like a3 kafka and we also",
    "start": "205920",
    "end": "208720"
  },
  {
    "text": "need to uh",
    "start": "208720",
    "end": "210480"
  },
  {
    "text": "forward request to",
    "start": "210480",
    "end": "212560"
  },
  {
    "text": "a number of like downstream uh analytics",
    "start": "212560",
    "end": "215040"
  },
  {
    "text": "components which can which want to",
    "start": "215040",
    "end": "216720"
  },
  {
    "text": "monitor the models to ensure that it",
    "start": "216720",
    "end": "219120"
  },
  {
    "text": "performs the uh reliable predictions",
    "start": "219120",
    "end": "222000"
  },
  {
    "text": "and last but not the least we also have",
    "start": "222000",
    "end": "224799"
  },
  {
    "text": "use case where we want to",
    "start": "224799",
    "end": "227680"
  },
  {
    "text": "have an infrared platform which can",
    "start": "227680",
    "end": "229599"
  },
  {
    "text": "change multiple inference services",
    "start": "229599",
    "end": "231040"
  },
  {
    "text": "together to get back a response and",
    "start": "231040",
    "end": "234400"
  },
  {
    "text": "or it may need to combine the outputs",
    "start": "234400",
    "end": "237280"
  },
  {
    "text": "from multiple services",
    "start": "237280",
    "end": "238959"
  },
  {
    "text": "for the model ensemble use case",
    "start": "238959",
    "end": "242480"
  },
  {
    "text": "um so uh why do we decide to build uh",
    "start": "243360",
    "end": "246879"
  },
  {
    "text": "the influence platform on top of",
    "start": "246879",
    "end": "248159"
  },
  {
    "text": "keynative right so can it give us a very",
    "start": "248159",
    "end": "251120"
  },
  {
    "text": "nice serverless",
    "start": "251120",
    "end": "252720"
  },
  {
    "text": "service obstructions",
    "start": "252720",
    "end": "255519"
  },
  {
    "text": "for service networking and routing and",
    "start": "255519",
    "end": "258560"
  },
  {
    "text": "adding both requests",
    "start": "258560",
    "end": "260799"
  },
  {
    "text": "based",
    "start": "260799",
    "end": "261759"
  },
  {
    "text": "driven auto scaling",
    "start": "261759",
    "end": "264800"
  },
  {
    "text": "both on cpu and gpu device",
    "start": "264800",
    "end": "267680"
  },
  {
    "text": "worked pretty nicely it also supports",
    "start": "267680",
    "end": "270400"
  },
  {
    "text": "both scale down two and front zero",
    "start": "270400",
    "end": "273120"
  },
  {
    "text": "key native also implements",
    "start": "273120",
    "end": "275919"
  },
  {
    "text": "immutable revision tracking which allows",
    "start": "275919",
    "end": "278000"
  },
  {
    "text": "for explaining traffic among multiple",
    "start": "278000",
    "end": "279840"
  },
  {
    "text": "revisions",
    "start": "279840",
    "end": "281040"
  },
  {
    "text": "for group blue green and the canary",
    "start": "281040",
    "end": "283040"
  },
  {
    "text": "route",
    "start": "283040",
    "end": "284560"
  },
  {
    "text": "and other nice features it provides is",
    "start": "284560",
    "end": "286639"
  },
  {
    "text": "like you can get",
    "start": "286639",
    "end": "288400"
  },
  {
    "text": "outbox",
    "start": "288400",
    "end": "289520"
  },
  {
    "text": "distributor tracing and metrics for free",
    "start": "289520",
    "end": "292080"
  },
  {
    "text": "and",
    "start": "292080",
    "end": "293680"
  },
  {
    "text": "and low balancing like it can low",
    "start": "293680",
    "end": "295840"
  },
  {
    "text": "balance based on like a concurrency on",
    "start": "295840",
    "end": "298880"
  },
  {
    "text": "each part to make smart low balance",
    "start": "298880",
    "end": "301120"
  },
  {
    "text": "decisions",
    "start": "301120",
    "end": "302240"
  },
  {
    "text": "so in order to",
    "start": "302240",
    "end": "303840"
  },
  {
    "text": "avoid the ring event wheels all these uh",
    "start": "303840",
    "end": "306240"
  },
  {
    "text": "already solved problems so we decided to",
    "start": "306240",
    "end": "309199"
  },
  {
    "text": "build",
    "start": "309199",
    "end": "311440"
  },
  {
    "text": "the ml platform as inference platform on",
    "start": "311759",
    "end": "314000"
  },
  {
    "text": "top of canadian so we can focus on to",
    "start": "314000",
    "end": "316400"
  },
  {
    "text": "solve like",
    "start": "316400",
    "end": "317680"
  },
  {
    "text": "our unique influence challenges",
    "start": "317680",
    "end": "321840"
  },
  {
    "text": "um so um so here is",
    "start": "322639",
    "end": "325680"
  },
  {
    "text": "how the case server was cases was an",
    "start": "325680",
    "end": "328160"
  },
  {
    "text": "open source project which was uh funded",
    "start": "328160",
    "end": "330320"
  },
  {
    "text": "by",
    "start": "330320",
    "end": "331360"
  },
  {
    "text": "companies like",
    "start": "331360",
    "end": "332639"
  },
  {
    "text": "google ibm and bloomberg",
    "start": "332639",
    "end": "335199"
  },
  {
    "text": "back in 2019 under the cool flow",
    "start": "335199",
    "end": "338160"
  },
  {
    "text": "umbrella",
    "start": "338160",
    "end": "339680"
  },
  {
    "text": "it was used to be a sub project and now",
    "start": "339680",
    "end": "342639"
  },
  {
    "text": "we grow",
    "start": "342639",
    "end": "343919"
  },
  {
    "text": "we grow tremendously afterwards and now",
    "start": "343919",
    "end": "346080"
  },
  {
    "text": "it's an independent project under the",
    "start": "346080",
    "end": "348160"
  },
  {
    "text": "governance by the airfare ai",
    "start": "348160",
    "end": "351039"
  },
  {
    "text": "um so k server in the service mode it",
    "start": "351039",
    "end": "353520"
  },
  {
    "text": "actually creates the uh canadian service",
    "start": "353520",
    "end": "356160"
  },
  {
    "text": "to provision um",
    "start": "356160",
    "end": "358080"
  },
  {
    "text": "to enable the servlets functionality",
    "start": "358080",
    "end": "360000"
  },
  {
    "text": "like auto scaling canary route and then",
    "start": "360000",
    "end": "362080"
  },
  {
    "text": "eventing capabilities",
    "start": "362080",
    "end": "363919"
  },
  {
    "text": "so um",
    "start": "363919",
    "end": "365680"
  },
  {
    "text": "in fact k native is actually installed",
    "start": "365680",
    "end": "368080"
  },
  {
    "text": "by default in the queue flow",
    "start": "368080",
    "end": "370400"
  },
  {
    "text": "so",
    "start": "370400",
    "end": "371520"
  },
  {
    "text": "it actually as a result it",
    "start": "371520",
    "end": "374639"
  },
  {
    "text": "it powers the tens of like a production",
    "start": "374639",
    "end": "376800"
  },
  {
    "text": "model deployment",
    "start": "376800",
    "end": "378479"
  },
  {
    "text": "currently um because of the huge base of",
    "start": "378479",
    "end": "380880"
  },
  {
    "text": "a huge user base of queue flow",
    "start": "380880",
    "end": "384639"
  },
  {
    "text": "um so",
    "start": "385120",
    "end": "386960"
  },
  {
    "text": "influence service is a combination",
    "start": "386960",
    "end": "388960"
  },
  {
    "text": "customer source we created uh under k",
    "start": "388960",
    "end": "391680"
  },
  {
    "text": "serve",
    "start": "391680",
    "end": "392800"
  },
  {
    "text": "which is a mf-friendly uh user interface",
    "start": "392800",
    "end": "396479"
  },
  {
    "text": "to allow people to describe",
    "start": "396479",
    "end": "399280"
  },
  {
    "text": "mirror",
    "start": "399280",
    "end": "400319"
  },
  {
    "text": "deployments so um in",
    "start": "400319",
    "end": "404560"
  },
  {
    "text": "a lot of time like people just need to",
    "start": "405280",
    "end": "406880"
  },
  {
    "text": "specify the model format and and the",
    "start": "406880",
    "end": "409360"
  },
  {
    "text": "model storage ui so um and then they can",
    "start": "409360",
    "end": "412800"
  },
  {
    "text": "deploy the infrared service with a",
    "start": "412800",
    "end": "414160"
  },
  {
    "text": "simple yamo and under the hood it gets",
    "start": "414160",
    "end": "416960"
  },
  {
    "text": "the infrared service gets translated",
    "start": "416960",
    "end": "418639"
  },
  {
    "text": "into a key native service which runs the",
    "start": "418639",
    "end": "421919"
  },
  {
    "text": "outbox model server which is implemented",
    "start": "421919",
    "end": "424319"
  },
  {
    "text": "in k-serve",
    "start": "424319",
    "end": "426080"
  },
  {
    "text": "it downloads the model in indeed",
    "start": "426080",
    "end": "427840"
  },
  {
    "text": "container",
    "start": "427840",
    "end": "429120"
  },
  {
    "text": "then once model is downloaded and then",
    "start": "429120",
    "end": "430800"
  },
  {
    "text": "spins off the service",
    "start": "430800",
    "end": "432639"
  },
  {
    "text": "in response to the the real-time",
    "start": "432639",
    "end": "434479"
  },
  {
    "text": "influence requests you can also choose",
    "start": "434479",
    "end": "436880"
  },
  {
    "text": "to use build pack or case of sdk to",
    "start": "436880",
    "end": "439520"
  },
  {
    "text": "build your custom model server",
    "start": "439520",
    "end": "442240"
  },
  {
    "text": "which works pretty much the same",
    "start": "442240",
    "end": "444319"
  },
  {
    "text": "same way",
    "start": "444319",
    "end": "446720"
  },
  {
    "text": "so the case of control plan provisions",
    "start": "446960",
    "end": "449120"
  },
  {
    "text": "uh",
    "start": "449120",
    "end": "449840"
  },
  {
    "text": "a few",
    "start": "449840",
    "end": "451039"
  },
  {
    "text": "core influence components like a",
    "start": "451039",
    "end": "453360"
  },
  {
    "text": "predictor transformer and explainer",
    "start": "453360",
    "end": "456160"
  },
  {
    "text": "predictor runs as a kinetic service",
    "start": "456160",
    "end": "459039"
  },
  {
    "text": "which is in the main container runs the",
    "start": "459039",
    "end": "460800"
  },
  {
    "text": "model server and uh sit along with the q",
    "start": "460800",
    "end": "463520"
  },
  {
    "text": "proxy which",
    "start": "463520",
    "end": "464960"
  },
  {
    "text": "exposes the auto scaling metrics and",
    "start": "464960",
    "end": "467120"
  },
  {
    "text": "can choose concurrency and we also have",
    "start": "467120",
    "end": "469199"
  },
  {
    "text": "a model agent",
    "start": "469199",
    "end": "470720"
  },
  {
    "text": "which does",
    "start": "470720",
    "end": "472960"
  },
  {
    "text": "influence related features like",
    "start": "472960",
    "end": "475039"
  },
  {
    "text": "log in the requests and perform batching",
    "start": "475039",
    "end": "478080"
  },
  {
    "text": "uh and then sends requests to the model",
    "start": "478080",
    "end": "480400"
  },
  {
    "text": "server and transformer is a",
    "start": "480400",
    "end": "482879"
  },
  {
    "text": "is a component which",
    "start": "482879",
    "end": "484560"
  },
  {
    "text": "um",
    "start": "484560",
    "end": "485599"
  },
  {
    "text": "which transforms the raw input request",
    "start": "485599",
    "end": "488000"
  },
  {
    "text": "and converts to the format model server",
    "start": "488000",
    "end": "490160"
  },
  {
    "text": "expects according to the standardized",
    "start": "490160",
    "end": "493120"
  },
  {
    "text": "influence protocol",
    "start": "493120",
    "end": "495039"
  },
  {
    "text": "and explainer sends requests to the",
    "start": "495039",
    "end": "497520"
  },
  {
    "text": "predict predictor to uh try to make a uh",
    "start": "497520",
    "end": "501520"
  },
  {
    "text": "to out to generate the human",
    "start": "501520",
    "end": "503280"
  },
  {
    "text": "interpretable uh predictions",
    "start": "503280",
    "end": "505759"
  },
  {
    "text": "explanations",
    "start": "505759",
    "end": "508560"
  },
  {
    "text": "so let's first look at the most",
    "start": "509919",
    "end": "512240"
  },
  {
    "text": "important feature kennedy provides which",
    "start": "512240",
    "end": "514159"
  },
  {
    "text": "is the request driven auto scaling so",
    "start": "514159",
    "end": "516880"
  },
  {
    "text": "the security auto scaler excuse and auto",
    "start": "516880",
    "end": "519599"
  },
  {
    "text": "scale um",
    "start": "519599",
    "end": "522000"
  },
  {
    "text": "based on the request amount so by",
    "start": "522000",
    "end": "524399"
  },
  {
    "text": "collecting the uh concurrency and",
    "start": "524399",
    "end": "527200"
  },
  {
    "text": "request rate metrics from the q proxy",
    "start": "527200",
    "end": "529920"
  },
  {
    "text": "uh and",
    "start": "529920",
    "end": "532080"
  },
  {
    "text": "it is a process both scale 2 and front",
    "start": "532080",
    "end": "534480"
  },
  {
    "text": "0. um so it's",
    "start": "534480",
    "end": "536560"
  },
  {
    "text": "uh it can be really useful when you when",
    "start": "536560",
    "end": "538560"
  },
  {
    "text": "you deploy influence service on gpu",
    "start": "538560",
    "end": "540160"
  },
  {
    "text": "device which can save the gpu resources",
    "start": "540160",
    "end": "542399"
  },
  {
    "text": "while the service is idle",
    "start": "542399",
    "end": "544160"
  },
  {
    "text": "and",
    "start": "544160",
    "end": "545760"
  },
  {
    "text": "and cold start is still a kind of",
    "start": "545760",
    "end": "548320"
  },
  {
    "text": "problem for the ameri deployments on",
    "start": "548320",
    "end": "550240"
  },
  {
    "text": "production environment because usually",
    "start": "550240",
    "end": "552399"
  },
  {
    "text": "it needs to download the model and which",
    "start": "552399",
    "end": "554399"
  },
  {
    "text": "takes sometimes takes a few minutes and",
    "start": "554399",
    "end": "556320"
  },
  {
    "text": "then like",
    "start": "556320",
    "end": "557760"
  },
  {
    "text": "and the part gets started like a few",
    "start": "557760",
    "end": "559200"
  },
  {
    "text": "seconds so",
    "start": "559200",
    "end": "560399"
  },
  {
    "text": "um",
    "start": "560399",
    "end": "561200"
  },
  {
    "text": "in case of actually uh",
    "start": "561200",
    "end": "563839"
  },
  {
    "text": "sets the default main replica to one so",
    "start": "563839",
    "end": "566640"
  },
  {
    "text": "uh and uh and you can also choose to set",
    "start": "566640",
    "end": "569600"
  },
  {
    "text": "up a bigger number uh on the production",
    "start": "569600",
    "end": "571680"
  },
  {
    "text": "environment so",
    "start": "571680",
    "end": "573040"
  },
  {
    "text": "it can scare automatically to handle the",
    "start": "573040",
    "end": "575440"
  },
  {
    "text": "burst in the peak time",
    "start": "575440",
    "end": "578640"
  },
  {
    "text": "so let's take a look how uh",
    "start": "579279",
    "end": "581440"
  },
  {
    "text": "scale down to and from zero works so uh",
    "start": "581440",
    "end": "584800"
  },
  {
    "text": "while the service is idle um k native",
    "start": "584800",
    "end": "587519"
  },
  {
    "text": "controller actually rewrites the http",
    "start": "587519",
    "end": "590640"
  },
  {
    "text": "router to the canadian activator",
    "start": "590640",
    "end": "594160"
  },
  {
    "text": "and once you receive the um",
    "start": "594160",
    "end": "596959"
  },
  {
    "text": "receiving the request volume and auto",
    "start": "596959",
    "end": "599680"
  },
  {
    "text": "scaler makes",
    "start": "599680",
    "end": "601120"
  },
  {
    "text": "based on the request demand it",
    "start": "601120",
    "end": "603360"
  },
  {
    "text": "automatically scale up the uh to the",
    "start": "603360",
    "end": "605839"
  },
  {
    "text": "desired number of parts based on the",
    "start": "605839",
    "end": "608320"
  },
  {
    "text": "auto scanning metrics um and uh canadian",
    "start": "608320",
    "end": "611440"
  },
  {
    "text": "activator uh stores the request until",
    "start": "611440",
    "end": "614240"
  },
  {
    "text": "the pods are ready to serve the uh live",
    "start": "614240",
    "end": "616800"
  },
  {
    "text": "traffic",
    "start": "616800",
    "end": "619800"
  },
  {
    "text": "um so",
    "start": "620720",
    "end": "623440"
  },
  {
    "text": "so canadian on the other hand also",
    "start": "623440",
    "end": "626079"
  },
  {
    "text": "scale down to zero after the default 30",
    "start": "626079",
    "end": "628399"
  },
  {
    "text": "seconds so sometimes when you do testing",
    "start": "628399",
    "end": "630560"
  },
  {
    "text": "or benchmark testing",
    "start": "630560",
    "end": "632480"
  },
  {
    "text": "you may want to avoid the cost of a spin",
    "start": "632480",
    "end": "635519"
  },
  {
    "text": "upper bound down this part so you can",
    "start": "635519",
    "end": "637760"
  },
  {
    "text": "also choose to add the additional",
    "start": "637760",
    "end": "639120"
  },
  {
    "text": "annotations to keep the part a little",
    "start": "639120",
    "end": "640959"
  },
  {
    "text": "longer so",
    "start": "640959",
    "end": "642560"
  },
  {
    "text": "to avoid the",
    "start": "642560",
    "end": "644880"
  },
  {
    "text": "code starter penalty cost",
    "start": "644880",
    "end": "647360"
  },
  {
    "text": "um",
    "start": "647360",
    "end": "649680"
  },
  {
    "text": "so",
    "start": "651920",
    "end": "652959"
  },
  {
    "text": "the kinetic auto scaler",
    "start": "652959",
    "end": "654959"
  },
  {
    "text": "makes the auto scaling decisions based",
    "start": "654959",
    "end": "656640"
  },
  {
    "text": "on the uh",
    "start": "656640",
    "end": "657760"
  },
  {
    "text": "your",
    "start": "657760",
    "end": "658720"
  },
  {
    "text": "concurrency target and the observed",
    "start": "658720",
    "end": "661360"
  },
  {
    "text": "observed concurrency matrix so let's say",
    "start": "661360",
    "end": "663920"
  },
  {
    "text": "if your target concurrency is one which",
    "start": "663920",
    "end": "665760"
  },
  {
    "text": "means like each part can only process",
    "start": "665760",
    "end": "667680"
  },
  {
    "text": "one request at a time and uh",
    "start": "667680",
    "end": "670480"
  },
  {
    "text": "if you are",
    "start": "670480",
    "end": "671920"
  },
  {
    "text": "if you are getting like a five requests",
    "start": "671920",
    "end": "674160"
  },
  {
    "text": "average average uh concurrency request",
    "start": "674160",
    "end": "676800"
  },
  {
    "text": "in your part then auto scale will",
    "start": "676800",
    "end": "678880"
  },
  {
    "text": "automatically scale up to five parts to",
    "start": "678880",
    "end": "681200"
  },
  {
    "text": "handle the your current traffic",
    "start": "681200",
    "end": "684800"
  },
  {
    "text": "so uh comparing to the kubernetes hpa so",
    "start": "686160",
    "end": "689600"
  },
  {
    "text": "um candidate for auto scalers processor",
    "start": "689600",
    "end": "692160"
  },
  {
    "text": "addition edition to the cpu memory",
    "start": "692160",
    "end": "694480"
  },
  {
    "text": "matrix it also supports the concurrency",
    "start": "694480",
    "end": "696240"
  },
  {
    "text": "in an rps metrics so it can auto scale",
    "start": "696240",
    "end": "699120"
  },
  {
    "text": "based on your request load",
    "start": "699120",
    "end": "701680"
  },
  {
    "text": "it it also can support a scale down to",
    "start": "701680",
    "end": "704399"
  },
  {
    "text": "and front zero while the uh kubernetes",
    "start": "704399",
    "end": "706800"
  },
  {
    "text": "hpl can only scale down to one",
    "start": "706800",
    "end": "709040"
  },
  {
    "text": "um internal symmetric scraping uh",
    "start": "709040",
    "end": "712160"
  },
  {
    "text": "activator and q process actually push",
    "start": "712160",
    "end": "714000"
  },
  {
    "text": "the metrics",
    "start": "714000",
    "end": "715360"
  },
  {
    "text": "to autoscaler so via websocket so",
    "start": "715360",
    "end": "719200"
  },
  {
    "text": "oftentimes we can react faster than",
    "start": "719200",
    "end": "721519"
  },
  {
    "text": "the the kubernetes uh",
    "start": "721519",
    "end": "724160"
  },
  {
    "text": "hpa which queries has to credit the",
    "start": "724160",
    "end": "727120"
  },
  {
    "text": "metrics from promises",
    "start": "727120",
    "end": "729920"
  },
  {
    "text": "and",
    "start": "729920",
    "end": "730880"
  },
  {
    "text": "um",
    "start": "730880",
    "end": "732639"
  },
  {
    "text": "canadian by default calculates the",
    "start": "732639",
    "end": "735040"
  },
  {
    "text": "average concurrency in a 60-second",
    "start": "735040",
    "end": "737040"
  },
  {
    "text": "window",
    "start": "737040",
    "end": "737920"
  },
  {
    "text": "it also enables a panic window which is",
    "start": "737920",
    "end": "741040"
  },
  {
    "text": "a six second which can react",
    "start": "741040",
    "end": "744000"
  },
  {
    "text": "faster when you are receiving burst of",
    "start": "744000",
    "end": "746000"
  },
  {
    "text": "traffic",
    "start": "746000",
    "end": "747440"
  },
  {
    "text": "why the hpa used a stable five minutes",
    "start": "747440",
    "end": "750000"
  },
  {
    "text": "window so sometimes it may not be able",
    "start": "750000",
    "end": "752079"
  },
  {
    "text": "to handle the uh the large breast of the",
    "start": "752079",
    "end": "754160"
  },
  {
    "text": "quest",
    "start": "754160",
    "end": "756399"
  },
  {
    "text": "uh so next important feature",
    "start": "757440",
    "end": "759839"
  },
  {
    "text": "we are implementing in case of is the uh",
    "start": "759839",
    "end": "763360"
  },
  {
    "text": "the model layout",
    "start": "763360",
    "end": "764800"
  },
  {
    "text": "so um",
    "start": "764800",
    "end": "767120"
  },
  {
    "text": "often time like when you wrote a new",
    "start": "767120",
    "end": "768800"
  },
  {
    "text": "model you need to to validate the model",
    "start": "768800",
    "end": "771920"
  },
  {
    "text": "if the model is performed actually with",
    "start": "771920",
    "end": "774079"
  },
  {
    "text": "the",
    "start": "774079",
    "end": "776079"
  },
  {
    "text": "you know accuracy and uh before moving",
    "start": "776079",
    "end": "778720"
  },
  {
    "text": "the traffic from the old model to the",
    "start": "778720",
    "end": "780320"
  },
  {
    "text": "new model",
    "start": "780320",
    "end": "781519"
  },
  {
    "text": "so we found that a canadian kubernetes",
    "start": "781519",
    "end": "784000"
  },
  {
    "text": "deployment is often limited by uh it's",
    "start": "784000",
    "end": "786480"
  },
  {
    "text": "inability to stage the traffic",
    "start": "786480",
    "end": "788800"
  },
  {
    "text": "um so um canadian so case of um",
    "start": "788800",
    "end": "793360"
  },
  {
    "text": "actually implements uh opinionated like",
    "start": "793360",
    "end": "795120"
  },
  {
    "text": "a two-way brokering and a canary route",
    "start": "795120",
    "end": "798320"
  },
  {
    "text": "based on the native uh revision",
    "start": "798320",
    "end": "800320"
  },
  {
    "text": "implementation so every time like when",
    "start": "800320",
    "end": "802399"
  },
  {
    "text": "you update info service it generates a",
    "start": "802399",
    "end": "804320"
  },
  {
    "text": "new revision and uh case of actually uh",
    "start": "804320",
    "end": "807040"
  },
  {
    "text": "automatically tracks last uh known good",
    "start": "807040",
    "end": "810000"
  },
  {
    "text": "revision by automatically tagging the",
    "start": "810000",
    "end": "812160"
  },
  {
    "text": "revisions that was rolled out a hundred",
    "start": "812160",
    "end": "813600"
  },
  {
    "text": "percent of traffic",
    "start": "813600",
    "end": "816480"
  },
  {
    "text": "yeah some of the limitations of default",
    "start": "818399",
    "end": "821120"
  },
  {
    "text": "kubernetes rolling upgrade uh",
    "start": "821120",
    "end": "823199"
  },
  {
    "text": "um it has very little control over the",
    "start": "823199",
    "end": "826000"
  },
  {
    "text": "speed of the road",
    "start": "826000",
    "end": "828480"
  },
  {
    "text": "and",
    "start": "828480",
    "end": "830079"
  },
  {
    "text": "it's enabled to control traffic flow to",
    "start": "830079",
    "end": "832800"
  },
  {
    "text": "how it flows to the new revision",
    "start": "832800",
    "end": "835600"
  },
  {
    "text": "and the readiness probe often are not",
    "start": "835600",
    "end": "838079"
  },
  {
    "text": "suitable for",
    "start": "838079",
    "end": "839680"
  },
  {
    "text": "validating models",
    "start": "839680",
    "end": "841360"
  },
  {
    "text": "and doing like deeper and stress tests",
    "start": "841360",
    "end": "844959"
  },
  {
    "text": "and",
    "start": "844959",
    "end": "845839"
  },
  {
    "text": "also it's not able to check the external",
    "start": "845839",
    "end": "847760"
  },
  {
    "text": "metrics to verify the model updates",
    "start": "847760",
    "end": "851760"
  },
  {
    "text": "and",
    "start": "851760",
    "end": "852800"
  },
  {
    "text": "rolling upgrade can hold the",
    "start": "852800",
    "end": "854959"
  },
  {
    "text": "route if something goes wrong but",
    "start": "854959",
    "end": "857839"
  },
  {
    "text": "it's not able to roll back automatically",
    "start": "857839",
    "end": "862160"
  },
  {
    "text": "so we wrote a new model i",
    "start": "863040",
    "end": "865839"
  },
  {
    "text": "actually want to stage the traffic on",
    "start": "865839",
    "end": "868240"
  },
  {
    "text": "the stable version before",
    "start": "868240",
    "end": "871040"
  },
  {
    "text": "i want the new version to be running",
    "start": "871040",
    "end": "873600"
  },
  {
    "text": "so i will where i i can like verify and",
    "start": "873600",
    "end": "876480"
  },
  {
    "text": "evaluate the model so in this case i can",
    "start": "876480",
    "end": "879519"
  },
  {
    "text": "add a canary traffic percent",
    "start": "879519",
    "end": "881920"
  },
  {
    "text": "field on the influence service yammer so",
    "start": "881920",
    "end": "884639"
  },
  {
    "text": "to zero",
    "start": "884639",
    "end": "885920"
  },
  {
    "text": "so in that case the traffic is uh still",
    "start": "885920",
    "end": "888399"
  },
  {
    "text": "it will spin up a new a new version but",
    "start": "888399",
    "end": "890560"
  },
  {
    "text": "the child will not receive the live",
    "start": "890560",
    "end": "892399"
  },
  {
    "text": "traffic",
    "start": "892399",
    "end": "894079"
  },
  {
    "text": "so here's how to",
    "start": "894079",
    "end": "895839"
  },
  {
    "text": "look so you initially have model",
    "start": "895839",
    "end": "899279"
  },
  {
    "text": "one and which is creates the kinetic",
    "start": "899279",
    "end": "901199"
  },
  {
    "text": "revision zero zero one uh and you get a",
    "start": "901199",
    "end": "904399"
  },
  {
    "text": "endpoint which is you can",
    "start": "904399",
    "end": "906399"
  },
  {
    "text": "can process the live traffic",
    "start": "906399",
    "end": "909199"
  },
  {
    "text": "so now you wrote your model to which is",
    "start": "909199",
    "end": "911519"
  },
  {
    "text": "the revision002",
    "start": "911519",
    "end": "913760"
  },
  {
    "text": "because we set the canary traffic",
    "start": "913760",
    "end": "915519"
  },
  {
    "text": "percent to zero so uh it doesn't",
    "start": "915519",
    "end": "917839"
  },
  {
    "text": "actually receive the live track but on",
    "start": "917839",
    "end": "920000"
  },
  {
    "text": "the end hand like it generates a",
    "start": "920000",
    "end": "922160"
  },
  {
    "text": "endpoint which was tagged with the",
    "start": "922160",
    "end": "923920"
  },
  {
    "text": "latest so you can use the latest",
    "start": "923920",
    "end": "927120"
  },
  {
    "text": "general attack url to test your model",
    "start": "927120",
    "end": "931040"
  },
  {
    "text": "so once you're happy with the models",
    "start": "931040",
    "end": "932639"
  },
  {
    "text": "then",
    "start": "932639",
    "end": "933519"
  },
  {
    "text": "you can bump the canary traffic canary",
    "start": "933519",
    "end": "937120"
  },
  {
    "text": "traffic percent to 100 then they will",
    "start": "937120",
    "end": "940959"
  },
  {
    "text": "move the tractor from the old model to",
    "start": "940959",
    "end": "942800"
  },
  {
    "text": "the new model",
    "start": "942800",
    "end": "945440"
  },
  {
    "text": "so",
    "start": "951199",
    "end": "953040"
  },
  {
    "text": "after you valid models there could could",
    "start": "953040",
    "end": "954959"
  },
  {
    "text": "be still something goes wrong so in case",
    "start": "954959",
    "end": "956639"
  },
  {
    "text": "you want to roll back to the",
    "start": "956639",
    "end": "958720"
  },
  {
    "text": "last model so can it so k server",
    "start": "958720",
    "end": "960959"
  },
  {
    "text": "actually automatically tracks the uh",
    "start": "960959",
    "end": "963279"
  },
  {
    "text": "previously loadout revision in the",
    "start": "963279",
    "end": "964959"
  },
  {
    "text": "infrared service status so it knows",
    "start": "964959",
    "end": "967199"
  },
  {
    "text": "which uh what was the revision uh it it",
    "start": "967199",
    "end": "970160"
  },
  {
    "text": "needs to roll back to",
    "start": "970160",
    "end": "971600"
  },
  {
    "text": "um so a user can simply set the setback",
    "start": "971600",
    "end": "974639"
  },
  {
    "text": "the canary traffic percent to zero",
    "start": "974639",
    "end": "976880"
  },
  {
    "text": "um so in this case the the model uh",
    "start": "976880",
    "end": "979680"
  },
  {
    "text": "traffic will automatically uh go back to",
    "start": "979680",
    "end": "981920"
  },
  {
    "text": "the previous version which was tagged as",
    "start": "981920",
    "end": "984480"
  },
  {
    "text": "previous so you can",
    "start": "984480",
    "end": "987759"
  },
  {
    "text": "now the traffic will get",
    "start": "988880",
    "end": "990720"
  },
  {
    "text": "rolled back to the previous version",
    "start": "990720",
    "end": "992320"
  },
  {
    "text": "automatically",
    "start": "992320",
    "end": "994959"
  },
  {
    "text": "um so this is for uh canon cri fans so",
    "start": "995519",
    "end": "998800"
  },
  {
    "text": "this is like uh equivalent commands we",
    "start": "998800",
    "end": "1001040"
  },
  {
    "text": "need to uh execute like to simulate to",
    "start": "1001040",
    "end": "1003920"
  },
  {
    "text": "this uh process kso basically um",
    "start": "1003920",
    "end": "1006639"
  },
  {
    "text": "automate the sum of steps by",
    "start": "1006639",
    "end": "1008160"
  },
  {
    "text": "automatically tagging the technical",
    "start": "1008160",
    "end": "1010160"
  },
  {
    "text": "revisions and",
    "start": "1010160",
    "end": "1013120"
  },
  {
    "text": "and also like track the track the last",
    "start": "1013360",
    "end": "1015680"
  },
  {
    "text": "known stable version uh automatically",
    "start": "1015680",
    "end": "1018000"
  },
  {
    "text": "for the user",
    "start": "1018000",
    "end": "1019120"
  },
  {
    "text": "um",
    "start": "1019120",
    "end": "1021040"
  },
  {
    "text": "but yeah this is equivalent commands you",
    "start": "1021040",
    "end": "1022880"
  },
  {
    "text": "can use to",
    "start": "1022880",
    "end": "1026000"
  },
  {
    "text": "run the exactly the same route workflow",
    "start": "1026000",
    "end": "1029839"
  },
  {
    "text": "so now i'm going to execute the demo",
    "start": "1030480",
    "end": "1033678"
  },
  {
    "text": "to automate all the",
    "start": "1033679",
    "end": "1036079"
  },
  {
    "text": "rollout steps i just showed so i",
    "start": "1036079",
    "end": "1039280"
  },
  {
    "text": "implemented with using the argo workflow",
    "start": "1039280",
    "end": "1041918"
  },
  {
    "text": "to execute these steps",
    "start": "1041919",
    "end": "1045880"
  },
  {
    "text": "so uh",
    "start": "1052960",
    "end": "1054720"
  },
  {
    "text": "so first step is to um create a new",
    "start": "1054720",
    "end": "1057840"
  },
  {
    "text": "model",
    "start": "1057840",
    "end": "1058880"
  },
  {
    "text": "um",
    "start": "1058880",
    "end": "1059760"
  },
  {
    "text": "with version 001",
    "start": "1059760",
    "end": "1062400"
  },
  {
    "text": "um and then and",
    "start": "1062400",
    "end": "1064880"
  },
  {
    "text": "and then next second step is to uh",
    "start": "1064880",
    "end": "1069360"
  },
  {
    "text": "after the after the first model is",
    "start": "1069360",
    "end": "1071360"
  },
  {
    "text": "really in the ready status uh it will",
    "start": "1071360",
    "end": "1074240"
  },
  {
    "text": "update the model uh starter ui to update",
    "start": "1074240",
    "end": "1077120"
  },
  {
    "text": "to the uh and to draw the model version",
    "start": "1077120",
    "end": "1079679"
  },
  {
    "text": "two",
    "start": "1079679",
    "end": "1080880"
  },
  {
    "text": "so now you can see that the first model",
    "start": "1080880",
    "end": "1082880"
  },
  {
    "text": "is ready",
    "start": "1082880",
    "end": "1084160"
  },
  {
    "text": "and now it's running the second step to",
    "start": "1084160",
    "end": "1086400"
  },
  {
    "text": "update the model",
    "start": "1086400",
    "end": "1087919"
  },
  {
    "text": "but uh i",
    "start": "1087919",
    "end": "1089360"
  },
  {
    "text": "i want to stage a traffic is still on",
    "start": "1089360",
    "end": "1091200"
  },
  {
    "text": "the old model so the new model doesn't",
    "start": "1091200",
    "end": "1093280"
  },
  {
    "text": "receive any live traffic",
    "start": "1093280",
    "end": "1096400"
  },
  {
    "text": "so you can see that the new model gets",
    "start": "1096400",
    "end": "1098160"
  },
  {
    "text": "the zero percent traffic and the all the",
    "start": "1098160",
    "end": "1100960"
  },
  {
    "text": "live target is still processed by the",
    "start": "1100960",
    "end": "1102400"
  },
  {
    "text": "old model",
    "start": "1102400",
    "end": "1103520"
  },
  {
    "text": "and the third step is actually uh",
    "start": "1103520",
    "end": "1106720"
  },
  {
    "text": "surround a model of addition job so",
    "start": "1106720",
    "end": "1109200"
  },
  {
    "text": "right now it's a simple curve just to",
    "start": "1109200",
    "end": "1110640"
  },
  {
    "text": "verify the request uh is uh i get the",
    "start": "1110640",
    "end": "1113520"
  },
  {
    "text": "expected response but you can also plug",
    "start": "1113520",
    "end": "1115679"
  },
  {
    "text": "in your like own jobs to do more like",
    "start": "1115679",
    "end": "1117919"
  },
  {
    "text": "advanced like kind of uh around a batch",
    "start": "1117919",
    "end": "1120960"
  },
  {
    "text": "request from a golden data set and",
    "start": "1120960",
    "end": "1123840"
  },
  {
    "text": "verify all the models produce the",
    "start": "1123840",
    "end": "1125520"
  },
  {
    "text": "accurate result or like around the",
    "start": "1125520",
    "end": "1128000"
  },
  {
    "text": "stress testing to make sure the latency",
    "start": "1128000",
    "end": "1131200"
  },
  {
    "text": "meets your requirements",
    "start": "1131200",
    "end": "1134600"
  },
  {
    "text": "so yeah so the once uh once the test job",
    "start": "1136080",
    "end": "1140720"
  },
  {
    "text": "is successful and then we automatically",
    "start": "1140720",
    "end": "1143440"
  },
  {
    "text": "bump the traffic percent move the",
    "start": "1143440",
    "end": "1145600"
  },
  {
    "text": "traffic from the old model to the new",
    "start": "1145600",
    "end": "1147440"
  },
  {
    "text": "model automatically",
    "start": "1147440",
    "end": "1149200"
  },
  {
    "text": "so in this way we can like uh uh",
    "start": "1149200",
    "end": "1151919"
  },
  {
    "text": "in the mr use case we can like uh uh",
    "start": "1151919",
    "end": "1154320"
  },
  {
    "text": "implement a continuous deployment",
    "start": "1154320",
    "end": "1155840"
  },
  {
    "text": "workflow where like without humans loop",
    "start": "1155840",
    "end": "1158320"
  },
  {
    "text": "so every time they've update model and",
    "start": "1158320",
    "end": "1160080"
  },
  {
    "text": "then they will run the model testing",
    "start": "1160080",
    "end": "1162000"
  },
  {
    "text": "model validation drop and everything's",
    "start": "1162000",
    "end": "1164240"
  },
  {
    "text": "successful then it'll automatically",
    "start": "1164240",
    "end": "1167360"
  },
  {
    "text": "reload the new model to production",
    "start": "1167360",
    "end": "1171039"
  },
  {
    "text": "yeah so",
    "start": "1175039",
    "end": "1176240"
  },
  {
    "text": "now you can see the traffic that all 100",
    "start": "1176240",
    "end": "1178880"
  },
  {
    "text": "traffic is moved to the new model",
    "start": "1178880",
    "end": "1182240"
  },
  {
    "text": "uh so another another requirement from k",
    "start": "1186799",
    "end": "1189520"
  },
  {
    "text": "service that we actually",
    "start": "1189520",
    "end": "1192559"
  },
  {
    "text": "in addition to the uh running inference",
    "start": "1192559",
    "end": "1194720"
  },
  {
    "text": "we also need to monitor the model to",
    "start": "1194720",
    "end": "1196480"
  },
  {
    "text": "make sure it produce the uh reliable uh",
    "start": "1196480",
    "end": "1199200"
  },
  {
    "text": "reliable uh predictions so this request",
    "start": "1199200",
    "end": "1202880"
  },
  {
    "text": "actually requests a event driven like a",
    "start": "1202880",
    "end": "1205520"
  },
  {
    "text": "architecture where you",
    "start": "1205520",
    "end": "1207200"
  },
  {
    "text": "need to for capture the original",
    "start": "1207200",
    "end": "1209280"
  },
  {
    "text": "influence requests and therefore to",
    "start": "1209280",
    "end": "1210960"
  },
  {
    "text": "accept",
    "start": "1210960",
    "end": "1212080"
  },
  {
    "text": "a set of model monitoring components",
    "start": "1212080",
    "end": "1214640"
  },
  {
    "text": "such as the outlier concept drift uh and",
    "start": "1214640",
    "end": "1218000"
  },
  {
    "text": "adversarial like detectors so um canada",
    "start": "1218000",
    "end": "1221520"
  },
  {
    "text": "eventing",
    "start": "1221520",
    "end": "1222640"
  },
  {
    "text": "provides uh composible primitives to",
    "start": "1222640",
    "end": "1225360"
  },
  {
    "text": "enable lay banning for",
    "start": "1225360",
    "end": "1227360"
  },
  {
    "text": "for event producers and consumers and i",
    "start": "1227360",
    "end": "1230000"
  },
  {
    "text": "also use the cloud event to standardize",
    "start": "1230000",
    "end": "1232320"
  },
  {
    "text": "the",
    "start": "1232320",
    "end": "1233120"
  },
  {
    "text": "passing the event data",
    "start": "1233120",
    "end": "1236158"
  },
  {
    "text": "so",
    "start": "1242559",
    "end": "1244720"
  },
  {
    "text": "so k-serve has a model agent",
    "start": "1244720",
    "end": "1247440"
  },
  {
    "text": "sidecar which like",
    "start": "1247440",
    "end": "1248880"
  },
  {
    "text": "intercepts the request and then forwards",
    "start": "1248880",
    "end": "1250960"
  },
  {
    "text": "the request to the canadian broker so",
    "start": "1250960",
    "end": "1254080"
  },
  {
    "text": "candidate bro parker",
    "start": "1254080",
    "end": "1256320"
  },
  {
    "text": "starts the events in a durable array and",
    "start": "1256320",
    "end": "1259520"
  },
  {
    "text": "and the you can have a set of consumers",
    "start": "1259520",
    "end": "1262000"
  },
  {
    "text": "which you can subscribe to the brokers",
    "start": "1262000",
    "end": "1264080"
  },
  {
    "text": "based on the filter certain filter uh of",
    "start": "1264080",
    "end": "1266400"
  },
  {
    "text": "the events it is interesting so here is",
    "start": "1266400",
    "end": "1268960"
  },
  {
    "text": "a we run a set of like a model",
    "start": "1268960",
    "end": "1271520"
  },
  {
    "text": "monitoring components to uh to run",
    "start": "1271520",
    "end": "1273600"
  },
  {
    "text": "analytics on the",
    "start": "1273600",
    "end": "1275200"
  },
  {
    "text": "on the inference request and then you",
    "start": "1275200",
    "end": "1277120"
  },
  {
    "text": "can also use to generate alerts if there",
    "start": "1277120",
    "end": "1278960"
  },
  {
    "text": "anything uh outlier or like uh all the",
    "start": "1278960",
    "end": "1282080"
  },
  {
    "text": "data is drifted you can generate a lot",
    "start": "1282080",
    "end": "1284080"
  },
  {
    "text": "based on that",
    "start": "1284080",
    "end": "1286640"
  },
  {
    "text": "so",
    "start": "1287679",
    "end": "1288480"
  },
  {
    "text": "another requirement from case service",
    "start": "1288480",
    "end": "1290240"
  },
  {
    "text": "that",
    "start": "1290240",
    "end": "1291200"
  },
  {
    "text": "we just talked about is like we want the",
    "start": "1291200",
    "end": "1292960"
  },
  {
    "text": "inference graph which",
    "start": "1292960",
    "end": "1294400"
  },
  {
    "text": "wants to change multiple requests much",
    "start": "1294400",
    "end": "1296159"
  },
  {
    "text": "for influence service to get back the",
    "start": "1296159",
    "end": "1298640"
  },
  {
    "text": "single response or sometimes we also",
    "start": "1298640",
    "end": "1300720"
  },
  {
    "text": "need to combine the outputs from",
    "start": "1300720",
    "end": "1302799"
  },
  {
    "text": "multiple service influence services to",
    "start": "1302799",
    "end": "1305280"
  },
  {
    "text": "for the model ensemble use case",
    "start": "1305280",
    "end": "1308000"
  },
  {
    "text": "so q native does provide the sequence",
    "start": "1308000",
    "end": "1310159"
  },
  {
    "text": "and parallel crd but that is mainly",
    "start": "1310159",
    "end": "1312880"
  },
  {
    "text": "designed for async eventing",
    "start": "1312880",
    "end": "1315039"
  },
  {
    "text": "but here we more like want a request",
    "start": "1315039",
    "end": "1317120"
  },
  {
    "text": "response style so we decided to uh",
    "start": "1317120",
    "end": "1319840"
  },
  {
    "text": "implement our own like influence graph",
    "start": "1319840",
    "end": "1321919"
  },
  {
    "text": "crd",
    "start": "1321919",
    "end": "1323360"
  },
  {
    "text": "which creates a uh implements a graph",
    "start": "1323360",
    "end": "1325679"
  },
  {
    "text": "orchestrator to change the request and",
    "start": "1325679",
    "end": "1328240"
  },
  {
    "text": "merge response from like a multiple",
    "start": "1328240",
    "end": "1329919"
  },
  {
    "text": "infinite service real time on the path",
    "start": "1329919",
    "end": "1332240"
  },
  {
    "text": "to deliver back the final response",
    "start": "1332240",
    "end": "1336158"
  },
  {
    "text": "um so on the influence graph you can",
    "start": "1336320",
    "end": "1338400"
  },
  {
    "text": "have a different type of nodes like a",
    "start": "1338400",
    "end": "1340640"
  },
  {
    "text": "single node a single service or like you",
    "start": "1340640",
    "end": "1342480"
  },
  {
    "text": "have a",
    "start": "1342480",
    "end": "1343520"
  },
  {
    "text": "multiple service based on conditions uh",
    "start": "1343520",
    "end": "1346400"
  },
  {
    "text": "based on all weights and you can also",
    "start": "1346400",
    "end": "1348480"
  },
  {
    "text": "like run inference services in parallel",
    "start": "1348480",
    "end": "1350480"
  },
  {
    "text": "and then merge the response at the end",
    "start": "1350480",
    "end": "1352559"
  },
  {
    "text": "and all these different nodes can be",
    "start": "1352559",
    "end": "1354320"
  },
  {
    "text": "chained together so it's very flexible",
    "start": "1354320",
    "end": "1356320"
  },
  {
    "text": "and",
    "start": "1356320",
    "end": "1358559"
  },
  {
    "text": "uh involves like you can compose pretty",
    "start": "1359280",
    "end": "1361120"
  },
  {
    "text": "much compose any arbitrary like",
    "start": "1361120",
    "end": "1362559"
  },
  {
    "text": "influence graph",
    "start": "1362559",
    "end": "1363760"
  },
  {
    "text": "um based on the city line and um",
    "start": "1363760",
    "end": "1367039"
  },
  {
    "text": "um i'm happy to like uh uh",
    "start": "1367039",
    "end": "1370000"
  },
  {
    "text": "discuss this like uh with a canadian",
    "start": "1370000",
    "end": "1372000"
  },
  {
    "text": "continuity if something like can be",
    "start": "1372000",
    "end": "1373919"
  },
  {
    "text": "useful to the",
    "start": "1373919",
    "end": "1375520"
  },
  {
    "text": "uh can be all more generic design can be",
    "start": "1375520",
    "end": "1377840"
  },
  {
    "text": "contributed to the canadian upstream",
    "start": "1377840",
    "end": "1380720"
  },
  {
    "text": "um",
    "start": "1380720",
    "end": "1383039"
  },
  {
    "text": "yeah that's all i have today and uh",
    "start": "1383840",
    "end": "1386320"
  },
  {
    "text": "both k server and the canadian kind of",
    "start": "1386320",
    "end": "1388640"
  },
  {
    "text": "community are",
    "start": "1388640",
    "end": "1389840"
  },
  {
    "text": "green the community so i think if we",
    "start": "1389840",
    "end": "1391919"
  },
  {
    "text": "combine two communities it will be",
    "start": "1391919",
    "end": "1393120"
  },
  {
    "text": "really powerful and then we can looking",
    "start": "1393120",
    "end": "1395200"
  },
  {
    "text": "forward to collaborating more with",
    "start": "1395200",
    "end": "1396799"
  },
  {
    "text": "canadian community to",
    "start": "1396799",
    "end": "1398880"
  },
  {
    "text": "um to approach the canada to next level",
    "start": "1398880",
    "end": "1401600"
  },
  {
    "text": "and",
    "start": "1401600",
    "end": "1402799"
  },
  {
    "text": "hopefully we can like",
    "start": "1402799",
    "end": "1404480"
  },
  {
    "text": "get a lot of more exciting features",
    "start": "1404480",
    "end": "1405919"
  },
  {
    "text": "there",
    "start": "1405919",
    "end": "1408158"
  },
  {
    "text": "yeah",
    "start": "1409200",
    "end": "1410320"
  },
  {
    "text": "happy to take any questions",
    "start": "1410320",
    "end": "1413679"
  },
  {
    "text": "anyone has questions",
    "start": "1414720",
    "end": "1418159"
  },
  {
    "text": "for dan okay",
    "start": "1418159",
    "end": "1421039"
  },
  {
    "text": "so uh you talk about the autoscaler i'm",
    "start": "1429600",
    "end": "1432240"
  },
  {
    "text": "curious if you needed to",
    "start": "1432240",
    "end": "1434480"
  },
  {
    "text": "customize somehow or if you're good with",
    "start": "1434480",
    "end": "1436720"
  },
  {
    "text": "the defaults and if you are hitting by",
    "start": "1436720",
    "end": "1439200"
  },
  {
    "text": "any chance the the panic window often",
    "start": "1439200",
    "end": "1443279"
  },
  {
    "text": "uh so the question is is there any uh",
    "start": "1443279",
    "end": "1446400"
  },
  {
    "text": "case where we need to tune the panic",
    "start": "1446400",
    "end": "1448640"
  },
  {
    "text": "window right",
    "start": "1448640",
    "end": "1450720"
  },
  {
    "text": "um if you hit the panic window and also",
    "start": "1450720",
    "end": "1453039"
  },
  {
    "text": "if you customize if you i'm assuming",
    "start": "1453039",
    "end": "1455360"
  },
  {
    "text": "you're using the concurrency settings",
    "start": "1455360",
    "end": "1457679"
  },
  {
    "text": "yeah yeah",
    "start": "1457679",
    "end": "1458799"
  },
  {
    "text": "yeah then i don't know because you put",
    "start": "1458799",
    "end": "1460799"
  },
  {
    "text": "there the defaults the 60 seconds and",
    "start": "1460799",
    "end": "1462720"
  },
  {
    "text": "six seconds for the panic window so i",
    "start": "1462720",
    "end": "1464799"
  },
  {
    "text": "don't know if you are customizing that",
    "start": "1464799",
    "end": "1466880"
  },
  {
    "text": "you are fine tuning somehow we are",
    "start": "1466880",
    "end": "1468640"
  },
  {
    "text": "mostly using the defaults we are more",
    "start": "1468640",
    "end": "1470799"
  },
  {
    "text": "like uh based on cases we need to tune",
    "start": "1470799",
    "end": "1473120"
  },
  {
    "text": "the uh target concurrency or the other",
    "start": "1473120",
    "end": "1475440"
  },
  {
    "text": "continued concurrency fields",
    "start": "1475440",
    "end": "1477360"
  },
  {
    "text": "um so um",
    "start": "1477360",
    "end": "1479919"
  },
  {
    "text": "so yeah we i think the defaults works",
    "start": "1479919",
    "end": "1483039"
  },
  {
    "text": "um okay i think that we just need to",
    "start": "1483039",
    "end": "1485679"
  },
  {
    "text": "tune those like concurrency fields",
    "start": "1485679",
    "end": "1487840"
  },
  {
    "text": "based on your based on different",
    "start": "1487840",
    "end": "1489360"
  },
  {
    "text": "applications",
    "start": "1489360",
    "end": "1491919"
  },
  {
    "text": "call any more questions",
    "start": "1496240",
    "end": "1500360"
  },
  {
    "text": "anyone",
    "start": "1501039",
    "end": "1502240"
  },
  {
    "text": "okay",
    "start": "1502240",
    "end": "1503360"
  },
  {
    "text": "awesome thanks dan",
    "start": "1503360",
    "end": "1505279"
  },
  {
    "text": "thanks thanks everyone",
    "start": "1505279",
    "end": "1508919"
  }
]