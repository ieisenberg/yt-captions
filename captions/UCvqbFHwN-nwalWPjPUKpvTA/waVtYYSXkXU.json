[
  {
    "start": "0",
    "end": "29000"
  },
  {
    "text": "okay cool so let's get started so this talk is about crackin a p2p talk",
    "start": "30",
    "end": "5910"
  },
  {
    "text": "originally developed at uber and how we got there so my name is Iran and this is a Cody we",
    "start": "5910",
    "end": "16170"
  },
  {
    "text": "have nano team member could Evelyn but she couldn't make it here because of visa issues so this is a current cracking team at Auburn of course there",
    "start": "16170",
    "end": "23490"
  },
  {
    "text": "are other contributors who contributed to this project and we wish there are more future contributors so the agenda",
    "start": "23490",
    "end": "31199"
  },
  {
    "start": "29000",
    "end": "62000"
  },
  {
    "text": "today is so first we'll spend a couple minutes explain how our image and the",
    "start": "31199",
    "end": "36270"
  },
  {
    "text": "dr. Terry works because I'm not sure if everyone here are very familiar with their internals then we'll talk about a",
    "start": "36270",
    "end": "42719"
  },
  {
    "text": "few common ways you can use to speed up talk her poo if that's bothering you retinol then we'll talk about p2p",
    "start": "42719",
    "end": "49620"
  },
  {
    "text": "solutions we exported before Kraken and then we talked about how we are how were",
    "start": "49620",
    "end": "55320"
  },
  {
    "text": "you working to solve the all the territory related issues and we'll have a few minutes in the end for questions",
    "start": "55320",
    "end": "61760"
  },
  {
    "text": "so to begin with let's let me explain how docker image works so the cream is",
    "start": "61760",
    "end": "67350"
  },
  {
    "start": "62000",
    "end": "111000"
  },
  {
    "text": "actually a very simple packaging format it's mostly composed of fuel tar files so each docker image layer is a very",
    "start": "67350",
    "end": "73350"
  },
  {
    "text": "regular tar as compressed there's one special layer called the image config",
    "start": "73350",
    "end": "78420"
  },
  {
    "text": "that contains like a murmur manoeuvrable use our entry point those metadata so",
    "start": "78420",
    "end": "83549"
  },
  {
    "text": "that we may trans or contains image manifest is another regular tree sound file that contains the list of all the",
    "start": "83549",
    "end": "90060"
  },
  {
    "text": "layers the last thing is a docker image tag so we had to talk her pool open to",
    "start": "90060",
    "end": "95130"
  },
  {
    "text": "latest that want to latest string is a tag so the tag to manifest mapping study",
    "start": "95130",
    "end": "100770"
  },
  {
    "text": "in docker registry so again so very simple formats basically apart or tars you can create a limit using like shell",
    "start": "100770",
    "end": "107250"
  },
  {
    "text": "script personwho - neither can alert you any tenderizer TOC registry is also a very",
    "start": "107250",
    "end": "113970"
  },
  {
    "text": "simple piece of technology used to distribute docker images expose expose",
    "start": "113970",
    "end": "119159"
  },
  {
    "text": "some very common-sense REST API s so I can use them to cat attack cat manifest",
    "start": "119159",
    "end": "124649"
  },
  {
    "text": "and cat blob that's it you can easily implement one in like a couple weeks",
    "start": "124649",
    "end": "130399"
  },
  {
    "start": "130000",
    "end": "154000"
  },
  {
    "text": "so knowing what his docker image and was at a registry and talk about how her pool works against very simple so at the",
    "start": "130539",
    "end": "137980"
  },
  {
    "text": "beginning of docker pool we talked her will resolve attack to manifest they put down the manifest take",
    "start": "137980",
    "end": "144069"
  },
  {
    "text": "a look at the list of image layer shots and then put down each image layer",
    "start": "144069",
    "end": "149170"
  },
  {
    "text": "separately in the end it decompress all the layers so given that talk abou is",
    "start": "149170",
    "end": "156760"
  },
  {
    "start": "154000",
    "end": "215000"
  },
  {
    "text": "basically a bunch of HTTP requests that put down if they're individual is very",
    "start": "156760",
    "end": "162370"
  },
  {
    "text": "simple to do a speed up dr. Poole so so the first thing you should do is",
    "start": "162370",
    "end": "167590"
  },
  {
    "text": "actually try to optimize your docker image to make it a more homogeneous but I mean usually looking to have a common",
    "start": "167590",
    "end": "174220"
  },
  {
    "text": "image for if you have a lot of images comment you should have a common base image and a looper we also have a docker",
    "start": "174220",
    "end": "181299"
  },
  {
    "text": "file template it's almost there out developers don't write their own docker file so by doing this all the docker",
    "start": "181299",
    "end": "186639"
  },
  {
    "text": "file look very similar they have a share of your common common layers we also to multistage peeled so the end result",
    "start": "186639",
    "end": "194019"
  },
  {
    "text": "imagery is very small in the end so recently there also a lot of build suicide supported distributed air cash",
    "start": "194019",
    "end": "200349"
  },
  {
    "text": "for in-app amake tsuzuku that over can come from Google built kit from docker there's a talk going on right about all",
    "start": "200349",
    "end": "206560"
  },
  {
    "text": "these tools so yeah they we also make the teeth between images smaller so if",
    "start": "206560",
    "end": "212709"
  },
  {
    "text": "you look into those so after you have done optimizing your docker images if",
    "start": "212709",
    "end": "219579"
  },
  {
    "text": "used to have issue with docker pools and you should start looking at how to optimize your hereditary so the first",
    "start": "219579",
    "end": "226239"
  },
  {
    "text": "the simplest thing to do is actually to deploy a layer of engine X cache in front of territory so very idea of",
    "start": "226239",
    "end": "233139"
  },
  {
    "text": "opacity workloads uber use that for a couple years it's very scalable very simple and this works",
    "start": "233139",
    "end": "239680"
  },
  {
    "text": "better if you apply a connection limit but solo person he using the criteria and the Internet's cache in 2015 but at",
    "start": "239680",
    "end": "247989"
  },
  {
    "text": "late 2016 in Linux is no longer enough that's because of boobers workload so at",
    "start": "247989",
    "end": "254739"
  },
  {
    "start": "251000",
    "end": "360000"
  },
  {
    "text": "over we have a lot of our images they are all fairly big either to size is about wengie but these days we see a lot",
    "start": "254739",
    "end": "260530"
  },
  {
    "text": "of 10g images at over we also schedule batch processing jobs on top of our",
    "start": "260530",
    "end": "265870"
  },
  {
    "text": "compute class sir those jobs they they permit is crazy much more than status jobs so concurrent",
    "start": "265870",
    "end": "271840"
  },
  {
    "text": "so they could concurrently a pool tens of thousands of images every machine so",
    "start": "271840",
    "end": "277030"
  },
  {
    "text": "because of this large talker pool you can't really use things like set for HDFS because those those oceans active",
    "start": "277030",
    "end": "284620"
  },
  {
    "text": "than for each machine for different file you've had gone for the same talking image on all the machines actually it",
    "start": "284620",
    "end": "290440"
  },
  {
    "text": "doesn't scale very well another scenario that has been bothered us bothering us is host maintenance so imagine you have",
    "start": "290440",
    "end": "297580"
  },
  {
    "text": "a machine cluster pure ethically have to up with colonel up with OSF take down hundreds of them offline at a time and",
    "start": "297580",
    "end": "303880"
  },
  {
    "text": "whenever that happens all containers running all those machine need to be rescheduled so if you if you have like",
    "start": "303880",
    "end": "309930"
  },
  {
    "text": "plant a container vending machine and you take down control machines and then you are rescheduling to southern",
    "start": "309930",
    "end": "315880"
  },
  {
    "text": "containers at the same time so as a $2,000 pool all most of them are",
    "start": "315880",
    "end": "320890"
  },
  {
    "text": "actually different images so this is where nginx actually cannot handle because if you are putting a lot of",
    "start": "320890",
    "end": "326620"
  },
  {
    "text": "different images but cache the standard works the cache get it past eight recent years uber also started building a lot",
    "start": "326620",
    "end": "333220"
  },
  {
    "text": "of smaller data centers so there's a that create a new problem that is how do you create hobby replica images across",
    "start": "333220",
    "end": "340870"
  },
  {
    "text": "multiple zones uber also on top of so purpose have one prompted the center",
    "start": "340870",
    "end": "346450"
  },
  {
    "text": "also use TCP and otherwise there is no common storage solution available across all three of them these are all issues",
    "start": "346450",
    "end": "352870"
  },
  {
    "text": "we need to solve so given the issues we started looking into a p2p solutions so",
    "start": "352870",
    "end": "361570"
  },
  {
    "start": "360000",
    "end": "458000"
  },
  {
    "text": "our design design consideration is first it has to be optimized for its entire internal usage which is a little",
    "start": "361570",
    "end": "367930"
  },
  {
    "text": "different from for the PPP in in a while in open Internet we also hope the",
    "start": "367930",
    "end": "374770"
  },
  {
    "text": "solutions who have low maintenance because I have a very small team we have three people and we work half that half",
    "start": "374770",
    "end": "379780"
  },
  {
    "text": "time on this and how to maintain like thousands of hosts we don't have time to you know debug if you every day there",
    "start": "379780",
    "end": "387430"
  },
  {
    "text": "shouldn't be any single point of failure if any any if any machine your new solution fails we shouldn't have to work",
    "start": "387430",
    "end": "392620"
  },
  {
    "text": "hard when the fix time machine we should be able to you know take a few days and look at it like that's a week or",
    "start": "392620",
    "end": "399050"
  },
  {
    "text": "something also we had to handle two kinds of workload as I mentioned before",
    "start": "399050",
    "end": "405229"
  },
  {
    "text": "so first is tens of thousands of same image for batch processing jobs and the",
    "start": "405229",
    "end": "411489"
  },
  {
    "text": "suddens of unique images in the case of hosts maintenance another thing is we",
    "start": "411489",
    "end": "418039"
  },
  {
    "text": "also care about do completion time so if you think about how you were loading up within the cluster you care about the",
    "start": "418039",
    "end": "425300"
  },
  {
    "text": "last one last motion I actually they finished auger pool you don't want a CUDA p50 you want a coupie 99 100 so",
    "start": "425300",
    "end": "433189"
  },
  {
    "text": "this is also where solutions like BitTorrent actually doesn't work because Pierce onesies didn't for multi mised",
    "start": "433189",
    "end": "441319"
  },
  {
    "text": "for like opening tonight it's more defensive it had to punish bad behaviors optimize for the for the people who",
    "start": "441319",
    "end": "447409"
  },
  {
    "text": "download the fast and that's not what we need we need a winning distortion that actually works works better for the",
    "start": "447409",
    "end": "454789"
  },
  {
    "text": "slower machines so the first thing we",
    "start": "454789",
    "end": "462709"
  },
  {
    "start": "458000",
    "end": "533000"
  },
  {
    "text": "thought about is way to some kind of layered structure by layer thrush I mean it's great actually we employ deploy a simple solution would",
    "start": "462709",
    "end": "469909"
  },
  {
    "text": "be you deploy more layers of in genetics but as we talked about before and genetics under work where well when you",
    "start": "469909",
    "end": "475789"
  },
  {
    "text": "have a lot of unique images because there's nothing to cache a slightly fancier variation of this is you're",
    "start": "475789",
    "end": "482569"
  },
  {
    "text": "somehow constructed differently dynamically for each each file each image but after some co-creation",
    "start": "482569",
    "end": "489079"
  },
  {
    "text": "realized tree is actually not a structure or not actually not optimal for this because realistically virtually",
    "start": "489079",
    "end": "496189"
  },
  {
    "text": "should be relatively fat think of Melaka Petri and as we get fatter",
    "start": "496189",
    "end": "501619"
  },
  {
    "text": "your your upload speed of the parents actually get care to get lower so it's",
    "start": "501619",
    "end": "507829"
  },
  {
    "text": "not ideal for our big files it's also very hard to maintain circuitry topology because if you think about energy every",
    "start": "507829",
    "end": "513709"
  },
  {
    "text": "point every node is a single point of failure so every failure need to be handled is a little hard to implement",
    "start": "513709",
    "end": "519740"
  },
  {
    "text": "but that being said there are there are companies actually implemented such solutions for example led from Facebook",
    "start": "519740",
    "end": "524899"
  },
  {
    "text": "but it's a very complex project I took them mad for years right Ober we want a solution simpler",
    "start": "524899",
    "end": "532360"
  },
  {
    "text": "than that so another thing we consider the in the beginning is a central over",
    "start": "532360",
    "end": "537610"
  },
  {
    "start": "533000",
    "end": "593000"
  },
  {
    "text": "overseer model so the Halep idea is you divide each it fell into smaller data",
    "start": "537610",
    "end": "543070"
  },
  {
    "text": "chunks for in about 4 megabytes or slightly bigger and you have where you have a central component that make all",
    "start": "543070",
    "end": "548650"
  },
  {
    "text": "the scheduling decisions you said that we should know the transfer has transmitted it needs to wear so it makes",
    "start": "548650",
    "end": "556690"
  },
  {
    "text": "sense it could at the universal optimal solution in theory but if you do some",
    "start": "556690",
    "end": "561790"
  },
  {
    "text": "compilations actually you have to go the central oversee I have to handle a very high amount of QPS we are not sure we",
    "start": "561790",
    "end": "568690"
  },
  {
    "text": "are able to implement such overseer so we give up this approach it's also not",
    "start": "568690",
    "end": "574450"
  },
  {
    "text": "clear what happens if you or overseer fail but again there are other companies who did this that is turning falaya from",
    "start": "574450",
    "end": "581440"
  },
  {
    "text": "Alibaba they have this central like super node that decide everything we actually talked to them recently they",
    "start": "581440",
    "end": "587740"
  },
  {
    "text": "are there our company is indeed the kewpie is on the super node so after",
    "start": "587740",
    "end": "594550"
  },
  {
    "start": "593000",
    "end": "677000"
  },
  {
    "text": "some exploration we found a ideal model that is a random graph so so in this in",
    "start": "594550",
    "end": "600970"
  },
  {
    "text": "this model you still need a central component but the central component responsibility is not much it just make",
    "start": "600970",
    "end": "607330"
  },
  {
    "text": "a connection decision decide who connected boom the actual data transfer case is still made by individual node in",
    "start": "607330",
    "end": "614260"
  },
  {
    "text": "the video notes so such graph is called so if you also enforce if you know that you have a fixed number of IPs then you",
    "start": "614260",
    "end": "621730"
  },
  {
    "text": "get a random regular graph so subsequent factories have very very nice properties",
    "start": "621730",
    "end": "627160"
  },
  {
    "text": "so it has very good connectivity there are like metal pass between I need to are between those and the distance",
    "start": "627160",
    "end": "634180"
  },
  {
    "text": "between any any nodes actually fairly small this is now acting a new idea which isn't they found there is a paper",
    "start": "634180",
    "end": "640270"
  },
  {
    "text": "form to some stuff called jellyfish describing such a network topology it's very very great so we did some",
    "start": "640270",
    "end": "646420"
  },
  {
    "text": "simulation so if you drop one single cedar a single upload or in this network",
    "start": "646420",
    "end": "651450"
  },
  {
    "text": "instantly every other node is able to download the data at 80 percent of max data speed so also our simulations",
    "start": "651450",
    "end": "658660"
  },
  {
    "text": "actually hoped for we assume the relative latency if it did a proper simulation number should be approaching 90% so as",
    "start": "658660",
    "end": "665980"
  },
  {
    "text": "you can easily tell such a model is also very resilient to failures it's very hard to create a natural partition in",
    "start": "665980",
    "end": "672040"
  },
  {
    "text": "sexual connection growth so this this is what we went ways for crackin so Cody",
    "start": "672040",
    "end": "677769"
  },
  {
    "start": "677000",
    "end": "695000"
  },
  {
    "text": "here we'll talk about how they translate this idea into actual project hey can",
    "start": "677769",
    "end": "683860"
  },
  {
    "text": "you hear me it's this cool yes I'm Cody I'm one of the developers on Kraken so",
    "start": "683860",
    "end": "691600"
  },
  {
    "text": "I'm gonna talk a bit about how we used cracking to solve these peer-to-peer problems so first some vocabulary so",
    "start": "691600",
    "end": "698019"
  },
  {
    "start": "695000",
    "end": "725000"
  },
  {
    "text": "we're on the same page a torrent is what we call a file that has been broken into multiple pieces typically four megabytes",
    "start": "698019",
    "end": "704620"
  },
  {
    "text": "each pieces are transferred independently a peer is a participant in",
    "start": "704620",
    "end": "711129"
  },
  {
    "text": "the torrent network connected peers transfer pieces between each other and appear with a hundred percent of pieces",
    "start": "711129",
    "end": "717730"
  },
  {
    "text": "is called a seeder I'll also be throwing around the word blob around a blob is just a file so so",
    "start": "717730",
    "end": "726459"
  },
  {
    "start": "725000",
    "end": "795000"
  },
  {
    "text": "this is the Kraken architecture we have three components an agent that runs on",
    "start": "726459",
    "end": "731920"
  },
  {
    "text": "every host this is the peer the agent implements a docker registry interface so you execute dock or poles against",
    "start": "731920",
    "end": "739509"
  },
  {
    "text": "your local agent and that pulls the image through the peer-to-peer network we have an origin the origin is a",
    "start": "739509",
    "end": "745810"
  },
  {
    "text": "dedicated seater they see data from some pluggable storage back-end such as s3",
    "start": "745810",
    "end": "752199"
  },
  {
    "text": "HDFS even another darker registry the origins form a self-healing hash string",
    "start": "752199",
    "end": "758649"
  },
  {
    "text": "so you can add more origins to distribute load if an origin dies no big deal they seal the gap this is in line",
    "start": "758649",
    "end": "766689"
  },
  {
    "text": "with like our low maintenance philosophy if an Origin dies we don't want to get woken up to fix it you know we can look",
    "start": "766689",
    "end": "772689"
  },
  {
    "text": "at it a week later even and then tracker tracker maintains a global view of who",
    "start": "772689",
    "end": "779199"
  },
  {
    "text": "the peers are who the cedars are who's trying to download what data and it does",
    "start": "779199",
    "end": "784420"
  },
  {
    "text": "all of this in memory again trackers also form a self-healing hash strings so you can add more trackers to distribute",
    "start": "784420",
    "end": "790720"
  },
  {
    "text": "load if a tracker dies no big deal they fill the gap so some key features of this",
    "start": "790720",
    "end": "797420"
  },
  {
    "start": "795000",
    "end": "882000"
  },
  {
    "text": "architecture cracking is just like cashing and distribution layer we don't",
    "start": "797420",
    "end": "802879"
  },
  {
    "text": "want to be in the business of owning data of doing backups of maintaining a database again there's just three of us",
    "start": "802879",
    "end": "809959"
  },
  {
    "text": "we don't have time or resources to do that so because our origin serves data",
    "start": "809959",
    "end": "815779"
  },
  {
    "text": "from another source like s3 we can we can suffer from total data loss and",
    "start": "815779",
    "end": "821480"
  },
  {
    "text": "recover automatically so you could wipe the disk of every single cracking component restart everything and no one",
    "start": "821480",
    "end": "827540"
  },
  {
    "text": "would even notice you just start reseeding the data back into the network we take a minimal dependency set your",
    "start": "827540",
    "end": "834589"
  },
  {
    "text": "darker registry is usually the at the base level base layer of your your",
    "start": "834589",
    "end": "839899"
  },
  {
    "text": "infrastructure it is essentially what deploys all other services so we're the",
    "start": "839899",
    "end": "846199"
  },
  {
    "text": "only dependency we can take is DMS which is actually optional and then finally",
    "start": "846199",
    "end": "853519"
  },
  {
    "text": "constant adjustable blobs so old blobs and cracking are constant ingestible what this means is the blob identifier",
    "start": "853519",
    "end": "859519"
  },
  {
    "text": "is a hash of the blog content by definition this means that blobs are immutable if I change the blob content",
    "start": "859519",
    "end": "865970"
  },
  {
    "text": "the blob receives a new blob identifier so they're fundamentally different blobs the disadvantage of content dressed Bowl",
    "start": "865970",
    "end": "872449"
  },
  {
    "text": "blobs is that they're not user friendly the blob identifiers are 64 hexadecimal",
    "start": "872449",
    "end": "878029"
  },
  {
    "text": "strings that are meaningless to a human so how does peer discovery work if I'm",
    "start": "878029",
    "end": "885740"
  },
  {
    "start": "882000",
    "end": "933000"
  },
  {
    "text": "an agent and I want to download some torrent I contact my tracker and I say hey I'm this peer I want to download",
    "start": "885740",
    "end": "892429"
  },
  {
    "text": "this torrent and the tracker says ok here are 50 random peers sorted by",
    "start": "892429",
    "end": "898220"
  },
  {
    "text": "preference to go try and connect to so the most preferred are other completed",
    "start": "898220",
    "end": "903350"
  },
  {
    "text": "agents seeding the torrent next are the origins ok the designated seeders and",
    "start": "903350",
    "end": "908660"
  },
  {
    "text": "then finally the lowest priority are other importers agents since they might not have all the data if any so once I",
    "start": "908660",
    "end": "915860"
  },
  {
    "text": "have this list of 50 I just iterate down the list until I have 10 connections and remember a random graph you have a",
    "start": "915860",
    "end": "921769"
  },
  {
    "text": "connection limit peers reject connections once they are at that limit so I might have all",
    "start": "921769",
    "end": "928130"
  },
  {
    "text": "down this list until I get 10 peers that want to have open connection slots so we",
    "start": "928130",
    "end": "935240"
  },
  {
    "start": "933000",
    "end": "963000"
  },
  {
    "text": "built this little visualization tool for seeing what's happening in the cracking Network here we can see the blue nodes",
    "start": "935240",
    "end": "943670"
  },
  {
    "text": "are the origins the designated Cedars everybody else is an agent up here they",
    "start": "943670",
    "end": "949580"
  },
  {
    "text": "go from gray to green as they download the data and this is the random graph that we're talking about each agent in",
    "start": "949580",
    "end": "956960"
  },
  {
    "text": "this diagram has a connection limit of 5 so the graph is very sparse but well connected so this actually isn't the",
    "start": "956960",
    "end": "966110"
  },
  {
    "start": "963000",
    "end": "1020000"
  },
  {
    "text": "whole picture to make darker poles work you actually need to do one step initially to resolve the darker tag",
    "start": "966110",
    "end": "973220"
  },
  {
    "text": "you know Alpine 3.6 into the image Shaw so old data interact in this",
    "start": "973220",
    "end": "980120"
  },
  {
    "text": "content-addressable which means that we have to map this human readable tag into a sha-256 digest so build index is a",
    "start": "980120",
    "end": "987710"
  },
  {
    "text": "super rudimentary piece of technology it has a pluggable storage back-end like",
    "start": "987710",
    "end": "993170"
  },
  {
    "text": "our origin it doesn't own any data we just put tags somewhere else has no consistency",
    "start": "993170",
    "end": "998570"
  },
  {
    "text": "guarantees turns out having a consistent key value stores you know kind of a hard",
    "start": "998570",
    "end": "1004030"
  },
  {
    "text": "problem to solve but it also has one more neat feature for powering image",
    "start": "1004030",
    "end": "1009820"
  },
  {
    "text": "replication between clusters so uber we have you know several on-prem data centers cloud zones and cracking",
    "start": "1009820",
    "end": "1017260"
  },
  {
    "text": "clusters in each zone so our setup sort of looks like this",
    "start": "1017260",
    "end": "1022360"
  },
  {
    "start": "1020000",
    "end": "1037000"
  },
  {
    "text": "you have zones that share storage a cracking cluster in the zone and if I",
    "start": "1022360",
    "end": "1027970"
  },
  {
    "text": "push an image to any zone it automatically replicates everywhere so it's available for pulling from any",
    "start": "1027970",
    "end": "1036188"
  },
  {
    "text": "other zone security is important for us",
    "start": "1036189",
    "end": "1041819"
  },
  {
    "start": "1037000",
    "end": "1061000"
  },
  {
    "text": "we support mutual TLS between all our central components the peer-to-peer traffic doesn't go through TLS but we're",
    "start": "1041819",
    "end": "1048850"
  },
  {
    "text": "not too worried about data integrity because all the blobs are content addressable they're immutable if I'm a",
    "start": "1048850",
    "end": "1055060"
  },
  {
    "text": "peer and I download some blob I recalculate the hash of the blob and make sure it's what I expected",
    "start": "1055060",
    "end": "1061260"
  },
  {
    "text": "so let's look at some results so we ran this pretty simple test we took a three",
    "start": "1061670",
    "end": "1068280"
  },
  {
    "text": "gigabyte image and we concurrently downloaded it onto 2600 hosts this is",
    "start": "1068280",
    "end": "1073770"
  },
  {
    "text": "what a typical batch processing job looks like it looks like a d'hubert and on each agent we enforced a 0.3 gigabyte",
    "start": "1073770",
    "end": "1082440"
  },
  {
    "text": "per second speed limit so this would make the theoretical max download speed 10 seconds the results were that the p50",
    "start": "1082440",
    "end": "1090120"
  },
  {
    "text": "was 10 seconds p99 18 seconds and the max was 32 seconds so essentially if you",
    "start": "1090120",
    "end": "1097440"
  },
  {
    "text": "had a batch processing job on 2,600 hosts with the 3 gigabyte image you could launch that job within 30 seconds",
    "start": "1097440",
    "end": "1103340"
  },
  {
    "text": "which definitely met our requirements when we were trying to solve this scaling issue so what does this actually",
    "start": "1103340",
    "end": "1110610"
  },
  {
    "start": "1109000",
    "end": "1150000"
  },
  {
    "text": "look like in production in our busiest zone cracking distributes close to 2 million blobs a day most of these are",
    "start": "1110610",
    "end": "1117120"
  },
  {
    "text": "medium-sized around a gigabyte and the daily peak is usually around 20,000 1",
    "start": "1117120",
    "end": "1123900"
  },
  {
    "text": "gig blobs distributed within 30 seconds so with our old registry and Gen X setup",
    "start": "1123900",
    "end": "1129750"
  },
  {
    "text": "this would have instantly cause an outage basically the batch jobs would start oh geez",
    "start": "1129750",
    "end": "1135950"
  },
  {
    "text": "that should all just start timing out and deployments would start failing and",
    "start": "1135950",
    "end": "1142590"
  },
  {
    "text": "we would start getting you know complaints from users now this you know these Peaks happen every day and nobody",
    "start": "1142590",
    "end": "1148200"
  },
  {
    "text": "even notices so some optimizations we made along the",
    "start": "1148200",
    "end": "1153960"
  },
  {
    "start": "1150000",
    "end": "1242000"
  },
  {
    "text": "way so in the random regular graph like you have a connection limit we found",
    "start": "1153960",
    "end": "1159390"
  },
  {
    "text": "that keeping that connection limit pretty low is actually pretty useful it lowers the overhead of the entire",
    "start": "1159390",
    "end": "1165840"
  },
  {
    "text": "network I think in production we use 10 as a connection limit another",
    "start": "1165840",
    "end": "1171300"
  },
  {
    "text": "optimization is to aggressively disconnect unneeded connections so when you have a connection limit and you're",
    "start": "1171300",
    "end": "1177660"
  },
  {
    "text": "going to reject connections if your your connection limit is full I want to do I",
    "start": "1177660",
    "end": "1184140"
  },
  {
    "text": "want to disconnect connections I don't need to make room for other peers that want to download data from me so idle",
    "start": "1184140",
    "end": "1190260"
  },
  {
    "text": "connections connections between cedars are all aggressively disconnected and this is the effect of rebalancing",
    "start": "1190260",
    "end": "1196350"
  },
  {
    "text": "the network and always making data available to new newcomers pipelining is",
    "start": "1196350",
    "end": "1201630"
  },
  {
    "text": "a technique where you maintain a request queue of size n for each connection basically instead of sending you know I",
    "start": "1201630",
    "end": "1208680"
  },
  {
    "text": "request a piece I wait for a response I request a piece I wait for a response I'll just do request request request and",
    "start": "1208680",
    "end": "1215130"
  },
  {
    "text": "then as the responses come in I send more requests this results in less idle time between peers you know I always",
    "start": "1215130",
    "end": "1222330"
  },
  {
    "text": "have something to chew on if I'm if I'm getting lots of requests and then finally in game mode this is something",
    "start": "1222330",
    "end": "1228240"
  },
  {
    "text": "we borrowed from BitTorrent basically for the last few pieces of a download just to request them from all your",
    "start": "1228240",
    "end": "1234390"
  },
  {
    "text": "connected neighbors this prevents a slope here from compromising you know",
    "start": "1234390",
    "end": "1239400"
  },
  {
    "text": "the last 1% of a download some unsuccessful optimizations initially we",
    "start": "1239400",
    "end": "1247680"
  },
  {
    "start": "1242000",
    "end": "1317000"
  },
  {
    "text": "had this idea that we could prefer peers on the same rack to reduce network overhead in the rack switches the",
    "start": "1247680",
    "end": "1255300"
  },
  {
    "text": "problem here is that once you start making more intelligent connection decisions you end up forming disjoint",
    "start": "1255300",
    "end": "1263130"
  },
  {
    "text": "graphs so you we would see an entire rack of peers format interconnected web",
    "start": "1263130",
    "end": "1268590"
  },
  {
    "text": "between each other and basically star for data and it turns out you don't even",
    "start": "1268590",
    "end": "1274410"
  },
  {
    "text": "need to do this if your network is over provisioned which ours are so we threw this out pretty quickly and then this",
    "start": "1274410",
    "end": "1282060"
  },
  {
    "text": "next one reject incoming connections based on number of mutual connections so if I'm a peer and I'm connected to",
    "start": "1282060",
    "end": "1289290"
  },
  {
    "text": "another peer and a third peer comes in and connects to both of us that would make a mutual connection right and so",
    "start": "1289290",
    "end": "1295590"
  },
  {
    "text": "one of those connections actually isn't needed we could transitively send the data between the the middle peer so we",
    "start": "1295590",
    "end": "1304260"
  },
  {
    "text": "were a little worried about getting tight clumps of interconnected peers we call this like graph density problems",
    "start": "1304260",
    "end": "1309360"
  },
  {
    "text": "but in reality we haven't seen any issues with this we just use strict randomization and things work out pretty",
    "start": "1309360",
    "end": "1315840"
  },
  {
    "text": "well so some takeaways this initially we",
    "start": "1315840",
    "end": "1321630"
  },
  {
    "start": "1317000",
    "end": "1402000"
  },
  {
    "text": "set out to solve this darker registry scaling problem but the solution we ended up designing",
    "start": "1321630",
    "end": "1327140"
  },
  {
    "text": "was agnostic to two darker images it just works on content-addressable blobs which means you can integrate with",
    "start": "1327140",
    "end": "1333710"
  },
  {
    "text": "any storage system so we started with darker images but now we use Kraken an uber to power config distribution",
    "start": "1333710",
    "end": "1340910"
  },
  {
    "text": "machine learning data large s3 files basically we have this peer-to-peer",
    "start": "1340910",
    "end": "1346100"
  },
  {
    "text": "network deployed in our data center and we can use it any way we want and the pluggable storage backends",
    "start": "1346100",
    "end": "1352790"
  },
  {
    "text": "really makes this possible so next PDP solutions work with data centers um there was some concern that deploying a",
    "start": "1352790",
    "end": "1360920"
  },
  {
    "text": "PDP solution would overload the network slow things down for production services",
    "start": "1360920",
    "end": "1366220"
  },
  {
    "text": "but we found this to not be the case and it works pretty well randomization works we started with",
    "start": "1366220",
    "end": "1373400"
  },
  {
    "text": "random policies because they're easy to implement but it turns out they're pretty well they work pretty well and",
    "start": "1373400",
    "end": "1378950"
  },
  {
    "text": "they're easy to reason about trying to be too intelligent about how you make connections what pieces your requests",
    "start": "1378950",
    "end": "1386530"
  },
  {
    "text": "can sort of make things more difficult for you and then finally this goes without saying get something working",
    "start": "1386530",
    "end": "1392450"
  },
  {
    "text": "before Optimas optimization it's very difficult to predict how PDP works",
    "start": "1392450",
    "end": "1398180"
  },
  {
    "text": "without experimenting yeah so future",
    "start": "1398180",
    "end": "1404690"
  },
  {
    "start": "1402000",
    "end": "1449000"
  },
  {
    "text": "plan right now cracking mostly addresses medium sized blob around a gigabyte but",
    "start": "1404690",
    "end": "1410330"
  },
  {
    "text": "we have some use cases for a massive number of tiny files that we want to distribute we want to improve",
    "start": "1410330",
    "end": "1416470"
  },
  {
    "text": "observability so you saw that simulation that we have we use that to sort of get an idea of what the network is doing at",
    "start": "1416470",
    "end": "1422660"
  },
  {
    "text": "a high level but of course like we always want to improve our insights into what's really going on with p2p we want",
    "start": "1422660",
    "end": "1431120"
  },
  {
    "text": "tighter integration with other registry registry implementations so this means adding more docker registry backends",
    "start": "1431120",
    "end": "1437380"
  },
  {
    "text": "such as GCR or artifactory and then of course tighter integration with",
    "start": "1437380",
    "end": "1442790"
  },
  {
    "text": "kubernetes modeling images as resources to speed up deployment times further so",
    "start": "1442790",
    "end": "1450530"
  },
  {
    "start": "1449000",
    "end": "1477000"
  },
  {
    "text": "this is our rolling github everything's open sourced we're looking for more",
    "start": "1450530",
    "end": "1455660"
  },
  {
    "text": "contributors if you have a storage back-end that you want to add we're happy to take a looking at it",
    "start": "1455660",
    "end": "1462890"
  },
  {
    "text": "yeah I think that's it okay so I think we have a few minutes for questions and",
    "start": "1462890",
    "end": "1469320"
  },
  {
    "text": "there any questions anyway",
    "start": "1469320",
    "end": "1473330"
  },
  {
    "text": "[Applause]",
    "start": "1475500",
    "end": "1478690"
  },
  {
    "start": "1477000",
    "end": "1528000"
  },
  {
    "text": "I think she's gonna take your mic",
    "start": "1484200",
    "end": "1487730"
  },
  {
    "text": "that's good cuz I kind of blew up my voice yesterday so you mentioned",
    "start": "1496070",
    "end": "1501729"
  },
  {
    "text": "beginning the talk that one of the things you wanted to get out of this was",
    "start": "1501729",
    "end": "1506779"
  },
  {
    "text": "not just multiple parallel poles of a single image but also the ability to",
    "start": "1506779",
    "end": "1513080"
  },
  {
    "text": "deal better with many poles of unique images did you see sort of similar gains",
    "start": "1513080",
    "end": "1519350"
  },
  {
    "text": "on that metric to what you saw and you put in your presentation about parallel",
    "start": "1519350",
    "end": "1525350"
  },
  {
    "text": "pulls against the same image so in the",
    "start": "1525350",
    "end": "1536029"
  },
  {
    "start": "1528000",
    "end": "1579000"
  },
  {
    "text": "end I think our our solution actually works pretty well always post cases if",
    "start": "1536029",
    "end": "1542539"
  },
  {
    "text": "you think about it the solution in fact I have been like bashing about BitTorrent solution is actually similar",
    "start": "1542539",
    "end": "1547700"
  },
  {
    "text": "to BitTorrent in a sense that a lot of unique downloads normally in your class",
    "start": "1547700",
    "end": "1554479"
  },
  {
    "text": "store I have those images yeah some other other machines so I think one more download you'll be just sitting from all",
    "start": "1554479",
    "end": "1559849"
  },
  {
    "text": "those other machines so that really either loaded three essential components and in the case of massive concurrent",
    "start": "1559849",
    "end": "1565549"
  },
  {
    "text": "downloads because the graph we connect like this random graph actually things also happen pretty fast so everyone was",
    "start": "1565549",
    "end": "1572599"
  },
  {
    "text": "able to download machspeed suppose case works very well I hope that answers your question",
    "start": "1572599",
    "end": "1578029"
  },
  {
    "text": "yeah two quick questions one is how easy it is to deploy tracker",
    "start": "1578029",
    "end": "1584419"
  },
  {
    "text": "how do you do it and secondly how does it deal with like very dynamic environment variable lot of nodes coming",
    "start": "1584419",
    "end": "1590899"
  },
  {
    "text": "in and going out for example how does that work because if you check out our",
    "start": "1590899",
    "end": "1596809"
  },
  {
    "text": "github we have a how much heart so I can't deploy this with one command but normally you probably want to configure",
    "start": "1596809",
    "end": "1603559"
  },
  {
    "text": "the config file little bit it's just so it can connect to your hostel registry normally what happens with doctorates",
    "start": "1603559",
    "end": "1610879"
  },
  {
    "text": "reviews you you have some custom configurations we never tried this on dynamic environment but statically it",
    "start": "1610879",
    "end": "1620210"
  },
  {
    "text": "probably won't work that well because our our tracker will keep track of who has what it if that knowledge just goes",
    "start": "1620210",
    "end": "1626359"
  },
  {
    "text": "away right away then you has incorrect data so probably won't work well in dynamic environment okay",
    "start": "1626359",
    "end": "1637580"
  },
  {
    "start": "1634000",
    "end": "1660000"
  },
  {
    "text": "and I was just wondering if you looked at I'm actually pushing blobs on two",
    "start": "1637580",
    "end": "1645170"
  },
  {
    "text": "nodes before 30 years so as far as I can tell at the minute you do like a docker",
    "start": "1645170",
    "end": "1651380"
  },
  {
    "text": "pull and that wall starts pulling the the blobs from the network onto the note but if you actually looked at pushing",
    "start": "1651380",
    "end": "1657980"
  },
  {
    "text": "them before that happens this is what we mean by story here so uh",
    "start": "1657980",
    "end": "1666800"
  },
  {
    "start": "1660000",
    "end": "1742000"
  },
  {
    "text": "Cooper started doing using docker in 2015 so at that time uber to basically",
    "start": "1666800",
    "end": "1672560"
  },
  {
    "text": "develop today's own operation framework that's actually similar to cronikeys except the upgrade on that framework is",
    "start": "1672560",
    "end": "1679280"
  },
  {
    "text": "much faster Lancome radius we upgraded like for example 5 500 container that",
    "start": "1679280",
    "end": "1684680"
  },
  {
    "text": "would take 5 minutes but if you if you do a back up word on communities that could take half an hour to an hour",
    "start": "1684680",
    "end": "1691040"
  },
  {
    "text": "because on communities when you do a rolling upgrade every batch have to you",
    "start": "1691040",
    "end": "1696080"
  },
  {
    "text": "know download the imagery again and if not each batch take a few minutes at least so end up with very long time so",
    "start": "1696080",
    "end": "1702710"
  },
  {
    "text": "our plan for this is we we are wrecking our own communities scheduler their",
    "start": "1702710",
    "end": "1708380"
  },
  {
    "text": "support in place upgrade so once you have once you have such a scheduler then",
    "start": "1708380",
    "end": "1715160"
  },
  {
    "text": "you know which machine will be hosting the next version of the images so we can preheat all those Tocker demon-like",
    "start": "1715160",
    "end": "1723470"
  },
  {
    "text": "all those machines so ideally you can you can do a very fast running up great",
    "start": "1723470",
    "end": "1728840"
  },
  {
    "text": "because you preheat all the machines with images and all you do when you know going out with is just a start a container and to how spec so yeah but",
    "start": "1728840",
    "end": "1735350"
  },
  {
    "text": "that takes quite a lot of work for every plan to get that down maybe next year",
    "start": "1735350",
    "end": "1741580"
  },
  {
    "text": "hey um two quick questions the first one how do you think about retention and deletion of images under what",
    "start": "1741790",
    "end": "1748280"
  },
  {
    "start": "1742000",
    "end": "1768000"
  },
  {
    "text": "circumstances do you delete have you run experiments that tell you which strategies work better second question",
    "start": "1748280",
    "end": "1753920"
  },
  {
    "text": "your network presumably also does real work and not just you know distribution of images how do you balance your",
    "start": "1753920",
    "end": "1759260"
  },
  {
    "text": "network band with between the tasks of actually seeding and loading and Ozma and distributing these images with the real",
    "start": "1759260",
    "end": "1766050"
  },
  {
    "text": "work that your clusters are supposed to do so the first question is actually a very tricky so there are two kinds of",
    "start": "1766050",
    "end": "1772830"
  },
  {
    "start": "1768000",
    "end": "1844000"
  },
  {
    "text": "image talking talking about years although the first line is about deleting from dr. demon a sacraments",
    "start": "1772830",
    "end": "1778290"
  },
  {
    "text": "body beating from Kraken so photogra demon at our policy is if the image is not used for one hour it",
    "start": "1778290",
    "end": "1786210"
  },
  {
    "text": "will get eveything for crack and we just use you know everywhere wheels about I",
    "start": "1786210",
    "end": "1791340"
  },
  {
    "text": "think our default tdrs six hour so if a layer is not seeded this not is not not",
    "start": "1791340",
    "end": "1798360"
  },
  {
    "text": "used anywhere than this DVD in six hours and that worked out pretty pretty well",
    "start": "1798360",
    "end": "1805309"
  },
  {
    "text": "first question so yeah was a second question again repeat second also so in",
    "start": "1808370",
    "end": "1816150"
  },
  {
    "text": "Kraken we implement our own battling logic and that worked pretty well we try",
    "start": "1816150",
    "end": "1823110"
  },
  {
    "text": "to make work and consume less than 30 percent of our planned waste and that",
    "start": "1823110",
    "end": "1829650"
  },
  {
    "text": "has been has been doing pretty well because all the rest of network command are not consuming as much bandwidth as",
    "start": "1829650",
    "end": "1837150"
  },
  {
    "text": "cracking itself so together we add up less than hundred percent hi one",
    "start": "1837150",
    "end": "1844860"
  },
  {
    "text": "question in the peer discovery part of the architecture what was the rationale",
    "start": "1844860",
    "end": "1850140"
  },
  {
    "text": "behind choosing 50 nodes and 10 connections just curious as to what role",
    "start": "1850140",
    "end": "1857600"
  },
  {
    "text": "better so is our simulation there was a magic number that's between 5 to 10 so",
    "start": "1862040",
    "end": "1868950"
  },
  {
    "text": "if you random regular graph if the degree number by design Evernote is between 5 to 10 you actually get the",
    "start": "1868950",
    "end": "1874950"
  },
  {
    "text": "best performance that's that's why we picked 50 actually I don't think there is work with the reason just not work",
    "start": "1874950",
    "end": "1882090"
  },
  {
    "text": "that's production another question do you see any like bottleneck at the",
    "start": "1882090",
    "end": "1889890"
  },
  {
    "text": "tracker at this point or is that in some way so today's actually works pretty well in production actually I",
    "start": "1889890",
    "end": "1896539"
  },
  {
    "text": "think our worst case scenario can still download at about 60% like Mike whatever",
    "start": "1896539",
    "end": "1903830"
  },
  {
    "text": "about her speed limit you specify that node will be able to tunnel at 6% of the 60% of the speed limit that's that's",
    "start": "1903830",
    "end": "1910400"
  },
  {
    "text": "okay but want to push to put that to 90% I think currently is our implementation",
    "start": "1910400",
    "end": "1915470"
  },
  {
    "text": "is not ideal so there are some small small tweaks we can make to push that enough are you asking about how how much",
    "start": "1915470",
    "end": "1926270"
  },
  {
    "text": "GPS the tracker can can handle yes yeah so I think it can handle way more than",
    "start": "1926270",
    "end": "1933500"
  },
  {
    "text": "we then we use our clusters are around 8,000 notes but when we just like hammer",
    "start": "1933500",
    "end": "1941090"
  },
  {
    "text": "it for benchmarking theoretically you get a handle you know tens of thousands",
    "start": "1941090",
    "end": "1946250"
  },
  {
    "text": "more than that so everything's in memory and you're not storing very much data",
    "start": "1946250",
    "end": "1951799"
  },
  {
    "text": "about each appear literally just IP port and a single bit as to whether the peer has completed the torrent or not and so",
    "start": "1951799",
    "end": "1960350"
  },
  {
    "text": "yeah QPS isn't really a bottleneck for us okay thank you it's a little bit so",
    "start": "1960350",
    "end": "1967309"
  },
  {
    "text": "at this point I think our bottleneck is more on our kubernetes scheduler instead of cracker so our our scheduler can",
    "start": "1967309",
    "end": "1973520"
  },
  {
    "text": "schedule maybe a northern part per second but that's still much lower than crackin so training docker images with",
    "start": "1973520",
    "end": "1985340"
  },
  {
    "text": "regions with that really terrible network connection such as China so so",
    "start": "1985340",
    "end": "1993620"
  },
  {
    "text": "actually our upper portion you also have a bad connection but we started this project uber has a lot emotion about when Gbps",
    "start": "1993620",
    "end": "2000299"
  },
  {
    "text": "network card and so so it worked also if",
    "start": "2000299",
    "end": "2007059"
  },
  {
    "text": "you look at this graph so we played treat each zone as a different cluster",
    "start": "2007059",
    "end": "2014460"
  },
  {
    "text": "so within those eyes the network some more reliable then there's no problem between yeah if there are problem maybe",
    "start": "2014460",
    "end": "2022030"
  },
  {
    "text": "replication take a little that's that's okay hello",
    "start": "2022030",
    "end": "2029600"
  },
  {
    "text": "you mentioned the generic nature of Kraken do you see this applying in some",
    "start": "2029600",
    "end": "2036590"
  },
  {
    "text": "sort of CDN capacity for example some edge deployment of kubernetes nodes",
    "start": "2036590",
    "end": "2043909"
  },
  {
    "text": "perhaps something like a digital billboard deployment across the nation",
    "start": "2043909",
    "end": "2048950"
  },
  {
    "text": "for delivering just you know JPEGs to show on the digital billboard is yeah I",
    "start": "2048950",
    "end": "2058010"
  },
  {
    "text": "mean we're we're really focused on internal data center usage like we we",
    "start": "2058010",
    "end": "2064250"
  },
  {
    "text": "even separate Kraken clusters by data center rooms so I'm not sure that's",
    "start": "2064250",
    "end": "2070040"
  },
  {
    "text": "really something we're aiming to you know solve I think what you're looking",
    "start": "2070040",
    "end": "2077030"
  },
  {
    "text": "for is actually i PF eyes I'm a little kind of actual public stuff any more",
    "start": "2077030",
    "end": "2088700"
  },
  {
    "text": "questions No okay I think that's that's it so",
    "start": "2088700",
    "end": "2097550"
  },
  {
    "text": "do you have plans to support more back in storage yeah well adding them slowly right now",
    "start": "2107250",
    "end": "2114630"
  },
  {
    "text": "we have as three HDFS docker registry we're adding in pcp this week is pretty",
    "start": "2114630",
    "end": "2121650"
  },
  {
    "text": "easy to add well if you have a specific need that you're welcome to a contribute to the project I think that's it",
    "start": "2121650",
    "end": "2132060"
  },
  {
    "text": "okay so yeah our working hard we also have a link to our select channel so if",
    "start": "2132060",
    "end": "2137609"
  },
  {
    "text": "more countries you can reach out to us through slack",
    "start": "2137609",
    "end": "2142309"
  },
  {
    "text": "[Applause]",
    "start": "2143180",
    "end": "2145989"
  }
]