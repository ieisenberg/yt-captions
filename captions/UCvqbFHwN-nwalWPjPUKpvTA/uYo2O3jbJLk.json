[
  {
    "text": "hello everybody we are going to talk a bit about observability and performance with Services",
    "start": "240",
    "end": "6640"
  },
  {
    "text": "right my name is Antonio I work for Google Staff software engineer and c and",
    "start": "6640",
    "end": "13320"
  },
  {
    "text": "some of you know me from working in kind with B there yeah my name is NAA I'm a",
    "start": "13320",
    "end": "19240"
  },
  {
    "text": "senior software engineer Trad okay so I'm not going to spray much",
    "start": "19240",
    "end": "24519"
  },
  {
    "text": "everybody here know what services but the important thing of services I I I want to call them as the magic that",
    "start": "24519",
    "end": "30240"
  },
  {
    "text": "connect all the application right you do ql apply and a lot of postes are to be created a lot of applications start to",
    "start": "30240",
    "end": "36920"
  },
  {
    "text": "be created and they somehow are able to communicate with them but I spend a lot",
    "start": "36920",
    "end": "42360"
  },
  {
    "text": "of time with users customers debugging problems and there is with this question my performance is not good but what what",
    "start": "42360",
    "end": "48920"
  },
  {
    "text": "do you mean by performance right this is a a complex topic how do you describe",
    "start": "48920",
    "end": "54280"
  },
  {
    "text": "performance for everybody something different so when I started commenting",
    "start": "54280",
    "end": "60320"
  },
  {
    "text": "this with n we started to say okay we need to do something right let's try to to find a methodology or some common",
    "start": "60320",
    "end": "66119"
  },
  {
    "text": "language so we can standardize what means performance on services but before going into that let's",
    "start": "66119",
    "end": "72960"
  },
  {
    "text": "let's be realistic we are we need to follow the the the rules of of the we we",
    "start": "72960",
    "end": "82759"
  },
  {
    "text": "cannot go faster than the speed of light right for the people that is familiar with this RFC the",
    "start": "82759",
    "end": "90479"
  },
  {
    "text": "1925 this is from 1996 and it still applies so let's try",
    "start": "90479",
    "end": "96159"
  },
  {
    "text": "to understand in in our contest on the networking performance in kubernetes what means speed on light okay let's try",
    "start": "96159",
    "end": "103360"
  },
  {
    "text": "to to simplify the problem to the most Comm the most common stack if you see in",
    "start": "103360",
    "end": "108439"
  },
  {
    "text": "most of the Clusters today you have a poort that has a network interface and the network interface connect to the Kel",
    "start": "108439",
    "end": "115880"
  },
  {
    "text": "and the Kel deliver this to another netork interface so if we want to understand the port toport communication",
    "start": "115880",
    "end": "122640"
  },
  {
    "text": "you can see that the ports send the packet through the different stages and",
    "start": "122640",
    "end": "128479"
  },
  {
    "text": "the the the packet start to go through all the ker Hooks and all perform all",
    "start": "128479",
    "end": "134560"
  },
  {
    "text": "these operations so our performance is going to be the size of the packet divided by the time that the packet",
    "start": "134560",
    "end": "141040"
  },
  {
    "text": "takes to get to the other P right simple then what happens with Services",
    "start": "141040",
    "end": "148519"
  },
  {
    "text": "Services is a we're talking about L4 Services right what services is going to do is we are",
    "start": "148519",
    "end": "155239"
  },
  {
    "text": "going to implement Services doing that typically you implement doing that right let's focus on the most common",
    "start": "155239",
    "end": "160360"
  },
  {
    "text": "implementation and let's focus on the standard implementation that's the one of Q proy how Q Pro Implement Services",
    "start": "160360",
    "end": "166680"
  },
  {
    "text": "is but doing datat right so the packet will go out to the port do his way and",
    "start": "166680",
    "end": "172560"
  },
  {
    "text": "we reach the datat stage in the pr routing hook there are the Q Pro",
    "start": "172560",
    "end": "178159"
  },
  {
    "text": "implementation using the technology they want is going to switch the destination",
    "start": "178159",
    "end": "183720"
  },
  {
    "text": "IP that is going to be the cluster IP for the IP of the PO so the performance",
    "start": "183720",
    "end": "188840"
  },
  {
    "text": "in Services is going to be always maximum the P top performance minus the",
    "start": "188840",
    "end": "196319"
  },
  {
    "text": "time that these operations in the Deni hook are going to happen right but you can say okay but now we have BF if BF is",
    "start": "196319",
    "end": "203599"
  },
  {
    "text": "faster okay let's take a look to the ebpf implementation the most common implementation of ebpf for services",
    "start": "203599",
    "end": "209480"
  },
  {
    "text": "right what the BPF does is it performs the same operation of implementing datat",
    "start": "209480",
    "end": "216680"
  },
  {
    "text": "but in the QD Ingress traffic control hook right so if we see at the diagram",
    "start": "216680",
    "end": "223560"
  },
  {
    "text": "in the most common of the cases the the life of the packet is going to be the",
    "start": "223560",
    "end": "229439"
  },
  {
    "text": "same so the difference in performance between the implementation with proxy",
    "start": "229439",
    "end": "234560"
  },
  {
    "text": "another BPF implementation is going to be the time of perform this operation for not in the datat hook or in the",
    "start": "234560",
    "end": "241360"
  },
  {
    "text": "Ingress hook but we always he about this is we have this thing of accelerated or",
    "start": "241360",
    "end": "249799"
  },
  {
    "text": "improv performance in and that's right because we can take shortcuts I mean performance we know we cannot go beyond",
    "start": "249799",
    "end": "257199"
  },
  {
    "text": "the speed of light but at the end of the day our speed of light is limited by the",
    "start": "257199",
    "end": "262520"
  },
  {
    "text": "number of operations that the can to needs to do in the packet so if we shortcut this number of operations or or",
    "start": "262520",
    "end": "270280"
  },
  {
    "text": "we offload this number of operations we are able to have what we call a fast",
    "start": "270280",
    "end": "275360"
  },
  {
    "text": "path right we know here that we need to go to the other part we just avoid to go through the C",
    "start": "275360",
    "end": "283320"
  },
  {
    "text": "we can do this with ebpf we can do this with NS tables I have a p request in Q",
    "start": "283320",
    "end": "288919"
  },
  {
    "text": "proxy that implement this so this is an improvement of performance right so far",
    "start": "288919",
    "end": "296000"
  },
  {
    "text": "so good we know okay we know more or less how service performance work for",
    "start": "296000",
    "end": "301120"
  },
  {
    "text": "one packet for one service but this is not our reality right we have clusters and the cluster has noes and the example",
    "start": "301120",
    "end": "308360"
  },
  {
    "text": "that I was putting it was two PS in the same Noe what happen when you have something in the middle like this cloud",
    "start": "308360",
    "end": "314600"
  },
  {
    "text": "Who what is the performan of this cloud is consistent is not consistent and then",
    "start": "314600",
    "end": "319800"
  },
  {
    "text": "how do I measure this okay I can start to put some metrics in some place some Medics other place and then people start",
    "start": "319800",
    "end": "326080"
  },
  {
    "text": "to deploy gr more pots Services whatever and you have I mean I don't know the performance what is the performance here",
    "start": "326080",
    "end": "333360"
  },
  {
    "text": "because I cannot have all the connections and all the things in my cluster reporting metrics and analyzing",
    "start": "333360",
    "end": "339919"
  },
  {
    "text": "one by one so the The Proposal we have and this a methodology that I was sing",
    "start": "339919",
    "end": "346960"
  },
  {
    "text": "for a long time and NAD and I working with the tooling and NAD is going to present the results of this methodology",
    "start": "346960",
    "end": "353440"
  },
  {
    "text": "let's focus on what performance means to me what the performance of the application is right instead of",
    "start": "353440",
    "end": "359720"
  },
  {
    "text": "measuring all this connection H all these technical metrics let's get",
    "start": "359720",
    "end": "365600"
  },
  {
    "text": "metrics from that are ER influencing the user experience",
    "start": "365600",
    "end": "372360"
  },
  {
    "text": "and we ended with these four SLI right one is the programming latency the programming latency is since you do qctl",
    "start": "372360",
    "end": "379880"
  },
  {
    "text": "apply the object Services created to the time that the data plan Implement these",
    "start": "379880",
    "end": "385639"
  },
  {
    "text": "these rules this is especially important if you have a application that needs",
    "start": "385639",
    "end": "390880"
  },
  {
    "text": "fast startup or fast file over right you need then these rules to be applied quickly another s that we Define is the",
    "start": "390880",
    "end": "398319"
  },
  {
    "text": "first packing latency that's we can say responsiveness right if you want your application to connect to the other",
    "start": "398319",
    "end": "404120"
  },
  {
    "text": "application and come back quickly right is this you have interactive application or this is super important another",
    "start": "404120",
    "end": "411960"
  },
  {
    "text": "important meic that is more tricky to understand now will go through it later is the connec total latency this is",
    "start": "411960",
    "end": "420000"
  },
  {
    "text": "because one thing is all the not all the connections are the same so you have long connections your latency is going",
    "start": "420000",
    "end": "425879"
  },
  {
    "text": "to be very large compar if you have a small connections but that's not necessarily about and the last SLI that",
    "start": "425879",
    "end": "434800"
  },
  {
    "text": "we consider important is throughput this is very well known uh SLI and it's very",
    "start": "434800",
    "end": "440039"
  },
  {
    "text": "important you have heavy data application Right Storage AI checkpoints",
    "start": "440039",
    "end": "445680"
  },
  {
    "text": "databases this this you know your suut is not good this applications is going to suffer so what this s allows us is to",
    "start": "445680",
    "end": "455520"
  },
  {
    "text": "move to a more uh user facing ER metrics that we understand because we know how",
    "start": "455520",
    "end": "463039"
  },
  {
    "text": "they impact our application that doesn't mean that we don't need to do custom Benchmark right we we know the problem",
    "start": "463039",
    "end": "469759"
  },
  {
    "text": "where the problem is we can perform this synthetic Benchmark and that are very useful to understand this um how can I",
    "start": "469759",
    "end": "478280"
  },
  {
    "text": "say in in a controll scenario but the problem is that we need to know when",
    "start": "478280",
    "end": "483960"
  },
  {
    "text": "this is happening in production this is what we want right this when it's impa in us because it's it's happening in",
    "start": "483960",
    "end": "489280"
  },
  {
    "text": "production and we cannot reproduce in a controll scenario we still have the problem and no",
    "start": "489280",
    "end": "495080"
  },
  {
    "text": "solution so so far this was the introduction the technical introduction",
    "start": "495080",
    "end": "500440"
  },
  {
    "text": "and Nadia did a great job created a tool apply this methodology and she's going",
    "start": "500440",
    "end": "506080"
  },
  {
    "text": "to tell us all the results of this work",
    "start": "506080",
    "end": "511039"
  },
  {
    "text": "all right so let's take a look at how we can use that and we are going to look at this user-driven performance validation",
    "start": "511240",
    "end": "517760"
  },
  {
    "text": "as Antonio said it is kind of as opposed to some synthetic benchmarking scenario",
    "start": "517760",
    "end": "523039"
  },
  {
    "text": "that you could have so how could we do that right one way is to instrument the",
    "start": "523039",
    "end": "528760"
  },
  {
    "text": "network so that not your applications or a benchmarking client will report to you actually what is going on in the network",
    "start": "528760",
    "end": "534720"
  },
  {
    "text": "but you want the network to tell you what's happening and in this in this case you can run whichever application you want in your cluster so how can you",
    "start": "534720",
    "end": "542120"
  },
  {
    "text": "do that the way we went with is using contract events so contract is this Orange Box that you can see twice inside",
    "start": "542120",
    "end": "548920"
  },
  {
    "text": "the kernel and the good side about that is that many different data planes will",
    "start": "548920",
    "end": "554279"
  },
  {
    "text": "go through the box so this method works for many different things like IP tables NF tables even ovs cbpf many things will",
    "start": "554279",
    "end": "562560"
  },
  {
    "text": "still end up using it in the end and contract is nice because it generates events for every single Connection in",
    "start": "562560",
    "end": "568959"
  },
  {
    "text": "the class let's see what we can find there okay first of all there is a start",
    "start": "568959",
    "end": "574839"
  },
  {
    "text": "time which is the time when the first packet was seen and if you're familiar with the contract there is this seen",
    "start": "574839",
    "end": "580800"
  },
  {
    "text": "reply stage which means the first connect the first packet in the opposite direction was seen so how quickly you",
    "start": "580800",
    "end": "586240"
  },
  {
    "text": "receive the reply if you just use these two metrics you can evaluate this first packet latency which is the one of the",
    "start": "586240",
    "end": "593399"
  },
  {
    "text": "SLI Antonio mentioned before okay next one for TCP connections",
    "start": "593399",
    "end": "598640"
  },
  {
    "text": "contract connection track different stages of it and the important one that we will use is TCP fin so that is the",
    "start": "598640",
    "end": "605839"
  },
  {
    "text": "stage when the connection is being closed it means every all the data was already sent and we are finishing this",
    "start": "605839",
    "end": "612279"
  },
  {
    "text": "connection so if you also use the start Tim stamp and this TCP fin time you can evaluate the total connection",
    "start": "612279",
    "end": "619000"
  },
  {
    "text": "latency and the last one uh is the counter of the number of btes and",
    "start": "619000",
    "end": "624880"
  },
  {
    "text": "packets that are being sent in each Direction and using that and the total connection latency you can evaluate the",
    "start": "624880",
    "end": "631560"
  },
  {
    "text": "throughput so it's not exactly the kind of throughput as I per would report to you so it will not tell you per second",
    "start": "631560",
    "end": "636880"
  },
  {
    "text": "per minute but on average during like the connection time you know how many packets were sent so you can evaluate",
    "start": "636880",
    "end": "642600"
  },
  {
    "text": "the average throughput using that okay so how do you do that you can",
    "start": "642600",
    "end": "647639"
  },
  {
    "text": "listen to contract events via netlink socket so then you can aggregate this events report all the time stamps",
    "start": "647639",
    "end": "654440"
  },
  {
    "text": "evaluate the sis as I've just explained and then expose this metrics to Prometheus grafana dashboard and have a",
    "start": "654440",
    "end": "660639"
  },
  {
    "text": "nice visualization so there is an app under this QR code that actually does that and it is really simple to apply to",
    "start": "660639",
    "end": "668320"
  },
  {
    "text": "your cluster it has like a Demian set that can be created it has all the Prometheus grafana dashboards everything",
    "start": "668320",
    "end": "673639"
  },
  {
    "text": "that you can see and even more on this slide okay let's see how we can use that",
    "start": "673639",
    "end": "682160"
  },
  {
    "text": "so first of all does it actually even work right so I'm used let's say I'm",
    "start": "682160",
    "end": "687720"
  },
  {
    "text": "used to running my benchmarks I have my application Apache Benchmark is the one we're going to be using because it's kind of popular um so how does it",
    "start": "687720",
    "end": "695079"
  },
  {
    "text": "compare right so let's Ry just like the first test which is fairly simple so we just create 10,000 Services a couple of",
    "start": "695079",
    "end": "701959"
  },
  {
    "text": "backends and then we run a couple of client pods that will generate thousands of connections and we'll see what it",
    "start": "701959",
    "end": "707760"
  },
  {
    "text": "will show us so here we'll have two types of metrics one is as reported by",
    "start": "707760",
    "end": "713720"
  },
  {
    "text": "client so Apache Benchmark and this is the diagram let's take a closer look at it because we're going to see a couple",
    "start": "713720",
    "end": "719560"
  },
  {
    "text": "more of those um so this is the client metrix to be more precise it's connection time or it's like a version",
    "start": "719560",
    "end": "727079"
  },
  {
    "text": "of aache Benchmark of this first packet round trip time it is reported in milliseconds and what I'm showing here",
    "start": "727079",
    "end": "733120"
  },
  {
    "text": "is the quantiles because we generate thousands of connections and we cannot look at each one of them so we need to",
    "start": "733120",
    "end": "738920"
  },
  {
    "text": "aggregate this data somehow right and the quantiles I hope it's like a familiar concept to you like for example",
    "start": "738920",
    "end": "745680"
  },
  {
    "text": "0.5 means that the median value for connection latency was 10 milliseconds as reported by a Pacha Benchmark",
    "start": "745680",
    "end": "753279"
  },
  {
    "text": "here okay now let's see what the network tells us uh the in the networking metrix",
    "start": "753279",
    "end": "759720"
  },
  {
    "text": "we're using this contract tool that I've just mentioned and um this it's analog of round trip time for the first packet",
    "start": "759720",
    "end": "766120"
  },
  {
    "text": "is the sin reply metric you can see that the absolute value is much less than",
    "start": "766120",
    "end": "773639"
  },
  {
    "text": "what is reported by the client so the client says average is 10 millisecond while the Network says that average is",
    "start": "773639",
    "end": "780600"
  },
  {
    "text": "actually less than 1 millisecond and it kind of makes sense because whatever client reports to you depends on this",
    "start": "780600",
    "end": "787560"
  },
  {
    "text": "benchmarking app performance when it sees the reply how it actually time stamps that right and so many other",
    "start": "787560",
    "end": "793639"
  },
  {
    "text": "different things okay so that was a really quick",
    "start": "793639",
    "end": "799279"
  },
  {
    "text": "network uh the delay is less than one millisecond super fast because it's actually a kind cluster one node so all",
    "start": "799279",
    "end": "804839"
  },
  {
    "text": "pods are on the same node makes sense but usually the delay in the network is somewhat bigger right can I emulate that",
    "start": "804839",
    "end": "812160"
  },
  {
    "text": "yeah so I can add an output delay to the server pod of 3 milliseconds what that means is before sending a reply the",
    "start": "812160",
    "end": "819720"
  },
  {
    "text": "server will wait for 3 millisecond and only then the packet will be sent all right so let's see what the",
    "start": "819720",
    "end": "825519"
  },
  {
    "text": "client tells us after that you can see there are two measurements the first one the blue one is no delay and the red one",
    "start": "825519",
    "end": "831320"
  },
  {
    "text": "is added 3 millisecond of delay and the client actually thinks that the second one is faster not much",
    "start": "831320",
    "end": "839680"
  },
  {
    "text": "but a little bit and this Precision is also very limited because it only reports in milliseconds so it doesn't know anything about that and it also",
    "start": "839680",
    "end": "846600"
  },
  {
    "text": "could be explained I'm not an Apache Benchmark expert uh but could be explained with this extra delay that it",
    "start": "846600",
    "end": "851880"
  },
  {
    "text": "the time it actually takes for the client to record the",
    "start": "851880",
    "end": "856920"
  },
  {
    "text": "metrics and if we look at what the network says you can actually figure out",
    "start": "856920",
    "end": "862720"
  },
  {
    "text": "what happen to the network by looking at this metric so you can very clearly see",
    "start": "862720",
    "end": "867759"
  },
  {
    "text": "in this comparison the blue one is like almost invisible at this point but you can clearly see that the 3 millisecond",
    "start": "867759",
    "end": "873519"
  },
  {
    "text": "delay was added here which leads us to one of the outcomes that every benchmarking or measuring tool has its",
    "start": "873519",
    "end": "880440"
  },
  {
    "text": "limitations and you need to understand it really well before actually using the results for",
    "start": "880440",
    "end": "886839"
  },
  {
    "text": "something okay so the tool is nice it works let's see what we can do with that as members of Sig Network the",
    "start": "886839",
    "end": "895360"
  },
  {
    "text": "original goal we actually had is um comparing this new new mode of Q proxy using NF tables versus IP tables for",
    "start": "895360",
    "end": "903240"
  },
  {
    "text": "those of you who may not know uh Q proxy first of all it Implement services in kubernetes and",
    "start": "903240",
    "end": "909519"
  },
  {
    "text": "then the default mode for it was using IP tables before but now we are switching to its successor NF tables",
    "start": "909519",
    "end": "916680"
  },
  {
    "text": "which is supposed to be faster better and we like it more so if you want to",
    "start": "916680",
    "end": "922160"
  },
  {
    "text": "know more about this um change please join this talk tomorrow uh you can",
    "start": "922160",
    "end": "928480"
  },
  {
    "text": "hopefully scan the QR code while we are here um and then Casey and then will'll",
    "start": "928480",
    "end": "933600"
  },
  {
    "text": "be talking more about why that happened and how it actually went but getting back to our performance what we want to",
    "start": "933600",
    "end": "940240"
  },
  {
    "text": "know is for we want to make sure that the new mode of Q proxy performs better",
    "start": "940240",
    "end": "946000"
  },
  {
    "text": "or at least not worse than the previous right one we don't want to release a new default version that is actually worse",
    "start": "946000",
    "end": "952279"
  },
  {
    "text": "than the old one so we wanted to double check that right and here is the first",
    "start": "952279",
    "end": "958120"
  },
  {
    "text": "benchmarking that was actually done by Antonio you can also find it on GitHub it's all recorded which is really nice",
    "start": "958120",
    "end": "964440"
  },
  {
    "text": "because you can double check that um and the theory was that IP tables performance should be worse than NF",
    "start": "964440",
    "end": "972120"
  },
  {
    "text": "tables for two main reasons one is the programming latency so once again it's",
    "start": "972120",
    "end": "978360"
  },
  {
    "text": "the time that it takes for Q proxy to make sure all the services are actually translated into the network and they",
    "start": "978360",
    "end": "984720"
  },
  {
    "text": "work and the second one is the latency of the first packet so this round trip time of the first packet is supposed to",
    "start": "984720",
    "end": "991680"
  },
  {
    "text": "be worse for IP tables because there is this lineer searge that we'll talk about more in a",
    "start": "991680",
    "end": "998000"
  },
  {
    "text": "minute okay to check our Theory we need to generate some workload that will actually make sure that that's the case",
    "start": "998000",
    "end": "1004680"
  },
  {
    "text": "right so what can we do let's create a one service that has lots and lots of",
    "start": "1004680",
    "end": "1009959"
  },
  {
    "text": "end points 1,000k end points 100K points um and we'll measure the time to program",
    "start": "1009959",
    "end": "1016519"
  },
  {
    "text": "the data plane so that's how we'll see the programming latency and then the second one will",
    "start": "1016519",
    "end": "1023480"
  },
  {
    "text": "create one more service service B that will have a real server back end and",
    "start": "1023480",
    "end": "1028760"
  },
  {
    "text": "then we'll send the connection to that and that is how we're supposed to check the latency on the first packet so the",
    "start": "1028760",
    "end": "1034918"
  },
  {
    "text": "idea here for this latency part is that in IP tables this is how this lineer",
    "start": "1034919",
    "end": "1041240"
  },
  {
    "text": "search is supposed to happen so to find the back end for service B we expect IP",
    "start": "1041240",
    "end": "1047600"
  },
  {
    "text": "tables to look through through all 100K back ends for service a first and only",
    "start": "1047600",
    "end": "1052960"
  },
  {
    "text": "then come to the service B back end okay so we run exactly this we get",
    "start": "1052960",
    "end": "1058480"
  },
  {
    "text": "the results the conclusion is we're also using Apache Benchmark as a client to just generate the workload and see how",
    "start": "1058480",
    "end": "1064720"
  },
  {
    "text": "the connection goes and it actually times out with IP tables while it works with n of tables so NF tables wins",
    "start": "1064720",
    "end": "1071799"
  },
  {
    "text": "everything as we expect and we come to this conclusion that Q proxy end of table seems to solve all our problems",
    "start": "1071799",
    "end": "1079880"
  },
  {
    "text": "right but if you take a second look that is not exactly what happens so let's go",
    "start": "1080919",
    "end": "1087440"
  },
  {
    "text": "with the first stressing the linear search one that is not exactly how the search",
    "start": "1087440",
    "end": "1093039"
  },
  {
    "text": "works so if you look at the implementation actually every service has its own little chain that contains",
    "start": "1093039",
    "end": "1099159"
  },
  {
    "text": "all the backends of it so when you go and try to Ping service B you will not",
    "start": "1099159",
    "end": "1104799"
  },
  {
    "text": "actually look through all the 100K backends of service a so all you will look through is just two Services which",
    "start": "1104799",
    "end": "1111320"
  },
  {
    "text": "is actually fast and doesn't stress the linear search so what it brings us to is",
    "start": "1111320",
    "end": "1117880"
  },
  {
    "text": "designing the test workload is hard you need to really really understand what's happening and this is the note from",
    "start": "1117880",
    "end": "1124080"
  },
  {
    "text": "Antonio think about your workloads and stress your cluster with that instead of copy pasting from",
    "start": "1124080",
    "end": "1130520"
  },
  {
    "text": "internet especially copy pasting from my guest uh right but okay remember we had",
    "start": "1130520",
    "end": "1138480"
  },
  {
    "text": "the Cent right Apache Benchmark it timed out with IP tables but didn't with n of tables uh why is that let's take a look",
    "start": "1138480",
    "end": "1145760"
  },
  {
    "text": "at some metrics we report metrics right we should use them okay so this one is",
    "start": "1145760",
    "end": "1151240"
  },
  {
    "text": "the total number of Ip tables rules that are created you'll see you create this huge service with 100K connections the",
    "start": "1151240",
    "end": "1158640"
  },
  {
    "text": "number of rules goes up and stays there right looks like everything is configured immediately everything is fine let's look at another one so this",
    "start": "1158640",
    "end": "1166640"
  },
  {
    "text": "one is actually the programming latency of C proxy what you'll see here is worst case 2 seconds not too bad also fast",
    "start": "1166640",
    "end": "1173280"
  },
  {
    "text": "enough everything should be fine that is until you actually take a",
    "start": "1173280",
    "end": "1178360"
  },
  {
    "text": "look at the CPU usage metric that will actually kind of explain what is happening so you can see here when this",
    "start": "1178360",
    "end": "1185159"
  },
  {
    "text": "huge service is created the CPU usage goes up 100% And then it stays like that for 20 minutes if you are patient enough",
    "start": "1185159",
    "end": "1192679"
  },
  {
    "text": "you'll wait till it actually goes down which is where the configuration was actually finished but then why these two",
    "start": "1192679",
    "end": "1199880"
  },
  {
    "text": "previous metrics were reported that that's how Q proxy works right",
    "start": "1199880",
    "end": "1204960"
  },
  {
    "text": "there is one large transaction that actually takes 20 minutes and this metric is reported right before this",
    "start": "1204960",
    "end": "1211799"
  },
  {
    "text": "transaction starts and this one report is reported right after the transaction is done so you will only see this",
    "start": "1211799",
    "end": "1219720"
  },
  {
    "text": "Infinity value after 20 minutes when the transaction is done and then for this 20 minutes of while the transaction is",
    "start": "1219720",
    "end": "1225960"
  },
  {
    "text": "going you will have zero idea that something is wrong",
    "start": "1225960",
    "end": "1230639"
  },
  {
    "text": "okay so now we've what we've actually figured out with that case is that the configuration time or programming",
    "start": "1231200",
    "end": "1236919"
  },
  {
    "text": "latency for one service with 100K end points for n of tables a bit more than a minute for IP tables around 20",
    "start": "1236919",
    "end": "1243480"
  },
  {
    "text": "minutes okay so what if we run a Pacha Benchmark now when IP tables actually finish doing its this thing the result",
    "start": "1243480",
    "end": "1250640"
  },
  {
    "text": "will be the same as with n of tables because we didn't stress the linear search if you",
    "start": "1250640",
    "end": "1256280"
  },
  {
    "text": "remember and the outcome of that is that confirmation bias is real so when you have some Theory or you have some",
    "start": "1256280",
    "end": "1262919"
  },
  {
    "text": "expected result as soon as you actually get it you're happy to accept that is everything is done it works exactly as",
    "start": "1262919",
    "end": "1269600"
  },
  {
    "text": "you expect and you can go home okay now we figured out what was",
    "start": "1269600",
    "end": "1275559"
  },
  {
    "text": "wrong let's try that again so the first metric we'll try to stress is this first packet latency now",
    "start": "1275559",
    "end": "1281960"
  },
  {
    "text": "we know that what actually stresses that is the amount of services not the amount of endpoints so we run the test with",
    "start": "1281960",
    "end": "1289880"
  },
  {
    "text": "variable amount of services and see what we get okay which Q proxy do you think will",
    "start": "1289880",
    "end": "1297480"
  },
  {
    "text": "program this faster who thinks that's going to be enough of",
    "start": "1297480",
    "end": "1302440"
  },
  {
    "text": "tables IP tables",
    "start": "1302840",
    "end": "1309039"
  },
  {
    "text": "correct even though if you remember IP tables performance was much worse on the previous case in this case it's actually",
    "start": "1309039",
    "end": "1315960"
  },
  {
    "text": "the opposite so do not overgeneralize if programs one work load faster doesn't mean programs everything",
    "start": "1315960",
    "end": "1322559"
  },
  {
    "text": "faster okay so when of tables is worse what exactly happens there if you run 30,000 Services it just will never",
    "start": "1322559",
    "end": "1329799"
  },
  {
    "text": "converge at all and the reason for that is that IP tables has this really nice",
    "start": "1329799",
    "end": "1335080"
  },
  {
    "text": "mode that is called partial sync so it only applies on a per service basis and then when the service itself doesn't",
    "start": "1335080",
    "end": "1341440"
  },
  {
    "text": "change it will only apply the new changes while NF tables every time reconciles everything that it has so",
    "start": "1341440",
    "end": "1348320"
  },
  {
    "text": "what what should we do well we have to fix it for n of tables and there is the pool request that actually does that and",
    "start": "1348320",
    "end": "1354000"
  },
  {
    "text": "some of the results for that is n of with n of tables when you create 10,000",
    "start": "1354000",
    "end": "1359120"
  },
  {
    "text": "Services before it took 25 minutes and after this Improvement it takes only 9ine minutes and one other interesting",
    "start": "1359120",
    "end": "1366080"
  },
  {
    "text": "thing that you may not think about is when you just add one extra service so now you have 10,000 and one service how",
    "start": "1366080",
    "end": "1372760"
  },
  {
    "text": "much time it takes to apply this extra change so before it took 8 minutes and after that it takes 140 milliseconds",
    "start": "1372760",
    "end": "1380120"
  },
  {
    "text": "which also brings us to this outcome that RS light driven performance testing actually makes software better in the",
    "start": "1380120",
    "end": "1387480"
  },
  {
    "text": "end okay so we fixed s of tables at least we can run our tests finally right",
    "start": "1387480",
    "end": "1392600"
  },
  {
    "text": "so let's see which results we will get if you think we're close to the end we are not quite yet so this network metrix",
    "start": "1392600",
    "end": "1399440"
  },
  {
    "text": "for seen reply we run IP tables we increase the amount of services what we expect to see the more services we have",
    "start": "1399440",
    "end": "1405919"
  },
  {
    "text": "the worse the latencies right if you look at this diagram you will actually see the opposite which doesn't make any",
    "start": "1405919",
    "end": "1413880"
  },
  {
    "text": "sense and then you will go and try to figure out what this like contract start timestamp actually does and it will take",
    "start": "1413880",
    "end": "1420880"
  },
  {
    "text": "you some time probably but then you can find in the the only place where I could find that is in the comment of the patch",
    "start": "1420880",
    "end": "1427600"
  },
  {
    "text": "that actually introduced the timestamp and it will tell you that this timestamp",
    "start": "1427600",
    "end": "1432720"
  },
  {
    "text": "is done once the contract entry has been confirmed so if you can see there are two contract squares on the this on this",
    "start": "1432720",
    "end": "1439279"
  },
  {
    "text": "picture and what you expect to happen probably is for it to happen in the first place so as soon as it's seen but",
    "start": "1439279",
    "end": "1445520"
  },
  {
    "text": "what is actually happening is it is time stamped on this second in this second uh",
    "start": "1445520",
    "end": "1451520"
  },
  {
    "text": "contract square and what it means is it actually does not reflect the whole time it took for the service to be",
    "start": "1451520",
    "end": "1460120"
  },
  {
    "text": "implemented okay Colonel problem so what you do when you see that uh you go and find some Colonel friend for me that was",
    "start": "1460120",
    "end": "1466360"
  },
  {
    "text": "floran westal thanks a lot for to him for this patch that you can also find um",
    "start": "1466360",
    "end": "1471520"
  },
  {
    "text": "via this QR code so what we've done is that uh we have introduced time stamps",
    "start": "1471520",
    "end": "1476600"
  },
  {
    "text": "for all contract events which means that now you can know when everything",
    "start": "1476600",
    "end": "1482120"
  },
  {
    "text": "happened in contract with kernel Precision which is really good okay and then if you patch your",
    "start": "1482120",
    "end": "1489600"
  },
  {
    "text": "kernel disclaimer all the other results are uh obtained with the patched kernel",
    "start": "1489600",
    "end": "1495880"
  },
  {
    "text": "this is finally the result that you can't you kind expect see so we increase the amount of services for IP tables and",
    "start": "1495880",
    "end": "1502360"
  },
  {
    "text": "the the delay for the first packet also grows so you can actually see this lineer growth here",
    "start": "1502360",
    "end": "1509240"
  },
  {
    "text": "finally and then if you do the same for NF tables mode you will see that it",
    "start": "1509240",
    "end": "1515240"
  },
  {
    "text": "actually stays kind of stable no matter how many services you have which is also kind of expected and",
    "start": "1515240",
    "end": "1521919"
  },
  {
    "text": "that's what we wanted to confirm now if we bring them both to the same diagram this is how it looks like",
    "start": "1521919",
    "end": "1528399"
  },
  {
    "text": "like so you can see that NF tables not only stays stable with the amount of services but it is also better",
    "start": "1528399",
    "end": "1535200"
  },
  {
    "text": "performance in almost every case than IP tables which is nice but here you can",
    "start": "1535200",
    "end": "1541399"
  },
  {
    "text": "ask me okay n isn't that confirmation bias you were talking about you've just got the results that you wanted and",
    "start": "1541399",
    "end": "1546880"
  },
  {
    "text": "happy with that if you can double check something always do that remember we",
    "start": "1546880",
    "end": "1551960"
  },
  {
    "text": "have this client side metric that Apache Benchmark is running this whole time and reports its own metrics so while",
    "start": "1551960",
    "end": "1557840"
  },
  {
    "text": "absolute value is very different as you can see here on the right you can see the metrix reported by Apache Benchmark",
    "start": "1557840",
    "end": "1563600"
  },
  {
    "text": "which goes up to 50 milliseconds which is not exactly the case but the pattern is really similar so if you take a look",
    "start": "1563600",
    "end": "1570000"
  },
  {
    "text": "it also the client also shows that the latency actually grows for IP tables with the amount of services and N of",
    "start": "1570000",
    "end": "1576559"
  },
  {
    "text": "table stays kind of stable okay so second s if you still",
    "start": "1576559",
    "end": "1584000"
  },
  {
    "text": "remember we had more than one is the connection time so for this again Apache",
    "start": "1584000",
    "end": "1589279"
  },
  {
    "text": "Benchmark generated connections they are very short they just connect send one request get one reply done and this for",
    "start": "1589279",
    "end": "1595760"
  },
  {
    "text": "such short connections you can see that the whole the whole duration of a connection is actually pretty much",
    "start": "1595760",
    "end": "1602360"
  },
  {
    "text": "affected by this first packet DeLay So that is kind of what you can see here",
    "start": "1602360",
    "end": "1608880"
  },
  {
    "text": "all right getting to the next one throughput we need a different type of workload to test throughput obviously so",
    "start": "1608880",
    "end": "1615840"
  },
  {
    "text": "what we'll do is we use iper server as a back end and then we'll create 10 clients that will all run one connection",
    "start": "1615840",
    "end": "1623000"
  },
  {
    "text": "and then again the same variables will check how it works with different amount of",
    "start": "1623000",
    "end": "1628679"
  },
  {
    "text": "services okay this one is quick no surprises after we figured out all the previous ones uh the throughput is",
    "start": "1628679",
    "end": "1635279"
  },
  {
    "text": "actually for large connection sending a lot of data is almost the same for all test cases no matter what back end",
    "start": "1635279",
    "end": "1641720"
  },
  {
    "text": "you're using so this is the part when the new Q proxy performs at least not worse",
    "start": "1641720",
    "end": "1649399"
  },
  {
    "text": "okay one more thing you may want to be interested in for your cluster around throughput is throughput fairness so the",
    "start": "1650399",
    "end": "1658279"
  },
  {
    "text": "network in a cluster is a shared resource and when you have multiple connections you want this resource to be",
    "start": "1658279",
    "end": "1663880"
  },
  {
    "text": "distributed fairly now you can see here there is one extra test at the very",
    "start": "1663880",
    "end": "1670399"
  },
  {
    "text": "bottom and you can see that the value that is reported for this test is actually the throughput is much smaller",
    "start": "1670399",
    "end": "1677279"
  },
  {
    "text": "for the first and 10th quantiles that is because I tweaked something in the",
    "start": "1677279",
    "end": "1683080"
  },
  {
    "text": "network or in the test case I would say you can take a second to think what that",
    "start": "1683080",
    "end": "1689000"
  },
  {
    "text": "could be because that's how we expect the Matrix to use right we want to see The Matrix and figure out what that means for our",
    "start": "1689000",
    "end": "1696279"
  },
  {
    "text": "Network and this is the result you'll get with like unfair bandwidth distribution so what I've done is one",
    "start": "1696679",
    "end": "1702519"
  },
  {
    "text": "client out of 10 gets a bandwidth limit so it cannot send as much data as all the other ones and then if you just get",
    "start": "1702519",
    "end": "1710399"
  },
  {
    "text": "all the results for 10 connections this is kind of what you'll see so the first one is really small and then everything",
    "start": "1710399",
    "end": "1715840"
  },
  {
    "text": "else is around 2 gigabytes per second and is very stable so this is",
    "start": "1715840",
    "end": "1721840"
  },
  {
    "text": "exactly what you see on this previous distribution quantal so you can see that something is not fairly distributed in",
    "start": "1721840",
    "end": "1728840"
  },
  {
    "text": "your network okay after all this tests what we can actually conclude that nft tables",
    "start": "1728840",
    "end": "1735679"
  },
  {
    "text": "Cube proxy has indeed better performance cized on IP tables for everything that we've",
    "start": "1735679",
    "end": "1743880"
  },
  {
    "text": "tested there is a little disclaimer there is a little case at least that I already know about that may not work",
    "start": "1743880",
    "end": "1750880"
  },
  {
    "text": "better for R of tables yet but we'll fix that in the future oh you have so just to wrap up",
    "start": "1750880",
    "end": "1758519"
  },
  {
    "text": "where where where what we do here is as s n kubernetes project we are really",
    "start": "1758519",
    "end": "1764519"
  },
  {
    "text": "worried about us right we develop an implementation of services and we focus",
    "start": "1764519",
    "end": "1770080"
  },
  {
    "text": "on um user facing metcs we are standardizing this metcs so all of you",
    "start": "1770080",
    "end": "1776200"
  },
  {
    "text": "can run in your classes in productiv and we want to hear more about you we are not focusing in technology we use it",
    "start": "1776200",
    "end": "1782200"
  },
  {
    "text": "this metrics Nadia created a reference implementation of this metrics and she",
    "start": "1782200",
    "end": "1787240"
  },
  {
    "text": "found one or two bucks in the kernel two or three bucks in Q proxy so this is clear that this metric works and now and",
    "start": "1787240",
    "end": "1796080"
  },
  {
    "text": "every project can implement this meic so as I say run these mecs are very simple",
    "start": "1796080",
    "end": "1803919"
  },
  {
    "text": "and if you have any problems come to see networ or open issues in you have and let us know right because we are",
    "start": "1803919",
    "end": "1809320"
  },
  {
    "text": "interested that in standardize this this way of measuring performance across the ecosystem and that's",
    "start": "1809320",
    "end": "1816320"
  },
  {
    "text": "all thank you for",
    "start": "1816320",
    "end": "1820000"
  },
  {
    "text": "coming okay we have around five more minutes for questions if you have",
    "start": "1824240",
    "end": "1830399"
  },
  {
    "text": "any question is what is the test case that you know of already that's a little slow I have to know you just piqued my",
    "start": "1837440",
    "end": "1845240"
  },
  {
    "text": "curiosity right um yeah so this one is actually what happens on uh Q proxy pod",
    "start": "1845240",
    "end": "1852399"
  },
  {
    "text": "restart so when you restart the Pod it has to like reconcile everything that it has and also and tables doesn't do as",
    "start": "1852399",
    "end": "1859159"
  },
  {
    "text": "well yet and there was actually there is already a fixed kernel bug for that because apparently n of tables was not",
    "start": "1859159",
    "end": "1865440"
  },
  {
    "text": "really optimal in like reconciling all the rules at the same time which I have don't have the scale test results for",
    "start": "1865440",
    "end": "1872000"
  },
  {
    "text": "yet so I cannot tell you how much better it is now and there is also of course something that you can improve in Q proxy code itself to also make this case",
    "start": "1872000",
    "end": "1878679"
  },
  {
    "text": "Works work better but doesn't mean there are no more cases that work worse cannot",
    "start": "1878679",
    "end": "1885120"
  },
  {
    "text": "cannot promise yes sure want to use the mic you have a",
    "start": "1885120",
    "end": "1891240"
  },
  {
    "text": "mic you have a mic behind you oh on the on the",
    "start": "1891240",
    "end": "1897360"
  },
  {
    "text": "corridor thanks how bad the difference actually is uh right now between Cube",
    "start": "1899000",
    "end": "1906519"
  },
  {
    "text": "Pro uh between uh NFD and the the regular",
    "start": "1906519",
    "end": "1914519"
  },
  {
    "text": "one like you there is a bug right it's going to Oh you mean this restart case",
    "start": "1914519",
    "end": "1919880"
  },
  {
    "text": "yeah oh I knew people will stick to that um you know what I can I cannot",
    "start": "1919880",
    "end": "1926120"
  },
  {
    "text": "actually I cannot give you the numbers now because it was quite some time since I've checked that but it also depends on",
    "start": "1926120",
    "end": "1932360"
  },
  {
    "text": "the workload right how many services how many backends each service has like this will vary but the difference was kind of",
    "start": "1932360",
    "end": "1938760"
  },
  {
    "text": "bad so worth fixing let's say but we're talking here about Mega services and",
    "start": "1938760",
    "end": "1943840"
  },
  {
    "text": "mega importance right this this is not ordinary this is a very strek right 10K",
    "start": "1943840",
    "end": "1950559"
  },
  {
    "text": "Services is very weird I only see a few cases in production of this",
    "start": "1950559",
    "end": "1957880"
  }
]