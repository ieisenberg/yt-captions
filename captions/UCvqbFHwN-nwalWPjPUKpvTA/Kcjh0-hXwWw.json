[
  {
    "text": "hi everyone welcome thank you for coming to my talk uh this is the museum of weird bugs uh this is uh our favorites",
    "start": "0",
    "end": "7600"
  },
  {
    "text": "from eight years of service mesh debugging on lingardy or you could have",
    "start": "7600",
    "end": "13200"
  },
  {
    "text": "alternately titled this each of these took a year off my life um there's some nasty some nasty bugs in here so I hope",
    "start": "13200",
    "end": "19680"
  },
  {
    "text": "you enjoy um so my name is Alex Leong i am a maintainer on the link project i've been",
    "start": "19680",
    "end": "26640"
  },
  {
    "text": "working on linkerd for eight years since the project began um and so I've seen",
    "start": "26640",
    "end": "32078"
  },
  {
    "text": "and created a lot of nasty bugs in that time even though I've heard it's official link policy that link should",
    "start": "32079",
    "end": "38160"
  },
  {
    "text": "have no bugs you know here we are um so if you're not familiar with what",
    "start": "38160",
    "end": "43600"
  },
  {
    "text": "linkertd is linkertd is a service mesh uh service mesh was a really really big buzzword a few years ago and I think now",
    "start": "43600",
    "end": "49840"
  },
  {
    "text": "has kind of descended into the the realm of being just boring technology which is great um I love that um it has been in",
    "start": "49840",
    "end": "58079"
  },
  {
    "text": "use for for over eight years um and it's been in use by a lot of different people",
    "start": "58079",
    "end": "63680"
  },
  {
    "text": "in a lot of different environments at a lot of different scales um and so that's",
    "start": "63680",
    "end": "68799"
  },
  {
    "text": "made uh kind of a ripe environment for a lot of different bugs to be exercised",
    "start": "68799",
    "end": "74000"
  },
  {
    "text": "and to occur and to happen in really weird ways that are very difficult to reproduce um and that also makes things",
    "start": "74000",
    "end": "80640"
  },
  {
    "text": "very challenging um so I wanted to just start by giving a",
    "start": "80640",
    "end": "85840"
  },
  {
    "text": "little bit of background on linkerd and the highle architecture very high level uh I don't want to get too much into it",
    "start": "85840",
    "end": "92079"
  },
  {
    "text": "but I want to make sure we have enough shared context and we're on the same page in order to be able to talk about these bugs and how they kind of relate",
    "start": "92079",
    "end": "99040"
  },
  {
    "text": "to the link architecture um and then I've kind of selected a a tasting of of",
    "start": "99040",
    "end": "104799"
  },
  {
    "text": "three bugs for us to go through uh each of these is very very delicious um I",
    "start": "104799",
    "end": "110960"
  },
  {
    "text": "think we probably will only have time to get through two of them we'll see how it goes uh they they each deserve their",
    "start": "110960",
    "end": "117439"
  },
  {
    "text": "fair share of time uh but we'll see uh so just to give some highlevel uh",
    "start": "117439",
    "end": "125520"
  },
  {
    "text": "architecture of of how linkerdy works um on the right hand side is what we call",
    "start": "125520",
    "end": "130959"
  },
  {
    "text": "the data plane and so that's where your application in Kubernetes is running each of those yellow boxes on the right",
    "start": "130959",
    "end": "136560"
  },
  {
    "text": "there is a pod and inside each pod is your applications container and then",
    "start": "136560",
    "end": "141840"
  },
  {
    "text": "additionally there is uh the linkd proxy which also runs as a sidecar container in those pods and that sidecar container",
    "start": "141840",
    "end": "149599"
  },
  {
    "text": "is written in rust that's the linker microproxy uh that runs alongside and handles all of your network traffic and",
    "start": "149599",
    "end": "156640"
  },
  {
    "text": "in order to do so whenever you try to send a request to some service linkd needs information about how to route",
    "start": "156640",
    "end": "162560"
  },
  {
    "text": "that request what kind of policy to apply to it and so on and so on and in order to get that information it reaches",
    "start": "162560",
    "end": "168959"
  },
  {
    "text": "out to the linkd control plane which is running in a dedicated name space and it does that with a gRPC streaming API so",
    "start": "168959",
    "end": "175440"
  },
  {
    "text": "it says hey I want to talk to this service please tell me all about it and furthermore keep me updated as that",
    "start": "175440",
    "end": "182159"
  },
  {
    "text": "information changes so if new endpoints come up and get added to that service or if the policy changes on that service",
    "start": "182159",
    "end": "188159"
  },
  {
    "text": "stream those responses down to me so that I can stay up to date and treat that traffic properly",
    "start": "188159",
    "end": "194319"
  },
  {
    "text": "uh so inside the control plane which is that middle yellow box there are mainly",
    "start": "194319",
    "end": "199519"
  },
  {
    "text": "two different processes that run there uh there's the destination controller which is that first blue box uh and that",
    "start": "199519",
    "end": "206480"
  },
  {
    "text": "is written in go and that provides um destination information to the proxy so",
    "start": "206480",
    "end": "212640"
  },
  {
    "text": "things like endpoint addresses and uh TLS identities and stuff like that um",
    "start": "212640",
    "end": "217920"
  },
  {
    "text": "and that information is all streamed down to the proxy over that gRPC API uh the second box there is the",
    "start": "217920",
    "end": "224239"
  },
  {
    "text": "policy controller which is actually written in Rust um and that's new a newer component which is why it's it's",
    "start": "224239",
    "end": "229680"
  },
  {
    "text": "written in a different language we made this decision a few years ago to start writing all of our new control plane components in Rust because we felt that",
    "start": "229680",
    "end": "236319"
  },
  {
    "text": "the the ecosystem there was mature enough and we were ready to start doing that um and that serves policy",
    "start": "236319",
    "end": "242319"
  },
  {
    "text": "information down to the proxies so information like is this request authorized can this service talk to that",
    "start": "242319",
    "end": "248239"
  },
  {
    "text": "other service um and how should routing be done are there uh routing overrides that we need to apply to this traffic",
    "start": "248239",
    "end": "255200"
  },
  {
    "text": "and so that information comes down from the policy controller to the proxy now both of those controllers the",
    "start": "255200",
    "end": "260400"
  },
  {
    "text": "destination controller and the policy controller get all the information they need from the Kubernetes API so they're",
    "start": "260400",
    "end": "266160"
  },
  {
    "text": "establishing HTTP watches on the Kubernetes API and listening for updates",
    "start": "266160",
    "end": "271360"
  },
  {
    "text": "about pods about service accounts about services about endpoints and all that",
    "start": "271360",
    "end": "276479"
  },
  {
    "text": "information is coming into them they're synthesizing that and turning it into something the proxy can use and",
    "start": "276479",
    "end": "282000"
  },
  {
    "text": "streaming it down to the proxy that's your super high level introduction to",
    "start": "282000",
    "end": "287199"
  },
  {
    "text": "the linkerd architecture um and in in addition to",
    "start": "287199",
    "end": "293280"
  },
  {
    "text": "kind of just serving that information these controllers do something a little bit more which is they manage some CRDs",
    "start": "293280",
    "end": "299360"
  },
  {
    "text": "and so uh linkerd uh integrates with the gateway API the gateway API is a set of",
    "start": "299360",
    "end": "305120"
  },
  {
    "text": "CRDs that includes um gateways and gateway classes and a bunch of others that we don't use but we do use HTTP",
    "start": "305120",
    "end": "311840"
  },
  {
    "text": "routes gRPC routes TLS routes TCP routes uh and those are all CRDs that come from",
    "start": "311840",
    "end": "317440"
  },
  {
    "text": "the gateway API uh that we interact with uh there's also a set of linky policy",
    "start": "317440",
    "end": "323199"
  },
  {
    "text": "CRDs that we use to describe uh policy that link should act on like whether",
    "start": "323199",
    "end": "328560"
  },
  {
    "text": "things are whether requests should be accepted or denied and in what conditions so we have things like servers authorization policies and HTTP",
    "start": "328560",
    "end": "335680"
  },
  {
    "text": "routes uh under the policy.io group which are kind of",
    "start": "335680",
    "end": "340800"
  },
  {
    "text": "linkard's uh CRDs and you may notice there's a little bit of duplication here",
    "start": "340800",
    "end": "346800"
  },
  {
    "text": "uh there's an HTTP route CRD from the gateway API there's also an HTTP route",
    "start": "346800",
    "end": "352000"
  },
  {
    "text": "CRD from the linkerd group uh and the reason for that is because at the time",
    "start": "352000",
    "end": "357840"
  },
  {
    "text": "that we were implementing this the gateway API CRDs didn't yet support the timeout field and we really wanted to",
    "start": "357840",
    "end": "364160"
  },
  {
    "text": "implement timeouts in this way so we forked the HTTP route CRD into the",
    "start": "364160",
    "end": "370680"
  },
  {
    "text": "policy.io group in order to get support for for timeouts and uh so now we",
    "start": "370680",
    "end": "377120"
  },
  {
    "text": "support both we those those two CRDs are virtually identical uh virtually identical the linky one has the timeout",
    "start": "377120",
    "end": "383199"
  },
  {
    "text": "field actually the gateway API one now has the timeout field too it didn't at the time now it does um but we now have",
    "start": "383199",
    "end": "388720"
  },
  {
    "text": "this kind of duplicate uh CRD this fork and we're in the process of kind of phasing that out we want to get back",
    "start": "388720",
    "end": "395039"
  },
  {
    "text": "onto the standard upstream uh gateway API CRDs for everyone but we want to do that in a backwards compatible way so",
    "start": "395039",
    "end": "401600"
  },
  {
    "text": "that's a a multi-phased um deprecation so it's the linkd policy",
    "start": "401600",
    "end": "408400"
  },
  {
    "text": "controller that's the one that was written in Rust that kind of handles all of the CRD management and the three",
    "start": "408400",
    "end": "413600"
  },
  {
    "text": "things that it it does with respect to these CRDs is one it validates them so it has a uh validation web hook where",
    "start": "413600",
    "end": "420720"
  },
  {
    "text": "when any resource of these types is created it's going to look at them and make sure that they're valid according to uh a certain set of rules and it'll",
    "start": "420720",
    "end": "428240"
  },
  {
    "text": "say yes this is allowed or no that's not um it obviously watches these resources because these all inform the policy that",
    "start": "428240",
    "end": "435199"
  },
  {
    "text": "linkd needs to act on so server servers and authorization policies and HTTP routes it watches all these resources",
    "start": "435199",
    "end": "441840"
  },
  {
    "text": "and uses that information to synthesize the data that it sends down to the proxy and it also updates the status on each",
    "start": "441840",
    "end": "448639"
  },
  {
    "text": "of these resources so each of these resources has a status subfield and the",
    "start": "448639",
    "end": "454560"
  },
  {
    "text": "uh policy controller will figure out hey is this resource accepted because it's it's all good and all of its things that",
    "start": "454560",
    "end": "460160"
  },
  {
    "text": "it references exist and it's valid so we'll accept it or uh if it references",
    "start": "460160",
    "end": "466000"
  },
  {
    "text": "things that don't exist or it's invalid in some way we will uh update the status field to say this has not been accepted",
    "start": "466000",
    "end": "471759"
  },
  {
    "text": "it's you know has an invalid back end or something like that",
    "start": "471759",
    "end": "476960"
  },
  {
    "text": "okay so I think that's all the background we need we can now move on to the fun part which is the bugs uh and",
    "start": "476960",
    "end": "483599"
  },
  {
    "text": "this first bug has to do with CRDs so here is a graph of policy",
    "start": "483599",
    "end": "490800"
  },
  {
    "text": "controller memory usage uh that's that orange line that's going up and up and up and up which is not what you want",
    "start": "490800",
    "end": "496080"
  },
  {
    "text": "your memory to look like uh this clearly looks like some kind of memory leak uh policy controller I think at the end",
    "start": "496080",
    "end": "502479"
  },
  {
    "text": "there is using 780 megabytes which is too many megabytes um so yeah this looks",
    "start": "502479",
    "end": "509280"
  },
  {
    "text": "like a policy controller memory leak of some kind it gets oo killed at at some point by Kubernetes something is clearly",
    "start": "509280",
    "end": "515360"
  },
  {
    "text": "wrong here um and we also when we saw reports of this bug again this is not a bug that we",
    "start": "515360",
    "end": "521680"
  },
  {
    "text": "initially found ourselves almost all of these bugs are things that are reported to us by users who are using linkerd in",
    "start": "521680",
    "end": "527920"
  },
  {
    "text": "configurations that are different than what we experience as linkd developers you know they're running at larger",
    "start": "527920",
    "end": "533680"
  },
  {
    "text": "scales they're running in uh different kind of cloud environments um and so a lot of the debugging we have to do on",
    "start": "533680",
    "end": "539600"
  },
  {
    "text": "linkerty is very like word of mouth telephone you know you run this command tell us what you see um or these things",
    "start": "539600",
    "end": "547519"
  },
  {
    "text": "could only be reproduced after linkery was running for a week when the moon was in this phase or whatever right these",
    "start": "547519",
    "end": "553120"
  },
  {
    "text": "are things are very difficult to reproduce so in this case they they came back to with a report saying that hey we",
    "start": "553120",
    "end": "560080"
  },
  {
    "text": "saw this memory leak we also saw that whenever we created an HTTP route the status was updated to accepted but it",
    "start": "560080",
    "end": "566720"
  },
  {
    "text": "took 40 minutes and and that's pretty weird because why would that take 40 minutes that that should be updated",
    "start": "566720",
    "end": "572959"
  },
  {
    "text": "instantly either accepted or not um something very strange is going on uh",
    "start": "572959",
    "end": "578880"
  },
  {
    "text": "and we also saw a bunch of uh spam error messages in the logs so we saw failed to patch HTTP route no available capacity",
    "start": "578880",
    "end": "587440"
  },
  {
    "text": "and so these are kind of all the clues we had to try to dig into like what is going on here um it seems like we're",
    "start": "587440",
    "end": "594000"
  },
  {
    "text": "trying to patch these HTTP routes but somehow the Kubernetes API is not able to keep up with that those patches",
    "start": "594000",
    "end": "599680"
  },
  {
    "text": "aren't going through why would that be the case this is pretty weird um and so we kind of dug into well what",
    "start": "599680",
    "end": "607360"
  },
  {
    "text": "what are the steps here you know in what cases do we patch these resources and why might that fail and so the general",
    "start": "607360",
    "end": "613760"
  },
  {
    "text": "logic here is uh well number one the policy controller is going to establish watches on all of these different",
    "start": "613760",
    "end": "619920"
  },
  {
    "text": "resource types uh and then number two because it's established watches it's going to",
    "start": "619920",
    "end": "625360"
  },
  {
    "text": "receive updates anytime one of those resources is created or is updated",
    "start": "625360",
    "end": "631120"
  },
  {
    "text": "um when it does receive an update it's going to look at that resource and it's going to compute well what should the",
    "start": "631120",
    "end": "636640"
  },
  {
    "text": "status on this resource be you know are all its backends valid do they exist and",
    "start": "636640",
    "end": "642320"
  },
  {
    "text": "what should the status be should it be accepted or should it be not accepted and then once it's determined",
    "start": "642320",
    "end": "647680"
  },
  {
    "text": "what that status should be it can compare it to what the status is and if there's a difference then it'll issue an",
    "start": "647680",
    "end": "653680"
  },
  {
    "text": "update to the Kubernetes API saying \"Hey we need to update the status here's what it should be.\" This makes sense",
    "start": "653680",
    "end": "661120"
  },
  {
    "text": "um and so maybe it's that that the last step that updating the status that's where we're patching the resource maybe",
    "start": "661120",
    "end": "666480"
  },
  {
    "text": "that's where this problem is occurring so let's dig a little deeper into how that",
    "start": "666480",
    "end": "673040"
  },
  {
    "text": "works so on the left is uh a little snippet from the CRD the custom resource",
    "start": "673320",
    "end": "679200"
  },
  {
    "text": "definition uh specifically inside the status subfield of the",
    "start": "679200",
    "end": "685640"
  },
  {
    "text": "httpouts.policy.linkery.io CRD um and then so inside the status there's a list of parents so we say what the status is",
    "start": "685640",
    "end": "692399"
  },
  {
    "text": "for each of the parents that that HTTP route is attached to and for each of those parents we have listed out its",
    "start": "692399",
    "end": "698560"
  },
  {
    "text": "group its kind its name its name space and its section name and then on the right hand side we",
    "start": "698560",
    "end": "705200"
  },
  {
    "text": "have the uh way that this is represented in the policy controller which is written in Rust uses Rust kind of",
    "start": "705200",
    "end": "711839"
  },
  {
    "text": "bindings to to interact with this API in Kubernetes um and so there's a strct",
    "start": "711839",
    "end": "717440"
  },
  {
    "text": "that we have that represents these parent references and the strct has a bunch of fields one for the group one",
    "start": "717440",
    "end": "722560"
  },
  {
    "text": "for the kind namespace name section name and port and if you kind of blur your",
    "start": "722560",
    "end": "727600"
  },
  {
    "text": "eyes and try to spot the difference uh there's a field on the right that doesn't exist there on the left so",
    "start": "727600",
    "end": "734240"
  },
  {
    "text": "there's this kind of mismatch between what's defined in the custom resource definition and what's in the strct uh",
    "start": "734240",
    "end": "740720"
  },
  {
    "text": "how it's represented and so what happens as a result of this",
    "start": "740720",
    "end": "746079"
  },
  {
    "text": "mismatch is that the Kubernetes API has this object that has no port in that uh",
    "start": "746079",
    "end": "753200"
  },
  {
    "text": "in that status because that's not a field it knows about and so whenever one of those resources is updated or created",
    "start": "753200",
    "end": "760000"
  },
  {
    "text": "uh it'll send an update to the policy controller saying \"Hey you're watching for updates on this resource here here's",
    "start": "760000",
    "end": "765120"
  },
  {
    "text": "an update here's a status.\" Uh it doesn't have a port because that's not a thing uh the policy controller will say",
    "start": "765120",
    "end": "771040"
  },
  {
    "text": "\"Hey okay great thanks for the update i'm going to compute what the status on this should be and because of the way",
    "start": "771040",
    "end": "776320"
  },
  {
    "text": "the code is written it's going to have a port in that status field and it's going to compare those two and say \"Hey",
    "start": "776320",
    "end": "782320"
  },
  {
    "text": "there's a difference here this one has a port this one doesn't so I'm going to issue an update and I'm going to write that to the Kubernetes API.\" Kubernetes",
    "start": "782320",
    "end": "789360"
  },
  {
    "text": "API says \"Great thanks for that update i'm going to take this i don't know what to do with this port field because that's not part of the CRD so we're just",
    "start": "789360",
    "end": "795279"
  },
  {
    "text": "going to throw that away.\" Um and so it'll store it uh in Kubernetes without the port that resource just got updated",
    "start": "795279",
    "end": "802079"
  },
  {
    "text": "so that update will go back to the policy controller without the port and you have an infinite loop where we're just going to spam the the Kubernetes",
    "start": "802079",
    "end": "808560"
  },
  {
    "text": "API over and over and over again with these uh updates that get",
    "start": "808560",
    "end": "813959"
  },
  {
    "text": "ignored uh and so that causes this ballooning of memory to happen that causes the policy controller to",
    "start": "813959",
    "end": "819839"
  },
  {
    "text": "eventually be oo killed the whole thing is so overloaded that any of these updates take 40 minutes in order to go",
    "start": "819839",
    "end": "824880"
  },
  {
    "text": "through before there's capacity it's a disaster",
    "start": "824880",
    "end": "830720"
  },
  {
    "text": "so fixing this was very easy once we once we determined what was going wrong um all we had to do was was correct the",
    "start": "830720",
    "end": "837600"
  },
  {
    "text": "CRD to add that missing field um and the reason that that field was even missing in the first place is because the time",
    "start": "837600",
    "end": "844720"
  },
  {
    "text": "at which we forked the uh gateway API CRD that that field did not exist but",
    "start": "844720",
    "end": "850959"
  },
  {
    "text": "when we generated the Rust bindings at a later time uh based on the gateway API CRDs that field did exist so there was",
    "start": "850959",
    "end": "857920"
  },
  {
    "text": "this mismatch in time between when we forked the CRD and when we generated the bindings and that caused this mismatch",
    "start": "857920",
    "end": "864480"
  },
  {
    "text": "here uh which was very nasty uh but once we fixed it everything worked magically again and and life was",
    "start": "864480",
    "end": "872120"
  },
  {
    "text": "good so the lesson here I think to learn is uh number one be careful with CRDs um",
    "start": "872120",
    "end": "878480"
  },
  {
    "text": "just because you write a resource to Kubernetes doesn't mean that it gets persisted that way uh you can even write",
    "start": "878480",
    "end": "884240"
  },
  {
    "text": "a resource to Kubernetes and then read that resource immediately back and these might not be the same thing so the CRD",
    "start": "884240",
    "end": "890320"
  },
  {
    "text": "is kind of the ultimate authority on which fields exist and and will get persisted to Kubernetes so it's it's",
    "start": "890320",
    "end": "896800"
  },
  {
    "text": "something that has bitten me more than once um and I think the other lesson is like don't maintain CR forks uh if we",
    "start": "896800",
    "end": "904800"
  },
  {
    "text": "could get out of this world uh more quickly I think we would and and we're in the process so uh hopefully very soon",
    "start": "904800",
    "end": "910959"
  },
  {
    "text": "we'll just be using the gateway API CRDs and and not have to maintain uh these forked ones that can kind of get out of",
    "start": "910959",
    "end": "917279"
  },
  {
    "text": "sync okay so that's that's bug number one let's let's move on to bug number two uh",
    "start": "917279",
    "end": "925440"
  },
  {
    "text": "this is probably of the three my favorite because this is just a very interesting confluence of a bunch of",
    "start": "925440",
    "end": "930560"
  },
  {
    "text": "different systems i think that's one of the really interesting things about linkerd and why some of these bugs are so interesting is because linkerdy as a",
    "start": "930560",
    "end": "937360"
  },
  {
    "text": "service mesh sits right at the center of uh network protocols and the kubernetes",
    "start": "937360",
    "end": "944240"
  },
  {
    "text": "API and CRDs and your application and all these different things kind of",
    "start": "944240",
    "end": "949680"
  },
  {
    "text": "interacting in sometimes unexpected and unpredictable ways",
    "start": "949680",
    "end": "955639"
  },
  {
    "text": "so uh this all starts with a bug report that says linkerd is routing to stale",
    "start": "956079",
    "end": "962519"
  },
  {
    "text": "addresses and anytime I hear this my heart sinks because these are so hard to",
    "start": "962519",
    "end": "968600"
  },
  {
    "text": "debug um so this is a class of bug where linkerd is routing to some address that",
    "start": "968600",
    "end": "974399"
  },
  {
    "text": "either doesn't exist anymore or is in use for something else in other words it's somehow missed an update about",
    "start": "974399",
    "end": "980480"
  },
  {
    "text": "service discovery um and that can manifest as connection refused because Linkerty is trying to connect to some IP",
    "start": "980480",
    "end": "986399"
  },
  {
    "text": "that's no longer in use nothing's listening there um or even worse it could be tried to connect to something",
    "start": "986399",
    "end": "991680"
  },
  {
    "text": "incorrect because that uh original pod it tried to connect to has gone down and then I that IP address has been reused",
    "start": "991680",
    "end": "999000"
  },
  {
    "text": "um so these are these are really nasty uh so usually this means that linkerd",
    "start": "999000",
    "end": "1004079"
  },
  {
    "text": "the proxy has missed an update of some kind it's working on stale data uh but there's a lot of different things that",
    "start": "1004079",
    "end": "1010000"
  },
  {
    "text": "could cause that like why is it acting on stale data did it somehow drop the update does the proxy have a bug or did",
    "start": "1010000",
    "end": "1016160"
  },
  {
    "text": "the destination controller never send that update to the proxy in the first place so maybe the destination controller has a bug or maybe the",
    "start": "1016160",
    "end": "1022880"
  },
  {
    "text": "destination controller never got that update from the Kubernetes API so maybe the Kubernetes API is in a bad state",
    "start": "1022880",
    "end": "1028558"
  },
  {
    "text": "there's just a lot of moving pieces and it's hard to know kind of where to look for the",
    "start": "1028559",
    "end": "1034839"
  },
  {
    "text": "problem and it's even harder because these types of errors are very um state",
    "start": "1034839",
    "end": "1040720"
  },
  {
    "text": "based uh across multiple different systems right there's the proxy there's the destination controller there's the",
    "start": "1040720",
    "end": "1045760"
  },
  {
    "text": "Kubernetes API and you know you need to make sure that all of these are in the correct state but what the correct state",
    "start": "1045760",
    "end": "1051200"
  },
  {
    "text": "is changes over time as you do rollouts as pods come up and down um and so kind",
    "start": "1051200",
    "end": "1057039"
  },
  {
    "text": "of to correlate what the correct state of each of these systems should be at a given time and make sure you're",
    "start": "1057039",
    "end": "1062320"
  },
  {
    "text": "collecting the right data in order to debug that is very very difficult um and these errors of course are very",
    "start": "1062320",
    "end": "1068080"
  },
  {
    "text": "transient usually once you restart things they often just go away and then you're like well what happened i don't know um and it's very hard to look for",
    "start": "1068080",
    "end": "1075919"
  },
  {
    "text": "the absence of something like we're missing an update so each individual system taken in isolation will seem like",
    "start": "1075919",
    "end": "1081600"
  },
  {
    "text": "it's doing the correct behavior um so how do you look for something that's missing how do you know what's missing",
    "start": "1081600",
    "end": "1088080"
  },
  {
    "text": "very tricky um so if we dig into kind of how these",
    "start": "1088080",
    "end": "1093760"
  },
  {
    "text": "updates flow from the Kubernetes API all the way down to the link proxy a very",
    "start": "1093760",
    "end": "1099200"
  },
  {
    "text": "simplified uh view of it is is this so these are things that happen in the destination controller that's the one",
    "start": "1099200",
    "end": "1105440"
  },
  {
    "text": "that was written in Go um and that's the one that serves addresses IP addresses destinations down to the link proxy um",
    "start": "1105440",
    "end": "1113679"
  },
  {
    "text": "and so whenever kind of a relevant resource like a uh endpoints resource or service resource changes in the",
    "start": "1113679",
    "end": "1120480"
  },
  {
    "text": "Kubernetes API well we get an update uh in the destination controller because we have those watches established uh and we",
    "start": "1120480",
    "end": "1127840"
  },
  {
    "text": "use client go informers to to establish those watches um so that will trigger some kind of on update call back in the",
    "start": "1127840",
    "end": "1135200"
  },
  {
    "text": "destination controller and so once we get that update we'll figure out kind of what the right state uh for all the the",
    "start": "1135200",
    "end": "1142160"
  },
  {
    "text": "proxies should be for that and then we'll go through a loop and we'll iterate over all of the proxies that",
    "start": "1142160",
    "end": "1148559"
  },
  {
    "text": "have subscribed to that service and for each one of them we'll send an update on that gRPC stream so if you have you know",
    "start": "1148559",
    "end": "1156160"
  },
  {
    "text": "10 different proxies that have all subscribed to a certain service that service updates then we'll loop through those 10 proxies and send an update to",
    "start": "1156160",
    "end": "1162880"
  },
  {
    "text": "each of them and the uh code that is sending those",
    "start": "1162880",
    "end": "1169440"
  },
  {
    "text": "gRPC update up updates uh looks something like this uh this is",
    "start": "1169440",
    "end": "1174559"
  },
  {
    "text": "stream.end is the relevant API here so this is sending on a streaming gRPC",
    "start": "1174559",
    "end": "1180840"
  },
  {
    "text": "response so we have some kind of addition which is the update and we're doing stream.s send that addition",
    "start": "1180840",
    "end": "1187880"
  },
  {
    "text": "down um and what you should notice here is that stream.send only takes the",
    "start": "1187880",
    "end": "1192960"
  },
  {
    "text": "update as a parameter it doesn't take a context object there's no mechanism for",
    "start": "1192960",
    "end": "1198120"
  },
  {
    "text": "cancellations there are no kind of channels involved here in any obvious way there's no obvious way to abort this",
    "start": "1198120",
    "end": "1204799"
  },
  {
    "text": "call it's a totally blocking call when you call stream.send that call will",
    "start": "1204799",
    "end": "1209919"
  },
  {
    "text": "block until it returns and it will not return until it has done the thing until it has sent the",
    "start": "1209919",
    "end": "1217160"
  },
  {
    "text": "message so there's no kind of asynchronous uh behavior baked in",
    "start": "1217160",
    "end": "1223840"
  },
  {
    "text": "and it's sending those messages over a gRPC stream uh so gRPC is transported",
    "start": "1223840",
    "end": "1229440"
  },
  {
    "text": "over HTTP2 and HTTP2 has this concept of flow control windows and the idea behind",
    "start": "1229440",
    "end": "1235840"
  },
  {
    "text": "flow control windows is that when you're talking over a HTTP2 connection the sender can only send so many bytes until",
    "start": "1235840",
    "end": "1243039"
  },
  {
    "text": "it receives something called a window update and the window update is the receivers's way of saying \"Hey I've",
    "start": "1243039",
    "end": "1248480"
  },
  {
    "text": "received those bytes i've processed them and I'm ready for you to send me more.\" Um and without that you know we could",
    "start": "1248480",
    "end": "1255520"
  },
  {
    "text": "get into a state where the sender is sending data faster than the receiver can process it and uh you know it gets",
    "start": "1255520",
    "end": "1261120"
  },
  {
    "text": "backed up it's cues overflow you know uh you drop data so in order to avoid that",
    "start": "1261120",
    "end": "1266559"
  },
  {
    "text": "situation we have this back pressure mechanism where the receiver can send okay or the receiver can say okay I'm",
    "start": "1266559",
    "end": "1272640"
  },
  {
    "text": "ready you can send me more now um and if the receiver doesn't send",
    "start": "1272640",
    "end": "1277679"
  },
  {
    "text": "those window updates then the sender will wait and and not send any more data so for example this is a sequence",
    "start": "1277679",
    "end": "1285360"
  },
  {
    "text": "diagram where the sender says \"Okay here's 64 kilobytes of data.\" The receiver says \"Okay I've read those i've",
    "start": "1285360",
    "end": "1291440"
  },
  {
    "text": "processed them i'm ready for more here's 64 more okay I'm ready here's 64 more.\"",
    "start": "1291440",
    "end": "1297200"
  },
  {
    "text": "And if the receiver doesn't send any more window updates at that point the sender has data to send but the sender",
    "start": "1297200",
    "end": "1302799"
  },
  {
    "text": "will just wait and say \"Okay the receiver is not ready for this i'm just going to hold on to it until they",
    "start": "1302799",
    "end": "1308799"
  },
  {
    "text": "are.\" So if we go back to uh this picture and we imagine that scenario",
    "start": "1309799",
    "end": "1315360"
  },
  {
    "text": "where the the client or the receiver which is the link proxy in this case is not sending window updates for some",
    "start": "1315360",
    "end": "1321120"
  },
  {
    "text": "reason well then that call to send will eventually fill up you know you call send repeatedly as updates happen",
    "start": "1321120",
    "end": "1328400"
  },
  {
    "text": "eventually that's going to fill up that connection window that flow control window and if the uh receiver is not",
    "start": "1328400",
    "end": "1335600"
  },
  {
    "text": "sending window updates then eventually that will fill up and it will block it'll say \"I'm not going to send any more data until you're ready for it.\" Uh",
    "start": "1335600",
    "end": "1342640"
  },
  {
    "text": "and that call to stream uh and that's really bad in this",
    "start": "1342640",
    "end": "1350039"
  },
  {
    "text": "case uh because if streams send blocks forever then everything kind of grinds",
    "start": "1350039",
    "end": "1356240"
  },
  {
    "text": "to a halt in the in the destination controller because now we're not able to proceed past that point we're not able",
    "start": "1356240",
    "end": "1362960"
  },
  {
    "text": "to continue looping through all of the uh proxies that have sub subscribed to these updates we're just going to be",
    "start": "1362960",
    "end": "1369200"
  },
  {
    "text": "stuck deadlocked um and so could this actually happen this is like kind of theoretical",
    "start": "1369200",
    "end": "1375440"
  },
  {
    "text": "up to this point why would a proxy like not send window updates um and the",
    "start": "1375440",
    "end": "1381039"
  },
  {
    "text": "answer is I don't know but there's a variety of reasons why it potentially could um there could be a bug in the",
    "start": "1381039",
    "end": "1386559"
  },
  {
    "text": "proxy in the proxy's network libraries um you know something weird could be",
    "start": "1386559",
    "end": "1392320"
  },
  {
    "text": "going on there that's causing it to not send those updates um this can happen because of CPU starvation so if the",
    "start": "1392320",
    "end": "1398559"
  },
  {
    "text": "proxy is not getting CPU cycles then it's not able to send window updates and it can kind of get stuck in this state",
    "start": "1398559",
    "end": "1405039"
  },
  {
    "text": "um there could be other network weirdness like we've seen things where contract loses track of a connection and",
    "start": "1405039",
    "end": "1410480"
  },
  {
    "text": "the connection gets into a weird state and updates go missing um or that thing on the other side of that HTTP2",
    "start": "1410480",
    "end": "1417120"
  },
  {
    "text": "connection that gRPC connection might not even be a linker proxy it might be some other client that's just calling",
    "start": "1417120",
    "end": "1422720"
  },
  {
    "text": "the destination controller and then not sending window updates and causing the whole thing to deadlock",
    "start": "1422720",
    "end": "1429480"
  },
  {
    "text": "so this is really bad as long as stream.send send is blocked none of the",
    "start": "1431120",
    "end": "1436320"
  },
  {
    "text": "other listeners are getting any updates every single uh proxy that is talking to that destination controller uh is going",
    "start": "1436320",
    "end": "1443440"
  },
  {
    "text": "to be starved for information and actually it's even worse than that because that for loop that's",
    "start": "1443440",
    "end": "1450240"
  },
  {
    "text": "looping over all of those um proxies all those listeners comes from the client go",
    "start": "1450240",
    "end": "1457919"
  },
  {
    "text": "call back the informer call back and so as long as the informer callback is blocked no other callbacks will fire so",
    "start": "1457919",
    "end": "1465440"
  },
  {
    "text": "that destination controller will actually stop getting updates from the Kubernetes API entirely or it'll stop",
    "start": "1465440",
    "end": "1470559"
  },
  {
    "text": "acting on them because the the callback is blocked so this leads to this state",
    "start": "1470559",
    "end": "1478400"
  },
  {
    "text": "where one misbehaving client whether that's a proxy or some other client uh can deadlock the entire system just by",
    "start": "1478400",
    "end": "1485120"
  },
  {
    "text": "not reading data or just by not sending window updates pretty pretty",
    "start": "1485120",
    "end": "1491679"
  },
  {
    "text": "nasty um yeah it'll just be kind of stuck in this state uh potentially",
    "start": "1492600",
    "end": "1497720"
  },
  {
    "text": "forever so and even fixing this was not totally straightforward we were able to do it by kind of splitting things out",
    "start": "1497720",
    "end": "1504400"
  },
  {
    "text": "and putting a queue in between so now uh we've moved on to an architecture where",
    "start": "1504400",
    "end": "1510000"
  },
  {
    "text": "whenever we receive these updates uh and we kind of loop through all of the subscribers who are listening for",
    "start": "1510000",
    "end": "1515760"
  },
  {
    "text": "updates uh on those services instead of just sending those directly to the gRPC stream we'll now incue those into a",
    "start": "1515760",
    "end": "1522799"
  },
  {
    "text": "channel um and we'll do that in a way that we can kind of guarantee that that will never block and then meanwhile in",
    "start": "1522799",
    "end": "1529279"
  },
  {
    "text": "another go routine we have a Q processor which is processing those updates off of that Q and then sending those down on",
    "start": "1529279",
    "end": "1535679"
  },
  {
    "text": "the gRPC stream and so by separating those we kind of isolate the the",
    "start": "1535679",
    "end": "1541120"
  },
  {
    "text": "blocking behavior now we have a Q per uh per listener and if that listener stops",
    "start": "1541120",
    "end": "1548080"
  },
  {
    "text": "listening if we stop getting window updates there it's only going to block itself it's not going to block any other",
    "start": "1548080",
    "end": "1553760"
  },
  {
    "text": "um it's not going to block the the controller call back and it's not going to block the updates to any other",
    "start": "1553760",
    "end": "1559520"
  },
  {
    "text": "listeners so it it can only sabotage",
    "start": "1559520",
    "end": "1564080"
  },
  {
    "text": "itself um and if that queue does become full then we can just simply terminate that stream we can say \"Hey you got too",
    "start": "1564679",
    "end": "1571919"
  },
  {
    "text": "far behind you're not reading updates for some reason we're just going to terminate you and and you can reconnect or or whatever.\"",
    "start": "1571919",
    "end": "1579278"
  },
  {
    "text": "So I I think the lessons we learned there well number one flow control is really cool uh the way that HTTP2 works",
    "start": "1580400",
    "end": "1586320"
  },
  {
    "text": "with flow control is very powerful uh and it was kind of in some sense working as intended it was exerting this back",
    "start": "1586320",
    "end": "1592159"
  },
  {
    "text": "pressure it's just that we also had this consequence of the architecture that that back pressure was getting exerted",
    "start": "1592159",
    "end": "1597360"
  },
  {
    "text": "on everyone instead of just onto that one stream uh the other lesson is just to be very very careful with blocking",
    "start": "1597360",
    "end": "1604000"
  },
  {
    "text": "calls um and to know which calls are blocking and which are not this was a behavior that surprised us because we",
    "start": "1604000",
    "end": "1610159"
  },
  {
    "text": "didn't expect just sending onto a gRPC stream could potentially block uh and maybe block forever um so something to",
    "start": "1610159",
    "end": "1618240"
  },
  {
    "text": "watch out for um and going handinhand in that hand inhand with that uh is just to",
    "start": "1618240",
    "end": "1624159"
  },
  {
    "text": "be extra careful to never block inside of an informer callback because that's especially nasty anytime the informer",
    "start": "1624159",
    "end": "1629919"
  },
  {
    "text": "callback thread is blocked you're going to not get any call backs at all from from client",
    "start": "1629919",
    "end": "1636799"
  },
  {
    "text": "go all right we have one one more bug but I don't think we have time to cover it here but these slides are um on",
    "start": "1637320",
    "end": "1645600"
  },
  {
    "text": "Sketch so you you know please please check them out this last one is a a very interesting memory leak i'll",
    "start": "1645600",
    "end": "1651400"
  },
  {
    "text": "just jump through here we kind of can look at some allocations um and um I'll",
    "start": "1651400",
    "end": "1658559"
  },
  {
    "text": "I'll I'll skip past all of this but I I definitely recommend you check it out because it's it's very interesting um",
    "start": "1658559",
    "end": "1664480"
  },
  {
    "text": "but the key lessons here uh are to not just look at allocations which is what we were doing initially but but also",
    "start": "1664480",
    "end": "1670640"
  },
  {
    "text": "look at deallocations those are kind of in some sense when you're looking at memory leaks more important um and and",
    "start": "1670640",
    "end": "1676480"
  },
  {
    "text": "be careful you what you put in map keys so watch out",
    "start": "1676480",
    "end": "1681799"
  },
  {
    "text": "um anyway uh so those are those are the the bugs that I want to talk about um I",
    "start": "1681799",
    "end": "1688559"
  },
  {
    "text": "hope you enjoyed those uh if you have any questions um this is all work that has been done in the open source on",
    "start": "1688559",
    "end": "1694799"
  },
  {
    "text": "linkerd so those poll requests are just up on the internet for anyone to look at um you can also come find me and talk to",
    "start": "1694799",
    "end": "1700880"
  },
  {
    "text": "me about these or or other link bugs that you find um I'm going to be hanging out at the buoyant booth in the projects",
    "start": "1700880",
    "end": "1706720"
  },
  {
    "text": "pavilion for for the rest of the week or in the buoyant booth the linky booth or the buoyant booth uh so come find me and",
    "start": "1706720",
    "end": "1712720"
  },
  {
    "text": "talk to me and uh if you're interested in learning more about linkerty there is a service mesh academy which is a",
    "start": "1712720",
    "end": "1718880"
  },
  {
    "text": "monthly live hands-on training that is very good uh I highly recommend you can",
    "start": "1718880",
    "end": "1724159"
  },
  {
    "text": "go to buoyant.iosma to sign up um or there's also a certification that is self-paced",
    "start": "1724159",
    "end": "1731360"
  },
  {
    "text": "and uh kind of a linkery one on 101 uh so also a good way to learn more about",
    "start": "1731360",
    "end": "1738520"
  },
  {
    "text": "linkerty thank [Applause]",
    "start": "1738520",
    "end": "1747000"
  },
  {
    "text": "you and I think we maybe have just a short time for questions",
    "start": "1747000",
    "end": "1753480"
  },
  {
    "text": "yeah I think there's a microphone",
    "start": "1754559",
    "end": "1758440"
  },
  {
    "text": "yeah so uh very good talk thank you uh is there a reason why you left out a",
    "start": "1768880",
    "end": "1773919"
  },
  {
    "text": "proxy injector because it's bug free or I don't know because I only had 30",
    "start": "1773919",
    "end": "1779360"
  },
  {
    "text": "minutes to talk about bugs oh okay but but certainly the proxy injector has been the source of of of bugs too so",
    "start": "1779360",
    "end": "1785520"
  },
  {
    "text": "there's there's no shortage okay thanks",
    "start": "1785520",
    "end": "1790440"
  },
  {
    "text": "all right if there's no other questions then just uh come find me anytime this week and happy to chat more [Applause]",
    "start": "1796399",
    "end": "1804290"
  }
]