[
  {
    "text": "very nice to be here with you today and it's it's really nice to see so many of you are interested in hcd and control",
    "start": "0",
    "end": "6060"
  },
  {
    "text": "plane performance So today we're going to talk about HD",
    "start": "6060",
    "end": "11460"
  },
  {
    "text": "performance but in a much more General way about the performance of control planes and interactions between API",
    "start": "11460",
    "end": "17820"
  },
  {
    "text": "servers and nhcd and I'm Lauren Bernard I work at datadog I'm Martin from isobond",
    "start": "17820",
    "end": "26519"
  },
  {
    "text": "and so to get started we're going to talk about scaling the control planes and one of the reason I can talk about",
    "start": "26519",
    "end": "32398"
  },
  {
    "text": "it is because at datadog we run a large numbers a large number of very large community clusters to give you an idea",
    "start": "32399",
    "end": "38820"
  },
  {
    "text": "we have hundreds of clusters from like between 1 000 to 6000 nodes and as you",
    "start": "38820",
    "end": "44940"
  },
  {
    "text": "reach like thousands of notes in a cluster you start to find interesting challenges and we started like everybody right we",
    "start": "44940",
    "end": "51840"
  },
  {
    "text": "started with a very simple control plane where you have a single master I'm sure you're all familiar with these",
    "start": "51840",
    "end": "57120"
  },
  {
    "text": "components where you have a single node where you have hcd which is responsible for storing the resource in your cluster",
    "start": "57120",
    "end": "63180"
  },
  {
    "text": "you have the API server which is responsible for the abuse the kubernetes apis and then you have two core",
    "start": "63180",
    "end": "69240"
  },
  {
    "text": "controllers the scheduler which is responsible for scheduling workloads and nodes and controllers which are running",
    "start": "69240",
    "end": "75240"
  },
  {
    "text": "very reconcile loops and making sure that the state of the cluster is what what's expected and of course we have",
    "start": "75240",
    "end": "81960"
  },
  {
    "text": "all the components interacting with the Masters uh so you start with this but of course this is not very resilient right",
    "start": "81960",
    "end": "88259"
  },
  {
    "text": "if you lose uh the node running all these components you lose your cluster so the next step is usually to have",
    "start": "88259",
    "end": "93659"
  },
  {
    "text": "multiple Masters right so exactly the same setup but instead of having a single Master with our stream Masters",
    "start": "93659",
    "end": "99659"
  },
  {
    "text": "running the same components this slide is a bit misleading though because all these components are",
    "start": "99659",
    "end": "106680"
  },
  {
    "text": "stateless except for LCD because this is the data store of the cluster so I have three LCD boxes but it's actually an SD",
    "start": "106680",
    "end": "113640"
  },
  {
    "text": "cluster and you can see here that all the components are stateless exact entity",
    "start": "113640",
    "end": "119159"
  },
  {
    "text": "which is stateful and so it's kind of weird that you designed this way so a very common optimization is to actually get hcd outside of the Masters and so",
    "start": "119159",
    "end": "126719"
  },
  {
    "text": "the next optimization is to do it this way right so you have a net City cluster where you have a utility nodes and then",
    "start": "126719",
    "end": "132540"
  },
  {
    "text": "you have your Masters when you're on API servers controllers and scheduler they're the next thing that's",
    "start": "132540",
    "end": "138180"
  },
  {
    "text": "interesting which is if you're familiar with kubernetes you know that schedulers and controllers are actually running a",
    "start": "138180",
    "end": "144180"
  },
  {
    "text": "single version a single active version at any given time so you have a leader election and even if you have three or",
    "start": "144180",
    "end": "150180"
  },
  {
    "text": "five schedules or controllers only a single one is going to be active at any given time and so you can see how in",
    "start": "150180",
    "end": "156420"
  },
  {
    "text": "terms of sizing your resources this is not very efficient right because you're going to run processes that won't be",
    "start": "156420",
    "end": "162060"
  },
  {
    "text": "active and won't be consuming resources in your cluster so the next step of optimization that you can do is to",
    "start": "162060",
    "end": "168180"
  },
  {
    "text": "actually move controllers in scheduler outside of Masters and only run two right so you have two one that's active",
    "start": "168180",
    "end": "174360"
  },
  {
    "text": "and one that's passive so you can do failover and then you can add as many API servers as you want",
    "start": "174360",
    "end": "180660"
  },
  {
    "text": "if if you've run a large quintessence clusters you probably've probably been impacted by something else which is when",
    "start": "180660",
    "end": "187379"
  },
  {
    "text": "you have a large cluster you can have large backup events and events are a specific resource and sometimes they",
    "start": "187379",
    "end": "193200"
  },
  {
    "text": "will end up consuming 80 90 of the space of your SED clusters",
    "start": "193200",
    "end": "198300"
  },
  {
    "text": "and of course when this happens this can impact the behavior of the cluster for advance and events are not really that useful so a common optimization you can",
    "start": "198300",
    "end": "205680"
  },
  {
    "text": "do for large clusters is to actually split dedicated LCD for events and you",
    "start": "205680",
    "end": "211200"
  },
  {
    "text": "end up with this setup here where you have an LCD dedicated for the event resource and so if you have a big spike",
    "start": "211200",
    "end": "217200"
  },
  {
    "text": "of events this actually might be slower but the rest of the controller is going to work fine because the other LCD is",
    "start": "217200",
    "end": "223260"
  },
  {
    "text": "going to be behaving as as normal and now that we've seen all these",
    "start": "223260",
    "end": "228480"
  },
  {
    "text": "components and the different optimization you can you can do we can talk about sizing the control plane",
    "start": "228480",
    "end": "234120"
  },
  {
    "text": "so for HCT clusters usually you have three or five nodes uh because you don't",
    "start": "234120",
    "end": "239459"
  },
  {
    "text": "you it makes little sense to have more because it's a current based system and it's really the most efficient is really",
    "start": "239459",
    "end": "244620"
  },
  {
    "text": "to have three for resiliency but we prefer to run five uh just because if you if you if you run three and you lose",
    "start": "244620",
    "end": "251400"
  },
  {
    "text": "one you end up in a situation you why you have to be extremely careful because if you lose the second nodes then you",
    "start": "251400",
    "end": "256680"
  },
  {
    "text": "host and you've lost everything so we prefer to run five because if you run one we can still lose another one before",
    "start": "256680",
    "end": "262380"
  },
  {
    "text": "being in a catastrophic situation in terms of key resources for LCD I'd",
    "start": "262380",
    "end": "267960"
  },
  {
    "text": "say the most important resource is disk so make sure you have very fast disk and monitor the latency of of your HTT disk",
    "start": "267960",
    "end": "275100"
  },
  {
    "text": "because this is really something that can slow down your cluster something that is little less obvious is",
    "start": "275100",
    "end": "282000"
  },
  {
    "text": "Etsy also needs quite a significant throughput from Network right because when API server starts and when the",
    "start": "282000",
    "end": "288000"
  },
  {
    "text": "fetch resources from hcd the volume of data to transfer can be significant and so it's denotes in its fast disk and",
    "start": "288000",
    "end": "294660"
  },
  {
    "text": "NFS Network API server can scale horizontally and that's amazing right because if you have",
    "start": "294660",
    "end": "299940"
  },
  {
    "text": "many things connecting to API servers having more API servers will always help except that if your pressure gets bigger",
    "start": "299940",
    "end": "306600"
  },
  {
    "text": "API sellers still need to Cache everything in the cluster so you will probably need to increase the amount of",
    "start": "306600",
    "end": "311699"
  },
  {
    "text": "memory for each API server so you can scale them horizontally but don't forget to add more RAM as your cluster gets",
    "start": "311699",
    "end": "316919"
  },
  {
    "text": "bigger controls and schedulers are easier to scale because they mostly consume CPU so",
    "start": "316919",
    "end": "322680"
  },
  {
    "text": "just like give a quite quite significant amount of CPU for your controls and you'll be good",
    "start": "322680",
    "end": "328880"
  },
  {
    "text": "so of course this only works if you run your own clusters which which we do right but in many cases you will be",
    "start": "329639",
    "end": "336900"
  },
  {
    "text": "running um cluster's managed services from from providers and of course they'll do all the work we we mentioned here in in",
    "start": "336900",
    "end": "344039"
  },
  {
    "text": "different ways but even if you use managed Services you can still play nice with the control plane provided by your",
    "start": "344039",
    "end": "349919"
  },
  {
    "text": "by your provider so for instance you can make sure that the number of nodes you have in your cluster remains uh",
    "start": "349919",
    "end": "355080"
  },
  {
    "text": "reasonable so avoid going to to merge above like three four thousand if you can the number of services can also have",
    "start": "355080",
    "end": "361800"
  },
  {
    "text": "a significant impact on the cluster so be careful on the more services you have and of course if you churn pods a lot",
    "start": "361800",
    "end": "367919"
  },
  {
    "text": "this is also pretty intent on the cluster some other thing you can do when you run",
    "start": "367919",
    "end": "374100"
  },
  {
    "text": "last courses is you can try and decrease the load on the control plane and I get",
    "start": "374100",
    "end": "379560"
  },
  {
    "text": "a few examples here um but there are many ways you can do that so a quick example for instance is",
    "start": "379560",
    "end": "385620"
  },
  {
    "text": "if you use grpc you probably don't need a normal cluster IP service in your cluster because grpc clients can",
    "start": "385620",
    "end": "392400"
  },
  {
    "text": "discover all the backend IPM dual advancing themselves and if you do this it means you don't have to run the",
    "start": "392400",
    "end": "397979"
  },
  {
    "text": "endpoint controller reconcile you for all these services and endpoint can have a very significant impact on the cluster",
    "start": "397979",
    "end": "403740"
  },
  {
    "text": "because you have to reconcile every time there's a change in Readiness and then you have to distribute this data back to",
    "start": "403740",
    "end": "409080"
  },
  {
    "text": "to proxy everywhere and this is pretty pretty heavy another thing that's been possible for",
    "start": "409080",
    "end": "414600"
  },
  {
    "text": "quite some time but still not the default is I don't know if you know but something I discovered uh it took me",
    "start": "414600",
    "end": "421259"
  },
  {
    "text": "some time to discover that when you run when you have a config map or a secret by default if you change the config mat",
    "start": "421259",
    "end": "427319"
  },
  {
    "text": "of the secret or a secret it's going to be updated in your pod right the qubit is going to actually see that the",
    "start": "427319",
    "end": "432600"
  },
  {
    "text": "resource has changed and it's going to modify data on this for you but and your pod can actually see that the data has",
    "start": "432600",
    "end": "437639"
  },
  {
    "text": "changed so it can be useful for some applications that in most cases it's not and so you can make your secrets and",
    "start": "437639",
    "end": "443880"
  },
  {
    "text": "config map immutable in which case the keypad won't be watching for them and the loading the control panel will be",
    "start": "443880",
    "end": "449039"
  },
  {
    "text": "much slower and finally some controllers have their own optimizations and we're mentioning",
    "start": "449039",
    "end": "454740"
  },
  {
    "text": "CDM here which has its own KV store to store info endpoint information and decrease the on the cluster",
    "start": "454740",
    "end": "460500"
  },
  {
    "text": "and and finally and this is going to be the main focus on the on the talk today is make sure the application you run in",
    "start": "460500",
    "end": "466380"
  },
  {
    "text": "your clusters or your operators or your demon set behave nicely and will be defining nicely in the talk",
    "start": "466380",
    "end": "474259"
  },
  {
    "text": "we already mentioned few of the components that are running in um in control plane so we have a API",
    "start": "475020",
    "end": "481139"
  },
  {
    "text": "server at CD and Cube controller manager scheduler but what else is there so",
    "start": "481139",
    "end": "486300"
  },
  {
    "text": "first of all like we have Google is running on each of the nodes and it's basically running the pods updating",
    "start": "486300",
    "end": "491759"
  },
  {
    "text": "events and this can actually have significant impact on the API server and at City performance",
    "start": "491759",
    "end": "499080"
  },
  {
    "text": "um except for that Q proxy for service load balancing then different demon sets",
    "start": "499080",
    "end": "504180"
  },
  {
    "text": "as example either serum agent or data dog agent and they are pretty powerful but with great power cons great",
    "start": "504180",
    "end": "510539"
  },
  {
    "text": "responsibility to to not over overload API server then we also have DNS cluster",
    "start": "510539",
    "end": "517500"
  },
  {
    "text": "autoscaler Ingress controllers other controllers that you might be running in your cluster and of course users with",
    "start": "517500",
    "end": "524880"
  },
  {
    "text": "Cube cattle um and what's really surprising is that",
    "start": "524880",
    "end": "530040"
  },
  {
    "text": "also API servers really like to talk to themselves so now we will be focusing on the cube",
    "start": "530040",
    "end": "535740"
  },
  {
    "text": "cattle to understand the interaction between Cube cattle and and the API server so let's see this simple example",
    "start": "535740",
    "end": "543779"
  },
  {
    "text": "where you try to get the information about one specific pod so what happens is that the cube Capital actually issues",
    "start": "543779",
    "end": "551640"
  },
  {
    "text": "one request to the API server with API server address API version namespace",
    "start": "551640",
    "end": "558180"
  },
  {
    "text": "resource type and resource name but what happens when you do the cubecut",
    "start": "558180",
    "end": "563279"
  },
  {
    "text": "will get pots well here you can see that basically we are trying to list all the pods within the",
    "start": "563279",
    "end": "569160"
  },
  {
    "text": "cluster and this request has limit 500 so if you are running thousands or tens",
    "start": "569160",
    "end": "575220"
  },
  {
    "text": "of thousands of PODS it means that the cube cattle will actually perform even tens or hundreds of API calls to the",
    "start": "575220",
    "end": "582959"
  },
  {
    "text": "control panel which can actually have significant impact on reliability of your control plane",
    "start": "582959",
    "end": "589440"
  },
  {
    "text": "another example you might be interested in seeing the changes that happen to",
    "start": "589440",
    "end": "595680"
  },
  {
    "text": "those spots so you can do the watch equals true and again first of all Cube cattle basically lists all the pots and",
    "start": "595680",
    "end": "603360"
  },
  {
    "text": "then it starts the watch and the most important part here is to see that the watch has resource version which means",
    "start": "603360",
    "end": "609600"
  },
  {
    "text": "that okay we want to observe all the changes that happen um from the specific version that we got",
    "start": "609600",
    "end": "615779"
  },
  {
    "text": "from the list call for that",
    "start": "615779",
    "end": "621899"
  },
  {
    "text": "before okay you basically describe the pot to see like what's going on there right so again we have the get call that",
    "start": "621899",
    "end": "630180"
  },
  {
    "text": "gets the information about pod but then also what we have is listing the events so as you can see",
    "start": "630180",
    "end": "637380"
  },
  {
    "text": "here we are looking for events with specific field selectors and please",
    "start": "637380",
    "end": "642600"
  },
  {
    "text": "remember this example because it will be important pretty soon as we will understand how it works underneath",
    "start": "642600",
    "end": "651180"
  },
  {
    "text": "another example let's say that you are trying to delete the Pod and probably you already saw that when you try to",
    "start": "651180",
    "end": "657000"
  },
  {
    "text": "delete something in kubernetes and basically first of all it just",
    "start": "657000",
    "end": "662399"
  },
  {
    "text": "deletes it but then you have to wait and this waiting is again the list call followed with the watch call",
    "start": "662399",
    "end": "670320"
  },
  {
    "text": "so to sum up there are multiple components that are talking to API server and also when you are interacting",
    "start": "670320",
    "end": "677220"
  },
  {
    "text": "with the kubernetes using Cube control then one simple Cube cattle command can",
    "start": "677220",
    "end": "683339"
  },
  {
    "text": "result in even tens or hundreds of API calls",
    "start": "683339",
    "end": "689600"
  },
  {
    "text": "so now that we have the basics of the control plane components and that we know how The Vedic API work we're going",
    "start": "691140",
    "end": "697800"
  },
  {
    "text": "to dive into specific issues that would illustrate what we meant by you we want our demon set and console to be in to",
    "start": "697800",
    "end": "704579"
  },
  {
    "text": "behave nicely and this issue here is actually a real one that happened in the production class right data there so",
    "start": "704579",
    "end": "709980"
  },
  {
    "text": "it's actually a real incident so in terms of how it started I mean users were started reporting",
    "start": "709980",
    "end": "715920"
  },
  {
    "text": "connectivity issue to a cluster they were not able to connect and it turns out I mean we looked at the metrics here",
    "start": "715920",
    "end": "721620"
  },
  {
    "text": "and you can see that API server were not very happy when all of them were analyzing of course because their analysis they can set traffic",
    "start": "721620",
    "end": "728040"
  },
  {
    "text": "so we started looking at logs and metrics and we could say that API server were not able to reach at CD right so",
    "start": "728040",
    "end": "733740"
  },
  {
    "text": "because they were not able to reach hcd they were crashing not the very good place to be at",
    "start": "733740",
    "end": "739740"
  },
  {
    "text": "so what was happening with Ed City well it turns out as you can see on the top graph well actually we're using a lot of",
    "start": "739740",
    "end": "746100"
  },
  {
    "text": "memory and we're actually getting umkyo which is once again not a very good place to to be in",
    "start": "746100",
    "end": "751800"
  },
  {
    "text": "so what what we knew right we knew that the cluster size had been the same for about four four weeks or even months",
    "start": "751800",
    "end": "758040"
  },
  {
    "text": "right so unrelated to size uh we hadn't updated the control plane so it was not related to a new version",
    "start": "758040",
    "end": "764160"
  },
  {
    "text": "of kubernetes and so we figured well it's very very likely related to some clients doing",
    "start": "764160",
    "end": "770279"
  },
  {
    "text": "something up against our API and so we started looking at API 7 metrics and in particular at the number",
    "start": "770279",
    "end": "777360"
  },
  {
    "text": "of requests and you can see here that during the two incidents the number of inside requests is very high like it's",
    "start": "777360",
    "end": "783360"
  },
  {
    "text": "typically like around 100 and during the incident respects above uh 1000.",
    "start": "783360",
    "end": "789600"
  },
  {
    "text": "and especially this is the graph at the bottom you can see that we have a very big spike in these calls and and we're",
    "start": "789600",
    "end": "795720"
  },
  {
    "text": "going to be talking a lot about these goals because these calls can be very expensive for your clusters especially large ones",
    "start": "795720",
    "end": "802200"
  },
  {
    "text": "so to understand why we say that list calls are expensive we have to understand how API server caching works",
    "start": "802200",
    "end": "809100"
  },
  {
    "text": "so API servers are basically big caches in front of Etc I mean they do a lot",
    "start": "809100",
    "end": "814320"
  },
  {
    "text": "more but this is a key part for what we're going to talk about today and so when an API server starts it's going to",
    "start": "814320",
    "end": "820440"
  },
  {
    "text": "list all the resources in FCD and start at the watches to get all the updates and maintain the cache",
    "start": "820440",
    "end": "827160"
  },
  {
    "text": "when a client has a query against an API server it doesn't get or all it is for a specific resource and it can specify a",
    "start": "827160",
    "end": "834720"
  },
  {
    "text": "resource version and with which Marset Gap in his example just before so if you",
    "start": "834720",
    "end": "840120"
  },
  {
    "text": "specify resource version if you say I want the risk this resource with resource version X you're gonna get it if you have a more",
    "start": "840120",
    "end": "846899"
  },
  {
    "text": "recent version of the cache right so if the version in the API seller cache is more recent than x you can just get it",
    "start": "846899",
    "end": "852540"
  },
  {
    "text": "if the API server doesn't have it it's going to wait a little because sometimes an API server can be a little behind",
    "start": "852540",
    "end": "858839"
  },
  {
    "text": "right if you have to process all the changes from hcd and so sometimes you're asking for x and the API server you're",
    "start": "858839",
    "end": "864660"
  },
  {
    "text": "asking for apps that haven't had it yet so it's gonna wait for a few seconds for its cache to be updated and if it gets",
    "start": "864660",
    "end": "870540"
  },
  {
    "text": "the new version uh it's going to it's going to send it to the plant otherwise it's going to throw an error",
    "start": "870540",
    "end": "876000"
  },
  {
    "text": "so what really matters is these two points right when you ask for X it means give me at least X",
    "start": "876000",
    "end": "881880"
  },
  {
    "text": "and there's a very specific case of zero which is well give me whatever you have in cash",
    "start": "881880",
    "end": "888420"
  },
  {
    "text": "so what about get Unleashed without a resource version I don't know if you remember the example that Marcel gave",
    "start": "888420",
    "end": "894899"
  },
  {
    "text": "before but Cube CTL when we're looking at the verbose output of cubectl we're seeing the the get command the HTTP get",
    "start": "894899",
    "end": "902100"
  },
  {
    "text": "commands and there was no resource version in there except for the kit of the watch so if you don't sell the",
    "start": "902100",
    "end": "907740"
  },
  {
    "text": "results version what happens is the aps ever understand this size meaning get me a quorum consistent read from LCD so if",
    "start": "907740",
    "end": "915120"
  },
  {
    "text": "you don't specify a resource version you're going to get the data from HTT and it's important because this is a",
    "start": "915120",
    "end": "920760"
  },
  {
    "text": "default behavior of qctl get so if you do a lot of cube Ctrl gets if you have shell script during Loops during Cube",
    "start": "920760",
    "end": "927600"
  },
  {
    "text": "CTL get it can be very intense on LCD and also and even more important and counter-intuitive if you use client go",
    "start": "927600",
    "end": "934380"
  },
  {
    "text": "the default list command for resources is also going to do a consistent read from LCD so if you want to do a read",
    "start": "934380",
    "end": "941880"
  },
  {
    "text": "from Cache you actually have to provide an option to the list called to say get me resource version 0. but if you just",
    "start": "941880",
    "end": "947519"
  },
  {
    "text": "do a list you're going to get a consistent list from from etcd so Aries and illustration of what this",
    "start": "947519",
    "end": "955079"
  },
  {
    "text": "means right if we get all the parts in a closer with 30 000 pods and we don't specify any resource version we're going",
    "start": "955079",
    "end": "961500"
  },
  {
    "text": "to get the data from hcd and as you can see here it's going to take more than four seconds in this example",
    "start": "961500",
    "end": "966540"
  },
  {
    "text": "however if you set results version to 0 it's going to be only 1.8 seconds and in here we actually had to cheat a",
    "start": "966540",
    "end": "973079"
  },
  {
    "text": "little to to make this work because of course given the size of the cluster if we had a full Json",
    "start": "973079",
    "end": "978620"
  },
  {
    "text": "output it could be a gigabyte of data and most of the time would have been spent processing the data and sending it",
    "start": "978620",
    "end": "984120"
  },
  {
    "text": "up to the network so we use a small trick which is the qctl trick where you say well get me the data as stable and",
    "start": "984120",
    "end": "989639"
  },
  {
    "text": "so you get a summarized version of the object which is much smaller and and so the time here is mostly that",
    "start": "989639",
    "end": "994680"
  },
  {
    "text": "processing time and getting the data from that City what about label filters I mean when you",
    "start": "994680",
    "end": "1000019"
  },
  {
    "text": "get resources you know you can say like give me all the pods from application a and this is what the query looks like",
    "start": "1000019",
    "end": "1005180"
  },
  {
    "text": "and this is slightly faster uh when you don't specify resource version because of course there's less data to to send",
    "start": "1005180",
    "end": "1011300"
  },
  {
    "text": "to the clients however when you set resource version to zero it's almost instance because",
    "start": "1011300",
    "end": "1016839"
  },
  {
    "text": "filtering is done on the API server and then you have little data to to send",
    "start": "1016839",
    "end": "1021980"
  },
  {
    "text": "so what's very important here is when you do resource Vision equals",
    "start": "1021980",
    "end": "1027140"
  },
  {
    "text": "nothing or if you don't set resource version you're going to get all the data from hcd and filtering which always happen on API server so even if the",
    "start": "1027140",
    "end": "1033798"
  },
  {
    "text": "output is only a few parts because application a is only five parts in our example you still need to retrieve 30",
    "start": "1033799",
    "end": "1039980"
  },
  {
    "text": "000 parts from hcd which is pretty intense and I don't know if you remember the",
    "start": "1039980",
    "end": "1045438"
  },
  {
    "text": "describe example from before if you look at the at the git here you can see that get events is actually not setting",
    "start": "1045439",
    "end": "1051919"
  },
  {
    "text": "resource version right which means the events are retrieved from hcd of course the filter lecture is very precise",
    "start": "1051919",
    "end": "1057679"
  },
  {
    "text": "because it's targeting the Pod you want to to get the events for however remember how featuring works right this",
    "start": "1057679",
    "end": "1063860"
  },
  {
    "text": "means that you're going to get all the events from this namespace from hcd into the API server and then apply filtering",
    "start": "1063860",
    "end": "1069559"
  },
  {
    "text": "so for large namespaces or for name faces with a lot of events or both it's going to be pretty intense on Etc and",
    "start": "1069559",
    "end": "1075740"
  },
  {
    "text": "API servers so of course I mean I mentioned a lot that filtering happens on API servers",
    "start": "1075740",
    "end": "1081740"
  },
  {
    "text": "and the reason for this is the way at city is structures and kubernetes so this is the way keys are organized right",
    "start": "1081740",
    "end": "1087500"
  },
  {
    "text": "so you have the resource side the namespace and resource name which means uh you can ask NCD for a specific",
    "start": "1087500",
    "end": "1093380"
  },
  {
    "text": "resource you can add for all the resources in any space or all resources of a given type but there's no other",
    "start": "1093380",
    "end": "1099980"
  },
  {
    "text": "type of filtering happening all the other types of filters are going to be happening in API servers and here you",
    "start": "1099980",
    "end": "1105799"
  },
  {
    "text": "have three examples right one is get me all support from App a and this means get all the parts from HD to API server",
    "start": "1105799",
    "end": "1112039"
  },
  {
    "text": "then filter on an API service so that's why it's slow the second example is get me all the",
    "start": "1112039",
    "end": "1118340"
  },
  {
    "text": "parts from namespace datadog from App a and this is much faster because of course in the namespace data log we only",
    "start": "1118340",
    "end": "1124340"
  },
  {
    "text": "have a thousand points and not 30 000. and the last one is the same one but get",
    "start": "1124340",
    "end": "1129740"
  },
  {
    "text": "me this with resource version 0 which means like get me this from from your cash and this is much much faster",
    "start": "1129740",
    "end": "1135320"
  },
  {
    "text": "because of course filtering is done in the memory of the API far more efficient so yeah whenever you write an operator",
    "start": "1135320",
    "end": "1142220"
  },
  {
    "text": "or controller think about using a new format because it's going to be much more efficient",
    "start": "1142220",
    "end": "1147679"
  },
  {
    "text": "so in in summary uh remember something we wanted you to remember is please call go to HD by default and can have a huge",
    "start": "1147679",
    "end": "1153799"
  },
  {
    "text": "impact even if you have um filter that will feature the data a lot in return a small amount of data you",
    "start": "1153799",
    "end": "1160940"
  },
  {
    "text": "might still get everything from hcd and that's very expensive and use informal as as much as you can",
    "start": "1160940",
    "end": "1167660"
  },
  {
    "text": "so let's get back to our to our incident so we know that the problem was coming from new schools right and because we",
    "start": "1167660",
    "end": "1174679"
  },
  {
    "text": "saw the number of requests with these calls and the next step was to understand which application was making business goals",
    "start": "1174679",
    "end": "1181039"
  },
  {
    "text": "and on our cluster we we use all these logs extensively to know what's happening in the cluster and all these",
    "start": "1181039",
    "end": "1187100"
  },
  {
    "text": "logs are very helpful because I can give you an idea what what's happening in your cluster you can see all the queries",
    "start": "1187100",
    "end": "1192380"
  },
  {
    "text": "but you can also see the query time so this view here is the aggregated query time the user for the lease call and we",
    "start": "1192380",
    "end": "1199400"
  },
  {
    "text": "can see that we have a single user accounting for two more two or two or more days of processing every of 20",
    "start": "1199400",
    "end": "1206059"
  },
  {
    "text": "minutes that's a lot of processing right and the reason it's more than 20 minutes is because of course you have multiple goal routine in API server and multiple",
    "start": "1206059",
    "end": "1212900"
  },
  {
    "text": "API servers and so if you aggregate all the queries it can it can be a lot more than 20 minutes",
    "start": "1212900",
    "end": "1218780"
  },
  {
    "text": "and this is another aggregation and here we can see that over a week we have a single service account responsible for",
    "start": "1218780",
    "end": "1225200"
  },
  {
    "text": "almost three days of processing and that's that's that's a lot and service account is called not group",
    "start": "1225200",
    "end": "1230780"
  },
  {
    "text": "controller so what what is it so we actually have a Nina's controller to manage pools of",
    "start": "1230780",
    "end": "1237260"
  },
  {
    "text": "node a databog so teams can create a crd saying I want a pull-off node of these",
    "start": "1237260",
    "end": "1242419"
  },
  {
    "text": "shapes and the control is going to create a Noto scaling group or manage instant Ruby fits on Google and and",
    "start": "1242419",
    "end": "1248000"
  },
  {
    "text": "managing right and we had been using this extensively for for two years at the time it was working perfectly",
    "start": "1248000",
    "end": "1254539"
  },
  {
    "text": "what had happened though is we had a we had an incident the week before or a few weeks before where someone I deleted",
    "start": "1254539",
    "end": "1261320"
  },
  {
    "text": "another group by mistake and you completely deleted the workload because of course when the notebook is deleted",
    "start": "1261320",
    "end": "1267200"
  },
  {
    "text": "the control is going to remove all the nodes and so we were like well we can very easily protect for this by prevent by",
    "start": "1267200",
    "end": "1274160"
  },
  {
    "text": "not allowing delays if ports are running on nodes so we wanted to implement division protection right that's seems",
    "start": "1274160",
    "end": "1279559"
  },
  {
    "text": "simple enough well let's look at how it works so it's actually a very naive approach",
    "start": "1279559",
    "end": "1285980"
  },
  {
    "text": "and a very simple one so when the node group is deleted the first thing you do is you list all the nodes for the node group based on labels it turns out sort",
    "start": "1285980",
    "end": "1292880"
  },
  {
    "text": "of this notebook add like dozens or even hundreds of nodes and this is not the big problem the big",
    "start": "1292880",
    "end": "1298220"
  },
  {
    "text": "problem is happening next and point two right so next we wanted to know if there were pods running on this node that were",
    "start": "1298220",
    "end": "1304400"
  },
  {
    "text": "not demon cells and to know these well we did a very simple list all parts on the nodes and to do this you do a get",
    "start": "1304400",
    "end": "1311240"
  },
  {
    "text": "Bud and you do a feature saying well node X except if you remember how it works in hcd this means getting all the",
    "start": "1311240",
    "end": "1318020"
  },
  {
    "text": "data from etcd and then filtering for node X so even if you have five parts for a given node use literature is 30",
    "start": "1318020",
    "end": "1324500"
  },
  {
    "text": "000 parts from that CD on API servers and then get the five unit for this node except well because we're very efficient",
    "start": "1324500",
    "end": "1331940"
  },
  {
    "text": "we did all this in parallel so if we had hundreds of nodes in this node Group Well we were doing hundreds of calls to",
    "start": "1331940",
    "end": "1338659"
  },
  {
    "text": "lcd4 put on on each node in parallel which means we were doing hundreds of",
    "start": "1338659",
    "end": "1344480"
  },
  {
    "text": "list spots from hcd and each one of them were retrieving 30 000 from LCD",
    "start": "1344480",
    "end": "1350360"
  },
  {
    "text": "as you've seen in the first graphs the attitude don't like it that much right and this Detroit LCD",
    "start": "1350360",
    "end": "1357760"
  },
  {
    "text": "so what what we learned is well this call can be very dangerous I mean the way we fixed it was actually pretty simple we replaced this by an Informer",
    "start": "1358100",
    "end": "1364640"
  },
  {
    "text": "it was just done um and and remember right use information",
    "start": "1364640",
    "end": "1371059"
  },
  {
    "text": "whenever you can and also I mean Auditors are extremely helpful because they will tell you would and when and",
    "start": "1371059",
    "end": "1377360"
  },
  {
    "text": "give you an idea what's processing uh what's using time and now that we've seen this incident",
    "start": "1377360",
    "end": "1383480"
  },
  {
    "text": "we'll see that it's it's very common we've seen this incident that many people have seen it and we've seen it multiple times this example is an",
    "start": "1383480",
    "end": "1390080"
  },
  {
    "text": "interesting one but we've seen uh other issues and of course the community is aware of it and Masa will tell us what",
    "start": "1390080",
    "end": "1395240"
  },
  {
    "text": "it's been working on to address it okay so kubernetes Community has been working pretty hard to address those of",
    "start": "1395240",
    "end": "1401960"
  },
  {
    "text": "issues that we've seen especially like the example that Laurence showed us so",
    "start": "1401960",
    "end": "1407179"
  },
  {
    "text": "one of the really cool features that are in 127 kubernetes in Alpha currently is",
    "start": "1407179",
    "end": "1413299"
  },
  {
    "text": "streaming lists so what happens when you try to do the list from the API server cache basically API server is preparing",
    "start": "1413299",
    "end": "1422179"
  },
  {
    "text": "the list in memory and it holds it for the whole duration of the request so you",
    "start": "1422179",
    "end": "1427400"
  },
  {
    "text": "can imagine that you know like if you have resource that is let's say one gigabyte of config Maps then preparing",
    "start": "1427400",
    "end": "1433820"
  },
  {
    "text": "the response for the list call will be actually consuming quite a lot of memory",
    "start": "1433820",
    "end": "1439640"
  },
  {
    "text": "and in 127 we have the streaming list which basically you can think of as kind",
    "start": "1439640",
    "end": "1444860"
  },
  {
    "text": "of like watch where we stream the data back to the client without necessarily having all the response in the in the",
    "start": "1444860",
    "end": "1452659"
  },
  {
    "text": "memory of the API server uh cache so",
    "start": "1452659",
    "end": "1458480"
  },
  {
    "text": "let's see the example so how it worked before again like let's say we have one gigabyte of config maps and one instance",
    "start": "1458480",
    "end": "1465919"
  },
  {
    "text": "of API server and with eight informers you can see that the memory usage of API",
    "start": "1465919",
    "end": "1471440"
  },
  {
    "text": "server was even like 9 gigabytes just eight informers trying to list one",
    "start": "1471440",
    "end": "1477140"
  },
  {
    "text": "gigabyte of config Maps which is quite a lot but then like with 16 informers it",
    "start": "1477140",
    "end": "1482659"
  },
  {
    "text": "actually um and killed the API server with streaming lists what happens is",
    "start": "1482659",
    "end": "1488059"
  },
  {
    "text": "that you can run even 1000 informers which consume only like 11 gigabytes of",
    "start": "1488059",
    "end": "1493700"
  },
  {
    "text": "memory of the API server so it's 100x Improvement because of the streaming list basically",
    "start": "1493700",
    "end": "1499520"
  },
  {
    "text": "and memory usage of the API server so what else what else is there so the",
    "start": "1499520",
    "end": "1505400"
  },
  {
    "text": "priority and fairness we saw that there are multiple reasons why the control plane can be overwhelmed and priority",
    "start": "1505400",
    "end": "1512059"
  },
  {
    "text": "and fairness has been work on like quite quite extensively for like couple of",
    "start": "1512059",
    "end": "1517159"
  },
  {
    "text": "like I think like two years and it was um and it is in beta since 120 and the",
    "start": "1517159",
    "end": "1523340"
  },
  {
    "text": "main goal is to protect the API server from being overloaded and the idea is",
    "start": "1523340",
    "end": "1528559"
  },
  {
    "text": "that you limit amount of requests that can be executed concurrently but then again like you",
    "start": "1528559",
    "end": "1535460"
  },
  {
    "text": "split those concurrencies into different priority levels and within priority",
    "start": "1535460",
    "end": "1540500"
  },
  {
    "text": "level you can still have multiple different users who try to execute calls",
    "start": "1540500",
    "end": "1546440"
  },
  {
    "text": "um and as the name suggests it's also fairness so the concurrency Shares are",
    "start": "1546440",
    "end": "1552380"
  },
  {
    "text": "distributed across different users in a fair way um so let's take a look at the example",
    "start": "1552380",
    "end": "1558559"
  },
  {
    "text": "how it works so let's say that we have requests that comes to the API server and we have bunch of flow schemas flow",
    "start": "1558559",
    "end": "1565159"
  },
  {
    "text": "schema basically describes that which kind of request should go to which priority levels so it's kind of like",
    "start": "1565159",
    "end": "1570679"
  },
  {
    "text": "classifying them and let's say it goes it checks for the close schema number one but it doesn't match then it checks",
    "start": "1570679",
    "end": "1577640"
  },
  {
    "text": "flow schema number two and let's say that this request matches the flow schema number two",
    "start": "1577640",
    "end": "1582860"
  },
  {
    "text": "so full schema basically points to one of the priority levels that you have in your kubernetes cluster in this case",
    "start": "1582860",
    "end": "1590179"
  },
  {
    "text": "let's say it was priority level work called high so we know that the new request goes to the priority level",
    "start": "1590179",
    "end": "1596059"
  },
  {
    "text": "workload High but what happens next so one priority level actually contains",
    "start": "1596059",
    "end": "1602179"
  },
  {
    "text": "set of cues let's say that in this example we have three different queues and they are actually only two users",
    "start": "1602179",
    "end": "1608960"
  },
  {
    "text": "that are using this priority level and let's call them A and B and each",
    "start": "1608960",
    "end": "1615260"
  },
  {
    "text": "user has fixed amount of cues that are assigned to to this particular user so a",
    "start": "1615260",
    "end": "1620659"
  },
  {
    "text": "is assigned to queues number one and two and user B is assigned to q's q1 and Q3",
    "start": "1620659",
    "end": "1627320"
  },
  {
    "text": "so when we have this new request from user B it checks for how much work",
    "start": "1627320",
    "end": "1633320"
  },
  {
    "text": "is in the queues that are assigned to to this particular user so in this case q1 and Q3 so we have two two requests that",
    "start": "1633320",
    "end": "1640760"
  },
  {
    "text": "are waiting to be executed in the q1 and only one request in Q3 so priority and",
    "start": "1640760",
    "end": "1646880"
  },
  {
    "text": "furnace decides to to put this request in the Q3",
    "start": "1646880",
    "end": "1653080"
  },
  {
    "text": "um as Lauren mentioned before um requests can have basically different",
    "start": "1654020",
    "end": "1659539"
  },
  {
    "text": "impact on API server and hcd as well so priority and fairness takes that into account and one request can have",
    "start": "1659539",
    "end": "1667100"
  },
  {
    "text": "different concurrency weight um simple bet gets just one concurrency",
    "start": "1667100",
    "end": "1672860"
  },
  {
    "text": "weight but if you try to list something and you have thousands of PODS or config",
    "start": "1672860",
    "end": "1677900"
  },
  {
    "text": "Maps whatever then the weight of the concurrency assigned to this particular request can grow up to 10. similarly we",
    "start": "1677900",
    "end": "1685940"
  },
  {
    "text": "haven't mentioned much about mutating requests but how they work is that you know you change something and it's",
    "start": "1685940",
    "end": "1691700"
  },
  {
    "text": "simple operation from hcd point of view but then afterwards what happens is that",
    "start": "1691700",
    "end": "1696799"
  },
  {
    "text": "all the Watchers that are interested and watching for the changes you need to get the event that it was",
    "start": "1696799",
    "end": "1703100"
  },
  {
    "text": "changed so the more Watchers you have watching for particular resource the more expensive it becomes so priority",
    "start": "1703100",
    "end": "1710059"
  },
  {
    "text": "and fairness takes that into account and when you are doing the mutation of some of the resource and again it checks how",
    "start": "1710059",
    "end": "1718159"
  },
  {
    "text": "many Watchers there are and can assign different weight to the request based on that",
    "start": "1718159",
    "end": "1723380"
  },
  {
    "text": "so what's the default configuration of priority and fairness so there are six priority levels by default there is",
    "start": "1723380",
    "end": "1731179"
  },
  {
    "text": "exempt which basically bypasses the whole mechanism then we have the system which is meant only for the couplets",
    "start": "1731179",
    "end": "1737360"
  },
  {
    "text": "leader election only use for leader election of the core components like Cube controller manager scheduler or",
    "start": "1737360",
    "end": "1744320"
  },
  {
    "text": "different Cube system service accounts that use leader election then we have",
    "start": "1744320",
    "end": "1749840"
  },
  {
    "text": "workload High which is for only Cube controller manager and scheduler workload for all other components",
    "start": "1749840",
    "end": "1756559"
  },
  {
    "text": "including the node controller that we mentioned before which would be using workload law in this case and Global",
    "start": "1756559",
    "end": "1762080"
  },
  {
    "text": "default for everything else so if you for example do Cube cut or whatever then",
    "start": "1762080",
    "end": "1768200"
  },
  {
    "text": "it goes to the global default and basically what you can think of is that you're basically",
    "start": "1768200",
    "end": "1774440"
  },
  {
    "text": "competing with your co-workers for the concurrency shares in global default so",
    "start": "1774440",
    "end": "1779779"
  },
  {
    "text": "if it doesn't work like your request maybe you should ask the person next to you if and if he or she is running",
    "start": "1779779",
    "end": "1785480"
  },
  {
    "text": "something there let's see the example um priority level configuration so let's",
    "start": "1785480",
    "end": "1793039"
  },
  {
    "text": "take a look at the workload High it has 40 concurrency shares and 128 queues six",
    "start": "1793039",
    "end": "1800360"
  },
  {
    "text": "hand side so hand size is basically the amount of cues that are assigned to one particular user then also the queue is",
    "start": "1800360",
    "end": "1808399"
  },
  {
    "text": "limited so we don't want to have infinite amount of requests waiting in",
    "start": "1808399",
    "end": "1813500"
  },
  {
    "text": "the queues so each queue can hold up to 50 50 requests and one more pretty cool",
    "start": "1813500",
    "end": "1819320"
  },
  {
    "text": "important um [Music] um feature that was implemented in 126 is",
    "start": "1819320",
    "end": "1827120"
  },
  {
    "text": "um borrowing in priority and fairness so when one priority level does not",
    "start": "1827120",
    "end": "1832700"
  },
  {
    "text": "really need those concurrency shares it can lend lend them to other priority levels and in this example workload high",
    "start": "1832700",
    "end": "1840080"
  },
  {
    "text": "can lend 50 of concurrency shares to other priority levels if they need them",
    "start": "1840080",
    "end": "1846140"
  },
  {
    "text": "so let's get back to some cool examples how you can use priority and fairness in",
    "start": "1846140",
    "end": "1851240"
  },
  {
    "text": "your clusters so you can have for example misbehaving",
    "start": "1851240",
    "end": "1856399"
  },
  {
    "text": "controller or demand set like we had in the case that Lauren mentioned and with",
    "start": "1856399",
    "end": "1861799"
  },
  {
    "text": "priority In fairness the cluster should still be working and it should be protected from uh from being overloaded",
    "start": "1861799",
    "end": "1869899"
  },
  {
    "text": "it still can impact other controllers running within the same priority level",
    "start": "1869899",
    "end": "1875240"
  },
  {
    "text": "um basically throttling them so what you can do is actually create new priority level and new flow schema that will",
    "start": "1875240",
    "end": "1882200"
  },
  {
    "text": "redirect all those requests to new priority level if you want to mitigate the issue that your controller is",
    "start": "1882200",
    "end": "1889159"
  },
  {
    "text": "um basically putting load on other basically studying the concurrency",
    "start": "1889159",
    "end": "1894559"
  },
  {
    "text": "shares from other controllers one other use case we mentioned that it's good idea to split LCD into like the main LCD",
    "start": "1894559",
    "end": "1902360"
  },
  {
    "text": "and the events at CD and well sometimes you cannot do that right so with high",
    "start": "1902360",
    "end": "1908360"
  },
  {
    "text": "churn of events what you can do is create new priority level and assign all the requests all the requests from from",
    "start": "1908360",
    "end": "1915020"
  },
  {
    "text": "related to events like creation or lists and and basically throttle how much",
    "start": "1915020",
    "end": "1920059"
  },
  {
    "text": "events are being processed of course worst case um you will miss some events but it's",
    "start": "1920059",
    "end": "1926000"
  },
  {
    "text": "still better than losing your control plane so conclusions running large clusters is",
    "start": "1926000",
    "end": "1933320"
  },
  {
    "text": "still challenging there has been many improvements from the community defaults are not always enough and most",
    "start": "1933320",
    "end": "1940820"
  },
  {
    "text": "importantly please avoid list calls and if you really need to list the list from the API cache and also use informers",
    "start": "1940820",
    "end": "1949340"
  },
  {
    "text": "thank you all for coming and now we have some time for questions [Applause]",
    "start": "1949340",
    "end": "1963230"
  },
  {
    "text": "hi we are wearing on Amazon and actually we have a similar but slightly different",
    "start": "1972460",
    "end": "1978380"
  },
  {
    "text": "challenge winning dense clusters with lots of Parts on bare metal that you do anything with those shoes cases",
    "start": "1978380",
    "end": "1986380"
  },
  {
    "text": "could you repeat with large number of the winning dance clusters with large numbers of bots mainly small apis so we",
    "start": "1987799",
    "end": "1994940"
  },
  {
    "text": "have a little bit better by the memory and we try to achieve 500 parts per notes but we see all kind of strange",
    "start": "1994940",
    "end": "2000820"
  },
  {
    "text": "things happening there yeah so actually it's it's pretty interesting I think there are like two different parts of",
    "start": "2000820",
    "end": "2006940"
  },
  {
    "text": "this problem right like one is the control plane that we mentioned here uh how how it works but then also I could",
    "start": "2006940",
    "end": "2013360"
  },
  {
    "text": "imagine that there might be some problems within like just the kublic right and and managing those spots so",
    "start": "2013360",
    "end": "2019720"
  },
  {
    "text": "um I'm not sure which kind of Strange Behaviors you've seen but um it can be either a control plane or",
    "start": "2019720",
    "end": "2025480"
  },
  {
    "text": "or maybe the scuba itself so",
    "start": "2025480",
    "end": "2029220"
  },
  {
    "text": "and and we have many issues with our cumulus clusters but we know we don't have these ones because we're one out of",
    "start": "2030640",
    "end": "2036820"
  },
  {
    "text": "dance nodes so at data dog we have a lot of nodes but most of them are running from a few to a few dozens of birds so",
    "start": "2036820",
    "end": "2043360"
  },
  {
    "text": "we don't have the issues you're imagining but I wouldn't be surprised if it's tough on on CNN and on the qubit 2.",
    "start": "2043360",
    "end": "2049240"
  },
  {
    "text": "yeah also I would add that you know like for large clusters um the kubernetes actually recommends 30 pots per note",
    "start": "2049240",
    "end": "2055658"
  },
  {
    "text": "like the maximum supported is 110 so with 500 you know like anything can",
    "start": "2055659",
    "end": "2062260"
  },
  {
    "text": "happen basically hello question here",
    "start": "2062260",
    "end": "2067378"
  },
  {
    "text": "so assuming that we keep the limit of 120 parts per node and we still have a",
    "start": "2067379",
    "end": "2073658"
  },
  {
    "text": "large cluster and even we try to go with the best practices that you presented uh",
    "start": "2073659",
    "end": "2079540"
  },
  {
    "text": "what is the bottle like how much can we scale a city database to actually",
    "start": "2079540",
    "end": "2085138"
  },
  {
    "text": "keep large clusters hundreds of notes working yeah so actually it depends on your use",
    "start": "2085139",
    "end": "2092919"
  },
  {
    "text": "case for example if you use Services heavily services are super super",
    "start": "2092919",
    "end": "2098859"
  },
  {
    "text": "um expensive for control plane because you have list of endpoints within the",
    "start": "2098859",
    "end": "2104200"
  },
  {
    "text": "service and they need to be broadcasted basically to to all the nodes so um I would say that",
    "start": "2104200",
    "end": "2110380"
  },
  {
    "text": "first of all you should take a look at your workloads that you're running and understand which parts of those",
    "start": "2110380",
    "end": "2116200"
  },
  {
    "text": "workloads are actually putting a heavy load on the APA server to give you an example like some batch workloads",
    "start": "2116200",
    "end": "2123280"
  },
  {
    "text": "um can be run with even like 15 000 nodes if they are like super simple",
    "start": "2123280",
    "end": "2128680"
  },
  {
    "text": "as compared to like serving workloads with services",
    "start": "2128680",
    "end": "2133740"
  },
  {
    "text": "question have you reached the limit of scaling AC to the database at some point",
    "start": "2134020",
    "end": "2141119"
  },
  {
    "text": "so I'm I'm can only talk for datadog and I don't think we we have I think the",
    "start": "2143560",
    "end": "2148839"
  },
  {
    "text": "biggest issue we had at one point is we hit the two gigabyte limits because for the default side of hcd but now that the",
    "start": "2148839",
    "end": "2155680"
  },
  {
    "text": "limit is eight gigabytes we've never reached it but also we're we're careful right I mean we're trying to make sure",
    "start": "2155680",
    "end": "2161200"
  },
  {
    "text": "that we never get pleasure with more than five to six thousand nodes and about probably 60 50 000 codes",
    "start": "2161200",
    "end": "2167859"
  },
  {
    "text": "uh and and within this envelope we've been good right um and and something we also we should have",
    "start": "2167859",
    "end": "2175060"
  },
  {
    "text": "mentioned too but it was hard hard to have everything in the targets sometimes chocolate server is very stable at a",
    "start": "2175060",
    "end": "2180940"
  },
  {
    "text": "very large scale and your everything is okay but an event happens and and then you get in a situation that's not stable",
    "start": "2180940",
    "end": "2187300"
  },
  {
    "text": "anymore and the problem is you don't have enough buffer to further closer to continue behaving so even if things are",
    "start": "2187300",
    "end": "2193060"
  },
  {
    "text": "running you still make sure that you have enough buffer in terms of memory CPU so if there's a bad event happening",
    "start": "2193060",
    "end": "2199300"
  },
  {
    "text": "your cluster can still recover because we've been in that situation where things are completely okay at a given",
    "start": "2199300",
    "end": "2204460"
  },
  {
    "text": "size and then there's a big thing happening and you can't really recover easily from from it yes and also I would",
    "start": "2204460",
    "end": "2210880"
  },
  {
    "text": "add that priority and fairness comes here pretty well that you know like you had stable cluster and then some events",
    "start": "2210880",
    "end": "2216040"
  },
  {
    "text": "happen and priority inference should take that into account and basically throttle you know let's say that you",
    "start": "2216040",
    "end": "2221140"
  },
  {
    "text": "want to create 100 pods right and um priority internet should start totally in creation of the pods scheduling those",
    "start": "2221140",
    "end": "2227200"
  },
  {
    "text": "spots taking into account like the whole broadcasting of events and basically offload kind of the you know spread it",
    "start": "2227200",
    "end": "2234339"
  },
  {
    "text": "across the time um thank you uh here uh thank you for",
    "start": "2234339",
    "end": "2241359"
  },
  {
    "text": "the talk um did you consider in the node controller thing you had to use",
    "start": "2241359",
    "end": "2246820"
  },
  {
    "text": "owner references with uh block owner deletion or was it kind of not a fit for your use case",
    "start": "2246820",
    "end": "2253859"
  },
  {
    "text": "um I don't know um",
    "start": "2254740",
    "end": "2259900"
  },
  {
    "text": "it might be the case now that we use on a reference right so you mean when you to make you mean to make sure that we",
    "start": "2259900",
    "end": "2266200"
  },
  {
    "text": "can delete nodes while Port are still running right I think it might be the case that we intend protection this way now",
    "start": "2266200",
    "end": "2271540"
  },
  {
    "text": "at the time it was I I the first approach was very naive because we just wanted to address the issue but I'm sure",
    "start": "2271540",
    "end": "2277300"
  },
  {
    "text": "we um we've been I know we've been improving it a lot and it's probably one of the approaches we use",
    "start": "2277300",
    "end": "2282760"
  },
  {
    "text": "okay yeah cool I was thinking from the perspective of like if it's not it should work that way and",
    "start": "2282760",
    "end": "2290260"
  },
  {
    "text": "should you work for that use case but if it was some kind of limiting factor in product in practice but maybe maybe you",
    "start": "2290260",
    "end": "2296440"
  },
  {
    "text": "can use it okay thank you very much",
    "start": "2296440",
    "end": "2300599"
  },
  {
    "text": "hello thank you for recession uh I have one question regarding the max request in Flight parameter does it affect at",
    "start": "2312420",
    "end": "2320619"
  },
  {
    "text": "all your your testing because I believe that by default it's some of the",
    "start": "2320619",
    "end": "2325660"
  },
  {
    "text": "mutating uh imitating requests and the those normal ones yeah yeah so before",
    "start": "2325660",
    "end": "2332619"
  },
  {
    "text": "priority in furnace this was like the only kind of like overload protection mechanism and now since the priority and",
    "start": "2332619",
    "end": "2338380"
  },
  {
    "text": "furnace as you mentioned like they're summed up right and those concurrency Shares are basically like the total",
    "start": "2338380",
    "end": "2344740"
  },
  {
    "text": "amount of requests that can be processed within the API priority and furnace so this is like super important to actually",
    "start": "2344740",
    "end": "2350820"
  },
  {
    "text": "configure it properly to understand like how many requests your control plane can can execute because if you set it like",
    "start": "2350820",
    "end": "2358180"
  },
  {
    "text": "to million then the priority and parents will not you know kick in basically when it when it's needed",
    "start": "2358180",
    "end": "2363880"
  },
  {
    "text": "okay so what values uh did you use to your test I mean",
    "start": "2363880",
    "end": "2370920"
  },
  {
    "text": "have you used the default ones or have you done it so I can speak from like six",
    "start": "2371320",
    "end": "2376599"
  },
  {
    "text": "collability point of view so we we run um performance tests to to ensure that there are no regressions in kubernetes",
    "start": "2376599",
    "end": "2382599"
  },
  {
    "text": "and for that particular case what we do is we specify 10 inflights per one core",
    "start": "2382599",
    "end": "2388359"
  },
  {
    "text": "of the API server cool thank you",
    "start": "2388359",
    "end": "2393780"
  }
]