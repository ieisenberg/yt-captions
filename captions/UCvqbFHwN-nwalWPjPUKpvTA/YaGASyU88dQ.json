[
  {
    "text": "myself is an amazing I'm a chief architect in for data in AI open-source platform in IBM his Clive Cox",
    "start": "0",
    "end": "8519"
  },
  {
    "text": "CG of Selden company based out of London doing a lot of great work around model inferencing and model serving so today",
    "start": "8519",
    "end": "16560"
  },
  {
    "text": "we are going to talk about in general introduction to machine learning and the challenges specifically in the context",
    "start": "16560",
    "end": "23100"
  },
  {
    "text": "of moral serving and the challenges we face there and then you know once you have served your models or deployed your",
    "start": "23100",
    "end": "29010"
  },
  {
    "text": "models how do you actually monitor it and finally you know wrap it up is some summary in description well this is a",
    "start": "29010",
    "end": "35309"
  },
  {
    "text": "port which you probably would have seen you know many times over right it took me three weeks to develop my model more",
    "start": "35309",
    "end": "41190"
  },
  {
    "text": "than 11 months and still not deployed a lot of people think it's exaggerated but",
    "start": "41190",
    "end": "46289"
  },
  {
    "text": "this is you know it came out of one of our VPS and actually you know talking to customers that's the comment we heard",
    "start": "46289",
    "end": "52199"
  },
  {
    "text": "right and we are surprised how much is resonated with a lot of p48 that essentially that's the case and why is",
    "start": "52199",
    "end": "60059"
  },
  {
    "text": "it the case because the general perception is that you know ml code is the biggest part of your machine",
    "start": "60059",
    "end": "65100"
  },
  {
    "text": "learning infrastructure in reality it's a very tiny part right a lot of efforts",
    "start": "65100",
    "end": "70439"
  },
  {
    "text": "are spent around you know data collection verification process engineering model serving infrastructure",
    "start": "70439",
    "end": "75770"
  },
  {
    "text": "suspend a lot more effort surrounding that ml code than in the ML code itself right and what is happening in industry",
    "start": "75770",
    "end": "82439"
  },
  {
    "text": "in general is that you know to handle this end-to-end ml lifecycle starting",
    "start": "82439",
    "end": "87570"
  },
  {
    "text": "from you know data preparation which includes you know data cleansing data ingestion data analysis and transformation validation and then going",
    "start": "87570",
    "end": "95280"
  },
  {
    "text": "through the whole model lifecycle which is you know building your models training your models validating deploying a lot of technologies are",
    "start": "95280",
    "end": "101460"
  },
  {
    "text": "emerging to actually you know condense that right and make them faster to take you from step A to Z much faster so",
    "start": "101460",
    "end": "109110"
  },
  {
    "text": "essentially a lot of us are now doing end-to-end machine learning on kubernetes right and to do that a you can become",
    "start": "109110",
    "end": "116820"
  },
  {
    "text": "you know an expert in a lot of these things containers you know config map secrets GPUs a lot",
    "start": "116820",
    "end": "123420"
  },
  {
    "text": "of things which you are hearing in this conference or if you want to bypass a lot of that then you can use something like cue flow",
    "start": "123420",
    "end": "130340"
  },
  {
    "text": "right it's an end-to-end ml platform being on top of kubernetes to take away the pains right so that you're not required",
    "start": "130340",
    "end": "137370"
  },
  {
    "text": "to be a community's admin or actually IDs expert to able to be able to do n2n",
    "start": "137370",
    "end": "142860"
  },
  {
    "text": "machine learning on top of cuban at ease it handles all the phases of machine learning lifecycle you know right from",
    "start": "142860",
    "end": "149790"
  },
  {
    "text": "building your models training your models deploying your models on different you know or model serving",
    "start": "149790",
    "end": "155460"
  },
  {
    "text": "infrastructures and then you know trying them all together through a pipeline so",
    "start": "155460",
    "end": "161430"
  },
  {
    "text": "if you're into distributed training there is a project in chi flow umbrella called catted plus you know there are",
    "start": "161430",
    "end": "167880"
  },
  {
    "text": "distributed training jobs for tensorflow pi taj actually boost chainer if you like Horrible's approach of MPI base",
    "start": "167880",
    "end": "174870"
  },
  {
    "text": "training where weights and gradients on st. on a more peer-to-peer basis that support it as well right so distributed",
    "start": "174870",
    "end": "181830"
  },
  {
    "text": "training at scale is supported through queue flow q flow pipelines right as I",
    "start": "181830",
    "end": "187800"
  },
  {
    "text": "mentioned there are multiple phases in end-to-end machine learning where you actually go from the process of you know",
    "start": "187800",
    "end": "193620"
  },
  {
    "text": "taking your data cleaning it and then going through you know creating your models training it deploying it managing",
    "start": "193620",
    "end": "200700"
  },
  {
    "text": "it chip flow pipeline is essentially you know allows you to do all these steps in a single consistent way so it's a",
    "start": "200700",
    "end": "207989"
  },
  {
    "text": "sequence of ML tasks each backed by a container so it allows you to go from step A to Z but that's not the only",
    "start": "207989",
    "end": "215459"
  },
  {
    "text": "important part right apart from growing from A to Z you know when you actually want to trace back the lineage from Z to",
    "start": "215459",
    "end": "222540"
  },
  {
    "text": "a that my model was trained on this particular data set on this version of tensor flow using these hyper parameters",
    "start": "222540",
    "end": "228840"
  },
  {
    "text": "it was validated using these libraries deployed and these are the scoring characteristics a lot of their data you",
    "start": "228840",
    "end": "235170"
  },
  {
    "text": "can actually find you know using cue flow pipelines so whole metadata tracking and lineage is enabled IBM in",
    "start": "235170",
    "end": "243209"
  },
  {
    "text": "Selden both are major contributors in queue flow right iBM is the second largest contributor over the course of",
    "start": "243209",
    "end": "248280"
  },
  {
    "text": "last year we are contributing heavily to queue flow but the community is strong and very big lot of great work is coming",
    "start": "248280",
    "end": "255030"
  },
  {
    "text": "you know from Microsoft Seldon Cisco electro plus you know AWS a",
    "start": "255030",
    "end": "262560"
  },
  {
    "text": "lot of companies have now started contributing and this community as you can see from this chart growing heavily and come January 2020 we",
    "start": "262560",
    "end": "272699"
  },
  {
    "text": "are actually going to launch Q flow 100 right which is focused on four major co",
    "start": "272699",
    "end": "278820"
  },
  {
    "text": "J's we are essentially you know want to make sure that our develop build deploy and training lifecycle is total stable",
    "start": "278820",
    "end": "286169"
  },
  {
    "text": "its performant and we are able to actually introduce you know a lot of features around user authentication",
    "start": "286169",
    "end": "291930"
  },
  {
    "text": "authorization multi-tenancy as part of 1.0 so now let's get to the context and",
    "start": "291930",
    "end": "299370"
  },
  {
    "text": "talk about you know production mmm model serving why is it you know typically",
    "start": "299370",
    "end": "305400"
  },
  {
    "text": "hard right so some of the problems are more genetic in nature right so things like you know is your infrastructure",
    "start": "305400",
    "end": "311280"
  },
  {
    "text": "under scale or over scale right can you monitor your deployed models are they healthy can you roll out a new version",
    "start": "311280",
    "end": "317610"
  },
  {
    "text": "of the model if things go wrong can you roll back right can you actually swap traffic you know without you know",
    "start": "317610",
    "end": "324240"
  },
  {
    "text": "changing the infrastructure and the deployed models how can you actually do a genetic prediction using different",
    "start": "324240",
    "end": "329789"
  },
  {
    "text": "protocols HTTP G RPC acceptor now when you move you know more to the model specific world right general problems",
    "start": "329789",
    "end": "337139"
  },
  {
    "text": "like I want to serve tensor flow models PI torch scikit-learn actually boost in a consistent way single spec I don't",
    "start": "337139",
    "end": "344160"
  },
  {
    "text": "want to write you know different sort of clients to deal with different kinds of models right and once the models are",
    "start": "344160",
    "end": "349289"
  },
  {
    "text": "deployed right can I explain the model predictions can I detect concept drift can I do bias detection can I do",
    "start": "349289",
    "end": "355800"
  },
  {
    "text": "adversarial detection can the model explain its predictions a lot of these problems which are very specific to the",
    "start": "355800",
    "end": "362280"
  },
  {
    "text": "model deployment world right and another thing which is very common is custom pre-processing and post-processing code",
    "start": "362280",
    "end": "368190"
  },
  {
    "text": "how do you actually handle because you know your models are essentially dealing",
    "start": "368190",
    "end": "373710"
  },
  {
    "text": "with tensors in tensors out right what an end user is sending is probably images text data right so a lot of times",
    "start": "373710",
    "end": "381030"
  },
  {
    "text": "in fact you know most of the times you will end up writing you know custom free processes and post processes can that be",
    "start": "381030",
    "end": "387510"
  },
  {
    "text": "supported so I think you know different parts of these problems were being done",
    "start": "387510",
    "end": "393330"
  },
  {
    "text": "by different teams at lost cube corn in Seattle we gave a talk you know how to",
    "start": "393330",
    "end": "398580"
  },
  {
    "text": "use server less inferencing using K native bloom horrible is actually doing great work on that side",
    "start": "398580",
    "end": "404910"
  },
  {
    "text": "you know Microsoft was doing a lot of work in terms of you know as your M alone kubernetes Selden was doing great",
    "start": "404910",
    "end": "411120"
  },
  {
    "text": "work around you know model graph inference saying and then you know Google obviously had been doing a lot of",
    "start": "411120",
    "end": "416310"
  },
  {
    "text": "work around tensorflow serving and optimizing heavily around it so if you're all solving you know different parts of the problem and what we did was",
    "start": "416310",
    "end": "422700"
  },
  {
    "text": "over the course of last year we all came together under one umbrella under to flow and launch this project right and",
    "start": "422700",
    "end": "429150"
  },
  {
    "text": "the goal was let's work together we are solving similar problems that solve it consistently and drive towards the",
    "start": "429150",
    "end": "434970"
  },
  {
    "text": "standard we don't want more fragmentation in the industry so that's how Jeff serving was born and before I",
    "start": "434970",
    "end": "441960"
  },
  {
    "text": "actually go I would like to know Ellis from Google and Rakesh from Microsoft you know just to stand up and wave you",
    "start": "441960",
    "end": "448380"
  },
  {
    "text": "know and Dan is here from Bloomberg so these are you know our co-conspirators",
    "start": "448380",
    "end": "454910"
  },
  {
    "text": "so definitely you know five of us five companies you know got together essentially to co-found this and over",
    "start": "456140",
    "end": "462960"
  },
  {
    "text": "the course of you know lost I would say seven to eight months this has become very popular if you are following you",
    "start": "462960",
    "end": "468270"
  },
  {
    "text": "know queue flow in the last launch of 0.7 K F serving was the major release",
    "start": "468270",
    "end": "474390"
  },
  {
    "text": "feature highlighted there and our goal initially was to focus on 80% of the use cases initially right which is",
    "start": "474390",
    "end": "480030"
  },
  {
    "text": "essentially single model rollout and the key things which we wanted to focus there was the Savalas characteristics",
    "start": "480030",
    "end": "486390"
  },
  {
    "text": "cannery explaining model predictions and then being able to give you an ability to inject pre and post processing if",
    "start": "486390",
    "end": "493500"
  },
  {
    "text": "needed so that's how you know we ended up building k of serving at the very",
    "start": "493500",
    "end": "499860"
  },
  {
    "text": "bottom we have our layers right the hardware layer GPU CPU CPU and then you",
    "start": "499860",
    "end": "505020"
  },
  {
    "text": "know kubernetes sits on top and then is to you and k native and finally with key of serving right we essentially strive",
    "start": "505020",
    "end": "511170"
  },
  {
    "text": "to bring you know these multiple frameworks together tensorflow pi touch on extensive RT and then give a common",
    "start": "511170",
    "end": "517260"
  },
  {
    "text": "set of functions around them being able to predict right get a request and get a response back but going beyond that",
    "start": "517260",
    "end": "523669"
  },
  {
    "text": "being able to explain model predictions being able to inject custom pre-processing and post-processing now",
    "start": "523670",
    "end": "530760"
  },
  {
    "text": "why actually did we choose K native one of the key things was definitely you know this scale to zero and the scale",
    "start": "530760",
    "end": "538470"
  },
  {
    "text": "from 0 to N was an important feature that was one of the core principles we wanted to follow as part of you know",
    "start": "538470",
    "end": "543890"
  },
  {
    "text": "model serving it was a personal experience that over a period of time these infrastructures become very huge",
    "start": "543890",
    "end": "550110"
  },
  {
    "text": "given that you know model serving requirements are very you know compute intensive right so something that we",
    "start": "550110",
    "end": "556800"
  },
  {
    "text": "wanted another thing is you know the queue or request queue based auto scaling feature in K native that lends",
    "start": "556800",
    "end": "562140"
  },
  {
    "text": "itself nicely when you are actually working on disparate hardware infrastructure like GPUs CPUs GPUs",
    "start": "562140",
    "end": "568490"
  },
  {
    "text": "because traditionally CPU based auto scaling can also be enabled in Kenya today but when you have these different",
    "start": "568490",
    "end": "574140"
  },
  {
    "text": "sets of hardware you know it's not easier to actually be consistent so I think request metal based auto scaling",
    "start": "574140",
    "end": "579480"
  },
  {
    "text": "works very nicely in this case history oh definitely was needed you know as I mentioned cannery was one of the key",
    "start": "579480",
    "end": "586050"
  },
  {
    "text": "features which we wanted to support so being able to you know do traffic splitting traffic control support",
    "start": "586050",
    "end": "592320"
  },
  {
    "text": "Canidae deployments and then also you know another feature of history which we really wanted to enable was you know",
    "start": "592320",
    "end": "598440"
  },
  {
    "text": "observability so being able to you know have metrics logging tracing which we can use as part of the model serving",
    "start": "598440",
    "end": "604830"
  },
  {
    "text": "infrastructure and then enhance and customize it for more serving so this is",
    "start": "604830",
    "end": "610770"
  },
  {
    "text": "how you know typical care serving or default in Canada configuration looks like infinite service is actually at the",
    "start": "610770",
    "end": "617040"
  },
  {
    "text": "heart of this right it manages the life cycle of the deployed models one key distinction which you see here from K",
    "start": "617040",
    "end": "623370"
  },
  {
    "text": "native is essentially the concept of cannery right so we actually have our",
    "start": "623370",
    "end": "629760"
  },
  {
    "text": "whole configuration for cannery so that you can you know I curate multiple times over cannery until you want to make it a",
    "start": "629760",
    "end": "635280"
  },
  {
    "text": "default and part of the reasoning here is that you know a it lends itself nicely to get ops be machine learning in",
    "start": "635280",
    "end": "642600"
  },
  {
    "text": "general is very very experimental in nature right the amount of iterations you will have to do in machine learning before you get to the you know the final",
    "start": "642600",
    "end": "649710"
  },
  {
    "text": "model which you want to deploy is very huge so we wanted to make sure that you know not we are not impacting the main",
    "start": "649710",
    "end": "654720"
  },
  {
    "text": "revision of the default deployed model before we actually activate multiple times on the cannery right so that's one",
    "start": "654720",
    "end": "660810"
  },
  {
    "text": "key distinction which we have here as I mentioned right we support multiple of",
    "start": "660810",
    "end": "665850"
  },
  {
    "text": "these models service chance flowpile oh Sh actually boo scikit-learn onyx and if you're falling in general",
    "start": "665850",
    "end": "671340"
  },
  {
    "text": "the onyx ecosystem onyx is a way to actually standardize you know your multiple models right so if you have",
    "start": "671340",
    "end": "677820"
  },
  {
    "text": "chain models using tensorflow Pytor scikit-learn you can convert it into onyx format and deploy onyx runtime one",
    "start": "677820",
    "end": "685470"
  },
  {
    "text": "dot whole was recently launched plus you know just last week onyx moved into linux foundation AI in total open",
    "start": "685470",
    "end": "691680"
  },
  {
    "text": "governance so that's an exciting news for us at IBM and for the community and we are actually you know also",
    "start": "691680",
    "end": "696750"
  },
  {
    "text": "contributing heavily to onyx as well we support you know multiple different types of storage you know GC is persistent volume claims as your blob s3",
    "start": "696750",
    "end": "704700"
  },
  {
    "text": "compatible storage etc if you look at the control plane as i mentioned right",
    "start": "704700",
    "end": "709950"
  },
  {
    "text": "some of the key characteristics which we wanted to focus on was definitely enabling default prediction but as the",
    "start": "709950",
    "end": "715440"
  },
  {
    "text": "requests are coming both for default and cannery we wanted to make sure that you know we are able to provide capability",
    "start": "715440",
    "end": "721080"
  },
  {
    "text": "to inject pre-processing and post-processing as you know the introduction of transformer plus a you",
    "start": "721080",
    "end": "726840"
  },
  {
    "text": "know it enables us to do some intelligent routing in terms of taking the request directly to predictor or to",
    "start": "726840",
    "end": "732270"
  },
  {
    "text": "explain it and then from there to predictor right once you actually deploy",
    "start": "732270",
    "end": "738600"
  },
  {
    "text": "care serving at your physical infrastructure level it may look like this right we actually use this to",
    "start": "738600",
    "end": "744330"
  },
  {
    "text": "increase for you know the external traffic coming in you can also use your",
    "start": "744330",
    "end": "750750"
  },
  {
    "text": "or load balancer you know if you want to configure like that there are certain deployments which we have done which you",
    "start": "750750",
    "end": "756210"
  },
  {
    "text": "can actually use you know external load balancer and then k native you know the key things which we are definitely",
    "start": "756210",
    "end": "761820"
  },
  {
    "text": "leveraging is autoscaler scale to zero zero to one that's one of the key things explained in a predictor if you see was",
    "start": "761820",
    "end": "768060"
  },
  {
    "text": "deployed they look you know pretty much the same one thing which you see here which is again a little bit different",
    "start": "768060",
    "end": "774180"
  },
  {
    "text": "from Cana demon from structure is storage initializer models machine learning models they are very heavy you",
    "start": "774180",
    "end": "779610"
  },
  {
    "text": "know typically you know you have a lot of the weights and the graphs right which can become you know gigabytes in",
    "start": "779610",
    "end": "785610"
  },
  {
    "text": "size and if you are actually downloading each of them from a remote storage at every time you are initializing your",
    "start": "785610",
    "end": "791670"
  },
  {
    "text": "model containers you know that can become a heavy cost right so we introduced you know quite a bit of caching using storage initializer so as",
    "start": "791670",
    "end": "799260"
  },
  {
    "text": "to be able to optimize that cost you know bring it up faster and another",
    "start": "799260",
    "end": "806100"
  },
  {
    "text": "key thing which we have been doing is not only being able to deploy models in a standard way but also being able to",
    "start": "806100",
    "end": "811740"
  },
  {
    "text": "interact with models in a standard way because what was happening is that you know if you're using tensorflow you are",
    "start": "811740",
    "end": "817410"
  },
  {
    "text": "using a custom protocol for pre-processing and pull or you know just to do request response same with you",
    "start": "817410",
    "end": "823829"
  },
  {
    "text": "know onyx or if you are using you know Selden tensorflow tensor RT server right they all have",
    "start": "823829",
    "end": "831660"
  },
  {
    "text": "their own data plane protocols how do you actually send a request how do you get a response right one of the key goes with respect to you know Kiev cell ring",
    "start": "831660",
    "end": "837600"
  },
  {
    "text": "was also to make sure that you know beyond the standardization of the model deployment spec we also focus on the",
    "start": "837600",
    "end": "844740"
  },
  {
    "text": "standardization of the interaction with the model right so once you have written a client to actually talk to K of",
    "start": "844740",
    "end": "851370"
  },
  {
    "text": "serving get a request response back get an explanation back you shall be able to use that same client idly with",
    "start": "851370",
    "end": "858240"
  },
  {
    "text": "tensorflow serving with onyx runtime with Selden right so the idea was that",
    "start": "858240",
    "end": "863250"
  },
  {
    "text": "you know bring that a standardization and in v1 we have actually focused on a lot of these capabilities some you know",
    "start": "863250",
    "end": "872820"
  },
  {
    "text": "sample Yama's in terms of you know the standardization of the deployment spec as you can see around seven lines at the",
    "start": "872820",
    "end": "880500"
  },
  {
    "text": "most you know seven to eight lines of course imple yeah Mille to actually deploy scikit-learn tensorflow potage",
    "start": "880500",
    "end": "886170"
  },
  {
    "text": "models in a very consistent standard manner there is no difference exposed to the",
    "start": "886170",
    "end": "891630"
  },
  {
    "text": "end-user right was deploying these models and then you can add you know three four more lines into these specs",
    "start": "891630",
    "end": "897630"
  },
  {
    "text": "and get kennedy and pinned rollouts right so in this case for example you see that you know we are specifying",
    "start": "897630",
    "end": "902640"
  },
  {
    "text": "there is a default there is a cannery and there is a ninety ten percent split between default and cannery right so",
    "start": "902640",
    "end": "909209"
  },
  {
    "text": "that's one of the key things which we have enabled as part of this I do have a",
    "start": "909209",
    "end": "915510"
  },
  {
    "text": "demo to show at this point you know it's on my laptop so I'll try and restart my laptop and flip but until then that",
    "start": "915510",
    "end": "922500"
  },
  {
    "text": "happens right at this point I will talk and pass on to Clyde to talk about you know the next phase right okay you were",
    "start": "922500",
    "end": "928500"
  },
  {
    "text": "able to deploy your models using a standard spec then you are able to interact with your models using a",
    "start": "928500",
    "end": "934500"
  },
  {
    "text": "standard right so you're able to get to that point now that your models are giving predictions can they be trusted right so",
    "start": "934500",
    "end": "941490"
  },
  {
    "text": "are they you know while they're able to adversarial attacks can you explain your model predictions is their concept drift",
    "start": "941490",
    "end": "946620"
  },
  {
    "text": "right is there an outlier so Clive is going to actually dive into some of these production requirements which we",
    "start": "946620",
    "end": "951899"
  },
  {
    "text": "are hearing from our clients with that clay please ball great thanks ash so",
    "start": "951899",
    "end": "957540"
  },
  {
    "text": "yeah I'm gonna take you into those he's told to me about how to get the core model into deployment and how to do a",
    "start": "957540",
    "end": "963209"
  },
  {
    "text": "roll out of that but now we want to take it to the next level whatever things you need to add to your core model to really",
    "start": "963209",
    "end": "969120"
  },
  {
    "text": "get a robust production ml system one of those is model explanations and we'll go why you might need that and how you do",
    "start": "969120",
    "end": "975089"
  },
  {
    "text": "it then you might want to also record your payloads as they're going through if you can record the payloads the",
    "start": "975089",
    "end": "980790"
  },
  {
    "text": "requests and responses then you can do things with them in their time after the models made the prediction you can attach them to some technology like a",
    "start": "980790",
    "end": "986940"
  },
  {
    "text": "native and then push those payloads out to things that will give you understanding what's happening to your",
    "start": "986940",
    "end": "993930"
  },
  {
    "text": "model and giving you a metrics as to whether it's what is working well in productions things like outlier",
    "start": "993930",
    "end": "999420"
  },
  {
    "text": "detection we'll go into that at the soil detection and concept Jeff so you can get a really good understanding is my",
    "start": "999420",
    "end": "1005000"
  },
  {
    "text": "model working as expected in production so go through those different stages so",
    "start": "1005000",
    "end": "1010430"
  },
  {
    "text": "why would you want to explain your models well first there's lots of regulation in Europe GDP are so people who are submitted to automated automated",
    "start": "1010430",
    "end": "1019000"
  },
  {
    "text": "decision processes need to be able to be given some explanations of why they were",
    "start": "1019000",
    "end": "1024610"
  },
  {
    "text": "given the particular predictions that they did so that's obviously key for any models you're putting in production in",
    "start": "1024610",
    "end": "1030770"
  },
  {
    "text": "those countries and there's also regulation in the u.s. in that area secondly for the people such as data scientists DevOps they need to",
    "start": "1030770",
    "end": "1037668"
  },
  {
    "text": "understand why is my model doing what is expected to be doing and if you want it",
    "start": "1037669",
    "end": "1043250"
  },
  {
    "text": "actually I'm investigate the actual behavior of the model on particular inputs which are unexpected such as",
    "start": "1043250",
    "end": "1048530"
  },
  {
    "text": "outliers so all these reasons why you might want to have explanations to give you a better understanding of what your",
    "start": "1048530",
    "end": "1054350"
  },
  {
    "text": "model is doing four bits for particular input so what is a goal of a good",
    "start": "1054350",
    "end": "1059360"
  },
  {
    "text": "explanation well one is for it to be human interpret also you might have a deep low on your network with many",
    "start": "1059360",
    "end": "1064549"
  },
  {
    "text": "millions of weights that's not going to be very interpretable word to understand you even have some have to have to have a high level of representation of what",
    "start": "1064549",
    "end": "1071659"
  },
  {
    "text": "is happening in your model but the same time you want it to be not to over simplified novice you can have a very high level explanation but if it's too",
    "start": "1071659",
    "end": "1078230"
  },
  {
    "text": "high level you're not getting a good idea what the actual model is doing this is going to be a trade off always with explanations between something that's",
    "start": "1078230",
    "end": "1084770"
  },
  {
    "text": "very intelligible and or something that's very close to what the model is actually doing so but that's just one of",
    "start": "1084770",
    "end": "1089960"
  },
  {
    "text": "the facts of life of explanations you need to take into account so how do you do it once the core thing we're going to",
    "start": "1089960",
    "end": "1096770"
  },
  {
    "text": "discuss there is black box model explanations so here you're treating the model purely as a black box that could be your new or net word it could be a",
    "start": "1096770",
    "end": "1102620"
  },
  {
    "text": "random forest it could be anything this input so here you've got a set of features and democratic features etc and",
    "start": "1102620",
    "end": "1108590"
  },
  {
    "text": "the model makes a prediction in this case to actually say denial own request with a high probability and you want to",
    "start": "1108590",
    "end": "1114080"
  },
  {
    "text": "understand why why did that white the model make that particular prediction says various ways to do it and the cool",
    "start": "1114080",
    "end": "1119929"
  },
  {
    "text": "way to explain is do it essentially is you've got your model that it takes inputs it gives a prediction the model takes the same input and it's going to",
    "start": "1119929",
    "end": "1126290"
  },
  {
    "text": "change those inputs slightly many times for each of the features and send it to the actual model get a response back and",
    "start": "1126290",
    "end": "1131990"
  },
  {
    "text": "see how the model is changing its prediction as the as the features that you're actually sending to the model are",
    "start": "1131990",
    "end": "1138650"
  },
  {
    "text": "change very slightly and that way you can try to judge how the model is coming to assert your opinion about it social",
    "start": "1138650",
    "end": "1145549"
  },
  {
    "text": "prediction and then you could turn all those particular predictions you may which could be tens tens of thousands to",
    "start": "1145549",
    "end": "1151190"
  },
  {
    "text": "then turn it into human understandable explanation so obviously as you'd see there it's going to be expensive so you",
    "start": "1151190",
    "end": "1156290"
  },
  {
    "text": "might have one prediction coming and it's going to be hundreds or tens of thousands of predictions you need to send it the model so we are certain have",
    "start": "1156290",
    "end": "1163610"
  },
  {
    "text": "a open source library alibi explain which you can see the URL there which is stated by state-of-the-art",
    "start": "1163610",
    "end": "1169059"
  },
  {
    "text": "implementations for different types of explanation algorithms such as anchored counterfactuals contrast of explanations",
    "start": "1169059",
    "end": "1175130"
  },
  {
    "text": "now the things connect to explanation such as trust scores so that gives you can you really put trust the predictions",
    "start": "1175130",
    "end": "1181130"
  },
  {
    "text": "come back from your model if they say how high probability that they actually this is true is it actually the case",
    "start": "1181130",
    "end": "1186710"
  },
  {
    "text": "that you can actually trust that prediction so in today we'll just concentrate on one particular type of",
    "start": "1186710",
    "end": "1191990"
  },
  {
    "text": "estimation called anchors so here you've got a representation of a image model it's an image net it's got a picture of",
    "start": "1191990",
    "end": "1198770"
  },
  {
    "text": "a cat goes to the moral and okay that's a Persian cat with 90% probability so four anchors it's going",
    "start": "1198770",
    "end": "1204660"
  },
  {
    "text": "to find the core features in this case pixels or super pixels part of the image that irrespective of what's around that",
    "start": "1204660",
    "end": "1210930"
  },
  {
    "text": "it's always going to give the same prediction so here you run it through the explainer it's obviously picked out the eyes and the whiskers of the cat and",
    "start": "1210930",
    "end": "1217380"
  },
  {
    "text": "irrespective of what happens in the image around there you know this model will always predict that this is a",
    "start": "1217380",
    "end": "1222480"
  },
  {
    "text": "Persian cat so this gives you a core idea of what is the model actually focusing on to actually make that",
    "start": "1222480",
    "end": "1227760"
  },
  {
    "text": "prediction and so you get some insight into how it's doing it so this is a version anchors has versions for images tabular",
    "start": "1227760",
    "end": "1234450"
  },
  {
    "text": "data and textual data and we'll take you through a tabular data example in a second so how do you add it to care",
    "start": "1234450",
    "end": "1240300"
  },
  {
    "text": "serving this is just a simple addition to issue of your yeah more that you do you either explain the section next to",
    "start": "1240300",
    "end": "1245340"
  },
  {
    "text": "your predictor you specify what type of algorithm you wants in this case anchor tabular as it's tabular data for this",
    "start": "1245340",
    "end": "1251880"
  },
  {
    "text": "income model I'm going to show you in a second and some explain this requires some analysis of the training data to",
    "start": "1251880",
    "end": "1257760"
  },
  {
    "text": "get some understanding of what is the likelihood of different features so there's some explainin need to be trained but it's not training on the",
    "start": "1257760",
    "end": "1263190"
  },
  {
    "text": "model that's just training on the training data to say okay this feature appears system out of x with these values so therefore you can give you a",
    "start": "1263190",
    "end": "1269370"
  },
  {
    "text": "response back this easy easy to understand so that's what that's an example on the left for Anka tabular one",
    "start": "1269370",
    "end": "1275700"
  },
  {
    "text": "for anchor text on the right so just simply just saying this is anchor text I won't explain it for that in this case for a movie sentiment model so it's very",
    "start": "1275700",
    "end": "1283440"
  },
  {
    "text": "easy to add I'll go for a quick demo there's two demos in the slides I probably just go through the income prediction demo so this is the structure",
    "start": "1283440",
    "end": "1290850"
  },
  {
    "text": "that we've added to now we've got the core model and we've added we're going to add it income explain that next to it using alibi explained or all deployed",
    "start": "1290850",
    "end": "1297420"
  },
  {
    "text": "using cash serving so we'll just go to the actual model",
    "start": "1297420",
    "end": "1303810"
  },
  {
    "text": "here so we'll have a look at the actual llaman for the actual model what we're",
    "start": "1303810",
    "end": "1310410"
  },
  {
    "text": "serving so we've got the actual model at the top there a sky kittler an income prediction model it's based on this sort",
    "start": "1310410",
    "end": "1316410"
  },
  {
    "text": "of census data from 1996 I've added a few things just to make it not use too much resources and we add in the anchor",
    "start": "1316410",
    "end": "1321660"
  },
  {
    "text": "tabular so apply that what I've done that already so I just go through some setup to actually get the IP address of",
    "start": "1321660",
    "end": "1330500"
  },
  {
    "text": "the service and the service names if you setup here set up just some of the data so that we",
    "start": "1330500",
    "end": "1335509"
  },
  {
    "text": "can send it to the model and get a explanation just wait a second for that",
    "start": "1335509",
    "end": "1342830"
  },
  {
    "text": "sir that's your run could be the internet it's gonna fail us here at the last",
    "start": "1342830",
    "end": "1349850"
  },
  {
    "text": "second for she'd ever know because I've got or where do you run okay this is going a bit slowly so obviously we can",
    "start": "1349850",
    "end": "1355820"
  },
  {
    "text": "send two examples to the model so here we've got some examples this is the demographic features say age marital",
    "start": "1355820",
    "end": "1361820"
  },
  {
    "text": "status except your occupation over here it's predicting the person will have lower than 50k salary this is from 1996",
    "start": "1361820",
    "end": "1368359"
  },
  {
    "text": "now here we've got a different set of features whereas printing hire hire more than 50k salary for those features and",
    "start": "1368359",
    "end": "1374779"
  },
  {
    "text": "we can ask okay what is an explanation for why this first example the one where you had low income predictions we send",
    "start": "1374779",
    "end": "1382159"
  },
  {
    "text": "that to the explainer that's gonna have called the model thousands of times and it's going to say okay so hits what it's",
    "start": "1382159",
    "end": "1387440"
  },
  {
    "text": "saying is that of those demographic features here we had at the top irrespective of what you check for the other features if it's person's mental",
    "start": "1387440",
    "end": "1393440"
  },
  {
    "text": "status is never married and they're working less than forty hours per week you'll always get lower low income prediction for that so",
    "start": "1393440",
    "end": "1400489"
  },
  {
    "text": "therefore you can look in that give me a lot of insight maybe you want to look into that is your model biased in some way is it taking marital status which it",
    "start": "1400489",
    "end": "1406549"
  },
  {
    "text": "shouldn't be doing maybe you want to take that out of your dataset so you can all these things which are obviously very important now to understand how",
    "start": "1406549",
    "end": "1412159"
  },
  {
    "text": "your model is behaving you could then get some understanding of your confidence in that actual prediction so",
    "start": "1412159",
    "end": "1418190"
  },
  {
    "text": "you can ask it for the actual precision in this case it's 96 percent position",
    "start": "1418190",
    "end": "1423739"
  },
  {
    "text": "that means that 96 percent of the time if those features just appear the amount",
    "start": "1423739",
    "end": "1428779"
  },
  {
    "text": "of status and hours per week you'll although your model will always give the same prediction of low salary and then",
    "start": "1428779",
    "end": "1435220"
  },
  {
    "text": "coverage says okay how much of the data set does this actually cover those features so it's actually reasonable",
    "start": "1435220",
    "end": "1441230"
  },
  {
    "text": "size the data at 25 percent or the dataset and you can dive into how that goes down feature by feature so if we",
    "start": "1441230",
    "end": "1447470"
  },
  {
    "text": "had just them the amount of status that will cover 32 percent and then you add Mausers an hours per week that goes down",
    "start": "1447470",
    "end": "1453950"
  },
  {
    "text": "so you give a idea of how much of a dataset is covered by just these type of features in for what you wanted to do",
    "start": "1453950",
    "end": "1458960"
  },
  {
    "text": "you can do a similar thing for high high income example I'll skip over that now just in terms of time",
    "start": "1458960",
    "end": "1465250"
  },
  {
    "text": "so it's very easy to set up and that's how you add explanations we're using alibi explained office or slab it is",
    "start": "1465250",
    "end": "1472600"
  },
  {
    "text": "also a great light before IBM called I explained ability 360 which you can also be used that would be integrated into cash diving in the near future",
    "start": "1472600",
    "end": "1479679"
  },
  {
    "text": "so if we just go back to the presentation so next thing because we've",
    "start": "1479679",
    "end": "1487030"
  },
  {
    "text": "got explanations for our models next thing is payload logging so why would you want payload logging well for obvious reasons you wanted it you want",
    "start": "1487030",
    "end": "1492280"
  },
  {
    "text": "to capture what's going for your model for future retraining of your model and you want to perform near timer analysis all of that model one that will request",
    "start": "1492280",
    "end": "1498700"
  },
  {
    "text": "some responses we've added in 2k of serving an alpha feature which allows you to add to any part of the care of",
    "start": "1498700",
    "end": "1504400"
  },
  {
    "text": "serving description to the predictor explain all the transformer to actually log request going through the model at",
    "start": "1504400",
    "end": "1509860"
  },
  {
    "text": "that point you can choose to choose to log both the request all the responses or both and all you all you need to do",
    "start": "1509860",
    "end": "1515620"
  },
  {
    "text": "is specify a URL of where to send those payloads so that means it's really easy to connect to anything you want to do to",
    "start": "1515620",
    "end": "1520720"
  },
  {
    "text": "connect it into systems and we using cloud events for this so cloud of instances standard for wrapping your of",
    "start": "1520720",
    "end": "1527070"
  },
  {
    "text": "event type data subsystems which one to work together event Inc and work",
    "start": "1527070",
    "end": "1532929"
  },
  {
    "text": "together seamlessly but with a standard set of headers in this case it's a very simple addition to the actual payload",
    "start": "1532929",
    "end": "1538330"
  },
  {
    "text": "which is going to go through and be sent to allow the various systems to work together how do you add it to care",
    "start": "1538330",
    "end": "1544539"
  },
  {
    "text": "serving you just add a logger section off on the part you want to add logging to give it a ul this case I'm using the K native standard service I'm going to",
    "start": "1544539",
    "end": "1551620"
  },
  {
    "text": "set up message dumper just to print out but you could set up to push telegram so",
    "start": "1551620",
    "end": "1556960"
  },
  {
    "text": "alerting system or whatever you want to assume it's going to save the data and then you specify a mode do you want a",
    "start": "1556960",
    "end": "1562630"
  },
  {
    "text": "low of the requests or the responses or both this case all so again because it",
    "start": "1562630",
    "end": "1567789"
  },
  {
    "text": "all you need is a URL you can tie it into anything you could tie it into K native broker and tie that into further systems like our Liza texture and then",
    "start": "1567789",
    "end": "1573940"
  },
  {
    "text": "push it on to it alerting or you could tell you into sake Africa have a Kafka bridge and push it onto a caracal cluster so it's up to you and it's very",
    "start": "1573940",
    "end": "1580179"
  },
  {
    "text": "flexible on how you actually use it so now we've got the payload logging which is easy to add in then we can do some",
    "start": "1580179",
    "end": "1585520"
  },
  {
    "text": "interesting things and really understand its model working correctly in production so three things have been a",
    "start": "1585520",
    "end": "1591490"
  },
  {
    "text": "chat about is out lies detection at the server detection in consecutive nobody come under the concept of you",
    "start": "1591490",
    "end": "1597059"
  },
  {
    "text": "product shouldn't trust predictions on instances which are outside your training data so as your model may be",
    "start": "1597059",
    "end": "1603029"
  },
  {
    "text": "trained a month ago on data and the data coming into it may be completely different and you shouldn't trust those predictions and these type of techniques",
    "start": "1603029",
    "end": "1609179"
  },
  {
    "text": "are trying to detect when those things are happening so outlier detection is",
    "start": "1609179",
    "end": "1614340"
  },
  {
    "text": "obviously when you the data it's not as expected for the actual model there's many different types of that of a lie",
    "start": "1614340",
    "end": "1620370"
  },
  {
    "text": "detectors available we won't go into all the details but there's ones that are trained online as the data is coming",
    "start": "1620370",
    "end": "1626159"
  },
  {
    "text": "through the system ones that are trained on your training data and we'll see different types of a lie detection for the types of data of your model and",
    "start": "1626159",
    "end": "1632039"
  },
  {
    "text": "different types of outliers it could be a global outlier say something if you've got a model that predicts temperature",
    "start": "1632039",
    "end": "1637110"
  },
  {
    "text": "and say it's predicting 1,000 degrees obviousiy that's a global outlier but it's predicting say 35 degrees in London",
    "start": "1637110",
    "end": "1644100"
  },
  {
    "text": "in say December that's probably a contextual out local it's really unlikely at that time and then you could",
    "start": "1644100",
    "end": "1649409"
  },
  {
    "text": "have collective outliers which apply to things like time and time series where whole series of events are together",
    "start": "1649409",
    "end": "1654539"
  },
  {
    "text": "taken as an outlier so another thing you want to do is maybe adversarial",
    "start": "1654539",
    "end": "1659580"
  },
  {
    "text": "detection this is the case where people could be attacking your model with very similar instances which are producing",
    "start": "1659580",
    "end": "1664799"
  },
  {
    "text": "radically different outputs from your model so here's some examples on the left you've got films some a chemist handwritten digits but even though for",
    "start": "1664799",
    "end": "1671039"
  },
  {
    "text": "us it looks like that's 0 & 6 the model is going to predict something completely different in this case 2 to the 0 and 0",
    "start": "1671039",
    "end": "1677460"
  },
  {
    "text": "for the 6 just by changing a few pixels and similar funny example here where you're an image classify of a pig adds a",
    "start": "1677460",
    "end": "1684389"
  },
  {
    "text": "certain amount of very specified noise to that still looks like at the same image to us but your models predicting",
    "start": "1684389",
    "end": "1689730"
  },
  {
    "text": "something completely different and you may want to be told if this is happening to your model okay so I'm just",
    "start": "1689730",
    "end": "1694799"
  },
  {
    "text": "going to go very quickly through a couple of things that concept drift or says is more similar to outlier detection but it's over a whole series",
    "start": "1694799",
    "end": "1700230"
  },
  {
    "text": "is my training data changing over time and then we've got a new library called",
    "start": "1700230",
    "end": "1705659"
  },
  {
    "text": "alibi detect which you just come out this week which has state of their implementations of outlines detection adverse elder detection and concept",
    "start": "1705659",
    "end": "1712740"
  },
  {
    "text": "drift and you can find that online and be great to get your feedback on that so I'll just go for a very quick example on",
    "start": "1712740",
    "end": "1720110"
  },
  {
    "text": "the ciphered cyber 10 model for out lie detection so this is a simple image",
    "start": "1720110",
    "end": "1725519"
  },
  {
    "text": "classification model just the 10 different types of images basically we've got it running here",
    "start": "1725519",
    "end": "1730830"
  },
  {
    "text": "so as we got really close to time I'll just go down to the actual example so you've got a prediction here of a truck",
    "start": "1730830",
    "end": "1737520"
  },
  {
    "text": "when the model predicts correctly it's a truck and then you can basically as we",
    "start": "1737520",
    "end": "1743490"
  },
  {
    "text": "got the outlier detection tied in that will then go through and check what this is outlier it's quickly saying that's a",
    "start": "1743490",
    "end": "1748650"
  },
  {
    "text": "not it's gonna just come out to the message dumper saying no this is not an outlier we can then create a outlet",
    "start": "1748650",
    "end": "1754020"
  },
  {
    "text": "prediction by adding some noise to the model in this case a little bit of noise to the left hand corner the actual model",
    "start": "1754020",
    "end": "1760140"
  },
  {
    "text": "still says it's a truck you want to know this is not actually part of the training data and should be highlighted again with the outlines detection",
    "start": "1760140",
    "end": "1766440"
  },
  {
    "text": "they're running you'll go through a few seconds later and we'll get a prediction this case the second example here in the",
    "start": "1766440",
    "end": "1772800"
  },
  {
    "text": "logs this is an outlier so you can quickly find out this is happening this could then go on to the alerting systems so what we call close to time yeah okay",
    "start": "1772800",
    "end": "1780180"
  },
  {
    "text": "so I think I'll just go back to the presentation and we'll finish off so",
    "start": "1780180",
    "end": "1786390"
  },
  {
    "text": "yeah so we shown you how to you can add these things easily there's also once for adversary tection in the demo and",
    "start": "1786390",
    "end": "1791580"
  },
  {
    "text": "and slides you can go to those demos there for traffic signs and digits and it's a great lobby from IBM or now just",
    "start": "1791580",
    "end": "1797760"
  },
  {
    "text": "over Buster's 360 we're showing you how what you need to do to add certain things to your model not just the model",
    "start": "1797760",
    "end": "1803820"
  },
  {
    "text": "but things you want to surround it with to really get a production grade system adding these things together there's a",
    "start": "1803820",
    "end": "1809700"
  },
  {
    "text": "list of the libraries that were all open source that you can use to achieve this there's various other talks including",
    "start": "1809700",
    "end": "1815640"
  },
  {
    "text": "the one yesterday on care server you want to catch that up online giving an introduction and there's a group in LFA",
    "start": "1815640",
    "end": "1821610"
  },
  {
    "text": "I called trusted AR community which is working on this how do you get this trusted and our models in production and be great to get your cooperation in that",
    "start": "1821610",
    "end": "1829530"
  },
  {
    "text": "thanks clay if you guys have five minutes I'll try and see you know if if the HDMI detection works and then just",
    "start": "1829530",
    "end": "1836220"
  },
  {
    "text": "do a very quick demo if it does work or I can probably you know it still nor",
    "start": "1836220",
    "end": "1842640"
  },
  {
    "text": "detection it was just working 20 minutes ago can we just use certain links from your slack",
    "start": "1842640",
    "end": "1847710"
  },
  {
    "text": "maybe just show very quickly the pipeline's part yeah it's lagged you some links let's just launch it here",
    "start": "1847710",
    "end": "1856130"
  },
  {
    "text": "while we are doing this you know if you have certain questions you can bring it up right we we can certainly tackle it",
    "start": "1856130",
    "end": "1864050"
  },
  {
    "text": "just your private children okay so just in terms of the scenario right",
    "start": "1864050",
    "end": "1871010"
  },
  {
    "text": "what we are trying to do here as affiliate loads so this is you know",
    "start": "1871010",
    "end": "1877310"
  },
  {
    "text": "typical model is the psychic learn model in this case you know this model essentially what it does is you know it",
    "start": "1877310",
    "end": "1882650"
  },
  {
    "text": "simulates shipping industry shipping industry carries your food items right as these food items are being carried",
    "start": "1882650",
    "end": "1888380"
  },
  {
    "text": "right through air place through sea right they might get spoiled right so because you know they have different temperatures carbon dioxide nitrogen co2",
    "start": "1888380",
    "end": "1895910"
  },
  {
    "text": "level right so this model actually you know is a psychic learn model which is trained on that particular data set these shipping containers which are",
    "start": "1895910",
    "end": "1902510"
  },
  {
    "text": "coming in and then there is you know pipeline right which is in this case is",
    "start": "1902510",
    "end": "1907580"
  },
  {
    "text": "a cue flow pipeline which essentially will train that model right deploy that",
    "start": "1907580",
    "end": "1912650"
  },
  {
    "text": "model using K of serving launch or default and a Canada version of it and then you know I'll probably you know",
    "start": "1912650",
    "end": "1919760"
  },
  {
    "text": "send some load from this side and then be able to show you you know a few things so for example you know this is a",
    "start": "1919760",
    "end": "1925880"
  },
  {
    "text": "typical cue flow pipeline right which you can launch in this case you know oh",
    "start": "1925880",
    "end": "1931960"
  },
  {
    "text": "I do need to ping one more thing let me just get this because I need to",
    "start": "1931960",
    "end": "1940120"
  },
  {
    "text": "okay so to launch this pipeline I need you know my model training code which I",
    "start": "1954250",
    "end": "1961970"
  },
  {
    "text": "just showed right and this is essentially in this github repository so let me just",
    "start": "1961970",
    "end": "1969070"
  },
  {
    "text": "is it come on we control we have not used Windows in a while can a paste it",
    "start": "1980870",
    "end": "1990700"
  },
  {
    "text": "and copy paste here this one",
    "start": "1992620",
    "end": "1997029"
  },
  {
    "text": "you know I think that's probably what we will do I'm trying to do a copy-paste but it seems it's paced okay",
    "start": "2012960",
    "end": "2022450"
  },
  {
    "text": "Network right so I will just launch it now",
    "start": "2022450",
    "end": "2027210"
  },
  {
    "text": "so it just launches this pipeline it will appear and what you will see is you know this one single pipeline you know",
    "start": "2033730",
    "end": "2039200"
  },
  {
    "text": "it will essentially go through all the different phases which we were talking about into the chip flow ecosystem as it",
    "start": "2039200",
    "end": "2045620"
  },
  {
    "text": "comes up right so essentially you know training the model or deploying the model and then launching you know",
    "start": "2045620",
    "end": "2051730"
  },
  {
    "text": "default in a Canada version of that model right so it's taking a bit of life",
    "start": "2051730",
    "end": "2057200"
  },
  {
    "text": "here yeah we'll probably it's I'm sorry",
    "start": "2057200",
    "end": "2071210"
  },
  {
    "text": "about it right it's certain links are",
    "start": "2071210",
    "end": "2077000"
  },
  {
    "text": "not working on this laptop yeah but we record it and and definitely you know share it your way and this actually will",
    "start": "2077000",
    "end": "2083870"
  },
  {
    "text": "show you know this end-to-end working in a pipeline context without any questions anybody has",
    "start": "2083870",
    "end": "2089638"
  },
  {
    "text": "[Applause]",
    "start": "2089639",
    "end": "2098018"
  }
]