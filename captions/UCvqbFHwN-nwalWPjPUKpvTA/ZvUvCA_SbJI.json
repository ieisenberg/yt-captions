[
  {
    "start": "0",
    "end": "52000"
  },
  {
    "text": "hello all right hi everyone my name is David and this is Yann we're super",
    "start": "30",
    "end": "7290"
  },
  {
    "text": "excited today to share with you our back to basics hands-on deployment of stateful workloads on kubernetes",
    "start": "7290",
    "end": "14490"
  },
  {
    "text": "workshop so what we're going to be doing today is first for about the first 20",
    "start": "14490",
    "end": "21689"
  },
  {
    "text": "minutes we're going to be covering some basic kubernetes storage concepts so",
    "start": "21689",
    "end": "28109"
  },
  {
    "text": "those include basic workload concepts some dynamic provisioning debugging techniques and how to use cube cuddle",
    "start": "28109",
    "end": "34880"
  },
  {
    "text": "and then after that we'll have a hands-on portion so that's about 20 minutes in where you can login with the",
    "start": "34880",
    "end": "42420"
  },
  {
    "text": "accounts given and work with yon to actually bring up a cluster and bring up",
    "start": "42420",
    "end": "47610"
  },
  {
    "text": "some stateful workloads on your own all right so before we get started some",
    "start": "47610",
    "end": "54750"
  },
  {
    "start": "52000",
    "end": "136000"
  },
  {
    "text": "general housekeeping we have some TAS that have positioned themselves around the room who have graciously volunteered",
    "start": "54750",
    "end": "61379"
  },
  {
    "text": "their time to help you so if you have any issues at all say your Internet's",
    "start": "61379",
    "end": "67590"
  },
  {
    "text": "not connecting or your clusters not coming up you can't find a certain button whatever problem you have please",
    "start": "67590",
    "end": "74340"
  },
  {
    "text": "raise your hand and look for one of these TAS to help you ok so they're here for your help feel free to utilize them",
    "start": "74340",
    "end": "81210"
  },
  {
    "text": "however necessary I'm gonna please ask for you to save your more general",
    "start": "81210",
    "end": "87080"
  },
  {
    "text": "questions for the end of the talk so we'll leave time for questions at the end this is just because you know we",
    "start": "87080",
    "end": "94500"
  },
  {
    "text": "don't want to slow down we don't want to bog down the hands-on portion or the concepts for for questions in the middle",
    "start": "94500",
    "end": "101600"
  },
  {
    "text": "and one loss one final thing in the hands-on portion of this tutorial we",
    "start": "101600",
    "end": "107310"
  },
  {
    "text": "just asked for you to not work ahead we'll have adequate time to cover all of",
    "start": "107310",
    "end": "112380"
  },
  {
    "text": "the material yawn will be going at a fairly comfortable pace but not super",
    "start": "112380",
    "end": "117899"
  },
  {
    "text": "fast and so yawn we'll also be covering more information as we go on so if you",
    "start": "117899",
    "end": "125100"
  },
  {
    "text": "work too far ahead you might not catch like the conceptual information that happens as we're going through the tour",
    "start": "125100",
    "end": "132410"
  },
  {
    "text": "all right so without further ado some conceptual information so I want to",
    "start": "132410",
    "end": "139520"
  },
  {
    "start": "136000",
    "end": "422000"
  },
  {
    "text": "start by showing you a basic staple workload so many of you may already be",
    "start": "139520",
    "end": "144530"
  },
  {
    "text": "familiar with kind of the very basic of kubernetes which is that it's a",
    "start": "144530",
    "end": "150080"
  },
  {
    "text": "container Orchestrator but it's sort of actually a pod Orchestrator where a pod",
    "start": "150080",
    "end": "155780"
  },
  {
    "text": "is a collection of containers as long as well as its configuration information",
    "start": "155780",
    "end": "163150"
  },
  {
    "text": "environment variables images some commands and in our case actually it",
    "start": "163150",
    "end": "169790"
  },
  {
    "text": "will also contain some storage information or persistent storage",
    "start": "169790",
    "end": "174830"
  },
  {
    "text": "information so in this example here our sleepy container has a volume mount and",
    "start": "174830",
    "end": "182120"
  },
  {
    "text": "this is called data and that's like that's the volume that we're mounting into our containers and into our",
    "start": "182120",
    "end": "188810"
  },
  {
    "text": "container so here's the pot that we're giving it and you can see that we've actually defined what data is in this",
    "start": "188810",
    "end": "195740"
  },
  {
    "text": "pod specification as well and we've defined it to be a reference to this",
    "start": "195740",
    "end": "201410"
  },
  {
    "text": "thing called a persistent volume clamp so you might be wondering what is that",
    "start": "201410",
    "end": "206540"
  },
  {
    "text": "persistent volume claim well we have two main storage objects in kubernetes one",
    "start": "206540",
    "end": "213830"
  },
  {
    "text": "of which is a persistent volume claim and one of which is a persistent volume so first let me go over the persistent",
    "start": "213830",
    "end": "220730"
  },
  {
    "text": "volume claim so this is a user object so this is generally created by a user",
    "start": "220730",
    "end": "226880"
  },
  {
    "text": "along with their workloads and it basically represents all of the",
    "start": "226880",
    "end": "232190"
  },
  {
    "text": "information that you might need from from the workload like the request for",
    "start": "232190",
    "end": "238970"
  },
  {
    "text": "storage that a user will give so in our example we have as a user I just want a",
    "start": "238970",
    "end": "245030"
  },
  {
    "text": "disk any disk with five gigabytes file system type and read/write ones right",
    "start": "245030",
    "end": "251510"
  },
  {
    "text": "and the persistent volume which is kind of on the other side of the equation is",
    "start": "251510",
    "end": "257560"
  },
  {
    "text": "the storage administrator side of the equation and so this is actually a",
    "start": "257560",
    "end": "263810"
  },
  {
    "text": "reference or to underlying storage so this generally has some more information",
    "start": "263810",
    "end": "269880"
  },
  {
    "text": "associated with it in our example here we have an NFS volume and so we actually",
    "start": "269880",
    "end": "275490"
  },
  {
    "text": "have the IP address of this and if the underlying disk right and so we can find the volume and what happens actually in",
    "start": "275490",
    "end": "283410"
  },
  {
    "text": "kubernetes is that these objects bind together so we have the user side the one that's created by the user",
    "start": "283410",
    "end": "289440"
  },
  {
    "text": "the precision volume claim and the persistent volume and they bind so here's a quick diagram of kind of how it",
    "start": "289440",
    "end": "296490"
  },
  {
    "text": "works in the system we have an admin pre provisioning persistent volumes which",
    "start": "296490",
    "end": "303930"
  },
  {
    "text": "reference persistent disks okay and when a user comes in and runs wants to run",
    "start": "303930",
    "end": "310500"
  },
  {
    "text": "their workload they're going to apply their workload gamal to the cluster so",
    "start": "310500",
    "end": "316770"
  },
  {
    "text": "that's their set of containers which is called a pod and that references of PVC",
    "start": "316770",
    "end": "323880"
  },
  {
    "text": "a PVC which can then bind to the most appropriate persistent volume so in this",
    "start": "323880",
    "end": "330930"
  },
  {
    "text": "case the persistent volume claim kind of finds the most appropriate persistent volume in the system and binds to it",
    "start": "330930",
    "end": "336590"
  },
  {
    "text": "alright so that's the basic that's the basics and then we have dynamic",
    "start": "336590",
    "end": "341760"
  },
  {
    "text": "provisioning ok so to explain dynamic provisioning we have to explain one more",
    "start": "341760",
    "end": "348750"
  },
  {
    "text": "API object and this is the storage class and what this is is a representation of",
    "start": "348750",
    "end": "356850"
  },
  {
    "text": "an entire class of storage so in our example we have two example storage",
    "start": "356850",
    "end": "363930"
  },
  {
    "text": "classes one is slow and one is fast and we can see that they contain some",
    "start": "363930",
    "end": "368970"
  },
  {
    "text": "parameters for each saying whether we're using like standard hard disk drives or",
    "start": "368970",
    "end": "374600"
  },
  {
    "text": "solid-state drives right so we're we're kind of grouping persistent volumes and",
    "start": "374600",
    "end": "379730"
  },
  {
    "text": "what we can use this storage clause to do actually is here here's the image",
    "start": "379730",
    "end": "385710"
  },
  {
    "text": "from before and what we can use the storage cost to do here is kind of like the admin creates the storage class but",
    "start": "385710",
    "end": "392940"
  },
  {
    "text": "now no longer has to pre provision all of their persistent volumes so what",
    "start": "392940",
    "end": "398070"
  },
  {
    "text": "happens is when you create your workload and your Christmas and volume claim it's going to",
    "start": "398070",
    "end": "403130"
  },
  {
    "text": "reference the storage class which then kubernetes knows how to use the storage",
    "start": "403130",
    "end": "408139"
  },
  {
    "text": "class and its parameters to actually provision the underlying disk if you're",
    "start": "408139",
    "end": "414169"
  },
  {
    "text": "on a cloud and generate that persistent volume object that references that disk",
    "start": "414169",
    "end": "419750"
  },
  {
    "text": "that we can then bind with okay so what are some of the benefits of dynamic",
    "start": "419750",
    "end": "425240"
  },
  {
    "start": "422000",
    "end": "746000"
  },
  {
    "text": "provisioning well we can one decrease overhead by only creating the disks that",
    "start": "425240",
    "end": "431210"
  },
  {
    "text": "we need when we need them right so that's the dynamic part we group volumes",
    "start": "431210",
    "end": "436850"
  },
  {
    "text": "by storage characteristics so they kind of have a natural grouping there and we",
    "start": "436850",
    "end": "442160"
  },
  {
    "text": "also decrease the burden on the cluster admin so that they're freed up to do other important things all right so",
    "start": "442160",
    "end": "450139"
  },
  {
    "text": "that's kind of the basics of storage now we'll talk a little bit more about higher-level workloads so we actually",
    "start": "450139",
    "end": "458150"
  },
  {
    "text": "don't recommend that you use pods by themselves in production workloads and",
    "start": "458150",
    "end": "463340"
  },
  {
    "text": "that's because pods are kind of ephemeral and what that means is when you schedule it to a node to a VM if",
    "start": "463340",
    "end": "471620"
  },
  {
    "text": "your node dies for any reason your pod gets kicked off right or if your pod",
    "start": "471620",
    "end": "478010"
  },
  {
    "text": "gets preempted say another pod with higher priority comes in your pod gets kicked off right and so you kind of lose",
    "start": "478010",
    "end": "485210"
  },
  {
    "text": "you don't have that kind of resilience that you want from a production system so we have some higher-level workload",
    "start": "485210",
    "end": "492289"
  },
  {
    "text": "concepts in kubernetes that can help you give you some higher-level building blocks to build your resilient systems",
    "start": "492289",
    "end": "499900"
  },
  {
    "text": "so one of these is a deployment and what this does is it just runs many replicas",
    "start": "499900",
    "end": "506270"
  },
  {
    "text": "of a single pod template so the pod that we have before it just runs like X many",
    "start": "506270",
    "end": "511310"
  },
  {
    "text": "of them right when a plot is deleted it replaces it you can scale these things",
    "start": "511310",
    "end": "517190"
  },
  {
    "text": "up and down right you don't have to have three replicas you can have five you can have eight but the caveat here is that",
    "start": "517190",
    "end": "524300"
  },
  {
    "text": "all pods share the same PVC okay so if we look at this image here it kind of",
    "start": "524300",
    "end": "530600"
  },
  {
    "text": "shows how this is working so all the pots kind of reference one PVC which means they end up referencing one",
    "start": "530600",
    "end": "537170"
  },
  {
    "text": "piece of storage and this is a pretty big caveat this means that all of your",
    "start": "537170",
    "end": "542360"
  },
  {
    "text": "replicas of your workload are actually writing and reading from the same disk",
    "start": "542360",
    "end": "547700"
  },
  {
    "text": "and some of you may be thinking like that's probably not what I want right so",
    "start": "547700",
    "end": "553730"
  },
  {
    "text": "a lot of times what happens is that these workloads will kind of stomp on each other some of them will refuse to start some",
    "start": "553730",
    "end": "560870"
  },
  {
    "text": "of them will crash unless you've built your workload specifically to all read and write from",
    "start": "560870",
    "end": "567200"
  },
  {
    "text": "the same data volume this is probably not what you want so for a little bit",
    "start": "567200",
    "end": "572330"
  },
  {
    "text": "more storage focused higher level workload we have what's called a stateful set and that's actually very",
    "start": "572330",
    "end": "578270"
  },
  {
    "text": "similar to a deployment and that it runs many replicas of your pod but the",
    "start": "578270",
    "end": "584840"
  },
  {
    "text": "difference here is that each pod gets its own persistent volume claim and this",
    "start": "584840",
    "end": "590810"
  },
  {
    "text": "is generated through a persistent volume claim template okay and Yan will actually talk about this a little bit",
    "start": "590810",
    "end": "596480"
  },
  {
    "text": "more later so we'll see an example of that but basically what that gets you is",
    "start": "596480",
    "end": "602120"
  },
  {
    "text": "that each pod now has its own storage back-end right it has its own volume and",
    "start": "602120",
    "end": "608090"
  },
  {
    "text": "this may look like it solves all your problems but actually this has its own caveats too in that the pods actually",
    "start": "608090",
    "end": "615500"
  },
  {
    "text": "generally in your workload have to be aware of each other they have to all be like members and do some leader election",
    "start": "615500",
    "end": "623170"
  },
  {
    "text": "so without that you know you kind of your replicas don't really know how to",
    "start": "623170",
    "end": "629900"
  },
  {
    "text": "interact with each other basically they just all your pods need to have some method of being aware of each other",
    "start": "629900",
    "end": "637270"
  },
  {
    "text": "doing some active active or active passive membership okay one other",
    "start": "637270",
    "end": "645680"
  },
  {
    "text": "concept that we need to cover is services okay and so what this is is",
    "start": "645680",
    "end": "651260"
  },
  {
    "text": "basically defining a logical set of pots so generally a service is related to our",
    "start": "651260",
    "end": "658340"
  },
  {
    "text": "higher-level workloads in that it kind of sits in front of it and what it does is defines these as one logical set",
    "start": "658340",
    "end": "665930"
  },
  {
    "text": "and gives you access to them so we have a couple different types of services the",
    "start": "665930",
    "end": "671540"
  },
  {
    "text": "standard one kind of being a DNS name for your grouping of pods as well as an",
    "start": "671540",
    "end": "678730"
  },
  {
    "text": "an IP address for your for your grouping of pots and at load balances between",
    "start": "678730",
    "end": "685040"
  },
  {
    "text": "them so if you hit this IP address it will it will load balance between your",
    "start": "685040",
    "end": "690380"
  },
  {
    "text": "pots right and what we have kind of a weird name but the load balancer service",
    "start": "690380",
    "end": "696730"
  },
  {
    "text": "it does all the things that a standard service does but it also provides you with an external IP so this allows you",
    "start": "696730",
    "end": "705290"
  },
  {
    "text": "to hit that kind of service from outside of your cluster as well so the standard",
    "start": "705290",
    "end": "711380"
  },
  {
    "text": "service is for inside load balancers for outside and we have a third concept called a headless service which is very",
    "start": "711380",
    "end": "719420"
  },
  {
    "text": "similar to a standard service but it doesn't have that IP and DNS name at the",
    "start": "719420",
    "end": "725660"
  },
  {
    "text": "top level so actually we're just gonna have an IP and DNS for each pod so we're",
    "start": "725660",
    "end": "733279"
  },
  {
    "text": "not kind of aggregating the pods anymore we're just giving we're assigning a DNS",
    "start": "733279",
    "end": "738320"
  },
  {
    "text": "name to each pod and that's if you want to kind of direct traffic to pods separately or individually okay so I've",
    "start": "738320",
    "end": "747860"
  },
  {
    "start": "746000",
    "end": "818000"
  },
  {
    "text": "explained like a ton of objects maybe like six or seven what do you do with",
    "start": "747860",
    "end": "753350"
  },
  {
    "text": "them well we have this command called cube cuddle or cube control I've heard",
    "start": "753350",
    "end": "760550"
  },
  {
    "text": "someone called it Cube ekdal or Quebec doll before I don't think that's right so don't call it that but so here are",
    "start": "760550",
    "end": "768709"
  },
  {
    "text": "some useful cube control commands we have and these are the ones that were going to be using later as well we have",
    "start": "768709",
    "end": "774980"
  },
  {
    "text": "apply all right this just kind of takes your yeah mol definition and puts it on to your cluster we have delete that does",
    "start": "774980",
    "end": "782870"
  },
  {
    "text": "the opposite right it removes your object turn your cluster you can get a",
    "start": "782870",
    "end": "788140"
  },
  {
    "text": "basic list of API objects you can describe the objects this one is",
    "start": "788140",
    "end": "794089"
  },
  {
    "text": "important remember describe I think a lot of people don't know about it and it gives you much",
    "start": "794089",
    "end": "799850"
  },
  {
    "text": "more information that helps you debug your cluster you can get logs you can",
    "start": "799850",
    "end": "804889"
  },
  {
    "text": "execute commands directly on your containers and of course this is not exhaustive we have a link here slides",
    "start": "804889",
    "end": "812389"
  },
  {
    "text": "are online you can go and find all the rest of the commands as well in your own time okay so before we move on to the",
    "start": "812389",
    "end": "820610"
  },
  {
    "start": "818000",
    "end": "946000"
  },
  {
    "text": "hands-on portion I want to cover a quick debugging technique that can be very helpful in debugging your hands on",
    "start": "820610",
    "end": "826699"
  },
  {
    "text": "cluster as well as your future kubernetes stateful workloads so here in",
    "start": "826699",
    "end": "833149"
  },
  {
    "text": "my example when I do a cube control and get pods I noticed that my webserver is",
    "start": "833149",
    "end": "838190"
  },
  {
    "text": "pending and it notice that it's pending for a long time now this is something that we don't want so what I do is I use",
    "start": "838190",
    "end": "845569"
  },
  {
    "text": "describe and I describe the pots and what this actually gives me is a list of",
    "start": "845569",
    "end": "850790"
  },
  {
    "text": "events so I can see that the pod has failed to schedule because it has an",
    "start": "850790",
    "end": "857360"
  },
  {
    "text": "unbound PVC this is a storage object as we talked about earlier right so now we",
    "start": "857360",
    "end": "863240"
  },
  {
    "text": "know kind of where the culprit is so we go in and we describe our PVCs on the",
    "start": "863240",
    "end": "868790"
  },
  {
    "text": "cluster to find out more all right and when we do that what we notice is that provisioning failed because the storage",
    "start": "868790",
    "end": "876290"
  },
  {
    "text": "class which we also talked about is not found right so this kind of gets to the root of the issue is kind of a contrived",
    "start": "876290",
    "end": "883610"
  },
  {
    "text": "example but you know we can't find the storage class so what do we do we check the storage clauses on the cluster we",
    "start": "883610",
    "end": "890600"
  },
  {
    "text": "notice that there is none and so we just create this storage class we describe it",
    "start": "890600",
    "end": "896949"
  },
  {
    "text": "we can see it okay we've put the we've put this storage class on the cluster now and now when we actually describe we",
    "start": "896949",
    "end": "905360"
  },
  {
    "text": "wait like a couple seconds and then we describe our webserver and we note that you know it's gotten scheduled and it's",
    "start": "905360",
    "end": "912259"
  },
  {
    "text": "you know pulling the images it's creating things and it's running now so",
    "start": "912259",
    "end": "919189"
  },
  {
    "text": "what actually happened is we don't need to do anything else besides creating that storage class because kubernetes is",
    "start": "919189",
    "end": "925370"
  },
  {
    "text": "self-healing right once that storage class exists the PVC and its loop will",
    "start": "925370",
    "end": "931069"
  },
  {
    "text": "eventually find that storage class and heal it self and make everything work and like",
    "start": "931069",
    "end": "936170"
  },
  {
    "text": "connect to a PV like it's supposed to and then the pod will then find that PVC and find the underlying storage we",
    "start": "936170",
    "end": "942320"
  },
  {
    "text": "didn't have to touch anything besides resolving the underlying error okay so",
    "start": "942320",
    "end": "947390"
  },
  {
    "start": "946000",
    "end": "1245000"
  },
  {
    "text": "that's the conceptual part done and now yon is going to walk you through the hands-on demo thank you well first we",
    "start": "947390",
    "end": "957260"
  },
  {
    "text": "need an opening this cluster everybody of you got tiny piece of paper with looking details so go to this webpage",
    "start": "957260",
    "end": "965000"
  },
  {
    "text": "console dot Cloud Google Chrome but please use anonymous windows otherwise",
    "start": "965000",
    "end": "970670"
  },
  {
    "text": "it will conflict with your use your own Google account if so anonymous in window",
    "start": "970670",
    "end": "977780"
  },
  {
    "text": "in your browser is the best thing to do and let's actually what I'm going to do I will open a new window though Google",
    "start": "977780",
    "end": "997700"
  },
  {
    "text": "Chrome and I will try my death star for",
    "start": "997700",
    "end": "1008250"
  },
  {
    "text": "788 and GCP me with super secret",
    "start": "1008250",
    "end": "1018400"
  },
  {
    "text": "password",
    "start": "1018400",
    "end": "1021030"
  },
  {
    "text": "sure okay I need to accept google spying",
    "start": "1037329",
    "end": "1043029"
  },
  {
    "text": "on me and doing whatever they want to do and I am at my cluster console whoo okay",
    "start": "1043029",
    "end": "1058360"
  },
  {
    "text": "I need to agree with more my country of resident is Czech Republic you have it",
    "start": "1058360",
    "end": "1064239"
  },
  {
    "text": "in the list come on Czechia nice and I don't want",
    "start": "1064239",
    "end": "1073269"
  },
  {
    "text": "very rich emails so this is a Google",
    "start": "1073269",
    "end": "1082299"
  },
  {
    "text": "Cloud console we need just a very tiny part of it we can find under this",
    "start": "1082299",
    "end": "1087970"
  },
  {
    "text": "hamburger icon and kubernetes engine clusters okay then I need to pick a",
    "start": "1087970",
    "end": "1103779"
  },
  {
    "text": "project I will pick this cube con state",
    "start": "1103779",
    "end": "1109629"
  },
  {
    "text": "for tutorial and it doesn't allow me to",
    "start": "1109629",
    "end": "1118119"
  },
  {
    "text": "create the cluster okay we have two minutes to create a roster nice so",
    "start": "1118119",
    "end": "1124720"
  },
  {
    "text": "everybody is on the same page everything is working so far",
    "start": "1124720",
    "end": "1130679"
  },
  {
    "text": "hello can we get the mic working hello",
    "start": "1136900",
    "end": "1146510"
  },
  {
    "text": "okay so that API enabling that only needs to happen the first time you",
    "start": "1146510",
    "end": "1152390"
  },
  {
    "text": "create an account so in this case all of you are using new accounts so they need to enable the API really quickly okay so",
    "start": "1152390",
    "end": "1159650"
  },
  {
    "text": "everything is fine I can now create my kubernetes cluster and I'll leave the",
    "start": "1159650",
    "end": "1170750"
  },
  {
    "text": "default try to do MIT more is this readable I will because special zone",
    "start": "1170750",
    "end": "1181480"
  },
  {
    "text": "Europe West one what is it it doesn't",
    "start": "1181480",
    "end": "1190070"
  },
  {
    "text": "really matter which a B C or D I choose I choose B it doesn't really matter just",
    "start": "1190070",
    "end": "1196760"
  },
  {
    "text": "because Google staff confirmed this zone has the right amount of resources I",
    "start": "1196760",
    "end": "1203270"
  },
  {
    "text": "don't need to configure or much we will get free notes let's use four CPUs with",
    "start": "1203270",
    "end": "1212630"
  },
  {
    "text": "15 gigabytes of memory that's enough Google days and well that's it actually",
    "start": "1212630",
    "end": "1219410"
  },
  {
    "text": "so just the zone you do request one for",
    "start": "1219410",
    "end": "1225169"
  },
  {
    "text": "CPUs and create now it takes a couple of",
    "start": "1225169",
    "end": "1233480"
  },
  {
    "text": "minutes to create the cluster like Google machines are booting up creating",
    "start": "1233480",
    "end": "1239480"
  },
  {
    "text": "a cluster joining starting logging starting monitoring everything it takes",
    "start": "1239480",
    "end": "1244549"
  },
  {
    "text": "a couple of minutes so let's look at what we are going to do in the head of",
    "start": "1244549",
    "end": "1250490"
  },
  {
    "start": "1245000",
    "end": "1523000"
  },
  {
    "text": "hands-on what's the goal and the goal is",
    "start": "1250490",
    "end": "1256040"
  },
  {
    "text": "to run a very simple application that will do just a counter we will show a",
    "start": "1256040",
    "end": "1263720"
  },
  {
    "text": "web page with a counter every you remove the page the counter bumps",
    "start": "1263720",
    "end": "1270230"
  },
  {
    "text": "for that for that we will start",
    "start": "1270230",
    "end": "1279440"
  },
  {
    "text": "Cassandra database we have chosen",
    "start": "1279440",
    "end": "1284690"
  },
  {
    "text": "Cassandra just because it's really easy to set up for this tutorial we are not",
    "start": "1284690",
    "end": "1289740"
  },
  {
    "text": "database experts we don't endorse Cassandra and in any way but you will",
    "start": "1289740",
    "end": "1295620"
  },
  {
    "text": "see it has pretty nice features and it works pretty well infirmities so that's why we use it it behaves the way we want",
    "start": "1295620",
    "end": "1304410"
  },
  {
    "text": "it to behave and we will the rest of the",
    "start": "1304410",
    "end": "1311520"
  },
  {
    "text": "tutorial will be on this github page so please open it it will contain all the commands you need to do so you can just",
    "start": "1311520",
    "end": "1318360"
  },
  {
    "text": "copy and paste and everything will work so please open your web browser github.com slash Joseph Rama - /cas caas",
    "start": "1318360",
    "end": "1329760"
  },
  {
    "text": "counter as a service that's what we do as our business plan to solve this counter",
    "start": "1329760",
    "end": "1336320"
  },
  {
    "text": "everybody has this left page opened or you are not paying attention at all",
    "start": "1345070",
    "end": "1350710"
  },
  {
    "text": "I assume you have it open so the go is",
    "start": "1350710",
    "end": "1358660"
  },
  {
    "text": "again a stateful set with Cassandra roster we will have three nodes in the",
    "start": "1358660",
    "end": "1364930"
  },
  {
    "text": "cluster free Cassandra ports in the cluster sorry and we will have just a tiny deployment of",
    "start": "1364930",
    "end": "1371650"
  },
  {
    "text": "the application with the web front-end that connects to the Cassandra cluster",
    "start": "1371650",
    "end": "1377470"
  },
  {
    "text": "using Cassandra query language now all the pods in kubernetes have",
    "start": "1377470",
    "end": "1383530"
  },
  {
    "text": "basically random IP addresses so how can our application find the database if the",
    "start": "1383530",
    "end": "1389860"
  },
  {
    "text": "IP addresses are random we use the service use headless service that",
    "start": "1389860",
    "end": "1395890"
  },
  {
    "text": "provides genus DNS names to these spots we stayed for sets",
    "start": "1395890",
    "end": "1401140"
  },
  {
    "text": "and one of the benefit of state was set is that it provides stable names for the",
    "start": "1401140",
    "end": "1406750"
  },
  {
    "text": "pot we will have pot named Cassandra - Cassandra - one in Cassandra - to headless service creates a DNS entries",
    "start": "1406750",
    "end": "1415630"
  },
  {
    "text": "for us for these pots so our application just resolved this DNS and can connect",
    "start": "1415630",
    "end": "1421690"
  },
  {
    "text": "to the database without knowing the IP addresses and we will create a second",
    "start": "1421690",
    "end": "1428110"
  },
  {
    "text": "service now with type of long balancer as David explains that service will just",
    "start": "1428110",
    "end": "1435010"
  },
  {
    "text": "do a load balancing between one pot so well not much not much useful but it",
    "start": "1435010",
    "end": "1441880"
  },
  {
    "text": "will expose external IP address we can reach from outside from the internet the",
    "start": "1441880",
    "end": "1452650"
  },
  {
    "text": "Yama file for the stateful set is pretty long but su will create the sickness",
    "start": "1452650",
    "end": "1459850"
  },
  {
    "text": "that you can look at the Yama file it's well documented and it's pretty straightforward here's just small",
    "start": "1459850",
    "end": "1465820"
  },
  {
    "text": "excerpt we will ask for where is it we",
    "start": "1465820",
    "end": "1471220"
  },
  {
    "text": "will ask for free replicas of of we as poor for free ports in the safest",
    "start": "1471220",
    "end": "1477820"
  },
  {
    "text": "set here is a template of the pot so kubernetes just takes this template and",
    "start": "1477820",
    "end": "1482850"
  },
  {
    "text": "copies it three times inside the template we're on Cassandra image",
    "start": "1482850",
    "end": "1489820"
  },
  {
    "text": "whatever it is and we mount something",
    "start": "1489820",
    "end": "1495279"
  },
  {
    "text": "called Cassandra data into slash Cassandra underscore data inside the pot inside the container this Cassandra data",
    "start": "1495279",
    "end": "1502419"
  },
  {
    "text": "refers to a persistent volume claim template so each pod gets its own",
    "start": "1502419",
    "end": "1509289"
  },
  {
    "text": "persistent volume claim it will be named Cassandra this data and we just ask for",
    "start": "1509289",
    "end": "1514330"
  },
  {
    "text": "one gigabyte of storage that's readable and writable by single port so what what",
    "start": "1514330",
    "end": "1524909"
  },
  {
    "start": "1523000",
    "end": "1626000"
  },
  {
    "text": "kubernetes will do we will create the fitful set with a template it will",
    "start": "1524909",
    "end": "1531190"
  },
  {
    "text": "instantiate the templates we will get three ports and free person volume",
    "start": "1531190",
    "end": "1536320"
  },
  {
    "text": "claims now dynamic provisioning will kick in Google magic will happen and it",
    "start": "1536320",
    "end": "1544000"
  },
  {
    "text": "will provision free persistent volumes somewhere deep in the Google Cloud and we will have persistent volumes in our",
    "start": "1544000",
    "end": "1549760"
  },
  {
    "text": "permit Iskra the persistent volume objects in our kubernetes cluster and each of them get bound it gets bound to",
    "start": "1549760",
    "end": "1557950"
  },
  {
    "text": "our claims after that when we have",
    "start": "1557950",
    "end": "1564100"
  },
  {
    "text": "everything set up oops kubernetes will start the pods and that's where kubernetes talks it starts",
    "start": "1564100",
    "end": "1571960"
  },
  {
    "text": "the pots so we will have three different pots each of them will have each own",
    "start": "1571960",
    "end": "1578460"
  },
  {
    "text": "Cassandra daemon with its own volume but if you write something to the one to one",
    "start": "1578460",
    "end": "1584889"
  },
  {
    "text": "of the pods here if you write something here it will be just stored here and if",
    "start": "1584889",
    "end": "1591880"
  },
  {
    "text": "we want to read the value from the different pot it's not written here so",
    "start": "1591880",
    "end": "1598648"
  },
  {
    "text": "that's not part of kubernetes but part of the configuration of the state was said you must configure the application",
    "start": "1599100",
    "end": "1606210"
  },
  {
    "text": "to join somehow to form a cluster so they spread the data and I don't know",
    "start": "1606210",
    "end": "1613450"
  },
  {
    "text": "do some redirection or act effective whatever you want but this is not part of kubernetes kubernetes will not do",
    "start": "1613450",
    "end": "1619720"
  },
  {
    "text": "that for you that's your job in Cassandra case it's super simple so by",
    "start": "1619720",
    "end": "1627159"
  },
  {
    "start": "1626000",
    "end": "1700000"
  },
  {
    "text": "now our cluster should be ready everybody has working posture nice and",
    "start": "1627159",
    "end": "1634479"
  },
  {
    "text": "running with green icon right so let's",
    "start": "1634479",
    "end": "1640450"
  },
  {
    "text": "do some command line interaction you press the connect button and you run",
    "start": "1640450",
    "end": "1647559"
  },
  {
    "text": "this magic command in a cloud shell what",
    "start": "1647559",
    "end": "1655200"
  },
  {
    "text": "this I never had it I never saw it anyway we start cloud show what happens",
    "start": "1655200",
    "end": "1661359"
  },
  {
    "text": "somewhere in Google cloud new VM is born and it's starting and it will start a",
    "start": "1661359",
    "end": "1672099"
  },
  {
    "text": "shell Linux command line shell don't worry and it will connect it to our",
    "start": "1672099",
    "end": "1679989"
  },
  {
    "text": "kubernetes cluster that is this tiny icon that opens it in a new window so",
    "start": "1679989",
    "end": "1687989"
  },
  {
    "text": "the cloud shell is actually provisioning a VM in the backend as well that runs",
    "start": "1687989",
    "end": "1694059"
  },
  {
    "text": "your shell commands on so that also takes the amount of time it takes to provision a VM behind so while it's",
    "start": "1694059",
    "end": "1700419"
  },
  {
    "text": "provisioning again link to the web page",
    "start": "1700419",
    "end": "1707320"
  },
  {
    "text": "with all the examples we are going to do please open it we will copy and paste",
    "start": "1707320",
    "end": "1712499"
  },
  {
    "text": "the commands are pretty long to type and contain some URLs so please open this",
    "start": "1712499",
    "end": "1718419"
  },
  {
    "text": "while the shell is starting so I have my",
    "start": "1718419",
    "end": "1723789"
  },
  {
    "text": "show come on",
    "start": "1723789",
    "end": "1727169"
  },
  {
    "text": "really okay Croucher is popular right",
    "start": "1730340",
    "end": "1736560"
  },
  {
    "text": "now we are still trying to find a VM for you please wait",
    "start": "1736560",
    "end": "1741620"
  },
  {
    "text": "nice so if you are not at this step",
    "start": "1741950",
    "end": "1751920"
  },
  {
    "text": "right now just please raise your hand and someone from the side will come and help you get where you need to be anyway",
    "start": "1751920",
    "end": "1757770"
  },
  {
    "text": "this is the web page I told you to open I will zoom it in a bit it's a complete",
    "start": "1757770",
    "end": "1764790"
  },
  {
    "text": "source of the counter as a service spot and it is pretty stupid going",
    "start": "1764790",
    "end": "1770640"
  },
  {
    "text": "application but it contains all the steps we are going to do",
    "start": "1770640",
    "end": "1776300"
  },
  {
    "text": "come on Google you can do better",
    "start": "1776300",
    "end": "1780290"
  },
  {
    "text": "can I correct from my own show I do have control okay",
    "start": "1790060",
    "end": "1801970"
  },
  {
    "text": "luckily I have my own kubernetes client installed right so another way to do",
    "start": "1801970",
    "end": "1809890"
  },
  {
    "text": "this is if you have your own terminal with the G cloud SDK installed you can",
    "start": "1809890",
    "end": "1816520"
  },
  {
    "text": "actually just go to your own terminal I am now logged in sorry I'm not looking I'm looking I'm locked in as much as me",
    "start": "1816520",
    "end": "1823480"
  },
  {
    "text": "okay this crazy yawn is not logged in but if you are logged in or you can log",
    "start": "1823480",
    "end": "1829330"
  },
  {
    "text": "in as the dev the test credentials that",
    "start": "1829330",
    "end": "1834700"
  },
  {
    "text": "we gave you and you can actually run this on your own terminal as well you just copy/paste that connection command to your own",
    "start": "1834700",
    "end": "1840160"
  },
  {
    "text": "terminal and that would work is there some animals g-cloud mode",
    "start": "1840160",
    "end": "1846870"
  },
  {
    "text": "I don't want to come on it's just me or",
    "start": "1860190",
    "end": "1872130"
  },
  {
    "text": "everybody is slow sorry",
    "start": "1872130",
    "end": "1878270"
  },
  {
    "text": "Chico off login yeah",
    "start": "1897960",
    "end": "1907700"
  },
  {
    "text": "oh I would just Death Star",
    "start": "1907700",
    "end": "1914500"
  },
  {
    "text": "he",
    "start": "1924090",
    "end": "1926870"
  },
  {
    "text": "so apologies for the cloud shell not working right now what Yan is doing is he is logging into",
    "start": "1932510",
    "end": "1939600"
  },
  {
    "text": "the G cloud SDK on his on his machine as",
    "start": "1939600",
    "end": "1945030"
  },
  {
    "text": "the dev account that we've given you so if you have the G cloud SDK installed you can follow the steps that he's doing",
    "start": "1945030",
    "end": "1952050"
  },
  {
    "text": "right now if not please just wait for the cloud shell to provision it will",
    "start": "1952050",
    "end": "1958110"
  },
  {
    "text": "finish eventually and if your cloud",
    "start": "1958110",
    "end": "1968400"
  },
  {
    "text": "shell is working then then you're good just um wait for us to get our thing working and we'll continue on with the",
    "start": "1968400",
    "end": "1974400"
  },
  {
    "text": "tutorial it's not really working for me",
    "start": "1974400",
    "end": "1979100"
  },
  {
    "text": "still waiting for Co yeah good I can try",
    "start": "1979580",
    "end": "1989870"
  },
  {
    "text": "and G cloud doesn't really work no photo",
    "start": "1991730",
    "end": "1998550"
  },
  {
    "text": "value from from G GCP I'm not G CP",
    "start": "1998550",
    "end": "2004450"
  },
  {
    "text": "yeah they'll work but this doesn't work",
    "start": "2011150",
    "end": "2015040"
  },
  {
    "text": "[Music]",
    "start": "2017630",
    "end": "2021039"
  },
  {
    "text": "okay Plan B",
    "start": "2026410",
    "end": "2029730"
  },
  {
    "text": "finally I do have my show",
    "start": "2042080",
    "end": "2053929"
  },
  {
    "text": "who so I have my Google Cloud show now",
    "start": "2060460",
    "end": "2066220"
  },
  {
    "text": "connect it I'm not sure where you are",
    "start": "2066220",
    "end": "2072940"
  },
  {
    "text": "you have your shells waiting just waiting for me it was just me and we will go through",
    "start": "2072940",
    "end": "2081940"
  },
  {
    "start": "2081000",
    "end": "2456000"
  },
  {
    "text": "the tutorial so the first we will create Cassandra state forces that first step",
    "start": "2081940",
    "end": "2088929"
  },
  {
    "text": "number one copy paste we are just",
    "start": "2088929",
    "end": "2095108"
  },
  {
    "text": "applying some Yama file from internet don't worry it's a super safe mo file",
    "start": "2095109",
    "end": "2102690"
  },
  {
    "text": "and what did get it downloaded the mo",
    "start": "2105599",
    "end": "2111910"
  },
  {
    "text": "file from the internet and gave it to you control applied and that created a",
    "start": "2111910",
    "end": "2118900"
  },
  {
    "text": "stateful set you let create instead for",
    "start": "2118900",
    "end": "2129069"
  },
  {
    "text": "set and what by default they trusted us it creates each replica at the time so",
    "start": "2129069",
    "end": "2134890"
  },
  {
    "text": "it at first it creates Cassandra - OH",
    "start": "2134890",
    "end": "2140880"
  },
  {
    "text": "it creates the PVC it plays the pot and the white until it started completely",
    "start": "2140880",
    "end": "2146290"
  },
  {
    "text": "and after that they will start Cassandra - one wait until it's completely started",
    "start": "2146290",
    "end": "2152589"
  },
  {
    "text": "and then start Cassandra - - you can configure in the state who said that you want to run them in parallel but then",
    "start": "2152589",
    "end": "2160119"
  },
  {
    "text": "it's slightly harder to build the cluster because what Cassandra does it",
    "start": "2160119",
    "end": "2165640"
  },
  {
    "text": "was that the first replicas and all the others replica will connect to Cassandra - oh and join join join it's coaster",
    "start": "2165640",
    "end": "2176730"
  },
  {
    "text": "while it's starting you can we can explore some options that creep control",
    "start": "2176730",
    "end": "2182200"
  },
  {
    "text": "gives us like creep control get what - w it enters a weight mode",
    "start": "2182200",
    "end": "2189420"
  },
  {
    "text": "and we can see all the replicas starting slowly or we can try this describe",
    "start": "2189420",
    "end": "2199349"
  },
  {
    "text": "command we will describe Cassandra - - for example a describe pot because we",
    "start": "2199349",
    "end": "2208140"
  },
  {
    "text": "need to know we describe pots and it will link this lot of stuff about the",
    "start": "2208140",
    "end": "2215609"
  },
  {
    "text": "pot what image we're on what ports are open of what IP address it has and in",
    "start": "2215609",
    "end": "2222299"
  },
  {
    "text": "the ends there are events if there was some error we would see some error in",
    "start": "2222299",
    "end": "2228450"
  },
  {
    "text": "these events everything is starting so that's good but it's starting for long",
    "start": "2228450",
    "end": "2235230"
  },
  {
    "text": "so we what we can do this doesn't work you can check docks for example class -",
    "start": "2235230",
    "end": "2246270"
  },
  {
    "text": "- and we get locks from the pot or if we used a chef it'll follow the rocks",
    "start": "2246270",
    "end": "2255420"
  },
  {
    "text": "everything is already started so right",
    "start": "2255420",
    "end": "2263190"
  },
  {
    "text": "now we should have three Cassandra pots running they form the cluster and we",
    "start": "2263190",
    "end": "2270540"
  },
  {
    "text": "have our database that's the database part everybody's here everybody has a",
    "start": "2270540",
    "end": "2275609"
  },
  {
    "text": "cluster no yes I don't see any I don't",
    "start": "2275609",
    "end": "2282000"
  },
  {
    "text": "hear any feedback",
    "start": "2282000",
    "end": "2284390"
  },
  {
    "text": "now we deployed application step number well we did Nam step number two waiting",
    "start": "2289270",
    "end": "2295030"
  },
  {
    "text": "and using these commands to debug our cluster but now we apply the second demo",
    "start": "2295030",
    "end": "2302500"
  },
  {
    "text": "file let's create deployment and service",
    "start": "2302500",
    "end": "2310020"
  },
  {
    "text": "if I look at the pot we have now one extra pot that's the application we just",
    "start": "2310020",
    "end": "2316300"
  },
  {
    "text": "need one application that's fine and if you look at the service we get this",
    "start": "2316300",
    "end": "2325410"
  },
  {
    "text": "headless service named Cassandra he doesn't expose any IP address it just",
    "start": "2325410",
    "end": "2331960"
  },
  {
    "text": "provides the DNS names for our Cassandra ports that we have country counter as a",
    "start": "2331960",
    "end": "2338650"
  },
  {
    "text": "service service of type of balancer which has cluster internal IP address 10",
    "start": "2338650",
    "end": "2347740"
  },
  {
    "text": "7 2 4 138 this is reachable only inside the comes inside the cluster so if I had",
    "start": "2347740",
    "end": "2355180"
  },
  {
    "text": "a different pot in the cluster they could use this IP address to reach the service but it takes some time to get",
    "start": "2355180",
    "end": "2361750"
  },
  {
    "text": "the external IP address so yeah now we have it I have the IP address Google did",
    "start": "2361750",
    "end": "2372460"
  },
  {
    "text": "some magic and exposed this IP address on the Internet and that's our application as I've said this is just",
    "start": "2372460",
    "end": "2382180"
  },
  {
    "text": "counter and it include it increases with each reload there are some the body",
    "start": "2382180",
    "end": "2391000"
  },
  {
    "text": "information for example the IP address of the database server that answered the",
    "start": "2391000",
    "end": "2397300"
  },
  {
    "text": "application worried of what query it was",
    "start": "2397300",
    "end": "2405880"
  },
  {
    "text": "update some counter how many items it go and how long it was each bump basically",
    "start": "2405880",
    "end": "2414609"
  },
  {
    "text": "means we need to update the counter and select the current value and you can see the IP IP addresses of the database",
    "start": "2414609",
    "end": "2421780"
  },
  {
    "text": "servers changing with each reload they are free so we have two point four one point four",
    "start": "2421780",
    "end": "2428780"
  },
  {
    "text": "and open eight and again two point four through the client does around bobbin",
    "start": "2428780",
    "end": "2435110"
  },
  {
    "text": "between the database servers the database demons and that's it we have",
    "start": "2435110",
    "end": "2441980"
  },
  {
    "text": "our stiff application that's the state Fassett with Cassandra the database is stateful and then we have a stateless",
    "start": "2441980",
    "end": "2449480"
  },
  {
    "text": "web front-end that just does what in the database right we are ready for",
    "start": "2449480",
    "end": "2455990"
  },
  {
    "text": "production",
    "start": "2455990",
    "end": "2458110"
  },
  {
    "start": "2456000",
    "end": "2909000"
  },
  {
    "text": "tutorial here so we are at the step four we tested the application use the",
    "start": "2461500",
    "end": "2468140"
  },
  {
    "text": "external IP address from cube control get service we can also look inside what",
    "start": "2468140",
    "end": "2477650"
  },
  {
    "text": "Cassandra thinks about itself what we do",
    "start": "2477650",
    "end": "2484520"
  },
  {
    "text": "we execute clip control exec that exacts a command inside Cassandra - OH",
    "start": "2484520",
    "end": "2491330"
  },
  {
    "text": "pot and it executes note to status you can imagine something like SSH",
    "start": "2491330",
    "end": "2496870"
  },
  {
    "text": "but with kubernetes and contain runtime instead of the SSH so we executed is no",
    "start": "2496870",
    "end": "2504680"
  },
  {
    "text": "two status which is which gives the status of the Cassandra roster and",
    "start": "2504680",
    "end": "2511310"
  },
  {
    "text": "Cassandra knows about this free replicas all of them are up and running and",
    "start": "2511310",
    "end": "2517940"
  },
  {
    "text": "everything works and we can and application explicitly asked to store",
    "start": "2517940",
    "end": "2525200"
  },
  {
    "text": "three replicas of each counter so here we can we can see that each each each",
    "start": "2525200",
    "end": "2533360"
  },
  {
    "text": "Cassandra pod owns 100% of the data and",
    "start": "2533360",
    "end": "2539710"
  },
  {
    "text": "what we are going to do we are going to scale up our country is super popular",
    "start": "2539710",
    "end": "2547000"
  },
  {
    "text": "Cassandra pods are super busy and we need to add more so let's add",
    "start": "2547000",
    "end": "2552390"
  },
  {
    "text": "Ondra what you cube controls KO this -",
    "start": "2552390",
    "end": "2558480"
  },
  {
    "text": "replicas for and what are we scaling we are scaling State vs / Cassandra what",
    "start": "2558480",
    "end": "2567119"
  },
  {
    "text": "happens what is that kubernetes will",
    "start": "2567119",
    "end": "2576000"
  },
  {
    "text": "start Cassandra - free for us and it's slowly starting joining Cassandra",
    "start": "2576000",
    "end": "2583349"
  },
  {
    "text": "cluster doing some replication and whatever what whatever Cassandra needs",
    "start": "2583349",
    "end": "2589039"
  },
  {
    "text": "important is that our application is still running it's still using free",
    "start": "2589039",
    "end": "2597480"
  },
  {
    "text": "replicas you can use the - we W - W",
    "start": "2597480",
    "end": "2603559"
  },
  {
    "text": "running now we should have four IP",
    "start": "2603559",
    "end": "2609450"
  },
  {
    "text": "addresses 1.6 1.4 0.8 2.4 yeah we have",
    "start": "2609450",
    "end": "2614579"
  },
  {
    "text": "for what Cassandra thinks about itself is this we execute the note2 status",
    "start": "2614579",
    "end": "2622920"
  },
  {
    "text": "again and now cassandra thinks it has 4 replicas and each of them owns part of",
    "start": "2622920",
    "end": "2631170"
  },
  {
    "text": "the database so a cassandra internally did some reshuffling of the data and spread itself across more nodes and",
    "start": "2631170",
    "end": "2636829"
  },
  {
    "text": "that's what we want from our database and now the fun part",
    "start": "2636829",
    "end": "2643019"
  },
  {
    "text": "they are going to kill things for example well what happens if one of the",
    "start": "2643019",
    "end": "2651660"
  },
  {
    "text": "paths fails like it crashes or whatever happens we just kill it with cube",
    "start": "2651660",
    "end": "2656970"
  },
  {
    "text": "control delete pod Cassandra - one and",
    "start": "2656970",
    "end": "2662490"
  },
  {
    "text": "what kubernetes will do well it's already dead and kubernetes started",
    "start": "2662490",
    "end": "2668490"
  },
  {
    "text": "immediately after the death started immediately a new Cassandra port with",
    "start": "2668490",
    "end": "2675150"
  },
  {
    "text": "the same name as before Cassandra - one that if a different IP",
    "start": "2675150",
    "end": "2684920"
  },
  {
    "text": "address than before 2.5 and before it",
    "start": "2684920",
    "end": "2691280"
  },
  {
    "text": "had IP address 1.6 is missing and",
    "start": "2691280",
    "end": "2699400"
  },
  {
    "text": "instead of that we have 2.5 5e address so ports have different IP addresses",
    "start": "2699400",
    "end": "2705230"
  },
  {
    "text": "whenever they start was important for for us that the DNS record is still the",
    "start": "2705230",
    "end": "2711470"
  },
  {
    "text": "same it's still Cassandra - 1 dot service dot whatever so it's important",
    "start": "2711470",
    "end": "2719150"
  },
  {
    "text": "to do clients use the I don't use the IP addresses they should use the DNS",
    "start": "2719150",
    "end": "2724880"
  },
  {
    "text": "entries because they are stable IP addresses are not I didn't show it but",
    "start": "2724880",
    "end": "2731030"
  },
  {
    "text": "the application survived and it should arrive also while the pod was down and well",
    "start": "2731030",
    "end": "2742160"
  },
  {
    "text": "that's basically it I can scale down back to free replicas just to show you",
    "start": "2742160",
    "end": "2751099"
  },
  {
    "text": "what Cassandra does but it's not really this is not really important for the",
    "start": "2751099",
    "end": "2756200"
  },
  {
    "text": "tutorial scale back to free replicas well it is important because what",
    "start": "2756200",
    "end": "2761270"
  },
  {
    "text": "happens kubernetes always close the last pot in the state was set oh it killed",
    "start": "2761270",
    "end": "2766640"
  },
  {
    "text": "Cassandra - free so we have a nice ordered set 0 1 2 & 3 is gone and if we",
    "start": "2766640",
    "end": "2778940"
  },
  {
    "text": "look what Cassandra thinks about itself is Cassandra thinks it has 2 4 replicas",
    "start": "2778940",
    "end": "2786190"
  },
  {
    "text": "because what kubernetes did and just killed the pot nothing else it thinks",
    "start": "2786190",
    "end": "2792470"
  },
  {
    "text": "that this 1.6 pot it is there it's still",
    "start": "2792470",
    "end": "2799730"
  },
  {
    "text": "waiting for it for it to be up so it's D is down and Cassandra did not research",
    "start": "2799730",
    "end": "2808490"
  },
  {
    "text": "all data so it's again it's important if you scale up or down you must sure must make sure that the",
    "start": "2808490",
    "end": "2815349"
  },
  {
    "text": "application knows your what you are doing in this case Cassandra - free will",
    "start": "2815349",
    "end": "2822589"
  },
  {
    "text": "never come back so we will execute this",
    "start": "2822589",
    "end": "2828050"
  },
  {
    "text": "magic command and we will tell Cassandra that this whose ID is never coming back",
    "start": "2828050",
    "end": "2838190"
  },
  {
    "text": "and we will remove it from the roster and Cassandra does some magic and reshef",
    "start": "2838190",
    "end": "2843470"
  },
  {
    "text": "of the data and so it keeps free replicas of each entry so if I look back",
    "start": "2843470",
    "end": "2854030"
  },
  {
    "text": "what happened I can see all the replicas oh oh they all all the data so that's",
    "start": "2854030",
    "end": "2865099"
  },
  {
    "text": "basically the tutorial the things to remember is use DNS names not IP",
    "start": "2865099",
    "end": "2870920"
  },
  {
    "text": "addresses and when you scale up or down you may need to poke the application to",
    "start": "2870920",
    "end": "2881210"
  },
  {
    "text": "tell it that the replica with very few replicas or the replicas that just died it's never coming back and back to the",
    "start": "2881210",
    "end": "2889550"
  },
  {
    "text": "slides how'd it work for you did you follow or was it too quick any",
    "start": "2889550",
    "end": "2896770"
  },
  {
    "text": "problems looks good back to the slides",
    "start": "2896770",
    "end": "2905960"
  },
  {
    "text": "oh nice show so that was Cassandra you can see Cassandra in work pretty well",
    "start": "2905960",
    "end": "2912800"
  },
  {
    "text": "it's held up pretty nicely it came down with some manual poking but I guess you",
    "start": "2912800",
    "end": "2920390"
  },
  {
    "text": "can find use cases but it won't work but Cassandra will not behave in a performance or perform differently than",
    "start": "2920390",
    "end": "2928579"
  },
  {
    "text": "you would expect because all the databases work differently in different",
    "start": "2928579",
    "end": "2934910"
  },
  {
    "text": "conditions we'll look at the other databases and state facet for them",
    "start": "2934910",
    "end": "2943540"
  },
  {
    "text": "initially before that it would show MySQL and WordPress yes",
    "start": "2943540",
    "end": "2949199"
  },
  {
    "text": "during this demo because that's what everybody knows everybody uses everybody knows how to deploy on bare metal",
    "start": "2949199",
    "end": "2955709"
  },
  {
    "text": "let's try main component is and very quickly we found out that that's not that easy as we thought it's not even",
    "start": "2955709",
    "end": "2962279"
  },
  {
    "text": "possible I think in our kubernetes documentation there is this Yama file",
    "start": "2962279",
    "end": "2970999"
  },
  {
    "text": "well I can show it to you because it's enlightening here is a Yama file for the",
    "start": "2970999",
    "end": "2978660"
  },
  {
    "text": "state for set and you can see it contains some crazy shot scripts that do",
    "start": "2978660",
    "end": "2986489"
  },
  {
    "text": "the magic and form form MySQL cluster",
    "start": "2986489",
    "end": "2992849"
  },
  {
    "text": "and the cluster it forms it forms a",
    "start": "2992849",
    "end": "3000739"
  },
  {
    "text": "cluster where the first replica is readwrite but all the others were all read only but WordPress needs all the",
    "start": "3000739",
    "end": "3008569"
  },
  {
    "text": "connections readwrite that means we would not that it would not scale basically for for for WordPress you can",
    "start": "3008569",
    "end": "3016009"
  },
  {
    "text": "maybe find a use case but it would care for you but in this example it will",
    "start": "3016009",
    "end": "3021140"
  },
  {
    "text": "create a read/write metric just the master and all the others are just slaves maybe you can find also a",
    "start": "3021140",
    "end": "3029209"
  },
  {
    "text": "difference it was set for my school I didn't find them but I didn't try hard enough",
    "start": "3029209",
    "end": "3034279"
  },
  {
    "text": "I confess for progress Postgres there is a company called crunchy data which",
    "start": "3034279",
    "end": "3040130"
  },
  {
    "text": "provides enterprise support for Perth's Postgres and they also provide a nice yama files how to run for space instead",
    "start": "3040130",
    "end": "3047719"
  },
  {
    "text": "facets they look nice but we give interested and they also publish an",
    "start": "3047719",
    "end": "3054890"
  },
  {
    "text": "operator that simplifies the deployment and actually it looks solid maybe it's",
    "start": "3054890",
    "end": "3062869"
  },
  {
    "text": "what you want we tried mango you can find couple of tetris for it looks nice it's",
    "start": "3062869",
    "end": "3070430"
  },
  {
    "text": "almost like Cassandra instead of an operator it uses a sidecar that does the",
    "start": "3070430",
    "end": "3077570"
  },
  {
    "text": "think and all the scaling up scaring down logic I found just one glitch and",
    "start": "3077570",
    "end": "3085220"
  },
  {
    "text": "that was if you connect to you need to provide a connection string",
    "start": "3085220",
    "end": "3090410"
  },
  {
    "text": "question you rate and in this connection you're a URI you must specify all the",
    "start": "3090410",
    "end": "3096800"
  },
  {
    "text": "replicas of that's fine in our staff asset we have stable database name DNS names that's fine but what if I",
    "start": "3096800",
    "end": "3104210"
  },
  {
    "text": "skill up the application I should I must add this new port to this URI I believe you can do it it's",
    "start": "3104210",
    "end": "3112820"
  },
  {
    "text": "not hard but that's something you need to think about and they're a bunch of",
    "start": "3112820",
    "end": "3119240"
  },
  {
    "start": "3116000",
    "end": "3162000"
  },
  {
    "text": "cloud native databases like I just pick few representatives but there are many",
    "start": "3119240",
    "end": "3125360"
  },
  {
    "text": "more and you can find them here in compute con so you can go go to the",
    "start": "3125360",
    "end": "3130820"
  },
  {
    "text": "booth area and ask them what they are what the features are what prom problems",
    "start": "3130820",
    "end": "3138200"
  },
  {
    "text": "they solve what problems they don't solve because I believe there is no write database like you should you",
    "start": "3138200",
    "end": "3150010"
  },
  {
    "text": "should learn about all the databases and choose the right one for your use case",
    "start": "3150130",
    "end": "3157640"
  },
  {
    "text": "every use case is different and I'm",
    "start": "3157640",
    "end": "3165260"
  },
  {
    "start": "3162000",
    "end": "3258000"
  },
  {
    "text": "going to cover some more advanced topics about stateful applications the first",
    "start": "3165260",
    "end": "3171470"
  },
  {
    "text": "one that is the installation part you saw the beans for some crazy llaman",
    "start": "3171470",
    "end": "3177140"
  },
  {
    "text": "fires don't worry from the internet that's usually not what you want there are two ways how to install applications",
    "start": "3177140",
    "end": "3185380"
  },
  {
    "text": "better on kubernetes the first in one is home and using home chart it's basically",
    "start": "3185380",
    "end": "3191780"
  },
  {
    "text": "a template of a demo file so you just fill in some variables like your DNS",
    "start": "3191780",
    "end": "3200840"
  },
  {
    "text": "names whatever number of replicas you want storage class you want to use this",
    "start": "3200840",
    "end": "3206540"
  },
  {
    "text": "kind of stuff and it will generate demo files for you and in the create them and that's a little where",
    "start": "3206540",
    "end": "3214920"
  },
  {
    "text": "helm actually stops it just creates the objects and doesn't care about them too much and the other approach that's being",
    "start": "3214920",
    "end": "3222660"
  },
  {
    "text": "more popular now is operator operator is",
    "start": "3222660",
    "end": "3227910"
  },
  {
    "text": "just an application that can run in or Craster's and it will create the",
    "start": "3227910",
    "end": "3234569"
  },
  {
    "text": "application for you like Cassandra or PostgreSQL and it will also monitor it",
    "start": "3234569",
    "end": "3240599"
  },
  {
    "text": "so if something breaks it can automatically fix it again it depends on",
    "start": "3240599",
    "end": "3247440"
  },
  {
    "text": "the operator some of them are very simple and don't do much but some of",
    "start": "3247440",
    "end": "3253049"
  },
  {
    "text": "them can be pretty smart and fix a lot of things for you and before you start",
    "start": "3253049",
    "end": "3260400"
  },
  {
    "start": "3258000",
    "end": "3329000"
  },
  {
    "text": "deploying your application and your databases you must also think there are some",
    "start": "3260400",
    "end": "3266609"
  },
  {
    "text": "differences to the bare metal work that you are maybe used to for example",
    "start": "3266609",
    "end": "3271980"
  },
  {
    "text": "updates happen completely differently in kubernetes than in the burn metal and",
    "start": "3271980",
    "end": "3277640"
  },
  {
    "text": "when Archie when you are choosing the database or the application you must think how I'm going to apply update",
    "start": "3277640",
    "end": "3286430"
  },
  {
    "text": "citrucel supports rolling update that means it will it will go from the last",
    "start": "3286430",
    "end": "3292230"
  },
  {
    "text": "replica Cassandra - - for example it will kill it start a new one with the new version then it will go to Cassandra",
    "start": "3292230",
    "end": "3298710"
  },
  {
    "text": "- one kill it and start the new version of it and Cassandra - Oh kill it and stop the new version of it that looks",
    "start": "3298710",
    "end": "3305670"
  },
  {
    "text": "fine if your database can support multiple versions of itself in the same",
    "start": "3305670",
    "end": "3313380"
  },
  {
    "text": "posture maybe they should do like because that's the only way I have to",
    "start": "3313380",
    "end": "3318510"
  },
  {
    "text": "update the posture but be very careful so you don't run incompatible versions",
    "start": "3318510",
    "end": "3324000"
  },
  {
    "text": "in the same vastra and set facade can do that very easily another thing you",
    "start": "3324000",
    "end": "3330569"
  },
  {
    "start": "3329000",
    "end": "3397000"
  },
  {
    "text": "should thought you should think about before deploying something stateful is the networking as I showed you IP",
    "start": "3330569",
    "end": "3337200"
  },
  {
    "text": "addresses are completely random they change only the DNS names are stable",
    "start": "3337200",
    "end": "3342450"
  },
  {
    "text": "so should the client resolve a DNS name before every connection before every",
    "start": "3342450",
    "end": "3349829"
  },
  {
    "text": "database query or in the case of Cassandra I found out that it",
    "start": "3349829",
    "end": "3357809"
  },
  {
    "text": "automatically changed it automatically discovers IP addresses of other members of the cluster from the old members so",
    "start": "3357809",
    "end": "3363900"
  },
  {
    "text": "if I pay IP address changed the client occurs that automatically without any configuration without anything that's",
    "start": "3363900",
    "end": "3370680"
  },
  {
    "text": "fine but can your client do that and",
    "start": "3370680",
    "end": "3375859"
  },
  {
    "text": "other network related topic is the network partition if you have two data",
    "start": "3375859",
    "end": "3381059"
  },
  {
    "text": "centers connected with cable cable is broken you have parts of your database",
    "start": "3381059",
    "end": "3386880"
  },
  {
    "text": "on different networks they can't talk to each other so what should happen and",
    "start": "3386880",
    "end": "3391890"
  },
  {
    "text": "what do you want to happen in your application actually backup this can",
    "start": "3391890",
    "end": "3402930"
  },
  {
    "start": "3397000",
    "end": "3512000"
  },
  {
    "text": "behave very differently than in bare metal the cutters are usually easy you can find some way how to dump your",
    "start": "3402930",
    "end": "3410730"
  },
  {
    "text": "database it's not that different from bare metal but how do you restore the backup because kubernetes it will create",
    "start": "3410730",
    "end": "3419790"
  },
  {
    "text": "a persistent volume claims for you it will bind them to empty volumes so how",
    "start": "3419790",
    "end": "3425970"
  },
  {
    "text": "do you populate the data with from the backup will you jump the you just load",
    "start": "3425970",
    "end": "3433980"
  },
  {
    "text": "in the dump is it possible if you have two terabytes of dump how long it will",
    "start": "3433980",
    "end": "3439589"
  },
  {
    "text": "take or can I do some manually pre provisioning of the volumes and get the",
    "start": "3439589",
    "end": "3445500"
  },
  {
    "text": "data from there you should practice that and test it that your approach really",
    "start": "3445500",
    "end": "3450960"
  },
  {
    "text": "works and well they told you that",
    "start": "3450960",
    "end": "3457410"
  },
  {
    "text": "kubernetes are just ephemeral they come and go they are often killed when a node goes down or some more important pod",
    "start": "3457410",
    "end": "3464970"
  },
  {
    "text": "comes up so what your database thus when opposed dies and we didn't do any",
    "start": "3464970",
    "end": "3473250"
  },
  {
    "text": "special configuration so in our case in Cassandra it could happen that all the ports could be on the same note so if",
    "start": "3473250",
    "end": "3481020"
  },
  {
    "text": "the note goes down over the post from from Cassandra will go down and your",
    "start": "3481020",
    "end": "3487349"
  },
  {
    "text": "database is down so you should probably make sure that they spread evenly across",
    "start": "3487349",
    "end": "3493380"
  },
  {
    "text": "the cluster kubernetes has a concept for that called anti affinity and it can",
    "start": "3493380",
    "end": "3501150"
  },
  {
    "text": "also help you to spread the path of Cassandra across different data centers or different availability zones so we",
    "start": "3501150",
    "end": "3508410"
  },
  {
    "text": "should plan ahead and well I hope is the",
    "start": "3508410",
    "end": "3514349"
  },
  {
    "start": "3512000",
    "end": "3580000"
  },
  {
    "text": "last one it's security currently the our Cassandra ports were",
    "start": "3514349",
    "end": "3521339"
  },
  {
    "text": "not secure at all you can connect to it just knowing its IP address they will not know username password you can add",
    "start": "3521339",
    "end": "3526950"
  },
  {
    "text": "usernames passwords certificates or whatever easily it's not the different",
    "start": "3526950",
    "end": "3535440"
  },
  {
    "text": "from Bell Metro but what may be different is that the ports are running on random nodes in the cluster and you",
    "start": "3535440",
    "end": "3542039"
  },
  {
    "text": "don't know what else runs on the node and for example in our example in our",
    "start": "3542039",
    "end": "3548190"
  },
  {
    "text": "case we have this web application if somebody hacks the web application and",
    "start": "3548190",
    "end": "3554630"
  },
  {
    "text": "they can get inside the container and if they find some security issue and security fees happen they can probably",
    "start": "3554630",
    "end": "3561630"
  },
  {
    "text": "escape the container and get to other processes on the machine and maybe they",
    "start": "3561630",
    "end": "3567029"
  },
  {
    "text": "can get to my database with credit card numbers instead is this what I'm going to risk or should I do something better",
    "start": "3567029",
    "end": "3575549"
  },
  {
    "text": "for example I can dedicate some nodes just for the database so all that you",
    "start": "3575549",
    "end": "3581730"
  },
  {
    "start": "3580000",
    "end": "3600000"
  },
  {
    "text": "should consider before deploying anything stateful in kubernetes because",
    "start": "3581730",
    "end": "3588210"
  },
  {
    "text": "it may behave completely differently than environmental case I believe that's",
    "start": "3588210",
    "end": "3595950"
  },
  {
    "text": "all from us so do you have any questions",
    "start": "3595950",
    "end": "3600650"
  },
  {
    "text": "yeah just thank you for for coming to our state for sartorial I just wanted to",
    "start": "3602390",
    "end": "3607859"
  },
  {
    "text": "add that those accounts that we gave you I think are active for 24 hours so if you missed",
    "start": "3607859",
    "end": "3613880"
  },
  {
    "text": "any part of it or you want to try out some of the extra steps that that we did",
    "start": "3613880",
    "end": "3619220"
  },
  {
    "text": "not cover if you go to that github repository there are some extra steps you can try them all out you can do that",
    "start": "3619220",
    "end": "3625550"
  },
  {
    "text": "for the next day or so besides that where we'll be here for questions if you have any but the main",
    "start": "3625550",
    "end": "3633590"
  },
  {
    "text": "portion of the talk is done and we are part of six storage in kubernetes we",
    "start": "3633590",
    "end": "3639230"
  },
  {
    "text": "work on all of the storage related things we have a mailing list that you",
    "start": "3639230",
    "end": "3645080"
  },
  {
    "text": "can email bi-weekly meetings you can join us on our slack if you're interested in contributing and here is",
    "start": "3645080",
    "end": "3652850"
  },
  {
    "text": "the contact information for me David and Yann as well are there any questions in",
    "start": "3652850",
    "end": "3658190"
  },
  {
    "text": "the audience yes I are there any use",
    "start": "3658190",
    "end": "3670130"
  },
  {
    "text": "cases where you use some sort of stickiness or our affinity or is it",
    "start": "3670130",
    "end": "3676430"
  },
  {
    "text": "always an anti-pattern to use we have and we have both affinity",
    "start": "3676430",
    "end": "3685160"
  },
  {
    "text": "and we have put anti affinity anti affinity says that I don't want to run a",
    "start": "3685160",
    "end": "3691100"
  },
  {
    "text": "pot with other pot with this characteristic on the note and that's what we want in the database case I",
    "start": "3691100",
    "end": "3697910"
  },
  {
    "text": "don't want to run two database notes the database pot on the same note we have",
    "start": "3697910",
    "end": "3703220"
  },
  {
    "text": "also affinity I never used that and that says I want always to run on a note when",
    "start": "3703220",
    "end": "3710480"
  },
  {
    "text": "where this pot runs I maybe I have I can have a driver port your driver and",
    "start": "3710480",
    "end": "3717200"
  },
  {
    "text": "something that uses the driver so they want to run on the same note maybe that's the use case but I never use that",
    "start": "3717200",
    "end": "3723170"
  },
  {
    "text": "actually right and just to add to that I wouldn't call using affinity and anti-pattern there there are lots of",
    "start": "3723170",
    "end": "3729350"
  },
  {
    "text": "valid reasons why you would want to use why you would want to schedule specific pause to specific nodes or not to",
    "start": "3729350",
    "end": "3736490"
  },
  {
    "text": "specific notes yes",
    "start": "3736490",
    "end": "3739390"
  },
  {
    "text": "session affinity if the client secession",
    "start": "3741630",
    "end": "3749740"
  },
  {
    "text": "esta native is the client what we told you that all the sell the service object",
    "start": "3749740",
    "end": "3756240"
  },
  {
    "text": "it does load balancing that's great but it also does load balancing based on",
    "start": "3756240",
    "end": "3761370"
  },
  {
    "text": "HTTP cookies so if I have two ports in",
    "start": "3761370",
    "end": "3766510"
  },
  {
    "text": "the web front end and then kubernetes",
    "start": "3766510",
    "end": "3773410"
  },
  {
    "text": "will route my browser just to one of the",
    "start": "3773410",
    "end": "3778480"
  },
  {
    "text": "port's because it's based on HTTP cookies does it answer your question any",
    "start": "3778480",
    "end": "3790360"
  },
  {
    "text": "other questions I think which part of component is does that and I think is",
    "start": "3790360",
    "end": "3796960"
  },
  {
    "text": "the service service or load balancer service right you might have to look",
    "start": "3796960",
    "end": "3802810"
  },
  {
    "text": "into what exactly is being provisioned for you and how that one works I saw a",
    "start": "3802810",
    "end": "3808780"
  },
  {
    "text": "question over there",
    "start": "3808780",
    "end": "3811440"
  },
  {
    "text": "could you describe what happens when a node fails what's supposed to happen to the Peavey's that are attached to that",
    "start": "3817260",
    "end": "3822720"
  },
  {
    "text": "node so the question was what happens to",
    "start": "3822720",
    "end": "3828420"
  },
  {
    "text": "the storage when the node fails storage storage you can remember what is my",
    "start": "3828420",
    "end": "3837570"
  },
  {
    "text": "cloud thingy you may remember that I skill I scale the cassandra' up and scale it down so I",
    "start": "3837570",
    "end": "3844890"
  },
  {
    "text": "have I have three pots in my cluster but",
    "start": "3844890",
    "end": "3852710"
  },
  {
    "text": "come on get PVC I still have four",
    "start": "3854900",
    "end": "3865520"
  },
  {
    "text": "participant claims what kubernetes does it deletes the pots but it does not",
    "start": "3865520",
    "end": "3870930"
  },
  {
    "text": "delete the claims good it was a good question actually because state Fassett",
    "start": "3870930",
    "end": "3877830"
  },
  {
    "text": "are precious and storage are precious even more what are easy you can delete",
    "start": "3877830",
    "end": "3884340"
  },
  {
    "text": "them start the new ones that they are stateless basically kubernetes will",
    "start": "3884340",
    "end": "3890670"
  },
  {
    "text": "never delete the state it will never delete the percent volume claim so if you'd delete the stateful set it will",
    "start": "3890670",
    "end": "3897600"
  },
  {
    "text": "delete all the pots but it will keep the storage around for you and you must manually delete the claims when you",
    "start": "3897600",
    "end": "3903540"
  },
  {
    "text": "don't need them sorry right the question",
    "start": "3903540",
    "end": "3908880"
  },
  {
    "text": "was I think what what happens to the storage when the node goes down and I think this I mean it depends on what",
    "start": "3908880",
    "end": "3916380"
  },
  {
    "text": "direction you look at it from the storage itself if it's like",
    "start": "3916380",
    "end": "3921480"
  },
  {
    "text": "network-attached storage and your node goes down your storage is still available somewhere else right or it's",
    "start": "3921480",
    "end": "3926520"
  },
  {
    "text": "still available externally so if you configure your cluster correctly and you",
    "start": "3926520",
    "end": "3932160"
  },
  {
    "text": "bring up your plot your workload that uses that storage somewhere else you're actually able to then attach to that",
    "start": "3932160",
    "end": "3938670"
  },
  {
    "text": "network storage again and just continue using it does that answer your question",
    "start": "3938670",
    "end": "3944000"
  },
  {
    "text": "this in our example on Google the storage is somewhere in the cloud is not",
    "start": "3945000",
    "end": "3950280"
  },
  {
    "text": "on the note so what Google does is just attaches the storage to different note",
    "start": "3950280",
    "end": "3956280"
  },
  {
    "text": "and starts a put there that's easy in kubernetes we also have local storage",
    "start": "3956280",
    "end": "3961680"
  },
  {
    "text": "which where you can use nodes that are attached that are physically on the main",
    "start": "3961680",
    "end": "3968610"
  },
  {
    "text": "on the nodes but then if the node goes down your storage goes down too so you",
    "start": "3968610",
    "end": "3976950"
  },
  {
    "text": "should use some storage back-end that does some replication or crowd or",
    "start": "3976950",
    "end": "3983210"
  },
  {
    "text": "something like that right this brings up a really good point that choosing your storage back-end is really important",
    "start": "3983210",
    "end": "3989100"
  },
  {
    "text": "right like if four different resiliency needs you can choose different backends right if you use a local if you use",
    "start": "3989100",
    "end": "3995880"
  },
  {
    "text": "local disk on a node if that node goes down you lose access to all of that data right but if you're using something like",
    "start": "3995880",
    "end": "4002180"
  },
  {
    "text": "attached network attached storage or even replicated network attached storage you're gonna have better resiliency",
    "start": "4002180",
    "end": "4007790"
  },
  {
    "text": "guarantees on that all right are there any other questions in the audience",
    "start": "4007790",
    "end": "4015010"
  },
  {
    "text": "hi my question is if you have situation",
    "start": "4021790",
    "end": "4027080"
  },
  {
    "text": "where you have like database my master",
    "start": "4027080",
    "end": "4032300"
  },
  {
    "text": "and slave and you have persistent storage and you do you need to use two",
    "start": "4032300",
    "end": "4041750"
  },
  {
    "text": "storage classes for that or you can use one storage place because you want to",
    "start": "4041750",
    "end": "4047180"
  },
  {
    "text": "ensure that master ovale gets the same persistent volume and the slave always",
    "start": "4047180",
    "end": "4052880"
  },
  {
    "text": "gets another one so you have multiple pause that eventually need to use the",
    "start": "4052880",
    "end": "4059360"
  },
  {
    "text": "same persistent volume really well that probably instead Fassett is not good for",
    "start": "4059360",
    "end": "4065900"
  },
  {
    "text": "you because it creates you expect volume for each part you can use deployment",
    "start": "4065900",
    "end": "4074240"
  },
  {
    "text": "that you have the picture I think from what I",
    "start": "4074240",
    "end": "4079430"
  },
  {
    "text": "understand you want a storage like you're asking if you should have a storage class for all of the storage for",
    "start": "4079430",
    "end": "4086059"
  },
  {
    "text": "your slave workloads and a different one for your master workloads is that your",
    "start": "4086059",
    "end": "4092059"
  },
  {
    "text": "question so this do the way you choose how many storage classes you need it actually",
    "start": "4092059",
    "end": "4097969"
  },
  {
    "text": "depends on the needs of how you group your storage right if the Masters and the slaves use the same type of storage",
    "start": "4097969",
    "end": "4104900"
  },
  {
    "text": "I'm assuming each one has its own volume right each replica of your masters and",
    "start": "4104900",
    "end": "4110540"
  },
  {
    "text": "slaves so if they need the same type of volume they could use the same storage",
    "start": "4110540",
    "end": "4115580"
  },
  {
    "text": "class so say they all need SSDs from from GCE or something you would have one",
    "start": "4115580",
    "end": "4121969"
  },
  {
    "text": "storage class for that and you would your masters and your slaves would all use that one storage clause if you want",
    "start": "4121969",
    "end": "4127278"
  },
  {
    "text": "SSDs for your masters but some other type maybe like NFS or you want",
    "start": "4127279",
    "end": "4132940"
  },
  {
    "text": "hard-disk for your slaves then that's when you would have two different storage class",
    "start": "4132940",
    "end": "4138199"
  },
  {
    "text": "right storage class is a concept that binds a class of persistent volumes a class of storage together do that answer",
    "start": "4138199",
    "end": "4145488"
  },
  {
    "text": "your question you can have multiple pots using the",
    "start": "4145489",
    "end": "4153290"
  },
  {
    "text": "same storage no if you no no no it's like there's master and slave and",
    "start": "4153290",
    "end": "4162580"
  },
  {
    "text": "basically when a master goes up want to or pickup the same files when you go",
    "start": "4162580",
    "end": "4168859"
  },
  {
    "text": "down and same case for slave if I can",
    "start": "4168859",
    "end": "4178370"
  },
  {
    "text": "just refract right ooh rephrase again I think are you asking if when your workloads go down and they come back up",
    "start": "4178370",
    "end": "4185960"
  },
  {
    "text": "you want them to use the same underlying storage that they were before that they",
    "start": "4185960",
    "end": "4191088"
  },
  {
    "text": "were using before right so so that's definitely possible with a stateful set they all have stable naming of the",
    "start": "4191089",
    "end": "4199280"
  },
  {
    "text": "workloads and so when they come back up I think they'll just pick they'll just pick up the same storage again from",
    "start": "4199280",
    "end": "4205790"
  },
  {
    "text": "where they left off yes with a state full set okay",
    "start": "4205790",
    "end": "4211010"
  },
  {
    "text": "one more question over here Thanks thank you for the talk quick question is it possible to resize the persistent",
    "start": "4211010",
    "end": "4218210"
  },
  {
    "text": "volume claim let's say created a database where the disk of ten gigabytes right and then I want to grow to 20 do I",
    "start": "4218210",
    "end": "4224000"
  },
  {
    "text": "need like to do load restore or is it possible to resize it so this is a great question we're actually currently",
    "start": "4224000",
    "end": "4231290"
  },
  {
    "text": "working on resize functionality so in kubernetes i believe in kubernetes 1.13",
    "start": "4231290",
    "end": "4239060"
  },
  {
    "text": "we have resize as offline resizes beta so this is a thing that you can do right",
    "start": "4239060",
    "end": "4244820"
  },
  {
    "text": "now your of course your storage back-end has to support it as well but I believe",
    "start": "4244820",
    "end": "4249830"
  },
  {
    "text": "the way you interact with it is just by changing the size on your PV or PVC what",
    "start": "4249830",
    "end": "4257570"
  },
  {
    "text": "you should look up the documentation for this but you change the size on it and if your storage back-end supports it",
    "start": "4257570",
    "end": "4263390"
  },
  {
    "text": "that will just kick off the whole process for resizing yeah and and",
    "start": "4263390",
    "end": "4269840"
  },
  {
    "text": "another plug is we're actually working on online resize as well so offline",
    "start": "4269840",
    "end": "4275090"
  },
  {
    "text": "resizes while your workload is using the PV you can't resize it you'll have to bring your workload down first but",
    "start": "4275090",
    "end": "4281810"
  },
  {
    "text": "currently in kubernetes we're working on online resize as well so if your storage back-end supports it and your file",
    "start": "4281810",
    "end": "4287540"
  },
  {
    "text": "system supports it you'll be able to resize your volumes on the fly so in",
    "start": "4287540",
    "end": "4296000"
  },
  {
    "text": "this example I can like resize the PVC from 5 from 0 to 5 why well it was",
    "start": "4296000",
    "end": "4312380"
  },
  {
    "text": "allowed that means the third mission plug-in is not there right all right are there any other questions on the floor I",
    "start": "4312380",
    "end": "4320260"
  },
  {
    "text": "think John is trying to demonstrate resize no no he's not",
    "start": "4320260",
    "end": "4327429"
  },
  {
    "text": "you have a disabled in your posture yeah I have a disabled but you just editable",
    "start": "4329179",
    "end": "4334739"
  },
  {
    "text": "his PVC and that's it one short question I tried to use the",
    "start": "4334739",
    "end": "4342060"
  },
  {
    "text": "state physiatrist elasticsearch and he was a second to host mount and to allow",
    "start": "4342060",
    "end": "4348960"
  },
  {
    "text": "this I need to make sure that first instance always lands on the same note because host mounted data for the first",
    "start": "4348960",
    "end": "4356130"
  },
  {
    "text": "instance is specifically on this note is this possible or do I have to use a more",
    "start": "4356130",
    "end": "4363270"
  },
  {
    "text": "clever back-end storage clapping so so",
    "start": "4363270",
    "end": "4368310"
  },
  {
    "text": "the question I think was if you're using a staple set but you have specifically",
    "start": "4368310",
    "end": "4374280"
  },
  {
    "text": "requirements for your zeroeth pod for some affinity to some note so okay but",
    "start": "4374280",
    "end": "4380670"
  },
  {
    "text": "not your other pods so I don't know actually yeah I think the answer for me",
    "start": "4380670",
    "end": "4386880"
  },
  {
    "text": "as well as well as I'm not not super sure yes so don't use house path for",
    "start": "4386880",
    "end": "4402210"
  },
  {
    "text": "your production applications for exactly the reason that you mentioned because you're gonna have to take care of",
    "start": "4402210",
    "end": "4408179"
  },
  {
    "text": "actually scheduling the pods kubernetes doesn't know that you're it's gonna have",
    "start": "4408179",
    "end": "4413280"
  },
  {
    "text": "to stay sticky to that note that it first gets scheduled to so that becomes",
    "start": "4413280",
    "end": "4418980"
  },
  {
    "text": "complicated what you should use instead are persistent local volumes and persistent local volumes the scheduler",
    "start": "4418980",
    "end": "4425340"
  },
  {
    "text": "is aware wherever this volume is first scheduled it will always be rescheduled",
    "start": "4425340",
    "end": "4432540"
  },
  {
    "text": "to the same location automatically and the caveat there of course is that it",
    "start": "4432540",
    "end": "4439560"
  },
  {
    "text": "may if your node goes down you're gonna lose that storage so you have to do",
    "start": "4439560",
    "end": "4446550"
  },
  {
    "text": "application level replication but with your database you're already going to be able to do that but the scheduling gets",
    "start": "4446550",
    "end": "4451650"
  },
  {
    "text": "taken care of for you automatically so so that was the lead of six storage",
    "start": "4451650",
    "end": "4458060"
  },
  {
    "text": "that saw dolly any other questions",
    "start": "4458060",
    "end": "4464680"
  },
  {
    "text": "- to update or database whether don't",
    "start": "4474790",
    "end": "4480400"
  },
  {
    "text": "like SQL much simpler the experiences of",
    "start": "4480400",
    "end": "4485950"
  },
  {
    "text": "Rossum primitive that beyond what we have in operators yeah",
    "start": "4485950",
    "end": "4493960"
  },
  {
    "text": "like yes key my wives I like",
    "start": "4493960",
    "end": "4500969"
  },
  {
    "text": "okay so the question is if I want to update my database schema or something",
    "start": "4522750",
    "end": "4529810"
  },
  {
    "text": "to do with my application I guess does kubernetes offer any primitives to help",
    "start": "4529810",
    "end": "4535120"
  },
  {
    "text": "with this update so okay so yeah we",
    "start": "4535120",
    "end": "4540130"
  },
  {
    "text": "don't so kubernetes schedules the workloads on two nodes we don't like kubernetes core",
    "start": "4540130",
    "end": "4548080"
  },
  {
    "text": "doesn't really have anything that will deal with your application directly but we do have primitives that can stop your",
    "start": "4548080",
    "end": "4555400"
  },
  {
    "text": "pods right you can stop your containers you can start them but I don't think",
    "start": "4555400",
    "end": "4560560"
  },
  {
    "text": "it's gonna do that in an application consistent way you would probably have to deal with your application",
    "start": "4560560",
    "end": "4565930"
  },
  {
    "text": "specifically for that kind of yeah you can use an operator to solve that",
    "start": "4565930",
    "end": "4571420"
  },
  {
    "text": "problem well you can write your own operators of the program I don't think",
    "start": "4571420",
    "end": "4577510"
  },
  {
    "text": "you can find any operator already-existing for this all right any",
    "start": "4577510",
    "end": "4586420"
  },
  {
    "text": "other questions",
    "start": "4586420",
    "end": "4588690"
  },
  {
    "text": "not on I'm not hundred percent sure if this was already covered but my question",
    "start": "4595320",
    "end": "4601330"
  },
  {
    "text": "is if I have some sort of a database say PostgreSQL or something and I need to have this running as a",
    "start": "4601330",
    "end": "4609490"
  },
  {
    "text": "database cluster are there are solutions available to get this up and running",
    "start": "4609490",
    "end": "4615130"
  },
  {
    "text": "within our native system so that I do not have to create my database cluster",
    "start": "4615130",
    "end": "4622530"
  },
  {
    "text": "outside of my community's world the",
    "start": "4622530",
    "end": "4628330"
  },
  {
    "text": "question being are there solutions to bring up my my database solution inside",
    "start": "4628330",
    "end": "4635140"
  },
  {
    "text": "of kubernetes like as an entire cluster or just inside my cluster I guess I",
    "start": "4635140",
    "end": "4642880"
  },
  {
    "text": "guess both are very similar so there are",
    "start": "4642880",
    "end": "4649350"
  },
  {
    "text": "other solutions right like this is not you won't have a kubernetes primitive",
    "start": "4649350",
    "end": "4654790"
  },
  {
    "text": "like there's not gonna be any kubernetes command that you can call this as bring up my Postgres cluster but of course",
    "start": "4654790",
    "end": "4660790"
  },
  {
    "text": "there's a very rich ecosystem of solutions that people have built on top of kubernetes that you can leverage so",
    "start": "4660790",
    "end": "4668110"
  },
  {
    "text": "we don't have any specific recommendations or suggestions for you unfortunately you can try this crunchy",
    "start": "4668110",
    "end": "4675550"
  },
  {
    "text": "data yan has tried this crunchy data but I don't think we you know personally",
    "start": "4675550",
    "end": "4682450"
  },
  {
    "text": "endorse it yeah I never tried it but we've never tried always but you can look into it and they provide enterprise support so",
    "start": "4682450",
    "end": "4690010"
  },
  {
    "text": "can you can blame them if it fails all right any other",
    "start": "4690010",
    "end": "4695620"
  },
  {
    "text": "questions no more questions and the end up every",
    "start": "4695620",
    "end": "4702220"
  },
  {
    "text": "cool all right because we're ending early thank you again for coming and thank you for all your questions",
    "start": "4702220",
    "end": "4708220"
  },
  {
    "text": "[Applause]",
    "start": "4708220",
    "end": "4711449"
  }
]