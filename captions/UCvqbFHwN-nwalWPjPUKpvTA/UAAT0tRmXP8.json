[
  {
    "start": "0",
    "end": "95000"
  },
  {
    "text": "okay should we start let's do it hi everyone welcome to the Prometheus",
    "start": "260",
    "end": "5520"
  },
  {
    "text": "deep dive session which is we you know just a group of Prometheus maintain errs",
    "start": "5520",
    "end": "10980"
  },
  {
    "text": "on stage answering your questions we don't have any other program so if that's not what you were looking for",
    "start": "10980",
    "end": "16260"
  },
  {
    "text": "now's the chance to leave but basically the idea was really just you know we",
    "start": "16260",
    "end": "22529"
  },
  {
    "text": "have so many Prometheus people at this conference it's probably outside of prom con which is the actual prometheus",
    "start": "22529",
    "end": "28080"
  },
  {
    "text": "conference the biggest gathering of all of us I've ever seen so that's really",
    "start": "28080",
    "end": "33960"
  },
  {
    "text": "cool and I just thought maybe that's a good opportunity for people to ask random questions about the project's",
    "start": "33960",
    "end": "40920"
  },
  {
    "text": "direction the community about technical things to us and then hopefully we'll find someone who's the most qualified to",
    "start": "40920",
    "end": "47820"
  },
  {
    "text": "answer that but if you have to ask questions please try to ask questions that are not like oh I've had this",
    "start": "47820",
    "end": "54719"
  },
  {
    "text": "really weird strange error on darker with Prometheus and it didn't connect to this port because they're not useful",
    "start": "54719",
    "end": "59730"
  },
  {
    "text": "like try to ask questions that are that don't go into too much depth and that",
    "start": "59730",
    "end": "65220"
  },
  {
    "text": "are use or that are like potentially interesting to other people than yourself in the room and the way we have",
    "start": "65220",
    "end": "73020"
  },
  {
    "text": "to do it because it would take a long time to actually pass around the mic is that you if you feel okay just yelling",
    "start": "73020",
    "end": "79979"
  },
  {
    "text": "out the question and then we can summarize it and then answer it that should work in this size of room if you",
    "start": "79979",
    "end": "87060"
  },
  {
    "text": "feel like you would really need the microphone we can also run there with you at you but yeah that's that's",
    "start": "87060",
    "end": "93180"
  },
  {
    "text": "basically the idea just as an intro I'm Julius I'm one of the initial two",
    "start": "93180",
    "end": "100500"
  },
  {
    "start": "95000",
    "end": "155000"
  },
  {
    "text": "creators of Prometheus back then nowadays I freelance completely independently helping companies use",
    "start": "100500",
    "end": "106770"
  },
  {
    "text": "Prometheus then we have matte layer here he works for digital ocean Tom Wilkie",
    "start": "106770",
    "end": "112920"
  },
  {
    "text": "Griffin alabs Richard Hartman space net one of the oldest German internet I his",
    "start": "112920",
    "end": "119189"
  },
  {
    "text": "pieces always what you say right yes Brian Brazil robust reception Prometheus",
    "start": "119189",
    "end": "125189"
  },
  {
    "text": "consulting company we have Mac's inland now Red Hat formerly core OS same for",
    "start": "125189",
    "end": "130739"
  },
  {
    "text": "Frederick branch shake let me Fabian who just freshly joined Google he",
    "start": "130739",
    "end": "136800"
  },
  {
    "text": "was on stage yesterday we have been kochi he's at good lab and we have Galton be over there who is also he just",
    "start": "136800",
    "end": "142830"
  },
  {
    "text": "joined katana labs as well so yeah all active maintainer Zoar / contributors to",
    "start": "142830",
    "end": "149550"
  },
  {
    "text": "the project that the Prometheus teamed",
    "start": "149550",
    "end": "158129"
  },
  {
    "start": "155000",
    "end": "245000"
  },
  {
    "text": "we don't have a company it's just the project and all of us work for a whole bunch of different places so yeah it",
    "start": "158129",
    "end": "164730"
  },
  {
    "text": "really contributes to the openness of the project it's also a challenge because we have one of the few CN CF",
    "start": "164730",
    "end": "169860"
  },
  {
    "text": "projects who were not started by a company in order to start an open source infrastructure project we kind of just",
    "start": "169860",
    "end": "177180"
  },
  {
    "text": "created Prometheus because we felt we really needed it to do our job at Sound Cloud but SoundCloud was not interested",
    "start": "177180",
    "end": "182489"
  },
  {
    "text": "in it not not initially at least you know and so we don't really have the one",
    "start": "182489",
    "end": "188400"
  },
  {
    "text": "Prometheus company or so which is a good thing and terms of Independence but also means we don't have as many full-time",
    "start": "188400",
    "end": "194220"
  },
  {
    "text": "staff and community members working on it getting paid by companies you know",
    "start": "194220",
    "end": "199410"
  },
  {
    "text": "Google has many many companies for example many many people working on kubernetes and so does Red Hat and other",
    "start": "199410",
    "end": "205410"
  },
  {
    "text": "companies so yeah that's that's also sometimes a challenge but yeah that's",
    "start": "205410",
    "end": "210599"
  },
  {
    "text": "that's kind of what sets us apart they're a bit cool and I think there's",
    "start": "210599",
    "end": "216450"
  },
  {
    "text": "one question already there and then we'll go to you afterwards",
    "start": "216450",
    "end": "221840"
  },
  {
    "text": "it's not so so okay depends what what you mean was difficult so in prometheus",
    "start": "243299",
    "end": "249129"
  },
  {
    "text": "itself we decided not to integrate long term storage because we want to keep Prometheus relatively simple single node",
    "start": "249129",
    "end": "255549"
  },
  {
    "text": "kind of setups we do have you're probably aware of the the remote right",
    "start": "255549",
    "end": "261010"
  },
  {
    "text": "integrations and several backends existing for that are those too",
    "start": "261010",
    "end": "266380"
  },
  {
    "text": "difficult to use currently or are they not working well enough or you don't",
    "start": "266380",
    "end": "278979"
  },
  {
    "text": "want to have two systems to maintain",
    "start": "278979",
    "end": "282330"
  },
  {
    "text": "okay so the question is how do we long terms how do we do long-term storage",
    "start": "294210",
    "end": "299470"
  },
  {
    "text": "while only having one kind of system to deal with this hey yeah yeah a",
    "start": "299470",
    "end": "307000"
  },
  {
    "text": "non-standard answer to this question do you wanna yeah I think Fabian might want",
    "start": "307000",
    "end": "312099"
  },
  {
    "text": "to say something about finals as well there's there's there's a bunch of systems except in this long-term storage",
    "start": "312099",
    "end": "318280"
  },
  {
    "text": "remote API one of them's cortex which is I work on and the other one stano's which having talked about a sec cortex",
    "start": "318280",
    "end": "325030"
  },
  {
    "text": "is too hard for you to run on your own like so generally it's mainly aimed at SAS vendors who want to sell you storage",
    "start": "325030",
    "end": "330970"
  },
  {
    "text": "which is why I work for labs but it's you know it's kind of cool just mentioning as well you can access",
    "start": "330970",
    "end": "337150"
  },
  {
    "text": "everything via / Matthias using Euro greed that's why it exists but nobody's",
    "start": "337150",
    "end": "343180"
  },
  {
    "text": "using it now I think like one one thing we see",
    "start": "343180",
    "end": "348490"
  },
  {
    "text": "was cortex antennas that it's very different approaches solving the same problem in a way but catering to",
    "start": "348490",
    "end": "353620"
  },
  {
    "start": "350000",
    "end": "438000"
  },
  {
    "text": "different use cases so like if we had some canonical answer in Prometheus probably someone good would get lost",
    "start": "353620",
    "end": "359710"
  },
  {
    "text": "along the way and tennis for example is a different approach it doesn't use even more api's instead you have a sidecar",
    "start": "359710",
    "end": "365289"
  },
  {
    "text": "that you basically then use to cluster up together multiple Prometheus servers and then you have sort of horizontal is",
    "start": "365289",
    "end": "371110"
  },
  {
    "text": "capable instead to scrub you layer and then can do long-term storage based on object storage which is entirely based",
    "start": "371110",
    "end": "378009"
  },
  {
    "text": "on the new storage from what we have as of promises to so that's a very different approach and that probably works much better for on Prem and then",
    "start": "378009",
    "end": "385240"
  },
  {
    "text": "the question is like what is what is prometheus right the query language is kind of you can have those elsewhere",
    "start": "385240",
    "end": "390749"
  },
  {
    "text": "cortex has it has it and it's kind of the same system you access right if you have the same query language you don't",
    "start": "390749",
    "end": "396699"
  },
  {
    "text": "probably don't care too much more user perspective like where this data is coming from and then this is the",
    "start": "396699",
    "end": "401889"
  },
  {
    "text": "operation perspective yeah it sounds relatively simple and it's not the same but it's relatively close so if we had",
    "start": "401889",
    "end": "410080"
  },
  {
    "text": "sort of some promises built in tool if it also have some additional complexities in terms of operations so",
    "start": "410080",
    "end": "416169"
  },
  {
    "text": "whether it has a different name or not that's probably not paying too much of around the end yeah the primary goal of",
    "start": "416169",
    "end": "422409"
  },
  {
    "text": "Prometheus itself has always been to have highly available alerting and everything else coming second so that",
    "start": "422409",
    "end": "428979"
  },
  {
    "text": "already informs kind of the no clustering decision and so on so yeah",
    "start": "428979",
    "end": "434229"
  },
  {
    "text": "you have to kind of focus on something but then yeah yeah another reason that we we specifically don't have clustering",
    "start": "434229",
    "end": "441879"
  },
  {
    "start": "438000",
    "end": "530000"
  },
  {
    "text": "in Prometheus as we wanted to make the the ability to deploy it and the ability",
    "start": "441879",
    "end": "447580"
  },
  {
    "text": "to adopt it as easy as possible and if we put clustering in the Prometheus it",
    "start": "447580",
    "end": "454209"
  },
  {
    "text": "would make it much harder for new developers to adopt it and start using",
    "start": "454209",
    "end": "459399"
  },
  {
    "text": "it and just getting getting it tried out to see if it would actually fit for them so by making clustering and external",
    "start": "459399",
    "end": "467409"
  },
  {
    "text": "storage an optional feature with Thanos or cortex or whatever other database you",
    "start": "467409",
    "end": "474370"
  },
  {
    "text": "want to send your data to it allows the the users with more complex problems that probably already have a",
    "start": "474370",
    "end": "481790"
  },
  {
    "text": "better idea on how they want to do it whereas if you're just a small group and you only have one Prometheus and you",
    "start": "481790",
    "end": "488150"
  },
  {
    "text": "only really need two weeks of data it's fine or to do it that way you and even",
    "start": "488150",
    "end": "493310"
  },
  {
    "text": "Prometheus itself is totally fine storing six months or a year of data internally and that's you know at gitlab",
    "start": "493310",
    "end": "499550"
  },
  {
    "text": "we started out simple we started out with one Prometheus server and we just store six months of data in there it's",
    "start": "499550",
    "end": "505640"
  },
  {
    "text": "totally fine as we grow and as our needs grow and complexity we can add the",
    "start": "505640",
    "end": "511910"
  },
  {
    "text": "Thanos complexity afterwards and we don't have to we don't you know it allows people to not have to jump into",
    "start": "511910",
    "end": "519200"
  },
  {
    "text": "the deep end right away with clustering and crazy configurations in our main",
    "start": "519200",
    "end": "530330"
  },
  {
    "text": "instance without any magic just with proper procedures delay the oldest data we have is from end of 2015 and they're",
    "start": "530330",
    "end": "538490"
  },
  {
    "text": "still active and we've still query stuff which goes all the way back so it's actually doable you just have to take",
    "start": "538490",
    "end": "543980"
  },
  {
    "text": "care also there's more free seats up front so just come by this one here",
    "start": "543980",
    "end": "549380"
  },
  {
    "start": "545000",
    "end": "572000"
  },
  {
    "text": "there's one we don't bite next question oh okay bad timing yeah okay there was",
    "start": "549380",
    "end": "559550"
  },
  {
    "text": "one out there already yeah I think",
    "start": "559550",
    "end": "573020"
  },
  {
    "start": "572000",
    "end": "598000"
  },
  {
    "text": "that's actually a good idea now what do you think yeah don't take too much time because but yeah oh yeah so I met I",
    "start": "573020",
    "end": "579140"
  },
  {
    "text": "primarily work on the node exporter in the apocrypha library I kind of like digging around with like system calls",
    "start": "579140",
    "end": "584150"
  },
  {
    "text": "and Linux interfaces so that's my big contribution to Prometheus itself I'm",
    "start": "584150",
    "end": "589640"
  },
  {
    "text": "Tom as I mentioned I I started working on cortex which is this Prometheus compatible thing and and then",
    "start": "589640",
    "end": "595190"
  },
  {
    "text": "contributed a bunch to the remote API so Richard mainly community doing prom con",
    "start": "595190",
    "end": "601130"
  },
  {
    "start": "598000",
    "end": "665000"
  },
  {
    "text": "and stuff and also making sure that's not only clone native but also brownfield the other list is shorter and",
    "start": "601130",
    "end": "609200"
  },
  {
    "text": "so I work on a lot of stuff like I maintained Java and my compliance yes Anna px boarded a black box exporter she was",
    "start": "609200",
    "end": "616160"
  },
  {
    "text": "like too much works thankfully and here are chunks of four ETS as well oh yeah",
    "start": "616160",
    "end": "622310"
  },
  {
    "text": "you just wrote the book Metis up and running is a personal project and not",
    "start": "622310",
    "end": "627830"
  },
  {
    "text": "endorsed by the fruitiest team that's so we always have to be careful because we are as Ben mentioned a project without",
    "start": "627830",
    "end": "634399"
  },
  {
    "text": "any one company backing ish we asked we care about who we don't do and don't endorse just so we can keep ourselves",
    "start": "634399",
    "end": "639890"
  },
  {
    "text": "open even if it would be nice right business but you know III do believe in keeping trying to keep searching state",
    "start": "639890",
    "end": "646850"
  },
  {
    "text": "reasonably separate I read the book I recommend it Lomax mostly alert manager maintenance",
    "start": "646850",
    "end": "654940"
  },
  {
    "text": "and then next to that kubernetes integration with the Prometheus operator which is also not part of this project",
    "start": "654940",
    "end": "661820"
  },
  {
    "text": "but part of chorus or Red Hat yeah I'm Fredrik I also work on the",
    "start": "661820",
    "end": "668600"
  },
  {
    "start": "665000",
    "end": "698000"
  },
  {
    "text": "prometheus operator and basically everything connecting kubernetes and",
    "start": "668600",
    "end": "674180"
  },
  {
    "text": "Prometheus so I work on the coop state matrix exporter or the coronaries",
    "start": "674180",
    "end": "681020"
  },
  {
    "text": "service discovery yeah yeah I think I've",
    "start": "681020",
    "end": "686690"
  },
  {
    "text": "touched most of Prometheus atmosphere at minja started off this new time series database a while ago then worked on the",
    "start": "686690",
    "end": "693140"
  },
  {
    "text": "operator initially I guess yeah then I",
    "start": "693140",
    "end": "699800"
  },
  {
    "start": "698000",
    "end": "744000"
  },
  {
    "text": "mostly work on exporters and integrations and some community stuff around integrations so like trying to",
    "start": "699800",
    "end": "707089"
  },
  {
    "text": "encourage projects to adopt previous metrics format not for Prometheus but",
    "start": "707089",
    "end": "712310"
  },
  {
    "text": "just because it's a good way to integrate with other systems so I I tell",
    "start": "712310",
    "end": "717589"
  },
  {
    "text": "people I don't care if you implement Prometheus metrics for Prometheus because somebody could use the",
    "start": "717589",
    "end": "723320"
  },
  {
    "text": "Prometheus metrics within flux DB or any of our competitors I just we want to get",
    "start": "723320",
    "end": "728660"
  },
  {
    "text": "a good standard data format for for getting data out of apps ie work on the",
    "start": "728660",
    "end": "735140"
  },
  {
    "text": "storage engine that Fabian started the current 2.0 storage engine and also upstream Prometheus or maybe me and last",
    "start": "735140",
    "end": "744140"
  },
  {
    "start": "744000",
    "end": "773000"
  },
  {
    "text": "one so in the past you so also touch pretty much almost on everything in prometheus in some way",
    "start": "744140",
    "end": "750460"
  },
  {
    "text": "organized the last two years of prom con as well the Prometheus conference now that I'm freelancing around it I'm doing",
    "start": "750460",
    "end": "756800"
  },
  {
    "text": "more like what companies need which sometimes is upstream work but more often than not is just something",
    "start": "756800",
    "end": "762890"
  },
  {
    "text": "internal so yeah if you need any if you want to sponsor upstream development talk to me okay well next question so",
    "start": "762890",
    "end": "797120"
  },
  {
    "text": "the question is how do people manage their alerting configuration or the API is to change things or other only config",
    "start": "797120",
    "end": "802730"
  },
  {
    "text": "files so I work for gate lab and I do some of the management of get labs alerting of course I teach the",
    "start": "802730",
    "end": "810020"
  },
  {
    "text": "production engineering team how to make their own alerts but we do everything we just have a git repo and we check in our",
    "start": "810020",
    "end": "816800"
  },
  {
    "text": "changes into the git repo and it's auto deployed into our production environment so it's just check it in wait wait for",
    "start": "816800",
    "end": "822920"
  },
  {
    "text": "it to get pushed important yeah get lab comm / get lab - comm / run books so",
    "start": "822920",
    "end": "836440"
  },
  {
    "text": "yeah it's so it's under the get lab comm project and we have we actually it's nice because we we put all of our",
    "start": "836560",
    "end": "842630"
  },
  {
    "text": "recording rules and alerts in the same repo as our run book so if we change an alert we can update the the run book at",
    "start": "842630",
    "end": "849110"
  },
  {
    "text": "the same time and and it's maybe important to say there is no API in premises itself to update a file on disk",
    "start": "849110",
    "end": "855530"
  },
  {
    "text": "because we want to have one path of configuration to enter Prometheus and not like Prometheus modifies a file on",
    "start": "855530",
    "end": "861470"
  },
  {
    "text": "disk but then it might also get overwritten again by contact management system or so alright so that's why everything comes through the file and if",
    "start": "861470",
    "end": "870140"
  },
  {
    "text": "you want to make changes to that you have to build your own API that changes that file somehow",
    "start": "870140",
    "end": "876460"
  },
  {
    "text": "you yes you can send send us a cup to Prometheus and make sure that it picks",
    "start": "876830",
    "end": "882630"
  },
  {
    "start": "877000",
    "end": "927000"
  },
  {
    "text": "up the new convict and I'll put a plug in for my talk if that's all right sure yeah I'm I've been working with",
    "start": "882630",
    "end": "888240"
  },
  {
    "text": "Frederick on a new way of kind of expressing alerts to make them agnostic",
    "start": "888240",
    "end": "893670"
  },
  {
    "text": "to how you label things in your infrastructure which hopefully should make them more redistributable and collaborative and so with Frederick and",
    "start": "893670",
    "end": "900630"
  },
  {
    "text": "I have published a set of like humanities alerts and dashboards and recording rules and we're calling the mix-ins and we're talking about it",
    "start": "900630",
    "end": "907020"
  },
  {
    "text": "tomorrow at 2 o'clock is there a",
    "start": "907020",
    "end": "918360"
  },
  {
    "text": "question back there yes I updated some",
    "start": "918360",
    "end": "929430"
  },
  {
    "text": "metrics and and I forgot to update a recording rule so I had to check in a change to the run books to to fix a",
    "start": "929430",
    "end": "935070"
  },
  {
    "text": "recording rule that was broken and not not recording our-our-our quantile data",
    "start": "935070",
    "end": "940830"
  },
  {
    "text": "for rails requests do you want to talk",
    "start": "940830",
    "end": "946500"
  },
  {
    "text": "about prom Khan from Khan oh yeah we have prom Khan coming up again in August",
    "start": "946500",
    "end": "951630"
  },
  {
    "start": "948000",
    "end": "991000"
  },
  {
    "text": "it's the third installment this time organised by Richard less to use by me as main kind of person",
    "start": "951630",
    "end": "957830"
  },
  {
    "text": "it's the Prometheus conference will have maintainer there but also we want to sort of the talks from users of",
    "start": "957830",
    "end": "965250"
  },
  {
    "text": "Prometheus to share their experience how you're using Prometheus what you've learned best practices the CFP is still",
    "start": "965250",
    "end": "972570"
  },
  {
    "text": "open I think until May 14 prom condyle you'll find it there submit talks buy",
    "start": "972570",
    "end": "982170"
  },
  {
    "text": "tickets the registration is open as well at the moment and it's in Munich again which is a lovely city to visit in",
    "start": "982170",
    "end": "989160"
  },
  {
    "text": "August and just in terms of talks we aren't just interested in us lecturing",
    "start": "989160",
    "end": "996210"
  },
  {
    "start": "991000",
    "end": "1018000"
  },
  {
    "text": "everyone we would like user stories people using it in different ways like an good diversity of talks and attendees",
    "start": "996210",
    "end": "1002030"
  },
  {
    "text": "that's like last year only like eternity to talk swear by us and you know we want to keep it that way",
    "start": "1002030",
    "end": "1007640"
  },
  {
    "text": "whereas dominating is actually a community event organized by both Richey mostly if if",
    "start": "1007640",
    "end": "1019160"
  },
  {
    "start": "1018000",
    "end": "1357000"
  },
  {
    "text": "not you if you know of somebody else hood who would be interested in talking about monitoring any any other related",
    "start": "1019160",
    "end": "1025520"
  },
  {
    "text": "topics let him know we're still still looking for more speakers",
    "start": "1025520",
    "end": "1031418"
  },
  {
    "text": "[Music]",
    "start": "1047240",
    "end": "1053510"
  },
  {
    "text": "so the question is there's one Prometheus monitoring multiple kubernetes clusters hits the RAM limit",
    "start": "1084490",
    "end": "1091040"
  },
  {
    "text": "on the single machine what can be done to scale it better so as currently implemented what really matters is",
    "start": "1091040",
    "end": "1097640"
  },
  {
    "text": "incised a head block in terms of RAM and there's not much you can do with - the head block is the last two hours of time",
    "start": "1097640",
    "end": "1103880"
  },
  {
    "text": "series roughly in memory and I do have a plural hairstyle because that Ram is",
    "start": "1103880",
    "end": "1109640"
  },
  {
    "text": "small the amount of RAM from QL uses is not I've uploaded a pressed out which will massively decreases and add four",
    "start": "1109640",
    "end": "1115850"
  },
  {
    "text": "questions of kubernetes architecture will go to Fredrik okay so in terms of",
    "start": "1115850",
    "end": "1122059"
  },
  {
    "text": "being able to horizontally scale I don't think this is actually very kubernetes specific but there is a possibility to",
    "start": "1122059",
    "end": "1129309"
  },
  {
    "text": "shard Prometheus with a with the hash mod function that's like a relabeling",
    "start": "1129309",
    "end": "1135020"
  },
  {
    "text": "rule in and Prometheus and then that way you can distribute the targets",
    "start": "1135020",
    "end": "1140660"
  },
  {
    "text": "multiple instances of Prometheus if that's really what you what you need but",
    "start": "1140660",
    "end": "1146330"
  },
  {
    "text": "what you could also do in terms of because you're you said that you're scraping 15 clusters you could also",
    "start": "1146330",
    "end": "1153680"
  },
  {
    "text": "adopt Thanos for example where you then have a Prometheus server per cluster I",
    "start": "1153680",
    "end": "1159020"
  },
  {
    "text": "don't know if that actually makes sense depends a bit on how large each individual cluster is but with Thanos",
    "start": "1159020",
    "end": "1165440"
  },
  {
    "text": "you could then globally query all of those and make it look like one but decrease the sample memory overhead yeah",
    "start": "1165440",
    "end": "1174470"
  },
  {
    "text": "so part of the part of the original like where Prometheus came from it was",
    "start": "1174470",
    "end": "1180140"
  },
  {
    "text": "designed with the idea that a previous server should be small and it should be",
    "start": "1180140",
    "end": "1187280"
  },
  {
    "text": "attached to the the thing that it's monitoring so if you have 15 kubernetes",
    "start": "1187280",
    "end": "1192680"
  },
  {
    "text": "clusters you really want to have 15 Prometheus servers one for each cluster and then you can either you can use one",
    "start": "1192680",
    "end": "1199280"
  },
  {
    "text": "of any of our techniques whether it's cortex Thanos Federation to to pull in a",
    "start": "1199280",
    "end": "1205070"
  },
  {
    "text": "global view but then you you get the date you you get the data from the",
    "start": "1205070",
    "end": "1210770"
  },
  {
    "text": "Prometheus cluster because what if you delete a what if you delete a kubernetes cluster well you have to think about the",
    "start": "1210770",
    "end": "1217070"
  },
  {
    "text": "multi-tenancy problem there where you have if you want to delete that cluster if you have a Prometheus server for that",
    "start": "1217070",
    "end": "1224180"
  },
  {
    "text": "cluster you just delete the Prometheus server for that cluster and you don't have to go back and like clean up data in a big global Prometheus server so",
    "start": "1224180",
    "end": "1232240"
  },
  {
    "text": "like when forget lab when we're starting to ship Prometheus servers on a per name",
    "start": "1232240",
    "end": "1239210"
  },
  {
    "text": "space so every application gets deployed to a namespace we deploy a single Prometheus server into that namespace",
    "start": "1239210",
    "end": "1244640"
  },
  {
    "text": "just for that app and then if you delete that app it you delete the name space and then your Prometheus server is gone",
    "start": "1244640",
    "end": "1250580"
  },
  {
    "text": "and then it's and then you know there's the cleanup is super easy what if you",
    "start": "1250580",
    "end": "1255590"
  },
  {
    "text": "want to keep the data Venice yeah or Kleenex yeah yeah obviously it's",
    "start": "1255590",
    "end": "1260720"
  },
  {
    "text": "trade-offs like if you have data spread over many Prometheus servers then you cannot create craft one prom ql query",
    "start": "1260720",
    "end": "1266990"
  },
  {
    "text": "that touches all of the data but yeah the classical way of using promises",
    "start": "1266990",
    "end": "1272060"
  },
  {
    "text": "would be really like one Prometheus per cluster you know and then you have a global one that federates and it doesn't have all",
    "start": "1272060",
    "end": "1277910"
  },
  {
    "text": "the details anymore but it would have the global view just the aggregations of each cluster if you're heading run",
    "start": "1277910",
    "end": "1294020"
  },
  {
    "text": "issues then your queries and for your time series especially like the number of current concurrently active time",
    "start": "1294020",
    "end": "1300800"
  },
  {
    "text": "series that your primitives is managing scraping that's probably incurring the",
    "start": "1300800",
    "end": "1306020"
  },
  {
    "text": "most memory cost so theoretically so for example you could say you're not so interested in some of the metrics that",
    "start": "1306020",
    "end": "1312470"
  },
  {
    "text": "either you drop entire types of targets or put them in a different primitive server or as a last resort there's",
    "start": "1312470",
    "end": "1319310"
  },
  {
    "text": "metrics relabeling there's the facility in the configuration file yeah where you can say like drop all the metrics that",
    "start": "1319310",
    "end": "1325670"
  },
  {
    "text": "match a certain pattern don't even store them yeah question about TST B",
    "start": "1325670",
    "end": "1358520"
  },
  {
    "start": "1357000",
    "end": "1427000"
  },
  {
    "text": "corruption and how that affects memory limits so you actually had a case of where your TRC B itself became corrupted",
    "start": "1358520",
    "end": "1364570"
  },
  {
    "text": "oh because how because premies has crashed because of the OEM and then it",
    "start": "1364570",
    "end": "1370850"
  },
  {
    "text": "came up that shouldn't really corrupt it but probably there was a barque do you know more details about this so did you",
    "start": "1370850",
    "end": "1377180"
  },
  {
    "text": "see lock message like is to be corrupted and maybe that gets into tomb in too",
    "start": "1377180",
    "end": "1383390"
  },
  {
    "text": "much detail now let's let's let's take that one offline because it maybe goes into too much like this is a back what",
    "start": "1383390",
    "end": "1389840"
  },
  {
    "text": "kind of bug yep he was next okay okay",
    "start": "1389840",
    "end": "1396039"
  },
  {
    "text": "I guess this is kind of like a best practices question yes across six six",
    "start": "1396410",
    "end": "1407390"
  },
  {
    "text": "clusters who's curious I mean ultimately",
    "start": "1407390",
    "end": "1429080"
  },
  {
    "text": "the question oh yeah the question was how do you migrate between 1.8 and 2.0",
    "start": "1429080",
    "end": "1434240"
  },
  {
    "text": "without having a sort of big data and data loss window I think we have a migration tool the Baggett lab right",
    "start": "1434240",
    "end": "1440510"
  },
  {
    "text": "yeah but it's kind of slow ish I think yeah so so we forget forget lab we paid",
    "start": "1440510",
    "end": "1448820"
  },
  {
    "text": "you loose to develop an actual upgrade tool that will take the old time series database and create the new time series",
    "start": "1448820",
    "end": "1455480"
  },
  {
    "text": "database but of course you know it was manie's a bit of memory and it takes a",
    "start": "1455480",
    "end": "1462020"
  },
  {
    "text": "long time and currently the way it works is you have to do it all offline so during the offline window you're not",
    "start": "1462020",
    "end": "1468920"
  },
  {
    "text": "collecting any metrics so really my my answer to this is just deploy a new Prometheus server wait 30 days and throw",
    "start": "1468920",
    "end": "1475700"
  },
  {
    "text": "away the old one yeah the problem why it's slow is it's just the old time to use database to read like hundreds of",
    "start": "1475700",
    "end": "1481280"
  },
  {
    "text": "gigabytes of structured time series data just is slow so it's hard to get really",
    "start": "1481280",
    "end": "1486950"
  },
  {
    "text": "around that",
    "start": "1486950",
    "end": "1489370"
  },
  {
    "text": "the question so just last week we released a version of the Prometheus",
    "start": "1498500",
    "end": "1504210"
  },
  {
    "start": "1500000",
    "end": "1602000"
  },
  {
    "text": "operator that has all the requirements for tannaz merged so I think Donna's",
    "start": "1504210",
    "end": "1510300"
  },
  {
    "text": "needs external labels on a Prometheus server in order to discover its identity",
    "start": "1510300",
    "end": "1517440"
  },
  {
    "text": "and potentially do some optimizations for queries so that's that's down and",
    "start": "1517440",
    "end": "1523910"
  },
  {
    "text": "you can also now inject arbitrary containers next to your Prometheus",
    "start": "1523910",
    "end": "1529140"
  },
  {
    "text": "server through the Prometheus operator which means you can inject the ethanol sidecar and necessary to do this and we",
    "start": "1529140",
    "end": "1535050"
  },
  {
    "text": "actually have some example manifests in the Prometheus operator repository as well I think it's in contrib coupe",
    "start": "1535050",
    "end": "1541710"
  },
  {
    "text": "Prometheus experimental you can check that out there's a question",
    "start": "1541710",
    "end": "1552110"
  },
  {
    "text": "[Music]",
    "start": "1590780",
    "end": "1594039"
  },
  {
    "start": "1602000",
    "end": "1624000"
  },
  {
    "text": "cool okay yeah let's keep maybe two questions first so first one is water Stannis was awesome second one was are",
    "start": "1602060",
    "end": "1610260"
  },
  {
    "text": "we planning on adding more ml kind of machine learning type functions functionality to promises itself by the",
    "start": "1610260",
    "end": "1616620"
  },
  {
    "text": "way if you could use the remote API is currently but you can also query Prometheus and get it out that way right",
    "start": "1616620",
    "end": "1624259"
  },
  {
    "start": "1624000",
    "end": "1721000"
  },
  {
    "text": "so Thanos essentially tries to solve your problems that you might encounter when you first scale out prometheus",
    "start": "1625530",
    "end": "1631070"
  },
  {
    "text": "first is you may end up as much of a permissive service I know you want to ask query across the data of all of them",
    "start": "1631070",
    "end": "1636960"
  },
  {
    "text": "and what you have to do currently essentially is you have to use Federation's you have to decide a certain subset of metrics that isn't",
    "start": "1636960",
    "end": "1643650"
  },
  {
    "text": "federated in that server that then has sort of a global view but you will never be able to access all of the data at",
    "start": "1643650",
    "end": "1648930"
  },
  {
    "text": "once and so that's something dinosaurs and the other part is that if you have nature of Prometheus service and you",
    "start": "1648930",
    "end": "1655650"
  },
  {
    "text": "might at some point have gaps in one of them and then depending on which one you hit you see different data does it gap",
    "start": "1655650",
    "end": "1661620"
  },
  {
    "text": "somewhere or not and there's really no way to kuribohs and sort of give get back a unified result that just shows",
    "start": "1661620",
    "end": "1667590"
  },
  {
    "text": "you a graph without gaps and then I guess finally like long term storage so Thanos essentially tries to soften",
    "start": "1667590",
    "end": "1674160"
  },
  {
    "text": "all the simple passes you deploy a side car next to Prometheus which then will",
    "start": "1674160",
    "end": "1680400"
  },
  {
    "text": "sort of cluster together side cars and then you have a horizontally scalable stateless crew layer which essentially",
    "start": "1680400",
    "end": "1687300"
  },
  {
    "text": "just implements he promises API and prompt you and this corrida then clusters who gathers all these side cars",
    "start": "1687300",
    "end": "1693420"
  },
  {
    "text": "and sort of fans out queries to all these servers and then you have basically one entry point for liquor is that can hit all servers at once and",
    "start": "1693420",
    "end": "1700260"
  },
  {
    "text": "it's trying to be smart about it so if you're sort of label selection your career and can exclude certain",
    "start": "1700260",
    "end": "1705810"
  },
  {
    "text": "Premiership servers it will just not go to them and so it's sort of functionality routing in a way it's open",
    "start": "1705810",
    "end": "1712260"
  },
  {
    "text": "source and it also detects when to promises servers are sort of HMS are correcting the same data and never then",
    "start": "1712260",
    "end": "1718860"
  },
  {
    "text": "apply some heuristics to merge this data back together so it's durability scalability merging",
    "start": "1718860",
    "end": "1724950"
  },
  {
    "start": "1721000",
    "end": "1762000"
  },
  {
    "text": "of data between different servers and h.a pairs it solves all these problems and it integrates with your existing",
    "start": "1724950",
    "end": "1730250"
  },
  {
    "text": "Promethea setup where you might have many servers and you just want to kind of get an integrated long-term view",
    "start": "1730250",
    "end": "1735900"
  },
  {
    "text": "over everything yes maybe interesting to note that CloudFlare is deploying it in a large scale Oxford apparently testing",
    "start": "1735900",
    "end": "1742560"
  },
  {
    "text": "it very heavily sorry intercept improbable yeah good lap as well",
    "start": "1742560",
    "end": "1748110"
  },
  {
    "text": "oh good love is interested okay so it has it has interest behind it so that's that's a good sign and the good thing is",
    "start": "1748110",
    "end": "1754620"
  },
  {
    "text": "you can kind of do a gradual rollout so the simple passes you just use a sidecar and the COBE layer that's already only",
    "start": "1754620",
    "end": "1759960"
  },
  {
    "text": "so it cannot really break anything just state the stuff that you deploy essentially and then if you want to sort",
    "start": "1759960",
    "end": "1765930"
  },
  {
    "start": "1762000",
    "end": "1824000"
  },
  {
    "text": "of extent this and have long term storage you can have the sidecars update the database files as I written to disk",
    "start": "1765930",
    "end": "1771810"
  },
  {
    "text": "to an object storage which can be cooked so it as three or something and that's",
    "start": "1771810",
    "end": "1777240"
  },
  {
    "text": "so yeah whatever and that's kind of like a simple backup strategy and then if you actually want end to CRO these backups",
    "start": "1777240",
    "end": "1783630"
  },
  {
    "text": "dynamically you can deploy the fan or sort of Store gateway which can really efficiently read data back from from the",
    "start": "1783630",
    "end": "1790950"
  },
  {
    "text": "object storage files basically in real time so it does like really sort of direct and range queries against the object storage and consolidates these",
    "start": "1790950",
    "end": "1797130"
  },
  {
    "text": "range requests together and so that you basically almost get like real-time response times even though your data's",
    "start": "1797130",
    "end": "1802380"
  },
  {
    "text": "like really cheaply stored in object storage and lastly what we added as well was down sampling and simply because the",
    "start": "1802380",
    "end": "1808890"
  },
  {
    "text": "computation gets at some point really expensive if you have like really long data sort of date ranges that you're",
    "start": "1808890",
    "end": "1814500"
  },
  {
    "text": "accruing and doesn't if it was just a crispy just this query processing and so we had done something and it can",
    "start": "1814500",
    "end": "1820800"
  },
  {
    "text": "actually query like ten years of data in a reason a lot of time cool who wants to answer the second",
    "start": "1820800",
    "end": "1826590"
  },
  {
    "start": "1824000",
    "end": "1927000"
  },
  {
    "text": "question Brian maybe so the first thing we need to be careful about is the word anomaly detection because it is largely",
    "start": "1826590",
    "end": "1833250"
  },
  {
    "text": "a marketing term you're using it in the way it's actual anomaly detection some other products might not it'll just mean",
    "start": "1833250",
    "end": "1840030"
  },
  {
    "text": "oh we've an alert if the JVM is spending a lot of times in garbage collection and which is it is technically an under",
    "start": "1840030",
    "end": "1846750"
  },
  {
    "text": "normal a level so people really mean so just took that for that one so the thing is that using any form of LnL and it's",
    "start": "1846750",
    "end": "1855660"
  },
  {
    "text": "my experience on engine bar toy examples is extremely difficult and someone",
    "start": "1855660",
    "end": "1860760"
  },
  {
    "text": "actually works google and like one of machine learning team said yeah it takes 18 months pay",
    "start": "1860760",
    "end": "1865920"
  },
  {
    "text": "wait to come up with a model that you can use for something so if you spend 18 months you can come up with a model for",
    "start": "1865920",
    "end": "1872430"
  },
  {
    "text": "a single nurse and spend your 3 or 4 engineers doing that simple static stress rules work really well you know",
    "start": "1872430",
    "end": "1879900"
  },
  {
    "text": "and just that kind of trade-off was also the question of inside parameters how heavily droite to get into stats it's my personal belief that you know Holt",
    "start": "1879900",
    "end": "1886740"
  },
  {
    "text": "winters is already slightly over the line that's largely because you don't actually have hold winters you've got double exponential rather than triple so",
    "start": "1886740",
    "end": "1892260"
  },
  {
    "text": "there's a limit of how much CPU Baretta manage and how much history you need like read he's only nominally has three",
    "start": "1892260",
    "end": "1898290"
  },
  {
    "text": "months of data is that enough to do this all and we all have two CPU to keep things reliable but you can do that externally there are however I know that",
    "start": "1898290",
    "end": "1905190"
  },
  {
    "text": "fresh tracks are investigating DML stuff for Prometheus and integrating it there so they'll have some I don't know where",
    "start": "1905190",
    "end": "1911340"
  },
  {
    "text": "does commercial open source option there at some point and what I would see it as generally this is something you do outside with Prometheus and then use",
    "start": "1911340",
    "end": "1917790"
  },
  {
    "text": "that to inform thresholds and then you can feed the data back in via and read or something if you want so let me just",
    "start": "1917790",
    "end": "1928290"
  },
  {
    "start": "1927000",
    "end": "1951000"
  },
  {
    "text": "look at the time we have three more minutes so maybe one more question who was first of all yeah okay so the",
    "start": "1928290",
    "end": "1952230"
  },
  {
    "start": "1951000",
    "end": "1999000"
  },
  {
    "text": "question is is how do you manage the resource usage of these these Prometheus servers in space 1:30 I've heard her",
    "start": "1952230",
    "end": "1960240"
  },
  {
    "text": "name space Prometheus servers that's a good question we don't actually have a very good plan for this yet so it's just",
    "start": "1960240",
    "end": "1968490"
  },
  {
    "text": "basically like Prometheus the Prometheus 2.0 server model is it uses what it",
    "start": "1968490",
    "end": "1973920"
  },
  {
    "text": "needs to scrape so you can kind of like you can kind of guess based on how many",
    "start": "1973920",
    "end": "1978960"
  },
  {
    "text": "metrics like there's cut there's kind of a rule of thumb of like 8 kilobytes of memory per per metric that it's",
    "start": "1978960",
    "end": "1985290"
  },
  {
    "text": "collecting simultaneously in memory so you so you can kind of get like how much",
    "start": "1985290",
    "end": "1991590"
  },
  {
    "text": "data you're collecting informs your your scale so",
    "start": "1991590",
    "end": "1997549"
  },
  {
    "text": "yeah or just leave it unlimited and and",
    "start": "1998010",
    "end": "2003929"
  },
  {
    "text": "set a set of set of razor in kubernetes you just set a request but not a limit",
    "start": "2003929",
    "end": "2008940"
  },
  {
    "text": "because you don't want to own the you you don't you want to avoid ooming the Prometheus circles you'll corrupt your data or you or you shouldn't you won't",
    "start": "2008940",
    "end": "2016620"
  },
  {
    "text": "corrupted eya but you'll lose data yeah",
    "start": "2016620",
    "end": "2026700"
  },
  {
    "start": "2025000",
    "end": "2110000"
  },
  {
    "text": "so yeah so yeah we have a wizard a monthly meeting yeah we have a monthly",
    "start": "2026700",
    "end": "2033960"
  },
  {
    "text": "team meeting it's not currently open enough yeah it's not it's not currently open you know you know once when",
    "start": "2033960",
    "end": "2041429"
  },
  {
    "text": "someone's people start the basically like when we see somebody who's",
    "start": "2041429",
    "end": "2046740"
  },
  {
    "text": "contributing quite a lot and they seem interested we have a we have a process in our our our governance to to invite",
    "start": "2046740",
    "end": "2055858"
  },
  {
    "text": "them to become a team member and then they're invited to join the meeting I think it would be cool in general to",
    "start": "2055859",
    "end": "2061648"
  },
  {
    "text": "have everything open I think at the moment the one blocker there's that we sometimes discuss like things like sponsorship for a conference or you know",
    "start": "2061649",
    "end": "2068850"
  },
  {
    "text": "things that shouldn't details that that shouldn't leak out to the entire world I like the way prometheus kubernetes is",
    "start": "2068850",
    "end": "2075898"
  },
  {
    "text": "doing it but they also have like professional resources behind it which we can't quite yet have but I think this",
    "start": "2075899",
    "end": "2081270"
  },
  {
    "text": "is an area where we should really improve yeah I think we have to wrap it",
    "start": "2081270",
    "end": "2086580"
  },
  {
    "text": "up unfortunately it's yes 35 or do we have one more question okay you were",
    "start": "2086580",
    "end": "2092520"
  },
  {
    "text": "kind of so I said when you okay",
    "start": "2092520",
    "end": "2096440"
  },
  {
    "text": "yep yeah yeah yeah yeah can I can I",
    "start": "2103190",
    "end": "2112320"
  },
  {
    "start": "2110000",
    "end": "2239000"
  },
  {
    "text": "answer that one or do you want to okay oh well he's also doing it do you want to go first or should I go first sure",
    "start": "2112320",
    "end": "2118470"
  },
  {
    "text": "so we also question repeat the question yeah the question was basically if there's any plans or any support for",
    "start": "2118470",
    "end": "2124950"
  },
  {
    "text": "integrating with different monitoring solutions something we saw said a few",
    "start": "2124950",
    "end": "2129990"
  },
  {
    "text": "companies actually started supporting prometheus stuff this is kind of a hot political topic because if you're",
    "start": "2129990",
    "end": "2136350"
  },
  {
    "text": "actually building and selling a product and all of a sudden you're plastering a different name all across your own",
    "start": "2136350",
    "end": "2141600"
  },
  {
    "text": "product this might not come across too well so what we actually did is there's",
    "start": "2141600",
    "end": "2147180"
  },
  {
    "text": "an effort called open metrics which actually splits out this standard into its own thing to",
    "start": "2147180",
    "end": "2153090"
  },
  {
    "text": "actually get new to a vendor support it's in the planning that we also have an RFC so you can just point your vendor",
    "start": "2153090",
    "end": "2158880"
  },
  {
    "text": "at RSC 1 2 3 and please support this and once they accept a tender and don't check the RFC's they're legally obliged",
    "start": "2158880",
    "end": "2164970"
  },
  {
    "text": "to to support from uses and all the other investors and and exporters which",
    "start": "2164970",
    "end": "2170430"
  },
  {
    "text": "you might have can I just say like we have a ton of ways which in which we do integrate with other monitoring service",
    "start": "2170430",
    "end": "2176640"
  },
  {
    "text": "and in the right get getting metrics out of existing sentient systems into Prometheus getting Prometheus metrics by",
    "start": "2176640",
    "end": "2183720"
  },
  {
    "text": "writing our nog your Sun Prometheus metrics sending from easiest metrics to a different long-term storage like we",
    "start": "2183720",
    "end": "2189330"
  },
  {
    "text": "integrate pretty much in any way you can imagine because all these points integration points are open so if you",
    "start": "2189330",
    "end": "2195480"
  },
  {
    "text": "there's sections in our talks there's two integration sections one about",
    "start": "2195480",
    "end": "2200820"
  },
  {
    "text": "general exporters and other monitoring systems and so on and then there's also one about existing software that has",
    "start": "2200820",
    "end": "2207210"
  },
  {
    "text": "Prometheus metrics instrumentation so definitely have a look at that yeah",
    "start": "2207210",
    "end": "2212490"
  },
  {
    "text": "they're like for example the Nagios Eisinger there's a we already have a script called check Prometheus so that",
    "start": "2212490",
    "end": "2218640"
  },
  {
    "text": "you can just put up a check Prometheus in your Nagios or icinga install at Sound Cloud this is what we have to do",
    "start": "2218640",
    "end": "2223830"
  },
  {
    "text": "yet be getting because we didn't have an alert manager before the alert manager we used Nagios to alert on Prometheus data I think we have to really wrap it",
    "start": "2223830",
    "end": "2230490"
  },
  {
    "text": "up there also the N or P exporter to go the other way stickers get your stickers yeah we can still take",
    "start": "2230490",
    "end": "2236470"
  },
  {
    "text": "questions in the hall so thank you all for coming",
    "start": "2236470",
    "end": "2240838"
  }
]