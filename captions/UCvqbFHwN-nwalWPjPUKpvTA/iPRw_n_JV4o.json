[
  {
    "start": "0",
    "end": "47000"
  },
  {
    "text": "hi we're here from argo and techton to talk about pushing the boundaries of",
    "start": "0",
    "end": "5359"
  },
  {
    "text": "what's possible on kubernetes so who are he well i'm alex i work at",
    "start": "5359",
    "end": "12160"
  },
  {
    "text": "intuit i specialize in kubernetes and kind of oltp stuff and the lead engineer on argo",
    "start": "12160",
    "end": "18480"
  },
  {
    "text": "workflows argo events and argo labs dataflow and i like coffee and cycling",
    "start": "18480",
    "end": "24640"
  },
  {
    "text": "ideally a bike ride to a nice coffee shop is my kind of ideal sunday yeah and i'm jason",
    "start": "24640",
    "end": "31840"
  },
  {
    "text": "hall i work at red hat i've been in involved with various developer tools for nyon eight years now i helped",
    "start": "31840",
    "end": "38480"
  },
  {
    "text": "co-found the tecton project and i like pizza and sitting and ideally sitting",
    "start": "38480",
    "end": "44079"
  },
  {
    "text": "while eating pizza um and what do we do um like alex said",
    "start": "44079",
    "end": "50079"
  },
  {
    "start": "47000",
    "end": "47000"
  },
  {
    "text": "he works on argo workflows argo workflows is a general purpose workflow execution engine uh steps in argo runs",
    "start": "50079",
    "end": "57360"
  },
  {
    "text": "sequentially and tasks in argo run uh in a dag or a directed acyclic graph",
    "start": "57360",
    "end": "63039"
  },
  {
    "text": "uh it's built on kubernetes and it has a cute logo techton is also a",
    "start": "63039",
    "end": "69760"
  },
  {
    "text": "continuous delivery focused workflow engine steps run sequentially tasks run in a dag it's built on kubernetes and we also",
    "start": "69760",
    "end": "77360"
  },
  {
    "text": "have a cute logo they're friends why did we decide to build on kubernetes",
    "start": "77360",
    "end": "82720"
  },
  {
    "start": "81000",
    "end": "81000"
  },
  {
    "text": "um when building a workflow service there are two well",
    "start": "82720",
    "end": "87759"
  },
  {
    "text": "there are a lot of problems uh two of the biggest ones are node management just managing the",
    "start": "87759",
    "end": "93520"
  },
  {
    "text": "resources uh to that will be doing the work and another is workload scheduling so",
    "start": "93520",
    "end": "99840"
  },
  {
    "text": "when a user's request comes in i want to do this work putting that on one of those nodes to do the work well",
    "start": "99840",
    "end": "105759"
  },
  {
    "text": "uh if you are at kubecon and know anything about kubernetes kubernetes is very good at these and so we by building",
    "start": "105759",
    "end": "111759"
  },
  {
    "text": "on kubernetes we don't ever have to well we do have to deal with it sometimes but mainly we just get to offload that and",
    "start": "111759",
    "end": "118000"
  },
  {
    "text": "make it kubernetes problem kubernetes also has a great",
    "start": "118000",
    "end": "123200"
  },
  {
    "text": "a great resource in custom resources this lets us build flexible extensible",
    "start": "123200",
    "end": "128319"
  },
  {
    "text": "apis inside the kubernetes api server and ecosystem and we basically get our back",
    "start": "128319",
    "end": "134879"
  },
  {
    "text": "for free our back is another huge source of work if you don't have one already built for you and kubernetes",
    "start": "134879",
    "end": "140560"
  },
  {
    "text": "built that for us so we love it and then there is the long tail of just",
    "start": "140560",
    "end": "145760"
  },
  {
    "text": "community stuff that's all of you that's everyone outside that's everyone watching uh watching all of this later there's a",
    "start": "145760",
    "end": "152000"
  },
  {
    "text": "huge community around kubernetes that provide people to look out for the security of the platform and the",
    "start": "152000",
    "end": "158160"
  },
  {
    "text": "performance of the platform and observing the platform and portability across different architectures and",
    "start": "158160",
    "end": "163680"
  },
  {
    "text": "platforms um client tooling for all of these things and multi-tenancy concerns and policy enforcement and tons and tons",
    "start": "163680",
    "end": "169920"
  },
  {
    "text": "and tons more all of these things are things that if we didn't build on kubernetes we would have to build ourselves",
    "start": "169920",
    "end": "176160"
  },
  {
    "text": "and that would be a massive amount of work and largely pretty wasteful and instead we get to",
    "start": "176160",
    "end": "181440"
  },
  {
    "text": "sit back and work on features while kubernetes improves underneath us every day thanks to all of you",
    "start": "181440",
    "end": "189120"
  },
  {
    "text": "however kubernetes was not really designed for this it was more designed for",
    "start": "189440",
    "end": "195120"
  },
  {
    "text": "long-running serving workloads um things like deployments and services and ingress you know the the the usual",
    "start": "195120",
    "end": "201360"
  },
  {
    "text": "suspects these things assume long-running pods well we have relatively short-running pods and they assume long-running",
    "start": "201360",
    "end": "207519"
  },
  {
    "text": "containers and we have fairly short-running containers and they assume no control over the life cycle of the",
    "start": "207519",
    "end": "212879"
  },
  {
    "text": "containers starting and stopping they just sort of assume they run forever well we need to start and stop these things a lot",
    "start": "212879",
    "end": "218239"
  },
  {
    "text": "um and they have no convention for passing data from one pod to another pods are pretty isolated on purpose in",
    "start": "218239",
    "end": "225360"
  },
  {
    "text": "the kubernetes ecosystem uh well we need to pass data between these things a lot um so",
    "start": "225360",
    "end": "231760"
  },
  {
    "text": "these are the the sort of four main points we're going to talk about container life cycle starting and stopping especially with regards to how",
    "start": "231760",
    "end": "238080"
  },
  {
    "start": "232000",
    "end": "232000"
  },
  {
    "text": "side cars are involved container ipc inter-process communication talking",
    "start": "238080",
    "end": "243120"
  },
  {
    "text": "between containers and a pod cross-pod communication talking between pods in a workflow",
    "start": "243120",
    "end": "248959"
  },
  {
    "text": "and custom resource proliferation which turns out to be a gigantic pain",
    "start": "248959",
    "end": "254480"
  },
  {
    "text": "uh and with that i will hand it off to alex to talk about container lifestyle okay thank you jason um",
    "start": "254480",
    "end": "260320"
  },
  {
    "text": "i just asked jason if we could do something oh i've completely forgotten no it's fine okay so put your hands up",
    "start": "260320",
    "end": "265680"
  },
  {
    "text": "if you're an argo workflows user yeah come on and put your hands up if you're a techton",
    "start": "265680",
    "end": "271840"
  },
  {
    "text": "user take down user let the record reflect they had the exact same number of people",
    "start": "271840",
    "end": "277199"
  },
  {
    "text": "all right um yeah the great thing about this talk is we get to talk about things that we have",
    "start": "277199",
    "end": "282880"
  },
  {
    "text": "in common which is brilliant and one thing that we have in common is a container life cycle",
    "start": "282880",
    "end": "288720"
  },
  {
    "text": "now both argo and tecton execute processes and graphs in a",
    "start": "288720",
    "end": "294800"
  },
  {
    "start": "289000",
    "end": "289000"
  },
  {
    "text": "directed acyclic graph and that's um not something that kubernetes supports out of the box and to make things",
    "start": "294800",
    "end": "302320"
  },
  {
    "text": "more complicated we actually can do things like modifying the graphs at runtime so in the way that you specify",
    "start": "302320",
    "end": "308000"
  },
  {
    "text": "in your pod spec what containers that you want to run in that pod without our workflows and our graphs we don't",
    "start": "308000",
    "end": "314080"
  },
  {
    "text": "necessarily even know what the containers are going to be when we start it and you know we have some nice simple",
    "start": "314080",
    "end": "320000"
  },
  {
    "text": "ones but we also have some pretty big graphs that we run you know 20 30 000",
    "start": "320000",
    "end": "325280"
  },
  {
    "text": "pod graphs sometimes and this is an example from the community i i can't even make out how many there are there",
    "start": "325280",
    "end": "332960"
  },
  {
    "text": "so what does kubernetes provide out of the box for startup well it kind of provides",
    "start": "332960",
    "end": "338560"
  },
  {
    "text": "two options the first option is init containers it's pretty simple but actually does fulfill some pretty you",
    "start": "338560",
    "end": "344960"
  },
  {
    "text": "know simple and useful use cases that allows you to start a whole a group of containers and they have to run to completion",
    "start": "344960",
    "end": "350960"
  },
  {
    "text": "before you then run your main containers and the other option is effectively run",
    "start": "350960",
    "end": "356800"
  },
  {
    "text": "one container in a pods and then you use the kubernetes api to",
    "start": "356800",
    "end": "361840"
  },
  {
    "text": "order the pods execution which is expensive because pod creation isn't actually that as cheap as you",
    "start": "361840",
    "end": "368160"
  },
  {
    "text": "might imagine and then the other thing we need to do for life cycle management",
    "start": "368160",
    "end": "373600"
  },
  {
    "start": "371000",
    "end": "371000"
  },
  {
    "text": "is ordered container shutdown so what we want to do is shut down the",
    "start": "373600",
    "end": "378720"
  },
  {
    "text": "containers in a specific controlled order you know stopping container b before stopping container a",
    "start": "378720",
    "end": "384240"
  },
  {
    "text": "and when they stop when they're shut down they need to be able to do some kind of graceful termination you know",
    "start": "384240",
    "end": "389360"
  },
  {
    "text": "clean up flushing their buffers that kind of stuff and that shut down really needs to work with",
    "start": "389360",
    "end": "395360"
  },
  {
    "text": "standard kubernetes shutdown and so the way that kubernetes shuts down a pod is firstly you get a sig term to the root",
    "start": "395360",
    "end": "401199"
  },
  {
    "text": "process then you get 30 seconds configurable yes i know uh followed by a sig kill a hard shutdown so you need to",
    "start": "401199",
    "end": "407600"
  },
  {
    "text": "work nicely with sig term so the way that we do this the primary way",
    "start": "407600",
    "end": "414319"
  },
  {
    "text": "that we do this is what we have termed the command up pattern which we termed wednesday last week that's why you've",
    "start": "414319",
    "end": "420800"
  },
  {
    "text": "not heard of it but you probably know this better in tecton as the entry point rewriting",
    "start": "420800",
    "end": "426880"
  },
  {
    "text": "or in argo workflows as the emissary executor which is very heavily influenced by tecton",
    "start": "426880",
    "end": "433759"
  },
  {
    "start": "433000",
    "end": "433000"
  },
  {
    "text": "how does this work well it's pretty it's actually pretty simple it's actually described completely by this yaml on the",
    "start": "433759",
    "end": "439919"
  },
  {
    "text": "right-hand side but let me walk you through it first what we want to do is we want to replace the user's command",
    "start": "439919",
    "end": "445360"
  },
  {
    "text": "with our own command that forks the user's command as a sub-process and the way that we do that",
    "start": "445360",
    "end": "450479"
  },
  {
    "text": "is we'll have an init container and both the init container and the main container will share an empty directory volume and the",
    "start": "450479",
    "end": "457360"
  },
  {
    "text": "init container will just copy that binary onto that volume and then that's made available to us in the main",
    "start": "457360",
    "end": "462560"
  },
  {
    "text": "container so so they don't need to be baked into the image of the container",
    "start": "462560",
    "end": "469280"
  },
  {
    "start": "469000",
    "end": "469000"
  },
  {
    "text": "this is i mean this is really handy actually just loads of things that are really useful for us so it allows us to do that ordered",
    "start": "470160",
    "end": "476319"
  },
  {
    "text": "startup because the the entry point our new command can wait for some condition before the subprocess",
    "start": "476319",
    "end": "482639"
  },
  {
    "text": "is started um that we can use that as a way to signal the sub process so to send sig terms and",
    "start": "482639",
    "end": "488479"
  },
  {
    "text": "signals when we want to send them to it we can actually capture the sub processes standard in standard out and",
    "start": "488479",
    "end": "495199"
  },
  {
    "text": "exit code and you can actually do things like remap the exit code to a different exit code if you want to",
    "start": "495199",
    "end": "500560"
  },
  {
    "text": "um we can also wait for a condition before shutdown which is pretty neat for debugging so the the user's main process",
    "start": "500560",
    "end": "507199"
  },
  {
    "text": "can finish that sub process finishes and then the content is held open still running while you can connect with cubex",
    "start": "507199",
    "end": "513360"
  },
  {
    "text": "to do some debugging and the great thing is it that is very difficult with two containers in the pod",
    "start": "513360",
    "end": "518479"
  },
  {
    "text": "is they can access the same file system so you can get access to any files on the user's file system because actually",
    "start": "518479",
    "end": "523760"
  },
  {
    "text": "your process is running in the in there in their file system has a couple of caveats a couple of",
    "start": "523760",
    "end": "530240"
  },
  {
    "text": "downsides if you're going to delay the start of that sub process based on some condition",
    "start": "530240",
    "end": "537680"
  },
  {
    "text": "then that container is still going to be running and it's going to be consuming memory and it's going to be costing you money as a result of that and that'll be",
    "start": "537680",
    "end": "544000"
  },
  {
    "text": "set by your resource requests and typically this kind of commandlet doesn't need very many resources at all it could be pretty pretty skinny pretty",
    "start": "544000",
    "end": "550560"
  },
  {
    "text": "lightweight um but your main container maybe do some really heavy lifting it might have some high cpu memory gpu",
    "start": "550560",
    "end": "556320"
  },
  {
    "text": "requirements so that could be costly and you can mitigate that by working with resource requests and the",
    "start": "556320",
    "end": "562480"
  },
  {
    "text": "other thing it doesn't really allow us to do is we're still tied to pod specs so we can't dynamically add containers to the graph",
    "start": "562480",
    "end": "569360"
  },
  {
    "text": "originally our titleist title just just shut down but it's really shut down his sig term i think so how can you ask a",
    "start": "570399",
    "end": "577839"
  },
  {
    "text": "process to gracefully exit um well you need to do with a sick term and there are two ways to send that signal",
    "start": "577839",
    "end": "583360"
  },
  {
    "text": "today one is to delete the pod which is a pretty um it has the downsides that the pod is",
    "start": "583360",
    "end": "589440"
  },
  {
    "text": "deleted so you can't inspect the pod afterwards can't look at its status afterwards once it's deleted and you can't you know unless you've archived",
    "start": "589440",
    "end": "595760"
  },
  {
    "text": "the logs you lose access to the logs and the other one is to use cube ctrl exec",
    "start": "595760",
    "end": "600880"
  },
  {
    "text": "kill one but i hope some people can realize the problems with that",
    "start": "600880",
    "end": "606079"
  },
  {
    "text": "so it's i mean it sounds like a great way to kill a pod but it has three big drawbacks um most pods",
    "start": "606079",
    "end": "613440"
  },
  {
    "text": "don't have a kill binary on them they might be a scratch or distralis images because security is more important now i think",
    "start": "613440",
    "end": "619279"
  },
  {
    "text": "for many people than a year ago and so those images are much more common or it could be something like debian where the kill isn't a binary it's a built-in",
    "start": "619279",
    "end": "625839"
  },
  {
    "text": "shell built in um it doesn't necessarily work with shell scripts",
    "start": "625839",
    "end": "630880"
  },
  {
    "text": "particularly well this can be hard to kill a shell script and if it gracefully shut down and finally if you're running this non-root uh your root process won't",
    "start": "630880",
    "end": "637920"
  },
  {
    "text": "be started with pin number one so kill one uh i mean won't do anything at all in those containers i think it'll return an exit code",
    "start": "637920",
    "end": "644560"
  },
  {
    "text": "um then so here are a couple of mitigations for that one is to use um dominic so there are i",
    "start": "644560",
    "end": "650240"
  },
  {
    "text": "think there are two or three implementations of this the one i know is of is by yelp and that um provides an",
    "start": "650240",
    "end": "655279"
  },
  {
    "text": "init process which handles those sync terms correctly and that can fix your shell script forking and get those guys",
    "start": "655279",
    "end": "660399"
  },
  {
    "text": "to shut down correctly um like we did for the entry point you can have your init container um copy a",
    "start": "660399",
    "end": "667360"
  },
  {
    "text": "kill binary onto the onto the image onto that shared directory and then you can actually just invoke that directory you'll know the path to that",
    "start": "667360",
    "end": "673200"
  },
  {
    "text": "and actually you can write kill in about 20 lines of go you can write your own kill command",
    "start": "673200",
    "end": "678959"
  },
  {
    "text": "and to mitigate the the pig one issue you can use the pit off command to figure out which pit's doing and there",
    "start": "678959",
    "end": "684640"
  },
  {
    "text": "are various other ways to do that now i can't talk about this with just",
    "start": "684640",
    "end": "689760"
  },
  {
    "text": "saying if you want to find out more about this jason and christy wilson spoke about this in depth at kubecon",
    "start": "689760",
    "end": "696240"
  },
  {
    "text": "2019 which feels like a very long time ago and there's a there's a link there in the slides if you want to find out in",
    "start": "696240",
    "end": "701600"
  },
  {
    "text": "like a load more depth about it it's hard to talk about",
    "start": "701600",
    "end": "708399"
  },
  {
    "text": "writing your own controller and creating pods i think without talking a little bit about side cars",
    "start": "708399",
    "end": "713440"
  },
  {
    "text": "so a little bit of revision for anybody who doesn't know what a sidecar is a sidecar is just a container that runs",
    "start": "713440",
    "end": "719200"
  },
  {
    "text": "next to your main container provides some kind of facility it's usually some kind of cross-cutting concern",
    "start": "719200",
    "end": "724959"
  },
  {
    "text": "this example is straight out of kubernetes documentation and it shows a log collection sidecar",
    "start": "724959",
    "end": "730720"
  },
  {
    "text": "pretty standard stuff uh the problems with side cars is how we",
    "start": "730720",
    "end": "737040"
  },
  {
    "text": "don't uh control them and they can become quite unruly you know we don't know how to behave we don't know if they're going to",
    "start": "737040",
    "end": "743519"
  },
  {
    "text": "accept sig term correctly and and run it we don't even know if they have an init process they're a bit of a black box to",
    "start": "743519",
    "end": "748800"
  },
  {
    "text": "us and it gets kind of even worse when we talk about injected side cars like uh",
    "start": "748800",
    "end": "753920"
  },
  {
    "start": "751000",
    "end": "751000"
  },
  {
    "text": "those that come with istio and vols so an ejected cycar is a term",
    "start": "753920",
    "end": "759440"
  },
  {
    "text": "we use to describe a a container that's added to the pod spec after creation by a mutating web hook",
    "start": "759440",
    "end": "767120"
  },
  {
    "text": "controller and because it's added by that mutating web controller we just don't we don't have any information about it we can't intercept it or change",
    "start": "767120",
    "end": "773839"
  },
  {
    "text": "it we can't rewrite the entry point now istio does provide a quick quick quick end point",
    "start": "773839",
    "end": "779120"
  },
  {
    "text": "i think that's how you pronounce it quick quick quick but it's not really a standard and our solution today certainly for",
    "start": "779120",
    "end": "785839"
  },
  {
    "text": "workflows is if people have issue or vault running we say well disable it and that's a shame because",
    "start": "785839",
    "end": "793040"
  },
  {
    "text": "people want to use those technologies for really good reasons and we'd really love to be able to support them doing that",
    "start": "793040",
    "end": "800120"
  },
  {
    "text": "container ipc or if i've termed it cipc which again i made up last wednesday",
    "start": "800639",
    "end": "807120"
  },
  {
    "text": "it was a big deal it's a big day it's productive day of making up new terms",
    "start": "807120",
    "end": "812560"
  },
  {
    "text": "so why would we want to do container ipc and what's our use cases for it well it's about typically about sharing data",
    "start": "812560",
    "end": "818399"
  },
  {
    "text": "between two containers so for example um you want to download a file from s3 or",
    "start": "818399",
    "end": "824959"
  },
  {
    "text": "some other kind of bucket storage and make it available for that main container or you might want to do some kind of remote procedure calls between the two",
    "start": "824959",
    "end": "830959"
  },
  {
    "text": "containers or you might want to do some kind of streaming data you know get messages from kafka and provide those the main container amongst you know",
    "start": "830959",
    "end": "837760"
  },
  {
    "text": "quite a few different use cases i would say now container ipc is just it's just",
    "start": "837760",
    "end": "843440"
  },
  {
    "text": "nick's ipc and there are about five to seven to nine different ways of",
    "start": "843440",
    "end": "849360"
  },
  {
    "text": "doing nyx ipc and i'm going to talk about a couple of them that we've found that",
    "start": "849360",
    "end": "854720"
  },
  {
    "text": "work really well with uh our systems",
    "start": "854720",
    "end": "859920"
  },
  {
    "text": "the first one that both argo and tekton use is this ubiquitous shared empty volume alex",
    "start": "860880",
    "end": "866800"
  },
  {
    "text": "what how do i solve this problem well i think it's probably solved by a shared empty directory but any argo workflows users know that",
    "start": "866800",
    "end": "872880"
  },
  {
    "text": "it's not too far from the truth um so the shared entry directory allows you to contact uh communicate between two",
    "start": "872880",
    "end": "879360"
  },
  {
    "text": "processes typically using some kind of marker file so one of the two processors will write a marker file into that",
    "start": "879360",
    "end": "884720"
  },
  {
    "text": "directory and the other process will let's sit there polling for changes to that file and when that file has changed it's going to raise the contents and",
    "start": "884720",
    "end": "890720"
  },
  {
    "text": "perform some kind of option um obviously on a shared empty directory you can you can mount a fifo using mk",
    "start": "890720",
    "end": "896399"
  },
  {
    "text": "fifo um or um you know whatever your language's fifo creation api is",
    "start": "896399",
    "end": "902399"
  },
  {
    "text": "and that's quite quite fast if you want to just do read and write of um bytes but it's not particularly proven",
    "start": "902399",
    "end": "908959"
  },
  {
    "text": "if you look out there there's not too much documentation on that but the nice thing about shared empty directory",
    "start": "908959",
    "end": "914720"
  },
  {
    "text": "volumes is that they're really simple i mean just so simple they're super secure and they're really robust so they're",
    "start": "914720",
    "end": "920639"
  },
  {
    "text": "great for what i would call slow ipc where that you don't have a lot of data",
    "start": "920639",
    "end": "926560"
  },
  {
    "text": "going through and those messages don't change a lot but if you want something faster then",
    "start": "926560",
    "end": "931680"
  },
  {
    "text": "there's another really great tool in the toolkit to use and that's just http um http has these great benefits of",
    "start": "931680",
    "end": "938160"
  },
  {
    "text": "being well known and easy to implement most programming languages come with http server and http client built into",
    "start": "938160",
    "end": "944079"
  },
  {
    "text": "them so you don't have to worry about checking your dependency tree for those you know problematic security issues",
    "start": "944079",
    "end": "949680"
  },
  {
    "text": "it's just part of the of the core sdk um you need to define an api contract not particularly difficult",
    "start": "949680",
    "end": "956160"
  },
  {
    "text": "um the nice thing about uh this is it's actually relatively secure you actually don't need https between two containers",
    "start": "956160",
    "end": "962079"
  },
  {
    "text": "within a pod because they have their own network name space you can just use http and it's actually pretty fast it's",
    "start": "962079",
    "end": "967920"
  },
  {
    "text": "pretty fast um especially if you are using http keeper live so the socket you have the socket establishment cost and",
    "start": "967920",
    "end": "974240"
  },
  {
    "text": "you're using unix domain sockets as well you can get a pretty nice performance and throughput benefit from",
    "start": "974240",
    "end": "979600"
  },
  {
    "text": "using those and um well when we rehearsed this earlier on i was telling jason how java 16 now",
    "start": "979600",
    "end": "986560"
  },
  {
    "text": "supports unique domain sockets and it turns out i should have also been talking about java 17 so that sounds recently as well so yeah well yeah",
    "start": "986560",
    "end": "993279"
  },
  {
    "text": "widely supported unix domain sockets and this is great for fast ipc if you've got a lot of throughput",
    "start": "993279",
    "end": "1000319"
  },
  {
    "start": "1001000",
    "end": "1001000"
  },
  {
    "text": "now are there other ways of doing container ipc so these are some of the examples of the",
    "start": "1001600",
    "end": "1007920"
  },
  {
    "text": "kind of tps messages per second you can get with some of the other technologies out there um so things like pipes",
    "start": "1007920",
    "end": "1014079"
  },
  {
    "text": "message queues posix and csv message queues um shared memory and memory map files and if you if you just take a look",
    "start": "1014079",
    "end": "1020079"
  },
  {
    "text": "at this graph you can see that the throughput you can get from memory map files is you know what",
    "start": "1020079",
    "end": "1026880"
  },
  {
    "text": "is that of 20 times 20 times faster so there's like",
    "start": "1026880",
    "end": "1032160"
  },
  {
    "text": "it there are other ways much much faster than tcp but these are kind of unproven and in the kind of research that i've",
    "start": "1032160",
    "end": "1038640"
  },
  {
    "text": "done on this topic it has been a bit hereby dragons on the internet and i'd love to hear from anybody who has looked",
    "start": "1038640",
    "end": "1044640"
  },
  {
    "text": "at memory map files between containers to be fascinated to know what their results were back to my colleague jason",
    "start": "1044640",
    "end": "1051440"
  },
  {
    "text": "thank you um so like with containers we also need to",
    "start": "1051440",
    "end": "1056559"
  },
  {
    "text": "pass data from one pod to another pod efficiently",
    "start": "1056559",
    "end": "1061679"
  },
  {
    "start": "1060000",
    "end": "1060000"
  },
  {
    "text": "for example the canonical use cases for this are if a task does a git clone of some revision of",
    "start": "1061679",
    "end": "1068080"
  },
  {
    "text": "some repository it might need to pass the commit that was actually checked out to the next task in the pipeline or if",
    "start": "1068080",
    "end": "1074799"
  },
  {
    "text": "it built a built a container image it would need to pass the digest to the next task that signs it or scans it or",
    "start": "1074799",
    "end": "1082000"
  },
  {
    "text": "does something else with it um we also need to expose that information up to the user that's",
    "start": "1082000",
    "end": "1087280"
  },
  {
    "text": "looking through the api or the ui or cli and these are all short-lived containers",
    "start": "1087280",
    "end": "1092559"
  },
  {
    "text": "that we don't necessarily control so we don't have a lot of options for having users request http endpoints",
    "start": "1092559",
    "end": "1099440"
  },
  {
    "text": "directly on those containers to get that information and so",
    "start": "1099440",
    "end": "1104640"
  },
  {
    "start": "1103000",
    "end": "1103000"
  },
  {
    "text": "in techton at least we've found a fun little work around using uh little known technology called",
    "start": "1104640",
    "end": "1110400"
  },
  {
    "text": "termination messages um not a lot of people know about this or i don't think they do um but if you write to this magical path in",
    "start": "1110400",
    "end": "1117840"
  },
  {
    "text": "your container slash dev termination dash log by default it will magically get",
    "start": "1117840",
    "end": "1123600"
  },
  {
    "text": "collected by the kubelet and uh written up to the pod status for that container",
    "start": "1123600",
    "end": "1128640"
  },
  {
    "text": "so this is a little way to ferry information out of your pod uh out of your container through the pod up to the",
    "start": "1128640",
    "end": "1134160"
  },
  {
    "text": "api server and it's configurable with the the container's termination message path",
    "start": "1134160",
    "end": "1139679"
  },
  {
    "text": "um so yeah the more you know so the way that tekton uses this is",
    "start": "1139679",
    "end": "1146480"
  },
  {
    "text": "if a container if a step container writes to slash tecton slash results something",
    "start": "1146480",
    "end": "1152000"
  },
  {
    "text": "the injected entry point that alex talked about before will after the step is complete it will scan all of tekton",
    "start": "1152000",
    "end": "1158320"
  },
  {
    "text": "results and see if there's anything in there it will collect that information it will stuff it into a json string and",
    "start": "1158320",
    "end": "1163679"
  },
  {
    "text": "write it to its termination message path that gets collected by kubelet written",
    "start": "1163679",
    "end": "1169039"
  },
  {
    "text": "up to the api server the controller watching that pod pulls out that json and then puts it",
    "start": "1169039",
    "end": "1174960"
  },
  {
    "text": "into the task run status where it goes and this lets us pass that data on to other tasks and",
    "start": "1174960",
    "end": "1181360"
  },
  {
    "text": "show it to end users we also use this to report the actual start time for the containers so like",
    "start": "1181360",
    "end": "1186720"
  },
  {
    "text": "alex said all the containers start at once and then only the first sub process starts and then only the second step sub",
    "start": "1186720",
    "end": "1192799"
  },
  {
    "text": "process starts when that's done so we lose the actual start time of each of these containers but we",
    "start": "1192799",
    "end": "1198640"
  },
  {
    "text": "write it to the termination message path as well so there are some limits with this",
    "start": "1198640",
    "end": "1204080"
  },
  {
    "text": "though that we have uh started to hit and that's that uh the kublet will only write we'll only collect 4k of data to",
    "start": "1204080",
    "end": "1212400"
  },
  {
    "text": "pass up per container and only 12k of data across all the containers in the pod um",
    "start": "1212400",
    "end": "1218640"
  },
  {
    "text": "this is mostly enough to get the job done if we're talking about you know git commit shas and uh container image",
    "start": "1218640",
    "end": "1225840"
  },
  {
    "text": "digests and time stamps and relatively small bits of information but it will start to break down if you do",
    "start": "1225840",
    "end": "1232559"
  },
  {
    "text": "anything more crazy than that and it's really only a matter of time before people come and ask if they can do",
    "start": "1232559",
    "end": "1237600"
  },
  {
    "text": "something more crazy than that um to poorly paraphrase steve jobs 4k should be enough for anybody",
    "start": "1237600",
    "end": "1244000"
  },
  {
    "text": "uh is not actually true as it turns out and we've we've talked about compressing this data",
    "start": "1244000",
    "end": "1249600"
  },
  {
    "text": "compressing the data better than json or encoding it in something better than json but ultimately if the 4k limit is there",
    "start": "1249600",
    "end": "1255600"
  },
  {
    "text": "you're going to hit it one way or another eventually so something we started to look into is",
    "start": "1255600",
    "end": "1262480"
  },
  {
    "start": "1260000",
    "end": "1260000"
  },
  {
    "text": "um instead of writing to that termination message uh path we will have the injected entry point contact the api",
    "start": "1262480",
    "end": "1269360"
  },
  {
    "text": "server and write to a config map the config map max size is much much larger than 12k",
    "start": "1269360",
    "end": "1275600"
  },
  {
    "text": "and so for every test run that will run we'll create a config map to hold its results the entry point will write that",
    "start": "1275600",
    "end": "1282080"
  },
  {
    "text": "data to the config map and we can tightly narrowly scope the r back to that results object",
    "start": "1282080",
    "end": "1288400"
  },
  {
    "text": "to um so that the entry point is only allowed to write to it and the controller is the only one",
    "start": "1288400",
    "end": "1295120"
  },
  {
    "text": "allowed to read from it so you can't have cross-task uh contamination in the results",
    "start": "1295120",
    "end": "1300480"
  },
  {
    "text": "and that's basically exactly what argo workflows does so it's nice to have proof that that works",
    "start": "1300480",
    "end": "1308559"
  },
  {
    "text": "there are however some disadvantages if we want to use config maps we can use configmaps if we wanted to use our own",
    "start": "1308559",
    "end": "1314559"
  },
  {
    "text": "type that we define we now have to define that type and manage that type and version and upgrade and validate",
    "start": "1314559",
    "end": "1321120"
  },
  {
    "text": "that type and the sort of bigger concern is the additional load on the api server so",
    "start": "1321120",
    "end": "1327600"
  },
  {
    "text": "instead of just writing to the pod that we already use and update all the time we're also",
    "start": "1327600",
    "end": "1333039"
  },
  {
    "text": "making frequent rights to this uh to this config mapper another custom resource and we have to create our back",
    "start": "1333039",
    "end": "1339200"
  },
  {
    "text": "for that on every new task and every new execution and manage it and delete things when they're done so they don't",
    "start": "1339200",
    "end": "1344240"
  },
  {
    "text": "leak and it's it can get um it can get difficult",
    "start": "1344240",
    "end": "1349840"
  },
  {
    "text": "that also leads me to my next uh issue that we have started to hit which is custom resource proliferation",
    "start": "1349840",
    "end": "1357600"
  },
  {
    "start": "1357000",
    "end": "1357000"
  },
  {
    "text": "as i said before custom resources are great tecton wouldn't exist without it argo wouldn't exist without it plenty of",
    "start": "1357679",
    "end": "1363919"
  },
  {
    "text": "other things in the ecosystem would not exist if kubernetes didn't provide an extensible api server but fundamentally they're not",
    "start": "1363919",
    "end": "1371679"
  },
  {
    "text": "magic at the end of the day crds are just writing to etcd and ncd while also",
    "start": "1371679",
    "end": "1377440"
  },
  {
    "text": "really great is not magic it's not the key to unlocking uh free infinite scalable storage",
    "start": "1377440",
    "end": "1383919"
  },
  {
    "text": "um and if you try like some people do you will hit limits and when you hit those limits uh you",
    "start": "1383919",
    "end": "1390240"
  },
  {
    "text": "will experience pain you will start to experience pain in a few dimensions uh one way you can mess",
    "start": "1390240",
    "end": "1396320"
  },
  {
    "text": "up etcd is to write too many bytes like i said it's not an infinite storage you will eventually hit some limit and scd",
    "start": "1396320",
    "end": "1402480"
  },
  {
    "text": "will start to fall over if you create a lot of tiny objects too many tiny objects however many bytes",
    "start": "1402480",
    "end": "1408880"
  },
  {
    "text": "total it is doesn't matter too many objects at cd will start to fall over and if you're just constantly writing",
    "start": "1408880",
    "end": "1414720"
  },
  {
    "text": "requests to etcd through the api server and constantly updating um ncd won't like it and will fall over",
    "start": "1414720",
    "end": "1421600"
  },
  {
    "text": "destabilizing scd is really really really bad uh the cluster just sort of",
    "start": "1421600",
    "end": "1426720"
  },
  {
    "text": "starts to act funny and things don't work and requests start to time out and what's up",
    "start": "1426720",
    "end": "1432720"
  },
  {
    "text": "yeah pedro's pagers go off and you get angry calls from sres and um uh",
    "start": "1432720",
    "end": "1438559"
  },
  {
    "text": "the worst thing is that you can't debug it because you're using the system that's destabilized to debug it and so",
    "start": "1438559",
    "end": "1444480"
  },
  {
    "text": "everything just sort of turns to mush underneath you and it's it is awful",
    "start": "1444480",
    "end": "1449679"
  },
  {
    "start": "1449000",
    "end": "1449000"
  },
  {
    "text": "um we have some we have discovered some mitigations for this um one really easy one is don't use jobs when you really",
    "start": "1449679",
    "end": "1456640"
  },
  {
    "text": "just want pods if you create a job it will just create a pod for you and",
    "start": "1456640",
    "end": "1462159"
  },
  {
    "text": "now you've created double the resources and double the qps because when the pod updates it'll update the job and then you read the job",
    "start": "1462159",
    "end": "1468320"
  },
  {
    "text": "um so that was an easy one that's like 50 off right there um avoid unnecessary updates to your",
    "start": "1468320",
    "end": "1475440"
  },
  {
    "text": "objects if you can in your reconcile loop instead of making you know 10 requests to update the status of something",
    "start": "1475440",
    "end": "1482400"
  },
  {
    "text": "uh batch those until the end and make one update at the end avoid duplicating the same information",
    "start": "1482400",
    "end": "1488799"
  },
  {
    "text": "across a bunch of objects so tekton actually doesn't do this well today when a task run",
    "start": "1488799",
    "end": "1495520"
  },
  {
    "text": "a test run status is actually copied and aggregated into the pipeline run status",
    "start": "1495520",
    "end": "1501039"
  },
  {
    "text": "for ease of the user but that means that we have to make two updates every time anything changes",
    "start": "1501039",
    "end": "1506480"
  },
  {
    "text": "um also avoid these monolithic mega objects like the pipeline run",
    "start": "1506480",
    "end": "1512080"
  },
  {
    "text": "because you will update them more often and they start to hit those size limits that we talked about",
    "start": "1512080",
    "end": "1518640"
  },
  {
    "text": "uh at the same time avoid lots and lots of little objects because you will also",
    "start": "1518640",
    "end": "1523760"
  },
  {
    "text": "end up making a bunch of qps to the api server and you'll end up with",
    "start": "1523760",
    "end": "1528880"
  },
  {
    "text": "maybe too many objects for ncd to be happy argo actually has a really interesting",
    "start": "1528880",
    "end": "1534960"
  },
  {
    "text": "feature that i didn't know about until earlier until we were working on this talk which is that if the status of an argo",
    "start": "1534960",
    "end": "1541679"
  },
  {
    "text": "object gets too big it will the controller will offload it to another database and just",
    "start": "1541679",
    "end": "1547360"
  },
  {
    "text": "give it a pointer to that give you so instead of your status you just say like go chase this pointer to the real",
    "start": "1547360",
    "end": "1552559"
  },
  {
    "text": "database to go uh get that information so that's really interesting",
    "start": "1552559",
    "end": "1557919"
  },
  {
    "text": "other mitigations we've had for for custom resource proliferation are just enforce reform",
    "start": "1557919",
    "end": "1563279"
  },
  {
    "text": "resource quota uh in a name space you can say this namespace is not allowed to have more than a thousand task runs ever",
    "start": "1563279",
    "end": "1569440"
  },
  {
    "text": "and if you try to create a thousand in first it will fail um you might also want to prune old",
    "start": "1569440",
    "end": "1575440"
  },
  {
    "text": "resources we do this uh in tecton a lot today but the question there is always like do you want to prune by age do you",
    "start": "1575440",
    "end": "1582320"
  },
  {
    "text": "want to say something i only want to keep the last week of history or do you want to prune by number of resources i",
    "start": "1582320",
    "end": "1588080"
  },
  {
    "text": "only want to keep the last 10 000 requests whether or not uh however old they are but fundamentally users don't",
    "start": "1588080",
    "end": "1594880"
  },
  {
    "text": "want to lose this data especially if it's security sensitive like what did we deploy three months ago",
    "start": "1594880",
    "end": "1602400"
  },
  {
    "text": "well sorry we needed that space so we deleted all record of that deployment ever happening is not a good answer for",
    "start": "1602400",
    "end": "1607760"
  },
  {
    "text": "users so um techton and argo have also solved this in a similar way or planning to",
    "start": "1607760",
    "end": "1613120"
  },
  {
    "text": "solve it in a similar way um in tecton we have a techton results project and in argo they have argo workflow archive",
    "start": "1613120",
    "end": "1619840"
  },
  {
    "text": "which effectively runs another controller to watch for these executions",
    "start": "1619840",
    "end": "1625039"
  },
  {
    "text": "when they finish it copies that data to another uh",
    "start": "1625039",
    "end": "1630159"
  },
  {
    "text": "relational database and then prunes that object from the kubernetes api server",
    "start": "1630159",
    "end": "1635440"
  },
  {
    "text": "this also gives us an opportunity to have better indexing and searching and you can search for",
    "start": "1635440",
    "end": "1640640"
  },
  {
    "text": "failed test runs that took more than 30 minutes in the last 20 days which is not something as far as i know you can do",
    "start": "1640640",
    "end": "1646559"
  },
  {
    "text": "with the kubernetes field selector today um but unfortunately this means like we",
    "start": "1646559",
    "end": "1652080"
  },
  {
    "text": "lose all of the some of the nice ecosystem stuff of like kube control doesn't work and uh",
    "start": "1652080",
    "end": "1659120"
  },
  {
    "text": "all of these things need to be sort of custom built to support that",
    "start": "1659120",
    "end": "1664320"
  },
  {
    "text": "so are we gonna do this okay we're gonna do it we're gonna okay what so what what",
    "start": "1667039",
    "end": "1673679"
  },
  {
    "text": "can we do about this well we could do nothing which is my first option i always",
    "start": "1673679",
    "end": "1679679"
  },
  {
    "start": "1675000",
    "end": "1675000"
  },
  {
    "text": "present everybody in my organization what what does doing nothing mean keep going around it yeah we just keep working",
    "start": "1679679",
    "end": "1685679"
  },
  {
    "text": "around it um it's kind of value-added for both tecton and argo you know because kubernetes doesn't support this out of the box we get to add that value",
    "start": "1685679",
    "end": "1691760"
  },
  {
    "text": "to it and if it was supported we wouldn't get to that what we could do",
    "start": "1691760",
    "end": "1698480"
  },
  {
    "text": "that was the thing we enjoyed that you didn't okay we won't do it again",
    "start": "1701760",
    "end": "1707039"
  },
  {
    "text": "we'll never do that again um so what it would be great to kind of add some additional features to",
    "start": "1707039",
    "end": "1713120"
  },
  {
    "text": "kubernetes so one of the things would be an api to start and stop containers that would be fantastic like a container sub",
    "start": "1713120",
    "end": "1718880"
  },
  {
    "text": "resource or something on those sort of lines where you can say stop the container start the container don't start this container just yet",
    "start": "1718880",
    "end": "1726399"
  },
  {
    "text": "it would be nice to be able to declare the daggers a dependency tree with inside the container spec so say",
    "start": "1726399",
    "end": "1732320"
  },
  {
    "text": "container two is dependent on container one we could have that that would be pretty neat um we thought a bit about standardizing",
    "start": "1732320",
    "end": "1738720"
  },
  {
    "text": "the commandlet pattern so maybe providing a library that people could use that is kind of well tested and",
    "start": "1738720",
    "end": "1744000"
  },
  {
    "text": "robust that you could just use that and that commandlet would expose some kind of api that people could use that would",
    "start": "1744000",
    "end": "1749919"
  },
  {
    "text": "allow you to use kubectl exec or curl to just invoke commands on it and it would deal with it for you um and for resource",
    "start": "1749919",
    "end": "1757760"
  },
  {
    "text": "quota so people use resource coders to limit the number of um custom resources in a name space typically used for",
    "start": "1757760",
    "end": "1764240"
  },
  {
    "text": "limiting number of pods but you can use it for customer resources but it'd be nice if in that we could specify",
    "start": "1764240",
    "end": "1770559"
  },
  {
    "text": "what to do when there are too many you know um some kind of strategy saying for example delete the oldest one of these",
    "start": "1770559",
    "end": "1776080"
  },
  {
    "text": "custom resources and kind of clean up afterwards i think that would be pretty neat um i guess we freestyle a lot of",
    "start": "1776080",
    "end": "1783360"
  },
  {
    "text": "these ideas don't we yeah yeah so i think it's kind of like an interesting space to kind of think",
    "start": "1783360",
    "end": "1788880"
  },
  {
    "text": "around in it um yeah so with that in mind",
    "start": "1788880",
    "end": "1795600"
  },
  {
    "text": "do we have any questions from the audience",
    "start": "1795600",
    "end": "1800600"
  },
  {
    "text": "so the question was given the a common use case for tecton at least is clone",
    "start": "1820720",
    "end": "1826320"
  },
  {
    "text": "some source from a repo uh and then build it run tests do other stuff scan",
    "start": "1826320",
    "end": "1833039"
  },
  {
    "text": "it do you know let's say five other things uh in parallel um currently today we",
    "start": "1833039",
    "end": "1839679"
  },
  {
    "text": "uh we would ask you to make a pvc to have one task right uh write that data to a",
    "start": "1839679",
    "end": "1845440"
  },
  {
    "text": "pvc and then share that pvc read only to the other ones um there's some and obviously there are downsides to that of",
    "start": "1845440",
    "end": "1851840"
  },
  {
    "text": "now you have this pvc you have to clean up or at least have a round um and it can limit your schedule ability",
    "start": "1851840",
    "end": "1859440"
  },
  {
    "text": "of those things um there's some interesting work going on in being able to run all of a pipeline in",
    "start": "1859440",
    "end": "1866320"
  },
  {
    "text": "one pod so effectively that would you would have one big pod that does all the resources of all of those things and",
    "start": "1866320",
    "end": "1872960"
  },
  {
    "text": "they would run in one shared on the same node um but uh in that way you wouldn't have to",
    "start": "1872960",
    "end": "1879360"
  },
  {
    "text": "write the persistent data outside of the execution of that pod there's um",
    "start": "1879360",
    "end": "1884640"
  },
  {
    "text": "if you find me afterwards i will uh send you a link to the actual uh proposal for that",
    "start": "1884640",
    "end": "1889919"
  },
  {
    "text": "but i think that's an exciting sort of area of frontier of uh solving that",
    "start": "1889919",
    "end": "1895200"
  },
  {
    "text": "problem",
    "start": "1895200",
    "end": "1898200"
  },
  {
    "text": "yeah so so you can also do the sorry the follow-up was um uh instead of using pvcs could you write to some external",
    "start": "1908000",
    "end": "1914480"
  },
  {
    "text": "object store s3 gcs uh whatever um absolutely that's that's absolutely an",
    "start": "1914480",
    "end": "1919600"
  },
  {
    "text": "option um it has more or less all the same problems as pbcs you have to write this",
    "start": "1919600",
    "end": "1925200"
  },
  {
    "text": "data somewhere and it costs money while it sits there and costs money to delete it and it costs money or you know it's management",
    "start": "1925200",
    "end": "1931279"
  },
  {
    "text": "overhead um i really think the the fundamental problem is you don't want that data to exist longer than it's",
    "start": "1931279",
    "end": "1938480"
  },
  {
    "text": "being operated on and in that case if it just was isolated to the pod it would you know disappear when the pop",
    "start": "1938480",
    "end": "1943919"
  },
  {
    "text": "disappears so the code of this is that we actually do something like this in argo you can share data between the",
    "start": "1943919",
    "end": "1950000"
  },
  {
    "text": "steps in your in your workflow by using s3 um and it's sometimes cheaper than",
    "start": "1950000",
    "end": "1955519"
  },
  {
    "text": "pvcs and sometimes faster and sometimes more expensive kind of depends on what your use case is okay next question",
    "start": "1955519",
    "end": "1963840"
  },
  {
    "text": "um",
    "start": "1963840",
    "end": "1966840"
  },
  {
    "text": "okay so the question was did you look at kubernetes events as a way to",
    "start": "1975360",
    "end": "1980559"
  },
  {
    "text": "message between pods i'm going to say no from argos size i don't i think we",
    "start": "1980559",
    "end": "1986799"
  },
  {
    "text": "did i mean it has all of the same api server scalability problems right like if that's if if that's your message bus if",
    "start": "1986799",
    "end": "1992799"
  },
  {
    "text": "the way you communicate between containers in a pod or pods in a workflow um",
    "start": "1992799",
    "end": "1999279"
  },
  {
    "text": "it's sort of uh you might be able to get by with it but it's going to going to be plagued by all the",
    "start": "1999279",
    "end": "2004880"
  },
  {
    "text": "problems we've described here which is it's not built for this it's not designed for that scale um",
    "start": "2004880",
    "end": "2009919"
  },
  {
    "text": "so maybe like by all means we can experiment and see what what falls over but i don't know if uh i don't know if",
    "start": "2009919",
    "end": "2016240"
  },
  {
    "text": "it's better than what we have today okay any questions from that from the middle from the back to the middle",
    "start": "2016240",
    "end": "2023720"
  },
  {
    "text": "um um",
    "start": "2028320",
    "end": "2035000"
  },
  {
    "text": "so the question was um with vault and istio is",
    "start": "2051119",
    "end": "2056960"
  },
  {
    "text": "is there an alternative a vault by using the csi injector ahead of time yeah yeah",
    "start": "2056960",
    "end": "2062480"
  },
  {
    "text": "yeah you could you could do that it's the fact that we they use a new setting web hook that that makes it difficult to work with",
    "start": "2062480",
    "end": "2068398"
  },
  {
    "text": "and specifically to inject new containers that we don't know about yeah like if they were injecting other stuff you know whatever i don't care volumes",
    "start": "2068399",
    "end": "2075118"
  },
  {
    "text": "we find yeah",
    "start": "2075119",
    "end": "2077760"
  },
  {
    "text": "that you know nodes um to cut down and these pods that are you know a piece of a dag you know the subsequent jobs waiting for",
    "start": "2087040",
    "end": "2093599"
  },
  {
    "text": "that termination what happens when those just get caught",
    "start": "2093599",
    "end": "2099359"
  },
  {
    "text": "in the head just in an air",
    "start": "2099359",
    "end": "2102960"
  },
  {
    "text": "i'll take this one sure so the question the question was i know you'll understand why i'm laughing in a second is um when you're running a dag and you",
    "start": "2108880",
    "end": "2116160"
  },
  {
    "text": "have a container and it get um i believe you said shot in the head",
    "start": "2116160",
    "end": "2121920"
  },
  {
    "text": "termination kubernetes might want the resources and i feel like the one contractual promise",
    "start": "2122560",
    "end": "2128800"
  },
  {
    "text": "kubernetes gives you is i will kill your pod at some point i'm going to kill your pod",
    "start": "2128800",
    "end": "2134160"
  },
  {
    "text": "when you don't want it and it's a really hard problem to solve um because we're trying to run really reliable robust",
    "start": "2134160",
    "end": "2140640"
  },
  {
    "text": "workloads on kubernetes and those two things are a diametric opposition yeah they they are fighting against one",
    "start": "2140640",
    "end": "2146079"
  },
  {
    "text": "another and there are kind of mitigating actions you can take like a pod disruption budget for example",
    "start": "2146079",
    "end": "2151359"
  },
  {
    "text": "um in workflows the main thing we do is allow you just retry those steps automatically and things like retry them",
    "start": "2151359",
    "end": "2157440"
  },
  {
    "text": "on a different node within your cluster chain change where it gets through one that's the main thing that we do um and the",
    "start": "2157440",
    "end": "2164079"
  },
  {
    "text": "good thing to do and i don't know if the case with tekton is is is don't have very long running processes",
    "start": "2164079",
    "end": "2171520"
  },
  {
    "text": "which take an hour to run cost an absolute fortune and if they get 55 minutes into that",
    "start": "2171520",
    "end": "2176640"
  },
  {
    "text": "being terminated gracefully you have to do all that work again because that can happen over again it's",
    "start": "2176640",
    "end": "2182640"
  },
  {
    "text": "better to have some kind of memorization going on in the process",
    "start": "2182640",
    "end": "2187119"
  },
  {
    "text": "so the question is do pre-stock books help we don't use them but yes i think they can yes yeah i mean in fundamentals so so this points",
    "start": "2188560",
    "end": "2194960"
  },
  {
    "text": "to another like way in which kubernetes was not designed for this kubernetes is designed for replicated serving",
    "start": "2194960",
    "end": "2200480"
  },
  {
    "text": "workloads where if somebody unplugs a node it should be fine right we very much need that pod to finish for",
    "start": "2200480",
    "end": "2206960"
  },
  {
    "text": "the next one to start and things to work so sort of a difference of assumptions between what",
    "start": "2206960",
    "end": "2212400"
  },
  {
    "text": "kubernetes assumes and what we assume um yeah do we have",
    "start": "2212400",
    "end": "2217440"
  },
  {
    "text": "she's giving a thumbs up we're good we're good we're good",
    "start": "2217440",
    "end": "2223040"
  },
  {
    "text": "okay any more questions all right so just one more thing if you want to",
    "start": "2223040",
    "end": "2230320"
  },
  {
    "text": "find out more um oh has the size gone a bit strange on this yes never mind",
    "start": "2230320",
    "end": "2236240"
  },
  {
    "text": "i don't know why that text is so small you can i'm at the intuit booth today we're in zone stage i'll be on the booth",
    "start": "2236240",
    "end": "2241839"
  },
  {
    "text": "from 3 30 if you want to talk about argo kind of related stuff or you can obviously go to the argo booth to speak",
    "start": "2241839",
    "end": "2247119"
  },
  {
    "text": "to some of the engineers there as well um will you be on the red hat booth i'll be around here cool cool",
    "start": "2247119",
    "end": "2253520"
  },
  {
    "text": "so that's where you can find that one thank you thank you very much thank you",
    "start": "2253520",
    "end": "2259480"
  }
]