[
  {
    "text": "right hope everyone is enjoying their",
    "start": "160",
    "end": "1439"
  },
  {
    "text": "day so far here in Chicago um yeah so",
    "start": "1439",
    "end": "4759"
  },
  {
    "text": "again my name is Ryan Drew um and we are",
    "start": "4759",
    "end": "7040"
  },
  {
    "text": "indeed talking about KVs for today and",
    "start": "7040",
    "end": "9080"
  },
  {
    "text": "scalability um before we get started I",
    "start": "9080",
    "end": "11320"
  },
  {
    "text": "do want to give a huge shout out to",
    "start": "11320",
    "end": "12719"
  },
  {
    "text": "these folks here um the project I'm",
    "start": "12719",
    "end": "15160"
  },
  {
    "text": "going to talk about in these slides was",
    "start": "15160",
    "end": "16480"
  },
  {
    "text": "a large group effort that we all worked",
    "start": "16480",
    "end": "18039"
  },
  {
    "text": "on I'm just the one who's presenting on",
    "start": "18039",
    "end": "20160"
  },
  {
    "text": "it uh so I want to give recognition",
    "start": "20160",
    "end": "21640"
  },
  {
    "text": "where it's",
    "start": "21640",
    "end": "22439"
  },
  {
    "text": "to um but just to break the ice a little",
    "start": "22439",
    "end": "24720"
  },
  {
    "text": "bit uh you can find me on GitHub and on",
    "start": "24720",
    "end": "27320"
  },
  {
    "text": "the community slack at learn itall I've",
    "start": "27320",
    "end": "29480"
  },
  {
    "text": "been up scale engineer at is veent for",
    "start": "29480",
    "end": "32279"
  },
  {
    "text": "just over a year and a half now and I'm",
    "start": "32279",
    "end": "34079"
  },
  {
    "text": "based out den Colorado um and this is a",
    "start": "34079",
    "end": "36559"
  },
  {
    "text": "picture of my cat just for fun um so",
    "start": "36559",
    "end": "40640"
  },
  {
    "text": "let's just dig into some background",
    "start": "40640",
    "end": "41800"
  },
  {
    "text": "information here so I'm going to assume",
    "start": "41800",
    "end": "43640"
  },
  {
    "text": "that we all know what celium is so I'm",
    "start": "43640",
    "end": "45160"
  },
  {
    "text": "just going to talk about cluster mesh um",
    "start": "45160",
    "end": "47960"
  },
  {
    "text": "but cluster mesh essentially allows you",
    "start": "47960",
    "end": "49920"
  },
  {
    "text": "to have Intelligent cross-cluster",
    "start": "49920",
    "end": "52280"
  },
  {
    "text": "Communication so if you have multiple",
    "start": "52280",
    "end": "54359"
  },
  {
    "text": "clusters connected to each other um or",
    "start": "54359",
    "end": "56520"
  },
  {
    "text": "multiple clusters set up in your",
    "start": "56520",
    "end": "57600"
  },
  {
    "text": "environment that are trying to talk to",
    "start": "57600",
    "end": "58800"
  },
  {
    "text": "each other without out cluster mes um",
    "start": "58800",
    "end": "61800"
  },
  {
    "text": "those clusters are going to view each",
    "start": "61800",
    "end": "62960"
  },
  {
    "text": "other as external um or World identity",
    "start": "62960",
    "end": "66840"
  },
  {
    "text": "when you enable cluster mesh these",
    "start": "66840",
    "end": "68560"
  },
  {
    "text": "clusters become aware of each other and",
    "start": "68560",
    "end": "70159"
  },
  {
    "text": "so you can do cool things like have a",
    "start": "70159",
    "end": "72320"
  },
  {
    "text": "load balancer Route traffic to a back",
    "start": "72320",
    "end": "74439"
  },
  {
    "text": "end and another cluster or do a really",
    "start": "74439",
    "end": "77759"
  },
  {
    "text": "intelligent policy where um a policy can",
    "start": "77759",
    "end": "81159"
  },
  {
    "text": "reference labels that are applied to",
    "start": "81159",
    "end": "82799"
  },
  {
    "text": "pods in a separate",
    "start": "82799",
    "end": "84360"
  },
  {
    "text": "cluster but one of the biggest things",
    "start": "84360",
    "end": "86560"
  },
  {
    "text": "about cluster mesh is the aspect of",
    "start": "86560",
    "end": "88840"
  },
  {
    "text": "scalability because cluster mesh allows",
    "start": "88840",
    "end": "91520"
  },
  {
    "text": "you to scale your environment Beyond",
    "start": "91520",
    "end": "93759"
  },
  {
    "text": "single cluster um max scale",
    "start": "93759",
    "end": "96119"
  },
  {
    "text": "recommendations right so if we look at",
    "start": "96119",
    "end": "97880"
  },
  {
    "text": "the scale recommendations that are",
    "start": "97880",
    "end": "99079"
  },
  {
    "text": "provided by kubernetes we see 150,000",
    "start": "99079",
    "end": "101680"
  },
  {
    "text": "total pods as well as 5,000 nodes per",
    "start": "101680",
    "end": "103960"
  },
  {
    "text": "cluster as a maximum but cluster mesh",
    "start": "103960",
    "end": "106439"
  },
  {
    "text": "can allow you to connect up to 255",
    "start": "106439",
    "end": "108079"
  },
  {
    "text": "clusters so theoretically you could have",
    "start": "108079",
    "end": "111439"
  },
  {
    "text": "38 million pods and 1.2 million nodes in",
    "start": "111439",
    "end": "114079"
  },
  {
    "text": "your environment all connected together",
    "start": "114079",
    "end": "116520"
  },
  {
    "text": "um and again this is very theoretically",
    "start": "116520",
    "end": "119159"
  },
  {
    "text": "and we'll talk about why here in a",
    "start": "119159",
    "end": "121079"
  },
  {
    "text": "second um so KV store mesh is a feature",
    "start": "121079",
    "end": "124960"
  },
  {
    "text": "as a part of cluster mesh that's beta in",
    "start": "124960",
    "end": "127200"
  },
  {
    "text": "114 um and it's deployed alongside",
    "start": "127200",
    "end": "130039"
  },
  {
    "text": "What's called the cluster mesh API",
    "start": "130039",
    "end": "131400"
  },
  {
    "text": "server and the cluster mesh API server",
    "start": "131400",
    "end": "133280"
  },
  {
    "text": "is the main component deployed with",
    "start": "133280",
    "end": "135360"
  },
  {
    "text": "cluster mesh that allows you to enable",
    "start": "135360",
    "end": "137160"
  },
  {
    "text": "that cross cross cluster functionality",
    "start": "137160",
    "end": "140080"
  },
  {
    "text": "um there is also KV store but this has",
    "start": "140080",
    "end": "142640"
  },
  {
    "text": "nothing to do with KV store mesh um so",
    "start": "142640",
    "end": "145480"
  },
  {
    "text": "this is a little confusing but we'll",
    "start": "145480",
    "end": "147280"
  },
  {
    "text": "we'll bear with",
    "start": "147280",
    "end": "148760"
  },
  {
    "text": "it so in this talk we're going to cover",
    "start": "148760",
    "end": "151879"
  },
  {
    "text": "the cluster Mish API server propagation",
    "start": "151879",
    "end": "154280"
  },
  {
    "text": "latency problem we'll talk about some",
    "start": "154280",
    "end": "156400"
  },
  {
    "text": "testing that we did in order to explore",
    "start": "156400",
    "end": "158160"
  },
  {
    "text": "this problem and we'll talk about how KV",
    "start": "158160",
    "end": "160480"
  },
  {
    "text": "storsh allows for higher scale and so",
    "start": "160480",
    "end": "162440"
  },
  {
    "text": "our big thesis statement for the day is",
    "start": "162440",
    "end": "164720"
  },
  {
    "text": "that KV stsh allows for higher scale",
    "start": "164720",
    "end": "166519"
  },
  {
    "text": "cluster mesh deployments by reducing the",
    "start": "166519",
    "end": "168720"
  },
  {
    "text": "load on each cluster's cluster mesh ETD",
    "start": "168720",
    "end": "171680"
  },
  {
    "text": "instance oh and if you can tell me how",
    "start": "171680",
    "end": "174480"
  },
  {
    "text": "many times I said the word cluster in",
    "start": "174480",
    "end": "176440"
  },
  {
    "text": "this talk I'll give you like a keychain",
    "start": "176440",
    "end": "178760"
  },
  {
    "text": "it's going to be a lot",
    "start": "178760",
    "end": "181760"
  },
  {
    "text": "um so let's get into this latency",
    "start": "181840",
    "end": "183200"
  },
  {
    "text": "problem here so if we look at how",
    "start": "183200",
    "end": "185000"
  },
  {
    "text": "cluster mesh API server Works under the",
    "start": "185000",
    "end": "186799"
  },
  {
    "text": "hood um you know it looks something like",
    "start": "186799",
    "end": "188959"
  },
  {
    "text": "this so in order for agents to be",
    "start": "188959",
    "end": "191080"
  },
  {
    "text": "able to intelligently talk to each other",
    "start": "191080",
    "end": "193680"
  },
  {
    "text": "across cluster there are four resources",
    "start": "193680",
    "end": "195959"
  },
  {
    "text": "that need to be synced across these",
    "start": "195959",
    "end": "197400"
  },
  {
    "text": "clusters Services celium nodes cium",
    "start": "197400",
    "end": "200319"
  },
  {
    "text": "identities and celium endpoints so the",
    "start": "200319",
    "end": "203519"
  },
  {
    "text": "agent and the operator inside of each",
    "start": "203519",
    "end": "205200"
  },
  {
    "text": "cluster are going to modify these",
    "start": "205200",
    "end": "206599"
  },
  {
    "text": "resources in the kuties kuties API",
    "start": "206599",
    "end": "209159"
  },
  {
    "text": "server and the custom API server is",
    "start": "209159",
    "end": "211360"
  },
  {
    "text": "going to watch for these resources and",
    "start": "211360",
    "end": "213040"
  },
  {
    "text": "essentially clone them into its own ETD",
    "start": "213040",
    "end": "215560"
  },
  {
    "text": "instance this then is going to be um",
    "start": "215560",
    "end": "218519"
  },
  {
    "text": "made available to remote CM agents and",
    "start": "218519",
    "end": "220840"
  },
  {
    "text": "other clusters that they that can then",
    "start": "220840",
    "end": "222680"
  },
  {
    "text": "pull these resources in and do that",
    "start": "222680",
    "end": "224319"
  },
  {
    "text": "syncing and as they get these resources",
    "start": "224319",
    "end": "226879"
  },
  {
    "text": "they'll Plum the data path to allow that",
    "start": "226879",
    "end": "228640"
  },
  {
    "text": "expected",
    "start": "228640",
    "end": "230239"
  },
  {
    "text": "connectivity but this process raises a",
    "start": "230239",
    "end": "232439"
  },
  {
    "text": "key question because this state",
    "start": "232439",
    "end": "235040"
  },
  {
    "text": "propagation has to occur in order for",
    "start": "235040",
    "end": "237200"
  },
  {
    "text": "your connectivity to be enabled so what",
    "start": "237200",
    "end": "239400"
  },
  {
    "text": "happens happens before that before that",
    "start": "239400",
    "end": "241280"
  },
  {
    "text": "you're not going to get that",
    "start": "241280",
    "end": "242120"
  },
  {
    "text": "connectivity so for instance if you have",
    "start": "242120",
    "end": "243720"
  },
  {
    "text": "a remote client pod trying to connect to",
    "start": "243720",
    "end": "245439"
  },
  {
    "text": "a database and another cluster that",
    "start": "245439",
    "end": "247200"
  },
  {
    "text": "traffic is going to be thrown right in",
    "start": "247200",
    "end": "248280"
  },
  {
    "text": "the trash by the data path because it's",
    "start": "248280",
    "end": "249599"
  },
  {
    "text": "not going to recognize that workload um",
    "start": "249599",
    "end": "252319"
  },
  {
    "text": "so two key questions we wanted to",
    "start": "252319",
    "end": "253519"
  },
  {
    "text": "explore with this was how long does it",
    "start": "253519",
    "end": "255480"
  },
  {
    "text": "take for this propagation to occur and",
    "start": "255480",
    "end": "257840"
  },
  {
    "text": "can this propagation ever fail at a high",
    "start": "257840",
    "end": "259799"
  },
  {
    "text": "enough",
    "start": "259799",
    "end": "260720"
  },
  {
    "text": "scale um so let's get into how we looked",
    "start": "260720",
    "end": "262800"
  },
  {
    "text": "into this so we created this workflow",
    "start": "262800",
    "end": "265360"
  },
  {
    "text": "for testing where we were focusing on",
    "start": "265360",
    "end": "267320"
  },
  {
    "text": "the number of nodes in the mesh as our",
    "start": "267320",
    "end": "268800"
  },
  {
    "text": "primary variable so we created 255",
    "start": "268800",
    "end": "271400"
  },
  {
    "text": "clusters and meshed them all together",
    "start": "271400",
    "end": "273240"
  },
  {
    "text": "and then we created a continuous",
    "start": "273240",
    "end": "274440"
  },
  {
    "text": "workload that consisted of two parts the",
    "start": "274440",
    "end": "276880"
  },
  {
    "text": "first one was a noop workload which is",
    "start": "276880",
    "end": "278720"
  },
  {
    "text": "essentially 10 pods per cluster this was",
    "start": "278720",
    "end": "281080"
  },
  {
    "text": "to get some identities and some",
    "start": "281080",
    "end": "282520"
  },
  {
    "text": "endpoints working in the cluster M API",
    "start": "282520",
    "end": "284400"
  },
  {
    "text": "server and then we deployed some",
    "start": "284400",
    "end": "286160"
  },
  {
    "text": "Benchmark tooling which I'll talk about",
    "start": "286160",
    "end": "287479"
  },
  {
    "text": "here on the next slide um in order to",
    "start": "287479",
    "end": "289240"
  },
  {
    "text": "add load on a customer Shi server",
    "start": "289240",
    "end": "292000"
  },
  {
    "text": "additionally um so as these are running",
    "start": "292000",
    "end": "294520"
  },
  {
    "text": "we just slowly increase the number of",
    "start": "294520",
    "end": "296000"
  },
  {
    "text": "notes and increments until we got to",
    "start": "296000",
    "end": "298560"
  },
  {
    "text": "50,000 so the Ping test was The",
    "start": "298560",
    "end": "300759"
  },
  {
    "text": "Benchmark that we wrote for this um and",
    "start": "300759",
    "end": "303680"
  },
  {
    "text": "the goal was to sort of try to create a",
    "start": "303680",
    "end": "306080"
  },
  {
    "text": "propagation latency heris that we could",
    "start": "306080",
    "end": "307880"
  },
  {
    "text": "look at throughout our test in order to",
    "start": "307880",
    "end": "309680"
  },
  {
    "text": "observe how the mesh behaved as the",
    "start": "309680",
    "end": "311440"
  },
  {
    "text": "number of nodes increased and it all",
    "start": "311440",
    "end": "313320"
  },
  {
    "text": "centers around this Ping Server here um",
    "start": "313320",
    "end": "316440"
  },
  {
    "text": "so the Ping Server in cluster B is going",
    "start": "316440",
    "end": "318440"
  },
  {
    "text": "to start by creating a Network",
    "start": "318440",
    "end": "320440"
  },
  {
    "text": "policy that allows traffic to it on",
    "start": "320440",
    "end": "323080"
  },
  {
    "text": "Ingress with a new random label and this",
    "start": "323080",
    "end": "325639"
  },
  {
    "text": "label is generated um again randomly on",
    "start": "325639",
    "end": "328360"
  },
  {
    "text": "every iteration of this test and that's",
    "start": "328360",
    "end": "329800"
  },
  {
    "text": "really important um then the Ping Server",
    "start": "329800",
    "end": "332479"
  },
  {
    "text": "is going to create What's called the",
    "start": "332479",
    "end": "333560"
  },
  {
    "text": "Ping client inside of cluster a by just",
    "start": "333560",
    "end": "336000"
  },
  {
    "text": "contacting the API server directly and",
    "start": "336000",
    "end": "338400"
  },
  {
    "text": "the goal of this ping client is to Ping",
    "start": "338400",
    "end": "340000"
  },
  {
    "text": "the Ping Server um as fast as it",
    "start": "340000",
    "end": "342240"
  },
  {
    "text": "possibly can on Startup but during this",
    "start": "342240",
    "end": "344639"
  },
  {
    "text": "process right there's a cubet in cluster",
    "start": "344639",
    "end": "347039"
  },
  {
    "text": "a that's going to pick up the request",
    "start": "347039",
    "end": "348840"
  },
  {
    "text": "for this pink client and then pass a cni",
    "start": "348840",
    "end": "351400"
  },
  {
    "text": "ad event onto the agent again is",
    "start": "351400",
    "end": "353720"
  },
  {
    "text": "Cam's primary responsibility as a cni",
    "start": "353720",
    "end": "355880"
  },
  {
    "text": "plugin um we have a cni listener pod",
    "start": "355880",
    "end": "359120"
  },
  {
    "text": "running and cluster a that tails the",
    "start": "359120",
    "end": "361600"
  },
  {
    "text": "logs inside of the agent in order to",
    "start": "361600",
    "end": "364000"
  },
  {
    "text": "get an estimate of how long the cni ad",
    "start": "364000",
    "end": "365960"
  },
  {
    "text": "duration took because we currently don't",
    "start": "365960",
    "end": "367800"
  },
  {
    "text": "have a metric to expose this right now",
    "start": "367800",
    "end": "370160"
  },
  {
    "text": "um and then this cni ad duration is sent",
    "start": "370160",
    "end": "372000"
  },
  {
    "text": "to the Ping Server so it can be exposed",
    "start": "372000",
    "end": "373639"
  },
  {
    "text": "and",
    "start": "373639",
    "end": "375280"
  },
  {
    "text": "recorded So during this process we're",
    "start": "375280",
    "end": "377560"
  },
  {
    "text": "recording three things right we're",
    "start": "377560",
    "end": "379520"
  },
  {
    "text": "recording the C ad duration but the pink",
    "start": "379520",
    "end": "381840"
  },
  {
    "text": "server is also going to record the time",
    "start": "381840",
    "end": "383800"
  },
  {
    "text": "that it sent a request to create the",
    "start": "383800",
    "end": "385520"
  },
  {
    "text": "Ping client and it's going to record the",
    "start": "385520",
    "end": "387680"
  },
  {
    "text": "time that it received a ping from the",
    "start": "387680",
    "end": "389160"
  },
  {
    "text": "Ping client for the first time and by",
    "start": "389160",
    "end": "391599"
  },
  {
    "text": "subtracting these two from each other we",
    "start": "391599",
    "end": "393639"
  },
  {
    "text": "can get a heris of how long the",
    "start": "393639",
    "end": "396560"
  },
  {
    "text": "propagation um took in order to get the",
    "start": "396560",
    "end": "398880"
  },
  {
    "text": "Ping client information over to Cluster",
    "start": "398880",
    "end": "400960"
  },
  {
    "text": "B so the data path could be plumped",
    "start": "400960",
    "end": "403599"
  },
  {
    "text": "properly um and now there's a lot that",
    "start": "403599",
    "end": "406080"
  },
  {
    "text": "happens during this time period because",
    "start": "406080",
    "end": "408960"
  },
  {
    "text": "this duration records how long it took",
    "start": "408960",
    "end": "411720"
  },
  {
    "text": "to send the request to create the pin",
    "start": "411720",
    "end": "413599"
  },
  {
    "text": "client over the network how long it took",
    "start": "413599",
    "end": "415919"
  },
  {
    "text": "the API server and cluster a to process",
    "start": "415919",
    "end": "418000"
  },
  {
    "text": "that request all these kinds of things",
    "start": "418000",
    "end": "420360"
  },
  {
    "text": "in bold are the things that is",
    "start": "420360",
    "end": "422000"
  },
  {
    "text": "directly responsible and what we were",
    "start": "422000",
    "end": "423599"
  },
  {
    "text": "really interested in but at the same",
    "start": "423599",
    "end": "425800"
  },
  {
    "text": "time if any of these things increase",
    "start": "425800",
    "end": "427440"
  },
  {
    "text": "it's something that's important and what",
    "start": "427440",
    "end": "428759"
  },
  {
    "text": "we want to look at um so theistic",
    "start": "428759",
    "end": "430879"
  },
  {
    "text": "although it isn't super detailed it did",
    "start": "430879",
    "end": "433120"
  },
  {
    "text": "give us a good jumping off point really",
    "start": "433120",
    "end": "434840"
  },
  {
    "text": "get into",
    "start": "434840",
    "end": "436319"
  },
  {
    "text": "things so we also measured a couple of",
    "start": "436319",
    "end": "438720"
  },
  {
    "text": "other things just to help us understand",
    "start": "438720",
    "end": "440160"
  },
  {
    "text": "what was going on uh such as CPU and",
    "start": "440160",
    "end": "442400"
  },
  {
    "text": "memory usage there were a couple of",
    "start": "442400",
    "end": "444120"
  },
  {
    "text": "celium agent metrics that we really",
    "start": "444120",
    "end": "445599"
  },
  {
    "text": "focused on in terms of policy um but we",
    "start": "445599",
    "end": "448680"
  },
  {
    "text": "also focused on cluster Mish API server",
    "start": "448680",
    "end": "450560"
  },
  {
    "text": "metrics related to",
    "start": "450560",
    "end": "453400"
  },
  {
    "text": "ETD so let's talk about the test",
    "start": "453400",
    "end": "455400"
  },
  {
    "text": "environment this is one of my favorite",
    "start": "455400",
    "end": "456599"
  },
  {
    "text": "parts of this project um because we",
    "start": "456599",
    "end": "459120"
  },
  {
    "text": "could have created 255 clusters in the",
    "start": "459120",
    "end": "461360"
  },
  {
    "text": "cloud and scaled them all up so we had",
    "start": "461360",
    "end": "463879"
  },
  {
    "text": "50,000 nodes in total but trying to do",
    "start": "463879",
    "end": "466400"
  },
  {
    "text": "that in the cloud would be really",
    "start": "466400",
    "end": "467720"
  },
  {
    "text": "difficult uh if you like trying to buy",
    "start": "467720",
    "end": "469360"
  },
  {
    "text": "tickets to a Taylor Swift concert it's",
    "start": "469360",
    "end": "471720"
  },
  {
    "text": "just it's just not going to happen um so",
    "start": "471720",
    "end": "474240"
  },
  {
    "text": "we had to come up with a creative way to",
    "start": "474240",
    "end": "476080"
  },
  {
    "text": "reduce the amount of resources that it",
    "start": "476080",
    "end": "477599"
  },
  {
    "text": "would take to run this test and we ended",
    "start": "477599",
    "end": "479840"
  },
  {
    "text": "up with this sort of architecture per",
    "start": "479840",
    "end": "481840"
  },
  {
    "text": "cluster so each cluster consisted of two",
    "start": "481840",
    "end": "484280"
  },
  {
    "text": "nodes a control plane node and a worker",
    "start": "484280",
    "end": "485960"
  },
  {
    "text": "node control plane node would run the",
    "start": "485960",
    "end": "487840"
  },
  {
    "text": "custom API server and the worker node",
    "start": "487840",
    "end": "490360"
  },
  {
    "text": "was responsible for running uh what we",
    "start": "490360",
    "end": "492479"
  },
  {
    "text": "called Hollow nodes and that consisted",
    "start": "492479",
    "end": "494800"
  },
  {
    "text": "of two parts um it was Cub Mark and C",
    "start": "494800",
    "end": "498319"
  },
  {
    "text": "Mark so Cub Mark if you're not familiar",
    "start": "498319",
    "end": "500720"
  },
  {
    "text": "allows you to run what's called a hollow",
    "start": "500720",
    "end": "502440"
  },
  {
    "text": "cuet where you have a cuet that's",
    "start": "502440",
    "end": "504879"
  },
  {
    "text": "running but all the lower level",
    "start": "504879",
    "end": "506440"
  },
  {
    "text": "implementation details are hollowed out",
    "start": "506440",
    "end": "508720"
  },
  {
    "text": "so the cuet will talk to the API server",
    "start": "508720",
    "end": "511360"
  },
  {
    "text": "as if it's starting containers and",
    "start": "511360",
    "end": "512640"
  },
  {
    "text": "mounting volumes but it actually won't",
    "start": "512640",
    "end": "514240"
  },
  {
    "text": "perform those actions and that allows",
    "start": "514240",
    "end": "516159"
  },
  {
    "text": "you to run 10 cuets or Hollow cuets",
    "start": "516159",
    "end": "518399"
  },
  {
    "text": "excuse me for every one CPU core on your",
    "start": "518399",
    "end": "520760"
  },
  {
    "text": "node which was great scalability for us",
    "start": "520760",
    "end": "523518"
  },
  {
    "text": "um but because this cuet can't run CM",
    "start": "523519",
    "end": "525560"
  },
  {
    "text": "agents we also needed a tool to mock the",
    "start": "525560",
    "end": "527600"
  },
  {
    "text": "load that celum is going to put on the",
    "start": "527600",
    "end": "529080"
  },
  {
    "text": "API server and the custom M API servers",
    "start": "529080",
    "end": "531519"
  },
  {
    "text": "so we developed a tool called Sly mock",
    "start": "531519",
    "end": "533240"
  },
  {
    "text": "to do this um and that's going to apply",
    "start": "533240",
    "end": "535480"
  },
  {
    "text": "that loads that way we're actually",
    "start": "535480",
    "end": "536600"
  },
  {
    "text": "testing",
    "start": "536600",
    "end": "538200"
  },
  {
    "text": "something",
    "start": "538200",
    "end": "539959"
  },
  {
    "text": "um now in order to run our ping test",
    "start": "539959",
    "end": "542279"
  },
  {
    "text": "Benchmark that we talked about earlier",
    "start": "542279",
    "end": "543640"
  },
  {
    "text": "we had two special clusters in the mesh",
    "start": "543640",
    "end": "546200"
  },
  {
    "text": "that we deemed specific to running these",
    "start": "546200",
    "end": "548440"
  },
  {
    "text": "benchmarks um these differ in two ways",
    "start": "548440",
    "end": "552040"
  },
  {
    "text": "so first they deploy Prometheus in order",
    "start": "552040",
    "end": "554040"
  },
  {
    "text": "to gather metrics but they also have a",
    "start": "554040",
    "end": "556360"
  },
  {
    "text": "higher uh worker node size because we",
    "start": "556360",
    "end": "558480"
  },
  {
    "text": "didn't want resource restrictions to",
    "start": "558480",
    "end": "560040"
  },
  {
    "text": "limit uh The Benchmark in anyway",
    "start": "560040",
    "end": "562000"
  },
  {
    "text": "especially since we're adding pereus on",
    "start": "562000",
    "end": "563720"
  },
  {
    "text": "top and the reason why we only ran the",
    "start": "563720",
    "end": "566040"
  },
  {
    "text": "Ping test Benchmark among two clusters",
    "start": "566040",
    "end": "568440"
  },
  {
    "text": "rather than all the clusters is because",
    "start": "568440",
    "end": "570519"
  },
  {
    "text": "we're assuming that every cluster in the",
    "start": "570519",
    "end": "572240"
  },
  {
    "text": "mesh has a similar view and experience",
    "start": "572240",
    "end": "574600"
  },
  {
    "text": "inside of the mesh and so we didn't",
    "start": "574600",
    "end": "576160"
  },
  {
    "text": "think there would be much value in",
    "start": "576160",
    "end": "577640"
  },
  {
    "text": "testing this between every single",
    "start": "577640",
    "end": "579120"
  },
  {
    "text": "cluster just two and so that gave us uh",
    "start": "579120",
    "end": "581680"
  },
  {
    "text": "another optimization as",
    "start": "581680",
    "end": "583480"
  },
  {
    "text": "well um if we draw the connections",
    "start": "583480",
    "end": "586079"
  },
  {
    "text": "between these two clusters that are",
    "start": "586079",
    "end": "587440"
  },
  {
    "text": "running the tests we can see these are",
    "start": "587440",
    "end": "588680"
  },
  {
    "text": "kind of the communication paths in green",
    "start": "588680",
    "end": "591760"
  },
  {
    "text": "is The Benchmark uh communication paths",
    "start": "591760",
    "end": "594360"
  },
  {
    "text": "in blue a c mock talking to its own API",
    "start": "594360",
    "end": "597279"
  },
  {
    "text": "server to simulate the load that the",
    "start": "597279",
    "end": "598600"
  },
  {
    "text": "agent would pump on and in red and",
    "start": "598600",
    "end": "601480"
  },
  {
    "text": "yellow are the connections that are made",
    "start": "601480",
    "end": "603560"
  },
  {
    "text": "cross clustered each cluster Mish API",
    "start": "603560",
    "end": "605480"
  },
  {
    "text": "server if we add an additional",
    "start": "605480",
    "end": "608680"
  },
  {
    "text": "cluster there we go if we add an",
    "start": "608680",
    "end": "610720"
  },
  {
    "text": "additional cluster these are the new",
    "start": "610720",
    "end": "612480"
  },
  {
    "text": "connections that are made between each",
    "start": "612480",
    "end": "614320"
  },
  {
    "text": "cluster M API server specifically three",
    "start": "614320",
    "end": "616760"
  },
  {
    "text": "new connections are added um because we",
    "start": "616760",
    "end": "619320"
  },
  {
    "text": "have three new nodes that are talking to",
    "start": "619320",
    "end": "621440"
  },
  {
    "text": "the new the cluster Mish API servers in",
    "start": "621440",
    "end": "623320"
  },
  {
    "text": "cluster one and two and so now each",
    "start": "623320",
    "end": "626480"
  },
  {
    "text": "clust M API server is supporting",
    "start": "626480",
    "end": "628519"
  },
  {
    "text": "connections from six different clients",
    "start": "628519",
    "end": "630519"
  },
  {
    "text": "and that's just with adding three more",
    "start": "630519",
    "end": "631720"
  },
  {
    "text": "nodes so things kind of scale up pretty",
    "start": "631720",
    "end": "633720"
  },
  {
    "text": "quick here so let's get into the results",
    "start": "633720",
    "end": "636720"
  },
  {
    "text": "so these are the number of nodes that we",
    "start": "636720",
    "end": "638920"
  },
  {
    "text": "had over time during our test we got to",
    "start": "638920",
    "end": "641120"
  },
  {
    "text": "just above 50,000 that was how the math",
    "start": "641120",
    "end": "643160"
  },
  {
    "text": "worked out for scaling this up um and I",
    "start": "643160",
    "end": "645639"
  },
  {
    "text": "won't make you squint uh this took about",
    "start": "645639",
    "end": "647639"
  },
  {
    "text": "3",
    "start": "647639",
    "end": "648880"
  },
  {
    "text": "hours um our cni ad duration wasn't too",
    "start": "648880",
    "end": "652120"
  },
  {
    "text": "interesting you know it had some spikes",
    "start": "652120",
    "end": "653880"
  },
  {
    "text": "here and there well not here and there",
    "start": "653880",
    "end": "655639"
  },
  {
    "text": "pretty much everywhere um but uh and had",
    "start": "655639",
    "end": "659160"
  },
  {
    "text": "a slow linear Trend uh of increasing but",
    "start": "659160",
    "end": "662000"
  },
  {
    "text": "it felt pretty normal so we just kind of",
    "start": "662000",
    "end": "663720"
  },
  {
    "text": "moved on um this was the same for policy",
    "start": "663720",
    "end": "667160"
  },
  {
    "text": "implementation delay in regeneration",
    "start": "667160",
    "end": "668920"
  },
  {
    "text": "time so if you don't know policy",
    "start": "668920",
    "end": "670920"
  },
  {
    "text": "implementation delay measures the amount",
    "start": "670920",
    "end": "672800"
  },
  {
    "text": "of time that it takes for celium Network",
    "start": "672800",
    "end": "674839"
  },
  {
    "text": "policy to be plumbed in the data path",
    "start": "674839",
    "end": "676959"
  },
  {
    "text": "after it's first received from a celium",
    "start": "676959",
    "end": "678680"
  },
  {
    "text": "agent or to a celium agent excuse me um",
    "start": "678680",
    "end": "682440"
  },
  {
    "text": "policy regeneration time measures the",
    "start": "682440",
    "end": "684240"
  },
  {
    "text": "amount of time that celium takes in",
    "start": "684240",
    "end": "686360"
  },
  {
    "text": "order to do its recalculation um of all",
    "start": "686360",
    "end": "689120"
  },
  {
    "text": "policies on the Node and so again these",
    "start": "689120",
    "end": "691279"
  },
  {
    "text": "look pretty normal so we just kind of",
    "start": "691279",
    "end": "692399"
  },
  {
    "text": "moved on the interesting part is",
    "start": "692399",
    "end": "694160"
  },
  {
    "text": "theistic here which had a lot of really",
    "start": "694160",
    "end": "696680"
  },
  {
    "text": "interesting spikes um the y- axis here",
    "start": "696680",
    "end": "699399"
  },
  {
    "text": "is logmar rithmic in scale so we had a",
    "start": "699399",
    "end": "702320"
  },
  {
    "text": "range of 1 millisecond to 30 seconds um",
    "start": "702320",
    "end": "705680"
  },
  {
    "text": "and we tried to correlate these to churn",
    "start": "705680",
    "end": "707760"
  },
  {
    "text": "inside of the cluster mesh so this",
    "start": "707760",
    "end": "709160"
  },
  {
    "text": "bottom graph shows the rate that we're",
    "start": "709160",
    "end": "710639"
  },
  {
    "text": "adding nodes to the mesh and there's a",
    "start": "710639",
    "end": "713120"
  },
  {
    "text": "rough correlation between these spikes",
    "start": "713120",
    "end": "715440"
  },
  {
    "text": "um and the spikes in theistic here but",
    "start": "715440",
    "end": "717720"
  },
  {
    "text": "one really interesting part was that",
    "start": "717720",
    "end": "719120"
  },
  {
    "text": "this Plateau didn't have that",
    "start": "719120",
    "end": "721079"
  },
  {
    "text": "correlating Spike and so we were really",
    "start": "721079",
    "end": "722920"
  },
  {
    "text": "curious what was going on there um and",
    "start": "722920",
    "end": "725440"
  },
  {
    "text": "it turns out we just had a complete",
    "start": "725440",
    "end": "726560"
  },
  {
    "text": "failure of our Benchmark at 45,000 noes",
    "start": "726560",
    "end": "729040"
  },
  {
    "text": "some reason it just stopped working for",
    "start": "729040",
    "end": "730920"
  },
  {
    "text": "a couple of minutes um and that led to",
    "start": "730920",
    "end": "733480"
  },
  {
    "text": "that weird plateau and trying to figure",
    "start": "733480",
    "end": "735160"
  },
  {
    "text": "out what was going on we traced it back",
    "start": "735160",
    "end": "737160"
  },
  {
    "text": "to the cluster MH API server ETD so",
    "start": "737160",
    "end": "740000"
  },
  {
    "text": "these two graphs show the resource usage",
    "start": "740000",
    "end": "741639"
  },
  {
    "text": "of ETD the bottom graph shows CPU and",
    "start": "741639",
    "end": "744240"
  },
  {
    "text": "the top graph shows the watches for ETD",
    "start": "744240",
    "end": "746240"
  },
  {
    "text": "over time the green line shows the API",
    "start": "746240",
    "end": "749639"
  },
  {
    "text": "server um that was running alongside the",
    "start": "749639",
    "end": "751800"
  },
  {
    "text": "Ping client and was responsible for",
    "start": "751800",
    "end": "754440"
  },
  {
    "text": "propagating the celium identity and",
    "start": "754440",
    "end": "756160"
  },
  {
    "text": "celium endpoint of that ping client to",
    "start": "756160",
    "end": "758839"
  },
  {
    "text": "the other cluster in order to enable",
    "start": "758839",
    "end": "760600"
  },
  {
    "text": "that connectivity the yellow line",
    "start": "760600",
    "end": "762800"
  },
  {
    "text": "represents the cluster M API server that",
    "start": "762800",
    "end": "764480"
  },
  {
    "text": "was running in a baseline cluster that",
    "start": "764480",
    "end": "766320"
  },
  {
    "text": "didn't have to go through this",
    "start": "766320",
    "end": "767760"
  },
  {
    "text": "additional load um and you can see that",
    "start": "767760",
    "end": "771040"
  },
  {
    "text": "around where we see the Benchmark you",
    "start": "771040",
    "end": "773040"
  },
  {
    "text": "know dropping we get to around 50 cores",
    "start": "773040",
    "end": "775360"
  },
  {
    "text": "of CPU usage uh on ETD in that loaded",
    "start": "775360",
    "end": "778399"
  },
  {
    "text": "customer API server and at the same time",
    "start": "778399",
    "end": "781800"
  },
  {
    "text": "um the number of watches on itd drops by",
    "start": "781800",
    "end": "784160"
  },
  {
    "text": "half and so we're assuming that some CPU",
    "start": "784160",
    "end": "787000"
  },
  {
    "text": "saturation led to these dropped watches",
    "start": "787000",
    "end": "789000"
  },
  {
    "text": "and one of those watches was critical",
    "start": "789000",
    "end": "790600"
  },
  {
    "text": "for our Benchmark that caused it to",
    "start": "790600",
    "end": "793279"
  },
  {
    "text": "fail um and so correlating the C the CPU",
    "start": "793279",
    "end": "796480"
  },
  {
    "text": "usage to our heuristic those line up",
    "start": "796480",
    "end": "798680"
  },
  {
    "text": "pretty",
    "start": "798680",
    "end": "800279"
  },
  {
    "text": "well so revisiting our problem questions",
    "start": "800279",
    "end": "803040"
  },
  {
    "text": "from earlier right how long does it take",
    "start": "803040",
    "end": "805000"
  },
  {
    "text": "for this propagation to occur again it's",
    "start": "805000",
    "end": "807399"
  },
  {
    "text": "just for our heuristic so calling this",
    "start": "807399",
    "end": "809920"
  },
  {
    "text": "the propagation for cluster M API server",
    "start": "809920",
    "end": "812560"
  },
  {
    "text": "isn't too accurate but again 1",
    "start": "812560",
    "end": "814320"
  },
  {
    "text": "millisecond 30 seconds and I think the",
    "start": "814320",
    "end": "815959"
  },
  {
    "text": "key thing here to take away is that this",
    "start": "815959",
    "end": "817680"
  },
  {
    "text": "range is pretty high um and yes this",
    "start": "817680",
    "end": "820800"
  },
  {
    "text": "propagation can fail um if ETD becomes",
    "start": "820800",
    "end": "823959"
  },
  {
    "text": "impacted",
    "start": "823959",
    "end": "825399"
  },
  {
    "text": "critically so how does KV store mesh",
    "start": "825399",
    "end": "827600"
  },
  {
    "text": "address these so again KV store mesh",
    "start": "827600",
    "end": "830639"
  },
  {
    "text": "allows for higher scale cluster mesh",
    "start": "830639",
    "end": "832000"
  },
  {
    "text": "deployments by reducing that load on ETD",
    "start": "832000",
    "end": "834759"
  },
  {
    "text": "so if you look at the consumer model for",
    "start": "834759",
    "end": "836360"
  },
  {
    "text": "the custom API server every ceiling",
    "start": "836360",
    "end": "839000"
  },
  {
    "text": "agent inside of the mesh has to connect",
    "start": "839000",
    "end": "840720"
  },
  {
    "text": "to every other cluster mesh API server",
    "start": "840720",
    "end": "842639"
  },
  {
    "text": "in order to do this sync KV store mesh",
    "start": "842639",
    "end": "846320"
  },
  {
    "text": "um deploys a binary alongside of the",
    "start": "846320",
    "end": "848399"
  },
  {
    "text": "cluster mesh API server that syncs",
    "start": "848399",
    "end": "850720"
  },
  {
    "text": "information from other clusters into the",
    "start": "850720",
    "end": "852880"
  },
  {
    "text": "cluster's cluster mesh API server and",
    "start": "852880",
    "end": "855680"
  },
  {
    "text": "then agents running inside of that",
    "start": "855680",
    "end": "858040"
  },
  {
    "text": "cluster connect to its own cluster Mish",
    "start": "858040",
    "end": "860000"
  },
  {
    "text": "API server instead to sync that",
    "start": "860000",
    "end": "861920"
  },
  {
    "text": "information so we're adding this",
    "start": "861920",
    "end": "863399"
  },
  {
    "text": "intermed intermediary cache here and if",
    "start": "863399",
    "end": "867079"
  },
  {
    "text": "we look at the number of clients that",
    "start": "867079",
    "end": "868279"
  },
  {
    "text": "are support Ed for every cluster Mish",
    "start": "868279",
    "end": "870199"
  },
  {
    "text": "API server between these two",
    "start": "870199",
    "end": "871600"
  },
  {
    "text": "implementations you can see the",
    "start": "871600",
    "end": "873079"
  },
  {
    "text": "difference in scalability pretty clearly",
    "start": "873079",
    "end": "875240"
  },
  {
    "text": "because the cluster Mish API server um",
    "start": "875240",
    "end": "877480"
  },
  {
    "text": "without KV store has to support the",
    "start": "877480",
    "end": "879680"
  },
  {
    "text": "number of clients that are equivalent to",
    "start": "879680",
    "end": "881600"
  },
  {
    "text": "the number of nodes in the mesh whereas",
    "start": "881600",
    "end": "884839"
  },
  {
    "text": "the KV store mesh um implementation",
    "start": "884839",
    "end": "888000"
  },
  {
    "text": "supports you know number of clusters in",
    "start": "888000",
    "end": "890160"
  },
  {
    "text": "the mesh minus one as clients outside of",
    "start": "890160",
    "end": "892440"
  },
  {
    "text": "the cluster and the number of nodes per",
    "start": "892440",
    "end": "894639"
  },
  {
    "text": "cluster um as clients coming from inside",
    "start": "894639",
    "end": "897160"
  },
  {
    "text": "the cluster",
    "start": "897160",
    "end": "899199"
  },
  {
    "text": "and using numbers from our our test from",
    "start": "899199",
    "end": "901519"
  },
  {
    "text": "earlier I rounded up the number of nodes",
    "start": "901519",
    "end": "903079"
  },
  {
    "text": "to a clean 200 um you can see that in",
    "start": "903079",
    "end": "905800"
  },
  {
    "text": "the customer shpi server case each one",
    "start": "905800",
    "end": "907920"
  },
  {
    "text": "was supporting you know 50k clients",
    "start": "907920",
    "end": "910279"
  },
  {
    "text": "whereas with the KV store mesh we would",
    "start": "910279",
    "end": "911639"
  },
  {
    "text": "only be supporting about",
    "start": "911639",
    "end": "914759"
  },
  {
    "text": "454 um so key takeaways here if there's",
    "start": "914759",
    "end": "917360"
  },
  {
    "text": "anything to take away from this uh this",
    "start": "917360",
    "end": "918839"
  },
  {
    "text": "is what it'll be right so KV stormish",
    "start": "918839",
    "end": "921959"
  },
  {
    "text": "allows you to reduce the overall SD CPU",
    "start": "921959",
    "end": "924519"
  },
  {
    "text": "resource usage um as well as memory by",
    "start": "924519",
    "end": "927680"
  },
  {
    "text": "spreading the load that it has to take",
    "start": "927680",
    "end": "929639"
  },
  {
    "text": "on proportionally throughout the mesh so",
    "start": "929639",
    "end": "931839"
  },
  {
    "text": "every cluster is now responsible for the",
    "start": "931839",
    "end": "933880"
  },
  {
    "text": "number of clust the number of nodes",
    "start": "933880",
    "end": "935920"
  },
  {
    "text": "inside of its own cluster rather than",
    "start": "935920",
    "end": "937959"
  },
  {
    "text": "really being responsible for handling",
    "start": "937959",
    "end": "939399"
  },
  {
    "text": "load from the entire mesh and this",
    "start": "939399",
    "end": "941959"
  },
  {
    "text": "allows you to increase greater scale but",
    "start": "941959",
    "end": "944279"
  },
  {
    "text": "it also reduces the mesh wide impact",
    "start": "944279",
    "end": "946160"
  },
  {
    "text": "from churn that is caused from a single",
    "start": "946160",
    "end": "948079"
  },
  {
    "text": "cluster because we get some isolation so",
    "start": "948079",
    "end": "950920"
  },
  {
    "text": "again in the cluster mesh API server",
    "start": "950920",
    "end": "952800"
  },
  {
    "text": "case let's say that cluster a is",
    "start": "952800",
    "end": "954920"
  },
  {
    "text": "scheduled for a rolling upgrade right of",
    "start": "954920",
    "end": "957839"
  },
  {
    "text": "of celium and we have celium agents that",
    "start": "957839",
    "end": "960000"
  },
  {
    "text": "are restarting as these celium agents",
    "start": "960000",
    "end": "962560"
  },
  {
    "text": "restart they're going to do list calls",
    "start": "962560",
    "end": "964360"
  },
  {
    "text": "to the cluster M API server to do an",
    "start": "964360",
    "end": "966040"
  },
  {
    "text": "initial sync which is going to add that",
    "start": "966040",
    "end": "967600"
  },
  {
    "text": "increased load and that could impact",
    "start": "967600",
    "end": "969639"
  },
  {
    "text": "availability for cluster",
    "start": "969639",
    "end": "971480"
  },
  {
    "text": "B in the KV store mesh case those agents",
    "start": "971480",
    "end": "975040"
  },
  {
    "text": "would instead contact their own cluster",
    "start": "975040",
    "end": "976560"
  },
  {
    "text": "M API server um and so cluster B would",
    "start": "976560",
    "end": "979160"
  },
  {
    "text": "remain unaffected because we have this",
    "start": "979160",
    "end": "981120"
  },
  {
    "text": "this isolation in place uh in the worst",
    "start": "981120",
    "end": "983480"
  },
  {
    "text": "case scenario if the cluster Mish API",
    "start": "983480",
    "end": "985519"
  },
  {
    "text": "server in cluster a restarts that's only",
    "start": "985519",
    "end": "987639"
  },
  {
    "text": "one client that's doing a reconnect to",
    "start": "987639",
    "end": "989399"
  },
  {
    "text": "Every Other clust M API server which is",
    "start": "989399",
    "end": "991680"
  },
  {
    "text": "a lot more",
    "start": "991680",
    "end": "993639"
  },
  {
    "text": "manageable and that's it so thank you",
    "start": "993639",
    "end": "995440"
  },
  {
    "text": "very much um yeah any",
    "start": "995440",
    "end": "998800"
  },
  {
    "text": "[Applause]",
    "start": "998800",
    "end": "1004360"
  },
  {
    "text": "questions oh and please feel free to",
    "start": "1004360",
    "end": "1006279"
  },
  {
    "text": "fill out the survey if you have feedback",
    "start": "1006279",
    "end": "1007560"
  },
  {
    "text": "it' be super",
    "start": "1007560",
    "end": "1008759"
  },
  {
    "text": "helpful cool uh I have two questions uh",
    "start": "1008759",
    "end": "1011639"
  },
  {
    "text": "first one uh would you not recommend uh",
    "start": "1011639",
    "end": "1015040"
  },
  {
    "text": "running kave store mesh like if I run",
    "start": "1015040",
    "end": "1018480"
  },
  {
    "text": "run cluster mesh why why would I not run",
    "start": "1018480",
    "end": "1021199"
  },
  {
    "text": "cavor mesh I mean is there a reason why",
    "start": "1021199",
    "end": "1024438"
  },
  {
    "text": "you would not want to do that by default",
    "start": "1024439",
    "end": "1027160"
  },
  {
    "text": "so that's a good question um I'd say",
    "start": "1027160",
    "end": "1028918"
  },
  {
    "text": "right now probably because it's beta um",
    "start": "1028919",
    "end": "1031880"
  },
  {
    "text": "that's that'd be the main thing the",
    "start": "1031880",
    "end": "1033319"
  },
  {
    "text": "other thing is that KV store mesh you're",
    "start": "1033319",
    "end": "1035760"
  },
  {
    "text": "going to see the greatest results with",
    "start": "1035760",
    "end": "1037319"
  },
  {
    "text": "the higher scale if you're running in",
    "start": "1037319",
    "end": "1039199"
  },
  {
    "text": "lower scale situations um you might",
    "start": "1039199",
    "end": "1041400"
  },
  {
    "text": "actually see increased load um from KV",
    "start": "1041400",
    "end": "1044000"
  },
  {
    "text": "stor mesh yeah okay and the second one",
    "start": "1044000",
    "end": "1047720"
  },
  {
    "text": "uh do you have any idea if it can work",
    "start": "1047720",
    "end": "1051280"
  },
  {
    "text": "uh if I use the identity allocation mod",
    "start": "1051280",
    "end": "1053160"
  },
  {
    "text": "kave store so I run at",
    "start": "1053160",
    "end": "1055640"
  },
  {
    "text": "CDs that formed the cluster mesh I don't",
    "start": "1055640",
    "end": "1058000"
  },
  {
    "text": "have the cluster mesh API server at",
    "start": "1058000",
    "end": "1060240"
  },
  {
    "text": "all so that's a good question um if",
    "start": "1060240",
    "end": "1063160"
  },
  {
    "text": "you're using KV storm mode you could um",
    "start": "1063160",
    "end": "1066880"
  },
  {
    "text": "use a similar implementation with KV",
    "start": "1066880",
    "end": "1069080"
  },
  {
    "text": "mesh I just don't think we support it",
    "start": "1069080",
    "end": "1070799"
  },
  {
    "text": "right now but it should theoretically be",
    "start": "1070799",
    "end": "1073520"
  },
  {
    "text": "possible we are thinking about oh cool",
    "start": "1073520",
    "end": "1076120"
  },
  {
    "text": "okay thanks yeah",
    "start": "1076120",
    "end": "1080400"
  },
  {
    "text": "um do you have any thoughts on how to",
    "start": "1087600",
    "end": "1089679"
  },
  {
    "text": "protect a lot of right activity from one",
    "start": "1089679",
    "end": "1092280"
  },
  {
    "text": "cluster and how do you protect the rest",
    "start": "1092280",
    "end": "1094159"
  },
  {
    "text": "of the Clusters from being impacted like",
    "start": "1094159",
    "end": "1096520"
  },
  {
    "text": "a shared fate kind of thing yeah so um",
    "start": "1096520",
    "end": "1100559"
  },
  {
    "text": "with KV stor mesh I think that's the",
    "start": "1100559",
    "end": "1102440"
  },
  {
    "text": "best way to do that in a cluster M",
    "start": "1102440",
    "end": "1104039"
  },
  {
    "text": "scenario is if you have a single cluster",
    "start": "1104039",
    "end": "1107080"
  },
  {
    "text": "that is going to put High churn in your",
    "start": "1107080",
    "end": "1109440"
  },
  {
    "text": "mesh which typically happens right",
    "start": "1109440",
    "end": "1111240"
  },
  {
    "text": "because we have maybe like two clusters",
    "start": "1111240",
    "end": "1113280"
  },
  {
    "text": "outside of the mesh that are having the",
    "start": "1113280",
    "end": "1114559"
  },
  {
    "text": "most amount of churn and are going to",
    "start": "1114559",
    "end": "1116159"
  },
  {
    "text": "put the most amount of load on Other",
    "start": "1116159",
    "end": "1117640"
  },
  {
    "text": "clust M API servers when you have KV",
    "start": "1117640",
    "end": "1120159"
  },
  {
    "text": "store mesh enabled that um extra load is",
    "start": "1120159",
    "end": "1123480"
  },
  {
    "text": "only applied to the cluster Mish API",
    "start": "1123480",
    "end": "1125400"
  },
  {
    "text": "server inside its own cluster rather",
    "start": "1125400",
    "end": "1127799"
  },
  {
    "text": "than throughout the entire mesh um and",
    "start": "1127799",
    "end": "1130520"
  },
  {
    "text": "so that that's that would be what I",
    "start": "1130520",
    "end": "1131679"
  },
  {
    "text": "would recommend",
    "start": "1131679",
    "end": "1134880"
  }
]