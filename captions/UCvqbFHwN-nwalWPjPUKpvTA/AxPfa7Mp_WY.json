[
  {
    "text": "okay so you stopped oh yes honey okay so this is just footballs generator G RPC",
    "start": "30",
    "end": "6960"
  },
  {
    "text": "unlink it in production if you skip the neck side sorry Ben I used to be the CTO",
    "start": "6960",
    "end": "14910"
  },
  {
    "text": "of just football and when this talk got approved I still was and then in the distance between not",
    "start": "14910",
    "end": "21210"
  },
  {
    "text": "talking now I accept the job as partly fire so I now work now instead but none of the content",
    "start": "21210",
    "end": "26460"
  },
  {
    "text": "in this talk made me leave just football so you're still going to learn a lot of stuff it's still relevant yeah",
    "start": "26460",
    "end": "33030"
  },
  {
    "text": "and I'm Kevin Lee your fault I work for buoyant I've been there for about four years and I am a core contributor to",
    "start": "33030",
    "end": "40590"
  },
  {
    "text": "linker D prior to buoyant I was working at Twitter for five years sort of took",
    "start": "40590",
    "end": "46590"
  },
  {
    "text": "part in ripping apart a giant monolithic application and converting it to micro services so I'm very familiar with just",
    "start": "46590",
    "end": "53340"
  },
  {
    "text": "footballs story as well I'm really excited to be speaking with Ben and he's gonna kick it off with talking about",
    "start": "53340",
    "end": "58680"
  },
  {
    "text": "just footballs yeah maybe if I do the slides in my busy I first get through oh yeah um so before I get started out just",
    "start": "58680",
    "end": "65820"
  },
  {
    "text": "football just kind of like a show of hands how many people using Cuban ideas in production today",
    "start": "65820",
    "end": "71780"
  },
  {
    "text": "okay fair few what about jlbc interesting okay linka d yeah cool okay",
    "start": "71780",
    "end": "82710"
  },
  {
    "text": "so let's get start by just football I'm not going to talk too much about the product itself like what it does but we were a startup founded in 2016 we were",
    "start": "82710",
    "end": "90720"
  },
  {
    "text": "basically in Sweden and we were basically designed to get kids into football so if you skip to the next",
    "start": "90720",
    "end": "97500"
  },
  {
    "text": "slide so this is what you still like so it was like a front-end game I bet okay I guess Android and iOS we used to do a",
    "start": "97500",
    "end": "105329"
  },
  {
    "text": "lot of challenges the idea is like it's pokemon golf or football so you go to a pitch you check in to the pitch with",
    "start": "105329",
    "end": "110850"
  },
  {
    "text": "your friends you challenge each other and then we give you some points as leaderboards and coin rewards so far we",
    "start": "110850",
    "end": "119399"
  },
  {
    "text": "also do some cool stuff with like a are so we could basically track a ball on a",
    "start": "119399",
    "end": "124409"
  },
  {
    "text": "football field kicked by your friends measure how fast it went we could do like fake goalkeepers so you",
    "start": "124409",
    "end": "129569"
  },
  {
    "text": "have to be the goalkeeper so far so there's a lot of cool challenges to solve didn't try and get kids on a football",
    "start": "129569",
    "end": "135200"
  },
  {
    "text": "Thanks so this is what our text art look like so for a start off using Cuba 90s that",
    "start": "135200",
    "end": "141630"
  },
  {
    "text": "was pretty fun a lot of node react native we run everything Lindy breaths we have bits of goal we were using",
    "start": "141630",
    "end": "148380"
  },
  {
    "text": "graphic wealth quite a while so before I kind of get into you like why we chose",
    "start": "148380",
    "end": "155160"
  },
  {
    "text": "the RPC and it's gonna go through the monolith the microscope is a story so in 2016 this is what our apologies for the",
    "start": "155160",
    "end": "162630"
  },
  {
    "text": "terrible diagram but I have this is what our stuck use look like so we have the rat net front end just call in our graph",
    "start": "162630",
    "end": "168959"
  },
  {
    "text": "QL monolith which means that all the business logic for the app was in graph QL like it was just dumped in there and",
    "start": "168959",
    "end": "175890"
  },
  {
    "text": "then we have on big massive database at the end so this is 2016 this worked kind",
    "start": "175890",
    "end": "181620"
  },
  {
    "text": "of fine for us and then by the time we prove that it all worked we wanted to split things out and make things a",
    "start": "181620",
    "end": "187080"
  },
  {
    "text": "little bit easy to build on like when you've got monolith everything just kind of push it and then it becomes really",
    "start": "187080",
    "end": "192270"
  },
  {
    "text": "hard to maintain and build on so by 2018 this is what we moved to so we still",
    "start": "192270",
    "end": "198120"
  },
  {
    "text": "have cabal in there but like it became a composition layer like it was it would call out to many many services aggregate",
    "start": "198120",
    "end": "204330"
  },
  {
    "text": "the responses together and then send that back to the client and then obviously we supply the databases they",
    "start": "204330",
    "end": "209459"
  },
  {
    "text": "became smaller and billing with the service now this was great until things start to",
    "start": "209459",
    "end": "216209"
  },
  {
    "text": "break so the thing is like building a lot of services and putting them behind",
    "start": "216209",
    "end": "221400"
  },
  {
    "text": "graph QL is great for splitting apart your monolith but not so great in terms",
    "start": "221400",
    "end": "226799"
  },
  {
    "text": "of performance so what we found is that under load under pressure graph QL",
    "start": "226799",
    "end": "232320"
  },
  {
    "text": "jatropha connections to any service in sinus tach so remember that one graph QL",
    "start": "232320",
    "end": "237390"
  },
  {
    "text": "call might call eighty different services inside our suck that's a lot of overhead of great and CPU creating",
    "start": "237390",
    "end": "243380"
  },
  {
    "text": "connections to other services internally so we had a lot we kind of had a lot of",
    "start": "243380",
    "end": "249090"
  },
  {
    "text": "thought about how do we fix this and then we settled on using your PC so geo",
    "start": "249090",
    "end": "256470"
  },
  {
    "text": "PC for people that haven't heard it's based on HTTP HB 2 it has this protobuf",
    "start": "256470",
    "end": "263789"
  },
  {
    "text": "kind of schema to define inputs and outputs a function we support multiplex in for us the most",
    "start": "263789",
    "end": "270419"
  },
  {
    "text": "important thing was that it supported one connection per service so that we could just basically not have to deal",
    "start": "270419",
    "end": "275490"
  },
  {
    "text": "with it overhead of kind of having all these different connections created by graph queue up so this is what so this",
    "start": "275490",
    "end": "284400"
  },
  {
    "text": "was another main reason that we switched from HTTP and rest is because no one",
    "start": "284400",
    "end": "291120"
  },
  {
    "text": "could ever decide how to build REST API inside just a wall so it's been days arguing about whether we should pass",
    "start": "291120",
    "end": "297780"
  },
  {
    "text": "stuff in the URL we should pass it in the post body and we could never agree on this so if you imagine here you can",
    "start": "297780",
    "end": "304590"
  },
  {
    "text": "see these are sorry if you say that these are real life calls like in the first bit we have the user ID we've",
    "start": "304590",
    "end": "311039"
  },
  {
    "text": "passed in through the URL and then the second bit is passed into the post but you've got different variables everywhere so this is what G RPC makes",
    "start": "311039",
    "end": "319350"
  },
  {
    "text": "us do it forces us to define our inputs and outputs upfront so you can see that we have this edit function which has",
    "start": "319350",
    "end": "326220"
  },
  {
    "text": "which takes in types of edit user requests which is an ID and a user name",
    "start": "326220",
    "end": "331229"
  },
  {
    "text": "and then it returns user response which it so it's all types basically gives you typed type pins around your inputs and",
    "start": "331229",
    "end": "336900"
  },
  {
    "text": "outputs so how we did GL PC just",
    "start": "336900",
    "end": "343289"
  },
  {
    "text": "football so we didn't have a mono repo either so we hand like say 30 services",
    "start": "343289",
    "end": "351510"
  },
  {
    "text": "they were all in different repositories in github so they were all over the place and the protobufs lived with the",
    "start": "351510",
    "end": "359039"
  },
  {
    "text": "service so the service defined its G OPC schema and it lived in the same repository I hear a lot of people like a",
    "start": "359039",
    "end": "367710"
  },
  {
    "text": "railroad into place is saying that managing job PC schemas is super hard so we think that we can't come up with a",
    "start": "367710",
    "end": "373919"
  },
  {
    "text": "nice way to do this which is to publish the g-o-p see service definition of",
    "start": "373919",
    "end": "380010"
  },
  {
    "text": "schema with the language that are using so whether that is NPM whether that's",
    "start": "380010",
    "end": "385500"
  },
  {
    "text": "Ruby gems whether that's get off we go whether that's whatever is to publish it to the native ecosystem because that's",
    "start": "385500",
    "end": "392910"
  },
  {
    "text": "really good at managing dependencies right so if we want to do like nested protobuf schemas so you can including",
    "start": "392910",
    "end": "398860"
  },
  {
    "text": "everywhere then let's like the package manage to deal with that like let's use",
    "start": "398860",
    "end": "404349"
  },
  {
    "text": "a packing only for its for if you wanna know more about that come talk to me then and we can go through a little bit",
    "start": "404349",
    "end": "410229"
  },
  {
    "text": "and then also get we used to write our own tax quick definitions to so tax crypt was kind of early support for",
    "start": "410229",
    "end": "417240"
  },
  {
    "text": "jealousy I think when we were generating clients so we ended up handwriting our own types for everything which was kind",
    "start": "417240",
    "end": "423310"
  },
  {
    "text": "of pain but now there is support for it I think yeah so G OPC was not a silver",
    "start": "423310",
    "end": "431650"
  },
  {
    "text": "bullet for us like they still cause a lot of problems so one was that we still have no idea what's going on inside the",
    "start": "431650",
    "end": "438400"
  },
  {
    "text": "system all like developers create new services every day like we're shipping",
    "start": "438400",
    "end": "443590"
  },
  {
    "text": "features every week like we still want to push new stuffs and kids and get them to test new stuff which means building",
    "start": "443590",
    "end": "449110"
  },
  {
    "text": "new features which means that the system grows out of hand pretty quickly and we have no idea what talks to what at any",
    "start": "449110",
    "end": "455529"
  },
  {
    "text": "point so that's kind of hard to understand and then we also have another problem with geography because it's",
    "start": "455529",
    "end": "461080"
  },
  {
    "text": "connection connection level load balancing we have a real problem then when we want to add more pods or",
    "start": "461080",
    "end": "469120"
  },
  {
    "text": "whatever to new services to scale them out because they're never going to get traffic because your graphical instance",
    "start": "469120",
    "end": "474879"
  },
  {
    "text": "in our case is just connected to that one GL PC pot and it's never going to change how do we load balance this we'd",
    "start": "474879",
    "end": "482050"
  },
  {
    "text": "get yeah how do we load balance this and this is why we then chose weekly so I'm",
    "start": "482050",
    "end": "490120"
  },
  {
    "text": "gonna hand over to Kevin now to talk a little more about linking cool yeah thanks Ben guess so let's just jump right into it",
    "start": "490120",
    "end": "497589"
  },
  {
    "text": "I'm gonna give a quick overview of what link it is and then how it can really help when you're using gyro PC let's see",
    "start": "497589",
    "end": "504550"
  },
  {
    "text": "here we go so yelling ready service mash to CN CF project it has sort of three",
    "start": "504550",
    "end": "512289"
  },
  {
    "text": "pillars that it really tries to help your application developers with one is observability one is reliability and the",
    "start": "512289",
    "end": "519399"
  },
  {
    "text": "last is security and I will talk a little bit about its architecture to see",
    "start": "519399",
    "end": "524680"
  },
  {
    "text": "how it accomplishes all three of those so we typically talk about the data plan in the control plane we're talking about",
    "start": "524680",
    "end": "530600"
  },
  {
    "text": "liquor D the data plane is sort of where your applications are running and we leverage the kubernetes pod model to add",
    "start": "530600",
    "end": "536420"
  },
  {
    "text": "a proxy in the pod where your application is running and then we reroute all the traffic to and from the",
    "start": "536420",
    "end": "543920"
  },
  {
    "text": "pod through the proxy and so and then at that point that gives us a lot of advanced features that we can use what's",
    "start": "543920",
    "end": "550790"
  },
  {
    "text": "the price of writing all traffic separately in a different namespace you'll be running a control plane and the control plane is sort of where all",
    "start": "550790",
    "end": "556490"
  },
  {
    "text": "the configuration and metrics collection is happening for the whole cluster and all the proxies are connected back to",
    "start": "556490",
    "end": "562880"
  },
  {
    "text": "the control plane vog RPC actually and so they are getting routing information",
    "start": "562880",
    "end": "568430"
  },
  {
    "text": "from the control plane on a RPC stream and then we have a Prometheus instance that's scraping all the proxies and pulling in all the request data so by",
    "start": "568430",
    "end": "574940"
  },
  {
    "text": "virtue of sending all your traffic through the proxies you get very good uniform visibility across all of your",
    "start": "574940",
    "end": "580880"
  },
  {
    "text": "applications now by default we will TLS all of the connections between proxies",
    "start": "580880",
    "end": "586250"
  },
  {
    "text": "as well and you can apply client policy like retries and timeouts that kind of",
    "start": "586250",
    "end": "591650"
  },
  {
    "text": "thing yeah so specifically with regards to G RPC you get a couple really great",
    "start": "591650",
    "end": "598820"
  },
  {
    "text": "things in the first is load balancing like been mentioned so since Jerry PC is",
    "start": "598820",
    "end": "604010"
  },
  {
    "text": "built on h-e-b 2 an issue Mewtwo is multiplexed basically all traffic will flow on one connection and so when you",
    "start": "604010",
    "end": "611440"
  },
  {
    "text": "connect to a back in with multiple replicas like basically as soon as the first connection is established that's",
    "start": "611440",
    "end": "616940"
  },
  {
    "text": "the connections that's used forever and requests aren't really balanced so you need an application aware load balancer",
    "start": "616940",
    "end": "622130"
  },
  {
    "text": "one that sort of knows the traffic that's flowing on the connections in order to fan out to all the backends and",
    "start": "622130",
    "end": "628640"
  },
  {
    "text": "so that's exactly what I think we do does in your d-does request level load balancing rather than connection level though balancing and in this way it can",
    "start": "628640",
    "end": "636110"
  },
  {
    "text": "really distribute the traffic across all of your backends evenly and also the",
    "start": "636110",
    "end": "641450"
  },
  {
    "text": "link it uses a pretty sophisticated load balancing algorithm based on perceived latency to all the backends so you'll",
    "start": "641450",
    "end": "647089"
  },
  {
    "text": "get very even distribution of requests as well",
    "start": "647089",
    "end": "652670"
  },
  {
    "text": "let's see and then like I said once all the traffic is flowing through the linker D proxies you get really good",
    "start": "652670",
    "end": "657720"
  },
  {
    "text": "observability and yeah and so we were talking about a lower-level proxy like a TCP proxy you might really only get sort",
    "start": "657720",
    "end": "665430"
  },
  {
    "text": "of bytes in and bytes out a number of connections but since we're introspecting the traffic on the connections we can give you a lot more",
    "start": "665430",
    "end": "671130"
  },
  {
    "text": "detail like number of requests and responses and with G RPC we can actually give you more structured data about the",
    "start": "671130",
    "end": "677610"
  },
  {
    "text": "individual endpoints so if you have a given G RPC back-end that has 10 different endpoints you can actually",
    "start": "677610",
    "end": "683970"
  },
  {
    "text": "just pipe that your protobuf file directly into linker D and then link or you can start reporting on all of the",
    "start": "683970",
    "end": "689190"
  },
  {
    "text": "endpoints for the service separately so you can see the success rate of like your get endpoints versus your create",
    "start": "689190",
    "end": "694500"
  },
  {
    "text": "endpoints and sometimes you know if the endpoints have really disparate levels of traffic a failure of one of the",
    "start": "694500",
    "end": "700470"
  },
  {
    "text": "endpoints might get lost in the noise but breaking it out by endpoint is like really a better way to slice and dice it",
    "start": "700470",
    "end": "705600"
  },
  {
    "text": "you know so you can see here this is linker D output but you would kind of be",
    "start": "705600",
    "end": "710700"
  },
  {
    "text": "able to see that in a single endpoint is failing whereas everything else looks really healthy let's see and then the",
    "start": "710700",
    "end": "716940"
  },
  {
    "text": "third thing that link or D can really help with overusing G RPC is this idea of centralized client configuration so G",
    "start": "716940",
    "end": "723690"
  },
  {
    "text": "RPC is has the same deficiency of every protocol and I would call it the service",
    "start": "723690",
    "end": "728910"
  },
  {
    "text": "owners dilemma and it's basically you don't really have control over who's calling your service so if I I'm a",
    "start": "728910",
    "end": "733980"
  },
  {
    "text": "service owner and you if it's a rest service or G or PC backend or anything the sort of callers of your service can",
    "start": "733980",
    "end": "740790"
  },
  {
    "text": "configure their clients and like really weird and wacky ways and that can cause problems for your service you can like",
    "start": "740790",
    "end": "746670"
  },
  {
    "text": "wake you up in the middle of night like when I was at Twitter I was working on the tweet service it turns out lots of",
    "start": "746670",
    "end": "752120"
  },
  {
    "text": "applications within Twitter's architecture need tweets and they would request them in very weird ways",
    "start": "752120",
    "end": "757200"
  },
  {
    "text": "sometimes with like really aggressive timeouts and really high retry budgets and that would cause like a cascade of",
    "start": "757200",
    "end": "763040"
  },
  {
    "text": "requests when we started to slow down and that was never good so with linger D",
    "start": "763040",
    "end": "768350"
  },
  {
    "text": "we have service profiles so you can basically create a it's a one-to-one mapping with kubernetes services you",
    "start": "768350",
    "end": "774210"
  },
  {
    "text": "create a profile for your community service and in the profile you can specify the client policy that you",
    "start": "774210",
    "end": "779610"
  },
  {
    "text": "expect callers to use and then the clients would sort of - for are creating their policy to link or T",
    "start": "779610",
    "end": "784769"
  },
  {
    "text": "and then Lincoln would use the correct policy so you can set timeout and a retry budget and that kind of thing",
    "start": "784769",
    "end": "791910"
  },
  {
    "text": "there and it sort of mitigates a lot of the more nuanced problems that you get",
    "start": "791910",
    "end": "796950"
  },
  {
    "text": "with like client configuration across multiple apps in a distributed system cool okay so I think we will do a quick",
    "start": "796950",
    "end": "805140"
  },
  {
    "text": "demo now of linker D and G RPC and then we'll have a little bit of time for questions cool ok so this demo is",
    "start": "805140",
    "end": "813029"
  },
  {
    "text": "available on my github if you want to see it it's pretty simple it's a we have a app that we created it's single",
    "start": "813029",
    "end": "821490"
  },
  {
    "text": "kubernetes manifest which I'll go through but basically it spawns four deployments one is a traffic generator",
    "start": "821490",
    "end": "827459"
  },
  {
    "text": "which sends HTTP 1.1 requests to a gateway application and then the Gateway",
    "start": "827459",
    "end": "832920"
  },
  {
    "text": "fans that out to two backends and one back-end is rest and one is G RPC and we",
    "start": "832920",
    "end": "839070"
  },
  {
    "text": "sort of leverage some phone open source projects to build it ok so let's see all",
    "start": "839070",
    "end": "844890"
  },
  {
    "text": "right so I have a kubernetes cluster running here and it is already running",
    "start": "844890",
    "end": "851660"
  },
  {
    "text": "linker D let me bump the font real quick yeah okay let me just make sure the",
    "start": "851660",
    "end": "860399"
  },
  {
    "text": "linker D control plane is up and running and it is a little bit cool alright so",
    "start": "860399",
    "end": "867810"
  },
  {
    "text": "in this structure I have an app file app demo and it really this is sort of should look like standard kubernetes",
    "start": "867810",
    "end": "873959"
  },
  {
    "text": "services and deployments are not really doing anything special but so here's the rest back in we have a service in front",
    "start": "873959",
    "end": "879540"
  },
  {
    "text": "of it and the deployment this deployment has an annotation on it this says link we do such inject enabled which means",
    "start": "879540",
    "end": "886260"
  },
  {
    "text": "that this deployment will pick up the proxy when we apply it to the kubernetes cluster same that's the rest back end",
    "start": "886260",
    "end": "892410"
  },
  {
    "text": "same goes for the G RPC back end and then the Gateway application and the",
    "start": "892410",
    "end": "897779"
  },
  {
    "text": "traffic application are not inject enabled so we're when we initially apply this configuration the two backends will",
    "start": "897779",
    "end": "903269"
  },
  {
    "text": "be injected with linker D the Gateway and the traffic service will not so let's go ahead and do that",
    "start": "903269",
    "end": "910730"
  },
  {
    "text": "it'll create all of our objects have a",
    "start": "913070",
    "end": "918480"
  },
  {
    "text": "look in the default clusters and these are initializing and I can actually run a link or D check - proxy which will",
    "start": "918480",
    "end": "925500"
  },
  {
    "text": "wait for everything to come up for me but basically the checks to make sure",
    "start": "925500",
    "end": "932550"
  },
  {
    "text": "all the pods are up and link ready uses a an admission controller mutating",
    "start": "932550",
    "end": "938700"
  },
  {
    "text": "webhook to actually do the proxy injection so it's watching all pod creates and when the pod with annotation",
    "start": "938700",
    "end": "943890"
  },
  {
    "text": "shows up it will add the proxy for you automatically and it should be all set okay so then we can start getting stats",
    "start": "943890",
    "end": "951030"
  },
  {
    "text": "from the injected deployments right away so we can run link read e-stat deploy",
    "start": "951030",
    "end": "958280"
  },
  {
    "text": "and yep and you can see that so the Gateway and the traffic service are unmatched in this you actually can bump",
    "start": "958280",
    "end": "964230"
  },
  {
    "text": "it just a little bit more even there we go yes a gateway and traffic are",
    "start": "964230",
    "end": "969870"
  },
  {
    "text": "unmatched but G RPC back-end and rest are meshed they're both at 100% success rate all these stats are coming from the",
    "start": "969870",
    "end": "976140"
  },
  {
    "text": "linker D proxy like the apps themselves are not instrumented to export any data but the proxies are routing the traffic",
    "start": "976140",
    "end": "981630"
  },
  {
    "text": "and so that's how we are able to collect it you can see and then you can see sort of golden metrics for all of the the",
    "start": "981630",
    "end": "987690"
  },
  {
    "text": "deployments you get their requests per second latency percentiles and the number of TCP connections and you can",
    "start": "987690",
    "end": "993330"
  },
  {
    "text": "see that the G RPC backend is much more efficient on TCP connections but if we",
    "start": "993330",
    "end": "999690"
  },
  {
    "text": "look at the individual pods in each of the backends Oh a little smaller yeah so",
    "start": "999690",
    "end": "1010460"
  },
  {
    "text": "this is this is exactly the load balancing problem that we were talking about right so the rest traffic is being sort of evenly distributed across all",
    "start": "1010460",
    "end": "1016700"
  },
  {
    "text": "three of the rest back ends but the G RPC back end is only a single replicas",
    "start": "1016700",
    "end": "1022430"
  },
  {
    "text": "receiving all the traffic and basically when when you use the default",
    "start": "1022430",
    "end": "1028668"
  },
  {
    "text": "load balancing strategy in kubernetes its connection level though balancing and you're gonna end up in this situation so there's a couple",
    "start": "1028669",
    "end": "1034938"
  },
  {
    "text": "workarounds actually there's another talk later today about gr PC load balancing too but I think liberty is a really elegant solution for this so we",
    "start": "1034939",
    "end": "1040938"
  },
  {
    "text": "just add linker D to the gateway deployment the client of the jaribg service that should start balancing",
    "start": "1040939",
    "end": "1046400"
  },
  {
    "text": "traffic right away so let's go ahead and do that actually I'll kook at will get deployed",
    "start": "1046400",
    "end": "1053230"
  },
  {
    "text": "gateway oh yeah mo and then I'll pipe that to link reading inject in order to",
    "start": "1053230",
    "end": "1059419"
  },
  {
    "text": "add the proxy annotation and then I will apply it back to my cluster there we go",
    "start": "1059419",
    "end": "1067040"
  },
  {
    "text": "and so now the deployment if we look at all the pods we'll see that there's a",
    "start": "1067040",
    "end": "1073790"
  },
  {
    "text": "new injected gateway pod coming up that has two containers in it and the old one",
    "start": "1073790",
    "end": "1080299"
  },
  {
    "text": "will be shut down momentarily and then right away I think we should be able to",
    "start": "1080299",
    "end": "1085940"
  },
  {
    "text": "start seeing the change in traffic I mean just shorten the time window in",
    "start": "1085940",
    "end": "1091130"
  },
  {
    "text": "this yeah there we go so then now the G RPC back-end is fully load balanced and",
    "start": "1091130",
    "end": "1097429"
  },
  {
    "text": "the traffic is is a lot there's a lot more even and still the number of TCP",
    "start": "1097429",
    "end": "1103130"
  },
  {
    "text": "sessions is really low for the GRP services so it's much more efficient there well and then we can sort of take",
    "start": "1103130",
    "end": "1111770"
  },
  {
    "text": "advantage of a bunch of other linker new features to help inspect the traffic's going on so we there's a feature called tap which will actually tap into one of",
    "start": "1111770",
    "end": "1119000"
  },
  {
    "text": "the proxies and get a sampling of the requests that it's routing and then that way you can get individual request data to sort of debug issues so we could say",
    "start": "1119000",
    "end": "1126110"
  },
  {
    "text": "tap one of the back-end deployments so link or DTaP deploy /e RPC back-end",
    "start": "1126110",
    "end": "1132380"
  },
  {
    "text": "this will dump a bunch of data but you can get this in different formats but basically you can see the source and",
    "start": "1132380",
    "end": "1138470"
  },
  {
    "text": "destination IP the method that's being called the endpoint that kind of thing",
    "start": "1138470",
    "end": "1145660"
  },
  {
    "text": "and then if so and there's a lot of other CLI functionality but I think he also has a dashboard that you can access",
    "start": "1145660",
    "end": "1153309"
  },
  {
    "text": "as well so let's pull that up and there was a lightning talk about the dashboard",
    "start": "1153309",
    "end": "1158640"
  },
  {
    "text": "on Monday I'll bump the phone up here a little bit yeah and basically the CLI is",
    "start": "1158640",
    "end": "1166179"
  },
  {
    "text": "fully featured has all the data you need the dashboard really lets us tell like a better narrative about like what's",
    "start": "1166179",
    "end": "1171549"
  },
  {
    "text": "happening within your services so it's the another useful view lately larger",
    "start": "1171549",
    "end": "1178500"
  },
  {
    "text": "cool so you know this is the sort of same thing we were seeing on the command line but if I click into one of these",
    "start": "1178500",
    "end": "1184659"
  },
  {
    "text": "then I can see kind of the up streams and downstream deployments and then all",
    "start": "1184659",
    "end": "1190210"
  },
  {
    "text": "the live request data right here you know we all make this is a sample Jerry co-op so we only have one endpoint that",
    "start": "1190210",
    "end": "1196289"
  },
  {
    "text": "kind of thing and then we also have built in graph and dashboards for all of the kubernetes pod owner types like",
    "start": "1196289",
    "end": "1203770"
  },
  {
    "text": "deployments staple sets replica sets that kind of thing so if I just click on this for final link that will bring up a",
    "start": "1203770",
    "end": "1212169"
  },
  {
    "text": "dashboard and it has sort of all the stats again good visibility one into an",
    "start": "1212169",
    "end": "1218230"
  },
  {
    "text": "appointment and then the other thing to note on this graph is that prior to injecting the gateway deployment the",
    "start": "1218230",
    "end": "1224559"
  },
  {
    "text": "traffic between the Gateway and the G RPC back end was not TLS and that's you",
    "start": "1224559",
    "end": "1229870"
  },
  {
    "text": "can sort of see it with the there's no lock symbol for the yellow line there but as soon as we injected the Gateway",
    "start": "1229870",
    "end": "1235600"
  },
  {
    "text": "when traffic resumed we ended up TLS in all the traffic on the connection to so without making any changes to the",
    "start": "1235600",
    "end": "1241450"
  },
  {
    "text": "application we're able to like encrypt that traffic as long as there's a proxy on either side of the client server",
    "start": "1241450",
    "end": "1247750"
  },
  {
    "text": "connection yeah and then I think that's",
    "start": "1247750",
    "end": "1253059"
  },
  {
    "text": "about it for the front end yeah I'm",
    "start": "1253059",
    "end": "1258730"
  },
  {
    "text": "gonna come back here nope here we go so",
    "start": "1258730",
    "end": "1264669"
  },
  {
    "text": "just a quick recap if you if you try the demo out it also there's",
    "start": "1264669",
    "end": "1271280"
  },
  {
    "text": "also service profiles in the demo I know it's it would take a while to to demo it",
    "start": "1271280",
    "end": "1277130"
  },
  {
    "text": "but you can actually use the protobuf definition to create a service profile for the GRP city back in and then start playing around with its client policy",
    "start": "1277130",
    "end": "1283310"
  },
  {
    "text": "and I recommend you try that oh yeah so that's about it for the MIDI",
    "start": "1283310",
    "end": "1288800"
  },
  {
    "text": "portion let me come here and I think we have a little bit of time for questions should",
    "start": "1288800",
    "end": "1294800"
  },
  {
    "text": "do that well I couldn't maybe run the mic around or Union okay thanks okay",
    "start": "1294800",
    "end": "1306530"
  },
  {
    "text": "who's first yeah from what I saw but the",
    "start": "1306530",
    "end": "1312740"
  },
  {
    "text": "stats you printed that was quite a surprise to see that the gr PC beckons had higher latencies than the rest back",
    "start": "1312740",
    "end": "1319550"
  },
  {
    "text": "ancestors yeah I actually was wondering about that too I think it's actually",
    "start": "1319550",
    "end": "1324680"
  },
  {
    "text": "more consequence of running my lap on my laptop I think it's sort of resource contention that's happening if I was",
    "start": "1324680",
    "end": "1330470"
  },
  {
    "text": "actually gonna like benchmark the two protocols I would do it on a dedicated cluster to get more accurate it's a good",
    "start": "1330470",
    "end": "1340880"
  },
  {
    "text": "question for Ben did you run any benchmarks on our PC yes we did but I",
    "start": "1340880",
    "end": "1346760"
  },
  {
    "text": "don't have the stats down I don't know but we found it way quicker though like gee obviously that the overhead so sorry",
    "start": "1346760",
    "end": "1352790"
  },
  {
    "text": "the overhead of like the connections for us were the biggest dream that we saw that wasn't just the latencies that was",
    "start": "1352790",
    "end": "1360110"
  },
  {
    "text": "slow it's like the entire even stuff that wasn't done over the wire right from the graphical point of view that",
    "start": "1360110",
    "end": "1365390"
  },
  {
    "text": "hole came to a halt to like the amount of memory that we the entire pod was using was ridiculous but then switches G obviously was much",
    "start": "1365390",
    "end": "1372080"
  },
  {
    "text": "quicker we did have a small issue with serials out serialization of the data",
    "start": "1372080",
    "end": "1377240"
  },
  {
    "text": "but I think they fix that in like a different version of GRDC next",
    "start": "1377240",
    "end": "1384850"
  },
  {
    "text": "yep thank you for the talk it's not out like Lincoln is adding a lot but you",
    "start": "1385350",
    "end": "1390610"
  },
  {
    "text": "said that yeah you had this problem with gr PC load balancing have you tried different client load balancing policies",
    "start": "1390610",
    "end": "1397660"
  },
  {
    "text": "that would even the load that's a second question what version of the linker G are you showing is it - I guess right",
    "start": "1397660",
    "end": "1404770"
  },
  {
    "text": "yes yes link ready - yep so for the first question was how we",
    "start": "1404770",
    "end": "1410890"
  },
  {
    "text": "tried clients sorry I'd load balancing I was not really aware of stuff at that",
    "start": "1410890",
    "end": "1416799"
  },
  {
    "text": "point with clients I love bouncing I know I met a few guys us neither doing a talk today about client side log balance",
    "start": "1416799",
    "end": "1422470"
  },
  {
    "text": "and in Cuba nineties I will be going to that by we didn't look at that no yep here's one over there okay so what did",
    "start": "1422470",
    "end": "1435010"
  },
  {
    "text": "you do to automatically switch the client application from connecting directly to connecting via the link ad",
    "start": "1435010",
    "end": "1440919"
  },
  {
    "text": "proxy ID how did it make you switch yeah so as part of the injection process we",
    "start": "1440919",
    "end": "1447760"
  },
  {
    "text": "add the proxy container into the pot spec we also add in an NIC container and the net container runs before the pod",
    "start": "1447760",
    "end": "1453419"
  },
  {
    "text": "initializes and it rewrites the IP tables rules of the pods so that all inbound and outbound TCP traffic go",
    "start": "1453419",
    "end": "1459700"
  },
  {
    "text": "through a port on the link or D router running in the pod there's one back",
    "start": "1459700",
    "end": "1465040"
  },
  {
    "text": "there",
    "start": "1465040",
    "end": "1467190"
  },
  {
    "text": "[Music] yeah two questions first do I need to",
    "start": "1471670",
    "end": "1481180"
  },
  {
    "text": "have an ingress in front of in front of in front of my service or not the second",
    "start": "1481180",
    "end": "1488320"
  },
  {
    "text": "one if I have my G RPC service and expose it on an odd part I want to do my",
    "start": "1488320",
    "end": "1495370"
  },
  {
    "text": "own load balancer I don't know a chai proxy or whatever can I target like as a proper load",
    "start": "1495370",
    "end": "1501010"
  },
  {
    "text": "balancer all the nodes where this Jerry PC is running so basically like does it",
    "start": "1501010",
    "end": "1506440"
  },
  {
    "text": "work through the ingress and does it work if I implement my own load answer as a certain node port sure yes",
    "start": "1506440",
    "end": "1516580"
  },
  {
    "text": "will it work if you're using ingress and will work if you have your own load",
    "start": "1516580",
    "end": "1521590"
  },
  {
    "text": "balancer and should link Rd does not provide an ingress so you would use one of the existing aggressive ticket sorry",
    "start": "1521590",
    "end": "1528190"
  },
  {
    "text": "oh yeah two mics yes there we go yeah so you would use one of the existing kubernetes",
    "start": "1528190",
    "end": "1533650"
  },
  {
    "text": "ingress controllers to get traffic into your cluster nginx works just fine and",
    "start": "1533650",
    "end": "1539560"
  },
  {
    "text": "then if you have your own load balancer it really I think the main takeaways",
    "start": "1539560",
    "end": "1545170"
  },
  {
    "text": "that you need request level load balancing for GR PC if you're doing connection level load balancing there's just not enough new connections created",
    "start": "1545170",
    "end": "1551490"
  },
  {
    "text": "to actually successfully balance across all your backends whew Mohit yeah maybe",
    "start": "1551490",
    "end": "1558820"
  },
  {
    "text": "if folks want to come up with questions - feel free I can keep running back hi",
    "start": "1558820",
    "end": "1564520"
  },
  {
    "text": "that and question around the request streams have you had any experience with leaving request streams or response",
    "start": "1564520",
    "end": "1570700"
  },
  {
    "text": "streams open for very long periods of time like hours or even days because I know you can just just do a simple request response but then you can also",
    "start": "1570700",
    "end": "1577480"
  },
  {
    "text": "reply with a stream can't you yeah that's right so yeah with your PC you can have long live streams and even",
    "start": "1577480",
    "end": "1583840"
  },
  {
    "text": "bi-directional streams as well and that all works just fine I mean link rity",
    "start": "1583840",
    "end": "1589330"
  },
  {
    "text": "will keep the connections open as long as they're in use did you all use any other streaming we",
    "start": "1589330",
    "end": "1594850"
  },
  {
    "text": "we didn't use any the stream stuff in jobs yeah but yeah they're really powerful and yeah I've seen mostly you",
    "start": "1594850",
    "end": "1603429"
  },
  {
    "text": "see I've seen is for like streaming data back you know from the client to the",
    "start": "1603429",
    "end": "1608559"
  },
  {
    "text": "server that kind of thing in it it's very seamless the other thing I like about GRP see it since I used it",
    "start": "1608559",
    "end": "1614500"
  },
  {
    "text": "protobuf you end up with language specific bindings so when you're working with the GF you see clients and servers",
    "start": "1614500",
    "end": "1620169"
  },
  {
    "text": "they feel really natural in whatever language you're working in so you know the go implementation or whatever",
    "start": "1620169",
    "end": "1626460"
  },
  {
    "text": "streaming is really straightforward to to work on thank you two questions about",
    "start": "1626460",
    "end": "1638230"
  },
  {
    "text": "self abilities so certainly architecture diagram that you have premier desistance which as same as the one is also telling",
    "start": "1638230",
    "end": "1643750"
  },
  {
    "text": "to the CLI latencies that all the other things would you suggest to run a",
    "start": "1643750",
    "end": "1649240"
  },
  {
    "text": "separate from existence and then pull metrics from that one or doing another way second questions you're pulling out",
    "start": "1649240",
    "end": "1656200"
  },
  {
    "text": "some requests as that anything we do with tracing and the sling caddy support tracing and now yep two questions so the",
    "start": "1656200",
    "end": "1663669"
  },
  {
    "text": "first one was about running your own observability stack my recommendation is",
    "start": "1663669",
    "end": "1669070"
  },
  {
    "text": "to scrape the link ready Prometheus you can use Prometheus Federation Meereenese",
    "start": "1669070",
    "end": "1674139"
  },
  {
    "text": "Prometheus is tuned for a very short live data because it's sort of for real-time Diagnostics so typically folks",
    "start": "1674139",
    "end": "1680769"
  },
  {
    "text": "in it it doesn't really won't take that my name resources in your cluster anyway so typically folks will just fetter a",
    "start": "1680769",
    "end": "1686379"
  },
  {
    "text": "temper Mathias or set up a script config to scrape all the proxies because the proxies are all labeled already for that",
    "start": "1686379",
    "end": "1693549"
  },
  {
    "text": "and then your second question was about tracing yeah so we have the tap feature to dump live requests we're also able to",
    "start": "1693549",
    "end": "1700450"
  },
  {
    "text": "draw service graphs using metrics data from Prometheus but we don't have support for tracing right now it's",
    "start": "1700450",
    "end": "1707289"
  },
  {
    "text": "definitely on the roadmap it's I think the thing that has been difficulty tracing evolution implemented is that it",
    "start": "1707289",
    "end": "1714100"
  },
  {
    "text": "requires participation from the applications themselves so not break the trace and one of the sort of ideals",
    "start": "1714100",
    "end": "1720490"
  },
  {
    "text": "behind service mesh is that the application should kind of not know that they are running Mestre environment right they should be",
    "start": "1720490",
    "end": "1725879"
  },
  {
    "text": "sort of kind of totally ignorant of the fact that the oldest magic is happening around them and I think tracing breaks",
    "start": "1725879",
    "end": "1731850"
  },
  {
    "text": "that model a little bit so we haven't quite figured out how to how to do it yeah hello a quick question about having",
    "start": "1731850",
    "end": "1746460"
  },
  {
    "text": "multiple services and kubernetes connecting to the same more clothes so now in your model you can have one",
    "start": "1746460",
    "end": "1752249"
  },
  {
    "text": "service going to say one deployment and I was wondering if you ran into that issue that's possible to have multiple",
    "start": "1752249",
    "end": "1757470"
  },
  {
    "text": "services pointing to the same multiple workloads and if a link Rd dashboard or the service graph you were talking about",
    "start": "1757470",
    "end": "1763529"
  },
  {
    "text": "handles that in a specific way and is that an issue even for you and not really no I mean I think it most of the",
    "start": "1763529",
    "end": "1770549"
  },
  {
    "text": "stats are that you see in the dashboard are reported from the pod the inbound link ID on the pod that's receiving the",
    "start": "1770549",
    "end": "1777210"
  },
  {
    "text": "traffic so the backend pod would just be reporting all the traffic source en regardless of which service is coming",
    "start": "1777210",
    "end": "1782610"
  },
  {
    "text": "from but you can also look at the outbound stats for the routers that are sending the traffic and then that way you could slice it up by those services",
    "start": "1782610",
    "end": "1788850"
  },
  {
    "text": "that they're going to yeah so I think it all just works in that in that way yeah",
    "start": "1788850",
    "end": "1796980"
  },
  {
    "text": "yeah yeah",
    "start": "1796980",
    "end": "1801980"
  },
  {
    "text": "thank you for the presentation my question is about the protocol buffer",
    "start": "1803820",
    "end": "1810559"
  },
  {
    "text": "publish mechanisms kind of things have you tried a variety flights like haha",
    "start": "1810559",
    "end": "1817169"
  },
  {
    "text": "single mono repo for all the proto buffers in your relation and try in this",
    "start": "1817169",
    "end": "1823739"
  },
  {
    "text": "way to share the bastions and how to manage diversions and kind of things can you explain a little more about dude",
    "start": "1823739",
    "end": "1830509"
  },
  {
    "text": "while you are doing the things okay so the probe of management is kind of a",
    "start": "1830509",
    "end": "1838139"
  },
  {
    "text": "facility Opik that everyone talks about um when think semana repos makes more sense I guess because everything's",
    "start": "1838139",
    "end": "1843809"
  },
  {
    "text": "contained in one repo but we never did that we start off with everything was in a lot of different repositories and that",
    "start": "1843809",
    "end": "1850950"
  },
  {
    "text": "brings a lot of problems with boiler play and everything also like create new repositories we never played about with",
    "start": "1850950",
    "end": "1856769"
  },
  {
    "text": "the mana reaper thing so I'm not sure if that's better or worse than the solution that we came up with like I said earlier our solution was to publish to the",
    "start": "1856769",
    "end": "1863580"
  },
  {
    "text": "native package manager this the reason",
    "start": "1863580",
    "end": "1869340"
  },
  {
    "text": "we put the probe off with the client the actual repository itself is because they were although they were in the same",
    "start": "1869340",
    "end": "1874590"
  },
  {
    "text": "repository they were deployed as two separate artifacts so you'd have the main app which was built at the docker container and then the actual probe of",
    "start": "1874590",
    "end": "1881190"
  },
  {
    "text": "schema was then built with the same version but then pushed to the package manager so they were still coupled with",
    "start": "1881190",
    "end": "1887279"
  },
  {
    "text": "the same version so you still had one version for both both artifacts but they were Sacre yeah I'm yeah I'm not sure if",
    "start": "1887279",
    "end": "1896070"
  },
  {
    "text": "that answers your question or not but all right yeah we didn't really look at mono repos so much yeah there's a yeah",
    "start": "1896070",
    "end": "1903419"
  },
  {
    "text": "there's a lot of oh yeah yeah it's a quite big pain point",
    "start": "1903419",
    "end": "1909000"
  },
  {
    "text": "yep cool thank you for mother goodie I thank you for coming",
    "start": "1909000",
    "end": "1914950"
  },
  {
    "text": "[Applause]",
    "start": "1914950",
    "end": "1918329"
  }
]