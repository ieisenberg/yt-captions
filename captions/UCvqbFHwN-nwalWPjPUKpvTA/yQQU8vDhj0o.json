[
  {
    "text": "hello welcome uh my name is Adam Berno i work at Google Cloud on the Google Cloud",
    "start": "160",
    "end": "6400"
  },
  {
    "text": "managed service for Prometheus uh hi my name is Brian Borum",
    "start": "6400",
    "end": "11519"
  },
  {
    "text": "uh I am a distinguished engineer at Graphana Labs uh my day job I work on",
    "start": "11519",
    "end": "18640"
  },
  {
    "text": "massively scalable storage for uh metrics logs traces and",
    "start": "18640",
    "end": "23720"
  },
  {
    "text": "profiles uh I'm also a Prometheus maintainer um so if you like that",
    "start": "23720",
    "end": "31439"
  },
  {
    "text": "yeah and uh feel free to uh follow us on our blue sky handles that are listed on",
    "start": "31439",
    "end": "36719"
  },
  {
    "text": "the screen uh we'd love to connect with you if you find this talk entertaining",
    "start": "36719",
    "end": "42399"
  },
  {
    "text": "uh we're going to be talking about how Damon sets run in Kubernetes and how to",
    "start": "42399",
    "end": "47520"
  },
  {
    "text": "use autoscaling to make them more efficient and more",
    "start": "47520",
    "end": "52600"
  },
  {
    "text": "effective um yeah so uh I show of hands",
    "start": "52600",
    "end": "57760"
  },
  {
    "text": "get get a bit of a audience participation going um how many people",
    "start": "57760",
    "end": "63199"
  },
  {
    "text": "have used a VPA a vertical pod autoscaler okay so that's about maybe",
    "start": "63199",
    "end": "71040"
  },
  {
    "text": "maybe a fifth of the audience something like that and um how many people have have used a demon",
    "start": "71040",
    "end": "77880"
  },
  {
    "text": "set oh nearly everyone um how many people have used a VPA with a demon set",
    "start": "77880",
    "end": "85360"
  },
  {
    "text": "yeah that guy well uh that's what our talk is",
    "start": "85360",
    "end": "92640"
  },
  {
    "text": "about um we uh we're going to kind of motivate the the problem that we see",
    "start": "92640",
    "end": "99280"
  },
  {
    "text": "with this uh and then talk about what we did to try and solve it and hopefully",
    "start": "99280",
    "end": "105680"
  },
  {
    "text": "give a demo and we're uh not we're the second last",
    "start": "105680",
    "end": "110799"
  },
  {
    "text": "thing between you and free beer so uh just you know chill stay with us and",
    "start": "110799",
    "end": "117280"
  },
  {
    "text": "we'll take you on a journey oh I Yeah picture okay demons demons demons are",
    "start": "117280",
    "end": "123040"
  },
  {
    "text": "not um coming from hell uh these uh what",
    "start": "123040",
    "end": "128560"
  },
  {
    "text": "we mean by by demons here is is a computer program that runs in the background um and apparently this comes",
    "start": "128560",
    "end": "135920"
  },
  {
    "text": "from from Greek mythology uh the idea of a a demon being a kind of generally helpful thing that that that hangs out",
    "start": "135920",
    "end": "142640"
  },
  {
    "text": "and uh does things for you um in Kubernetes terms a demon set will run",
    "start": "142640",
    "end": "150080"
  },
  {
    "text": "one pod on every node in your cluster and uh why would you want to do this",
    "start": "150080",
    "end": "155120"
  },
  {
    "text": "well something like a like a network controller that that needs to run on every node um that will run a demon set",
    "start": "155120",
    "end": "162239"
  },
  {
    "text": "uh logging demon um something collecting logs from every node that that will run",
    "start": "162239",
    "end": "167760"
  },
  {
    "text": "a metrics collection on every node is this kind of thing that we're talking about that that's what you would use a",
    "start": "167760",
    "end": "173200"
  },
  {
    "text": "demon set for and um they they need to run everywhere",
    "start": "173200",
    "end": "179040"
  },
  {
    "text": "yeah so just a little bit more level setting to before we get into the meat of the talk but uh let's talk about",
    "start": "179040",
    "end": "185920"
  },
  {
    "text": "resource requests for a second since we'll be talking about them throughout when you set up anything to run in",
    "start": "185920",
    "end": "192040"
  },
  {
    "text": "Kubernetes you can specify how much CPU and how much memory it will use uh the",
    "start": "192040",
    "end": "199159"
  },
  {
    "text": "Kubernetes looks across the available nodes and picks a node where your",
    "start": "199159",
    "end": "204640"
  },
  {
    "text": "workload will fit so these resource requests are important in theuler being",
    "start": "204640",
    "end": "210080"
  },
  {
    "text": "able to do its work effectively uh specifying requests is optional uh but it's highly recommended",
    "start": "210080",
    "end": "217440"
  },
  {
    "text": "as uh you'll realize as we talk through some of the implications in just a",
    "start": "217440",
    "end": "223360"
  },
  {
    "text": "moment so our fundamental problem is this",
    "start": "224280",
    "end": "230319"
  },
  {
    "text": "imagine you have a cluster where you have perhaps some small nodes and some big nodes and you're running a damon",
    "start": "230319",
    "end": "238200"
  },
  {
    "text": "set like a logging uh collector like a metrics collector something like that uh",
    "start": "238200",
    "end": "244159"
  },
  {
    "text": "where the work that your damon set is doing depends on the other work that's happening on the",
    "start": "244159",
    "end": "249799"
  },
  {
    "text": "node so you only get to pick one set of requests that will apply to all of the",
    "start": "249799",
    "end": "255280"
  },
  {
    "text": "pods under the damon set how do you decide the correct request to make before we answer that let's",
    "start": "255280",
    "end": "262479"
  },
  {
    "text": "consider the consequences if you're wrong uh if you pick a limit that's too",
    "start": "262479",
    "end": "268320"
  },
  {
    "text": "low the pod might be starved for resources or some of your pods or all of",
    "start": "268320",
    "end": "273360"
  },
  {
    "text": "your pods uh for CPU that means the workload gets throttled uh for memory uh",
    "start": "273360",
    "end": "281040"
  },
  {
    "text": "that means that the application can encounter an out of memory error and crash that's not",
    "start": "281040",
    "end": "287000"
  },
  {
    "text": "great uh if you pick a request that's too low with a high limit uh the",
    "start": "287000",
    "end": "292720"
  },
  {
    "text": "workload can misbehave or become a noisy neighbor and interfere with other workloads uh and here be dragons there's",
    "start": "292720",
    "end": "299680"
  },
  {
    "text": "kind of unpredictable behavior when you're requests don't match your actual usage uh you can also run out of",
    "start": "299680",
    "end": "306639"
  },
  {
    "text": "capacity at the container level or for the entire node in either way the consequences are",
    "start": "306639",
    "end": "314000"
  },
  {
    "text": "similar for Damon said in particular if your resource requests are much too high",
    "start": "314280",
    "end": "319680"
  },
  {
    "text": "some or all of your pods may become unscheduable so in this example uh you",
    "start": "319680",
    "end": "324720"
  },
  {
    "text": "might have a large node that wants more resources than some of the small nodes have to begin with uh so that obviously",
    "start": "324720",
    "end": "332160"
  },
  {
    "text": "won't work",
    "start": "332160",
    "end": "335479"
  },
  {
    "text": "um yeah so if you uh on the other side if you put the uh request much higher than you actually need uh then you",
    "start": "338320",
    "end": "345440"
  },
  {
    "text": "you're just you're kind of wasting that resource it it can't be scheduled for uh it can't be allocated for for other pods",
    "start": "345440",
    "end": "351919"
  },
  {
    "text": "on the system um so you have this terrible choice right if you go uh too",
    "start": "351919",
    "end": "358800"
  },
  {
    "text": "low you might get throttling or crashing if you go too high um you're you're wasting money",
    "start": "358800",
    "end": "365520"
  },
  {
    "text": "yeah in the particular service that I work on the Google Cloud managed service for Prometheus it runs with high",
    "start": "365520",
    "end": "370720"
  },
  {
    "text": "priority class because we want to prioritize collecting metrics uh and so",
    "start": "370720",
    "end": "376400"
  },
  {
    "text": "uh overprovisioning can also lead to user workloads being evicted uh so in",
    "start": "376400",
    "end": "382080"
  },
  {
    "text": "that case the um the Prometheus instance that's running will start to take up",
    "start": "382080",
    "end": "387600"
  },
  {
    "text": "more and more resources uh and push out the workloads that our customers actually care about",
    "start": "387600",
    "end": "394440"
  },
  {
    "text": "um and if this gets really bad you might not be able to schedule any user workloads on some nodes so obviously",
    "start": "394440",
    "end": "402080"
  },
  {
    "text": "that's not a good situation to be in you don't need to collect metrics from",
    "start": "402080",
    "end": "407280"
  },
  {
    "text": "nothing so I wanted to uh bring some real data uh this is a logging demon",
    "start": "407880",
    "end": "413840"
  },
  {
    "text": "running in in one of our dev dev clusters at Graphana Labs um and so these are the actual CPU usage",
    "start": "413840",
    "end": "422880"
  },
  {
    "text": "of a of a set of about 15 pods that are they're all the same but but some of",
    "start": "422880",
    "end": "428000"
  },
  {
    "text": "them are doing more more than more work than others so yeah the question is where do you set the request",
    "start": "428000",
    "end": "436599"
  },
  {
    "text": "so your first option is YOLO if you don't set resource requests at all uh",
    "start": "436880",
    "end": "442720"
  },
  {
    "text": "you're likely to have your workloads eventually misbehave because theuler is missing some of the information that it",
    "start": "442720",
    "end": "448240"
  },
  {
    "text": "needs to be able to schedule workloads appropriately it's very popular though",
    "start": "448240",
    "end": "453280"
  },
  {
    "text": "right it is very popular from what I understand um there are certain platforms like if you're running on GKE",
    "start": "453280",
    "end": "460720"
  },
  {
    "text": "autopilot for instance where the requests are actually required and it will set a default value for you uh if",
    "start": "460720",
    "end": "466560"
  },
  {
    "text": "you don't set them yourself so uh this is uh not even an option on certain",
    "start": "466560",
    "end": "474199"
  },
  {
    "text": "platforms another potential approach is a conservative approach uh where you set",
    "start": "474199",
    "end": "480240"
  },
  {
    "text": "your request high enough to cover every instance as I mentioned this can have",
    "start": "480240",
    "end": "486800"
  },
  {
    "text": "some problems if there's not enough available capacity uh but you're also wasting resources so in this example if",
    "start": "486800",
    "end": "493039"
  },
  {
    "text": "you set your request to the highest node you're going to have all of that empty",
    "start": "493039",
    "end": "499280"
  },
  {
    "text": "space under the purple line is going to be wasted capacity that you're paying for but you're not",
    "start": "499280",
    "end": "506520"
  },
  {
    "text": "using a third option uh is being very uh aggressive uh you can set your request",
    "start": "506520",
    "end": "513680"
  },
  {
    "text": "very low and allow bursting and hope that there's uh resource capacity on the",
    "start": "513680",
    "end": "518839"
  },
  {
    "text": "node in this case you end up with a different kind of problem that's similar",
    "start": "518839",
    "end": "524720"
  },
  {
    "text": "to the issues with the YOLO approach where you've set a request that is inadequate and you might not have space",
    "start": "524720",
    "end": "530800"
  },
  {
    "text": "reserved for the workload that you need a fourth approach is divide and",
    "start": "530800",
    "end": "538440"
  },
  {
    "text": "conquer uh you can divide a damon set into multiple demon sets and set",
    "start": "538440",
    "end": "543839"
  },
  {
    "text": "requests and limits for each subdammon set uh in this case you limit the blast",
    "start": "543839",
    "end": "551680"
  },
  {
    "text": "radius of your over and underprovisioning issues uh however with",
    "start": "551680",
    "end": "556800"
  },
  {
    "text": "each within each set of pods you still have some of the same",
    "start": "556800",
    "end": "562800"
  },
  {
    "text": "problems so uh in the example on the far right uh you're still seeing a lot of",
    "start": "562920",
    "end": "569040"
  },
  {
    "text": "wasted capacity within that subset for example uh also this becomes more work",
    "start": "569040",
    "end": "577320"
  },
  {
    "text": "intensive each subsequent update requires you to update multiple resources and you start undermining some",
    "start": "577320",
    "end": "583760"
  },
  {
    "text": "of the value of the damon set controller you have to make sure that your node selectors are appropriate and that you",
    "start": "583760",
    "end": "589839"
  },
  {
    "text": "don't have overlapping groups uh or that you're not missing things from your selection groups so it becomes much more",
    "start": "589839",
    "end": "596320"
  },
  {
    "text": "tedious to try to manage this but what if there was a way to",
    "start": "596320",
    "end": "601600"
  },
  {
    "text": "automate away your problems that is instead of trying to manage multiple Damon sets yourself what if we did the",
    "start": "601600",
    "end": "608560"
  },
  {
    "text": "hard work for you yeah so um let's uh get into a little",
    "start": "608560",
    "end": "616320"
  },
  {
    "text": "bit more detail the vertical pod autoscaler so so most of you did not put your hands up as as having seen this so",
    "start": "616320",
    "end": "623120"
  },
  {
    "text": "let's just talk about it a little bit uh so this is the the YAML to define a VPA",
    "start": "623120",
    "end": "629839"
  },
  {
    "text": "object um you basically point it at a set of pods in this case I've pointed it at a",
    "start": "629839",
    "end": "637360"
  },
  {
    "text": "demon set which is the the graphana logging demon that we're going to use in the demo um you can add more parameters",
    "start": "637360",
    "end": "646800"
  },
  {
    "text": "you can you can set a minimum and a maximum and you can set policies and things like that but that's that's the",
    "start": "646800",
    "end": "651839"
  },
  {
    "text": "basic idea you you define one VPA object um that that uh points at the pods you",
    "start": "651839",
    "end": "659360"
  },
  {
    "text": "want to control it then runs as three",
    "start": "659360",
    "end": "665240"
  },
  {
    "text": "processes uh so let's just walk through those um the top one is the recommener",
    "start": "665240",
    "end": "670959"
  },
  {
    "text": "uh so this watches the actual resource usage of your pods the ones you told it",
    "start": "670959",
    "end": "676320"
  },
  {
    "text": "to to control um that's defined by the VPA object and it also writes a",
    "start": "676320",
    "end": "682240"
  },
  {
    "text": "recommendation into the VPA object um the next one is the updater",
    "start": "682240",
    "end": "689839"
  },
  {
    "text": "who knows what the updater updates yeah it's a mystery isn't it it's a",
    "start": "689839",
    "end": "696320"
  },
  {
    "text": "trick question doesn't update anything um what the updater does is evicts a pod",
    "start": "696320",
    "end": "702800"
  },
  {
    "text": "that is out of range um and uh I didn't actually know",
    "start": "702800",
    "end": "708480"
  },
  {
    "text": "that until I started working on this project but here we are um so uh the",
    "start": "708480",
    "end": "714240"
  },
  {
    "text": "third process is an admission web hook and this is something that runs as the pod is created it it it gets called from",
    "start": "714240",
    "end": "721440"
  },
  {
    "text": "the Kubernetes API server so as the pod kind of flies past on its way to being",
    "start": "721440",
    "end": "727560"
  },
  {
    "text": "created the uh the VPA admission web hook applies the",
    "start": "727560",
    "end": "733480"
  },
  {
    "text": "recommendation so this is why evicting works that we um kick the pod off",
    "start": "733480",
    "end": "739440"
  },
  {
    "text": "wherever it's running modify its request and it it reappears uh so a little dance and uh",
    "start": "739440",
    "end": "746639"
  },
  {
    "text": "that that's how the current VP VPA works um so we'll get into uh how we modified",
    "start": "746639",
    "end": "756240"
  },
  {
    "text": "it oh yeah that's Yep uh so in uh in",
    "start": "756279",
    "end": "763040"
  },
  {
    "text": "some of these cases where uh one size does not fit all for your requests we propose that instead of applying a",
    "start": "763040",
    "end": "769200"
  },
  {
    "text": "single recommendation across the entire Damon set that we create a customized rep recommendation",
    "start": "769200",
    "end": "775600"
  },
  {
    "text": "uh for each pod scheduled on each node",
    "start": "775600",
    "end": "780839"
  },
  {
    "text": "so by adding a scope field to the vertical pod auto scaler spec you can",
    "start": "781440",
    "end": "787920"
  },
  {
    "text": "specify how you want the VPA to behave in this example we use the uh Kubernetes",
    "start": "787920",
    "end": "794240"
  },
  {
    "text": "host name to declare that it treats each pod with a unique host name effectively",
    "start": "794240",
    "end": "800480"
  },
  {
    "text": "the pod on each node uh as its own unique scope that gets its own",
    "start": "800480",
    "end": "805560"
  },
  {
    "text": "recommendation based on only the history from that host so I'll turn it over to Brian to show",
    "start": "805560",
    "end": "811600"
  },
  {
    "text": "you how this prototype works yeah so we have um a demo environment which is a",
    "start": "811600",
    "end": "817279"
  },
  {
    "text": "three node cluster it's running in kind at least I hope it's running uh we'll",
    "start": "817279",
    "end": "823279"
  },
  {
    "text": "find out in a second um so we have a we have a three node cluster and we have",
    "start": "823279",
    "end": "828399"
  },
  {
    "text": "monitoring stack uh and I'm I'm using Graphfana Cloud for that um so we're",
    "start": "828399",
    "end": "833519"
  },
  {
    "text": "sending um metrics and and logs and so on and we're actually going to use the",
    "start": "833519",
    "end": "838639"
  },
  {
    "text": "logging demon as our um guinea pig for the test so let's see if I can make the demo",
    "start": "838639",
    "end": "847160"
  },
  {
    "text": "appear uh",
    "start": "847160",
    "end": "854120"
  },
  {
    "text": "okay [Music]",
    "start": "854120",
    "end": "859909"
  },
  {
    "text": "uh okay there's one okay",
    "start": "859959",
    "end": "866959"
  },
  {
    "text": "not too bad not too bad i I I should just stop the demo there right not uh not temp fate um how",
    "start": "866959",
    "end": "876160"
  },
  {
    "text": "are we doing for time actually uh pretty good pretty good okay is um so let me",
    "start": "876160",
    "end": "883519"
  },
  {
    "text": "see if I can run commands yeah okay there's my three node",
    "start": "883519",
    "end": "890800"
  },
  {
    "text": "cluster um so we are looking at uh the",
    "start": "890800",
    "end": "896160"
  },
  {
    "text": "uh CPU usage is the the thin line and the uh request is the thick",
    "start": "896160",
    "end": "903720"
  },
  {
    "text": "line um and it's it's sitting uh it's not doing much right now right because",
    "start": "903720",
    "end": "910240"
  },
  {
    "text": "I'm I'm it's just a kind of empty cluster i could uh I could show you",
    "start": "910240",
    "end": "916480"
  },
  {
    "text": "that um so basically the uh the only things that are running are the cube system",
    "start": "919959",
    "end": "926320"
  },
  {
    "text": "pods and the uh observability demons",
    "start": "926320",
    "end": "931639"
  },
  {
    "text": "um so uh I'm going to introduce some load i'm going to run",
    "start": "931639",
    "end": "938839"
  },
  {
    "text": "um a pod that generates some",
    "start": "938839",
    "end": "943519"
  },
  {
    "text": "logs now well let's just uh see if that's",
    "start": "944279",
    "end": "950000"
  },
  {
    "text": "[Music] running what happened oh minus F thank",
    "start": "951120",
    "end": "957720"
  },
  {
    "text": "you everyone's screaming at me minus F minus F minus F",
    "start": "957720",
    "end": "963680"
  },
  {
    "text": "uh shows you it's a live demo though huh um okay so um the uh the VPA by default",
    "start": "963680",
    "end": "974079"
  },
  {
    "text": "um might take uh a couple of hours to kind of think about its recommendation",
    "start": "974079",
    "end": "980320"
  },
  {
    "text": "and uh apply it and so forth and we we don't have that long in the system in in",
    "start": "980320",
    "end": "985360"
  },
  {
    "text": "the in the talk so um I have u modified the settings of the VPA uh so that it",
    "start": "985360",
    "end": "993360"
  },
  {
    "text": "it's it's going to react a lot faster so there's there's the load coming in right you see that I I'm only running the load",
    "start": "993360",
    "end": "999759"
  },
  {
    "text": "on the right hand node um and it's generating logs so the logging demon is having to do more",
    "start": "999759",
    "end": "1007240"
  },
  {
    "text": "work so um what I'm hoping is there you go so the VPA has recognized that the",
    "start": "1007240",
    "end": "1015199"
  },
  {
    "text": "load on that pod was higher uh and has evicted it um and has uh applied a new",
    "start": "1015199",
    "end": "1022639"
  },
  {
    "text": "recommendation which is higher uh while not changing anything on the",
    "start": "1022639",
    "end": "1028640"
  },
  {
    "text": "other two nodes",
    "start": "1028640",
    "end": "1031959"
  },
  {
    "text": "[Applause]",
    "start": "1034430",
    "end": "1040029"
  },
  {
    "text": "there we go so yeah there's there's like a burst of CPU as the as the thing takes off but uh but it it it it'll hopefully",
    "start": "1041679",
    "end": "1049440"
  },
  {
    "text": "stabilize about there um for long enough for me to get the slides back up anyway",
    "start": "1049440",
    "end": "1056360"
  },
  {
    "text": "okay uh where are the slides",
    "start": "1056360",
    "end": "1061960"
  },
  {
    "text": "okay is that going to work",
    "start": "1066120",
    "end": "1070559"
  },
  {
    "text": "no yes maybe whoa okay h",
    "start": "1071280",
    "end": "1077679"
  },
  {
    "text": "Yeah just like talk amongst yourselves",
    "start": "1077679",
    "end": "1083679"
  },
  {
    "text": "uh right there we go",
    "start": "1086039",
    "end": "1092080"
  },
  {
    "text": "okay thank you um right so we we um we demoed uh",
    "start": "1092200",
    "end": "1099760"
  },
  {
    "text": "modified VPA um and uh thank you uh we we demoed the",
    "start": "1099760",
    "end": "1107520"
  },
  {
    "text": "modified VPA we demoed it reacting to increased CPU usage on a per node basis",
    "start": "1107520",
    "end": "1113120"
  },
  {
    "text": "and that is with this field in the VPA called uh scope so I guess you're you're",
    "start": "1113120",
    "end": "1118960"
  },
  {
    "text": "wondering how can I get my hands on this of course so uh currently I have a",
    "start": "1118960",
    "end": "1125919"
  },
  {
    "text": "autoscaling enhancement proposal that's open and it's uh under active review uh",
    "start": "1125919",
    "end": "1132160"
  },
  {
    "text": "and the code that you just saw for the demo uh is linked within the proposal",
    "start": "1132160",
    "end": "1137679"
  },
  {
    "text": "and we'll give you a direct uh issue or a PR reference at the end of the talk",
    "start": "1137679",
    "end": "1143600"
  },
  {
    "text": "here uh so there's a few phases before this will roll out into general",
    "start": "1143600",
    "end": "1149080"
  },
  {
    "text": "availability but uh the next step will be the approval of the proposal after we get all the feedback we need and it's",
    "start": "1149080",
    "end": "1156000"
  },
  {
    "text": "reviewed by uh the sig autoscaling and the folks that are uh reviewing the proposal um hopefully that will be",
    "start": "1156000",
    "end": "1163600"
  },
  {
    "text": "approved uh we'll begin to develop a more robust implementation and uh work",
    "start": "1163600",
    "end": "1169280"
  },
  {
    "text": "on solving some uh additional problems related to uh scaling and production readiness to make sure that this will",
    "start": "1169280",
    "end": "1176400"
  },
  {
    "text": "work effectively uh anywhere it might be deployed so with this feature we hope",
    "start": "1176400",
    "end": "1184000"
  },
  {
    "text": "that we'll both reduce wasted resources and cost uh due to overprovisioning that",
    "start": "1184000",
    "end": "1190559"
  },
  {
    "text": "we'll be able to mitigate stability problems associated with bursting and uh we would love for you to get involved uh",
    "start": "1190559",
    "end": "1197600"
  },
  {
    "text": "you can find the proposal uh on the Kubernetes autoscaler repo",
    "start": "1197600",
    "end": "1202720"
  },
  {
    "text": "that's pull request 7942 and the uh prototype code is also",
    "start": "1202720",
    "end": "1208960"
  },
  {
    "text": "up there it's PR number 7978 uh and you're welcome to visit my",
    "start": "1208960",
    "end": "1214720"
  },
  {
    "text": "blog as well where I have a short write up of the problem that uh we're trying to",
    "start": "1214720",
    "end": "1220280"
  },
  {
    "text": "solve with that we'll turn it over to questions [Applause]",
    "start": "1220280",
    "end": "1230039"
  },
  {
    "text": "thanks please approach the mic if you have questions uh yeah microphone is kind of front and center",
    "start": "1230039",
    "end": "1238280"
  },
  {
    "text": "hi uh you showed the demo and in that we see a spike of CPU when the part re uh",
    "start": "1242640",
    "end": "1250400"
  },
  {
    "text": "the new part is created right when it was restarted it spiked yeah and that that's that's basically its",
    "start": "1250400",
    "end": "1256080"
  },
  {
    "text": "initialization and it it uh because it's a logging demon it kind of rereads the logs to catch up to where it was before",
    "start": "1256080",
    "end": "1263039"
  },
  {
    "text": "exactly so how does it know that that's just a spike of the restart of of like it catching up and not like a general",
    "start": "1263039",
    "end": "1270400"
  },
  {
    "text": "load right yeah well so so yeah it's a great question so the um the VPA uh",
    "start": "1270400",
    "end": "1276360"
  },
  {
    "text": "already the way it works already is it's capturing a histogram of of CPU usage",
    "start": "1276360",
    "end": "1282720"
  },
  {
    "text": "and it is applying a decaying uh waiting to those to those those samples and uh",
    "start": "1282720",
    "end": "1291360"
  },
  {
    "text": "then it takes a 90th percentile so basically you would expect under normal",
    "start": "1291360",
    "end": "1296880"
  },
  {
    "text": "circumstances it would ignore a quick spike and is there like a configuration so you",
    "start": "1296880",
    "end": "1303600"
  },
  {
    "text": "could actually change it for specific types so for example my port starts up",
    "start": "1303600",
    "end": "1308799"
  },
  {
    "text": "with like extra load for some reason right um there's a lot of configuration on the",
    "start": "1308799",
    "end": "1315919"
  },
  {
    "text": "VPA i'm not sure that you can do much about I mean it it it you can configure",
    "start": "1315919",
    "end": "1321280"
  },
  {
    "text": "how long the exponential decay is and and things like that but that's the basic behavior of it so if you if you",
    "start": "1321280",
    "end": "1327840"
  },
  {
    "text": "want a a radically different behavior uh you you might need to find uh some other project it will depend a lot on your",
    "start": "1327840",
    "end": "1334720"
  },
  {
    "text": "workload as well if you know that there's going to be an initial spike and then it will settle in to a different value you might want it to react more",
    "start": "1334720",
    "end": "1342720"
  },
  {
    "text": "slowly uh if you know that initially it's going to keep handling the amount of load that it is uh and that that that",
    "start": "1342720",
    "end": "1350080"
  },
  {
    "text": "is immediately your steady state you might tune different options yeah thank you you're welcome",
    "start": "1350080",
    "end": "1358279"
  },
  {
    "text": "hey um so what happens if you are approaching um GitHubs meaning for",
    "start": "1358960",
    "end": "1366159"
  },
  {
    "text": "example I have Algo CD and he's it's always monitoring my cluster and PPA",
    "start": "1366159",
    "end": "1371919"
  },
  {
    "text": "comes in and change some of the resources which is not in my git repository argo will just go and run it",
    "start": "1371919",
    "end": "1379919"
  },
  {
    "text": "over so do you want to take that one yeah yeah so uh the way that the vertical pod autoscaler works it's not",
    "start": "1379919",
    "end": "1387280"
  },
  {
    "text": "actually changing your initial resource so in the example of a damon set uh it",
    "start": "1387280",
    "end": "1393919"
  },
  {
    "text": "doesn't change the spec of your damon set it just modifies the pod at the very",
    "start": "1393919",
    "end": "1399280"
  },
  {
    "text": "last moment with the admission web hook so uh the damon set is responsible for",
    "start": "1399280",
    "end": "1404880"
  },
  {
    "text": "creating a pod uh and just before it creates the pod it asks the VPA oh by",
    "start": "1404880",
    "end": "1411520"
  },
  {
    "text": "the way how much CPU should this have how much memory it has so you would not",
    "start": "1411520",
    "end": "1416960"
  },
  {
    "text": "expect to schedule individual pods under a Damon set in your GitOps if that makes",
    "start": "1416960",
    "end": "1422720"
  },
  {
    "text": "sense so essentially it's just like a quick solution for a local problem",
    "start": "1422720",
    "end": "1430159"
  },
  {
    "text": "quick solution for a local problem i'm not sure I understand meaning so if if you if you want the changes to be",
    "start": "1430159",
    "end": "1436159"
  },
  {
    "text": "written into git then this is not a solution uh the the way the VPA works is",
    "start": "1436159",
    "end": "1441600"
  },
  {
    "text": "it is it modifies the um pod uh requests",
    "start": "1441600",
    "end": "1447440"
  },
  {
    "text": "on the on the fly using an admission web hook that's that's the way VPA works",
    "start": "1447440",
    "end": "1452880"
  },
  {
    "text": "okay and second question does it take into consideration cube reserved and system reserved",
    "start": "1452880",
    "end": "1459760"
  },
  {
    "text": "does it take uh Kubernetes takes those into effect",
    "start": "1459760",
    "end": "1465039"
  },
  {
    "text": "the the the VPA is is is obeying uh however you configure it so so you say",
    "start": "1465039",
    "end": "1470799"
  },
  {
    "text": "watch these pods and make a recommendation within these bounds you know using this policy it that that's um",
    "start": "1470799",
    "end": "1479600"
  },
  {
    "text": "it yeah it doesn't really care about the the system request and and anything like that those are those are handled by",
    "start": "1479600",
    "end": "1485760"
  },
  {
    "text": "Cublet thank you hi um first of all great feature um",
    "start": "1485760",
    "end": "1493679"
  },
  {
    "text": "thank you question is when you've been developing that and playing with different demon",
    "start": "1493679",
    "end": "1500279"
  },
  {
    "text": "sets have you encountered a problem where",
    "start": "1500279",
    "end": "1505559"
  },
  {
    "text": "your default memory CPU or mainly memory",
    "start": "1505559",
    "end": "1510679"
  },
  {
    "text": "request was too low for some big nodes and the VPA was not able to catch up and",
    "start": "1510679",
    "end": "1519760"
  },
  {
    "text": "recommend a new memory because it was constantly crash looping yeah so the the VPA",
    "start": "1519760",
    "end": "1528480"
  },
  {
    "text": "doesn't really care about the initial request uh once it makes a recommendation it'll do so because it",
    "start": "1528480",
    "end": "1534400"
  },
  {
    "text": "has a confidence interval that it feels good about it those some of those options are tunable to some extent uh",
    "start": "1534400",
    "end": "1541840"
  },
  {
    "text": "but if you're just using out of the box VPA it's going to be fairly conservative",
    "start": "1541840",
    "end": "1547120"
  },
  {
    "text": "and it's going to try not to uh cycle your pods more than it has to but",
    "start": "1547120",
    "end": "1554320"
  },
  {
    "text": "So there there's one exception if if you're actually getting uh out of memory crashes it it it increases more all",
    "start": "1554320",
    "end": "1561600"
  },
  {
    "text": "right so it the special provision in the code to watch for for events okay that",
    "start": "1561600",
    "end": "1566880"
  },
  {
    "text": "that answered my question and all all of that's in the VPA already uh so yeah",
    "start": "1566880",
    "end": "1572400"
  },
  {
    "text": "that's not a new feature it's existing functionality that will be included no yep cool thanks",
    "start": "1572400",
    "end": "1580919"
  },
  {
    "text": "hi hello in your demo how the how the new logging pod knew where to catch up",
    "start": "1580960",
    "end": "1588400"
  },
  {
    "text": "on logs uh compared to the previous one that got killed how did it know to catch",
    "start": "1588400",
    "end": "1593919"
  },
  {
    "text": "up on the logs uh I mean that's that's what logging demons do it it's um uh it",
    "start": "1593919",
    "end": "1601840"
  },
  {
    "text": "writes out a snapshot periodically of where it got to well let's say it got killed uh writing log a yeah how does it",
    "start": "1601840",
    "end": "1609760"
  },
  {
    "text": "know that when it restarts it still need to write log B and not log A again",
    "start": "1609760",
    "end": "1615200"
  },
  {
    "text": "uh yeah it it it snapshots that information to disk so that when the when it",
    "start": "1615200",
    "end": "1621360"
  },
  {
    "text": "restarts it can pick that up okay so you you kind of cache that information on the node itself on the node yeah",
    "start": "1621360",
    "end": "1629039"
  },
  {
    "text": "yeah it's um and and also I think we we assume that we can resend some logs",
    "start": "1629039",
    "end": "1635919"
  },
  {
    "text": "uh that it's item potent um yeah but I mean that's that's just you know that",
    "start": "1635919",
    "end": "1643440"
  },
  {
    "text": "whatever logging demon you use it it it it will have some other behavior this is this is not really a talk about logging",
    "start": "1643440",
    "end": "1649440"
  },
  {
    "text": "demons this we just used it as a demo yeah yeah of course in the more general behavior uh with any workload where you",
    "start": "1649440",
    "end": "1657279"
  },
  {
    "text": "expect to cycle the pods you want them to be resilient to that um and there's",
    "start": "1657279",
    "end": "1662559"
  },
  {
    "text": "some very exciting work going on with in place pod resizes uh that will make it",
    "start": "1662559",
    "end": "1668400"
  },
  {
    "text": "so you don't necessarily have to kill the pod every time you can resize it without restarting it which interesting",
    "start": "1668400",
    "end": "1674640"
  },
  {
    "text": "for cube proxy for example if you want to apply that to critical demon sets that you can't afford to lose or Yeah",
    "start": "1674640",
    "end": "1681200"
  },
  {
    "text": "yeah cool thank you",
    "start": "1681200",
    "end": "1685720"
  },
  {
    "text": "hi thanks for the talk uh I have a like one one question on the uh the mechanism",
    "start": "1686760",
    "end": "1695440"
  },
  {
    "text": "you're proposing here i I think that works pretty nicely in a static cluster where all the nodes already exist do you",
    "start": "1695440",
    "end": "1702960"
  },
  {
    "text": "think about how will it work when nodes are not yet there and so there is no VP",
    "start": "1702960",
    "end": "1708000"
  },
  {
    "text": "recommendation and the the thing that is scaling a cluster is to predict uh the",
    "start": "1708000",
    "end": "1713520"
  },
  {
    "text": "resources that that will be used by a demon set so yeah how how will that work",
    "start": "1713520",
    "end": "1719600"
  },
  {
    "text": "this is this is one of the issues that's being actively discussed on the proposal um there's a few different ideas about",
    "start": "1719600",
    "end": "1726159"
  },
  {
    "text": "how initial recommendations should be made um and so we're definitely work on",
    "start": "1726159",
    "end": "1732320"
  },
  {
    "text": "working on sorting that out um I think that in general uh the expectation",
    "start": "1732320",
    "end": "1739760"
  },
  {
    "text": "should be that uh once the vertical pod autoscaler makes a recommendation in",
    "start": "1739760",
    "end": "1745679"
  },
  {
    "text": "this per node model that uh you'll be getting the behavior that you want at",
    "start": "1745679",
    "end": "1751919"
  },
  {
    "text": "that point so yeah there there are some open questions that people are debating about uh what you do between the initial",
    "start": "1751919",
    "end": "1759559"
  },
  {
    "text": "recommendation or the initial request and the first recommendation",
    "start": "1759559",
    "end": "1766880"
  },
  {
    "text": "okay thank you hi you showed the host name uh scope but",
    "start": "1766880",
    "end": "1775760"
  },
  {
    "text": "are there any plans to uh get more scopes available yeah I guess I I put",
    "start": "1775760",
    "end": "1782320"
  },
  {
    "text": "that on screen as a as a kind of a a hint um I uh so scope",
    "start": "1782320",
    "end": "1791480"
  },
  {
    "text": "um scope equals instance type uh so if your if your problem is basically that",
    "start": "1791480",
    "end": "1797840"
  },
  {
    "text": "that bigger nodes use more um resource then instance type looks",
    "start": "1797840",
    "end": "1804480"
  },
  {
    "text": "like a a more more efficient more attractive target um it just it so happened that this is uh host is the one",
    "start": "1804480",
    "end": "1812480"
  },
  {
    "text": "that's in the the demo in the prototype um uh instance type sounds attractive",
    "start": "1812480",
    "end": "1818399"
  },
  {
    "text": "but it's actually harder to implement um node pool is another one that's been",
    "start": "1818399",
    "end": "1825320"
  },
  {
    "text": "suggested so scope equals node pool just to clarify too the scope is",
    "start": "1825320",
    "end": "1831120"
  },
  {
    "text": "intentionally extensible uh the current proposal just includes the host name so",
    "start": "1831120",
    "end": "1836559"
  },
  {
    "text": "uh yeah that's certainly an area where additional proposals could be added uh we may get feedback during the proposal",
    "start": "1836559",
    "end": "1843520"
  },
  {
    "text": "cycle that we want to include things initially but yeah we're definitely thinking about that and open to it",
    "start": "1843520",
    "end": "1849679"
  },
  {
    "text": "different subsets of scaling actually one of one of my colleagues wants uh wants to use this on stateful sets but",
    "start": "1849679",
    "end": "1856799"
  },
  {
    "text": "Adam's going to kill me if I say that so um I didn't say that cool thanks",
    "start": "1856799",
    "end": "1865159"
  },
  {
    "text": "hey uh have you guys thought about how this would play with a tool like",
    "start": "1865520",
    "end": "1870559"
  },
  {
    "text": "Carpenter or if you guys have any opinions on this in terms of it trying to carpenter trying to right size your",
    "start": "1870559",
    "end": "1876880"
  },
  {
    "text": "nodes and then this trying to right size to those nodes would these contend yeah so um I don't know a lot about",
    "start": "1876880",
    "end": "1885760"
  },
  {
    "text": "carpenter specifically uh but in Google cloud I know that we have many",
    "start": "1885760",
    "end": "1890960"
  },
  {
    "text": "autoscaling options uh for like cluster autoscaler and all those so we have some",
    "start": "1890960",
    "end": "1896480"
  },
  {
    "text": "of the same concerns um yes there we are thinking about that",
    "start": "1896480",
    "end": "1902640"
  },
  {
    "text": "um and trying to uh make sure that they will behave in alignment with the",
    "start": "1902640",
    "end": "1909679"
  },
  {
    "text": "general expectations of the users so yeah uh there's a lot of complicated",
    "start": "1909679",
    "end": "1914720"
  },
  {
    "text": "issues that you get into when you're trying to do like multi-dimensional scaling in several ways uh and we've run",
    "start": "1914720",
    "end": "1921360"
  },
  {
    "text": "into some of those already with uh without the prototype so uh an",
    "start": "1921360",
    "end": "1926640"
  },
  {
    "text": "interesting space and uh if you have comments please get on the proposal and",
    "start": "1926640",
    "end": "1931679"
  },
  {
    "text": "let us know cool thanks uh hi you mentioned at the beginning of",
    "start": "1931679",
    "end": "1938320"
  },
  {
    "text": "the demo BPA normally runs over you know a long period of time like I think you said hours and that you lowered it down",
    "start": "1938320",
    "end": "1944320"
  },
  {
    "text": "but for these uh dammons like a lot of their load is typically driven by what happens to land on the machine so",
    "start": "1944320",
    "end": "1950399"
  },
  {
    "text": "suddenly some pod you know a lot of logging or a lot of like you know proxied request to a proxy you know 2x's",
    "start": "1950399",
    "end": "1956399"
  },
  {
    "text": "capacity and so I guess my assumption would be you generally want to run BPA for dammons at a faster rate and I guess",
    "start": "1956399",
    "end": "1962159"
  },
  {
    "text": "I'm wondering like why wouldn't that be the default or like what are the risks of doing that so there's a few",
    "start": "1962159",
    "end": "1968000"
  },
  {
    "text": "mechanisms that you can use to respond more quickly so um one is that if you're",
    "start": "1968000",
    "end": "1974960"
  },
  {
    "text": "anticipating a memory spike uh and you hit your request and limits are close uh",
    "start": "1974960",
    "end": "1981440"
  },
  {
    "text": "you might hit your memory limit and you get an out of me memory error and then it'll apply a new recommendation quickly",
    "start": "1981440",
    "end": "1989279"
  },
  {
    "text": "after the pod crashes uh in CPU you can um again kind of toy with your requests",
    "start": "1989279",
    "end": "1998799"
  },
  {
    "text": "uh and your limits to know what you want what behavior you want um and then of",
    "start": "1998799",
    "end": "2006799"
  },
  {
    "text": "course you can also tune the recommener to behave differently or respond more quickly i think in general there's a lot",
    "start": "2006799",
    "end": "2014320"
  },
  {
    "text": "of uh work in the space right now with the in place pod resize that will change",
    "start": "2014320",
    "end": "2020320"
  },
  {
    "text": "some of the assumptions about how quickly we want to respond in general because up to this point the assumption",
    "start": "2020320",
    "end": "2027200"
  },
  {
    "text": "has been to make a new uh request to make a set a new limit you need to kill",
    "start": "2027200",
    "end": "2034000"
  },
  {
    "text": "the pod and that we shouldn't just kill pods for fun I was also just curious like are",
    "start": "2034000",
    "end": "2039519"
  },
  {
    "text": "there internal scaling limits if you start driving BPA recommendation more quickly that you'd have to be concerned",
    "start": "2039519",
    "end": "2044960"
  },
  {
    "text": "about or is it primarily around more of the mental model for you know Yeah so I",
    "start": "2044960",
    "end": "2050320"
  },
  {
    "text": "would I would be worried about the amount of resource the VPA itself would start to take if if you um if you if you",
    "start": "2050320",
    "end": "2057520"
  },
  {
    "text": "tried to drive it really quickly and and make it you know really clever um yeah",
    "start": "2057520",
    "end": "2063679"
  },
  {
    "text": "it's pulling information from the metric server from from meththeus and so it",
    "start": "2063679",
    "end": "2068800"
  },
  {
    "text": "does have to gather the usage regularly so if you start to tune that really fast",
    "start": "2068800",
    "end": "2074720"
  },
  {
    "text": "you're going to run into problems but it's it's an interesting space i mean you know we're we're here to kind of",
    "start": "2074720",
    "end": "2080638"
  },
  {
    "text": "talk about about future directions and experimentation and so on we're we're we're uh we're not claiming to have",
    "start": "2080639",
    "end": "2086480"
  },
  {
    "text": "solved all the problems so well just a specific one",
    "start": "2086480",
    "end": "2093398"
  },
  {
    "text": "and yeah and thanks for this actually because it's a big issue where I work at",
    "start": "2094240",
    "end": "2100040"
  },
  {
    "text": "um so um my own case was more on the scope field like is there like a plan to",
    "start": "2100040",
    "end": "2107000"
  },
  {
    "text": "extend like it from being just like a string to being like the anti-affinity kind of definition like",
    "start": "2107000",
    "end": "2113880"
  },
  {
    "text": "more like broken down in that sense because there are some like high priority notes you might just want to skip doing this on um yeah the there's",
    "start": "2113880",
    "end": "2122880"
  },
  {
    "text": "definitely opportunities to extend and uh we're out of time so I'll have to cut",
    "start": "2122880",
    "end": "2129040"
  },
  {
    "text": "us short before we get to the last questions but uh yeah there's definitely some opportunities and please jump on",
    "start": "2129040",
    "end": "2134640"
  },
  {
    "text": "the proposal to comment yeah yeah thank you i guess we're we're out of time",
    "start": "2134640",
    "end": "2139680"
  },
  {
    "text": "right yeah yeah thank you one last question yeah i'm happy to speak with",
    "start": "2139680",
    "end": "2144720"
  },
  {
    "text": "you at the front",
    "start": "2144720",
    "end": "2147520"
  }
]