[
  {
    "text": "[Applause]",
    "start": "520",
    "end": "4970"
  },
  {
    "text": "and this is how I learn",
    "start": "15560",
    "end": "21320"
  },
  {
    "text": "Spanish and in Brazil we every every word we end with qua or EO so Coca-Cola",
    "start": "22920",
    "end": "30920"
  },
  {
    "text": "is quaa right enough of nonsense",
    "start": "30920",
    "end": "36840"
  },
  {
    "text": "um right I got 30 minutes to go over the wonderful world of how to",
    "start": "36840",
    "end": "44440"
  },
  {
    "text": "manage kubernetes resources if I can get all right right again my name is Rafa",
    "start": "44440",
    "end": "51039"
  },
  {
    "text": "Britto I'm a Staff engineer for storm Forge I have one little slide about stor",
    "start": "51039",
    "end": "56480"
  },
  {
    "text": "Forge um going to very quick about me brto Rafa I'm mu veito just started my",
    "start": "56480",
    "end": "65760"
  },
  {
    "text": "career in 90s with Noel natur in hpox leant Manos who did Noel",
    "start": "65760",
    "end": "72799"
  },
  {
    "text": "natur one person two yeah in Portuguese we",
    "start": "72799",
    "end": "79520"
  },
  {
    "text": "say um from Rio",
    "start": "79520",
    "end": "84840"
  },
  {
    "text": "kioa um and live in New York for 12 years and Austin 12 years",
    "start": "85200",
    "end": "90960"
  },
  {
    "text": "I'm local for cloud optimization and reduced waste for many reasons mucha",
    "start": "90960",
    "end": "98159"
  },
  {
    "text": "Plata energy save the world Etc um my first kubernetes in production was",
    "start": "98159",
    "end": "106200"
  },
  {
    "text": "2016 uh I was the platform engineer for City group for many years um I know all",
    "start": "106200",
    "end": "112520"
  },
  {
    "text": "the pain of scaling uh kubernetes and training Etc managing",
    "start": "112520",
    "end": "118399"
  },
  {
    "text": "users uh before ret is my background has had been uh great Computing in high",
    "start": "118399",
    "end": "124600"
  },
  {
    "text": "frequency trading and I work for VMware for three years and um when I moved to",
    "start": "124600",
    "end": "132560"
  },
  {
    "text": "VMware what I really wanted is tackle problems that I had at City group and one of the problems was multicloud",
    "start": "132560",
    "end": "139519"
  },
  {
    "text": "migration Etc and I inv invented the the kubernetes migrator I think you",
    "start": "139519",
    "end": "145640"
  },
  {
    "text": "mentioned tons of Mission Control um and tons of service match that's that's what I did over",
    "start": "145640",
    "end": "151440"
  },
  {
    "text": "there right now in storm Forge what we do is optimize clusters at scale using",
    "start": "151440",
    "end": "157400"
  },
  {
    "text": "machine learning there is a talk with John plat uh the CTO in a couple hours",
    "start": "157400",
    "end": "163000"
  },
  {
    "text": "we're going to talk about this as well but how we can manage at scale kubernetes it's not easy you need some",
    "start": "163000",
    "end": "169159"
  },
  {
    "text": "something like machine learning to help um I'm involved in the community I",
    "start": "169159",
    "end": "175560"
  },
  {
    "text": "am uh I co-run with Chad the cncf Meetup",
    "start": "175560",
    "end": "182319"
  },
  {
    "text": "um and kcd Texas it's mid I shouldn't say about this right now it's unofficial",
    "start": "182319",
    "end": "188519"
  },
  {
    "text": "but it's something happen next year in Texas that starts with the letter kcd um",
    "start": "188519",
    "end": "195159"
  },
  {
    "text": "and um and I speaker kcd multiple kcds KD kcd s Paulo leant Mano who is from",
    "start": "195159",
    "end": "201760"
  },
  {
    "text": "Brazilian who is Brazilian ch right um and uh I was in Porto last week",
    "start": "201760",
    "end": "210360"
  },
  {
    "text": "all right one thing I tend to say bad word so be me and I usually translate",
    "start": "210360",
    "end": "216439"
  },
  {
    "text": "this why should to give a about Resource Management um three reasons uh",
    "start": "216439",
    "end": "223640"
  },
  {
    "text": "one avoid performances performance problems right you never know when each each workload",
    "start": "223640",
    "end": "231040"
  },
  {
    "text": "running on kubernetes need second reduce Cloud costs when you ask everybody",
    "start": "231040",
    "end": "238599"
  },
  {
    "text": "requests more than actually need that's part of life um I I've done no",
    "start": "238599",
    "end": "245799"
  },
  {
    "text": "judgmental but not being judgmental but that's that's part of life um and when",
    "start": "245799",
    "end": "251959"
  },
  {
    "text": "you have multi-tenant in a large cluster when we",
    "start": "251959",
    "end": "257400"
  },
  {
    "text": "have everyone asking more and more and more you got a problem even if you're own Prem let's",
    "start": "257400",
    "end": "264440"
  },
  {
    "text": "say you don't use cloud the number one and number three are very important",
    "start": "264440",
    "end": "270360"
  },
  {
    "text": "right let's go let's dive into the basics right I after all I'm an",
    "start": "270360",
    "end": "276880"
  },
  {
    "text": "engineer what are requesting limits those are very fundamental",
    "start": "276880",
    "end": "283520"
  },
  {
    "text": "Primitives reques and limits limits is very limited request of resources",
    "start": "283960",
    "end": "289199"
  },
  {
    "text": "I so requests are the minimum that you give to the workload right easy request",
    "start": "289199",
    "end": "297400"
  },
  {
    "text": "minute minimal limits maximum um when you have in this",
    "start": "297400",
    "end": "304600"
  },
  {
    "text": "example giving for let's talk about CPU giving 250",
    "start": "304600",
    "end": "310240"
  },
  {
    "text": "millor a limit of 500 so that workload can burst okay very simple very simple",
    "start": "310240",
    "end": "319360"
  },
  {
    "text": "this is C stuff by the way now let's go a little bit more",
    "start": "319360",
    "end": "325199"
  },
  {
    "text": "complex this is light a little bit busy you got to bear with me on this one what what actually requests are if I tell you",
    "start": "325199",
    "end": "332520"
  },
  {
    "text": "there's not such a concept of mes in Linux kernel going to blow your mind it's just",
    "start": "332520",
    "end": "339720"
  },
  {
    "text": "an abstraction of time of CPU shares so in this little busy table I",
    "start": "339720",
    "end": "349280"
  },
  {
    "text": "have this node of 2,000 cores like two cores and then I have a bunch of PODS",
    "start": "349280",
    "end": "356360"
  },
  {
    "text": "running and each one the engine X request 125 then Gen X another one and so on if",
    "start": "356360",
    "end": "364440"
  },
  {
    "text": "you add those up you're only allocating half of the",
    "start": "364440",
    "end": "370400"
  },
  {
    "text": "node but what cubet does behind the scenes the magic is converting those and",
    "start": "370400",
    "end": "377080"
  },
  {
    "text": "c and CFS shares those are indicating to the Linux kernel how how much of the",
    "start": "377080",
    "end": "384240"
  },
  {
    "text": "share of the CPU each workload will have so if I still have only 1,000 M cores",
    "start": "384240",
    "end": "391000"
  },
  {
    "text": "given basically those are proportion of the shares in reality I'm giving 250 M",
    "start": "391000",
    "end": "397599"
  },
  {
    "text": "cores I know it's a little complicated but if you have to just think as",
    "start": "397599",
    "end": "402960"
  },
  {
    "text": "shares not actual CPU and here's the",
    "start": "402960",
    "end": "408360"
  },
  {
    "text": "beauty of requests and by the way I am the crazy guy on the train that yes you",
    "start": "408360",
    "end": "414520"
  },
  {
    "text": "have just set kubernetes requests so so local in the in the the bus telling",
    "start": "414520",
    "end": "421319"
  },
  {
    "text": "requests are very important because I've been there I suffer this in a",
    "start": "421319",
    "end": "427160"
  },
  {
    "text": "firstand so basically when you give the request you're telling the Linux",
    "start": "427160",
    "end": "432720"
  },
  {
    "text": "konel it's yours nobody else will take so if you give more requests you're",
    "start": "432720",
    "end": "439639"
  },
  {
    "text": "pretty much sitting on on CPU not used if you're not using",
    "start": "439639",
    "end": "444720"
  },
  {
    "text": "it so what's the role of kuet I already said it it kind of",
    "start": "444720",
    "end": "450080"
  },
  {
    "text": "converts this abstraction into CFS shares runs C advisor for checking the",
    "start": "450080",
    "end": "458080"
  },
  {
    "text": "resource of the node it keep tracks how much of the",
    "start": "458080",
    "end": "464000"
  },
  {
    "text": "resource of the node is allocated and we're going to talk now",
    "start": "464000",
    "end": "469120"
  },
  {
    "text": "about Kos that's another cka concept that's very important to have because",
    "start": "469120",
    "end": "475479"
  },
  {
    "text": "they're going to talk a little bit later about when the node is under pressure",
    "start": "475479",
    "end": "480599"
  },
  {
    "text": "and that's what Kos is very important so basically kubernetes detects ass signs a",
    "start": "480599",
    "end": "487319"
  },
  {
    "text": "qos for the workload if you don't set any request and limits again no not",
    "start": "487319",
    "end": "493479"
  },
  {
    "text": "being judgmental I been there it's best effort like we try to get it um what you",
    "start": "493479",
    "end": "500599"
  },
  {
    "text": "got um burstable when you have requests",
    "start": "500599",
    "end": "505960"
  },
  {
    "text": "lower than the limits now as you know a part is compound of one or more",
    "start": "505960",
    "end": "512880"
  },
  {
    "text": "containers so for a guaranteed when request and limits are the same you have",
    "start": "512880",
    "end": "519560"
  },
  {
    "text": "to set for every single container of the",
    "start": "519560",
    "end": "524800"
  },
  {
    "text": "pot then kubernetes will say will detect and tell that's",
    "start": "524800",
    "end": "531519"
  },
  {
    "text": "guaranteed okay why is this important all right let's go for the first demo oh by the way I used to do this a workshop",
    "start": "531519",
    "end": "538079"
  },
  {
    "text": "for 90 minutes um I was a coward so I I set up instead of live demo I put videos",
    "start": "538079",
    "end": "545959"
  },
  {
    "text": "because it happens to me um that I couldn't run my Demo's life so",
    "start": "545959",
    "end": "553839"
  },
  {
    "text": "basically um I have one cluster with one worker node and this worker node has two",
    "start": "553839",
    "end": "561519"
  },
  {
    "text": "CPUs and two gigs of memory and by the way there's a link if you can do this",
    "start": "561519",
    "end": "567079"
  },
  {
    "text": "yourself how to set up using mini uh mini Cube Etc so what I'm doing in this video",
    "start": "567079",
    "end": "573880"
  },
  {
    "text": "showing the node um and I will set up three workloads so right now um just",
    "start": "573880",
    "end": "584079"
  },
  {
    "text": "showing the capacity and allocatable of the node",
    "start": "584079",
    "end": "592079"
  },
  {
    "text": "note the difference between the two very important capacity is almost the",
    "start": "592079",
    "end": "598320"
  },
  {
    "text": "physical located is what cubet going to give to workloads by Design look at the",
    "start": "598320",
    "end": "605519"
  },
  {
    "text": "difference between the memory I set when I create this cluster I put hard evict",
    "start": "605519",
    "end": "612720"
  },
  {
    "text": "hard less than 200 so that's why there's a difference by default the difference is",
    "start": "612720",
    "end": "619320"
  },
  {
    "text": "almost minimal but I want to give a lot of room for um the",
    "start": "619320",
    "end": "626240"
  },
  {
    "text": "cubet okay right play again right very well qctl top nodes I'm big fan of top",
    "start": "626240",
    "end": "633600"
  },
  {
    "text": "nodes I have one gig of memory being used at this point on this",
    "start": "633600",
    "end": "640920"
  },
  {
    "text": "node um now I'm going to run three workloads very important what",
    "start": "640920",
    "end": "648279"
  },
  {
    "text": "the difference about those workload this workload is just to consume",
    "start": "648279",
    "end": "654839"
  },
  {
    "text": "CPU and I have three different one",
    "start": "654839",
    "end": "660079"
  },
  {
    "text": "no requests No Limits C1 C2",
    "start": "660079",
    "end": "665720"
  },
  {
    "text": "requests I'm going to show on the screen how much I'm I'm asking pause right here",
    "start": "665720",
    "end": "671160"
  },
  {
    "text": "so the C2 is requesting 500 millor and the third one it's requesting 500 M cor but a",
    "start": "671160",
    "end": "679360"
  },
  {
    "text": "limit of one CPU and I have again a node with two",
    "start": "679360",
    "end": "686519"
  },
  {
    "text": "CPUs okay very well oh and um",
    "start": "686519",
    "end": "694279"
  },
  {
    "text": "now not only on boarding those this app I'm injecting utilization so this app is",
    "start": "694279",
    "end": "700600"
  },
  {
    "text": "from Google basically just is a resource uh consumer you tell how much memory how",
    "start": "700600",
    "end": "706079"
  },
  {
    "text": "much CPU you want basically what I'm doing here telling each one consume 550",
    "start": "706079",
    "end": "713360"
  },
  {
    "text": "M course and um and I happen to have grafana running",
    "start": "713360",
    "end": "719920"
  },
  {
    "text": "here and I'll will show you those are in",
    "start": "719920",
    "end": "725040"
  },
  {
    "text": "CPU um how much each one is using so green no requests No Limits yellow",
    "start": "725040",
    "end": "732120"
  },
  {
    "text": "requests and limits and blue request with no limits",
    "start": "732120",
    "end": "737920"
  },
  {
    "text": "okay very good now the cool part and I'm going to ask",
    "start": "737920",
    "end": "743240"
  },
  {
    "text": "question if you answer I'll give you a a little gift a r",
    "start": "743240",
    "end": "751160"
  },
  {
    "text": "regalito um I'm killing you my perol",
    "start": "751320",
    "end": "757320"
  },
  {
    "text": "um very good they're running happy each one 450 M course no problem no",
    "start": "757320",
    "end": "766920"
  },
  {
    "text": "contention then I go to the one with requests and",
    "start": "766920",
    "end": "774800"
  },
  {
    "text": "no limits use to cores",
    "start": "774800",
    "end": "780279"
  },
  {
    "text": "2,000 cores remember my node has two",
    "start": "780279",
    "end": "787839"
  },
  {
    "text": "cores what you think will happen",
    "start": "787839",
    "end": "795320"
  },
  {
    "text": "guess which",
    "start": "795600",
    "end": "798839"
  },
  {
    "text": "process okay so you think that container will not use more it will use more that",
    "start": "803959",
    "end": "811000"
  },
  {
    "text": "requested okay that's going to that's will happen but I have two cores and I'm",
    "start": "811000",
    "end": "816279"
  },
  {
    "text": "asking that guy choose two cores and there is something that I",
    "start": "816279",
    "end": "821959"
  },
  {
    "text": "haven't mentioned yet CPU is a compressed resource what",
    "start": "821959",
    "end": "829720"
  },
  {
    "text": "does it mean compressed resource it expands and not you don't finish it",
    "start": "829720",
    "end": "837440"
  },
  {
    "text": "doesn't end so if I putting two if I have two cores and asking someone to run",
    "start": "837440",
    "end": "844839"
  },
  {
    "text": "two cores let's see what's going to",
    "start": "844839",
    "end": "852839"
  },
  {
    "text": "happen okay look what's happening look the",
    "start": "854480",
    "end": "861639"
  },
  {
    "text": "blue look at the yellow and look at the green the blue is the one that I tell us",
    "start": "861639",
    "end": "869160"
  },
  {
    "text": "more CPUs use the entire node",
    "start": "869160",
    "end": "874079"
  },
  {
    "text": "CPU come on uh did it go all the way okay let let",
    "start": "875920",
    "end": "885199"
  },
  {
    "text": "me pause here look at what happened with the green with no requests no shares for you dude you",
    "start": "885199",
    "end": "893279"
  },
  {
    "text": "didn't request any I'm taking from you I'm giving the",
    "start": "893279",
    "end": "898320"
  },
  {
    "text": "one to the blue blue because that guy had request of 500 mes and is using as",
    "start": "898320",
    "end": "906040"
  },
  {
    "text": "much look at the yellow yellow did not",
    "start": "906040",
    "end": "911240"
  },
  {
    "text": "starve why because it had requests is guaranteed who suffered was the green",
    "start": "911240",
    "end": "919880"
  },
  {
    "text": "one now it's easy to reproduce this and showing this now imagine this in the",
    "start": "919880",
    "end": "925079"
  },
  {
    "text": "production system happened this in a millisecond",
    "start": "925079",
    "end": "930199"
  },
  {
    "text": "and you someone call you said my application is running very",
    "start": "930199",
    "end": "935720"
  },
  {
    "text": "slow the first thing you have to see at least in kubernetes is looking the",
    "start": "935720",
    "end": "941880"
  },
  {
    "text": "requests right very quick on the second demo this one is for the",
    "start": "941880",
    "end": "948240"
  },
  {
    "text": "limits um I I injected some extra utilization now",
    "start": "948240",
    "end": "955000"
  },
  {
    "text": "my application is running 450 M cores each one but now I'm going to",
    "start": "955000",
    "end": "961519"
  },
  {
    "text": "inject on the one that has a limit same thing uses two two cores the entire",
    "start": "961519",
    "end": "968720"
  },
  {
    "text": "node what we're going to see this one has limits so the kernel",
    "start": "968720",
    "end": "976920"
  },
  {
    "text": "will throttle that utilization so that's the whole point of limits CPU limits",
    "start": "976920",
    "end": "983360"
  },
  {
    "text": "there is a religious War about CPU limits and not CPU limits I have a slide on it right let's just move on so the",
    "start": "983360",
    "end": "990920"
  },
  {
    "text": "the more of the story here set the",
    "start": "990920",
    "end": "997279"
  },
  {
    "text": "requests otherwise you're going to starve of",
    "start": "997279",
    "end": "1003440"
  },
  {
    "text": "resources so what the best practi is that's the religion War if you have your",
    "start": "1003680",
    "end": "1009480"
  },
  {
    "text": "CPU request right on the money exactly what each application needs you're",
    "start": "1009480",
    "end": "1016040"
  },
  {
    "text": "giving the exactly shares of CPU you don't even need limits because the CFS",
    "start": "1016040",
    "end": "1021639"
  },
  {
    "text": "is going to take care of yourself um and some there is some arguments for CPU limits that came from",
    "start": "1021639",
    "end": "1028480"
  },
  {
    "text": "a multi-tenant high regulated company had a lot of applications fighting to",
    "start": "1028480",
    "end": "1034480"
  },
  {
    "text": "each other so that I I see the point for using CPU limits but they're more",
    "start": "1034480",
    "end": "1039678"
  },
  {
    "text": "important when your requests are shitty that's when you have the CPU",
    "start": "1039679",
    "end": "1045438"
  },
  {
    "text": "limits okay",
    "start": "1045439",
    "end": "1049439"
  },
  {
    "text": "now let's jump how this SCH works that's a little bit of internal how how the",
    "start": "1051360",
    "end": "1057880"
  },
  {
    "text": "kubernetes schler works so the kubernetes schler does not give a for limits that's another myth it only",
    "start": "1057880",
    "end": "1066559"
  },
  {
    "text": "uses the request limits are good for nothing for",
    "start": "1066559",
    "end": "1071679"
  },
  {
    "text": "request so basically the cubet looks how much I gave on this node the schedle has a list of the no",
    "start": "1071679",
    "end": "1078840"
  },
  {
    "text": "noes kind of have a score and assign to pod a",
    "start": "1078840",
    "end": "1085600"
  },
  {
    "text": "node um now I have another question I have the I still have the",
    "start": "1085600",
    "end": "1092799"
  },
  {
    "text": "regalito um if the SCH uses the requests to find",
    "start": "1092799",
    "end": "1100559"
  },
  {
    "text": "out where going to put this SP what about the ones that do not have request set what does it do",
    "start": "1100559",
    "end": "1110520"
  },
  {
    "text": "here's what he has the",
    "start": "1113360",
    "end": "1119400"
  },
  {
    "text": "pod kill it no no you have a pod you just scat a",
    "start": "1119919",
    "end": "1125559"
  },
  {
    "text": "pod you tell run this pod for me then this the scatter are going to find out",
    "start": "1125559",
    "end": "1131320"
  },
  {
    "text": "oh I have a pod without a node right so what pretty much what the",
    "start": "1131320",
    "end": "1138000"
  },
  {
    "text": "SK does just set the No Name on spec and then the node name pick that pod and",
    "start": "1138000",
    "end": "1144200"
  },
  {
    "text": "start to run it but the schedule has to look at all the nodes in the cluster",
    "start": "1144200",
    "end": "1149320"
  },
  {
    "text": "have a thousand nodes it's going to score all the Thousand nodes actually kind of have a a more Speedy way to do",
    "start": "1149320",
    "end": "1156679"
  },
  {
    "text": "this but it will look for every node what is the best place for this part and",
    "start": "1156679",
    "end": "1162200"
  },
  {
    "text": "it does not have request",
    "start": "1162200",
    "end": "1166320"
  },
  {
    "text": "good try but no I going to break the news for you",
    "start": "1171120",
    "end": "1178000"
  },
  {
    "text": "it's hardcoded on the scler and we will assume that pod needs",
    "start": "1178000",
    "end": "1185600"
  },
  {
    "text": "200 M cores and 200 mags of memory it will assume and will find any node that",
    "start": "1185600",
    "end": "1193120"
  },
  {
    "text": "has that amount of resource free that means that if you set if you have a Pod",
    "start": "1193120",
    "end": "1199080"
  },
  {
    "text": "without requesting limits without requests and use three gigs of memory",
    "start": "1199080",
    "end": "1204440"
  },
  {
    "text": "guess what going to happen you're going to put in a shitty node so you have to be careful again I",
    "start": "1204440",
    "end": "1211480"
  },
  {
    "text": "am the local at the autobus",
    "start": "1211480",
    "end": "1216120"
  },
  {
    "text": "yelling resources now very quick about Resource Management um",
    "start": "1218320",
    "end": "1228280"
  },
  {
    "text": "now let's talk about memory memory differently from CPU is",
    "start": "1228799",
    "end": "1234880"
  },
  {
    "text": "not is uncompressed",
    "start": "1234880",
    "end": "1240120"
  },
  {
    "text": "resource so let's say you have a node you set your requests let's say you don't set the",
    "start": "1240200",
    "end": "1248200"
  },
  {
    "text": "limits and then one of those applications just run out of control of",
    "start": "1248200",
    "end": "1256240"
  },
  {
    "text": "memory there are two scenarios I'm going to show them uh basically first one is",
    "start": "1256919",
    "end": "1264120"
  },
  {
    "text": "no doubt of memory no off on memory actually is a system model memory the Linux kernel",
    "start": "1264120",
    "end": "1271360"
  },
  {
    "text": "itself going to figure out you know you don't have memory i g to I GNA kill",
    "start": "1271360",
    "end": "1277360"
  },
  {
    "text": "you so now how the the konel will decide",
    "start": "1277360",
    "end": "1285760"
  },
  {
    "text": "this it goes back to the Q but the Linux kernel does not know what",
    "start": "1285760",
    "end": "1293360"
  },
  {
    "text": "qos is it's not such a concept in Linux kernel but it does what cubet does",
    "start": "1293360",
    "end": "1300600"
  },
  {
    "text": "converts the qos and set what we call um score a",
    "start": "1300600",
    "end": "1308320"
  },
  {
    "text": "adj into the into the process SL proc so",
    "start": "1308320",
    "end": "1313960"
  },
  {
    "text": "it's not deterministic it might be kicking out",
    "start": "1313960",
    "end": "1319159"
  },
  {
    "text": "or killing not kicking out but killing is different than kicking out and killing is different I'm going to kill",
    "start": "1319159",
    "end": "1325799"
  },
  {
    "text": "that most likely first the best effort then the Bible and the guarantee",
    "start": "1325799",
    "end": "1333159"
  },
  {
    "text": "is going to be the last one so if you're running production and you really have a super duper",
    "start": "1333159",
    "end": "1340159"
  },
  {
    "text": "important um workload you might consider yourself to run with a Kos",
    "start": "1340159",
    "end": "1347520"
  },
  {
    "text": "guaranteed so this is system out of memory this is the Linux kernel there's a second",
    "start": "1347520",
    "end": "1354440"
  },
  {
    "text": "scenario where this time is the cubet realizing the",
    "start": "1354440",
    "end": "1361559"
  },
  {
    "text": "pressure and that's when the memory utilization Falls between the those two",
    "start": "1361559",
    "end": "1367520"
  },
  {
    "text": "allocatable capacity allocatable that's the cubet it will",
    "start": "1367520",
    "end": "1373080"
  },
  {
    "text": "evict the PO is different from the previous scenario",
    "start": "1373080",
    "end": "1379080"
  },
  {
    "text": "that was terminating when the cubet evicts a pod",
    "start": "1379080",
    "end": "1384799"
  },
  {
    "text": "it removes um this dopc do node no node name from it calls the API there's an",
    "start": "1384799",
    "end": "1391799"
  },
  {
    "text": "eviction API but basically that pod goes back to the scler and is for grabs for",
    "start": "1391799",
    "end": "1399200"
  },
  {
    "text": "any other node so basically it expels the Pod differ from the system o",
    "start": "1399200",
    "end": "1407000"
  },
  {
    "text": "that kills the Pod and the pod will restart on the same Noe with less memory",
    "start": "1407000",
    "end": "1412799"
  },
  {
    "text": "consumption of course but on the o the Pod the process it's a it's a",
    "start": "1412799",
    "end": "1419320"
  },
  {
    "text": "Linux process it's not doesn't care if it's a pod or not we'll restart on the same note so there's two different",
    "start": "1419320",
    "end": "1428120"
  },
  {
    "text": "scenarios and one thing I again the crazy guy on the train is I see this on",
    "start": "1428200",
    "end": "1434039"
  },
  {
    "text": "the internet all the time people oh my body eviction because of depress of the node and this I'm not picking up on this",
    "start": "1434039",
    "end": "1441000"
  },
  {
    "text": "guy but he wrote the whole medium post saying that when you have a a pod",
    "start": "1441000",
    "end": "1446400"
  },
  {
    "text": "running hot in the CPU it will get evicted no it will not get",
    "start": "1446400",
    "end": "1452960"
  },
  {
    "text": "evicted it will get evicted because of the memory CPU what will happen is on",
    "start": "1452960",
    "end": "1458840"
  },
  {
    "text": "the very first lab you're going to have a process running without CPU shares so",
    "start": "1458840",
    "end": "1464679"
  },
  {
    "text": "there's no eviction for CPU utilization okay right let's go demo",
    "start": "1464679",
    "end": "1472679"
  },
  {
    "text": "to um very similar I do have the same node and I",
    "start": "1472679",
    "end": "1479360"
  },
  {
    "text": "will start three three",
    "start": "1479360",
    "end": "1484559"
  },
  {
    "text": "workloads um one",
    "start": "1484559",
    "end": "1490080"
  },
  {
    "text": "with same deal right no request No Limits the second one with",
    "start": "1490080",
    "end": "1496480"
  },
  {
    "text": "requests and no limits and third one requests and",
    "start": "1496480",
    "end": "1502640"
  },
  {
    "text": "limits um Let me show here um I'm GNA this",
    "start": "1502640",
    "end": "1508640"
  },
  {
    "text": "command line is going to show the Kos and the resources of each the Kos again",
    "start": "1508640",
    "end": "1514880"
  },
  {
    "text": "kubernetes does this for you you don't set it you set it the request and limits and kubernetes put this so if you see on",
    "start": "1514880",
    "end": "1522200"
  },
  {
    "text": "the screen I have those one is best effort the second is burstable and the third is guaranteed so the ones for",
    "start": "1522200",
    "end": "1529520"
  },
  {
    "text": "burstable in guarantee they request 250 Megs of memory and I have a 2",
    "start": "1529520",
    "end": "1535480"
  },
  {
    "text": "gig 2 gig node and I think was by I think was using one",
    "start": "1535480",
    "end": "1543559"
  },
  {
    "text": "gig so this is a very similar um application I'm just injecting hey use",
    "start": "1543559",
    "end": "1549760"
  },
  {
    "text": "150 Megs each app so let's see how in graphon",
    "start": "1549760",
    "end": "1556000"
  },
  {
    "text": "is um I put on a memory not um name space very well I have the same deal",
    "start": "1556000",
    "end": "1562159"
  },
  {
    "text": "right but now is memory each workload taking",
    "start": "1562159",
    "end": "1568279"
  },
  {
    "text": "150 mags okay there's nothing excited about it let's start to get a little bit more",
    "start": "1568279",
    "end": "1573880"
  },
  {
    "text": "excited um first I will simulate what they call",
    "start": "1573880",
    "end": "1580480"
  },
  {
    "text": "application out of memory for the one has a limit of",
    "start": "1580480",
    "end": "1586159"
  },
  {
    "text": "250 I'm telling it hey use 500 mags look at this look at what happened right off",
    "start": "1586159",
    "end": "1592760"
  },
  {
    "text": "the bat the application all of a sudden use more memory what the limit",
    "start": "1592760",
    "end": "1599919"
  },
  {
    "text": "is look at the status of this pod um",
    "start": "1599919",
    "end": "1605639"
  },
  {
    "text": "killed um look if you do a described pod um describe pod blah blah blah blah",
    "start": "1605799",
    "end": "1613799"
  },
  {
    "text": "blah blah look at what you got the state is terminated the reason um killed this is when the",
    "start": "1613799",
    "end": "1621679"
  },
  {
    "text": "application itself is um not the system not the node",
    "start": "1621679",
    "end": "1627960"
  },
  {
    "text": "is the application okay that",
    "start": "1627960",
    "end": "1634159"
  },
  {
    "text": "scenario now what happened is the application owned and restarted to use",
    "start": "1634159",
    "end": "1640840"
  },
  {
    "text": "the same initial settings of memory right so let me now go to the second",
    "start": "1640840",
    "end": "1647080"
  },
  {
    "text": "scenario which is the system o when the Linux",
    "start": "1647080",
    "end": "1652120"
  },
  {
    "text": "kernel decides decides that's not enough we have to kill",
    "start": "1652120",
    "end": "1658880"
  },
  {
    "text": "someone so in this scenario the application restarted now I have the three workloads three pods running 150",
    "start": "1659960",
    "end": "1668360"
  },
  {
    "text": "mags any okay too fast um yeah I'm showing",
    "start": "1668360",
    "end": "1675720"
  },
  {
    "text": "here again the capacity and allocatable look at the",
    "start": "1675720",
    "end": "1681039"
  },
  {
    "text": "difference between the two so I can give up to 1.8 gigs on allocatable and I ran",
    "start": "1681039",
    "end": "1689960"
  },
  {
    "text": "uh top nodes so right now on that node is 1.4",
    "start": "1689960",
    "end": "1697320"
  },
  {
    "text": "gigs being used so the when you do described node",
    "start": "1697320",
    "end": "1702960"
  },
  {
    "text": "describe node will only show allocatable allocated not going to give the the",
    "start": "1702960",
    "end": "1708279"
  },
  {
    "text": "reutilization right you need something like Cub C top um to to give",
    "start": "1708279",
    "end": "1715440"
  },
  {
    "text": "that um what I'm going to do now is going to the two parts that there's no",
    "start": "1720000",
    "end": "1727480"
  },
  {
    "text": "limit and increase for 150 to 400 Megs",
    "start": "1727480",
    "end": "1733279"
  },
  {
    "text": "each so they will increase and they don't have limits remember this",
    "start": "1733279",
    "end": "1739480"
  },
  {
    "text": "I'm injecting hey use more memory use more",
    "start": "1739480",
    "end": "1744960"
  },
  {
    "text": "memory and what happening is the consumption went so",
    "start": "1745720",
    "end": "1752679"
  },
  {
    "text": "fast that Cub CTL didn't evict didn't do anything but",
    "start": "1752679",
    "end": "1760399"
  },
  {
    "text": "the Linux Colonel did look the the last state terminated and",
    "start": "1760399",
    "end": "1767120"
  },
  {
    "text": "the reason ER so the the process was killed by the",
    "start": "1767120",
    "end": "1773840"
  },
  {
    "text": "Linux kernel and finally the last um I'm going have to hurry a little",
    "start": "1773840",
    "end": "1780720"
  },
  {
    "text": "bit um finally the last scenario is when the eviction so what I had to do in this",
    "start": "1780720",
    "end": "1787600"
  },
  {
    "text": "um um scenario I need to pick one process and",
    "start": "1787600",
    "end": "1794039"
  },
  {
    "text": "slowly increasing the memory utilization almost like a memory leak so I'm picking the one that has no",
    "start": "1794039",
    "end": "1800760"
  },
  {
    "text": "request low limits and slowly consuming 50 mags of each second",
    "start": "1800760",
    "end": "1807559"
  },
  {
    "text": "and now if I do a getp pod look at what you got the status",
    "start": "1807559",
    "end": "1813799"
  },
  {
    "text": "evicted so there's different scenarios so got to be",
    "start": "1813799",
    "end": "1819399"
  },
  {
    "text": "super um cautious about it again requests are very important limits are not important only",
    "start": "1819399",
    "end": "1826559"
  },
  {
    "text": "if the requests are wrong how you do this in",
    "start": "1826559",
    "end": "1833360"
  },
  {
    "text": "multi-tenant large clusters you have tools Primitives coming from kubernetes",
    "start": "1833360",
    "end": "1840640"
  },
  {
    "text": "so limit ranges is one of them basically you set by Nam space What going to be the minimum the",
    "start": "1840640",
    "end": "1848080"
  },
  {
    "text": "maximums of requests you can set up default requests for if someone does not set it",
    "start": "1848080",
    "end": "1856919"
  },
  {
    "text": "will go and do it for you um you can set up ratio um and one thing is when you do",
    "start": "1856919",
    "end": "1864799"
  },
  {
    "text": "limit ranges um it will not change automatically for",
    "start": "1864799",
    "end": "1870960"
  },
  {
    "text": "you so if you change the limit ranges for default request and limits I have a whole blog post how I it up at City",
    "start": "1870960",
    "end": "1877440"
  },
  {
    "text": "Group my first versions not setting the requests and limits I had to change",
    "start": "1877440",
    "end": "1883600"
  },
  {
    "text": "multiple times killing people's jobs I mean pods people calling me what what you doing like I'm I I don't know what",
    "start": "1883600",
    "end": "1890760"
  },
  {
    "text": "I'm doing just bringing down the requests and limits resource Coda that's how you define limits for name",
    "start": "1890760",
    "end": "1900279"
  },
  {
    "text": "spaces very important as well but you still need something more than this because you can end up adding more name",
    "start": "1900279",
    "end": "1907600"
  },
  {
    "text": "spaces and then you end up having more requests I mean more resources across",
    "start": "1907600",
    "end": "1914840"
  },
  {
    "text": "name spaces than actually your your cluster has so in practice what we do see is Storm",
    "start": "1914840",
    "end": "1921039"
  },
  {
    "text": "Forge first people have their clusters they don't bother to set anything and",
    "start": "1921039",
    "end": "1926159"
  },
  {
    "text": "then you start to have performance problems then I'll pick one request and",
    "start": "1926159",
    "end": "1931440"
  },
  {
    "text": "one value for limits you can end up having the same situation you have some people using",
    "start": "1931440",
    "end": "1938840"
  },
  {
    "text": "more using less um to expensive then you start to go to every application",
    "start": "1938840",
    "end": "1945559"
  },
  {
    "text": "manually looking and then all of sudden had a team of five to 10 Engineers I had",
    "start": "1945559",
    "end": "1950960"
  },
  {
    "text": "applications knocking my door my applications running slow and then I had to dedicate U an engineer it does not",
    "start": "1950960",
    "end": "1958639"
  },
  {
    "text": "scale so basically you have to get some like artificial intelligence machine learning to do it for you so this is the",
    "start": "1958639",
    "end": "1965600"
  },
  {
    "text": "kubernetes elephant in the room who going to set up the requests it's like one pointing to each",
    "start": "1965600",
    "end": "1971720"
  },
  {
    "text": "other and here's when I might plug um that's what we do for living",
    "start": "1971720",
    "end": "1978240"
  },
  {
    "text": "that's the QR code we got um we got free trials um you basically you're going to",
    "start": "1978240",
    "end": "1984240"
  },
  {
    "text": "be open open bar of requesting limits for any kind of workloads batches Etc",
    "start": "1984240",
    "end": "1991120"
  },
  {
    "text": "and I really appreciate if you click this because my CTO going to come in two sessions and we had a a a competition",
    "start": "1991120",
    "end": "1998000"
  },
  {
    "text": "who going to get more clicks all right hi",
    "start": "1998000",
    "end": "2004519"
  },
  {
    "text": "John all right I'm out of the time if you have questions please come uh I",
    "start": "2004519",
    "end": "2009600"
  },
  {
    "text": "still have the two regalitos um if you really want to click and have a meaningful conversation I have two mugs",
    "start": "2009600",
    "end": "2015880"
  },
  {
    "text": "to give all right thank you so much [Applause]",
    "start": "2015880",
    "end": "2022790"
  }
]