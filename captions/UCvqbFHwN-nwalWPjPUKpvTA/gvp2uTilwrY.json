[
  {
    "text": "all right good afternoon guys i'm Tim Wickberg i'm the chief technical officer for Sketmd and I'm here to talk about",
    "start": "80",
    "end": "7520"
  },
  {
    "text": "Slinky Slurm in Kubernetes performant AI and HPC workload management um I also",
    "start": "7520",
    "end": "13200"
  },
  {
    "text": "have our title slide uh broken out into our own uh format as well here backing it up with our logo um and I do also",
    "start": "13200",
    "end": "19520"
  },
  {
    "text": "need to uh give a lot of appreciation to the developers behind a lot of this effort",
    "start": "19520",
    "end": "25039"
  },
  {
    "text": "skyler Owlet and Marlo uh who aren't here with me this week but are behind all of the underpinnings I'm about to",
    "start": "25039",
    "end": "30560"
  },
  {
    "text": "talk about today uh wow some of the slides are out of",
    "start": "30560",
    "end": "36800"
  },
  {
    "text": "order that's going to be fun okay introduction first of all what is SLURM",
    "start": "36800",
    "end": "42320"
  },
  {
    "text": "slurm is a leading HPC workload manager uh workload manager roughly means a job",
    "start": "42320",
    "end": "48920"
  },
  {
    "text": "scheduler combined with a resource manager um roughly equivalent to an",
    "start": "48920",
    "end": "54000"
  },
  {
    "text": "orchestrator uh such as Kubernetes and and what it does and manages on the compute nodes uh those two components",
    "start": "54000",
    "end": "60399"
  },
  {
    "text": "there are scheduler prioritizes and decides which compute jobs to run on what parts of the system sort of when",
    "start": "60399",
    "end": "67439"
  },
  {
    "text": "where and why and then the resource management layer of that is tracking node state node resources and dealing",
    "start": "67439",
    "end": "74960"
  },
  {
    "text": "with actual job launch job dispatch potentially pulling in different container run times and",
    "start": "74960",
    "end": "81720"
  },
  {
    "text": "ecosystems slur manages the majority of the top 500 supercomputers in the world",
    "start": "81720",
    "end": "86799"
  },
  {
    "text": "um roughly 60 to 70% last we checked um also manages most AIM ML training",
    "start": "86799",
    "end": "93560"
  },
  {
    "text": "workloads even for a lot of companies that run cloud native for the rest of their stack slurm is usually involved in",
    "start": "93560",
    "end": "100880"
  },
  {
    "text": "managing those training workloads for them today uh slurm scales well beyond 15,000 nodes in a single cluster um we",
    "start": "100880",
    "end": "108479"
  },
  {
    "text": "are able to on some machines launch 10,000 plus node simulation work in sub",
    "start": "108479",
    "end": "116079"
  },
  {
    "text": "10 seconds uh which is something that we don't believe uh other orchestrators are",
    "start": "116079",
    "end": "121680"
  },
  {
    "text": "able to accomplish today slurm is open source uh GPLv2 plus with an openSSL",
    "start": "121680",
    "end": "127280"
  },
  {
    "text": "exception technically um and has been available to the broad community for a couple",
    "start": "127280",
    "end": "133760"
  },
  {
    "text": "decades it's going to be real fun how I got to figure out why the slides are out of order here uh so who is SKTMD uh",
    "start": "135239",
    "end": "141360"
  },
  {
    "text": "SKTMD are the developers of Slurm and also now Slinky we originally spun off from from Lawrence Livermore National",
    "start": "141360",
    "end": "147520"
  },
  {
    "text": "Lab back in 2012 to support SLURM's rapid adoption in the HPC industry um",
    "start": "147520",
    "end": "152879"
  },
  {
    "text": "our founders are Mo and Danny they're the M andD in SKEMD we are not medical",
    "start": "152879",
    "end": "158319"
  },
  {
    "text": "doctors please do not solicit medical advice from many of our support staff they are not qualified to answer",
    "start": "158319",
    "end": "164360"
  },
  {
    "text": "that sketmd provides commercial support for SLURM uh for our customers alongside training consultation and custom",
    "start": "164360",
    "end": "171280"
  },
  {
    "text": "development and then what you're here to hear about today is what is Slinky slinky is Slurm",
    "start": "171280",
    "end": "178160"
  },
  {
    "text": "and Kubernetes combined um it's a toolkit of different projects to",
    "start": "178160",
    "end": "183519"
  },
  {
    "text": "integrate Slurm into Kubernetes uh everything we've been building here is open source under the Apache 2 license",
    "start": "183519",
    "end": "189760"
  },
  {
    "text": "instead of GPL and is broadly broken out into three major components the slurm",
    "start": "189760",
    "end": "194879"
  },
  {
    "text": "operator slurm bridge and a lot of associated tooling slurm operator is designed to manage",
    "start": "194879",
    "end": "202800"
  },
  {
    "text": "slurm clusters running underneath Kubernetes um be able to autoscale them",
    "start": "202800",
    "end": "208000"
  },
  {
    "text": "uh give you easier ways to deploy reference slurm installations underneath",
    "start": "208000",
    "end": "213920"
  },
  {
    "text": "Kubernetes potentially a managed Kubernetes offering from one of the cloud providers original releases back",
    "start": "213920",
    "end": "220000"
  },
  {
    "text": "in November just ahead of uh CubeCon and Salt Lake uh that was VO.1.0 uh VO.2.0",
    "start": "220000",
    "end": "226480"
  },
  {
    "text": "actually came out about uh a week ago uh and we're looking forward to getting v3.0 out in June alongside the slurm",
    "start": "226480",
    "end": "235640"
  },
  {
    "text": "bridge the slurm bridge is a Kubernetes scheduling plugin the idea here is to",
    "start": "235640",
    "end": "241200"
  },
  {
    "text": "enable slurm scheduling wherewithal not just for slurm based",
    "start": "241200",
    "end": "246319"
  },
  {
    "text": "workloads but for kubernetes pods and kubernetes workflows uh the intent is to release",
    "start": "246319",
    "end": "252720"
  },
  {
    "text": "this in June um it is gated currently on some new features and capabilities we're",
    "start": "252720",
    "end": "257919"
  },
  {
    "text": "introducing in the slurm 25505 release in May to better enable this cross",
    "start": "257919",
    "end": "263080"
  },
  {
    "text": "integration um and it is in early access with some sketched customers",
    "start": "263080",
    "end": "268840"
  },
  {
    "text": "today then alongside this is a lot of associated tooling um you can't really play around in the cube ecosystem",
    "start": "268840",
    "end": "275400"
  },
  {
    "text": "without some other baggage uh helmchart container images uh we also have published a client library um this",
    "start": "275400",
    "end": "282960"
  },
  {
    "text": "interacts with slurm's rest API um but is translating it uh using an open API",
    "start": "282960",
    "end": "289040"
  },
  {
    "text": "generator into Golang uh so it's a little easier to adopt and modify there we also have a slurm exporter Prometheus",
    "start": "289040",
    "end": "295600"
  },
  {
    "text": "exporter that interacts again with the rest API um it is publishing metrics right now that are fine-tuned to slurm",
    "start": "295600",
    "end": "302759"
  },
  {
    "text": "operator so it can uh hook in and and manage autoscaling for us slinky repositories are on GitHub um",
    "start": "302759",
    "end": "310800"
  },
  {
    "text": "that QR code will take you there uh everything we're doing here is captured under the Slinky project organization um",
    "start": "310800",
    "end": "317520"
  },
  {
    "text": "separate away from the normal Slurm",
    "start": "317520",
    "end": "321198"
  },
  {
    "text": "repository so cloud native HBC and slurm um a key part of my talk today is really",
    "start": "323000",
    "end": "328560"
  },
  {
    "text": "kind of laying the groundwork for where I think these two communities can start to converge and where I think slurm and",
    "start": "328560",
    "end": "335440"
  },
  {
    "text": "the slinky project itself sits at the intersection so uh the subtext to this is why isn't HPC scheduling guy standing",
    "start": "335440",
    "end": "342639"
  },
  {
    "text": "around at CubeCon so starting off here with a bit of a disclaimer um everything I'm about",
    "start": "342639",
    "end": "349600"
  },
  {
    "text": "to say is a gross oversimplification of two incredibly complex and intertwined",
    "start": "349600",
    "end": "355400"
  },
  {
    "text": "communities every point I make here I can come up with at least a half a dozen counter examples i'm trying to focus on",
    "start": "355400",
    "end": "362639"
  },
  {
    "text": "the broad context of what I'm saying here and use that to highlight where I",
    "start": "362639",
    "end": "368400"
  },
  {
    "text": "think the two communities can learn from one another and start to cross adopt certain uh technologies and",
    "start": "368400",
    "end": "375639"
  },
  {
    "text": "capabilities so to start with HBC and cloud native why are they",
    "start": "375639",
    "end": "380680"
  },
  {
    "text": "different uh it really stems from different sets of assumptions that that both have and and the evolution of each",
    "start": "380680",
    "end": "387600"
  },
  {
    "text": "of these ecosystems slinky is meant to sort of be our",
    "start": "387600",
    "end": "394000"
  },
  {
    "text": "SKEDMD's approach to to bringing slurm's wherewithal into here and crossing over that boundary the way I've described",
    "start": "394000",
    "end": "400720"
  },
  {
    "text": "this a lot is at the very highest level the the perspectives are shaped by this",
    "start": "400720",
    "end": "406160"
  },
  {
    "text": "idea that HPC has finite resources a finite system scale what you built from",
    "start": "406160",
    "end": "413520"
  },
  {
    "text": "that vendor stood up as a particular supercomput but then infinite workload",
    "start": "413520",
    "end": "418639"
  },
  {
    "text": "demand hpc researchers are not hesitant to bring more and more simulations to",
    "start": "418639",
    "end": "425840"
  },
  {
    "text": "bear on the system and you have to figure out how to fit that into the finite resources you have at hand",
    "start": "425840",
    "end": "433360"
  },
  {
    "text": "cloud native on the other hand assumes nearly infinite resources that you can always hit some sort of cloud providers",
    "start": "433360",
    "end": "440800"
  },
  {
    "text": "API to bring and provision additional capacity into the cluster but that at",
    "start": "440800",
    "end": "446160"
  },
  {
    "text": "the same time there is finite workload demand that people are placing on this machine that you can",
    "start": "446160",
    "end": "453199"
  },
  {
    "text": "always simultaneously run everything that that system has to offer or uh that that workload has to",
    "start": "453199",
    "end": "460759"
  },
  {
    "text": "demand so to qualify this a little further HPC assumes finite resources but",
    "start": "460759",
    "end": "466240"
  },
  {
    "text": "infinite workload demand systems cannot simultaneously execute everything that HPC researchers",
    "start": "466240",
    "end": "473120"
  },
  {
    "text": "would like q prioritization and figuring out what slice of that demand is",
    "start": "473120",
    "end": "480000"
  },
  {
    "text": "relevant to run on the machine right now is paramount this results in incredibly complicated priority schemes fair share",
    "start": "480000",
    "end": "488080"
  },
  {
    "text": "um different schemes building the size shape and time duration of a job into",
    "start": "488080",
    "end": "493840"
  },
  {
    "text": "account to decide when and where on a machine to give people access to run the simulation",
    "start": "493840",
    "end": "500240"
  },
  {
    "text": "complementing that is a detailed accounting system that is then",
    "start": "500240",
    "end": "505280"
  },
  {
    "text": "cross-tied into very fine grained limits to say exactly how many resources given",
    "start": "505280",
    "end": "511680"
  },
  {
    "text": "user populations are expected to have access to on the machine at a given time",
    "start": "511680",
    "end": "517120"
  },
  {
    "text": "alongside this is that historically HPC has always been focused on the high end",
    "start": "517120",
    "end": "522399"
  },
  {
    "text": "of simulation work talking about how to build machines to run workloads that",
    "start": "522399",
    "end": "528080"
  },
  {
    "text": "demand access to thousands tens of thousands hundreds of thousands of CPUs",
    "start": "528080",
    "end": "534040"
  },
  {
    "text": "simultaneously in lock step alongside this again jobs have time",
    "start": "534040",
    "end": "540880"
  },
  {
    "text": "limits uh time limits are a a subtle piece that is actually kind of missing out of the cloud native ecosystem this",
    "start": "540880",
    "end": "546480"
  },
  {
    "text": "idea that a simulation should respect a certain boundary uh incomplete within",
    "start": "546480",
    "end": "552240"
  },
  {
    "text": "that scheduled duration this is also then fed into how workload is prioritized backed off and different",
    "start": "552240",
    "end": "560480"
  },
  {
    "text": "jobs can be managed and forward planned out for that system again encompassed",
    "start": "560480",
    "end": "566080"
  },
  {
    "text": "within those finite resources also then uh wrapping this",
    "start": "566080",
    "end": "571279"
  },
  {
    "text": "slide up here systems are more statically defined um traditional HPC systems are bought on a a order of years",
    "start": "571279",
    "end": "579839"
  },
  {
    "text": "procurements go out vendor responses come back two to three year deployment",
    "start": "579839",
    "end": "584959"
  },
  {
    "text": "life cycle of the machines say fiveish years uh and you're not just spinning up additional racks of capacity",
    "start": "584959",
    "end": "592519"
  },
  {
    "text": "quickly cloud native on the other hand flipping this around infinite resources but finite workload uh cloud",
    "start": "592519",
    "end": "598080"
  },
  {
    "text": "orchestration was designed around microservices first and foremost all pods in the Kubernetes system are",
    "start": "598080",
    "end": "604320"
  },
  {
    "text": "expected by default to be running simultaneously again massive asterisk on this please forgive",
    "start": "604320",
    "end": "610920"
  },
  {
    "text": "me workloads scale horizontally running additional pods and then load balancing",
    "start": "610920",
    "end": "616399"
  },
  {
    "text": "between them tightly coupled processes across multi-node multi-node compute",
    "start": "616399",
    "end": "622160"
  },
  {
    "text": "workloads are not a core design element of Kubernetes",
    "start": "622160",
    "end": "627440"
  },
  {
    "text": "and require certain scheduler extensions to be able to integrate and manage pods run indefinitely time limits are not",
    "start": "627440",
    "end": "634160"
  },
  {
    "text": "baked in by default that there is nothing policing that a simulation wraps up on schedule to turn resources over to",
    "start": "634160",
    "end": "642000"
  },
  {
    "text": "deferred and delay or delayed workflows capacity issues are are fixed",
    "start": "642000",
    "end": "647839"
  },
  {
    "text": "by just getting more resources call into the cloud API bring more compute uh",
    "start": "647839",
    "end": "652880"
  },
  {
    "text": "wherewithal to to bear to be able to run everything simultaneously usually in service of an",
    "start": "652880",
    "end": "658720"
  },
  {
    "text": "external say web interface or external set of microservices that are driving presumably some sort of revenue",
    "start": "658720",
    "end": "665200"
  },
  {
    "text": "generation for your company and application support for application resilience and dynamic resource",
    "start": "665200",
    "end": "671839"
  },
  {
    "text": "management though are presumed this does stand in stark contrast to the HPC space",
    "start": "671839",
    "end": "676959"
  },
  {
    "text": "where jobs dispatch of a fixed size and scope and are expected to continue",
    "start": "676959",
    "end": "682399"
  },
  {
    "text": "occupying that same footprint on the system until they terminate some number of hours days or even weeks later um",
    "start": "682399",
    "end": "690399"
  },
  {
    "text": "this also has interestingly enough led to some very different scheduling semantics growing out of this space uh one thing I point to commonly is the",
    "start": "690399",
    "end": "697200"
  },
  {
    "text": "affinity anti-affffinity patterns within Kubernetes scheduling framework don't have equivalent models in the HPC",
    "start": "697200",
    "end": "704000"
  },
  {
    "text": "scheduling community um this isn't something that HPC worries about as part of the way their jobs are run and",
    "start": "704000",
    "end": "711399"
  },
  {
    "text": "managed so why converge these two spaces systems are are under increased",
    "start": "711399",
    "end": "718320"
  },
  {
    "text": "demand to run these kind of batch style workloads even on these cloudnative systems a IML folks are probably driving",
    "start": "718320",
    "end": "725519"
  },
  {
    "text": "the greatest uptake of this and are certainly absorbing the greatest slice of compute um possibly in the history of",
    "start": "725519",
    "end": "732480"
  },
  {
    "text": "the world and those folks are running Kubernetes generally for inference",
    "start": "732480",
    "end": "737519"
  },
  {
    "text": "workloads um something that is serving an end application but the models that",
    "start": "737519",
    "end": "743040"
  },
  {
    "text": "they're building are being built on slurm slurm is running those training workloads for them um it's kind of",
    "start": "743040",
    "end": "750399"
  },
  {
    "text": "awkward to have these two divergent control planes involved in different parts of potentially your same systems",
    "start": "750399",
    "end": "757839"
  },
  {
    "text": "potentially trying to overlap on the same hardware or at least within your company having to run two very",
    "start": "757839",
    "end": "766360"
  },
  {
    "text": "different clusters just to service the sort of two responsibilities of of these",
    "start": "766360",
    "end": "772320"
  },
  {
    "text": "modern a IML shops so converging these come becomes very",
    "start": "772320",
    "end": "777519"
  },
  {
    "text": "tempting trying to fit one into the other um is is what we're trying to sort",
    "start": "777519",
    "end": "782560"
  },
  {
    "text": "of sus out here is it possible to bridge the gap between the two of these and Slinky is meant as our take on how to",
    "start": "782560",
    "end": "790000"
  },
  {
    "text": "cover this again in two slightly different ways one through the slurm",
    "start": "790000",
    "end": "795040"
  },
  {
    "text": "operator where we can provide that traditional HPC environment within a Kubernetes",
    "start": "795040",
    "end": "801600"
  },
  {
    "text": "system that exists and then a second what I'm going to focus on a little bit more today is",
    "start": "801600",
    "end": "807279"
  },
  {
    "text": "this new slurm bridge idea of bringing slurm's scheduling wherewithal to cover",
    "start": "807279",
    "end": "812560"
  },
  {
    "text": "some of the gaps that we see within the kubernetes scheduling ecosystem and be able to better provide for scheduling",
    "start": "812560",
    "end": "819360"
  },
  {
    "text": "and management of especially largecale batch style workloads",
    "start": "819360",
    "end": "825279"
  },
  {
    "text": "So it's building off again these additional capabilities that we see and things that we see in slurm that we don't see modeled in the cloudnative",
    "start": "826560",
    "end": "833120"
  },
  {
    "text": "scheduling ecosystem first and foremost this idea of efficient multi-node scheduling and resource allocation um",
    "start": "833120",
    "end": "840240"
  },
  {
    "text": "than planning around future system state understanding that the workload that",
    "start": "840240",
    "end": "845320"
  },
  {
    "text": "exists might not all fit into the machine immediately and that we do need to bring",
    "start": "845320",
    "end": "851040"
  },
  {
    "text": "some sort of prioritization model to bear but also then to use that priority",
    "start": "851040",
    "end": "856639"
  },
  {
    "text": "model to inform not just what is running on the system instantaneously but what",
    "start": "856639",
    "end": "861680"
  },
  {
    "text": "is running over a much broader period of time then network topology management",
    "start": "861680",
    "end": "868160"
  },
  {
    "text": "something around NV link management especially something we've been working um very closely with Nvidia for the last",
    "start": "868160",
    "end": "874480"
  },
  {
    "text": "three years to model the network topology that exists in the hardware",
    "start": "874480",
    "end": "879920"
  },
  {
    "text": "that these systems are being built off of and efficiently plan out use of the",
    "start": "879920",
    "end": "885279"
  },
  {
    "text": "machine again instantaneously and into the future so onto the slurm operator and",
    "start": "885279",
    "end": "892639"
  },
  {
    "text": "I'm going to skip through this a little quickly in the interest of time slurm operator's use case again it's managing",
    "start": "892639",
    "end": "898240"
  },
  {
    "text": "slurm clusters built within a kubernetes environment compute nodes are expected to map directly to kubernetes pods",
    "start": "898240",
    "end": "905160"
  },
  {
    "text": "running an individual slurmd process um which is slurm's component very much",
    "start": "905160",
    "end": "910560"
  },
  {
    "text": "akin to the cublet responsible for managing a compute node then alongside",
    "start": "910560",
    "end": "916560"
  },
  {
    "text": "that supporting autoscaling based on cluster utilization metrics the Prometheus exporter is what we then",
    "start": "916560",
    "end": "922480"
  },
  {
    "text": "build off of to to drive that within this we're running slurm jobs natively",
    "start": "922480",
    "end": "927600"
  },
  {
    "text": "so we're running slurm jobs which are are just batch scripts usually at their",
    "start": "927600",
    "end": "932639"
  },
  {
    "text": "heart directly in those pods the pods as a result may be",
    "start": "932639",
    "end": "938360"
  },
  {
    "text": "fairly generously proportioned let's say we we've seen certain cluster",
    "start": "938360",
    "end": "943519"
  },
  {
    "text": "environments built under this kind of model where that may be a 20 40 gig even compute node image uh which as a",
    "start": "943519",
    "end": "950639"
  },
  {
    "text": "container is admittedly a little awkward um but the HPC folks are are still a",
    "start": "950639",
    "end": "956160"
  },
  {
    "text": "little sluggish to adopt a lot of container strategies um they're they're getting there when they get there",
    "start": "956160",
    "end": "961839"
  },
  {
    "text": "that'll be a lot easier to manage kubernetes is not involved in then scheduling and managing compute jobs",
    "start": "961839",
    "end": "968000"
  },
  {
    "text": "that are running under slurm with slurm running under kubernetes uh Kubernetes doesn't see",
    "start": "968000",
    "end": "974600"
  },
  {
    "text": "them and also doesn't sort of interfere with their dispatch this lets slurm run",
    "start": "974600",
    "end": "980160"
  },
  {
    "text": "the way it wants to run a cluster um it is able to efficiently manage fine grain resource limits on those nodes it's able",
    "start": "980160",
    "end": "987440"
  },
  {
    "text": "to do the back fill scheduling it's able to manage network topology and do everything it's expecting to for jobs",
    "start": "987440",
    "end": "994880"
  },
  {
    "text": "modeled as slurm workloads documentation uh we actually",
    "start": "994880",
    "end": "1000720"
  },
  {
    "text": "just got this out last month um slinky.skemd.com has our initial cut uh we will certainly be expanding this to",
    "start": "1000720",
    "end": "1007199"
  },
  {
    "text": "better document how to spin this up into different cloud environments um especially different managed Kubernetes",
    "start": "1007199",
    "end": "1012560"
  },
  {
    "text": "environments the broad picture here again uh the slurm operators plugged straight into the cube API uh communicates with",
    "start": "1012560",
    "end": "1020160"
  },
  {
    "text": "slurm's control plane exclusively through slurm's rest api slurm's control plane is a couple different components",
    "start": "1020160",
    "end": "1026720"
  },
  {
    "text": "principally the slurm controller slurm controld uh uh from the HPC space we",
    "start": "1026720",
    "end": "1032319"
  },
  {
    "text": "call it controld not cuddled d um you can fight with us later on that we also",
    "start": "1032319",
    "end": "1037839"
  },
  {
    "text": "have an accounting process called the slur dbd slurm database um that's driving a lot of granular resource",
    "start": "1037839",
    "end": "1043678"
  },
  {
    "text": "limits for different users on that machine it's talking to a managed MARB instance and then we have the compute",
    "start": "1043679",
    "end": "1051200"
  },
  {
    "text": "nodes modeled by a whole bunch of independent Stormd processes all on separate pods on the",
    "start": "1051200",
    "end": "1056440"
  },
  {
    "text": "cluster last but not least uh metrics are there able to drive uh autoscaling",
    "start": "1056440",
    "end": "1062720"
  },
  {
    "text": "decisions as well so this is built off of a couple uh custom resources there's",
    "start": "1062720",
    "end": "1068000"
  },
  {
    "text": "a cluster CR a node set CR um these need to get installed off into the control",
    "start": "1068000",
    "end": "1073120"
  },
  {
    "text": "plane to then build the rest of this model off of um and I'm going to kind of jump through these a little quickly uh I",
    "start": "1073120",
    "end": "1079200"
  },
  {
    "text": "did attach the slides to uh this schedule saw in the schedule if you guys want to download them and look at these",
    "start": "1079200",
    "end": "1084960"
  },
  {
    "text": "in a little more detail um and the kind of one uh",
    "start": "1084960",
    "end": "1091760"
  },
  {
    "text": "operator slide demo slide um is here just showing that for a user logging",
    "start": "1091760",
    "end": "1098480"
  },
  {
    "text": "into this machine logging into the system they're just seeing a traditional slurm environment they're able to submit",
    "start": "1098480",
    "end": "1104160"
  },
  {
    "text": "workload into slurm it's queuing it up prioritizing it much the same as it would any other workload and then we're",
    "start": "1104160",
    "end": "1110960"
  },
  {
    "text": "dispatching it out that out onto the cluster um in reaction to this cluster",
    "start": "1110960",
    "end": "1116960"
  },
  {
    "text": "load the autoscaler here is kicking in um and it's able to scale uh if you're",
    "start": "1116960",
    "end": "1122720"
  },
  {
    "text": "using ketta from zero up to however many nodes you care to feed into this cluster",
    "start": "1122720",
    "end": "1128120"
  },
  {
    "text": "environment um and it's able to do that in response to the demand that the slurm",
    "start": "1128120",
    "end": "1133200"
  },
  {
    "text": "cluster itself is seeing internally",
    "start": "1133200",
    "end": "1137320"
  },
  {
    "text": "onto the slurm bridge this is what we're working on right now plugging slurm scheduling directly into the Kubernetes",
    "start": "1140559",
    "end": "1147360"
  },
  {
    "text": "scheduling APIs why do we want to do this um Kubernetes lacks this idea of fine grain",
    "start": "1147360",
    "end": "1153280"
  },
  {
    "text": "control of native resources CPU memory um in ways that",
    "start": "1153280",
    "end": "1159160"
  },
  {
    "text": "are usable to a certain class of HPC applications that exist um so we expect",
    "start": "1159160",
    "end": "1165200"
  },
  {
    "text": "that there are still HPC applications that want those semantics and want to be",
    "start": "1165200",
    "end": "1170320"
  },
  {
    "text": "able to run underneath slurm but be co-resident on a system that is also running Kubernetes",
    "start": "1170320",
    "end": "1176840"
  },
  {
    "text": "workloads we also want to be able to do faster scheduling than we believe is",
    "start": "1176840",
    "end": "1182160"
  },
  {
    "text": "possible natively within the Kubernetes stack um and mix and match which tooling",
    "start": "1182160",
    "end": "1187200"
  },
  {
    "text": "and which ecosystem to bring these batch workloads in um if you have simulation",
    "start": "1187200",
    "end": "1192240"
  },
  {
    "text": "work that's best modeled as a Kubernetes deployment with a bunch of different pods we want to be able to schedule and",
    "start": "1192240",
    "end": "1199039"
  },
  {
    "text": "manage that on the same hardware as native slurm workloads running as slurp",
    "start": "1199039",
    "end": "1205600"
  },
  {
    "text": "jobs offsetting this is that this is not necessarily a perfect solution for everyone where we're not proposing to",
    "start": "1206840",
    "end": "1213440"
  },
  {
    "text": "replace the default scheduler certainly um it's meant as another alternative to exist in this ecosystem um the",
    "start": "1213440",
    "end": "1220160"
  },
  {
    "text": "Kubernetes API does very conveniently make it possible to provision multiple schedulers um this is the same approach",
    "start": "1220160",
    "end": "1225600"
  },
  {
    "text": "Q volcano the MPI operator the plethora of other sort of batch computing oriented projects have using have used",
    "start": "1225600",
    "end": "1233520"
  },
  {
    "text": "to build their own systems and own scheduling capabilities",
    "start": "1233520",
    "end": "1238720"
  },
  {
    "text": "one huge gotcha and something that we are working with the broader community on is this idea that the Kubernetes API",
    "start": "1238720",
    "end": "1244720"
  },
  {
    "text": "doesn't currently have a way to subdivide a nodes",
    "start": "1244720",
    "end": "1251240"
  },
  {
    "text": "resources exclusively for the remit of different scheduling plugins operating",
    "start": "1251240",
    "end": "1256400"
  },
  {
    "text": "on the system um so currently the slurm bridge is assuming that it exclusively",
    "start": "1256400",
    "end": "1262080"
  },
  {
    "text": "owns resources in a given node that you assign to it that may or may not be a",
    "start": "1262080",
    "end": "1267760"
  },
  {
    "text": "workable assumption on your",
    "start": "1267760",
    "end": "1271039"
  },
  {
    "text": "machine so um in terms of its architecture everything we're building here is meant to be very flexible which",
    "start": "1274840",
    "end": "1281039"
  },
  {
    "text": "also makes it a little hard to draw diagrams we are able to for a portion of the",
    "start": "1281039",
    "end": "1288480"
  },
  {
    "text": "machine that you want to overlap between Kubernetes's orchestration and slurm's",
    "start": "1288480",
    "end": "1295039"
  },
  {
    "text": "resource management we're able to bring the slurm bridge to bear on that set of compute nodes it does not need to be a",
    "start": "1295039",
    "end": "1302000"
  },
  {
    "text": "perfectly overlapped set of compute nodes though you may well have parts of the cluster environment that are",
    "start": "1302000",
    "end": "1307919"
  },
  {
    "text": "dedicated exclusively to running Kubernetes workloads or potentially parts of that same system that are",
    "start": "1307919",
    "end": "1313200"
  },
  {
    "text": "exclusively set up and managed with slurm so this vin diagram may not perfectly",
    "start": "1313200",
    "end": "1319400"
  },
  {
    "text": "overlap but the slurm bridge sits at the intersection of those two spaces we do",
    "start": "1319400",
    "end": "1325360"
  },
  {
    "text": "expect most deployments will probably look kind of like this instead which definitely makes everything a little",
    "start": "1325360",
    "end": "1330799"
  },
  {
    "text": "simpler to talk about so the design goals here is to run both slur and Kubernetes workloads on that",
    "start": "1330799",
    "end": "1338400"
  },
  {
    "text": "overlap pool of nodes that sits in the middle here the slurm bridge translates",
    "start": "1338400",
    "end": "1343520"
  },
  {
    "text": "the resource requirements for Kubernetes workloads into corresponding slurm jobs we call these placeholder jobs within",
    "start": "1343520",
    "end": "1349880"
  },
  {
    "text": "slurm we're able to reconstruct multi-node workloads coming from",
    "start": "1349880",
    "end": "1354960"
  },
  {
    "text": "Kubernetes um whether they're pod group job set we'll probably add support for leader worker set and uh whichever other",
    "start": "1354960",
    "end": "1362720"
  },
  {
    "text": "representations of multi-node batch computing in Kubernetes we find interesting to to translate translate",
    "start": "1362720",
    "end": "1369280"
  },
  {
    "text": "those into a single job within slurm's control plane so slurm can provision and",
    "start": "1369280",
    "end": "1374960"
  },
  {
    "text": "manage that multi-node job as a multi-node job from its own scheduling standpoint",
    "start": "1374960",
    "end": "1381600"
  },
  {
    "text": "we also need to handle device plugins GPU uh we need to be able to plug into the DRRA ecosystem and manage that",
    "start": "1381600",
    "end": "1388320"
  },
  {
    "text": "translate those resource requests translate the resource claims back out the other direction we also want to be",
    "start": "1388320",
    "end": "1394240"
  },
  {
    "text": "able to filter out nodes that SLRM is not intended to manage um we also need to filter out pods that slur is not",
    "start": "1394240",
    "end": "1400320"
  },
  {
    "text": "intended to manage uh we don't want to mess with the Damon sets we also don't want to manage everything in the core",
    "start": "1400320",
    "end": "1405919"
  },
  {
    "text": "control plane if you're running Slurm's own control stack under cube we don't",
    "start": "1405919",
    "end": "1411679"
  },
  {
    "text": "want the bridge to attempt to schedule the slurm controller by talking to the",
    "start": "1411679",
    "end": "1417520"
  },
  {
    "text": "slurm controller because that little catch22 is not going to let us",
    "start": "1417520",
    "end": "1423320"
  },
  {
    "text": "work so there are a number of these restrictions here um at the moment each node can run slurm or kubernetes",
    "start": "1423320",
    "end": "1430320"
  },
  {
    "text": "workloads we can overlap multiple slurm jobs on a node we can overlap multiple",
    "start": "1430320",
    "end": "1435440"
  },
  {
    "text": "cube pods on a node but we can't safely mix and match uh the reason for this is",
    "start": "1435440",
    "end": "1440640"
  },
  {
    "text": "that the slurm workloads are going to be pinned to specific CPU sets on the node",
    "start": "1440640",
    "end": "1447200"
  },
  {
    "text": "and the Kubernetes ones won't this means that the slurm workloads would get",
    "start": "1447200",
    "end": "1453039"
  },
  {
    "text": "stepped on potentially by anything running as a Kubernetes pod introducing a lot of undesired jitter into the slurm",
    "start": "1453039",
    "end": "1459840"
  },
  {
    "text": "workloads this is where one of the things that we're working on longer term is this this discussion around adding",
    "start": "1459840",
    "end": "1466400"
  },
  {
    "text": "semantics for CPU allocation info into the cube APIs um if we can do that or",
    "start": "1466400",
    "end": "1473039"
  },
  {
    "text": "have a model that lets that work then we can safely mix and match the two between them you need to configure this bridge",
    "start": "1473039",
    "end": "1479360"
  },
  {
    "text": "as a cube scheduling profile um it takes control of everything in that allow list of the name spaces you've provided as",
    "start": "1479360",
    "end": "1485360"
  },
  {
    "text": "part of the setup um default is still there and handling workloads elsewhere on the machine potentially including",
    "start": "1485360",
    "end": "1491440"
  },
  {
    "text": "slurm own control plane slurm only schedules the nodes with a slurmd running which is something we'll talk",
    "start": "1491440",
    "end": "1497760"
  },
  {
    "text": "about again in a second so this is broken up into a pair",
    "start": "1497760",
    "end": "1502960"
  },
  {
    "text": "of pieces one is theuler plugin and a second part we call the workload controller um these are tied into the",
    "start": "1502960",
    "end": "1511200"
  },
  {
    "text": "two different chunks of the cube scheduling framework that we need to integrate with to get this to work this",
    "start": "1511200",
    "end": "1518559"
  },
  {
    "text": "is um one of the reference diagrams that the cube scheduling group has um and then this is the way we've sort of cut",
    "start": "1518559",
    "end": "1525440"
  },
  {
    "text": "it down considerably to just translate what we need to off the cube scheduling",
    "start": "1525440",
    "end": "1530559"
  },
  {
    "text": "APIs into something that the bridge can consume inject into slurm as a",
    "start": "1530559",
    "end": "1535919"
  },
  {
    "text": "placeholder job and then once slurm actually schedules and lands um jobs out",
    "start": "1535919",
    "end": "1542240"
  },
  {
    "text": "onto compute nodes we translate those back into bind API calls within the cube",
    "start": "1542240",
    "end": "1547360"
  },
  {
    "text": "APIs to inform Kubernetes to go ahead and launch the cube workloads so the",
    "start": "1547360",
    "end": "1553440"
  },
  {
    "text": "broad model here is to translate everything into a slurm job slurm is then able to",
    "start": "1553440",
    "end": "1559640"
  },
  {
    "text": "prioritize those placeholder jobs representing Kubernetes pods alongside",
    "start": "1559640",
    "end": "1566000"
  },
  {
    "text": "any work that is submitted and being managed as direct slurm workload",
    "start": "1566000",
    "end": "1571360"
  },
  {
    "text": "filter through that with all of Slurm's complex priority models backfill scheduling support and then make those",
    "start": "1571360",
    "end": "1578240"
  },
  {
    "text": "placement decisions as to when and where these different workloads are going to land on the machine for the",
    "start": "1578240",
    "end": "1583440"
  },
  {
    "text": "Kubernetes-based stuff the workload controller here has to kick back in and",
    "start": "1583440",
    "end": "1589120"
  },
  {
    "text": "communicate back out through the bind API calls to the cube stack for native slurm workloads none of this kicks in",
    "start": "1589120",
    "end": "1595440"
  },
  {
    "text": "the native slurm workloads are just running under slurm's own resource management layer built into the cluster",
    "start": "1595440",
    "end": "1602639"
  },
  {
    "text": "so I'm just going to very briefly flip through some of the demo screenshots here i'm not going to try to run this live uh because the internet",
    "start": "1604400",
    "end": "1610799"
  },
  {
    "text": "connection's a little fiddly this is a pod coming in uh just getting applied into the cluster slurm",
    "start": "1610799",
    "end": "1617520"
  },
  {
    "text": "is translating it um and then running it on this node uh node name is slurmbridge",
    "start": "1617520",
    "end": "1623039"
  },
  {
    "text": "one you can see both from sq the slurm command that's monitoring the qate it's",
    "start": "1623039",
    "end": "1628159"
  },
  {
    "text": "landed it on slurp bridge one and communicated that placement decision back through the bind API call landing",
    "start": "1628159",
    "end": "1634080"
  },
  {
    "text": "it on slurp one this is",
    "start": "1634080",
    "end": "1640278"
  },
  {
    "text": "uh that same job just calling out as part of this uh we have these annotations that we've attached here",
    "start": "1640640",
    "end": "1646880"
  },
  {
    "text": "slurm node is indicating where we've placed it and then importantly the slurm job ID label maps into that placeholder",
    "start": "1646880",
    "end": "1654080"
  },
  {
    "text": "job within slurm's own control plane and is used to cross tie the two entities back together this is quickly going through a",
    "start": "1654080",
    "end": "1662159"
  },
  {
    "text": "two pod damon set uh applying this",
    "start": "1662159",
    "end": "1667679"
  },
  {
    "text": "setting it up to get scheduled we can see here that the two pods are being translated into a single slurm job",
    "start": "1667679",
    "end": "1675520"
  },
  {
    "text": "requiring two nodes um this is kind of a key capability of this of natively",
    "start": "1675520",
    "end": "1681120"
  },
  {
    "text": "treating these multi-node pod groups as single slurm jobs and scheduling and",
    "start": "1681120",
    "end": "1686720"
  },
  {
    "text": "managing them as proper multi-node job entities and then placing them out on",
    "start": "1686720",
    "end": "1691919"
  },
  {
    "text": "the system um you can see here it landed it on uh slurp bridge one and",
    "start": "1691919",
    "end": "1697080"
  },
  {
    "text": "two this is again similar uh thing running through rather than using a",
    "start": "1697080",
    "end": "1702159"
  },
  {
    "text": "replica set to build the pair of pods uh it's actually just enumerating them directly as part of the pod group uh",
    "start": "1702159",
    "end": "1708559"
  },
  {
    "text": "this is being submitted while this other job is sitting there in this environment we only have access to three compute",
    "start": "1708559",
    "end": "1714480"
  },
  {
    "text": "nodes both of the jobs want two something is going to have to wait we",
    "start": "1714480",
    "end": "1719760"
  },
  {
    "text": "don't attempt to schedule half of the job we know that we have to queue it up and wait until sufficient resources what",
    "start": "1719760",
    "end": "1727120"
  },
  {
    "text": "we're calling out here are available for this to actually execute and run on the cluster",
    "start": "1727120",
    "end": "1732720"
  },
  {
    "text": "then uh that first workload on this slide finally finished that second pod",
    "start": "1732720",
    "end": "1738159"
  },
  {
    "text": "group actually came in has finally been able to execute on SER bridge one and two and then uh last but not least",
    "start": "1738159",
    "end": "1745520"
  },
  {
    "text": "everything is wrapped up here it's just calling attention to the fact that if the cube pods have terminated the bridge",
    "start": "1745520",
    "end": "1753440"
  },
  {
    "text": "and the work cycle workflow controller part of the",
    "start": "1753440",
    "end": "1758480"
  },
  {
    "text": "bridge is kicking in and making sure that the corresponding slurm jobs are being terminated as well if the slurm",
    "start": "1758480",
    "end": "1765440"
  },
  {
    "text": "jobs are terminated or killed or hit a time limit the corresponding pods will",
    "start": "1765440",
    "end": "1771039"
  },
  {
    "text": "also be deleted future work there's a lot of",
    "start": "1771039",
    "end": "1776720"
  },
  {
    "text": "different things that we're we're expecting to expand on these capabilities with um one of the things that we're pushing for especially is",
    "start": "1776720",
    "end": "1782880"
  },
  {
    "text": "this idea of DRRA but for CPU management on the compute nodes um I'm talking with a lot of different people this week",
    "start": "1782880",
    "end": "1788880"
  },
  {
    "text": "about ways to to push for that other things that we're expecting to do um is",
    "start": "1788880",
    "end": "1794000"
  },
  {
    "text": "to be able to run slurm not just with this hybrid environment where the slurmd",
    "start": "1794000",
    "end": "1799840"
  },
  {
    "text": "compute node image still needs to be running on a piece of hardware but be able to run it as just a pure Kubernetes",
    "start": "1799840",
    "end": "1807080"
  },
  {
    "text": "scheduler without slurm's resource management layer running underneath it",
    "start": "1807080",
    "end": "1812159"
  },
  {
    "text": "um that's something we expect probably later this summer we'll have that capability as well",
    "start": "1812159",
    "end": "1818158"
  },
  {
    "text": "uh and then with that thank you guys any questions",
    "start": "1818799",
    "end": "1824180"
  },
  {
    "text": "[Applause]",
    "start": "1824180",
    "end": "1832069"
  },
  {
    "text": "hi this is Can you hear me yeah hi this is Abhishek from IBM research uh great",
    "start": "1836960",
    "end": "1842159"
  },
  {
    "text": "presentation um so I guess u the translation that you showed from Kubernetes to slum um it was mostly",
    "start": "1842159",
    "end": "1850279"
  },
  {
    "text": "around maybe static CPU resources but an",
    "start": "1850279",
    "end": "1855360"
  },
  {
    "text": "AI training would need multiple ns networks so there are a lot of other parameters that a pod or a job could",
    "start": "1855360",
    "end": "1862640"
  },
  {
    "text": "request so what's the story there so what what I'm not showing is that yes",
    "start": "1862640",
    "end": "1868480"
  },
  {
    "text": "we're looking at other resource requirements that the pods may have um millores memory requirements we're",
    "start": "1868480",
    "end": "1874880"
  },
  {
    "text": "translating that into the slurm resource request as well we're expecting to be able to take certain flavors of DRL",
    "start": "1874880",
    "end": "1884159"
  },
  {
    "text": "syntax and translate that directly into requests for GPUs for example um the",
    "start": "1884159",
    "end": "1889600"
  },
  {
    "text": "other thing that I dropped these slides out here is that we do have an extensive set of annotations corresponding to",
    "start": "1889600",
    "end": "1895760"
  },
  {
    "text": "other slurm native resource types and resource models that let the pod request",
    "start": "1895760",
    "end": "1901440"
  },
  {
    "text": "things that we can't directly infer from the pod um which is g gives us kind of an un end that end run around some",
    "start": "1901440",
    "end": "1909279"
  },
  {
    "text": "limitations of of sort of the description that exists today and gives us our own syntactical sort of nuance",
    "start": "1909279",
    "end": "1915120"
  },
  {
    "text": "that we can provide thank you thanks",
    "start": "1915120",
    "end": "1921559"
  },
  {
    "text": "uh say I have um a static set of nodes say 100 nodes um and I want to run a lot",
    "start": "1921600",
    "end": "1928799"
  },
  {
    "text": "of Kubernetes jobs on them sometimes i want to run a lot of uh slurm jobs sometimes and it's basically chaotic",
    "start": "1928799",
    "end": "1934000"
  },
  {
    "text": "whether that happens how easy is it right now for it to switch between Kubernetes mode and slurm mode on the",
    "start": "1934000",
    "end": "1940880"
  },
  {
    "text": "entire set of nodes or a subset can it be done dynamically at runtime or so on is it a config I've set that sort of",
    "start": "1940880",
    "end": "1946159"
  },
  {
    "text": "thing so that that's going to depend a lot on how you've tuned Slurm's priority mechanisms um you can adjust on the fly",
    "start": "1946159",
    "end": "1954240"
  },
  {
    "text": "the importance of different partitions um partitions for slurm are different cues um you you could go and alter the",
    "start": "1954240",
    "end": "1961440"
  },
  {
    "text": "config to say that you want that partition to have the highest service level right now make sure stuff in there",
    "start": "1961440",
    "end": "1967600"
  },
  {
    "text": "which is corresponding to the Kubernetes workload that that we're bridging in um make sure that gets the highest service",
    "start": "1967600",
    "end": "1973679"
  },
  {
    "text": "class and then you can alter the the cluster configuration to flip that back on its head later um so this does give",
    "start": "1973679",
    "end": "1979840"
  },
  {
    "text": "you a way to potentially shift the focus of those workloads without needing to",
    "start": "1979840",
    "end": "1986240"
  },
  {
    "text": "manually reprovision the compute hardware within the cluster between the two control",
    "start": "1986240",
    "end": "1992880"
  },
  {
    "text": "planes okay thank you very much uh uh so uh from your design here",
    "start": "1992880",
    "end": "2001600"
  },
  {
    "text": "there's the slam d operator and the slam breeze there i have two questions the",
    "start": "2001600",
    "end": "2007760"
  },
  {
    "text": "first question is when the slumber D to control nodes there is is it a can",
    "start": "2007760",
    "end": "2014960"
  },
  {
    "text": "control the pure bare metal or is kind of a po contain ps or something there so",
    "start": "2014960",
    "end": "2022240"
  },
  {
    "text": "the the slurmd process traditionally runs on bare metal um on HPC systems um",
    "start": "2022240",
    "end": "2027679"
  },
  {
    "text": "but it it also runs just fine in the pod the one caveat there being that if it's",
    "start": "2027679",
    "end": "2033200"
  },
  {
    "text": "running in a pod that container image is what slurm jobs will see and execute",
    "start": "2033200",
    "end": "2039640"
  },
  {
    "text": "under um so if they're expecting usually a fairly fully featured Linux",
    "start": "2039640",
    "end": "2045120"
  },
  {
    "text": "distribution to be available there um if you're not able to provide that as part of that container image for the slurmd",
    "start": "2045120",
    "end": "2051118"
  },
  {
    "text": "process uh that that actual slurm job itself may have a problem executing and",
    "start": "2051119",
    "end": "2057358"
  },
  {
    "text": "running um that's where we expect people will put a lot of software into those",
    "start": "2057359",
    "end": "2062398"
  },
  {
    "text": "initially um within the HPC space if you can",
    "start": "2062399",
    "end": "2067560"
  },
  {
    "text": "model your slurm job with containers all of a sudden that means that you can",
    "start": "2067560",
    "end": "2073040"
  },
  {
    "text": "reduce the footprint of that that compute node container image itself uh considerably but that ecosystem is still",
    "start": "2073040",
    "end": "2080800"
  },
  {
    "text": "a little slow to come around to the idea of containerizing all all of these existing applications um so it's a work",
    "start": "2080800",
    "end": "2086960"
  },
  {
    "text": "in progress there okay thanks uh the second question so comes to the slum",
    "start": "2086960",
    "end": "2092878"
  },
  {
    "text": "bridge here so when uh considering this is a GPU situation like uh thinking",
    "start": "2092879",
    "end": "2099520"
  },
  {
    "text": "about the granularity here uh what what's the granularity level here when",
    "start": "2099520",
    "end": "2105280"
  },
  {
    "text": "we sign it would like the for example if 80 GPU in one node there would control",
    "start": "2105280",
    "end": "2111520"
  },
  {
    "text": "per node per GPU or even smaller than per DPU like a sharing resource there so",
    "start": "2111520",
    "end": "2118800"
  },
  {
    "text": "what's kind of the current situation depends on configuration um is the complicated answer so slurm itself",
    "start": "2118800",
    "end": "2125760"
  },
  {
    "text": "natively can model usually a GPU as an individual allocatable entity um that's",
    "start": "2125760",
    "end": "2132800"
  },
  {
    "text": "usually the level that most systems run with um but slurm does support Nvidia",
    "start": "2132800",
    "end": "2138720"
  },
  {
    "text": "MPS um it can work with Nvidia's MIG mode of operation um although it does require static partitioning for that um",
    "start": "2138720",
    "end": "2146640"
  },
  {
    "text": "and that's something that we can extend in the future if we need to um but it it",
    "start": "2146640",
    "end": "2152000"
  },
  {
    "text": "depends on what sort of the the cluster preference and policy are and and what you're trying to accomplish the first",
    "start": "2152000",
    "end": "2158560"
  },
  {
    "text": "pass for the slurm bridge is really oriented around the the place we think we can",
    "start": "2158560",
    "end": "2163839"
  },
  {
    "text": "make the biggest impact right now is is better management for multi-node workloads",
    "start": "2163839",
    "end": "2169480"
  },
  {
    "text": "um but th those are things that we're looking at at better serving in the",
    "start": "2169480",
    "end": "2174640"
  },
  {
    "text": "future and again a large part of that I think is going to be ways to talk about CPU affinity management and CPU resource",
    "start": "2174640",
    "end": "2182040"
  },
  {
    "text": "management alongside the GPUs okay sounds good thank you thank you",
    "start": "2182040",
    "end": "2187359"
  },
  {
    "text": "very much and a good presentation yes",
    "start": "2187359",
    "end": "2192640"
  },
  {
    "text": "hi great talk um questions about elastic workloads do you support them do you have thoughts about that you said leader",
    "start": "2192640",
    "end": "2199760"
  },
  {
    "text": "worker set for instance what about something that grows or starts in two phases you know what are your thoughts",
    "start": "2199760",
    "end": "2205839"
  },
  {
    "text": "uh elastic workloads in an HPC context are tricky um because they directly",
    "start": "2205839",
    "end": "2212320"
  },
  {
    "text": "interfere with the scheduler's ability to plan for future use of the system",
    "start": "2212320",
    "end": "2219400"
  },
  {
    "text": "um there is support in slurm for being able to do that but it's usually",
    "start": "2219400",
    "end": "2224640"
  },
  {
    "text": "disabled in most clusters by default um there are definitely things I think we could improve there and that that is",
    "start": "2224640",
    "end": "2231040"
  },
  {
    "text": "actually a thing I think we could bring back from the cloud native workloads better into the HPC space um that that",
    "start": "2231040",
    "end": "2237920"
  },
  {
    "text": "there are certainly some development projects there that could better tackle that um I'm happy to talk about that",
    "start": "2237920",
    "end": "2243119"
  },
  {
    "text": "more at length at at some other point thank you um okay I know I am at least five",
    "start": "2243119",
    "end": "2250400"
  },
  {
    "text": "minutes over time uh okay one last question quickly",
    "start": "2250400",
    "end": "2256119"
  },
  {
    "text": "uh so it's a question about um whether you can use cgroups uh with the slurm",
    "start": "2256400",
    "end": "2262000"
  },
  {
    "text": "bridge um because obviously containers are using croups underneath and I know",
    "start": "2262000",
    "end": "2267680"
  },
  {
    "text": "slurm can uh use cgroups for resource control how does that work that is the",
    "start": "2267680",
    "end": "2274000"
  },
  {
    "text": "one complication with running slurm in a pod having the slurm processors live in",
    "start": "2274000",
    "end": "2280160"
  },
  {
    "text": "the pod um whether or not you can successfully delegate to slurm the ability to further subdivide those",
    "start": "2280160",
    "end": "2287280"
  },
  {
    "text": "depends on which flavor of croups you have enabled and a number of other things um that's why for those workloads",
    "start": "2287280",
    "end": "2294400"
  },
  {
    "text": "I would usually suggest sticking to having the slurd process run on the bare metal um but we're working to make sure",
    "start": "2294400",
    "end": "2301520"
  },
  {
    "text": "we can cooperate with the higher level croup hierarchies uh as best we can um",
    "start": "2301520",
    "end": "2308000"
  },
  {
    "text": "it varies a little bit between croup v1 and croup v2 unfortunately um and that that's a fight we've been well aware of",
    "start": "2308000",
    "end": "2316480"
  },
  {
    "text": "um but but don't have any magical solution to uh in in all",
    "start": "2316480",
    "end": "2322000"
  },
  {
    "text": "cases uh with that thank you guys um I will be floating around all week i'm happy to take other questions here but I",
    "start": "2324920",
    "end": "2331280"
  },
  {
    "text": "know I am already massively over time",
    "start": "2331280",
    "end": "2335040"
  }
]