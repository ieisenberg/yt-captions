[
  {
    "text": "hello everyone Welcome to The Rook intro and deep di with SEF talk I'm here with",
    "start": "320",
    "end": "8120"
  },
  {
    "text": "bla yeah I'm blae Gardner I'm a storage engineer at",
    "start": "8120",
    "end": "14080"
  },
  {
    "text": "IBM and Rec maintainer hello I'm I'm SEF and open",
    "start": "14320",
    "end": "23240"
  },
  {
    "text": "shap data Foundation which is work and St and lots of cool stuff in open shift at IBM architect at IBM",
    "start": "23240",
    "end": "31640"
  },
  {
    "text": "M and I'm JC Lopez so I do uh pre-sales and Consulting and whatever you want",
    "start": "31640",
    "end": "37800"
  },
  {
    "text": "around RF and anything containers related to",
    "start": "37800",
    "end": "42960"
  },
  {
    "text": "storage so and really quick to myself I'm also maintain of The Rook project and um working for C Technologies in",
    "start": "43160",
    "end": "50360"
  },
  {
    "text": "ging engineer okay so what do we want to talk about today in The Rook intro and",
    "start": "50360",
    "end": "56879"
  },
  {
    "text": "deep dive we want to give you an introduction to rook and SE for anyone who's new we want to talk about the",
    "start": "56879",
    "end": "63280"
  },
  {
    "text": "state of The Rook project what's in the making any cool features we have cooking up and some real life example and",
    "start": "63280",
    "end": "71000"
  },
  {
    "text": "additionally which I think is interesting for a lot of people also the newcomers to Rook is application",
    "start": "71000",
    "end": "77040"
  },
  {
    "text": "disaster recovery and especially also day two because second day",
    "start": "77040",
    "end": "83759"
  },
  {
    "text": "operations well first day it works second day it's",
    "start": "83759",
    "end": "88840"
  },
  {
    "text": "uh broken we don't want wanted so let's talk about it so first of all a few",
    "start": "88840",
    "end": "94159"
  },
  {
    "text": "questions um who's here to learn about Rook for the first time quite quite a few people has anyone",
    "start": "94159",
    "end": "103000"
  },
  {
    "text": "of you already experimented with Rook okay and who has Rook deployed in",
    "start": "103000",
    "end": "110040"
  },
  {
    "text": "production but like I guess we can see production is a bit of a stretchy term even like in test environment okay and",
    "start": "110040",
    "end": "117759"
  },
  {
    "text": "question which is not on the slide um where do you run Rook do you run it",
    "start": "117759",
    "end": "123479"
  },
  {
    "text": "on uh in the cloud in a virtualized",
    "start": "123479",
    "end": "128560"
  },
  {
    "text": "environment on Prem okay on bare metal on Prem then few",
    "start": "129440",
    "end": "136200"
  },
  {
    "text": "more people okay on Prim and bare metal okay um yeah let's talk about",
    "start": "136200",
    "end": "144760"
  },
  {
    "text": "Rook yeah that's you um yeah to get us started I want to",
    "start": "145800",
    "end": "151160"
  },
  {
    "text": "talk talk about just kind of an introduction to Rook starting from the",
    "start": "151160",
    "end": "156480"
  },
  {
    "text": "very beginning what are the questions that led to Rook um so in a in a cloud world where Cloud",
    "start": "156480",
    "end": "164720"
  },
  {
    "text": "providers are usually providing storage what you do in your own Data",
    "start": "164720",
    "end": "169879"
  },
  {
    "text": "Center and also like why you know what storage",
    "start": "169879",
    "end": "176640"
  },
  {
    "text": "is normally part of uh uh or like not part of the kubernetes cluster but why",
    "start": "176640",
    "end": "182319"
  },
  {
    "text": "why can't it be in the kubernetes cluster for the kubernetes cluster and then managed in the same way as",
    "start": "182319",
    "end": "187720"
  },
  {
    "text": "kubernetes applications so these questions and some iteration led us to some goals for rook",
    "start": "187720",
    "end": "195280"
  },
  {
    "text": "and so that's make storage available to kubernetes applications and have them be",
    "start": "195280",
    "end": "200799"
  },
  {
    "text": "kubernetes native um just like the storage that you would get from cloud",
    "start": "200799",
    "end": "206440"
  },
  {
    "text": "vendors and we also want to make that easy um so automating the deployment",
    "start": "206440",
    "end": "213239"
  },
  {
    "text": "configuration upgrades and at the time we were seeing",
    "start": "213239",
    "end": "219280"
  },
  {
    "text": "this new operator pattern and how good it was so we wanted to to implement that and use that and leverage that and we",
    "start": "219280",
    "end": "227080"
  },
  {
    "text": "also wanted to make sure that it was open source that it's like free for whoever wants",
    "start": "227080",
    "end": "233519"
  },
  {
    "text": "it also I think understandably the project did not want",
    "start": "234319",
    "end": "239920"
  },
  {
    "text": "to roll its own data uh platform uh but",
    "start": "239920",
    "end": "245920"
  },
  {
    "text": "instead looked at existing data platforms and what they offered and settled on seph and some of the reasons",
    "start": "245920",
    "end": "252239"
  },
  {
    "text": "why seph was chosen is because it is a distributed",
    "start": "252239",
    "end": "257239"
  },
  {
    "text": "software defined storage solution it provides all three major types of storage it provides block it provides",
    "start": "257239",
    "end": "263840"
  },
  {
    "text": "shared file system and it has Best in Class for for S3 apis and is often even",
    "start": "263840",
    "end": "269919"
  },
  {
    "text": "bug compatible with AWS and beyond that it's it was already W uh widely trusted in Enterprise used",
    "start": "269919",
    "end": "278479"
  },
  {
    "text": "by thousands of organizations and something that that I find just personally very cool is that",
    "start": "278479",
    "end": "285600"
  },
  {
    "text": "an old uh like a very longtime Seth contributor and user is CERN who uses it",
    "start": "285600",
    "end": "292280"
  },
  {
    "text": "for their Large Hadron Collider and their particle physics data um and they",
    "start": "292280",
    "end": "297720"
  },
  {
    "text": "have one of the largest Seth clusters in the world um additionally SEF is very",
    "start": "297720",
    "end": "306840"
  },
  {
    "text": "durable it's designed to be consistent so it's not eventually consistent it is consistent data safety is se's",
    "start": "306840",
    "end": "314160"
  },
  {
    "text": "priority um and it does this by offering sharding and this can be across availability zones or racks or nodes or",
    "start": "314160",
    "end": "320880"
  },
  {
    "text": "discs or whatever you know whatever structures you have in your data centers",
    "start": "320880",
    "end": "326680"
  },
  {
    "text": "the replication is configurable it's durable and even in disasters uh we've",
    "start": "326680",
    "end": "333440"
  },
  {
    "text": "seen that it's it's almost always possible to manually recover data even",
    "start": "333440",
    "end": "338759"
  },
  {
    "text": "in the worst possible disaster as far as the architectural",
    "start": "338759",
    "end": "344919"
  },
  {
    "text": "layers that we're dealing with um Rook itself is really just the management",
    "start": "344919",
    "end": "351120"
  },
  {
    "text": "layer um CSI we also have a a container storage interface driver and this",
    "start": "351120",
    "end": "356759"
  },
  {
    "text": "dynamically Provisions SEF storage and then mounts it to user applications so that when user",
    "start": "356759",
    "end": "363240"
  },
  {
    "text": "applications are running they get access to seph which is the data layer uh as directly as",
    "start": "363240",
    "end": "369199"
  },
  {
    "text": "possible neither Rook nor CSI sit between SEF and the application this",
    "start": "369199",
    "end": "375120"
  },
  {
    "text": "application directly to SEF installation can be via Helm charts",
    "start": "375120",
    "end": "381400"
  },
  {
    "text": "via tons of example manifests that we have um and there's also a quick start",
    "start": "381400",
    "end": "386680"
  },
  {
    "text": "guide uh at rip. I saw someone taking a picture sure I'll hold here for a second if there's",
    "start": "386680",
    "end": "392280"
  },
  {
    "text": "anyone um and you can just click get started and and that can get you",
    "start": "392280",
    "end": "397960"
  },
  {
    "text": "going um Rook can be installed in short",
    "start": "397960",
    "end": "403720"
  },
  {
    "text": "anywhere kubernetes runs this can be on cloud uh or on premises you can have",
    "start": "403720",
    "end": "409319"
  },
  {
    "text": "virtual Hardware bare metal hardware and even the underlying storage it can be discs attached directly to",
    "start": "409319",
    "end": "416160"
  },
  {
    "text": "your nodes it can be uh Cloud volume even like EBS and if you're just looking",
    "start": "416160",
    "end": "422199"
  },
  {
    "text": "to do some testing we even support loop back devices if you already have a SE cluster",
    "start": "422199",
    "end": "428840"
  },
  {
    "text": "Rook can attach to it and then help offer that native solution using",
    "start": "428840",
    "end": "434039"
  },
  {
    "text": "pre-existing storage and all of this kind of adds up to Rook being something that helps with",
    "start": "434039",
    "end": "440199"
  },
  {
    "text": "cross Cloud support um something that's dear to me is uh object storage provisioning so",
    "start": "440199",
    "end": "447199"
  },
  {
    "text": "Rook was an early investor in um self-service provisioning of object storage buckets um so this was done with",
    "start": "447199",
    "end": "455360"
  },
  {
    "text": "a project that uh provided object bucket claims uh and this was like back in 2019",
    "start": "455360",
    "end": "462039"
  },
  {
    "text": "and that evolved into a uh kuat enhancement project now which we're uh",
    "start": "462039",
    "end": "467639"
  },
  {
    "text": "helping to try to like Usher into beta uh which is uh uh this cozy project the",
    "start": "467639",
    "end": "474199"
  },
  {
    "text": "container object storage interface to allow a little bit more flexibility there",
    "start": "474199",
    "end": "479960"
  },
  {
    "text": "I'll pass it back to Alexander yeah so to talk about the uh",
    "start": "479960",
    "end": "485840"
  },
  {
    "text": "Rook project in the community as Blain pointed out Rook wants to be open source it's always as",
    "start": "485840",
    "end": "493199"
  },
  {
    "text": "far as I'm aware has been open source on the Apache 2.0 license we have uh for",
    "start": "493199",
    "end": "498560"
  },
  {
    "text": "communications we have slack GitHub discussions and so on and uh what is",
    "start": "498560",
    "end": "504720"
  },
  {
    "text": "especially vital to any open source project there's also quite some big companies behind it",
    "start": "504720",
    "end": "511680"
  },
  {
    "text": "that maintain contribute to it and uh it just shows in the numbers of",
    "start": "511680",
    "end": "517518"
  },
  {
    "text": "contributors to the G project and the amount of container downloads which I",
    "start": "517519",
    "end": "522640"
  },
  {
    "text": "think we're already at 400,000 I think now but the uh numers always a bit hard",
    "start": "522640",
    "end": "528560"
  },
  {
    "text": "to calculate based on like which architecture and the old uh dockerhub um download counts per architect EDS",
    "start": "528560",
    "end": "535880"
  },
  {
    "text": "well a lot of people use it a lot of people like it in the end it's the magic",
    "start": "535880",
    "end": "542079"
  },
  {
    "text": "of an operator for SEF storage in kubernetes we have uh graduated as a",
    "start": "542079",
    "end": "549680"
  },
  {
    "text": "project from the cncf and um well it's been quite some years",
    "start": "549680",
    "end": "555399"
  },
  {
    "text": "2024 it's October 2020 it's",
    "start": "555399",
    "end": "562720"
  },
  {
    "text": "um it's been some time but it's a time where as we've graduated project with",
    "start": "562720",
    "end": "568160"
  },
  {
    "text": "most of them it's more and more yeah evolving into what people need making it",
    "start": "568160",
    "end": "573839"
  },
  {
    "text": "even easier for them to run the SE storage in kues and um",
    "start": "573839",
    "end": "579040"
  },
  {
    "text": "yeah well I part partially talked about this already I said we graduated since",
    "start": "579040",
    "end": "585440"
  },
  {
    "text": "almost four years now um it's already been declared stable for quite some years um even yeah as a maintainer I",
    "start": "585440",
    "end": "592839"
  },
  {
    "text": "think saying it is always a bit uh finicky but I've been running rooki for",
    "start": "592839",
    "end": "598320"
  },
  {
    "text": "like oh damnn six years seven years or so and",
    "start": "598320",
    "end": "605079"
  },
  {
    "text": "um as we headed with the questions where people run Rook already like for example on Prem on bare metal there that's",
    "start": "605079",
    "end": "611320"
  },
  {
    "text": "exactly my use case um that I was able to cover with it in the beginning and well still am and um yeah I said with",
    "start": "611320",
    "end": "620079"
  },
  {
    "text": "the companies that contributed with people being able to use it as it is on the root project and downstream as a",
    "start": "620079",
    "end": "627000"
  },
  {
    "text": "product it's showing how stable it is especially also nowadays and with more",
    "start": "627000",
    "end": "632480"
  },
  {
    "text": "and more features making more and more targeted towards the kubernetes",
    "start": "632480",
    "end": "637639"
  },
  {
    "text": "environment regarding the release cycle we uh try to do a release every like four months that's the schedule um we",
    "start": "637639",
    "end": "644959"
  },
  {
    "text": "have uh the well in December 113 so this year December right yeah 113 coming this",
    "start": "644959",
    "end": "652560"
  },
  {
    "text": "December 113 in the next year then and we on the go have the uh pet releases if",
    "start": "652560",
    "end": "662680"
  },
  {
    "text": "there's not necessarily anything critical there are sometimes smaller features which are able to be uh put",
    "start": "662680",
    "end": "668440"
  },
  {
    "text": "into dep patch releases as well so um",
    "start": "668440",
    "end": "674160"
  },
  {
    "text": "yeah just that was still my",
    "start": "674360",
    "end": "680480"
  },
  {
    "text": "right well go that's fine so yes so uh",
    "start": "680480",
    "end": "686240"
  },
  {
    "text": "one of the things about you know all the real life um things that we've seen is that we see",
    "start": "686240",
    "end": "692519"
  },
  {
    "text": "that RF is just like go getting everywhere really it's really there's a great adoption to that and um obviously",
    "start": "692519",
    "end": "700360"
  },
  {
    "text": "the collaboration we have with different like uh universities and other places so",
    "start": "700360",
    "end": "706480"
  },
  {
    "text": "you can actually watch um a talk that we did right here on that link that will",
    "start": "706480",
    "end": "711959"
  },
  {
    "text": "tell you about the partnership that we have with all the different institutions around the globe and it's um it's very",
    "start": "711959",
    "end": "719200"
  },
  {
    "text": "interesting to see where it's actually popping out and who's using it is just like any Goos and bit different types of",
    "start": "719200",
    "end": "727720"
  },
  {
    "text": "universities so it's really interesting to just like go through that wanted to tell you about um",
    "start": "727720",
    "end": "735240"
  },
  {
    "text": "application and business continuity too and this is something that we didn't really have originally we were really",
    "start": "735240",
    "end": "742120"
  },
  {
    "text": "focused with RF thanks to SEF on high availability and um the fact that the",
    "start": "742120",
    "end": "748480"
  },
  {
    "text": "cluster would operational even if a a node was to go down or a single Drive",
    "start": "748480",
    "end": "753959"
  },
  {
    "text": "used to go down and as we saw more and more stageful applications getting into",
    "start": "753959",
    "end": "759760"
  },
  {
    "text": "the kubernetes environments obviously we realize that we needed more than just High availability so we started other",
    "start": "759760",
    "end": "767199"
  },
  {
    "text": "projects and we're going to name them on the next slide and it was how do we do Dr on top of ha uh incl these",
    "start": "767199",
    "end": "775480"
  },
  {
    "text": "environment for State full applications State L are slightly different so we really focus on stage four so for those",
    "start": "775480",
    "end": "781399"
  },
  {
    "text": "who are not familiar we just come back on two terms RPO and RTO so RPO is",
    "start": "781399",
    "end": "786760"
  },
  {
    "text": "recovery Point objective so where is the data when you're going to be able to do the recovery right so what basically",
    "start": "786760",
    "end": "793199"
  },
  {
    "text": "what amount of data have you lost uh until you can restart the application and RTO is recovery time objective which",
    "start": "793199",
    "end": "800079"
  },
  {
    "text": "is how long is it going to take you to get the application back up and",
    "start": "800079",
    "end": "805600"
  },
  {
    "text": "running so a chair remember what we said so the kubernetes cluster has built-in",
    "start": "805600",
    "end": "811079"
  },
  {
    "text": "ha and seph has the same functionality that basically Maps so it uses different",
    "start": "811079",
    "end": "816240"
  },
  {
    "text": "protocols but seph has built in ha with the monitors maintain the cluster map so",
    "start": "816240",
    "end": "821480"
  },
  {
    "text": "even if one mon dies so because the the monitor itself crashes the Pod or the",
    "start": "821480",
    "end": "827000"
  },
  {
    "text": "node where one of the mons is running goes down we're still highly available so we behave and we're aligned to with",
    "start": "827000",
    "end": "833720"
  },
  {
    "text": "what basically kubernetes does so we can limit the actual physical topology of",
    "start": "833720",
    "end": "839720"
  },
  {
    "text": "the cluster so when we deploy the RF cluster we can specify where we want the components to go so we can like if you",
    "start": "839720",
    "end": "847440"
  },
  {
    "text": "have a kubernetes cluster that is multi so you're going to be able to apply that very same topology and tell RF will",
    "start": "847440",
    "end": "854639"
  },
  {
    "text": "deploy my components aligned to the rack so that if I lose an entire rack my",
    "start": "854639",
    "end": "860000"
  },
  {
    "text": "cluster is still highly available so we can even just go even further we can",
    "start": "860000",
    "end": "865240"
  },
  {
    "text": "push to the extent rarely seen uh to most data centers you could actually do",
    "start": "865240",
    "end": "870759"
  },
  {
    "text": "that thanks to the SEF topology and the features that we have built in rook and the selectors and everything we use to",
    "start": "870759",
    "end": "876800"
  },
  {
    "text": "select where the pieces go now something that was missing kind of",
    "start": "876800",
    "end": "883320"
  },
  {
    "text": "uh in the beginning a lot of people said well it's a kubernetes cluster you know",
    "start": "883320",
    "end": "888399"
  },
  {
    "text": "we get we start pods we start application but with stateful came another need which is how do we back up",
    "start": "888399",
    "end": "895639"
  },
  {
    "text": "and restore an application right the state of the data is inside a PVC right",
    "start": "895639",
    "end": "900839"
  },
  {
    "text": "so we maintain the state of the data through that data so what we had ignored",
    "start": "900839",
    "end": "906040"
  },
  {
    "text": "in the beginning because of stat less became more of an Evidence we need to do what we used to do in older ways of",
    "start": "906040",
    "end": "912600"
  },
  {
    "text": "doing when we had like ra servers or main frames or whatever so provide the",
    "start": "912600",
    "end": "918320"
  },
  {
    "text": "concept of backup and restore so you're going to grab the state of the application or all the CR Secrets config",
    "start": "918320",
    "end": "924639"
  },
  {
    "text": "Maps whatever and also the content of the PVC so that when you want to do a restore not only you restore the actual",
    "start": "924639",
    "end": "931720"
  },
  {
    "text": "data but you also restore the context of all the CRS of the application at the moment you took the backup so that when",
    "start": "931720",
    "end": "937639"
  },
  {
    "text": "you restart the app after the restore you're in the exact same state so very",
    "start": "937639",
    "end": "943279"
  },
  {
    "text": "good for logical protection um it usually offers a good granularity depending on the tool you use to do the",
    "start": "943279",
    "end": "949639"
  },
  {
    "text": "backup some backup tools can actually do file level recovery others do not so",
    "start": "949639",
    "end": "956040"
  },
  {
    "text": "every implementation and the the the the tool you choose to do the backup will just like be different and offer various",
    "start": "956040",
    "end": "963360"
  },
  {
    "text": "levels of granularity when you do the backup specifically the data that is actually on the",
    "start": "963360",
    "end": "968959"
  },
  {
    "text": "PVC now the next step was how do we do Dr so business continuity and Celine 2",
    "start": "968959",
    "end": "975959"
  },
  {
    "text": "concept Metro drr so that you want to be able to basically lose the least possible amount of data and restart the",
    "start": "975959",
    "end": "983279"
  },
  {
    "text": "application as soon as possible so that's the going to be the lowest y the lowest RPO and the lowest R",
    "start": "983279",
    "end": "991680"
  },
  {
    "text": "RTO and we also have Regional Dr where your two data centers are too far apart",
    "start": "991680",
    "end": "996959"
  },
  {
    "text": "so that you can do synchronous replication so you need to do asynchronous replication and you assume",
    "start": "996959",
    "end": "1003519"
  },
  {
    "text": "and you accept that you're going to at some point be missing some data because of the a synchronous replication",
    "start": "1003519",
    "end": "1011240"
  },
  {
    "text": "process so for a remember that's the built-in uh set thing for backup so we",
    "start": "1011240",
    "end": "1017839"
  },
  {
    "text": "have external solution ions can be Valero for the people that run um open",
    "start": "1017839",
    "end": "1023279"
  },
  {
    "text": "shift it can be oadp based type of backup and restore everyone uses and",
    "start": "1023279",
    "end": "1030280"
  },
  {
    "text": "that's the whole beauty of kubernetes and the environment so you really choose the tool that fits the bill for Metro",
    "start": "1030280",
    "end": "1036600"
  },
  {
    "text": "and Regional Dr so Metro Dr actually leverages two separate kubernetes",
    "start": "1036600",
    "end": "1042480"
  },
  {
    "text": "clusters attached to the same external SEF cluster so when you want to do a fa",
    "start": "1042480",
    "end": "1048960"
  },
  {
    "text": "over what you do is that you fail over the app from one kubernetes cluster to the other one and you reattach the",
    "start": "1048960",
    "end": "1055679"
  },
  {
    "text": "resources the prist and volume claims and the the underlying PVS to the",
    "start": "1055679",
    "end": "1061039"
  },
  {
    "text": "application that is restarted in the other cluster so basically you lose no data your RTO is basically the time it's",
    "start": "1061039",
    "end": "1067760"
  },
  {
    "text": "going to take to trigger the failover of the containers from one cluster that died to another cluster always be",
    "start": "1067760",
    "end": "1075679"
  },
  {
    "text": "careful a lot of people like to say we want that as automatic as possible",
    "start": "1075679",
    "end": "1081120"
  },
  {
    "text": "there's always someone that needs to check if you really want to fail over because once you start the fail over",
    "start": "1081120",
    "end": "1086960"
  },
  {
    "text": "well before you you are able to fail back you need to wait for a full failover so that you avoid any problem",
    "start": "1086960",
    "end": "1093559"
  },
  {
    "text": "so you can automate that but just make sure on the checks you do um so we also",
    "start": "1093559",
    "end": "1099640"
  },
  {
    "text": "leverage another project open source project called Ramen drr that helps you automate the collection of the CRS of an",
    "start": "1099640",
    "end": "1106960"
  },
  {
    "text": "app so that we can basically scale down the app in the source cluster if the cluster is still",
    "start": "1106960",
    "end": "1113120"
  },
  {
    "text": "operational but you can also Force the failover to say I cannot contact that cluster so I cannot scale down the app",
    "start": "1113120",
    "end": "1120360"
  },
  {
    "text": "but the project is designed so that either the two clusters and you want to test the failover the project ryen drr",
    "start": "1120360",
    "end": "1126960"
  },
  {
    "text": "will actually scale down all the pods all the deployments and everything in the first cluster and then fail over the",
    "start": "1126960",
    "end": "1132799"
  },
  {
    "text": "app in the other cluster Regional Dr is the same concept except that we're going",
    "start": "1132799",
    "end": "1138200"
  },
  {
    "text": "to do a asynchronous replication between two clusters two Rook SEF clusters so we",
    "start": "1138200",
    "end": "1145240"
  },
  {
    "text": "uh can do the asynchronous mirroring for RBD based so RBD is a Blog device",
    "start": "1145240",
    "end": "1150799"
  },
  {
    "text": "virtual blog device feature in SEF and SEF FS is the shared file system that uh",
    "start": "1150799",
    "end": "1156039"
  },
  {
    "text": "Alexander was mentioning so we do support asynchronous mirroring for the two types of PVCs so depending on the",
    "start": "1156039",
    "end": "1162120"
  },
  {
    "text": "app you have some PV will some PVCs will be RBD based and other PVCs will be CFS",
    "start": "1162120",
    "end": "1167679"
  },
  {
    "text": "based in both both cases is the RDR that will um take in charge or be in charge",
    "start": "1167679",
    "end": "1173480"
  },
  {
    "text": "of the failover of the app between the two clusters and now is going to give more",
    "start": "1173480",
    "end": "1180080"
  },
  {
    "text": "details about day two hello everyone uh so I want to talk a very",
    "start": "1180080",
    "end": "1187039"
  },
  {
    "text": "important day to operation upgrades we all need at some point to upgrade the",
    "start": "1187039",
    "end": "1192600"
  },
  {
    "text": "software either to get a security fix new features bug fixes and",
    "start": "1192600",
    "end": "1199280"
  },
  {
    "text": "upgrades can be cool and important but they are sometimes putting as a bit with",
    "start": "1199280",
    "end": "1206039"
  },
  {
    "text": "risk if something goes wrong uh and with RF when we actually store your data if",
    "start": "1206039",
    "end": "1212440"
  },
  {
    "text": "something goes wrong in the upgrade it means you actually cannot access the data also for other PS so we need to be",
    "start": "1212440",
    "end": "1220080"
  },
  {
    "text": "bit more careful and and we have several dependencies that sometimes people",
    "start": "1220080",
    "end": "1226039"
  },
  {
    "text": "forget about the first one is obvious SEF version you can upgrade the SE",
    "start": "1226039",
    "end": "1231159"
  },
  {
    "text": "version separately but you need make sure that you don't need to upgrade work",
    "start": "1231159",
    "end": "1236799"
  },
  {
    "text": "as well we support backod competively for few version but sometimes if there's",
    "start": "1236799",
    "end": "1242159"
  },
  {
    "text": "a big change in St or a new feature you have to update also work and as as Bland",
    "start": "1242159",
    "end": "1249640"
  },
  {
    "text": "mentions uh we use S CSI to to provision uh persistent volume um it's basically B",
    "start": "1249640",
    "end": "1259000"
  },
  {
    "text": "with Rook so if you need a new SF CSR you may need to upgrade R especially for",
    "start": "1259000",
    "end": "1264600"
  },
  {
    "text": "new features and of course in both cases I always recommend use stable versions for",
    "start": "1264600",
    "end": "1271159"
  },
  {
    "text": "production workloads don't put Upstream latest",
    "start": "1271159",
    "end": "1276760"
  },
  {
    "text": "workloads your workloads on actually always never I think up latest is only",
    "start": "1276760",
    "end": "1282120"
  },
  {
    "text": "for as developer or if you try something and then we have the",
    "start": "1282120",
    "end": "1287200"
  },
  {
    "text": "kubernetes version and again it's because Rook is an operator you can actually upgrade kubernetes without",
    "start": "1287200",
    "end": "1293480"
  },
  {
    "text": "upgrading the operator but and we actually support at least I think around",
    "start": "1293480",
    "end": "1298640"
  },
  {
    "text": "six version difference but at some point you may need to upgrade Rook because",
    "start": "1298640",
    "end": "1304159"
  },
  {
    "text": "maybe kubernetes change API or even decate them or change something in the",
    "start": "1304159",
    "end": "1309440"
  },
  {
    "text": "CSI interface and you want to consume it so don't forget when you up at kubernetes there's the r operator to",
    "start": "1309440",
    "end": "1315760"
  },
  {
    "text": "consider and look at the support metrics and last especially on permise",
    "start": "1315760",
    "end": "1321320"
  },
  {
    "text": "kubernetes runs on an operating system and you may need to upgrade that and",
    "start": "1321320",
    "end": "1326960"
  },
  {
    "text": "it's actually the hardest part sometimes and we have a dependencies uh with the",
    "start": "1326960",
    "end": "1332760"
  },
  {
    "text": "operating system for example sometimes new features in the oper system are required for new featur in CSI uh we are",
    "start": "1332760",
    "end": "1340080"
  },
  {
    "text": "working on quality service for SE CSI that going to use C V2 you need new Kel",
    "start": "1340080",
    "end": "1346400"
  },
  {
    "text": "for us um we also use use the safe kernel clients if you need a new client",
    "start": "1346400",
    "end": "1351960"
  },
  {
    "text": "that uses a new kernel you may need to upgrade your operating system and",
    "start": "1351960",
    "end": "1357320"
  },
  {
    "text": "another things happens when you upgrade a kernel you need to restart the nodes",
    "start": "1357320",
    "end": "1362720"
  },
  {
    "text": "and then uh you need to join the note and at that point first of all it can",
    "start": "1362720",
    "end": "1368600"
  },
  {
    "text": "take time starting physical servers can take few minutes sometimes long few",
    "start": "1368600",
    "end": "1375320"
  },
  {
    "text": "minutes large number H you may need to drain out of P running on your node uh",
    "start": "1375320",
    "end": "1380880"
  },
  {
    "text": "it could be a database that need to sync it straight into the volume it can be a",
    "start": "1380880",
    "end": "1385960"
  },
  {
    "text": "VM that live you need to live marret and in that case uh because you're actually",
    "start": "1385960",
    "end": "1391919"
  },
  {
    "text": "stopping uh the se fors you don't have access to the data and we care about the",
    "start": "1391919",
    "end": "1398240"
  },
  {
    "text": "data we want to in SE especially but also in work we want to make sure you're",
    "start": "1398240",
    "end": "1403640"
  },
  {
    "text": "safe so I'm go going to talk a bit about the AJ that JC mention and how we do it",
    "start": "1403640",
    "end": "1410880"
  },
  {
    "text": "so default fellow domain is node um what it means we don't only",
    "start": "1410880",
    "end": "1417400"
  },
  {
    "text": "place your data on different osds which maps to different diss we actually store",
    "start": "1417400",
    "end": "1422440"
  },
  {
    "text": "it in a different node so if your node goes down you still have two copies of your data you have a quorum you have a",
    "start": "1422440",
    "end": "1429559"
  },
  {
    "text": "majority if there will be an additional failure we won't allow right because",
    "start": "1429559",
    "end": "1435360"
  },
  {
    "text": "then we have only one copy we can lose it and if if the other two copies o these are up they would not know they",
    "start": "1435360",
    "end": "1442000"
  },
  {
    "text": "are not up to date so so you need so you want to avoid to uh two Fellers in the same time and",
    "start": "1442000",
    "end": "1450480"
  },
  {
    "text": "we want to make sure that if you are doing an upgrade or any operation stopping the osds or other safe pods you",
    "start": "1450480",
    "end": "1458400"
  },
  {
    "text": "won't get that situation of every one copy so we use a very cool kubernetes",
    "start": "1458400",
    "end": "1464799"
  },
  {
    "text": "mechanism called pdb po distribution budget that allows you to Define rules",
    "start": "1464799",
    "end": "1470480"
  },
  {
    "text": "of a dependencies on a specific uh types of pod in our case if a pod of type this",
    "start": "1470480",
    "end": "1479080"
  },
  {
    "text": "is stopped you cannot stop another pod with a different node label because the",
    "start": "1479080",
    "end": "1484399"
  },
  {
    "text": "fellow domain is not we do support as better or more",
    "start": "1484399",
    "end": "1491159"
  },
  {
    "text": "granular fellow domain a w and avity Zone aity zone is basically like it",
    "start": "1491159",
    "end": "1498919"
  },
  {
    "text": "col collocated data center usually a bit far not very far but far enough that it",
    "start": "1498919",
    "end": "1505919"
  },
  {
    "text": "won't be it will be fault Al related different power so if there is a fault in a f in a Zone the other zone is not",
    "start": "1505919",
    "end": "1513240"
  },
  {
    "text": "affected you can see a is like a the cloud Concepts and we can actually",
    "start": "1513240",
    "end": "1519480"
  },
  {
    "text": "deploy R we call it stretch because the usually the network latency between the zones is",
    "start": "1519480",
    "end": "1527279"
  },
  {
    "text": "high and uh and we make sure uh like mentioned we have the mon monitors we",
    "start": "1527279",
    "end": "1533480"
  },
  {
    "text": "put each s monitor in a different Zone monitors are like the control plane for",
    "start": "1533480",
    "end": "1539000"
  },
  {
    "text": "Stu and we put make sure do have yeah M make sure that each",
    "start": "1539000",
    "end": "1546039"
  },
  {
    "text": "replica is in a different zone so if you have a Zone failure you have two copies",
    "start": "1546039",
    "end": "1551240"
  },
  {
    "text": "of the data and you can easily write to the data and also move your pods to the",
    "start": "1551240",
    "end": "1557039"
  },
  {
    "text": "other Zone and have the data this is high availability and again here we use pdbs but instead of No Label we either",
    "start": "1557039",
    "end": "1564679"
  },
  {
    "text": "use W label or Zone label uh so now uh let's go back to the",
    "start": "1564679",
    "end": "1572159"
  },
  {
    "text": "OS upgrades that require not rest starts not rest start I think the if it's not",
    "start": "1572159",
    "end": "1578559"
  },
  {
    "text": "virtualized will take more than 10 minutes usually 20 minutes um so we want",
    "start": "1578559",
    "end": "1584760"
  },
  {
    "text": "to make sure that the upgrades are not too long so if we do one node at a time",
    "start": "1584760",
    "end": "1590440"
  },
  {
    "text": "so it's fre no cluster every fine half an hour you but we usually run larger",
    "start": "1590440",
    "end": "1596600"
  },
  {
    "text": "cluster so 30 NES 5 hours and those who actually run really long large clusters",
    "start": "1596600",
    "end": "1602960"
  },
  {
    "text": "300 can get to 50 hours that's more than two days of the cluster upgrading we",
    "start": "1602960",
    "end": "1610360"
  },
  {
    "text": "want to make it better so if you use more better fellow domain we can",
    "start": "1610360",
    "end": "1616640"
  },
  {
    "text": "actually if you think about it upgrade all the node in the same fellow domain",
    "start": "1616640",
    "end": "1621919"
  },
  {
    "text": "simultaneously so here we have H fre nodes they the same but 30 nodes if you",
    "start": "1621919",
    "end": "1629000"
  },
  {
    "text": "split them let's say to three Rs 10 nodes at a w basically you can Sim land",
    "start": "1629000",
    "end": "1634559"
  },
  {
    "text": "this upgrading 10 nodes and you get the same time to upgrade as you have for free node cluster larger cluster you",
    "start": "1634559",
    "end": "1643559"
  },
  {
    "text": "would say yes I can do 100 node per fellow domain but you don't want to upgrade one of the notes this is not",
    "start": "1643559",
    "end": "1651279"
  },
  {
    "text": "healthy things can go on so I would say you should do around 10 notes at a time",
    "start": "1651279",
    "end": "1657240"
  },
  {
    "text": "and but you still have five only five hours for the upgrade it's much better",
    "start": "1657240",
    "end": "1662360"
  },
  {
    "text": "and faster and you can ask but what if I don't have the fellow domain physically",
    "start": "1662360",
    "end": "1669000"
  },
  {
    "text": "what if uh the nodes are Splitter on W so I cannot use wck fellow domain but",
    "start": "1669000",
    "end": "1674399"
  },
  {
    "text": "you actually can because we don't look really on the physical Hardware we use",
    "start": "1674399",
    "end": "1680080"
  },
  {
    "text": "the kubernetes labels so if you can actually manually add re label let's say",
    "start": "1680080",
    "end": "1685919"
  },
  {
    "text": "your 30 node cluster you can add uh each divided 10 nodes per a virtual W and",
    "start": "1685919",
    "end": "1693679"
  },
  {
    "text": "this way you can experise your upgrade I want a c a fellow domain should have at",
    "start": "1693679",
    "end": "1699880"
  },
  {
    "text": "least five nodes why five because let's say you have two so you lose a node at",
    "start": "1699880",
    "end": "1706840"
  },
  {
    "text": "that moment we want want to put the data on a different fellow domain so in that fellow domain we have one option one",
    "start": "1706840",
    "end": "1713640"
  },
  {
    "text": "note to write the data and it's get even harder if you need to recover if it's a long-term failure you will have one node",
    "start": "1713640",
    "end": "1721480"
  },
  {
    "text": "H SE will start self itself s healing and we want to move the data in that fellow domain to a different node but",
    "start": "1721480",
    "end": "1728440"
  },
  {
    "text": "again you have one node to choose from H so not only you have all the rights",
    "start": "1728440",
    "end": "1733559"
  },
  {
    "text": "going to that node also all the recovery traffic and also you'll need the",
    "start": "1733559",
    "end": "1738799"
  },
  {
    "text": "capacity uh so that's why I recommend at least five nod you lose a node you have four node to split the load",
    "start": "1738799",
    "end": "1747158"
  },
  {
    "text": "on so we presented your work we try to start with the basic the introduction",
    "start": "1747240",
    "end": "1753519"
  },
  {
    "text": "for any one new and added um talking about business",
    "start": "1753519",
    "end": "1759120"
  },
  {
    "text": "continuity backup and restore and upgrades and for those who are more",
    "start": "1759120",
    "end": "1764840"
  },
  {
    "text": "experienc if you have question want to hear more about Rook um want of me hear more about SEF",
    "start": "1764840",
    "end": "1772320"
  },
  {
    "text": "we have a boo in the project Pavilion we are there every day those are the hours",
    "start": "1772320",
    "end": "1778440"
  },
  {
    "text": "uh come ask us talk with us join the Rooks Community thank",
    "start": "1778440",
    "end": "1786240"
  },
  {
    "text": "you I think we have five minutes left as well we can probably take some Q&A I",
    "start": "1795440",
    "end": "1802679"
  },
  {
    "text": "think I think I I saw a question over there I don't know if there's a a microphone that we have",
    "start": "1803919",
    "end": "1810960"
  },
  {
    "text": "available yeah yeah okay so um especially in kubernetes the PVS are",
    "start": "1823559",
    "end": "1830679"
  },
  {
    "text": "small ah yeah I repeat question what's the benefit of running Rook St on AWS",
    "start": "1830679",
    "end": "1836559"
  },
  {
    "text": "EBS so at EBS you pay for the size of the volume but in kubernetes we usually",
    "start": "1836559",
    "end": "1844240"
  },
  {
    "text": "don't have those big volumes we have small H and they're not all also people",
    "start": "1844240",
    "end": "1849760"
  },
  {
    "text": "sometimes try let say I need 10 Giga but actually use one giga so you don't want",
    "start": "1849760",
    "end": "1854799"
  },
  {
    "text": "to do a EBS volume of 10 giga and pay for all the 10 Giga you want to",
    "start": "1854799",
    "end": "1860399"
  },
  {
    "text": "be thin and that's the issue with the EBS for example gp3 is the iops depends",
    "start": "1860399",
    "end": "1867000"
  },
  {
    "text": "on the volume size so small volumes have lower iops you need at least one Terra",
    "start": "1867000",
    "end": "1872279"
  },
  {
    "text": "ABS volume to get really good iops so you can install SEF with one Tera EBS",
    "start": "1872279",
    "end": "1878960"
  },
  {
    "text": "volumes and then provision lots of smaller prvs on top and you get also we",
    "start": "1878960",
    "end": "1885120"
  },
  {
    "text": "are a uh BBS is sp a it's not course a",
    "start": "1885120",
    "end": "1891840"
  },
  {
    "text": "and you you can stretch it and actually it's our default in AWS to use fellow domain a and then we put a replica in it",
    "start": "1891840",
    "end": "1899760"
  },
  {
    "text": "a so if you a Zone failure you have the data in the other zones and everything",
    "start": "1899760",
    "end": "1905600"
  },
  {
    "text": "is highly available thank thank",
    "start": "1905600",
    "end": "1909880"
  },
  {
    "text": "you",
    "start": "1916840",
    "end": "1919840"
  },
  {
    "text": "um yeah the question was whether we have any documents officially on sizing um I",
    "start": "1927320",
    "end": "1932440"
  },
  {
    "text": "don't correct me if I'm wrong I don't think we have any like really strong dedicated sizing guide I think we defer",
    "start": "1932440",
    "end": "1939320"
  },
  {
    "text": "to seph's guide for that do you have any we basically yeah we basically defer to",
    "start": "1939320",
    "end": "1944720"
  },
  {
    "text": "the SEF documentation because in the end you can to some degree one to one map T to RF um if you for example go to the",
    "start": "1944720",
    "end": "1953200"
  },
  {
    "text": "documentation for the C definition yes for example the section for resources there's at least a link if uh um you",
    "start": "1953200",
    "end": "1961080"
  },
  {
    "text": "might not have that link yet um for sizing in regards to uh dis sizes and so",
    "start": "1961080",
    "end": "1967799"
  },
  {
    "text": "on what uh amount of memory there's some info in the SEF do as well I think we also have like some notes on that um",
    "start": "1967799",
    "end": "1974880"
  },
  {
    "text": "besides that I think it's best to also ask on the oh well",
    "start": "1974880",
    "end": "1980679"
  },
  {
    "text": "um uh it's best to ask on the Rooks leg um with like what your what is your goal",
    "start": "1980679",
    "end": "1987200"
  },
  {
    "text": "in regards to usable storage and so on like this would be a point to keep in mind there um in regards to storage",
    "start": "1987200",
    "end": "1993320"
  },
  {
    "text": "sizing not necessarily CPU and memory but like I said using the SEF dos as a guideline is a good starting",
    "start": "1993320",
    "end": "2001480"
  },
  {
    "text": "point I think we might be able to squeeze in one short last",
    "start": "2001480",
    "end": "2006760"
  },
  {
    "text": "question ising encrytion is it many of hi guys so my question is regarding",
    "start": "2006760",
    "end": "2014080"
  },
  {
    "text": "encryption does The Rook provide encryption at rest for example or what kind of encryption it provides thanks um",
    "start": "2014080",
    "end": "2022480"
  },
  {
    "text": "yeah so Rook provides encryption at rest so you can encrypt the the like the osds",
    "start": "2022480",
    "end": "2029480"
  },
  {
    "text": "the the demons that run on top of each dis um it also provides encryption in flight and that's something that I think",
    "start": "2029480",
    "end": "2035559"
  },
  {
    "text": "was added in uh the one 13 release or maybe even recently in",
    "start": "2035559",
    "end": "2041760"
  },
  {
    "text": "1.14 um and then there are also like um some different encryption options for uh",
    "start": "2041760",
    "end": "2048919"
  },
  {
    "text": "yeah for each like PV or PVC yeah I can add about the PV",
    "start": "2048919",
    "end": "2056440"
  },
  {
    "text": "encryption so we support for um RBD based PVS uh we use the encrypt basic to",
    "start": "2056440",
    "end": "2063760"
  },
  {
    "text": "encrypt the data so it's basically client sign encryption the data is written encrypted goes across the network",
    "start": "2063760",
    "end": "2070878"
  },
  {
    "text": "encrypted and everywhere encrypted uh for sefs rwx PVS uh we depend on a",
    "start": "2070879",
    "end": "2078079"
  },
  {
    "text": "kernel model called FS script H it's in the main kernel but we are waiting it to",
    "start": "2078079",
    "end": "2083480"
  },
  {
    "text": "get to well and other Downstream well Linux version and then we'll have also a",
    "start": "2083480",
    "end": "2089800"
  },
  {
    "text": "for aw xpv",
    "start": "2089800",
    "end": "2093440"
  },
  {
    "text": "encryption yeah I think that puts us at time but yeah thank you",
    "start": "2096640",
    "end": "2102359"
  },
  {
    "text": "everyone",
    "start": "2106640",
    "end": "2109640"
  }
]