[
  {
    "text": "hi um thank you for being here today so uh we are going to talk about our recent",
    "start": "599",
    "end": "7520"
  },
  {
    "text": "work with cluster API my name is Karo I am a software engineer at ly Corporation",
    "start": "7520",
    "end": "15440"
  },
  {
    "text": "and I'm sharo I'm also a sof engineer from ly Corporation yeah nice to meet",
    "start": "15440",
    "end": "23400"
  },
  {
    "text": "you and these are today's topic first let us briefly introduce our",
    "start": "23400",
    "end": "30000"
  },
  {
    "text": "company and services then let us introduce our platform and its",
    "start": "30000",
    "end": "35559"
  },
  {
    "text": "scale line is a communication app that connects people services and information",
    "start": "35559",
    "end": "41920"
  },
  {
    "text": "through various Services especially free messages voice and video calls line was",
    "start": "41920",
    "end": "47320"
  },
  {
    "text": "launched in 2011 and we now have over 178 million users in a total of four",
    "start": "47320",
    "end": "54320"
  },
  {
    "text": "major markets almost all of our services are running in our data center centers Verda",
    "start": "54320",
    "end": "61199"
  },
  {
    "text": "is our private Cloud platform that helps line service developers build and run their services on our",
    "start": "61199",
    "end": "68520"
  },
  {
    "text": "infrastructure our mission is to build a platform that enables uh infrastructure",
    "start": "68520",
    "end": "74680"
  },
  {
    "text": "automation for both provisioning and operation Verda consists of",
    "start": "74680",
    "end": "80079"
  },
  {
    "text": "infrastructures of service like VM or BTO platform other service and a set of",
    "start": "80079",
    "end": "85840"
  },
  {
    "text": "managed Services just like the various public Cloud platforms",
    "start": "85840",
    "end": "91240"
  },
  {
    "text": "um this is the service catalog of Vera now we have over 40 services in Verda",
    "start": "91240",
    "end": "96880"
  },
  {
    "text": "and users can use them easily via the Verda dashboard Verda is built on top of",
    "start": "96880",
    "end": "102560"
  },
  {
    "text": "the Sal service principle users can manage their resources uh via a graphical interface",
    "start": "102560",
    "end": "109200"
  },
  {
    "text": "but also use API to operate Vera",
    "start": "109200",
    "end": "114479"
  },
  {
    "text": "Services Verda is now becoming larger we have three regions over over 100,000 VMS",
    "start": "114600",
    "end": "122200"
  },
  {
    "text": "and over 30,000 B machines in total our team provides one of the",
    "start": "122200",
    "end": "130280"
  },
  {
    "text": "managed services in Verda called Verda kubernetes service bks is a managed",
    "start": "130280",
    "end": "136879"
  },
  {
    "text": "kubernetes platform for better our service aims not only to just simplify",
    "start": "136879",
    "end": "143280"
  },
  {
    "text": "the cluster life cycle but also to provide native integration of various",
    "start": "143280",
    "end": "148560"
  },
  {
    "text": "corporate platforms to reduce engineering costs our team has seven members and we",
    "start": "148560",
    "end": "155120"
  },
  {
    "text": "manage over 1,000 clusters in total okay then let's move on to the",
    "start": "155120",
    "end": "161959"
  },
  {
    "text": "next section about the story of why we chose cluster API and how to adopt it",
    "start": "161959",
    "end": "169000"
  },
  {
    "text": "with minimizing users effort as much as possible this is the overview our our",
    "start": "169000",
    "end": "176120"
  },
  {
    "text": "Legacy system we have a cluster provisioner that that is responsible for bootstrapping kubernetes clusters and",
    "start": "176120",
    "end": "183159"
  },
  {
    "text": "machines used by those clusters machines are turned into kubernetes nose by cluster provisioner",
    "start": "183159",
    "end": "191120"
  },
  {
    "text": "via SSH Connections in front of the provisioner we have an API server to abstract basic",
    "start": "191120",
    "end": "199159"
  },
  {
    "text": "operations for the cluster itself like creation and",
    "start": "199159",
    "end": "204440"
  },
  {
    "text": "deletion since we launched bks we have faced several issues with our Legacy",
    "start": "204440",
    "end": "211560"
  },
  {
    "text": "provisioner the first thing is the management of the cluster provisioner",
    "start": "211560",
    "end": "216760"
  },
  {
    "text": "itself our cluster provisioner was the one forked from the Upstream code base",
    "start": "216760",
    "end": "223480"
  },
  {
    "text": "including multiple patches for custom features and Bug fixes we've been maintaining six reposes",
    "start": "223480",
    "end": "231360"
  },
  {
    "text": "and their patches which makes it hard to backport Upstream changes into our Fork",
    "start": "231360",
    "end": "238239"
  },
  {
    "text": "this will be bad in ter terms of both security and stability and we got to",
    "start": "238239",
    "end": "243959"
  },
  {
    "text": "need to find another solution the second thing is about the",
    "start": "243959",
    "end": "249360"
  },
  {
    "text": "node management of the cluster provisioner our provisioner uses SSH for",
    "start": "249360",
    "end": "255400"
  },
  {
    "text": "the initial note bootstop and established state for connections to all nodes for managing",
    "start": "255400",
    "end": "262120"
  },
  {
    "text": "them the more users use our platform the more clusters and noes are needed to be",
    "start": "262120",
    "end": "268600"
  },
  {
    "text": "managed as the scale of our platform grows those connections become often",
    "start": "268600",
    "end": "275880"
  },
  {
    "text": "unstable we would like to create establishing stateful connections for all Nos and make it more",
    "start": "275880",
    "end": "283360"
  },
  {
    "text": "scalable to overcome existing issues we need a new cluster provisioner that",
    "start": "283360",
    "end": "289160"
  },
  {
    "text": "meets our requirements Verda is built with the power of Open Source",
    "start": "289160",
    "end": "294520"
  },
  {
    "text": "Technologies so the new cluster provisioner should be an open- source project lower maintenance cost is also",
    "start": "294520",
    "end": "301720"
  },
  {
    "text": "an important point we don't want to maintain any Fork so pluggable interface",
    "start": "301720",
    "end": "307919"
  },
  {
    "text": "will be better to have scalability is also required for our Dex provision based on these",
    "start": "307919",
    "end": "315360"
  },
  {
    "text": "requirements we finally chose cluster API so what is cluster",
    "start": "315360",
    "end": "321800"
  },
  {
    "text": "API cluster API is a project hosted by the kubernetes community focusing on",
    "start": "321800",
    "end": "327479"
  },
  {
    "text": "simplifying provisioning upgrading in and operating multiple kubernetes clusters it has a declarative API and a",
    "start": "327479",
    "end": "335680"
  },
  {
    "text": "set of controllers that are useful for cluster and node",
    "start": "335680",
    "end": "341039"
  },
  {
    "text": "management the key Point here is that cluster API has pluggable interfaces",
    "start": "341039",
    "end": "347240"
  },
  {
    "text": "called provider there are three types of providers in cluster",
    "start": "347240",
    "end": "352759"
  },
  {
    "text": "API infrastructure provider is in charge of preparing actual resources like load",
    "start": "352759",
    "end": "358400"
  },
  {
    "text": "balancer or VM booster provider is in charge of turning the machine provided",
    "start": "358400",
    "end": "364280"
  },
  {
    "text": "by the infrastructure provider into kubernetes node contol plane provider is",
    "start": "364280",
    "end": "370000"
  },
  {
    "text": "in charge of managing kubernetes contol planes we can use Community provided",
    "start": "370000",
    "end": "375479"
  },
  {
    "text": "providers for each and also we can Implement our own",
    "start": "375479",
    "end": "381440"
  },
  {
    "text": "providers okay so which provider did we choose basically we want to use",
    "start": "382199",
    "end": "388599"
  },
  {
    "text": "Community provider providers as much as possible as for the bootst provider and",
    "start": "388599",
    "end": "394400"
  },
  {
    "text": "Contra plane provider we decided to choose providers powered by Cube ADM",
    "start": "394400",
    "end": "400479"
  },
  {
    "text": "which is provided by cluster API community so how about the",
    "start": "400479",
    "end": "406400"
  },
  {
    "text": "infrastructure provider Verda is built on top of openstack and we have cluster API",
    "start": "406400",
    "end": "413080"
  },
  {
    "text": "provider open stack which is maintained by the cluster API community so why",
    "start": "413080",
    "end": "418440"
  },
  {
    "text": "don't we use it um first we tried adop adopting Capo",
    "start": "418440",
    "end": "425360"
  },
  {
    "text": "however we weren't able to adopt it directly most of you case should be covered by Capo but we have several",
    "start": "425360",
    "end": "432560"
  },
  {
    "text": "reasons coming from company specific limitations Verda uses its own",
    "start": "432560",
    "end": "438479"
  },
  {
    "text": "customized API for both infrastructure the service and load balancers the service so forking Caple is required to",
    "start": "438479",
    "end": "446680"
  },
  {
    "text": "use it also we cannot use the community provided image builder for building VM",
    "start": "446680",
    "end": "453560"
  },
  {
    "text": "images because of our internal image management policy we don't want to maintain an",
    "start": "453560",
    "end": "460479"
  },
  {
    "text": "internal Fork anymore so what should we do our decision is to implement our own",
    "start": "460479",
    "end": "468120"
  },
  {
    "text": "infrastructure provider cluster API provider Vera is the infrastructure provider for the ver platform the key",
    "start": "468120",
    "end": "475680"
  },
  {
    "text": "Point here is that we only need to implement three custom resources cluster",
    "start": "475680",
    "end": "481800"
  },
  {
    "text": "machine template and machine based on the provider contracts defined by cluster",
    "start": "481800",
    "end": "487680"
  },
  {
    "text": "API cap V has ver the cluster ver the machine template and ver the machine",
    "start": "487680",
    "end": "493759"
  },
  {
    "text": "respectively also we have a load balancer in front of each cluster so",
    "start": "493759",
    "end": "499039"
  },
  {
    "text": "where the load balancer resource is also included but this isn't required from the perspective of cluster",
    "start": "499039",
    "end": "507000"
  },
  {
    "text": "API building our own infrastructure provider makes it easy to introduce",
    "start": "507000",
    "end": "512200"
  },
  {
    "text": "custom features based on our internal requirements let us share several",
    "start": "512200",
    "end": "517440"
  },
  {
    "text": "examples of our custom features one of them is user script",
    "start": "517440",
    "end": "523518"
  },
  {
    "text": "users often want to customize their worker noes one use case is configuring",
    "start": "523519",
    "end": "529360"
  },
  {
    "text": "kernel parameters of such nodes several users want to increase AR",
    "start": "529360",
    "end": "535720"
  },
  {
    "text": "table interest when they want to access a lot of host another use case is installing node",
    "start": "535720",
    "end": "542680"
  },
  {
    "text": "level software some users installed Network emulator kernel module and kosd for",
    "start": "542680",
    "end": "549320"
  },
  {
    "text": "chaos testing purpose our ver Machine controller internally merges the user",
    "start": "549320",
    "end": "555399"
  },
  {
    "text": "provided script with the cloud unit config provided by the cluster API bootstrap",
    "start": "555399",
    "end": "562519"
  },
  {
    "text": "provider another example is a static IP notebook this feature assigns redefined",
    "start": "562519",
    "end": "568440"
  },
  {
    "text": "IPS for or worker NOS it is useful when working with IP EOS system because no",
    "start": "568440",
    "end": "574720"
  },
  {
    "text": "have the same set of ips even requirting",
    "start": "574720",
    "end": "579600"
  },
  {
    "text": "them everything discussed here was on our side but how does it look like from",
    "start": "579959",
    "end": "586040"
  },
  {
    "text": "the user thanks to the vks MPI in front of",
    "start": "586040",
    "end": "591680"
  },
  {
    "text": "the provisioner the user interface and user experiences won't be changed before",
    "start": "591680",
    "end": "597760"
  },
  {
    "text": "and after introducing cluster API users still can use the same CLI or",
    "start": "597760",
    "end": "603760"
  },
  {
    "text": "dashboard to operate both clusters provision by different",
    "start": "603760",
    "end": "610120"
  },
  {
    "text": "providers thank you uh from here I will explain our failure domain design uh",
    "start": "611680",
    "end": "618760"
  },
  {
    "text": "with clust II specifically regarding how we treat regions and a",
    "start": "618760",
    "end": "627399"
  },
  {
    "text": "zones let me start with our region design our region design is simple we",
    "start": "627399",
    "end": "634120"
  },
  {
    "text": "provide our CET service independent among regions in our private Cloud we",
    "start": "634120",
    "end": "640680"
  },
  {
    "text": "have one region in Tokyo and another in Osaka so we built our whole system in",
    "start": "640680",
    "end": "646959"
  },
  {
    "text": "Tokyo and build another separated system in nosaka by the way this design diagram",
    "start": "646959",
    "end": "654399"
  },
  {
    "text": "implies that each cluster has NOS in only one region in in other words we",
    "start": "654399",
    "end": "660760"
  },
  {
    "text": "doesn't support multi- Regional cluster thanks to this Simplicity we now",
    "start": "660760",
    "end": "666480"
  },
  {
    "text": "can manage our product with small team for seven",
    "start": "666480",
    "end": "672000"
  },
  {
    "text": "members next let me explain our much EIT cluster",
    "start": "672040",
    "end": "677320"
  },
  {
    "text": "design we take different strategies between control nose and worker",
    "start": "677320",
    "end": "682720"
  },
  {
    "text": "nose as for worker no users can specify availabil Zone per node pool",
    "start": "682720",
    "end": "690079"
  },
  {
    "text": "so if users I mean application developers uh create mulle not pools",
    "start": "690079",
    "end": "695800"
  },
  {
    "text": "with different AIT settings they can distribute their nodes and Achieve AIT",
    "start": "695800",
    "end": "701440"
  },
  {
    "text": "outage Toleration in cluster API uh our node pool is implemented with",
    "start": "701440",
    "end": "708720"
  },
  {
    "text": "machine development customer resource so much is it is achieved with",
    "start": "708720",
    "end": "714360"
  },
  {
    "text": "multiple machine deployment distributed across multiple AIT",
    "start": "714360",
    "end": "720240"
  },
  {
    "text": "on the other hand contol PR nose Belongs to Only One custom resource called Q ADM",
    "start": "720240",
    "end": "727480"
  },
  {
    "text": "control plane so we cannot take the same strategy with worker no so how we",
    "start": "727480",
    "end": "735440"
  },
  {
    "text": "achieved AIT distribution for contempary",
    "start": "735440",
    "end": "739920"
  },
  {
    "text": "NOS here's the answer we utilized cluster API built in Failure domain",
    "start": "740480",
    "end": "747880"
  },
  {
    "text": "support here in this example uh build cluster is",
    "start": "747880",
    "end": "753160"
  },
  {
    "text": "our cluster api's infrastructure cluster resource and we specify a availability",
    "start": "753160",
    "end": "760399"
  },
  {
    "text": "zones in specs of failure domains in this example we specify Tokyo",
    "start": "760399",
    "end": "766959"
  },
  {
    "text": "1 Tokyo 2 Tokyo 3 at a these values are automatically",
    "start": "766959",
    "end": "774040"
  },
  {
    "text": "propagated to Cluster resource and eventually one a is picked per machine",
    "start": "774040",
    "end": "780000"
  },
  {
    "text": "resources so that our contrain nodes are equally distributed across three",
    "start": "780000",
    "end": "785560"
  },
  {
    "text": "specified AIT the key point is that this is done",
    "start": "785560",
    "end": "790600"
  },
  {
    "text": "by cluster API core controllers we haven't done control",
    "start": "790600",
    "end": "795760"
  },
  {
    "text": "implementation for this feature itself so we could reduce implementation",
    "start": "795760",
    "end": "801800"
  },
  {
    "text": "and code maintenance cost we believe that this kind of plugable buit features",
    "start": "801800",
    "end": "808079"
  },
  {
    "text": "are good point of clust API next let me move on to in place",
    "start": "808079",
    "end": "816120"
  },
  {
    "text": "migration to clust API to begin why we need in place",
    "start": "816120",
    "end": "824199"
  },
  {
    "text": "migration right now we manages two cluster provisioner Legacy provisioner",
    "start": "824199",
    "end": "829920"
  },
  {
    "text": "and cluster API based provisioner but as you can easily guess",
    "start": "829920",
    "end": "836519"
  },
  {
    "text": "it's hardly costy to manage different component for the same purpose so we",
    "start": "836519",
    "end": "842959"
  },
  {
    "text": "wanted to resolve this situation but on the other hand from the",
    "start": "842959",
    "end": "848240"
  },
  {
    "text": "users perspective they want to keep using existing clusters with with Legacy",
    "start": "848240",
    "end": "855560"
  },
  {
    "text": "provision they don't want to create another cluster and they don't want to migrate the workload by their",
    "start": "855560",
    "end": "863240"
  },
  {
    "text": "own this is the motivation of our in place CLA migration or conversion from",
    "start": "863240",
    "end": "869519"
  },
  {
    "text": "Legacy provisioner to Cluster API to achieve our goal we referred a",
    "start": "869519",
    "end": "877360"
  },
  {
    "text": "cubic presentation last year from mercedesbenz they had also worked on",
    "start": "877360",
    "end": "884079"
  },
  {
    "text": "migrating clusters from their legacy system to Cluster API based system in",
    "start": "884079",
    "end": "890360"
  },
  {
    "text": "that presentation they explained how they handled cluster API related custom",
    "start": "890360",
    "end": "897399"
  },
  {
    "text": "resources to replace Legacy resources to Cluster API based resources on top of",
    "start": "897399",
    "end": "903639"
  },
  {
    "text": "their open stack based on premise platform like bua in our",
    "start": "903639",
    "end": "911079"
  },
  {
    "text": "case so it gave us a great knowledge to us however we had another critical",
    "start": "911079",
    "end": "919000"
  },
  {
    "text": "challenge or issue to overcome let me explain",
    "start": "919000",
    "end": "925399"
  },
  {
    "text": "it that's the inconsistency of in store between Legacy and cluster API like QB",
    "start": "925399",
    "end": "935000"
  },
  {
    "text": "ADM in cluster API context we Face issue in node boot",
    "start": "935000",
    "end": "940839"
  },
  {
    "text": "strapping right now with our cluster API based system we use Q",
    "start": "940839",
    "end": "947959"
  },
  {
    "text": "ADM on the other hand with our Legacy architecture a DOA based kues installer",
    "start": "948120",
    "end": "955519"
  },
  {
    "text": "was used at the beginning beginning of this migration project we tried to reproduce",
    "start": "955519",
    "end": "963759"
  },
  {
    "text": "the way that's taken by Mercedes-Benz I mean we naively tried to",
    "start": "963759",
    "end": "969680"
  },
  {
    "text": "run QB ADM joint command to add nodes to",
    "start": "969680",
    "end": "975480"
  },
  {
    "text": "Legacy cluster but it failed uh that's due to some kubernetes",
    "start": "975480",
    "end": "983199"
  },
  {
    "text": "set up inconsistency let me pick up some example",
    "start": "983199",
    "end": "989639"
  },
  {
    "text": "of the issues we faced in the process of migration the first Gap is API request",
    "start": "989639",
    "end": "997920"
  },
  {
    "text": "load balancing in kuet cluster each node or",
    "start": "997920",
    "end": "1003759"
  },
  {
    "text": "more specifically cuate access Q AP servers in ha cluster such quet AP",
    "start": "1003759",
    "end": "1012440"
  },
  {
    "text": "request from kuet should be load balanced among repas of KU AP server",
    "start": "1012440",
    "end": "1019279"
  },
  {
    "text": "in our Legacy cluster that's done with client side load balancing on the other",
    "start": "1019279",
    "end": "1024678"
  },
  {
    "text": "hand our clust API and QB ADM based cluster it uses our cloudbased load",
    "start": "1024679",
    "end": "1033760"
  },
  {
    "text": "balances provided by verer to resolve this Gap we prepared",
    "start": "1033760",
    "end": "1039760"
  },
  {
    "text": "two load balances also for legacy clusters to be",
    "start": "1039760",
    "end": "1045438"
  },
  {
    "text": "migrated next Gap came after the first Gap is",
    "start": "1045959",
    "end": "1051799"
  },
  {
    "text": "resolved that is subjective alternative name in consistency and TLS",
    "start": "1051799",
    "end": "1058880"
  },
  {
    "text": "certificates kubernetes API access is done with https",
    "start": "1058880",
    "end": "1064520"
  },
  {
    "text": "requests generally such TLS requests are verified by comparing HTTP request and",
    "start": "1064520",
    "end": "1071679"
  },
  {
    "text": "TLS certificates in Legacy cluster Comm API",
    "start": "1071679",
    "end": "1077520"
  },
  {
    "text": "access is done with domain name on the other hand our new cluster uses domain",
    "start": "1077520",
    "end": "1083960"
  },
  {
    "text": "for low balances as a Target host to access kuet",
    "start": "1083960",
    "end": "1089679"
  },
  {
    "text": "API so request variation was failed in this example https request to",
    "start": "1089679",
    "end": "1099080"
  },
  {
    "text": "ci. example.com was denied by TLS validation mechanism because the host",
    "start": "1099080",
    "end": "1105520"
  },
  {
    "text": "name ci. example.com isn't included in the SN and",
    "start": "1105520",
    "end": "1115039"
  },
  {
    "text": "certificate so we added the expected domain name to SN before we proceed",
    "start": "1115039",
    "end": "1123039"
  },
  {
    "text": "migration the third Gap is configuration management this issue is more specific",
    "start": "1124240",
    "end": "1131200"
  },
  {
    "text": "to QB ADM in order to manage its configuration",
    "start": "1131200",
    "end": "1136600"
  },
  {
    "text": "QB ADM uses cetes API I mean it stores",
    "start": "1136600",
    "end": "1142159"
  },
  {
    "text": "config map in the QB system name space like qbm config or QB config or like",
    "start": "1142159",
    "end": "1150840"
  },
  {
    "text": "that on the flip side QB ADM cannot properly handle nodes without such",
    "start": "1150840",
    "end": "1158159"
  },
  {
    "text": "configurations So to avoid the issue we prepare such resources I mean config",
    "start": "1158159",
    "end": "1164280"
  },
  {
    "text": "Maps based on the cluster setups of the Legacy clust to be",
    "start": "1164280",
    "end": "1170840"
  },
  {
    "text": "migrated the last issue I pick here is ET member",
    "start": "1172240",
    "end": "1179559"
  },
  {
    "text": "Discovery in our system we manages ET clusters together with contol pring",
    "start": "1179559",
    "end": "1186720"
  },
  {
    "text": "components with qbm unlike QBs server where load one is",
    "start": "1186720",
    "end": "1192919"
  },
  {
    "text": "prepared in front QB ADM have to know every members of Managed IT",
    "start": "1192919",
    "end": "1201120"
  },
  {
    "text": "C in based cluster it C members are",
    "start": "1201120",
    "end": "1206200"
  },
  {
    "text": "managed as kuet static Port which can be seen from kubernetes",
    "start": "1206200",
    "end": "1213440"
  },
  {
    "text": "API uh like this kubernetes I mean QB ADM",
    "start": "1213440",
    "end": "1219400"
  },
  {
    "text": "list the ports with Filter component equals FCD and tier equals control",
    "start": "1219400",
    "end": "1226640"
  },
  {
    "text": "pre by this QB ADM get theity member however our Legacy ET City",
    "start": "1226640",
    "end": "1234360"
  },
  {
    "text": "members are managed just as doer containers independently from kubernetes",
    "start": "1234360",
    "end": "1242600"
  },
  {
    "text": "API this cannot be seen from kuet API so we needed a way to tell at City member",
    "start": "1242600",
    "end": "1249720"
  },
  {
    "text": "locations to QB ADM our solution is to prepare dummy",
    "start": "1249720",
    "end": "1255360"
  },
  {
    "text": "ports on the nodes where Legacy three instances are",
    "start": "1255360",
    "end": "1261520"
  },
  {
    "text": "running like these gaps we had to overcome some issues of node boot",
    "start": "1263640",
    "end": "1269960"
  },
  {
    "text": "strapping or cluster boot strapping for imper migration to sum up our project for",
    "start": "1269960",
    "end": "1277320"
  },
  {
    "text": "cluster migration we have two things to do for in place cluster",
    "start": "1277320",
    "end": "1282360"
  },
  {
    "text": "migration the first one is to resolve cluster setup inconsistency spe specifically with no",
    "start": "1282360",
    "end": "1289440"
  },
  {
    "text": "boost dropper like QB ADM and the second point is to handle",
    "start": "1289440",
    "end": "1294799"
  },
  {
    "text": "cluster API customer resources like the way Mercedes Benz has explained in the",
    "start": "1294799",
    "end": "1301039"
  },
  {
    "text": "past kubecon session if you are more interested in the details please contact us and let's",
    "start": "1301039",
    "end": "1310840"
  },
  {
    "text": "discuss yep okay thank you sho and next",
    "start": "1311200",
    "end": "1316279"
  },
  {
    "text": "we're going to share some of the obstacles that we faced and the insights we got from",
    "start": "1316279",
    "end": "1322679"
  },
  {
    "text": "them today let us share two topics the first topic is controller",
    "start": "1322679",
    "end": "1329679"
  },
  {
    "text": "scalability now the number of clusters and nose is increasing for both",
    "start": "1329679",
    "end": "1334799"
  },
  {
    "text": "environments for the production environment over 3,000 NOS are managed",
    "start": "1334799",
    "end": "1340159"
  },
  {
    "text": "by the single management cluster since we have a lot of nodes the",
    "start": "1340159",
    "end": "1347520"
  },
  {
    "text": "machine controller work becomes constantly long the operation for the cluster like cluster creation or node",
    "start": "1347520",
    "end": "1355360"
  },
  {
    "text": "provisioning takes time one of the solution will be scaling up controllers",
    "start": "1355360",
    "end": "1361360"
  },
  {
    "text": "another solution will be scaling out however it wouldn't be easy since controllers have later forward",
    "start": "1361360",
    "end": "1369559"
  },
  {
    "text": "architecture scalability of classer API is one of our interests currently",
    "start": "1369559",
    "end": "1375559"
  },
  {
    "text": "Community seems to be also working on scale SC testing our Target for now is",
    "start": "1375559",
    "end": "1381600"
  },
  {
    "text": "1,000 noes per single cluster so we'll continue working on the scaleability by",
    "start": "1381600",
    "end": "1387720"
  },
  {
    "text": "the way um there was a maintenance session about the recent improvements to the performance of the cluster P",
    "start": "1387720",
    "end": "1394559"
  },
  {
    "text": "controller yesterday yeah yeah really good thanks for the um uh yeah F was",
    "start": "1394559",
    "end": "1402200"
  },
  {
    "text": "introduced in 1.5.0 so check it out yeah thanks for the great work and for sharing the",
    "start": "1402200",
    "end": "1408080"
  },
  {
    "text": "effective ways of improving controller scal de yeah uh if you have interested yeah check it",
    "start": "1408080",
    "end": "1415200"
  },
  {
    "text": "out next topic is sdd Snapshot and restore Disaster Recovery is one of the",
    "start": "1415200",
    "end": "1422320"
  },
  {
    "text": "crucial parts for cruster operators taking yd snapshots will be easy to",
    "start": "1422320",
    "end": "1428240"
  },
  {
    "text": "implement in our case we have a job to take snapshot using sedd cutle and",
    "start": "1428240",
    "end": "1435120"
  },
  {
    "text": "upload them to our internal object St they are deployed for each cluster and",
    "start": "1435120",
    "end": "1441760"
  },
  {
    "text": "running periodically however it's more complicated when we try to restore the",
    "start": "1441760",
    "end": "1448320"
  },
  {
    "text": "cluster from snapshots for those clusters managed by cluster API we need to work together",
    "start": "1448320",
    "end": "1455520"
  },
  {
    "text": "with cluster controllers to restore clusters from snapshots for example when we try to",
    "start": "1455520",
    "end": "1462559"
  },
  {
    "text": "restore the cluster from snapshots we need to prevent cluster AP controllers",
    "start": "1462559",
    "end": "1468120"
  },
  {
    "text": "from reconciling the contol plane and worker nodes we currently perform the",
    "start": "1468120",
    "end": "1473799"
  },
  {
    "text": "interre process man but plan to automate the whole process if this use case will be common",
    "start": "1473799",
    "end": "1481000"
  },
  {
    "text": "among classia users it would be nice to have such functionalities on the class API side I was wondering if there is any",
    "start": "1481000",
    "end": "1488919"
  },
  {
    "text": "good architecture to include this as part of the cluster API ecosystem if",
    "start": "1488919",
    "end": "1494600"
  },
  {
    "text": "you're interested I'd really appreciate it if you could join the this",
    "start": "1494600",
    "end": "1500000"
  },
  {
    "text": "proposal um this is all of our presentation in conclusion let me summarize to this Contents I would like",
    "start": "1500440",
    "end": "1508200"
  },
  {
    "text": "to say that cluster API is great for a private Cloud platform",
    "start": "1508200",
    "end": "1514559"
  },
  {
    "text": "environment we can reduce the cost of code management because of its plugable",
    "start": "1514559",
    "end": "1520120"
  },
  {
    "text": "provider concept uh it allows us to have custom features which is great for such a large",
    "start": "1520120",
    "end": "1527799"
  },
  {
    "text": "platform with with its own needs also cluster API is friendly with",
    "start": "1527799",
    "end": "1533960"
  },
  {
    "text": "the multi- regional model with the faia domain concept adopting cluster API as a Next",
    "start": "1533960",
    "end": "1542640"
  },
  {
    "text": "Generation cluster pro pro provisioner could be a long journey for a large",
    "start": "1542640",
    "end": "1547840"
  },
  {
    "text": "scale environment however we minimized various cost for cluster operators with platform",
    "start": "1547840",
    "end": "1555799"
  },
  {
    "text": "engineering we provide consist consistent UI and ux with API",
    "start": "1555799",
    "end": "1561000"
  },
  {
    "text": "abstraction it allows users to operate their clusters without knowing the",
    "start": "1561000",
    "end": "1566600"
  },
  {
    "text": "details of underlying Technologies also we support in place migration with class API and QB ADM to",
    "start": "1566600",
    "end": "1574760"
  },
  {
    "text": "minimize the migration cost and thank you very much for your",
    "start": "1574760",
    "end": "1580000"
  },
  {
    "text": "time and attention we uh on kuet stock so questions are welcome also we can",
    "start": "1580000",
    "end": "1586640"
  },
  {
    "text": "talk in person so feel free to ask anything about our session today thank you very",
    "start": "1586640",
    "end": "1593559"
  },
  {
    "text": "much so uh thank you for great presentation and uh I have a question",
    "start": "1599440",
    "end": "1605320"
  },
  {
    "text": "related to custom feature that sta IP not pool yes yeah uh the first one is um",
    "start": "1605320",
    "end": "1613760"
  },
  {
    "text": "when was the processed resource created and is is tied to a cluster or not pool",
    "start": "1613760",
    "end": "1621360"
  },
  {
    "text": "life cycle and sorry could you repeat that uh",
    "start": "1621360",
    "end": "1629559"
  },
  {
    "text": "when were the P set result created in open stack",
    "start": "1629559",
    "end": "1635600"
  },
  {
    "text": "Neutron um so the static IP resources are created",
    "start": "1635600",
    "end": "1642720"
  },
  {
    "text": "when user create uh node pool know uh but also",
    "start": "1642720",
    "end": "1650880"
  },
  {
    "text": "sometimes user want to expand the no pool size so that also triggers the creation of",
    "start": "1650880",
    "end": "1659039"
  },
  {
    "text": "static IPS it's internally you know open stack Port resources are",
    "start": "1659039",
    "end": "1665880"
  },
  {
    "text": "used uh is this answering your",
    "start": "1665880",
    "end": "1672080"
  },
  {
    "text": "question so I think and is this tied to Cluster or noral life cycle",
    "start": "1672080",
    "end": "1678519"
  },
  {
    "text": "sorry uh is this tied to the cluster or not pool life",
    "start": "1678519",
    "end": "1685200"
  },
  {
    "text": "cycle so uh earlier you said that um the process created after user create a",
    "start": "1685200",
    "end": "1692919"
  },
  {
    "text": "notp um when user created the crush the",
    "start": "1692919",
    "end": "1699399"
  },
  {
    "text": "static not po has not created when user uh uh call our apis yeah it's Cor",
    "start": "1699399",
    "end": "1707480"
  },
  {
    "text": "created mhm so and I have another question for",
    "start": "1707480",
    "end": "1712840"
  },
  {
    "text": "this does the number of ports need to be equal to the number of",
    "start": "1712840",
    "end": "1718120"
  },
  {
    "text": "no yes so uh uh wait sorry um it's more",
    "start": "1718120",
    "end": "1723960"
  },
  {
    "text": "complicated but each node has can have one private IPS and one public IP",
    "start": "1723960",
    "end": "1732880"
  },
  {
    "text": "so user can specify the policy independently so if you us specify both",
    "start": "1732880",
    "end": "1740440"
  },
  {
    "text": "uh public static IP and public uh private static IP yeah the two ports are",
    "start": "1740440",
    "end": "1745919"
  },
  {
    "text": "created for one no okay so thank you yeah thank",
    "start": "1745919",
    "end": "1753440"
  },
  {
    "text": "you first of all a great presentation um I have a question about the capability",
    "start": "1753799",
    "end": "1759640"
  },
  {
    "text": "for users to uh customize their init",
    "start": "1759640",
    "end": "1765240"
  },
  {
    "text": "scripts when creating their notes um is that something that's actually exposed",
    "start": "1765240",
    "end": "1771679"
  },
  {
    "text": "like if I were to be an user and I wanted me myself to I don't know increment the number of TCP ports of",
    "start": "1771679",
    "end": "1778799"
  },
  {
    "text": "it's something that I can do right so how often do people shoot themselves in",
    "start": "1778799",
    "end": "1785039"
  },
  {
    "text": "the foot and do like wrong things with those scripts um and if it happens very",
    "start": "1785039",
    "end": "1790440"
  },
  {
    "text": "frequently how do you guys deal with that uh yeah yeah sometime users abuse",
    "start": "1790440",
    "end": "1798880"
  },
  {
    "text": "that feature actually but [Music] um in that case the no will fa fail to",
    "start": "1798880",
    "end": "1806279"
  },
  {
    "text": "provision so um maybe that's not arho and user can notice that so the user",
    "start": "1806279",
    "end": "1813440"
  },
  {
    "text": "will fix it so actually yeah also one another background is our Quan nodes are",
    "start": "1813440",
    "end": "1820760"
  },
  {
    "text": "built as users Es infrastru as a service VM so it's users VM so yeah we treat it",
    "start": "1820760",
    "end": "1831159"
  },
  {
    "text": "as a user responsibility right so um you I'm I'm",
    "start": "1831159",
    "end": "1837320"
  },
  {
    "text": "assuming that you guys have metrics internally like trying to like predict like that your nodes like nodes are",
    "start": "1837320",
    "end": "1843600"
  },
  {
    "text": "being created by uh capier are healthy does that not impact those metrics how",
    "start": "1843600",
    "end": "1849600"
  },
  {
    "text": "do you differentiate between oh like there's an issue with the nodes that we're creating versus oh it's just",
    "start": "1849600",
    "end": "1856039"
  },
  {
    "text": "people doing stupid things with their nodes yeah that's good",
    "start": "1856039",
    "end": "1861519"
  },
  {
    "text": "point so actually we cannot distinguish the you",
    "start": "1861519",
    "end": "1868320"
  },
  {
    "text": "know code but hopefully um and fortunately yeah",
    "start": "1868320",
    "end": "1876039"
  },
  {
    "text": "the users using such feature is not so large so we can manually help such users",
    "start": "1876039",
    "end": "1884000"
  },
  {
    "text": "when they face issues so thank you yeah thank you thank you fantastic talks uh",
    "start": "1884000",
    "end": "1891440"
  },
  {
    "text": "uh arato I have few questions um you talked in the beginning that uh your",
    "start": "1891440",
    "end": "1899000"
  },
  {
    "text": "like private uh private cloud is bit different to what uh cluster API can do",
    "start": "1899000",
    "end": "1904559"
  },
  {
    "text": "and uh you don't like use the Octavia API have you considered writing your own",
    "start": "1904559",
    "end": "1909919"
  },
  {
    "text": "provider like so that the it can be used actually uh because we are in a similar",
    "start": "1909919",
    "end": "1916519"
  },
  {
    "text": "situation we we also run like open State clusters with 8,000 noes let's say and",
    "start": "1916519",
    "end": "1922000"
  },
  {
    "text": "we we implemented our own uh Octavia provider so that we can actually use the",
    "start": "1922000",
    "end": "1927880"
  },
  {
    "text": "uh cluster API provider for open stack have you considered that like writing your own uh actually we haven't",
    "start": "1927880",
    "end": "1935440"
  },
  {
    "text": "considered that that's more because of our organizational structure so this",
    "start": "1935440",
    "end": "1943120"
  },
  {
    "text": "crust API adoption is decided only by ketes team okay uh so yeah also the code",
    "start": "1943120",
    "end": "1951279"
  },
  {
    "text": "base is not so large so that's why we decided to write by our own okay perfect",
    "start": "1951279",
    "end": "1957039"
  },
  {
    "text": "uh a second question if I can um can you share what uh type of like cni do you",
    "start": "1957039",
    "end": "1963360"
  },
  {
    "text": "use in youres is it Calico or yeah we use Calico yeah it's the same uh and",
    "start": "1963360",
    "end": "1970600"
  },
  {
    "text": "have you considered using crossplane I'm kind of new to this but it seems to me",
    "start": "1970600",
    "end": "1976559"
  },
  {
    "text": "uh the these two like options are similar in a in a in a way can you say",
    "start": "1976559",
    "end": "1982720"
  },
  {
    "text": "more about this if you have or have not or if you can uh maybe um just say",
    "start": "1982720",
    "end": "1991240"
  },
  {
    "text": "what what do you think is better is better for us because we are now like in a situation where we are trying to like",
    "start": "1991240",
    "end": "1997279"
  },
  {
    "text": "get as much information and decide which way we will we will like go for it um",
    "start": "1997279",
    "end": "2003960"
  },
  {
    "text": "honestly uh the crossplay is Safeway is not on the list when we consider the",
    "start": "2003960",
    "end": "2010240"
  },
  {
    "text": "another solution so maybe it will be better to consider when you uh uh try to",
    "start": "2010240",
    "end": "2016159"
  },
  {
    "text": "decide the final solution will be but uh as a result of our uh adaption the",
    "start": "2016159",
    "end": "2022440"
  },
  {
    "text": "classer could be one of the um the option so maybe it will be yeah I'd say",
    "start": "2022440",
    "end": "2028760"
  },
  {
    "text": "that's one of the good uh option okay thank you so much yeah thank",
    "start": "2028760",
    "end": "2034320"
  },
  {
    "text": "you hi I'll try to make my question pretty quick quick but um I really liked how you had the infrastructure providers",
    "start": "2034320",
    "end": "2040720"
  },
  {
    "text": "like you kind of made your own kind of custom crds which is really cool um do you ever have any problems with like",
    "start": "2040720",
    "end": "2046440"
  },
  {
    "text": "your development teams trying to take those and make them kind of a little too fine grain for maybe their particular",
    "start": "2046440",
    "end": "2052118"
  },
  {
    "text": "application or maybe use case that they're using for it kind of feels like you kind of built a really nice",
    "start": "2052119",
    "end": "2059118"
  },
  {
    "text": "framework for people to do that but I don't know if that's like the intention or people take it too",
    "start": "2059119",
    "end": "2065440"
  },
  {
    "text": "far um sorry I'm not uh grasp your question",
    "start": "2065480",
    "end": "2071679"
  },
  {
    "text": "questions details but you asking",
    "start": "2071679",
    "end": "2079118"
  },
  {
    "text": "um our controllers uh sorry could you repeat",
    "start": "2079200",
    "end": "2085200"
  },
  {
    "text": "your question yeah no worries um yeah so like in the case of like the verta machine template right do you ever have",
    "start": "2085200",
    "end": "2091280"
  },
  {
    "text": "problems with people kind of saying hey like I need to deploy my application for X Y or Z right but I need this really",
    "start": "2091280",
    "end": "2099240"
  },
  {
    "text": "complex config added to it so they kind of like kind of really focus that template down on to their P persistent",
    "start": "2099240",
    "end": "2106440"
  },
  {
    "text": "view case and kind of break the formula that you kind of made for them uh actually you uh uh",
    "start": "2106440",
    "end": "2117880"
  },
  {
    "text": "wait uh actually uh I'm not sure if I understand correct correctly but um user cannot create the",
    "start": "2122400",
    "end": "2130359"
  },
  {
    "text": "ver the machine uh resource directly so we abstruct that layer by via the API so",
    "start": "2130359",
    "end": "2137520"
  },
  {
    "text": "user just uh request that something like the uh create uh node it it",
    "start": "2137520",
    "end": "2146040"
  },
  {
    "text": "translates uh by the vks API to create the ver the machine resour so I think",
    "start": "2146040",
    "end": "2153680"
  },
  {
    "text": "there is no um special request to add some functionality hey I need ver the",
    "start": "2153680",
    "end": "2159680"
  },
  {
    "text": "machine I need this feature for to ver the machine so it's completely",
    "start": "2159680",
    "end": "2165319"
  },
  {
    "text": "independent and also I think another good information is with because this",
    "start": "2165319",
    "end": "2174119"
  },
  {
    "text": "clust API cluster is just QB ADM cluster so sometimes we allow user to run QB ADM",
    "start": "2174119",
    "end": "2183720"
  },
  {
    "text": "command by themselves for example to add PMS to the Clusters so it's the",
    "start": "2183720",
    "end": "2191400"
  },
  {
    "text": "man okay yeah that makes sense I appreciate your time guys thank",
    "start": "2191400",
    "end": "2196880"
  },
  {
    "text": "you uh maybe is that all okay again thanks for being here",
    "start": "2197960",
    "end": "2203079"
  },
  {
    "text": "today yeah yeah thank you",
    "start": "2203079",
    "end": "2208119"
  }
]