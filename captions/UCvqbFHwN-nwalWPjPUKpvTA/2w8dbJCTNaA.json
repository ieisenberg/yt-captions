[
  {
    "start": "0",
    "end": "5000"
  },
  {
    "text": "good morning good afternoon everyone thanks for coming out today it's a pleasure to meet you and I'm Jonathan",
    "start": "30",
    "end": "8690"
  },
  {
    "text": "King Jenkins running and kubernetes definitely broke our brands you know if",
    "start": "25730",
    "end": "30929"
  },
  {
    "text": "we had heard this talk year and a half ago it would have saved us a lot of time a lot of pain we hope what you learned",
    "start": "30929",
    "end": "37829"
  },
  {
    "text": "today that keeps you from breaking your brain - yeah so before we get into what",
    "start": "37829",
    "end": "42840"
  },
  {
    "text": "we're up to let's learn a little bit more about you so I'm gonna ask for a bit of audience participation here if",
    "start": "42840",
    "end": "48480"
  },
  {
    "text": "you're from Seattle if this is your hometown please raise your hand let's see oh thank you for coming all right",
    "start": "48480",
    "end": "55640"
  },
  {
    "text": "which is the best coffee is it Starbucks or Pete's or intelligentsia or anyone",
    "start": "55640",
    "end": "64790"
  },
  {
    "text": "slate none of the above okay come find me afterwards I need to",
    "start": "64790",
    "end": "70110"
  },
  {
    "text": "know which one's the best one all right wait hang on we got a couple more questions so who okay who is brand new",
    "start": "70110",
    "end": "76890"
  },
  {
    "text": "to kubernetes this year who's new all right let's give these people around applause thank you for coming",
    "start": "76890",
    "end": "83520"
  },
  {
    "text": "welcome to the community or making us all stronger all right who actually here is sitting in this room bill part of",
    "start": "83520",
    "end": "90240"
  },
  {
    "text": "kubernetes anyone raise your hands all right look around folks now keep those hands up I see you everyone else see",
    "start": "90240",
    "end": "97560"
  },
  {
    "text": "these people that's who you need to talk to they have all the answers all right so finally let's talk about what we're",
    "start": "97560",
    "end": "104490"
  },
  {
    "text": "going to talk about today so first we're going to tell you the story of the bad old Jenkins that we used to have this is",
    "start": "104490",
    "end": "111149"
  },
  {
    "text": "what led us to replace everything that we're talking about here today led us down this journey into kubernetes then",
    "start": "111149",
    "end": "117659"
  },
  {
    "text": "we're going to talk about the three easy steps it was simple to get this Jenkins running on kubernetes obviously and then",
    "start": "117659",
    "end": "125189"
  },
  {
    "text": "what happened when we actually got it right working but then we needed to get the developers to work with it and our",
    "start": "125189",
    "end": "130289"
  },
  {
    "text": "early stages of getting it smoothly running into production so and if we're all done at the end",
    "start": "130289",
    "end": "135390"
  },
  {
    "text": "we'll leave some lessons for you and maybe some time for questions so alright",
    "start": "135390",
    "end": "142340"
  },
  {
    "start": "142000",
    "end": "243000"
  },
  {
    "text": "alright so we're gonna start back in January 2017 we had ourselves a bad old",
    "start": "142340",
    "end": "148290"
  },
  {
    "text": "Jenkins developers often refer to it as Jerkins it's we could never rely on it",
    "start": "148290",
    "end": "157230"
  },
  {
    "text": "it never ran the same way twice therefore we couldn't be trusted it couldn't be trusted and a developers end",
    "start": "157230",
    "end": "162450"
  },
  {
    "text": "up ignoring it right it's like that day you have your low tire pressure sensor",
    "start": "162450",
    "end": "167580"
  },
  {
    "text": "on your dashboard and you just leave it there and it doesn't seem to be any problem until one day you find yourself",
    "start": "167580",
    "end": "173519"
  },
  {
    "text": "on the road side of the road with a smoking tire right so Jenkins was always",
    "start": "173519",
    "end": "178560"
  },
  {
    "text": "breaking it was always breaking because the VM said it was running on wood just up and died and then we try and bring",
    "start": "178560",
    "end": "185730"
  },
  {
    "text": "those VMs back up but we had this teetering pile of Chef scripts that was completely indecipherable so we'd never",
    "start": "185730",
    "end": "191730"
  },
  {
    "text": "get the same version of Java or the same version of maven or the same version of my sequel running and that made it super",
    "start": "191730",
    "end": "197790"
  },
  {
    "text": "unreliable and so every time we ran the build sometimes as it would succeed and sometimes it would fail and we never",
    "start": "197790",
    "end": "203430"
  },
  {
    "text": "knew if it was the build script or the machine that it was running on on top of that our build scripts were nested",
    "start": "203430",
    "end": "209880"
  },
  {
    "text": "within our Jenkins jobs not versions we never knew what had changed and we",
    "start": "209880",
    "end": "215370"
  },
  {
    "text": "couldn't easily roll back really hard to roll back this is the story of our battle Jenkins so I'm gonna ask the",
    "start": "215370",
    "end": "221609"
  },
  {
    "text": "audience how many of you have a battle Jenkins back in the office raise your hands",
    "start": "221609",
    "end": "227629"
  },
  {
    "text": "alright those of you who didn't raise your hands are probably lying alright",
    "start": "227629",
    "end": "235290"
  },
  {
    "text": "since Jenkins was burning up so much of our time we finally decided to scrap it",
    "start": "235290",
    "end": "241170"
  },
  {
    "text": "and build something new so as we thought",
    "start": "241170",
    "end": "246360"
  },
  {
    "start": "243000",
    "end": "305000"
  },
  {
    "text": "through what it meant to have a good Jenkins we realized there really was one important goal to our effort and that",
    "start": "246360",
    "end": "251700"
  },
  {
    "text": "was to make it simple enough that our developers could manage it because we're the DevOps guys right and we just didn't",
    "start": "251700",
    "end": "258959"
  },
  {
    "text": "want to be always answering questions from our development team you know why is this build broken why is this thing down why can't I could you please fix it",
    "start": "258959",
    "end": "265740"
  },
  {
    "text": "and and so forth we wanted Jenkins to be everybody's problem not just our problem",
    "start": "265740",
    "end": "272960"
  },
  {
    "text": "so that meant that what we really needed to do is get the developers to bandit",
    "start": "272960",
    "end": "279360"
  },
  {
    "text": "manage their own build systems so that Jenkins was their problem right but it also meant and this is the important",
    "start": "279360",
    "end": "285000"
  },
  {
    "text": "part it was simple enough that they could do that right we need things simple for developers bless their hearts",
    "start": "285000",
    "end": "290280"
  },
  {
    "text": "and so yeah so we needed things simple enough for developers and so that meant",
    "start": "290280",
    "end": "296850"
  },
  {
    "text": "that we actually had to take all that system stuff that is really complicated and simplify it and make it reliable",
    "start": "296850",
    "end": "303060"
  },
  {
    "text": "enough that Jenkins would just work also important security we needed to have it",
    "start": "303060",
    "end": "309360"
  },
  {
    "start": "305000",
    "end": "346000"
  },
  {
    "text": "run on our corporate network it needed to be free so we're gonna reuse the infrastructure we already have our",
    "start": "309360",
    "end": "315139"
  },
  {
    "text": "Jenkins need to be repeatable and efficient so we can run more builds of",
    "start": "315139",
    "end": "320460"
  },
  {
    "text": "the same infrastructure also reprovision from scratch if it were to break it",
    "start": "320460",
    "end": "326340"
  },
  {
    "text": "needed to be fast we wanted to get the build times down to 30 minutes from two hours three hours to thirty minutes so",
    "start": "326340",
    "end": "332820"
  },
  {
    "text": "we picked Jenkins 2.0 and kubernetes cuz it checked all those boxes and because",
    "start": "332820",
    "end": "339330"
  },
  {
    "text": "it's fashionable and because it's fashionable isn't that why we're all here anyways it's fashionable way to go",
    "start": "339330",
    "end": "345660"
  },
  {
    "text": "kubernetes alright awesome we figured it would just take three easy steps right yeah three easy steps",
    "start": "345660",
    "end": "351720"
  },
  {
    "start": "346000",
    "end": "361000"
  },
  {
    "text": "prepare the VMS prepare kubernetes and configure Jenkins how hard could it be",
    "start": "351720",
    "end": "357360"
  },
  {
    "text": "oh man but remember kubernetes is open-source",
    "start": "357360",
    "end": "363370"
  },
  {
    "start": "361000",
    "end": "412000"
  },
  {
    "text": "and so is jenkins so this is early 2017",
    "start": "363370",
    "end": "368440"
  },
  {
    "text": "it's kubernetes 1.6 that we're trying to build this on right so we're a la there",
    "start": "368440",
    "end": "374020"
  },
  {
    "text": "are lots of bugs and there's very little documentation right and everything about",
    "start": "374020",
    "end": "379750"
  },
  {
    "text": "kubernetes today is much better everything about Jenkins today is much better which is fantastic because we",
    "start": "379750",
    "end": "384940"
  },
  {
    "text": "were just barely Jenkins 2.0 barely able to run kubernetes there was no K admin",
    "start": "384940",
    "end": "391240"
  },
  {
    "text": "when I when we started it was too too early for that but what we did is we updated this talk",
    "start": "391240",
    "end": "396310"
  },
  {
    "text": "the lessons we're sharing with you today are for the latest Jenkins 2.0 and kubernetes 112 so you can take what",
    "start": "396310",
    "end": "404229"
  },
  {
    "text": "we've got today and actually apply it to your builds at work tomorrow or next week I mean we've got some drinking to",
    "start": "404229",
    "end": "410979"
  },
  {
    "text": "do right alright so so we suggest you check out our example repo the scripts",
    "start": "410979",
    "end": "417190"
  },
  {
    "start": "412000",
    "end": "443000"
  },
  {
    "text": "and our repo will enable you to set up a Jenkins on a kubernetes cluster and just 20 easy steps includes our best stuff 20",
    "start": "417190",
    "end": "425409"
  },
  {
    "text": "easy steps includes everything we describe here today and several the",
    "start": "425409",
    "end": "431050"
  },
  {
    "text": "brain breakers we didn't have time to cover now Jonathan if we had these scripts one and a half years ago our",
    "start": "431050",
    "end": "437229"
  },
  {
    "text": "brains would only be half broken that's true so this is our gift to you only half break your brains so here's what we",
    "start": "437229",
    "end": "444639"
  },
  {
    "text": "started out with we had some sort of common good developer services so this is our our docker repository this is our",
    "start": "444639",
    "end": "452979"
  },
  {
    "text": "git repository corporate DNS all of that stuff that we already had working then",
    "start": "452979",
    "end": "459370"
  },
  {
    "text": "we had 24 Debian VMs running in our data center and those are running our Jenkins",
    "start": "459370",
    "end": "465520"
  },
  {
    "text": "master and our Jenkins worker nodes and that's what we had for compute and then",
    "start": "465520",
    "end": "470680"
  },
  {
    "text": "of course we've got those developers with their laptops that have to use this and so this is what we were going for is",
    "start": "470680",
    "end": "477840"
  },
  {
    "text": "this is what we wanted to build it was sort of we wanted to continue to use those common developer infrastructure",
    "start": "477840",
    "end": "483729"
  },
  {
    "text": "pieces so the developers could keep following their current workflow but we wanted to take those Debian VMS",
    "start": "483729",
    "end": "490360"
  },
  {
    "text": "clean them off get a kubernetes cluster running on them and install Jenkins on that and so that is what we did",
    "start": "490360",
    "end": "499530"
  },
  {
    "start": "499000",
    "end": "516000"
  },
  {
    "text": "so you recall those three easy steps right how hard could it be this is when our brains really started",
    "start": "499530",
    "end": "506380"
  },
  {
    "text": "to feel the pain these these are our top three brain breakers that popped up as we tried to build the kubernetes cluster",
    "start": "506380",
    "end": "512800"
  },
  {
    "text": "a year and a half ago let's go through these here's what we what you noticed a",
    "start": "512800",
    "end": "519940"
  },
  {
    "text": "vm would suddenly become a responsive and completely unreachable it couldn't even peeing at responded that the vm's",
    "start": "519940",
    "end": "526420"
  },
  {
    "text": "why are they slow why are they hanging right this happened to over and over again with no pattern no reason what",
    "start": "526420",
    "end": "531580"
  },
  {
    "text": "could have happened it must be the networking surely nope disks full yep why are the discs getting",
    "start": "531580",
    "end": "538930"
  },
  {
    "text": "full what's going on here yeah well it turns out docker and kulit create lots",
    "start": "538930",
    "end": "545560"
  },
  {
    "start": "541000",
    "end": "625000"
  },
  {
    "text": "of files on far Lib I see a few people nodding you might have seen this before alright okay you know where we're going",
    "start": "545560",
    "end": "552160"
  },
  {
    "text": "here right so though they create all these files on vara Lib and Varley abuse",
    "start": "552160",
    "end": "557200"
  },
  {
    "text": "Lee lands on the root partition of your VM and if you're using a VM your root",
    "start": "557200",
    "end": "562600"
  },
  {
    "text": "partition is probably really small and really slow and so it doesn't take long",
    "start": "562600",
    "end": "568180"
  },
  {
    "text": "to fill it up and even if you ask to for your people to attach a disk to it so you can have lots of space to run your",
    "start": "568180",
    "end": "574060"
  },
  {
    "text": "builds that's not I'm gonna get mounted by default you actually have to do it yourself so it's like you're driving a",
    "start": "574060",
    "end": "580570"
  },
  {
    "text": "truck and all the freight is in the cab with you and the trailer is empty so what do you do well you have to mount",
    "start": "580570",
    "end": "587770"
  },
  {
    "text": "and move so first of all you have to make sure that you mount that extra file system that got attached the big fast",
    "start": "587770",
    "end": "595180"
  },
  {
    "text": "file system and this shows you how to how to do that the four unit Unix",
    "start": "595180",
    "end": "600279"
  },
  {
    "text": "commands and then you have to get all get it excuse me you have to set up the",
    "start": "600279",
    "end": "605440"
  },
  {
    "text": "paths for docker and for couplets so that they land on that big file system here's a bit snip of code that shows how",
    "start": "605440",
    "end": "610810"
  },
  {
    "text": "to do it with the sim link I'm sure this isn't new to anybody but that solved the problem for us and we've got the whole",
    "start": "610810",
    "end": "618040"
  },
  {
    "text": "thing scripted out in our repo so go ahead and check out if you're planning to do this yourself save yourself some brain pain",
    "start": "618040",
    "end": "624260"
  },
  {
    "text": "problem solved so next what's up with the networking not connecting right so at this point we",
    "start": "624260",
    "end": "631560"
  },
  {
    "start": "625000",
    "end": "703000"
  },
  {
    "text": "have our disc is she sure it's sorted out capacities looking good and then we ran against this networking issue what",
    "start": "631560",
    "end": "636990"
  },
  {
    "text": "we saw was when kubernetes tried to connect sorry we went where we saw was",
    "start": "636990",
    "end": "642900"
  },
  {
    "text": "when kubernetes was running the nodes we never show is ready right okay this is uh this is quite peculiar this is where",
    "start": "642900",
    "end": "650070"
  },
  {
    "text": "a VM could talk to a VM but a couplet cannot talk to a couplet a pod cannot talk to a pod our viens were looking",
    "start": "650070",
    "end": "656490"
  },
  {
    "text": "good our docker demons running VMs could talk to each other what's going on here I don't know let's find out well it",
    "start": "656490",
    "end": "663600"
  },
  {
    "text": "turns out the answer is an IP address ranges so there are three private ipv4 address ranges you can see here and our",
    "start": "663600",
    "end": "670680"
  },
  {
    "text": "corporate network was running on the one that starts with ten and it turns out that Kubb admin by default yeah I see",
    "start": "670680",
    "end": "677280"
  },
  {
    "text": "some nods here also runs on ten and so you end up with a pod network running on",
    "start": "677280",
    "end": "682830"
  },
  {
    "text": "the same IP range as your corporate network and as you can imagine if you're sending packets from one 10 dot address",
    "start": "682830",
    "end": "689460"
  },
  {
    "text": "to another 10 dot a dress it expects it all to be one continuous network range and the putt the packets get lost and",
    "start": "689460",
    "end": "695190"
  },
  {
    "text": "the pods can't talk to each other and everything goes haywire so it took us a",
    "start": "695190",
    "end": "700560"
  },
  {
    "text": "long time to figure out that this was actually the problem but it turns out the solution is pretty simple you just",
    "start": "700560",
    "end": "707100"
  },
  {
    "start": "703000",
    "end": "739000"
  },
  {
    "text": "pick a different IP address range from a different private IP network range and then all right everyone got pictures and",
    "start": "707100",
    "end": "716550"
  },
  {
    "text": "then you pass it into the coop admin and it function it's as simple as that",
    "start": "716550",
    "end": "722720"
  },
  {
    "text": "problem solved the kubernetes cluster can come together ok now that we have a",
    "start": "722720",
    "end": "729360"
  },
  {
    "text": "working kubernetes cluster the networking is running smoothly the discs have plenty of capacity but where should",
    "start": "729360",
    "end": "735180"
  },
  {
    "text": "we put jenkins home and this kubernetes land yeah so jenkins is old-fashioned it",
    "start": "735180",
    "end": "743340"
  },
  {
    "start": "739000",
    "end": "812000"
  },
  {
    "text": "expects to have a disk that it can rely on to put its files in and get them back but the problem is now it's running in a",
    "start": "743340",
    "end": "750480"
  },
  {
    "text": "pod and a pod is not of em they don't have discs so you have to",
    "start": "750480",
    "end": "756370"
  },
  {
    "text": "set up a persistent volume to put that Jenkins home on right and that's fine if",
    "start": "756370",
    "end": "761650"
  },
  {
    "text": "you're running in Google Cloud or Azure cloud or any other managed kubernetes because they've gone to the trouble to",
    "start": "761650",
    "end": "767410"
  },
  {
    "text": "build persistent disks for you but if you're running it in your data center on a bunch of machines that you scrap together your options are pretty slim so",
    "start": "767410",
    "end": "775360"
  },
  {
    "text": "we tried a couple of different things first we had a complicated backup scheme involving init containers and git",
    "start": "775360",
    "end": "782170"
  },
  {
    "text": "repositories and we could never get that to be really reliable then we tried a",
    "start": "782170",
    "end": "787630"
  },
  {
    "text": "bunch of roll-your-own contestant volume things like rook or port works or",
    "start": "787630",
    "end": "792970"
  },
  {
    "text": "cluster FS and at the time remember this is a year and a half ago none of those were reliable although we understand",
    "start": "792970",
    "end": "799210"
  },
  {
    "text": "they're getting better so if anyone hears brook or cluster FS or any of those thank you for doing the work it's",
    "start": "799210",
    "end": "805120"
  },
  {
    "text": "valuable we want to see more but here's what we ended up doing we ended up using",
    "start": "805120",
    "end": "813640"
  },
  {
    "start": "812000",
    "end": "881000"
  },
  {
    "text": "network file share it's good old 40 year old technology to the rescue here what",
    "start": "813640",
    "end": "819520"
  },
  {
    "text": "we did is we treated a single node in the cluster like an old timey file server right so it mounts a directory it",
    "start": "819520",
    "end": "825880"
  },
  {
    "text": "shares it in NFS then we can mount that using the standard kubernetes stuff the benefit is it is super simple and it is",
    "start": "825880",
    "end": "832630"
  },
  {
    "text": "super stable it's been over six months we haven't looked back it's been stable that's the specific right yeah drawback is it's an old-timey file",
    "start": "832630",
    "end": "839050"
  },
  {
    "text": "share it's not redundant and you just have to treat it as such but good news hey guess what we have 40 years of",
    "start": "839050",
    "end": "845980"
  },
  {
    "text": "experience running these kind of file systems so I think we can figure this out",
    "start": "845980",
    "end": "851740"
  },
  {
    "text": "so here's what you do you first step you see on the left you pin your NFS server deployment to a particular node in your",
    "start": "851740",
    "end": "858460"
  },
  {
    "text": "cluster the second thing is you do is you create a service and you have to give it a static cluster IP address",
    "start": "858460",
    "end": "864100"
  },
  {
    "text": "that's important because NFS doesn't talk host names it only talks IPs and",
    "start": "864100",
    "end": "870700"
  },
  {
    "text": "then third you provision a persistent volume referencing that cluster IP address and voila you have a persistent",
    "start": "870700",
    "end": "878860"
  },
  {
    "text": "volume okay now we're ready to move our Jenkins",
    "start": "878860",
    "end": "883920"
  },
  {
    "text": "jobs into our new kubernetes cluster we have distilled our experience into these two recommendations for you how and why",
    "start": "883920",
    "end": "890640"
  },
  {
    "text": "to benchmark your disk subsystems and how and why your Jenkins should be a",
    "start": "890640",
    "end": "896150"
  },
  {
    "text": "repeatable configuration yep on benchmarking you want to if you want",
    "start": "896150",
    "end": "902460"
  },
  {
    "start": "899000",
    "end": "930000"
  },
  {
    "text": "fast builds you're gonna need fast disks there are two important aspects for performance throughput and AI ops",
    "start": "902460",
    "end": "909330"
  },
  {
    "text": "throughputs how many how fast can you read and write one big blob of data I ops I apps per second is how fast can",
    "start": "909330",
    "end": "916500"
  },
  {
    "text": "you read or write many lots of tiny blobs of data your kubernetes nodes must",
    "start": "916500",
    "end": "921540"
  },
  {
    "text": "have fast disks that are at least as as fast or faster than your old build system benchmarking is easy and here's",
    "start": "921540",
    "end": "927540"
  },
  {
    "text": "how no software to install you can use",
    "start": "927540",
    "end": "932580"
  },
  {
    "start": "930000",
    "end": "956000"
  },
  {
    "text": "these simple unix commands to benchmark your disks we have this in our example repo you can just paste this into a",
    "start": "932580",
    "end": "940230"
  },
  {
    "text": "Jenkins job and run it or you can run it on a command line it's gonna produce some numbers for throughput and I ops",
    "start": "940230",
    "end": "945750"
  },
  {
    "text": "that'll help you build a better performant kubernetes cluster that's faster than your old build system yeah",
    "start": "945750",
    "end": "951930"
  },
  {
    "text": "it's really easy it's worth the time just do it alright the next thing is how",
    "start": "951930",
    "end": "958860"
  },
  {
    "start": "956000",
    "end": "972000"
  },
  {
    "text": "do you make the Jenkins configuration repeatable and we went through a lot of trouble with this dealing with bugs and",
    "start": "958860",
    "end": "963930"
  },
  {
    "text": "earlier Lee Jenkins too a lot of those are shaken out but we want to share two recommendations with you about how you",
    "start": "963930",
    "end": "969930"
  },
  {
    "text": "can handle Jenkins configuration our first recommendation is this just do the",
    "start": "969930",
    "end": "975750"
  },
  {
    "start": "972000",
    "end": "1019000"
  },
  {
    "text": "global Jenkins configuration through the Jenkins user interface right this is where you're on your laptop you log into",
    "start": "975750",
    "end": "982290"
  },
  {
    "text": "Jenkins you go to the config screen you install plug-ins etc etc etc all your",
    "start": "982290",
    "end": "987330"
  },
  {
    "text": "changes get written to Jenkins home and then you have a backup job that can keep that backed up and you have a restored",
    "start": "987330",
    "end": "993060"
  },
  {
    "text": "job that can bring it back when your cluster goes down or if you need to move a reprovision or do whatever you need to",
    "start": "993060",
    "end": "998130"
  },
  {
    "text": "do the good news is in the example repo we have a backup job and a restore job baked into the helm chart and so if you",
    "start": "998130",
    "end": "1005690"
  },
  {
    "text": "just pull it down you can see how that works adapted to your needs and do whatever you want to do it is way simpler than",
    "start": "1005690",
    "end": "1012530"
  },
  {
    "text": "trying to figure out how to write all those groovy scripts to do what add menu I can do in just a few minutes okay",
    "start": "1012530",
    "end": "1019600"
  },
  {
    "start": "1019000",
    "end": "1051000"
  },
  {
    "text": "the next thing we'd say is run your build configuration in Jenkins files and check those Jenkins files in with the",
    "start": "1019600",
    "end": "1026270"
  },
  {
    "text": "code repo that that way the developers really truly own their own build process",
    "start": "1026270",
    "end": "1033230"
  },
  {
    "text": "right if the build breaks they can log in they can write code in the Jenkins",
    "start": "1033230",
    "end": "1038449"
  },
  {
    "text": "file they can write scripts they can patch around they can test out different build configurations on different",
    "start": "1038449",
    "end": "1044180"
  },
  {
    "text": "branches it's their problem and they can have the tools at their disposal to just fix it so that's what we did and then we",
    "start": "1044180",
    "end": "1054770"
  },
  {
    "start": "1051000",
    "end": "1097000"
  },
  {
    "text": "let the developers drive it so at this point in the journey kubernetes is",
    "start": "1054770",
    "end": "1059810"
  },
  {
    "text": "running nicely we have sample workloads going through flowing yep we're",
    "start": "1059810",
    "end": "1065090"
  },
  {
    "text": "benchmarked our i/o so we're performing we've migrated our pipeline job over and brought our old bad Jenkins down",
    "start": "1065090",
    "end": "1071990"
  },
  {
    "text": "scrapped and now we're ready to turn over to developers and this is the point like on Friday we sent out a reminder",
    "start": "1071990",
    "end": "1078800"
  },
  {
    "text": "email to all the development team on Monday morning when you come in you're gonna have a shiny new build system yes",
    "start": "1078800",
    "end": "1084890"
  },
  {
    "text": "so Johnson's we did it we got through it we've got all our goals met and then I'm",
    "start": "1084890",
    "end": "1091370"
  },
  {
    "text": "gonna show you what happened on Monday morning here all right there it goes Oh all right",
    "start": "1091370",
    "end": "1098080"
  },
  {
    "start": "1097000",
    "end": "1131000"
  },
  {
    "text": "here we go we're barely working ha ha we're barely working and if you see in the back there that red shirt that's",
    "start": "1098080",
    "end": "1104640"
  },
  {
    "text": "Jonathan and I back there we're putting some chains on the bus to get it out of the snow we're fixing the engine in the",
    "start": "1104640",
    "end": "1111010"
  },
  {
    "text": "front of the bus there you can see all the developers standing around with the hands in their pockets staring at us and",
    "start": "1111010",
    "end": "1116250"
  },
  {
    "text": "you see that guy in the front with a crazy jacket that's our UI developer",
    "start": "1116250",
    "end": "1122610"
  },
  {
    "text": "that's that's Mike that's Mike yep good way Mike I'm like all right all right",
    "start": "1123390",
    "end": "1131950"
  },
  {
    "start": "1131000",
    "end": "1146000"
  },
  {
    "text": "well this is when our brains were really started to break because we didn't have a fallback plan we had to move forward",
    "start": "1131950",
    "end": "1137860"
  },
  {
    "text": "and solve these problems so let's have a look at the top three brain breakers that happened once the developers",
    "start": "1137860",
    "end": "1143290"
  },
  {
    "text": "actually started using this for real first intermittently failing builds",
    "start": "1143290",
    "end": "1149760"
  },
  {
    "start": "1146000",
    "end": "1199000"
  },
  {
    "text": "we're good alright first thing we started seeing is boom airs in the log",
    "start": "1149760",
    "end": "1155620"
  },
  {
    "text": "now what the heck are all mare's does anybody know what it mares oh yeah",
    "start": "1155620",
    "end": "1161350"
  },
  {
    "text": "someone's got it out for me out of memory people raise their hands I wish",
    "start": "1161350",
    "end": "1167230"
  },
  {
    "text": "you guys were here a year and half ago could help alright what we saw was",
    "start": "1167230",
    "end": "1172720"
  },
  {
    "text": "unresponsive nodes intermittently failing builds pods randomly turn terminating with room airs in the logs",
    "start": "1172720",
    "end": "1179680"
  },
  {
    "text": "and nodes from memory starved what we first tried is adding more nodes to the",
    "start": "1179680",
    "end": "1185860"
  },
  {
    "text": "help nope wrong add more memory surely and more memories gonna help this problem no how about",
    "start": "1185860",
    "end": "1193390"
  },
  {
    "text": "throttle the builds there too many bills running at once nope so what is the problem here so the problem is that Java",
    "start": "1193390",
    "end": "1201430"
  },
  {
    "start": "1199000",
    "end": "1359000"
  },
  {
    "text": "doesn't play nice with containers it just doesn't alright so and how many of",
    "start": "1201430",
    "end": "1208600"
  },
  {
    "text": "you are running Java workloads almost all how many of you are running Jenkins",
    "start": "1208600",
    "end": "1213990"
  },
  {
    "text": "alright if you're running Jenkins you're running Java okay just just so we're all",
    "start": "1213990",
    "end": "1219520"
  },
  {
    "text": "clear if you're running Jenkins you're running Java all right so what we saw so so let's let me pose a",
    "start": "1219520",
    "end": "1227500"
  },
  {
    "text": "hypothetical here's happens here on this screen is my little drawing and you can see there's an 8",
    "start": "1227500",
    "end": "1233109"
  },
  {
    "text": "gigabyte container running on a 16 gig machine and in it we've got a Java process and that Java process is working",
    "start": "1233109",
    "end": "1239709"
  },
  {
    "text": "really hard and so it's trying to allocate memory and allocate memory and allocate memory and keep up with all the",
    "start": "1239709",
    "end": "1245289"
  },
  {
    "text": "tests that those blessed developers have done for us and eventually it needs to",
    "start": "1245289",
    "end": "1251289"
  },
  {
    "text": "allocate more memory so it asks the operating system how much memory do you have and does anyone know how much operator memory the operating system",
    "start": "1251289",
    "end": "1258279"
  },
  {
    "text": "tells it all of it 16 gigabytes yeah so",
    "start": "1258279",
    "end": "1265389"
  },
  {
    "text": "whatever your heap size is it allocates 16 gigabytes so if if you don't tell Java how much memory it has it's going",
    "start": "1265389",
    "end": "1272769"
  },
  {
    "text": "to use up every bit byte of memory in your entire virtual machine it completely ignores whatever limits you",
    "start": "1272769",
    "end": "1278440"
  },
  {
    "text": "put on the container and so if you've done your kubernetes right you've set limits on your pods as soon as Java",
    "start": "1278440",
    "end": "1285339"
  },
  {
    "text": "crosses over that 8 gigabyte limit by out of memory all right so yeah Java has",
    "start": "1285339",
    "end": "1294159"
  },
  {
    "text": "no respect so what do you do it's disrespectful it's so disrespectful so what do you do well first of all always",
    "start": "1294159",
    "end": "1300009"
  },
  {
    "text": "always always always always set container memory limits not just request",
    "start": "1300009",
    "end": "1307169"
  },
  {
    "text": "limits you want kubernetes to kill your pods when they try to take too much memory all right always if you aren't",
    "start": "1307169",
    "end": "1316719"
  },
  {
    "text": "sure what I mean come talk to me afterwards ok second always set the",
    "start": "1316719",
    "end": "1321729"
  },
  {
    "text": "explicitly set the Java memory limit on the Java process always because that's",
    "start": "1321729",
    "end": "1328899"
  },
  {
    "text": "the only way you can be sure it's going to stay within the memory limits of the container I know that the Java folks",
    "start": "1328899",
    "end": "1334239"
  },
  {
    "text": "have worked on all kinds of extra economics and check the C group and blah blah blah I don't trust any of that just",
    "start": "1334239",
    "end": "1340539"
  },
  {
    "text": "set the memory level and that also means if you're running",
    "start": "1340539",
    "end": "1346650"
  },
  {
    "text": "maven or aunt or Gradle you have to go dig through the documentation and find",
    "start": "1346650",
    "end": "1352020"
  },
  {
    "text": "those environment variables so you can pass that information into those may have in Gradle other build processes as",
    "start": "1352020",
    "end": "1357930"
  },
  {
    "text": "well all right the next cow the next",
    "start": "1357930",
    "end": "1363480"
  },
  {
    "start": "1359000",
    "end": "1382000"
  },
  {
    "text": "issue we ran into is dangling docker berries that's a pet name we came up with all right so this is where a half a",
    "start": "1363480",
    "end": "1371310"
  },
  {
    "text": "half of the cluster had full disks disks full disk didn't we already fix that I",
    "start": "1371310",
    "end": "1376440"
  },
  {
    "text": "thought we already fixed that seriously is this happening again okay what was the real problem here well the real",
    "start": "1376440",
    "end": "1384270"
  },
  {
    "start": "1382000",
    "end": "1423000"
  },
  {
    "text": "problem is this when a docker container exits it leaves some stuff behind it drops some berries on those partitions",
    "start": "1384270",
    "end": "1390540"
  },
  {
    "text": "right and that includes volumes that maybe someone referenced it includes",
    "start": "1390540",
    "end": "1396060"
  },
  {
    "text": "images that you downloaded it includes container layers especially if that docker image crashed and so no matter",
    "start": "1396060",
    "end": "1402870"
  },
  {
    "text": "how big your partition is eventually these things are gonna build up and build up and build up over time until",
    "start": "1402870",
    "end": "1408480"
  },
  {
    "text": "your partition is full so remember all those who mares we're having earlier",
    "start": "1408480",
    "end": "1413550"
  },
  {
    "text": "that was just tons and tons of unused docker layers left behind also and so",
    "start": "1413550",
    "end": "1418710"
  },
  {
    "text": "nothing will automatically clean this up for you so what do you have to do someone said",
    "start": "1418710",
    "end": "1424890"
  },
  {
    "start": "1423000",
    "end": "1471000"
  },
  {
    "text": "it right all right so some solutions are what we did ended up doing a year and",
    "start": "1424890",
    "end": "1431160"
  },
  {
    "text": "half ago is we installed a cron job to clean hourly so we've created a script that looks for removed dangling docker",
    "start": "1431160",
    "end": "1438120"
  },
  {
    "text": "berries that's right that's scripts in our repo you guys can use it to also we",
    "start": "1438120",
    "end": "1443370"
  },
  {
    "text": "started monitoring our node partition usage with we're using Prometheus and grow fauna and we get alerts when things",
    "start": "1443370",
    "end": "1449250"
  },
  {
    "text": "are getting full so that's that's a big help and there's something out there called couplet GC why you should maybe",
    "start": "1449250",
    "end": "1455310"
  },
  {
    "text": "have a look at that give it a try yeah and some of the lessons learned are you got dangling docker berries pruning them",
    "start": "1455310",
    "end": "1462410"
  },
  {
    "text": "because they will grow in your partitions and I heard it can be quite painful at times yeah",
    "start": "1462410",
    "end": "1469940"
  },
  {
    "text": "some of you guys got that so now the builds are stable but we're getting",
    "start": "1470800",
    "end": "1476480"
  },
  {
    "start": "1471000",
    "end": "1537000"
  },
  {
    "text": "really long waits for our Jenkins workers and we run during what we can do about that",
    "start": "1476480",
    "end": "1482230"
  },
  {
    "text": "sometimes it takes like four hours for a bill to start because the workers are",
    "start": "1482230",
    "end": "1487790"
  },
  {
    "text": "all used so we said maybe we'll try a couple of things we will try throttling back the number of builds that run that",
    "start": "1487790",
    "end": "1494480"
  },
  {
    "text": "wasn't great we tried to get the builds to run faster that didn't solve the problem really either so what do we do",
    "start": "1494480",
    "end": "1501250"
  },
  {
    "text": "well we have this thing called pod Tetris right so this is how to lose a pod Tetris if you notice we have a 24",
    "start": "1501250",
    "end": "1509000"
  },
  {
    "text": "node cluster here and we have one size pod for every step every stage of the",
    "start": "1509000",
    "end": "1514070"
  },
  {
    "text": "build that's not a good idea if you look at our pod count on the top left you'll",
    "start": "1514070",
    "end": "1519230"
  },
  {
    "text": "see we only have 23 pods running and our cluster is 100% used there's no more no",
    "start": "1519230",
    "end": "1525920"
  },
  {
    "text": "more pods can be scheduled on there so this this may be only two about two developer pipelines running every all",
    "start": "1525920",
    "end": "1530960"
  },
  {
    "text": "the other bills will be queued up so we're gonna show you how to win it pod detrás right here okay so this is if you",
    "start": "1530960",
    "end": "1540650"
  },
  {
    "text": "look at this picture you still have 23 pods running cluster II percent uses",
    "start": "1540650",
    "end": "1545690"
  },
  {
    "text": "only it's down to 30% so what we ended up doing I'll tell you we profiled our build stages we watched the stages run",
    "start": "1545690",
    "end": "1553010"
  },
  {
    "text": "and identified how much CPU each stage need how much memory and we saw that",
    "start": "1553010",
    "end": "1558800"
  },
  {
    "text": "some stages require very little resources and others required more so we",
    "start": "1558800",
    "end": "1564980"
  },
  {
    "text": "use the kubernetes jenkins plugin to define three pod sizes a tiny pod for",
    "start": "1564980",
    "end": "1570200"
  },
  {
    "text": "checking out repos or slack notifications a small pod for build stages like deploy sonar and unit tests",
    "start": "1570200",
    "end": "1577970"
  },
  {
    "text": "and then we have some large pauses for some intensive and in testing so we",
    "start": "1577970",
    "end": "1583640"
  },
  {
    "text": "declare these these custom pod sizes in the Jenkins file in different build stages the results were profound we saw",
    "start": "1583640",
    "end": "1590930"
  },
  {
    "text": "a 5 up to a 500% increase in our build rates per hour we were able to run more",
    "start": "1590930",
    "end": "1596270"
  },
  {
    "text": "pods in the same exact cluster and the queueing went away a developer committed with",
    "start": "1596270",
    "end": "1601850"
  },
  {
    "text": "a minute that we're getting feedback and the pipeline was running brilliant additionally if you're running in a",
    "start": "1601850",
    "end": "1607610"
  },
  {
    "text": "managed cloud this could save you a lot of money a year too because you don't need so many VMs in a managed cloud you",
    "start": "1607610",
    "end": "1614600"
  },
  {
    "text": "might be getting charged where every milli core you use or every time you use more ram so this could save you money as",
    "start": "1614600",
    "end": "1620570"
  },
  {
    "text": "well you're gonna get more yield out of your hardware this is a big win for us this may like Tetris so we wanted to",
    "start": "1620570",
    "end": "1625940"
  },
  {
    "text": "show that yeah it's worth it to try and figure this out so finally we have",
    "start": "1625940",
    "end": "1631840"
  },
  {
    "text": "Jenkins all working we got those short builds they down from two hours to 30",
    "start": "1631840",
    "end": "1637490"
  },
  {
    "text": "minutes we've eliminated our build queues developers get feedback right away we can recover from a broken",
    "start": "1637490",
    "end": "1643940"
  },
  {
    "text": "cluster within a day the developers are starting to actually trust the build results and we are free to work on other",
    "start": "1643940",
    "end": "1651679"
  },
  {
    "text": "things it was a big takeaway for us that we had the same exact infrastructure we",
    "start": "1651679",
    "end": "1657080"
  },
  {
    "text": "started with the battle jenk it's the same VMs right and we got our builds faster we were able to put more builds",
    "start": "1657080",
    "end": "1662929"
  },
  {
    "text": "through it we and it was more reliable repot we reliable repeatable all the",
    "start": "1662929",
    "end": "1668030"
  },
  {
    "text": "good vibes is because we moved our Jenkins into kubernetes we could run everything way more efficient we could",
    "start": "1668030",
    "end": "1673970"
  },
  {
    "text": "deal the hardware it just wasn't worth it in the end it was worth it yes but",
    "start": "1673970",
    "end": "1679730"
  },
  {
    "text": "our brains oh they're definitely broken definitely broken all right so let's",
    "start": "1679730",
    "end": "1687179"
  },
  {
    "start": "1686000",
    "end": "1734000"
  },
  {
    "text": "bring this home so if you want to do this too these are the four things you need to do number one benchmark your i/o",
    "start": "1687179",
    "end": "1693090"
  },
  {
    "text": "performance make sure that your disks on your new system are gonna be faster than the disks on your old system okay",
    "start": "1693090",
    "end": "1699240"
  },
  {
    "text": "second Jenkins files for builds get your developers to learn how to manage it themselves that's what they need",
    "start": "1699240",
    "end": "1705270"
  },
  {
    "text": "it'll that way Jenkins becomes everybody's problem and don't bother scripting the global config it's not",
    "start": "1705270",
    "end": "1711750"
  },
  {
    "text": "worth your time and it'll slow you down third thing play pod Tetris this is worth your time get more more builds",
    "start": "1711750",
    "end": "1718950"
  },
  {
    "text": "through the same hardware that's what kubernetes is for and second forth don't try and build kubernetes yourself",
    "start": "1718950",
    "end": "1724830"
  },
  {
    "text": "there's so many great options out there including one from sa P that I get to tell you about now thank you for your",
    "start": "1724830",
    "end": "1730919"
  },
  {
    "text": "patience it's called gardener so here's gardener ready a gardener is an open",
    "start": "1730919",
    "end": "1736710"
  },
  {
    "start": "1734000",
    "end": "1774000"
  },
  {
    "text": "source kubernetes management tool from sa P so if you are running kubernetes in",
    "start": "1736710",
    "end": "1741809"
  },
  {
    "text": "your organization and you need to supply lots and lots and lots and lots of kubernetes clusters to different teams",
    "start": "1741809",
    "end": "1747720"
  },
  {
    "text": "this is a good solution for you it gives you the ability a Ford for all your kubernetes clusters it gives you user",
    "start": "1747720",
    "end": "1754890"
  },
  {
    "text": "access controls so some teams can get on some clusters and other teams can get on other clusters and it works on Azure",
    "start": "1754890",
    "end": "1761450"
  },
  {
    "text": "Google cloud AWS and OpenStack and I think Alibaba and a couple of others are",
    "start": "1761450",
    "end": "1767640"
  },
  {
    "text": "coming soon so that's that there's a super cool architecture diagram that I'm gonna skip right over because we have",
    "start": "1767640",
    "end": "1774990"
  },
  {
    "start": "1774000",
    "end": "1821000"
  },
  {
    "text": "the gardener guys here who's the gardener guys where are you stand up if you would well ok yeah if you're here go",
    "start": "1774990",
    "end": "1787799"
  },
  {
    "text": "down to the booth you can talk to them there's another open-source project called kima yeah the kima allows you to extend your",
    "start": "1787799",
    "end": "1794159"
  },
  {
    "text": "commerce cloud with using micro services with k' native yeah all the buzzwords and so let's see stand up if you're from",
    "start": "1794159",
    "end": "1802230"
  },
  {
    "text": "sa P you want to wait wave your hands at everybody we got a couple of people from sa P here there they are",
    "start": "1802230",
    "end": "1807809"
  },
  {
    "text": "if you want to talk to these folks please go down to the booth we're here we're friendly we like you and s AP",
    "start": "1807809",
    "end": "1814740"
  },
  {
    "text": "source of course a ton of open source software and we'd love to chat with you about that so thank you all for coming",
    "start": "1814740",
    "end": "1821500"
  },
  {
    "start": "1821000",
    "end": "2134000"
  },
  {
    "text": "[Applause]",
    "start": "1821500",
    "end": "1829220"
  },
  {
    "text": "are there any questions we have a question here we have a single Jenkins",
    "start": "1829400",
    "end": "1836429"
  },
  {
    "text": "master no no it can handle it yeah I",
    "start": "1836429",
    "end": "1845970"
  },
  {
    "text": "know the Jenkins we have one Jenkins master and it doesn't topple over under the load all right who else yes where",
    "start": "1845970",
    "end": "1857220"
  },
  {
    "text": "the Jenkins plugins affected by moving over to kubernetes the yeah it seemed",
    "start": "1857220",
    "end": "1866040"
  },
  {
    "text": "like the Jenkins plugins didn't really have any problems oh yeah all of our",
    "start": "1866040",
    "end": "1872700"
  },
  {
    "text": "plugins work just the same it was seamless yeah yeah we didn't have any problems with there other questions",
    "start": "1872700",
    "end": "1879830"
  },
  {
    "text": "which hypervisor are we using for our VMs that's a great question I don't know",
    "start": "1882559",
    "end": "1888120"
  },
  {
    "text": "we get those from the corporation anywhere Minnie built it she can answer",
    "start": "1888120",
    "end": "1894528"
  },
  {
    "text": "okay KTM idioms answer our underlying themes rpm all right other questions yes",
    "start": "1895190",
    "end": "1905539"
  },
  {
    "text": "how do we ID the network issue we just were watching packet traces man it was",
    "start": "1905539",
    "end": "1910710"
  },
  {
    "text": "brutal yes so why didn't we scale it up",
    "start": "1910710",
    "end": "1924480"
  },
  {
    "text": "with our dev teams a little at a time so we did we just had a couple of devs with",
    "start": "1924480",
    "end": "1930029"
  },
  {
    "text": "really small new projects and one big old project and the big old project was",
    "start": "1930029",
    "end": "1935370"
  },
  {
    "text": "the real reason we were doing this and the small new projects we'd moved over and they worked great it was no problem",
    "start": "1935370",
    "end": "1940679"
  },
  {
    "text": "at all but then we got this big old project over with all these unit tests and system tests and that's what really",
    "start": "1940679",
    "end": "1946049"
  },
  {
    "text": "broke it control underneath Macy's thank",
    "start": "1946049",
    "end": "1951929"
  },
  {
    "text": "you yes good yes",
    "start": "1951929",
    "end": "1956120"
  },
  {
    "text": "how did we profile the build stages okay so here's what we did yeah",
    "start": "1956980",
    "end": "1964010"
  },
  {
    "text": "so currently installed Griffon and Prometheus and then we brought up the",
    "start": "1964010",
    "end": "1969290"
  },
  {
    "text": "Jenkins pipeline in one browser window and the Prometheus pods in the other",
    "start": "1969290",
    "end": "1974570"
  },
  {
    "text": "browser window and then we correlated which stages were running in which pods and how much memory and disk they were",
    "start": "1974570",
    "end": "1980990"
  },
  {
    "text": "using and that's how he did it it was painless it we used a little technology and we also would just watch the build -",
    "start": "1980990",
    "end": "1987230"
  },
  {
    "text": "yeah all right who else what image did",
    "start": "1987230",
    "end": "1994070"
  },
  {
    "text": "we Google cloud 0.8 yeah oh man I don't",
    "start": "1994070",
    "end": "2004180"
  },
  {
    "text": "know but it's in the the repo so if you check out the repo you can find it",
    "start": "2004180",
    "end": "2009960"
  },
  {
    "text": "that's what we used so we that's when we did our pod touches we define their different pod sizes with that exactly",
    "start": "2012990",
    "end": "2018670"
  },
  {
    "text": "good question yeah sorry yes Patrick",
    "start": "2018670",
    "end": "2024750"
  },
  {
    "text": "yeah we still use a kubernetes plugin yeah this pipeline is still running today running right now so we use the",
    "start": "2024750",
    "end": "2030360"
  },
  {
    "text": "kubernetes chicas kubernetes plugin yes no but we should be okay",
    "start": "2030360",
    "end": "2038159"
  },
  {
    "text": "all right yes",
    "start": "2038159",
    "end": "2041960"
  },
  {
    "text": "well how do you do clarifies let's take that one offline yeah yeah well let's chat afterwards yes yeah yes",
    "start": "2051700",
    "end": "2061250"
  },
  {
    "text": "NFS sir a pod is tied to a single note and a single folder on that note serves all the files yeah yeah that was the did",
    "start": "2061250",
    "end": "2071750"
  },
  {
    "text": "we consider local storage local storage doesn't quite solve the problem that",
    "start": "2071750",
    "end": "2077090"
  },
  {
    "text": "we're trying to solve it's close but it's not quite a good fit and we could",
    "start": "2077090",
    "end": "2082399"
  },
  {
    "text": "talk about that a little more later how did we do the Jenkins upgrade yeah we",
    "start": "2082400",
    "end": "2094190"
  },
  {
    "text": "bring it all down we change the base image and then we bring it all back up again so we update the docker image",
    "start": "2094190",
    "end": "2099440"
  },
  {
    "text": "that's pooled and when we run yeah we don't we don't need high availability yeah I mean if Jenkins has to come down",
    "start": "2099440",
    "end": "2105080"
  },
  {
    "text": "for an hour or two in the middle of the day it's it's no big deal for our developers yeah",
    "start": "2105080",
    "end": "2110630"
  },
  {
    "text": "all right should we get off the mics okay all right no no that's mercifully",
    "start": "2110630",
    "end": "2120380"
  },
  {
    "text": "that that we don't have to do all right well thank you all well we'll be hanging around right here you can ask us all the",
    "start": "2120380",
    "end": "2125930"
  },
  {
    "text": "questions if you'd love to chat thank you thank you thank you",
    "start": "2125930",
    "end": "2130840"
  }
]