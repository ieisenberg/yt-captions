[
  {
    "text": "hey everyone good afternoon uh welcome to join our session today so we are going to share about uh the self-service",
    "start": "880",
    "end": "8240"
  },
  {
    "text": "stream processing platform on kubernetes uh M chya from Apple's AIML data",
    "start": "8240",
    "end": "16160"
  },
  {
    "text": "platform this is our agenda today so we will talk about our journey",
    "start": "16160",
    "end": "21279"
  },
  {
    "text": "moving to the cloud and kubernetes and why we adopt a Pache Flink for real-time data processing then our challenges",
    "start": "21279",
    "end": "28439"
  },
  {
    "text": "while moving to production at large scale and the solutions that we have figured out so",
    "start": "28439",
    "end": "35320"
  },
  {
    "text": "far so first about our journey moving to the cloud and",
    "start": "35320",
    "end": "40600"
  },
  {
    "text": "kubernetes so our users expect very quickly scale up and down of the cluster",
    "start": "40600",
    "end": "46640"
  },
  {
    "text": "resource to match their real-time traffic pattern and we cannot wait for long right and we also don't want to",
    "start": "46640",
    "end": "54039"
  },
  {
    "text": "preempt user applications to honor some of them so we can see this is like a for",
    "start": "54039",
    "end": "60480"
  },
  {
    "text": "the deployment itself we have the autoscaling and how about the",
    "start": "60480",
    "end": "65719"
  },
  {
    "text": "cluster and from a platform's perspective sometimes it can be ideal that we decouple ourself a little bit",
    "start": "67119",
    "end": "73759"
  },
  {
    "text": "away from the infrastructure maintainance overhead and when we move from virtualized deployment to",
    "start": "73759",
    "end": "80600"
  },
  {
    "text": "Containers it actually help us abstract away the physical Hardware",
    "start": "80600",
    "end": "86560"
  },
  {
    "text": "complexities so here we can see the containers can move free around clouds and also the OS",
    "start": "86560",
    "end": "94240"
  },
  {
    "text": "distributions and in the multicloud environment there are a lot of benefits for us to Leverage The Available support",
    "start": "94240",
    "end": "101880"
  },
  {
    "text": "from different Cloud providers the hardware resource and the cost efficiency and kubernetes can help us to",
    "start": "101880",
    "end": "109600"
  },
  {
    "text": "organize this containers workloads in a consistent approach to provide the",
    "start": "109600",
    "end": "117799"
  },
  {
    "text": "compatibility so there can always be limited admins",
    "start": "118840",
    "end": "125360"
  },
  {
    "text": "who can help us address cluster issues at a very large scale luckily we can",
    "start": "125360",
    "end": "130800"
  },
  {
    "text": "leverage kubernetes automatic recovery capability to solve some issues and to",
    "start": "130800",
    "end": "136599"
  },
  {
    "text": "keep the cluster resilient and self-healing in case of failures and we know many platforms have",
    "start": "136599",
    "end": "143160"
  },
  {
    "text": "this need to add additional components on top of an orchestration layer and here we can kubernetes has",
    "start": "143160",
    "end": "150800"
  },
  {
    "text": "this modularized architecture that can allow us to integrate with multiple components and",
    "start": "150800",
    "end": "158319"
  },
  {
    "text": "extensions okay so that's about our journey moving to the cloud and kubernetes and why we adopt a Pache",
    "start": "158319",
    "end": "164120"
  },
  {
    "text": "Flink for real-time data processing so we know users have a very",
    "start": "164120",
    "end": "170159"
  },
  {
    "text": "high expectation in terms of latency when they try to process data in real",
    "start": "170159",
    "end": "175800"
  },
  {
    "text": "time right they expect the data can be immediately processed upon arriving the",
    "start": "175800",
    "end": "181640"
  },
  {
    "text": "the system and we can see Flink supports a pipeline in memory execution model so",
    "start": "181640",
    "end": "188599"
  },
  {
    "text": "data can be accessed from local and when there is a slow",
    "start": "188599",
    "end": "193959"
  },
  {
    "text": "operation Flink is able to adjust its data processing speed so we can see the",
    "start": "193959",
    "end": "201159"
  },
  {
    "text": "bus operator can propagate the back pressure to Upstream",
    "start": "201159",
    "end": "206480"
  },
  {
    "text": "task and here it can avoid system crash overall right to slow down the through",
    "start": "206480",
    "end": "212239"
  },
  {
    "text": "poot a little bit but to avoid the system crash and when there is a very large",
    "start": "212239",
    "end": "218799"
  },
  {
    "text": "State Flink is able to take checkpoints incrementally to capture the Delta and",
    "start": "218799",
    "end": "225000"
  },
  {
    "text": "asynchronously in the background and to restore from the state fling can support",
    "start": "225000",
    "end": "230439"
  },
  {
    "text": "this exactly once guarantee with replayable Source like Kafka and the atomic State",
    "start": "230439",
    "end": "238319"
  },
  {
    "text": "commit so we know many users have this need to connect their streaming Pipeline with",
    "start": "238680",
    "end": "246040"
  },
  {
    "text": "upstream or Downstream data source and sys right and Flink has this very rich",
    "start": "246040",
    "end": "251519"
  },
  {
    "text": "connector ecosystem not limited to Kafka Iceberg but also cassendra elastic",
    "start": "251519",
    "end": "256840"
  },
  {
    "text": "search paa right and users don't need to write extra jobs to complete their",
    "start": "256840",
    "end": "262880"
  },
  {
    "text": "entire workflow we also know users have",
    "start": "262880",
    "end": "270280"
  },
  {
    "text": "background and needs when they're working with flank gr and luckily we have this multi-layered API on the very",
    "start": "270280",
    "end": "278320"
  },
  {
    "text": "top there is SQL table API that comes with optimization and building borderer",
    "start": "278320",
    "end": "283840"
  },
  {
    "text": "plate codes right and in the middle we have this data stream API to unify stream and batch processing with some",
    "start": "283840",
    "end": "290320"
  },
  {
    "text": "Advanced window functions and at the very low level there is this process function that can provide fine",
    "start": "290320",
    "end": "296520"
  },
  {
    "text": "granularity for users to manage their state and time in a",
    "start": "296520",
    "end": "302280"
  },
  {
    "text": "very granular way okay but still with fling and kubernetes we have",
    "start": "302280",
    "end": "310639"
  },
  {
    "text": "challenges when moving to production at a large scale what can be some of them",
    "start": "310639",
    "end": "315880"
  },
  {
    "text": "the first is that we need an automated approach to deploy flank on kubernetes",
    "start": "315880",
    "end": "321880"
  },
  {
    "text": "right and we also want a control plane that can scale Beyond a single kubernetes",
    "start": "321880",
    "end": "328479"
  },
  {
    "text": "cluster we would like to help our users self on board observe and troubleshoot their",
    "start": "328479",
    "end": "335280"
  },
  {
    "text": "applications without admins in the loop because there can be so many of them",
    "start": "335280",
    "end": "341240"
  },
  {
    "text": "right Al once support users and admins they have this need to bulk operate a",
    "start": "341240",
    "end": "347680"
  },
  {
    "text": "large number of deployments how can we help them and also to automatically scale not only",
    "start": "347680",
    "end": "354800"
  },
  {
    "text": "stateless streaming applications but also stateful applications",
    "start": "354800",
    "end": "361479"
  },
  {
    "text": "and how about managed resource availability this can also be challenges right and also for streaming",
    "start": "362680",
    "end": "368639"
  },
  {
    "text": "applications to have cluster upgrade in the cloud",
    "start": "368639",
    "end": "374240"
  },
  {
    "text": "environment so first let's take a look at deploying kubernetes uh deploying Flank In kubernetes right there are",
    "start": "377000",
    "end": "384199"
  },
  {
    "text": "multiple options and the first option coming to our site is the flank kubernetes native integration it's first",
    "start": "384199",
    "end": "391360"
  },
  {
    "text": "introduced in fling 1.10 years ago right so how does it work let's take a look at",
    "start": "391360",
    "end": "397199"
  },
  {
    "text": "this diagram so we can see the kubernetes client is actually embedded inside the Flink client to talk to the",
    "start": "397199",
    "end": "404039"
  },
  {
    "text": "kubernetes API server directly and the fling CL can then create a job Master",
    "start": "404039",
    "end": "409319"
  },
  {
    "text": "deployment including the job manager as well as the kubernetes resource",
    "start": "409319",
    "end": "415400"
  },
  {
    "text": "manager and now we see the kubernetes resource manager has actually responsible to dynamically allocate task",
    "start": "415400",
    "end": "422599"
  },
  {
    "text": "manager parts right through the kubernetes API",
    "start": "422599",
    "end": "427720"
  },
  {
    "text": "server so for this Flink kubernetes native integration we can understand in a way that Flink is self-contained right",
    "start": "427720",
    "end": "435479"
  },
  {
    "text": "it doesn't need to rely on any external tools from kubernetes like the coup CDL command but it then also creates the",
    "start": "435479",
    "end": "442039"
  },
  {
    "text": "challenge to automate the applic the application upgrade in kuis",
    "start": "442039",
    "end": "448319"
  },
  {
    "text": "environment so we must have already been familiar with these two concepts from cordin",
    "start": "448319",
    "end": "454319"
  },
  {
    "text": "entities right the customer resource and also the",
    "start": "454319",
    "end": "459440"
  },
  {
    "text": "operator can we combine the custom resource and operator to capture the",
    "start": "459759",
    "end": "465840"
  },
  {
    "text": "core responsibilities of a human operator to manage Flink",
    "start": "465840",
    "end": "471080"
  },
  {
    "text": "deployments so we know human operators they have deep knowledge of how a Flink",
    "start": "471080",
    "end": "476520"
  },
  {
    "text": "application ought to behave and what to do when they are problems with a Flink kubernetes",
    "start": "476520",
    "end": "482599"
  },
  {
    "text": "operator then it can help us to monitor to upgrade and to deploy the applications in an automatic",
    "start": "482599",
    "end": "491120"
  },
  {
    "text": "way and yes we have this Flint kubernetes operator introduced in flip",
    "start": "491400",
    "end": "496759"
  },
  {
    "text": "212 uh with apple uh being a major contribution to this as well so the",
    "start": "496759",
    "end": "503360"
  },
  {
    "text": "Flint kubernetes operator it actually leverages the Java operator SDK because",
    "start": "503360",
    "end": "508960"
  },
  {
    "text": "it wants to directly talks to the Flink Java client libraries and enter the hood",
    "start": "508960",
    "end": "514320"
  },
  {
    "text": "it still uses the Flink kubernetes native integration to launch the flank deployment in a cluster both the native",
    "start": "514320",
    "end": "522440"
  },
  {
    "text": "integration and the operator will use the fabric8 kubernetes client to talk to",
    "start": "522440",
    "end": "527839"
  },
  {
    "text": "the kubernetes API server so here from this chart we can see all possible current state and",
    "start": "527839",
    "end": "535399"
  },
  {
    "text": "transitions of a Flink resource life cycle",
    "start": "535399",
    "end": "541200"
  },
  {
    "text": "and let's take a deeper look on how the Flint kubernetes operator work from a high level it follows this",
    "start": "542440",
    "end": "550240"
  },
  {
    "text": "control Loop principle from kubernetes so we can see the user creates a custom",
    "start": "550240",
    "end": "557000"
  },
  {
    "text": "resource and then the operator will try to check the status of this resource",
    "start": "557000",
    "end": "562120"
  },
  {
    "text": "from the cluster The Observer will record a pointing time",
    "start": "562120",
    "end": "567839"
  },
  {
    "text": "status but we need to be careful here right before the control Loop finish",
    "start": "567839",
    "end": "573079"
  },
  {
    "text": "anything can change so the status recorded can also change after the observation phase the",
    "start": "573079",
    "end": "580760"
  },
  {
    "text": "operator will have a validator to check the application resource back if it is",
    "start": "580760",
    "end": "586640"
  },
  {
    "text": "in good terms right and then are we ready to do the reconciliation and",
    "start": "586640",
    "end": "592360"
  },
  {
    "text": "execute the upgrade the answer is actually no there are mple things that we need to check",
    "start": "592360",
    "end": "598760"
  },
  {
    "text": "for ex example we want to make sure there is no pending operation for this custom resource it's very common that we",
    "start": "598760",
    "end": "605680"
  },
  {
    "text": "trigger manual save points and it will take some time right so we want to wait any pening operation to finish another",
    "start": "605680",
    "end": "612959"
  },
  {
    "text": "thing is that the status can change what if in the middle something is wrong and",
    "start": "612959",
    "end": "618279"
  },
  {
    "text": "the application is not stable anymore right then the operator is responsible",
    "start": "618279",
    "end": "623480"
  },
  {
    "text": "to bring this customer resource back to its last successful state so something about the operator",
    "start": "623480",
    "end": "632480"
  },
  {
    "text": "and let's assume we now have the Flint kubernetes operator in the production right there can still be challenges to",
    "start": "632480",
    "end": "639399"
  },
  {
    "text": "run p l large so the first one what if the",
    "start": "639399",
    "end": "644760"
  },
  {
    "text": "kubernetes cluster where Flint KU netes operator fails and goes",
    "start": "644760",
    "end": "651360"
  },
  {
    "text": "down can User Group map Beyond one kubernetes name space for mod tendency",
    "start": "652680",
    "end": "659360"
  },
  {
    "text": "Etc right authentication and authorization might need to meet extra",
    "start": "659360",
    "end": "664920"
  },
  {
    "text": "requirements and how can we support that and where should we support it the deployment information need to be",
    "start": "664920",
    "end": "672079"
  },
  {
    "text": "persistent for audit or recovery right what if the cluster is done and the",
    "start": "672079",
    "end": "677560"
  },
  {
    "text": "operator is gone as well and admins need to operate across",
    "start": "677560",
    "end": "683880"
  },
  {
    "text": "multiple clouds multiple accounts and clusters",
    "start": "683880",
    "end": "689760"
  },
  {
    "text": "there can also be other service or platforms that want to integrate for real-time data",
    "start": "689760",
    "end": "696519"
  },
  {
    "text": "processing lessons from BPG right what is BPG okay this is the badge processing",
    "start": "698360",
    "end": "704160"
  },
  {
    "text": "Gateway service that Apple open sourced earlier last year so essentially it helps spark users to",
    "start": "704160",
    "end": "712440"
  },
  {
    "text": "launch spark applications in a kubernetes environment and manage there and there are a bunch of rest AP P",
    "start": "712440",
    "end": "719399"
  },
  {
    "text": "that expose to clients and users directly so how does it work let's take",
    "start": "719399",
    "end": "724440"
  },
  {
    "text": "a look here the batch processing Gateway act as an intermediary layer between the",
    "start": "724440",
    "end": "731720"
  },
  {
    "text": "user request and the kubernetes customer resource and we can deploy spark",
    "start": "731720",
    "end": "736920"
  },
  {
    "text": "kubernetes operator in each cluster and it defines a spark application customer",
    "start": "736920",
    "end": "743839"
  },
  {
    "text": "resource and now it's the spark batch processing gateways respon",
    "start": "743839",
    "end": "748880"
  },
  {
    "text": "responsibility to transform the user request into a crd format that can be recognized by the operator then this",
    "start": "748880",
    "end": "756440"
  },
  {
    "text": "crds can be sent to the spark kubernetes cluster have the operator to take over",
    "start": "756440",
    "end": "761600"
  },
  {
    "text": "there so essentially the idea here is that the Gateway service is trying to",
    "start": "761600",
    "end": "768079"
  },
  {
    "text": "take over all the heavy lifting on behalf of the users for users they only",
    "start": "768079",
    "end": "774440"
  },
  {
    "text": "need to worry about specify a few configurations maybe from a UI hitting",
    "start": "774440",
    "end": "779680"
  },
  {
    "text": "several buttons and that's it right they don't need to worry about the underlying infrastructure details or even be",
    "start": "779680",
    "end": "786880"
  },
  {
    "text": "familiar with kubernetes it also provides these flexibilities to support",
    "start": "786880",
    "end": "792199"
  },
  {
    "text": "more Frameworks in the future it doesn't need to be spark it can be anything behind",
    "start": "792199",
    "end": "799440"
  },
  {
    "text": "right and more lessons from this batch processing Gateway that we can learn",
    "start": "801079",
    "end": "806440"
  },
  {
    "text": "from so we talk about this m talcing challenge let's say users from different",
    "start": "806440",
    "end": "812720"
  },
  {
    "text": "teams right they want to submit their spark applications into a single cluster",
    "start": "812720",
    "end": "819240"
  },
  {
    "text": "or maybe multiple ones how can we help them there from this graph we can see in",
    "start": "819240",
    "end": "825480"
  },
  {
    "text": "the middle the batch processing Gateway tries to define a list of spark logical",
    "start": "825480",
    "end": "831519"
  },
  {
    "text": "clusters and each logical cluster can support multiple cues very interestingly right",
    "start": "831519",
    "end": "839560"
  },
  {
    "text": "this q1 can map to both the kubernetes name space one and two in the back end",
    "start": "839560",
    "end": "845800"
  },
  {
    "text": "on the spark physical cluster let's see when the user submitt a request asking for",
    "start": "845800",
    "end": "852360"
  },
  {
    "text": "q1 then both c01 and C02 have q1 included in their mapping",
    "start": "852360",
    "end": "860360"
  },
  {
    "text": "and how can we decide so we can have this routing algorithm to consider the",
    "start": "860360",
    "end": "866040"
  },
  {
    "text": "weight of each cluster and our current status right plus some Randomness or anything",
    "start": "866040",
    "end": "871680"
  },
  {
    "text": "if we want to increase the load of certain cluster or namespace we can have all the flexibility there and then we",
    "start": "871680",
    "end": "877399"
  },
  {
    "text": "will help the users to launch their applications in the corresponding backend namespace or cluster and they",
    "start": "877399",
    "end": "884839"
  },
  {
    "text": "don't need to worry about it right they only need to know okay I want to submit this application to a",
    "start": "884839",
    "end": "891639"
  },
  {
    "text": "q1 besides the multi tendency we also know users deeply care about the",
    "start": "892120",
    "end": "897680"
  },
  {
    "text": "observability of their application right they want to trouble shoot they want to debug so the batch processing Gateway",
    "start": "897680",
    "end": "905399"
  },
  {
    "text": "service can have this capability to support a log mover to move the log of a",
    "start": "905399",
    "end": "911560"
  },
  {
    "text": "kubernetes pod to a persistent storage why do we want to move it it's possible",
    "start": "911560",
    "end": "916600"
  },
  {
    "text": "the Pod can be gone right after the application is finished or completed and sometimes we really want to recycle this",
    "start": "916600",
    "end": "923399"
  },
  {
    "text": "pods right because it will create a lot of pressure if there are so many terminated pods and uh the kubernetes",
    "start": "923399",
    "end": "929959"
  },
  {
    "text": "aps will have to monitor it for the health check right so coming back here",
    "start": "929959",
    "end": "935600"
  },
  {
    "text": "when the user hitting the log rest end point that it can go to two",
    "start": "935600",
    "end": "940959"
  },
  {
    "text": "routes first it will directly check from the executor party if it's still available if the log is still there then",
    "start": "940959",
    "end": "948160"
  },
  {
    "text": "it's great right we'll stream from there if not then we'll use a log log indexer",
    "start": "948160",
    "end": "954480"
  },
  {
    "text": "to search for this executor pod or driver pod log and load from a remote",
    "start": "954480",
    "end": "959800"
  },
  {
    "text": "storage so there are a lot of functionalities that can be supported from this kind of Gateway",
    "start": "959800",
    "end": "965920"
  },
  {
    "text": "service okay so with all these Lessons Learned let's take a look at our Flink",
    "start": "965920",
    "end": "972440"
  },
  {
    "text": "control plane we want to support real time data processing here right it's a",
    "start": "972440",
    "end": "977839"
  },
  {
    "text": "busy diagram but let's decipher it right in the middle we have these control",
    "start": "977839",
    "end": "983199"
  },
  {
    "text": "plane instances that can be deployed in multiple accounts or even",
    "start": "983199",
    "end": "989040"
  },
  {
    "text": "Cloud providers with its own persistent storage and on the right hand side we",
    "start": "989040",
    "end": "994560"
  },
  {
    "text": "have this flank physical clusters running the queue right similar to the batch processing",
    "start": "994560",
    "end": "1000160"
  },
  {
    "text": "Gateway and if a user issue a request to our control plane it will first hit a",
    "start": "1000160",
    "end": "1006680"
  },
  {
    "text": "proxy sidecar to get its identity authenticated right after that if the",
    "start": "1006680",
    "end": "1013519"
  },
  {
    "text": "have the identity it will go to the Gateway service to get further authorized for example whether this",
    "start": "1013519",
    "end": "1020360"
  },
  {
    "text": "request indeed have has the permission to use a queue to submit application",
    "start": "1020360",
    "end": "1027160"
  },
  {
    "text": "right so there can be a lot of distinctive features when managing a realtime application platform for",
    "start": "1027160",
    "end": "1034640"
  },
  {
    "text": "example we need to consider how to help the applications to upgrade suspended triggering safe points right these are",
    "start": "1034640",
    "end": "1041038"
  },
  {
    "text": "different from the batch processing Gateway but still from a high level we share this similar architecture and it",
    "start": "1041039",
    "end": "1047918"
  },
  {
    "text": "has this great benefit to reduce our maintenance overhead of multiple platforms and also unify the real time",
    "start": "1047919",
    "end": "1056280"
  },
  {
    "text": "and batch data processing experience in the long",
    "start": "1056280",
    "end": "1061320"
  },
  {
    "text": "term let's picture several scenarios and how we can Leverage The flank control plane to help our",
    "start": "1063520",
    "end": "1071160"
  },
  {
    "text": "users the first one is self-service user on boarding right so imagine that we",
    "start": "1071160",
    "end": "1077240"
  },
  {
    "text": "have so many users and everyone want to have a queue and increase their resource we don't have that many admins to help",
    "start": "1077240",
    "end": "1083559"
  },
  {
    "text": "them right then what we can do from the client side they can issue a",
    "start": "1083559",
    "end": "1090120"
  },
  {
    "text": "request maybe just from a UI letting us know if they need a queue or they want to increase the",
    "start": "1090120",
    "end": "1096200"
  },
  {
    "text": "resource the request will go to the flame control",
    "start": "1096200",
    "end": "1101360"
  },
  {
    "text": "plane the control plane can automatically approve it or in some",
    "start": "1101679",
    "end": "1107039"
  },
  {
    "text": "maybe rare cases we ask the admin to do some manual approval maybe the resource is too",
    "start": "1107039",
    "end": "1113799"
  },
  {
    "text": "much and then the control plan can automatically update cluster resource",
    "start": "1114919",
    "end": "1120280"
  },
  {
    "text": "back for the peration right so it will go to the kubernetes cluster update the",
    "start": "1120280",
    "end": "1127559"
  },
  {
    "text": "Clusters it's also possible that we need to create a new cluster and we can also",
    "start": "1127720",
    "end": "1132840"
  },
  {
    "text": "do it automatically right and it's important that after all this",
    "start": "1132840",
    "end": "1138880"
  },
  {
    "text": "provision the control plane will trigger the smoke test to validate that this queue is successfully created and the",
    "start": "1138880",
    "end": "1145480"
  },
  {
    "text": "smoke test can be running there and then just let the users know",
    "start": "1145480",
    "end": "1151120"
  },
  {
    "text": "either your Q is provision successfully or",
    "start": "1151120",
    "end": "1155519"
  },
  {
    "text": "not similarly for admins right if we want to help them to automatically migrate",
    "start": "1156720",
    "end": "1162679"
  },
  {
    "text": "clusters what we can do here admin can submit request to the control plane okay",
    "start": "1162679",
    "end": "1167880"
  },
  {
    "text": "I want migrate all the workloads from cluster A to B maybe due to some maintainance or cluster",
    "start": "1167880",
    "end": "1175039"
  },
  {
    "text": "upgrade it's also possible that the cluster is filling for some reason and",
    "start": "1175039",
    "end": "1180159"
  },
  {
    "text": "we want to notify the admin to keep them in the",
    "start": "1180159",
    "end": "1184960"
  },
  {
    "text": "loop then the fling control plane is able to trigger safe points for all",
    "start": "1187919",
    "end": "1193039"
  },
  {
    "text": "Flink applications on this cluster a suspend them before moving them right",
    "start": "1193039",
    "end": "1201280"
  },
  {
    "text": "and then on the other cluster that we want to migrate",
    "start": "1201280",
    "end": "1205840"
  },
  {
    "text": "to the control plane can help to automatically redeploy all these",
    "start": "1206880",
    "end": "1212679"
  },
  {
    "text": "applications using the previously triggered safe points and our Pur this back maybe in a remote storage or",
    "start": "1212679",
    "end": "1220919"
  },
  {
    "text": "rdbms and then the control plane again is responsible to verify the Flink",
    "start": "1221520",
    "end": "1226919"
  },
  {
    "text": "application health before letting the admins know okay the",
    "start": "1226919",
    "end": "1232760"
  },
  {
    "text": "migration is successful or not so there can be so many features and",
    "start": "1232760",
    "end": "1238200"
  },
  {
    "text": "functionalities are able to support from this Flint control",
    "start": "1238200",
    "end": "1243480"
  },
  {
    "text": "plane now let's move one step further into our story of Automation and cost",
    "start": "1244200",
    "end": "1251520"
  },
  {
    "text": "efficiency Flink application autoscaling is introduced in this flip 271 also with",
    "start": "1251520",
    "end": "1258520"
  },
  {
    "text": "major contribution from our group at Apple the main problem we want to solve",
    "start": "1258520",
    "end": "1263720"
  },
  {
    "text": "is actually when and how much to scale up or",
    "start": "1263720",
    "end": "1269200"
  },
  {
    "text": "down so we know when there are too much when there's too much resource rate it can be costly for users over time and",
    "start": "1269200",
    "end": "1275039"
  },
  {
    "text": "too few can cause the job to be unstable but it's hard to scale a Flink application in",
    "start": "1275039",
    "end": "1282520"
  },
  {
    "text": "place because um the application need to take a safe point right and then to stop",
    "start": "1282520",
    "end": "1288880"
  },
  {
    "text": "and resume from the previously taken state with a new configuration then there will be this",
    "start": "1288880",
    "end": "1295760"
  },
  {
    "text": "Associated cost especially when there's data backlog the data is waiting CU",
    "start": "1295760",
    "end": "1301400"
  },
  {
    "text": "there to be processed so in this case it require us",
    "start": "1301400",
    "end": "1306840"
  },
  {
    "text": "when making every scaling decision to consider if the backlog can be fully absorbed per user's configuration or",
    "start": "1306840",
    "end": "1314039"
  },
  {
    "text": "before the data retention time comes right",
    "start": "1314039",
    "end": "1319200"
  },
  {
    "text": "and how does Flink application Autos scaling work can we simply change let's say the",
    "start": "1319200",
    "end": "1326919"
  },
  {
    "text": "job parallelism of the entire application the answer is no why because",
    "start": "1326919",
    "end": "1333080"
  },
  {
    "text": "there can be workload distribution very differently across all the flank job",
    "start": "1333080",
    "end": "1339240"
  },
  {
    "text": "operators and our proposed solution is to change the parallelism of a job",
    "start": "1339240",
    "end": "1346960"
  },
  {
    "text": "vertex what is a job vertex is a group of chained operators that can be",
    "start": "1346960",
    "end": "1353000"
  },
  {
    "text": "executed together without data shuffling then our goal becomes to find",
    "start": "1353000",
    "end": "1360679"
  },
  {
    "text": "the minimum parallelism of each job vertex that can still ensure there is no",
    "start": "1360679",
    "end": "1368159"
  },
  {
    "text": "back pressure of the flank pipeline so here in this diagram we can",
    "start": "1368159",
    "end": "1375720"
  },
  {
    "text": "see previously everything is running normally right so",
    "start": "1375840",
    "end": "1381080"
  },
  {
    "text": "the data processing rate matches the input record rate and then suddenly our log ingestion",
    "start": "1381080",
    "end": "1388440"
  },
  {
    "text": "rate increases we can see in the middle the",
    "start": "1388440",
    "end": "1393919"
  },
  {
    "text": "map filter operators they cannot catch up right they are almost 100%",
    "start": "1393919",
    "end": "1400480"
  },
  {
    "text": "busy and now we mentioned about the back pressure propagating right so they're propagating the back pressure to their",
    "start": "1400480",
    "end": "1406760"
  },
  {
    "text": "Upstream sources and this sources tries to slow down reduce their throughput to make sure the",
    "start": "1406760",
    "end": "1412720"
  },
  {
    "text": "application can still run and then Autos scaling can come into",
    "start": "1412720",
    "end": "1418039"
  },
  {
    "text": "the rescue to calculate the scaling factors and the key here is that we want",
    "start": "1418039",
    "end": "1424760"
  },
  {
    "text": "to make sure we predict the effect of the",
    "start": "1424760",
    "end": "1430400"
  },
  {
    "text": "changing parallelism on the downstream job",
    "start": "1430400",
    "end": "1435640"
  },
  {
    "text": "vertices and now we get this scaling factors 5x 10x or",
    "start": "1435640",
    "end": "1442559"
  },
  {
    "text": "4X as we mentioned before our goal is to achieve back pressure free it's okay if",
    "start": "1442559",
    "end": "1448720"
  },
  {
    "text": "the operators are still busy but there's going to be no back back pressure because the incoming record rates can",
    "start": "1448720",
    "end": "1455679"
  },
  {
    "text": "now match the true data processing",
    "start": "1455679",
    "end": "1460320"
  },
  {
    "text": "rate okay so where do we implement this Flink Autos scalar the kubernetes",
    "start": "1462640",
    "end": "1467840"
  },
  {
    "text": "operator can be a natural place why because it has access to all the",
    "start": "1467840",
    "end": "1473000"
  },
  {
    "text": "deployment Matrix the way we make the predictions heavily relies on The Matrix",
    "start": "1473000",
    "end": "1478919"
  },
  {
    "text": "right we cannot use magic to predict also the fling kubernetes operator has is highly available and",
    "start": "1478919",
    "end": "1486399"
  },
  {
    "text": "able to reconfigure the deployment for its rescaling and the autoscaler itself",
    "start": "1486399",
    "end": "1492000"
  },
  {
    "text": "also needs to emit some metrics right to review to generate insights on why some",
    "start": "1492000",
    "end": "1497200"
  },
  {
    "text": "scaling dis decisions are",
    "start": "1497200",
    "end": "1500240"
  },
  {
    "text": "made okay besides autoscaling a Flink",
    "start": "1503799",
    "end": "1508840"
  },
  {
    "text": "application we also need to be resource aware of the cluster in",
    "start": "1508840",
    "end": "1514679"
  },
  {
    "text": "general so both cluster autoscaler and Carpenter can automatically SK",
    "start": "1514679",
    "end": "1520440"
  },
  {
    "text": "kubernetes cluster resource and for us we choose Carpenter",
    "start": "1520440",
    "end": "1525679"
  },
  {
    "text": "and we move from cluster Auto scalar to car penter because it can provide us the",
    "start": "1525679",
    "end": "1531600"
  },
  {
    "text": "flexibility to use a wide range of instance types available so we are no",
    "start": "1531600",
    "end": "1537360"
  },
  {
    "text": "longer bounded to just one instance type for a queue right we can specify five or",
    "start": "1537360",
    "end": "1542880"
  },
  {
    "text": "six of them it also has less limitations of the orchestration node group now it",
    "start": "1542880",
    "end": "1549559"
  },
  {
    "text": "can directly communicate to kubernetes note so the interval to retry can be",
    "start": "1549559",
    "end": "1556159"
  },
  {
    "text": "greatly reduced to millisecond seconds instead of minutes now this is extremely handy for our users to have the resource",
    "start": "1556159",
    "end": "1563360"
  },
  {
    "text": "they need in the cloud environment in order to help",
    "start": "1563360",
    "end": "1571440"
  },
  {
    "text": "Carpenter to get our cost bringing down right then it need to recycle the nodes",
    "start": "1571440",
    "end": "1578960"
  },
  {
    "text": "and to recycle the node there's a prerequisite there's no running PO on that note that we don't want to get rid",
    "start": "1578960",
    "end": "1585039"
  },
  {
    "text": "of right especially challenging for streaming ation because if we move those running pods around it can cause",
    "start": "1585039",
    "end": "1591760"
  },
  {
    "text": "potential disruptions to a streaming pipeline then what can we do here we",
    "start": "1591760",
    "end": "1597000"
  },
  {
    "text": "want to solve the problem from The Source right so when we first allocate",
    "start": "1597000",
    "end": "1603480"
  },
  {
    "text": "those parts those containers we want to put them into those nodes that are",
    "start": "1603480",
    "end": "1608600"
  },
  {
    "text": "already pretty busy instead of just finding a random note maybe there are two parts running there right we want to",
    "start": "1608600",
    "end": "1614399"
  },
  {
    "text": "find a Noe that maybe there are hundreds or thousands parts it's already running there so this is the ban packing we try",
    "start": "1614399",
    "end": "1621760"
  },
  {
    "text": "to out to a customize schedular alongside the kubernetes",
    "start": "1621760",
    "end": "1627240"
  },
  {
    "text": "Scher and to move the pods on the same node and have more empty nodes for the",
    "start": "1627240",
    "end": "1632279"
  },
  {
    "text": "carbon to to recycle and reduce our",
    "start": "1632279",
    "end": "1636399"
  },
  {
    "text": "cost okay so we are almost running out of time and we want to do a quick wrap",
    "start": "1638120",
    "end": "1644080"
  },
  {
    "text": "up so for our self-service platform right we put everything into this big",
    "start": "1644080",
    "end": "1649320"
  },
  {
    "text": "diagram that we mentioned so far so we have this user interface for the",
    "start": "1649320",
    "end": "1654480"
  },
  {
    "text": "experience and also the admin experience right and they will interact with the",
    "start": "1654480",
    "end": "1659559"
  },
  {
    "text": "Flint control play Maybe through mod client toolings and on this right hand side we",
    "start": "1659559",
    "end": "1665880"
  },
  {
    "text": "have all these giant boxes with the goodies like um the custom Schuler the",
    "start": "1665880",
    "end": "1671240"
  },
  {
    "text": "kubernetes operator right with Autos scaling implemented on it and the Flink",
    "start": "1671240",
    "end": "1676799"
  },
  {
    "text": "application system ET and we are happy to open for questions",
    "start": "1676799",
    "end": "1683639"
  },
  {
    "text": "[Applause]",
    "start": "1686480",
    "end": "1696200"
  },
  {
    "text": "okay oh so",
    "start": "1696200",
    "end": "1700320"
  },
  {
    "text": "how do you mean uh cluster upgrade yeah so that's a great question so we",
    "start": "1705200",
    "end": "1711000"
  },
  {
    "text": "mentioned before for example we have this uh automatic cluster migration right so we don't want to interrupt the",
    "start": "1711000",
    "end": "1717919"
  },
  {
    "text": "Flink applications basically we want to still keeping them healthy and redeploy",
    "start": "1717919",
    "end": "1723240"
  },
  {
    "text": "them into another cluster and after that we'll do the upgrade on the original cluster so basically they still need to",
    "start": "1723240",
    "end": "1730960"
  },
  {
    "text": "stop uh with the safe point but then with minimal time we'll have the control plane to automatically do the",
    "start": "1730960",
    "end": "1736720"
  },
  {
    "text": "redeployment uh if you are worried about a massive for example uh disruption to a lot of",
    "start": "1736720",
    "end": "1742760"
  },
  {
    "text": "applications you can do it batch in batch right or even one application at the beginning and then increase it by",
    "start": "1742760",
    "end": "1748919"
  },
  {
    "text": "like uh some index numbers yeah",
    "start": "1748919",
    "end": "1754960"
  },
  {
    "text": "okay yeah that's a good question so we don't use service match so for that one it's only a like a simple authentication",
    "start": "1759440",
    "end": "1766240"
  },
  {
    "text": "right so we use like gway service embedded there and to cause some identity Checker",
    "start": "1766240",
    "end": "1773919"
  },
  {
    "text": "yeah go",
    "start": "1774120",
    "end": "1777200"
  },
  {
    "text": "ahead yeah that's a definitely a great question so uh yeah so at this moment we",
    "start": "1782720",
    "end": "1788720"
  },
  {
    "text": "are running uh GPU machines as well for both our streaming applications and also the offline applications yeah and they",
    "start": "1788720",
    "end": "1795519"
  },
  {
    "text": "also fit into this MTI tendency story that we just shared um so ideally you",
    "start": "1795519",
    "end": "1802480"
  },
  {
    "text": "can have a different mix of resources right it can be a dedicated GPU Q if you want it can also mix with CPU resource",
    "start": "1802480",
    "end": "1810080"
  },
  {
    "text": "and it's actually more ideal because sometimes uh certain data workflows they are more suitable for CPU processing and",
    "start": "1810080",
    "end": "1816440"
  },
  {
    "text": "for some maybe for ML workflows you can Leverage The gpq",
    "start": "1816440",
    "end": "1822039"
  },
  {
    "text": "yeah okay great engineering um experience",
    "start": "1823360",
    "end": "1829799"
  },
  {
    "text": "that you have shared with us so I have a question like you know when I like you",
    "start": "1829799",
    "end": "1835039"
  },
  {
    "text": "know you referen about Carpenter and the carpenter uh cluster autoscaler is kind",
    "start": "1835039",
    "end": "1842120"
  },
  {
    "text": "of proprietary to AWS uh managed kubernetes service so in",
    "start": "1842120",
    "end": "1847679"
  },
  {
    "text": "case you are using your manage kubernetes Service uh on AWS so what",
    "start": "1847679",
    "end": "1853679"
  },
  {
    "text": "prevented you to adopt kyes instead of like building this huge engineering uh",
    "start": "1853679",
    "end": "1861399"
  },
  {
    "text": "initiative oh yeah great question yeah so uh as mentioned so",
    "start": "1861399",
    "end": "1867039"
  },
  {
    "text": "Carpenter and cluster autoscar they well first both have open source versions right so we are just free to use them",
    "start": "1867039",
    "end": "1874799"
  },
  {
    "text": "and uh B asking since we're already leveraging the hardware resource from the cloud providers why don't we just",
    "start": "1874799",
    "end": "1881799"
  },
  {
    "text": "use maybe the uh AWS kesis service the manag service yeah uh for in our case",
    "start": "1881799",
    "end": "1888840"
  },
  {
    "text": "because um we are moving to a very large scale and a lot of users have this",
    "start": "1888840",
    "end": "1893960"
  },
  {
    "text": "customized needs it can be hard to handle by uh like the a managed service",
    "start": "1893960",
    "end": "1899799"
  },
  {
    "text": "so we want to have the kind of the control to help our users to build some customiz solutions and for some of them",
    "start": "1899799",
    "end": "1906720"
  },
  {
    "text": "they want the service for example the entire service to deploy to their own AWS account and for some they want to",
    "start": "1906720",
    "end": "1912760"
  },
  {
    "text": "want it to be shared right and also the cost can be a concern as well if you have so many workloads uh then the way",
    "start": "1912760",
    "end": "1921120"
  },
  {
    "text": "that we calculate the cost etc for managed service it can be very",
    "start": "1921120",
    "end": "1926679"
  },
  {
    "text": "expensive",
    "start": "1926679",
    "end": "1929679"
  },
  {
    "text": "okay yeah so for a Flink cluster it doesn't spend to multiple kubernetes",
    "start": "1941519",
    "end": "1946760"
  },
  {
    "text": "cluster it will just sit it into one cluster and if one cluster goes down right so we mentioned there can be a",
    "start": "1946760",
    "end": "1952440"
  },
  {
    "text": "persistent layer for our control plane to manage so basically we want to save all the spec of the Flink deployments",
    "start": "1952440",
    "end": "1960240"
  },
  {
    "text": "and also their save point information it can be saved into for example S3 Etc so",
    "start": "1960240",
    "end": "1965559"
  },
  {
    "text": "it's okay if the cluster goes down we have a way to bring up all the Flink deployments back as soon as possible or",
    "start": "1965559",
    "end": "1970960"
  },
  {
    "text": "migrate them to another cluster okay so we are running out of",
    "start": "1970960",
    "end": "1976080"
  },
  {
    "text": "time yeah thank thank you",
    "start": "1976080",
    "end": "1984000"
  }
]